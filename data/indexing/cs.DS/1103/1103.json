[{"id": "1103.0040", "submitter": "Shaddin Dughmi", "authors": "Shaddin Dughmi, Tim Roughgarden, Qiqi Yan", "title": "From Convex Optimization to Randomized Mechanisms: Toward Optimal\n  Combinatorial Auctions", "comments": "Extended abstract appears in Proceedings of the 43rd ACM Symposium on\n  Theory of Computing (STOC), 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design an expected polynomial-time, truthful-in-expectation,\n(1-1/e)-approximation mechanism for welfare maximization in a fundamental class\nof combinatorial auctions. Our results apply to bidders with valuations that\nare m matroid rank sums (MRS), which encompass most concrete examples of\nsubmodular functions studied in this context, including coverage functions,\nmatroid weighted-rank functions, and convex combinations thereof. Our\napproximation factor is the best possible, even for known and explicitly given\ncoverage valuations, assuming P != NP. Ours is the first\ntruthful-in-expectation and polynomial-time mechanism to achieve a\nconstant-factor approximation for an NP-hard welfare maximization problem in\ncombinatorial auctions with heterogeneous goods and restricted valuations.\n  Our mechanism is an instantiation of a new framework for designing\napproximation mechanisms based on randomized rounding algorithms. A typical\nsuch algorithm first optimizes over a fractional relaxation of the original\nproblem, and then randomly rounds the fractional solution to an integral one.\nWith rare exceptions, such algorithms cannot be converted into truthful\nmechanisms. The high-level idea of our mechanism design framework is to\noptimize directly over the (random) output of the rounding algorithm, rather\nthan over the input to the rounding algorithm. This approach leads to\ntruthful-in-expectation mechanisms, and these mechanisms can be implemented\nefficiently when the corresponding objective function is concave. For bidders\nwith MRS valuations, we give a novel randomized rounding algorithm that leads\nto both a concave objective function and a (1-1/e)-approximation of the optimal\nwelfare.\n", "versions": [{"version": "v1", "created": "Mon, 28 Feb 2011 22:18:01 GMT"}, {"version": "v2", "created": "Sun, 6 Mar 2011 21:05:13 GMT"}, {"version": "v3", "created": "Sat, 16 Apr 2011 07:19:37 GMT"}], "update_date": "2011-04-19", "authors_parsed": [["Dughmi", "Shaddin", ""], ["Roughgarden", "Tim", ""], ["Yan", "Qiqi", ""]]}, {"id": "1103.0041", "submitter": "Shaddin Dughmi", "authors": "Shaddin Dughmi", "title": "A Truthful Randomized Mechanism for Combinatorial Public Projects via\n  Convex Optimization", "comments": "Extended abstract appears in Proceedings of the 12th ACM Conference\n  on Electronic Commerce (EC), 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Combinatorial Public Projects, there is a set of projects that may be\nundertaken, and a set of self-interested players with a stake in the set of\nprojects chosen. A public planner must choose a subset of these projects,\nsubject to a resource constraint, with the goal of maximizing social welfare.\nCombinatorial Public Projects has emerged as one of the paradigmatic problems\nin Algorithmic Mechanism Design, a field concerned with solving fundamental\nresource allocation problems in the presence of both selfish behavior and the\ncomputational constraint of polynomial-time.\n  We design a polynomial-time, truthful-in-expectation, (1-1/e)-approximation\nmechanism for welfare maximization in a fundamental variant of combinatorial\npublic projects. Our results apply to combinatorial public projects when\nplayers have valuations that are matroid rank sums (MRS), which encompass most\nconcrete examples of submodular functions studied in this context, including\ncoverage functions, matroid weighted-rank functions, and convex combinations\nthereof. Our approximation factor is the best possible, assuming P != NP. Ours\nis the first mechanism that achieves a constant factor approximation for a\nnatural NP-hard variant of combinatorial public projects.\n", "versions": [{"version": "v1", "created": "Mon, 28 Feb 2011 22:18:57 GMT"}, {"version": "v2", "created": "Sat, 16 Apr 2011 07:30:42 GMT"}], "update_date": "2011-04-19", "authors_parsed": [["Dughmi", "Shaddin", ""]]}, {"id": "1103.0106", "submitter": "Ali Pinar", "authors": "Enver Kayaaslan, Ali Pinar, Umit V. Catalyurek, and Cevdet Aykanat", "title": "Hypergraph Partitioning through Vertex Separators on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modeling flexibility provided by hypergraphs has drawn a lot of interest\nfrom the combinatorial scientific community, leading to novel models and\nalgorithms, their applications, and development of associated tools.\nHypergraphs are now a standard tool in combinatorial scientific computing. The\nmodeling flexibility of hypergraphs however, comes at a cost: algorithms on\nhypergraphs are inherently more complicated than those on graphs, which\nsometimes translate to nontrivial increases in processing times. Neither the\nmodeling flexibility of hypergraphs, nor the runtime efficiency of graph\nalgorithms can be overlooked. Therefore, the new research thrust should be how\nto cleverly trade-off between the two. This work addresses one method for this\ntrade-off by solving the hypergraph partitioning problem by finding vertex\nseparators on graphs. Specifically, we investigate how to solve the hypergraph\npartitioning problem by seeking a vertex separator on its net intersection\ngraph (NIG), where each net of the hypergraph is represented by a vertex, and\ntwo vertices share an edge if their nets have a common vertex. We propose a\nvertex-weighting scheme to attain good node-balanced hypergraphs, since NIG\nmodel cannot preserve node balancing information. Vertex-removal and\nvertex-splitting techniques are described to optimize cutnet and connectivity\nmetrics, respectively, under the recursive bipartitioning paradigm. We also\ndeveloped an implementation for our GPVS-based HP formulations by adopting and\nmodifying a state-of-the-art GPVS tool onmetis. Experiments conducted on a\nlarge collection of sparse matrices confirmed the validity of our proposed\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2011 08:42:24 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Kayaaslan", "Enver", ""], ["Pinar", "Ali", ""], ["Catalyurek", "Umit V.", ""], ["Aykanat", "Cevdet", ""]]}, {"id": "1103.0116", "submitter": "Remous-Aris Koutsiamanis", "authors": "Remous-Aris Koutsiamanis, Pavlos S. Efraimidis", "title": "An exact and O(1) time heaviest and lightest hitters algorithm for\n  sliding-window data streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we focus on the problem of finding the heaviest-k and lightest-k\nhitters in a sliding window data stream. The most recent research endeavours\nhave yielded an epsilon-approximate algorithm with update operations in\nconstant time with high probability and O(1/epsilon) query time for the\nheaviest hitters case. We propose a novel algorithm which for the first time,\nto our knowledge, provides exact, not approximate, results while at the same\ntime achieves O(1) time with high probability complexity on both update and\nquery operations. Furthermore, our algorithm is able to provide both the\nheaviest-k and the lightest-k hitters at the same time without any overhead. In\nthis work, we describe the algorithm and the accompanying data structure that\nsupports it and perform quantitative experiments with synthetic data to verify\nour theoretical predictions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2011 09:42:01 GMT"}], "update_date": "2011-03-02", "authors_parsed": [["Koutsiamanis", "Remous-Aris", ""], ["Efraimidis", "Pavlos S.", ""]]}, {"id": "1103.0260", "submitter": "Ali Pinar", "authors": "Ekow Otoo, Ali Pinar, Doron Rotem", "title": "A Linear Approximation Algorithm for 2-Dimensional Vector Packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the 2-dimensional vector packing problem, which is a generalization\nof the classical bin packing problem where each item has 2 distinct weights and\neach bin has 2 corresponding capacities. The goal is to group items into\nminimum number of bins, without violating the bin capacity constraints. We\npropose a \\Theta}(n)-time approximation algorithm that is inspired by the\nO(n^2) algorithm proposed by Chang, Hwang, and Park.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2011 20:04:45 GMT"}], "update_date": "2011-03-02", "authors_parsed": [["Otoo", "Ekow", ""], ["Pinar", "Ali", ""], ["Rotem", "Doron", ""]]}, {"id": "1103.0318", "submitter": "Darren Strash", "authors": "David Eppstein and Darren Strash", "title": "Listing All Maximal Cliques in Large Sparse Real-World Graphs", "comments": "12 pages, 4 figures. To appear at the 10th International Symposium on\n  Experimental Algorithms (SEA 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implement a new algorithm for listing all maximal cliques in sparse graphs\ndue to Eppstein, L\\\"offler, and Strash (ISAAC 2010) and analyze its performance\non a large corpus of real-world graphs. Our analysis shows that this algorithm\nis the first to offer a practical solution to listing all maximal cliques in\nlarge sparse graphs. All other theoretically-fast algorithms for sparse graphs\nhave been shown to be significantly slower than the algorithm of Tomita et al.\n(Theoretical Computer Science, 2006) in practice. However, the algorithm of\nTomita et al. uses an adjacency matrix, which requires too much space for large\nsparse graphs. Our new algorithm opens the door for fast analysis of large\nsparse graphs whose adjacency matrix will not fit into working memory.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2011 00:00:26 GMT"}], "update_date": "2011-03-03", "authors_parsed": [["Eppstein", "David", ""], ["Strash", "Darren", ""]]}, {"id": "1103.0337", "submitter": "Duc-Phong Le", "authors": "Duc-Phong Le and Chao-Liang Liu", "title": "Refinements of Miller's Algorithm over Weierstrass Curves Revisited", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1986 Victor Miller described an algorithm for computing the Weil pairing\nin his unpublished manuscript. This algorithm has then become the core of all\npairing-based cryptosystems. Many improvements of the algorithm have been\npresented. Most of them involve a choice of elliptic curves of a \\emph{special}\nforms to exploit a possible twist during Tate pairing computation. Other\nimprovements involve a reduction of the number of iterations in the Miller's\nalgorithm. For the generic case, Blake, Murty and Xu proposed three refinements\nto Miller's algorithm over Weierstrass curves. Though their refinements which\nonly reduce the total number of vertical lines in Miller's algorithm, did not\ngive an efficient computation as other optimizations, but they can be applied\nfor computing \\emph{both} of Weil and Tate pairings on \\emph{all}\npairing-friendly elliptic curves. In this paper we extend the Blake-Murty-Xu's\nmethod and show how to perform an elimination of all vertical lines in Miller's\nalgorithm during Weil/Tate pairings computation on \\emph{general} elliptic\ncurves. Experimental results show that our algorithm is faster about 25% in\ncomparison with the original Miller's algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2011 02:50:33 GMT"}], "update_date": "2011-03-03", "authors_parsed": [["Le", "Duc-Phong", ""], ["Liu", "Chao-Liang", ""]]}, {"id": "1103.0368", "submitter": "Ali Pinar", "authors": "Matthew Rocklin and Ali Pinar", "title": "Computing an Aggregate Edge-Weight Function for Clustering Graphs with\n  Multiple Edge Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the community detection problem on graphs in the existence of\nmultiple edge types. Our main motivation is that similarity between objects can\nbe defined by many different metrics and aggregation of these metrics into a\nsingle one poses several important challenges, such as recovering this\naggregation function from ground-truth, investigating the space of different\nclusterings, etc. In this paper, we address how to find an aggregation function\nto generate a composite metric that best resonates with the ground-truth. We\ndescribe two approaches: solving an inverse problem where we try to find\nparameters that generate a graph whose clustering gives the ground-truth\nclustering, and choosing parameters to maximize the quality of the ground-truth\nclustering. We present experimental results on real and synthetic benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2011 09:13:03 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2011 20:35:24 GMT"}], "update_date": "2011-03-16", "authors_parsed": [["Rocklin", "Matthew", ""], ["Pinar", "Ali", ""]]}, {"id": "1103.0534", "submitter": "Marek Cygan", "authors": "Marek Cygan and Jesper Nederlof and Marcin Pilipczuk and Micha{\\l}\n  Pilipczuk and Johan van Rooij and Jakub Onufry Wojtaszczyk", "title": "Solving connectivity problems parameterized by treewidth in single\n  exponential time", "comments": "89 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the vast majority of local graph problems standard dynamic programming\ntechniques give c^tw V^O(1) algorithms, where tw is the treewidth of the input\ngraph. On the other hand, for problems with a global requirement (usually\nconnectivity) the best-known algorithms were naive dynamic programming schemes\nrunning in tw^O(tw) V^O(1) time.\n  We breach this gap by introducing a technique we dubbed Cut&Count that allows\nto produce c^tw V^O(1) Monte Carlo algorithms for most connectivity-type\nproblems, including Hamiltonian Path, Feedback Vertex Set and Connected\nDominating Set, consequently answering the question raised by Lokshtanov, Marx\nand Saurabh [SODA'11] in a surprising way. We also show that (under reasonable\ncomplexity assumptions) the gap cannot be breached for some problems for which\nCut&Count does not work, like CYCLE PACKING.\n  The constant c we obtain is in all cases small (at most 4 for undirected\nproblems and at most 6 for directed ones), and in several cases we are able to\nshow that improving those constants would cause the Strong Exponential Time\nHypothesis to fail.\n  Our results have numerous consequences in various fields, like FPT\nalgorithms, exact and approximate algorithms on planar and H-minor-free graphs\nand algorithms on graphs of bounded degree. In all these fields we are able to\nimprove the best-known results for some problems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2011 20:36:20 GMT"}], "update_date": "2011-03-03", "authors_parsed": [["Cygan", "Marek", ""], ["Nederlof", "Jesper", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""], ["van Rooij", "Johan", ""], ["Wojtaszczyk", "Jakub Onufry", ""]]}, {"id": "1103.0842", "submitter": "Aleksandrs Belovs", "authors": "Aleksandrs Belovs", "title": "Span-program-based quantum algorithm for the rank problem", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, span programs have been shown to be equivalent to quantum query\nalgorithms. It is an open problem whether this equivalence can be utilized in\norder to come up with new quantum algorithms. We address this problem by\nproviding span programs for some linear algebra problems.\n  We develop a notion of a high level span program, that abstracts from loading\ninput vectors into a span program. Then we give a high level span program for\nthe rank problem. The last section of the paper deals with reducing a high\nlevel span program to an ordinary span program that can be solved using known\nquantum query algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2011 08:15:34 GMT"}], "update_date": "2011-03-07", "authors_parsed": [["Belovs", "Aleksandrs", ""]]}, {"id": "1103.0985", "submitter": "Viswanath Nagarajan", "authors": "Inge Li Goertz and Viswanath Nagarajan", "title": "Locating Depots for Capacitated Vehicle Routing", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a location-routing problem in the context of capacitated vehicle\nrouting. The input is a set of demand locations in a metric space and a fleet\nof k vehicles each of capacity Q. The objective is to locate k depots, one for\neach vehicle, and compute routes for the vehicles so that all demands are\nsatisfied and the total cost is minimized. Our main result is a constant-factor\napproximation algorithm for this problem. To achieve this result, we reduce to\nthe k-median-forest problem, which generalizes both k-median and minimum\nspanning tree, and which might be of independent interest. We give a\n(3+c)-approximation algorithm for k-median-forest, which leads to a\n(12+c)-approximation algorithm for the above location-routing problem, for any\nconstant c>0. The algorithm for k-median-forest is just t-swap local search,\nand we prove that it has locality gap 3+2/t; this generalizes the corresponding\nresult known for k-median. Finally we consider the \"non-uniform\"\nk-median-forest problem which has different cost functions for the MST and\nk-median parts. We show that the locality gap for this problem is unbounded\neven under multi-swaps, which contrasts with the uniform case. Nevertheless, we\nobtain a constant-factor approximation algorithm, using an LP based approach.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2011 21:15:44 GMT"}], "update_date": "2011-03-08", "authors_parsed": [["Goertz", "Inge Li", ""], ["Nagarajan", "Viswanath", ""]]}, {"id": "1103.0995", "submitter": "Christos Boutsidis", "authors": "Christos Boutsidis, Petros Drineas, Malik Magdon-Ismail", "title": "Near-Optimal Column-Based Matrix Reconstruction", "comments": "SIAM Journal on Computing (SICOMP), invited to special issue of FOCS\n  2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider low-rank reconstruction of a matrix using its columns and we\npresent asymptotically optimal algorithms for both spectral norm and Frobenius\nnorm reconstruction. The main tools we introduce to obtain our r esults are:\n(i) the use of fast approximate SVD-like decompositions for column\nreconstruction, and (ii) two deter ministic algorithms for selecting rows from\nmatrices with orthonormal columns, building upon the sparse represen tation\ntheorem for decompositions of the identity that appeared in \\cite{BSS09}.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2011 23:50:35 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2012 18:08:22 GMT"}, {"version": "v3", "created": "Fri, 21 Jun 2013 20:49:06 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Boutsidis", "Christos", ""], ["Drineas", "Petros", ""], ["Magdon-Ismail", "Malik", ""]]}, {"id": "1103.1091", "submitter": "J\\'an Katreni\\v{c}", "authors": "J\\'an Katrenic and Gabriel Semanisin", "title": "A generalization of Hopcroft-Karp algorithm for semi-matchings and\n  covers in bipartite graphs (Maximum semi-matching problem in bipartite\n  graphs)", "comments": null, "journal-ref": "J\\'an Katrenic, Gabriel Semanisin: Maximum semi-matching problem\n  in bipartite graphs. Discussiones Mathematicae Graph Theory 33(3): 559-569\n  (2013)", "doi": "10.7151/dmgt.1694", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $(f,g)$-semi-matching in a bipartite graph $G=(U \\cup V,E)$ is a set of\nedges $M \\subseteq E$ such that each vertex $u\\in U$ is incident with at most\n$f(u)$ edges of $M$, and each vertex $v\\in V$ is incident with at most $g(v)$\nedges of $M$. In this paper we give an algorithm that for a graph with $n$\nvertices and $m$ edges, $n\\le m$, constructs a maximum $(f,g)$-semi-matching in\nrunning time $O(m\\cdot \\min (\\sqrt{\\sum_{u\\in U}f(u)}, \\sqrt{\\sum_{v\\in\nV}g(v)}))$. Using the reduction of [5], our result on maximum\n$(f,g)$-semi-matching problem directly implies an algorithm for the optimal\nsemi-matching problem with running time $O(\\sqrt{n}m \\log n)$.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2011 00:54:03 GMT"}, {"version": "v2", "created": "Tue, 8 May 2012 18:31:37 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 05:59:45 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Katrenic", "J\u00e1n", ""], ["Semanisin", "Gabriel", ""]]}, {"id": "1103.1109", "submitter": "Manoj Gupta", "authors": "Surender Baswana, Manoj Gupta, Sandeep Sen", "title": "Fully dynamic maximal matching in O(log n) update time", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for maintaining maximal matching in a graph under\naddition and deletion of edges. Our data structure is randomized that takes\nO(log n) expected amortized time for each edge update where n is the number of\nvertices in the graph. While there is a trivial O(n) algorithm for edge update,\nthe previous best known result for this problem for a graph with n vertices and\nm edges is O({(n+ m)}^{0.7072})which is sub-linear only for a sparse graph.\n  For the related problem of maximum matching, Onak and Rubinfield designed a\nrandomized data structure that achieves O(log^2 n) amortized time for each\nupdate for maintaining a c-approximate maximum matching for some large constant\nc. In contrast, we can maintain a factor two approximate maximum matching in\nO(log n) expected time per update as a direct corollary of the maximal matching\nscheme. This in turn also implies a two approximate vertex cover maintenance\nscheme that takes O(log n) expected time per update.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2011 08:38:10 GMT"}, {"version": "v2", "created": "Sun, 15 Apr 2012 06:13:50 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2013 12:01:17 GMT"}, {"version": "v4", "created": "Tue, 2 Aug 2016 14:27:48 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Baswana", "Surender", ""], ["Gupta", "Manoj", ""], ["Sen", "Sandeep", ""]]}, {"id": "1103.1264", "submitter": "Leo Liberti", "authors": "Leo Liberti, Carlile Lavor, Benoit Masson, Antonio Mucherino", "title": "Polynomial cases of the Discretizable Molecular Distance Geometry\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CE cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important application of distance geometry to biochemistry studies the\nembeddings of the vertices of a weighted graph in the three-dimensional\nEuclidean space such that the edge weights are equal to the Euclidean distances\nbetween corresponding point pairs. When the graph represents the backbone of a\nprotein, one can exploit the natural vertex order to show that the search space\nfor feasible embeddings is discrete. The corresponding decision problem can be\nsolved using a binary tree based search procedure which is exponential in the\nworst case. We discuss assumptions that bound the search tree width to a\npolynomial size.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2011 12:41:44 GMT"}], "update_date": "2011-03-08", "authors_parsed": [["Liberti", "Leo", ""], ["Lavor", "Carlile", ""], ["Masson", "Benoit", ""], ["Mucherino", "Antonio", ""]]}, {"id": "1103.1503", "submitter": "Gunnar W. Klau", "authors": "Stefan Canzar and Nora C. Toussaint and Gunnar W. Klau", "title": "An Exact Algorithm for Side-Chain Placement in Protein Design", "comments": null, "journal-ref": null, "doi": "10.1007/s11590-011-0308-0", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational protein design aims at constructing novel or improved functions\non the structure of a given protein backbone and has important applications in\nthe pharmaceutical and biotechnical industry. The underlying combinatorial\nside-chain placement problem consists of choosing a side-chain placement for\neach residue position such that the resulting overall energy is minimum. The\nchoice of the side-chain then also determines the amino acid for this position.\nMany algorithms for this NP-hard problem have been proposed in the context of\nhomology modeling, which, however, reach their limits when faced with large\nprotein design instances.\n  In this paper, we propose a new exact method for the side-chain placement\nproblem that works well even for large instance sizes as they appear in protein\ndesign. Our main contribution is a dedicated branch-and-bound algorithm that\ncombines tight upper and lower bounds resulting from a novel Lagrangian\nrelaxation approach for side-chain placement. Our experimental results show\nthat our method outperforms alternative state-of-the art exact approaches and\nmakes it possible to optimally solve large protein design instances routinely.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2011 12:32:12 GMT"}], "update_date": "2011-03-29", "authors_parsed": [["Canzar", "Stefan", ""], ["Toussaint", "Nora C.", ""], ["Klau", "Gunnar W.", ""]]}, {"id": "1103.1542", "submitter": "P\\'aid\\'i Creed", "authors": "David A. Cohen, Martin C. Cooper, P\\'aid\\'i Creed, Andr\\'as Z. Salamon", "title": "The tractability of CSP classes defined by forbidden patterns", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 45, pages\n  47-78, 2012", "doi": "10.1613/jair.3651", "report-no": null, "categories": "cs.AI cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constraint satisfaction problem (CSP) is a general problem central to\ncomputer science and artificial intelligence. Although the CSP is NP-hard in\ngeneral, considerable effort has been spent on identifying tractable\nsubclasses. The main two approaches consider structural properties\n(restrictions on the hypergraph of constraint scopes) and relational properties\n(restrictions on the language of constraint relations). Recently, some authors\nhave considered hybrid properties that restrict the constraint hypergraph and\nthe relations simultaneously.\n  Our key contribution is the novel concept of a CSP pattern and classes of\nproblems defined by forbidden patterns (which can be viewed as forbidding\ngeneric subproblems). We describe the theoretical framework which can be used\nto reason about classes of problems defined by forbidden patterns. We show that\nthis framework generalises relational properties and allows us to capture known\nhybrid tractable classes.\n  Although we are not close to obtaining a dichotomy concerning the\ntractability of general forbidden patterns, we are able to make some progress\nin a special case: classes of problems that arise when we can only forbid\nbinary negative patterns (generic subproblems in which only inconsistent tuples\nare specified). In this case we are able to characterise very large classes of\ntractable and NP-hard forbidden patterns. This leaves the complexity of just\none case unresolved and we conjecture that this last case is tractable.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2011 14:43:17 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 17:38:33 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Cohen", "David A.", ""], ["Cooper", "Martin C.", ""], ["Creed", "P\u00e1id\u00ed", ""], ["Salamon", "Andr\u00e1s Z.", ""]]}, {"id": "1103.1771", "submitter": "Dennis Schieferdecker", "authors": "Dennis Schieferdecker, Markus V\\\"olker, Dorothea Wagner", "title": "Efficient Algorithms for Distributed Detection of Holes and Boundaries\n  in Wireless Networks", "comments": "extended version of accepted submission to SEA 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two novel algorithms for distributed and location-free boundary\nrecognition in wireless sensor networks. Both approaches enable a node to\ndecide autonomously whether it is a boundary node, based solely on connectivity\ninformation of a small neighborhood. This makes our algorithms highly\napplicable for dynamic networks where nodes can move or become inoperative.\n  We compare our algorithms qualitatively and quantitatively with several\nprevious approaches. In extensive simulations, we consider various models and\nscenarios. Although our algorithms use less information than most other\napproaches, they produce significantly better results. They are very robust\nagainst variations in node degree and do not rely on simplified assumptions of\nthe communication model. Moreover, they are much easier to implement on real\nsensor nodes than most existing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2011 13:08:13 GMT"}], "update_date": "2011-03-10", "authors_parsed": [["Schieferdecker", "Dennis", ""], ["V\u00f6lker", "Markus", ""], ["Wagner", "Dorothea", ""]]}, {"id": "1103.1917", "submitter": "Francisco Soulignac", "authors": "Martiniano Egu\\'ia and Francisco J. Soulignac", "title": "Hereditary biclique-Helly graphs: recognition and maximal biclique\n  enumeration", "comments": "23 pages, 4 figures", "journal-ref": "Discrete Math. Theor. Comput. Sci. 15 (2013), 55--74", "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A biclique is a set of vertices that induce a bipartite complete graph. A\ngraph G is biclique-Helly when its family of maximal bicliques satisfies the\nHelly property. If every induced subgraph of G is also biclique-Helly, then G\nis hereditary biclique-Helly. A graph is C_4-dominated when every cycle of\nlength 4 contains a vertex that is dominated by the vertex of the cycle that is\nnot adjacent to it. In this paper we show that the class of hereditary\nbiclique-Helly graphs is formed precisely by those C_4-dominated graphs that\ncontain no triangles and no induced cycles of length either 5, or 6. Using this\ncharacterization, we develop an algorithm for recognizing hereditary\nbiclique-Helly graphs in O(n^2+\\alpha m) time and O(m) space. (Here n, m, and\n\\alpha = O(m^{1/2}) are the number of vertices and edges, and the arboricity of\nthe graph, respectively.) As a subprocedure, we show how to recognize those\nC_4-dominated graphs that contain no triangles in O(\\alpha m) time and O(m)\nspace. Finally, we show how to enumerate all the maximal bicliques of a\nC_4-dominated graph with no triangles in O(n^2 + \\alpha m) time and O(\\alpha m)\nspace, and we discuss how some biclique problems can be solved in O(\\alpha m)\ntime and O(n+m) space.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2011 23:28:47 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Egu\u00eda", "Martiniano", ""], ["Soulignac", "Francisco J.", ""]]}, {"id": "1103.2102", "submitter": "Magnus Wahlstr\\\"om", "authors": "Michael Gnewuch and Magnus Wahlstr\\\"om and Carola Winzen", "title": "A Randomized Algorithm Based on Threshold Accepting to Approximate the\n  Star Discrepancy", "comments": null, "journal-ref": "SIAM Journal of Numerical Analysis 50, 781-807, 2012", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for estimating the star discrepancy of arbitrary\npoint sets. Similar to the algorithm for discrepancy approximation of Winker\nand Fang [SIAM J. Numer. Anal. 34 (1997), 2028--2042] it is based on the\noptimization algorithm threshold accepting. Our improvements include, amongst\nothers, a non-uniform sampling strategy which is more suited for\nhigher-dimensional inputs, and rounding steps which transform axis-parallel\nboxes, on which the discrepancy is to be tested, into \\emph{critical test\nboxes}. These critical test boxes provably yield higher discrepancy values, and\ncontain the box that exhibits the maximum value of the local discrepancy. We\nprovide comprehensive experiments to test the new algorithm. Our randomized\nalgorithm computes the exact discrepancy frequently in all cases where this can\nbe checked (i.e., where the exact discrepancy of the point set can be computed\nin feasible time). Most importantly, in higher dimension the new method behaves\nclearly better than all previously known methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2011 18:50:01 GMT"}, {"version": "v2", "created": "Thu, 12 May 2011 09:30:28 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Gnewuch", "Michael", ""], ["Wahlstr\u00f6m", "Magnus", ""], ["Winzen", "Carola", ""]]}, {"id": "1103.2165", "submitter": "Timon Hertli", "authors": "Timon Hertli", "title": "3-SAT Faster and Simpler - Unique-SAT Bounds for PPSZ Hold in General", "comments": "12 pages, no figures; critical variables are now called frozen, added\n  reference to Makino et al., shortened some proofs and fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PPSZ algorithm by Paturi, Pudl\\'ak, Saks, and Zane [1998] is the fastest\nknown algorithm for Unique k-SAT, where the input formula does not have more\nthan one satisfying assignment. For k>=5 the same bounds hold for general\nk-SAT. We show that this is also the case for k=3,4, using a slightly modified\nPPSZ algorithm. We do the analysis by defining a cost for satisfiable CNF\nformulas, which we prove to decrease in each PPSZ step by a certain amount.\nThis improves our previous best bounds with Moser and Scheder [2011] for 3-SAT\nto O(1.308^n) and for 4-SAT to O(1.469^n).\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2011 23:18:35 GMT"}, {"version": "v2", "created": "Thu, 5 May 2011 12:37:10 GMT"}], "update_date": "2011-05-06", "authors_parsed": [["Hertli", "Timon", ""]]}, {"id": "1103.2167", "submitter": "Djamal Belazzougui", "authors": "Djamal Belazzougui", "title": "Improved space-time tradeoffs for approximate full-text indexing with\n  one edit error", "comments": "Accepted for publication in a journal (28 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are interested in indexing texts for substring matching\nqueries with one edit error. That is, given a text $T$ of $n$ characters over\nan alphabet of size $\\sigma$, we are asked to build a data structure that\nanswers the following query: find all the $occ$ substrings of the text that are\nat edit distance at most $1$ from a given string $q$ of length $m$. In this\npaper we show two new results for this problem. The first result, suitable for\nan unbounded alphabet, uses $O(n\\log^\\epsilon n)$ (where $\\epsilon$ is any\nconstant such that $0<\\epsilon<1$) words of space and answers to queries in\ntime $O(m+occ)$. This improves simultaneously in space and time over the result\nof Cole et al. The second result, suitable only for a constant alphabet, relies\non compressed text indices and comes in two variants: the first variant uses\n$O(n\\log^{\\epsilon} n)$ bits of space and answers to queries in time\n$O(m+occ)$, while the second variant uses $O(n\\log\\log n)$ bits of space and\nanswers to queries in time $O((m+occ)\\log\\log n)$. This second result improves\non the previously best results for constant alphabets achieved in Lam et al.\n(Algorithmica 2008) and Chan et al. (Algorithmica 2010).\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2011 23:25:45 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2013 00:56:34 GMT"}, {"version": "v3", "created": "Thu, 21 Aug 2014 21:28:27 GMT"}], "update_date": "2014-08-25", "authors_parsed": [["Belazzougui", "Djamal", ""]]}, {"id": "1103.2275", "submitter": "Marek Cygan", "authors": "Marek Cygan and {\\L}ukasz Kowalik", "title": "Channel Assignment via Fast Zeta Transform", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show an O*((l+1)^n)-time algorithm for the channel assignment problem,\nwhere l is the maximum edge weight. This improves on the previous\nO*((l+2)^n)-time algorithm by Kral, as well as algorithms for important special\ncases, like L(2,1)-labelling. For the latter problem, our algorithm works in\nO*(3^n) time. The progress is achieved by applying the fast zeta transform in\ncombination with the inclusion-exclusion principle.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2011 14:12:12 GMT"}], "update_date": "2011-03-14", "authors_parsed": [["Cygan", "Marek", ""], ["Kowalik", "\u0141ukasz", ""]]}, {"id": "1103.2409", "submitter": "Bolin Ding", "authors": "Bolin Ding (UIUC), Arnd Christian K\\\"onig (Microsoft Research)", "title": "Fast Set Intersection in Memory", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 4, pp.\n  255-266 (2011)", "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set intersection is a fundamental operation in information retrieval and\ndatabase systems. This paper introduces linear space data structures to\nrepresent sets such that their intersection can be computed in a worst-case\nefficient way. In general, given k (preprocessed) sets, with totally n\nelements, we will show how to compute their intersection in expected time\nO(n/sqrt(w)+kr), where r is the intersection size and w is the number of bits\nin a machine-word. In addition,we introduce a very simple version of this\nalgorithm that has weaker asymptotic guarantees but performs even better in\npractice; both algorithms outperform the state of the art techniques in terms\nof execution time for both synthetic and real data sets and workloads.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2011 01:08:36 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Ding", "Bolin", "", "UIUC"], ["K\u00f6nig", "Arnd Christian", "", "Microsoft Research"]]}, {"id": "1103.2429", "submitter": "Carola Winzen", "authors": "Carola Winzen", "title": "Direction-Reversing Quasi-Random Rumor Spreading with Restarts", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent work, Doerr and Fouz [\\emph{Asymptotically Optimal Randomized\nRumor Spreading}, in ArXiv] present a new quasi-random PUSH algorithm for the\nrumor spreading problem (also known as gossip spreading or message propagation\nproblem). Their \\emph{hybrid protocol} outperforms all known PUSH protocols.\n  In this work, we add to the hybrid protocol a direction-reversing element. We\nshow that this \\emph{direction-reversing quasi-random rumor spreading protocol\nwith random restarts} yields a constant factor improvement over the hybrid\nmodel, if we allow the same dose of randomness.\n  Put differently, our protocol achieves the same broadcasting time as the\nhybrid model by employing only (roughly) half the number of random choices.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2011 09:25:23 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Winzen", "Carola", ""]]}, {"id": "1103.2566", "submitter": "Andrew Twigg", "authors": "Andrew Byde, Andy Twigg", "title": "Optimal query/update tradeoffs in versioned dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  External-memory dictionaries are a fundamental data structure in file systems\nand databases. Versioned (or fully-persistent) dictionaries have an associated\nversion tree where queries can be performed at any version, updates can be\nperformed on leaf versions, and any version can be `cloned' by adding a child.\nVarious query/update tradeoffs are known for unversioned dictionaries, many of\nthem with matching upper and lower bounds. No fully-versioned external-memory\ndictionaries are known with optimal space/query/update tradeoffs. In\nparticular, no versioned constructions are known that offer updates in $o(1)$\nI/Os using O(N) space. We present the first cache-oblivious and cache-aware\nconstructions that achieve a wide range of optimal points on this tradeoff.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2011 23:35:48 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2011 12:17:18 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Byde", "Andrew", ""], ["Twigg", "Andy", ""]]}, {"id": "1103.2581", "submitter": "Yuichi Yoshida", "authors": "Hiro Ito, Shin-ichi Tanigawa and Yuichi Yoshida", "title": "Constant-Time Algorithms for Sparsity Matroids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph $G=(V,E)$ is called $(k,\\ell)$-full if $G$ contains a subgraph\n$H=(V,F)$ of $k|V|-\\ell$ edges such that, for any non-empty $F' \\subseteq F$,\n$|F'| \\leq k|V(F')| - \\ell$ holds. Here, $V(F')$ denotes the set of vertices\nincident to $F'$. It is known that the family of edge sets of $(k,\\ell)$-full\ngraphs forms a family of matroid, known as the sparsity matroid of $G$. In this\npaper, we give a constant-time approximation algorithm for the rank of the\nsparsity matroid of a degree-bounded undirected graph. This leads to a\nconstant-time tester for $(k,\\ell)$-fullness in the bounded-degree model,\n(i.e., we can decide with high probability whether an input graph satisfies a\nproperty $P$ or far from $P$). Depending on the values of $k$ and $\\ell$, it\ncan test various properties of a graph such as connectivity, rigidity, and how\nmany spanning trees can be packed. Based on this result, we also propose a\nconstant-time tester for $(k,\\ell)$-edge-connected-orientability in the\nbounded-degree model, where an undirected graph $G$ is called\n$(k,\\ell)$-edge-connected-orientable if there exists an orientation $\\vec{G}$\nof $G$ with a vertex $r \\in V$ such that $\\vec{G}$ contains $k$ arc-disjoint\ndipaths from $r$ to each vertex $v \\in V$ and $\\ell$ arc-disjoint dipaths from\neach vertex $v \\in V$ to $r$. A tester is called a one-sided error tester for\n$P$ if it always accepts a graph satisfying $P$. We show, for $k \\geq 2$ and\n(proper) $\\ell \\geq 0$, any one-sided error tester for $(k,\\ell)$-fullness and\n$(k,\\ell)$-edge-connected-orientability requires $\\Omega(n)$ queries.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 03:51:27 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Ito", "Hiro", ""], ["Tanigawa", "Shin-ichi", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1103.2613", "submitter": "Tatiana Starikovskaya", "authors": "Roman Kolpakov, Gregory Kucherov and Tatiana Starikovskaya", "title": "Linear pattern matching on sparse suffix trees", "comments": null, "journal-ref": null, "doi": "10.1109/CCP.2011.45", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Packing several characters into one computer word is a simple and natural way\nto compress the representation of a string and to speed up its processing.\nExploiting this idea, we propose an index for a packed string, based on a {\\em\nsparse suffix tree} \\cite{KU-96} with appropriately defined suffix links.\nAssuming, under the standard unit-cost RAM model, that a word can store up to\n$\\log_{\\sigma}n$ characters ($\\sigma$ the alphabet size), our index takes\n$O(n/\\log_{\\sigma}n)$ space, i.e. the same space as the packed string itself.\nThe resulting pattern matching algorithm runs in time $O(m+r^2+r\\cdot occ)$,\nwhere $m$ is the length of the pattern, $r$ is the actual number of characters\nstored in a word and $occ$ is the number of pattern occurrences.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 09:49:10 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Kolpakov", "Roman", ""], ["Kucherov", "Gregory", ""], ["Starikovskaya", "Tatiana", ""]]}, {"id": "1103.2635", "submitter": "Lawrence Cayton", "authors": "Lawrence Cayton", "title": "Accelerating Nearest Neighbor Search on Manycore Systems", "comments": null, "journal-ref": "In Proceedings of the 2012 IEEE 26th International Parallel and\n  Distributed Processing Symposium (IPDPS '12). IEEE Computer Society,\n  Washington, DC, USA, 402-413", "doi": "10.1109/IPDPS.2012.45", "report-no": null, "categories": "cs.DB cs.CG cs.DC cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methods for accelerating metric similarity search that are\neffective on modern hardware. Our algorithms factor into easily parallelizable\ncomponents, making them simple to deploy and efficient on multicore CPUs and\nGPUs. Despite the simple structure of our algorithms, their search performance\nis provably sublinear in the size of the database, with a factor dependent only\non its intrinsic dimensionality. We demonstrate that our methods provide\nsubstantial speedups on a range of datasets and hardware platforms. In\nparticular, we present results on a 48-core server machine, on graphics\nhardware, and on a multicore desktop.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 11:39:23 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2011 18:26:44 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Cayton", "Lawrence", ""]]}, {"id": "1103.2793", "submitter": "Anastasios Zouzias", "authors": "Anastasios Zouzias", "title": "A Matrix Hyperbolic Cosine Algorithm and Applications", "comments": "16 pages, simplified proof and corrected acknowledging of prior work\n  in (current) Section 4", "journal-ref": null, "doi": "10.1007/978-3-642-31594-7_71", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we generalize Spencer's hyperbolic cosine algorithm to the\nmatrix-valued setting. We apply the proposed algorithm to several problems by\nanalyzing its computational efficiency under two special cases of matrices; one\nin which the matrices have a group structure and an other in which they have\nrank-one. As an application of the former case, we present a deterministic\nalgorithm that, given the multiplication table of a finite group of size $n$,\nit constructs an expanding Cayley graph of logarithmic degree in near-optimal\nO(n^2 log^3 n) time. For the latter case, we present a fast deterministic\nalgorithm for spectral sparsification of positive semi-definite matrices, which\nimplies an improved deterministic algorithm for spectral graph sparsification\nof dense graphs. In addition, we give an elementary connection between spectral\nsparsification of positive semi-definite matrices and element-wise matrix\nsparsification. As a consequence, we obtain improved element-wise\nsparsification algorithms for diagonally dominant-like matrices.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 21:43:27 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2012 02:21:23 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Zouzias", "Anastasios", ""]]}, {"id": "1103.3075", "submitter": "Charles Cartledge", "authors": "Charles L. Cartledge and Michael L. Nelson", "title": "Connectivity Damage to a Graph by the Removal of an Edge or a Vertex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approach of quantifying the damage inflicted on a graph in Albert, Jeong\nand Barabsi's (AJB) report \"Error and Attack Tolerance of Complex Networks\"\nusing the size of the largest connected component and the average size of the\nremaining components does not capture our intuitive idea of the damage to a\ngraph caused by disconnections. We evaluate an alternative metric based on\naverage inverse path lengths (AIPLs) that better fits our intuition that a\ngraph can still be reasonably functional even when it is disconnected. We\ncompare our metric with AJB's using a test set of graphs and report the\ndifferences. AJB's report should not be confused with a report by Crucitti et\nal. with the same name. Based on our analysis of graphs of different sizes and\ntypes, and using various numerical and statistical tools; the ratio of the\naverage inverse path lengths of a connected graph of the same size as the sum\nof the size of the fragments of the disconnected graph can be used as a metric\nabout the damage of a graph by the removal of an edge or a node. This damage is\nreported in the range (0,1) where 0 means that the removal had no effect on the\ngraph's capability to perform its functions. A 1 means that the graph is\ntotally dysfunctional. We exercise our metric on a collection of sample graphs\nthat have been subjected to various attack profiles that focus on edge, node or\ndegree betweenness values. We believe that this metric can be used to quantify\nthe damage done to the graph by an attacker, and that it can be used in\nevaluating the positive effect of adding additional edges to an existing graph.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2011 01:33:42 GMT"}], "update_date": "2011-03-17", "authors_parsed": [["Cartledge", "Charles L.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1103.3102", "submitter": "Aditya Parameswaran", "authors": "Aditya Parameswaran (Stanford University), Anish Das Sarma (Yahoo!\n  Research), Hector Garcia-Molina (Stanford University), Neoklis Polyzotis (UC\n  Santa Cruz), Jennifer Widom (Stanford University)", "title": "Human-Assisted Graph Search: It's Okay to Ask Questions", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 5, pp.\n  267-278 (2011)", "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of human-assisted graph search: given a directed\nacyclic graph with some (unknown) target node(s), we consider the problem of\nfinding the target node(s) by asking an omniscient human questions of the form\n\"Is there a target node that is reachable from the current node?\". This general\nproblem has applications in many domains that can utilize human intelligence,\nincluding curation of hierarchies, debugging workflows, image segmentation and\ncategorization, interactive search and filter synthesis. To our knowledge, this\nwork provides the first formal algorithmic study of the optimization of human\ncomputation for this problem. We study various dimensions of the problem space,\nproviding algorithms and complexity results. Our framework and algorithms can\nbe used in the design of an optimizer for crowd-sourcing platforms such as\nMechanical Turk.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2011 05:51:08 GMT"}], "update_date": "2011-03-17", "authors_parsed": [["Parameswaran", "Aditya", "", "Stanford University"], ["Sarma", "Anish Das", "", "Yahoo!\n  Research"], ["Garcia-Molina", "Hector", "", "Stanford University"], ["Polyzotis", "Neoklis", "", "UC\n  Santa Cruz"], ["Widom", "Jennifer", "", "Stanford University"]]}, {"id": "1103.3114", "submitter": "Hideo Bannai", "authors": "Keisuke Goto, Hideo Bannai, Shunsuke Inenaga, Masayuki Takeda", "title": "Fast $q$-gram Mining on SLP Compressed Strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present simple and efficient algorithms for calculating $q$-gram\nfrequencies on strings represented in compressed form, namely, as a straight\nline program (SLP). Given an SLP of size $n$ that represents string $T$, we\npresent an $O(qn)$ time and space algorithm that computes the occurrence\nfrequencies of $q$-grams in $T$. Computational experiments show that our\nalgorithm and its variation are practical for small $q$, actually running\nfaster on various real string data, compared to algorithms that work on the\nuncompressed text. We also discuss applications in data mining and\nclassification of string data, for which our algorithms can be useful.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2011 07:16:10 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2011 12:42:38 GMT"}], "update_date": "2011-07-14", "authors_parsed": [["Goto", "Keisuke", ""], ["Bannai", "Hideo", ""], ["Inenaga", "Shunsuke", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1103.3224", "submitter": "Yangyang Xu", "authors": "Yangyang Xu, Jinchuan Cui", "title": "A variant of multitask n-vehicle exploration problem: maximizing every\n  processor's average profit", "comments": "This work is part of what I did as a graduate student in the Academy\n  of Mathematics and Systems Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a variant of multitask n-vehicle exploration problem. Instead of\nrequiring an optimal permutation of vehicles in every group, the new problem\nasks all vehicles in a group to arrive at a same destination. It can also be\nviewed as to maximize every processor's average profit, given n tasks, and each\ntask's consume-time and profit. Meanwhile, we propose a new kind of partition\nproblem in fractional form, and analyze its computational complexity. Moreover,\nby regarding fractional partition as a special case, we prove that the\nmaximizing average profit problem is NP-hard when the number of processors is\nfixed and it is strongly NP-hard in general. At last, a pseudo-polynomial time\nalgorithm for the maximizing average profit problem and the fractional\npartition problem is presented, thanks to the idea of the pseudo-polynomial\ntime algorithm for the classical partition problem.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2011 16:34:06 GMT"}], "update_date": "2011-03-17", "authors_parsed": [["Xu", "Yangyang", ""], ["Cui", "Jinchuan", ""]]}, {"id": "1103.3585", "submitter": "Fredrik Sandin", "authors": "Fredrik Sandin, Blerim Emruli, Magnus Sahlgren", "title": "Incremental dimension reduction of tensors with random index", "comments": "36 pages, 9 figures", "journal-ref": "Revised version published in Knowl. Inf. Syst. 2016 (Open Access)", "doi": "10.1007/s10115-016-1012-2", "report-no": null, "categories": "cs.DS cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an incremental, scalable and efficient dimension reduction\ntechnique for tensors that is based on sparse random linear coding. Data is\nstored in a compactified representation with fixed size, which makes memory\nrequirements low and predictable. Component encoding and decoding are performed\non-line without computationally expensive re-analysis of the data set. The\nrange of tensor indices can be extended dynamically without modifying the\ncomponent representation. This idea originates from a mathematical model of\nsemantic memory and a method known as random indexing in natural language\nprocessing. We generalize the random-indexing algorithm to tensors and present\nsignal-to-noise-ratio simulations for representations of vectors and matrices.\nWe present also a mathematical analysis of the approximate orthogonality of\nhigh-dimensional ternary vectors, which is a property that underpins this and\nother similar random-coding approaches to dimension reduction. To further\ndemonstrate the properties of random indexing we present results of a synonym\nidentification task. The method presented here has some similarities with\nrandom projection and Tucker decomposition, but it performs well at high\ndimensionality only (n>10^3). Random indexing is useful for a range of complex\npractical problems, e.g., in natural language processing, data mining, pattern\nrecognition, event detection, graph searching and search engines. Prototype\nsoftware is provided. It supports encoding and decoding of tensors of order >=\n1 in a unified framework, i.e., vectors, matrices and higher order tensors.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2011 10:07:13 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Sandin", "Fredrik", ""], ["Emruli", "Blerim", ""], ["Sahlgren", "Magnus", ""]]}, {"id": "1103.3911", "submitter": "Haitao Wang", "authors": "Danny Z. Chen, Haitao Wang", "title": "Computing Shortest Paths among Curved Obstacles in the Plane", "comments": "45 pages, 21 figures; to appear in TALG; an extended-abstract\n  appeared in SoCG 2013", "journal-ref": "ACM Transactions on Algorithms, Vol. 11(4), Article No. 26, 2015", "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in computational geometry is to compute an\nobstacle-avoiding Euclidean shortest path between two points in the plane. The\ncase of this problem on polygonal obstacles is well studied. In this paper, we\nconsider the problem version on curved obstacles, commonly modeled as\nsplinegons. A splinegon can be viewed as replacing each edge of a polygon by a\nconvex curved edge (polygons are special splinegons). Each curved edge is\nassumed to be of O(1) complexity. Given in the plane two points s and t and a\nset of $h$ pairwise disjoint splinegons with a total of $n$ vertices, we\ncompute a shortest s-to-t path avoiding the splinegons, in\n$O(n+h\\log^{1+\\epsilon}h+k)$ time, where k is a parameter sensitive to the\nstructures of the input splinegons and is upper-bounded by $O(h^2)$. In\nparticular, when all splinegons are convex, $k$ is proportional to the number\nof common tangents in the free space (called \"free common tangents\") among the\nsplinegons. We develop techniques for solving the problem on the general\n(non-convex) splinegon domain, which also improve several previous results. In\nparticular, our techniques produce an optimal output-sensitive algorithm for a\nbasic visibility problem of computing all free common tangents among $h$\npairwise disjoint convex splinegons with a total of $n$ vertices. Our algorithm\nruns in $O(n+h\\log h+k)$ time and $O(n)$ space, where $k$ is the number of all\nfree common tangents. Even for the special case where all splinegons are convex\npolygons, the previously best algorithm for this visibility problem takes\n$O(n+h^2\\log n)$ time.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2011 02:30:05 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 22:30:47 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Chen", "Danny Z.", ""], ["Wang", "Haitao", ""]]}, {"id": "1103.4071", "submitter": "Richard Cole", "authors": "Richard Cole, Vijaya Ramachandran", "title": "Efficient Resource Oblivious Algorithms for Multicores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the design of efficient algorithms for a multicore computing\nenvironment with a global shared memory and p cores, each having a cache of\nsize M, and with data organized in blocks of size B. We characterize the class\nof `Hierarchical Balanced Parallel (HBP)' multithreaded computations for\nmulticores. HBP computations are similar to the hierarchical divide & conquer\nalgorithms considered in recent work, but have some additional features that\nguarantee good performance even when accounting for the cache misses due to\nfalse sharing. Most of our HBP algorithms are derived from known\ncache-oblivious algorithms with high parallelism, however we incorporate new\ntechniques that reduce the effect of false-sharing.\n  Our approach to addressing false sharing costs (or more generally, block\nmisses) is to ensure that any task that can be stolen shares O(1) blocks with\nother tasks. We use a gapping technique for computations that have larger than\nO(1) block sharing. We also incorporate the property of limited access writes\nanalyzed in a companion paper, and we bound the cost of accessing shared blocks\non the execution stacks of tasks.\n  We present the Priority Work Stealing (PWS) scheduler, and we establish that,\ngiven a sufficiently `tall' cache, PWS deterministically schedules several\nhighly parallel HBP algorithms, including those for scans, matrix computations\nand FFT, with cache misses bounded by the sequential complexity, when\naccounting for both traditional cache misses and for false sharing. We also\npresent a list ranking algorithm with almost optimal bounds. PWS schedules\nwithout using cache or block size information, and uses knowledge of processors\nonly to the extent of determining the available locations from which tasks may\nbe stolen; thus it schedules resource-obliviously.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2011 16:39:54 GMT"}], "update_date": "2011-03-22", "authors_parsed": [["Cole", "Richard", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1103.4142", "submitter": "Richard Cole", "authors": "Richard Cole, Vijaya Ramachandran", "title": "Analysis of Randomized Work Stealing with False Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the cache miss cost of algorithms when scheduled using\nrandomized work stealing (RWS) in a parallel environment, taking into account\nthe effects of false sharing.\n  First, prior analyses (due to Acar et al.) are extended to incorporate false\nsharing. However, to control the possible delays due to false sharing, some\nrestrictions on the algorithms seem necessary. Accordingly, the class of\nHierarchical Tree algorithms is introduced and their performance analyzed.\n  In addition, the paper analyzes the performance of a subclass of the\nHierarchical Tree Algorithms, called HBP algorithms, when scheduled using RWS;\nimproved complexity bounds are obtained for this subclass. This class was\nintroduced in a companion paper with efficient resource oblivious computation\nin mind.\n  Finally, we note that in a scenario in which there is no false sharing the\nresults in this paper match prior bounds for cache misses but with reduced\nassumptions, and in particular with no need for a bounding concave function for\nthe cost of cache misses as in prior work by Frigo and Strumpen. This allows\nnon-trivial cache miss bounds in this case to be obtained for a larger class of\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2011 20:25:29 GMT"}], "update_date": "2011-03-23", "authors_parsed": [["Cole", "Richard", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1103.4195", "submitter": "Sewoong Oh", "authors": "Satish Babu Korada and Andrea Montanari and Sewoong Oh", "title": "Gossip PCA", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eigenvectors of data matrices play an important role in many computational\nproblems, ranging from signal processing to machine learning and control. For\ninstance, algorithms that compute positions of the nodes of a wireless network\non the basis of pairwise distance measurements require a few leading\neigenvectors of the distances matrix. While eigenvector calculation is a\nstandard topic in numerical linear algebra, it becomes challenging under severe\ncommunication or computation constraints, or in absence of central scheduling.\nIn this paper we investigate the possibility of computing the leading\neigenvectors of a large data matrix through gossip algorithms.\n  The proposed algorithm amounts to iteratively multiplying a vector by\nindependent random sparsification of the original matrix and averaging the\nresulting normalized vectors. This can be viewed as a generalization of gossip\nalgorithms for consensus, but the resulting dynamics is significantly more\nintricate. Our analysis is based on controlling the convergence to stationarity\nof the associated Kesten-Furstenberg Markov chain.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2011 03:05:47 GMT"}], "update_date": "2011-03-23", "authors_parsed": [["Korada", "Satish Babu", ""], ["Montanari", "Andrea", ""], ["Oh", "Sewoong", ""]]}, {"id": "1103.4282", "submitter": "Andrew Twigg", "authors": "Andy Twigg, Andrew Byde, Grzegorz Milos, Tim Moreton, John Wilkes, Tom\n  Wilkie", "title": "Stratified B-trees and versioning dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classic versioned data structure in storage and computer science is the\ncopy-on-write (CoW) B-tree -- it underlies many of today's file systems and\ndatabases, including WAFL, ZFS, Btrfs and more. Unfortunately, it doesn't\ninherit the B-tree's optimality properties; it has poor space utilization,\ncannot offer fast updates, and relies on random IO to scale. Yet, nothing\nbetter has been developed since. We describe the `stratified B-tree', which\nbeats all known semi-external memory versioned B-trees, including the CoW\nB-tree. In particular, it is the first versioned dictionary to achieve optimal\ntradeoffs between space, query and update performance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2011 14:58:20 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2011 14:07:56 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Twigg", "Andy", ""], ["Byde", "Andrew", ""], ["Milos", "Grzegorz", ""], ["Moreton", "Tim", ""], ["Wilkes", "John", ""], ["Wilkie", "Tom", ""]]}, {"id": "1103.4521", "submitter": "Vissarion Fisikopoulos", "authors": "Vissarion Fisikopoulos", "title": "An implementation of range trees with fractional cascading in C++", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.SE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Range trees are multidimensional binary trees which are used to perform\nd-dimensional orthogonal range searching. In this technical report we study the\nimplementation issues of range trees with fractional cascading, named layered\nrange trees. We also document our implementation of range trees with fractional\ncascading in C++ using STL and generic programming techniques.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2011 13:57:18 GMT"}], "update_date": "2011-03-24", "authors_parsed": [["Fisikopoulos", "Vissarion", ""]]}, {"id": "1103.4566", "submitter": "Parter Merav", "authors": "Erez Kantor, Zvi Lotker, Merav Parter and David Peleg", "title": "The Topology of Wireless Communication", "comments": "64 pages, appeared in STOC'11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the topological properties of wireless communication\nmaps and their usability in algorithmic design. We consider the SINR model,\nwhich compares the received power of a signal at a receiver against the sum of\nstrengths of other interfering signals plus background noise. To describe the\nbehavior of a multi-station network, we use the convenient representation of a\n\\emph{reception map}. In the SINR model, the resulting \\emph{SINR diagram}\npartitions the plane into reception zones, one per station, and the\ncomplementary region of the plane where no station can be heard. We consider\nthe general case where transmission energies are arbitrary (or non-uniform).\nUnder that setting, the reception zones are not necessarily convex or even\nconnected. This poses the algorithmic challenge of designing efficient point\nlocation techniques as well as the theoretical challenge of understanding the\ngeometry of SINR diagrams. We achieve several results in both directions. We\nestablish a form of weaker convexity in the case where stations are aligned on\na line. In addition, one of our key results concerns the behavior of a\n$(d+1)$-dimensional map. Specifically, although the $d$-dimensional map might\nbe highly fractured, drawing the map in one dimension higher \"heals\" the zones,\nwhich become connected. In addition, as a step toward establishing a weaker\nform of convexity for the $d$-dimensional map, we study the interference\nfunction and show that it satisfies the maximum principle. Finally, we turn to\nconsider algorithmic applications, and propose a new variant of approximate\npoint location.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2011 16:03:19 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2011 13:16:47 GMT"}], "update_date": "2011-03-25", "authors_parsed": [["Kantor", "Erez", ""], ["Lotker", "Zvi", ""], ["Parter", "Merav", ""], ["Peleg", "David", ""]]}, {"id": "1103.4875", "submitter": "Isabelle Stanton", "authors": "Isabelle Stanton and Ali Pinar", "title": "Constructing and Sampling Graphs with a Prescribed Joint Degree\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most influential recent results in network analysis is that many\nnatural networks exhibit a power-law or log-normal degree distribution. This\nhas inspired numerous generative models that match this property. However, more\nrecent work has shown that while these generative models do have the right\ndegree distribution, they are not good models for real life networks due to\ntheir differences on other important metrics like conductance. We believe this\nis, in part, because many of these real-world networks have very different\njoint degree distributions, i.e. the probability that a randomly selected edge\nwill be between nodes of degree k and l. Assortativity is a sufficient\nstatistic of the joint degree distribution, and it has been previously noted\nthat social networks tend to be assortative, while biological and technological\nnetworks tend to be disassortative.\n  We suggest understanding the relationship between network structure and the\njoint degree distribution of graphs is an interesting avenue of further\nresearch. An important tool for such studies are algorithms that can generate\nrandom instances of graphs with the same joint degree distribution. This is the\nmain topic of this paper and we study the problem from both a theoretical and\npractical perspective. We provide an algorithm for constructing simple graphs\nfrom a given joint degree distribution, and a Monte Carlo Markov Chain method\nfor sampling them. We also show that the state space of simple graphs with a\nfixed degree distribution is connected via end point switches. We empirically\nevaluate the mixing time of this Markov Chain by using experiments based on the\nautocorrelation of each edge. These experiments show that our Markov Chain\nmixes quickly on real graphs, allowing for utilization of our techniques in\npractice.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2011 21:05:17 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2011 18:41:54 GMT"}], "update_date": "2011-09-01", "authors_parsed": [["Stanton", "Isabelle", ""], ["Pinar", "Ali", ""]]}, {"id": "1103.5102", "submitter": "Michael Goodrich", "authors": "Michael T. Goodrich", "title": "Data-Oblivious External-Memory Algorithms for the Compaction, Selection,\n  and Sorting of Outsourced Data", "comments": "Full version of a paper appearing in 2011 ACM Symp. on Parallelism in\n  Algorithms and Architectures (SPAA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present data-oblivious algorithms in the external-memory model for\ncompaction, selection, and sorting. Motivation for such problems comes from\nclients who use outsourced data storage services and wish to mask their data\naccess patterns. We show that compaction and selection can be done\ndata-obliviously using $O(N/B)$ I/Os, and sorting can be done, with a high\nprobability of success, using $O((N/B)\\log_{M/B} (N/B))$ I/Os. Our methods use\na number of new algorithmic techniques, including data-oblivious uses of\ninvertible Bloom lookup tables, a butterfly-like compression network,\nrandomized data thinning, and \"shuffle-and-deal\" data perturbation. In\naddition, since data-oblivious sorting is the bottleneck in the \"inner loop\" in\nexisting oblivious RAM simulations, our sorting result improves the amortized\ntime overhead to do oblivious RAM simulation by a logarithmic factor in the\nexternal-memory model.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2011 03:18:03 GMT"}], "update_date": "2011-03-29", "authors_parsed": [["Goodrich", "Michael T.", ""]]}, {"id": "1103.5453", "submitter": "Malik Magdon-Ismail", "authors": "Malik Magdon-Ismail", "title": "Using a Non-Commutative Bernstein Bound to Approximate Some Matrix\n  Algorithms in the Spectral Norm", "comments": "Working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on \\emph{row sampling} based approximations for matrix algorithms,\nin particular matrix multipication, sparse matrix reconstruction, and\n\\math{\\ell_2} regression. For \\math{\\matA\\in\\R^{m\\times d}} (\\math{m} points in\n\\math{d\\ll m} dimensions), and appropriate row-sampling probabilities, which\ntypically depend on the norms of the rows of the \\math{m\\times d} left singular\nmatrix of \\math{\\matA} (the \\emph{leverage scores}), we give row-sampling\nalgorithms with linear (up to polylog factors) dependence on the stable rank of\n\\math{\\matA}. This result is achieved through the application of\nnon-commutative Bernstein bounds. Keywords: row-sampling; matrix\nmultiplication; matrix reconstruction; estimating spectral norm; linear\nregression; randomized\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2011 19:37:20 GMT"}], "update_date": "2011-03-29", "authors_parsed": [["Magdon-Ismail", "Malik", ""]]}, {"id": "1103.5510", "submitter": "Mihai Patrascu", "authors": "Timothy M. Chan and Kasper Green Larsen and Mihai Patrascu", "title": "Orthogonal Range Searching on the RAM, Revisited", "comments": "To appear in SoCG 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present several new results on one of the most extensively studied topics\nin computational geometry, orthogonal range searching. All our results are in\nthe standard word RAM model for points in rank space:\n  ** We present two data structures for 2-d orthogonal range emptiness. The\nfirst achieves O(n lglg n) space and O(lglg n) query time. This improves the\nprevious results by Alstrup, Brodal, and Rauhe(FOCS'00), with O(n lg^eps n)\nspace and O(lglg n) query time, or with O(nlglg n) space and O(lg^2 lg n) query\ntime. Our second data structure uses O(n) space and answers queries in O(lg^eps\nn) time. The best previous O(n)-space data structure, due to Nekrich (WADS'07),\nanswers queries in O(lg n/lglg n) time.\n  ** For 3-d orthogonal range reporting, we obtain space O(n lg^{1+eps} n) and\nquery time O(lglg n + k), for any constant eps>0. This improves previous\nresults by Afshani (ESA'08), Karpinski and Nekrich (COCOON'09), and Chan\n(SODA'11), with O(n lg^3 n) space and O(lglg n + k) query time, or with O(n\nlg^{1+eps} n) space and O(lg^2 lg n + k) query time. This implies improved\nbounds for orthogonal range reporting in all constant dimensions above 3.\n  ** We give a randomized algorithm for 4-d offline dominance range\nreporting/emptiness with running time O(n lg n + k). This resolves two open\nproblems from Preparata and Shamos' seminal book:\n  **** given n axis-aligned rectangles in the plane, we can report all k\nenclosure pairs in O(n lg n + k) expected time. The best known result was an\nO([n lg n + k] lglg n) algorithm from SoCG'95 by Gupta, Janardan, Smid, and\nDasgupta.\n  **** given n points in 4-d, we can find all maximal points in O(n lg n)\nexpected time. The best previous result was an O(n lg n lglg n) algorithm due\nto Gabow, Bentley, and Tarjan (STOC'84). This implies record time bounds for\nthe maxima problem in all constant dimensions above 4.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2011 23:32:35 GMT"}], "update_date": "2011-03-30", "authors_parsed": [["Chan", "Timothy M.", ""], ["Larsen", "Kasper Green", ""], ["Patrascu", "Mihai", ""]]}, {"id": "1103.5531", "submitter": "Andrew Lyons", "authors": "Andrew Lyons", "title": "Acyclic and Star Colorings of Cographs", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An \\emph{acyclic coloring} of a graph is a proper vertex coloring such that\nthe union of any two color classes induces a disjoint collection of trees. The\nmore restricted notion of \\emph{star coloring} requires that the union of any\ntwo color classes induces a disjoint collection of stars. We prove that every\nacyclic coloring of a cograph is also a star coloring and give a linear-time\nalgorithm for finding an optimal acyclic and star coloring of a cograph. If the\ngraph is given in the form of a cotree, the algorithm runs in O(n) time. We\nalso show that the acyclic chromatic number, the star chromatic number, the\ntreewidth plus one, and the pathwidth plus one are all equal for cographs.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2011 02:25:49 GMT"}], "update_date": "2011-03-30", "authors_parsed": [["Lyons", "Andrew", ""]]}, {"id": "1103.5599", "submitter": "Anthony Perez", "authors": "St\\'ephane Bessy and Anthony Perez", "title": "Polynomial kernels for Proper Interval Completion and related problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph G = (V,E) and a positive integer k, the Proper Interval\nCompletion problem asks whether there exists a set F of at most k pairs of (V\n\\times V)\\E such that the graph H = (V,E \\cup F) is a proper interval graph.\nThe Proper Interval Completion problem finds applications in molecular biology\nand genomic research. First announced by Kaplan, Tarjan and Shamir in FOCS '94,\nthis problem is known to be FPT, but no polynomial kernel was known to exist.\nWe settle this question by proving that Proper Interval Completion admits a\nkernel with at most O(k^5) vertices. Moreover, we prove that a related problem,\nthe so-called Bipartite Chain Deletion problem, admits a kernel with at most\nO(k^2) vertices, completing a previous result of Guo.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2011 10:45:31 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2011 20:03:43 GMT"}], "update_date": "2011-04-18", "authors_parsed": [["Bessy", "St\u00e9phane", ""], ["Perez", "Anthony", ""]]}, {"id": "1103.5609", "submitter": "Daniel Reichman", "authors": "Uriel Feige and Daniel Reichman", "title": "Recoverable Values for Independent Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of {\\em recoverable value} was advocated in work of Feige,\nImmorlica, Mirrokni and Nazerzadeh [Approx 2009] as a measure of quality for\napproximation algorithms. There this concept was applied to facility location\nproblems. In the current work we apply a similar framework to the maximum\nindependent set problem (MIS). We say that an approximation algorithm has {\\em\nrecoverable value} $\\rho$, if for every graph it recovers an independent set of\nsize at least $\\max_I \\sum_{v\\in I} \\min[1,\\rho/(d(v) + 1)]$, where $d(v)$ is\nthe degree of vertex $v$, and $I$ ranges over all independent sets in $G$.\nHence, in a sense, from every vertex $v$ in the maximum independent set the\nalgorithm recovers a value of at least $\\rho/(d_v + 1)$ towards the solution.\nThis quality measure is most effective in graphs in which the maximum\nindependent set is composed of low degree vertices. It easily follows from\nknown results that some simple algorithms for MIS ensure $\\rho \\ge 1$. We\ndesign a new randomized algorithm for MIS that ensures an expected recoverable\nvalue of at least $\\rho \\ge 7/3$. In addition, we show that approximating MIS\nin graphs with a given $k$-coloring within a ratio larger than $2/k$ is unique\ngames hard. This rules out a natural approach for obtaining $\\rho \\ge 2$.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2011 11:47:04 GMT"}], "update_date": "2011-03-30", "authors_parsed": [["Feige", "Uriel", ""], ["Reichman", "Daniel", ""]]}, {"id": "1103.6049", "submitter": "Kamal Al-Bawani", "authors": "Kamal Al-Bawani, Alexander Souza", "title": "Buffer Overflow Management with Class Segregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a new model for buffer management of network switches with\nQuality of Service (QoS) requirements. A stream of packets, each attributed\nwith a value representing its Class of Service (CoS), arrives over time at a\nnetwork switch and demands a further transmission. The switch is equipped with\nmultiple queues of limited capacities, where each queue stores packets of one\nvalue only. The objective is to maximize the total value of the transmitted\npackets (i.e., the weighted throughput).\n  We analyze a natural greedy algorithm, GREEDY, which sends in each time step\na packet with the greatest value. For general packet values $(v_1 < \\cdots <\nv_m)$, we show that GREEDY is $(1+r)$-competitive, where $r = \\max_{1\\le i \\le\nm-1} \\{v_i/v_{i+1}\\}$. Furthermore, we show a lower bound of $2 - v_m /\n\\sum_{i=1}^m v_i$ on the competitiveness of any deterministic online algorithm.\nIn the special case of two packet values (1 and $\\alpha > 1$), GREEDY is shown\nto be optimal with a competitive ratio of $(\\alpha + 2)/(\\alpha + 1)$.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2011 21:17:05 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2011 10:01:21 GMT"}, {"version": "v3", "created": "Fri, 8 Feb 2013 14:24:16 GMT"}, {"version": "v4", "created": "Tue, 12 Feb 2013 15:02:18 GMT"}], "update_date": "2013-02-13", "authors_parsed": [["Al-Bawani", "Kamal", ""], ["Souza", "Alexander", ""]]}, {"id": "1103.6073", "submitter": "Charalampos Tsourakakis", "authors": "Rasmus Pagh and Charalampos E. Tsourakakis", "title": "Colorful Triangle Counting and a MapReduce Implementation", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we introduce a new randomized algorithm for counting triangles\nin graphs. We show that under mild conditions, the estimate of our algorithm is\nstrongly concentrated around the true number of triangles. Specifically, if $p\n\\geq \\max{(\\frac{\\Delta \\log{n}}{t}, \\frac{\\log{n}}{\\sqrt{t}})}$, where $n$,\n$t$, $\\Delta$ denote the number of vertices in $G$, the number of triangles in\n$G$, the maximum number of triangles an edge of $G$ is contained, then for any\nconstant $\\epsilon>0$ our unbiased estimate $T$ is concentrated around its\nexpectation, i.e., $ \\Prob{|T - \\Mean{T}| \\geq \\epsilon \\Mean{T}} = o(1)$.\nFinally, we present a \\textsc{MapReduce} implementation of our algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2011 01:26:13 GMT"}], "update_date": "2011-04-01", "authors_parsed": [["Pagh", "Rasmus", ""], ["Tsourakakis", "Charalampos E.", ""]]}, {"id": "1103.6161", "submitter": "Assaf Naor", "authors": "Mark Braverman, Konstantin Makarychev, Yury Makarychev, Assaf Naor", "title": "The Grothendieck constant is strictly smaller than Krivine's bound", "comments": "An extended abstract describing the contents of this work will appear\n  in FOCS 2011. Suggestions of the FOCS reviewers have been addressed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that $K_G<\\frac{\\pi}{2\\log(1+\\sqrt{2})}$, where $K_G$ is the\nGrothendieck constant.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2011 12:54:41 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2011 17:42:46 GMT"}, {"version": "v3", "created": "Wed, 17 Aug 2011 14:54:02 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Braverman", "Mark", ""], ["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""], ["Naor", "Assaf", ""]]}, {"id": "1103.6246", "submitter": "Bob Sturm", "authors": "Bob L. Sturm", "title": "Sparse Vector Distributions and Recovery from Compressed Sensing", "comments": "Originally submitted to IEEE Signal Processing Letters in March 2011,\n  but rejected June 2011. Revised, expanded, and submitted July 2011 to EURASIP\n  Journal special issue on sparse signal processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the performance of sparse vector recovery algorithms\nfrom compressive measurements can depend on the distribution underlying the\nnon-zero elements of a sparse vector. However, the extent of these effects has\nyet to be explored, and formally presented. In this paper, I empirically\ninvestigate this dependence for seven distributions and fifteen recovery\nalgorithms. The two morals of this work are: 1) any judgement of the recovery\nperformance of one algorithm over that of another must be prefaced by the\nconditions for which this is observed to be true, including sparse vector\ndistributions, and the criterion for exact recovery; and 2) a recovery\nalgorithm must be selected carefully based on what distribution one expects to\nunderlie the sensed sparse signal.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2011 17:26:34 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2011 09:04:19 GMT"}], "update_date": "2011-07-18", "authors_parsed": [["Sturm", "Bob L.", ""]]}]