[{"id": "1809.00077", "submitter": "Andreas B\\\"artschi", "authors": "Andreas B\\\"artschi, Daniel Graf, Matus Mihalak", "title": "Collective fast delivery by energy-efficient agents", "comments": "In an extended abstract of this paper [MFCS 2018], we erroneously\n  claimed the single agent approach for variant (iii) to have approximation\n  ratio 2", "journal-ref": "In 43rd International Symposium on Mathematical Foundations of\n  Computer Science, MFCS'18, pages 56:1-56:16, 2018", "doi": "10.4230/LIPIcs.MFCS.2018.56", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider k mobile agents initially located at distinct nodes of an\nundirected graph (on n nodes, with edge lengths) that have to deliver a single\nitem from a given source node s to a given target node t. The agents can move\nalong the edges of the graph, starting at time 0 with respect to the following:\nEach agent i has a weight w_i that defines the rate of energy consumption while\ntravelling a distance in the graph, and a velocity v_i with which it can move.\n  We are interested in schedules (operating the k agents) that result in a\nsmall delivery time T (time when the package arrives at t), and small total\nenergy consumption E. Concretely, we ask for a schedule that: either (i)\nMinimizes T, (ii) Minimizes lexicographically (T,E) (prioritizing fast\ndelivery), or (iii) Minimizes epsilon*T + (1-epsilon)*E, for a given epsilon,\n0<epsilon<1.\n  We show that (i) is solvable in polynomial time, and show that (ii) is\npolynomial-time solvable for uniform velocities and solvable in time O(n + k\nlog k) for arbitrary velocities on paths, but in general is NP-hard even on\nplanar graphs. As a corollary of our hardness result, (iii) is NP-hard, too. We\nshow that there is a 3-approximation algorithm for (iii) using a single agent.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 22:53:32 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["B\u00e4rtschi", "Andreas", ""], ["Graf", "Daniel", ""], ["Mihalak", "Matus", ""]]}, {"id": "1809.00190", "submitter": "Xavier Ouvrard", "authors": "Xavier Ouvrard, Jean-Marie Le Goff and Stephane Marchand-Maillet", "title": "Exchange-Based Diffusion in Hb-Graphs: Highlighting Complex\n  Relationships", "comments": "arXiv:1809.00190v1: Accepted version of article submitted at CBMI\n  2018 IEEE This version is an extended version of arXiv:1809.00190v1 currently\n  in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most networks tend to show complex and multiple relationships between\nentities. Networks are usually modeled by graphs or hypergraphs; nonetheless a\ngiven entity can occur many times in a relationship: this brings the need to\ndeal with multisets instead of sets or simple edges. Diffusion processes are\nuseful to highlight interesting parts of a network: they usually start with a\nstroke at one vertex and diffuse throughout the network to reach a uniform\ndistribution. Several iterations of the process are required prior to reaching\na stable solution. We propose an alternative solution to highlighting the main\ncomponents of a network using a diffusion process based on exchanges: it is an\niterative two-phase step exchange process. This process allows to evaluate the\nimportance not only of the vertices but also of the regrouping level. To model\nthe diffusion process, we extend the concept of hypergraphs that are families\nof sets to families of multisets, that we call hb-graphs. This version is an\nextended version of arXiv:1809.00190v1: the overlaps with the v1 are in black,\nthe new content is in blue. The contributions of this extended version are: the\nproofs of conservation and convergence of the extracted sequences of the\ndiffusion process, as well as the illustration of the speed of convergence and\ncomparison to classical and modified random walks; the algorithms of the\nexchange-based diffusion and the modified random walk; the application to a use\ncase based on Arxiv publications. All the figures except one have been either\nmodified or added in this extended version to take into account the new\ndevelopments.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 14:15:16 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 19:34:18 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Ouvrard", "Xavier", ""], ["Goff", "Jean-Marie Le", ""], ["Marchand-Maillet", "Stephane", ""]]}, {"id": "1809.00394", "submitter": "Muhammad Anis Uddin Nasir", "authors": "Cigdem Aslay, Muhammad Anis Uddin Nasir, Gianmarco De Francisci\n  Morales, Aristides Gionis", "title": "Mining Frequent Patterns in Evolving Graphs", "comments": "10 pages, accepted at CIKM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a labeled graph, the frequent-subgraph mining (FSM) problem asks to\nfind all the $k$-vertex subgraphs that appear with frequency greater than a\ngiven threshold. FSM has numerous applications ranging from biology to network\nscience, as it provides a compact summary of the characteristics of the graph.\nHowever, the task is challenging, even more so for evolving graphs due to the\nstreaming nature of the input and the exponential time complexity of the\nproblem.\n  In this paper, we initiate the study of the approximate FSM problem in both\nincremental and fully-dynamic streaming settings, where arbitrary edges can be\nadded or removed from the graph. For each streaming setting, we propose\nalgorithms that can extract a high-quality approximation of the frequent\n$k$-vertex subgraphs for a given threshold, at any given time instance, with\nhigh probability. In contrast to the existing state-of-the-art solutions that\nrequire iterating over the entire set of subgraphs for any update, our\nalgorithms operate by maintaining a uniform sample of $k$-vertex subgraphs with\noptimized neighborhood-exploration procedures local to the updates. We provide\ntheoretical analysis of the proposed algorithms and empirically demonstrate\nthat the proposed algorithms generate high-quality results compared to\nbaselines.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 21:25:48 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 17:51:20 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Aslay", "Cigdem", ""], ["Nasir", "Muhammad Anis Uddin", ""], ["Morales", "Gianmarco De Francisci", ""], ["Gionis", "Aristides", ""]]}, {"id": "1809.00585", "submitter": "Robert Ganian", "authors": "Robert Ganian, Sebastian Ordyniak", "title": "The Complexity Landscape of Decompositional Parameters for ILP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integer Linear Programming (ILP) can be seen as the archetypical problem for\nNP-complete optimization problems, and a wide range of problems in artificial\nintelligence are solved in practice via a translation to ILP. Despite its huge\nrange of applications, only few tractable fragments of ILP are known, probably\nthe most prominent of which is based on the notion of total unimodularity.\nUsing entirely different techniques, we identify new tractable fragments of ILP\nby studying structural parameterizations of the constraint matrix within the\nframework of parameterized complexity.\n  In particular, we show that ILP is fixed-parameter tractable when\nparameterized by the treedepth of the constraint matrix and the maximum\nabsolute value of any coefficient occurring in the ILP instance. Together with\nmatching hardness results for the more general parameter treewidth, we give an\noverview of the complexity of ILP w.r.t. decompositional parameters defined on\nthe constraint matrix.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 12:48:47 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Ganian", "Robert", ""], ["Ordyniak", "Sebastian", ""]]}, {"id": "1809.00643", "submitter": "Ronald de Wolf", "authors": "Joran van Apeldoorn, Andr\\'as Gily\\'en, Sander Gribling, Ronald de\n  Wolf", "title": "Convex optimization using quantum oracles", "comments": "29 pages. Version 4: appears in Quantum, alongside an independent\n  paper with similar results by Shouvanik Chakrabarti, Andrew Childs, Tongyang\n  Li, and Xiaodi Wu <arXiv:1809.01731>", "journal-ref": "Quantum 4, 220 (2020)", "doi": "10.22331/q-2020-01-13-220", "report-no": null, "categories": "quant-ph cs.DS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study to what extent quantum algorithms can speed up solving convex\noptimization problems. Following the classical literature we assume access to a\nconvex set via various oracles, and we examine the efficiency of reductions\nbetween the different oracles. In particular, we show how a separation oracle\ncan be implemented using $\\tilde{O}(1)$ quantum queries to a membership oracle,\nwhich is an exponential quantum speed-up over the $\\Omega(n)$ membership\nqueries that are needed classically. We show that a quantum computer can very\nefficiently compute an approximate subgradient of a convex Lipschitz function.\nCombining this with a simplification of recent classical work of Lee, Sidford,\nand Vempala gives our efficient separation oracle. This in turn implies, via a\nknown algorithm, that $\\tilde{O}(n)$ quantum queries to a membership oracle\nsuffice to implement an optimization oracle (the best known classical upper\nbound on the number of membership queries is quadratic). We also prove several\nlower bounds: $\\Omega(\\sqrt{n})$ quantum separation (or membership) queries are\nneeded for optimization if the algorithm knows an interior point of the convex\nset, and $\\Omega(n)$ quantum separation queries are needed if it does not.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 16:30:24 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 11:43:11 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 09:26:14 GMT"}, {"version": "v4", "created": "Fri, 20 Dec 2019 17:14:21 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["van Apeldoorn", "Joran", ""], ["Gily\u00e9n", "Andr\u00e1s", ""], ["Gribling", "Sander", ""], ["de Wolf", "Ronald", ""]]}, {"id": "1809.00725", "submitter": "Kuan Cheng", "authors": "Kuan Cheng, Zhengzhong Jin, Xin Li, Ke Wu", "title": "Block Edit Errors with Transpositions: Deterministic Document Exchange\n  Protocols and Almost Optimal Binary Codes", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document exchange and error correcting codes are two fundamental problems\nregarding communications. In the first problem, Alice and Bob each holds a\nstring, and the goal is for Alice to send a short sketch to Bob, so that Bob\ncan recover Alice's string. In the second problem, Alice sends a message with\nsome redundant information to Bob through a channel that can add adversarial\nerrors, and the goal is for Bob to correctly recover the message despite the\nerrors.\n  In a recent work \\cite{CJLW18}, the authors constructed explicit\ndeterministic document exchange protocols and binary error correcting codes for\nedit errors with almost optimal parameters.\\ Unfortunately, the constructions\nin \\cite{CJLW18} do not work for other common errors such as block\ntranspositions.\n  In this paper, we generalize the constructions in \\cite{CJLW18} to handle a\nmuch larger class of errors. These include bursts of insertions and deletions,\nas well as block transpositions. Specifically, we consider document exchange\nand error correcting codes where the total number of block insertions, block\ndeletions, and block transpositions is at most $k \\leq \\alpha n/\\log n$ for\nsome constant $0<\\alpha<1$. In addition, the total number of bits inserted and\ndeleted by the first two kinds of operations is at most $t \\leq \\beta n$ for\nsome constant $0<\\beta<1$, where $n$ is the length of Alice's string or\nmessage. We construct explicit, deterministic document exchange protocols with\nsketch size $ O( (k \\log n +t) \\log^2 \\frac{n}{k\\log n + t} )$ and explicit\nbinary error correcting code with $O(k \\log n \\log \\log \\log n+t)$ redundant\nbits.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 21:38:04 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 15:45:42 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 17:27:58 GMT"}, {"version": "v4", "created": "Sat, 27 Apr 2019 19:14:10 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Cheng", "Kuan", ""], ["Jin", "Zhengzhong", ""], ["Li", "Xin", ""], ["Wu", "Ke", ""]]}, {"id": "1809.00792", "submitter": "Srikumar Krishnamoorthy", "authors": "Srikumar Krishnamoorthy", "title": "A comparative study of top-k high utility itemset mining methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Utility Itemset (HUI) mining problem is one of the important problems in\nthe data mining literature. The problem offers greater flexibility to a\ndecision maker to incorporate her/his notion of utility into the pattern mining\nprocess. The problem, however, requires the decision maker to choose a minimum\nutility threshold value for discovering interesting patterns. This is quite\nchallenging due to the disparate itemset characteristics and their utility\ndistributions. In order to address this issue, Top-K High Utility Itemset\n(THUI) mining problem was introduced in the literature. THUI mining problem is\nprimarily a variant of the HUI mining problem that allows a decision maker to\nspecify the desired number of HUIs rather than the minimum utility threshold\nvalue. Several algorithms have been introduced in the literature to efficiently\nmine top-k HUIs. This paper systematically analyses the top-k HUI mining\nmethods in the literature, describes the methods, and performs a comparative\nanalysis. The data structures, threshold raising strategies, and pruning\nstrategies adopted for efficient top-k HUI mining are also presented and\nanalysed. Furthermore, the paper reviews several extensions of the top-k HUI\nmining problem such as data stream mining, sequential pattern mining and\non-shelf utility mining. The paper is likely to be useful for researchers to\nexamine the key methods in top-k HUI mining, evaluate the gaps in literature,\nexplore new research opportunities and enhance the state-of-the-art in high\nutility pattern mining.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 04:18:52 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Krishnamoorthy", "Srikumar", ""]]}, {"id": "1809.00907", "submitter": "Steven Kelk", "authors": "Remie Janssen, Mark Jones, Steven Kelk, Georgios Stamoulis, Taoyang Wu", "title": "Treewidth of display graphs: bounds, brambles and applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic trees and networks are leaf-labelled graphs used to model\nevolution. Display graphs are created by identifying common leaf labels in two\nor more phylogenetic trees or networks. The treewidth of such graphs is bounded\nas a function of many common dissimilarity measures between phylogenetic trees\nand this has been leveraged in fixed parameter tractability results. Here we\nfurther elucidate the properties of display graphs and their interaction with\ntreewidth. We show that it is NP-hard to recognize display graphs, but that\ndisplay graphs of bounded treewidth can be recognized in linear time. Next we\nshow that if a phylogenetic network displays (i.e. topologically embeds) a\nphylogenetic tree, the treewidth of their display graph is bounded by a\nfunction of the treewidth of the original network (and also by various other\nparameters). In fact, using a bramble argument we show that this treewidth\nbound is sharp up to an additive term of 1. We leverage this bound to give an\nFPT algorithm, parameterized by treewidth, for determining whether a network\ndisplays a tree, which is an intensively-studied problem in the field. We\nconclude with a discussion on the future use of display graphs and treewidth in\nphylogenetics.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 11:56:59 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Janssen", "Remie", ""], ["Jones", "Mark", ""], ["Kelk", "Steven", ""], ["Stamoulis", "Georgios", ""], ["Wu", "Taoyang", ""]]}, {"id": "1809.00932", "submitter": "Hu Ding", "authors": "Hu Ding", "title": "Faster Balanced Clusterings in High Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of constrained clustering has attracted significant attention in\nthe past decades. In this paper, we study the balanced $k$-center, $k$-median,\nand $k$-means clustering problems where the size of each cluster is constrained\nby the given lower and upper bounds. The problems are motivated by the\napplications in processing large-scale data in high dimension. Existing methods\noften need to compute complicated matchings (or min cost flows) to satisfy the\nbalance constraint, and thus suffer from high complexities especially in high\ndimension. We develop an effective framework for the three balanced clustering\nproblems to address this issue, and our method is based on a novel spatial\npartition idea in geometry. For the balanced $k$-center clustering, we provide\na $4$-approximation algorithm that improves the existing approximation factors;\nfor the balanced $k$-median and $k$-means clusterings, our algorithms yield\nconstant and $(1+\\epsilon)$-approximation factors with any $\\epsilon>0$. More\nimportantly, our algorithms achieve linear or nearly linear running times when\n$k$ is a constant, and significantly improve the existing ones. Our results can\nbe easily extended to metric balanced clusterings and the running times are\nsub-linear in terms of the complexity of $n$-point metric.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 13:07:52 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 01:59:04 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Ding", "Hu", ""]]}, {"id": "1809.00942", "submitter": "Ohad Trabelsi", "authors": "Arnold Filtser, Robert Krauthgamer, Ohad Trabelsi", "title": "Relaxed Voronoi: a Simple Framework for Terminal-Clustering Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reprove three known algorithmic bounds for terminal-clustering problems,\nusing a single framework that leads to simpler proofs. In this genre of\nproblems, the input is a metric space $(X,d)$ (possibly arising from a graph)\nand a subset of terminals $K\\subset X$, and the goal is to partition the points\n$X$ such that each part, called a cluster, contains exactly one terminal\n(possibly with connectivity requirements) so as to minimize some objective. The\nthree bounds we reprove are for Steiner Point Removal on trees [Gupta, SODA\n2001], for Metric $0$-Extension in bounded doubling dimension [Lee and Naor,\nunpublished 2003], and for Connected Metric $0$-Extension [Englert et al.,\nSICOMP 2014].\n  A natural approach is to cluster each point with its closest terminal, which\nwould partition $X$ into so-called Voronoi cells, but this approach can fail\nmiserably due to its stringent cluster boundaries. A now-standard fix, which we\ncall the Relaxed-Voronoi framework, is to use enlarged Voronoi cells, but to\nobtain disjoint clusters, the cells are computed greedily according to some\norder. This method, first proposed by Calinescu, Karloff and Rabani [SICOMP\n2004], was employed successfully to provide state-of-the-art results for\nterminal-clustering problems on general metrics. However, for restricted\nfamilies of metrics, e.g., trees and doubling metrics, only more complicated,\nad-hoc algorithms are known. Our main contribution is to demonstrate that the\nRelaxed-Voronoi algorithm is applicable to restricted metrics, and actually\nleads to relatively simple algorithms and analyses.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 13:29:28 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 14:17:20 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Filtser", "Arnold", ""], ["Krauthgamer", "Robert", ""], ["Trabelsi", "Ohad", ""]]}, {"id": "1809.01017", "submitter": "Tamara Mchedlidze David", "authors": "Moritz Klammler and Tamara Mchedlidze and Alexey Pak", "title": "Aesthetic Discrimination of Graph Layouts", "comments": "Appears in the Proceedings of the 26th International Symposium on\n  Graph Drawing and Network Visualization (GD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the following basic question: given two layouts of the\nsame graph, which one is more aesthetically pleasing? We propose a neural\nnetwork-based discriminator model trained on a labeled dataset that decides\nwhich of two layouts has a higher aesthetic quality. The feature vectors used\nas inputs to the model are based on known graph drawing quality metrics,\nclassical statistics, information-theoretical quantities, and two-point\nstatistics inspired by methods of condensed matter physics. The large corpus of\nlayout pairs used for training and testing is constructed using force-directed\ndrawing algorithms and the layouts that naturally stem from the process of\ngraph generation. It is further extended using data augmentation techniques.\nThe mean prediction accuracy of our model is 95.70%, outperforming\ndiscriminators based on stress and on the linear combination of popular quality\nmetrics by a statistically significant margin.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 14:19:43 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Klammler", "Moritz", ""], ["Mchedlidze", "Tamara", ""], ["Pak", "Alexey", ""]]}, {"id": "1809.01207", "submitter": "Tselil Schramm", "authors": "Pravesh Kothari and Ryan O'Donnell and Tselil Schramm", "title": "SOS lower bounds with hard constraints: think global, act local", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many previous Sum-of-Squares (SOS) lower bounds for CSPs had two deficiencies\nrelated to global constraints. First, they were not able to support a\n\"cardinality constraint\", as in, say, the Min-Bisection problem. Second, while\nthe pseudoexpectation of the objective function was shown to have some value\n$\\beta$, it did not necessarily actually \"satisfy\" the constraint \"objective =\n$\\beta$\". In this paper we show how to remedy both deficiencies in the case of\nrandom CSPs, by translating \\emph{global} constraints into \\emph{local}\nconstraints. Using these ideas, we also show that degree-$\\Omega(\\sqrt{n})$ SOS\ndoes not provide a $(\\frac{4}{3} - \\epsilon)$-approximation for Min-Bisection,\nand degree-$\\Omega(n)$ SOS does not provide a $(\\frac{11}{12} +\n\\epsilon)$-approximation for Max-Bisection or a $(\\frac{5}{4} -\n\\epsilon)$-approximation for Min-Bisection. No prior SOS lower bounds for these\nproblems were known.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 19:11:03 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Kothari", "Pravesh", ""], ["O'Donnell", "Ryan", ""], ["Schramm", "Tselil", ""]]}, {"id": "1809.01246", "submitter": "Xiangyang Gou", "authors": "Xiangyang Gou, Lei Zou, Chenxingyu Zhao, Tong Yang", "title": "Fast and Accurate Graph Stream Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph stream is a continuous sequence of data items, in which each item\nindicates an edge, including its two endpoints and edge weight. It forms a\ndynamic graph that changes with every item in the stream. Graph streams play\nimportant roles in cyber security, social networks, cloud troubleshooting\nsystems and other fields. Due to the vast volume and high update speed of graph\nstreams, traditional data structures for graph storage such as the adjacency\nmatrix and the adjacency list are no longer sufficient. However, prior art of\ngraph stream summarization, like CM sketches, gSketches, TCM and gMatrix,\neither supports limited kinds of queries or suffers from poor accuracy of query\nresults. In this paper, we propose a novel Graph Stream Sketch (GSS for short)\nto summarize the graph streams, which has the linear space cost (O(|E|), E is\nthe edge set of the graph) and the constant update time complexity (O(1)) and\nsupports all kinds of queries over graph streams with the controllable errors.\nBoth theoretical analysis and experiment results confirm the superiority of our\nsolution with regard to the time/space complexity and query results' precision\ncompared with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 21:19:40 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Gou", "Xiangyang", ""], ["Zou", "Lei", ""], ["Zhao", "Chenxingyu", ""], ["Yang", "Tong", ""]]}, {"id": "1809.01398", "submitter": "Chen Yuan", "authors": "Chen Yuan, Guangyi Liu, Renchang Dai, Zhiwei Wang", "title": "Power Flow Analysis Using Graph based Combination of Iterative Methods\n  and Vertex Contraction Approach", "comments": "8 pages, 8 figures, 2018 International Conference on Power System\n  Technology (POWERCON 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.PF math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with relational database (RDB), graph database (GDB) is a more\nintuitive expression of the real world. Each node in the GDB is a both storage\nand logic unit. Since it is connected to its neighboring nodes through edges,\nand its neighboring information could be easily obtained in one-step graph\ntraversal. It is able to conduct local computation independently and all nodes\ncan do their local work in parallel. Then the whole system can be maximally\nanalyzed and assessed in parallel to largely improve the computation\nperformance without sacrificing the precision of final results. This paper\nfirstly introduces graph database, power system graph modeling and potential\ngraph computing applications in power systems. Two iterative methods based on\ngraph database and PageRank are presented and their convergence are discussed.\nVertex contraction is proposed to improve the performance by eliminating\nzero-impedance branch. A combination of the two iterative methods is proposed\nto make use of their advantages. Testing results based on a provincial 1425-bus\nsystem demonstrate that the proposed comprehensive approach is a good candidate\nfor power flow analysis.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 09:17:56 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Yuan", "Chen", ""], ["Liu", "Guangyi", ""], ["Dai", "Renchang", ""], ["Wang", "Zhiwei", ""]]}, {"id": "1809.01731", "submitter": "Tongyang Li", "authors": "Shouvanik Chakrabarti, Andrew M. Childs, Tongyang Li, Xiaodi Wu", "title": "Quantum algorithms and lower bounds for convex optimization", "comments": "44 pages, 2 figures. Similar results were independently obtained by\n  Joran van Apeldoorn, Andras Gilyen, Sander Gribling, and Ronald de Wolf\n  <arXiv:1809.00643>", "journal-ref": "Quantum 4, 221 (2020)", "doi": "10.22331/q-2020-01-13-221", "report-no": null, "categories": "quant-ph cs.DS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While recent work suggests that quantum computers can speed up the solution\nof semidefinite programs, little is known about the quantum complexity of more\ngeneral convex optimization. We present a quantum algorithm that can optimize a\nconvex function over an $n$-dimensional convex body using $\\tilde{O}(n)$\nqueries to oracles that evaluate the objective function and determine\nmembership in the convex body. This represents a quadratic improvement over the\nbest-known classical algorithm. We also study limitations on the power of\nquantum computers for general convex optimization, showing that it requires\n$\\tilde{\\Omega}(\\sqrt n)$ evaluation queries and $\\Omega(\\sqrt{n})$ membership\nqueries.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 14:05:38 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 15:46:19 GMT"}, {"version": "v3", "created": "Wed, 18 Dec 2019 06:07:23 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Chakrabarti", "Shouvanik", ""], ["Childs", "Andrew M.", ""], ["Li", "Tongyang", ""], ["Wu", "Xiaodi", ""]]}, {"id": "1809.01759", "submitter": "L\\'aszl\\'o Kozma", "authors": "Parinya Chalermsook, Mayank Goswami, L\\'aszl\\'o Kozma, Kurt Mehlhorn,\n  and Thatchaphol Saranurak", "title": "Multi-finger binary search trees", "comments": "To be presented at ISAAC 2018. Also extends (and supersedes parts of)\n  arXiv:1603.04892, with possible text overlaps", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study multi-finger binary search trees (BSTs), a far-reaching extension of\nthe classical BST model, with connections to the well-studied $k$-server\nproblem. Finger search is a popular technique for speeding up BST operations\nwhen a query sequence has locality of reference. BSTs with multiple fingers can\nexploit more general regularities in the input. In this paper we consider the\ncost of serving a sequence of queries in an optimal (offline) BST with $k$\nfingers, a powerful benchmark against which other algorithms can be measured.\n  We show that the $k$-finger optimum can be matched by a standard dynamic BST\n(having a single root-finger) with an $O(\\log{k})$ factor overhead. This result\nis tight for all $k$, improving the $O(k)$ factor implicit in earlier work.\nFurthermore, we describe new online BSTs that match this bound up to a\n$(\\log{k})^{O(1)}$ factor. Previously only the \"one-finger\" special case was\nknown to hold for an online BST (Iacono, Langerman, 2016; Cole et al., 2000).\nSplay trees, assuming their conjectured optimality (Sleator and Tarjan, 1983),\nwould have to match our bounds for all $k$.\n  Our online algorithms are randomized and combine techniques developed for the\n$k$-server problem with a multiplicative-weights scheme for learning tree\nmetrics. To our knowledge, this is the first time when tools developed for the\n$k$-server problem are used in BSTs. As an application of our $k$-finger\nresults, we show that BSTs can efficiently serve queries that are close to some\nrecently accessed item. This is a (restricted) form of the unified property\n(Iacono, 2001) that was previously not known to hold for any BST algorithm,\nonline or offline.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 22:52:51 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Chalermsook", "Parinya", ""], ["Goswami", "Mayank", ""], ["Kozma", "L\u00e1szl\u00f3", ""], ["Mehlhorn", "Kurt", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "1809.01896", "submitter": "Laurent Viennot", "authors": "Laurent Viennot (IRIF, GANG), Yacine Boufkhad (GANG, IRIF), Leonardo\n  Linguaglossa (LINCS), Fabien Mathieu (LINCS), Diego Perino", "title": "Efficient Loop Detection in Forwarding Networks and Representing Atoms\n  in a Field of Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of detecting loops in a forwarding network is known to be\nNP-complete when general rules such as wildcard expressions are used. Yet,\nnetwork analyzer tools such as Netplumber (Kazemian et al., NSDI'13) or\nVeriflow (Khurshid et al., NSDI'13) efficiently solve this problem in networks\nwith thousands of forwarding rules. In this paper, we complement such\nexperimental validation of practical heuristics with the first provably\nefficient algorithm in the context of general rules. Our main tool is a\ncanonical representation of the atoms (i.e. the minimal non-empty sets) of the\nfield of sets generated by a collection of sets. This tool is particularly\nsuited when the intersection of two sets can be efficiently computed and\nrepresented. In the case of forwarding networks, each forwarding rule is\nassociated with the set of packet headers it matches. The atoms then correspond\nto classes of headers with same behavior in the network. We propose an\nalgorithm for atom computation and provide the first polynomial time algorithm\nfor loop detection in terms of number of classes (which can be exponential in\ngeneral). This contrasts with previous methods that can be exponential, even in\nsimple cases with linear number of classes. Second, we introduce a notion of\nnetwork dimension captured by the overlapping degree of forwarding rules. The\nvalues of this measure appear to be very low in practice and constant\noverlapping degree ensures polynomial number of header classes. Forwarding loop\ndetection is thus polynomial in forwarding networks with constant overlapping\ndegree.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 09:18:50 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Viennot", "Laurent", "", "IRIF, GANG"], ["Boufkhad", "Yacine", "", "GANG, IRIF"], ["Linguaglossa", "Leonardo", "", "LINCS"], ["Mathieu", "Fabien", "", "LINCS"], ["Perino", "Diego", ""]]}, {"id": "1809.01998", "submitter": "Irena Rusu Ph.D.", "authors": "Irena Rusu", "title": "Min (A)cyclic Feedback Vertex Sets and Min Ones Monotone 3-SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In directed graphs, we investigate the problems of finding: 1) a minimum\nfeedback vertex set (also called the Feedback Vertex Set problem, or MFVS), 2)\na feedback vertex set inducing an acyclic graph (also called the Vertex\n2-Coloring without Monochromatic Cycles problem, or Acyclic FVS) and 3) a\nminimum feedback vertex set inducing an acyclic graph (Acyclic MFVS).\n  We show that these problems are strongly related to (variants of) Monotone\n3-SAT and Monotone NAE 3-SAT, where monotone means that all literals are in\npositive form. As a consequence, we deduce several NP-completeness results on\nrestricted versions of these problems. In particular, we define the 2-Choice\nversion of an optimization problem to be its restriction where the optimum\nvalue is known to be either D or D+1 for some integer D, and the problem is\nreduced to decide which of D or D+1 is the optimum value. We show that the\n2-Choice versions of MFVS, Acyclic MFVS, Min Ones Monotone 3-SAT and Min Ones\nMonotone NAE 3-SAT are NP-complete. The two latter problems are the variants of\nMonotone 3-SAT and respectively Monotone NAE 3-SAT requiring that the truth\nassignment minimize the number of variables set to true.\n  Finally, we propose two classes of directed graphs for which Acyclic FVS is\npolynomially solvable, namely flow reducible graphs (for which MFVS is already\nknown to be polynomially solvable) and C1P-digraphs (defined by an adjacency\nmatrix with the Consecutive Ones Property).\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 13:58:03 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Rusu", "Irena", ""]]}, {"id": "1809.02016", "submitter": "Alessandro Arlotto", "authors": "Alessandro Arlotto and Xinchang Xie", "title": "Logarithmic regret in the dynamic and stochastic knapsack problem with\n  equal rewards", "comments": "33 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DM cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a dynamic and stochastic knapsack problem in which a decision maker\nis sequentially presented with items arriving according to a Bernoulli process\nover $n$ discrete time periods. Items have equal rewards and independent\nweights that are drawn from a known non-negative continuous distribution $F$.\nThe decision maker seeks to maximize the expected total reward of the items\nthat she includes in the knapsack while satisfying a capacity constraint and\nwhile making terminal decisions as soon as each item weight is revealed. Under\nmild regularity conditions on the weight distribution $F$, we prove that the\nregret---the expected difference between the performance of the best sequential\nalgorithm and that of a prophet who sees all of the weights before making any\ndecision---is, at most, logarithmic in $n$. Our proof is constructive. We\ndevise a reoptimized heuristic that achieves this regret bound.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 14:34:21 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 17:05:31 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 15:16:33 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Arlotto", "Alessandro", ""], ["Xie", "Xinchang", ""]]}, {"id": "1809.02271", "submitter": "David Harris", "authors": "David G. Harris, Shi Li, Thomas Pensyl, Aravind Srinivasan, Khoa Trinh", "title": "Approximation algorithms for stochastic clustering", "comments": "The version of this paper published in JMLR is incorrect;\n  specifically, Theorem 14 of that paper appears to be fatally flawed. The\n  version posted here on arxiv removes the claimed incorrect results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic settings for clustering, and develop provably-good\napproximation algorithms for a number of these notions. These algorithms yield\nbetter approximation ratios compared to the usual deterministic clustering\nsetting. Additionally, they offer a number of advantages including clustering\nwhich is fairer and has better long-term behavior for each user. In particular,\nthey ensure that *every user* is guaranteed to get good service (on average).\nWe also complement some of these with impossibility results.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 01:35:02 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 02:26:32 GMT"}, {"version": "v3", "created": "Thu, 27 Jun 2019 01:02:41 GMT"}, {"version": "v4", "created": "Tue, 10 Sep 2019 11:34:55 GMT"}, {"version": "v5", "created": "Wed, 16 Dec 2020 23:57:30 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Harris", "David G.", ""], ["Li", "Shi", ""], ["Pensyl", "Thomas", ""], ["Srinivasan", "Aravind", ""], ["Trinh", "Khoa", ""]]}, {"id": "1809.02314", "submitter": "Kaito Fujii", "authors": "Kaito Fujii and Tasuku Soma", "title": "Fast greedy algorithms for dictionary selection with generalized\n  sparsity constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dictionary selection, several atoms are selected from finite candidates\nthat successfully approximate given data points in the sparse representation.\nWe propose a novel efficient greedy algorithm for dictionary selection. Not\nonly does our algorithm work much faster than the known methods, but it can\nalso handle more complex sparsity constraints, such as average sparsity. Using\nnumerical experiments, we show that our algorithm outperforms the known methods\nfor dictionary selection, achieving competitive performances with dictionary\nlearning algorithms in a smaller running time.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 05:20:12 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Fujii", "Kaito", ""], ["Soma", "Tasuku", ""]]}, {"id": "1809.02350", "submitter": "Francesco Silvestri", "authors": "Matteo Ceccarello and Anne Driemel and Francesco Silvestri", "title": "FRESH: Fr\\'echet Similarity with Hashing", "comments": null, "journal-ref": "Proc. of Algorithms and Data Structures Symposium (WADS) 2019", "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the $r$-range search problem for curves under the\ncontinuous Fr\\'echet distance: given a dataset $S$ of $n$ polygonal curves and\na threshold $r>0$, construct a data structure that, for any query curve $q$,\nefficiently returns all entries in $S$ with distance at most $r$ from $q$. We\npropose FRESH, an approximate and randomized approach for $r$-range search,\nthat leverages on a locality sensitive hashing scheme for detecting candidate\nnear neighbors of the query curve, and on a subsequent pruning step based on a\ncascade of curve simplifications. We experimentally compare \\fresh to exact and\ndeterministic solutions, and we show that high performance can be reached by\nsuitably relaxing precision and recall.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 08:29:18 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 10:20:24 GMT"}, {"version": "v3", "created": "Wed, 5 Jun 2019 07:16:07 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Ceccarello", "Matteo", ""], ["Driemel", "Anne", ""], ["Silvestri", "Francesco", ""]]}, {"id": "1809.02376", "submitter": "Assaf Naor", "authors": "Assaf Naor", "title": "Metric dimension reduction: A snapshot of the Ribe program", "comments": "proceedings of ICM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.DS math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this article is to survey some of the context, achievements,\nchallenges and mysteries of the field of metric dimension reduction, including\nnew perspectives on major older results as well as recent advances.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 09:55:00 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Naor", "Assaf", ""]]}, {"id": "1809.02434", "submitter": "Riccardo Dondi", "authors": "Riccardo Dondi, Mohammad Mehdi Hosseinzadeh, Giancarlo Mauri, Italo\n  Zoppis", "title": "Top-k Overlapping Densest Subgraphs: Approximation and Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in graph mining is finding dense subgraphs, with several\napplications in different fields, a notable example being identifying\ncommunities. While a lot of effort has been put on the problem of finding a\nsingle dense subgraph, only recently the focus has been shifted to the problem\nof finding a set of densest subgraphs. Some approaches aim at finding disjoint\nsubgraphs, while in many real-world networks communities are often overlapping.\nAn approach introduced to find possible overlapping subgraphs is the Top-k\nOverlapping Densest Subgraphs problem. For a given integer k >= 1, the goal of\nthis problem is to find a set of k densest subgraphs that may share some\nvertices. The objective function to be maximized takes into account both the\ndensity of the subgraphs and the distance between subgraphs in the solution.\n  The Top-k Overlapping Densest Subgraphs problem has been shown to admit a\n1/10-factor approximation algorithm. Furthermore, the computational complexity\nof the problem has been left open. In this paper, we present contributions\nconcerning the approximability and the computational complexity of the problem.\nFor the approximability, we present approximation algorithms that improves the\napproximation factor to 1/2 , when k is bounded by the vertex set, and to 2/3\nwhen k is a constant. For the computational complexity, we show that the\nproblem is NP-hard even when k = 3.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 12:21:03 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 12:33:42 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Dondi", "Riccardo", ""], ["Hosseinzadeh", "Mohammad Mehdi", ""], ["Mauri", "Giancarlo", ""], ["Zoppis", "Italo", ""]]}, {"id": "1809.02513", "submitter": "Yixin Cao", "authors": "Jie You and Yixin Cao and Jianxin Wang", "title": "Local Coloring and its Complexity", "comments": "There is a crucial mistake in our first result", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $k$-coloring of a graph is an assignment of integers between $1$ and $k$ to\nvertices in the graph such that the endpoints of each edge receive different\nnumbers. We study a local variation of the coloring problem, which imposes\nfurther requirements on three vertices: We are not allowed to use two\nconsecutive numbers for a path on three vertices, or three consecutive numbers\nfor a cycle on three vertices. Given a graph $G$ and a positive integer $k$,\nthe local coloring problem asks for whether $G$ admits a local $k$-coloring. We\ngive a characterization of graphs admitting local $3$-coloring, which implies a\nsimple polynomial-time algorithm for it. Li et\nal.~[\\href{http://dx.doi.org/10.1016/j.ipl.2017.09.013} {Inf.~Proc.~Letters 130\n(2018)}] recently showed it is NP-hard when $k$ is an odd number of at least\n$5$, or $k = 4$. We show that it is NP-hard when $k$ is any fixed even number\nat least $6$, thereby completing the complexity picture of this problem. We\nclose the paper with a short remark on local colorings of perfect graphs.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 14:47:13 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 10:04:59 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["You", "Jie", ""], ["Cao", "Yixin", ""], ["Wang", "Jianxin", ""]]}, {"id": "1809.02517", "submitter": "Tatiana Starikovskaya", "authors": "Pawe{\\l} Gawrychowski, Tatiana Starikovskaya", "title": "Streaming dictionary matching with mismatches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $k$-mismatch problem we are given a pattern of length $n$ and a text\nand must find all locations where the Hamming distance between the pattern and\nthe text is at most $k$. A series of recent breakthroughs have resulted in an\nultra-efficient streaming algorithm for this problem that requires only $O(k\n\\log \\frac{n}{k})$ space and $O(\\log \\frac{n}{k} (\\sqrt{k \\log k} + \\log^3 n))$\ntime per letter [Clifford, Kociumaka, Porat, SODA 2019]. In this work, we\nconsider a strictly harder problem called dictionary matching with $k$\nmismatches. In this problem, we are given a dictionary of $d$ patterns, where\nthe length of each pattern is at most $n$, and must find all substrings of the\ntext that are within Hamming distance $k$ from one of the patterns. We develop\na streaming algorithm for this problem with $O(k d \\log^k d\n\\mathrm{polylog}(n))$ space and $O(k \\log^{k} d \\mathrm{polylog}(n) +\n|\\mathrm{occ}|)$ time per position of the text. The algorithm is randomised and\noutputs correct answers with high probability. On the lower bound side, we show\nthat any streaming algorithm for dictionary matching with $k$ mismatches\nrequires $\\Omega(k d)$ bits of space.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 14:54:53 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 12:10:23 GMT"}, {"version": "v3", "created": "Sun, 20 Jun 2021 15:16:20 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Starikovskaya", "Tatiana", ""]]}, {"id": "1809.02575", "submitter": "Shuang Song", "authors": "Shuang Song, Susan Little, Sanjay Mehta, Staal Vinterbo, Kamalika\n  Chaudhuri", "title": "Differentially Private Continual Release of Graph Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by understanding the dynamics of sensitive social networks over\ntime, we consider the problem of continual release of statistics in a network\nthat arrives online, while preserving privacy of its participants. For our\nprivacy notion, we use differential privacy -- the gold standard in privacy for\nstatistical data analysis. The main challenge in this problem is maintaining a\ngood privacy-utility tradeoff; naive solutions that compose across time, as\nwell as solutions suited to tabular data either lead to poor utility or do not\ndirectly apply. In this work, we show that if there is a publicly known upper\nbound on the maximum degree of any node in the entire network sequence, then we\ncan release many common graph statistics such as degree distributions and\nsubgraph counts continually with a better privacy-accuracy tradeoff.\n  Code available at https://bitbucket.org/shs037/graphprivacycode\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 16:58:59 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 02:01:44 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Song", "Shuang", ""], ["Little", "Susan", ""], ["Mehta", "Sanjay", ""], ["Vinterbo", "Staal", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1809.02636", "submitter": "Dekel Tsur", "authors": "Dekel Tsur", "title": "Parameterized algorithm for 3-path vertex cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the 3-path vertex cover problem, the input is an undirected graph $G$ and\nan integer $k$. The goal is to decide whether there is a set of vertices $S$ of\nsize at most $k$ such that every path with 3 vertices in $G$ contains at least\none vertex of $S$. In this paper we give parameterized algorithm for 3-path\ncover whose time complexity is $O^*(1.713^k)$. Our algorithm is faster than\nprevious algorithms for this problem.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 18:46:11 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Tsur", "Dekel", ""]]}, {"id": "1809.02656", "submitter": "Satu Elisa Schaeffer", "authors": "Nancy A. Arellano-Arriaga and Juli\\'an Molina and Satu Elisa Schaeffer\n  and Ada M. \\'Alvarez-Socarr\\'as and Iris A. Mart\\'inez-Salazar", "title": "Complexity of MLDP", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We carry out an explicit examination of the NP-hardness of a bi- objective\noptimization problem to minimize distance and latency of a single-vehicle route\ndesigned to serve a set of client requests. In addition to being a Hamiltonian\ncycle the route is to minimize the traveled distance of the vehicle as well as\nthe total waiting time of the clients along the route.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 20:02:47 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Arellano-Arriaga", "Nancy A.", ""], ["Molina", "Juli\u00e1n", ""], ["Schaeffer", "Satu Elisa", ""], ["\u00c1lvarez-Socarr\u00e1s", "Ada M.", ""], ["Mart\u00ednez-Salazar", "Iris A.", ""]]}, {"id": "1809.02680", "submitter": "Chinmoy Dutta", "authors": "Chinmoy Dutta", "title": "When Hashing Met Matching: Efficient Spatio-Temporal Search for\n  Ridesharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Carpooling, or sharing a ride with other passengers, holds immense potential\nfor urban transportation. Ridesharing platforms enable such sharing of rides\nusing real-time data. Finding ride matches in real-time at urban scale is a\ndifficult combinatorial optimization task and mostly heuristic approaches are\napplied. In this work, we mathematically model the problem as that of finding\nnear-neighbors and devise a novel efficient spatio-temporal search algorithm\nbased on the theory of locality sensitive hashing for Maximum Inner Product\nSearch (MIPS). The proposed algorithm can find $k$ near-optimal potential\nmatches for every ride from a pool of $n$ rides in time $O(n^{1 + \\rho} (k +\n\\log n) \\log k)$ and space $O(n^{1 + \\rho} \\log k)$ for a small $\\rho < 1$. Our\nalgorithm can be extended in several useful and interesting ways increasing its\npractical appeal. Experiments with large NY yellow taxi trip datasets show that\nour algorithm consistently outperforms state-of-the-art heuristic methods\nthereby proving its practical applicability.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 21:24:01 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 06:03:45 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Dutta", "Chinmoy", ""]]}, {"id": "1809.02688", "submitter": "Sebastian Perez-Salazar", "authors": "Sebastian Perez-Salazar, Ishai Menache, Mohit Singh, Alejandro\n  Toriello", "title": "Dynamic Resource Allocation in the Cloud with Near-Optimal Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has motivated renewed interest in resource allocation\nproblems with new consumption models. A common goal is to share a resource,\nsuch as CPU or I/O bandwidth, among distinct users with different demand\npatterns as well as different quality of service requirements. To ensure these\nservice requirements, cloud offerings often come with a service level agreement\n(SLA) between the provider and the users. An SLA specifies the amount of a\nresource a user is entitled to utilize. In many cloud settings, providers would\nlike to operate resources at high utilization while simultaneously respecting\nindividual SLAs. There is typically a tradeoff between these two objectives;\nfor example, utilization can be increased by shifting away resources from idle\nusers to \"scavenger\" workload, but with the risk of the former then becoming\nactive again. We study this fundamental tradeoff by formulating a resource\nallocation model that captures basic properties of cloud computing systems,\nincluding SLAs, highly limited feedback about the state of the system, and\nvariable and unpredictable input sequences. Our main result is a simple and\npractical algorithm that achieves near-optimal performance on the above two\nobjectives. First, we guarantee nearly optimal utilization of the resource even\nif compared to the omniscient offline dynamic optimum. Second, we\nsimultaneously satisfy all individual SLAs up to a small error. The main\nalgorithmic tool is a multiplicative weight update algorithm, and a primal-dual\nargument to obtain its guarantees. We also provide numerical validation on real\ndata to demonstrate the performance of our algorithm in practical applications.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 21:47:13 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 14:33:23 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 04:45:23 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Perez-Salazar", "Sebastian", ""], ["Menache", "Ishai", ""], ["Singh", "Mohit", ""], ["Toriello", "Alejandro", ""]]}, {"id": "1809.02703", "submitter": "Tianyu Liu", "authors": "Tianyu Liu", "title": "Torpid Mixing of Markov Chains for the Six-vertex Model on\n  $\\mathbb{Z}^2$", "comments": "Appeared in RANDOM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the mixing time of two widely used Markov chain\nalgorithms for the six-vertex model, Glauber dynamics and the directed-loop\nalgorithm, on the square lattice $\\mathbb{Z}^2$. We prove, for the first time\nthat, on finite regions of the square lattice these Markov chains are torpidly\nmixing under parameter settings in the ferroelectric phase and the\nanti-ferroelectric phase.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 22:42:52 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Liu", "Tianyu", ""]]}, {"id": "1809.02792", "submitter": "Nicola Prezza", "authors": "Travis Gagie, Gonzalo Navarro, and Nicola Prezza", "title": "Fully-Functional Suffix Trees and Optimal Text Searching in BWT-runs\n  Bounded Space", "comments": "submitted version; optimal count and locate in smaller space: O(r log\n  log_w(n/r + sigma))", "journal-ref": null, "doi": "10.1145/3375890", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing highly repetitive texts - such as genomic databases, software\nrepositories and versioned text collections - has become an important problem\nsince the turn of the millennium. A relevant compressibility measure for\nrepetitive texts is r, the number of runs in their Burrows-Wheeler Transforms\n(BWTs). One of the earliest indexes for repetitive collections, the Run-Length\nFM-index, used O(r) space and was able to efficiently count the number of\noccurrences of a pattern of length m in the text (in loglogarithmic time per\npattern symbol, with current techniques). However, it was unable to locate the\npositions of those occurrences efficiently within a space bounded in terms of\nr. In this paper we close this long-standing problem, showing how to extend the\nRun-Length FM-index so that it can locate the occ occurrences efficiently\nwithin O(r) space (in loglogarithmic time each), and reaching optimal time, O(m\n+ occ), within O(r log log w ({\\sigma} + n/r)) space, for a text of length n\nover an alphabet of size {\\sigma} on a RAM machine with words of w =\n{\\Omega}(log n) bits. Within that space, our index can also count in optimal\ntime, O(m). Multiplying the space by O(w/ log {\\sigma}), we support count and\nlocate in O(dm log({\\sigma})/we) and O(dm log({\\sigma})/we + occ) time, which\nis optimal in the packed setting and had not been obtained before in compressed\nspace. We also describe a structure using O(r log(n/r)) space that replaces the\ntext and extracts any text substring of length ` in almost-optimal time\nO(log(n/r) + ` log({\\sigma})/w). Within that space, we similarly provide direct\naccess to suffix array, inverse suffix array, and longest common prefix array\ncells, and extend these capabilities to full suffix tree functionality,\ntypically in O(log(n/r)) time per operation.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 12:15:58 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 15:31:22 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Gagie", "Travis", ""], ["Navarro", "Gonzalo", ""], ["Prezza", "Nicola", ""]]}, {"id": "1809.02835", "submitter": "Igor Shinkar", "authors": "Noga Alon, Jonathan D. Cohen, Thomas L. Griffiths, Pasin Manurangsi,\n  Daniel Reichman, Igor Shinkar, Tal Wagner, Alexander Yu", "title": "Multitasking Capacity: Hardness Results and Improved Constructions", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of determining the maximal $\\alpha \\in (0,1]$ such\nthat every matching $M$ of size $k$ (or at most $k$) in a bipartite graph $G$\ncontains an induced matching of size at least $\\alpha |M|$. This measure was\nrecently introduced in Alon et al. (NIPS 2018) and is motivated by\nconnectionist models of cognition as well as modeling interference in wireless\nand communication networks.\n  We prove various hardness results for computing $\\alpha$ either exactly or\napproximately. En route to our results, we also consider the maximum connected\nmatching problem: determining the largest matching $N$ in a graph $G$ such that\nevery two edges in $N$ are connected by an edge. We prove a nearly optimal\n$n^{1-\\epsilon}$ hardness of approximation result (under randomized reductions)\nfor connected matching in bipartite graphs (with both sides of cardinality\n$n$). Towards this end we define bipartite half-covers: A new combinatorial\nobject that may be of independent interest. To the best of our knowledge, the\nbest previous hardness result for the connected matching problem was some\nconstant $\\beta>1$.\n  Finally, we demonstrate the existence of bipartite graphs with $n$ vertices\non each side of average degree $d$, that achieve $\\alpha=1/2-\\epsilon$ for\nmatchings of size sufficiently smaller than $n/poly(d)$. This nearly matches\nthe trivial upper bound of $1/2$ on $\\alpha$ which holds for any graph\ncontaining a path of length 3.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 17:04:36 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Alon", "Noga", ""], ["Cohen", "Jonathan D.", ""], ["Griffiths", "Thomas L.", ""], ["Manurangsi", "Pasin", ""], ["Reichman", "Daniel", ""], ["Shinkar", "Igor", ""], ["Wagner", "Tal", ""], ["Yu", "Alexander", ""]]}, {"id": "1809.02961", "submitter": "Christian Sohler", "authors": "Christian Sohler and David P. Woodruff", "title": "Strong Coresets for k-Median and Subspace Approximation: Goodbye\n  Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain the first strong coresets for the $k$-median and subspace\napproximation problems with sum of distances objective function, on $n$ points\nin $d$ dimensions, with a number of weighted points that is independent of both\n$n$ and $d$; namely, our coresets have size $\\text{poly}(k/\\epsilon)$. A strong\ncoreset $(1+\\epsilon)$-approximates the cost function for all possible sets of\ncenters simultaneously. We also give efficient $\\text{nnz}(A) +\n(n+d)\\text{poly}(k/\\epsilon) + \\exp(\\text{poly}(k/\\epsilon))$ time algorithms\nfor computing these coresets.\n  We obtain the result by introducing a new dimensionality reduction technique\nfor coresets that significantly generalizes an earlier result of Feldman,\nSohler and Schmidt \\cite{FSS13} for squared Euclidean distances to sums of\n$p$-th powers of Euclidean distances for constant $p\\ge1$.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 12:23:57 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Sohler", "Christian", ""], ["Woodruff", "David P.", ""]]}, {"id": "1809.02995", "submitter": "Robert Krauthgamer", "authors": "Alexandr Andoni and Robert Krauthgamer and Yosef Pogrow", "title": "On Solving Linear Systems in Sublinear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study \\emph{sublinear} algorithms that solve linear systems locally. In\nthe classical version of this problem the input is a matrix $S\\in\n\\mathbb{R}^{n\\times n}$ and a vector $b\\in\\mathbb{R}^n$ in the range of $S$,\nand the goal is to output $x\\in \\mathbb{R}^n$ satisfying $Sx=b$. For the case\nwhen the matrix $S$ is symmetric diagonally dominant (SDD), the breakthrough\nalgorithm of Spielman and Teng [STOC 2004] approximately solves this problem in\nnear-linear time (in the input size which is the number of non-zeros in $S$),\nand subsequent papers have further simplified, improved, and generalized the\nalgorithms for this setting.\n  Here we focus on computing one (or a few) coordinates of $x$, which\npotentially allows for sublinear algorithms. Formally, given an index $u\\in\n[n]$ together with $S$ and $b$ as above, the goal is to output an approximation\n$\\hat{x}_u$ for $x^*_u$, where $x^*$ is a fixed solution to $Sx=b$.\n  Our results show that there is a qualitative gap between SDD matrices and the\nmore general class of positive semidefinite (PSD) matrices. For SDD matrices,\nwe develop an algorithm that approximates a single coordinate $x_{u}$ in time\nthat is polylogarithmic in $n$, provided that $S$ is sparse and has a small\ncondition number (e.g., Laplacian of an expander graph). The approximation\nguarantee is additive $| \\hat{x}_u-x^*_u | \\le \\epsilon \\| x^* \\|_\\infty$ for\naccuracy parameter $\\epsilon>0$. We further prove that the condition-number\nassumption is necessary and tight.\n  In contrast to the SDD matrices, we prove that for certain PSD matrices $S$,\nthe running time must be at least polynomial in $n$. This holds even when one\nwants to obtain the same additive approximation, and $S$ has bounded sparsity\nand condition number.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 15:58:46 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Andoni", "Alexandr", ""], ["Krauthgamer", "Robert", ""], ["Pogrow", "Yosef", ""]]}, {"id": "1809.03264", "submitter": "Therese Biedl", "authors": "Therese Biedl, Andreas Kerren", "title": "Proceedings of the 26th International Symposium on Graph Drawing and\n  Network Visualization (GD 2018)", "comments": "41 papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS cs.SI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are the revised accepted papers from the 26th International Symposium\non Graph Drawing and Network Visualization (GD 2018), Barcelona, Spain,\nSeptember 26 - September 28, 2018. Proceedings are also to be published by\nSpringer in the Lecture Notes in Computer Science series.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 12:18:04 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 19:49:07 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Biedl", "Therese", ""], ["Kerren", "Andreas", ""]]}, {"id": "1809.03392", "submitter": "Masahiro Okubo", "authors": "Masahiro Okubo, Tesshu Hanaka, Hirotaka Ono", "title": "Optimal Partition of a Tree with Social Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem to find a partition of \\textcolor{black}{a} graph $G$\nwith maximum social welfare based on social distance between vertices in $G$,\ncalled MaxSWP. This problem is known to be NP-hard in general. In this paper,\nwe first give a complete characterization of optimal partitions of trees with\nsmall diameters. Then, by utilizing these results, we show that MaxSWP can be\nsolved in linear time for trees. Moreover, we show that MaxSWP is NP-hard even\nfor 4-regular graphs.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 15:24:00 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 09:04:54 GMT"}, {"version": "v3", "created": "Fri, 21 Sep 2018 07:11:52 GMT"}, {"version": "v4", "created": "Mon, 12 Nov 2018 05:26:16 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Okubo", "Masahiro", ""], ["Hanaka", "Tesshu", ""], ["Ono", "Hirotaka", ""]]}, {"id": "1809.03685", "submitter": "Soheil Behnezhad", "authors": "MohammadHossein Bateni, Soheil Behnezhad, Mahsa Derakhshan,\n  MohammadTaghi Hajiaghayi, Vahab Mirrokni", "title": "Massively Parallel Dynamic Programming on Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic programming is a powerful technique that is, unfortunately, often\ninherently sequential. That is, there exists no unified method to parallelize\nalgorithms that use dynamic programming. In this paper, we attempt to address\nthis issue in the Massively Parallel Computations (MPC) model which is a\npopular abstraction of MapReduce-like paradigms. Our main result is an\nalgorithmic framework to adapt a large family of dynamic programs defined over\ntrees.\n  We introduce two classes of graph problems that admit dynamic programming\nsolutions on trees. We refer to them as \"(polylog)-expressible\" and\n\"linear-expressible\" problems. We show that both classes can be parallelized in\n$O(\\log n)$ rounds using a sublinear number of machines and a sublinear memory\nper machine. To achieve this result, we introduce a series of techniques that\ncan be plugged together. To illustrate the generality of our framework, we\nimplement in $O(\\log n)$ rounds of MPC, the dynamic programming solution of\ngraph problems such as minimum bisection, $k$-spanning tree, maximum\nindependent set, longest path, etc., when the input graph is a tree.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 05:24:43 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 23:41:03 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Bateni", "MohammadHossein", ""], ["Behnezhad", "Soheil", ""], ["Derakhshan", "Mahsa", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Mirrokni", "Vahab", ""]]}, {"id": "1809.03795", "submitter": "Soheyl Khalilpourazari", "authors": "Soheyl Khalilpourazari, Mohammad Mohammadi", "title": "A new exact algorithm for solving single machine scheduling problems\n  with learning effects and deteriorating jobs", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the single machine scheduling problem with deteriorating jobs\nand learning effects are considered, which is shown in the previous research\nthat the SDR method no longer provides an optimal solution for the problem. In\norder to solve the problem, a new exact algorithm is proposed. Various test\nproblems are solved to evaluate the performance of the proposed heuristic\nalgorithm using different measures. The results indicate that the algorithm can\nsolve various test problems with small, medium and large sizes in a few seconds\nwith an error around 1% where solving the test problems with more than 15 jobs\nis almost impossible by examining all possible permutations in both complexity\nand time aspects.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 11:34:09 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Khalilpourazari", "Soheyl", ""], ["Mohammadi", "Mohammad", ""]]}, {"id": "1809.03986", "submitter": "Emmanouil Zampetakis", "authors": "Constantinos Daskalakis and Themis Gouleakis and Christos Tzamos and\n  Manolis Zampetakis", "title": "Efficient Statistics, in High Dimensions, from Truncated Samples", "comments": "Appeared at 59th Annual IEEE Symposium on Foundations of Computer\n  Science (FOCS), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an efficient algorithm for the classical problem, going back to\nGalton, Pearson, and Fisher, of estimating, with arbitrary accuracy the\nparameters of a multivariate normal distribution from truncated samples.\nTruncated samples from a $d$-variate normal ${\\cal\nN}(\\mathbf{\\mu},\\mathbf{\\Sigma})$ means a samples is only revealed if it falls\nin some subset $S \\subseteq \\mathbb{R}^d$; otherwise the samples are hidden and\ntheir count in proportion to the revealed samples is also hidden. We show that\nthe mean $\\mathbf{\\mu}$ and covariance matrix $\\mathbf{\\Sigma}$ can be\nestimated with arbitrary accuracy in polynomial-time, as long as we have oracle\naccess to $S$, and $S$ has non-trivial measure under the unknown $d$-variate\nnormal distribution. Additionally we show that without oracle access to $S$,\nany non-trivial estimation is impossible.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 15:42:43 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 18:39:09 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Gouleakis", "Themis", ""], ["Tzamos", "Christos", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "1809.04052", "submitter": "Ryan Moulton", "authors": "Ryan Moulton, Yunjiang Jiang", "title": "Maximally Consistent Sampling and the Jaccard Index of Probability\n  Distributions", "comments": "To appear in ICDMW 2018", "journal-ref": null, "doi": "10.1109/ICDM.2018.00050", "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce simple, efficient algorithms for computing a MinHash of a\nprobability distribution, suitable for both sparse and dense data, with\nequivalent running times to the state of the art for both cases. The collision\nprobability of these algorithms is a new measure of the similarity of positive\nvectors which we investigate in detail. We describe the sense in which this\ncollision probability is optimal for any Locality Sensitive Hash based on\nsampling. We argue that this similarity measure is more useful for probability\ndistributions than the similarity pursued by other algorithms for weighted\nMinHash, and is the natural generalization of the Jaccard index.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 17:41:36 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 16:47:56 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Moulton", "Ryan", ""], ["Jiang", "Yunjiang", ""]]}, {"id": "1809.04355", "submitter": "Jian-Jia Chen", "authors": "Jian-Jia Chen and Nikhil Bansal and Samarjit Chakraborty and Georg von\n  der Br\\\"uggen", "title": "Packing Sporadic Real-Time Tasks on Identical Multiprocessor Systems", "comments": "Accepted and to appear in ISAAC 2018, Yi-Lan, Taiwan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-time systems, in addition to the functional correctness recurrent\ntasks must fulfill timing constraints to ensure the correct behavior of the\nsystem. Partitioned scheduling is widely used in real-time systems, i.e., the\ntasks are statically assigned onto processors while ensuring that all timing\nconstraints are met. The decision version of the problem, which is to check\nwhether the deadline constraints of tasks can be satisfied on a given number of\nidentical processors, has been known ${\\cal NP}$-complete in the strong sense.\nSeveral studies on this problem are based on approximations involving resource\naugmentation, i.e., speeding up individual processors. This paper studies\nanother type of resource augmentation by allocating additional processors, a\ntopic that has not been explored until recently. We provide polynomial-time\nalgorithms and analysis, in which the approximation factors are dependent upon\nthe input instances. Specifically, the factors are related to the maximum ratio\nof the period to the relative deadline of a task in the given task set. We also\nshow that these algorithms unfortunately cannot achieve a constant\napproximation factor for general cases. Furthermore, we prove that the problem\ndoes not admit any asymptotic polynomial-time approximation scheme (APTAS)\nunless ${\\cal P}={\\cal NP}$ when the task set has constrained deadlines, i.e.,\nthe relative deadline of a task is no more than the period of the task.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 10:55:23 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Chen", "Jian-Jia", ""], ["Bansal", "Nikhil", ""], ["Chakraborty", "Samarjit", ""], ["von der Br\u00fcggen", "Georg", ""]]}, {"id": "1809.04396", "submitter": "Yuuki Takai", "authors": "Masahiro Ikeda, Atsushi Miyauchi, Yuuki Takai, Yuichi Yoshida", "title": "Finding Cheeger Cuts in Hypergraphs via Heat Equation", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.NA math.AP math.NA math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cheeger's inequality states that a tightly connected subset can be extracted\nfrom a graph $G$ using an eigenvector of the normalized Laplacian associated\nwith $G$. More specifically, we can compute a subset with conductance\n$O(\\sqrt{\\phi_G})$, where $\\phi_G$ is the minimum conductance of a set in $G$.\nIt has recently been shown that Cheeger's inequality can be extended to\nhypergraphs. However, as the normalized Laplacian of a hypergraph is no longer\na matrix, we can only approximate to its eigenvectors; this causes a loss in\nthe conductance of the obtained subset. To address this problem, we here\nconsider the heat equation on hypergraphs, which is a differential equation\nexploiting the normalized Laplacian. We show that the heat equation has a\nunique solution and that we can extract a subset with conductance\n$\\sqrt{\\phi_G}$ from the solution. An analogous result also holds for directed\ngraphs.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 13:06:37 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 09:05:39 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Ikeda", "Masahiro", ""], ["Miyauchi", "Atsushi", ""], ["Takai", "Yuuki", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1809.04578", "submitter": "Jon Kleinberg", "authors": "Jon Kleinberg and Sendhil Mullainathan", "title": "Simplicity Creates Inequity: Implications for Fairness, Stereotypes, and\n  Interpretability", "comments": "Updated version incorporating additional results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.DS cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms are increasingly used to aid, or in some cases supplant, human\ndecision-making, particularly for decisions that hinge on predictions. As a\nresult, two additional features in addition to prediction quality have\ngenerated interest: (i) to facilitate human interaction and understanding with\nthese algorithms, we desire prediction functions that are in some fashion\nsimple or interpretable; and (ii) because they influence consequential\ndecisions, we also want them to produce equitable allocations. We develop a\nformal model to explore the relationship between the demands of simplicity and\nequity. Although the two concepts appear to be motivated by qualitatively\ndistinct goals, we show a fundamental inconsistency between them. Specifically,\nwe formalize a general framework for producing simple prediction functions, and\nin this framework we establish two basic results. First, every simple\nprediction function is strictly improvable: there exists a more complex\nprediction function that is both strictly more efficient and also strictly more\nequitable. Put another way, using a simple prediction function both reduces\nutility for disadvantaged groups and reduces overall welfare relative to other\noptions. Second, we show that simple prediction functions necessarily create\nincentives to use information about individuals' membership in a disadvantaged\ngroup --- incentives that weren't present before simplification, and that work\nagainst these individuals. Thus, simplicity transforms disadvantage into bias\nagainst the disadvantaged group. Our results are not only about algorithms but\nabout any process that produces simple models, and as such they connect to the\npsychology of stereotypes and to an earlier economics literature on statistical\ndiscrimination.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 17:40:18 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 02:50:47 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Kleinberg", "Jon", ""], ["Mullainathan", "Sendhil", ""]]}, {"id": "1809.04726", "submitter": "Ankur Moitra", "authors": "Linus Hamilton and Ankur Moitra", "title": "The Paulsen Problem Made Simple", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Paulsen problem is a basic problem in operator theory that was resolved\nin a recent tour-de-force work of Kwok, Lau, Lee and Ramachandran. In\nparticular, they showed that every $\\epsilon$-nearly equal norm Parseval frame\nin $d$ dimensions is within squared distance $O(\\epsilon d^{13/2})$ of an equal\nnorm Parseval frame. We give a dramatically simpler proof based on the notion\nof radial isotropic position, and along the way show an improved bound of\n$O(\\epsilon d^2)$.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 01:05:12 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 15:02:26 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Hamilton", "Linus", ""], ["Moitra", "Ankur", ""]]}, {"id": "1809.04802", "submitter": "Atsushi Miyauchi", "authors": "Atsushi Miyauchi and Akiko Takeda", "title": "Robust Densest Subgraph Discovery", "comments": "10 pages; Accepted to ICDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense subgraph discovery is an important primitive in graph mining, which has\na wide variety of applications in diverse domains. In the densest subgraph\nproblem, given an undirected graph $G=(V,E)$ with an edge-weight vector\n$w=(w_e)_{e\\in E}$, we aim to find $S\\subseteq V$ that maximizes the density,\ni.e., $w(S)/|S|$, where $w(S)$ is the sum of the weights of the edges in the\nsubgraph induced by $S$. Although the densest subgraph problem is one of the\nmost well-studied optimization problems for dense subgraph discovery, there is\nan implicit strong assumption; it is assumed that the weights of all the edges\nare known exactly as input. In real-world applications, there are often cases\nwhere we have only uncertain information of the edge weights. In this study, we\nprovide a framework for dense subgraph discovery under the uncertainty of edge\nweights. Specifically, we address such an uncertainty issue using the theory of\nrobust optimization. First, we formulate our fundamental problem, the robust\ndensest subgraph problem, and present a simple algorithm. We then formulate the\nrobust densest subgraph problem with sampling oracle that models dense subgraph\ndiscovery using an edge-weight sampling oracle, and present an algorithm with a\nstrong theoretical performance guarantee. Computational experiments using both\nsynthetic graphs and popular real-world graphs demonstrate the effectiveness of\nour proposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 06:57:07 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Miyauchi", "Atsushi", ""], ["Takeda", "Akiko", ""]]}, {"id": "1809.04818", "submitter": "Emmanuel Abbe A", "authors": "Emmanuel Abbe, Enric Boix, Peter Ralli, Colin Sandon", "title": "Graph powering and spectral robustness", "comments": null, "journal-ref": null, "doi": "10.1137/19M1257135", "report-no": null, "categories": "cs.DS cs.DM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral algorithms, such as principal component analysis and spectral\nclustering, typically require careful data transformations to be effective:\nupon observing a matrix $A$, one may look at the spectrum of $\\psi(A)$ for a\nproperly chosen $\\psi$. The issue is that the spectrum of $A$ might be\ncontaminated by non-informational top eigenvalues, e.g., due to scale`\nvariations in the data, and the application of $\\psi$ aims to remove these.\n  Designing a good functional $\\psi$ (and establishing what good means) is\noften challenging and model dependent. This paper proposes a simple and generic\nconstruction for sparse graphs, $$\\psi(A) = \\1((I+A)^r \\ge1),$$ where $A$\ndenotes the adjacency matrix and $r$ is an integer (less than the graph\ndiameter). This produces a graph connecting vertices from the original graph\nthat are within distance $r$, and is referred to as graph powering. It is shown\nthat graph powering regularizes the graph and decontaminates its spectrum in\nthe following sense: (i) If the graph is drawn from the sparse\nErd\\H{o}s-R\\'enyi ensemble, which has no spectral gap, it is shown that graph\npowering produces a `maximal' spectral gap, with the latter justified by\nestablishing an Alon-Boppana result for powered graphs; (ii) If the graph is\ndrawn from the sparse SBM, graph powering is shown to achieve the fundamental\nlimit for weak recovery (the KS threshold) similarly to \\cite{massoulie-STOC},\nsettling an open problem therein. Further, graph powering is shown to be\nsignificantly more robust to tangles and cliques than previous spectral\nalgorithms based on self-avoiding or nonbacktracking walk counts\n\\cite{massoulie-STOC,Mossel_SBM2,bordenave,colin3}. This is illustrated on a\ngeometric block model that is dense in cliques.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 08:05:17 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Abbe", "Emmanuel", ""], ["Boix", "Enric", ""], ["Ralli", "Peter", ""], ["Sandon", "Colin", ""]]}, {"id": "1809.05058", "submitter": "Matthias Becker", "authors": "Matthias Becker and Nicolas Ginoux and Sebastien Martin and Zsuzsanna\n  Roka", "title": "Tire Noise Optimization Problem: a Mixed Integer Linear Program Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Mixed Integer Linear Program (MILP) approach in order to model\nthe nonlinear problem of minimizing the tire noise. We first take more\nindustrial constraints into account than in a former work of the authors. Then,\nwe associate a Branch-and-Cut algorithm to the MILP to obtain exact solutions.\nWe compare our experimental results with those obtained by other methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 07:51:14 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Becker", "Matthias", ""], ["Ginoux", "Nicolas", ""], ["Martin", "Sebastien", ""], ["Roka", "Zsuzsanna", ""]]}, {"id": "1809.05082", "submitter": "Shipra Agrawal", "authors": "Shipra Agrawal, Mohammad Shadravan, Cliff Stein", "title": "Submodular Secretary Problem with Shortlists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In submodular $k$-secretary problem, the goal is to select $k$ items in a\nrandomly ordered input so as to maximize the expected value of a given monotone\nsubmodular function on the set of selected items. In this paper, we introduce a\nrelaxation of this problem, which we refer to as submodular $k$-secretary\nproblem with shortlists. In the proposed problem setting, the algorithm is\nallowed to choose more than $k$ items as part of a shortlist. Then, after\nseeing the entire input, the algorithm can choose a subset of size $k$ from the\nbigger set of items in the shortlist. We are interested in understanding to\nwhat extent this relaxation can improve the achievable competitive ratio for\nthe submodular $k$-secretary problem. In particular, using an $O(k)$ shortlist,\ncan an online algorithm achieve a competitive ratio close to the best\nachievable online approximation factor for this problem? We answer this\nquestion affirmatively by giving a polynomial time algorithm that achieves a\n$1-1/e-\\epsilon -O(k^{-1})$ competitive ratio for any constant $\\epsilon > 0$,\nusing a shortlist of size $\\eta_\\epsilon(k) = O(k)$. Also, for the special case\nof m-submodular functions, we demonstrate an algorithm that achieves a\n$1-\\epsilon$ competitive ratio for any constant $\\epsilon > 0$, using an $O(1)$\nshortlist. Finally, we show that our algorithm can be implemented in the\nstreaming setting using a memory buffer of size $\\eta_\\epsilon(k) = O(k)$ to\nachieve a $1 - 1/e - \\epsilon-O(k^{-1})$ approximation for submodular function\nmaximization in the random order streaming model. This substantially improves\nupon the previously best known approximation factor of $1/2 + 8 \\times\n10^{-14}$ [Norouzi-Fard et al. 2018] that used a memory buffer of size $O(k\n\\log k)$.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 17:43:01 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 00:39:02 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Agrawal", "Shipra", ""], ["Shadravan", "Mohammad", ""], ["Stein", "Cliff", ""]]}, {"id": "1809.05419", "submitter": "Ran Ben Basat", "authors": "Ran Ben Basat, Seungbum Jo, Srinivasa Rao Satti, and Shubham Ugare", "title": "Approximate Query Processing over Static Sets and Sliding Windows", "comments": "To appear in ISAAC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing of static and dynamic sets is fundamental to a large set of\napplications such as information retrieval and caching. Denoting the\ncharacteristic vector of the set by B, we consider the problem of encoding sets\nand multisets to support approximate versions of the operations rank(i) (i.e.,\ncomputing sum_{j <= i}B[j]) and select(i) (i.e., finding min{p | rank(p) >= i})\nqueries. We study multiple types of approximations (allowing an error in the\nquery or the result) and present lower bounds and succinct data structures for\nseveral variants of the problem. We also extend our model to sliding windows,\nin which we process a stream of elements and compute suffix sums. This is a\ngeneralization of the window summation problem that allows the user to specify\nthe window size at query time. Here, we provide an algorithm that supports\nupdates and queries in constant time while requiring just (1+o(1)) factor more\nspace than the fixed-window summation algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 13:45:27 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Basat", "Ran Ben", ""], ["Jo", "Seungbum", ""], ["Satti", "Srinivasa Rao", ""], ["Ugare", "Shubham", ""]]}, {"id": "1809.05481", "submitter": "Daniel Tischner", "authors": "Daniel Tischner", "title": "Multi-Modal Route Planning in Road and Transit Networks", "comments": "Master's thesis, 80 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present algorithms for multi-modal route planning in road and public\ntransit networks, as well as in combined networks.\n  Therefore, we explore the nearest neighbor and shortest path problem and\npropose solutions based on Cover-Trees, ALT and CSA.\n  Further, we illustrate the theory behind the algorithms, give a short\noverview of other techniques, present experimental results and compare the\ntechniques with each other.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 16:02:44 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Tischner", "Daniel", ""]]}, {"id": "1809.05745", "submitter": "Guohui Lin", "authors": "Longcheng Liu, Yong Chen, Jianming Dong, Randy Goebel, Guohui Lin, Yue\n  Luo, Guanqun Ni, Bing Su, and An Zhang", "title": "Approximation algorithms for the three-machine proportionate mixed shop\n  scheduling", "comments": "An extended abstract containing a subset of results has been accepted\n  by AAIM 2018. This is the full version with 20 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixed shop is a manufacturing infrastructure designed to process a mixture\nof a set of flow-shop jobs and a set of open-shop jobs. Mixed shops are in\ngeneral much more complex to schedule than flow-shops and open-shops, and have\nbeen studied since the 1980's. We consider the three machine proportionate\nmixed shop problem denoted as $M3 \\mid prpt \\mid C_{\\max}$, in which each job\nhas equal processing times on all three machines. Koulamas and Kyparisis [{\\it\nEuropean Journal of Operational Research}, 243:70--74,2015] showed that the\nproblem is solvable in polynomial time in some very special cases; for the\nnon-solvable case, they proposed a $5/3$-approximation algorithm. In this\npaper, we present an improved $4/3$-approximation algorithm and show that this\nratio of $4/3$ is asymptotically tight; when the largest job is a flow-shop\njob, we present a fully polynomial-time approximation scheme (FPTAS). On the\nnegative side, while the $F3 \\mid prpt \\mid C_{\\max}$ problem is\npolynomial-time solvable, we show an interesting hardness result that adding\none open-shop job to the job set makes the problem NP-hard if this open-shop\njob is larger than any flow-shop job. We are able to design an FPTAS for this\nspecial case too.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 17:16:26 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Liu", "Longcheng", ""], ["Chen", "Yong", ""], ["Dong", "Jianming", ""], ["Goebel", "Randy", ""], ["Lin", "Guohui", ""], ["Luo", "Yue", ""], ["Ni", "Guanqun", ""], ["Su", "Bing", ""], ["Zhang", "An", ""]]}, {"id": "1809.05791", "submitter": "Marek Adamczyk", "authors": "Marek Adamczyk, Jaros{\\l}aw Byrka, Jan Marcinkowski, Syed M. Meesum,\n  Micha{\\l} W{\\l}odarczyk", "title": "Constant factor FPT approximation for capacitated k-median", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capacitated k-median is one of the few outstanding optimization problems for\nwhich the existence of a polynomial time constant factor approximation\nalgorithm remains an open problem. In a series of recent papers algorithms\nproducing solutions violating either the number of facilities or the capacity\nby a multiplicative factor were obtained. However, to produce solutions without\nviolations appears to be hard and potentially requires different algorithmic\ntechniques. Notably, if parameterized by the number of facilities $k$, the\nproblem is also $W[2]$ hard, making the existence of an exact FPT algorithm\nunlikely. In this work we provide an FPT-time constant factor approximation\nalgorithm preserving both cardinality and capacity of the facilities. The\nalgorithm runs in time $2^{\\mathcal{O}(k\\log k)}n^{\\mathcal{O}(1)}$ and\nachieves an approximation ratio of $7+\\varepsilon$.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 01:42:13 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Adamczyk", "Marek", ""], ["Byrka", "Jaros\u0142aw", ""], ["Marcinkowski", "Jan", ""], ["Meesum", "Syed M.", ""], ["W\u0142odarczyk", "Micha\u0142", ""]]}, {"id": "1809.05844", "submitter": "Marc Wolf", "authors": "Marc Wolf, Fran\\c{c}ois Wolf, Corentin Le Coz", "title": "Calculation of extended gcd by normalization", "comments": "9 pages", "journal-ref": "SCIREA Journal of Mathematics. Vol. 3, No. 3, 2018, pp. 118 - 131", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a new algorithm solving the extended gcd problem, which provides a\nsolution minimizing one of the two coordinates. The algorithm relies on\nelementary arithmetic properties.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 09:43:53 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Wolf", "Marc", ""], ["Wolf", "Fran\u00e7ois", ""], ["Coz", "Corentin Le", ""]]}, {"id": "1809.06041", "submitter": "Guillaume Ducoffe", "authors": "Guillaume Ducoffe and Arne Leitert", "title": "Equivalence between pathbreadth and strong pathbreadth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We say that a given graph $G = (V, E)$ has \\emph{pathbreadth} at most $\\rho$,\ndenoted $\\pb(G) \\leq \\rho$, if there exists a Roberston and Seymour's path\ndecomposition where every bag is contained in the $\\rho$-neighbourhood of some\nvertex. Similarly, we say that $G$ has \\emph{strong pathbreadth} at most\n$\\rho$, denoted $\\spb(G) \\leq \\rho$, if there exists a Roberston and Seymour's\npath decomposition where every bag is the complete $\\rho$-neighbourhood of some\nvertex. It is straightforward that $\\pb(G) \\leq \\spb(G)$ for any graph $G$.\nInspired from a close conjecture in [Leitert and Dragan, COCOA'16], we prove in\nthis note that $\\spb(G) \\leq 4 \\cdot \\pb(G)$.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 06:40:31 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Ducoffe", "Guillaume", ""], ["Leitert", "Arne", ""]]}, {"id": "1809.06141", "submitter": "Andreas Alpers", "authors": "Andreas Alpers and Peter Gritzmann", "title": "On the Reconstruction of Static and Dynamic Discrete Structures", "comments": "38 pages, 11 figures, 211 references", "journal-ref": "In: R. Ramlau and O. Scherzer (Eds.), The Radon Transform: The\n  First 100 Years and Beyond, De Gruyter, (ISBN 978-3-11-056085-5), 2019", "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study inverse problems of reconstructing static and dynamic discrete\nstructures from tomographic data (with a special focus on the `classical' task\nof reconstructing finite point sets in $\\mathbb{R}^d$). The main emphasis is on\nrecent mathematical developments and new applications, which emerge in\nscientific areas such as physics and materials science, but also in inner\nmathematical fields such as number theory, optimization, and imaging. Along\nwith a concise introduction to the field of discrete tomography, we give\npointers to related aspects of computerized tomography in order to contrast the\nworlds of continuous and discrete inverse problems.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 11:42:06 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Alpers", "Andreas", ""], ["Gritzmann", "Peter", ""]]}, {"id": "1809.06171", "submitter": "Astrid Pieterse", "authors": "Hubie Chen, Bart M. P. Jansen, Astrid Pieterse", "title": "Best-case and Worst-case Sparsifiability of Boolean CSPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We continue the investigation of polynomial-time sparsification for\nNP-complete Boolean Constraint Satisfaction Problems (CSPs). The goal in\nsparsification is to reduce the number of constraints in a problem instance\nwithout changing the answer, such that a bound on the number of resulting\nconstraints can be given in terms of the number of variables n. We investigate\nhow the worst-case sparsification size depends on the types of constraints\nallowed in the problem formulation (the constraint language). Two algorithmic\nresults are presented. The first result essentially shows that for any arity k,\nthe only constraint type for which no nontrivial sparsification is possible has\nexactly one falsifying assignment, and corresponds to logical OR (up to\nnegations). Our second result concerns linear sparsification, that is, a\nreduction to an equivalent instance with O(n) constraints. Using linear algebra\nover rings of integers modulo prime powers, we give an elegant necessary and\nsufficient condition for a constraint type to be captured by a degree-1\npolynomial over such a ring, which yields linear sparsifications. The\ncombination of these algorithmic results allows us to prove two\ncharacterizations that capture the optimal sparsification sizes for a range of\nBoolean CSPs. For NP-complete Boolean CSPs whose constraints are symmetric (the\nsatisfaction depends only on the number of 1 values in the assignment, not on\ntheir positions), we give a complete characterization of which constraint\nlanguages allow for a linear sparsification. For Boolean CSPs in which every\nconstraint has arity at most three, we characterize the optimal size of\nsparsifications in terms of the largest OR that can be expressed by the\nconstraint language.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 13:00:18 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Chen", "Hubie", ""], ["Jansen", "Bart M. P.", ""], ["Pieterse", "Astrid", ""]]}, {"id": "1809.06266", "submitter": "L\\'aszl\\'o V\\'egh", "authors": "Jugal Garg and L\\'aszl\\'o A. V\\'egh", "title": "A Strongly Polynomial Algorithm for Linear Exchange Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a strongly polynomial algorithm for computing an equilibrium in\nArrow-Debreu exchange markets with linear utilities. The main measure of\nprogress is identifying a set of edges that must correspond to best\nbang-per-buck ratios in every equilibrium, called the revealed edge set. We use\na variant of the combinatorial algorithm by Duan and Mehlhorn to identify a new\nrevealed edge in a strongly polynomial number of iterations. Every time a new\nedge is found, we use a subroutine to identify an approximately best possible\nsolution corresponding to the current revealed edge set. Finding the best\nsolution can be reduced to solving a linear program. Even though we are unable\nto solve this LP in strongly polynomial time, we show that it can be\napproximated by a simpler LP with two variables per inequality that is solvable\nin strongly polynomial time.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 15:02:21 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 20:37:14 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Garg", "Jugal", ""], ["V\u00e9gh", "L\u00e1szl\u00f3 A.", ""]]}, {"id": "1809.06474", "submitter": "Krishnakumar Balasubramanian", "authors": "Krishnakumar Balasubramanian, Saeed Ghadimi", "title": "Zeroth-order Nonconvex Stochastic Optimization: Handling Constraints,\n  High-Dimensionality and Saddle-Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and analyze zeroth-order stochastic approximation\nalgorithms for nonconvex and convex optimization, with a focus on addressing\nconstrained optimization, high-dimensional setting and saddle-point avoiding.\nTo handle constrained optimization, we first propose generalizations of the\nconditional gradient algorithm achieving rates similar to the standard\nstochastic gradient algorithm using only zeroth-order information. To\nfacilitate zeroth-order optimization in high-dimensions, we explore the\nadvantages of structural sparsity assumptions. Specifically, (i) we highlight\nan implicit regularization phenomenon where the standard stochastic gradient\nalgorithm with zeroth-order information adapts to the sparsity of the problem\nat hand by just varying the step-size and (ii) propose a truncated stochastic\ngradient algorithm with zeroth-order information, whose rate of convergence\ndepends only poly-logarithmically on the dimensionality. We next focus on\navoiding saddle-points in non-convex setting. Towards that, we interpret the\nGaussian smoothing technique for estimating gradient based on zeroth-order\ninformation as an instantiation of first-order Stein's identity. Based on this,\nwe provide a novel linear-(in dimension) time estimator of the Hessian matrix\nof a function using only zeroth-order information, which is based on\nsecond-order Stein's identity. We then provide an algorithm for avoiding\nsaddle-points, which is based on a zeroth-order cubic regularization Newton's\nmethod and discuss its convergence rates.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 23:30:05 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 02:53:46 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Balasubramanian", "Krishnakumar", ""], ["Ghadimi", "Saeed", ""]]}, {"id": "1809.06506", "submitter": "Tanmay Inamdar", "authors": "Tanmay Inamdar, Kasturi Varadarajan", "title": "On the Partition Set Cover Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several algorithms with an approximation guarantee of $O(\\log n)$ are known\nfor the Set Cover problem, where $n$ is the number of elements. We study a\ngeneralization of the Set Cover problem, called the Partition Set Cover\nproblem. Here, the elements are partitioned into $r$ \\emph{color classes}, and\nwe are required to cover at least $k_t$ elements from each color class\n$\\mathcal{C}_t$, using the minimum number of sets. We give a randomized\nLP-rounding algorithm that is an $O(\\beta + \\log r)$ approximation for the\nPartition Set Cover problem. Here $\\beta$ denotes the approximation guarantee\nfor a related Set Cover instance obtained by rounding the standard LP. As a\ncorollary, we obtain improved approximation guarantees for various set systems\nfor which $\\beta$ is known to be sublogarithmic in $n$. We also extend the LP\nrounding algorithm to obtain $O(\\log r)$ approximations for similar\ngeneralizations of the Facility Location type problems. Finally, we show that\nmany of these results are essentially tight, by showing that it is NP-hard to\nobtain an $o(\\log r)$-approximation for any of these problems.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 02:26:03 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 00:39:06 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Inamdar", "Tanmay", ""], ["Varadarajan", "Kasturi", ""]]}, {"id": "1809.06523", "submitter": "David Bryant", "authors": "Pei Wu and David Bryant and Paul F. Tupper", "title": "Negative type diversities, a multi-dimensional analogue of negative type\n  metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diversities are a generalization of metric spaces in which a non-negative\nvalue is assigned to all finite subsets of a set, rather than just to pairs of\npoints. Here we provide an analogue of the theory of negative type metrics for\ndiversities. We introduce negative type diversities, and show that, as in the\nmetric space case, they are a generalization of $L_1$-embeddable diversities.\nWe provide a number of characterizations of negative type diversities,\nincluding a geometric characterisation. Much of the recent interest in negative\ntype metrics stems from the connections between metric embeddings and\napproximation algorithms. We extend some of this work into the diversity\nsetting, showing that lower bounds for embeddings of negative type metrics into\n$L_1$ can be extended to diversities by using recently established extremal\nresults on hypergraphs.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 04:14:34 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Wu", "Pei", ""], ["Bryant", "David", ""], ["Tupper", "Paul F.", ""]]}, {"id": "1809.06562", "submitter": "Rupei Xu", "authors": "Rupei Xu and Andr\\'as Farag\\'o", "title": "A Simple Approximation for a Hard Routing Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a routing problem which plays an important role in several\napplications, primarily in communication network planning and VLSI layout\ndesign. The original underlying graph algorithmic task is called Disjoint Paths\nproblem. In most applications, one can encounter its capacitated\ngeneralization, which is known as the Unsplitting Flow problem. These\nalgorithmic tasks are very hard in general, but various efficient\n(polynomial-time) approximate solutions are known. Nevertheless, the\napproximations tend to be rather complicated, often rendering them impractical\nin large, complex networks. Our goal is to present a solution that provides a\nsimple, efficient algorithm for the unsplittable flow problem in large directed\ngraphs. The simplicity is achieved by sacrificing a small part of the solution\nspace. This also represents a novel paradigm of approximation: rather than\ngiving up finding an exact solution, we restrict the solution space to its most\nimportant subset and exclude those that are marginal in some sense. Then we\nfind the exact optimum efficiently within the subset. Specifically, the\nsacrificed parts (i.e., the marginal instances) only contain scenarios where\nsome edges are very close to saturation. Therefore, the excluded part is not\nsignificant, since the excluded almost saturated solutions are typically\nundesired in practical applications, anyway.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 07:20:16 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Xu", "Rupei", ""], ["Farag\u00f3", "Andr\u00e1s", ""]]}, {"id": "1809.06564", "submitter": "Rupei Xu", "authors": "Ahmad Askarian, Rupei Xu and Andr\\'as Farag\\'o", "title": "Utilizing Network Structure to Bound the Convergence Rate in Markov\n  Chain Monte Carlo Algorithms", "comments": null, "journal-ref": null, "doi": "10.3390/a9030050", "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the measure of subsets in very large\nnetworks. A prime tool for this purpose is the Markov Chain Monte Carlo (MCMC)\nalgorithm. This algorithm, while extremely useful in many cases, still often\nsuffers from the drawback of very slow convergence. We show that in a special,\nbut important case, it is possible to obtain significantly better bounds on the\nconvergence rate. This special case is when the huge state space can be\naggregated into a smaller number of clusters, in which the states behave {\\em\napproximately} the same way (but their behavior still may not be identical). A\nMarkov chain with this structure is called {\\em quasi-lumpable}. This property\nallows the {\\em aggregation} of states (nodes) into clusters. Our main\ncontribution is a rigorously proved bound on the rate at which the aggregated\nstate distribution approaches its limit in quasi-lumpable Markov chains. We\nalso demonstrate numerically that in certain cases this can indeed lead to a\nsignificantly accelerated way of estimating the measure of subsets. The result\ncan be a useful tool in the analysis of complex networks, whenever they have a\nclustering that aggregates nodes with similar (but not necessarily identical)\nbehavior.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 07:30:27 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Askarian", "Ahmad", ""], ["Xu", "Rupei", ""], ["Farag\u00f3", "Andr\u00e1s", ""]]}, {"id": "1809.06568", "submitter": "Rupei Xu", "authors": "Andr\\'as Farag\\'o and Rupei Xu", "title": "Connectivity and Structure in Large Networks", "comments": null, "journal-ref": null, "doi": "10.4018/978-1-4666-9964-9.ch007", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large real-life complex networks are often modeled by various random graph\nconstructions and hundreds of further references therein. In many cases it is\nnot at all clear how the modeling strength of differently generated random\ngraph model classes relate to each other. We would like to systematically\ninvestigate such issues. Our approach was originally motivated to capture\nproperties of the random network topology of wireless communication networks.\nWe started some investigations, but here we elevate it to a more general level\nthat makes it possible to compare the strength of different classes of random\nnetwork models. Specially, we introduce various classes of random graph models\nthat are significantly more general than the ones that are usually treated in\nthe literature, and show relationships among them. One of our main results is\nthat no random graph model can fall in the following three classes at the same\ntime: (1) random graph models with bounded expected degrees; (2) random graph\nmodels that are asymptotically almost connected; (3) an abstracted version of\ngeometric random graph models with two mild restrictions that we call locality\nand name invariance. In other words, in a mildly restricted, but still very\ngeneral, class of generalized geometric-style models the requirements of\nbounded expected degrees and asymptotic almost connectivity are incompatible.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 07:38:33 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Farag\u00f3", "Andr\u00e1s", ""], ["Xu", "Rupei", ""]]}, {"id": "1809.06727", "submitter": "Anders Martinsson", "authors": "Bernhard Haeupler, Fabian Kuhn, Anders Martinsson, Kalina Petrova, and\n  Pascal Pfister", "title": "Optimal strategies for patrolling fences", "comments": "19 pages, 3 figures. Part of our main result (circle strategy) is new\n  to this version of the paper. A shorter version of this is to appear in the\n  proceedings of ICALP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.MA math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical multi-agent fence patrolling problem asks: What is the maximum\nlength $L$ of a line that $k$ agents with maximum speeds $v_1,\\ldots,v_k$ can\npatrol if each point on the line needs to be visited at least once every unit\nof time. It is easy to see that $L = \\alpha \\sum_{i=1}^k v_i$ for some\nefficiency $\\alpha \\in [\\frac{1}{2},1)$. After a series of works giving better\nand better efficiencies, it was conjectured that the best possible efficiency\napproaches $\\frac{2}{3}$. No upper bounds on the efficiency below $1$ were\nknown. We prove the first such upper bounds and tightly bound the optimal\nefficiency in terms of the minimum ratio of speeds $s = {v_{\\max}}/{v_{\\min}}$\nand the number of agents $k$. Guided by our upper bounds, we construct a scheme\nwhose efficiency approaches $1$, disproving the conjecture of Kawamura and\nSoejima. Our scheme asymptotically matches our upper bounds in terms of the\nmaximal speed difference and the number of agents used, proving them to be\nasymptotically tight.\n  A variation of the fence patrolling problem considers a circular fence\ninstead and asks for its circumference to be maximized. We consider the\nunidirectional case of this variation, where all agents are only allowed to\nmove in one direction, say clockwise. At first, a strategy yielding $L =\n\\max_{r \\in [k]} r \\cdot v_r$ where $v_1 \\geq v_2 \\geq \\dots \\geq v_k$ was\nconjectured to be optimal by Czyzowicz et al. This was proven not to be the\ncase by giving constructions for only specific numbers of agents with marginal\nimprovements of $L$. We give a general construction that yields $L =\n\\frac{1}{33 \\log_e\\log_2(k)} \\sum_{i=1}^k v_i$ for any set of agents, which in\nparticular for the case $1, 1/2, \\dots, 1/k$ diverges as $k \\rightarrow\n\\infty$, thus resolving a conjecture by Kawamura and Soejima affirmatively.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 13:45:14 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 20:46:27 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Kuhn", "Fabian", ""], ["Martinsson", "Anders", ""], ["Petrova", "Kalina", ""], ["Pfister", "Pascal", ""]]}, {"id": "1809.06823", "submitter": "Fabien Tricoire", "authors": "Sophie N. Parragh and Fabien Tricoire", "title": "Branch-and-bound for bi-objective integer programming", "comments": null, "journal-ref": null, "doi": "10.1287/ijoc.2018.0856", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In bi-objective integer optimization the optimal result corresponds to a set\nof non-dominated solutions. We propose a generic bi-objective branch-and-bound\nalgorithm that uses a problem-independent branching rule exploiting available\ninteger solutions and takes advantage of integer objective coefficients. The\ndeveloped algorithm is applied to bi-objective facility location problems, to\nthe bi-objective set covering problem, as well as to the bi-objective team\norienteering problem with time windows. In the latter case, lower bound sets\nare computed by means of column generation. Comparison to state-of-the-art\nexact algorithms shows the effectiveness of the proposed branch-and-bound\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 16:43:38 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Parragh", "Sophie N.", ""], ["Tricoire", "Fabien", ""]]}, {"id": "1809.06831", "submitter": "Theodoros Chondrogiannis", "authors": "Theodoros Chondrogiannis, Panagiotis Bouros, Johann Gamper, Ulf Leser,\n  David B. Blumenthal", "title": "Finding k-Dissimilar Paths with Minimum Collective Length", "comments": "Extended version of the SIGSPATIAL'18 paper under the same title", "journal-ref": "26th ACM SIGSPATIAL International Conference on Advances in\n  Geographic Information Systems (ACM SIGSPATIAL GIS 2018), Seattle,\n  Washington, USA, November 6-9, 2018", "doi": "10.1145/3274895.3274903", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shortest path computation is a fundamental problem in road networks. However,\nin many real-world scenarios, determining solely the shortest path is not\nenough. In this paper, we study the problem of finding k-Dissimilar Paths with\nMinimum Collective Length (kDPwML), which aims at computing a set of paths from\na source s to a target t such that all paths are pairwise dissimilar by at\nleast \\theta and the sum of the path lengths is minimal. We introduce an exact\nalgorithm for the kDPwML problem, which iterates over all possible s-t paths\nwhile employing two pruning techniques to reduce the prohibitively expensive\ncomputational cost. To achieve scalability, we also define the much smaller set\nof the simple single-via paths, and we adapt two algorithms for kDPwML queries\nto iterate over this set. Our experimental analysis on real road networks shows\nthat iterating over all paths is impractical, while iterating over the set of\nsimple single-via paths can lead to scalable solutions with only a small\ntrade-off in the quality of the results.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 17:02:55 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 14:20:34 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Chondrogiannis", "Theodoros", ""], ["Bouros", "Panagiotis", ""], ["Gamper", "Johann", ""], ["Leser", "Ulf", ""], ["Blumenthal", "David B.", ""]]}, {"id": "1809.06986", "submitter": "Ainesh Bakshi", "authors": "Ainesh Bakshi and David P. Woodruff", "title": "Sublinear Time Low-Rank Approximation of Distance Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathbf{P}=\\{ p_1, p_2, \\ldots p_n \\}$ and $\\mathbf{Q} = \\{ q_1, q_2\n\\ldots q_m \\}$ be two point sets in an arbitrary metric space. Let $\\mathbf{A}$\nrepresent the $m\\times n$ pairwise distance matrix with $\\mathbf{A}_{i,j} =\nd(p_i, q_j)$. Such distance matrices are commonly computed in software packages\nand have applications to learning image manifolds, handwriting recognition, and\nmulti-dimensional unfolding, among other things. In an attempt to reduce their\ndescription size, we study low rank approximation of such matrices. Our main\nresult is to show that for any underlying distance metric $d$, it is possible\nto achieve an additive error low-rank approximation in sublinear time. We note\nthat it is provably impossible to achieve such a guarantee in sublinear time\nfor arbitrary matrices $\\mathbf{A}$, and consequently our proof exploits\nspecial properties of distance matrices. We develop a recursive algorithm based\non additive projection-cost preserving sampling. We then show that in general,\nrelative error approximation in sublinear time is impossible for distance\nmatrices, even if one allows for bicriteria solutions. Additionally, we show\nthat if $\\mathbf{P} = \\mathbf{Q}$ and $d$ is the squared Euclidean distance,\nwhich is not a metric but rather the square of a metric, then a relative error\nbicriteria solution can be found in sublinear time.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 02:22:39 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Bakshi", "Ainesh", ""], ["Woodruff", "David P.", ""]]}, {"id": "1809.06987", "submitter": "Colin White", "authors": "Maria-Florina Balcan, Travis Dick, Colin White", "title": "Data-Driven Clustering via Parameterized Lloyd's Families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for clustering points in metric spaces is a long-studied area of\nresearch. Clustering has seen a multitude of work both theoretically, in\nunderstanding the approximation guarantees possible for many objective\nfunctions such as k-median and k-means clustering, and experimentally, in\nfinding the fastest algorithms and seeding procedures for Lloyd's algorithm.\nThe performance of a given clustering algorithm depends on the specific\napplication at hand, and this may not be known up front. For example, a\n\"typical instance\" may vary depending on the application, and different\nclustering heuristics perform differently depending on the instance.\n  In this paper, we define an infinite family of algorithms generalizing\nLloyd's algorithm, with one parameter controlling the initialization procedure,\nand another parameter controlling the local search procedure. This family of\nalgorithms includes the celebrated k-means++ algorithm, as well as the classic\nfarthest-first traversal algorithm. We design efficient learning algorithms\nwhich receive samples from an application-specific distribution over clustering\ninstances and learn a near-optimal clustering algorithm from the class. We show\nthe best parameters vary significantly across datasets such as MNIST, CIFAR,\nand mixtures of Gaussians. Our learned algorithms never perform worse than\nk-means++, and on some datasets we see significant improvements.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 02:36:25 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2019 03:09:42 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 06:44:07 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Dick", "Travis", ""], ["White", "Colin", ""]]}, {"id": "1809.07067", "submitter": "Seungbum Jo", "authors": "Seungbum Jo and Srinivasa Rao Satti", "title": "Encoding two-dimensional range top-k queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of encoding two-dimensional arrays, whose elements\ncome from a total order, for answering \\topk{} queries. The aim is to obtain\nencodings that use space close to the information-theoretic lower bound, which\ncan be constructed efficiently. For an $m \\times n$ array, with $m \\le n$, we\nfirst propose an encoding for answering 1-sided \\topk{} queries, whose query\nrange is restricted to $[1 \\dots m][1 \\dots a]$, for $1 \\le a \\le n$. Next, we\npropose an encoding for answering for the general (4-sided) \\topk{} queries\nthat takes $(m\\lg{{(k+1)n \\choose n}}+2nm(m-1)+o(n))$ bits, which generalizes\nthe \\textit{joint Cartesian tree} of Golin et al. [TCS 2016]. Compared with\ntrivial $O(nm\\lg{n})$-bit encoding, our encoding takes less space when $m =\no(\\lg{n})$. In addition to the upper bound results for the encodings, we also\ngive lower bounds on encodings for answering $1$ and $4$-sided \\topk{} queries,\nwhich show that our upper bound results are almost optimal.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 08:51:58 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 14:01:39 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 15:45:32 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Jo", "Seungbum", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "1809.07299", "submitter": "Argyris Kalogeratos", "authors": "Mathilde Fekom, Nicolas Vayatis, Argyris Kalogeratos", "title": "The Warm-starting Sequential Selection Problem and its Multi-round\n  Extension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Sequential Selection Problem (SSP), immediate and irrevocable\ndecisions need to be made as candidates randomly arrive for a job interview.\nStandard SSP variants, such as the well-known secretary problem, begin with an\nempty selection set (cold-start) and perform the selection process once over a\nsingle candidate set (single-round). In this paper we address these two\nlimitations. First, we introduce the novel Warm-starting SSP (WSSP) setting\nwhich considers at hand a reference set, a set of previously selected items of\na given quality, and tries to update optimally that set by (re-)assigning each\njob at most once. We adopt a cutoff-based approach to optimize a rank-based\nobjective function over the final assignment of the jobs. In our technical\ncontribution, we provide analytical results regarding the proposed WSSP\nsetting, we introduce the algorithm Cutoff-based Cost Minimization (CCM) (and\nthe low failures-CCM, which is more robust to high rate of resignations) that\nadapts to changes in the quality of the reference set thanks to the translation\nmethod we propose. Finally, we implement and test CCM in a multi-round setting\nthat is particularly interesting for real-world application scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 17:01:42 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 10:54:45 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Fekom", "Mathilde", ""], ["Vayatis", "Nicolas", ""], ["Kalogeratos", "Argyris", ""]]}, {"id": "1809.07320", "submitter": "Travis Gagie", "authors": "Travis Gagie, Garance Gourdel and Giovanni Manzini", "title": "Compressing and Indexing Aligned Readsets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how to use one or more assembled or partially assembled\ngenome as the basis for a compressed full-text index of its readset.\nSpecifically, we build a labelled tree by taking the assembled genome as a\ntrunk and grafting onto it the reads that align to it, at the starting\npositions of their alignments. Next, we compute the eXtended Burrows-Wheeler\nTransform (XBWT) of the resulting labelled tree and build a compressed\nfull-text index on that. Although this index can occasionally return false\npositives, it is usually much more compact than the alternatives. Following the\nestablished practice for datasets with many repetitions, we compare different\nfull-text indices by looking at the number of runs in the transformed strings.\nFor a human Chr19 readset our preliminary experiments show that eliminating\nseparators characters from the EBWT reduces the number of runs by 19\\%, from\n220 million to 178 million, and using the XBWT reduces it by a further 15\\%, to\n150 million.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 15:58:53 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 15:09:51 GMT"}, {"version": "v3", "created": "Fri, 1 Feb 2019 11:28:28 GMT"}, {"version": "v4", "created": "Wed, 10 Feb 2021 17:48:30 GMT"}, {"version": "v5", "created": "Tue, 1 Jun 2021 17:49:25 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Gagie", "Travis", ""], ["Gourdel", "Garance", ""], ["Manzini", "Giovanni", ""]]}, {"id": "1809.07425", "submitter": "Samuel Hopkins", "authors": "Samuel B. Hopkins", "title": "Mean Estimation with Sub-Gaussian Rates in Polynomial Time", "comments": "v4: improvements to exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study polynomial time algorithms for estimating the mean of a heavy-tailed\nmultivariate random vector. We assume only that the random vector $X$ has\nfinite mean and covariance. In this setting, the radius of confidence intervals\nachieved by the empirical mean are large compared to the case that $X$ is\nGaussian or sub-Gaussian.\n  We offer the first polynomial time algorithm to estimate the mean with\nsub-Gaussian-size confidence intervals under such mild assumptions. Our\nalgorithm is based on a new semidefinite programming relaxation of a\nhigh-dimensional median. Previous estimators which assumed only existence of\nfinitely-many moments of $X$ either sacrifice sub-Gaussian performance or are\nonly known to be computable via brute-force search procedures requiring time\nexponential in the dimension.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 23:08:29 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 04:35:35 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2018 06:59:56 GMT"}, {"version": "v4", "created": "Tue, 4 Jun 2019 00:47:49 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Hopkins", "Samuel B.", ""]]}, {"id": "1809.07471", "submitter": "Xian Wu", "authors": "Xian Wu, Moses Charikar, Vishnu Natchu", "title": "Local Density Estimation in High Dimensions", "comments": "Preliminary version appeared in ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important question that arises in the study of high dimensional vector\nrepresentations learned from data is: given a set $\\mathcal{D}$ of vectors and\na query $q$, estimate the number of points within a specified distance\nthreshold of $q$. We develop two estimators, LSH Count and Multi-Probe Count\nthat use locality sensitive hashing to preprocess the data to accurately and\nefficiently estimate the answers to such questions via importance sampling. A\nkey innovation is the ability to maintain a small number of hash tables via\npreprocessing data structures and algorithms that sample from multiple buckets\nin each hash table. We give bounds on the space requirements and sample\ncomplexity of our schemes, and demonstrate their effectiveness in experiments\non a standard word embedding dataset.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 04:32:51 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Wu", "Xian", ""], ["Charikar", "Moses", ""], ["Natchu", "Vishnu", ""]]}, {"id": "1809.07481", "submitter": "Haitao Wang", "authors": "Sang Won Bae and Haitao Wang", "title": "$L_1$ Shortest Path Queries in Simple Polygons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $P$ be a simple polygon of $n$ vertices. We consider two-point $L_1$\nshortest path queries in $P$. We build a data structure of $O(n)$ size in\n$O(n)$ time such that given any two query points $s$ and $t$, the length of an\n$L_1$ shortest path from $s$ to $t$ in $P$ can be computed in $O(\\log n)$ time,\nor in $O(1)$ time if both $s$ and $t$ are vertices of $P$, and an actual\nshortest path can be output in additional linear time in the number of edges of\nthe path. To achieve the result, we propose a mountain decomposition of simple\npolygons, which may be interesting in its own right. Most importantly, our\napproach is much simpler than the previous work on this problem.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 05:28:56 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Bae", "Sang Won", ""], ["Wang", "Haitao", ""]]}, {"id": "1809.07599", "submitter": "Jean-Baptiste Cordonnier", "authors": "Sebastian U. Stich, Jean-Baptiste Cordonnier and Martin Jaggi", "title": "Sparsified SGD with Memory", "comments": "to appear at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huge scale machine learning problems are nowadays tackled by distributed\noptimization algorithms, i.e. algorithms that leverage the compute power of\nmany devices for training. The communication overhead is a key bottleneck that\nhinders perfect scalability. Various recent works proposed to use quantization\nor sparsification techniques to reduce the amount of data that needs to be\ncommunicated, for instance by only sending the most significant entries of the\nstochastic gradient (top-k sparsification). Whilst such schemes showed very\npromising performance in practice, they have eluded theoretical analysis so\nfar.\n  In this work we analyze Stochastic Gradient Descent (SGD) with\nk-sparsification or compression (for instance top-k or random-k) and show that\nthis scheme converges at the same rate as vanilla SGD when equipped with error\ncompensation (keeping track of accumulated errors in memory). That is,\ncommunication can be reduced by a factor of the dimension of the problem\n(sometimes even more) whilst still converging at the same rate. We present\nnumerical experiments to illustrate the theoretical findings and the better\nscalability for distributed applications.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 13:02:14 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 21:13:10 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Stich", "Sebastian U.", ""], ["Cordonnier", "Jean-Baptiste", ""], ["Jaggi", "Martin", ""]]}, {"id": "1809.07661", "submitter": "Torben Hagerup", "authors": "Torben Hagerup", "title": "Small Uncolored and Colored Choice Dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A choice dictionary can be initialized with a parameter $n\\in\\mathbb{N}$ and\nsubsequently maintains an initially empty subset $S$ of $\\{1,\\ldots,n\\}$ under\ninsertion, deletion, membership queries and an operation $\\textit{choice}$ that\nreturns an arbitrary element of $S$. The choice dictionary is fundamental in\nspace-efficient computing and has numerous applications. The best previous\nchoice dictionary can be initialized with $n$ and $t\\in\\mathbb{N}$ and\nsubsequently executes all operations in $O(t)$ time and occupies\n$n+O(n({t/w})^t+\\log n)$ bits on a word RAM with a word length of\n$w=\\Omega(\\log n)$ bits. We describe a new choice dictionary that executes all\noperations in constant time and, in addition to the space needed to store the\ninteger $n$, occupies only $n+1$ bits, which is shown to be optimal if\n$w=o(n)$.\n  A generalization of the choice dictionary called a colored choice dictionary\nis initialized with $c\\in\\mathbb{N}$ in addition to $n$ and subsequently\nmaintains a semipartition $(S_0,\\ldots,S_{c-1})$ of $\\{1,\\ldots,n\\}$ under the\noperations $\\textit{setcolor}(j,\\ell)$, which moves $\\ell$ from its current\nsubset to $S_j$, $\\textit{color}(\\ell)$, which returns the unique\n$j\\in\\{0,\\ldots,c-1\\}$ with $\\ell\\in S_j$, and $\\textit{choice}(j)$, which\nreturns an arbitrary element of $S_j$. We describe new colored choice\ndictionaries that, if initialized with constant $c$, execute\n$\\textit{setcolor}$, $\\textit{color}$ and $\\textit{choice}$ in constant time\nand occupy $n\\log_2\\!c+1$ bits plus the space needed to store $n$ if $c$ is a\npower of 2, and at most $n\\log_2\\!c+n^\\epsilon$ bits in general, for arbitrary\nfixed $\\epsilon>0$. We also study the possibility of iterating over the set $S$\nor over $S_j$ for given $j\\in\\{0,\\ldots,c-1\\}$ and an application of this to\nbreadth-first search.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 14:57:04 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Hagerup", "Torben", ""]]}, {"id": "1809.07858", "submitter": "Mohammed Alser", "authors": "Mohammed Alser, Hasan Hassan, Akash Kumar, Onur Mutlu, and Can Alkan", "title": "Shouji: A Fast and Efficient Pre-Alignment Filter for Sequence Alignment", "comments": "https://academic.oup.com/bioinformatics/advance-article-abstract/doi/10.1093/bioinformatics/btz234/5421509,\n  Bioinformatics Journal 2019", "journal-ref": "Bioinformatics, Nov 1; 35 (21): 4255 - 4263, 2019", "doi": "10.1093/bioinformatics/btz234", "report-no": null, "categories": "cs.CE cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: The ability to generate massive amounts of sequencing data\ncontinues to overwhelm the processing capability of existing algorithms and\ncompute infrastructures. In this work, we explore the use of hardware/software\nco-design and hardware acceleration to significantly reduce the execution time\nof short sequence alignment, a crucial step in analyzing sequenced genomes. We\nintroduce Shouji, a highly-parallel and accurate pre-alignment filter that\nremarkably reduces the need for computationally-costly dynamic programming\nalgorithms. The first key idea of our proposed pre-alignment filter is to\nprovide high filtering accuracy by correctly detecting all common subsequences\nshared between two given sequences. The second key idea is to design a hardware\naccelerator that adopts modern FPGA (Field-Programmable Gate Array)\narchitectures to further boost the performance of our algorithm.\n  Results: Shouji significantly improves the accuracy of pre-alignment\nfiltering by up to two orders of magnitude compared to the state-of-the-art\npre-alignment filters, GateKeeper and SHD. Our FPGA-based accelerator is up to\nthree orders of magnitude faster than the equivalent CPU implementation of\nShouji. Using a single FPGA chip, we benchmark the benefits of integrating\nShouji with five state-of-the-art sequence aligners, designed for different\ncomputing platforms. The addition of Shouji as a pre-alignment step reduces the\nexecution time of the five state-of-the-art sequence aligners by up to 18.8x.\nShouji can be adapted for any bioinformatics pipeline that performs sequence\nalignment for verification. Unlike most existing methods that aim to accelerate\nsequence alignment, Shouji does not sacrifice any of the aligner capabilities,\nas it does not modify or replace the alignment step.\n  Availability: https://github.com/CMU-SAFARI/Shouji\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 16:38:32 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 20:53:34 GMT"}, {"version": "v3", "created": "Wed, 27 Mar 2019 09:49:52 GMT"}, {"version": "v4", "created": "Mon, 15 Apr 2019 14:43:18 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Alser", "Mohammed", ""], ["Hassan", "Hasan", ""], ["Kumar", "Akash", ""], ["Mutlu", "Onur", ""], ["Alkan", "Can", ""]]}, {"id": "1809.07910", "submitter": "Fotis Iliopoulos", "authors": "Dimitris Achlioptas, Themis Gouleakis, Fotis Iliopoulos", "title": "Simple Local Computation Algorithms for the General Lovasz Local Lemma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of designing Local Computation Algorithms (LCA) for\napplications of the Lov\\'{a}sz Local Lemma (LLL). LCA is a class of sublinear\nalgorithms proposed by Rubinfeld et al.~\\cite{Ronitt} that have received a lot\nof attention in recent years. The LLL is an existential, sufficient condition\nfor a collection of sets to have non-empty intersection (in applications,\noften, each set comprises all objects having a certain property). The\nground-breaking algorithm of Moser and Tardos~\\cite{MT} made the LLL fully\nconstructive, following earlier results by Beck~\\cite{beck_lll} and\nAlon~\\cite{alon_lll} giving algorithms under significantly stronger LLL-like\nconditions. LCAs under those stronger conditions were given in~\\cite{Ronitt},\nwhere it was asked if the Moser-Tardos algorithm can be used to design LCAs\nunder the standard LLL condition. The main contribution of this paper is to\nanswer this question affirmatively. In fact, our techniques yield LCAs for\nsettings beyond the standard LLL condition.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 01:44:01 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 00:45:03 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 17:50:30 GMT"}, {"version": "v4", "created": "Mon, 6 Jul 2020 23:19:02 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Achlioptas", "Dimitris", ""], ["Gouleakis", "Themis", ""], ["Iliopoulos", "Fotis", ""]]}, {"id": "1809.08055", "submitter": "Sushrut Karmalkar", "authors": "Sushrut Karmalkar, Eric Price", "title": "Compressed Sensing with Adversarial Sparse Noise via L1 Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and effective algorithm for the problem of \\emph{sparse\nrobust linear regression}. In this problem, one would like to estimate a sparse\nvector $w^* \\in \\mathbb{R}^n$ from linear measurements corrupted by sparse\nnoise that can arbitrarily change an adversarially chosen $\\eta$ fraction of\nmeasured responses $y$, as well as introduce bounded norm noise to the\nresponses. For Gaussian measurements, we show that a simple algorithm based on\nL1 regression can successfully estimate $w^*$ for any $\\eta < \\eta_0 \\approx\n0.239$, and that this threshold is tight for the algorithm. The number of\nmeasurements required by the algorithm is $O(k \\log \\frac{n}{k})$ for\n$k$-sparse estimation, which is within constant factors of the number needed\nwithout any sparse noise. Of the three properties we show---the ability to\nestimate sparse, as well as dense, $w^*$; the tolerance of a large constant\nfraction of outliers; and tolerance of adversarial rather than distributional\n(e.g., Gaussian) dense noise---to the best of our knowledge, no previous result\nachieved more than two.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 12:15:49 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 06:34:17 GMT"}, {"version": "v3", "created": "Fri, 9 Nov 2018 17:06:07 GMT"}, {"version": "v4", "created": "Sun, 6 Jan 2019 15:53:46 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Karmalkar", "Sushrut", ""], ["Price", "Eric", ""]]}, {"id": "1809.08140", "submitter": "Louis Esperet", "authors": "\\'Etienne Bamas and Louis Esperet", "title": "Distributed coloring of graphs with an optimal number of colors", "comments": "19 pages, 2 figures - full version of a paper accepted to STACS 2019\n  (with improved presentation and results compared to v2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies sufficient conditions to obtain efficient distributed\nalgorithms coloring graphs optimally (i.e.\\ with the minimum number of colors)\nin the LOCAL model of computation. Most of the work on distributed vertex\ncoloring so far has focused on coloring graphs of maximum degree $\\Delta$ with\nat most $\\Delta+1$ colors (or $\\Delta$ colors when some simple obstructions are\nforbidden). When $\\Delta$ is sufficiently large and $c\\ge \\Delta-k_\\Delta+1$,\nfor some integer $k_\\Delta\\approx \\sqrt{\\Delta}-2$, we give a distributed\nalgorithm that given a $c$-colorable graph $G$ of maximum degree $\\Delta$,\nfinds a $c$-coloring of $G$ in $\\min\\{O((\\log\\Delta)^{1/12}\\log n), 2^{O(\\log\n\\Delta+\\sqrt{\\log \\log n})}\\}$ rounds, with high probability. The lower bound\n$\\Delta-k_\\Delta+1$ is best possible in the sense that for infinitely many\nvalues of $\\Delta$, we prove that when $\\chi(G)\\le \\Delta -k_\\Delta$, finding\nan optimal coloring of $G$ requires $\\Omega(n)$ rounds. Our proof is a light\nadaptation of a remarkable result of Molloy and Reed, who proved that for\n$\\Delta$ large enough, for any $c\\ge \\Delta - k_\\Delta$ deciding whether\n$\\chi(G)\\le c$ is in {\\textsf{P}}, while Embden-Weinert \\emph{et al.}\\ proved\nthat for $c\\le \\Delta-k_\\Delta-1$, the same problem is {\\textsf{NP}}-complete.\nNote that the sequential and distributed thresholds differ by one. We also show\nthat for any sufficiently large $\\Delta$, and $\\Omega(\\log \\Delta)\\le k \\le\n\\Delta/100$, every graph of maximum degree $\\Delta$ and clique number at most\n$\\Delta-k$ can be efficiently colored with at most $\\Delta-\\varepsilon k$\ncolors, for some absolute constant $\\varepsilon >0$, with a randomized\nalgorithm running in $O(\\log n/\\log \\log n)$ rounds with high probability.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 14:16:46 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 12:35:28 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 09:08:20 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Bamas", "\u00c9tienne", ""], ["Esperet", "Louis", ""]]}, {"id": "1809.08160", "submitter": "Dimitrios Thilikos", "authors": "Eun Jung Kim, Maria Serna, Dimitrios M. Thilikos", "title": "Data-compression for Parametrized Counting Problems on Sparse graphs", "comments": "An extended abstract of this paper was accepted to ISAAC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the concept of \\emph{compactor}, which may be seen as a\ncounting-analogue of kernelization in counting parameterized complexity. For a\nfunction $F:\\Sigma^*\\to \\Bbb{N}$ and a parameterization $\\kappa: \\Sigma^*\\to\n\\Bbb{N}$, a compactor $({\\sf P},{\\sf M})$ consists of a polynomial-time\ncomputable function ${\\sf P}$, called \\emph{condenser}, and a computable\nfunction ${\\sf M}$, called \\emph{extractor}, such that $F={\\sf M}\\circ {\\sf\nP}$, and the condensing ${\\sf P}(x)$ of $x$ has length at most $s(\\kappa(x))$,\nfor any input $x\\in \\Sigma^*.$ If $s$ is a polynomial function, then the\ncompactor is said to be of polynomial-size. Although the study on\ncounting-analogue of kernelization is not unprecedented, it has received little\nattention so far. We study a family of vertex-certified counting problems on\ngraphs that are MSOL-expressible; that is, for an MSOL-formula $\\phi$ with one\nfree set variable to be interpreted as a vertex subset, we want to count all\n$A\\subseteq V(G)$ where $|A|=k$ and $(G,A)\\models \\phi.$ In this paper, we\nprove that every vertex-certified counting problems on graphs that is\n\\emph{MSOL-expressible} and \\emph{treewidth modulable}, when parameterized by\n$k$, admits a polynomial-size compactor on $H$-topological-minor-free graphs\nwith condensing time $O(k^2n^2)$ and decoding time $2^{O(k)}.$ This implies the\nexistence of an {\\sf FPT}-algorithm of running time $O(n^2k^2)+2^{O(k)}.$ All\naforementioned complexities are under the Uniform Cost Measure (UCM) model\nwhere numbers can be stored in constant space and arithmetic operations can be\ndone in constant time.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 14:52:04 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 15:35:25 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Kim", "Eun Jung", ""], ["Serna", "Maria", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1809.08411", "submitter": "Vincent Jug\\'e", "authors": "Vincent Jug\\'e", "title": "Adaptive Shivers Sort: An Alternative Sorting Algorithm", "comments": "Full version of the article published in the proceedings of the 31th\n  Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2020). 39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present one stable mergesort algorithm, called \\Adaptive Shivers Sort,\nthat exploits the existence of monotonic runs for sorting efficiently partially\nsorted data. We also prove that, although this algorithm is simple to\nimplement, its computational cost, in number of comparisons performed, is\noptimal up to a small additive linear term.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 08:50:24 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 14:54:46 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 15:39:24 GMT"}, {"version": "v4", "created": "Tue, 8 Sep 2020 15:33:22 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Jug\u00e9", "Vincent", ""]]}, {"id": "1809.08437", "submitter": "Pranabendu Misra", "authors": "Daniel Lokshtanov, Pranabendu Misra, Joydeep Mukherjee, Geevarghese\n  Philip, Fahad Panolan, Saket Saurabh", "title": "A 2-Approximation Algorithm for Feedback Vertex Set in Tournaments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A {\\em tournament} is a directed graph $T$ such that every pair of vertices\nis connected by an arc. A {\\em feedback vertex set} is a set $S$ of vertices in\n$T$ such that $T - S$ is acyclic. We consider the {\\sc Feedback Vertex Set}\nproblem in tournaments. Here the input is a tournament $T$ and a weight\nfunction $w : V(T) \\rightarrow \\mathbb{N}$ and the task is to find a feedback\nvertex set $S$ in $T$ minimizing $w(S) = \\sum_{v \\in S} w(v)$. We give the\nfirst polynomial time factor $2$ approximation algorithm for this problem.\nAssuming the Unique Games conjecture, this is the best possible approximation\nratio achievable in polynomial time.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 13:52:50 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Misra", "Pranabendu", ""], ["Mukherjee", "Joydeep", ""], ["Philip", "Geevarghese", ""], ["Panolan", "Fahad", ""], ["Saurabh", "Saket", ""]]}, {"id": "1809.08506", "submitter": "Xuan Zhang", "authors": "Yuri Faenza and Xuan Zhang", "title": "Legal Assignments and fast EADAM with consent via classical theory of\n  stable matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gale and Shapley's stable assignment problem has been extensively studied,\napplied, and extended. In the context of school choice, mechanisms often aim at\nfinding an assignment that is more favorable to students. We investigate two\nextensions introduced in this framework -- legal assignments and the EADAM\nalgorithm -- through the lens of classical theory of stable matchings. In any\ninstance, the set ${\\cal L}$ of legal assignments is known to contain all\nstable assignments. We prove that ${\\cal L}$ is exactly the set of stable\nassignments in another instance. Moreover, we show that essentially all\noptimization problems over ${\\cal L}$ can be solved within the same time bound\nneeded for solving it over the set of stable assignments. A key tool for this\nlatter result is an algorithm that finds the student-optimal legal assignment.\nWe then generalize our algorithm to obtain the assignment output of EADAM with\nany given set of consenting students without sacrificing the running time,\nhence largely improving in both theory and practice over known algorithms.\nLastly, we show that the set ${\\cal L}$ can be much larger than the set of\nstable matchings, connecting legal matchings with certain concepts and open\nproblems in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 00:04:37 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 16:41:26 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Faenza", "Yuri", ""], ["Zhang", "Xuan", ""]]}, {"id": "1809.08669", "submitter": "Alexander Golovnev", "authors": "Alexander Golovnev, Alexander S. Kulikov, Alexander Logunov, Ivan\n  Mihajlin, Maksim Nikolaev", "title": "Collapsing Superstring Conjecture", "comments": "visualization available at: http://compsciclub.ru/scs/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Shortest Common Superstring (SCS) problem, one is given a collection\nof strings, and needs to find a shortest string containing each of them as a\nsubstring. SCS admits $2\\frac{11}{23}$-approximation in polynomial time (Mucha,\nSODA'13). While this algorithm and its analysis are technically involved, the\n30 years old Greedy Conjecture claims that the trivial and efficient Greedy\nAlgorithm gives a 2-approximation for SCS.\n  We develop a graph-theoretic framework for studying approximation algorithms\nfor SCS. The framework is reminiscent of the classical 2-approximation for\nTraveling Salesman: take two copies of an optimal solution, apply a trivial\nedge-collapsing procedure, and get an approximate solution. In this framework,\nwe observe two surprising properties of SCS solutions, and we conjecture that\nthey hold for all input instances. The first conjecture, that we call\nCollapsing Superstring conjecture, claims that there is an elementary way to\ntransform any solution repeated twice into the same graph $G$. This conjecture\nwould give an elementary 2-approximate algorithm for SCS. The second conjecture\nclaims that not only the resulting graph $G$ is the same for all solutions, but\nthat $G$ can be computed by an elementary greedy procedure called Greedy\nHierarchical Algorithm.\n  While the second conjecture clearly implies the first one, perhaps\nsurprisingly we prove their equivalence. We support these equivalent\nconjectures by giving a proof for the special case where all input strings have\nlength at most 3. We prove that the standard Greedy Conjecture implies Greedy\nHierarchical Conjecture, while the latter is sufficient for an efficient greedy\n2-approximate approximation of SCS. Except for its (conjectured) good\napproximation ratio, the Greedy Hierarchical Algorithm provably finds a\n3.5-approximation.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 20:13:14 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 15:58:52 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2020 03:23:53 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Golovnev", "Alexander", ""], ["Kulikov", "Alexander S.", ""], ["Logunov", "Alexander", ""], ["Mihajlin", "Ivan", ""], ["Nikolaev", "Maksim", ""]]}, {"id": "1809.08822", "submitter": "Davide Bil\\`o", "authors": "Davide Bil\\`o", "title": "Almost optimal algorithms for diameter-optimally augmenting trees", "comments": "The paper has been accepted at the 29th International Symposium on\n  Algorithms and Computation (ISAAC 2018). 22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of augmenting an $n$-vertex tree with one shortcut in\norder to minimize the diameter of the resulting graph. The tree is embedded in\nan unknown space and we have access to an oracle that, when queried on a pair\nof vertices $u$ and $v$, reports the weight of the shortcut $(u,v)$ in constant\ntime. Previously, the problem was solved in $O(n^2 \\log^3 n)$ time for general\nweights [Oh and Ahn, ISAAC 2016], in $O(n^2 \\log n)$ time for trees embedded in\na metric space [Gro{\\ss}e et al., {\\tt arXiv:1607.05547}], and in $O(n \\log n)$\ntime for paths embedded in a metric space [Wang, WADS 2017]. Furthermore, a\n$(1+\\varepsilon)$-approximation algorithm running in $O(n+1/\\varepsilon^{3})$\nhas been designed for paths embedded in $\\mathbb{R}^d$, for constant values of\n$d$ [Gro{\\ss}e et al., ICALP 2015].\n  The contribution of this paper is twofold: we address the problem for trees\n(not only paths) and we also improve upon all known results. More precisely, we\ndesign a {\\em time-optimal} $O(n^2)$ time algorithm for general weights.\nMoreover, for trees embedded in a metric space, we design (i) an exact $O(n\n\\log n)$ time algorithm and (ii) a $(1+\\varepsilon)$-approximation algorithm\nthat runs in $O\\big(n+ \\varepsilon^{-1}\\log \\varepsilon^{-1}\\big)$ time.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 09:55:06 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 07:12:55 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Bil\u00f2", "Davide", ""]]}, {"id": "1809.09165", "submitter": "Vitaly Feldman", "authors": "Amit Daniely and Vitaly Feldman", "title": "Locally Private Learning without Interaction Requires Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning under the constraint of local differential privacy\n(LDP). For many learning problems known efficient algorithms in this model\nrequire many rounds of communication between the server and the clients holding\nthe data points. Yet multi-round protocols are prohibitively slow in practice\ndue to network latency and, as a result, currently deployed large-scale systems\nare limited to a single round. Despite significant research interest, very\nlittle is known about which learning problems can be solved by such\nnon-interactive systems. The only lower bound we are aware of is for PAC\nlearning an artificial class of functions with respect to a uniform\ndistribution (Kasiviswanathan et al. 2011).\n  We show that the margin complexity of a class of Boolean functions is a lower\nbound on the complexity of any non-interactive LDP algorithm for\ndistribution-independent PAC learning of the class. In particular, the classes\nof linear separators and decision lists require exponential number of samples\nto learn non-interactively even though they can be learned in polynomial time\nby an interactive LDP algorithm. This gives the first example of a natural\nproblem that is significantly harder to solve without interaction and also\nresolves an open problem of Kasiviswanathan et al. (2011). We complement this\nlower bound with a new efficient learning algorithm whose complexity is\npolynomial in the margin complexity of the class. Our algorithm is\nnon-interactive on labeled samples but still needs interactive access to\nunlabeled samples. All of our results also apply to the statistical query model\nand any model in which the number of bits communicated about each data point is\nconstrained.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 18:57:36 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 02:35:29 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 06:20:12 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Daniely", "Amit", ""], ["Feldman", "Vitaly", ""]]}, {"id": "1809.09330", "submitter": "Yan Gu", "authors": "Guy E. Blleloch, Yan Gu", "title": "Improved Parallel Cache-Oblivious Algorithms for Dynamic Programming and\n  Linear Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging non-volatile main memory (NVRAM) technologies provide\nbyte-addressability, low idle power, and improved memory-density, and are\nlikely to be a key component in the future memory hierarchy. However, a\ncritical challenge in achieving high performance is in accounting for the\nasymmetry that NVRAM writes can be significantly more expensive than NVRAM\nreads.\n  In this paper, we consider a large class of cache-oblivious algorithms for\ndynamic programming (DP) and linear algebra, and try to reduce the writes in\nthe asymmetric setting while maintaining high parallelism. To achieve that, our\nkey approach is to show the correspondence between these problems and an\nabstraction for their computation, which is referred to as the $k$-d grids.\nThen by showing lower bound and new algorithms for computing $k$-d grids, we\nshow a list of improved cache-oblivious algorithms of many DP recurrences and\nin linear algebra in the asymmetric setting, both sequentially and in parallel.\n  Surprisingly, even without considering the read-write asymmetry (i.e.,\nsetting the write cost to be the same as the read cost in the algorithms), the\nnew algorithms improve the existing cache complexity of many problems. We\nbelieve the reason is that the extra level of abstraction of $k$-d grids helps\nus to better understand the complexity and difficulties of these problems. We\nbelieve that the novelty of our framework is of interests and leads to many new\nquestions for future work.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 05:47:44 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 19:40:27 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Blleloch", "Guy E.", ""], ["Gu", "Yan", ""]]}, {"id": "1809.09345", "submitter": "Pawe{\\l} Rz\\k{a}\\.zewski", "authors": "Karolina Okrasa and Pawe{\\l} Rz\\k{a}\\.zewski", "title": "Subexponential algorithms for variants of homomorphism problem in string\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the complexity of finding weighted homomorphisms from\nintersection graphs of curves (string graphs) with $n$ vertices to a fixed\ngraph $H$. We provide a complete dichotomy for the problem: if $H$ has no two\nvertices sharing two common neighbors, then the problem can be solved in time\n$2^{O(n^{2/3} \\log n)}$, otherwise there is no algorithm working in time\n$2^{o(n)}$, even in intersection graphs of segments, unless the ETH fails. This\ngeneralizes several known results concerning the complexity of computatational\nproblems in geometric intersection graphs. Then we consider two variants of\ngraph homomorphism problem, called locally injective homomorphism and locally\nbijective homomorphism, where we require the homomorphism to be injective or\nbijective on the neighborhood of each vertex. We show that for each target\ngraph $H$, both problems can always be solved in time $2^{O(\\sqrt{n} \\log n)}$\nin string graphs. For the locally surjecive homomorphism, defined in an\nanalogous way, the situation seems more complicated. We show the dichotomy\ntheorem for simple connected graphs $H$ with maximum degree 2. If $H$ is\nisomorphic to $P_3$ or $C_4$, then the existence of a locally surjective\nhomomorphism from a string graph with $n$ vertices to $H$ can be decided in\ntime $2^{O(n^{2/3} \\log^{3/2} n)}$, otherwise the problem cannot be solved in\ntime $2^{o(n)}$, unless the ETH fails. As a byproduct, we obtain several\nresults concerning the complexity of variants of homomorphism problem in\n$P_t$-free graphs. In particular, we obtain the dichotomy theorem for weighted\nhomomorphism, analogous to the one for string graphs.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 07:00:55 GMT"}, {"version": "v2", "created": "Sat, 8 Dec 2018 17:02:45 GMT"}, {"version": "v3", "created": "Fri, 21 Jun 2019 12:34:21 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Okrasa", "Karolina", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""]]}, {"id": "1809.09493", "submitter": "Nate Veldt", "authors": "David F. Gleich and Nate Veldt and Anthony Wirth", "title": "Correlation Clustering Generalized", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new results for LambdaCC and MotifCC, two recently introduced\nvariants of the well-studied correlation clustering problem. Both variants are\nmotivated by applications to network analysis and community detection, and have\nnon-trivial approximation algorithms. We first show that the standard linear\nprogramming relaxation of LambdaCC has a $\\Theta(\\log n)$ integrality gap for a\ncertain choice of the parameter $\\lambda$. This sheds light on previous\nchallenges encountered in obtaining parameter-independent approximation results\nfor LambdaCC. We generalize a previous constant-factor algorithm to provide the\nbest results, from the LP-rounding approach, for an extended range of\n$\\lambda$. MotifCC generalizes correlation clustering to the hypergraph\nsetting. In the case of hyperedges of degree $3$ with weights satisfying\nprobability constraints, we improve the best approximation factor from $9$ to\n$8$. We show that in general our algorithm gives a $4(k-1)$ approximation when\nhyperedges have maximum degree $k$ and probability weights. We additionally\npresent approximation results for LambdaCC and MotifCC where we restrict to\nforming only two clusters.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 13:56:30 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Gleich", "David F.", ""], ["Veldt", "Nate", ""], ["Wirth", "Anthony", ""]]}, {"id": "1809.09698", "submitter": "Khaled Elbassioni", "authors": "Khaled Elbassioni and Kazuhisa Makino", "title": "Finding Sparse Solutions for Packing and Covering Semidefinite Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Packing and covering semidefinite programs (SDPs) appear in natural\nrelaxations of many combinatorial optimization problems as well as a number of\nother applications. Recently, several techniques were proposed, that utilize\nthe particular structure of this class of problems, to obtain more efficient\nalgorithms than those offered by general SDP solvers. For certain applications,\nsuch as those described in this paper, it may be desirable to obtain {\\it\nsparse} dual solutions, i.e., those with support size (almost) independent of\nthe number of primal constraints. In this paper, we give an algorithm that\nfinds such solutions, which is an extension of a {\\it logarithmic-potential}\nbased algorithm of Grigoriadis, Khachiyan, Porkolab and Villavicencio (SIAM\nJournal of Optimization 41 (2001)) for packing/covering linear programs.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 20:19:48 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 01:20:40 GMT"}, {"version": "v3", "created": "Fri, 15 Feb 2019 21:07:10 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Elbassioni", "Khaled", ""], ["Makino", "Kazuhisa", ""]]}, {"id": "1809.09776", "submitter": "Hengzhao Ma", "authors": "Hengzhao Ma, Jianzhong Li", "title": "An Algorithm for Reducing Approximate Nearest Neighbor to Approximate\n  Near Neighbor with O(logn) Query Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new algorithm for reducing Approximate Nearest Neighbor\nproblem to Approximate Near Neighbor problem. The advantage of this algorithm\nis that it achieves O(log n) query time. As a reduction problem, the uery time\ncomplexity is the times of invoking the algorithm for Approximate Near Neighbor\nproblem. All former algorithms for the same reduction need polylog(n) query\ntime. A box split method proposed by Vaidya is used in our paper to achieve the\nO(log n) query time complexity.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 01:42:05 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Ma", "Hengzhao", ""], ["Li", "Jianzhong", ""]]}, {"id": "1809.10325", "submitter": "Yan Jin", "authors": "Yan Jin, Elchanan Mossel, Govind Ramnarayan", "title": "Being Corrupt Requires Being Clever, But Detecting Corruption Doesn't", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variation of the problem of corruption detection on networks\nposed by Alon, Mossel, and Pemantle '15. In this model, each vertex of a graph\ncan be either truthful or corrupt. Each vertex reports about the types\n(truthful or corrupt) of all its neighbors to a central agency, where truthful\nnodes report the true types they see and corrupt nodes report adversarially.\nThe central agency aggregates these reports and attempts to find a single\ntruthful node. Inspired by real auditing networks, we pose our problem for\narbitrary graphs and consider corruption through a computational lens. We\nidentify a key combinatorial parameter of the graph $m(G)$, which is the\nminimal number of corrupted agents needed to prevent the central agency from\nidentifying a single truthful node. We give an efficient (in fact, linear time)\nalgorithm for the central agency to identify a truthful node that is successful\nwhenever the number of corrupt nodes is less than $m(G)/2$. On the other hand,\nwe prove that for any constant $\\alpha > 1$, it is NP-hard to find a subset of\nnodes $S$ in $G$ such that corrupting $S$ prevents the central agency from\nfinding one truthful node and $|S| \\leq \\alpha m(G)$, assuming the Small Set\nExpansion Hypothesis (Raghavendra and Steurer, STOC '10). We conclude that\nbeing corrupt requires being clever, while detecting corruption does not.\n  Our main technical insight is a relation between the minimum number of\ncorrupt nodes required to hide all truthful nodes and a certain notion of\nvertex separability for the underlying graph. Additionally, this insight lets\nus design an efficient algorithm for a corrupt party to decide which graphs\nrequire the fewest corrupted nodes, up to a multiplicative factor of $O(\\log\nn)$.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 03:14:47 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 02:34:23 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Jin", "Yan", ""], ["Mossel", "Elchanan", ""], ["Ramnarayan", "Govind", ""]]}, {"id": "1809.10428", "submitter": "Alexander M\\\"acker", "authors": "Klaus Jansen, Marten Maack, Alexander M\\\"acker", "title": "Scheduling on (Un-)Related Machines with Setup Times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a natural generalization of scheduling $n$ jobs on $m$ parallel\nmachines so as to minimize the makespan. In our extension the set of jobs is\npartitioned into several classes and a machine requires a setup whenever it\nswitches from processing jobs of one class to jobs of a different class. During\nsuch a setup, a machine cannot process jobs and the duration of a setup may\ndepend on the machine as well as the class of the job to be processed next.\n  For this problem, we study approximation algorithms for non-identical\nmachines. We develop a polynomial-time approximation scheme for uniformly\nrelated machines. For unrelated machines we obtain an $O(\\log n + \\log\nm)$-approximation, which we show to be optimal (up to constant factors) unless\n$NP \\subset RP$. We also identify two special cases that admit constant factor\napproximations.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 09:41:39 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Jansen", "Klaus", ""], ["Maack", "Marten", ""], ["M\u00e4cker", "Alexander", ""]]}, {"id": "1809.10469", "submitter": "Xianghui Zhong", "authors": "Xianghui Zhong", "title": "Smoothed Analysis of Edge Elimination for Euclidean TSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way to speed up the calculation of optimal TSP tours in practice is\neliminating edges that are certainly not in the optimal tour as a preprocessing\nstep. In order to do so several edge elimination approaches have been proposed\nin the past. In this work we investigate two of them in the scenario where the\ninput consists of $n$ independently distributed random points with bounded\ndensity function from above and below by arbitrary positive constants. We show\nthat after the edge elimination procedure of Hougardy and Schroeder the\nexpected number of remaining edges is $\\Theta(n)$, while after that of Jonker\nand Volgenant the expected number of remaining edges is $\\Theta(n^2)$.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 11:56:36 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 13:19:20 GMT"}, {"version": "v3", "created": "Wed, 21 Nov 2018 10:20:43 GMT"}, {"version": "v4", "created": "Fri, 26 Apr 2019 16:10:28 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Zhong", "Xianghui", ""]]}, {"id": "1809.10508", "submitter": "Sebastien Ratel", "authors": "Victor Chepoi and Arnaud Labourel and Sebastien Ratel", "title": "Distance and routing labeling schemes for cube-free median graphs", "comments": "34 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance labeling schemes are schemes that label the vertices of a graph with\nshort labels in such a way that the distance between any two vertices $u$ and\n$v$ can be determined efficiently by merely inspecting the labels of $u$ and\n$v$, without using any other information. Similarly, routing labeling schemes\nlabel the vertices of a graph in a such a way that given the labels of a source\nnode and a destination node, it is possible to compute efficiently the port\nnumber of the edge from the source that heads in the direction of the\ndestination. One of important problems is finding natural classes of graphs\nadmitting distance and/or routing labeling schemes with labels of\npolylogarithmic size. In this paper, we show that the class of cube-free median\ngraphs on $n$ nodes enjoys distance and routing labeling schemes with labels of\n$O(\\log^3 n)$ bits.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 13:30:48 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 13:20:44 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Chepoi", "Victor", ""], ["Labourel", "Arnaud", ""], ["Ratel", "Sebastien", ""]]}, {"id": "1809.10578", "submitter": "Elisabet Burjons", "authors": "Hans-Joachim B\\\"ockenhauer, Elisabet Burjons, Martin Raszyk, and Peter\n  Rossmanith", "title": "Reoptimization of Parameterized Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameterized complexity allows us to analyze the time complexity of problems\nwith respect to a natural parameter depending on the problem. Reoptimization\nlooks for solutions or approximations for problem instances when given\nsolutions to neighboring instances. We try to combine both techniques, in order\nto better classify the complexity of problems in the parameterized setting.\nSpecifically, we see that some problems in the class of compositional problems,\nwhich do not have polynomial kernels under standard complexity-theoretic\nassumptions, do have polynomial kernels under reoptimization for some local\nmodifications. Moreover, we find that the reoptimization version of Vertex\nCover has a polynomial kernel of size 2k using crown decomposition. Finally, in\na negative result, we prove that the reoptimization version of Connected Vertex\nCover does not have a Turing kernelization unless Set Cover has a polynomial\nkernel\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 15:31:32 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 08:57:05 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["B\u00f6ckenhauer", "Hans-Joachim", ""], ["Burjons", "Elisabet", ""], ["Raszyk", "Martin", ""], ["Rossmanith", "Peter", ""]]}, {"id": "1809.10584", "submitter": "Thibaut Vidal", "authors": "Gabriel Homsi, Rafael Martinelli, Thibaut Vidal, Kjetil Fagerholt", "title": "Industrial and Tramp Ship Routing Problems: Closing the Gap for\n  Real-Scale Instances", "comments": null, "journal-ref": null, "doi": "10.1016/j.ejor.2019.11.068", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies in maritime logistics have introduced a general ship routing\nproblem and a benchmark suite based on real shipping segments, considering\npickups and deliveries, cargo selection, ship-dependent starting locations,\ntravel times and costs, time windows, and incompatibility constraints, among\nother features. Together, these characteristics pose considerable challenges\nfor exact and heuristic methods, and some cases with as few as 18 cargoes\nremain unsolved. To face this challenge, we propose an exact branch-and-price\n(B&P) algorithm and a hybrid metaheuristic. Our exact method generates\nelementary routes, but exploits decremental state-space relaxation to speed up\ncolumn generation, heuristic strong branching, as well as advanced\npreprocessing and route enumeration techniques. Our metaheuristic is a\nsophisticated extension of the unified hybrid genetic search. It exploits a\nset-partitioning phase and uses problem-tailored variation operators to\nefficiently handle all the problem characteristics. As shown in our\nexperimental analyses, the B&P optimally solves 239/240 existing instances\nwithin one hour. Scalability experiments on even larger problems demonstrate\nthat it can optimally solve problems with around 60 ships and 200 cargoes\n(i.e., 400 pickup and delivery services) and find optimality gaps below 1.04%\non the largest cases with up to 260 cargoes. The hybrid metaheuristic\noutperforms all previous heuristics and produces near-optimal solutions within\nminutes. These results are noteworthy, since these instances are comparable in\nsize with the largest problems routinely solved by shipping companies.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 15:44:26 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 15:52:25 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Homsi", "Gabriel", ""], ["Martinelli", "Rafael", ""], ["Vidal", "Thibaut", ""], ["Fagerholt", "Kjetil", ""]]}]