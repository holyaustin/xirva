[{"id": "1711.00046", "submitter": "Vikash Singh", "authors": "Vikash Singh", "title": "Replace or Retrieve Keywords In Documents at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce, the FlashText algorithm for replacing keywords or\nfinding keywords in a given text. FlashText can search or replace keywords in\none pass over a document. The time complexity of this algorithm is not\ndependent on the number of terms being searched or replaced. For a document of\nsize N (characters) and a dictionary of M keywords, the time complexity will be\nO(N). This algorithm is much faster than Regex, because regex time complexity\nis O(MxN). It is also different from Aho Corasick Algorithm, as it doesn't\nmatch substrings. FlashText is designed to only match complete words (words\nwith boundary characters on both sides). For an input dictionary of {Apple},\nthis algorithm won't match it to 'I like Pineapple'. This algorithm is also\ndesigned to go for the longest match first. For an input dictionary {Machine,\nLearning, Machine learning} on a string 'I like Machine learning', it will only\nconsider the longest match, which is Machine Learning. We have made python\nimplementation of this algorithm available as open-source on GitHub, released\nunder the permissive MIT License.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 18:34:03 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 18:56:44 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Singh", "Vikash", ""]]}, {"id": "1711.00081", "submitter": "Alexander Ageev A.", "authors": "Alexander Ageev", "title": "Approximating the $2$-Machine Flow Shop Problem with Exact Delays Taking\n  Two Values", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $2$-Machine Flow Shop problem with exact delays the operations of each\njob are separated by a given time lag (delay). Leung et al. (2007) established\nthat the problem is strongly NP-hard when the delays may have at most two\ndifferent values. We present further results for this case: we prove that the\nexistence of $(1.25-\\varepsilon)$-approximation implies P$=$NP and present a\n$2$-approximation algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 19:55:50 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Ageev", "Alexander", ""]]}, {"id": "1711.00103", "submitter": "Felix Land", "authors": "Klaus Jansen, Felix Land", "title": "Scheduling Monotone Moldable Jobs in Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A moldable job is a job that can be executed on an arbitrary number of\nprocessors, and whose processing time depends on the number of processors\nallotted to it. A moldable job is monotone if its work doesn't decrease for an\nincreasing number of allotted processors. We consider the problem of scheduling\nmonotone moldable jobs to minimize the makespan.\n  We argue that for certain compact input encodings a polynomial algorithm has\na running time polynomial in n and log(m), where n is the number of jobs and m\nis the number of machines. We describe how monotony of jobs can be used to\ncounteract the increased problem complexity that arises from compact encodings,\nand give tight bounds on the approximability of the problem with compact\nencoding: it is NP-hard to solve optimally, but admits a PTAS.\n  The main focus of this work are efficient approximation algorithms. We\ndescribe different techniques to exploit the monotony of the jobs for better\nrunning times, and present a (3/2+{\\epsilon})-approximate algorithm whose\nrunning time is polynomial in log(m) and 1/{\\epsilon}, and only linear in the\nnumber n of jobs.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 20:47:39 GMT"}, {"version": "v2", "created": "Sun, 7 Jan 2018 20:53:40 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Jansen", "Klaus", ""], ["Land", "Felix", ""]]}, {"id": "1711.00275", "submitter": "Mikko Berggren Ettienne", "authors": "Philip Bille, Anders Roy Christiansen, Mikko Berggren Ettienne, Inge\n  Li G{\\o}rtz", "title": "Fast Dynamic Arrays", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.ESA.2017.16", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a highly optimized implementation of tiered vectors, a data\nstructure for maintaining a sequence of $n$ elements supporting access in time\n$O(1)$ and insertion and deletion in time $O(n^\\epsilon)$ for $\\epsilon > 0$\nwhile using $o(n)$ extra space. We consider several different implementation\noptimizations in C++ and compare their performance to that of vector and\nmultiset from the standard library on sequences with up to $10^8$ elements. Our\nfastest implementation uses much less space than multiset while providing\nspeedups of $40\\times$ for access operations compared to multiset and speedups\nof $10.000\\times$ compared to vector for insertion and deletion operations\nwhile being competitive with both data structures for all other operations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 10:31:32 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Bille", "Philip", ""], ["Christiansen", "Anders Roy", ""], ["Ettienne", "Mikko Berggren", ""], ["G\u00f8rtz", "Inge Li", ""]]}, {"id": "1711.00284", "submitter": "David Holzm\\\"uller", "authors": "David Holzm\\\"uller", "title": "Improved Approximation Schemes for the Restricted Shortest Path Problem", "comments": "Incorporated more review suggestions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Restricted Shortest Path (RSP) problem, also known as the\nDelay-Constrained Least-Cost (DCLC) problem, is an NP-hard bicriteria\noptimization problem on graphs with $n$ vertices and $m$ edges. In a graph\nwhere each edge is assigned a cost and a delay, the goal is to find a min-cost\npath which does not exceed a delay bound. In this paper, we present improved\napproximation schemes for RSP on several graph classes. For planar graphs,\nundirected graphs with positive integer resource (= delay) values, and graphs\nwith $m \\in \\Omega(n \\log n)$, we obtain $(1 + \\varepsilon)$-approximations in\ntime $O(mn/\\varepsilon)$. For general graphs and directed acyclic graphs, we\nmatch the results by Xue et al. (2008, [10]) and Ergun et al. (2002, [1]),\nrespectively, but with arguably simpler algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 10:55:17 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 11:11:19 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Holzm\u00fcller", "David", ""]]}, {"id": "1711.00405", "submitter": "Sahil Singla", "authors": "Sahil Singla", "title": "The Price of Information in Combinatorial Optimization", "comments": "SODA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a network design application where we wish to lay down a\nminimum-cost spanning tree in a given graph; however, we only have stochastic\ninformation about the edge costs. To learn the precise cost of any edge, we\nhave to conduct a study that incurs a price. Our goal is to find a spanning\ntree while minimizing the disutility, which is the sum of the tree cost and the\ntotal price that we spend on the studies. In a different application, each edge\ngives a stochastic reward value. Our goal is to find a spanning tree while\nmaximizing the utility, which is the tree reward minus the prices that we pay.\n  Situations such as the above two often arise in practice where we wish to\nfind a good solution to an optimization problem, but we start with only some\npartial knowledge about the parameters of the problem. The missing information\ncan be found only after paying a probing price, which we call the price of\ninformation. What strategy should we adopt to optimize our expected\nutility/disutility?\n  A classical example of the above setting is Weitzman's \"Pandora's box\"\nproblem where we are given probability distributions on values of $n$\nindependent random variables. The goal is to choose a single variable with a\nlarge value, but we can find the actual outcomes only after paying a price. Our\nwork is a generalization of this model to other combinatorial optimization\nproblems such as matching, set cover, facility location, and prize-collecting\nSteiner tree. We give a technique that reduces such problems to their non-price\ncounterparts, and use it to design exact/approximation algorithms to optimize\nour utility/disutility. Our techniques extend to situations where there are\nadditional constraints on what parameters can be probed or when we can\nsimultaneously probe a subset of the parameters.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 15:46:48 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Singla", "Sahil", ""]]}, {"id": "1711.00501", "submitter": "Tengyu Ma", "authors": "Rong Ge, Jason D. Lee, Tengyu Ma", "title": "Learning One-hidden-layer Neural Networks with Landscape Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a one-hidden-layer neural network: we\nassume the input $x\\in \\mathbb{R}^d$ is from Gaussian distribution and the\nlabel $y = a^\\top \\sigma(Bx) + \\xi$, where $a$ is a nonnegative vector in\n$\\mathbb{R}^m$ with $m\\le d$, $B\\in \\mathbb{R}^{m\\times d}$ is a full-rank\nweight matrix, and $\\xi$ is a noise vector. We first give an analytic formula\nfor the population risk of the standard squared loss and demonstrate that it\nimplicitly attempts to decompose a sequence of low-rank tensors simultaneously.\n  Inspired by the formula, we design a non-convex objective function $G(\\cdot)$\nwhose landscape is guaranteed to have the following properties: 1. All local\nminima of $G$ are also global minima.\n  2. All global minima of $G$ correspond to the ground truth parameters.\n  3. The value and gradient of $G$ can be estimated using samples.\n  With these properties, stochastic gradient descent on $G$ provably converges\nto the global minimum and learn the ground-truth parameters. We also prove\nfinite sample complexity result and validate the results by simulations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 18:27:42 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 00:19:35 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Ge", "Rong", ""], ["Lee", "Jason D.", ""], ["Ma", "Tengyu", ""]]}, {"id": "1711.00571", "submitter": "Arun Jambulapati", "authors": "Arun Jambulapati and Aaron Sidford", "title": "Efficient $\\widetilde{O}(n/\\epsilon)$ Spectral Sketches for the\n  Laplacian and its Pseudoinverse", "comments": "Accepted to SODA 2018; v2 fixes a small bug in the proof of lemma 3.\n  This does not affect correctness of any of our results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of efficiently computing\n$\\epsilon$-sketches for the Laplacian and its pseudoinverse. Given a Laplacian\nand an error tolerance $\\epsilon$, we seek to construct a function $f$ such\nthat for any vector $x$ (chosen obliviously from $f$), with high probability\n$(1-\\epsilon) x^\\top A x \\leq f(x) \\leq (1 + \\epsilon) x^\\top A x$ where $A$ is\neither the Laplacian or its pseudoinverse. Our goal is to construct such a\nsketch $f$ efficiently and to store it in the least space possible.\n  We provide nearly-linear time algorithms that, when given a Laplacian matrix\n$\\mathcal{L} \\in \\mathbb{R}^{n \\times n}$ and an error tolerance $\\epsilon$,\nproduce $\\tilde{O}(n/\\epsilon)$-size sketches of both $\\mathcal{L}$ and its\npseudoinverse. Our algorithms improve upon the previous best sketch size of\n$\\widetilde{O}(n / \\epsilon^{1.6})$ for sketching the Laplacian form by Andoni\net al (2015) and $O(n / \\epsilon^2)$ for sketching the Laplacian pseudoinverse\nby Batson, Spielman, and Srivastava (2008).\n  Furthermore we show how to compute all-pairs effective resistances from\n$\\widetilde{O}(n/\\epsilon)$ size sketch in $\\widetilde{O}(n^2/\\epsilon)$ time.\nThis improves upon the previous best running time of\n$\\widetilde{O}(n^2/\\epsilon^2)$ by Spielman and Srivastava (2008).\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 00:06:55 GMT"}, {"version": "v2", "created": "Sun, 7 Jan 2018 06:36:44 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Jambulapati", "Arun", ""], ["Sidford", "Aaron", ""]]}, {"id": "1711.00599", "submitter": "Samson Zhou", "authors": "Greg N. Frederickson, Samson Zhou", "title": "Optimal Parametric Search for Path and Tree Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present linear-time algorithms for partitioning a path or a tree with\nweights on the vertices by removing $k$ edges to maximize the minimum-weight\ncomponent. We also use the same framework to partition a path with weight on\nthe vertices, removing $k$ edges to minimize the maximum-weight component. The\nalgorithms use the parametric search paradigm, testing candidate values until\nan optimum is found while simultaneously reducing the running time needed for\neach test. For path-partitioning, the algorithm employs a synthetic weighting\nscheme that results in a constant fraction reduction in running time after each\ntest. For tree-partitioning, our dual-pronged strategy makes progress no matter\nwhat the specific structure of our tree is.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 02:46:56 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Frederickson", "Greg N.", ""], ["Zhou", "Samson", ""]]}, {"id": "1711.00667", "submitter": "O-Joung Kwon", "authors": "Eun Jung Kim and O-joung Kwon", "title": "Erd\\H{o}s-P\\'osa property of chordless cycles and its applications", "comments": "35 pages, 11 figures, accepted to JCTB", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A chordless cycle, or equivalently a hole, in a graph $G$ is an induced\nsubgraph of $G$ which is a cycle of length at least $4$. We prove that the\nErd\\H{o}s-P\\'osa property holds for chordless cycles, which resolves the major\nopen question concerning the Erd\\H{o}s-P\\'osa property. Our proof for chordless\ncycles is constructive: in polynomial time, one can find either $k+1$\nvertex-disjoint chordless cycles, or $c_1k^2 \\log k+c_2$ vertices hitting every\nchordless cycle for some constants $c_1$ and $c_2$. It immediately implies an\napproximation algorithm of factor $\\mathcal{O}(\\sf{opt}\\log {\\sf opt})$ for\nChordal Vertex Deletion. We complement our main result by showing that\nchordless cycles of length at least $\\ell$ for any fixed $\\ell\\ge 5$ do not\nhave the Erd\\H{o}s-P\\'osa property.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 09:54:37 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 23:24:27 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 14:18:08 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Kim", "Eun Jung", ""], ["Kwon", "O-joung", ""]]}, {"id": "1711.00788", "submitter": "Arnaud de Mesmay", "authors": "Erin Wolf Chambers and Arnaud de Mesmay and Tim Ophelders", "title": "On the complexity of optimal homotopies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we provide new structural results and algorithms for the\nHomotopy Height problem. In broad terms, this problem quantifies how much a\ncurve on a surface needs to be stretched to sweep continuously between two\npositions. More precisely, given two homotopic curves $\\gamma_1$ and $\\gamma_2$\non a combinatorial (say, triangulated) surface, we investigate the problem of\ncomputing a homotopy between $\\gamma_1$ and $\\gamma_2$ where the length of the\nlongest intermediate curve is minimized. Such optimal homotopies are relevant\nfor a wide range of purposes, from very theoretical questions in quantitative\nhomotopy theory to more practical applications such as similarity measures on\nmeshes and graph searching problems.\n  We prove that Homotopy Height is in the complexity class NP, and the\ncorresponding exponential algorithm is the best one known for this problem.\nThis result builds on a structural theorem on monotonicity of optimal\nhomotopies, which is proved in a companion paper. Then we show that this\nproblem encompasses the Homotopic Fr\\'echet distance problem which we therefore\nalso establish to be in NP, answering a question which has previously been\nconsidered in several different settings. We also provide an O(log\nn)-approximation algorithm for Homotopy Height on surfaces by adapting an\nearlier algorithm of Har-Peled, Nayyeri, Salvatipour and Sidiropoulos in the\nplanar setting.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 15:49:59 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Chambers", "Erin Wolf", ""], ["de Mesmay", "Arnaud", ""], ["Ophelders", "Tim", ""]]}, {"id": "1711.00808", "submitter": "Torben Hagerup", "authors": "Torben Hagerup", "title": "An Optimal Choice Dictionary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A choice dictionary is a data structure that can be initialized with a\nparameter $n\\in\\{1,2,\\ldots\\}$ and subsequently maintains an initially empty\nsubset $S$ of $\\{1,\\ldots,n\\}$ under insertion, deletion, membership queries\nand an operation 'choice' that returns an arbitrary element of $S$. The choice\ndictionary is fundamental in space-efficient computing and has numerous\napplications. The best previous choice dictionary can be initialized with $n$\nand a second parameter $t\\in\\{1,2,\\ldots\\}$ in constant time and subsequently\nexecutes all operations in $O(t)$ time and occupies $n+O(n({t/w})^t+\\log n)$\nbits on a word RAM with a word length of $w=\\Omega(\\log n)$ bits. We describe a\nnew choice dictionary that, following a constant-time initialization, executes\nall operations in constant time and, in addition to the space needed to store\n$n$, occupies only $n+1$ bits, which is shown to be optimal if $w=o(n)$.\nAllowed $\\lceil\\log_2(n+1)\\rceil$ bits of additional space, the new data\nstructure also supports iteration over the set $S$ in constant time per\nelement.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 16:39:24 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Hagerup", "Torben", ""]]}, {"id": "1711.00814", "submitter": "Nirmal Shende", "authors": "Jayadev Acharya, Ibrahim Issa, Nirmal V. Shende, Aaron B. Wagner", "title": "Measuring Quantum Entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The entropy of a quantum system is a measure of its randomness, and has\napplications in measuring quantum entanglement. We study the problem of\nmeasuring the von Neumann entropy, $S(\\rho)$, and R\\'enyi entropy,\n$S_\\alpha(\\rho)$ of an unknown mixed quantum state $\\rho$ in $d$ dimensions,\ngiven access to independent copies of $\\rho$.\n  We provide an algorithm with copy complexity $O(d^{2/\\alpha})$ for estimating\n$S_\\alpha(\\rho)$ for $\\alpha<1$, and copy complexity $O(d^{2})$ for estimating\n$S(\\rho)$, and $S_\\alpha(\\rho)$ for non-integral $\\alpha>1$. These bounds are\nat least quadratic in $d$, which is the order dependence on the number of\ncopies required for learning the entire state $\\rho$. For integral $\\alpha>1$,\non the other hand, we provide an algorithm for estimating $S_\\alpha(\\rho)$ with\na sub-quadratic copy complexity of $O(d^{2-2/\\alpha})$. We characterize the\ncopy complexity for integral $\\alpha>1$ up to constant factors by providing\nmatching lower bounds. For other values of $\\alpha$, and the von Neumann\nentropy, we show lower bounds on the algorithm that achieves the upper bound.\nThis shows that we either need new algorithms for better upper bounds, or\nbetter lower bounds to tighten the results.\n  For non-integral $\\alpha$, and the von Neumann entropy, we consider the well\nknown Empirical Young Diagram (EYD) algorithm, which is the analogue of\nempirical plug-in estimator in classical distribution estimation. As a\ncorollary, we strengthen a lower bound on the copy complexity of the EYD\nalgorithm for learning the maximally mixed state by showing that the lower\nbound holds with exponential probability (which was previously known to hold\nwith a constant probability). For integral $\\alpha>1$, we provide new\nconcentration results of certain polynomials that arise in Kerov algebra of\nYoung diagrams.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 16:53:17 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 18:21:42 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Acharya", "Jayadev", ""], ["Issa", "Ibrahim", ""], ["Shende", "Nirmal V.", ""], ["Wagner", "Aaron B.", ""]]}, {"id": "1711.00817", "submitter": "Martin Zhang", "authors": "Vivek Bagaria, Govinda M. Kamath, Vasilis Ntranos, Martin J. Zhang,\n  and David Tse", "title": "Medoids in almost linear time via multi-armed bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the medoid of a large number of points in high-dimensional space is\nan increasingly common operation in many data science problems. We present an\nalgorithm Med-dit which uses O(n log n) distance evaluations to compute the\nmedoid with high probability. Med-dit is based on a connection with the\nmulti-armed bandit problem. We evaluate the performance of Med-dit empirically\non the Netflix-prize and the single-cell RNA-Seq datasets, containing hundreds\nof thousands of points living in tens of thousands of dimensions, and observe a\n5-10x improvement in performance over the current state of the art. Med-dit is\navailable at https://github.com/bagavi/Meddit\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 17:00:05 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 01:58:25 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 07:15:42 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Bagaria", "Vivek", ""], ["Kamath", "Govinda M.", ""], ["Ntranos", "Vasilis", ""], ["Zhang", "Martin J.", ""], ["Tse", "David", ""]]}, {"id": "1711.00821", "submitter": "Hung Le", "authors": "Glencora Borradaile and Hung Le and Christian Wulff-Nilsen", "title": "Minor-free graphs have light spanners", "comments": "22 pages, 4 figures. Accepted to FOCS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that every $H$-minor-free graph has a light $(1+\\epsilon)$-spanner,\nresolving an open problem of Grigni and Sissokho and proving a conjecture of\nGrigni and Hung. Our lightness bound is\n\\[O\\left(\\frac{\\sigma_H}{\\epsilon^3}\\log \\frac{1}{\\epsilon}\\right)\\] where\n$\\sigma_H = |V(H)|\\sqrt{\\log |V(H)|}$ is the sparsity coefficient of\n$H$-minor-free graphs. That is, it has a practical dependency on the size of\nthe minor $H$. Our result also implies that the polynomial time approximation\nscheme (PTAS) for the Travelling Salesperson Problem (TSP) in $H$-minor-free\ngraphs by Demaine, Hajiaghayi and Kawarabayashi is an efficient PTAS whose\nrunning time is $2^{O_H\\left(\\frac{1}{\\epsilon^4}\\log\n\\frac{1}{\\epsilon}\\right)}n^{O(1)}$ where $O_H$ ignores dependencies on the\nsize of $H$. Our techniques significantly deviate from existing lines of\nresearch on spanners for $H$-minor-free graphs, but build upon the work of\nChechik and Wulff-Nilsen for spanners of general graphs.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 17:04:58 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Borradaile", "Glencora", ""], ["Le", "Hung", ""], ["Wulff-Nilsen", "Christian", ""]]}, {"id": "1711.00831", "submitter": "Christophe Picouleau", "authors": "Thomas Ridremont, Dimitri Watel, Pierre-Louis Poirion, Christophe\n  Picouleau", "title": "Adaptive Network Flow with $k$-Arc Destruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a flow is not allowed to be reoriented the Maximum Residual Flow Problem\nwith $k$-Arc Destruction is known to be $NP$-hard for $k=2$. We show that when\na flow is allowed to be adaptive the problem becomes polynomial for every fixed\n$k$.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 17:32:28 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Ridremont", "Thomas", ""], ["Watel", "Dimitri", ""], ["Poirion", "Pierre-Louis", ""], ["Picouleau", "Christophe", ""]]}, {"id": "1711.00963", "submitter": "Philipp Zschoche", "authors": "Philipp Zschoche, Till Fluschnik, Hendrik Molter, Rolf Niedermeier", "title": "The Complexity of Finding Small Separators in Temporal Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal graphs are graphs with time-stamped edges. We study the problem of\nfinding a small vertex set (the separator) with respect to two designated\nterminal vertices such that the removal of the set eliminates all temporal\npaths connecting one terminal to the other. Herein, we consider two models of\ntemporal paths: paths that pass through arbitrarily many edges per time step\n(non-strict) and paths that pass through at most one edge per time step\n(strict). Regarding the number of time steps of a temporal graph, we show a\ncomplexity dichotomy (NP-hardness versus polynomial-time solvability) for both\nproblem variants. Moreover we prove both problem variants to be NP-complete\neven on temporal graphs whose underlying graph is planar. We further show that,\non temporal graphs with planar underlying graph, if additionally the number of\ntime steps is constant, then the problem variant for strict paths is solvable\nin quasi-linear time. Finally, we introduce and motivate the notion of a\ntemporal core (vertices whose incident edges change over time). We prove that\nthe non-strict variant is fixed-parameter tractable when parameterized by the\nsize of the temporal core, while the strict variant remains NP-complete, even\nfor constant-size temporal cores.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 22:30:07 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 14:20:43 GMT"}, {"version": "v3", "created": "Fri, 8 Dec 2017 15:50:52 GMT"}, {"version": "v4", "created": "Mon, 19 Feb 2018 14:14:05 GMT"}, {"version": "v5", "created": "Wed, 25 Jul 2018 08:16:25 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Zschoche", "Philipp", ""], ["Fluschnik", "Till", ""], ["Molter", "Hendrik", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "1711.01032", "submitter": "Robbie Weber", "authors": "Anna R. Karlin, Shayan Oveis Gharan, Robbie Weber", "title": "A Simply Exponential Upper Bound on the Maximum Number of Stable\n  Matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stable matching is a classical combinatorial problem that has been the\nsubject of intense theoretical and empirical study since its introduction in\n1962 in a seminal paper by Gale and Shapley. In this paper, we provide a new\nupper bound on $f(n)$, the maximum number of stable matchings that a stable\nmatching instance with $n$ men and $n$ women can have. It has been a\nlong-standing open problem to understand the asymptotic behavior of $f(n)$ as\n$n\\to\\infty$, first posed by Donald Knuth in the 1970s. Until now the best\nlower bound was approximately $2.28^n$, and the best upper bound was $2^{n\\log\nn- O(n)}$. In this paper, we show that for all $n$, $f(n) \\leq c^n$ for some\nuniversal constant $c$. This matches the lower bound up to the base of the\nexponent. Our proof is based on a reduction to counting the number of downsets\nof a family of posets that we call \"mixing\". The latter might be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 05:55:38 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 05:50:38 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Karlin", "Anna R.", ""], ["Gharan", "Shayan Oveis", ""], ["Weber", "Robbie", ""]]}, {"id": "1711.01085", "submitter": "James Lee", "authors": "Sebastien Bubeck and Michael B. Cohen and James R. Lee and Yin Tat Lee\n  and Aleksander Madry", "title": "k-server via multiscale entropic regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an $O((\\log k)^2)$-competitive randomized algorithm for the\n$k$-server problem on hierarchically separated trees (HSTs). This is the first\n$o(k)$-competitive randomized algorithm for which the competitive ratio is\nindependent of the size of the underlying HST. Our algorithm is designed in the\nframework of online mirror descent where the mirror map is a multiscale\nentropy. When combined with Bartal's static HST embedding reduction, this leads\nto an $O((\\log k)^2 \\log n)$-competitive algorithm on any $n$-point metric\nspace. We give a new dynamic HST embedding that yields an $O((\\log k)^3 \\log\n\\Delta)$-competitive algorithm on any metric space where the ratio of the\nlargest to smallest non-zero distance is at most $\\Delta$.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 09:58:00 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Bubeck", "Sebastien", ""], ["Cohen", "Michael B.", ""], ["Lee", "James R.", ""], ["Lee", "Yin Tat", ""], ["Madry", "Aleksander", ""]]}, {"id": "1711.01171", "submitter": "Arnaud de Mesmay", "authors": "Vincent Cohen-Addad, Arnaud de Mesmay and Eva Rotenberg and Alan\n  Roytman", "title": "The Bane of Low-Dimensionality Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give a conditional lower bound of $n^{\\Omega(k)}$ on\nrunning time for the classic k-median and k-means clustering objectives (where\nn is the size of the input), even in low-dimensional Euclidean space of\ndimension four, assuming the Exponential Time Hypothesis (ETH). We also\nconsider k-median (and k-means) with penalties where each point need not be\nassigned to a center, in which case it must pay a penalty, and extend our lower\nbound to at least three-dimensional Euclidean space.\n  This stands in stark contrast to many other geometric problems such as the\ntraveling salesman problem, or computing an independent set of unit spheres.\nWhile these problems benefit from the so-called (limited) blessing of\ndimensionality, as they can be solved in time $n^{O(k^{1-1/d})}$ or\n$2^{n^{1-1/d}}$ in d dimensions, our work shows that widely-used clustering\nobjectives have a lower bound of $n^{\\Omega(k)}$, even in dimension four.\n  We complete the picture by considering the two-dimensional case: we show that\nthere is no algorithm that solves the penalized version in time less than\n$n^{o(\\sqrt{k})}$, and provide a matching upper bound of $n^{O(\\sqrt{k})}$.\n  The main tool we use to establish these lower bounds is the placement of\npoints on the moment curve, which takes its inspiration from constructions of\npoint sets yielding Delaunay complexes of high complexity.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 14:05:48 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["de Mesmay", "Arnaud", ""], ["Rotenberg", "Eva", ""], ["Roytman", "Alan", ""]]}, {"id": "1711.01231", "submitter": "Bj\\\"orn Feldkord", "authors": "Bj\\\"orn Feldkord and Matthias Feldotto and S\\\"oren Riechers", "title": "A Tight Approximation for Fully Dynamic Bin Packing without Bundling", "comments": "To appear in ICALP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variant of the classical Bin Packing Problem, called Fully\nDynamic Bin Packing. In this variant, items of a size in $(0,1]$ must be packed\nin bins of unit size. In each time step, an item either arrives or departs from\nthe packing. An algorithm for this problem must maintain a feasible packing\nwhile only repacking a bounded number of items in each time step.\n  We develop an algorithm which repacks only a constant number of items per\ntime step and, unlike previous work, does not rely on bundling of small items\nwhich allowed those solutions to move an unbounded number of small items as\none. Our algorithm has an asymptotic approximation ratio of roughly $1.3871$\nwhich is complemented by a lower bound of Balogh et al., resulting in a tight\napproximation ratio for this problem. As a direct corollary, we also close the\ngap to the lower bound of the Relaxed Online Bin Packing Problem in which only\ninsertions of items occur.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 16:41:23 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 07:03:06 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Feldkord", "Bj\u00f6rn", ""], ["Feldotto", "Matthias", ""], ["Riechers", "S\u00f6ren", ""]]}, {"id": "1711.01262", "submitter": "He Sun", "authors": "He Sun and Luca Zanetti", "title": "Distributed Graph Clustering and Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph clustering is a fundamental computational problem with a number of\napplications in algorithm design, machine learning, data mining, and analysis\nof social networks. Over the past decades, researchers have proposed a number\nof algorithmic design methods for graph clustering. Most of these methods,\nhowever, are based on complicated spectral techniques or convex optimisation,\nand cannot be directly applied for clustering many networks that occur in\npractice, whose information is often collected on different sites. Designing a\nsimple and distributed clustering algorithm is of great interest, and has wide\napplications for processing big datasets.\n  In this paper we present a simple and distributed algorithm for graph\nclustering: for a wide class of graphs that are characterised by a strong\ncluster-structure, our algorithm finishes in a poly-logarithmic number of\nrounds, and recovers a partition of the graph close to optimal. One of the main\ncomponents behind our algorithm is a sampling scheme that, given a dense graph\nas input, produces a sparse subgraph that provably preserves the\ncluster-structure of the input. Compared with previous sparsification\nalgorithms that require Laplacian solvers or involve combinatorial\nconstructions, this component is easy to implement in a distributed way and\nruns fast in practice.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 17:52:28 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Sun", "He", ""], ["Zanetti", "Luca", ""]]}, {"id": "1711.01295", "submitter": "Juba Ziani", "authors": "Yiling Chen, Nicole Immorlica, Brendan Lucier, Vasilis Syrgkanis, Juba\n  Ziani", "title": "Optimal Data Acquisition for Statistical Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a data analyst's problem of purchasing data from strategic agents\nto compute an unbiased estimate of a statistic of interest. Agents incur\nprivate costs to reveal their data and the costs can be arbitrarily correlated\nwith their data. Once revealed, data are verifiable. This paper focuses on\nlinear unbiased estimators. We design an individually rational and incentive\ncompatible mechanism that optimizes the worst-case mean-squared error of the\nestimation, where the worst-case is over the unknown correlation between costs\nand data, subject to a budget constraint in expectation. We characterize the\nform of the optimal mechanism in closed-form. We further extend our results to\nacquiring data for estimating a parameter in regression analysis, where private\ncosts can correlate with the values of the dependent variable but not with the\nvalues of the independent variables.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 18:47:58 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 07:46:24 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Chen", "Yiling", ""], ["Immorlica", "Nicole", ""], ["Lucier", "Brendan", ""], ["Syrgkanis", "Vasilis", ""], ["Ziani", "Juba", ""]]}, {"id": "1711.01323", "submitter": "Shi Li", "authors": "Ravishankar Krishnaswamy, Shi Li, Sai Sandeep", "title": "Constant Approximation for $k$-Median and $k$-Means with Outliers via\n  Iterative Rounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new iterative rounding framework for many\nclustering problems. Using this, we obtain an $(\\alpha_1 + \\epsilon \\leq 7.081\n+ \\epsilon)$-approximation algorithm for $k$-median with outliers, greatly\nimproving upon the large implicit constant approximation ratio of Chen [Chen,\nSODA 2018]. For $k$-means with outliers, we give an $(\\alpha_2+\\epsilon \\leq\n53.002 + \\epsilon)$-approximation, which is the first $O(1)$-approximation for\nthis problem. The iterative algorithm framework is very versatile; we show how\nit can be used to give $\\alpha_1$- and $(\\alpha_1 + \\epsilon)$-approximation\nalgorithms for matroid and knapsack median problems respectively, improving\nupon the previous best approximations ratios of $8$ [Swamy, ACM Trans.\nAlgorithms] and $17.46$ [Byrka et al, ESA 2015].\n  The natural LP relaxation for the $k$-median/$k$-means with outliers problem\nhas an unbounded integrality gap. In spite of this negative result, our\niterative rounding framework shows that we can round an LP solution to an\nalmost-integral solution of small cost, in which we have at most two\nfractionally open facilities. Thus, the LP integrality gap arises due to the\ngap between almost-integral and fully-integral solutions. Then, using a\npre-processing procedure, we show how to convert an almost-integral solution to\na fully-integral solution losing only a constant-factor in the approximation\nratio. By further using a sparsification technique, the additive factor loss\nincurred by the conversion can be reduced to any $\\epsilon > 0$.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 20:27:12 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 14:14:22 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Krishnaswamy", "Ravishankar", ""], ["Li", "Shi", ""], ["Sandeep", "Sai", ""]]}, {"id": "1711.01328", "submitter": "Yin Tat Lee", "authors": "S\\'ebastien Bubeck and Michael B. Cohen and Yin Tat Lee and Yuanzhi Li", "title": "An homotopy method for $\\ell_p$ regression provably beyond\n  self-concordance and in input-sparsity time", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of linear regression where the $\\ell_2^n$ norm loss\n(i.e., the usual least squares loss) is replaced by the $\\ell_p^n$ norm. We\nshow how to solve such problems up to machine precision in $O^*(n^{|1/2 -\n1/p|})$ (dense) matrix-vector products and $O^*(1)$ matrix inversions, or\nalternatively in $O^*(n^{|1/2 - 1/p|})$ calls to a (sparse) linear system\nsolver. This improves the state of the art for any $p\\not\\in \\{1,2,+\\infty\\}$.\nFurthermore we also propose a randomized algorithm solving such problems in\n{\\em input sparsity time}, i.e., $O^*(Z + \\mathrm{poly}(d))$ where $Z$ is the\nsize of the input and $d$ is the number of variables. Such a result was only\nknown for $p=2$. Finally we prove that these results lie outside the scope of\nthe Nesterov-Nemirovski's theory of interior point methods by showing that any\nsymmetric self-concordant barrier on the $\\ell_p^n$ unit ball has\nself-concordance parameter $\\tilde{\\Omega}(n)$.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 20:50:43 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 17:03:22 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Cohen", "Michael B.", ""], ["Lee", "Yin Tat", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1711.01334", "submitter": "Haoze Wu", "authors": "Haoze Wu", "title": "Two Error Bounds of Imperfect Binary Search", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we know that an object is in a sorted table and we want to determine\nthe index of that object. To achieve this goal we could perform a binary\nsearch. However, suppose it is time-consuming to determine the relative\nposition of that object to any other objects in the table. In this scenario, we\nmight want to resort to an incomplete solution: we could device an algorithm\nthat quickly predicts the result of comparing two objects, and replace the\nactual comparison with this algorithm during a binary search. The question then\nis how far away are the results yielded by the imperfect binary search from the\ncorrect answers. We present two quick lemmas that answer this question.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 22:28:27 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Wu", "Haoze", ""]]}, {"id": "1711.01361", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang, Wenzheng Li, Seth Pettie", "title": "An Optimal Distributed $(\\Delta+1)$-Coloring Algorithm?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vertex coloring is one of the classic symmetry breaking problems studied in\ndistributed computing. In this paper we present a new algorithm for\n$(\\Delta+1)$-list coloring in the randomized ${\\sf LOCAL}$ model running in\n$O(\\mathsf{Det}_{\\scriptscriptstyle d}(\\text{poly} \\log n))$ time, where\n$\\mathsf{Det}_{\\scriptscriptstyle d}(n')$ is the deterministic complexity of\n$(\\text{deg}+1)$-list coloring on $n'$-vertex graphs. (In this problem, each\n$v$ has a palette of size $\\text{deg}(v)+1$.) This improves upon a previous\nrandomized algorithm of Harris, Schneider, and Su [STOC'16, JACM'18] with\ncomplexity $O(\\sqrt{\\log \\Delta} + \\log\\log n +\n\\mathsf{Det}_{\\scriptscriptstyle d}(\\text{poly} \\log n))$, and, for some range\nof $\\Delta$, is much faster than the best known deterministic algorithm of\nFraigniaud, Heinrich, and Kosowski [FOCS'16] and Barenboim, Elkin, and\nGoldenberg [PODC'18], with complexity $O(\\sqrt{\\Delta\\log \\Delta}\\log^\\ast\n\\Delta + \\log^* n)$.\n  Our algorithm \"appears to be\" optimal, in view of the\n$\\Omega(\\mathsf{Det}(\\text{poly} \\log n))$ randomized lower bound due to Chang,\nKopelowitz, and Pettie [FOCS'16], where $\\mathsf{Det}$ is the deterministic\ncomplexity of $(\\Delta+1)$-list coloring. At present, the best upper bounds on\n$\\mathsf{Det}_{\\scriptscriptstyle d}(n')$ and $\\mathsf{Det}(n')$ are both\n$2^{O(\\sqrt{\\log n'})}$ and use a black box application of network\ndecompositions (Panconesi and Srinivasan [Journal of Algorithms'96]). It is\nquite possible that the true complexities of both problems are the same,\nasymptotically, which would imply the randomized optimality of our\n$(\\Delta+1)$-list coloring algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 23:46:52 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 17:58:09 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2019 19:10:04 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Li", "Wenzheng", ""], ["Pettie", "Seth", ""]]}, {"id": "1711.01364", "submitter": "Sebastian Forster", "authors": "Sebastian Forster, Danupon Nanongkai", "title": "A Faster Distributed Single-Source Shortest Paths Algorithm", "comments": "Presented at the the 59th Annual IEEE Symposium on Foundations of\n  Computer Science (FOCS 2018)", "journal-ref": null, "doi": "10.1109/FOCS.2018.00071", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise new algorithms for the single-source shortest paths (SSSP) problem\nwith non-negative edge weights in the CONGEST model of distributed computing.\nWhile close-to-optimal solutions, in terms of the number of rounds spent by the\nalgorithm, have recently been developed for computing SSSP approximately, the\nfastest known exact algorithms are still far away from matching the lower bound\nof $ \\tilde \\Omega (\\sqrt{n} + D) $ rounds by Peleg and Rubinovich [SIAM\nJournal on Computing 2000], where $ n $ is the number of nodes in the network\nand $ D $ is its diameter. The state of the art is Elkin's randomized algorithm\n[STOC 2017] that performs $ \\tilde O(n^{2/3} D^{1/3} + n^{5/6}) $ rounds. We\nsignificantly improve upon this upper bound with our two new randomized\nalgorithms for polynomially bounded integer edge weights, the first performing\n$ \\tilde O (\\sqrt{n D}) $ rounds and the second performing $ \\tilde O (\\sqrt{n}\nD^{1/4} + n^{3/5} + D) $ rounds. Our bounds also compare favorably to the\nindependent result by Ghaffari and Li [STOC 2018]. As side results, we obtain a\n$ (1 + \\epsilon) $-approximation $ \\tilde O ((\\sqrt{n} D^{1/4} + D) / \\epsilon)\n$-round algorithm for directed SSSP and a new work/depth trade-off for exact\nSSSP on directed graphs in the PRAM model.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 23:57:49 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 07:48:09 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2018 08:58:32 GMT"}, {"version": "v4", "created": "Wed, 31 Jul 2019 08:04:29 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Forster", "Sebastian", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1711.01370", "submitter": "Ario Salmasi", "authors": "Ario Salmasi, Anastasios Sidiropoulos, Vijay Sridhar", "title": "On constant multi-commodity flow-cut gaps for directed minor-free graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-commodity flow-cut gap is a fundamental parameter that affects the\nperformance of several divide \\& conquer algorithms, and has been extensively\nstudied for various classes of undirected graphs. It has been shown by Linial,\nLondon and Rabinovich \\cite{linial1994geometry} and by Aumann and Rabani\n\\cite{aumann1998log} that for general $n$-vertex graphs it is bounded by\n$O(\\log n)$ and the Gupta-Newman-Rabinovich-Sinclair conjecture\n\\cite{gupta2004cuts} asserts that it is $O(1)$ for any family of graphs that\nexcludes some fixed minor.\n  The flow-cut gap is poorly understood for the case of directed graphs. We\nshow that for uniform demands it is $O(1)$ on directed series-parallel graphs,\nand on directed graphs of bounded pathwidth. These are the first constant upper\nbounds of this type for some non-trivial family of directed graphs. We also\nobtain $O(1)$ upper bounds for the general multi-commodity flow-cut gap on\ndirected trees and cycles. These bounds are obtained via new embeddings and\nLipschitz quasipartitions for quasimetric spaces, which generalize analogous\nresults form the metric case, and could be of independent interest. Finally, we\ndiscuss limitations of methods that were developed for undirected graphs, such\nas random partitions, and random embeddings.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 00:38:57 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Salmasi", "Ario", ""], ["Sidiropoulos", "Anastasios", ""], ["Sridhar", "Vijay", ""]]}, {"id": "1711.01520", "submitter": "Tal Wagner", "authors": "Piotr Indyk, Ilya Razenshteyn and Tal Wagner", "title": "Practical Data-Dependent Metric Compression with Provable Guarantees", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new distance-preserving compact representation of\nmulti-dimensional point-sets. Given $n$ points in a $d$-dimensional space where\neach coordinate is represented using $B$ bits (i.e., $dB$ bits per point), it\nproduces a representation of size $O( d \\log(d B/\\epsilon) + \\log n)$ bits per\npoint from which one can approximate the distances up to a factor of $1 \\pm\n\\epsilon$. Our algorithm almost matches the recent bound\nof~\\cite{indyk2017near} while being much simpler. We compare our algorithm to\nProduct Quantization (PQ)~\\cite{jegou2011product}, a state of the art heuristic\nmetric compression method. We evaluate both algorithms on several data sets:\nSIFT (used in \\cite{jegou2011product}), MNIST~\\cite{lecun1998mnist}, New York\nCity taxi time series~\\cite{guha2016robust} and a synthetic one-dimensional\ndata set embedded in a high-dimensional space. With appropriately tuned\nparameters, our algorithm produces representations that are comparable to or\nbetter than those produced by PQ, while having provable guarantees on its\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 02:33:17 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Indyk", "Piotr", ""], ["Razenshteyn", "Ilya", ""], ["Wagner", "Tal", ""]]}, {"id": "1711.01521", "submitter": "Jing Qin", "authors": "Jing Qin, Shuang Li, Deanna Needell, Anna Ma, Rachel Grotheer, Chenxi\n  Huang, Natalie Durgin", "title": "Stochastic Greedy Algorithms For Multiple Measurement Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation of a single measurement vector (SMV) has been explored\nin a variety of compressive sensing applications. Recently, SMV models have\nbeen extended to solve multiple measurement vectors (MMV) problems, where the\nunderlying signal is assumed to have joint sparse structures. To circumvent the\nNP-hardness of the $\\ell_0$ minimization problem, many deterministic MMV\nalgorithms solve the convex relaxed models with limited efficiency. In this\npaper, we develop stochastic greedy algorithms for solving the joint sparse MMV\nreconstruction problem. In particular, we propose the MMV Stochastic Iterative\nHard Thresholding (MStoIHT) and MMV Stochastic Gradient Matching Pursuit\n(MStoGradMP) algorithms, and we also utilize the mini-batching technique to\nfurther improve their performance. Convergence analysis indicates that the\nproposed algorithms are able to converge faster than their SMV counterparts,\ni.e., concatenated StoIHT and StoGradMP, under certain conditions. Numerical\nexperiments have illustrated the superior effectiveness of the proposed\nalgorithms over their SMV counterparts.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 02:41:00 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 13:15:08 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Qin", "Jing", ""], ["Li", "Shuang", ""], ["Needell", "Deanna", ""], ["Ma", "Anna", ""], ["Grotheer", "Rachel", ""], ["Huang", "Chenxi", ""], ["Durgin", "Natalie", ""]]}, {"id": "1711.01596", "submitter": "Cameron Musco", "authors": "Cameron Musco and David P. Woodruff", "title": "Is Input Sparsity Time Possible for Kernel Low-Rank Approximation?", "comments": "To appear, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank approximation is a common tool used to accelerate kernel methods:\nthe $n \\times n$ kernel matrix $K$ is approximated via a rank-$k$ matrix\n$\\tilde K$ which can be stored in much less space and processed more quickly.\nIn this work we study the limits of computationally efficient low-rank kernel\napproximation. We show that for a broad class of kernels, including the popular\nGaussian and polynomial kernels, computing a relative error $k$-rank\napproximation to $K$ is at least as difficult as multiplying the input data\nmatrix $A \\in \\mathbb{R}^{n \\times d}$ by an arbitrary matrix $C \\in\n\\mathbb{R}^{d \\times k}$. Barring a breakthrough in fast matrix multiplication,\nwhen $k$ is not too large, this requires $\\Omega(nnz(A)k)$ time where $nnz(A)$\nis the number of non-zeros in $A$. This lower bound matches, in many parameter\nregimes, recent work on subquadratic time algorithms for low-rank approximation\nof general kernels [MM16,MW17], demonstrating that these algorithms are\nunlikely to be significantly improved, in particular to $O(nnz(A))$ input\nsparsity runtimes. At the same time there is hope: we show for the first time\nthat $O(nnz(A))$ time approximation is possible for general radial basis\nfunction kernels (e.g., the Gaussian kernel) for the closely related problem of\nlow-rank approximation of the kernelized dataset.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 14:36:25 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Musco", "Cameron", ""], ["Woodruff", "David P.", ""]]}, {"id": "1711.01616", "submitter": "Shikha Singh", "authors": "Michael A. Bender, Martin Farach-Colton, Mayank Goswami, Rob Johnson,\n  Samuel McCauley, and Shikha Singh", "title": "Bloom Filters, Adaptivity, and the Dictionary Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bloom filter---or, more generally, an approximate membership query data\nstructure (AMQ)---maintains a compact, probabilistic representation of a set S\nof keys from a universe U. An AMQ supports lookups, inserts, and (for some\nAMQs) deletes. A query for an x in S is guaranteed to return \"present.\" A query\nfor x not in S returns \"absent\" with probability at least 1-epsilon, where\nepsilon is a tunable false positive probability. If a query returns \"present,\"\nbut x is not in S, then x is a false positive of the AMQ. Because AMQs have a\nnonzero probability of false-positives, they require far less space than\nexplicit set representations.\n  AMQs are widely used to speed up dictionaries that are stored remotely (e.g.,\non disk/across a network). Most AMQs offer weak guarantees on the number of\nfalse positives they will return on a sequence of queries. The false-positive\nprobability of epsilon holds only for a single query. It is easy for an\nadversary to drive an AMQ's false-positive rate towards 1 by simply repeating\nfalse positives.\n  This paper shows what it takes to get strong guarantees on the number of\nfalse positives. We say that an AMQs is adaptive if it guarantees a\nfalse-positive probability of epsilon for every query, regardless of answers to\nprevious queries. First, we prove that it is impossible to build a small\nadaptive AMQ, even when the AMQ is immediately told whenever it returns a false\npositive. We then show how to build an adaptive AMQ that partitions its state\ninto a small local component and a larger remote component. In addition to\nbeing adaptive, the local component of our AMQ dominates existing AMQs in all\nregards. It uses optimal space up to lower-order terms and supports queries and\nupdates in worst-case constant time, with high probability. Thus, we show that\nadaptivity has no cost.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 17:10:40 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 22:00:55 GMT"}, {"version": "v3", "created": "Mon, 27 Aug 2018 00:33:14 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Bender", "Michael A.", ""], ["Farach-Colton", "Martin", ""], ["Goswami", "Mayank", ""], ["Johnson", "Rob", ""], ["McCauley", "Samuel", ""], ["Singh", "Shikha", ""]]}, {"id": "1711.01623", "submitter": "Seri Khoury", "authors": "Amir Abboud, Keren Censor-Hillel, Seri Khoury, and Christoph Lenzen", "title": "Fooling Views: A New Lower Bound Technique for Distributed Computations\n  under Congestion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel lower bound technique for distributed graph algorithms\nunder bandwidth limitations.\n  We define the notion of \\emph{fooling views} and exemplify its strength by\nproving two new lower bounds for triangle membership in the CONGEST(B) model:\n  (i) Any $1$-round algorithm requires $B\\geq c\\Delta \\log n$ for a constant\n$c>0$.\n  (ii) If $B=1$, even in constant-degree graphs any algorithm must take\n$\\Omega(\\log^* n)$ rounds.\n  The implication of the former is the first proven separation between the\nLOCAL and the CONGEST models for deterministic triangle membership.\n  The latter result is the first non-trivial lower bound on the number of\nrounds required, even for \\emph{triangle detection}, under limited bandwidth.\n  All previous known techniques are provably incapable of giving these bounds.\n  We hope that our approach may pave the way for proving lower bounds for\nadditional problems in various settings of distributed computing for which\nprevious techniques do not suffice.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 17:45:37 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 18:37:48 GMT"}, {"version": "v3", "created": "Wed, 13 Dec 2017 10:22:45 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Abboud", "Amir", ""], ["Censor-Hillel", "Keren", ""], ["Khoury", "Seri", ""], ["Lenzen", "Christoph", ""]]}, {"id": "1711.01655", "submitter": "Elchanan Mossel", "authors": "Vishesh Jain, Frederic Koehler, Elchanan Mossel", "title": "Approximating Partition Functions in Constant Time", "comments": "This preprint is completely subsumed by preprints arXiv:1802.06126\n  and arXiv:1802.06129 by the same authors which also include important\n  references that are missing in the current preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study approximations of the partition function of dense graphical models.\nPartition functions of graphical models play a fundamental role is statistical\nphysics, in statistics and in machine learning. Two of the main methods for\napproximating the partition function are Markov Chain Monte Carlo and\nVariational Methods. An impressive body of work in mathematics, physics and\ntheoretical computer science provides conditions under which Markov Chain Monte\nCarlo methods converge in polynomial time. These methods often lead to\npolynomial time approximation algorithms for the partition function in cases\nwhere the underlying model exhibits correlation decay. There are very few\ntheoretical guarantees for the performance of variational methods. One\nexception is recent results by Risteski (2016) who considered dense graphical\nmodels and showed that using variational methods, it is possible to find an\n$O(\\epsilon n)$ additive approximation to the log partition function in time\n$n^{O(1/\\epsilon^2)}$ even in a regime where correlation decay does not hold.\n  We show that under essentially the same conditions, an $O(\\epsilon n)$\nadditive approximation of the log partition function can be found in constant\ntime, independent of $n$. In particular, our results cover dense Ising and\nPotts models as well as dense graphical models with $k$-wise interaction. They\nalso apply for low threshold rank models.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 20:06:01 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 16:31:41 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Jain", "Vishesh", ""], ["Koehler", "Frederic", ""], ["Mossel", "Elchanan", ""]]}, {"id": "1711.01692", "submitter": "Timothy Carpenter", "authors": "Timothy Carpenter, Ario Salmasi, Anastasios Sidiropoulos", "title": "Routing Symmetric Demands in Directed Minor-Free Graphs with Constant\n  Congestion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of routing in graphs using node-disjoint paths has received a lot\nof attention and a polylogarithmic approximation algorithm with constant\ncongestion is known for undirected graphs [Chuzhoy and Li 2016] and [Chekuri\nand Ene 2013]. However, the problem is hard to approximate within polynomial\nfactors on directed graphs, for any constant congestion [Chuzhoy, Kim and Li\n2016].\n  Recently, [Chekuri, Ene and Pilipczuk 2016] have obtained a polylogarithmic\napproximation with constant congestion on directed planar graphs, for the\nspecial case of symmetric demands. We extend their result by obtaining a\npolylogarithmic approximation with constant congestion on arbitrary directed\nminor-free graphs, for the case of symmetric demands.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 01:31:19 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Carpenter", "Timothy", ""], ["Salmasi", "Ario", ""], ["Sidiropoulos", "Anastasios", ""]]}, {"id": "1711.01700", "submitter": "Jeremy Fineman", "authors": "Jeremy T. Fineman", "title": "Nearly Work-Efficient Parallel Algorithm for Digraph Reachability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the simplest problems on directed graphs is that of identifying the\nset of vertices reachable from a designated source vertex. This problem can be\nsolved easily sequentially by performing a graph search, but efficient parallel\nalgorithms have eluded researchers for decades. For sparse high-diameter graphs\nin particular, there is no known work-efficient parallel algorithm with\nnontrivial parallelism. This amounts to one of the most fundamental open\nquestions in parallel graph algorithms: Is there a parallel algorithm for\ndigraph reachability with nearly linear work? This paper shows that the answer\nis yes.\n  This paper presents a randomized parallel algorithm for digraph reachability\nand related problems with expected work $\\tilde{O}(m)$ and span\n$\\tilde{O}(n^{2/3})$, and hence parallelism $\\tilde{\\Omega}(n^{1/3})$, on any\ngraph with $n$ vertices and $m$ arcs. This is the first parallel algorithm\nhaving both nearly linear work and strongly sublinear span. The algorithm can\nbe extended to produce a directed spanning tree, determine whether the graph is\nacyclic, topologically sort the strongly connected components of the graph, or\nproduce a directed ear decomposition of a strongly connected graph, all with\nsimilar work and span.\n  The main technical contribution is an \\emph{efficient} Monte Carlo algorithm\nthat, through the addition of $\\tilde{O}(n)$ shortcuts, reduces the diameter of\nthe graph to $\\tilde{O}(n^{2/3})$ with high probability. While both sequential\nand parallel algorithms are known with those combinatorial properties, even the\nsequential algorithms are not efficient. This paper presents a surprisingly\nsimple sequential algorithm that achieves the stated diameter reduction and\nruns in $\\tilde{O}(m)$ time. Parallelizing that algorithm yields the main\nresult, but doing so involves overcoming several other challenges.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 02:44:03 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Fineman", "Jeremy T.", ""]]}, {"id": "1711.01789", "submitter": "James Lee", "authors": "James R. Lee", "title": "Fusible HSTs and the randomized k-server conjecture", "comments": "There is a gap in the argument in Section 5.3.2 that requires a\n  substantial revision to correct. See the author's web page for up to date\n  information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.MG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exhibit an $O((\\log k)^6)$-competitive randomized algorithm for the\n$k$-server problem on any metric space. It is shown that a potential-based\nalgorithm for the fractional $k$-server problem on hierarchically separated\ntrees (HSTs) with competitive ratio $f(k)$ can be used to obtain a randomized\nalgorithm for any metric space with competitive ratio $f(k)^2 O((\\log k)^2)$.\nEmploying the $O((\\log k)^2)$-competitive algorithm for HSTs from our joint\nwork with Bubeck, Cohen, Lee, and M\\k{a}dry (2017) yields the claimed bound.\n  The best previous result independent of the geometry of the underlying metric\nspace is the $2k-1$ competitive ratio established for the deterministic work\nfunction algorithm by Koutsoupias and Papadimitriou (1995). Even for the\nspecial case when the underlying metric space is the real line, the best known\ncompetitive ratio was $k$. Since deterministic algorithms can do no better than\n$k$ on any metric space with at least $k+1$ points, this establishes that for\nevery metric space on which the problem is non-trivial, randomized algorithms\ngive an exponential improvement over deterministic algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 08:55:39 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 07:10:14 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 20:22:07 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Lee", "James R.", ""]]}, {"id": "1711.01834", "submitter": "Ashish Chiplunkar", "authors": "Yossi Azar, Ashish Chiplunkar, Haim Kaplan", "title": "Prophet Secretary: Surpassing the $1-1/e$ Barrier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Prophet Secretary problem, samples from a known set of probability\ndistributions arrive one by one in a uniformly random order, and an algorithm\nmust irrevocably pick one of the samples as soon as it arrives. The goal is to\nmaximize the expected value of the sample picked relative to the expected\nmaximum of the distributions. This is one of the most simple and fundamental\nproblems in online decision making that models the process selling one item to\na sequence of costumers. For a closely related problem called the Prophet\nInequality where the order of the random variables is adversarial, it is known\nthat one can achieve in expectation $1/2$ of the expected maximum, and no\nbetter ratio is possible. For the Prophet Secretary problem, that is, when the\nvariables arrive in a random order, Esfandiari et al.\\ (ESA 2015) showed that\none can actually get $1-1/e$ of the maximum. The $1-1/e$ bound was recently\nextended to more general settings (Ehsani et al., 2017). Given these results,\none might be tempted to believe that $1-1/e$ is the correct bound. We show that\nthis is not the case by providing an algorithm for the Prophet Secretary\nproblem that beats the $1-1/e$ bound and achieves $1-1/e+1/400$ of the optimum\nvalue. We also prove a hardness result on the performance of algorithms under a\nnatural restriction which we call deterministic distribution-insensitivity.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 11:14:37 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Azar", "Yossi", ""], ["Chiplunkar", "Ashish", ""], ["Kaplan", "Haim", ""]]}, {"id": "1711.01853", "submitter": "Hannaneh Najdataei", "authors": "Hannaneh Najdataei, Yiannis Nikolakopoulos, Vincenzo Gulisano, Marina\n  Papatriantafilou", "title": "Lisco: A Continuous Approach in LiDAR Point-cloud Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The light detection and ranging (LiDAR) technology allows to sense\nsurrounding objects with fine-grained resolution in a large areas. Their data\n(aka point clouds), generated continuously at very high rates, can provide\ninformation to support automated functionality in cyberphysical systems.\nClustering of point clouds is a key problem to extract this type of\ninformation. Methods for solving the problem in a continuous fashion can\nfacilitate improved processing in e.g. fog architectures, allowing continuous,\nstreaming processing of data close to the sources. We propose Lisco, a\nsingle-pass continuous Euclidean-distance-based clustering of LiDAR point\nclouds, that maximizes the granularity of the data processing pipeline. Besides\nits algorithmic analysis, we provide a thorough experimental evaluation and\nhighlight its up to 3x improvements and its scalability benefits compared to\nthe baseline, using both real-world datasets as well as synthetic ones to fully\nexplore the worst-cases.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 12:21:22 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Najdataei", "Hannaneh", ""], ["Nikolakopoulos", "Yiannis", ""], ["Gulisano", "Vincenzo", ""], ["Papatriantafilou", "Marina", ""]]}, {"id": "1711.01972", "submitter": "Krzysztof Sornat", "authors": "Jaros{\\l}aw Byrka, Krzysztof Sornat, Joachim Spoerhase", "title": "Constant-Factor Approximation for Ordered k-Median", "comments": "22 pages, 3 algorithms, a preliminary conference version of this work\n  will appear in STOC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Ordered k-Median problem, in which the solution is evaluated by\nfirst sorting the client connection costs and then multiplying them with a\npredefined non-increasing weight vector (higher connection costs are taken with\nlarger weights). Since the 1990s, this problem has been studied extensively in\nthe discrete optimization and operations research communities and has emerged\nas a framework unifying many fundamental clustering and location problems such\nas k-Median and k-Center. This generality, however, renders the problem\nintriguing from the algorithmic perspective and obtaining non-trivial\napproximation algorithms was an open problem even for simple topologies such as\ntrees. Recently, Aouad and Segev were able to obtain an O(log n) approximation\nalgorithm for Ordered k-Median using a sophisticated local-search approach and\nthe concept of surrogate models thereby extending the result by Tamir (2001)\nfor the case of a rectangular weight vector, also known as k-Facility\np-Centrum.\n  In this paper, we provide an LP-rounding constant-factor approximation\nalgorithm for the Ordered k-Median problem.\n  We first provide a new analysis of the rounding process by Charikar and Li\n(2012) for k-Median, when applied to a fractional solution obtained from\nsolving an LP relaxation over a non-metric, truncated cost vector, resulting in\nan elegant 15-approximation for the rectangular case. Then, we show that a\nsimple weight bucketing can be applied to the general case resulting in O(log\nn) rectangles and hence in a constant-factor approximation in quasi-polynomial\ntime. Finally, we show that also the clever distance bucketing by Aouad and\nSegev can be combined with the objective-oblivious version of our LP-rounding\nfor the rectangular case, and that it results in a true, polynomial time,\nconstant-factor approximation algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 15:54:16 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 18:34:54 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Byrka", "Jaros\u0142aw", ""], ["Sornat", "Krzysztof", ""], ["Spoerhase", "Joachim", ""]]}, {"id": "1711.01980", "submitter": "Julia Chuzhoy", "authors": "Julia Chuzhoy, David H. K. Kim, Rachit Nimavat", "title": "Almost Polynomial Hardness of Node-Disjoint Paths in Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical Node-Disjoint Paths (NDP) problem, we are given an\n$n$-vertex graph $G=(V,E)$, and a collection $M=\\{(s_1,t_1),\\ldots,(s_k,t_k)\\}$\nof pairs of its vertices, called source-destination, or demand pairs. The goal\nis to route as many of the demand pairs as possible, where to route a pair we\nneed to select a path connecting it, so that all selected paths are disjoint in\ntheir vertices. The best current algorithm for NDP achieves an\n$O(\\sqrt{n})$-approximation, while, until recently, the best negative result\nwas a factor $\\Omega(\\log^{1/2-\\epsilon}n)$-hardness of approximation, for any\nconstant $\\epsilon$, unless $NP \\subseteq ZPTIME(n^{poly \\log n})$. In a recent\nwork, the authors have shown an improved $2^{\\Omega(\\sqrt{\\log n})}$-hardness\nof approximation for NDP, unless $NP\\subseteq DTIME(n^{O(\\log n)})$, even if\nthe underlying graph is a subgraph of a grid graph, and all source vertices lie\non the boundary of the grid. Unfortunately, this result does not extend to grid\ngraphs.\n  The approximability of the NDP problem on grid graphs has remained a\ntantalizing open question, with the best current upper bound of\n$\\tilde{O}(n^{1/4})$, and the best current lower bound of APX-hardness. In this\npaper we come close to resolving the approximability of NDP in general, and NDP\nin grids in particular. Our main result is that NDP is\n$2^{\\Omega(\\log^{1-\\epsilon} n)}$-hard to approximate for any constant\n$\\epsilon$, assuming that $NP\\not\\subseteq RTIME(n^{poly\\log n})$, and that it\nis $n^{\\Omega (1/(\\log \\log n)^2)}$-hard to approximate, assuming that for some\nconstant $\\delta>0$, $NP \\not \\subseteq RTIME(2^{n^{\\delta}})$. These results\nhold even for grid graphs and wall graphs, and extend to the closely related\nEdge-Disjoint Paths problem, even in wall graphs.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 16:06:10 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Chuzhoy", "Julia", ""], ["Kim", "David H. K.", ""], ["Nimavat", "Rachit", ""]]}, {"id": "1711.02032", "submitter": "Du\\v{s}an Knop", "authors": "Tom\\'a\\v{s} Gaven\\v{c}iak and Du\\v{s}an Knop and Martin Kouteck\\'y", "title": "Integer Programming in Parameterized Complexity: Three Miniatures", "comments": "27 pages, extended abstract to appear in proceeding of IPEC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powerful results from the theory of integer programming have recently led to\nsubstantial advances in parameterized complexity. However, our perception is\nthat, except for Lenstra's algorithm for solving integer linear programming in\nfixed dimension, there is still little understanding in the parameterized\ncomplexity community of the strengths and limitations of the available tools.\nThis is understandable: it is often difficult to infer exact runtimes or even\nthe distinction between FPT and XP algorithms, and some knowledge is simply\nunwritten folklore in a different community. We wish to make a step in\nremedying this situation.\n  To that end, we first provide an easy to navigate quick reference guide of\ninteger programming algorithms from the perspective of parameterized\ncomplexity. Then, we show their applications in three case studies, obtaining\nFPT algorithms with runtime $f(k)poly(n)$. We focus on:\n  * Modeling: since the algorithmic results follow by applying existing\nalgorithms to new models, we shift the focus from the complexity result to the\nmodeling result, highlighting common patterns and tricks which are used.\n  * Optimality program: after giving an FPT algorithm, we are interested in\nreducing the dependence on the parameter; we show which algorithms and tricks\nare often useful for speed-ups.\n  * Minding the poly(n): reducing $f(k)$ often has the unintended consequence\nof increasing poly(n); so we highlight the common trade-offs and show how to\nget the best of both worlds.\n  Specifically, we consider graphs of bounded neighborhood diversity which are\nin a sense the simplest of dense graphs, and we show several FPT algorithms for\nCapacitated Dominating Set, Sum Coloring, and Max-q-Cut by modeling them as\nconvex programs in fixed dimension, n-fold integer programs, bounded dual\ntreewidth programs, and indefinite quadratic programs in fixed dimension.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 17:37:27 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 07:18:37 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2018 08:07:25 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Gaven\u010diak", "Tom\u00e1\u0161", ""], ["Knop", "Du\u0161an", ""], ["Kouteck\u00fd", "Martin", ""]]}, {"id": "1711.02035", "submitter": "Kiavash Kianfar", "authors": "Kiavash Kianfar, Christopher Pockrandt, Bahman Torkamandi, Haochen\n  Luo, Knut Reinert", "title": "Optimum Search Schemes for Approximate String Matching Using\n  Bidirectional FM-Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding approximate occurrences of a pattern in a text using a full-text\nindex is a central problem in bioinformatics and has been extensively\nresearched. Bidirectional indices have opened new possibilities in this regard\nallowing the search to start from anywhere within the pattern and extend in\nboth directions. In particular, use of search schemes (partitioning the pattern\nand searching the pieces in certain orders with given bounds on errors) can\nyield significant speed-ups. However, finding optimal search schemes is a\ndifficult combinatorial optimization problem.\n  Here for the first time, we propose a mixed integer program (MIP) capable to\nsolve this optimization problem for Hamming distance with given number of\npieces. Our experiments show that the optimal search schemes found by our MIP\nsignificantly improve the performance of search in bidirectional FM-index upon\nprevious ad-hoc solutions. For example, approximate matching of 101-bp Illumina\nreads (with two errors) becomes 35 times faster than standard backtracking.\nMoreover, despite being performed purely in the index, the running time of\nsearch using our optimal schemes (for up to two errors) is comparable to the\nbest state-of-the-art aligners, which benefit from combining search in index\nwith in-text verification using dynamic programming. As a result, we anticipate\na full-fledged aligner that employs an intelligent combination of search in the\nbidirectional FM-index using our optimal search schemes and in-text\nverification using dynamic programming outperforms today's best aligners. The\ndevelopment of such an aligner, called FAMOUS (Fast Approximate string Matching\nusing OptimUm search Schemes), is ongoing as our future work.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 17:41:38 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 18:39:29 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Kianfar", "Kiavash", ""], ["Pockrandt", "Christopher", ""], ["Torkamandi", "Bahman", ""], ["Luo", "Haochen", ""], ["Reinert", "Knut", ""]]}, {"id": "1711.02036", "submitter": "Damian Straszak", "authors": "Damian Straszak and Nisheeth K. Vishnoi", "title": "Maximum Entropy Distributions: Bit Complexity and Stability", "comments": "To appear in COLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum entropy distributions with discrete support in $m$ dimensions arise\nin machine learning, statistics, information theory, and theoretical computer\nscience. While structural and computational properties of max-entropy\ndistributions have been extensively studied, basic questions such as: Do\nmax-entropy distributions over a large support (e.g., $2^m$) with a specified\nmarginal vector have succinct descriptions (polynomial-size in the input\ndescription)? and: Are entropy maximizing distributions \"stable\" under the\nperturbation of the marginal vector? have resisted a rigorous resolution.\n  Here we show that these questions are related and resolve both of them. Our\nmain result shows a ${\\rm poly}(m, \\log 1/\\varepsilon)$ bound on the bit\ncomplexity of $\\varepsilon$-optimal dual solutions to the maximum entropy\nconvex program -- for very general support sets and with no restriction on the\nmarginal vector. Applications of this result include polynomial time algorithms\nto compute max-entropy distributions over several new and old polytopes for any\nmarginal vector in a unified manner, a polynomial time algorithm to compute the\nBrascamp-Lieb constant in the rank-1 case. The proof of this result allows us\nto show that changing the marginal vector by $\\delta$ changes the max-entropy\ndistribution in the total variation distance roughly by a factor of ${\\rm\npoly}(m, \\log 1/\\delta)\\sqrt{\\delta}$ -- even when the size of the support set\nis exponential. Together, our results put max-entropy distributions on a\nmathematically sound footing -- these distributions are robust and\ncomputationally feasible models for data.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 17:42:18 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 20:26:05 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Straszak", "Damian", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1711.02076", "submitter": "Robert Ganian", "authors": "Robert Ganian, Sebastian Ordyniak, M. S. Ramanujan", "title": "On Structural Parameterizations of the Edge Disjoint Paths Problem", "comments": "Accepted for ISAAC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit the classical Edge Disjoint Paths (EDP) problem,\nwhere one is given an undirected graph G and a set of terminal pairs P and asks\nwhether G contains a set of pairwise edge-disjoint paths connecting every\nterminal pair in P. Our focus lies on structural parameterizations for the\nproblem that allow for efficient (polynomial-time or fpt) algorithms. As our\nfirst result, we answer an open question stated in Fleszar, Mnich, and\nSpoerhase (2016), by showing that the problem can be solved in polynomial time\nif the input graph has a feedback vertex set of size one. We also show that EDP\nparameterized by the treewidth and the maximum degree of the input graph is\nfixed-parameter tractable.\n  Having developed two novel algorithms for EDP using structural restrictions\non the input graph, we then turn our attention towards the augmented graph,\ni.e., the graph obtained from the input graph after adding one edge between\nevery terminal pair. In constrast to the input graph, where EDP is known to\nremain NP-hard even for treewidth two, a result by Zhou et al. (2000) shows\nthat EDP can be solved in non-uniform polynomial time if the augmented graph\nhas constant treewidth; we note that the possible improvement of this result to\nan fpt-algorithm has remained open since then. We show that this is highly\nunlikely by establishing the W[1]-hardness of the problem parameterized by the\ntreewidth (and even feedback vertex set) of the augmented graph. Finally, we\ndevelop an fpt-algorithm for EDP by exploiting a novel structural parameter of\nthe augmented graph.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 18:43:18 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Ganian", "Robert", ""], ["Ordyniak", "Sebastian", ""], ["Ramanujan", "M. S.", ""]]}, {"id": "1711.02078", "submitter": "David Wajc", "authors": "Anupam Gupta, Guru Guruganesh, Amit Kumar, David Wajc", "title": "Fully-Dynamic Bin Packing with Limited Repacking", "comments": "To appear in ICALP 2018. Improved worst-case recourse for unit costs\n  added (Theorem 2.7)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classic Bin Packing problem in a fully-dynamic setting, where\nnew items can arrive and old items may depart. We want algorithms with low\nasymptotic competitive ratio \\emph{while repacking items sparingly} between\nupdates. Formally, each item $i$ has a \\emph{movement cost} $c_i\\geq 0$, and we\nwant to use $\\alpha \\cdot OPT$ bins and incur a movement cost $\\gamma\\cdot\nc_i$, either in the worst case, or in an amortized sense, for $\\alpha, \\gamma$\nas small as possible. We call $\\gamma$ the \\emph{recourse} of the algorithm.\nThis is motivated by cloud storage applications, where fully-dynamic Bin\nPacking models the problem of data backup to minimize the number of disks used,\nas well as communication incurred in moving file backups between disks. Since\nthe set of files changes over time, we could recompute a solution periodically\nfrom scratch, but this would give a high number of disk rewrites, incurring a\nhigh energy cost and possible wear and tear of the disks. In this work, we\npresent optimal tradeoffs between number of bins used and number of items\nrepacked, as well as natural extensions of the latter measure.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 18:49:46 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 18:43:02 GMT"}, {"version": "v3", "created": "Wed, 16 May 2018 21:53:23 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Gupta", "Anupam", ""], ["Guruganesh", "Guru", ""], ["Kumar", "Amit", ""], ["Wajc", "David", ""]]}, {"id": "1711.02120", "submitter": "Robert Ganian", "authors": "Eduard Eiben, Robert Ganian, Sebastian Ordyniak", "title": "Small Resolution Proofs for QBF using Dependency Treewidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the close connection between the evaluation of quantified Boolean\nformulas (QBF) and propositional satisfiability (SAT), tools and techniques\nwhich exploit structural properties of SAT instances are known to fail for QBF.\nThis is especially true for the structural parameter treewidth, which has\nallowed the design of successful algorithms for SAT but cannot be\nstraightforwardly applied to QBF since it does not take into account the\ninterdependencies between quantified variables.\n  In this work we introduce and develop dependency treewidth, a new structural\nparameter based on treewidth which allows the efficient solution of QBF\ninstances. Dependency treewidth pushes the frontiers of tractability for QBF by\novercoming the limitations of previously introduced variants of treewidth for\nQBF. We augment our results by developing algorithms for computing the\ndecompositions that are required to use the parameter.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 19:14:43 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Eiben", "Eduard", ""], ["Ganian", "Robert", ""], ["Ordyniak", "Sebastian", ""]]}, {"id": "1711.02194", "submitter": "David Harris", "authors": "Mohsen Ghaffari and David G. Harris and Fabian Kuhn", "title": "On Derandomizing Local Distributed Algorithms", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gap between the known randomized and deterministic local distributed\nalgorithms underlies arguably the most fundamental and central open question in\ndistributed graph algorithms. In this paper, we develop a generic and clean\nrecipe for derandomizing LOCAL algorithms. We also exhibit how this simple\nrecipe leads to significant improvements on a number of problem. Two main\nresults are:\n  - An improved distributed hypergraph maximal matching algorithm, improving on\nFischer, Ghaffari, and Kuhn [FOCS'17], and giving improved algorithms for\nedge-coloring, maximum matching approximation, and low out-degree edge\norientation. The first gives an improved algorithm for Open Problem 11.4 of the\nbook of Barenboim and Elkin, and the last gives the first positive resolution\nof their Open Problem 11.10.\n  - An improved distributed algorithm for the Lov\\'{a}sz Local Lemma, which\ngets closer to a conjecture of Chang and Pettie [FOCS'17], and moreover leads\nto improved distributed algorithms for problems such as defective coloring and\n$k$-SAT.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 22:14:06 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 23:13:23 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 15:08:21 GMT"}, {"version": "v4", "created": "Tue, 17 Sep 2019 20:09:04 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Harris", "David G.", ""], ["Kuhn", "Fabian", ""]]}, {"id": "1711.02195", "submitter": "Hunter Lang", "authors": "Hunter Lang, David Sontag, Aravindan Vijayaraghavan", "title": "Optimality of Approximate Inference Algorithms on Stable Instances", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate algorithms for structured prediction problems---such as LP\nrelaxations and the popular alpha-expansion algorithm (Boykov et al.\n2001)---typically far exceed their theoretical performance guarantees on\nreal-world instances. These algorithms often find solutions that are very close\nto optimal. The goal of this paper is to partially explain the performance of\nalpha-expansion and an LP relaxation algorithm on MAP inference in\nFerromagnetic Potts models (FPMs). Our main results give stability conditions\nunder which these two algorithms provably recover the optimal MAP solution.\nThese theoretical results complement numerous empirical observations of good\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 22:14:34 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 16:02:44 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Lang", "Hunter", ""], ["Sontag", "David", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1711.02305", "submitter": "Kai Sheng Tai", "authors": "Kai Sheng Tai, Vatsal Sharan, Peter Bailis, Gregory Valiant", "title": "Sketching Linear Classifiers over Data Streams", "comments": "Full version of paper appearing at SIGMOD 2018 with more detailed\n  proofs of theoretical results. Code available at\n  https://github.com/stanford-futuredata/wmsketch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new sub-linear space sketch---the Weight-Median Sketch---for\nlearning compressed linear classifiers over data streams while supporting the\nefficient recovery of large-magnitude weights in the model. This enables\nmemory-limited execution of several statistical analyses over streams,\nincluding online feature selection, streaming data explanation, relative\ndeltoid detection, and streaming estimation of pointwise mutual information.\nUnlike related sketches that capture the most frequently-occurring features (or\nitems) in a data stream, the Weight-Median Sketch captures the features that\nare most discriminative of one stream (or class) compared to another. The\nWeight-Median Sketch adopts the core data structure used in the Count-Sketch,\nbut, instead of sketching counts, it captures sketched gradient updates to the\nmodel parameters. We provide a theoretical analysis that establishes recovery\nguarantees for batch and online learning, and demonstrate empirical\nimprovements in memory-accuracy trade-offs over alternative memory-budgeted\nmethods, including count-based sketches and feature hashing.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 06:37:27 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 18:08:41 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Tai", "Kai Sheng", ""], ["Sharan", "Vatsal", ""], ["Bailis", "Peter", ""], ["Valiant", "Gregory", ""]]}, {"id": "1711.02455", "submitter": "Leqi Zhu", "authors": "Faith Ellen, Rati Gelashvili, Leqi Zhu", "title": "Revisionist Simulations: A New Approach to Proving Space Lower Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the space complexity of $x$-obstruction-free $k$-set agreement\nfor $x\\leq k$ is an open problem. In $x$-obstruction-free protocols, processes\nare required to return in executions where at most $x$ processes take steps.\nThe best known upper bound on the number of registers needed to solve this\nproblem among $n>k$ processes is $n-k+x$ registers. No general lower bound\nbetter than $2$ was known.\n  We prove that any $x$-obstruction-free protocol solving $k$-set agreement\namong $n>k$ processes uses at least $\\lfloor(n-x)/(k+1-x)\\rfloor+1$ registers.\nOur main tool is a simulation that serves as a reduction from the impossibility\nof deterministic wait-free $k$-set agreement: if a protocol uses fewer\nregisters, then it is possible for $k+1$ processes to simulate the protocol and\ndeterministically solve $k$-set agreement in a wait-free manner, which is\nimpossible. A critical component of the simulation is the ability of simulating\nprocesses to revise the past of simulated processes. We introduce a new\naugmented snapshot object, which facilitates this.\n  We also prove that any space lower bound on the number of registers used by\nobstruction-free protocols applies to protocols that satisfy nondeterministic\nsolo termination. Hence, our lower bound of $\\lfloor(n-1)/k\\rfloor+1$ for the\nobstruction-free ($x=1$) case also holds for randomized wait-free free\nprotocols. In particular, this gives a tight lower bound of exactly $n$\nregisters for solving obstruction-free and randomized wait-free consensus.\n  Finally, our new techniques can be applied to get a space lower of $\\lfloor\nn/2\\rfloor+1$ for $\\epsilon$-approximate agreement, for sufficiently small\n$\\epsilon$. It requires participating processes to return values within\n$\\epsilon$ of each other. The best known upper bounds are\n$\\lceil\\log(1/\\epsilon)\\rceil$ and $n$, while no general lower bounds were\nknown.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 13:34:52 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 21:42:50 GMT"}, {"version": "v3", "created": "Sat, 6 Jan 2018 04:01:03 GMT"}, {"version": "v4", "created": "Mon, 26 Feb 2018 11:00:07 GMT"}, {"version": "v5", "created": "Wed, 10 Oct 2018 17:18:27 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Ellen", "Faith", ""], ["Gelashvili", "Rati", ""], ["Zhu", "Leqi", ""]]}, {"id": "1711.02524", "submitter": "Anastasios Kyrillidis", "authors": "Anastasios Kyrillidis, Amir Kalev, Dohuyng Park, Srinadh Bhojanapalli,\n  Constantine Caramanis, Sujay Sanghavi", "title": "Provable quantum state tomography via non-convex methods", "comments": "21 pages, 26 figures, code included", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With nowadays steadily growing quantum processors, it is required to develop\nnew quantum tomography tools that are tailored for high-dimensional systems. In\nthis work, we describe such a computational tool, based on recent ideas from\nnon-convex optimization. The algorithm excels in the compressed-sensing-like\nsetting, where only a few data points are measured from a low-rank or\nhighly-pure quantum state of a high-dimensional system. We show that the\nalgorithm can practically be used in quantum tomography problems that are\nbeyond the reach of convex solvers, and, moreover, is faster than other\nstate-of-the-art non-convex approaches. Crucially, we prove that, despite being\na non-convex program, under mild conditions, the algorithm is guaranteed to\nconverge to the global minimum of the problem; thus, it constitutes a provable\nquantum state tomography protocol.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 22:05:59 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 22:18:26 GMT"}, {"version": "v3", "created": "Sun, 19 Nov 2017 00:34:39 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Kyrillidis", "Anastasios", ""], ["Kalev", "Amir", ""], ["Park", "Dohuyng", ""], ["Bhojanapalli", "Srinadh", ""], ["Caramanis", "Constantine", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1711.02598", "submitter": "Slobodan Mitrovi\\'c", "authors": "Slobodan Mitrovi\\'c, Ilija Bogunovic, Ashkan Norouzi-Fard, Jakub\n  Tarnawski, Volkan Cevher", "title": "Streaming Robust Submodular Maximization: A Partitioned Thresholding\n  Approach", "comments": "To appear in NIPS 2017", "journal-ref": "Proc. of 30th Advances in Neural Information Processing Systems\n  (NIPS) 2017, pages 4558-4567", "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classical problem of maximizing a monotone submodular function\nsubject to a cardinality constraint k, with two additional twists: (i) elements\narrive in a streaming fashion, and (ii) m items from the algorithm's memory are\nremoved after the stream is finished. We develop a robust submodular algorithm\nSTAR-T. It is based on a novel partitioning structure and an exponentially\ndecreasing thresholding rule. STAR-T makes one pass over the data and retains a\nshort but robust summary. We show that after the removal of any m elements from\nthe obtained summary, a simple greedy algorithm STAR-T-GREEDY that runs on the\nremaining elements achieves a constant-factor approximation guarantee. In two\ndifferent data summarization tasks, we demonstrate that it matches or\noutperforms existing greedy and streaming methods, even if they are allowed the\nbenefit of knowing the removed subset in advance.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 16:37:25 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Mitrovi\u0107", "Slobodan", ""], ["Bogunovic", "Ilija", ""], ["Norouzi-Fard", "Ashkan", ""], ["Tarnawski", "Jakub", ""], ["Cevher", "Volkan", ""]]}, {"id": "1711.02621", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi, Nisheeth K. Vishnoi", "title": "Convex Optimization with Unbounded Nonconvex Oracles using Simulated\n  Annealing", "comments": "To appear in COLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing a convex objective function $F$ when\none can only evaluate its noisy approximation $\\hat{F}$. Unless one assumes\nsome structure on the noise, $\\hat{F}$ may be an arbitrary nonconvex function,\nmaking the task of minimizing $F$ intractable. To overcome this, prior work has\noften focused on the case when $F(x)-\\hat{F}(x)$ is uniformly-bounded. In this\npaper we study the more general case when the noise has magnitude $\\alpha F(x)\n+ \\beta$ for some $\\alpha, \\beta > 0$, and present a polynomial time algorithm\nthat finds an approximate minimizer of $F$ for this noise model. Previously,\nMarkov chains, such as the stochastic gradient Langevin dynamics, have been\nused to arrive at approximate solutions to these optimization problems.\nHowever, for the noise model considered in this paper, no single temperature\nallows such a Markov chain to both mix quickly and concentrate near the global\nminimizer. We bypass this by combining \"simulated annealing\" with the\nstochastic gradient Langevin dynamics, and gradually decreasing the temperature\nof the chain in order to approach the global minimizer. As a corollary one can\napproximately minimize a nonconvex function that is close to a convex function;\nhowever, the closeness can deteriorate as one moves away from the optimum.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 17:36:47 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 09:00:43 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Mangoubi", "Oren", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1711.02724", "submitter": "Karthik Abinav Sankararaman", "authors": "Brian Brubach, Karthik Abinav Sankararaman, Aravind Srinivasan and Pan\n  Xu", "title": "Algorithms to Approximate Column-Sparse Packing Problems", "comments": "Extended abstract appeared in SODA 2018. Full version in ACM\n  Transactions of Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Column-sparse packing problems arise in several contexts in both\ndeterministic and stochastic discrete optimization. We present two unifying\nideas, (non-uniform) attenuation and multiple-chance algorithms, to obtain\nimproved approximation algorithms for some well-known families of such\nproblems. As three main examples, we attain the integrality gap, up to\nlower-order terms, for known LP relaxations for k-column sparse packing integer\nprograms (Bansal et al., Theory of Computing, 2012) and stochastic k-set\npacking (Bansal et al., Algorithmica, 2012), and go \"half the remaining\ndistance\" to optimal for a major integrality-gap conjecture of Furedi, Kahn and\nSeymour on hypergraph matching (Combinatorica, 1993).\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 20:54:28 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 22:17:45 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 16:22:13 GMT"}, {"version": "v4", "created": "Mon, 11 Dec 2017 23:09:11 GMT"}, {"version": "v5", "created": "Wed, 31 Jul 2019 02:07:01 GMT"}, {"version": "v6", "created": "Mon, 5 Aug 2019 20:01:51 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Brubach", "Brian", ""], ["Sankararaman", "Karthik Abinav", ""], ["Srinivasan", "Aravind", ""], ["Xu", "Pan", ""]]}, {"id": "1711.02743", "submitter": "Rachel Grotheer", "authors": "Natalie Durgin, Rachel Grotheer, Chenxi Huang, Shuang Li, Anna Ma,\n  Deanna Needell, Jing Qin", "title": "Sparse Randomized Kaczmarz for Support Recovery of Jointly Sparse\n  Corrupted Multiple Measurement Vectors", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While single measurement vector (SMV) models have been widely studied in\nsignal processing, there is a surging interest in addressing the multiple\nmeasurement vectors (MMV) problem. In the MMV setting, more than one\nmeasurement vector is available and the multiple signals to be recovered share\nsome commonalities such as a common support. Applications in which MMV is a\nnaturally occurring phenomenon include online streaming, medical imaging, and\nvideo recovery. This work presents a stochastic iterative algorithm for the\nsupport recovery of jointly sparse corrupted MMV. We present a variant of the\nSparse Randomized Kaczmarz algorithm for corrupted MMV and compare our proposed\nmethod with an existing Kaczmarz type algorithm for MMV problems. We also\nshowcase the usefulness of our approach in the online (streaming) setting and\nprovide empirical evidence that suggests the robustness of the proposed method\nto the distribution of the corruption and the number of corruptions occurring.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 21:54:43 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 20:25:36 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2018 13:20:07 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Durgin", "Natalie", ""], ["Grotheer", "Rachel", ""], ["Huang", "Chenxi", ""], ["Li", "Shuang", ""], ["Ma", "Anna", ""], ["Needell", "Deanna", ""], ["Qin", "Jing", ""]]}, {"id": "1711.02855", "submitter": "Takaaki Nishimoto", "authors": "Takaaki Nishimoto, Yoshimasa Takabatake, Yasuo Tabei", "title": "A compressed dynamic self-index for highly repetitive text collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel compressed dynamic self-index for highly repetitive text\ncollections. Signature encoding is a compressed dynamic self-index for highly\nrepetitive texts and has a large disadvantage that the pattern search for short\npatterns is slow. We improve this disadvantage for faster pattern search by\nleveraging an idea behind truncated suffix tree and present the first\ncompressed dynamic self-index named TST-index that supports not only fast\npattern search but also dynamic update operation of index for highly repetitive\ntexts. Experiments using a benchmark dataset of highly repetitive texts show\nthat the pattern search of TST-index is significantly improved.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 07:43:35 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 02:08:58 GMT"}, {"version": "v3", "created": "Wed, 17 Jan 2018 09:23:16 GMT"}, {"version": "v4", "created": "Tue, 24 Apr 2018 09:59:51 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Nishimoto", "Takaaki", ""], ["Takabatake", "Yoshimasa", ""], ["Tabei", "Yasuo", ""]]}, {"id": "1711.02860", "submitter": "Kasper Green Larsen", "authors": "Kasper Green Larsen", "title": "Constructive Discrepancy Minimization with Hereditary L2 Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In discrepancy minimization problems, we are given a family of sets\n$\\mathcal{S} = \\{S_1,\\dots,S_m\\}$, with each $S_i \\in \\mathcal{S}$ a subset of\nsome universe $U = \\{u_1,\\dots,u_n\\}$ of $n$ elements. The goal is to find a\ncoloring $\\chi : U \\to \\{-1,+1\\}$ of the elements of $U$ such that each set $S\n\\in \\mathcal{S}$ is colored as evenly as possible. Two classic measures of\ndiscrepancy are $\\ell_\\infty$-discrepancy defined as\n$\\textrm{disc}_\\infty(\\mathcal{S},\\chi):=\\max_{S \\in \\mathcal{S}} | \\sum_{u_i\n\\in S} \\chi(u_i) |$ and $\\ell_2$-discrepancy defined as\n$\\textrm{disc}_2(\\mathcal{S},\\chi):=\\sqrt{(1/|\\mathcal{S}|)\\sum_{S \\in\n\\mathcal{S}} \\left(\\sum_{u_i \\in S}\\chi(u_i)\\right)^2}$. Breakthrough work by\nBansal gave a polynomial time algorithm, based on rounding an SDP, for finding\na coloring $\\chi$ such that $\\textrm{disc}_\\infty(\\mathcal{S},\\chi) = O(\\lg n\n\\cdot \\textrm{herdisc}_\\infty(\\mathcal{S}))$ where\n$\\textrm{herdisc}_\\infty(\\mathcal{S})$ is the hereditary\n$\\ell_\\infty$-discrepancy of $\\mathcal{S}$. We complement his work by giving a\nsimple $O((m+n)n^2)$ time algorithm for finding a coloring $\\chi$ such\n$\\textrm{disc}_2(\\mathcal{S},\\chi) = O(\\sqrt{\\lg n} \\cdot\n\\textrm{herdisc}_2(\\mathcal{S}))$ where $\\textrm{herdisc}_2(\\mathcal{S})$ is\nthe hereditary $\\ell_2$-discrepancy of $\\mathcal{S}$. Interestingly, our\nalgorithm avoids solving an SDP and instead relies on computing\neigendecompositions of matrices. Moreover, we use our ideas to speed up the\nEdge-Walk algorithm by Lovett and Meka [SICOMP'15]. To prove that our algorithm\nhas the claimed guarantees, we show new inequalities relating\n$\\textrm{herdisc}_\\infty$ and $\\textrm{herdisc}_2$ to the eigenvalues of the\nmatrix corresponding to $\\mathcal{S}$. Our inequalities improve over previous\nwork by Chazelle and Lvov, and by Matousek et al. Finally, we also implement\nour algorithm and show that it far outperforms random sampling.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 08:05:42 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 13:48:15 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 09:21:00 GMT"}, {"version": "v4", "created": "Thu, 13 Dec 2018 09:06:01 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Larsen", "Kasper Green", ""]]}, {"id": "1711.02910", "submitter": "Dmitry Kosolobov", "authors": "Jos\\'e Fuentes-Sep\\'ulveda, Juha K\\\"arkk\\\"ainen, Dmitry Kosolobov,\n  Simon J. Puglisi", "title": "Run Compressed Rank/Select for Large Alphabets", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. 10 pages, 1 figure, 4\n  tables; published in DCC'2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a string of length $n$ that is composed of $r$ runs of letters from the\nalphabet $\\{0,1,\\ldots,\\sigma{-}1\\}$ such that $2 \\le \\sigma \\le r$, we\ndescribe a data structure that, provided $r \\le n / \\log^{\\omega(1)} n$, stores\nthe string in $r\\log\\frac{n\\sigma}{r} + o(r\\log\\frac{n\\sigma}{r})$ bits and\nsupports select and access queries in $O(\\log\\frac{\\log(n/r)}{\\log\\log n})$\ntime and rank queries in $O(\\log\\frac{\\log(n\\sigma/r)}{\\log\\log n})$ time. We\nshow that $r\\log\\frac{n(\\sigma-1)}{r} - O(\\log\\frac{n}{r})$ bits are necessary\nfor any such data structure and, thus, our solution is succinct. We also\ndescribe a data structure that uses $(1 + \\epsilon)r\\log\\frac{n\\sigma}{r} +\nO(r)$ bits, where $\\epsilon > 0$ is an arbitrary constant, with the same query\ntimes but without the restriction $r \\le n / \\log^{\\omega(1)} n$. By simple\nreductions to the colored predecessor problem, we show that the query times are\noptimal in the important case $r \\ge 2^{\\log^\\delta n}$, for an arbitrary\nconstant $\\delta > 0$. We implement our solution and compare it with the state\nof the art, showing that the closest competitors consume 31-46% more space.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 12:01:47 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 11:51:21 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 14:19:58 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Fuentes-Sep\u00falveda", "Jos\u00e9", ""], ["K\u00e4rkk\u00e4inen", "Juha", ""], ["Kosolobov", "Dmitry", ""], ["Puglisi", "Simon J.", ""]]}, {"id": "1711.03076", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, MohammadHossein Bateni, Aaron Bernstein, Vahab\n  Mirrokni, Cliff Stein", "title": "Coresets Meet EDCS: Algorithms for Matching and Vertex Cover on Massive\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As massive graphs become more prevalent, there is a rapidly growing need for\nscalable algorithms that solve classical graph problems, such as maximum\nmatching and minimum vertex cover, on large datasets. For massive inputs,\nseveral different computational models have been introduced, including the\nstreaming model, the distributed communication model, and the massively\nparallel computation (MPC) model that is a common abstraction of\nMapReduce-style computation. In each model, algorithms are analyzed in terms of\nresources such as space used or rounds of communication needed, in addition to\nthe more traditional approximation ratio.\n  In this paper, we give a single unified approach that yields better\napproximation algorithms for matching and vertex cover in all these models. The\nhighlights include:\n  * The first one pass, significantly-better-than-2-approximation for matching\nin random arrival streams that uses subquadratic space, namely a\n$(1.5+\\epsilon)$-approximation streaming algorithm that uses $O(n^{1.5})$ space\nfor constant $\\epsilon > 0$.\n  * The first 2-round, better-than-2-approximation for matching in the MPC\nmodel that uses subquadratic space per machine, namely a\n$(1.5+\\epsilon)$-approximation algorithm with $O(\\sqrt{mn} + n)$ memory per\nmachine for constant $\\epsilon > 0$.\n  By building on our unified approach, we further develop parallel algorithms\nin the MPC model that give a $(1 + \\epsilon)$-approximation to matching and an\n$O(1)$-approximation to vertex cover in only $O(\\log\\log{n})$ MPC rounds and\n$O(n/poly\\log{(n)})$ memory per machine. These results settle multiple open\nquestions posed in the recent paper of Czumaj~et.al. [STOC 2018].\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 18:13:20 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 16:08:52 GMT"}, {"version": "v3", "created": "Thu, 27 Dec 2018 21:28:24 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Assadi", "Sepehr", ""], ["Bateni", "MohammadHossein", ""], ["Bernstein", "Aaron", ""], ["Mirrokni", "Vahab", ""], ["Stein", "Cliff", ""]]}, {"id": "1711.03165", "submitter": "Tselil Schramm", "authors": "Aviad Rubinstein, Tselil Schramm, S. Matthew Weinberg", "title": "Computing exact minimum cuts without knowing the graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give query-efficient algorithms for the global min-cut and the s-t cut\nproblem in unweighted, undirected graphs. Our oracle model is inspired by the\nsubmodular function minimization problem: on query $S \\subset V$, the oracle\nreturns the size of the cut between $S$ and $V \\setminus S$.\n  We provide algorithms computing an exact minimum $s$-$t$ cut in $G$ with\n$\\tilde{O}(n^{5/3})$ queries, and computing an exact global minimum cut of $G$\nwith only $\\tilde{O}(n)$ queries (while learning the graph requires\n$\\tilde{\\Theta}(n^2)$ queries).\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 21:07:27 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 19:31:55 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Rubinstein", "Aviad", ""], ["Schramm", "Tselil", ""], ["Weinberg", "S. Matthew", ""]]}, {"id": "1711.03178", "submitter": "Long Gong", "authors": "Long Gong and Jun (Jim) Xu", "title": "R(QPS-Serena) and R(QPS-Serenade): Two Novel Augmenting-Path Based\n  Algorithms for Computing Approximate Maximum Weight Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this addendum, we show that the switching algorithm QPS-SERENA can be\nconverted R(QPS-SERENA), an algorithm for computing approximate Maximum Weight\nMatching (MWM). Empirically, R(QPS-SERENA) computes $(1-\\epsilon)$-MWM within\nlinear time (with respect to the number of edges $N^2$) for any fixed\n$\\epsilon\\in (0,1)$, for complete bipartite graphs with {\\it i.i.d.} uniform\nedge weight distributions. This efficacy matches that of the state-of-art\nsolution, although we so far cannot prove any theoretical guarantees on the\ntime complexities needed to attain a certain approximation ratio. Then, we have\nsimilarly converted QPS-SERENADE to R(QPS-SERENADE), which empirically should\noutput $(1-\\epsilon)$-MWM within only $O(N \\log N)$ time for the same type of\ncomplete bipartite graphs as described above.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 21:43:12 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Gong", "Long", "", "Jim"], ["Jun", "", "", "Jim"], ["Xu", "", ""]]}, {"id": "1711.03205", "submitter": "Felipe A. Louza", "authors": "Daniel Saad Nogueira Nunes, Felipe A. Louza, Simon Gog, Mauricio\n  Ayala-Rinc\\'on, Gonzalo Navarro", "title": "A Grammar Compression Algorithm based on Induced Suffix Sorting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce GCIS, a grammar compression algorithm based on the induced\nsuffix sorting algorithm SAIS, introduced by Nong et al. in 2009. Our solution\nbuilds on the factorization performed by SAIS during suffix sorting. We\nconstruct a context-free grammar on the input string which can be further\nreduced into a shorter string by substituting each substring by its\ncorrespondent factor. The resulting grammar is encoded by exploring some\nredundancies, such as common prefixes between suffix rules, which are sorted\naccording to SAIS framework. When compared to well-known compression tools such\nas Re-Pair and 7-zip, our algorithm is competitive and very effective at\nhandling repetitive string regarding compression ratio, compression and\ndecompression running time.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 23:17:48 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Nunes", "Daniel Saad Nogueira", ""], ["Louza", "Felipe A.", ""], ["Gog", "Simon", ""], ["Ayala-Rinc\u00f3n", "Mauricio", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1711.03256", "submitter": "Bhadrachalam Chitturi", "authors": "Bhadrachalam Chitturi and Priyanshu Das", "title": "Distances in and Layering of a DAG", "comments": "4 Pages. Minor modification in the way affiliation is written. The\n  time complexites of stretch and diameter are reversed. In big O notation V\n  and E are changed to |V| and |E| respectively. The last sentence in the\n  conclusion is rewritten", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diameter of an undirected unweighted graph $G=(V,E)$ is the maximum value\nof the distance from any vertex $u$ to another vertex $v$ for $u,v \\in V$ where\ndistance i.e. $d(u,v)$ is the length of the shortest path from $u$ to $v$ in\n$G$. DAG, is a directed graph without a cycle. We denote the diameter of an\nunweighted DAG $G=(V,E)$ by $\\delta (G)$. The stretch of a DAG $G$ is the\nlength of longest path from $u$ to $v$ in $G$, for all choices of $(u, v) \\in\nV$ denoted by $\\Delta (G)$. The diameter of an undirected graph can be computed\nin $O(|V|(|V|+|E|))$ time by executing breadth first search $|V|$ times. We\nshow that stretch and diameter of a DAG can be computed in $O(|V|+|E|)$ time\nand $O(|V||E|)$ time respectively.\n  A DAG is balanced if and only if a consistent assignment of level numbers to\nall vertices is possible. Layering refers to such an assignment. A balanced DAG\nis defined. An efficient algorithm that either detects whether a given DAG is\nunbalanced or layers it otherwise is designed with a running time of\n$O(|V|+|E|)$. \\\\ Key words: Diameter, directed acyclic graph, longest directed\npath, graph algorithms, complexity.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 05:25:04 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 04:40:31 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Chitturi", "Bhadrachalam", ""], ["Das", "Priyanshu", ""]]}, {"id": "1711.03272", "submitter": "Siddharth Krishna", "authors": "Siddharth Krishna, Dennis Shasha, Thomas Wies", "title": "Go with the Flow: Compositional Abstractions for Concurrent Data\n  Structures (Extended Version)", "comments": "This is an extended version of a POPL 2018 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrent separation logics have helped to significantly simplify\ncorrectness proofs for concurrent data structures. However, a recurring problem\nin such proofs is that data structure abstractions that work well in the\nsequential setting are much harder to reason about in a concurrent setting due\nto complex sharing and overlays. To solve this problem, we propose a novel\napproach to abstracting regions in the heap by encoding the data structure\ninvariant into a local condition on each individual node. This condition may\ndepend on a quantity associated with the node that is computed as a fixpoint\nover the entire heap graph. We refer to this quantity as a flow. Flows can\nencode both structural properties of the heap (e.g. the reachable nodes from\nthe root form a tree) as well as data invariants (e.g. sortedness). We then\nintroduce the notion of a flow interface, which expresses the relies and\nguarantees that a heap region imposes on its context to maintain the local flow\ninvariant with respect to the global heap. Our main technical result is that\nthis notion leads to a new semantic model of separation logic. In this model,\nflow interfaces provide a general abstraction mechanism for describing complex\ndata structures. This abstraction mechanism admits proof rules that generalize\nover a wide variety of data structures. To demonstrate the versatility of our\napproach, we show how to extend the logic RGSep with flow interfaces. We have\nused this new logic to prove linearizability and memory safety of nontrivial\nconcurrent data structures. In particular, we obtain parametric linearizability\nproofs for concurrent dictionary algorithms that abstract from the details of\nthe underlying data structure representation. These proofs cannot be easily\nexpressed using the abstraction mechanisms provided by existing separation\nlogics.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 06:45:24 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Krishna", "Siddharth", ""], ["Shasha", "Dennis", ""], ["Wies", "Thomas", ""]]}, {"id": "1711.03336", "submitter": "Antoine Limasset", "authors": "Antoine Limasset, Jean-Francois Flot, Pierre Peterlongo", "title": "Toward perfect reads: self-correction of short reads via mapping on de\n  Bruijn graphs", "comments": "RECOMB SEQ 2018 Submission", "journal-ref": "Bioinformatics 36(5) 1374-1381 (2020)", "doi": "10.1093/bioinformatics/btz102", "report-no": null, "categories": "cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivations Short-read accuracy is important for downstream analyses such as\ngenome assembly and hybrid long-read correction. Despite much work on\nshort-read correction, present-day correctors either do not scale well on large\ndata sets or consider reads as mere suites of k-mers, without taking into\naccount their full-length read information. Results We propose a new method to\ncorrect short reads using de Bruijn graphs, and implement it as a tool called\nBcool. As a first st ep, Bcool constructs a compacted de Bruijn graph from the\nreads. This graph is filtered on the basis of k-mer abundance then of unitig\nabundance, thereby removing from most sequencing errors. The cleaned graph is\nthen used as a reference on which the reads are mapped to correct them. We show\nthat this approach yields more accurate reads than k-mer-spectrum correctors\nwhile being scalable to human-size genomic datasets and beyond. Availability\nand Implementation The implementation is open source and available at\nhttp://github.com/Malfoy/BCOOL under the Affero GPL license. Contact Antoine\nLimasset antoine.limasset@gmail.com & Jean-Fran\\c{c}ois Flot jflot@ulb.ac.be &\nPierre Peterlongo pierre.peterlongo@inria.fr\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 11:47:40 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 09:44:44 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Limasset", "Antoine", ""], ["Flot", "Jean-Francois", ""], ["Peterlongo", "Pierre", ""]]}, {"id": "1711.03359", "submitter": "Michal Dory", "authors": "Keren Censor-Hillel, Michal Dory", "title": "Fast Distributed Approximation for TAP and 2-Edge-Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tree augmentation problem (TAP) is a fundamental network design problem,\nin which the input is a graph $G$ and a spanning tree $T$ for it, and the goal\nis to augment $T$ with a minimum set of edges $Aug$ from $G$, such that $T \\cup\nAug$ is 2-edge-connected.\n  TAP has been widely studied in the sequential setting. The best known\napproximation ratio of 2 for the weighted case dates back to the work of\nFrederickson and J\\'{a}J\\'{a}, SICOMP 1981. Recently, a 3/2-approximation was\ngiven for unweighted TAP by Kortsarz and Nutov, TALG 2016. Recent breakthroughs\ngive an approximation of 1.458 for unweighted TAP [Grandoni et al., STOC 2018],\nand approximations better than 2 for bounded weights [Adjiashvili, SODA 2017;\nFiorini et al., SODA 2018].\n  In this paper, we provide the first fast distributed approximations for TAP.\nWe present a distributed $2$-approximation for weighted TAP which completes in\n$O(h)$ rounds, where $h$ is the height of $T$. When $h$ is large, we show a\nmuch faster 4-approximation algorithm for the unweighted case, completing in\n$O(D+\\sqrt{n}\\log^*{n})$ rounds, where $n$ is the number of vertices and $D$ is\nthe diameter of $G$.\n  Immediate consequences of our results are an $O(D)$-round 2-approximation\nalgorithm for the minimum size 2-edge-connected spanning subgraph, which\nsignificantly improves upon the running time of previous approximation\nalgorithms, and an $O(h_{MST}+\\sqrt{n}\\log^{*}{n})$-round 3-approximation\nalgorithm for the weighted case, where $h_{MST}$ is the height of the MST of\nthe graph. Additional applications are algorithms for verifying\n2-edge-connectivity and for augmenting the connectivity of any connected\nspanning subgraph to 2.\n  Finally, we complement our study with proving lower bounds for distributed\napproximations of TAP.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 12:58:12 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 09:31:50 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Dory", "Michal", ""]]}, {"id": "1711.03396", "submitter": "Chihao Zhang", "authors": "Heng Guo, Chao Liao, Pinyan Lu, Chihao Zhang", "title": "Counting hypergraph colorings in the local lemma regime", "comments": "v3: Constants Changed. Accepted to SICOMP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a fully polynomial-time approximation scheme (FPTAS) to count the\nnumber of $q$-colorings for $k$-uniform hypergraphs with maximum degree\n$\\Delta$ if $k\\ge 28$ and $q >357\\Delta^{\\frac{14}{k-14}}$ . We also obtain a\npolynomial-time almost uniform sampler if $q>931\\Delta^{\\frac{16}{k-16/3}}$.\nThese are the first approximate counting and sampling algorithms in the regime\n$q\\ll\\Delta$ (for large $\\Delta$ and $k$) without any additional assumptions.\nOur method is based on the recent work of Moitra (STOC, 2017). One important\ncontribution of ours is to remove the dependency of $k$ and $\\Delta$ in\nMoitra's approach.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 14:49:59 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 08:52:45 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 13:48:14 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Guo", "Heng", ""], ["Liao", "Chao", ""], ["Lu", "Pinyan", ""], ["Zhang", "Chihao", ""]]}, {"id": "1711.03440", "submitter": "Zhao Song", "authors": "Kai Zhong, Zhao Song, Inderjit S. Dhillon", "title": "Learning Non-overlapping Convolutional Neural Networks with Multiple\n  Kernels", "comments": "arXiv admin note: text overlap with arXiv:1706.03175", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider parameter recovery for non-overlapping\nconvolutional neural networks (CNNs) with multiple kernels. We show that when\nthe inputs follow Gaussian distribution and the sample size is sufficiently\nlarge, the squared loss of such CNNs is $\\mathit{~locally~strongly~convex}$ in\na basin of attraction near the global optima for most popular activation\nfunctions, like ReLU, Leaky ReLU, Squared ReLU, Sigmoid and Tanh. The required\nsample complexity is proportional to the dimension of the input and polynomial\nin the number of kernels and a condition number of the parameters. We also show\nthat tensor methods are able to initialize the parameters to the local strong\nconvex region. Hence, for most smooth activations, gradient descent following\ntensor initialization is guaranteed to converge to the global optimal with time\nthat is linear in input dimension, logarithmic in precision and polynomial in\nother factors. To the best of our knowledge, this is the first work that\nprovides recovery guarantees for CNNs with multiple kernels under polynomial\nsample and computational complexities.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 14:45:31 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Zhong", "Kai", ""], ["Song", "Zhao", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1711.03639", "submitter": "Thodoris Lykouris", "authors": "Thodoris Lykouris, Karthik Sridharan, and Eva Tardos", "title": "Small-loss bounds for online learning with partial information", "comments": "The current version represents the content that will appear in\n  Mathematics of Operations Research. An extended abstract of the paper\n  appeared at the 31st Annual Conference on Learning Theory (COLT 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of adversarial (non-stochastic) online learning with\npartial information feedback, where at each round, a decision maker selects an\naction from a finite set of alternatives. We develop a black-box approach for\nsuch problems where the learner observes as feedback only losses of a subset of\nthe actions that includes the selected action. When losses of actions are\nnon-negative, under the graph-based feedback model introduced by Mannor and\nShamir, we offer algorithms that attain the so called \"small-loss\" $o(\\alpha\nL^{\\star})$ regret bounds with high probability, where $\\alpha$ is the\nindependence number of the graph, and $L^{\\star}$ is the loss of the best\naction. Prior to our work, there was no data-dependent guarantee for general\nfeedback graphs even for pseudo-regret (without dependence on the number of\nactions, i.e. utilizing the increased information feedback). Taking advantage\nof the black-box nature of our technique, we extend our results to many other\napplications such as semi-bandits (including routing in networks), contextual\nbandits (even with an infinite comparator class), as well as learning with\nslowly changing (shifting) comparators.\n  In the special case of classical bandit and semi-bandit problems, we provide\noptimal small-loss, high-probability guarantees of\n$\\tilde{O}(\\sqrt{dL^{\\star}})$ for actual regret, where $d$ is the number of\nactions, answering open questions of Neu. Previous bounds for bandits and\nsemi-bandits were known only for pseudo-regret and only in expectation. We also\noffer an optimal $\\tilde{O}(\\sqrt{\\kappa L^{\\star}})$ regret guarantee for\nfixed feedback graphs with clique-partition number at most $\\kappa$.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 23:06:55 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 01:13:42 GMT"}, {"version": "v3", "created": "Tue, 12 Jun 2018 05:12:21 GMT"}, {"version": "v4", "created": "Sat, 8 Feb 2020 16:41:42 GMT"}, {"version": "v5", "created": "Mon, 26 Jul 2021 19:01:09 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Lykouris", "Thodoris", ""], ["Sridharan", "Karthik", ""], ["Tardos", "Eva", ""]]}, {"id": "1711.03885", "submitter": "D\\'aniel Marx", "authors": "Daniel Lokshtanov and D\\'aniel Marx", "title": "Clustering with Local Restrictions", "comments": "Conference version in ICALP 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a family of graph clustering problems where each cluster has to\nsatisfy a certain local requirement. Formally, let $\\mu$ be a function on the\nsubsets of vertices of a graph $G$. In the $(\\mu,p,q)$-PARTITION problem, the\ntask is to find a partition of the vertices into clusters where each cluster\n$C$ satisfies the requirements that (1) at most $q$ edges leave $C$ and (2)\n$\\mu(C)\\le p$. Our first result shows that if $\\mu$ is an {\\em arbitrary}\npolynomial-time computable monotone function, then $(\\mu,p,q)$-PARTITION can be\nsolved in time $n^{O(q)}$, i.e., it is polynomial-time solvable {\\em for every\nfixed $q$}. We study in detail three concrete functions $\\mu$ (the number of\nvertices in the cluster, number of nonedges in the cluster, maximum number of\nnon-neighbors a vertex has in the cluster), which correspond to natural\nclustering problems. For these functions, we show that $(\\mu,p,q)$-PARTITION\ncan be solved in time $2^{O(p)}\\cdot n^{O(1)}$ and in time $2^{O(q)}\\cdot\nn^{O(1)}$ on $n$-vertex graphs, i.e., the problem is fixed-parameter tractable\nparameterized by $p$ or by $q$.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 15:41:00 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1711.03887", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Daniel Graf, Karim Labib, Przemys{\\l}aw Uzna\\'nski", "title": "Hamming distance completeness and sparse matrix multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a broad class of $(+,\\diamond)$ vector products (for binary\ninteger functions $\\diamond$) are equivalent under one-to-polylog reductions to\nthe computation of the Hamming distance. Examples include: the dominance\nproduct, the threshold product and $\\ell_{2p+1}$ distances for constant $p$.\nOur results imply equivalence (up to polylog factors) between complexity of\ncomputation of All Pairs: Hamming Distances, $\\ell_{2p+1}$ Distances, Dominance\nProducts and Threshold Products. As a consequence, Yuster's~(SODA'09) algorithm\nimproves not only Matou\\v{s}ek's (IPL'91), but also the results of Indyk,\nLewenstein, Lipsky and Porat (ICALP'04) and Min, Kao and Zhu (COCOON'09).\nFurthermore, our reductions apply to the pattern matching setting, showing\nequivalence (up to polylog factors) between pattern matching under Hamming\nDistance, $\\ell_{2p+1}$ Distance, Dominance Product and Threshold Product, with\ncurrent best upperbounds due to results of Abrahamson (SICOMP'87), Amir and\nFarach (Ann.~Math.~Artif.~Intell.'91), Atallah and Duket (IPL'11), Clifford,\nClifford and Iliopoulous (CPM'05) and Amir, Lipsky, Porat and Umanski (CPM'05).\nThe resulting algorithms for $\\ell_{2p+1}$ Pattern Matching and All Pairs\n$\\ell_{2p+1}$, for $2p+1 = 3,5,7,\\dots$ are new.\n  Additionally, we show that the complexity of AllPairsHammingDistances (and\nthus of other aforementioned AllPairs- problems) is within polylog from the\ntime it takes to multiply matrices $n \\times (n\\cdot d)$ and $(n\\cdot d) \\times\nn$, each with $(n\\cdot d)$ non-zero entries. This means that the current\nupperbounds by Yuster (SODA'09) cannot be improved without improving the sparse\nmatrix multiplication algorithm by Yuster and Zwick~(ACM TALG'05) and vice\nversa.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 15:41:53 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 22:20:51 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Graf", "Daniel", ""], ["Labib", "Karim", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1711.03894", "submitter": "D\\'aniel Marx", "authors": "Andrei Krokhin and D\\'aniel Marx", "title": "On the hardness of losing weight", "comments": "Conference version in ICALP 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of local search for the Boolean constraint\nsatisfaction problem (CSP), in the following form: given a CSP instance, that\nis, a collection of constraints, and a solution to it, the question is whether\nthere is a better (lighter, i.e., having strictly less Hamming weight) solution\nwithin a given distance from the initial solution. We classify the complexity,\nboth classical and parameterized, of such problems by a Schaefer-style\ndichotomy result, that is, with a restricted set of allowed types of\nconstraints. Our results show that there is a considerable amount of such\nproblems that are NP-hard, but fixed-parameter tractable when parameterized by\nthe distance.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 15:48:24 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Krokhin", "Andrei", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1711.03948", "submitter": "Palash Dey", "authors": "Palash Dey", "title": "Manipulative Elicitation -- A New Attack on Elections with Incomplete\n  Preferences", "comments": "To appear in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lu and Boutilier proposed a novel approach based on \"minimax regret\" to use\nclassical score based voting rules in the setting where preferences can be any\npartial (instead of complete) orders over the set of alternatives. We show here\nthat such an approach is vulnerable to a new kind of manipulation which was not\npresent in the classical (where preferences are complete orders) world of\nvoting. We call this attack \"manipulative elicitation.\" More specifically, it\nmay be possible to (partially) elicit the preferences of the agents in a way\nthat makes some distinguished alternative win the election who may not be a\nwinner if we elicit every preference completely. More alarmingly, we show that\nthe related computational task is polynomial time solvable for a large class of\nvoting rules which includes all scoring rules, maximin, Copeland$^\\alpha$ for\nevery $\\alpha\\in[0,1]$, simplified Bucklin voting rules, etc. We then show that\nintroducing a parameter per pair of alternatives which specifies the minimum\nnumber of partial preferences where this pair of alternatives must be\ncomparable makes the related computational task of manipulative elicitation\n\\NPC for all common voting rules including a class of scoring rules which\nincludes the plurality, $k$-approval, $k$-veto, veto, and Borda voting rules,\nmaximin, Copeland$^\\alpha$ for every $\\alpha\\in[0,1]$, and simplified Bucklin\nvoting rules. Hence, in this work, we discover a fundamental vulnerability in\nusing minimax regret based approach in partial preferential setting and propose\na novel way to tackle it.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 18:21:05 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Dey", "Palash", ""]]}, {"id": "1711.04049", "submitter": "Vasileios Nakos", "authors": "Vasileios Nakos", "title": "One-Bit ExpanderSketch for One-Bit Compressed Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to obliviously construct a set of hyperplanes H such that you\ncan approximate a unit vector x when you are given the side on which the vector\nlies with respect to every h in H? In the sparse recovery literature, where x\nis approximately k-sparse, this problem is called one-bit compressed sensing\nand has received a fair amount of attention the last decade. In this paper we\nobtain the first scheme that achieves almost optimal measurements and sublinear\ndecoding time for one-bit compressed sensing in the non-uniform case. For a\nlarge range of parameters, we improve the state of the art in both the number\nof measurements and the decoding time.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 01:02:36 GMT"}, {"version": "v2", "created": "Sun, 13 Jan 2019 20:19:32 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Nakos", "Vasileios", ""]]}, {"id": "1711.04297", "submitter": "Yuanhong Wang", "authors": "Yuanhong Wang, Yuyi Wang, Xingwu Liu, Juhua Pu", "title": "On the ERM Principle with Networked Data", "comments": "accepted by AAAI. arXiv admin note: substantial text overlap with\n  arXiv:math/0702683 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Networked data, in which every training example involves two objects and may\nshare some common objects with others, is used in many machine learning tasks\nsuch as learning to rank and link prediction. A challenge of learning from\nnetworked examples is that target values are not known for some pairs of\nobjects. In this case, neither the classical i.i.d.\\ assumption nor techniques\nbased on complete U-statistics can be used. Most existing theoretical results\nof this problem only deal with the classical empirical risk minimization (ERM)\nprinciple that always weights every example equally, but this strategy leads to\nunsatisfactory bounds. We consider general weighted ERM and show new universal\nrisk bounds for this problem. These new bounds naturally define an optimization\nproblem which leads to appropriate weights for networked examples. Though this\noptimization problem is not convex in general, we devise a new fully\npolynomial-time approximation scheme (FPTAS) to solve it.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 14:00:44 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 10:41:42 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Wang", "Yuanhong", ""], ["Wang", "Yuyi", ""], ["Liu", "Xingwu", ""], ["Pu", "Juhua", ""]]}, {"id": "1711.04326", "submitter": "Amin Saied", "authors": "Amin Saied", "title": "An efficient algorithm computing composition factors of $T(V)^{\\otimes\n  n}$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm that computes the composition factors of the n-th\ntensor power of the free associative algebra on a vector space. The composition\nfactors admit a description in terms of certain coefficients $c_{\\lambda\\mu}$\ndetermining their irreducible structure. By reinterpreting these coefficients\nas counting the number of ways to solve certain `decomposition-puzzles' we are\nable to design an efficient algorithm extending the range of computation by a\nfactor of over 750. Furthermore, by visualising the data appropriately, we gain\ninsights into the nature of the coefficients leading to the development of a\nnew representation theoretic framework called PD-modules.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 17:48:16 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Saied", "Amin", ""]]}, {"id": "1711.04355", "submitter": "Sayan Bhattacharya", "authors": "Sayan Bhattacharya and Deeparnab Chakrabarty and Monika Henzinger and\n  Danupon Nanongkai", "title": "Dynamic Algorithms for Graph Coloring", "comments": "To appear in SODA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design fast dynamic algorithms for proper vertex and edge colorings in a\ngraph undergoing edge insertions and deletions. In the static setting, there\nare simple linear time algorithms for $(\\Delta+1)$- vertex coloring and\n$(2\\Delta-1)$-edge coloring in a graph with maximum degree $\\Delta$. It is\nnatural to ask if we can efficiently maintain such colorings in the dynamic\nsetting as well. We get the following three results. (1) We present a\nrandomized algorithm which maintains a $(\\Delta+1)$-vertex coloring with\n$O(\\log \\Delta)$ expected amortized update time. (2) We present a deterministic\nalgorithm which maintains a $(1+o(1))\\Delta$-vertex coloring with\n$O(\\text{poly} \\log \\Delta)$ amortized update time. (3) We present a simple,\ndeterministic algorithm which maintains a $(2\\Delta-1)$-edge coloring with\n$O(\\log \\Delta)$ worst-case update time. This improves the recent\n$O(\\Delta)$-edge coloring algorithm with $\\tilde{O}(\\sqrt{\\Delta})$ worst-case\nupdate time by Barenboim and Maimon.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 21:05:29 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Bhattacharya", "Sayan", ""], ["Chakrabarty", "Deeparnab", ""], ["Henzinger", "Monika", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1711.04367", "submitter": "Samson Zhou", "authors": "Elena Grigorescu, Erfan Sadeqi Azer, Samson Zhou", "title": "Longest Alignment with Edits in Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing patterns in data streams generated by network traffic, sensor\nnetworks, or satellite feeds is a challenge for systems in which the available\nstorage is limited. In addition, real data is noisy, which makes designing data\nstream algorithms even more challenging.\n  Motivated by such challenges, we study algorithms for detecting the\nsimilarity of two data streams that can be read in sync. Two strings $S, T\\in\n\\Sigma^n$ form a $d$-near-alignment if the distance between them in some given\nmetric is at most $d$. We study the problem of identifying a longest substring\nof $S$ and $T$ that forms a $d$-near-alignment under the edit distance, in the\nsimultaneous streaming model. In this model, symbols of strings $S$ and $T$ are\nstreamed at the same time, and the amount of available processing space is\nsublinear in the length of the strings.\n  We give several algorithms, including an exact one-pass algorithm that uses\n$\\mathcal{O}(d^2+d\\log n)$ bits of space. We couple these results with\ncomparable lower bounds.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 21:53:25 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Grigorescu", "Elena", ""], ["Azer", "Erfan Sadeqi", ""], ["Zhou", "Samson", ""]]}, {"id": "1711.04467", "submitter": "Lior Kamma", "authors": "Diptarka Chakraborty, Lior Kamma, Kasper Green Larsen", "title": "Tight Cell Probe Bounds for Succinct Boolean Matrix-Vector\n  Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conjectured hardness of Boolean matrix-vector multiplication has been\nused with great success to prove conditional lower bounds for numerous\nimportant data structure problems, see Henzinger et al. [STOC'15]. In recent\nwork, Larsen and Williams [SODA'17] attacked the problem from the upper bound\nside and gave a surprising cell probe data structure (that is, we only charge\nfor memory accesses, while computation is free). Their cell probe data\nstructure answers queries in $\\tilde{O}(n^{7/4})$ time and is succinct in the\nsense that it stores the input matrix in read-only memory, plus an additional\n$\\tilde{O}(n^{7/4})$ bits on the side. In this paper, we essentially settle the\ncell probe complexity of succinct Boolean matrix-vector multiplication. We\npresent a new cell probe data structure with query time $\\tilde{O}(n^{3/2})$\nstoring just $\\tilde{O}(n^{3/2})$ bits on the side. We then complement our data\nstructure with a lower bound showing that any data structure storing $r$ bits\non the side, with $n < r < n^2$ must have query time $t$ satisfying $t r =\n\\tilde{\\Omega}(n^3)$. For $r \\leq n$, any data structure must have $t =\n\\tilde{\\Omega}(n^2)$. Since lower bounds in the cell probe model also apply to\nclassic word-RAM data structures, the lower bounds naturally carry over. We\nalso prove similar lower bounds for matrix-vector multiplication over\n$\\mathbb{F}_2$.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 08:29:45 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Chakraborty", "Diptarka", ""], ["Kamma", "Lior", ""], ["Larsen", "Kasper Green", ""]]}, {"id": "1711.04496", "submitter": "Boris Klemz", "authors": "Boris Klemz, G\\\"unter Rote", "title": "Linear-Time Algorithms for Maximum-Weight Induced Matchings and Minimum\n  Chain Covers in Convex Bipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bipartite graph $G=(U,V,E)$ is convex if the vertices in $V$ can be\nlinearly ordered such that for each vertex $u\\in U$, the neighbors of $u$ are\nconsecutive in the ordering of $V$. An induced matching $H$ of $G$ is a\nmatching such that no edge of $E$ connects endpoints of two different edges of\n$H$. We show that in a convex bipartite graph with $n$ vertices and $m$\nweighted edges, an induced matching of maximum total weight can be computed in\n$O(n+m)$ time. An unweighted convex bipartite graph has a representation of\nsize $O(n)$ that records for each vertex $u\\in U$ the first and last neighbor\nin the ordering of $V$. Given such a compact representation, we compute an\ninduced matching of maximum cardinality in $O(n)$ time.\n  In convex bipartite graphs, maximum-cardinality induced matchings are dual to\nminimum chain covers. A chain cover is a covering of the edge set by chain\nsubgraphs, that is, subgraphs that do not contain induced matchings of more\nthan one edge. Given a compact representation, we compute a representation of a\nminimum chain cover in $O(n)$ time. If no compact representation is given, the\ncover can be computed in $O(n+m)$ time.\n  All of our algorithms achieve optimal running time for the respective problem\nand model. Previous algorithms considered only the unweighted case, and the\nbest algorithm for computing a maximum-cardinality induced matching or a\nminimum chain cover in a convex bipartite graph had a running time of $O(n^2)$.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 10:04:52 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Klemz", "Boris", ""], ["Rote", "G\u00fcnter", ""]]}, {"id": "1711.04506", "submitter": "D\\'aniel Marx", "authors": "Martin Grohe, D\\'aniel Marx", "title": "Constraint Solving via Fractional Edge Covers", "comments": "Conference version in SODA 2006", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important combinatorial problems can be modeled as constraint\nsatisfaction problems. Hence identifying polynomial-time solvable classes of\nconstraint satisfaction problems has received a lot of attention. In this\npaper, we are interested in structural properties that can make the problem\ntractable. So far, the largest structural class that is known to be\npolynomial-time solvable is the class of bounded hypertree width instances\nintroduced by Gottlob et al. Here we identify a new class of polynomial-time\nsolvable instances: those having bounded fractional edge cover number.\n  Combining hypertree width and fractional edge cover number, we then introduce\nthe notion of fractional hypertree width. We prove that constraint satisfaction\nproblems with bounded fractional hypertree width can be solved in polynomial\ntime (provided that a the tree decomposition is given in the input). Together\nwith a recent approximation algorithm for finding such decompositions by Marx,\nit follows that bounded fractional hypertree width is now the most general\nknown structural property that guarantees polynomial-time solvability.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 10:37:36 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Grohe", "Martin", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1711.04520", "submitter": "Till Fluschnik", "authors": "Till Fluschnik, Piotr Skowron, Mervin Triphaus, and Kai Wilker", "title": "Fair Knapsack", "comments": "Extended abstract will appear in Proc. of 33rd AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following multiagent variant of the knapsack problem. We are\ngiven a set of items, a set of voters, and a value of the budget; each item is\nendowed with a cost and each voter assigns to each item a certain value. The\ngoal is to select a subset of items with the total cost not exceeding the\nbudget, in a way that is consistent with the voters' preferences. Since the\npreferences of the voters over the items can vary significantly, we need a way\nof aggregating these preferences, in order to select the socially best valid\nknapsack. We study three approaches to aggregating voters' preferences, which\nare motivated by the literature on multiwinner elections and fair allocation.\nThis way we introduce the concepts of individually best, diverse, and fair\nknapsack. We study the computational complexity (including parameterized\ncomplexity, and complexity under restricted domains) of the aforementioned\nmultiagent variants of knapsack.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 11:00:21 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 19:28:18 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Fluschnik", "Till", ""], ["Skowron", "Piotr", ""], ["Triphaus", "Mervin", ""], ["Wilker", "Kai", ""]]}, {"id": "1711.04604", "submitter": "Eva-Maria Hols", "authors": "Eva-Maria C. Hols and Stefan Kratsch", "title": "Smaller parameters for vertex cover kernelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the topic of polynomial kernels for Vertex Cover relative to\nstructural parameters. Our starting point is a recent paper due to Fomin and\nStr{\\o}mme [WG 2016] who gave a kernel with $\\mathcal{O}(|X|^{12})$ vertices\nwhen $X$ is a vertex set such that each connected component of $G-X$ contains\nat most one cycle, i.e., $X$ is a modulator to a pseudoforest. We strongly\ngeneralize this result by using modulators to $d$-quasi-forests, i.e., graphs\nwhere each connected component has a feedback vertex set of size at most $d$,\nand obtain kernels with $\\mathcal{O}(|X|^{3d+9})$ vertices. Our result relies\non proving that minimal blocking sets in a $d$-quasi-forest have size at most\n$d+2$. This bound is tight and there is a related lower bound of\n$\\mathcal{O}(|X|^{d+2-\\epsilon})$ on the bit size of kernels.\n  In fact, we also get bounds for minimal blocking sets of more general graph\nclasses: For $d$-quasi-bipartite graphs, where each connected component can be\nmade bipartite by deleting at most $d$ vertices, we get the same tight bound of\n$d+2$ vertices. For graphs whose connected components each have a vertex cover\nof cost at most $d$ more than the best fractional vertex cover, which we call\n$d$-quasi-integral, we show that minimal blocking sets have size at most\n$2d+2$, which is also tight. Combined with existing randomized polynomial\nkernelizations this leads to randomized polynomial kernelizations for\nmodulators to $d$-quasi-bipartite and $d$-quasi-integral graphs. There are\nlower bounds of $\\mathcal{O}(|X|^{d+2-\\epsilon})$ and\n$\\mathcal{O}(|X|^{2d+2-\\epsilon})$ for the bit size of such kernels.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 14:48:26 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Hols", "Eva-Maria C.", ""], ["Kratsch", "Stefan", ""]]}, {"id": "1711.04709", "submitter": "Ad\\'an S\\'anchez De Pedro Crespo", "authors": "Ad\\'an S\\'anchez de Pedro Crespo and Luis Iv\\'an Cuende Garc\\'ia", "title": "Stampery Blockchain Timestamping Architecture (BTA) - Version 6", "comments": "21 pages, 16 figures", "journal-ref": null, "doi": "10.13140/RG.2.2.17223.80805", "report-no": null, "categories": "cs.CR cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A method for timestamping, anchoring and certification of a virtually\nunlimited amount of data in one or more blockchains, focusing on scalability\nand cost-effectiveness while ensuring existence, integrity and ownership by\nusing cryptographic proofs that are independently verifiable by anyone in the\nworld without disclosure of the original data and without the intervention of\nthe certifying party.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 17:17:03 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Crespo", "Ad\u00e1n S\u00e1nchez de Pedro", ""], ["Garc\u00eda", "Luis Iv\u00e1n Cuende", ""]]}, {"id": "1711.04712", "submitter": "George Linderman", "authors": "George C. Linderman, Gal Mishne, Yuval Kluger, Stefan Steinerberger", "title": "Randomized Near Neighbor Graphs, Giant Components, and Applications in\n  Data Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If we pick $n$ random points uniformly in $[0,1]^d$ and connect each point to\nits $k-$nearest neighbors, then it is well known that there exists a giant\nconnected component with high probability. We prove that in $[0,1]^d$ it\nsuffices to connect every point to $ c_{d,1} \\log{\\log{n}}$ points chosen\nrandomly among its $ c_{d,2} \\log{n}-$nearest neighbors to ensure a giant\ncomponent of size $n - o(n)$ with high probability. This construction yields a\nmuch sparser random graph with $\\sim n \\log\\log{n}$ instead of $\\sim n \\log{n}$\nedges that has comparable connectivity properties. This result has nontrivial\nimplications for problems in data science where an affinity matrix is\nconstructed: instead of picking the $k-$nearest neighbors, one can often pick\n$k' \\ll k$ random points out of the $k-$nearest neighbors without sacrificing\nefficiency. This can massively simplify and accelerate computation, we\nillustrate this with several numerical examples.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 17:22:00 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Linderman", "George C.", ""], ["Mishne", "Gal", ""], ["Kluger", "Yuval", ""], ["Steinerberger", "Stefan", ""]]}, {"id": "1711.04740", "submitter": "Uri Stemmer", "authors": "Mark Bun, Jelani Nelson, Uri Stemmer", "title": "Heavy Hitters and the Structure of Local Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new locally differentially private algorithm for the heavy\nhitters problem which achieves optimal worst-case error as a function of all\nstandardly considered parameters. Prior work obtained error rates which depend\noptimally on the number of users, the size of the domain, and the privacy\nparameter, but depend sub-optimally on the failure probability.\n  We strengthen existing lower bounds on the error to incorporate the failure\nprobability, and show that our new upper bound is tight with respect to this\nparameter as well. Our lower bound is based on a new understanding of the\nstructure of locally private protocols. We further develop these ideas to\nobtain the following general results beyond heavy hitters.\n  $\\bullet$ Advanced Grouposition: In the local model, group privacy for $k$\nusers degrades proportionally to $\\approx \\sqrt{k}$, instead of linearly in $k$\nas in the central model. Stronger group privacy yields improved max-information\nguarantees, as well as stronger lower bounds (via \"packing arguments\"), over\nthe central model.\n  $\\bullet$ Building on a transformation of Bassily and Smith (STOC 2015), we\ngive a generic transformation from any non-interactive approximate-private\nlocal protocol into a pure-private local protocol. Again in contrast with the\ncentral model, this shows that we cannot obtain more accurate algorithms by\nmoving from pure to approximate local privacy.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 18:12:51 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Bun", "Mark", ""], ["Nelson", "Jelani", ""], ["Stemmer", "Uri", ""]]}, {"id": "1711.04881", "submitter": "Pan Peng", "authors": "Pan Peng, Christian Sohler", "title": "Estimating Graph Parameters from Random Order Streams", "comments": "SODA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new algorithmic technique that allows to transfer some constant\ntime approximation algorithms for general graphs into random order streaming\nalgorithms. We illustrate our technique by proving that in random order streams\nwith probability at least $2/3$,\n  $\\bullet$ the number of connected components of $G$ can be approximated up to\nan additive error of $\\varepsilon n$ using\n$(\\frac{1}{\\varepsilon})^{O(1/\\varepsilon^3)}$ space,\n  $\\bullet$ the weight of a minimum spanning tree of a connected input graph\nwith integer edges weights from $\\{1,\\dots,W\\}$ can be approximated within a\nmultiplicative factor of $1+\\varepsilon$ using\n$\\big(\\frac{1}{\\varepsilon}\\big)^{\\tilde O(W^3/\\varepsilon^3)}$ space,\n  $\\bullet$ the size of a maximum independent set in planar graphs can be\napproximated within a multiplicative factor of $1+\\varepsilon$ using space\n$2^{(1/\\varepsilon)^{(1/\\varepsilon)^{\\log^{O(1)} (1/\\varepsilon)}}}$.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 22:50:09 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Peng", "Pan", ""], ["Sohler", "Christian", ""]]}, {"id": "1711.04882", "submitter": "Tanmay Inamdar", "authors": "Tanmay Inamdar, Kasturi Varadarajan", "title": "On Partial Covering For Geometric Set Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a generalization of the Set Cover problem called the \\emph{Partial\nSet Cover} in the context of geometric set systems. The input to this problem\nis a set system $(X, \\mathcal{S})$, where $X$ is a set of elements and\n$\\mathcal{S}$ is a collection of subsets of $X$, and an integer $k \\le |X|$.\nThe goal is to cover at least $k$ elements of $X$ by using a minimum-weight\ncollection of sets from $\\mathcal{S}$. The main result of this article is an LP\nrounding scheme which shows that the integrality gap of the Partial Set Cover\nLP is at most a constant times that of the Set Cover LP for a certain\nprojection of the set system $(X, \\mathcal{S})$. As a corollary of this result,\nwe get improved approximation guarantees for the Partial Set Cover problem for\na large class of geometric set systems.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 22:50:49 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 17:03:09 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Inamdar", "Tanmay", ""], ["Varadarajan", "Kasturi", ""]]}, {"id": "1711.04889", "submitter": "Salvatore Mandr\\`a", "authors": "Tobias Stollenwerk, Bryan O'Gorman, Davide Venturelli, Salvatore\n  Mandr\\`a, Olga Rodionova, Hok K. Ng, Banavar Sridhar, Eleanor G. Rieffel,\n  Rupak Biswas", "title": "Quantum Annealing Applied to De-Conflicting Optimal Trajectories for Air\n  Traffic Management", "comments": "Paper accepted for publication on: IEEE Transactions on Intelligent\n  Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2019.2891235", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the mapping of a class of simplified air traffic management (ATM)\nproblems (strategic conflict resolution) to quadratic unconstrained boolean\noptimization (QUBO) problems. The mapping is performed through an original\nrepresentation of the conflict-resolution problem in terms of a conflict graph,\nwhere nodes of the graph represent flights and edges represent a potential\nconflict between flights. The representation allows a natural decomposition of\na real world instance related to wind-optimal trajectories over the Atlantic\nocean into smaller subproblems, that can be discretized and are amenable to be\nprogrammed in quantum annealers. In the study, we tested the new programming\ntechniques and we benchmark the hardness of the instances using both classical\nsolvers and the D-Wave 2X and D-Wave 2000Q quantum chip. The preliminary\nresults show that for reasonable modeling choices the most challenging\nsubproblems which are programmable in the current devices are solved to\noptimality with 99% of probability within a second of annealing time.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 23:23:55 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 01:03:12 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Stollenwerk", "Tobias", ""], ["O'Gorman", "Bryan", ""], ["Venturelli", "Davide", ""], ["Mandr\u00e0", "Salvatore", ""], ["Rodionova", "Olga", ""], ["Ng", "Hok K.", ""], ["Sridhar", "Banavar", ""], ["Rieffel", "Eleanor G.", ""], ["Biswas", "Rupak", ""]]}, {"id": "1711.04978", "submitter": "Rahul Vaze", "authors": "Goonwanth Reddy and Rahul Vaze", "title": "Robust Online Speed Scaling With Deadline Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A speed scaling problem is considered, where time is divided into slots, and\njobs with payoff $v$ arrive at the beginning of the slot with associated\ndeadlines $d$. Each job takes one slot to be processed, and multiple jobs can\nbe processed by the server in each slot with energy cost $g(k)$ for processing\n$k$ jobs in one slot. The payoff is accrued by the algorithm only if the job is\nprocessed by its deadline. We consider a robust version of this speed scaling\nproblem, where a job on its arrival reveals its payoff $v$, however, the\ndeadline is hidden to the online algorithm, which could potentially be chosen\nadversarially and known to the optimal offline algorithm. The objective is to\nderive a robust (to deadlines) and optimal online algorithm that achieves the\nbest competitive ratio. We propose an algorithm (called min-LCR) and show that\nit is an optimal online algorithm for any convex energy cost function $g(.)$.\nWe do so without actually evaluating the optimal competitive ratio, and give a\ngeneral proof that works for any convex $g$, which is rather novel. For the\npopular choice of energy cost function $g(k) = k^\\alpha, \\alpha \\ge 2$, we give\nconcrete bounds on the competitive ratio of the algorithm, which ranges between\n$2.618$ and $3$ depending on the value of $\\alpha$. The best known online\nalgorithm for the same problem, but where deadlines are revealed to the online\nalgorithm has competitive ratio of $2$ and a lower bound of $\\sqrt{2}$. Thus,\nimportantly, lack of deadline knowledge does not make the problem degenerate,\nand the effect of deadline information on the optimal competitive ratio is\nlimited.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 07:00:07 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Reddy", "Goonwanth", ""], ["Vaze", "Rahul", ""]]}, {"id": "1711.04979", "submitter": "Jun Song", "authors": "Chenchao Zhao and Jun S. Song", "title": "Quantum transport senses community structure in networks", "comments": null, "journal-ref": "Phys. Rev. E 98, 022301 (2018)", "doi": "10.1103/PhysRevE.98.022301", "report-no": null, "categories": "quant-ph cond-mat.other cs.DS q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum time evolution exhibits rich physics, attributable to the interplay\nbetween the density and phase of a wave function. However, unlike classical\nheat diffusion, the wave nature of quantum mechanics has not yet been\nextensively explored in modern data analysis. We propose that the Laplace\ntransform of quantum transport (QT) can be used to construct an ensemble of\nmaps from a given complex network to a circle $S^1$, such that closely-related\nnodes on the network are grouped into sharply concentrated clusters on $S^1$.\nThe resulting QT clustering (QTC) algorithm is as powerful as the\nstate-of-the-art spectral clustering in discerning complex geometric patterns\nand more robust when clusters show strong density variations or heterogeneity\nin size. The observed phenomenon of QTC can be interpreted as a collective\nbehavior of the microscopic nodes that evolve as macroscopic cluster orbitals\nin an effective tight-binding model recapitulating the network. Python source\ncode implementing the algorithm and examples are available at\nhttps://github.com/jssong-lab/QTC.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 07:03:05 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 05:52:07 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Zhao", "Chenchao", ""], ["Song", "Jun S.", ""]]}, {"id": "1711.05135", "submitter": "Zhuo Feng", "authors": "Zhuo Feng", "title": "Similarity-Aware Spectral Sparsification by Edge Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, spectral graph sparsification techniques that can compute\nultra-sparse graph proxies have been extensively studied for accelerating\nvarious numerical and graph-related applications. Prior nearly-linear-time\nspectral sparsification methods first extract low-stretch spanning tree from\nthe original graph to form the backbone of the sparsifier, and then recover\nsmall portions of spectrally-critical off-tree edges to the spanning tree to\nsignificantly improve the approximation quality. However, it is not clear how\nmany off-tree edges should be recovered for achieving a desired spectral\nsimilarity level within the sparsifier. Motivated by recent graph signal\nprocessing techniques, this paper proposes a similarity-aware spectral graph\nsparsification framework that leverages efficient spectral off-tree edge\nembedding and filtering schemes to construct spectral sparsifiers with\nguaranteed spectral similarity (relative condition number) level. An iterative\ngraph densification scheme is introduced to facilitate efficient and effective\nfiltering of off-tree edges for highly ill-conditioned problems. The proposed\nmethod has been validated using various kinds of graphs obtained from public\ndomain sparse matrix collections relevant to VLSI CAD, finite element analysis,\nas well as social and data networks frequently studied in many machine learning\nand data mining applications.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 14:58:41 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 06:26:55 GMT"}, {"version": "v3", "created": "Sat, 7 Apr 2018 23:37:42 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Feng", "Zhuo", ""]]}, {"id": "1711.05144", "submitter": "Seth  Neel", "authors": "Michael Kearns, Seth Neel, Aaron Roth, Zhiwei Steven Wu", "title": "Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup\n  Fairness", "comments": "Added new experimental results and a slightly modified fairness\n  definition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most prevalent notions of fairness in machine learning are statistical\ndefinitions: they fix a small collection of pre-defined groups, and then ask\nfor parity of some statistic of the classifier across these groups. Constraints\nof this form are susceptible to intentional or inadvertent \"fairness\ngerrymandering\", in which a classifier appears to be fair on each individual\ngroup, but badly violates the fairness constraint on one or more structured\nsubgroups defined over the protected attributes. We propose instead to demand\nstatistical notions of fairness across exponentially (or infinitely) many\nsubgroups, defined by a structured class of functions over the protected\nattributes. This interpolates between statistical definitions of fairness and\nrecently proposed individual notions of fairness, but raises several\ncomputational challenges. It is no longer clear how to audit a fixed classifier\nto see if it satisfies such a strong definition of fairness. We prove that the\ncomputational problem of auditing subgroup fairness for both equality of false\npositive rates and statistical parity is equivalent to the problem of weak\nagnostic learning, which means it is computationally hard in the worst case,\neven for simple structured subclasses.\n  We then derive two algorithms that provably converge to the best fair\nclassifier, given access to oracles which can solve the agnostic learning\nproblem. The algorithms are based on a formulation of subgroup fairness as a\ntwo-player zero-sum game between a Learner and an Auditor. Our first algorithm\nprovably converges in a polynomial number of steps. Our second algorithm enjoys\nonly provably asymptotic convergence, but has the merit of simplicity and\nfaster per-step computation. We implement the simpler algorithm using linear\nregression as a heuristic oracle, and show that we can effectively both audit\nand learn fair classifiers on real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 15:34:27 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 13:55:17 GMT"}, {"version": "v3", "created": "Mon, 8 Jan 2018 01:15:28 GMT"}, {"version": "v4", "created": "Thu, 12 Apr 2018 21:15:28 GMT"}, {"version": "v5", "created": "Mon, 3 Dec 2018 18:18:34 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Kearns", "Michael", ""], ["Neel", "Seth", ""], ["Roth", "Aaron", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1711.05157", "submitter": "Lars Jaffke", "authors": "Lars Jaffke and O-joung Kwon and Jan Arne Telle", "title": "A note on the complexity of Feedback Vertex Set parameterized by\n  mim-width", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We complement the recent algorithmic result that Feedback Vertex Set is\nXP-time solvable parameterized by the mim-width of a given branch decomposition\nof the input graph [3] by showing that the problem is W[1]-hard in this\nparameterization. The hardness holds even for linear mim-width, as well as for\nH-graphs, where the parameter is the number of edges in H. To obtain this\nresult, we adapt a reduction due to Fomin, Golovach and Raymond [2], following\nthe same line of reasoning but adding a new gadget.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 15:56:37 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Jaffke", "Lars", ""], ["Kwon", "O-joung", ""], ["Telle", "Jan Arne", ""]]}, {"id": "1711.05295", "submitter": "Kianna Wan", "authors": "Michael Jarret and Kianna Wan", "title": "Improved quantum backtracking algorithms using effective resistance\n  estimates", "comments": "27 pages; typos fixed", "journal-ref": "Phys. Rev. A 97, 022337 (2018)", "doi": "10.1103/PhysRevA.97.022337", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate quantum backtracking algorithms of a type previously\nintroduced by Montanaro (arXiv:1509.02374). These algorithms explore trees of\nunknown structure, and in certain cases exponentially outperform classical\nprocedures (such as DPLL). Some of the previous work focused on obtaining a\nquantum advantage for trees in which a unique marked vertex is promised to\nexist. We remove this restriction and re-characterise the problem in terms of\nthe effective resistance of the search space. To this end, we present a\ngeneralisation of one of Montanaro's algorithms to trees containing $k \\geq 1$\nmarked vertices, where $k$ is not necessarily known \\textit{a priori}.\n  Our approach involves using amplitude estimation to determine a near-optimal\nweighting of a diffusion operator, which can then be applied to prepare a\nsuperposition state that has support only on marked vertices and ancestors\nthereof. By repeatedly sampling this state and updating the input vertex, a\nmarked vertex is reached in a logarithmic number of steps. The algorithm\nthereby achieves the conjectured bound of\n$\\widetilde{\\mathcal{O}}(\\sqrt{TR_{\\mathrm{max}}})$ for finding a single marked\nvertex and $\\widetilde{\\mathcal{O}}\\left(k\\sqrt{T R_{\\mathrm{max}}}\\right)$ for\nfinding all $k$ marked vertices, where $T$ is an upper bound on the tree size\nand $R_{\\mathrm{max}}$ is the maximum effective resistance encountered by the\nalgorithm. This constitutes a speedup over Montanaro's original procedure in\nboth the case of finding one and finding multiple marked vertices in an\narbitrary tree. If there are no marked vertices, the effective resistance\nbecomes infinite, and we recover the scaling of Montanaro's existence\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 19:43:44 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 04:20:07 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Jarret", "Michael", ""], ["Wan", "Kianna", ""]]}, {"id": "1711.05469", "submitter": "Yannic Maus", "authors": "Mohsen Ghaffari, Fabian Kuhn, Yannic Maus, Jara Uitto", "title": "Deterministic Distributed Edge-Coloring with Fewer Colors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic distributed algorithm, in the LOCAL model, that\ncomputes a $(1+o(1))\\Delta$-edge-coloring in polylogarithmic-time, so long as\nthe maximum degree $\\Delta=\\tilde{\\Omega}(\\log n)$. For smaller $\\Delta$, we\ngive a polylogarithmic-time $3\\Delta/2$-edge-coloring. These are the first\ndeterministic algorithms to go below the natural barrier of $2\\Delta-1$ colors,\nand they improve significantly on the recent polylogarithmic-time\n$(2\\Delta-1)(1+o(1))$-edge-coloring of Ghaffari and Su [SODA'17] and the\n$(2\\Delta-1)$-edge-coloring of Fischer, Ghaffari, and Kuhn [FOCS'17],\npositively answering the main open question of the latter. The key technical\ningredient of our algorithm is a simple and novel gradual packing of\njudiciously chosen near-maximum matchings, each of which becomes one of the\ncolor classes.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 09:22:14 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""], ["Uitto", "Jara", ""]]}, {"id": "1711.05667", "submitter": "Sophie Huiberts", "authors": "Daniel Dadush and Sophie Huiberts", "title": "A Friendly Smoothed Analysis of the Simplex Method", "comments": "Revision 4: an improved algorithm to strengthen the complexity bound\n  in the large-noise regime", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining the excellent practical performance of the simplex method for\nlinear programming has been a major topic of research for over 50 years. One of\nthe most successful frameworks for understanding the simplex method was given\nby Spielman and Teng (JACM `04), who developed the notion of smoothed analysis.\nStarting from an arbitrary linear program with $d$ variables and $n$\nconstraints, Spielman and Teng analyzed the expected runtime over random\nperturbations of the LP (smoothed LP), where variance $\\sigma^2$ Gaussian noise\nis added to the LP data. In particular, they gave a two-stage shadow vertex\nsimplex algorithm which uses an expected $\\widetilde{O}(d^{55} n^{86}\n\\sigma^{-30})$ number of simplex pivots to solve the smoothed LP. Their\nanalysis and runtime was substantially improved by Deshpande and Spielman (FOCS\n`05) and later Vershynin (SICOMP `09). The fastest current algorithm, due to\nVershynin, solves the smoothed LP using an expected $O(d^3 \\sigma^{-4} \\log^3 n\n+ d^9\\log^7 n)$ number of pivots, improving the dependence on $n$ from\npolynomial to poly-logarithmic.\n  While the original proof of Spielman and Teng has now been substantially\nsimplified, the resulting analyses are still quite long and complex and the\nparameter dependencies far from optimal. In this work, we make substantial\nprogress on this front, providing an improved and simpler analysis of shadow\nsimplex methods, where our algorithm requires an expected \\[ O(d^2 \\sqrt{\\log\nn} \\sigma^{-2} + d^3 \\log^{3/2} n) \\] number of simplex pivots. We obtain our\nresults via an improved \\emph{shadow bound}, key to earlier analyses as well,\ncombined with improvements on algorithmic techniques of Vershynin. As an added\nbonus, our analysis is completely modular, allowing us to obtain non-trivial\nbounds for perturbations beyond Gaussians, such as Laplace perturbations.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 17:04:52 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 10:10:02 GMT"}, {"version": "v3", "created": "Sun, 22 Jul 2018 11:17:22 GMT"}, {"version": "v4", "created": "Tue, 11 Jun 2019 17:42:20 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Dadush", "Daniel", ""], ["Huiberts", "Sophie", ""]]}, {"id": "1711.05764", "submitter": "Hossein Esfandiari", "authors": "Hossein Esfandiari, Nitish Korula, Vahab Mirrokni", "title": "Online Allocation with Traffic Spikes: Mixing Adversarial and Stochastic\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by Internet advertising applications, online allocation problems\nhave been studied extensively in various adversarial and stochastic models.\nWhile the adversarial arrival models are too pessimistic, many of the\nstochastic (such as i.i.d or random-order) arrival models do not realistically\ncapture uncertainty in predictions. A significant cause for such uncertainty is\nthe presence of unpredictable traffic spikes, often due to breaking news or\nsimilar events. To address this issue, a simultaneous approximation framework\nhas been proposed to develop algorithms that work well both in the adversarial\nand stochastic models; however, this framework does not enable algorithms that\nmake good use of partially accurate forecasts when making online decisions. In\nthis paper, we propose a robust online stochastic model that captures the\nnature of traffic spikes in online advertising. In our model, in addition to\nthe stochastic input for which we have good forecasting, an unknown number of\nimpressions arrive that are adversarially chosen. We design algorithms that\ncombine a stochastic algorithm with an online algorithm that adaptively reacts\nto inaccurate predictions. We provide provable bounds for our new algorithms in\nthis framework. We accompany our positive results with a set of hardness\nresults showing that our algorithms are not far from optimal in this framework.\nAs a byproduct of our results, we also present improved online algorithms for a\nslight variant of the simultaneous approximation framework.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 19:23:23 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Esfandiari", "Hossein", ""], ["Korula", "Nitish", ""], ["Mirrokni", "Vahab", ""]]}, {"id": "1711.05812", "submitter": "Yangyang Xu", "authors": "Yangyang Xu", "title": "Iteration complexity of inexact augmented Lagrangian methods for\n  constrained convex programming", "comments": "Iteration complexity results are significantly improved. Nonergodic\n  convergence rate result is established for the inexact ALM, and numerical\n  results are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented Lagrangian method (ALM) has been popularly used for solving\nconstrained optimization problems. Practically, subproblems for updating primal\nvariables in the framework of ALM usually can only be solved inexactly. The\nconvergence and local convergence speed of ALM have been extensively studied.\nHowever, the global convergence rate of inexact ALM is still open for problems\nwith nonlinear inequality constraints. In this paper, we work on general convex\nprograms with both equality and inequality constraints. For these problems, we\nestablish the global convergence rate of inexact ALM and estimate its iteration\ncomplexity in terms of the number of gradient evaluations to produce a solution\nwith a specified accuracy.\n  We first establish an ergodic convergence rate result of inexact ALM that\nuses constant penalty parameters or geometrically increasing penalty\nparameters. Based on the convergence rate result, we apply Nesterov's optimal\nfirst-order method on each primal subproblem and estimate the iteration\ncomplexity of the inexact ALM. We show that if the objective is convex, then\n$O(\\varepsilon^{-1})$ gradient evaluations are sufficient to guarantee an\n$\\varepsilon$-optimal solution in terms of both primal objective and\nfeasibility violation. If the objective is strongly convex, the result can be\nimproved to $O(\\varepsilon^{-\\frac{1}{2}}|\\log\\varepsilon|)$. Finally, by\nrelating to the inexact proximal point algorithm, we establish a nonergodic\nconvergence rate result of inexact ALM that uses geometrically increasing\npenalty parameters. We show that the nonergodic iteration complexity result is\nin the same order as that for the ergodic result. Numerical experiments on\nquadratically constrained quadratic programming are conducted to compare the\nperformance of the inexact ALM with different settings.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 21:20:54 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 02:56:21 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Xu", "Yangyang", ""]]}, {"id": "1711.05839", "submitter": "Marcus Ludwig", "authors": "W. Timothy J. White, Marcus Ludwig and Sebastian B\\\"ocker", "title": "Exact and heuristic algorithms for Cograph Editing", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dynamic programming algorithm for optimally solving the Cograph\nEditing problem on an $n$-vertex graph that runs in $O(3^n n)$ time and uses\n$O(2^n)$ space. In this problem, we are given a graph $G = (V, E)$ and the task\nis to find a smallest possible set $F \\subseteq V \\times V$ of vertex pairs\nsuch that $(V, E \\bigtriangleup F)$ is a cograph (or $P_4$-free graph), where\n$\\bigtriangleup$ represents the symmetric difference operator. We also describe\na technique for speeding up the performance of the algorithm in practice.\nAdditionally, we present a heuristic for solving the Cograph Editing problem\nwhich produces good results on small to medium datasets. In application it is\nmuch more important to find the ground truth, not some optimal solution. For\nthe first time, we evaluate whether the cograph property is strict enough to\nrecover the true graph from data to which noise has been added.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 22:56:24 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 11:08:40 GMT"}, {"version": "v3", "created": "Wed, 10 Jan 2018 11:02:20 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["White", "W. Timothy J.", ""], ["Ludwig", "Marcus", ""], ["B\u00f6cker", "Sebastian", ""]]}, {"id": "1711.06319", "submitter": "Hao Chen", "authors": "Hao Chen", "title": "Optimizing relinearization in circuits for homomorphic encryption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully homomorphic encryption (FHE) allows an untrusted party to evaluate\narithmetic cir- cuits, i.e., perform additions and multiplications on encrypted\ndata, without having the decryp- tion key. One of the most efficient class of\nFHE schemes include BGV and FV schemes, which are based on the hardness of the\nRLWE problem. They share some common features: ciphertext sizes grow after each\nhomomorphic multiplication; multiplication is much more costly than addition,\nand the cost of homomorphic multiplication scales linearly with the input\nciphertext sizes. Furthermore, there is a special relinearization operation\nthat reduce the size of a ciphertext, and the cost of relinearization is on the\nsame order of magnitude as homomorpic multiplication. This motivates us to\ndefine a discrete optimization problem, which is to decide where (and how much)\nin a given circuit to relinearize, in order to minimize the total computational\ncost. In this paper, we formally define the relinearize problem. We prove that\nthe problem is NP-hard. In addition, in the special case where each vertex has\nat most one outgoing edge, we give a polynomial-time algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 21:30:35 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Chen", "Hao", ""]]}, {"id": "1711.06347", "submitter": "Daniel Karapetyan Dr", "authors": "Daniel Karapetyan and Boris Goldengorin", "title": "Conditional Markov Chain Search for the Simple Plant Location Problem\n  improves upper bounds on twelve K\\\"orkel-Ghosh instances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a family of hard benchmark instances for the Simple Plant Location\nProblem (also known as the Uncapacitated Facility Location Problem). The recent\nattempt by Fischetti et al. to tackle the K\\\"orkel-Ghosh instances resulted in\nseven new optimal solutions and 22 improved upper bounds. We use automated\ngeneration of heuristics to obtain a new algorithm for the Simple Plant\nLocation Problem. In our experiments, our new algorithm matched all the\nprevious best known and optimal solutions, and further improved 12 upper\nbounds, all within shorter time budgets compared to the previous efforts.\n  Our algorithm design process is split into two phases: (i) development of\nalgorithmic components such as local search procedures and mutation operators,\nand (ii) composition of a metaheuristic from the available components. Phase\n(i) requires human expertise and often can be completed by implementing several\nsimple domain-specific routines known from the literature. Phase (ii) is\nentirely automated by employing the Conditional Markov Chain Search (CMCS)\nframework. In CMCS, a metaheuristic is flexibly defined by a set of parameters,\ncalled configuration. Then the process of composition of a metaheuristic from\nthe algorithmic components is reduced to an optimisation problem seeking the\nbest performing CMCS configuration.\n  We discuss the problem of comparing configurations, and propose a new\nefficient technique to select the best performing configuration from a large\nset. To employ this method, we restrict the original CMCS to a simple\ndeterministic case that leaves us with a finite and manageable number of\nmeaningful configurations.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 23:00:42 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Karapetyan", "Daniel", ""], ["Goldengorin", "Boris", ""]]}, {"id": "1711.06397", "submitter": "Yixin Cao", "authors": "Yixin Cao, Jianer Chen, Jia-Hao Fan", "title": "An $O^*(1.84^k)$ Parameterized Algorithm for the Multiterminal Cut\n  Problem", "comments": "To fulfill the request of the European Research Council (ERC)", "journal-ref": "Information Processing Letters, 114(4): 167--173 (2014)", "doi": "10.1016/j.ipl.2013.12.001", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the \\emph{multiterminal cut} problem, which, given an $n$-vertex\ngraph whose edges are integer-weighted and a set of terminals, asks for a\npartition of the vertex set such that each terminal is in a distinct part, and\nthe total weight of crossing edges is at most $k$. Our weapons shall be two\nclassical results known for decades: \\emph{maximum volume minimum ($s,t$)-cuts}\nby [Ford and Fulkerson, \\emph{Flows in Networks}, 1962] and \\emph{isolating\ncuts} by [Dahlhaus et al., \\emph{SIAM J. Comp.} 23(4):864-894, 1994]. We\nsharpen these old weapons with the help of submodular functions, and apply them\nto this problem, which enable us to design a more elaborated branching scheme\non deciding whether a non-terminal vertex is with a terminal or not. This\nbounded search tree algorithm can be shown to run in $1.84^k\\cdot n^{O(1)}$\ntime, thereby breaking the $2^k\\cdot n^{O(1)}$ barrier. As a by-product, it\ngives a $1.36^k\\cdot n^{O(1)}$ time algorithm for $3$-terminal cut. The\npreprocessing applied on non-terminal vertices might be of use for study of\nthis problem from other aspects.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 04:26:10 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Cao", "Yixin", ""], ["Chen", "Jianer", ""], ["Fan", "Jia-Hao", ""]]}, {"id": "1711.06428", "submitter": "Rajan Udwani", "authors": "Rajan Udwani", "title": "Multi-Objective Maximization of Monotone Submodular Functions with\n  Cardinality Constraint", "comments": "Most recent version fixes an error in the journal as well as\n  conference versions (INFORMS Journal on Optimization, Neurips 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of multi-objective maximization of monotone\nsubmodular functions subject to cardinality constraint, often formulated as\n$\\max_{|A|=k}\\min_{i\\in\\{1,\\dots,m\\}}f_i(A)$. While it is widely known that\ngreedy methods work well for a single objective, the problem becomes much\nharder with multiple objectives. In fact, Krause et al.\\ (2008) showed that\nwhen the number of objectives $m$ grows as the cardinality $k$ i.e.,\n$m=\\Omega(k)$, the problem is inapproximable (unless $P=NP$). On the other\nhand, when $m$ is constant Chekuri et al.\\ (2010) showed a randomized\n$(1-1/e)-\\epsilon$ approximation with runtime (number of queries to function\noracle) $n^{m/\\epsilon^3}$. %In fact, the result of Chekuri et al.\\ (2010) is\nfor the far more general case of matroid constant.\n  We focus on finding a fast and practical algorithm that has (asymptotic)\napproximation guarantees even when $m$ is super constant. We first modify the\nalgorithm of Chekuri et al.\\ (2010) to achieve a $(1-1/e)$ approximation for\n$m=o(\\frac{k}{\\log^3 k})$. This demonstrates a steep transition from constant\nfactor approximability to inapproximability around $m=\\Omega(k)$. Then using\nMultiplicative-Weight-Updates (MWU), we find a much faster\n$\\tilde{O}(n/\\delta^3)$ time asymptotic $(1-1/e)^2-\\delta$ approximation. While\nthe above results are all randomized, we also give a simple deterministic\n$(1-1/e)-\\epsilon$ approximation with runtime $kn^{m/\\epsilon^4}$. Finally, we\nrun synthetic experiments using Kronecker graphs and find that our MWU inspired\nheuristic outperforms existing heuristics.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 06:44:24 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 16:08:39 GMT"}, {"version": "v3", "created": "Mon, 24 Aug 2020 15:43:01 GMT"}, {"version": "v4", "created": "Sat, 1 May 2021 18:12:22 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Udwani", "Rajan", ""]]}, {"id": "1711.06455", "submitter": "Aaron Schild", "authors": "Aaron Schild", "title": "An almost-linear time algorithm for uniform random spanning tree\n  generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an $m^{1+o(1)}\\beta^{o(1)}$-time algorithm for generating a uniformly\nrandom spanning tree in an undirected, weighted graph with max-to-min weight\nratio $\\beta$. We also give an $m^{1+o(1)}\\epsilon^{-o(1)}$-time algorithm for\ngenerating a random spanning tree with total variation distance $\\epsilon$ from\nthe true uniform distribution. Our second algorithm's runtime does not depend\non the edge weights. Our $m^{1+o(1)}\\beta^{o(1)}$-time algorithm is the first\nalmost-linear time algorithm for the problem --- even on unweighted graphs ---\nand is the first subquadratic time algorithm for sparse weighted graphs.\n  Our algorithms improve on the random walk-based approach given in\nKelner-M\\k{a}dry and M\\k{a}dry-Straszak-Tarnawski. We introduce a new way of\nusing Laplacian solvers to shortcut a random walk. In order to fully exploit\nthis shortcutting technique, we prove a number of new facts about electrical\nflows in graphs. These facts seek to better understand sets of vertices that\nare well-separated in the effective resistance metric in connection with Schur\ncomplements, concentration phenomena for electrical flows after conditioning on\npartial samples of a random spanning tree, and more.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 08:30:28 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Schild", "Aaron", ""]]}, {"id": "1711.06530", "submitter": "Vedat Levi Alev", "authors": "Vedat Levi Alev, Nima Anari, Lap Chi Lau, Shayan Oveis Gharan", "title": "Graph Clustering using Effective Resistance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $ \\def\\vecc#1{\\boldsymbol{#1}} $We design a polynomial time algorithm that\nfor any weighted undirected graph $G = (V, E,\\vecc w)$ and sufficiently large\n$\\delta > 1$, partitions $V$ into subsets $V_1, \\ldots, V_h$ for some $h\\geq\n1$, such that\n  $\\bullet$ at most $\\delta^{-1}$ fraction of the weights are between clusters,\ni.e. \\[ w(E - \\cup_{i = 1}^h E(V_i)) \\lesssim \\frac{w(E)}{\\delta};\\]\n  $\\bullet$ the effective resistance diameter of each of the induced subgraphs\n$G[V_i]$ is at most $\\delta^3$ times the average weighted degree, i.e. \\[\n\\max_{u, v \\in V_i} \\mathsf{Reff}_{G[V_i]}(u, v) \\lesssim \\delta^3 \\cdot\n\\frac{|V|}{w(E)} \\quad \\text{ for all } i=1, \\ldots, h.\\]\n  In particular, it is possible to remove one percent of weight of edges of any\ngiven graph such that each of the resulting connected components has effective\nresistance diameter at most the inverse of the average weighted degree.\n  Our proof is based on a new connection between effective resistance and low\nconductance sets. We show that if the effective resistance between two vertices\n$u$ and $v$ is large, then there must be a low conductance cut separating $u$\nfrom $v$. This implies that very mildly expanding graphs have constant\neffective resistance diameter. We believe that this connection could be of\nindependent interest in algorithm design.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 13:39:29 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Alev", "Vedat Levi", ""], ["Anari", "Nima", ""], ["Lau", "Lap Chi", ""], ["Gharan", "Shayan Oveis", ""]]}, {"id": "1711.06625", "submitter": "David Wajc", "authors": "Moab Arar, Shiri Chechik, Sarel Cohen, Cliff Stein, David Wajc", "title": "Dynamic Matching: Reducing Integral Algorithms to Approximately-Maximal\n  Fractional Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple randomized reduction from fully-dynamic integral matching\nalgorithms to fully-dynamic \"approximately-maximal\" fractional matching\nalgorithms. Applying this reduction to the recent fractional matching algorithm\nof Bhattacharya, Henzinger, and Nanongkai (SODA 2017), we obtain a novel result\nfor the integral problem. Specifically, our main result is a randomized\nfully-dynamic $(2+\\epsilon)$-approximate integral matching algorithm with small\npolylog worst-case update time. For the $(2+\\epsilon)$-approximation regime\nonly a \\emph{fractional} fully-dynamic $(2+\\epsilon)$-matching algorithm with\nworst-case polylog update time was previously known, due to Bhattacharya et\nal.~(SODA 2017). Our algorithm is the first algorithm that maintains\napproximate matchings with worst-case update time better than polynomial, for\nany constant approximation ratio. As a consequence, we also obtain the first\nconstant-approximate worst-case polylogarithmic update time maximum weight\nmatching algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 16:58:48 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 21:21:32 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Arar", "Moab", ""], ["Chechik", "Shiri", ""], ["Cohen", "Sarel", ""], ["Stein", "Cliff", ""], ["Wajc", "David", ""]]}, {"id": "1711.06673", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Yuanzhi Li", "title": "Neon2: Finding Local Minima via First-Order Oracles", "comments": "version 2 and 3 improve writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a reduction for non-convex optimization that can (1) turn an\nstationary-point finding algorithm into an local-minimum finding one, and (2)\nreplace the Hessian-vector product computations with only gradient\ncomputations. It works both in the stochastic and the deterministic settings,\nwithout hurting the algorithm's performance.\n  As applications, our reduction turns Natasha2 into a first-order method\nwithout hurting its performance. It also converts SGD, GD, SCSG, and SVRG into\nalgorithms finding approximate local minima, outperforming some best known\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 18:59:01 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 06:34:18 GMT"}, {"version": "v3", "created": "Fri, 20 Apr 2018 07:58:25 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1711.06883", "submitter": "Shay Solomon", "authors": "Moses Charikar and Shay Solomon", "title": "Fully Dynamic Almost-Maximal Matching: Breaking the Polynomial Barrier\n  for Worst-Case Time Bounds", "comments": "An improved presentation for a manuscript that started to circulate\n  in November 2016. Section 8 of the previous version is omitted from the\n  current version; other than minor changes triggered by the omission of this\n  section, the two versions are identical", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant research efforts, the state-of-the-art algorithm for\nmaintaining an approximate matching in fully dynamic graphs has a polynomial\n{worst-case} update time, even for very poor approximation guarantees. In a\nrecent breakthrough, Bhattacharya, Henzinger and Nanongkai showed how to\nmaintain a constant approximation to the minimum vertex cover, and thus also a\nconstant-factor estimate of the maximum matching size, with polylogarithmic\nworst-case update time. Later (in SODA'17 Proc.) they improved the\napproximation factor all the way to $2+\\epsilon$. Nevertheless, the\nlongstanding fundamental problem of {maintaining} an approximate matching with\nsub-polynomial worst-case time bounds remained open.\n  We present a randomized algorithm for maintaining an {almost-maximal}\nmatching in fully dynamic graphs with polylogarithmic worst-case update time.\nSuch a matching provides $(2+\\epsilon)$-approximations for both the maximum\nmatching and the minimum vertex cover, for any $\\epsilon > 0$. Our result was\ndone independently of the $(2+\\epsilon)$-approximation result of Bhattacharya\net al., so it provides the first $(2+\\epsilon)$-approximation for minimum\nvertex cover (together with Bhattacharya et al.'s result) and the first\n$(2+\\epsilon)$-approximation for maximum (integral) matching.\n  The polylogarithmic worst-case update time of our algorithm holds\ndeterministically, while the almost-maximality guarantee holds with high\nprobability. This result not only settles the aforementioned problem on dynamic\nmatchings, but also provides essentially the best possible approximation\nguarantee for dynamic vertex cover (assuming the unique games conjecture).\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 16:14:22 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 17:03:50 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Charikar", "Moses", ""], ["Solomon", "Shay", ""]]}, {"id": "1711.06920", "submitter": "Orr Fischer", "authors": "Orr Fischer, Tzlil Gonen, Rotem Oshman", "title": "Superlinear Lower Bounds for Distributed Subgraph Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the distributed subgraph-freeness problem, we are given a graph $H$, and\nasked to determine whether the network graph contains $H$ as a subgraph or not.\nSubgraph-freeness is an extremely local problem: if the network had no\nbandwidth constraints, we could detect any subgraph $H$ in $|H|$ rounds, by\nhaving each node of the network learn its entire $|H|$-neighborhood. However,\nwhen bandwidth is limited, the problem becomes harder.\n  Upper and lower bounds in the presence of congestion have been established\nfor several classes of subgraphs, including cycles, trees, and more complicated\nsubgraphs. All bounds shown so far have been linear or sublinear. We show that\nthe subgraph-freeness problem is not, in general, solvable in linear time: for\nany $k \\geq 2$, there exists a subgraph $H_k$ such that $H_k$-freeness requires\n$\\Omega( n^{2-1/k} / (Bk) )$ rounds to solve. Here $B$ is the bandwidth of each\ncommunication link. The lower bound holds even for diameter-3 subgraphs and\ndiameter-3 network graphs. In particular, taking $k = \\Theta(\\log n)$, we\nobtain a lower bound of $\\Omega(n^2 / (B \\log n))$.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 19:14:28 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Fischer", "Orr", ""], ["Gonen", "Tzlil", ""], ["Oshman", "Rotem", ""]]}, {"id": "1711.07112", "submitter": "Ehsan Kazemi", "authors": "Ehsan Kazemi and Morteza Zadimoghaddam and Amin Karbasi", "title": "Deletion-Robust Submodular Maximization at Scale", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we efficiently extract useful information from a large user-generated\ndataset while protecting the privacy of the users and/or ensuring fairness in\nrepresentation. We cast this problem as an instance of a deletion-robust\nsubmodular maximization where part of the data may be deleted due to privacy\nconcerns or fairness criteria. We propose the first memory-efficient\ncentralized, streaming, and distributed methods with constant-factor\napproximation guarantees against any number of adversarial deletions. We\nextensively evaluate the performance of our algorithms against prior\nstate-of-the-art on real-world applications, including (i) Uber-pick up\nlocations with location privacy constraints; (ii) feature selection with\nfairness constraints for income prediction and crime rate prediction; and (iii)\nrobust to deletion summarization of census data, consisting of 2,458,285\nfeature vectors.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 01:05:17 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 02:20:36 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Kazemi", "Ehsan", ""], ["Zadimoghaddam", "Morteza", ""], ["Karbasi", "Amin", ""]]}, {"id": "1711.07211", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane and Alistair Stewart", "title": "List-Decodable Robust Mean Estimation and Learning Mixtures of Spherical\n  Gaussians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of list-decodable Gaussian mean estimation and the\nrelated problem of learning mixtures of separated spherical Gaussians. We\ndevelop a set of techniques that yield new efficient algorithms with\nsignificantly improved guarantees for these problems.\n  {\\bf List-Decodable Mean Estimation.} Fix any $d \\in \\mathbb{Z}_+$ and $0<\n\\alpha <1/2$. We design an algorithm with runtime $O\n(\\mathrm{poly}(n/\\alpha)^{d})$ that outputs a list of $O(1/\\alpha)$ many\ncandidate vectors such that with high probability one of the candidates is\nwithin $\\ell_2$-distance $O(\\alpha^{-1/(2d)})$ from the true mean. The only\nprevious algorithm for this problem achieved error $\\tilde O(\\alpha^{-1/2})$\nunder second moment conditions. For $d = O(1/\\epsilon)$, our algorithm runs in\npolynomial time and achieves error $O(\\alpha^{\\epsilon})$. We also give a\nStatistical Query lower bound suggesting that the complexity of our algorithm\nis qualitatively close to best possible.\n  {\\bf Learning Mixtures of Spherical Gaussians.} We give a learning algorithm\nfor mixtures of spherical Gaussians that succeeds under significantly weaker\nseparation assumptions compared to prior work. For the prototypical case of a\nuniform mixture of $k$ identity covariance Gaussians we obtain: For any\n$\\epsilon>0$, if the pairwise separation between the means is at least\n$\\Omega(k^{\\epsilon}+\\sqrt{\\log(1/\\delta)})$, our algorithm learns the unknown\nparameters within accuracy $\\delta$ with sample complexity and running time\n$\\mathrm{poly} (n, 1/\\delta, (k/\\epsilon)^{1/\\epsilon})$. The previously best\nknown polynomial time algorithm required separation at least $k^{1/4}\n\\mathrm{polylog}(k/\\delta)$.\n  Our main technical contribution is a new technique, using degree-$d$\nmultivariate polynomials, to remove outliers from high-dimensional datasets\nwhere the majority of the points are corrupted.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 09:07:08 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1711.07214", "submitter": "Chao Qian", "authors": "Chao Qian, Yang Yu, Ke Tang, Xin Yao, Zhi-Hua Zhou", "title": "Maximizing Non-monotone/Non-submodular Functions by Multi-objective\n  Evolutionary Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms (EAs) are a kind of nature-inspired general-purpose\noptimization algorithm, and have shown empirically good performance in solving\nvarious real-word optimization problems. However, due to the highly randomized\nand complex behavior, the theoretical analysis of EAs is difficult and is an\nongoing challenge, which has attracted a lot of research attentions. During the\nlast two decades, promising results on the running time analysis (one essential\ntheoretical aspect) of EAs have been obtained, while most of them focused on\nisolated combinatorial optimization problems, which do not reflect the\ngeneral-purpose nature of EAs. To provide a general theoretical explanation of\nthe behavior of EAs, it is desirable to study the performance of EAs on a\ngeneral class of combinatorial optimization problems. To the best of our\nknowledge, this direction has been rarely touched and the only known result is\nthe provably good approximation guarantees of EAs for the problem class of\nmaximizing monotone submodular set functions with matroid constraints, which\nincludes many NP-hard combinatorial optimization problems. The aim of this work\nis to contribute to this line of research. As many combinatorial optimization\nproblems also involve non-monotone or non-submodular objective functions, we\nconsider these two general problem classes, maximizing non-monotone submodular\nfunctions without constraints and maximizing monotone non-submodular functions\nwith a size constraint. We prove that a simple multi-objective EA called GSEMO\ncan generally achieve good approximation guarantees in polynomial expected\nrunning time.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 09:21:19 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Qian", "Chao", ""], ["Yu", "Yang", ""], ["Tang", "Ke", ""], ["Yao", "Xin", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1711.07227", "submitter": "Kubilay Atasu", "authors": "Kubilay Atasu, Thomas Parnell, Celestine D\\\"unner, Manolis Sifalakis,\n  Haralampos Pozidis, Vasileios Vasileiadis, Michail Vlachos, Cesar Berrospi,\n  Abdel Labbi", "title": "Linear-Complexity Relaxed Word Mover's Distance with GPU Acceleration", "comments": "To appear in the 2017 IEEE International Conference on Big Data (Big\n  Data 2017) http://cci.drexel.edu/bigdata/bigdata2017/ December 11-14, 2017,\n  Boston, MA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of unstructured text-based data is growing every day. Querying,\nclustering, and classifying this big data requires similarity computations\nacross large sets of documents. Whereas low-complexity similarity metrics are\navailable, attention has been shifting towards more complex methods that\nachieve a higher accuracy. In particular, the Word Mover's Distance (WMD)\nmethod proposed by Kusner et al. is a promising new approach, but its time\ncomplexity grows cubically with the number of unique words in the documents.\nThe Relaxed Word Mover's Distance (RWMD) method, again proposed by Kusner et\nal., reduces the time complexity from qubic to quadratic and results in a\nlimited loss in accuracy compared with WMD. Our work contributes a\nlow-complexity implementation of the RWMD that reduces the average time\ncomplexity to linear when operating on large sets of documents. Our\nlinear-complexity RWMD implementation, henceforth referred to as LC-RWMD, maps\nwell onto GPUs and can be efficiently distributed across a cluster of GPUs. Our\nexperiments on real-life datasets demonstrate 1) a performance improvement of\ntwo orders of magnitude with respect to our GPU-based distributed\nimplementation of the quadratic RWMD, and 2) a performance improvement of three\nto four orders of magnitude with respect to our distributed WMD implementation\nthat uses GPU-based RWMD for pruning.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 09:45:02 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Atasu", "Kubilay", ""], ["Parnell", "Thomas", ""], ["D\u00fcnner", "Celestine", ""], ["Sifalakis", "Manolis", ""], ["Pozidis", "Haralampos", ""], ["Vasileiadis", "Vasileios", ""], ["Vlachos", "Michail", ""], ["Berrospi", "Cesar", ""], ["Labbi", "Abdel", ""]]}, {"id": "1711.07270", "submitter": "Nicola Prezza", "authors": "Philip Bille, Travis Gagie, Inge Li G{\\o}rtz, Nicola Prezza", "title": "A Separation Between Run-Length SLPs and LZ77", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we give an infinite family of strings for which the length of\nthe Lempel-Ziv'77 parse is a factor $\\Omega(\\log n/\\log\\log n)$ smaller than\nthe smallest run-length grammar.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 12:00:24 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Bille", "Philip", ""], ["Gagie", "Travis", ""], ["G\u00f8rtz", "Inge Li", ""], ["Prezza", "Nicola", ""]]}, {"id": "1711.07423", "submitter": "Ahad N. Zehmakan", "authors": "Bernd G\\\"artner and Ahad N. Zehmakan", "title": "Majority Model on Random Regular Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a graph $G=(V,E)$ and an initial random coloring where each vertex\n$v \\in V$ is blue with probability $P_b$ and red otherwise, independently from\nall other vertices. In each round, all vertices simultaneously switch their\ncolor to the most frequent color in their neighborhood and in case of a tie, a\nvertex keeps its current color. The main goal of the present paper is to\nanalyze the behavior of this basic and natural process on the random\n$d$-regular graph $\\mathbb{G}_{n,d}$. It is shown that for all $\\epsilon>0$,\n$P_b \\le 1/2-\\epsilon$ results in final complete occupancy by red in\n$\\mathcal{O}(\\log_d\\log n)$ rounds with high probability, provided that $d\\geq\nc/\\epsilon^2$ for a suitable constant $c$. Furthermore, we show that with high\nprobability, $\\mathbb{G}_{n,d}$ is immune; i.e., the smallest dynamic monopoly\nis of linear size. A dynamic monopoly is a subset of vertices that can take\nover in the sense that a commonly chosen initial color eventually spreads\nthroughout the whole graph, irrespective of the colors of other vertices. This\nanswers an open question of Peleg.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 20:50:35 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["G\u00e4rtner", "Bernd", ""], ["Zehmakan", "Ahad N.", ""]]}, {"id": "1711.07454", "submitter": "Samuel Hopkins", "authors": "Samuel B. Hopkins and Jerry Li", "title": "Mixture Models, Robustness, and Sum of Squares Proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the Sum of Squares method to develop new efficient algorithms for\nlearning well-separated mixtures of Gaussians and robust mean estimation, both\nin high dimensions, that substantially improve upon the statistical guarantees\nachieved by previous efficient algorithms.\n  Firstly, we study mixtures of $k$ distributions in $d$ dimensions, where the\nmeans of every pair of distributions are separated by at least\n$k^{\\varepsilon}$. In the special case of spherical Gaussian mixtures, we give\na $(dk)^{O(1/\\varepsilon^2)}$-time algorithm that learns the means assuming\nseparation at least $k^{\\varepsilon}$, for any $\\varepsilon > 0$. This is the\nfirst algorithm to improve on greedy (\"single-linkage\") and spectral\nclustering, breaking a long-standing barrier for efficient algorithms at\nseparation $k^{1/4}$.\n  We also study robust estimation. When an unknown $(1-\\varepsilon)$-fraction\nof $X_1,\\ldots,X_n$ are chosen from a sub-Gaussian distribution with mean $\\mu$\nbut the remaining points are chosen adversarially, we give an algorithm\nrecovering $\\mu$ to error $\\varepsilon^{1-1/t}$ in time $d^{O(t^2)}$, so long\nas sub-Gaussian-ness up to $O(t)$ moments can be certified by a Sum of Squares\nproof. This is the first polynomial-time algorithm with guarantees approaching\nthe information-theoretic limit for non-Gaussian distributions. Previous\nalgorithms could not achieve error better than $\\varepsilon^{1/2}$.\n  Both of these results are based on a unified technique. Inspired by recent\nalgorithms of Diakonikolas et al. in robust statistics, we devise an SDP based\non the Sum of Squares method for the following setting: given $X_1,\\ldots,X_n\n\\in \\mathbb{R}^d$ for large $d$ and $n = poly(d)$ with the promise that a\nsubset of $X_1,\\ldots,X_n$ were sampled from a probability distribution with\nbounded moments, recover some information about that distribution.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 18:33:23 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Hopkins", "Samuel B.", ""], ["Li", "Jerry", ""]]}, {"id": "1711.07545", "submitter": "Philip Ginzboorg", "authors": "Philip Ginzboorg", "title": "On estimating the alphabet size of a discrete random source", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with estimating alphabet size $N$ from a stream of symbols\ntaken uniformly at random from that alphabet. We define and analyze a\nmemory-restricted variant of an algorithm that have been earlier proposed for\nthis purpose. The alphabet size $N$ can be estimated in $O(\\sqrt{N})$ time and\nspace by the memory-restricted variant of this algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 21:10:15 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Ginzboorg", "Philip", ""]]}, {"id": "1711.07567", "submitter": "Sariel Har-Peled", "authors": "Paul Beame, Sariel Har-Peled, Sivaramakrishnan Natarajan Ramamoorthy,\n  Cyrus Rashtchian, Makrand Sinha", "title": "Edge Estimation with Independent Set Oracles", "comments": "A preliminary version appeared in the proceedings of ITCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of estimating the number of edges in a graph with access to\nonly an independent set oracle. Independent set queries draw motivation from\ngroup testing and have applications to the complexity of decision versus\ncounting problems. We give two algorithms to estimate the number of edges in an\n$n$-vertex graph, using (i) $\\mathrm{polylog}(n)$ bipartite independent set\nqueries, or (ii) ${n}^{2/3} \\cdot\\mathrm{polylog}(n)$ independent set queries.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 22:25:17 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 17:57:12 GMT"}, {"version": "v3", "created": "Fri, 7 Sep 2018 20:59:21 GMT"}, {"version": "v4", "created": "Wed, 11 Mar 2020 18:48:48 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Beame", "Paul", ""], ["Har-Peled", "Sariel", ""], ["Ramamoorthy", "Sivaramakrishnan Natarajan", ""], ["Rashtchian", "Cyrus", ""], ["Sinha", "Makrand", ""]]}, {"id": "1711.07710", "submitter": "Arindam Khan", "authors": "Waldo G\\'alvez and Fabrizio Grandoni and Sandy Heydrich and Salvatore\n  Ingala and Arindam Khan and Andreas Wiese", "title": "Approximating Geometric Knapsack via L-packings", "comments": "64pages, full version of FOCS 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the two-dimensional geometric knapsack problem (2DK) in which we are\ngiven a set of n axis-aligned rectangular items, each one with an associated\nprofit, and an axis-aligned square knapsack. The goal is to find a\n(non-overlapping) packing of a maximum profit subset of items inside the\nknapsack (without rotating items). The best-known polynomial-time approximation\nfactor for this problem (even just in the cardinality case) is (2 + \\epsilon)\n[Jansen and Zhang, SODA 2004].\n  In this paper, we break the 2 approximation barrier, achieving a\npolynomial-time (17/9 + \\epsilon) < 1.89 approximation, which improves to\n(558/325 + \\epsilon) < 1.72 in the cardinality case. Essentially all prior work\non 2DK approximation packs items inside a constant number of rectangular\ncontainers, where items inside each container are packed using a simple greedy\nstrategy. We deviate for the first time from this setting: we show that there\nexists a large profit solution where items are packed inside a constant number\nof containers plus one L-shaped region at the boundary of the knapsack which\ncontains items that are high and narrow and items that are wide and thin. As a\nsecond major and the main algorithmic contribution of this paper, we present a\nPTAS for this case. We believe that this will turn out to be useful in future\nwork in geometric packing problems.\n  We also consider the variant of the problem with rotations (2DKR), where\nitems can be rotated by 90 degrees. Also, in this case, the best-known\npolynomial-time approximation factor (even for the cardinality case) is (2 +\n\\epsilon) [Jansen and Zhang, SODA 2004]. Exploiting part of the machinery\ndeveloped for 2DK plus a few additional ideas, we obtain a polynomial-time (3/2\n+ \\epsilon)-approximation for 2DKR, which improves to (4/3 + \\epsilon) in the\ncardinality case.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 10:46:35 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["G\u00e1lvez", "Waldo", ""], ["Grandoni", "Fabrizio", ""], ["Heydrich", "Sandy", ""], ["Ingala", "Salvatore", ""], ["Khan", "Arindam", ""], ["Wiese", "Andreas", ""]]}, {"id": "1711.07736", "submitter": "Aadhavan Sadasivam", "authors": "S Aadhavan, P Renjith, and N Sadagopan", "title": "On $P_5$-free Chordal bipartite graphs", "comments": "Presented in ICMCE 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bipartite graph is chordal bipartite if every cycle of length at least 6\nhas a chord in it. In this paper, we investigate the structure of $P_5$-free\nchordal bipartite graphs and show that these graphs have a Nested Neighborhood\nOrdering, a special ordering among its vertices. Further, using this ordering,\nwe present polynomial-time algorithms for classical problems such as\nHamiltonian cycle (path) and longest path. Two variants of Hamiltonian path\ninclude Steiner path and minimum leaf spanning tree, and we obtain\npolynomial-time algorithms for these problems as well restricted to $P_5$-free\nchordal bipartite graphs.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 12:04:03 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 11:43:22 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Aadhavan", "S", ""], ["Renjith", "P", ""], ["Sadagopan", "N", ""]]}, {"id": "1711.07746", "submitter": "Saulo Queiroz", "authors": "Saulo Queiroz", "title": "The Hidden Binary Search Tree:A Balanced Rotation-Free Search Tree in\n  the AVL RAM Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we generalize the definition of \"Search Trees\" (ST) to enable\nreference values other than the key of prior inserted nodes. The idea builds on\nthe assumption an $n$-node AVL (or Red-Black) requires to assure $O(\\log_2n)$\nworst-case search time, namely, a single comparison between two keys takes\nconstant time. This means the size of each key in bits is fixed to $B=c\\log_2\nn$ ($c\\geq1$) once $n$ is determined, otherwise the $O(1)$-time comparison\nassumption does not hold. Based on this we calculate \\emph{ideal} reference\nvalues from the mid-point of the interval $0..2^B$. This idea follows\n`recursively' to assure each node along the search path is provided a reference\nvalue that guarantees an overall logarithmic time. Because the search tree\nproperty works only when keys are compared to reference values and these values\nare calculated only during searches, we term the data structure as the Hidden\nBinary Search Tree (HBST). We show elementary functions to maintain the HSBT\nheight $O(B)=O(\\log_2n)$. This result requires no special order on the input --\nas does BST -- nor self-balancing procedures, as do AVL and Red-Black.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 12:32:50 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 01:24:40 GMT"}, {"version": "v3", "created": "Tue, 3 Apr 2018 12:17:47 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Queiroz", "Saulo", ""]]}, {"id": "1711.07851", "submitter": "Salvatore Ingala", "authors": "Salvatore Ingala", "title": "Approximation Algorithms for Rectangle Packing Problems (PhD Thesis)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In rectangle packing problems we are given the task of placing axis-aligned\nrectangles in a given plane region, so that they do not overlap with each\nother. In Maximum Weight Independent Set of Rectangles (MWISR), their position\nis given and we can only select which rectangles to choose, while trying to\nmaximize their total weight. In Strip Packing (SP), we have to pack all the\ngiven rectangles in a rectangular region of fixed width, while minimizing its\nheight. In 2-Dimensional Geometric Knapsack (2DGK), the target region is a\nsquare of a given size, and our goal is to select and pack a subset of the\ngiven rectangles of maximum weight. We study a generalization of MWISR and use\nit to improve the approximation for a resource allocation problem called\nbagUFP. We revisit some classical results on SP and 2DGK, by proposing a\nframework based on smaller containers that are packed with simpler rules; while\nvariations of this scheme are indeed a standard technique in this area, we\nabstract away some of the problem-specific differences, obtaining simpler\nalgorithms that work for different problems. We obtain improved approximations\nfor SP in pseudo-polynomial time, and for a variant of 2DGK where one can to\nrotate the rectangles by 90{\\deg}. For the latter, we propose the first\nalgorithms with approximation factor better than 2. For the main variant of\n2DGK (without rotations), a container-based approach seems to face a natural\nbarrier of 2 in the approximation factor. Thus, we consider a generalized kind\nof packing that combines container packings with another packing problem that\nwe call L-packing problem, where we have to pack rectangles in an L-shaped\nregion of the plane. By finding a (1 + {\\epsilon})-approximation for this\nproblem and exploiting the combinatorial structure of 2DGK, we obtain the first\nalgorithms that break the barrier of 2 for the approximation factor of this\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 15:44:14 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Ingala", "Salvatore", ""]]}, {"id": "1711.07872", "submitter": "Diptapriyo Majumdar", "authors": "R. Krithika, Diptapriyo Majumdar and Venkatesh Raman", "title": "Revisiting Connected Vertex Cover: FPT Algorithms and Lossy Kernels", "comments": "21 pages, 2 figures", "journal-ref": "Theory of Computing Systems, 2018", "doi": "10.1007/s00224-017-9837-y", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CONNECTED VERTEX COVER problem asks for a vertex cover in a graph that\ninduces a connected subgraph. The problem is known to be fixed-parameter\ntractable (FPT), and is unlikely to have a polynomial sized kernel (under\ncomplexity theoretic assumptions) when parameterized by the solution size. In a\nrecent paper, Lokshtanov et al.[STOC 2017], have shown an $\\alpha$-approximate\nkernel for the problem for every $\\alpha > 1$, in the framework of approximate\nor lossy kernelization. In this work, we exhibit lossy kernels and FPT\nalgorithms for CONNECTED VERTEX COVER for parameters that are more natural and\nfunctions of the input, and in some cases, smaller than the solution size. The\nparameters we consider are the sizes of a split deletion set, clique deletion\nset, clique cover, cluster deletion set and chordal deletion set.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 16:17:26 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Krithika", "R.", ""], ["Majumdar", "Diptapriyo", ""], ["Raman", "Venkatesh", ""]]}, {"id": "1711.07960", "submitter": "Quanquan C. Liu", "authors": "Erik D. Demaine, Andrea Lincoln, Quanquan C. Liu, Jayson Lynch,\n  Virginia Vassilevska Williams", "title": "Fine-Grained I/O Complexity via Reductions: New lower bounds, faster\n  algorithms, and a time hierarchy", "comments": "To appear in ITCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper initiates the study of I/O algorithms (minimizing cache misses)\nfrom the perspective of fine-grained complexity (conditional polynomial lower\nbounds). Specifically, we aim to answer why sparse graph problems are so hard,\nand why the Longest Common Subsequence problem gets a savings of a factor of\nthe size of cache times the length of a cache line, but no more. We take the\nreductions and techniques from complexity and fine-grained complexity and apply\nthem to the I/O model to generate new (conditional) lower bounds as well as\nfaster algorithms. We also prove the existence of a time hierarchy for the I/O\nmodel, which motivates the fine-grained reductions.\n  Using fine-grained reductions, we give an algorithm for distinguishing 2 vs.\n3 diameter and radius that runs in $O(|E|^2/(MB))$ cache misses, which for\nsparse graphs improves over the previous $O(|V|^2/B)$ running time. We give new\nreductions from radius and diameter to Wiener index and median. We show\nmeaningful reductions between problems that have linear-time solutions in the\nRAM model. The reductions use low I/O complexity (typically $O(n/B)$), and thus\nhelp to finely capture the relationship between \"I/O linear time\" $\\Theta(n/B)$\nand RAM linear time $\\Theta(n)$. We generate new I/O assumptions based on the\ndifficulty of improving sparse graph problem running times in the I/O model. We\ncreate conjectures that the current best known algorithms for Single Source\nShortest Paths (SSSP), diameter, and radius are optimal. From these I/O-model\nassumptions, we show that many of the known reductions in the word-RAM model\ncan naturally extend to hold in the I/O model as well (e.g., a lower bound on\nthe I/O complexity of Longest Common Subsequence that matches the best known\nrunning time). Finally, we prove an analog of the Time Hierarchy Theorem in the\nI/O model.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 18:37:27 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 21:39:31 GMT"}, {"version": "v3", "created": "Tue, 5 Dec 2017 01:22:10 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Demaine", "Erik D.", ""], ["Lincoln", "Andrea", ""], ["Liu", "Quanquan C.", ""], ["Lynch", "Jayson", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "1711.08020", "submitter": "Yangyang Xu", "authors": "Yangyang Xu", "title": "First-order methods for constrained convex programming based on\n  linearized augmented Lagrangian function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-order methods have been popularly used for solving large-scale\nproblems. However, many existing works only consider unconstrained problems or\nthose with simple constraint. In this paper, we develop two first-order methods\nfor constrained convex programs, for which the constraint set is represented by\naffine equations and smooth nonlinear inequalities. Both methods are based on\nthe classic augmented Lagrangian function. They update the multipliers in the\nsame way as the augmented Lagrangian method (ALM) but employ different primal\nvariable updates. The first method, at each iteration, performs a single\nproximal gradient step to the primal variable, and the second method is a block\nupdate version of the first one.\n  For the first method, we establish its global iterate convergence as well as\nglobal sublinear and local linear convergence, and for the second method, we\nshow a global sublinear convergence result in expectation. Numerical\nexperiments are carried out on the basis pursuit denoising and a convex\nquadratically constrained quadratic program to show the empirical performance\nof the proposed methods. Their numerical behaviors closely match the\nestablished theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 20:22:04 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Xu", "Yangyang", ""]]}, {"id": "1711.08041", "submitter": "Ohad Trabelsi", "authors": "Robert Krauthgamer and Ohad Trabelsi", "title": "The Set Cover Conjecture and Subgraph Isomorphism with a Tree Pattern", "comments": "Merged works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Set Cover problem, the input is a ground set of $n$ elements and a\ncollection of $m$ sets, and the goal is to find the smallest sub-collection of\nsets whose union is the entire ground set. The fastest algorithm known runs in\ntime $O(mn2^n)$ [Fomin et al., WG 2004], and the Set Cover Conjecture (SeCoCo)\n[Cygan et al., TALG 2016] asserts that for every fixed $\\varepsilon>0$, no\nalgorithm can solve Set Cover in time $2^{(1-\\varepsilon)n}poly(m)$, even if\nset sizes are bounded by $\\Delta=\\Delta(\\varepsilon)$. We show strong\nconnections between this problem and kTree, a special case of Subgraph\nIsomorphism where the input is an $n$-node graph $G$ and a $k$-node tree $T$,\nand the goal is to determine whether $G$ has a subgraph isomorphic to $T$.\n  First, we propose a weaker conjecture Log-SeCoCo, that allows input sets of\nsize $\\Delta=O(1/\\varepsilon \\cdot\\log n)$, and show that an algorithm breaking\nLog-SeCoCo would imply a faster algorithm than the currently known $2^n\npoly(n)$-time algorithm [Koutis and Williams, TALG 2016] for Directed nTree,\nwhich is kTree with $k=n$ and arbitrary directions to the edges of $G$ and $T$.\nThis would also improve the running time for Directed Hamiltonicity, for which\nno algorithm significantly faster than $2^n poly(n)$ is known despite extensive\nresearch.\n  Second, we prove that if Set Cover cannot be solved significantly faster than\n$2^npoly(m)$ (an assumption even weaker than Log-SeCoCo), then kTree cannot be\ncomputed significantly faster than $2^kpoly(n)$, the running time of the Koutis\nand Williams' algorithm. Applying the same techniques to the p-Partial Cover\nproblem, a parameterized version of Set Cover that requires covering at least\n$p$ elements, we obtain a new algorithm with running time $(2+\\varepsilon)^p\n(m+n)^{O(1/\\varepsilon)}$ for arbitrary $\\varepsilon>0$, which improves\nprevious work and is nearly optimal assuming say Log-SeCoCo.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 21:22:31 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 12:41:22 GMT"}, {"version": "v3", "created": "Wed, 16 Jan 2019 22:36:24 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Krauthgamer", "Robert", ""], ["Trabelsi", "Ohad", ""]]}, {"id": "1711.08082", "submitter": "Jakub Mare\\v{c}ek", "authors": "Jing Xu, Jakub Marecek", "title": "Parameter Estimation in Gaussian Mixture Models with Malicious Noise,\n  without Balanced Mixing Coefficients", "comments": null, "journal-ref": null, "doi": "10.1109/ALLERTON.2018.8635825", "report-no": null, "categories": "math.ST cs.CC cs.DS stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating means of two Gaussians in a 2-Gaussian\nmixture, which is not balanced and is corrupted by noise of an arbitrary\ndistribution. We present a robust algorithm to estimate the parameters,\ntogether with upper bounds on the numbers of samples required for the estimate\nto be correct, where the bounds are parametrised by the dimension, ratio of the\nmixing coefficients, a measure of the separation of the two Gaussians, related\nto Mahalanobis distance, and a condition number of the covariance matrix. In\ntheory, this is the first sample-complexity result for imbalanced mixtures\ncorrupted by adversarial noise. In practice, our algorithm outperforms the\nvanilla Expectation-Maximisation (EM) algorithm in terms of estimation error.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 23:20:12 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Xu", "Jing", ""], ["Marecek", "Jakub", ""]]}, {"id": "1711.08217", "submitter": "Mikko Berggren Ettienne", "authors": "Anders Roy Christiansen, Mikko Berggren Ettienne", "title": "Compressed Indexing with Signature Grammars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The compressed indexing problem is to preprocess a string $S$ of length $n$\ninto a compressed representation that supports pattern matching queries. That\nis, given a string $P$ of length $m$ report all occurrences of $P$ in $S$.\n  We present a data structure that supports pattern matching queries in $O(m +\nocc (\\lg\\lg n + \\lg^\\epsilon z))$ time using $O(z \\lg(n / z))$ space where $z$\nis the size of the LZ77 parse of $S$ and $\\epsilon > 0$ is an arbitrarily small\nconstant, when the alphabet is small or $z = O(n^{1 - \\delta})$ for any\nconstant $\\delta > 0$. We also present two data structures for the general\ncase; one where the space is increased by $O(z\\lg\\lg z)$, and one where the\nquery time changes from worst-case to expected. These results improve the\npreviously best known solutions. Notably, this is the first data structure that\ndecides if $P$ occurs in $S$ in $O(m)$ time using $O(z\\lg(n/z))$ space.\n  Our results are mainly obtained by a novel combination of a randomized\ngrammar construction algorithm with well known techniques relating pattern\nmatching to 2D-range reporting.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 10:40:06 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 12:53:43 GMT"}, {"version": "v3", "created": "Wed, 11 Apr 2018 12:55:35 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Christiansen", "Anders Roy", ""], ["Ettienne", "Mikko Berggren", ""]]}, {"id": "1711.08289", "submitter": "Jakob Lykke Andersen", "authors": "Jakob L. Andersen, Daniel Merkle", "title": "A Generic Framework for Engineering Graph Canonization Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art tools for practical graph canonization are all based on\nthe individualization-refinement paradigm, and their difference is primarily in\nthe choice of heuristics they include and in the actual tool implementation. It\nis thus not possible to make a direct comparison of how individual algorithmic\nideas affect the performance on different graph classes.\n  We present an algorithmic software framework that facilitates implementation\nof heuristics as independent extensions to a common core algorithm. It\ntherefore becomes easy to perform a detailed comparison of the performance and\nbehaviour of different algorithmic ideas. Implementations are provided of a\nrange of algorithms for tree traversal, target cell selection, and node\ninvariant, including choices from the literature and new variations. The\nframework readily supports extraction and visualization of detailed data from\nseparate algorithm executions for subsequent analysis and development of new\nheuristics.\n  Using collections of different graph classes we investigate the effect of\nvarying the selections of heuristics, often revealing exactly which individual\nalgorithmic choice is responsible for particularly good or bad performance. On\nseveral benchmark collections, including a newly proposed class of difficult\ninstances, we additionally find that our implementation performs better than\nthe current state-of-the-art tools.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 14:17:08 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Andersen", "Jakob L.", ""], ["Merkle", "Daniel", ""]]}, {"id": "1711.08398", "submitter": "Filippo Maria Bianchi", "authors": "Filippo Maria Bianchi, Antonello Rizzi, Alireza Sadeghian, Corrado\n  Moiso", "title": "Identifying user habits through data mining on call data records", "comments": null, "journal-ref": "Engineering Applications of Artificial Intelligence, Elsevier,\n  2016", "doi": "10.1016/j.engappai.2016.05.007", "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a framework for identifying patterns and\nregularities in the pseudo-anonymized Call Data Records (CDR) pertaining a\ngeneric subscriber of a mobile operator. We face the challenging task of\nautomatically deriving meaningful information from the available data, by using\nan unsupervised procedure of cluster analysis and without including in the\nmodel any \\textit{a-priori} knowledge on the applicative context. Clusters\nmining results are employed for understanding users' habits and to draw their\ncharacterizing profiles. We propose two implementations of the data mining\nprocedure; the first is based on a novel system for clusters and knowledge\ndiscovery called LD-ABCD, capable of retrieving clusters and, at the same time,\nto automatically discover for each returned cluster the most appropriate\ndissimilarity measure (local metric). The second approach instead is based on\nPROCLUS, the well-know subclustering algorithm. The dataset under analysis\ncontains records characterized only by few features and, consequently, we show\nhow to generate additional fields which describe implicit information hidden in\ndata. Finally, we propose an effective graphical representation of the results\nof the data-mining procedure, which can be easily understood and employed by\nanalysts for practical applications.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 17:12:50 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Bianchi", "Filippo Maria", ""], ["Rizzi", "Antonello", ""], ["Sadeghian", "Alireza", ""], ["Moiso", "Corrado", ""]]}, {"id": "1711.08421", "submitter": "Ryan Urbanowicz", "authors": "Ryan J. Urbanowicz, Melissa Meeker, William LaCava, Randal S. Olson,\n  Jason H. Moore", "title": "Relief-Based Feature Selection: Introduction and Review", "comments": "Submitted revisions for publication based on reviews by the Journal\n  of Biomedical Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature selection plays a critical role in biomedical data mining, driven by\nincreasing feature dimensionality in target problems and growing interest in\nadvanced but computationally expensive methodologies able to model complex\nassociations. Specifically, there is a need for feature selection methods that\nare computationally efficient, yet sensitive to complex patterns of\nassociation, e.g. interactions, so that informative features are not mistakenly\neliminated prior to downstream modeling. This paper focuses on Relief-based\nalgorithms (RBAs), a unique family of filter-style feature selection algorithms\nthat have gained appeal by striking an effective balance between these\nobjectives while flexibly adapting to various data characteristics, e.g.\nclassification vs. regression. First, this work broadly examines types of\nfeature selection and defines RBAs within that context. Next, we introduce the\noriginal Relief algorithm and associated concepts, emphasizing the intuition\nbehind how it works, how feature weights generated by the algorithm can be\ninterpreted, and why it is sensitive to feature interactions without evaluating\ncombinations of features. Lastly, we include an expansive review of RBA\nmethodological research beyond Relief and its popular descendant, ReliefF. In\nparticular, we characterize branches of RBA research, and provide comparative\nsummaries of RBA algorithms including contributions, strategies, functionality,\ntime complexity, adaptation to key data characteristics, and software\navailability.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 18:06:25 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 20:46:42 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Urbanowicz", "Ryan J.", ""], ["Meeker", "Melissa", ""], ["LaCava", "William", ""], ["Olson", "Randal S.", ""], ["Moore", "Jason H.", ""]]}, {"id": "1711.08446", "submitter": "Matthew Fahrbach", "authors": "Matthew Fahrbach, Gary L. Miller, Richard Peng, Saurabh Sawlani,\n  Junxing Wang, Shen Chen Xu", "title": "On Computing Min-Degree Elimination Orderings", "comments": "57 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study faster algorithms for producing the minimum degree ordering used to\nspeed up Gaussian elimination. This ordering is based on viewing the non-zero\nelements of a symmetric positive definite matrix as edges of an undirected\ngraph, and aims at reducing the additional non-zeros (fill) in the matrix by\nrepeatedly removing the vertex of minimum degree. It is one of the most widely\nused primitives for pre-processing sparse matrices in scientific computing.\n  Our result is in part motivated by the observation that sub-quadratic time\nalgorithms for finding min-degree orderings are unlikely, assuming the strong\nexponential time hypothesis (SETH). This provides justification for the lack of\nprovably efficient algorithms for generating such orderings, and leads us to\nstudy speedups via degree-restricted algorithms as well as approximations. Our\ntwo main results are: (1) an algorithm that produces a min-degree ordering\nwhose maximum degree is bounded by $\\Delta$ in $O(m \\Delta \\log^3{n})$ time,\nand (2) an algorithm that finds an $(1 + \\epsilon)$-approximate marginal\nmin-degree ordering in $O(m \\log^{5}n \\epsilon^{-2})$ time.\n  Both of our algorithms rely on a host of randomization tools related to the\n$\\ell_0$-estimator by [Cohen `97]. A key technical issue for the final\nnearly-linear time algorithm are the dependencies of the vertex removed on the\nrandomness in the data structures. To address this, we provide a method for\ngenerating a pseudo-deterministic access sequence, which then allows the\nincorporation of data structures that only work under the oblivious adversary\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 18:46:03 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Fahrbach", "Matthew", ""], ["Miller", "Gary L.", ""], ["Peng", "Richard", ""], ["Sawlani", "Saurabh", ""], ["Wang", "Junxing", ""], ["Xu", "Shen Chen", ""]]}, {"id": "1711.08475", "submitter": "Aleksander Cis{\\l}ak", "authors": "Aleksander Cis{\\l}ak, Szymon Grabowski", "title": "Lightweight Fingerprints for Fast Approximate Keyword Matching Using\n  Bitwise Operations", "comments": "16 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to speed up approximate keyword matching by storing a lightweight,\nfixed-size block of data for each string, called a fingerprint. These work in a\nsimilar way to hash values; however, they can be also used for matching with\nerrors. They store information regarding symbol occurrences using individual\nbits, and they can be compared against each other with a constant number of\nbitwise operations. In this way, certain strings can be deduced to be at least\nwithin the distance $k$ from each other (using Hamming or Levenshtein distance)\nwithout performing an explicit verification. We show experimentally that for a\npreprocessed collection of strings, fingerprints can provide substantial\nspeedups for $k = 1$, namely over $2.5$ times for the Hamming distance and over\n$10$ times for the Levenshtein distance. Tests were conducted on synthetic and\nreal-world English and URL data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 19:13:08 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Cis\u0142ak", "Aleksander", ""], ["Grabowski", "Szymon", ""]]}, {"id": "1711.08494", "submitter": "David Harris", "authors": "David G. Harris", "title": "Deterministic parallel algorithms for bilinear objective functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many randomized algorithms can be derandomized efficiently using either the\nmethod of conditional expectations or probability spaces with low independence.\nA series of papers, beginning with work by Luby (1988), showed that in many\ncases these techniques can be combined to give deterministic parallel (NC)\nalgorithms for a variety of combinatorial optimization problems, with low time-\nand processor-complexity.\n  We extend and generalize a technique of Luby for efficiently handling\nbilinear objective functions. One noteworthy application is an NC algorithm for\nmaximal independent set. On a graph $G$ with $m$ edges and $n$ vertices, this\ntakes $\\tilde O(\\log^2 n)$ time and $(m + n) n^{o(1)}$ processors, nearly\nmatching the best randomized parallel algorithms. Other applications include\nreduced processor counts for algorithms of Berger (1997) for maximum acyclic\nsubgraph and Gale-Berlekamp switching games.\n  This bilinear factorization also gives better algorithms for problems\ninvolving discrepancy. An important application of this is to automata-fooling\nprobability spaces, which are the basis of a notable derandomization technique\nof Sivakumar (2002). Our method leads to large reduction in processor\ncomplexity for a number of derandomization algorithms based on\nautomata-fooling, including set discrepancy and the Johnson-Lindenstrauss\nLemma.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 20:21:26 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 21:34:24 GMT"}, {"version": "v3", "created": "Thu, 21 Jun 2018 16:15:21 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Harris", "David G.", ""]]}, {"id": "1711.08513", "submitter": "Michael P. Kim", "authors": "\\'Ursula H\\'ebert-Johnson, Michael P. Kim, Omer Reingold, Guy N.\n  Rothblum", "title": "Calibration for the (Computationally-Identifiable) Masses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As algorithms increasingly inform and influence decisions made about\nindividuals, it becomes increasingly important to address concerns that these\nalgorithms might be discriminatory. The output of an algorithm can be\ndiscriminatory for many reasons, most notably: (1) the data used to train the\nalgorithm might be biased (in various ways) to favor certain populations over\nothers; (2) the analysis of this training data might inadvertently or\nmaliciously introduce biases that are not borne out in the data. This work\nfocuses on the latter concern.\n  We develop and study multicalbration -- a new measure of algorithmic fairness\nthat aims to mitigate concerns about discrimination that is introduced in the\nprocess of learning a predictor from data. Multicalibration guarantees accurate\n(calibrated) predictions for every subpopulation that can be identified within\na specified class of computations. We think of the class as being quite rich;\nin particular, it can contain many overlapping subgroups of a protected group.\n  We show that in many settings this strong notion of protection from\ndiscrimination is both attainable and aligned with the goal of obtaining\naccurate predictions. Along the way, we present new algorithms for learning a\nmulticalibrated predictor, study the computational complexity of this task, and\ndraw new connections to computational learning models such as agnostic\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 21:47:55 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2018 17:50:06 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["H\u00e9bert-Johnson", "\u00darsula", ""], ["Kim", "Michael P.", ""], ["Reingold", "Omer", ""], ["Rothblum", "Guy N.", ""]]}, {"id": "1711.08715", "submitter": "Chaitanya Swamy", "authors": "Deeparnab Chakrabarty and Chaitanya Swamy", "title": "Interpolating between $k$-Median and $k$-Center: Approximation\n  Algorithms for Ordered $k$-Median", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a generalization of $k$-median and $k$-center, called the {\\em\nordered $k$-median} problem. In this problem, we are given a metric space\n$(\\mathcal{D},\\{c_{ij}\\})$ with $n=|\\mathcal{D}|$ points, and a non-increasing\nweight vector $w\\in\\mathbb{R}_+^n$, and the goal is to open $k$ centers and\nassign each point each point $j\\in\\mathcal{D}$ to a center so as to minimize\n$w_1\\cdot\\text{(largest assignment cost)}+w_2\\cdot\\text{(second-largest\nassignment cost)}+\\ldots+w_n\\cdot\\text{($n$-th largest assignment cost)}$. We\ngive an $(18+\\epsilon)$-approximation algorithm for this problem. Our\nalgorithms utilize Lagrangian relaxation and the primal-dual schema, combined\nwith an enumeration procedure of Aouad and Segev. For the special case of\n$\\{0,1\\}$-weights, which models the problem of minimizing the $\\ell$ largest\nassignment costs that is interesting in and of by itself, we provide a novel\nreduction to the (standard) $k$-median problem showing that LP-relative\nguarantees for $k$-median translate to guarantees for the ordered $k$-median\nproblem; this yields a nice and clean $(8.5+\\epsilon)$-approximation algorithm\nfor $\\{0,1\\}$ weights.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 14:42:24 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "1711.08797", "submitter": "S{\\o}ren Dahlgaard", "authors": "S{\\o}ren Dahlgaard, Mathias B{\\ae}k Tejs Knudsen, Mikkel Thorup", "title": "Practical Hash Functions for Similarity Estimation and Dimensionality\n  Reduction", "comments": "Preliminary version of this paper will appear at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing is a basic tool for dimensionality reduction employed in several\naspects of machine learning. However, the perfomance analysis is often carried\nout under the abstract assumption that a truly random unit cost hash function\nis used, without concern for which concrete hash function is employed. The\nconcrete hash function may work fine on sufficiently random input. The question\nis if it can be trusted in the real world when faced with more structured\ninput.\n  In this paper we focus on two prominent applications of hashing, namely\nsimilarity estimation with the one permutation hashing (OPH) scheme of Li et\nal. [NIPS'12] and feature hashing (FH) of Weinberger et al. [ICML'09], both of\nwhich have found numerous applications, i.e. in approximate near-neighbour\nsearch with LSH and large-scale classification with SVM.\n  We consider mixed tabulation hashing of Dahlgaard et al.[FOCS'15] which was\nproved to perform like a truly random hash function in many applications,\nincluding OPH. Here we first show improved concentration bounds for FH with\ntruly random hashing and then argue that mixed tabulation performs similar for\nsparse input. Our main contribution, however, is an experimental comparison of\ndifferent hashing schemes when used inside FH, OPH, and LSH.\n  We find that mixed tabulation hashing is almost as fast as the\nmultiply-mod-prime scheme ax+b mod p. Mutiply-mod-prime is guaranteed to work\nwell on sufficiently random data, but we demonstrate that in the above\napplications, it can lead to bias and poor concentration on both real-world and\nsynthetic data. We also compare with the popular MurmurHash3, which has no\nproven guarantees. Mixed tabulation and MurmurHash3 both perform similar to\ntruly random hashing in our experiments. However, mixed tabulation is 40%\nfaster than MurmurHash3, and it has the proven guarantee of good performance on\nall possible input.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 18:27:37 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Dahlgaard", "S\u00f8ren", ""], ["Knudsen", "Mathias B\u00e6k Tejs", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1711.08837", "submitter": "Konrad Dabrowski", "authors": "Konrad K. Dabrowski and Vadim V. Lozin and Dani\\\"el Paulusma", "title": "Clique-width and Well-Quasi-Ordering of Triangle-Free Graph Classes", "comments": "32 pages, 4 figures. An extended abstract of this paper appeared in\n  the proceedings of WG 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Daligault, Rao and Thomass\\'e asked whether every hereditary graph class that\nis well-quasi-ordered by the induced subgraph relation has bounded\nclique-width. Lozin, Razgon and Zamaraev (JCTB 2017+) gave a negative answer to\nthis question, but their counterexample is a class that can only be\ncharacterised by infinitely many forbidden induced subgraphs. This raises the\nissue of whether the question has a positive answer for finitely defined\nhereditary graph classes. Apart from two stubborn cases, this has been\nconfirmed when at most two induced subgraphs $H_1,H_2$ are forbidden. We\nconfirm it for one of the two stubborn cases, namely for the\n$(H_1,H_2)=(\\mbox{triangle},P_2+P_4)$ case, by proving that the class of\n$(\\mbox{triangle},P_2+P_4)$-free graphs has bounded clique-width and is\nwell-quasi-ordered. Our technique is based on a special decomposition of\n$3$-partite graphs. We also use this technique to prove that the class of\n$(\\mbox{triangle},P_1+P_5)$-free graphs, which is known to have bounded\nclique-width, is well-quasi-ordered. Our results enable us to complete the\nclassification of graphs $H$ for which the class of $(\\mbox{triangle},H)$-free\ngraphs is well-quasi-ordered.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 22:43:34 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Dabrowski", "Konrad K.", ""], ["Lozin", "Vadim V.", ""], ["Paulusma", "Dani\u00ebl", ""]]}, {"id": "1711.08841", "submitter": "Pranjal Awasthi", "authors": "Pranjal Awasthi, Aravindan Vijayaraghavan", "title": "Clustering Semi-Random Mixtures of Gaussians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian mixture models (GMM) are the most widely used statistical model for\nthe $k$-means clustering problem and form a popular framework for clustering in\nmachine learning and data analysis. In this paper, we propose a natural\nsemi-random model for $k$-means clustering that generalizes the Gaussian\nmixture model, and that we believe will be useful in identifying robust\nalgorithms. In our model, a semi-random adversary is allowed to make arbitrary\n\"monotone\" or helpful changes to the data generated from the Gaussian mixture\nmodel.\n  Our first contribution is a polynomial time algorithm that provably recovers\nthe ground-truth up to small classification error w.h.p., assuming certain\nseparation between the components. Perhaps surprisingly, the algorithm we\nanalyze is the popular Lloyd's algorithm for $k$-means clustering that is the\nmethod-of-choice in practice. Our second result complements the upper bound by\ngiving a nearly matching information-theoretic lower bound on the number of\nmisclassified points incurred by any $k$-means clustering algorithm on the\nsemi-random model.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 23:17:37 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1711.08866", "submitter": "Charles Semple", "authors": "Magnus Bordewich, Katharina T Huber, Vincent Moulton, Charles Semple", "title": "Recovering tree-child networks from shortest inter-taxa distance\n  information", "comments": "24 pages 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic networks are a type of leaf-labelled, acyclic, directed graph\nused by biologists to represent the evolutionary history of species whose past\nincludes reticulation events. A phylogenetic network is tree-child if each\nnon-leaf vertex is the parent of a tree vertex or a leaf. Up to a certain\nequivalence, it has been recently shown that, under two different types of\nweightings, edge-weighted tree-child networks are determined by their\ncollection of distances between each pair of taxa. However, the size of these\ncollections can be exponential in the size of the taxa set. In this paper, we\nshow that, if we ignore redundant edges, the same results are obtained with\nonly a quadratic number of inter-taxa distances by using the shortest distance\nbetween each pair of taxa. The proofs are constructive and give cubic-time\nalgorithms in the size of the taxa sets for building such weighted networks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 03:59:02 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Bordewich", "Magnus", ""], ["Huber", "Katharina T", ""], ["Moulton", "Vincent", ""], ["Semple", "Charles", ""]]}, {"id": "1711.08885", "submitter": "Murali Krishna Enduri", "authors": "Bireswar Das, Murali Krishna Enduri, I. Vinod Reddy", "title": "On the Parallel Parameterized Complexity of the Graph Isomorphism\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the parallel and the space complexity of the graph\nisomorphism problem (\\GI{}) for several parameterizations. Let\n$\\mathcal{H}=\\{H_1,H_2,\\cdots,H_l\\}$ be a finite set of graphs where\n$|V(H_i)|\\leq d$ for all $i$ and for some constant $d$. Let $\\mathcal{G}$ be an\n$\\mathcal{H}$-free graph class i.e., none of the graphs $G\\in \\mathcal{G}$\ncontain any $H \\in \\mathcal{H}$ as an induced subgraph. We show that \\GI{}\nparameterized by vertex deletion distance to $\\mathcal{G}$ is in a\nparameterized version of $\\AC^1$, denoted $\\PL$-$\\AC^1$, provided the colored\ngraph isomorphism problem for graphs in $\\mathcal{G}$ is in $\\AC^1$. From this,\nwe deduce that \\GI{} parameterized by the vertex deletion distance to cographs\nis in $\\PL$-$\\AC^1$.\n  The parallel parameterized complexity of \\GI{} parameterized by the size of a\nfeedback vertex set remains an open problem. Towards this direction we show\nthat the graph isomorphism problem is in $\\PL$-$\\TC^0$ when parameterized by\nvertex cover or by twin-cover.\n  Let $\\mathcal{G}'$ be a graph class such that recognizing graphs from\n$\\mathcal{G}'$ and the colored version of \\GI{} for $\\mathcal{G}'$ is in\nlogspace ($\\L$). We show that \\GI{} for bounded vertex deletion distance to\n$\\mathcal{G}'$ is in $\\L$. From this, we obtain logspace algorithms for \\GI{}\nfor graphs with bounded vertex deletion distance to interval graphs and graphs\nwith bounded vertex deletion distance to cographs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 07:14:33 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 11:01:38 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Das", "Bireswar", ""], ["Enduri", "Murali Krishna", ""], ["Reddy", "I. Vinod", ""]]}, {"id": "1711.08921", "submitter": "Pascal Kerschke", "authors": "Pascal Kerschke and Heike Trautmann", "title": "Automated Algorithm Selection on Continuous Black-Box Problems By\n  Combining Exploratory Landscape Analysis and Machine Learning", "comments": "This is the author's final version, and the article has been accepted\n  for publication in Evolutionary Computation", "journal-ref": null, "doi": "10.1162/evco_a_00236", "report-no": null, "categories": "stat.ML cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we build upon previous work on designing informative and\nefficient Exploratory Landscape Analysis features for characterizing problems'\nlandscapes and show their effectiveness in automatically constructing algorithm\nselection models in continuous black-box optimization problems. Focussing on\nalgorithm performance results of the COCO platform of several years, we\nconstruct a representative set of high-performing complementary solvers and\npresent an algorithm selection model that - compared to the portfolio's single\nbest solver - on average requires less than half of the resources for solving a\ngiven problem. Therefore, there is a huge gain in efficiency compared to\nclassical ensemble methods combined with an increased insight into problem\ncharacteristics and algorithm properties by using informative features. Acting\non the assumption that the function set of the Black-Box Optimization Benchmark\nis representative enough for practical applications the model allows for\nselecting the best suited optimization algorithm within the considered set for\nunseen problems prior to the optimization itself based on a small sample of\nfunction evaluations. Note that such a sample can even be reused for the\ninitial population of an evolutionary (optimization) algorithm so that even the\nfeature costs become negligible.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 10:35:02 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 18:37:29 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 07:05:21 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Kerschke", "Pascal", ""], ["Trautmann", "Heike", ""]]}, {"id": "1711.09148", "submitter": "Wolfgang Dvo\\v{r}\\'ak", "authors": "Krishnendu Chatterjee, Wolfgang Dvo\\v{r}\\'ak, Monika Henzinger,\n  Veronika Loitzenbauer", "title": "Lower Bounds for Symbolic Computation on Graphs: Strongly Connected\n  Components, Liveness, Safety, and Diameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model of computation that is widely used in the formal analysis of reactive\nsystems is symbolic algorithms. In this model the access to the input graph is\nrestricted to consist of symbolic operations, which are expensive in comparison\nto the standard RAM operations. We give lower bounds on the number of symbolic\noperations for basic graph problems such as the computation of the strongly\nconnected components and of the approximate diameter as well as for fundamental\nproblems in model checking such as safety, liveness, and co-liveness. Our lower\nbounds are linear in the number of vertices of the graph, even for\nconstant-diameter graphs. For none of these problems lower bounds on the number\nof symbolic operations were known before. The lower bounds show an interesting\nseparation of these problems from the reachability problem, which can be solved\nwith $O(D)$ symbolic operations, where $D$ is the diameter of the graph.\n  Additionally we present an approximation algorithm for the graph diameter\nwhich requires $\\tilde{O}(n \\sqrt{D})$ symbolic steps to achieve a\n$(1+\\epsilon)$-approximation for any constant $\\epsilon > 0$. This compares to\n$O(n \\cdot D)$ symbolic steps for the (naive) exact algorithm and $O(D)$\nsymbolic steps for a 2-approximation. Finally we also give a refined analysis\nof the strongly connected components algorithms of Gentilini et al., showing\nthat it uses an optimal number of symbolic steps that is proportional to the\nsum of the diameters of the strongly connected components.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 21:47:11 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Dvo\u0159\u00e1k", "Wolfgang", ""], ["Henzinger", "Monika", ""], ["Loitzenbauer", "Veronika", ""]]}, {"id": "1711.09155", "submitter": "Thibaut Stimpfling", "authors": "Thibaut Stimpfling, Normand B\\'elanger, J.M. Pierre Langlois, and Yvon\n  Savaria", "title": "SHIP: A Scalable High-performance IPv6 Lookup Algorithm that Exploits\n  Prefix Characteristics", "comments": "Submitted to EEE/ACM Transactions on Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the emergence of new network applications, current IP lookup engines\nmust support high-bandwidth, low lookup latency and the ongoing growth of IPv6\nnetworks. However, existing solutions are not designed to address jointly those\nthree requirements. This paper introduces SHIP, an IPv6 lookup algorithm that\nexploits prefix characteristics to build a two-level data structure designed to\nmeet future application requirements. Using both prefix length distribution and\nprefix density, SHIP first clusters prefixes into groups sharing similar\ncharacteristics, then it builds a hybrid trie-tree for each prefix group. The\ncompact and scalable data structure built can be stored in on-chip low-latency\nmemories, and allows the traversal process to be parallelized and pipelined at\neach level in order to support high packet bandwidth. Evaluated on real and\nsynthetic prefix tables holding up to 580 k IPv6 prefixes, SHIP has a\nlogarithmic scaling factor in terms of the number of memory accesses, and a\nlinear memory consumption scaling. Using the largest synthetic prefix table,\nsimulations show that compared to other well-known approaches, SHIP uses at\nleast 44% less memory per prefix, while reducing the memory latency by 61%.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 22:21:23 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Stimpfling", "Thibaut", ""], ["B\u00e9langer", "Normand", ""], ["Langlois", "J. M. Pierre", ""], ["Savaria", "Yvon", ""]]}, {"id": "1711.09189", "submitter": "Lijun Chang", "authors": "Lijun Chang", "title": "A Near-optimal Algorithm for Edge Connectivity-based Hierarchical Graph\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Driven by many applications in graph analytics, the problem of computing\n$k$-edge connected components ($k$-ECCs) of a graph $G$ for a user-given $k$\nhas been extensively studied recently. In this paper, we investigate the\nproblem of constructing the hierarchy of edge connectivity-based graph\ndecomposition, which compactly represents the $k$-ECCs of a graph for all\npossible $k$ values. This is based on the fact that each $k$-ECC is entirely\ncontained in a $(k-1)$-ECC. In contrast to the existing approaches that conduct\nthe computation either in a bottom-up or a top-down manner, we propose a binary\nsearch-based framework which invokes a $k$-ECC computation algorithm as a black\nbox. Let $T_{kecc}(G)$ be the time complexity of computing all $k$-ECCs of $G$\nfor a specific $k$ value. We prove that the time complexity of our framework is\n${\\cal O}\\big( (\\log \\delta(G))\\times T_{kecc}(G)\\big)$, where $\\delta(G)$ is\nthe degeneracy of $G$ and equals the maximum value among the minimum vertex\ndegrees of all subgraphs of $G$. As $\\delta(G)$ is typically small for\nreal-world graphs, this time complexity is optimal up to a logarithmic factor.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 04:43:12 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Chang", "Lijun", ""]]}, {"id": "1711.09258", "submitter": "Hsin-Hao Su", "authors": "Bernhard Haeupler, Jeet Mohapatra, Hsin-Hao Su", "title": "Optimal Gossip Algorithms for Exact and Approximate Quantile\n  Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives drastically faster gossip algorithms to compute exact and\napproximate quantiles.\n  Gossip algorithms, which allow each node to contact a uniformly random other\nnode in each round, have been intensely studied and been adopted in many\napplications due to their fast convergence and their robustness to failures.\nKempe et al. [FOCS'03] gave gossip algorithms to compute important aggregate\nstatistics if every node is given a value. In particular, they gave a beautiful\n$O(\\log n + \\log \\frac{1}{\\epsilon})$ round algorithm to $\\epsilon$-approximate\nthe sum of all values and an $O(\\log^2 n)$ round algorithm to compute the exact\n$\\phi$-quantile, i.e., the the $\\lceil \\phi n \\rceil$ smallest value.\n  We give an quadratically faster and in fact optimal gossip algorithm for the\nexact $\\phi$-quantile problem which runs in $O(\\log n)$ rounds. We furthermore\nshow that one can achieve an exponential speedup if one allows for an\n$\\epsilon$-approximation. We give an $O(\\log \\log n + \\log \\frac{1}{\\epsilon})$\nround gossip algorithm which computes a value of rank between $\\phi n$ and\n$(\\phi+\\epsilon)n$ at every node.% for any $0 \\leq \\phi \\leq 1$ and $0 <\n\\epsilon < 1$. Our algorithms are extremely simple and very robust - they can\nbe operated with the same running times even if every transmission fails with\na, potentially different, constant probability. We also give a matching\n$\\Omega(\\log \\log n + \\log \\frac{1}{\\epsilon})$ lower bound which shows that\nour algorithm is optimal for all values of $\\epsilon$.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 16:30:42 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Mohapatra", "Jeet", ""], ["Su", "Hsin-Hao", ""]]}, {"id": "1711.09384", "submitter": "Harry Lang", "authors": "Harry Lang", "title": "Online Facility Location on Semi-Random Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the streaming model, the order of the stream can significantly affect the\ndifficulty of a problem. A $t$-semirandom stream was introduced as an\ninterpolation between random-order ($t=1$) and adversarial-order ($t=n$)\nstreams where an adversary intercepts a random-order stream and can delay up to\n$t$ elements at a time. IITK Sublinear Open Problem \\#15 asks to find\nalgorithms whose performance degrades smoothly as $t$ increases. We show that\nthe celebrated online facility location algorithm achieves an expected\ncompetitive ratio of $O(\\frac{\\log t}{\\log \\log t})$. We present a matching\nlower bound that any randomized algorithm has an expected competitive ratio of\n$\\Omega(\\frac{\\log t}{\\log \\log t})$.\n  We use this result to construct an $O(1)$-approximate streaming algorithm for\n$k$-median clustering that stores $O(k \\log t)$ points and has $O(k \\log t)$\nworst-case update time. Our technique generalizes to any dissimilarity measure\nthat satisfies a weak triangle inequality, including $k$-means, $M$-estimators,\nand $\\ell_p$ norms. The special case $t=1$ yields an optimal $O(k)$ space\nalgorithm for random-order streams as well as an optimal $O(nk)$ time algorithm\nin the RAM model, closing a long line of research on this problem.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 13:20:06 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Lang", "Harry", ""]]}, {"id": "1711.09400", "submitter": "Elham Taghizadeh", "authors": "Elham Taghizadeh and Mostafa Abedzadeh and Mostafa Setak", "title": "A Multi Objective Reliable Location-Inventory Capacitated Disruption\n  Facility Problem with Penalty Cost Solve with Efficient Meta Historic\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Logistics network is expected that opened facilities work continuously for a\nlong time horizon without any failure, but in real world problems, facilities\nmay face disruptions. This paper studies a reliable joint inventory location\nproblem to optimize the cost of facility locations, customers assignment, and\ninventory management decisions when facilities face failure risks and do not\nwork. In our model we assume when a facility is out of work, its customers may\nbe reassigned to other operational facilities otherwise they must endure high\npenalty costs associated with losing service. For defining the model closer to\nreal world problems, the model is proposed based on pmedian problem and the\nfacilities are considered to have limited capacities. We define a new binary\nvariable for showing that customers are not assigned to any facilities. Our\nproblem involves a biobjective model, the first one minimizes the sum of\nfacility construction costs and expected inventory holding costs, the second\none function that mentions for the first one is minimized maximum expected\ncustomer costs under normal and failure scenarios. For solving this model we\nuse NSGAII and MOSS algorithms have been applied to find the Pareto archive\nsolution. Also, Response Surface Methodology (RSM) is applied for optimizing\nthe NSGAII Algorithm Parameters. We compare the performance of two algorithms\nwith three metrics and the results show NSGAII is more suitable for our model.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 15:04:06 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Taghizadeh", "Elham", ""], ["Abedzadeh", "Mostafa", ""], ["Setak", "Mostafa", ""]]}, {"id": "1711.09457", "submitter": "Lior Eldar", "authors": "Lior Eldar, Saeed Mehraban", "title": "Approximating the Permanent of a Random Matrix with Vanishing Mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show an algorithm for computing the permanent of a random matrix with\nvanishing mean in quasi-polynomial time. Among special cases are the Gaussian,\nand biased-Bernoulli random matrices with mean 1/lnln(n)^{1/8}. In addition, we\ncan compute the permanent of a random matrix with mean 1/poly(ln(n)) in time\n2^{O(n^{\\eps})} for any small constant \\eps>0. Our algorithm counters the\nintuition that the permanent is hard because of the \"sign problem\" - namely the\ninterference between entries of a matrix with different signs. A major open\nquestion then remains whether one can provide an efficient algorithm for random\nmatrices of mean 1/poly(n), whose conjectured #P-hardness is one of the\nbaseline assumptions of the BosonSampling paradigm.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 20:55:35 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 18:27:50 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Eldar", "Lior", ""], ["Mehraban", "Saeed", ""]]}, {"id": "1711.09564", "submitter": "Girish Raguvir J", "authors": "Girish Raguvir J, Rahul Ramesh, Sachin Sridhar, Vignesh Manoharan", "title": "AUPCR Maximizing Matchings : Towards a Pragmatic Notion of Optimality\n  for One-Sided Preference Matchings", "comments": "AAAI-2018 Multidisciplinary Workshop on Advances in Preference\n  Handling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing a matching in a bipartite graph in the\npresence of one-sided preferences. There are several well studied notions of\noptimality which include pareto optimality, rank maximality, fairness and\npopularity. In this paper, we conduct an in-depth experimental study comparing\ndifferent notions of optimality based on a variety of metrics like cardinality,\nnumber of rank-1 edges, popularity, to name a few. Observing certain\nshortcomings in the standard notions of optimality, we propose an algorithm\nwhich maximizes an alternative metric called the Area under Profile Curve ratio\n(AUPCR). To the best of our knowledge, the AUPCR metric was used earlier but\nthere is no known algorithm to compute an AUPCR maximizing matching. Finally,\nwe illustrate the superiority of the AUPCR-maximizing matching by comparing its\nperformance against other optimal matchings on synthetic instances modeling\nreal-world data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 07:31:31 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 04:36:52 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["J", "Girish Raguvir", ""], ["Ramesh", "Rahul", ""], ["Sridhar", "Sachin", ""], ["Manoharan", "Vignesh", ""]]}, {"id": "1711.09859", "submitter": "Sankardeep Chakraborty", "authors": "Sankardeep Chakraborty, Anish Mukherjee, Venkatesh Raman, Srinivasa\n  Rao Satti", "title": "Frameworks for Designing In-place Graph Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Read-only memory model is a classical model of computation to study\ntime-space tradeoffs of algorithms. One of the classical results on the ROM\nmodel is that any sorting algorithm that uses O(s) words of extra space\nrequires $\\Omega (n^2/s)$ comparisons for $ \\lg n \\leq s \\leq n/\\lg n$ and the\nbound has also been recently matched by an algorithm. However, if we relax the\nmodel (from ROM), we do have sorting algorithms (say Heapsort) that can sort\nusing $O(n \\lg n)$ comparisons using $O(\\lg n)$ bits of extra space, even\nkeeping a permutation of the given input sequence at any point of time during\nthe algorithm.\n  We address similar questions for graph algorithms. We show that a simple\nnatural relaxation of ROM model allows us to implement fundamental graph search\nmethods like BFS and DFS more space efficiently than in ROM. By simply allowing\nelements in the adjacency list of a vertex to be permuted, we show that, on an\nundirected or directed connected graph $G$ having $n$ vertices and $m$ edges,\nthe vertices of $G$ can be output in a DFS or BFS order using $O(\\lg n)$ bits\nof extra space and $O(n^3 \\lg n)$ time. Thus we obtain similar bounds for\nreachability and shortest path distance (both for undirected and directed\ngraphs). With a little more (but still polynomial) time, we can also output\nvertices in the lex-DFS order. As reachability in directed graphs and shortest\npath distance are NL-complete, and lex-DFS is P-complete, our results show that\nour model is more powerful than ROM if L $\\neq$ P. En route, we also introduce\nand develop algorithms for another relaxation of ROM where the adjacency lists\nof the vertices are circular lists and we can modify only the heads of the\nlists. All our algorithms are simple but quite subtle, and we believe that\nthese models are practical enough to spur interest for other graph problems in\nthese models.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 18:10:01 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Chakraborty", "Sankardeep", ""], ["Mukherjee", "Anish", ""], ["Raman", "Venkatesh", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "1711.09964", "submitter": "Vaneet Aggarwal", "authors": "Vaneet Aggarwal and Tian Lan and Suresh Subramaniam and Maotong Xu", "title": "On the Approximability of Related Machine Scheduling under Arbitrary\n  Precedence", "comments": "Accepted to IEEE Transactions on Network and Service Management, Apr\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed computing systems often need to consider the scheduling problem\ninvolving a collection of highly dependent data-processing tasks that must work\nin concert to achieve mission-critical objectives. This paper considers the\nunrelated machine scheduling problem for minimizing weighted sum completion\ntime under arbitrary precedence constraints and on heterogeneous machines with\ndifferent processing speeds. The problem is known to be strongly NP-hard even\nin the single machine setting. By making use of Queyranne's constraint set and\nconstructing a novel Linear Programming relaxation for the scheduling problem\nunder arbitrary precedence constraints, our results in this paper advance the\nstate of the art. We develop a $2(1+(m-1)/D)$-approximation algorithm (and\n$2(1+(m-1)/D)+1$-approximation) for the scheduling problem with zero release\ntime (and arbitrary release time), where $m$ is the number of servers and $D$\nis the task-skewness product. The algorithm can be efficiently computed in\npolynomial time using the Ellipsoid method and achieves nearly optimal\nperformance in practice as $D>O(m)$ when the number of tasks per job to\nschedule is sufficiently larger than the number of machines available. Our\nimplementation and evaluation using a heterogeneous testbed and real-world\nbenchmarks confirms significant improvement in weighted sum completion time for\ndependent computing tasks.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 20:14:29 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 17:44:37 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Aggarwal", "Vaneet", ""], ["Lan", "Tian", ""], ["Subramaniam", "Suresh", ""], ["Xu", "Maotong", ""]]}, {"id": "1711.10051", "submitter": "Xue Chen", "authors": "Xue Chen and Eric Price", "title": "Active Regression via Linear-Sample Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach that improves the sample complexity for a variety of\ncurve fitting problems, including active learning for linear regression,\npolynomial regression, and continuous sparse Fourier transforms. In the active\nlinear regression problem, one would like to estimate the least squares\nsolution $\\beta^*$ minimizing $\\|X\\beta - y\\|_2$ given the entire unlabeled\ndataset $X \\in \\mathbb{R}^{n \\times d}$ but only observing a small number of\nlabels $y_i$. We show that $O(d)$ labels suffice to find a constant factor\napproximation $\\tilde{\\beta}$:\n  \\[\n  \\mathbb{E}[\\|X\\tilde{\\beta} - y\\|_2^2] \\leq 2 \\mathbb{E}[\\|X \\beta^* -\ny\\|_2^2].\n  \\] This improves on the best previous result of $O(d \\log d)$ from leverage\nscore sampling. We also present results for the \\emph{inductive} setting,\nshowing when $\\tilde{\\beta}$ will generalize to fresh samples; these apply to\ncontinuous settings such as polynomial regression. Finally, we show how the\ntechniques yield improved results for the non-linear sparse Fourier transform\nsetting.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 23:59:30 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 15:21:26 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2019 21:30:10 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Chen", "Xue", ""], ["Price", "Eric", ""]]}, {"id": "1711.10088", "submitter": "Martin F\\\"urer", "authors": "Mahdi Belbasi, Martin F\\\"urer", "title": "Saving Space by Dynamic Algebraization Based on Tree Decomposition:\n  Minimum Dominating Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm is presented that solves the Minimum Dominating Set problem\nexactly using polynomial space based on dynamic programming for a tree\ndecomposition. A direct application of dynamic programming based on a tree\ndecomposition would result in an exponential space algorithm, but we use zeta\ntransforms to obtain a polynomial space algorithm in exchange for a moderate\nincrease of the time. This framework was pioneered by Lokshtanov and Nederlof\n2010 and adapted to a dynamic setting by F\\\"urer and Yu 2017. Our\nspace-efficient algorithm is a parametrized algorithm based on tree-depth and\ntreewidth. The naive algorithm for Minimum Dominating Set runs in\n$\\mathcal{O}^*(2^n)$ time. Most of the previous works have focused on time\ncomplexity. But space optimization is a crucial aspect of algorithm design,\nsince in several scenarios space is a more valuable resource than time. Our\nparametrized algorithm runs in $\\mathcal{O}^*(3^{d})$, and its space complexity\nis $\\mathcal{O}(nk)$, where $d$ is the depth and $k$ is the width of the given\ntree decomposition. We observe that Reed's 1992 algorithm constructing a tree\ndecomposition of a graph uses only polynomial space. So, even if the tree\ndecomposition is not given, we still obtain an efficient polynomial space\nalgorithm. There are some other algorithms which use polynomial space for this\nproblem, but they are not efficient for graphs with small tree depth.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 02:35:59 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Belbasi", "Mahdi", ""], ["F\u00fcrer", "Martin", ""]]}, {"id": "1711.10155", "submitter": "Gregory Schwartzman", "authors": "Ken-ichi Kawarabayashi, Gregory Schwartzman", "title": "Adapting Local Sequential Algorithms to the Distributed Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a well known fact that sequential algorithms which exhibit a strong\n\"local\" nature can be adapted to the distributed setting given a legal graph\ncoloring. The running time of the distributed algorithm will then be at least\nthe number of colors. Surprisingly, this well known idea was never formally\nstated as a unified framework. In this paper we aim to define a robust family\nof local sequential algorithms which can be easily adapted to the distributed\nsetting. We then develop new tools to further enhance these algorithms,\nachieving state of the art results for fundamental problems.\n  We define a simple class of greedy-like algorithms which we call\n\\emph{orderless-local} algorithms. We show that given a legal $c$-coloring of\nthe graph, every algorithm in this family can be converted into a distributed\nalgorithm running in $O(c)$ communication rounds in the CONGEST model. We show\nthat this family is indeed robust as both the method of conditional\nexpectations and the unconstrained submodular maximization algorithm of\nBuchbinder \\etal \\cite{BuchbinderFNS15} can be expressed as orderless-local\nalgorithms for \\emph{local utility functions} --- Utility functions which have\na strong local nature to them.\n  We use the above algorithms as a base for new distributed approximation\nalgorithms for the weighted variants of some fundamental problems: Max $k$-Cut,\nMax-DiCut, Max 2-SAT and correlation clustering. We develop algorithms which\nhave the same approximation guarantees as their sequential counterparts, up to\na constant additive $\\epsilon$ factor, while achieving an $O(\\log^* n)$ running\ntime for deterministic algorithms and $O(\\epsilon^{-1})$ running time for\nrandomized ones. This improves exponentially upon the currently best known\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 07:18:21 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 02:40:10 GMT"}, {"version": "v3", "created": "Sat, 12 May 2018 08:16:46 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Kawarabayashi", "Ken-ichi", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "1711.10227", "submitter": "Vinod Reddy", "authors": "Bireswar Das, Murali Krishna Enduri, Neeldhara Misra, I. Vinod Reddy", "title": "On Structural Parameterizations of Firefighting", "comments": "19 pages, To be appeared in CALDAM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Firefighting problem is defined as follows. At time $t=0$, a fire breaks\nout at a vertex of a graph. At each time step $t \\geq 0$, a firefighter\npermanently defends (protects) an unburned vertex, and the fire then spread to\nall undefended neighbors from the vertices on fire. This process stops when the\nfire cannot spread anymore. The goal is to find a sequence of vertices for the\nfirefighter that maximizes the number of saved (non burned) vertices.\n  The Firefighting problem turns out to be NP-hard even when restricted to\nbipartite graphs or trees of maximum degree three. We study the parameterized\ncomplexity of the Firefighting problem for various structural\nparameterizations. All our parameters measure the distance to a graph class (in\nterms of vertex deletion) on which the firefighting problem admits a polynomial\ntime algorithm. Specifically, for a graph class $\\mathcal{F}$ and a graph $G$,\na vertex subset $S$ is called a modulator to $\\mathcal{F}$ if $G \\setminus S$\nbelongs to $\\mathcal{F}$. The parameters we consider are the sizes of\nmodulators to graph classes such as threshold graphs, bounded diameter graphs,\ndisjoint unions of stars, and split graphs.\n  To begin with, we show that the problem is W[1]-hard when parameterized by\nthe size of a modulator to diameter at most two graphs and split graphs. In\ncontrast to the above intractability results, we show that Firefighting is\nfixed parameter tractable (FPT) when parameterized by the size of a modulator\nto threshold graphs and disjoint unions of stars, which are subclasses of\ndiameter at most two graphs. We further investigate the kernelization\ncomplexity of these problems to find that firefighting admits a polynomial\nkernel when parameterized by the size of a modulator to a clique, while it is\nunlikely to admit a polynomial kernel when parameterized by the size of a\nmodulator to a disjoint union of stars.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 11:08:16 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Das", "Bireswar", ""], ["Enduri", "Murali Krishna", ""], ["Misra", "Neeldhara", ""], ["Reddy", "I. Vinod", ""]]}, {"id": "1711.10385", "submitter": "Szymon Grabowski", "authors": "Tomasz Kowalski, Szymon Grabowski", "title": "Faster range minimum queries", "comments": "A (very preliminary) version of the manuscript was presented in\n  Prague Stringology Conference 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Range Minimum Query (RMQ) is an important building brick of many compressed\ndata structures and string matching algorithms. Although this problem is\nessentially solved in theory, with sophisticated data structures allowing for\nconstant time queries, practical performance and construction time also matter.\nAdditionally, there are offline scenarios in which the number of queries, $q$,\nis rather small and given beforehand, which encourages to use a simpler\napproach. In this work, we present a simple data structure, with very fast\nconstruction, which allows to handle queries in constant time on average. This\nalgorithm, however, requires access to the input data during queries (which is\nnot the case of sophisticated RMQ solutions). We subsequently refine our\ntechnique, combining it with one of the existing succinct solutions with $O(1)$\nworst-case time queries and no access to the input array. The resulting hybrid\nis still a memory frugal data structure, spending usually up to about $3n$\nbits, and providing competitive query times, especially for wide ranges. We\nalso show how to make our baseline data structure more compact. Experimental\nresults demonstrate that the proposed BbST (Block-based Sparse Table) variants\nare competitive to existing solutions, also in the offline scenario.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 16:34:46 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Kowalski", "Tomasz", ""], ["Grabowski", "Szymon", ""]]}, {"id": "1711.10634", "submitter": "Ali Pinar", "authors": "Yusuf Ozkaya, A. Erdem Sariyuce, Umit V. Catalyurek, Ali Pinar", "title": "Active Betweenness Cardinality: Algorithms and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Centrality rankings such as degree, closeness, betweenness, Katz, PageRank,\netc. are commonly used to identify critical nodes in a graph. These methods are\nbased on two assumptions that restrict their wider applicability. First, they\nassume the exact topology of the network is available. Secondly, they do not\ntake into account the activity over the network and only rely on its topology.\nHowever, in many applications, the network is autonomous, vast, and\ndistributed, and it is hard to collect the exact topology. At the same time,\nthe underlying pairwise activity between node pairs is not uniform and node\ncriticality strongly depends on the activity on the underlying network.\n  In this paper, we propose active betweenness cardinality, as a new measure,\nwhere the node criticalities are based on not the static structure, but the\nactivity of the network. We show how this metric can be computed efficiently by\nusing only local information for a given node and how we can find the most\ncritical nodes starting from only a few nodes. We also show how this metric can\nbe used to monitor a network and identify failed nodes.We present experimental\nresults to show effectiveness by demonstrating how the failed nodes can be\nidentified by measuring active betweenness cardinality of a few nodes in the\nsystem.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 01:15:55 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Ozkaya", "Yusuf", ""], ["Sariyuce", "A. Erdem", ""], ["Catalyurek", "Umit V.", ""], ["Pinar", "Ali", ""]]}, {"id": "1711.10652", "submitter": "Rahul Vaze", "authors": "Rahul Vaze", "title": "Online Knapsack Problem under Expected Capacity Constraint", "comments": "To appear in IEEE INFOCOM 2018, April 2018, Honolulu HI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online knapsack problem is considered, where items arrive in a sequential\nfashion that have two attributes; value and weight. Each arriving item has to\nbe accepted or rejected on its arrival irrevocably. The objective is to\nmaximize the sum of the value of the accepted items such that the sum of their\nweights is below a budget/capacity. Conventionally a hard budget/capacity\nconstraint is considered, for which variety of results are available. In modern\napplications, e.g., in wireless networks, data centres, cloud computing, etc.,\nenforcing the capacity constraint in expectation is sufficient. With this\nmotivation, we consider the knapsack problem with an expected capacity\nconstraint. For the special case of knapsack problem, called the secretary\nproblem, where the weight of each item is unity, we propose an algorithm whose\nprobability of selecting any one of the optimal items is equal to $1-1/e$ and\nprovide a matching lower bound. For the general knapsack problem, we propose an\nalgorithm whose competitive ratio is shown to be $1/4e$ that is significantly\nbetter than the best known competitive ratio of $1/10e$ for the knapsack\nproblem with the hard capacity constraint.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 02:49:24 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Vaze", "Rahul", ""]]}, {"id": "1711.10665", "submitter": "Kai Han", "authors": "Kai Han, Yuntian He, Xiaokui Xiao, Shaojie Tang, Jingxin Xu, Liusheng\n  Huang", "title": "Cost-Effective Seed Selection in Online Social Networks", "comments": "submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the min-cost seed selection problem in online social networks, where\nthe goal is to select a set of seed nodes with the minimum total cost such that\nthe expected number of influenced nodes in the network exceeds a predefined\nthreshold. We propose several algorithms that outperform the previous studies\nboth on the theoretical approximation ratios and on the experimental\nperformance. Under the case where the nodes have heterogeneous costs, our\nalgorithms are the first bi- criteria approximation algorithms with polynomial\nrunning time and provable logarithmic performance bounds using a general\ncontagion model. Under the case where the users have uniform costs, our\nalgorithms achieve logarithmic approximation ratio and provable time complexity\nwhich is smaller than that of existing algorithms in orders of magnitude. We\nconduct extensive experiments using real social networks. The experimental\nresults show that, our algorithms significantly outperform the existing\nalgorithms both on the total cost and on the running time, and also scale well\nto billion-scale networks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 03:34:01 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 03:26:04 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Han", "Kai", ""], ["He", "Yuntian", ""], ["Xiao", "Xiaokui", ""], ["Tang", "Shaojie", ""], ["Xu", "Jingxin", ""], ["Huang", "Liusheng", ""]]}, {"id": "1711.10680", "submitter": "Tung-Wei Kuo", "authors": "Tung-Wei Kuo", "title": "On the Approximability and Hardness of the Minimum Connected Dominating\n  Set with Routing Cost Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the problem of minimum connected dominating set with routing cost\nconstraint, we are given a graph $G=(V,E)$, and the goal is to find the\nsmallest connected dominating set $D$ of $G$ such that, for any two\nnon-adjacent vertices $u$ and $v$ in $G$, the number of internal nodes on the\nshortest path between $u$ and $v$ in the subgraph of $G$ induced by $D \\cup\n\\{u,v\\}$ is at most $\\alpha$ times that in $G$. For general graphs, the only\nknown previous approximability result is an $O(\\log n)$-approximation algorithm\n($n=|V|$) for $\\alpha = 1$ by Ding et al. For any constant $\\alpha > 1$, we\ngive an $O(n^{1-\\frac{1}{\\alpha}}(\\log n)^{\\frac{1}{\\alpha}})$-approximation\nalgorithm. When $\\alpha \\geq 5$, we give an $O(\\sqrt{n}\\log n)$-approximation\nalgorithm. Finally, we prove that, when $\\alpha =2$, unless $NP \\subseteq\nDTIME(n^{poly\\log n})$, for any constant $\\epsilon > 0$, the problem admits no\npolynomial-time $2^{\\log^{1-\\epsilon}n}$-approximation algorithm, improving\nupon the $\\Omega(\\log n)$ bound by Du et al. (albeit under a stronger hardness\nassumption).\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 04:56:43 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 02:58:56 GMT"}, {"version": "v3", "created": "Fri, 9 Feb 2018 09:27:15 GMT"}, {"version": "v4", "created": "Sat, 17 Feb 2018 14:03:13 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Kuo", "Tung-Wei", ""]]}, {"id": "1711.10692", "submitter": "Amartya Shankha Biswas", "authors": "Amartya Shankha Biswas, Ronitt Rubinfeld, Anak Yodpinyanee", "title": "Local Access to Huge Random Objects through Partial Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an algorithm performing a computation on a huge random object. Is it\nnecessary to generate the entire object up front, or is it possible to provide\nquery access to the object and sample it incrementally \"on-the-fly\"? Such an\nimplementation should emulate the object by answering queries in a manner\nconsistent with a random instance sampled from the true distribution.\n  Our first set of results focus on undirected graphs with independent edge\nprobabilities, under certain assumptions. Then, we use this to obtain the first\nefficient implementations for the Erdos-Renyi model and the Stochastic Block\nmodel. As in previous local-access implementations for random graphs, we\nsupport Vertex-Pair and Next-Neighbor queries. We also introduce a new\nRandom-Neighbor query.\n  Next, we show how to implement random Catalan objects, specifically focusing\non Dyck paths (always positive random walks on the integer line). Here, we\nsupport Height queries to find the position of the walk, and First-Return\nqueries to find the time when the walk returns to a specified height. This in\nturn can be used to implement Next-Neighbor queries on random rooted/binary\ntrees, and Matching-Bracket queries on random well bracketed expressions.\n  Finally, we define a new model that: (1) allows multiple independent\nsimultaneous instantiations of the same implementation to be consistent with\neach other without communication (2) allows us to generate a richer class of\nrandom objects that do not have a succinct description. Specifically, we study\nuniformly random valid $q$-colorings of an input graph $G$ with max degree\n$\\Delta$. The distribution over valid colorings is specified via a \"huge\"\nunderlying graph $G$, that is far too large to be read in sub-linear time.\nInstead, we access $G$ through local neighborhood probes. We are able to answer\nqueries to the color of any vertex in sub-linear time for $q > 9\\Delta$.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 06:08:01 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 02:26:10 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 12:32:28 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Biswas", "Amartya Shankha", ""], ["Rubinfeld", "Ronitt", ""], ["Yodpinyanee", "Anak", ""]]}, {"id": "1711.10920", "submitter": "Ahad N. Zehmakan", "authors": "Bernd G\\\"artner and Ahad N. Zehmakan", "title": "(Biased) Majority Rule Cellular Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a graph $G=(V,E)$ and a random initial vertex-coloring, where each\nvertex is blue independently with probability $p_{b}$, and red with probability\n$p_r=1-p_b$. In each step, all vertices change their current color\nsynchronously to the most frequent color in their neighborhood and in case of a\ntie, a vertex conserves its current color; this model is called majority model.\nIf in case of a tie a vertex always chooses blue color, it is called biased\nmajority model. We are interested in the behavior of these deterministic\nprocesses, especially in a two-dimensional torus (i.e., cellular automaton with\n(biased) majority rule). In the present paper, as a main result we prove both\nmajority and biased majority cellular automata exhibit a threshold behavior\nwith two phase transitions. More precisely, it is shown that for a\ntwo-dimensional torus $T_{n,n}$, there are two thresholds $0\\leq p_1, p_2\\leq\n1$ such that $p_b \\ll p_1$, $p_1 \\ll p_b \\ll p_2$, and $p_2 \\ll p_b$ result in\nmonochromatic configuration by red, stable coexistence of both colors, and\nmonochromatic configuration by blue, respectively in $\\mathcal{O}(n^2)$ number\nof steps\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 22:45:35 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["G\u00e4rtner", "Bernd", ""], ["Zehmakan", "Ahad N.", ""]]}, {"id": "1711.10945", "submitter": "Genevieve Flaspohler", "authors": "Genevieve Flaspohler, Nicholas Roy, Yogesh Girdhar", "title": "Near-optimal irrevocable sample selection for periodic data streams with\n  applications to marine robotics", "comments": "8 pages, accepted for presentation in IEEE Int. Conf. on Robotics and\n  Automation, ICRA '18, Brisbane, Australia, May 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of monitoring spatiotemporal phenomena in real-time by\ndeploying limited sampling resources at locations of interest irrevocably and\nwithout knowledge of future observations. This task can be modeled as an\ninstance of the classical secretary problem. Although this problem has been\nstudied extensively in theoretical domains, existing algorithms require that\ndata arrive in random order to provide performance guarantees. These algorithms\nwill perform arbitrarily poorly on data streams such as those encountered in\nrobotics and environmental monitoring domains, which tend to have\nspatiotemporal structure. We focus on the problem of selecting representative\nsamples from phenomena with periodic structure and introduce a novel sample\nselection algorithm that recovers a near-optimal sample set according to any\nmonotone submodular utility function. We evaluate our algorithm on a seven-year\nenvironmental dataset collected at the Martha's Vineyard Coastal Observatory\nand show that it selects phytoplankton sample locations that are nearly optimal\nin an information-theoretic sense for predicting phytoplankton concentrations\nin locations that were not directly sampled. The proposed periodic secretary\nalgorithm can be used with theoretical performance guarantees in many real-time\nsensing and robotics applications for streaming, irrevocable sample selection\nfrom periodic data streams.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 16:28:36 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 16:32:24 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Flaspohler", "Genevieve", ""], ["Roy", "Nicholas", ""], ["Girdhar", "Yogesh", ""]]}, {"id": "1711.11150", "submitter": "Wouter Kuijper", "authors": "Wouter Kuijper", "title": "A Cooperative Proof of Work Scheme for Distributed Consensus Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a refinement to the well known, and widely used, proof-of-work\nscheme of zeroing a cryptographic hash. Our refinement allows multiple\nautonomous users to cooperate on the proof-of-work for their own transactions\nin order to bring about consensus on the order of said transactions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 23:26:41 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Kuijper", "Wouter", ""]]}, {"id": "1711.11316", "submitter": "Simon Bruggmann", "authors": "Simon Bruggmann, Rico Zenklusen", "title": "Submodular Maximization through the Lens of Linear Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simplex algorithm for linear programming is based on the fact that any\nlocal optimum with respect to the polyhedral neighborhood is also a global\noptimum. We show that a similar result carries over to submodular maximization.\nIn particular, every local optimum of a constrained monotone submodular\nmaximization problem yields a $1/2$-approximation, and we also present an\nappropriate extension to the non-monotone setting. However, reaching a local\noptimum quickly is a non-trivial task. Moreover, we describe a fast and very\ngeneral local search procedure that applies to a wide range of constraint\nfamilies, and unifies as well as extends previous methods. In our framework, we\nmatch known approximation guarantees while disentangling and simplifying\nprevious approaches. Moreover, despite its generality, we are able to show that\nour local search procedure is slightly faster than previous specialized\nmethods. Furthermore, we resolve an open question on the relation between\nlinear optimization and submodular maximization; namely, whether a linear\noptimization oracle may be enough to obtain strong approximation algorithms for\nsubmodular maximization. We show that this is not the case by providing an\nexample of a constraint family on a ground set of size $n$ for which, if only\ngiven a linear optimization oracle, any algorithm for submodular maximization\nwith a polynomial number of calls to the linear optimization oracle will have\nan approximation ratio of only $O ( \\frac{1}{\\sqrt{n}} \\cdot \\frac{\\log\nn}{\\log\\log n} )$.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 10:49:38 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Bruggmann", "Simon", ""], ["Zenklusen", "Rico", ""]]}, {"id": "1711.11354", "submitter": "Henning Sulzbach", "authors": "Nicolas Broutin and Henning Sulzbach", "title": "A limit field for orthogonal range searches in two-dimensional random\n  point search trees", "comments": "24 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the cost of general orthogonal range queries in random quadtrees.\nThe cost of a given query is encoded into a (random) function of four variables\nwhich characterize the coordinates of two opposite corners of the query\nrectangle. We prove that, when suitably shifted and rescaled, the random cost\nfunction converges uniformly in probability towards a random field that is\ncharacterized as the unique solution to a distributional fixed-point equation.\nWe also state similar results for $2$-d trees. Our results imply for instance\nthat the worst case query satisfies the same asymptotic estimates as a typical\nquery, and thereby resolve an old question of Chanzy, Devroye and Zamora-Cura\n[\\emph{Acta Inf.}, 37:355--383, 2000]\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 12:47:13 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 10:50:19 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Broutin", "Nicolas", ""], ["Sulzbach", "Henning", ""]]}, {"id": "1711.11392", "submitter": "Reza Alijani", "authors": "Reza Alijani, Siddhartha Banerjee, Sreenivas Gollapudi, Kostas\n  Kollias, Kamesh Munagala", "title": "Two-sided Facility Location", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the rise of many successful e-commerce\nmarketplace platforms like the Amazon marketplace, AirBnB, Uber/Lyft, and\nUpwork, where a central platform mediates economic transactions between buyers\nand sellers. Motivated by these platforms, we formulate a set of facility\nlocation problems that we term Two-sided Facility location. In our model,\nagents arrive at nodes in an underlying metric space, where the metric distance\nbetween any buyer and seller captures the quality of the corresponding match.\nThe platform posts prices and wages at the nodes, and opens a set of facilities\nto route the agents to. The agents at any facility are assumed to be matched.\nThe platform ensures high match quality by imposing a distance constraint\nbetween a node and the facilities it is routed to. It ensures high service\navailability by ensuring flow to the facility is at least a pre-specified lower\nbound. Subject to these constraints, the goal of the platform is to maximize\nthe social surplus (or gains from trade) subject to weak budget balance, i.e.,\nprofit being non-negative.\n  We present an approximation algorithm for this problem that yields a $(1 +\n\\epsilon)$ approximation to surplus for any constant $\\epsilon > 0$, while\nrelaxing the match quality (i.e., maximum distance of any match) by a constant\nfactor. We use an LP rounding framework that easily extends to other objectives\nsuch as maximizing volume of trade or profit.\n  We justify our models by considering a dynamic marketplace setting where\nagents arrive according to a stochastic process and have finite patience (or\ndeadlines) for being matched. We perform queueing analysis to show that for\npolicies that route agents to facilities and match them, ensuring a low\nabandonment probability of agents reduces to ensuring sufficient flow arrives\nat each facility.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 13:48:40 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 22:09:10 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Alijani", "Reza", ""], ["Banerjee", "Siddhartha", ""], ["Gollapudi", "Sreenivas", ""], ["Kollias", "Kostas", ""], ["Munagala", "Kamesh", ""]]}, {"id": "1711.11560", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne, Ilias Diakonikolas, Daniel M. Kane, Alistair\n  Stewart", "title": "Testing Conditional Independence of Discrete Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of testing \\emph{conditional independence} for discrete\ndistributions. Specifically, given samples from a discrete random variable $(X,\nY, Z)$ on domain $[\\ell_1]\\times[\\ell_2] \\times [n]$, we want to distinguish,\nwith probability at least $2/3$, between the case that $X$ and $Y$ are\nconditionally independent given $Z$ from the case that $(X, Y, Z)$ is\n$\\epsilon$-far, in $\\ell_1$-distance, from every distribution that has this\nproperty. Conditional independence is a concept of central importance in\nprobability and statistics with a range of applications in various scientific\ndomains. As such, the statistical task of testing conditional independence has\nbeen extensively studied in various forms within the statistics and\neconometrics communities for nearly a century. Perhaps surprisingly, this\nproblem has not been previously considered in the framework of distribution\nproperty testing and in particular no tester with sublinear sample complexity\nis known, even for the important special case that the domains of $X$ and $Y$\nare binary.\n  The main algorithmic result of this work is the first conditional\nindependence tester with {\\em sublinear} sample complexity for discrete\ndistributions over $[\\ell_1]\\times[\\ell_2] \\times [n]$. To complement our upper\nbounds, we prove information-theoretic lower bounds establishing that the\nsample complexity of our algorithm is optimal, up to constant factors, for a\nnumber of settings. Specifically, for the prototypical setting when $\\ell_1,\n\\ell_2 = O(1)$, we show that the sample complexity of testing conditional\nindependence (upper bound and matching lower bound) is\n  \\[\n  \\Theta\\left({\\max\\left(n^{1/2}/\\epsilon^2,\\min\\left(n^{7/8}/\\epsilon,n^{6/7}/\\epsilon^{8/7}\\right)\\right)}\\right)\\,.\n  \\]\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:30:02 GMT"}, {"version": "v2", "created": "Sun, 1 Jul 2018 19:59:50 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1711.11581", "submitter": "David Steurer", "authors": "Pravesh K. Kothari and David Steurer", "title": "Outlier-robust moment-estimation via sum-of-squares", "comments": "Fix references for robust mean estimation without exploiting\n  higher-order moments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop efficient algorithms for estimating low-degree moments of unknown\ndistributions in the presence of adversarial outliers. The guarantees of our\nalgorithms improve in many cases significantly over the best previous ones,\nobtained in recent works of Diakonikolas et al, Lai et al, and Charikar et al.\nWe also show that the guarantees of our algorithms match information-theoretic\nlower-bounds for the class of distributions we consider. These improved\nguarantees allow us to give improved algorithms for independent component\nanalysis and learning mixtures of Gaussians in the presence of outliers.\n  Our algorithms are based on a standard sum-of-squares relaxation of the\nfollowing conceptually-simple optimization problem: Among all distributions\nwhose moments are bounded in the same way as for the unknown distribution, find\nthe one that is closest in statistical distance to the empirical distribution\nof the adversarially-corrupted sample.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:54:33 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 22:24:18 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Kothari", "Pravesh K.", ""], ["Steurer", "David", ""]]}]