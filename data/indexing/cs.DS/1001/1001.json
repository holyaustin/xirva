[{"id": "1001.0338", "submitter": "Anil Hirani", "authors": "Tamal K. Dey, Anil N. Hirani, Bala Krishnamoorthy", "title": "Optimal Homologous Cycles, Total Unimodularity, and Linear Programming", "comments": "Earlier version of this paper appeared in the 42nd ACM Symposium on\n  Theory of Computing (STOC 2010). In this version we complete the\n  characterization in terms of Moebius complexes. Added more information to the\n  experimental results section. Fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a simplicial complex with weights on its simplices, and a nontrivial\ncycle on it, we are interested in finding the cycle with minimal weight which\nis homologous to the given one. Assuming that the homology is defined with\ninteger coefficients, we show the following : For a finite simplicial complex\n$K$ of dimension greater than $p$, the boundary matrix $[\\partial_{p+1}]$ is\ntotally unimodular if and only if $H_p(L, L_0)$ is torsion-free, for all pure\nsubcomplexes $L_0, L$ in $K$ of dimensions $p$ and $p+1$ respectively, where\n$L_0$ is a subset of $L$. Because of the total unimodularity of the boundary\nmatrix, we can solve the optimization problem, which is inherently an integer\nprogramming problem, as a linear program and obtain integer solution. Thus the\nproblem of finding optimal cycles in a given homology class can be solved in\npolynomial time. This result is surprising in the backdrop of a recent result\nwhich says that the problem is NP-hard under $\\mathbb{Z}_2$ coefficients which,\nbeing a field, is in general easier to deal with. One consequence of our\nresult, among others, is that one can compute in polynomial time an optimal\n2-cycle in a given homology class for any finite simplicial complex embedded in\n$\\mathbb{R}^3$. Our optimization approach can also be used for various related\nproblems, such as finding an optimal chain homologous to a given one when these\nare not cycles.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2010 23:30:11 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2010 05:01:44 GMT"}, {"version": "v3", "created": "Thu, 27 Jan 2011 15:04:10 GMT"}], "update_date": "2011-01-28", "authors_parsed": [["Dey", "Tamal K.", ""], ["Hirani", "Anil N.", ""], ["Krishnamoorthy", "Bala", ""]]}, {"id": "1001.0340", "submitter": "Stefan Kiefer", "authors": "Javier Esparza, Stefan Kiefer, Michael Luttenberger", "title": "Computing the Least Fixed Point of Positive Polynomial Systems", "comments": "This is a technical report that goes along with an article to appear\n  in SIAM Journal on Computing.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider equation systems of the form X_1 = f_1(X_1, ..., X_n), ..., X_n =\nf_n(X_1, ..., X_n) where f_1, ..., f_n are polynomials with positive real\ncoefficients. In vector form we denote such an equation system by X = f(X) and\ncall f a system of positive polynomials, short SPP. Equation systems of this\nkind appear naturally in the analysis of stochastic models like stochastic\ncontext-free grammars (with numerous applications to natural language\nprocessing and computational biology), probabilistic programs with procedures,\nweb-surfing models with back buttons, and branching processes. The least\nnonnegative solution mu f of an SPP equation X = f(X) is of central interest\nfor these models. Etessami and Yannakakis have suggested a particular version\nof Newton's method to approximate mu f.\n  We extend a result of Etessami and Yannakakis and show that Newton's method\nstarting at 0 always converges to mu f. We obtain lower bounds on the\nconvergence speed of the method. For so-called strongly connected SPPs we prove\nthe existence of a threshold k_f such that for every i >= 0 the (k_f+i)-th\niteration of Newton's method has at least i valid bits of mu f. The proof\nyields an explicit bound for k_f depending only on syntactic parameters of f.\nWe further show that for arbitrary SPP equations Newton's method still\nconverges linearly: there are k_f>=0 and alpha_f>0 such that for every i>=0 the\n(k_f+alpha_f i)-th iteration of Newton's method has at least i valid bits of mu\nf. The proof yields an explicit bound for alpha_f; the bound is exponential in\nthe number of equations, but we also show that it is essentially optimal.\nConstructing a bound for k_f is still an open problem. Finally, we also provide\na geometric interpretation of Newton's method for SPPs.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2010 17:44:41 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2010 11:49:40 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2010 16:57:00 GMT"}], "update_date": "2010-03-17", "authors_parsed": [["Esparza", "Javier", ""], ["Kiefer", "Stefan", ""], ["Luttenberger", "Michael", ""]]}, {"id": "1001.0393", "submitter": "Chinmay Karande", "authors": "Sourav Chakraborty, Nikhil Devanur, Chinmay Karande", "title": "Market Equilibrium with Transaction Costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identical products being sold at different prices in different locations is a\ncommon phenomenon. Price differences might occur due to various reasons such as\nshipping costs, trade restrictions and price discrimination. To model such\nscenarios, we supplement the classical Fisher model of a market by introducing\n{\\em transaction costs}. For every buyer $i$ and every good $j$, there is a\ntransaction cost of $\\cij$; if the price of good $j$ is $p_j$, then the cost to\nthe buyer $i$ {\\em per unit} of $j$ is $p_j + \\cij$. This allows the same good\nto be sold at different (effective) prices to different buyers.\n  We provide a combinatorial algorithm that computes $\\epsilon$-approximate\nequilibrium prices and allocations in\n$O\\left(\\frac{1}{\\epsilon}(n+\\log{m})mn\\log(B/\\epsilon)\\right)$ operations -\nwhere $m$ is the number goods, $n$ is the number of buyers and $B$ is the sum\nof the budgets of all the buyers.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2010 18:25:18 GMT"}, {"version": "v2", "created": "Fri, 30 Jul 2010 18:02:05 GMT"}], "update_date": "2010-08-02", "authors_parsed": [["Chakraborty", "Sourav", ""], ["Devanur", "Nikhil", ""], ["Karande", "Chinmay", ""]]}, {"id": "1001.0608", "submitter": "Francois Le Gall", "authors": "Fran\\c{c}ois Le Gall", "title": "An Efficient Quantum Algorithm for some Instances of the Group\n  Isomorphism Problem", "comments": "20 pages; this is the full version of a paper that will appear in the\n  Proceedings of the 27th International Symposium on Theoretical Aspects of\n  Computer Science (STACS 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of testing whether two finite groups\nare isomorphic. Whereas the case where both groups are abelian is well\nunderstood and can be solved efficiently, very little is known about the\ncomplexity of isomorphism testing for nonabelian groups. Le Gall has\nconstructed an efficient classical algorithm for a class of groups\ncorresponding to one of the most natural ways of constructing nonabelian groups\nfrom abelian groups: the groups that are extensions of an abelian group $A$ by\na cyclic group $Z_m$ with the order of $A$ coprime with $m$. More precisely,\nthe running time of that algorithm is almost linear in the order of the input\ngroups. In this paper we present a quantum algorithm solving the same problem\nin time polynomial in the logarithm of the order of the input groups. This\nalgorithm works in the black-box setting and is the first quantum algorithm\nsolving instances of the nonabelian group isomorphism problem exponentially\nfaster than the best known classical algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2010 12:31:34 GMT"}], "update_date": "2010-01-07", "authors_parsed": [["Gall", "Fran\u00e7ois Le", ""]]}, {"id": "1001.0639", "submitter": "Arnaud Labourel", "authors": "Jurek Czyzowicz, David Ilcinkas (LaBRI, INRIA Bordeaux - Sud-Ouest),\n  Arnaud Labourel (LaBRI), Andrzej Pelc", "title": "Optimal Exploration of Terrains with Obstacles", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-13731-0_1", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mobile robot represented by a point moving in the plane has to explore an\nunknown terrain with obstacles. Both the terrain and the obstacles are modeled\nas arbitrary polygons. We consider two scenarios: the unlimited vision, when\nthe robot situated at a point p of the terrain explores (sees) all points q of\nthe terrain for which the segment pq belongs to the terrain, and the limited\nvision, when we require additionally that the distance between p and q be at\nmost 1. All points of the terrain (except obstacles) have to be explored and\nthe performance of an exploration algorithm is measured by the length of the\ntrajectory of the robot. For unlimited vision we show an exploration algorithm\nwith complexity O(P + D?k), where P is the total perimeter of the terrain\n(including perimeters of obstacles), D is the diameter of the convex hull of\nthe terrain, and k is the number of obstacles. We do not assume knowledge of\nthese parameters. We also prove a matching lower bound showing that the above\ncomplexity is optimal, even if the terrain is known to the robot. For limited\nvision we show exploration algorithms with complexity O(P + A + ?Ak), where A\nis the area of the terrain (excluding obstacles). Our algorithms work either\nfor arbitrary terrains, if one of the parameters A or k is known, or for c-fat\nterrains, where c is any constant (unknown to the robot) and no additional\nknowledge is assumed. (A terrain T with obstacles is c-fat if R/r ? c, where R\nis the radius of the smallest disc containing T and r is the radius of the\nlargest disc contained in T .) We also prove a matching lower bound ?(P + A +\n?Ak) on the complexity of exploration for limited vision, even if the terrain\nis known to the robot.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2010 07:29:11 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Czyzowicz", "Jurek", "", "LaBRI, INRIA Bordeaux - Sud-Ouest"], ["Ilcinkas", "David", "", "LaBRI, INRIA Bordeaux - Sud-Ouest"], ["Labourel", "Arnaud", "", "LaBRI"], ["Pelc", "Andrzej", ""]]}, {"id": "1001.0821", "submitter": "Fedor Fomin", "authors": "Frederic Dorn, Fedor V. Fomin, Daniel Lokshtanov, Venkatesh Raman and\n  Saket Saurabh", "title": "Beyond Bidimensionality: Parameterized Subexponential Algorithms on\n  Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop two different methods to achieve subexponential time parameterized\nalgorithms for problems on sparse directed graphs. We exemplify our approaches\nwith two well studied problems.\n  For the first problem, {\\sc $k$-Leaf Out-Branching}, which is to find an\noriented spanning tree with at least $k$ leaves, we obtain an algorithm solving\nthe problem in time $2^{O(\\sqrt{k} \\log k)} n+ n^{O(1)}$ on directed graphs\nwhose underlying undirected graph excludes some fixed graph $H$ as a minor. For\nthe special case when the input directed graph is planar, the running time can\nbe improved to $2^{O(\\sqrt{k})}n + n^{O(1)}$. The second example is a\ngeneralization of the {\\sc Directed Hamiltonian Path} problem, namely {\\sc\n$k$-Internal Out-Branching}, which is to find an oriented spanning tree with at\nleast $k$ internal vertices. We obtain an algorithm solving the problem in time\n$2^{O(\\sqrt{k} \\log k)} + n^{O(1)}$ on directed graphs whose underlying\nundirected graph excludes some fixed apex graph $H$ as a minor. Finally, we\nobserve that for any $\\epsilon>0$, the {\\sc $k$-Directed Path} problem is\nsolvable in time $O((1+\\epsilon)^k n^{f(\\epsilon)})$, where $f$ is some\nfunction of $\\ve$.\n  Our methods are based on non-trivial combinations of obstruction theorems for\nundirected graphs, kernelization, problem specific combinatorial structures and\na layering technique similar to the one employed by Baker to obtain PTAS for\nplanar graphs.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2010 06:20:01 GMT"}], "update_date": "2010-01-07", "authors_parsed": [["Dorn", "Frederic", ""], ["Fomin", "Fedor V.", ""], ["Lokshtanov", "Daniel", ""], ["Raman", "Venkatesh", ""], ["Saurabh", "Saket", ""]]}, {"id": "1001.0824", "submitter": "Neelesh Khanna", "authors": "Neelesh Khanna Surender Baswana", "title": "Approximate Shortest Paths Avoiding a Failed Vertex: Optimal Size Data\n  Structures for Unweighted Graph", "comments": "12 pages STACS 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Let $G=(V,E)$ be any undirected graph on $V$ vertices and $E$ edges. A path\n$\\textbf{P}$ between any two vertices $u,v\\in V$ is said to be $t$-approximate\nshortest path if its length is at most $t$ times the length of the shortest\npath between $u$ and $v$. We consider the problem of building a compact data\nstructure for a given graph $G$ which is capable of answering the following\nquery for any $u,v,z\\in V$ and $t>1$:\n  Report $t$-approximate shortest path between $u$ and $v$ when vertex $z$\nfails\n  We present data structures for the single source as well all-pairs versions\nof this problem. Our data structures guarantee optimal query time. Most\nimpressive feature of our data structures is that their size {\\em nearly} match\nthe size of their best static counterparts.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2010 07:02:30 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2010 12:53:25 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Baswana", "Neelesh Khanna Surender", ""]]}, {"id": "1001.0827", "submitter": "Chris De Vries", "authors": "Christopher M. De Vries and Shlomo Geva", "title": "Document Clustering with K-tree", "comments": "12 pages, INEX 2008", "journal-ref": null, "doi": "10.1007/978-3-642-03761-0_43", "report-no": null, "categories": "cs.IR cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the approach taken to the XML Mining track at INEX 2008\nby a group at the Queensland University of Technology. We introduce the K-tree\nclustering algorithm in an Information Retrieval context by adapting it for\ndocument clustering. Many large scale problems exist in document clustering.\nK-tree scales well with large inputs due to its low complexity. It offers\npromising results both in terms of efficiency and quality. Document\nclassification was completed using Support Vector Machines.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2010 07:51:23 GMT"}], "update_date": "2010-01-07", "authors_parsed": [["De Vries", "Christopher M.", ""], ["Geva", "Shlomo", ""]]}, {"id": "1001.0830", "submitter": "Chris De Vries", "authors": "Christopher M. De Vries and Shlomo Geva", "title": "K-tree: Large Scale Document Clustering", "comments": "2 pages, SIGIR 2009", "journal-ref": null, "doi": "10.1145/1571941.1572094", "report-no": null, "categories": "cs.IR cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce K-tree in an information retrieval context. It is an efficient\napproximation of the k-means clustering algorithm. Unlike k-means it forms a\nhierarchy of clusters. It has been extended to address issues with sparse\nrepresentations. We compare performance and quality to CLUTO using document\ncollections. The K-tree has a low time complexity that is suitable for large\ndocument collections. This tree structure allows for efficient disk based\nimplementations where space requirements exceed that of main memory.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2010 07:43:31 GMT"}], "update_date": "2010-01-07", "authors_parsed": [["De Vries", "Christopher M.", ""], ["Geva", "Shlomo", ""]]}, {"id": "1001.0833", "submitter": "Chris De Vries", "authors": "Christopher M. De Vries and Lance De Vine and Shlomo Geva", "title": "Random Indexing K-tree", "comments": "8 pages, ADCS 2009; Hyperref and cleveref LaTeX packages conflicted.\n  Removed cleveref", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Indexing (RI) K-tree is the combination of two algorithms for\nclustering. Many large scale problems exist in document clustering. RI K-tree\nscales well with large inputs due to its low complexity. It also exhibits\nfeatures that are useful for managing a changing collection. Furthermore, it\nsolves previous issues with sparse document vectors when using K-tree. The\nalgorithms and data structures are defined, explained and motivated. Specific\nmodifications to K-tree are made for use with RI. Experiments have been\nexecuted to measure quality. The results indicate that RI K-tree improves\ndocument cluster quality over the original K-tree algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2010 08:03:20 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2010 02:46:22 GMT"}], "update_date": "2010-02-02", "authors_parsed": [["De Vries", "Christopher M.", ""], ["De Vine", "Lance", ""], ["Geva", "Shlomo", ""]]}, {"id": "1001.0889", "submitter": "Arnaud Labourel", "authors": "Jurek Czyzowicz, David Ilcinkas (LaBRI, INRIA Bordeaux - Sud-Ouest),\n  Arnaud Labourel (LaBRI), Andrzej Pelc", "title": "Asynchronous deterministic rendezvous in bounded terrains", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-13284-1_7", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two mobile agents (robots) have to meet in an a priori unknown bounded\nterrain modeled as a polygon, possibly with polygonal obstacles. Agents are\nmodeled as points, and each of them is equipped with a compass. Compasses of\nagents may be incoherent. Agents construct their routes, but the actual walk of\neach agent is decided by the adversary: the movement of the agent can be at\narbitrary speed, the agent may sometimes stop or go back and forth, as long as\nthe walk of the agent in each segment of its route is continuous, does not\nleave it and covers all of it. We consider several scenarios, depending on\nthree factors: (1) obstacles in the terrain are present, or not, (2) compasses\nof both agents agree, or not, (3) agents have or do not have a map of the\nterrain with their positions marked. The cost of a rendezvous algorithm is the\nworst-case sum of lengths of the agents' trajectories until their meeting. For\neach scenario we design a deterministic rendezvous algorithm and analyze its\ncost. We also prove lower bounds on the cost of any deterministic rendezvous\nalgorithm in each case. For all scenarios these bounds are tight.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2010 13:25:55 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Czyzowicz", "Jurek", "", "LaBRI, INRIA Bordeaux - Sud-Ouest"], ["Ilcinkas", "David", "", "LaBRI, INRIA Bordeaux - Sud-Ouest"], ["Labourel", "Arnaud", "", "LaBRI"], ["Pelc", "Andrzej", ""]]}, {"id": "1001.0890", "submitter": "Arnaud Labourel", "authors": "Jurek Czyzowicz, Arnaud Labourel (LaBRI), Andrzej Pelc", "title": "How to meet asynchronously (almost) everywhere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two mobile agents (robots) with distinct labels have to meet in an arbitrary,\npossibly infinite, unknown connected graph or in an unknown connected terrain\nin the plane. Agents are modeled as points, and the route of each of them only\ndepends on its label and on the unknown environment. The actual walk of each\nagent also depends on an asynchronous adversary that may arbitrarily vary the\nspeed of the agent, stop it, or even move it back and forth, as long as the\nwalk of the agent in each segment of its route is continuous, does not leave it\nand covers all of it. Meeting in a graph means that both agents must be at the\nsame time in some node or in some point inside an edge of the graph, while\nmeeting in a terrain means that both agents must be at the same time in some\npoint of the terrain. Does there exist a deterministic algorithm that allows\nany two agents to meet in any unknown environment in spite of this very\npowerfull adversary? We give deterministic rendezvous algorithms for agents\nstarting at arbitrary nodes of any anonymous connected graph (finite or\ninfinite) and for agents starting at any interior points with rational\ncoordinates in any closed region of the plane with path-connected interior.\nWhile our algorithms work in a very general setting ? agents can, indeed, meet\nalmost everywhere ? we show that none of the above few limitations imposed on\nthe environment can be removed. On the other hand, our algorithm also\nguarantees the following approximate rendezvous for agents starting at\narbitrary interior points of a terrain as above: agents will eventually get at\nan arbitrarily small positive distance from each other.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2010 13:27:31 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Czyzowicz", "Jurek", "", "LaBRI"], ["Labourel", "Arnaud", "", "LaBRI"], ["Pelc", "Andrzej", ""]]}, {"id": "1001.0920", "submitter": "Ocan Sankur", "authors": "Claire Mathieu, Ocan Sankur, Warren Schudy", "title": "Online Correlation Clustering", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We study the online clustering problem where data items arrive in an online\nfashion. The algorithm maintains a clustering of data items into similarity\nclasses. Upon arrival of v, the relation between v and previously arrived items\nis revealed, so that for each u we are told whether v is similar to u. The\nalgorithm can create a new cluster for v and merge existing clusters.\n  When the objective is to minimize disagreements between the clustering and\nthe input, we prove that a natural greedy algorithm is O(n)-competitive, and\nthis is optimal.\n  When the objective is to maximize agreements between the clustering and the\ninput, we prove that the greedy algorithm is .5-competitive; that no online\nalgorithm can be better than .834-competitive; we prove that it is possible to\nget better than 1/2, by exhibiting a randomized algorithm with competitive\nratio .5+c for a small positive fixed constant c.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2010 15:54:38 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2010 13:23:16 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Mathieu", "Claire", ""], ["Sankur", "Ocan", ""], ["Schudy", "Warren", ""]]}, {"id": "1001.0961", "submitter": "Charles Sauerbier", "authors": "Charles Sauerbier", "title": "Computing a Frobenius Coin Problem decision problem in O(n^2)", "comments": "7 pages, 0 figures; corrected misspelling of Chemakani's name,\n  reformated, added larger images of algorithm listings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expanding on recent results of another an algorithm is presented that\nprovides solution to the Frobenius Coin Problem in worst case O(n^2) in the\nmagnitude of the largest denomination.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2010 20:38:17 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2010 22:04:21 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2010 18:32:59 GMT"}], "update_date": "2010-01-08", "authors_parsed": [["Sauerbier", "Charles", ""]]}, {"id": "1001.1139", "submitter": "Gonzalo Abal", "authors": "G. Abal, R. Donangelo, F.L. Marquezino, R. Portugal", "title": "Spatial search in a honeycomb network", "comments": "10 pages, 2 figures; Minor typos corrected, one Reference added.\n  accepted in Math. Structures in Computer Science, special volume on Quantum\n  Computing", "journal-ref": "Mathematical Structures in Computer Science, v. 20, p. 999-1009,\n  2010", "doi": "10.1017/S0960129510000332", "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spatial search problem consists in minimizing the number of steps\nrequired to find a given site in a network, under the restriction that only\noracle queries or translations to neighboring sites are allowed. In this paper,\na quantum algorithm for the spatial search problem on a honeycomb lattice with\n$N$ sites and torus-like boundary conditions. The search algorithm is based on\na modified quantum walk on a hexagonal lattice and the general framework\nproposed by Ambainis, Kempe and Rivosh is used to show that the time complexity\nof this quantum search algorithm is $O(\\sqrt{N \\log N})$.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2010 19:22:59 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2010 18:57:06 GMT"}, {"version": "v3", "created": "Fri, 28 May 2010 19:39:15 GMT"}], "update_date": "2012-05-18", "authors_parsed": [["Abal", "G.", ""], ["Donangelo", "R.", ""], ["Marquezino", "F. L.", ""], ["Portugal", "R.", ""]]}, {"id": "1001.1210", "submitter": "Gianluca Della Vedova", "authors": "Paola Bonizzoni, Gianluca Della Vedova, Riccardo Dondi, Yuri Pirola,\n  Romeo Rizzi", "title": "Pure Parsimony Xor Haplotyping", "comments": null, "journal-ref": "IEEE/ACM Trans. on Computational Biology and Bioinformatics 7.4\n  (2010) 598-610", "doi": "10.1109/TCBB.2010.52", "report-no": null, "categories": "cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The haplotype resolution from xor-genotype data has been recently formulated\nas a new model for genetic studies. The xor-genotype data is a cheaply\nobtainable type of data distinguishing heterozygous from homozygous sites\nwithout identifying the homozygous alleles. In this paper we propose a\nformulation based on a well-known model used in haplotype inference: pure\nparsimony. We exhibit exact solutions of the problem by providing polynomial\ntime algorithms for some restricted cases and a fixed-parameter algorithm for\nthe general case. These results are based on some interesting combinatorial\nproperties of a graph representation of the solutions. Furthermore, we show\nthat the problem has a polynomial time k-approximation, where k is the maximum\nnumber of xor-genotypes containing a given SNP. Finally, we propose a heuristic\nand produce an experimental analysis showing that it scales to real-world large\ninstances taken from the HapMap project.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2010 07:55:44 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Bonizzoni", "Paola", ""], ["Della Vedova", "Gianluca", ""], ["Dondi", "Riccardo", ""], ["Pirola", "Yuri", ""], ["Rizzi", "Romeo", ""]]}, {"id": "1001.1231", "submitter": "Barna Saha", "authors": "Bernhard Haeupler, Barna Saha, and Aravind Srinivasan", "title": "New Constructive Aspects of the Lovasz Local Lemma", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lov\\'{a}sz Local Lemma (LLL) states that the probability that none of a\nset of \"bad\" events happens is nonzero if the probability of each event is\nsmall compared to the number of bad events it depends on. A series of results\nhave provided algorithms to efficiently construct structures whose existence is\n(non-constructively) guaranteed by the full asymmetric LLL, culminating in the\nrecent breakthrough of Moser & Tardos. We show that the output distribution of\nthe Moser-Tardos procedure has sufficient randomness, leading to two classes of\nalgorithmic applications. We first show that when an LLL application provides a\nsmall amount of slack, the running time of the Moser-Tardos algorithm is\npolynomial in the number of underlying independent variables (not events!), and\ncan thus be used to give efficient constructions in cases where the underlying\nproof applies the LLL to super-polynomially many events (or where finding a bad\nevent that holds is computationally hard). We demonstrate our method on\napplications including: the first constant-factor approximation algorithm for\nthe Santa Claus problem, as well as efficient algorithms for acyclic edge\ncoloring, non-repetitive graph colorings, and Ramsey-type graphs. Second, we\nshow applications to cases where a few of the bad events can hold, leading to\nthe first such algorithmic applications of the LLL: MAX $k$-SAT is an\nillustrative example of this.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2010 09:40:34 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2010 07:30:29 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2010 19:44:53 GMT"}, {"version": "v4", "created": "Mon, 31 May 2010 21:26:43 GMT"}, {"version": "v5", "created": "Sun, 2 Oct 2011 05:41:41 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Saha", "Barna", ""], ["Srinivasan", "Aravind", ""]]}, {"id": "1001.1373", "submitter": "Anna Blasiak", "authors": "Anna Blasiak, Robert Kleinberg", "title": "The Serializability of Network Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network coding theory studies the transmission of information in networks\nwhose vertices may perform nontrivial encoding and decoding operations on data\nas it passes through the network. The main approach to deciding the feasibility\nof network coding problems aims to reduce the problem to optimization over a\npolytope of entropic vectors subject to constraints imposed by the network\nstructure. In the case of directed acyclic graphs, these constraints are\ncompletely understood, but for general graphs the problem of enumerating them\nremains open: it is not known how to classify the constraints implied by a\nproperty that we call serializability, which refers to the absence of\nparadoxical circular dependencies in a network code.\n  In this work we initiate the first systematic study of the constraints\nimposed on a network code by serializability. We find that serializability\ncannot be detected solely by evaluating the Shannon entropy of edge sets in the\ngraph, but nevertheless, we give a polynomial-time algorithm that decides the\nserializability of a network code. We define a certificate of\nnon-serializability, called an information vortex, that plays a role in the\ntheory of serializability comparable to the role of fractional cuts in\nmulticommodity flow theory, including a type of min-max relation. Finally, we\nstudy the serializability deficit of a network code, defined as the minimum\nnumber of extra bits that must be sent in order to make it serializable. For\nlinear codes, we show that it is NP-hard to approximate this parameter within a\nconstant factor, and we demonstrate some surprising facts about the behavior of\nthis parameter under parallel composition of codes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2010 22:15:48 GMT"}], "update_date": "2010-01-12", "authors_parsed": [["Blasiak", "Anna", ""], ["Kleinberg", "Robert", ""]]}, {"id": "1001.1451", "submitter": "Mugurel Ionut Andreica", "authors": "Mugurel Ionut Andreica, Nicolae Tapus", "title": "Efficient Upload Bandwidth Estimation and Communication Resource\n  Allocation Techniques", "comments": "Proceedings of the 9th WSEAS International Conference on Multimedia,\n  Internet & Video Technologies (MIV), Budapest, Hungary, 3-5 September, 2009;\n  ISBN: 978-960-474-114-4 / ISSN: 1790-5109", "journal-ref": "Recent Advances in Signals & Systems, pp. 186-191, 2009", "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address two problems, for which we present novel, efficient,\nalgorithmic solutions. The first problem is motivated by practical situations\nand is concerned with the efficient estimation of the upload bandwidth of a\nmachine, particularly in the context of a peer-to-peer content sharing and\ndistribution application. The second problem is more of a theoretical nature\nand considers a constrained communication resource allocation situation.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2010 20:54:57 GMT"}], "update_date": "2010-01-12", "authors_parsed": [["Andreica", "Mugurel Ionut", ""], ["Tapus", "Nicolae", ""]]}, {"id": "1001.1454", "submitter": "Mugurel Ionut Andreica", "authors": "Madalina Ecaterina Andreica, Mugurel Ionut Andreica, Nicolae Cataniciu", "title": "Multidimensional Data Structures and Techniques for Efficient Decision\n  Making", "comments": null, "journal-ref": "Proc. of the 10th WSEAS Intl. Conf. on Mathematics and Computers\n  in Business and Economics (MCBE), pp. 249-254, Prague, Czech Republic, 23-25\n  March, 2009. (ISBN: 978-960-474-063-5 / ISSN: 1790-5109)", "doi": null, "report-no": null, "categories": "cs.CG cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present several novel efficient techniques and\nmultidimensional data structures which can improve the decision making process\nin many domains. We consider online range aggregation, range selection and\nrange weighted median queries; for most of them, the presented data structures\nand techniques can provide answers in polylogarithmic time. The presented\nresults have applications in many business and economic scenarios, some of\nwhich are described in detail in the paper.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2010 22:04:02 GMT"}], "update_date": "2010-01-12", "authors_parsed": [["Andreica", "Madalina Ecaterina", ""], ["Andreica", "Mugurel Ionut", ""], ["Cataniciu", "Nicolae", ""]]}, {"id": "1001.1470", "submitter": "Barna Saha", "authors": "Barna Saha, Aravind Srinivasan", "title": "A New Approximation Technique for Resource-Allocation Problems", "comments": "Journal version in Random Structures & Algorithms, Conference version\n  in Proceedings of Innovations in Computer Science (ICS) 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a rounding method based on random walks in polytopes, which leads\nto improved approximation algorithms and integrality gaps for several\nassignment problems that arise in resource allocation and scheduling. In\nparticular, it generalizes the work of Shmoys and Tardos on the generalized\nassignment problem to the setting where some jobs can be dropped. New\nconcentration bounds for random bipartite matching are developed as well.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2010 07:16:02 GMT"}, {"version": "v2", "created": "Sun, 17 Dec 2017 01:18:13 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Saha", "Barna", ""], ["Srinivasan", "Aravind", ""]]}, {"id": "1001.1526", "submitter": "Fatih Basciftci", "authors": "Fatih Basciftci, Sirzat Kahramanli", "title": "A Reduced Offset Based Method for Fast Computation of the Prime\n  Implicants Covering a Given Cube", "comments": "35 pages, no figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to generate prime implicants for a given cube (minterm), most of\nminimization methods increase the dimension of this cube by removing one\nliteral from it at a time. But there are two problems of exponential\ncomplexity. One of them is the selection of the order in which the literals are\nto be removed from the implicant at hand. The latter is the mechanism that\nchecks whether a tentative literal removal is acceptable. The reduced Offset\nconcept has been developed to avoid of these problems. This concept is based on\npositional-cube representation where each cube is represented by two n-bit\nstrings. We show that each reduced Off-cube may be represented by a single\nn-bit string and propose a set of bitwise operations to be performed on such\nstrings. The experiments on single-output benchmarks show that this approach\ncan significantly speed up the minimization process, improve the quality of its\nresults and reduce the amount of memory required for this aim.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2010 15:42:36 GMT"}], "update_date": "2010-01-12", "authors_parsed": [["Basciftci", "Fatih", ""], ["Kahramanli", "Sirzat", ""]]}, {"id": "1001.1565", "submitter": "Rajeev Raman", "authors": "Philip Bille, Gad M. Landau, Rajeev Raman, Kunihiko Sadakane,\n  Srinivasa Rao Satti, Oren Weimann", "title": "Random Access to Grammar Compressed Strings", "comments": "Preliminary version in SODA 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grammar based compression, where one replaces a long string by a small\ncontext-free grammar that generates the string, is a simple and powerful\nparadigm that captures many popular compression schemes. In this paper, we\npresent a novel grammar representation that allows efficient random access to\nany character or substring without decompressing the string.\n  Let $S$ be a string of length $N$ compressed into a context-free grammar\n$\\mathcal{S}$ of size $n$. We present two representations of $\\mathcal{S}$\nachieving $O(\\log N)$ random access time, and either $O(n\\cdot \\alpha_k(n))$\nconstruction time and space on the pointer machine model, or $O(n)$\nconstruction time and space on the RAM. Here, $\\alpha_k(n)$ is the inverse of\nthe $k^{th}$ row of Ackermann's function. Our representations also efficiently\nsupport decompression of any substring in $S$: we can decompress any substring\nof length $m$ in the same complexity as a single random access query and\nadditional $O(m)$ time. Combining these results with fast algorithms for\nuncompressed approximate string matching leads to several efficient algorithms\nfor approximate string matching on grammar-compressed strings without\ndecompression. For instance, we can find all approximate occurrences of a\npattern $P$ with at most $k$ errors in time $O(n(\\min\\{|P|k, k^4 + |P|\\} + \\log\nN) + occ)$, where $occ$ is the number of occurrences of $P$ in $S$. Finally, we\ngeneralize our results to navigation and other operations on grammar-compressed\nordered trees.\n  All of the above bounds significantly improve the currently best known\nresults. To achieve these bounds, we introduce several new techniques and data\nstructures of independent interest, including a predecessor data structure, two\n\"biased\" weighted ancestor data structures, and a compact representation of\nheavy paths in grammars.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2010 20:29:41 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2013 10:57:52 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2013 08:44:10 GMT"}], "update_date": "2013-10-30", "authors_parsed": [["Bille", "Philip", ""], ["Landau", "Gad M.", ""], ["Raman", "Rajeev", ""], ["Sadakane", "Kunihiko", ""], ["Satti", "Srinivasa Rao", ""], ["Weimann", "Oren", ""]]}, {"id": "1001.1654", "submitter": "Sebastian F. Walter", "authors": "S.F. Walter, L. Lehmann", "title": "Algorithmic Differentiation of Linear Algebra Functions with Application\n  in Optimum Experimental Design (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive algorithms for higher order derivative computation of the\nrectangular $QR$ and eigenvalue decomposition of symmetric matrices with\ndistinct eigenvalues in the forward and reverse mode of algorithmic\ndifferentiation (AD) using univariate Taylor propagation of matrices (UTPM).\nLinear algebra functions are regarded as elementary functions and not as\nalgorithms. The presented algorithms are implemented in the BSD licensed AD\ntool \\texttt{ALGOPY}. Numerical tests show that the UTPM algorithms derived in\nthis paper produce results close to machine precision accuracy. The theory\ndeveloped in this paper is applied to compute the gradient of an objective\nfunction motivated from optimum experimental design: $\\nabla_x\n\\Phi(C(J(F(x,y))))$, where $\\Phi = \\{\\lambda_1 : \\lambda_1 C\\}$, $C = (J^T\nJ)^{-1}$, $J = \\frac{\\dd F}{\\dd y}$ and $F = F(x,y)$.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2010 13:37:38 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2010 09:30:31 GMT"}], "update_date": "2010-02-19", "authors_parsed": [["Walter", "S. F.", ""], ["Lehmann", "L.", ""]]}, {"id": "1001.1686", "submitter": "Piotr Sankowski", "authors": "Amos Fiat, Stefano Leonardi, Jared Saia and Piotr Sankowski", "title": "Combinatorial Auctions with Budgets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider budget constrained combinatorial auctions where bidder $i$ has a\nprivate value $v_i$, a budget $b_i$, and is interested in all the items in\n$S_i$. The value to agent $i$ of a set of items $R$ is $|R \\cap S_i| \\cdot\nv_i$. Such auctions capture adword auctions, where advertisers offer a bid for\nads in response to an advertiser-dependent set of adwords, and advertisers have\nbudgets. It is known that even of all items are identical and all budgets are\npublic it is not possible to be truthful and efficient. Our main result is a\nnovel auction that runs in polynomial time, is incentive compatible, and\nensures Pareto-optimality for such auctions when the valuations are private and\nthe budgets are public knowledge. This extends the result of Dobzinski et al.\n(FOCS 2008) for auctions of multiple {\\sl identical} items and public budgets\nto single-valued {\\sl combinatorial} auctions with public budgets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2010 16:16:39 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2010 10:27:18 GMT"}], "update_date": "2010-04-21", "authors_parsed": [["Fiat", "Amos", ""], ["Leonardi", "Stefano", ""], ["Saia", "Jared", ""], ["Sankowski", "Piotr", ""]]}, {"id": "1001.1730", "submitter": "Jonathan Yedidia Dr.", "authors": "Jonathan S. Yedidia, Yige Wang, and Stark C. Draper", "title": "Divide & Concur and Difference-Map BP Decoders for LDPC Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"Divide and Concur'' (DC) algorithm, recently introduced by Gravel and\nElser, can be considered a competitor to the belief propagation (BP) algorithm,\nin that both algorithms can be applied to a wide variety of constraint\nsatisfaction, optimization, and probabilistic inference problems. We show that\nDC can be interpreted as a message-passing algorithm on a constraint graph,\nwhich helps make the comparison with BP more clear. The \"difference-map''\ndynamics of the DC algorithm enables it to avoid \"traps'' which may be related\nto the \"trapping sets'' or \"pseudo-codewords'' that plague BP decoders of\nlow-density parity check (LDPC) codes in the error-floor regime.\n  We investigate two decoders for low-density parity-check (LDPC) codes based\non these ideas. The first decoder is based directly on DC, while the second\ndecoder borrows the important \"difference-map'' concept from the DC algorithm\nand translates it into a BP-like decoder. We show that this \"difference-map\nbelief propagation'' (DMBP) decoder has dramatically improved error-floor\nperformance compared to standard BP decoders, while maintaining a similar\ncomputational complexity. We present simulation results for LDPC codes on the\nadditive white Gaussian noise and binary symmetric channels, comparing DC and\nDMBP decoders with other decoders based on BP, linear programming, and\nmixed-integer linear programming.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2010 22:19:54 GMT"}], "update_date": "2010-01-13", "authors_parsed": [["Yedidia", "Jonathan S.", ""], ["Wang", "Yige", ""], ["Draper", "Stark C.", ""]]}, {"id": "1001.1819", "submitter": "Rdv Ijcsis", "authors": "Sumitra Nuanmeesri, Chanasak Baitiang, Phayung Meesad", "title": "Genealogical Information Search by Using Parent Bidirectional Breadth\n  Algorithm and Rule Based Relationship", "comments": "6 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS December 2009, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 6, No. 3, pp. 001-006, December 2009, USA", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genealogical information is the best histories resources for culture study\nand cultural heritage. The genealogical research generally presents family\ninformation and depict tree diagram. This paper presents Parent Bidirectional\nBreadth Algorithm (PBBA) to find consanguine relationship between two persons.\nIn addition, the paper utilizes rules based system in order to identify\nconsanguine relationship. The study reveals that PBBA is fast to solve the\ngenealogical information search problem and the Rule Based Relationship\nprovides more benefits in blood relationship identification.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2010 08:48:17 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2010 12:56:40 GMT"}], "update_date": "2010-02-12", "authors_parsed": [["Nuanmeesri", "Sumitra", ""], ["Baitiang", "Chanasak", ""], ["Meesad", "Phayung", ""]]}, {"id": "1001.2101", "submitter": "Jouni Sir\\'en", "authors": "Jouni Sir\\'en", "title": "Sampled Longest Common Prefix Array", "comments": "This is a slightly extended version of the paper that was presented\n  at CPM 2010. The implementation is available at\n  http://www.cs.helsinki.fi/group/suds/rlcsa/", "journal-ref": null, "doi": "10.1007/978-3-642-13509-5_21", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When augmented with the longest common prefix (LCP) array and some other\nstructures, the suffix array can solve many string processing problems in\noptimal time and space. A compressed representation of the LCP array is also\none of the main building blocks in many compressed suffix tree proposals. In\nthis paper, we describe a new compressed LCP representation: the sampled LCP\narray. We show that when used with a compressed suffix array (CSA), the sampled\nLCP array often offers better time/space trade-offs than the existing\nalternatives. We also show how to construct the compressed representations of\nthe LCP array directly from a CSA.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2010 09:18:24 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2010 16:18:11 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2010 11:07:23 GMT"}], "update_date": "2010-06-30", "authors_parsed": [["Sir\u00e9n", "Jouni", ""]]}, {"id": "1001.2613", "submitter": "Aditya Bhaskara", "authors": "Aditya Bhaskara, Aravindan Vijayaraghavan", "title": "Approximating Matrix p-norms", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing the q->p norm of a matrix A, which is\ndefined for p,q \\ge 1, as |A|_{q->p} = max_{x !=0 } |Ax|_p / |x|_q. This is in\ngeneral a non-convex optimization problem, and is a natural generalization of\nthe well-studied question of computing singular values (this corresponds to\np=q=2). Different settings of parameters give rise to a variety of known\ninteresting problems (such as the Grothendieck problem when p=1 and q=\\infty).\nHowever, very little is understood about the approximability of the problem for\ndifferent values of p,q. Our first result is an efficient algorithm for\ncomputing the q->p norm of matrices with non-negative entries, when q \\ge p \\ge\n1. The algorithm we analyze is based on a natural fixed point iteration, which\ncan be seen as an analog of power iteration for computing eigenvalues. We then\npresent an application of our techniques to the problem of constructing a\nscheme for oblivious routing in the l_p norm. This makes constructive a recent\nexistential result of Englert and R\\\"acke [ER] on O(log n)-competitive\noblivious routing schemes (which they make constructive only for p=2). On the\nother hand, when we do not have any restrictions on the entries (such as\nnon-negativity), we prove that the problem is NP-hard to approximate to any\nconstant factor, for 2 < p \\le q, and p \\le q < 2 (these are precisely the\nranges of p,q with p\\le q, where constant factor approximations are not known).\nIn this range, our techniques also show that if NP does not have\nquasi-polynomial time algorithms, the q->p cannot be approximated to a factor\n2^{(log n)^{1-eps}}, for any \\eps>0.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2010 05:48:21 GMT"}, {"version": "v2", "created": "Sun, 2 May 2010 19:19:42 GMT"}], "update_date": "2010-05-04", "authors_parsed": [["Bhaskara", "Aditya", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1001.2767", "submitter": "Mangesh Gupte", "authors": "Mangesh Gupte and Mukund Sundararajan", "title": "Universally Optimal Privacy Mechanisms for Minimax Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A scheme that publishes aggregate information about sensitive data must\nresolve the trade-off between utility to information consumers and privacy of\nthe database participants. Differential privacy is a well-established\ndefinition of privacy--this is a universal guarantee against all attackers,\nwhatever their side-information or intent. In this paper, we present a\nuniversal treatment of utility based on the standard minimax rule from decision\ntheory (in contrast to the utility model in, which is Bayesian). In our model,\ninformation consumers are minimax (risk-averse) agents, each possessing some\nside-information about the query, and each endowed with a loss-function which\nmodels their tolerance to inaccuracies. Further, information consumers are\nrational in the sense that they actively combine information from the mechanism\nwith their side-information in a way that minimizes their loss. Under this\nassumption of rational behavior, we show that for every fixed count query, a\ncertain geometric mechanism is universally optimal for all minimax information\nconsumers. Additionally, our solution makes it possible to release query\nresults at multiple levels of privacy in a collusion-resistant manner.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2010 20:56:37 GMT"}], "update_date": "2010-01-18", "authors_parsed": [["Gupte", "Mangesh", ""], ["Sundararajan", "Mukund", ""]]}, {"id": "1001.2860", "submitter": "Djamal Belazzougui", "authors": "Djamal Belazzougui", "title": "Succinct Dictionary Matching With No Slowdown", "comments": "Corrected typos and other minor errors", "journal-ref": null, "doi": "10.1007/978-3-642-13509-5_9", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of dictionary matching is a classical problem in string matching:\ngiven a set S of d strings of total length n characters over an (not\nnecessarily constant) alphabet of size sigma, build a data structure so that we\ncan match in a any text T all occurrences of strings belonging to S. The\nclassical solution for this problem is the Aho-Corasick automaton which finds\nall occ occurrences in a text T in time O(|T| + occ) using a data structure\nthat occupies O(m log m) bits of space where m <= n + 1 is the number of states\nin the automaton. In this paper we show that the Aho-Corasick automaton can be\nrepresented in just m(log sigma + O(1)) + O(d log(n/d)) bits of space while\nstill maintaining the ability to answer to queries in O(|T| + occ) time. To the\nbest of our knowledge, the currently fastest succinct data structure for the\ndictionary matching problem uses space O(n log sigma) while answering queries\nin O(|T|log log n + occ) time. In this paper we also show how the space\noccupancy can be reduced to m(H0 + O(1)) + O(d log(n/d)) where H0 is the\nempirical entropy of the characters appearing in the trie representation of the\nset S, provided that sigma < m^epsilon for any constant 0 < epsilon < 1. The\nquery time remains unchanged.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2010 22:10:57 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2010 21:06:23 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Belazzougui", "Djamal", ""]]}, {"id": "1001.2862", "submitter": "Emden R. Gansner", "authors": "Emden R. Gansner, Yifan Hu, Stephen G. Kobourov", "title": "On Touching Triangle Graphs", "comments": "13 pages, 9 figures, 19 references, 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of representing graphs by triangles\nwhose sides touch. As a simple necessary condition, we show that pairs of\nvertices must have a small common neighborhood. On the positive side, we\npresent linear time algorithms for creating touching triangle representations\nfor outerplanar graphs, square grid graphs, and hexagonal grid graphs. We note\nthat this class of graphs is not closed under minors, making characterization\ndifficult. However, we present a complete characterization of the subclass of\nbiconnected graphs that can be represented as triangulations of some polygon.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2010 01:01:29 GMT"}], "update_date": "2010-01-19", "authors_parsed": [["Gansner", "Emden R.", ""], ["Hu", "Yifan", ""], ["Kobourov", "Stephen G.", ""]]}, {"id": "1001.2891", "submitter": "Eden Chlamtac", "authors": "Aditya Bhaskara, Moses Charikar, Eden Chlamtac, Uriel Feige and\n  Aravindan Vijayaraghavan", "title": "Detecting High Log-Densities -- an O(n^1/4) Approximation for Densest\n  k-Subgraph", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Densest k-Subgraph problem, given a graph G and a parameter k, one\nneeds to find a subgraph of G induced on k vertices that contains the largest\nnumber of edges. There is a significant gap between the best known upper and\nlower bounds for this problem. It is NP-hard, and does not have a PTAS unless\nNP has subexponential time algorithms. On the other hand, the current best\nknown algorithm of Feige, Kortsarz and Peleg, gives an approximation ratio of\nn^(1/3-epsilon) for some specific epsilon > 0 (estimated at around 1/60).\n  We present an algorithm that for every epsilon > 0 approximates the Densest\nk-Subgraph problem within a ratio of n^(1/4+epsilon) in time n^O(1/epsilon). In\nparticular, our algorithm achieves an approximation ratio of O(n^1/4) in time\nn^O(log n). Our algorithm is inspired by studying an average-case version of\nthe problem where the goal is to distinguish random graphs from graphs with\nplanted dense subgraphs. The approximation ratio we achieve for the general\ncase matches the distinguishing ratio we obtain for this planted problem.\n  At a high level, our algorithms involve cleverly counting appropriately\ndefined trees of constant size in G, and using these counts to identify the\nvertices of the dense subgraph. Our algorithm is based on the following\nprinciple. We say that a graph G(V,E) has log-density alpha if its average\ndegree is Theta(|V|^alpha). The algorithmic core of our result is a family of\nalgorithms that output k-subgraphs of nontrivial density whenever the\nlog-density of the densest k-subgraph is larger than the log-density of the\nhost graph.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2010 13:31:39 GMT"}], "update_date": "2010-01-19", "authors_parsed": [["Bhaskara", "Aditya", ""], ["Charikar", "Moses", ""], ["Chlamtac", "Eden", ""], ["Feige", "Uriel", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1001.2913", "submitter": "Mirela Damian", "authors": "Prosenjit Bose, Mirela Damian, Karim Douieb, Joseph O'Rourke, Ben\n  Seamone, Michiel Smid and Stefanie Wuhrer", "title": "Pi/2-Angle Yao Graphs are Spanners", "comments": "20 pages, 9 figures", "journal-ref": "International Journal of Computational Geometry & Applications,\n  22(1):61-82, 2012", "doi": "10.1142/S0218195912600047", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Yao graph Y4 in the L2 metric is a spanner with stretch\nfactor 8(29+23sqrt(2)). Enroute to this, we also show that the Yao graph Y4 in\nthe Linf metric is a planar spanner with stretch factor 8.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2010 18:38:36 GMT"}], "update_date": "2012-11-15", "authors_parsed": [["Bose", "Prosenjit", ""], ["Damian", "Mirela", ""], ["Douieb", "Karim", ""], ["O'Rourke", "Joseph", ""], ["Seamone", "Ben", ""], ["Smid", "Michiel", ""], ["Wuhrer", "Stefanie", ""]]}, {"id": "1001.2951", "submitter": "Haijun Zhou", "authors": "Haijun Zhou", "title": "Solution space heterogeneity of the random K-satisfiability problem:\n  Theory and simulations", "comments": "11 pages, 4 figures. Final version as will appear in Journal of\n  Physics: Conference Series (Proceedings of the International Workshop on\n  Statistical-Mechanical Informatics, March 7-10, 2010, Kyoto, Japan)", "journal-ref": null, "doi": "10.1088/1742-6596/233/1/012011", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random K-satisfiability (K-SAT) problem is an important problem for\nstudying typical-case complexity of NP-complete combinatorial satisfaction; it\nis also a representative model of finite-connectivity spin-glasses. In this\npaper we review our recent efforts on the solution space fine structures of the\nrandom K-SAT problem. A heterogeneity transition is predicted to occur in the\nsolution space as the constraint density alpha reaches a critical value\nalpha_cm. This transition marks the emergency of exponentially many solution\ncommunities in the solution space. After the heterogeneity transition the\nsolution space is still ergodic until alpha reaches a larger threshold value\nalpha_d, at which the solution communities disconnect from each other to become\ndifferent solution clusters (ergodicity-breaking). The existence of solution\ncommunities in the solution space is confirmed by numerical simulations of\nsolution space random walking, and the effect of solution space heterogeneity\non a stochastic local search algorithm SEQSAT, which performs a random walk of\nsingle-spin flips, is investigated. The relevance of this work to glassy\ndynamics studies is briefly mentioned.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2010 03:55:35 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2010 03:19:04 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Zhou", "Haijun", ""]]}, {"id": "1001.3044", "submitter": "Miroslaw Korzeniowski", "authors": "Marcin Bienkowski, Marek Klonowski, Miroslaw Korzeniowski, Dariusz R.\n  Kowalski", "title": "Dynamic sharing of a multiple access channel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the mutual exclusion problem on a multiple access\nchannel. Mutual exclusion is one of the fundamental problems in distributed\ncomputing. In the classic version of this problem, n processes perform a\nconcurrent program which occasionally triggers some of them to use shared\nresources, such as memory, communication channel, device, etc. The goal is to\ndesign a distributed algorithm to control entries and exits to/from the shared\nresource in such a way that in any time there is at most one process accessing\nit. We consider both the classic and a slightly weaker version of mutual\nexclusion, called ep-mutual-exclusion, where for each period of a process\nstaying in the critical section the probability that there is some other\nprocess in the critical section is at most ep. We show that there are channel\nsettings, where the classic mutual exclusion is not feasible even for\nrandomized algorithms, while ep-mutual-exclusion is. In more relaxed channel\nsettings, we prove an exponential gap between the makespan complexity of the\nclassic mutual exclusion problem and its weaker ep-exclusion version. We also\nshow how to guarantee fairness of mutual exclusion algorithms, i.e., that each\nprocess that wants to enter the critical section will eventually succeed.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2010 12:51:04 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2010 10:58:53 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Bienkowski", "Marcin", ""], ["Klonowski", "Marek", ""], ["Korzeniowski", "Miroslaw", ""], ["Kowalski", "Dariusz R.", ""]]}, {"id": "1001.3242", "submitter": "Gopal Pandurangan", "authors": "Jen-Yeu Chen and Gopal Pandurangan", "title": "Optimal Gossip-Based Aggregate Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first provably almost-optimal gossip-based algorithms for\naggregate computation that are both time optimal and message-optimal. Given a\n$n$-node network, our algorithms guarantee that all the nodes can compute the\ncommon aggregates (such as Min, Max, Count, Sum, Average, Rank etc.) of their\nvalues in optimal $O(\\log n)$ time and using $O(n \\log \\log n)$ messages. Our\nresult improves on the algorithm of Kempe et al. \\cite{kempe} that is\ntime-optimal, but uses $O(n \\log n)$ messages as well as on the algorithm of\nKashyap et al. \\cite{efficient-gossip} that uses $O(n \\log \\log n)$ messages,\nbut is not time-optimal (takes $O(\\log n \\log \\log n)$ time). Furthermore, we\nshow that our algorithms can be used to improve gossip-based aggregate\ncomputation in sparse communication networks, such as in peer-to-peer networks.\n  The main technical ingredient of our algorithm is a technique called {\\em\ndistributed random ranking (DRR)} that can be useful in other applications as\nwell. DRR gives an efficient distributed procedure to partition the network\ninto a forest of (disjoint) trees of small size.\n  Our algorithms are non-address oblivious. In contrast, we show a lower bound\nof $\\Omega(n\\log n)$ on the message complexity of any address-oblivious\nalgorithm for computing aggregates. This shows that non-address oblivious\nalgorithms are needed to obtain significantly better message complexity. Our\nlower bound holds regardless of the number of rounds taken or the size of the\nmessages used. Our lower bound is the first non-trivial lower bound for\ngossip-based aggregate computation and also gives the first formal proof that\ncomputing aggregates is strictly harder than rumor spreading in the\naddress-oblivious model.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2010 09:51:48 GMT"}], "update_date": "2010-01-20", "authors_parsed": [["Chen", "Jen-Yeu", ""], ["Pandurangan", "Gopal", ""]]}, {"id": "1001.3332", "submitter": "Dror Rawitz", "authors": "Reuven Bar-Yehuda, Danny Hermelin, Dror Rawitz", "title": "Minimum Vertex Cover in Rectangle Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Vertex Cover problem in intersection graphs of axis-parallel\nrectangles on the plane. We present two algorithms: The first is an EPTAS for\nnon-crossing rectangle families, rectangle families $\\calR$ where $R_1\n\\setminus R_2$ is connected for every pair of rectangles $R_1,R_2 \\in \\calR$.\nThis algorithm extends to intersection graphs of pseudo-disks. The second\nalgorithm achieves a factor of $(1.5 + \\varepsilon)$ in general rectangle\nfamilies, for any fixed $\\varepsilon > 0$, and works also for the weighted\nvariant of the problem. Both algorithms exploit the plane properties of\naxis-parallel rectangles in a non-trivial way.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2010 15:39:45 GMT"}], "update_date": "2010-01-20", "authors_parsed": [["Bar-Yehuda", "Reuven", ""], ["Hermelin", "Danny", ""], ["Rawitz", "Dror", ""]]}, {"id": "1001.3364", "submitter": "David Robillard", "authors": "David E. Robillard", "title": "Practical Parallel External Memory Algorithms via Simulation of Parallel\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis introduces PEMS2, an improvement to PEMS (Parallel External\nMemory System). PEMS executes Bulk-Synchronous Parallel (BSP) algorithms in an\nExternal Memory (EM) context, enabling computation with very large data sets\nwhich exceed the size of main memory. Many parallel algorithms have been\ndesigned and implemented for Bulk-Synchronous Parallel models of computation.\nSuch algorithms generally assume that the entire data set is stored in main\nmemory at once. PEMS overcomes this limitation without requiring any\nmodification to the algorithm by using disk space as memory for additional\n\"virtual processors\". Previous work has shown this to be a promising approach\nwhich scales well as computational resources (i.e. processors and disks) are\nadded. However, the technique incurs significant overhead when compared with\npurpose-built EM algorithms. PEMS2 introduces refinements to the simulation\nprocess intended to reduce this overhead as well as the amount of disk space\nrequired to run the simulation. New functionality is also introduced, including\nasynchronous I/O and support for multi-core processors. Experimental results\nshow that these changes significantly improve the runtime of the simulation.\nPEMS2 narrows the performance gap between simulated BSP algorithms and their\nhand-crafted EM counterparts, providing a practical system for using BSP\nalgorithms with data sets which exceed the size of RAM.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2010 17:24:08 GMT"}], "update_date": "2010-01-20", "authors_parsed": [["Robillard", "David E.", ""]]}, {"id": "1001.3480", "submitter": "Sebastian Roch", "authors": "Elchanan Mossel and Sebastien Roch and Allan Sly", "title": "On the inference of large phylogenies with long branches: How long is\n  too long?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CE cs.DS math.ST q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has highlighted deep connections between sequence-length\nrequirements for high-probability phylogeny reconstruction and the related\nproblem of the estimation of ancestral sequences. In [Daskalakis et al.'09],\nbuilding on the work of [Mossel'04], a tight sequence-length requirement was\nobtained for the CFN model. In particular the required sequence length for\nhigh-probability reconstruction was shown to undergo a sharp transition (from\n$O(\\log n)$ to $\\hbox{poly}(n)$, where $n$ is the number of leaves) at the\n\"critical\" branch length $\\critmlq$ (if it exists) of the ancestral\nreconstruction problem.\n  Here we consider the GTR model. For this model, recent results of [Roch'09]\nshow that the tree can be accurately reconstructed with sequences of length\n$O(\\log(n))$ when the branch lengths are below $\\critksq$, known as the\nKesten-Stigum (KS) bound. Although for the CFN model $\\critmlq = \\critksq$, it\nis known that for the more general GTR models one has $\\critmlq \\geq \\critksq$\nwith a strict inequality in many cases. Here, we show that this phenomenon also\nholds for phylogenetic reconstruction by exhibiting a family of symmetric\nmodels $Q$ and a phylogenetic reconstruction algorithm which recovers the tree\nfrom $O(\\log n)$-length sequences for some branch lengths in the range\n$(\\critksq,\\critmlq)$. Second we prove that phylogenetic reconstruction under\nGTR models requires a polynomial sequence-length for branch lengths above\n$\\critmlq$.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2010 07:34:33 GMT"}], "update_date": "2011-09-30", "authors_parsed": [["Mossel", "Elchanan", ""], ["Roch", "Sebastien", ""], ["Sly", "Allan", ""]]}, {"id": "1001.3493", "submitter": "William Jackson", "authors": "A. K.Ojha, K.K.Biswal", "title": "Posynomial Geometric Programming Problems with Multiple Parameters", "comments": null, "journal-ref": "Journal of Computing, Vol. 2, Issue 1, January 2010", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric programming problem is a powerful tool for solving some special\ntype non-linear programming problems. It has a wide range of applications in\noptimization and engineering for solving some complex optimization problems.\nMany applications of geometric programming are on engineering design problems\nwhere parameters are estimated using geometric programming. When the parameters\nin the problems are imprecise, the calculated objective value should be\nimprecise as well. In this paper we have developed a method to solve geometric\nprogramming problems where the exponent of the variables in the objective\nfunction, cost coefficients and right hand side are multiple parameters. The\nequivalent mathematical programming problems are formulated to find their\ncorresponding value of the objective function based on the duality theorem. By\napplying a variable separable technique the multi-choice mathematical\nprogramming problem is transformed into multiple one level geometric\nprogramming problem which produces multiple objective values that helps\nengineers to handle more realistic engineering design problems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2010 08:00:12 GMT"}], "update_date": "2010-03-25", "authors_parsed": [["Ojha", "A. K.", ""], ["Biswal", "K. K.", ""]]}, {"id": "1001.3713", "submitter": "Yuriy Reznik", "authors": "Yuriy A. Reznik", "title": "On Fast Algorithm for Computing Even-Length DCT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study recursive algorithm for computing DCT of lengths $N=q 2^m$ ($m,q \\in\n\\mathbb{N}$, $q$ is odd) due to C.W.Kok. We show that this algorithm has the\nsame multiplicative complexity as theoretically achievable by the prime factor\ndecomposition, when $m \\leqslant 2$. We also show that C.W.Kok's factorization\nallows a simple conversion to a scaled form. We analyze complexity of such a\nscaled factorization, and show that for some lengths it achieves lower\nmultiplicative complexity than one of known prime factor-based scaled\ntransforms.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2010 03:13:44 GMT"}], "update_date": "2010-01-22", "authors_parsed": [["Reznik", "Yuriy A.", ""]]}, {"id": "1001.3749", "submitter": "Sourav Chakraborty", "authors": "Eldar Fischer, Oded Lachish, Raphael Yuster", "title": "Two-phase algorithms for the parametric shortest path problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A {\\em parametric weighted graph} is a graph whose edges are labeled with\ncontinuous real functions of a single common variable. For any instantiation of\nthe variable, one obtains a standard edge-weighted graph. Parametric weighted\ngraph problems are generalizations of weighted graph problems, and arise in\nvarious natural scenarios. Parametric weighted graph algorithms consist of two\nphases. A {\\em preprocessing phase} whose input is a parametric weighted graph,\nand whose output is a data structure, the advice, that is later used by the\n{\\em instantiation phase}, where a specific value for the variable is given.\nThe instantiation phase outputs the solution to the (standard) weighted graph\nproblem that arises from the instantiation. The goal is to have the running\ntime of the instantiation phase supersede the running time of any algorithm\nthat solves the weighted graph problem from scratch, by taking advantage of the\nadvice.\n  In this paper we construct several parametric algorithms for the shortest\npath problem. For the case of linear function weights we present an algorithm\nfor the single source shortest path problem.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2010 09:35:46 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2010 12:34:18 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Fischer", "Eldar", ""], ["Lachish", "Oded", ""], ["Yuster", "Raphael", ""]]}, {"id": "1001.4003", "submitter": "Robert Bredereck", "authors": "Robert Bredereck", "title": "Fixed-Parameter Algorithms for Computing Kemeny Scores - Theory and\n  Practice", "comments": "Studienarbeit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The central problem in this work is to compute a ranking of a set of elements\nwhich is \"closest to\" a given set of input rankings of the elements. We define\n\"closest to\" in an established way as having the minimum sum of Kendall-Tau\ndistances to each input ranking. Unfortunately, the resulting problem Kemeny\nconsensus is NP-hard for instances with n input rankings, n being an even\ninteger greater than three. Nevertheless this problem plays a central role in\nmany rank aggregation problems. It was shown that one can compute the\ncorresponding Kemeny consensus list in f(k) + poly(n) time, being f(k) a\ncomputable function in one of the parameters \"score of the consensus\", \"maximum\ndistance between two input rankings\", \"number of candidates\" and \"average\npairwise Kendall-Tau distance\" and poly(n) a polynomial in the input size. This\nwork will demonstrate the practical usefulness of the corresponding algorithms\nby applying them to randomly generated and several real-world data. Thus, we\nshow that these fixed-parameter algorithms are not only of theoretical\ninterest. In a more theoretical part of this work we will develop an improved\nfixed-parameter algorithm for the parameter \"score of the consensus\" having a\nbetter upper bound for the running time than previous algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2010 14:19:04 GMT"}], "update_date": "2011-08-11", "authors_parsed": [["Bredereck", "Robert", ""]]}, {"id": "1001.4341", "submitter": "Dariusz Dereniowski", "authors": "Dariusz Dereniowski", "title": "Connected searching of weighted trees", "comments": null, "journal-ref": "Theoretical Computer Science 412 (2011) 5700-5713", "doi": "10.1016/j.tcs.2011.06.017", "report-no": "Technical Report no 21/2009, Faculty of Electronics,\n  Telecommunications and Informatics, Gdansk University of Technology", "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of connected edge searching of weighted\ntrees. It is shown that there exists a polynomial-time algorithm for finding\noptimal connected search strategy for bounded degree trees with arbitrary\nweights on the edges and vertices of the tree. The problem is NP-complete for\ngeneral node-weighted trees (the weight of each edge is 1).\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2010 18:30:47 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Dereniowski", "Dariusz", ""]]}, {"id": "1001.4420", "submitter": "Markus Jalsenius", "authors": "Raphael Clifford, Markus Jalsenius, Ashley Montanaro and Benjamin Sach", "title": "The Complexity of Flood Filling Games", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the popular one player combinatorial game known as\nFlood-It. In this game the player is given an n by n board of tiles where each\ntile is allocated one of c colours. The goal is to make the colours of all\ntiles equal via the shortest possible sequence of flooding operations. In the\nstandard version, a flooding operation consists of the player choosing a colour\nk, which then changes the colour of all the tiles in the monochromatic region\nconnected to the top left tile to k. After this operation has been performed,\nneighbouring regions which are already of the chosen colour k will then also\nbecome connected, thereby extending the monochromatic region of the board. We\nshow that finding the minimum number of flooding operations is NP-hard for c>=3\nand that this even holds when the player can perform flooding operations from\nany position on the board. However, we show that this \"free\" variant is in P\nfor c=2. We also prove that for an unbounded number of colours, Flood-It\nremains NP-hard for boards of height at least 3, but is in P for boards of\nheight 2. Next we show how a c-1 approximation and a randomised 2c/3\napproximation algorithm can be derived, and that no polynomial time constant\nfactor, independent of c, approximation algorithm exists unless P=NP. We then\ninvestigate how many moves are required for the \"most demanding\" n by n boards\n(those requiring the most moves) and show that the number grows as fast as\nTheta(n*c^0.5). Finally, we consider boards where the colours of the tiles are\nchosen at random and show that for c>=2, the number of moves required to flood\nthe whole board is Omega(n) with high probability.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2010 13:40:57 GMT"}, {"version": "v2", "created": "Thu, 19 Aug 2010 16:57:48 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2011 13:12:47 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Clifford", "Raphael", ""], ["Jalsenius", "Markus", ""], ["Montanaro", "Ashley", ""], ["Sach", "Benjamin", ""]]}, {"id": "1001.4493", "submitter": "Sandor P. Fekete", "authors": "Josef Angermeier, Sandor P. Fekete, Tom Kamphans, Nils Schweer,\n  Juergen Teich", "title": "Maintaining Virtual Areas on FPGAs using Strip Packing with Delays", "comments": "9 pages, 10 figures, 1 table, Latex, to appear in 17th Reconfigurable\n  Architectures Workshop (RAW 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every year, the computing resources available on dynamically partially\nreconfigurable devices increase enormously. In the near future, we expect many\napplications to run on a single reconfigurable device. In this paper, we\npresent a concept for multitasking on dynamically partially reconfigurable\nsystems called virtual area management. We explain its advantages, show its\nchallenges, and discuss possible solutions. Furthermore, we investigate one\nproblem in more detail: Packing modules with time-varying resource requests.\nThis problem from the reconfigurable computing field results in a completely\nnew optimization problem not tackled before. ILP-based and heuristic approaches\nare compared in an experimental study and the drawbacks and benefits discussed.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2010 17:36:32 GMT"}], "update_date": "2010-01-26", "authors_parsed": [["Angermeier", "Josef", ""], ["Fekete", "Sandor P.", ""], ["Kamphans", "Tom", ""], ["Schweer", "Nils", ""], ["Teich", "Juergen", ""]]}, {"id": "1001.4499", "submitter": "Tom\\'a\\v{s} Vina\\v{r}", "authors": "Michal N\\'an\\'asi, Tom\\'a\\v{s} Vina\\v{r}, Bro\\v{n}a Brejov\\'a", "title": "The Highest Expected Reward Decoding for HMMs with Application to\n  Recombination Detection", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-13509-5_16", "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models are traditionally decoded by the Viterbi algorithm which\nfinds the highest probability state path in the model. In recent years, several\nlimitations of the Viterbi decoding have been demonstrated, and new algorithms\nhave been developed to address them\n\\citep{Kall2005,Brejova2007,Gross2007,Brown2010}.\n  In this paper, we propose a new efficient highest expected reward decoding\nalgorithm (HERD) that allows for uncertainty in boundaries of individual\nsequence features. We demonstrate usefulness of our approach on jumping HMMs\nfor recombination detection in viral genomes.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2010 17:53:43 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["N\u00e1n\u00e1si", "Michal", ""], ["Vina\u0159", "Tom\u00e1\u0161", ""], ["Brejov\u00e1", "Bro\u0148a", ""]]}, {"id": "1001.5019", "submitter": "Siamak Tazari", "authors": "Stephan Kreutzer and Siamak Tazari", "title": "Lower Bounds for the Complexity of Monadic Second-Order Logic", "comments": "Preliminary version appeared in proceedings of the 25th IEEE\n  symposium on Logic in Computer Science (LICS'10), Edinburgh, Scotland, UK,\n  pp. 189-198, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Courcelle's famous theorem from 1990 states that any property of graphs\ndefinable in monadic second-order logic (MSO) can be decided in linear time on\nany class of graphs of bounded treewidth, or in other words, MSO is\nfixed-parameter tractable in linear time on any such class of graphs. From a\nlogical perspective, Courcelle's theorem establishes a sufficient condition, or\nan upper bound, for tractability of MSO-model checking.\n  Whereas such upper bounds on the complexity of logics have received\nsignificant attention in the literature, almost nothing is known about\ncorresponding lower bounds. In this paper we establish a strong lower bound for\nthe complexity of monadic second-order logic. In particular, we show that if C\nis any class of graphs which is closed under taking subgraphs and whose\ntreewidth is not bounded by a polylogarithmic function (in fact, $\\log^c n$ for\nsome small c suffices) then MSO-model checking is intractable on C (under a\nsuitable assumption from complexity theory).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2010 20:51:47 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2011 20:50:43 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Kreutzer", "Stephan", ""], ["Tazari", "Siamak", ""]]}, {"id": "1001.5076", "submitter": "Nitish Korula", "authors": "Jon Feldman, Monika Henzinger, Nitish Korula, Vahab S. Mirrokni, Cliff\n  Stein", "title": "Online Stochastic Packing Applied to Display Ad Allocation", "comments": "19 pages, 3 figures, 3 tables. The new version generalizes results to\n  a broader class of packing problems, and gives additional applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by online ad allocation, we study online stochastic packing linear\nprograms from theoretical and practical standpoints. We first present a\nnear-optimal online algorithm for a general class of packing linear programs\nwhich model various online resource allocation problems including online\nvariants of routing, ad allocations, generalized assignment, and combinatorial\nauctions. As our main theoretical result, we prove that a simple primal-dual\ntraining-based algorithm achieves a (1 - o(1))-approximation guarantee in the\nrandom order stochastic model. This is a significant improvement over\nlogarithmic or constant-factor approximations for the adversarial variants of\nthe same problems (e.g. factor 1 - 1/e for online ad allocation, and \\log m for\nonline routing). We then focus on the online display ad allocation problem and\nstudy the efficiency and fairness of various training-based and online\nallocation algorithms on data sets collected from real-life display ad\nallocation system. Our experimental evaluation confirms the effectiveness of\ntraining-based primal-dual algorithms on real data sets, and also indicate an\nintrinsic trade-off between fairness and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2010 00:51:03 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2010 00:03:38 GMT"}], "update_date": "2010-02-17", "authors_parsed": [["Feldman", "Jon", ""], ["Henzinger", "Monika", ""], ["Korula", "Nitish", ""], ["Mirrokni", "Vahab S.", ""], ["Stein", "Cliff", ""]]}, {"id": "1001.5272", "submitter": "Daniel Roche", "authors": "David Harvey (New York University) and Daniel S. Roche (University of\n  Waterloo)", "title": "An in-place truncated Fourier transform and applications to polynomial\n  multiplication", "comments": "5 pages, 1 figure, pdflatex", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The truncated Fourier transform (TFT) was introduced by van der Hoeven in\n2004 as a means of smoothing the \"jumps\" in running time of the ordinary FFT\nalgorithm that occur at power-of-two input sizes. However, the TFT still\nintroduces these jumps in memory usage. We describe in-place variants of the\nforward and inverse TFT algorithms, achieving time complexity O(n log n) with\nonly O(1) auxiliary space. As an application, we extend the second author's\nresults on space-restricted FFT-based polynomial multiplication to polynomials\nof arbitrary degree.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2010 21:10:41 GMT"}], "update_date": "2010-02-01", "authors_parsed": [["Harvey", "David", "", "New York University"], ["Roche", "Daniel S.", "", "University of\n  Waterloo"]]}]