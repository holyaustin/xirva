[{"id": "2012.00079", "submitter": "Micha{\\l} Pilipczuk", "authors": "Eduard Eiben and Robert Ganian and Du\\v{s}an Knop and Sebastian\n  Ordyniak and Micha{\\l} Pilipczuk and Marcin Wrochna", "title": "Integer Programming and Incidence Treedepth", "comments": "11 pages, 1 figure. This is an extended version of an article that\n  appeared at IPCO 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a strong connection has been shown between the tractability of\ninteger programming (IP) with bounded coefficients on the one side and the\nstructure of its constraint matrix on the other side. To that end, integer\nlinear programming is fixed-parameter tractable with respect to the primal (or\ndual) treedepth of the Gaifman graph of its constraint matrix and the largest\ncoefficient (in absolute value). Motivated by this, Kouteck\\'y, Levin, and Onn\n[ICALP 2018] asked whether it is possible to extend these result to a more\nbroader class of integer linear programs. More formally, is integer linear\nprogramming fixed-parameter tractable with respect to the incidence treedepth\nof its constraint matrix and the largest coefficient (in absolute value)?\n  We answer this question in negative. In particular, we prove that deciding\nthe feasibility of a system in the standard form, ${A\\mathbf{x} = \\mathbf{b}},\n{\\mathbf{l} \\le \\mathbf{x} \\le \\mathbf{u}}$, is $\\mathsf{NP}$-hard even when\nthe absolute value of any coefficient in $A$ is 1 and the incidence treedepth\nof $A$ is 5. Consequently, it is not possible to decide feasibility in\npolynomial time even if both the assumed parameters are constant, unless\n$\\mathsf{P}=\\mathsf{NP}$. Moreover, we complement this intractability result by\nshowing tractability for natural and only slightly more restrictive settings,\nnamely: (1) treedepth with an additional bound on either the maximum arity of\nconstraints or the maximum number of occurrences of variables and (2) the\nvertex cover number.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 20:03:11 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Eiben", "Eduard", ""], ["Ganian", "Robert", ""], ["Knop", "Du\u0161an", ""], ["Ordyniak", "Sebastian", ""], ["Pilipczuk", "Micha\u0142", ""], ["Wrochna", "Marcin", ""]]}, {"id": "2012.00086", "submitter": "Rico Zenklusen", "authors": "Federica Cecchetto and Vera Traub and Rico Zenklusen", "title": "Bridging the Gap Between Tree and Connectivity Augmentation: Unified and\n  Stronger Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Connectivity Augmentation Problem (CAP), a classical problem\nin the area of Survivable Network Design. It is about increasing the\nedge-connectivity of a graph by one unit in the cheapest possible way. More\nprecisely, given a $k$-edge-connected graph $G=(V,E)$ and a set of extra edges,\nthe task is to find a minimum cardinality subset of extra edges whose addition\nto $G$ makes the graph $(k+1)$-edge-connected. If $k$ is odd, the problem is\nknown to reduce to the Tree Augmentation Problem (TAP) -- i.e., $G$ is a\nspanning tree -- for which significant progress has been achieved recently,\nleading to approximation factors below $1.5$ (the currently best factor is\n$1.458$). However, advances on TAP did not carry over to CAP so far. Indeed,\nonly very recently, Byrka, Grandoni, and Ameli (STOC 2020) managed to obtain\nthe first approximation factor below $2$ for CAP by presenting a\n$1.91$-approximation algorithm based on a method that is disjoint from recent\nadvances for TAP.\n  We first bridge the gap between TAP and CAP, by presenting techniques that\nallow for leveraging insights and methods from TAP to approach CAP. We then\nintroduce a new way to get approximation factors below $1.5$, based on a new\nanalysis technique. Through these ingredients, we obtain a\n$1.393$-approximation algorithm for CAP, and therefore also TAP. This leads to\nthe currently best approximation result for both problems in a unified way, by\nsignificantly improving on the above-mentioned $1.91$-approximation for CAP and\nalso the previously best approximation factor of $1.458$ for TAP by Grandoni,\nKalaitzis, and Zenklusen (STOC 2018). Additionally, a feature we inherit from\nrecent TAP advances is that our approach can deal with the weighted setting\nwhen the ratio between the largest to smallest cost on extra links is bounded,\nin which case we obtain approximation factors below $1.5$.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 20:40:53 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Cecchetto", "Federica", ""], ["Traub", "Vera", ""], ["Zenklusen", "Rico", ""]]}, {"id": "2012.00127", "submitter": "William Kuszmaul", "authors": "William Kuszmaul and Alek Westover", "title": "The Variable-Processor Cup Game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of scheduling tasks on $p$ processors so that no task ever gets\ntoo far behind is often described as a game with cups and water. In the\n$p$-processor cup game on $n$ cups, there are two players, a filler and an\nemptier, that take turns adding and removing water from a set of $n$ cups. In\neach turn, the filler adds $p$ units of water to the cups, placing at most $1$\nunit of water in each cup, and then the emptier selects $p$ cups to remove up\nto $1$ unit of water from. The emptier's goal is to minimize the backlog, which\nis the height of the fullest cup.\n  The $p$-processor cup game has been studied in many different settings,\ndating back to the late 1960's. All of the past work shares one common\nassumption: that $p$ is fixed. This paper initiates the study of what happens\nwhen the number of available processors $p$ varies over time, resulting in what\nwe call the \\emph{variable-processor cup game}.\n  Remarkably, the optimal bounds for the variable-processor cup game differ\ndramatically from its classical counterpart. Whereas the $p$-processor cup has\noptimal backlog $\\Theta(\\log n)$, the variable-processor game has optimal\nbacklog $\\Theta(n)$. Moreover, there is an efficient filling strategy that\nyields backlog $\\Omega(n^{1 - \\epsilon})$ in quasi-polynomial time against any\ndeterministic emptying strategy.\n  We additionally show that straightforward uses of randomization cannot be\nused to help the emptier. In particular, for any positive constant $\\Delta$,\nand any $\\Delta$-greedy-like randomized emptying algorithm $\\mathcal{A}$, there\nis a filling strategy that achieves backlog $\\Omega(n^{1 - \\epsilon})$ against\n$\\mathcal{A}$ in quasi-polynomial time.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 21:58:41 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 19:13:20 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Kuszmaul", "William", ""], ["Westover", "Alek", ""]]}, {"id": "2012.00292", "submitter": "Anish Sevekari", "authors": "Wesley Pegden and Anish Sevekari", "title": "Comb inequalities for typical Euclidean TSP instances", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that even in average case, the Euclidean Traveling Salesman Problem\nexhibits an integrality gap of $(1+\\epsilon)$ for $\\epsilon>0$ when the\nHeld-Karp Linear Programming relaxation is augmented by all comb inequalities\nof bounded size. This implies that large classes of branch-and-cut algorithms\ntake exponential time for the Euclidean TSP, even on random inputs.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 06:25:21 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Pegden", "Wesley", ""], ["Sevekari", "Anish", ""]]}, {"id": "2012.00356", "submitter": "Lorenzo Severini", "authors": "Francesco Bonchi, Lorenzo Severini, Mauro Sozio", "title": "Better Fewer but Better: Community Search with Outliers", "comments": "2020 IEEE/WIC/ACM International Joint Conference on Web Intelligence\n  and Intelligent Agent Technology (WI-IAT'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of vertices in a network, that we believe are of interest for the\napplication under analysis, community search is the problem of producing a\nsubgraph potentially explaining the relationships existing among the vertices\nof interest. In practice this means that the solution should add some vertices\nto the query ones, so to create a connected subgraph that exhibits some\n\"cohesiveness\" property. This problem has received increasing attention in\nrecent years: while several cohesiveness functions have been studied, the bulk\nof the literature looks for a solution subgraphs containing all the query\nvertices. However, in many exploratory analyses we might only have a reasonable\nbelief about the vertices of interest: if only one of them is not really\nrelated to the others, forcing the solution to include all of them might hide\nthe existence of much more cohesive and meaningful subgraphs, that we could\nhave found by allowing the solution to detect and drop the outlier vertex. In\nthis paper we study the problem of community search with outliers, where we are\nallowed to drop up to $k$ query vertices, with $k$ being an input parameter. We\nconsider three of the most used measures of cohesiveness: the minimum degree,\nthe diameter of the subgraph and the maximum distance with a query vertex. By\noptimizing one and using one of the others as a constraint we obtain three\noptimization problems: we study their hardness and we propose different exact\nand approximation algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 09:31:17 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 09:46:58 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Bonchi", "Francesco", ""], ["Severini", "Lorenzo", ""], ["Sozio", "Mauro", ""]]}, {"id": "2012.00488", "submitter": "Leon Ladewig", "authors": "Susanne Albers and Leon Ladewig", "title": "New Results for the $k$-Secretary Problem", "comments": "Full and revised version of ISAAC 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Suppose that $n$ items arrive online in random order and the goal is to\nselect $k$ of them such that the expected sum of the selected items is\nmaximized. The decision for any item is irrevocable and must be made on arrival\nwithout knowing future items. This problem is known as the $k$-secretary\nproblem, which includes the classical secretary problem with the special case\n$k=1$. It is well-known that the latter problem can be solved by a simple\nalgorithm of competitive ratio $1/e$ which is optimal for $n \\to \\infty$.\nExisting algorithms beating the threshold of $1/e$ either rely on involved\nselection policies already for $k=2$, or assume that $k$ is large.\n  In this paper we present results for the $k$-secretary problem, considering\nthe interesting and relevant case that $k$ is small. We focus on simple\nselection algorithms, accompanied by combinatorial analyses. As a main\ncontribution we propose a natural deterministic algorithm designed to have\ncompetitive ratios strictly greater than $1/e$ for small $k \\geq 2$. This\nalgorithm is hardly more complex than the elegant strategy for the classical\nsecretary problem, optimal for $k=1$, and works for all $k \\geq 1$. We derive\nits competitive ratios for $k \\leq 100$, ranging from $0.41$ for $k=2$ to\n$0.75$ for $k=100$.\n  Moreover, we consider an algorithm proposed earlier in the literature, for\nwhich no rigorous analysis is known. We show that its competitive ratio is\n$0.4168$ for $k=2$, implying that the previous analysis was not tight. Our\nanalysis reveals a surprising combinatorial property of this algorithm, which\nmight be helpful to find a tight analysis for all $k$.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 13:49:28 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Albers", "Susanne", ""], ["Ladewig", "Leon", ""]]}, {"id": "2012.00497", "submitter": "Leon Ladewig", "authors": "Susanne Albers and Arindam Khan and Leon Ladewig", "title": "Improved Online Algorithms for Knapsack and GAP in the Random Order\n  Model", "comments": "Full and revised version of APPROX 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The knapsack problem is one of the classical problems in combinatorial\noptimization: Given a set of items, each specified by its size and profit, the\ngoal is to find a maximum profit packing into a knapsack of bounded capacity.\nIn the online setting, items are revealed one by one and the decision, if the\ncurrent item is packed or discarded forever, must be done immediately and\nirrevocably upon arrival. We study the online variant in the random order model\nwhere the input sequence is a uniform random permutation of the item set.\n  We develop a randomized (1/6.65)-competitive algorithm for this problem,\noutperforming the current best algorithm of competitive ratio 1/8.06\n[Kesselheim et al. SIAM J. Comp. 47(5)]. Our algorithm is based on two new\ninsights: We introduce a novel algorithmic approach that employs two given\nalgorithms, optimized for restricted item classes, sequentially on the input\nsequence. In addition, we study and exploit the relationship of the knapsack\nproblem to the 2-secretary problem.\n  The generalized assignment problem (GAP) includes, besides the knapsack\nproblem, several important problems related to scheduling and matching. We show\nthat in the same online setting, applying the proposed sequential approach\nyields a (1/6.99)-competitive randomized algorithm for GAP. Again, our proposed\nalgorithm outperforms the current best result of competitive ratio 1/8.06\n[Kesselheim et al. SIAM J. Comp. 47(5)].\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 14:08:00 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Albers", "Susanne", ""], ["Khan", "Arindam", ""], ["Ladewig", "Leon", ""]]}, {"id": "2012.00511", "submitter": "Leon Ladewig", "authors": "Susanne Albers and Arindam Khan and Leon Ladewig", "title": "Best Fit Bin Packing with Random Order Revisited", "comments": "Full version of MFCS 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Best Fit is a well known online algorithm for the bin packing problem, where\na collection of one-dimensional items has to be packed into a minimum number of\nunit-sized bins. In a seminal work, Kenyon [SODA 1996] introduced the\n(asymptotic) random order ratio as an alternative performance measure for\nonline algorithms. Here, an adversary specifies the items, but the order of\narrival is drawn uniformly at random. Kenyon's result establishes lower and\nupper bounds of 1.08 and 1.5, respectively, for the random order ratio of Best\nFit. Although this type of analysis model became increasingly popular in the\nfield of online algorithms, no progress has been made for the Best Fit\nalgorithm after the result of Kenyon.\n  We study the random order ratio of Best Fit and tighten the long-standing gap\nby establishing an improved lower bound of 1.10. For the case where all items\nare larger than 1/3, we show that the random order ratio converges quickly to\n1.25. It is the existence of such large items that crucially determines the\nperformance of Best Fit in the general case. Moreover, this case is closely\nrelated to the classical maximum-cardinality matching problem in the fully\nonline model. As a side product, we show that Best Fit satisfies a monotonicity\nproperty on such instances, unlike in the general case.\n  In addition, we initiate the study of the absolute random order ratio for\nthis problem. In contrast to asymptotic ratios, absolute ratios must hold even\nfor instances that can be packed into a small number of bins. We show that the\nabsolute random order ratio of Best Fit is at least 1.3. For the case where all\nitems are larger than 1/3, we derive upper and lower bounds of 21/16 and 1.2,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 14:23:08 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Albers", "Susanne", ""], ["Khan", "Arindam", ""], ["Ladewig", "Leon", ""]]}, {"id": "2012.00589", "submitter": "William Kuszmaul", "authors": "William Kuszmaul", "title": "Train Tracks with Gaps: Applying the Probabilistic Method to Trains", "comments": null, "journal-ref": "Fun With Algorithms, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify a tradeoff curve between the number of wheels on a train car, and\nthe amount of track that must be installed in order to ensure that the train\ncar is supported by the track at all times. The goal is to build an elevated\ntrack that covers some large distance $\\ell$, but that consists primarily of\ngaps, so that the total amount of feet of train track that is actually\ninstalled is only a small fraction of $\\ell$. In order so that the train track\ncan support the train at all points, the requirement is that as the train\ndrives across the track, at least one set of wheels from the rear quarter and\nat least one set of wheels from the front quarter of the train must be touching\nthe track at all times.\n  We show that, if a train car has $n$ sets of wheels evenly spaced apart in\nits rear and $n$ sets of wheels evenly spaced apart in its front, then it is\npossible to build a train track that supports the train car but uses only\n$\\Theta( \\ell / n )$ feet of track. We then consider what happens if the wheels\non the train car are not evenly spaced (and may even be configured\nadversarially). We show that for any configuration of the train car, with $n$\nwheels in each of the front and rear quarters of the car, it is possible to\nbuild a track that supports the car for distance $\\ell$ and uses only\n$O\\left(\\frac{\\ell \\log n}{n}\\right)$ feet of track. Additionally, we show that\nthere exist configurations of the train car for which this tradeoff curve is\nasymptotically optimal. Both the upper and lower bounds are achieved via\napplications of the probabilistic method.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 15:58:28 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Kuszmaul", "William", ""]]}, {"id": "2012.00689", "submitter": "Brendan Lucier", "authors": "Natalie Collina, Nicole Immorlica, Kevin Leyton-Brown, Brendan Lucier,\n  Neil Newman", "title": "Dynamic Weighted Matching with Heterogeneous Arrival and Departure Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a dynamic non-bipartite matching problem. There is a fixed set of\nagent types, and agents of a given type arrive and depart according to\ntype-specific Poisson processes. Agent departures are not announced in advance.\nThe value of a match is determined by the types of the matched agents. We\npresent an online algorithm that is (1/8)-competitive with respect to the value\nof the optimal-in-hindsight policy, for arbitrary weighted graphs. Our\nalgorithm treats agents heterogeneously, interpolating between immediate and\ndelayed matching in order to thicken the market while still matching valuable\nagents opportunistically.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 17:49:53 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 15:52:12 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2021 23:07:48 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Collina", "Natalie", ""], ["Immorlica", "Nicole", ""], ["Leyton-Brown", "Kevin", ""], ["Lucier", "Brendan", ""], ["Newman", "Neil", ""]]}, {"id": "2012.00738", "submitter": "Simina Br\\^anzei", "authors": "Simina Br\\^anzei and Dimitris Paparas and Nicholas J. Recker", "title": "Searching, Sorting, and Cake Cutting in Rounds", "comments": "33 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sorting and searching in rounds motivated by a cake cutting problem.\nThe search problem we consider is: we are given an array $x = (x_1, \\ldots,\nx_n)$ and an element $z$ promised to be in the array. We have access to an\noracle that answers comparison queries: \"How is $x_i$ compared to $x_j$?\",\nwhere the answer can be \"$<$\", \"$=$\", or \"$>$\". The goal is to find the\nlocation of $z$ with success probability at least $p \\in [0,1]$ in at most $k$\nrounds of interaction with the oracle. The problem is called ordered or\nunordered search, depending on whether the array $x$ is sorted or unsorted,\nrespectively.\n  For ordered search, we show the expected query complexity of randomized\nalgorithms is $\\Theta\\bigl(k\\cdot p \\cdot n^{1/k}\\bigr)$ in the worst case. In\ncontrast, the expected query complexity of deterministic algorithms searching\nfor a uniformly random element is $\\Theta\\bigl(k\\cdot p^{1/k} \\cdot\nn^{1/k}\\bigr)$. The uniform distribution is the worst case for deterministic\nalgorithms.\n  For unordered search, the expected query complexity of randomized algorithms\nis $np\\bigl(\\frac{k+1}{2k}\\bigr) \\pm 1$ in the worst case, while the expected\nquery complexity of deterministic algorithms searching for a uniformly random\nelement is $np \\bigl(1 - \\frac{k-1}{2k}p \\bigr) \\pm 1$.\n  We also discuss the connections of these search problems to the rank query\nmodel, where the array $x$ can be accessed via queries of the form \"Is\nrank$(x_i) \\leq k$?\". Unordered search is equivalent to Select with rank\nqueries (given $q$, find $x_i$ with rank $q$) and ordered search to Locate with\nrank queries (given $x_i$, find its rank). We show an equivalence between\nsorting with rank queries and proportional cake cutting with contiguous pieces\nfor any number of rounds, as well as an improved lower bound for deterministic\nsorting in rounds with rank queries.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 18:55:51 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 19:58:02 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Br\u00e2nzei", "Simina", ""], ["Paparas", "Dimitris", ""], ["Recker", "Nicholas J.", ""]]}, {"id": "2012.00854", "submitter": "Tim Roughgarden", "authors": "Tim Roughgarden", "title": "Transaction Fee Mechanism Design for the Ethereum Blockchain: An\n  Economic Analysis of EIP-1559", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.DS econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EIP-1559 is a proposal to make several tightly coupled additions to\nEthereum's transaction fee mechanism, including variable-size blocks and a\nburned base fee that rises and falls with demand. This report assesses the\ngame-theoretic strengths and weaknesses of the proposal and explores some\nalternative designs.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 21:48:57 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Roughgarden", "Tim", ""]]}, {"id": "2012.00866", "submitter": "Sai Vineeth K R", "authors": "R.C. Hillyard, Yunlu Liaozheng, Sai Vineeth K.R", "title": "Huskysort", "comments": "9 pages, Github repo for the algorithm included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much of the copious literature on the subject of sorting has concentrated on\nminimizing the number of comparisons and/or exchanges/copies. However, a more\nappropriate yardstick for the performance of sorting algorithms is based on the\ntotal number of array accesses that are required (the \"work\"). For a sort that\nis based on divide-and-conquer (including iterative variations on that theme),\nwe can divide the work into linear, i.e. $\\textbf{O}(N)$, work and\nlinearithmic, i.e. $\\textbf{O}(N log N)$, work. An algorithm that moves work\nfrom the linearithmic phase to the linear phase may be able to reduce the total\nnumber of array accesses and, indirectly, processing time. This paper describes\nan approach to sorting which reduces the number of expensive comparisons in the\nlinearithmic phase as much as possible by substituting inexpensive comparisons.\nIn Java, the two system sorts are dual-pivot quicksort (for primitives) and\nTimsort for objects. We demonstrate that a combination of these two algorithms\ncan run significantly faster than either algorithm alone for the types of\nobjects which are expensive to compare. We call this improved sorting algorithm\nHuskysort.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 22:07:59 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Hillyard", "R. C.", ""], ["Liaozheng", "Yunlu", ""], ["R", "Sai Vineeth K.", ""]]}, {"id": "2012.00883", "submitter": "Maximilian Schiffer", "authors": "Marianne Guillet, Gerhard Hiermann, Alexander Kr\\\"oller, Maximilian\n  Schiffer", "title": "Electric Vehicle Charging Station Search in Stochastic Environments", "comments": "38 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electric vehicles are a central component of future mobility systems as they\npromise to reduce local noxious and fine dust emissions and CO2 emissions, if\nfed by clean energy sources. However, the adoption of electric vehicles so far\nfell short of expectations despite significant governmental incentives. One\nreason for this slow adoption is the drivers' perceived range anxiety,\nespecially for individually owned vehicles. Here, bad user-experiences, e.g.,\nconventional cars blocking charging stations or inconsistent real-time\navailability data, manifest the drivers' range anxiety. Against this\nbackground, we study stochastic search algorithms, that can be readily deployed\nin today's navigation systems in order to minimize detours to reach an\navailable charging station. We model such a search as a finite horizon Markov\ndecision process and present a comprehensive framework that considers different\nproblem variants, speed-up techniques, and three solution algorithms: an exact\nlabeling algorithm, a heuristic labeling algorithm, and a rollout algorithm.\nExtensive numerical studies show that our algorithms significantly decrease the\nexpected time to find a free charging station while increasing the solution\nquality robustness and the likelihood that a search is successful compared to\nmyopic approaches.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 23:00:28 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Guillet", "Marianne", ""], ["Hiermann", "Gerhard", ""], ["Kr\u00f6ller", "Alexander", ""], ["Schiffer", "Maximilian", ""]]}, {"id": "2012.00959", "submitter": "Milutin Brankovic", "authors": "Milutin Brankovic, Joachim Gudmundsson, Andr\\'e van Renssen", "title": "Local Routing in a Tree Metric 1-Spanner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Solomon and Elkin constructed a shortcutting scheme for weighted trees which\nresults in a 1-spanner for the tree metric induced by the input tree. The\nspanner has logarithmic lightness, logarithmic diameter, a linear number of\nedges and bounded degree (provided the input tree has bounded degree). This\nspanner has been applied in a series of papers devoted to designing bounded\ndegree, low-diameter, low-weight $(1+\\epsilon)$-spanners in Euclidean and\ndoubling metrics. In this paper, we present a simple local routing algorithm\nfor this tree metric spanner. The algorithm has a routing ratio of 1, is\nguaranteed to terminate after $O(\\log n)$ hops and requires $O(\\Delta \\log n)$\nbits of storage per vertex where $\\Delta$ is the maximum degree of the tree on\nwhich the spanner is constructed. This local routing algorithm can be adapted\nto a local routing algorithm for a doubling metric spanner which makes use of\nthe shortcutting scheme.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 04:14:08 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Brankovic", "Milutin", ""], ["Gudmundsson", "Joachim", ""], ["van Renssen", "Andr\u00e9", ""]]}, {"id": "2012.01085", "submitter": "Youming Qiao", "authors": "Joshua A. Grochow, Youming Qiao, Gang Tang", "title": "Average-case algorithms for testing isomorphism of polynomials,\n  algebras, and multilinear forms", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problems of testing isomorphism of polynomials, algebras, and\nmultilinear forms. Our first main results are average-case algorithms for these\nproblems. For example, we develop an algorithm that takes two cubic forms $f,\ng\\in \\mathbb{F}_q[x_1,\\dots, x_n]$, and decides whether $f$ and $g$ are\nisomorphic in time $q^{O(n)}$ for most $f$. This average-case setting has\ndirect practical implications, having been studied in multivariate cryptography\nsince the 1990s. Our second result concerns the complexity of testing\nequivalence of alternating trilinear forms. This problem is of interest in both\nmathematics and cryptography. We show that this problem is polynomial-time\nequivalent to testing equivalence of symmetric trilinear forms, by showing that\nthey are both Tensor Isomorphism-complete (Grochow-Qiao, ITCS, 2021), therefore\nis equivalent to testing isomorphism of cubic forms over most fields.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 11:20:06 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Grochow", "Joshua A.", ""], ["Qiao", "Youming", ""], ["Tang", "Gang", ""]]}, {"id": "2012.01276", "submitter": "Shelby Kimmel", "authors": "Noel T. Anderson, Jay-U Chung, Shelby Kimmel", "title": "Leveraging Unknown Structure in Quantum Query Algorithms", "comments": "19 pages, v2: organization improved, typos fixed, function evaluation\n  error bound improved", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantum span program algorithms for function evaluation commonly have reduced\nquery complexity when promised that the input has a certain structure. We\ndesign a modified span program algorithm to show these speed-ups persist even\nwithout having a promise ahead of time, and we extend this approach to the more\ngeneral problem of state conversion. For example, there is a span program\nalgorithm that decides whether two vertices are connected in an $n$-vertex\ngraph with $O(n^{3/2})$ queries in general, but with $O(\\sqrt{k}n)$ queries if\npromised that, if there is a path, there is one with at most $k$ edges. Our\nalgorithm uses $\\tilde{O}(\\sqrt{k}n)$ queries to solve this problem if there is\na path with at most $k$ edges, without knowing $k$ ahead of time.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 15:32:52 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 13:43:48 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Anderson", "Noel T.", ""], ["Chung", "Jay-U", ""], ["Kimmel", "Shelby", ""]]}, {"id": "2012.01323", "submitter": "Johannes Klaus Fichte", "authors": "Johannes K. Fichte and Markus Hecher and Florim Hamiti", "title": "The Model Counting Competition 2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computational problems in modern society account to probabilistic\nreasoning, statistics, and combinatorics. A variety of these real-world\nquestions can be solved by representing the question in (Boolean) formulas and\nassociating the number of models of the formula directly with the answer to the\nquestion. Since there has been an increasing interest in practical problem\nsolving for model counting over the last years, the Model Counting (MC)\nCompetition was conceived in fall 2019. The competition aims to foster\napplications, identify new challenging benchmarks, and to promote new solvers\nand improve established solvers for the model counting problem and versions\nthereof. We hope that the results can be a good indicator of the current\nfeasibility of model counting and spark many new applications. In this paper,\nwe report on details of the Model Counting Competition 2020, about carrying out\nthe competition, and the results. The competition encompassed three versions of\nthe model counting problem, which we evaluated in separate tracks. The first\ntrack featured the model counting problem (MC), which asks for the number of\nmodels of a given Boolean formula. On the second track, we challenged\ndevelopers to submit programs that solve the weighted model counting problem\n(WMC). The last track was dedicated to projected model counting (PMC). In\ntotal, we received a surprising number of 9 solvers in 34 versions from 8\ngroups.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 16:52:07 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Fichte", "Johannes K.", ""], ["Hecher", "Markus", ""], ["Hamiti", "Florim", ""]]}, {"id": "2012.01499", "submitter": "Nikolai Karpov", "authors": "Nikolai Karpov, Qin Zhang", "title": "Instance-Sensitive Algorithms for Pure Exploration in Multinomial Logit\n  Bandit", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by real-world applications such as fast fashion retailing and\nonline advertising, the Multinomial Logit Bandit (MNL-bandit) is a popular\nmodel in online learning and operations research, and has attracted much\nattention in the past decade. However, it is a bit surprising that pure\nexploration, a basic problem in bandit theory, has not been well studied in\nMNL-bandit so far. In this paper we give efficient algorithms for pure\nexploration in MNL-bandit. Our algorithms achieve instance-sensitive pull\ncomplexities. We also complement the upper bounds by an almost matching lower\nbound.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 20:02:45 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Karpov", "Nikolai", ""], ["Zhang", "Qin", ""]]}, {"id": "2012.01752", "submitter": "Louis Esperet", "authors": "Nicolas Bousquet, Louis Esperet, Fran\\c{c}ois Pirot", "title": "Distributed algorithms for fractional coloring", "comments": "16 pages, 2 figures. Full version of a paper accepted at SIROCCO 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study fractional coloring from the angle of distributed\ncomputing. Fractional coloring is the linear relaxation of the classical notion\nof coloring, and has many applications, in particular in scheduling. It was\nproved by Hasemann, Hirvonen, Rybicki and Suomela (2016) that for every real\n$\\alpha>1$ and integer $\\Delta$, a fractional coloring of total weight at most\n$\\alpha(\\Delta+1)$ can be obtained deterministically in a single round in\ngraphs of maximum degree $\\Delta$, in the LOCAL model of computation. However,\na major issue of this result is that the output of each vertex has unbounded\nsize. Here we prove that even if we impose the more realistic assumption that\nthe output of each vertex has constant size, we can find fractional colorings\nof total weight arbitrarily close to known tight bounds for the fractional\nchromatic number in several cases of interest. More precisely, we show that for\nany fixed $\\epsilon > 0$ and $\\Delta$, a fractional coloring of total weight at\nmost $\\Delta+\\epsilon$ can be found in $O(\\log^*n)$ rounds in graphs of maximum\ndegree $\\Delta$ with no $K_{\\Delta+1}$, while finding a fractional coloring of\ntotal weight at most $\\Delta$ in this case requires $\\Omega(\\log \\log n)$\nrounds for randomized algorithms and $\\Omega( \\log n)$ rounds for deterministic\nalgorithms. We also show how to obtain fractional colorings of total weight at\nmost $2+\\epsilon$ in grids of any fixed dimension, for any $\\epsilon>0$, in\n$O(\\log^*n)$ rounds. Finally, we prove that in sparse graphs of large girth\nfrom any proper minor-closed family we can find a fractional coloring of total\nweight at most $2+\\epsilon$, for any $\\epsilon>0$, in $O(\\log n)$ rounds.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 08:37:14 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 10:57:46 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Bousquet", "Nicolas", ""], ["Esperet", "Louis", ""], ["Pirot", "Fran\u00e7ois", ""]]}, {"id": "2012.01764", "submitter": "Louis Esperet", "authors": "Marthe Bonamy, Louis Esperet, Carla Groenland, Alex Scott", "title": "Optimal labelling schemes for adjacency, comparability, and reachability", "comments": "17 pages - to appear in the proceedings of STOC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct asymptotically optimal adjacency labelling schemes for every\nhereditary class containing $2^{\\Omega(n^2)}$ $n$-vertex graphs as $n\\to\n\\infty$. This regime contains many classes of interest, for instance perfect\ngraphs or comparability graphs, for which we obtain an adjacency labelling\nscheme with labels of $n/4+o(n)$ bits per vertex. This implies the existence of\na reachability labelling scheme for digraphs with labels of $n/4+o(n)$ bits per\nvertex and comparability labelling scheme for posets with labels of $n/4+o(n)$\nbits per element. All these results are best possible, up to the lower order\nterm.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 08:51:44 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 20:15:15 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 07:40:14 GMT"}, {"version": "v4", "created": "Thu, 3 Jun 2021 14:34:21 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Bonamy", "Marthe", ""], ["Esperet", "Louis", ""], ["Groenland", "Carla", ""], ["Scott", "Alex", ""]]}, {"id": "2012.01845", "submitter": "Linh Anh Nguyen D.Sc.", "authors": "Linh Anh Nguyen", "title": "Computing Crisp Simulations and Crisp Directed Simulations for Fuzzy\n  Graph-Based Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like bisimulations, simulations and directed simulations are used for\nanalyzing graph-based structures such as automata, labeled transition systems,\nlinked data networks, Kripke models and interpretations in description logic.\nSimulations characterize the class of existential modal formulas, whereas\ndirected simulations characterize the class of positive modal formulas. These\nnotions are worth studying. For example, one may be interested in checking\nwhether a given finite automaton simulates another or whether an object in a\nlinked data network has all positive properties that another object has. To\ndeal with vagueness and uncertainty, fuzzy graph-based structures are used\ninstead of crisp ones. In this article, we design efficient algorithms with the\ncomplexity $O((m+n)n)$ for computing the largest crisp simulation and the\nlargest crisp directed simulation between two finite fuzzy labeled graphs,\nwhere $n$ is the number of vertices and $m$ is the number of nonzero edges of\nthe input fuzzy graphs. We also adapt them to computing the largest crisp\nsimulation and the largest crisp directed simulation between two finite fuzzy\nautomata.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 11:40:17 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Nguyen", "Linh Anh", ""]]}, {"id": "2012.01880", "submitter": "Arnab Maiti", "authors": "Arnab Maiti, Palash Dey", "title": "On Parameterized Complexity of Binary Networked Public Goods Game", "comments": "25 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Binary Networked Public Goods game, every player needs to decide if\nshe participates in a public project whose utility is shared equally by the\ncommunity. We study the problem of deciding if there exists a pure strategy\nNash equilibrium (PSNE) in such games. The problem is already known to be\nNP-complete. We provide fine-grained analysis of this problem under the lens of\nparameterized complexity theory. We consider various natural graph parameters\nand show either W[1]-hardness or exhibit an FPT algorithm. We finally exhibit\nsome special graph classes, for example path, cycle, bi-clique, complete graph,\netc., which always have a PSNE if the utility function of the players are fully\nhomogeneous.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 12:50:42 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 16:37:16 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 10:53:55 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Maiti", "Arnab", ""], ["Dey", "Palash", ""]]}, {"id": "2012.01883", "submitter": "Louis Abraham", "authors": "Louis Abraham", "title": "Competition analysis on the over-the-counter credit default swap market", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS econ.EM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two questions related to competition on the OTC CDS market using\ndata collected as part of the EMIR regulation.\n  First, we study the competition between central counterparties through\ncollateral requirements. We present models that successfully estimate the\ninitial margin requirements. However, our estimations are not precise enough to\nuse them as input to a predictive model for CCP choice by counterparties in the\nOTC market.\n  Second, we model counterpart choice on the interdealer market using a novel\nsemi-supervised predictive task. We present our methodology as part of the\nliterature on model interpretability before arguing for the use of conditional\nentropy as the metric of interest to derive knowledge from data through a\nmodel-agnostic approach. In particular, we justify the use of deep neural\nnetworks to measure conditional entropy on real-world datasets. We create the\n$\\textit{Razor entropy}$ using the framework of algorithmic information theory\nand derive an explicit formula that is identical to our semi-supervised\ntraining objective. Finally, we borrow concepts from game theory to define\n$\\textit{top-k Shapley values}$. This novel method of payoff distribution\nsatisfies most of the properties of Shapley values, and is of particular\ninterest when the value function is monotone submodular. Unlike classical\nShapley values, top-k Shapley values can be computed in quadratic time of the\nnumber of features instead of exponential. We implement our methodology and\nreport the results on our particular task of counterpart choice.\n  Finally, we present an improvement to the $\\textit{node2vec}$ algorithm that\ncould for example be used to further study intermediation. We show that the\nneighbor sampling used in the generation of biased walks can be performed in\nlogarithmic time with a quasilinear time pre-computation, unlike the current\nimplementations that do not scale well.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 13:02:53 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Abraham", "Louis", ""]]}, {"id": "2012.01942", "submitter": "Fuqi Song", "authors": "Fuqi Song and \\'Eric de la Clergerie", "title": "Clustering-based Automatic Construction of Legal Entity Knowledge Base\n  from Contracts", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": "10.1109/BigData50022.2020.9378166", "report-no": null, "categories": "cs.CL cs.AI cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In contract analysis and contract automation, a knowledge base (KB) of legal\nentities is fundamental for performing tasks such as contract verification,\ncontract generation and contract analytic. However, such a KB does not always\nexist nor can be produced in a short time. In this paper, we propose a\nclustering-based approach to automatically generate a reliable knowledge base\nof legal entities from given contracts without any supplemental references. The\nproposed method is robust to different types of errors brought by\npre-processing such as Optical Character Recognition (OCR) and Named Entity\nRecognition (NER), as well as editing errors such as typos. We evaluate our\nmethod on a dataset that consists of 800 real contracts with various qualities\nfrom 15 clients. Compared to the collected ground-truth data, our method is\nable to recall 84\\% of the knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 17:51:27 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 09:49:26 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Song", "Fuqi", ""], ["de la Clergerie", "\u00c9ric", ""]]}, {"id": "2012.02119", "submitter": "Ainesh Bakshi", "authors": "Ainesh Bakshi, Ilias Diakonikolas, He Jia, Daniel M. Kane, Pravesh K.\n  Kothari and Santosh S. Vempala", "title": "Robustly Learning Mixtures of $k$ Arbitrary Gaussians", "comments": "This version extends the previous one to yield 1) robust proper\n  learning algorithm with poly(eps) error and 2) an information theoretic\n  argument proving that the same algorithms in fact also yield parameter\n  recovery guarantees. The updates are included in Sections 7,8, and 9 and the\n  main result from the previous version (Thm 1.4) is presented and proved in\n  Section 6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give a polynomial-time algorithm for the problem of robustly estimating a\nmixture of $k$ arbitrary Gaussians in $\\mathbb{R}^d$, for any fixed $k$, in the\npresence of a constant fraction of arbitrary corruptions. This resolves the\nmain open problem in several previous works on algorithmic robust statistics,\nwhich addressed the special cases of robustly estimating (a) a single Gaussian,\n(b) a mixture of TV-distance separated Gaussians, and (c) a uniform mixture of\ntwo Gaussians. Our main tools are an efficient \\emph{partial clustering}\nalgorithm that relies on the sum-of-squares method, and a novel \\emph{tensor\ndecomposition} algorithm that allows errors in both Frobenius norm and low-rank\nterms.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 17:54:03 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 17:24:52 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 16:26:50 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bakshi", "Ainesh", ""], ["Diakonikolas", "Ilias", ""], ["Jia", "He", ""], ["Kane", "Daniel M.", ""], ["Kothari", "Pravesh K.", ""], ["Vempala", "Santosh S.", ""]]}, {"id": "2012.02213", "submitter": "Pedro Felzenszwalb", "authors": "Pedro Felzenszwalb, Caroline Klivans, Alice Paul", "title": "Iterated Linear Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a fixed point iteration process built on optimization of a\nlinear function over a compact domain. We prove the process always converges to\na fixed point and explore the set of fixed points in various convex sets. In\nparticular, we consider elliptopes and derive an algebraic characterization of\ntheir fixed points. We show that the attractive fixed points of an elliptope\nare exactly its vertices. Finally, we discuss how fixed point iteration can be\nused for rounding the solution of a semidefinite programming relaxation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 19:02:27 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 20:17:33 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Felzenszwalb", "Pedro", ""], ["Klivans", "Caroline", ""], ["Paul", "Alice", ""]]}, {"id": "2012.02226", "submitter": "Christian Coester", "authors": "Niv Buchbinder, Christian Coester, Joseph (Seffi) Naor", "title": "Online $k$-Taxi via Double Coverage and Time-Reverse Primal-Dual", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online $k$-taxi problem, a generalization of the $k$-server\nproblem, in which $k$ servers are located in a metric space. A sequence of\nrequests is revealed one by one, where each request is a pair of two points,\nrepresenting the start and destination of a travel request by a passenger. The\ngoal is to serve all requests while minimizing the distance traveled without\ncarrying a passenger.\n  We show that the classic Double Coverage algorithm has competitive ratio\n$2^k-1$ on HSTs, matching a recent lower bound for deterministic algorithms.\nFor bounded depth HSTs, the competitive ratio turns out to be much better and\nwe obtain tight bounds. When the depth is $d\\ll k$, these bounds are\napproximately $k^d/d!$. By standard embedding results, we obtain a randomized\nalgorithm for arbitrary $n$-point metrics with (polynomial) competitive ratio\n$O(k^c\\Delta^{1/c}\\log_{\\Delta} n)$, where $\\Delta$ is the aspect ratio and\n$c\\ge 1$ is an arbitrary positive integer constant. The only previous known\nbound was $O(2^k\\log n)$. For general (weighted) tree metrics, we prove the\ncompetitive ratio of Double Coverage to be $\\Theta(k^d)$ for any fixed depth\n$d$, but unlike on HSTs it is not bounded by $2^k-1$.\n  We obtain our results by a dual fitting analysis where the dual solution is\nconstructed step-by-step backwards in time. Unlike the forward-time approach\ntypical of online primal-dual analyses, this allows us to combine information\nfrom the past and the future when assigning dual variables. We believe this\nmethod can be useful also for other problems. Using this technique, we also\nprovide a dual fitting proof of the $k$-competitiveness of Double Coverage for\nthe $k$-server problem on trees.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 19:39:49 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Buchbinder", "Niv", "", "Seffi"], ["Coester", "Christian", "", "Seffi"], ["Joseph", "", "", "Seffi"], ["Naor", "", ""]]}, {"id": "2012.02243", "submitter": "Dmitriy Kunisky", "authors": "Afonso S. Bandeira, Dmitriy Kunisky, Alexander S. Wein", "title": "Average-Case Integrality Gap for Non-Negative Principal Component\n  Analysis", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Montanari and Richard (2015) asked whether a natural semidefinite programming\n(SDP) relaxation can effectively optimize $\\mathbf{x}^{\\top}\\mathbf{W}\n\\mathbf{x}$ over $\\|\\mathbf{x}\\| = 1$ with $x_i \\geq 0$ for all coordinates\n$i$, where $\\mathbf{W} \\in \\mathbb{R}^{n \\times n}$ is drawn from the Gaussian\northogonal ensemble (GOE) or a spiked matrix model. In small numerical\nexperiments, this SDP appears to be tight for the GOE, producing a rank-one\noptimal matrix solution aligned with the optimal vector $\\mathbf{x}$. We prove,\nhowever, that as $n \\to \\infty$ the SDP is not tight, and certifies an upper\nbound asymptotically no better than the simple spectral bound\n$\\lambda_{\\max}(\\mathbf{W})$ on this objective function. We also provide\nevidence, using tools from recent literature on hypothesis testing with\nlow-degree polynomials, that no subexponential-time certification algorithm can\nimprove on this behavior. Finally, we present further numerical experiments\nestimating how large $n$ would need to be before this limiting behavior becomes\nevident, providing a cautionary example against extrapolating asymptotics of\nSDPs in high dimension from their efficacy in small \"laptop scale\"\ncomputations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 20:19:56 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Bandeira", "Afonso S.", ""], ["Kunisky", "Dmitriy", ""], ["Wein", "Alexander S.", ""]]}, {"id": "2012.02363", "submitter": "Ge Xia", "authors": "Jianer Chen, Qin Huang, Iyad Kanj, Ge Xia", "title": "Near-Optimal Algorithms for Point-Line Covering Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study fundamental point-line covering problems in computational geometry,\nin which the input is a set $S$ of points in the plane. The first is the Rich\nLines problem, which asks for the set of all lines that each covers at least\n$\\lambda$ points from $S$, for a given integer parameter $\\lambda \\geq 2$; this\nproblem subsumes the 3-Points-on-Line problem and the Exact Fitting problem,\nwhich -- the latter -- asks for a line containing the maximum number of points.\nThe second is the NP-hard problem Line Cover, which asks for a set of $k$ lines\nthat cover the points of $S$, for a given parameter $k \\in \\mathbb{N}$. Both\nproblems have been extensively studied. In particular, the Rich Lines problem\nis a fundamental problem whose solution serves as a building block for several\nalgorithms in computational geometry.\n  For Rich Lines and Exact Fitting, we present a randomized Monte Carlo\nalgorithm that achieves a lower running time than that of Guibas et al.'s\nalgorithm [Computational Geometry 1996], for a wide range of the parameter\n$\\lambda$. We derive lower-bound results showing that, for $\\lambda\n=\\Omega(\\sqrt{n \\log n})$, the upper bound on the running time of this\nrandomized algorithm matches the lower bound that we derive on the time\ncomplexity of Rich Lines in the algebraic computation trees model.\n  For Line Cover, we present two kernelization algorithms: a randomized Monte\nCarlo algorithm and a deterministic algorithm. Both algorithms improve the\nrunning time of existing kernelization algorithms for Line Cover. We derive\nlower-bound results showing that the running time of the randomized algorithm\nwe present comes close to the lower bound we derive on the time complexity of\nkernelization algorithms for Line Cover in the algebraic computation trees\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 02:08:40 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 15:18:01 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2021 15:00:31 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Chen", "Jianer", ""], ["Huang", "Qin", ""], ["Kanj", "Iyad", ""], ["Xia", "Ge", ""]]}, {"id": "2012.02551", "submitter": "Pascal Su", "authors": "Rajko Nenadov, Angelika Steger and Pascal Su", "title": "An O(n) time algorithm for finding Hamilton cycles with high probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a randomized algorithm that finds a Hamilton cycle in\n$\\mathcal{O}(n)$ time with high probability in a random graph $G_{n,p}$ with\nedge probability $p\\ge C \\log n / n$. This closes a gap left open in a seminal\npaper by Angluin and Valiant from 1979.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 12:21:56 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Nenadov", "Rajko", ""], ["Steger", "Angelika", ""], ["Su", "Pascal", ""]]}, {"id": "2012.02617", "submitter": "Shmuel Onn", "authors": "Shmuel Onn", "title": "On Line Sum Optimization", "comments": null, "journal-ref": "Linear Algebra and its Applications, 610:474--479, 2021", "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the {\\em column sum optimization problem}, of finding a\n$(0,1)$-matrix with prescribed row sums which minimizes the sum of evaluations\nof given functions at its column sums, can be solved in polynomial time, either\nwhen all functions are the same or when all row sums are bounded by any\nconstant. We conjecture that the more general {\\em line sum optimization\nproblem}, of finding a matrix minimizing the sum of given functions evaluated\nat its row sums and column sums, can also be solved in polynomial time.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 14:25:28 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Onn", "Shmuel", ""]]}, {"id": "2012.02701", "submitter": "Sebastian Siebertz", "authors": "Simeon Kublenz, Sebastian Siebertz, Alexandre Vigny", "title": "Constant round distributed domination on graph classes with bounded\n  expansion", "comments": "Paper accepted at SIROCCO 2021, implemented reviews, corrected an\n  error in Lemma 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that the dominating set problem admits a constant factor\napproximation in a constant number of rounds in the LOCAL model of distributed\ncomputing on graph classes with bounded expansion. This generalizes a result of\nCzygrinow et al. for graphs with excluded topological minors.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 16:15:42 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 16:31:18 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 14:44:35 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kublenz", "Simeon", ""], ["Siebertz", "Sebastian", ""], ["Vigny", "Alexandre", ""]]}, {"id": "2012.02844", "submitter": "Sandip Sinha", "authors": "Xi Chen, Anindya De, Chin Ho Lee, Rocco A. Servedio, Sandip Sinha", "title": "Polynomial-time trace reconstruction in the low deletion rate regime", "comments": "ITCS 2021. Updated with minor correction of extraneous file reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the \\emph{trace reconstruction problem}, an unknown source string $x \\in\n\\{0,1\\}^n$ is transmitted through a probabilistic \\emph{deletion channel} which\nindependently deletes each bit with some fixed probability $\\delta$ and\nconcatenates the surviving bits, resulting in a \\emph{trace} of $x$. The\nproblem is to reconstruct $x$ given access to independent traces.\n  Trace reconstruction of arbitrary (worst-case) strings is a challenging\nproblem, with the current state of the art for poly$(n)$-time algorithms being\nthe 2004 algorithm of Batu et al. \\cite{BKKM04}. This algorithm can reconstruct\nan arbitrary source string $x \\in \\{0,1\\}^n$ in poly$(n)$ time provided that\nthe deletion rate $\\delta$ satisfies $\\delta \\leq n^{-(1/2 + \\varepsilon)}$ for\nsome $\\varepsilon > 0$.\n  In this work we improve on the result of \\cite{BKKM04} by giving a\npoly$(n)$-time algorithm for trace reconstruction for any deletion rate $\\delta\n\\leq n^{-(1/3 + \\varepsilon)}$. Our algorithm works by alternating an\nalignment-based procedure, which we show effectively reconstructs portions of\nthe source string that are not \"highly repetitive\", with a novel procedure that\nefficiently determines the length of highly repetitive subwords of the source\nstring.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 20:47:16 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 03:25:52 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Chen", "Xi", ""], ["De", "Anindya", ""], ["Lee", "Chin Ho", ""], ["Servedio", "Rocco A.", ""], ["Sinha", "Sandip", ""]]}, {"id": "2012.02888", "submitter": "Pranav Nuti", "authors": "Pranav Nuti", "title": "On the best-choice prophet secretary problem", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a variant of the secretary problem where candidates come from\nindependent, not necessarily identical distributions known to us, and show that\nwe can do at least as well as in the IID setting. This resolves a conjecture of\nEsfandiari et al.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 23:08:11 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Nuti", "Pranav", ""]]}, {"id": "2012.02981", "submitter": "Takanori Maehara", "authors": "Soh Kumabe, Takanori Maehara", "title": "r-Gathering Problems on Spiders:Hardness, FPT Algorithms, and PTASes", "comments": "This is work is a merger of arXiv:1907.04088 and arXiv:1907.04087", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the min-max $r$-gathering problem described as follows: We are\ngiven a set of users and facilities in a metric space. We open some of the\nfacilities and assign each user to an opened facility such that each facility\nhas at least $r$ users. The goal is to minimize the maximum distance between\nthe users and the assigned facility. We also consider the min-max $r$-gather\nclustering problem, which is a special case of the $r$-gathering problem in\nwhich the facilities are located everywhere. In this paper, we study the\ntractability and the hardness when the underlying metric space is a spider,\nwhich answers the open question posed by Ahmed et al. [WALCOM'19]. First, we\nshow that the problems are NP-hard even if the underlying space is a spider.\nThen, we propose FPT algorithms parameterized by the degree $d$ of the center.\nThis improves the previous algorithms because they are parameterized by both\n$r$ and $d$. Finally, we propose PTASes to the problems. These are best\npossible because there are no FPTASes unless P=NP.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 09:10:33 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Kumabe", "Soh", ""], ["Maehara", "Takanori", ""]]}, {"id": "2012.03138", "submitter": "Stefan Neumann", "authors": "Stefan Neumann, Pauli Miettinen", "title": "Biclustering and Boolean Matrix Factorization in Data Streams", "comments": "This technical report is the slightly extended version of a paper\n  [34] which appeared at VLDB'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the clustering of bipartite graphs and Boolean matrix factorization\nin data streams. We consider a streaming setting in which the vertices from the\nleft side of the graph arrive one by one together with all of their incident\nedges. We provide an algorithm that, after one pass over the stream, recovers\nthe set of clusters on the right side of the graph using sublinear space; to\nthe best of our knowledge, this is the first algorithm with this property. We\nalso show that after a second pass over the stream, the left clusters of the\nbipartite graph can be recovered and we show how to extend our algorithm to\nsolve the Boolean matrix factorization problem (by exploiting the\ncorrespondence of Boolean matrices and bipartite graphs). We evaluate an\nimplementation of the algorithm on synthetic data and on real-world data. On\nreal-world datasets the algorithm is orders of magnitudes faster than a static\nbaseline algorithm while providing quality results within a factor 2 of the\nbaseline algorithm. Our algorithm scales linearly in the number of edges in the\ngraph. Finally, we analyze the algorithm theoretically and provide sufficient\nconditions under which the algorithm recovers a set of planted clusters under a\nstandard random graph model.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 23:02:43 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Neumann", "Stefan", ""], ["Miettinen", "Pauli", ""]]}, {"id": "2012.03143", "submitter": "Ahad N. Zehmakan", "authors": "Ahad N. Zehmakan", "title": "Majority Opinion Diffusion in Social Networks: An Adversarial Approach", "comments": "To appear in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DM cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce and study a novel majority-based opinion diffusion model.\nConsider a graph $G$, which represents a social network. Assume that initially\na subset of nodes, called seed nodes or early adopters, are colored either\nblack or white, which correspond to positive or negative opinion regarding a\nconsumer product or a technological innovation. Then, in each round an\nuncolored node, which is adjacent to at least one colored node, chooses the\nmost frequent color among its neighbors.\n  Consider a marketing campaign which advertises a product of poor quality and\nits ultimate goal is that more than half of the population believe in the\nquality of the product at the end of the opinion diffusion process. We focus on\nthree types of attackers which can select the seed nodes in a deterministic or\nrandom fashion and manipulate almost half of them to adopt a positive opinion\ntoward the product (that is, to choose black color). We say that an attacker\nsucceeds if a majority of nodes are black at the end of the process. Our main\npurpose is to characterize classes of graphs where an attacker cannot succeed.\nIn particular, we prove that if the maximum degree of the underlying graph is\nnot too large or if it has strong expansion properties, then it is fairly\nresilient to such attacks.\n  Furthermore, we prove tight bounds on the stabilization time of the process\n(that is, the number of rounds it needs to end) in both settings of choosing\nthe seed nodes deterministically and randomly. We also provide several hardness\nresults for some optimization problems regarding stabilization time and choice\nof seed nodes.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 23:30:16 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zehmakan", "Ahad N.", ""]]}, {"id": "2012.03348", "submitter": "Anupam Prakash", "authors": "Tudor Giurgica-Tiron, Iordanis Kerenidis, Farrokh Labib, Anupam\n  Prakash and William Zeng", "title": "Low depth algorithms for quantum amplitude estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We design and analyze two new low depth algorithms for amplitude estimation\n(AE) achieving an optimal tradeoff between the quantum speedup and circuit\ndepth. For $\\beta \\in (0,1]$, our algorithms require $N= O( \\frac{1}{\n\\epsilon^{1+\\beta}})$ oracle calls and require the oracle to be called\nsequentially $D= O( \\frac{1}{ \\epsilon^{1-\\beta}})$ times to perform amplitude\nestimation within additive error $\\epsilon$. These algorithms interpolate\nbetween the classical algorithm $(\\beta=1)$ and the standard quantum algorithm\n($\\beta=0$) and achieve a tradeoff $ND= O(1/\\epsilon^{2})$. These algorithms\nbring quantum speedups for Monte Carlo methods closer to realization, as they\ncan provide speedups with shallower circuits.\n  The first algorithm (Power law AE) uses power law schedules in the framework\nintroduced by Suzuki et al \\cite{S20}. The algorithm works for $\\beta \\in\n(0,1]$ and has provable correctness guarantees when the log-likelihood function\nsatisfies regularity conditions required for the Bernstein Von-Mises theorem.\nThe second algorithm (QoPrime AE) uses the Chinese remainder theorem for\ncombining lower depth estimates to achieve higher accuracy. The algorithm works\nfor discrete $\\beta =q/k$ where $k \\geq 2$ is the number of distinct coprime\nmoduli used by the algorithm and $1 \\leq q \\leq k-1$, and has a fully rigorous\ncorrectness proof. We analyze both algorithms in the presence of depolarizing\nnoise and provide experimental comparisons with the state of the art amplitude\nestimation algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 18:39:20 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Giurgica-Tiron", "Tudor", ""], ["Kerenidis", "Iordanis", ""], ["Labib", "Farrokh", ""], ["Prakash", "Anupam", ""], ["Zeng", "William", ""]]}, {"id": "2012.03367", "submitter": "Moshe Vardi", "authors": "James E. Newman and Moshe Y. Vardi", "title": "FPRAS Approximation of the Matrix Permanent in Practice", "comments": "This article is based on an MS thesis by the first author, submitted\n  to Rice University on June 12, 2020. Research partially supported by NSF\n  Grant no. IIS-1527668", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The matrix permanent belongs to the complexity class #P-Complete. It is\ngenerally believed to be computationally infeasible for large problem sizes,\nand significant research has been done on approximation algorithms for the\nmatrix permanent. We present an implementation and detailed runtime analysis of\none such Markov Chain Monte Carlo (MCMC) based Fully Polynomial Randomized\nApproximation Scheme (FPRAS) for the matrix permanent, which has previously\nonly been described theoretically and with big-Oh runtime analysis. We\ndemonstrate by analysis and experiment that the constant factors hidden by\nprevious big-Oh analyses result in computational infeasibility.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 20:25:28 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Newman", "James E.", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "2012.03454", "submitter": "Mingda Qiao", "authors": "Mingda Qiao, Gregory Valiant", "title": "Stronger Calibration Lower Bounds via Sidestepping", "comments": "To appear in STOC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an online binary prediction setting where a forecaster observes a\nsequence of $T$ bits one by one. Before each bit is revealed, the forecaster\npredicts the probability that the bit is $1$. The forecaster is called\nwell-calibrated if for each $p \\in [0, 1]$, among the $n_p$ bits for which the\nforecaster predicts probability $p$, the actual number of ones, $m_p$, is\nindeed equal to $p \\cdot n_p$. The calibration error, defined as $\\sum_p |m_p -\np n_p|$, quantifies the extent to which the forecaster deviates from being\nwell-calibrated. It has long been known that an $O(T^{2/3})$ calibration error\nis achievable even when the bits are chosen adversarially, and possibly based\non the previous predictions. However, little is known on the lower bound side,\nexcept an $\\Omega(\\sqrt{T})$ bound that follows from the trivial example of\nindependent fair coin flips.\n  In this paper, we prove an $\\Omega(T^{0.528})$ bound on the calibration\nerror, which is the first super-$\\sqrt{T}$ lower bound for this setting to the\nbest of our knowledge. The technical contributions of our work include two\nlower bound techniques, early stopping and sidestepping, which circumvent the\nobstacles that have previously hindered strong calibration lower bounds. We\nalso propose an abstraction of the prediction setting, termed the\nSign-Preservation game, which may be of independent interest. This game has a\nmuch smaller state space than the full prediction setting and allows simpler\nanalyses. The $\\Omega(T^{0.528})$ lower bound follows from a general reduction\ntheorem that translates lower bounds on the game value of Sign-Preservation\ninto lower bounds on the calibration error.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 05:29:28 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 18:27:14 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Qiao", "Mingda", ""], ["Valiant", "Gregory", ""]]}, {"id": "2012.03582", "submitter": "Vijay Vazirani", "authors": "Vijay V. Vazirani", "title": "A Proof of the MV Matching Algorithm", "comments": "43 pages. arXiv admin note: text overlap with arXiv:1210.4594", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Micali-Vazirani (MV) algorithm for maximum cardinality matching in\ngeneral graphs, which was published in 1980 \\cite{MV}, remains to this day the\nmost efficient known algorithm for the problem.\n  This paper gives the first complete and correct proof of this algorithm.\nCentral to our proof are some purely graph-theoretic facts, capturing\nproperties of minimum length alternating paths; these may be of independent\ninterest. An attempt is made to render the algorithm easier to comprehend.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 10:59:15 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Vazirani", "Vijay V.", ""]]}, {"id": "2012.03612", "submitter": "Hiroyuki Kasai", "authors": "Jianming Huang, Zhongxi Fang, Hiroyuki Kasai", "title": "LCS Graph Kernel Based on Wasserstein Distance in Longest Common\n  Subsequence Metric Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For graph classification tasks, many methods use a common strategy to\naggregate information of vertex neighbors. Although this strategy provides an\nefficient means of extracting graph topological features, it brings excessive\namounts of information that might greatly reduce its accuracy when dealing with\nlarge-scale neighborhoods. Learning graphs using paths or walks will not suffer\nfrom this difficulty, but many have low utilization of each path or walk, which\nmight engender information loss and high computational costs. To solve this, we\npropose a graph kernel using a longest common subsequence (LCS kernel) to\ncompute more comprehensive similarity between paths and walks, which resolves\nsubstructure isomorphism difficulties. We also combine it with optimal\ntransport theory to extract more in-depth features of graphs. Furthermore, we\npropose an LCS metric space and apply an adjacent point merge operation to\nreduce its computational costs. Finally, we demonstrate that our proposed\nmethod outperforms many state-of-the-art graph kernel methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 11:59:14 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Huang", "Jianming", ""], ["Fang", "Zhongxi", ""], ["Kasai", "Hiroyuki", ""]]}, {"id": "2012.03817", "submitter": "Yuval Dagan", "authors": "Yuval Dagan, Gil Kur", "title": "A bounded-noise mechanism for differential privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Answering multiple counting queries is one of the best-studied problems in\ndifferential privacy. Its goal is to output an approximation of the average\n$\\frac{1}{n}\\sum_{i=1}^n \\vec{x}^{(i)}$ of vectors $\\vec{x}^{(i)} \\in [0,1]^k$,\nwhile preserving the privacy with respect to any $\\vec{x}^{(i)}$. We present an\n$(\\epsilon,\\delta)$-private mechanism with optimal $\\ell_\\infty$ error for most\nvalues of $\\delta$. This result settles the conjecture of Steinke and Ullman\n[2020] for the these values of $\\delta$. Our algorithm adds independent noise\nof bounded magnitude to each of the $k$ coordinates, while prior solutions\nrelied on unbounded noise such as the Laplace and Gaussian mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 16:03:21 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Dagan", "Yuval", ""], ["Kur", "Gil", ""]]}, {"id": "2012.03879", "submitter": "Mayank Kakodkar", "authors": "Carlos H. C. Teixeira, Mayank Kakodkar, Vin\\'icius Dias, Wagner Meira\n  Jr., Bruno Ribeiro", "title": "Sequential Stratified Regeneration: MCMC for Large State Spaces with an\n  Application to Subgraph Count Estimation", "comments": "Markov Chain Monte Carlo, Random Walk, Regenerative Sampling, Motif\n  Analysis, Subgraph Counting, Graph Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers the general task of estimating the sum of a bounded\nfunction over the edges of a graph, given neighborhood query access and where\naccess to the entire network is prohibitively expensive. To estimate this sum,\nprior work proposes Markov chain Monte Carlo (MCMC) methods that use random\nwalks started at some seed vertex and whose equilibrium distribution is the\nuniform distribution over all edges, eliminating the need to iterate over all\nedges. Unfortunately, these existing estimators are not scalable to massive\nreal-world graphs. In this paper, we introduce Ripple, an MCMC-based estimator\nthat achieves unprecedented scalability by stratifying the Markov chain state\nspace into ordered strata with a new technique that we denote {\\em sequential\nstratified regenerations}. We show that the Ripple estimator is consistent,\nhighly parallelizable, and scales well.\n  We empirically evaluate our method by applying Ripple to the task of\nestimating connected, induced subgraph counts given some input graph. Therein,\nwe demonstrate that Ripple is accurate and can estimate counts of up to\n$12$-node subgraphs, which is a task at a scale that has been considered\nunreachable, not only by prior MCMC-based methods but also by other sampling\napproaches. For instance, in this target application, we present results in\nwhich the Markov chain state space is as large as $10^{43}$, for which Ripple\ncomputes estimates in less than $4$ hours, on average.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 17:47:59 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 16:33:55 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 04:52:49 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Teixeira", "Carlos H. C.", ""], ["Kakodkar", "Mayank", ""], ["Dias", "Vin\u00edcius", ""], ["Meira", "Wagner", "Jr."], ["Ribeiro", "Bruno", ""]]}, {"id": "2012.03906", "submitter": "Pavel Vesel\\'y", "authors": "Antonios Antoniadis, Matthias Englert, Nicolaos Matsakis, Pavel\n  Vesel\\'y", "title": "Breaking the Barrier of 2 for the Competitiveness of Longest Queue Drop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of managing the buffer of a shared-memory switch that\ntransmits packets of unit value. A shared-memory switch consists of an input\nport, a number of output ports, and a buffer with a specific capacity. In each\ntime step, an arbitrary number of packets arrive at the input port, each packet\ndesignated for one output port. Each packet is added to the queue of the\nrespective output port. If the total number of packets exceeds the capacity of\nthe buffer, some packets have to be irrevocably rejected. At the end of each\ntime step, each output port transmits a packet in its queue and the goal is to\nmaximize the number of transmitted packets.\n  The Longest Queue Drop (LQD) online algorithm accepts any arriving packet to\nthe buffer. However, if this results in the buffer exceeding its memory\ncapacity, then LQD drops a packet from the back of whichever queue is currently\nthe longest, breaking ties arbitrarily. The LQD algorithm was first introduced\nin 1991, and is known to be $2$-competitive since 2001. Although LQD remains\nthe best known online algorithm for the problem and is of great practical\ninterest, determining its true competitiveness is a long-standing open problem.\nWe show that LQD is 1.707-competitive, establishing the first $(2-\\varepsilon)$\nupper bound for the competitive ratio of LQD, for a constant $\\varepsilon>0$.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:35:15 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Antoniadis", "Antonios", ""], ["Englert", "Matthias", ""], ["Matsakis", "Nicolaos", ""], ["Vesel\u00fd", "Pavel", ""]]}, {"id": "2012.03923", "submitter": "Nathaniel Harms", "authors": "Eric Blais, Renato Ferreira Pinto Jr., Nathaniel Harms", "title": "VC Dimension and Distribution-Free Sample-Based Testing", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of determining which classes of functions can be\ntested more efficiently than they can be learned, in the distribution-free\nsample-based model that corresponds to the standard PAC learning setting. Our\nmain result shows that while VC dimension by itself does not always provide\ntight bounds on the number of samples required to test a class of functions in\nthis model, it can be combined with a closely-related variant that we call\n\"lower VC\" (or LVC) dimension to obtain strong lower bounds on this sample\ncomplexity.\n  We use this result to obtain strong and in many cases nearly optimal lower\nbounds on the sample complexity for testing unions of intervals, halfspaces,\nintersections of halfspaces, polynomial threshold functions, and decision\ntrees. Conversely, we show that two natural classes of functions, juntas and\nmonotone functions, can be tested with a number of samples that is polynomially\nsmaller than the number of samples required for PAC learning.\n  Finally, we also use the connection between VC dimension and property testing\nto establish new lower bounds for testing radius clusterability and testing\nfeasibility of linear constraint systems.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:50:46 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Blais", "Eric", ""], ["Pinto", "Renato Ferreira", "Jr."], ["Harms", "Nathaniel", ""]]}, {"id": "2012.03996", "submitter": "Vincent Jug\\'e", "authors": "Vincent Jug\\'e and Ghazal Khalighinejad", "title": "Galloping in natural merge sorts", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the algorithm TimSort and the sub-routine it uses to merge monotonic\n(non-decreasing) sub-arrays, hereafter called runs. More precisely, we look at\nthe impact on the number of element comparisons performed of using this\nsub-routine instead of a naive routine.\n  In this article, we introduce a new object for measuring the complexity of\narrays. This notion dual to the notion of runs on which TimSort built its\nsuccess so far, hence we call it dual runs. It induces complexity measures that\nare dual to those induced by runs. We prove, for this new complexity measure,\nresults that are similar to those already known when considering standard\nrun-induced measures. Although our new results do not lead to any improvement\non the number of element moves performed, they may lead to dramatic\nimprovements on the number of element comparisons performed by the algorithm.\n  In order to do so, we introduce new notions of fast- and middle-growth for\nnatural merge sorts, which allow deriving the same upper bounds. After using\nthese notions successfully on TimSort, we prove that they can be applied to a\nwealth of variants of TimSort and other natural merge sorts.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 19:08:31 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Jug\u00e9", "Vincent", ""], ["Khalighinejad", "Ghazal", ""]]}, {"id": "2012.04090", "submitter": "Talya Eden", "authors": "Talya Eden, Dana Ron, Will Rosenbaum", "title": "Almost Optimal Bounds for Sublinear-Time Sampling of $k$-Cliques:\n  Sampling Cliques is Harder Than Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we consider the problem of sampling a $k$-clique in a graph\nfrom an almost uniform distribution in sublinear time in the general graph\nquery model. Specifically the algorithm should output each $k$-clique with\nprobability $(1\\pm \\epsilon)/n_k$, where $n_k$ denotes the number of\n$k$-cliques in the graph and $\\epsilon$ is a given approximation parameter.\n  We prove that the query complexity of this problem is \\[\n\\Theta^*\\left(\\max\\left\\{ \\left(\\frac{(n\\alpha)^{k/2}}{\nn_k}\\right)^{\\frac{1}{k-1}} ,\\; \\min\\left\\{n\\alpha,\\frac{n\\alpha^{k-1}}{n_k}\n\\right\\}\\right\\}\\right). \\] where $n$ is the number of vertices in the graph,\n$\\alpha$ is its arboricity, and $\\Theta^*$ suppresses the dependence on $(\\log\nn/\\epsilon)^{O(k)}$. Interestingly, this establishes a separation between\napproximate counting and approximate uniform sampling in the sublinear regime.\nFor example, if $k=3$, $\\alpha = O(1)$, and $n_3$ (the number of triangles) is\n$\\Theta(n)$, then we get a lower bound of $\\Omega(n^{1/4})$ (for constant\n$\\epsilon$), while under these conditions, a $(1\\pm \\epsilon)$-approximation of\n$n_3$ can be obtained by performing $\\textrm{poly}(\\log(n/\\epsilon))$ queries\n(Eden, Ron and Seshadhri, SODA20).\n  Our lower bound follows from a construction of a family of graphs with\narboricity $\\alpha$ such that in each graph there are $n_k$ cliques (of size\n$k$), where one of these cliques is \"hidden\" and hence hard to sample. Our\nupper bound is based on defining a special auxiliary graph $H_k$, such that\nsampling edges almost uniformly in $H_k$ translates to sampling $k$-cliques\nalmost uniformly in the original graph $G$. We then build on a known\nedge-sampling algorithm (Eden, Ron and Rosenbaum, ICALP19) to sample edges in\n$H_k$, where the challenge is simulate queries to $H_k$ while being given\naccess only to $G$.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 22:23:11 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Eden", "Talya", ""], ["Ron", "Dana", ""], ["Rosenbaum", "Will", ""]]}, {"id": "2012.04343", "submitter": "Elizaveta Kovalevskaya", "authors": "Andreas Karrenbauer, Elizaveta Kovalevskaya", "title": "Reading Articles Online", "comments": "Manuscript of COCOA 2020 paper", "journal-ref": "In: Wu W., Zhang Z. (eds) Combinatorial Optimization and\n  Applications. COCOA 2020. Lecture Notes in Computer Science, vol 12577.\n  Springer, Cham", "doi": "10.1007/978-3-030-64843-5_43", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the online problem of reading articles that are listed in an\naggregated form in a dynamic stream, e.g., in news feeds, as abbreviated social\nmedia posts, or in the daily update of new articles on arXiv. In such a\ncontext, the brief information on an article in the listing only hints at its\ncontent. We consider readers who want to maximize their information gain within\na limited time budget, hence either discarding an article right away based on\nthe hint or accessing it for reading. The reader can decide at any point\nwhether to continue with the current article or skip the remaining part\nirrevocably. In this regard, Reading Articles Online, RAO, does differ\nsubstantially from the Online Knapsack Problem, but also has its similarities.\nUnder mild assumptions, we show that any $\\alpha$-competitive algorithm for the\nOnline Knapsack Problem in the random order model can be used as a black box to\nobtain an $(\\mathrm{e} + \\alpha)C$-competitive algorithm for RAO, where $C$\nmeasures the accuracy of the hints with respect to the information profiles of\nthe articles. Specifically, with the current best algorithm for Online\nKnapsack, which is $6.65<2.45\\mathrm{e}$-competitive, we obtain an upper bound\nof $3.45\\mathrm{e} C$ on the competitive ratio of RAO. Furthermore, we study a\nnatural algorithm that decides whether or not to read an article based on a\nsingle threshold value, which can serve as a model of human readers. We show\nthat this algorithmic technique is $O(C)$-competitive. Hence, our algorithms\nare constant-competitive whenever the accuracy $C$ is a constant.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 10:31:07 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Karrenbauer", "Andreas", ""], ["Kovalevskaya", "Elizaveta", ""]]}, {"id": "2012.04388", "submitter": "Amit Kumar", "authors": "Chiranjib Bhattacharyya and Ravindran Kannan and Amit Kumar", "title": "Algorithms for finding $k$ in $k$-means", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  $k-$means Clustering requires as input the exact value of $k$, the number of\nclusters. Two challenges are open: (i) Is there a data-determined definition of\n$k$ which is provably correct and (ii) Is there a polynomial time algorithm to\nfind $k$ from data ? This paper provides the first affirmative answers to both\nthese questions. As common in the literature, we assume that the data admits an\nunknown Ground Truth (GT) clustering with cluster centers separated. This\nassumption alone is not sufficient to answer Yes to (i). We assume a novel, but\nnatural second constraint called no tight sub-cluster (NTSC) which stipulates\nthat no substantially large subset of a GT cluster can be \"tighter\" (in a sense\nwe define) than the cluster. Our yes answer to (i) and (ii) are under these two\ndeterministic assumptions. We also give polynomial time algorithm to identify\n$k$. Our algorithm relies on NTSC to peel off one cluster at a time by\nidentifying points which are tightly packed. We are also able to show that our\nalgorithm(s) apply to data generated by mixtures of Gaussians and more\ngenerally to mixtures of sub-Gaussian pdf's and hence are able to find the\nnumber of components of the mixture from data. To our knowledge, previous\nresults for these specialized settings as well, assume generally that $k$ is\ngiven besides the data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 12:08:05 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Bhattacharyya", "Chiranjib", ""], ["Kannan", "Ravindran", ""], ["Kumar", "Amit", ""]]}, {"id": "2012.04400", "submitter": "Jannis Harder", "authors": "Jannis Harder", "title": "An Answer to the Bose-Nelson Sorting Problem for 11 and 12 Channels", "comments": "Revised attribution of previous results in the introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that 11-channel sorting networks have at least 35 comparators and\nthat 12-channel sorting networks have at least 39 comparators. This positively\nsettles the optimality of the corresponding sorting networks given in The Art\nof Computer Programming vol. 3 and closes the two smallest open instances of\nthe Bose-Nelson sorting problem. We obtain these bounds by generalizing a\nresult of Van Voorhis from sorting networks to a more general class of\ncomparator networks. From this we derive a dynamic programming algorithm that\ncomputes the optimal size for a sorting network with a given number of\nchannels. From an execution of this algorithm we construct a certificate\ncontaining a derivation of the corresponding lower size bound, which we check\nusing a program formally verified using the Isabelle/HOL proof assistant.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 12:37:10 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 10:52:45 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Harder", "Jannis", ""]]}, {"id": "2012.04420", "submitter": "Bernard Zweers", "authors": "Guido Sch\\\"afer and Bernard G. Zweers", "title": "Maximum Coverage with Cluster Constraints: An LP-Based Approximation\n  Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Packing problems constitute an important class of optimization problems, both\nbecause of their high practical relevance and theoretical appeal. However,\ndespite the large number of variants that have been studied in the literature,\nmost packing problems encompass a single tier of capacity restrictions only.\nFor example, in the Multiple Knapsack Problem, we assign items to multiple\nknapsacks such that their capacities are not exceeded. But what if these\nknapsacks are partitioned into clusters, each imposing an additional capacity\nrestriction on the knapsacks contained in that cluster? In this paper, we study\nthe Maximum Coverage Problem with Cluster Constraints (MCPC), which generalizes\nthe Maximum Coverage Problem with Knapsack Constraints (MCPK) by incorporating\ncluster constraints. Our main contribution is a general LP-based technique to\nderive approximation algorithms for cluster capacitated problems. Our technique\nallows us to reduce the cluster capacitated problem to the respective original\npacking problem. By using an LP-based approximation algorithm for the original\nproblem, we can then obtain an effective rounding scheme for the problem, which\nonly loses a small fraction in the approximation guarantee. We apply our\ntechnique to derive approximation algorithms for MCPC. To this aim, we develop\nan LP-based $\\frac12(1-\\frac1e)$-approximation algorithm for MCPK by adapting\nthe pipage rounding technique. Combined with our reduction technique, we obtain\na $\\frac13(1-\\frac1e)$-approximation algorithm for MCPC. We also derive\nimproved results for a special case of MCPC, the Multiple Knapsack Problem with\nCluster Constraints (MKPC). Based on a simple greedy algorithm, our approach\nyields a $\\frac13$-approximation algorithm. By combining our technique with a\nmore sophisticated iterative rounding approach, we obtain a\n$\\frac12$-approximation algorithm for certain special cases of MKPC.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 13:33:30 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Sch\u00e4fer", "Guido", ""], ["Zweers", "Bernard G.", ""]]}, {"id": "2012.04488", "submitter": "Sandeep Silwal", "authors": "Sandeep Silwal", "title": "A Concentration Inequality for the Facility Location Problem", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give a concentration inequality for a stochastic version of the facility\nlocation problem on the plane. We show the objective \\[ C_n(X) = \\min_{F\n\\subseteq [0,1]^2} \\, |F| + \\sum_{x\\in X} \\min_{f \\in F} \\| x-f\\| \\] is\nconcentrated in an interval of length $O(n^{1/6})$ and $\\mathbb{E}[C_n] =\n\\Theta(n^{2/3})$ if the input $X$ consists of $n$ i.i.d. uniform points in the\nunit square. Our main tool is to use a suitable geometric quantity, previously\nused in the design of approximation algorithms for the facility location\nproblem, to analyze a martingale process.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 15:27:24 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Silwal", "Sandeep", ""]]}, {"id": "2012.04910", "submitter": "Yasuaki Kobayashi", "authors": "Yuuki Aoike, Tatsuya Gima, Tesshu Hanaka, Masashi Kiyomi, Yasuaki\n  Kobayashi, Yusuke Kobayashi, Kazuhiro Kurita, Yota Otachi", "title": "An Improved Deterministic Parameterized Algorithm for Cactus Vertex\n  Deletion", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cactus is a connected graph that does not contain $K_4 - e$ as a minor.\nGiven a graph $G = (V, E)$ and integer $k \\ge 0$, Cactus Vertex Deletion (also\nknown as Diamond Hitting Set) is the problem of deciding whether $G$ has a\nvertex set of size at most $k$ whose removal leaves a forest of cacti. The\ncurrent best deterministic parameterized algorithm for this problem was due to\nBonnet et al. [WG 2016], which runs in time $26^kn^{O(1)}$, where $n$ is the\nnumber of vertices of $G$. In this paper, we design a deterministic algorithm\nfor Cactus Vertex Deletion, which runs in time $17.64^kn^{O(1)}$. As a\nstraightforward application of our algorithm, we give a $17.64^kn^{O(1)}$-time\nalgorithm for Even Cycle Transversal. The idea behind this improvement is to\napply the measure and conquer analysis with a slightly elaborate measure of\ninstances.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 08:26:38 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 08:43:02 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 05:39:50 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Aoike", "Yuuki", ""], ["Gima", "Tatsuya", ""], ["Hanaka", "Tesshu", ""], ["Kiyomi", "Masashi", ""], ["Kobayashi", "Yasuaki", ""], ["Kobayashi", "Yusuke", ""], ["Kurita", "Kazuhiro", ""], ["Otachi", "Yota", ""]]}, {"id": "2012.05142", "submitter": "Arnab Maiti", "authors": "Arnab Maiti, Vishakha Patil, Arindam Khan", "title": "Streaming Algorithms for Stochastic Multi-armed Bandits", "comments": "24 pages, 2 figures, 4 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Stochastic Multi-armed Bandit problem under bounded arm-memory.\nIn this setting, the arms arrive in a stream, and the number of arms that can\nbe stored in the memory at any time, is bounded. The decision-maker can only\npull arms that are present in the memory. We address the problem from the\nperspective of two standard objectives: 1) regret minimization, and 2) best-arm\nidentification. For regret minimization, we settle an important open question\nby showing an almost tight hardness. We show {\\Omega}(T^{2/3}) cumulative\nregret in expectation for arm-memory size of (n-1), where n is the number of\narms. For best-arm identification, we study two algorithms. First, we present\nan O(r) arm-memory r-round adaptive streaming algorithm to find an\n{\\epsilon}-best arm. In r-round adaptive streaming algorithm for best-arm\nidentification, the arm pulls in each round are decided based on the observed\noutcomes in the earlier rounds. The best-arm is the output at the end of r\nrounds. The upper bound on the sample complexity of our algorithm matches with\nthe lower bound for any r-round adaptive streaming algorithm. Secondly, we\npresent a heuristic to find the {\\epsilon}-best arm with optimal sample\ncomplexity, by storing only one extra arm in the memory.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 16:28:05 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Maiti", "Arnab", ""], ["Patil", "Vishakha", ""], ["Khan", "Arindam", ""]]}, {"id": "2012.05213", "submitter": "Krzysztof Sornat", "authors": "Pallavi Jain, Krzysztof Sornat, Nimrod Talmon, Meirav Zehavi", "title": "Participatory Budgeting with Project Groups", "comments": "23 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a generalization of the standard approval-based model of\nparticipatory budgeting (PB), in which voters are providing approval ballots\nover a set of predefined projects and -- in addition to a global budget limit,\nthere are several groupings of the projects, each group with its own budget\nlimit. We study the computational complexity of identifying project bundles\nthat maximize voter satisfaction while respecting all budget limits. We show\nthat the problem is generally intractable and describe efficient exact\nalgorithms for several special cases, including instances with only few groups\nand instances where the group structure is close to be hierarchical, as well as\nefficient approximation algorithms. Our results could allow, e.g.,\nmunicipalities to hold richer PB processes that are thematically and\ngeographically inclusive.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 18:23:04 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Jain", "Pallavi", ""], ["Sornat", "Krzysztof", ""], ["Talmon", "Nimrod", ""], ["Zehavi", "Meirav", ""]]}, {"id": "2012.05361", "submitter": "Ali Zeynali", "authors": "Ali Zeynali, Bo Sun, Mohammad Hajiesmaili, Adam Wierman", "title": "Data-driven Competitive Algorithms for Online Knapsack and Set Cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of online algorithms has tended to focus on algorithms with\nworst-case guarantees, e.g., bounds on the competitive ratio. However, it is\nwell-known that such algorithms are often overly pessimistic, performing\nsub-optimally on non-worst-case inputs. In this paper, we develop an approach\nfor data-driven design of online algorithms that maintain near-optimal\nworst-case guarantees while also performing learning in order to perform well\nfor typical inputs. Our approach is to identify policy classes that admit\nglobal worst-case guarantees, and then perform learning using historical data\nwithin the policy classes. We demonstrate the approach in the context of two\nclassical problems, online knapsack and online set cover, proving competitive\nbounds for rich policy classes in each case. Additionally, we illustrate the\npractical implications via a case study on electric vehicle charging.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 23:16:23 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Zeynali", "Ali", ""], ["Sun", "Bo", ""], ["Hajiesmaili", "Mohammad", ""], ["Wierman", "Adam", ""]]}, {"id": "2012.05365", "submitter": "Conrad Slagle", "authors": "CNP Slagle and Lance Fortnow", "title": "Matrix Multiplication and Binary Space Partitioning Trees : An\n  Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Herein we explore a dual tree algorithm for matrix multiplication of $A\\in\n\\mathbb{R}^{M\\times D}$ and $B\\in\\mathbb{R}^{D\\times N}$, very narrowly\neffective if the normalized rows of $A$ and columns of $B$, treated as vectors\nin $\\mathbb{R}^{D}$, fall into clusters of order proportionate to\n$\\Omega(D^{\\tau})$ with radii less than $\\arcsin(\\epsilon/\\sqrt{2})$ on the\nsurface of the unit $D$-ball. The algorithm leverages a pruning rule necessary\nto guarantee $\\epsilon$ precision proportionate to vector magnitude products in\nthe resultant matrix. \\textit{ Unfortunately, if the rows and columns are\nuniformly distributed on the surface of the unit $D$-ball, then the expected\npoints per required cluster approaches zero exponentially fast in $D$; thus,\nthe approach requires a great deal of work to pass muster.}\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 23:20:47 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Slagle", "CNP", ""], ["Fortnow", "Lance", ""]]}, {"id": "2012.05398", "submitter": "Jason Altschuler", "authors": "Jason M. Altschuler and Enric Boix-Adsera", "title": "Hardness results for Multimarginal Optimal Transport problems", "comments": "For expository purposes, some of these results were moved from v1 of\n  arXiv 2008.03006. The current drafts of these papers have no overlapping\n  results. arXiv admin note: text overlap with arXiv:2008.03006", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimarginal Optimal Transport (MOT) is the problem of linear programming\nover joint probability distributions with fixed marginals. A key issue in many\napplications is the complexity of solving MOT: the linear program has\nexponential size in the number of marginals k and their support sizes n. A\nrecent line of work has shown that MOT is poly(n,k)-time solvable for certain\nfamilies of costs that have poly(n,k)-size implicit representations. However,\nit is unclear what further families of costs this line of algorithmic research\ncan encompass. In order to understand these fundamental limitations, this paper\ninitiates the study of intractability results for MOT.\n  Our main technical contribution is developing a toolkit for proving\nNP-hardness and inapproximability results for MOT problems. We demonstrate this\ntoolkit by using it to establish the intractability of a number of MOT problems\nstudied in the literature that have resisted previous algorithmic efforts. For\ninstance, we provide evidence that repulsive costs make MOT intractable by\nshowing that several such problems of interest are NP-hard to solve--even\napproximately.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 01:36:12 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Altschuler", "Jason M.", ""], ["Boix-Adsera", "Enric", ""]]}, {"id": "2012.05430", "submitter": "Saigopal Thota", "authors": "Saigopal Thota, Mridul Jain, Nishad Kamat, Saikiran Malikireddy,\n  Pruthvi Raj Eranti, Albin Kuruvilla", "title": "Building Graphs at a Large Scale: Union Find Shuffle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Large scale graph processing using distributed computing frameworks is\nbecoming pervasive and efficient in the industry. In this work, we present a\nhighly scalable and configurable distributed algorithm for building connected\ncomponents, called Union Find Shuffle (UFS) with Path Compression. The scale\nand complexity of the algorithm are a function of the number of partitions into\nwhich the data is initially partitioned, and the size of the connected\ncomponents. We discuss the complexity and the benchmarks compared to similar\napproaches. We also present current benchmarks of our production system,\nrunning on commodity out-of-the-box cloud Hadoop infrastructure, where the\nalgorithm was deployed over a year ago, scaled to around 75 Billion nodes and\n60 Billions linkages (and growing). We highlight the key aspects of our\nalgorithm which enable seamless scaling and performance even in the presence of\nskewed data with large connected components in the size of 10 Billion nodes\neach.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 03:00:39 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 18:39:07 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Thota", "Saigopal", ""], ["Jain", "Mridul", ""], ["Kamat", "Nishad", ""], ["Malikireddy", "Saikiran", ""], ["Eranti", "Pruthvi Raj", ""], ["Kuruvilla", "Albin", ""]]}, {"id": "2012.05564", "submitter": "Alexandru Popa Dr.", "authors": "Lucian-Ionut Gavrila and Alexandru Popa", "title": "A novel algorithm for clearing financial obligations between companies\n  -- an application within the Romanian Ministry of Economy", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The concept of clearing or netting, as defined in the glossaries of European\nCentral Bank, has a great impact on the economy of a country influencing the\nexchanges and the interactions between companies. On short, netting refers to\nan alternative to the usual way in which the companies make the payments to\neach other: it is an agreement in which each party sets off amounts it owes\nagainst amounts owed to it. Based on the amounts two or more parties owe\nbetween them, the payment is substituted by a direct settlement. In this paper\nwe introduce a set of graph algorithms which provide optimal netting solutions\nfor the scale of a country economy. The set of algorithms computes results in\nan efficient time and is tested on invoice data provided by the Romanian\nMinistry of Economy. Our results show that classical graph algorithms are still\ncapable of solving very important modern problems.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 10:22:11 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Gavrila", "Lucian-Ionut", ""], ["Popa", "Alexandru", ""]]}, {"id": "2012.05869", "submitter": "Karim Abu-Affash", "authors": "A. Karim Abu-Affash, Paz Carmi, Adi Krasin", "title": "A Linear-Time Algorithm for Minimum $k$-Hop Dominating Set of a Cactus\n  Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Given a graph $G=(V,E)$ and an integer $k \\ge 1$, a $k$-hop dominating set\n$D$ of $G$ is a subset of $V$, such that, for every vertex $v \\in V$, there\nexists a node $u \\in D$ whose hop-distance from $v$ is at most $k$. A $k$-hop\ndominating set of minimum cardinality is called a minimum $k$-hop dominating\nset. In this paper, we present linear-time algorithms that find a minimum\n$k$-hop dominating set in unicyclic and cactus graphs. To achieve this, we show\nthat the $k$-dominating set problem on unicycle graph reduces to the piercing\ncircular arcs problem, and show a linear-time algorithm for piercing sorted\ncircular arcs, which improves the best known $O(n\\log n)$-time algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 18:27:22 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Abu-Affash", "A. Karim", ""], ["Carmi", "Paz", ""], ["Krasin", "Adi", ""]]}, {"id": "2012.05963", "submitter": "Vida Vuka\\v{s}inovi\\'c", "authors": "Rok Hribar, Timotej Hrga, Gregor Papa, Ga\\v{s}per Petelin, Janez Povh,\n  Nata\\v{s}a Pr\\v{z}ulj, Vida Vuka\\v{s}inovi\\'c", "title": "Four algorithms to solve symmetric multi-type non-negative matrix\n  tri-factorization problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the symmetric multi-type non-negative matrix\ntri-factorization problem (SNMTF), which attempts to factorize several\nsymmetric non-negative matrices simultaneously. This can be considered as a\ngeneralization of the classical non-negative matrix tri-factorization problem\nand includes a non-convex objective function which is a multivariate sixth\ndegree polynomial and a has convex feasibility set. It has a special importance\nin data science, since it serves as a mathematical model for the fusion of\ndifferent data sources in data clustering.\n  We develop four methods to solve the SNMTF. They are based on four\ntheoretical approaches known from the literature: the fixed point method (FPM),\nthe block-coordinate descent with projected gradient (BCD), the gradient method\nwith exact line search (GM-ELS) and the adaptive moment estimation method\n(ADAM). For each of these methods we offer a software implementation: for the\nformer two methods we use Matlab and for the latter Python with the TensorFlow\nlibrary.\n  We test these methods on three data-sets: the synthetic data-set we\ngenerated, while the others represent real-life similarities between different\nobjects.\n  Extensive numerical results show that with sufficient computing time all four\nmethods perform satisfactorily and ADAM most often yields the best mean square\nerror ($\\mathrm{MSE}$). However, if the computation time is limited, FPM gives\nthe best $\\mathrm{MSE}$ because it shows the fastest convergence at the\nbeginning.\n  All data-sets and codes are publicly available on our GitLab profile.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 20:43:19 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Hribar", "Rok", ""], ["Hrga", "Timotej", ""], ["Papa", "Gregor", ""], ["Petelin", "Ga\u0161per", ""], ["Povh", "Janez", ""], ["Pr\u017eulj", "Nata\u0161a", ""], ["Vuka\u0161inovi\u0107", "Vida", ""]]}, {"id": "2012.05974", "submitter": "Michael Moy", "authors": "Michael Moy, Robert Cardona, Robert Green, Jacob Cleveland, Alan\n  Hylton, Robert Short", "title": "Path Optimization Sheaves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by efforts to incorporate sheaves into networking, we seek to\nreinterpret pathfinding algorithms in terms of cellular sheaves, using\nDijkstra's algorithm as an example. We construct sheaves on a graph with\ndistinguished source and sink vertices, in which paths are represented by\nsections. The first sheaf is a very general construction that can be applied to\nother algorithms, while the second is created specifically to capture the\ndecision making of Dijkstra's algorithm. In both cases, Dijkstra's algorithm\ncan be described as a systematic process of extending local sections to global\nsections. We discuss the relationship between the two sheaves and summarize how\nother pathfinding algorithms can be interpreted in a similar way. While the\nsheaves presented here address paths and pathfinding algorithms, we suggest\nthat future work could explore connections to other concepts from graph theory\nand other networking algorithms. This work was supported by the NASA Internship\nProject and SCaN Internship Project during the summer of 2020.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 20:59:27 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Moy", "Michael", ""], ["Cardona", "Robert", ""], ["Green", "Robert", ""], ["Cleveland", "Jacob", ""], ["Hylton", "Alan", ""], ["Short", "Robert", ""]]}, {"id": "2012.06062", "submitter": "Krzysztof Pot\\k{e}pa", "authors": "Krzysztof Pot\\k{e}pa", "title": "Faster Deterministic Modular Subset Sum", "comments": "16 pages, accepted at ESA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the Modular Subset Sum problem: given a multiset $X$ of integers\nfrom $\\mathbb{Z}_m$ and a target integer $t$, decide if there exists a subset\nof $X$ with a sum equal to $t \\pmod{m}$. Recent independent works by Cardinal\nand Iacono (SOSA'21), and Axiotis et al. (SOSA'21) provided simple and\nnear-linear algorithms for this problem. Cardinal and Iacono gave a randomized\nalgorithm that runs in $O(m \\log m)$ time, while Axiotis et al. gave a\ndeterministic algorithm that runs in $O(m \\text{ polylog } m)$ time. Both\nresults work by reduction to a text problem, which is solved using a dynamic\nstrings data structure.\n  In this work, we develop a simple data structure, designed specifically to\nhandle the text problem that arises in the algorithms for Modular Subset Sum.\nOur data structure, which we call the shift-tree, is a simple variant of a\nsegment tree. We provide both a hashing-based and a deterministic variant of\nthe shift-trees.\n  We then apply our data structure to the Modular Subset Sum problem and obtain\ntwo algorithms. The first algorithm is Monte-Carlo randomized and matches the\n$O(m \\log m)$ runtime of the Las-Vegas algorithm by Cardinal and Iacono. The\nsecond algorithm is fully deterministic and runs in $O(m \\log m \\cdot\n\\alpha(m))$ time, where $\\alpha$ is the inverse Ackermann function.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 01:00:34 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 20:31:48 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Pot\u0119pa", "Krzysztof", ""]]}, {"id": "2012.06519", "submitter": "Tongyang Li", "authors": "Tongyang Li, Chunhao Wang, Shouvanik Chakrabarti, and Xiaodi Wu", "title": "Sublinear classical and quantum algorithms for general matrix games", "comments": "16 pages, 2 figures. To appear in the Thirty-Fifth AAAI Conference on\n  Artificial Intelligence (AAAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate sublinear classical and quantum algorithms for matrix games, a\nfundamental problem in optimization and machine learning, with provable\nguarantees. Given a matrix $A\\in\\mathbb{R}^{n\\times d}$, sublinear algorithms\nfor the matrix game $\\min_{x\\in\\mathcal{X}}\\max_{y\\in\\mathcal{Y}} y^{\\top} Ax$\nwere previously known only for two special cases: (1) $\\mathcal{Y}$ being the\n$\\ell_{1}$-norm unit ball, and (2) $\\mathcal{X}$ being either the $\\ell_{1}$-\nor the $\\ell_{2}$-norm unit ball. We give a sublinear classical algorithm that\ncan interpolate smoothly between these two cases: for any fixed $q\\in (1,2]$,\nwe solve the matrix game where $\\mathcal{X}$ is a $\\ell_{q}$-norm unit ball\nwithin additive error $\\epsilon$ in time $\\tilde{O}((n+d)/{\\epsilon^{2}})$. We\nalso provide a corresponding sublinear quantum algorithm that solves the same\ntask in time $\\tilde{O}((\\sqrt{n}+\\sqrt{d})\\textrm{poly}(1/\\epsilon))$ with a\nquadratic improvement in both $n$ and $d$. Both our classical and quantum\nalgorithms are optimal in the dimension parameters $n$ and $d$ up to\npoly-logarithmic factors. Finally, we propose sublinear classical and quantum\nalgorithms for the approximate Carath\\'eodory problem and the $\\ell_{q}$-margin\nsupport vector machines as applications.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 17:36:33 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Li", "Tongyang", ""], ["Wang", "Chunhao", ""], ["Chakrabarti", "Shouvanik", ""], ["Wu", "Xiaodi", ""]]}, {"id": "2012.06522", "submitter": "Supratim Shit", "authors": "Rachit Chhaya, Jayesh Choudhari, Anirban Dasgupta, Supratim Shit", "title": "Online Coresets for Clustering with Bregman Divergences", "comments": "Work in Progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present algorithms that create coresets in an online setting for\nclustering problems according to a wide subset of Bregman divergences. Notably,\nour coresets have a small additive error, similar in magnitude to the\nlightweight coresets Bachem et. al. 2018, and take update time $O(d)$ for every\nincoming point where $d$ is dimension of the point. Our first algorithm gives\nonline coresets of size $\\tilde{O}(\\mbox{poly}(k,d,\\epsilon,\\mu))$ for\n$k$-clusterings according to any $\\mu$-similar Bregman divergence. We further\nextend this algorithm to show existence of a non-parametric coresets, where the\ncoreset size is independent of $k$, the number of clusters, for the same\nsubclass of Bregman divergences. Our non-parametric coresets are larger by a\nfactor of $O(\\log n)$ ($n$ is number of points) and have similar (small)\nadditive guarantee. At the same time our coresets also function as lightweight\ncoresets for non-parametric versions of the Bregman clustering like DP-Means.\nWhile these coresets provide additive error guarantees, they are also\nsignificantly smaller (scaling with $O(\\log n)$ as opposed to $O(d^d)$ for\npoints in $\\~R^d$) than the (relative-error) coresets obtained in Bachem et.\nal. 2015 for DP-Means. While our non-parametric coresets are existential, we\ngive an algorithmic version under certain assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 17:39:21 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Chhaya", "Rachit", ""], ["Choudhari", "Jayesh", ""], ["Dasgupta", "Anirban", ""], ["Shit", "Supratim", ""]]}, {"id": "2012.06702", "submitter": "Henry Adams", "authors": "Henry Adams, Leah Gibson, Jack Pfaffinger", "title": "Lions and contamination, triangular grids, and Cheeger constants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose each vertex of a graph is originally occupied by contamination,\nexcept for those vertices occupied by lions. As the lions wander on the graph,\nthey clear the contamination from each vertex they visit. However, the\ncontamination simultaneously spreads to any adjacent vertex not occupied by a\nlion. How many lions are required in order to clear the graph of contamination?\nWe give a lower bound on the number of lions needed in terms of the Cheeger\nconstant of the graph. Furthermore, the lion and contamination problem has been\nstudied in detail on square grid graphs by Brass et al. and Berger et al., and\nwe extend this analysis to the setting of triangular grid graphs.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 02:17:28 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 14:56:06 GMT"}, {"version": "v3", "created": "Sat, 3 Jul 2021 15:46:06 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Adams", "Henry", ""], ["Gibson", "Leah", ""], ["Pfaffinger", "Jack", ""]]}, {"id": "2012.06713", "submitter": "Sami Davies", "authors": "Sami Davies, Miklos Z. Racz, Cyrus Rashtchian, Benjamin G. Schiffer", "title": "Approximate Trace Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.IT cs.LG math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the usual trace reconstruction problem, the goal is to exactly reconstruct\nan unknown string of length $n$ after it passes through a deletion channel many\ntimes independently, producing a set of traces (i.e., random subsequences of\nthe string). We consider the relaxed problem of approximate reconstruction.\nHere, the goal is to output a string that is close to the original one in edit\ndistance while using much fewer traces than is needed for exact reconstruction.\nWe present several algorithms that can approximately reconstruct strings that\nbelong to certain classes, where the estimate is within $n/\\mathrm{polylog}(n)$\nedit distance, and where we only use $\\mathrm{polylog}(n)$ traces (or sometimes\njust a single trace). These classes contain strings that require a linear\nnumber of traces for exact reconstruction and which are quite different from a\ntypical random string. From a technical point of view, our algorithms\napproximately reconstruct consecutive substrings of the unknown string by\naligning dense regions of traces and using a run of a suitable length to\napproximate each region. To complement our algorithms, we present a general\nblack-box lower bound for approximate reconstruction, building on a lower bound\nfor distinguishing between two candidate input strings in the worst case. In\nparticular, this shows that approximating to within $n^{1/3 - \\delta}$ edit\ndistance requires $n^{1 + 3\\delta/2}/\\mathrm{polylog}(n)$ traces for $0< \\delta\n< 1/3$ in the worst case.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 03:34:26 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 18:27:11 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Davies", "Sami", ""], ["Racz", "Miklos Z.", ""], ["Rashtchian", "Cyrus", ""], ["Schiffer", "Benjamin G.", ""]]}, {"id": "2012.06845", "submitter": "Yifan Xu", "authors": "Yifan Xu, Pan Xu, Jianping Pan and Jun Tao", "title": "A Unified Model for the Two-stage Offline-then-Online Resource\n  Allocation", "comments": "Accepted by IJCAI 2020\n  (http://static.ijcai.org/2020-accepted_papers.html) and SOLE copyright holder\n  is IJCAI (International Joint Conferences on Artificial Intelligence), all\n  rights reserved", "journal-ref": "IJCAI 2020", "doi": "10.24963/ijcai.2020/581", "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the popularity of the Internet, traditional offline resource allocation\nhas evolved into a new form, called online resource allocation. It features the\nonline arrivals of agents in the system and the real-time decision-making\nrequirement upon the arrival of each online agent. Both offline and online\nresource allocation have wide applications in various real-world matching\nmarkets ranging from ridesharing to crowdsourcing. There are some emerging\napplications such as rebalancing in bike sharing and trip-vehicle dispatching\nin ridesharing, which involve a two-stage resource allocation process. The\nprocess consists of an offline phase and another sequential online phase, and\nboth phases compete for the same set of resources. In this paper, we propose a\nunified model which incorporates both offline and online resource allocation\ninto a single framework. Our model assumes non-uniform and known arrival\ndistributions for online agents in the second online phase, which can be\nlearned from historical data. We propose a parameterized linear programming\n(LP)-based algorithm, which is shown to be at most a constant factor of $1/4$\nfrom the optimal. Experimental results on the real dataset show that our\nLP-based approaches outperform the LP-agnostic heuristics in terms of\nrobustness and effectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 15:55:13 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Xu", "Yifan", ""], ["Xu", "Pan", ""], ["Pan", "Jianping", ""], ["Tao", "Jun", ""]]}, {"id": "2012.06850", "submitter": "Yifan Xu", "authors": "Yifan Xu and Pan Xu", "title": "Trading the System Efficiency for the Income Equality of Drivers in\n  Rideshare", "comments": "Accepted by IJCAI2020\n  (http://static.ijcai.org/2020-accepted_papers.html) and SOLE copyright holder\n  is IJCAI (International Joint Conferences on Artificial Intelligence), all\n  rights reserved", "journal-ref": "IJCAI 2020", "doi": "10.24963/ijcai.2020/580", "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several scientific studies have reported the existence of the income gap\namong rideshare drivers based on demographic factors such as gender, age, race,\netc. In this paper, we study the income inequality among rideshare drivers due\nto discriminative cancellations from riders, and the tradeoff between the\nincome inequality (called fairness objective) with the system efficiency\n(called profit objective). We proposed an online bipartite-matching model where\nriders are assumed to arrive sequentially following a distribution known in\nadvance. The highlight of our model is the concept of acceptance rate between\nany pair of driver-rider types, where types are defined based on demographic\nfactors. Specially, we assume each rider can accept or cancel the driver\nassigned to her, each occurs with a certain probability which reflects the\nacceptance degree from the rider type towards the driver type. We construct a\nbi-objective linear program as a valid benchmark and propose two LP-based\nparameterized online algorithms. Rigorous online competitive ratio analysis is\noffered to demonstrate the flexibility and efficiency of our online algorithms\nin balancing the two conflicting goals, promotions of fairness and profit.\nExperimental results on a real-world dataset are provided as well, which\nconfirm our theoretical predictions.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 16:04:06 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Xu", "Yifan", ""], ["Xu", "Pan", ""]]}, {"id": "2012.07135", "submitter": "Rajni Dabas", "authors": "Rajni Dabas, Neelima Gupta and Sapna Grover", "title": "Uniform Capacitated Facility Location Problems with Penalties/Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a framework to design approximation algorithms for\ncapacitated facility location problems with penalties/outliers using\nLP-rounding. Primal-dual technique, which has been particularly successful in\ndealing with outliers and penalties, has not been very successful in dealing\nwith capacities. On the other hand, no primal-dual solution has been able to\nbreak the hardness of capacitated facility location problem(CFLP). LP-rounding\ntechniques had also not been very successful in dealing with capacities until a\nrecent work by Grover et al. \\cite{GroverGKP18}. Their constant factor\napproximation violating the capacities by a small factor ($1 + \\epsilon$) is\npromising while dealing with capacities.Though LP-rounding has not been very\npromising while dealing with penalties and outliers, we successfully apply it\nto deal with them along with capacities. That is, our results are obtained by\nrounding the solution to the natural LP once again exhibiting the power of\nLP-rounding technique. Solutions obtained by LP-rounding are easy to integrate\nwith other LP-based algorithms.In this paper, we apply our framework to obtain\nfirst constant factor approximations for capacitated facility location problem\nwith outlier (CFLPO) and capacitated $k$-facility location problem with\npenalty(C$k$FLPP) for hard uniform capacities using LP-rounding. Our solutions\nincur slight violations in capacities, ($1 + \\epsilon$) for the problems\nwithout cardinality($k$) constraint and ($2 + \\epsilon$) for the problems with\nthe cardinality constraint. For the outlier variant, we also incur a small loss\n($1 + \\epsilon$) in outliers. Due to the hardness of the underlying problems,\nthe violations are inevitable. Thus we achieve the best possible by rounding\nthe solution of natural LP for these problems. To the best of our knowledge, no\nresults are known for CFLPO and C$k$FLPP.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 20:09:48 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Dabas", "Rajni", ""], ["Gupta", "Neelima", ""], ["Grover", "Sapna", ""]]}, {"id": "2012.07214", "submitter": "Shangsen Li", "authors": "Shangsen Li, Lailong Luo, Deke Guo, Qianzhen Zhang, Pengtao Fu", "title": "A survey of sketches in traffic measurement: Design, Optimization,\n  Application and Implementation", "comments": "39 pages,13 figures. arXiv admin note: text overlap with\n  arXiv:1910.10441, arXiv:1903.05728, arXiv:1710.05697 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network measurement probes the underlying network to support upper-level\ndecisions such as network management, network update, network maintenance,\nnetwork defense and beyond. Due to the massive, speedy, unpredictable features\nof network flows, sketches are widely implemented in measurement nodes to\napproximately record the frequency or estimate the cardinality of flows. At\ntheir cores, sketches usually maintain one or multiple counter array(s), and\nrely on hash functions to select the counter(s) for each flow. Then the\nspace-efficient sketches from the distributed measurement nodes are aggregated\nto provide statistics of the undergoing flows. Currently, tremendous redesigns\nand optimizations have been proposed to improve the sketches for better network\nmeasurement performance. However, existing reviews or surveys mainly focus on\none particular aspect of measurement tasks. Researchers and engineers in the\nnetwork measurement community desire an all-in-one survey that covers the\nentire processing pipeline of sketch-based network measurement. To this end, we\npresent the first comprehensive survey of this area. We first introduce the\npreparation of flows for measurement, then detail the most recent\ninvestigations of design, aggregation, decoding, application and implementation\nof sketches for network measurement. To summarize the existing efforts, we\ncarry out an in-depth study of the existing literature, covering more than 90\nsketch designs and optimization strategies. Furthermore, we conduct a\ncomprehensive analysis and qualitative/quantitative comparison of the sketch\ndesigns. Finally,we highlight the open issues for future sketch-based network\nmeasurement research.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 02:21:13 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 13:23:02 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Li", "Shangsen", ""], ["Luo", "Lailong", ""], ["Guo", "Deke", ""], ["Zhang", "Qianzhen", ""], ["Fu", "Pengtao", ""]]}, {"id": "2012.07354", "submitter": "Steven Kelk", "authors": "Rim van Wersch, Steven Kelk, Simone Linz, Georgios Stamoulis", "title": "Reflections on kernelizing and computing unrooted agreement forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic trees are leaf-labelled trees used to model the evolution of\nspecies. Here we explore the practical impact of kernelization (i.e. data\nreduction) on the NP-hard problem of computing the TBR distance between two\nunrooted binary phylogenetic trees. This problem is better-known in the\nliterature as the maximum agreement forest problem, where the goal is to\npartition the two trees into a minimum number of common, non-overlapping\nsubtrees. We have implemented two well-known reduction rules, the subtree and\nchain reduction, and five more recent, theoretically stronger reduction rules,\nand compare the reduction achieved with and without the stronger rules. We find\nthat the new rules yield smaller reduced instances and thus have clear\npractical added value. In many cases they also cause the TBR distance to\ndecrease in a controlled fashion. Next, we compare the achieved reduction to\nthe known worst-case theoretical bounds of 15k-9 and 11k-9 respectively, on the\nnumber of leaves of the two reduced trees, where k is the TBR distance,\nobserving in both cases a far larger reduction in practice. As a by-product of\nour experimental framework we obtain a number of new insights into the actual\ncomputation of TBR distance. We find, for example, that very strong lower\nbounds on TBR distance can be obtained efficiently by randomly sampling certain\ncarefully constructed partitions of the leaf labels, and identify instances\nwhich seem particularly challenging to solve exactly. The reduction rules have\nbeen implemented within our new solver Tubro which combines kernelization with\nan Integer Linear Programming (ILP) approach. Tubro also incorporates a number\nof additional features, such as a cluster reduction and a practical\nupper-bounding heuristic, and it can leverage combinatorial insights emerging\nfrom the proofs of correctness of the reduction rules to simplify the ILP.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 09:09:52 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["van Wersch", "Rim", ""], ["Kelk", "Steven", ""], ["Linz", "Simone", ""], ["Stamoulis", "Georgios", ""]]}, {"id": "2012.07457", "submitter": "Fabian Klute", "authors": "Robert Ganian, Thekla Hamm, Fabian Klute, Irene Parada, Birgit\n  Vogtenhuber", "title": "Crossing-Optimal Extension of Simple Drawings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In extension problems of partial graph drawings one is given an incomplete\ndrawing of an input graph $G$ and is asked to complete the drawing while\nmaintaining certain properties. A prominent area where such problems arise is\nthat of crossing minimization. For plane drawings and various relaxations of\nthese, there is a number of tractability as well as lower-bound results\nexploring the computational complexity of crossing-sensitive drawing extension\nproblems. In contrast, comparatively few results are known on extension\nproblems for the fundamental and broad class of simple drawings, that is,\ndrawings in which each pair of edges intersects in at most one point. In fact,\nonly recently it has been shown that the extension problem of simple drawings\nis NP-hard even when the task is to insert a single edge.\n  In this paper we present tractability results for the crossing-sensitive\nextension problem of simple drawings. In particular, we show that the problem\nof inserting edges into a simple drawing is fixed-parameter tractable when\nparameterized by the number of edges to insert and an upper bound on newly\ncreated crossings. Using the same proof techniques, we are also able to answer\nseveral closely related variants of this problem, among others the extension\nproblem for $k$-plane drawings. Moreover, using a different approach, we\nprovide a single-exponential fixed-parameter algorithm for the case in which we\nare only trying to insert a single edge into the drawing.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 12:06:51 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Ganian", "Robert", ""], ["Hamm", "Thekla", ""], ["Klute", "Fabian", ""], ["Parada", "Irene", ""], ["Vogtenhuber", "Birgit", ""]]}, {"id": "2012.07764", "submitter": "Laurent Guigues", "authors": "Laurent Guigues", "title": "Concerning Iterative Graph Normalization and Maximum Weight Independent\n  Sets", "comments": "34 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO math.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a very simple dynamical system on weighted graphs which we call\nIterative Graph Normalization (IGN) and a variant in which we apply a\nnon-linear activation function to the weights after each normalization. We show\nthat the indicator vectors of the Maximal Independent Sets of the graph are the\nonly binary fixed points of IGN, that they are attractive under simple\nconditions on the activation function and we characterize their basins of\nattraction. We enumerate a number of other fixed points and we prove\nrepulsivity for some classes. Based on extensive experiments and different\ntheoretical arguments we conjecture that IGN always converges and converges to\na binary solution for non-linear activations. If our conjectures are correct,\nIGN would thus be a differentiable approximation algorithm for the Maximum\nWeight Independent Set problem (MWIS), a central NP-hard optimization problem\nwith numerous applications. IGN is closely related to a greedy approximation\nalgorithm of MWIS by Kako et al. which has a proven approximation ratio.\nExperimental results show that IGN provides solutions of very similar quality.\nIn the context of the Assignment Problem, IGN corresponds to an iterative\nmatrix normalization scheme which is closely related to the Sinkhorn-Knopp\nalgorithm except that it projects to a permutation matrix instead of a doubly\nstochastic matrix. We relate our scheme to the Softassign algorithm and provide\ncomparative results. As Graph Normalization is differentiable, its iterations\ncan be embedded into a machine learning framework and used to train end-to-end\nany model which includes a graphical optimization step which can be cast as a\nmaximum weight independent set problem. This includes problems such as graph\nand hypergraph matching, sequence alignment, clustering, ranking, etc. with\napplications in multiple domains.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 17:59:48 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Guigues", "Laurent", ""]]}, {"id": "2012.07774", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane", "title": "Small Covers for Near-Zero Sets of Polynomials and Learning Latent\n  Variable Models", "comments": "Full version of FOCS'20 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS math.AG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $V$ be any vector space of multivariate degree-$d$ homogeneous\npolynomials with co-dimension at most $k$, and $S$ be the set of points where\nall polynomials in $V$ {\\em nearly} vanish. We establish a qualitatively\noptimal upper bound on the size of $\\epsilon$-covers for $S$, in the\n$\\ell_2$-norm. Roughly speaking, we show that there exists an $\\epsilon$-cover\nfor $S$ of cardinality $M = (k/\\epsilon)^{O_d(k^{1/d})}$. Our result is\nconstructive yielding an algorithm to compute such an $\\epsilon$-cover that\nruns in time $\\mathrm{poly}(M)$.\n  Building on our structural result, we obtain significantly improved learning\nalgorithms for several fundamental high-dimensional probabilistic models with\nhidden variables. These include density and parameter estimation for\n$k$-mixtures of spherical Gaussians (with known common covariance), PAC\nlearning one-hidden-layer ReLU networks with $k$ hidden units (under the\nGaussian distribution), density and parameter estimation for $k$-mixtures of\nlinear regressions (with Gaussian covariates), and parameter estimation for\n$k$-mixtures of hyperplanes. Our algorithms run in time {\\em quasi-polynomial}\nin the parameter $k$. Previous algorithms for these problems had running times\nexponential in $k^{\\Omega(1)}$.\n  At a high-level our algorithms for all these learning problems work as\nfollows: By computing the low-degree moments of the hidden parameters, we are\nable to find a vector space of polynomials that nearly vanish on the unknown\nparameters. Our structural result allows us to compute a quasi-polynomial sized\ncover for the set of hidden parameters, which we exploit in our learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 18:14:08 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""]]}, {"id": "2012.07892", "submitter": "Felipe A. Louza", "authors": "Felipe A. Louza, Neerja Mhaskar, W. F. Smyth", "title": "A New Approach to Regular & Indeterminate Strings", "comments": "Accepted to TCS", "journal-ref": null, "doi": "10.1016/j.tcs.2020.12.007", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose a new, more appropriate definition of regular and\nindeterminate strings. A regular string is one that is \"isomorphic\" to a string\nwhose entries all consist of a single letter, but which nevertheless may itself\ninclude entries containing multiple letters. A string that is not regular is\nsaid to be indeterminate. We begin by proposing a new model for the\nrepresentation of strings, regular or indeterminate, then go on to describe a\nlinear time algorithm to determine whether or not a string $x = x[1..n]$ is\nregular and, if so, to replace it by a lexicographically least (lex-least)\nstring $y$ whose entries are all single letters. Furthermore, we connect the\nregularity of a string to the transitive closure problem on a graph, which in\nour special case can be efficiently solved. We then introduce the idea of a\nfeasible palindrome array MP of a string, and prove that every feasible MP\ncorresponds to some (regular or indeterminate) string. We describe an algorithm\nthat constructs a string $x$ corresponding to given feasible MP, while ensuring\nthat whenever possible $x$ is regular and if so, then lex-least. A final\nsection outlines new research directions suggested by this changed perspective\non regular and indeterminate strings.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 19:15:19 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Louza", "Felipe A.", ""], ["Mhaskar", "Neerja", ""], ["Smyth", "W. F.", ""]]}, {"id": "2012.07936", "submitter": "Lan N. Nguyen", "authors": "Lan N. Nguyen and My T. Thai", "title": "Minimum Robust Multi-Submodular Cover for Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study a novel problem, Minimum Robust Multi-Submodular\nCover for Fairness (MinRF), as follows: given a ground set $V$; $m$ monotone\nsubmodular functions $f_1,...,f_m$; $m$ thresholds $T_1,...,T_m$ and a\nnon-negative integer $r$, MinRF asks for the smallest set $S$ such that for all\n$i \\in [m]$, $\\min_{|X| \\leq r} f_i(S \\setminus X) \\geq T_i$. We prove that\nMinRF is inapproximable within $(1-\\epsilon)\\ln m$; and no algorithm, taking\nfewer than exponential number of queries in term of $r$, is able to output a\nfeasible set to MinRF with high certainty. Three bicriteria approximation\nalgorithms with performance guarantees are proposed: one for $r=0$, one for\n$r=1$, and one for general $r$. We further investigate our algorithms'\nperformance in two applications of MinRF, Information Propagation for Multiple\nGroups and Movie Recommendation for Multiple Users. Our algorithms have shown\nto outperform baseline heuristics in both solution quality and the number of\nqueries in most cases.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 20:54:12 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Nguyen", "Lan N.", ""], ["Thai", "My T.", ""]]}, {"id": "2012.08083", "submitter": "Alireza Samadian", "authors": "Mahmoud Abo-Khamis, Sungjin Im, Benjamin Moseley, Kirk Pruhs, Alireza\n  Samadian", "title": "Instance Optimal Join Size Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of efficiently estimating the size of the inner join\nof a collection of preprocessed relational tables from the perspective of\ninstance optimality analysis. The run time of instance optimal algorithms is\ncomparable to the minimum time needed to verify the correctness of a solution.\nPreviously instance optimal algorithms were only known when the size of the\njoin was small (as one component of their run time that was linear in the join\nsize). We give an instance optimal algorithm for estimating the join size for\nall instances, including when the join size is large, by removing the\ndependency on the join size. As a byproduct, we show how to sample rows from\nthe join uniformly at random in a comparable amount of time.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 04:24:18 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Abo-Khamis", "Mahmoud", ""], ["Im", "Sungjin", ""], ["Moseley", "Benjamin", ""], ["Pruhs", "Kirk", ""], ["Samadian", "Alireza", ""]]}, {"id": "2012.08152", "submitter": "Boris Goldengorin", "authors": "Artem Fomin and Boris Goldengorin", "title": "An efficient model for the preemptive single machine scheduling of\n  equal-length jobs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.DS math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a Boolean Linear Programming model for the preemptive single\nmachine scheduling problem with equal processing times, arbitrary release dates\nand weights(priorities) minimizing the total weighted completion time. Almost\nalways an optimal solution of the Linear Programming relaxation is integral and\ncan be straightforwardly converted into an optimal schedule. To deal with the\nfractional solutions we present two heuristics. Very often our heuristics find\nsolutions with objective function values equal to the lower bound found by the\nLinear Programming relaxation. For the cases when upper bound returned by our\nheuristics differs from the lower bound we embed the bounds into a Branch and\nBound algorithm, which solves the problem to optimality. Exhaustive\ncomputational study showed that the algorithm substantially surpasses\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 08:48:37 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Fomin", "Artem", ""], ["Goldengorin", "Boris", ""]]}, {"id": "2012.08346", "submitter": "Daniel Dadush", "authors": "Sander Borst, Daniel Dadush, Sophie Huiberts, Samarth Tiwari", "title": "On the Integrality Gap of Binary Integer Programs with Gaussian Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a binary integer program (IP) ${\\rm max} ~ c^\\mathsf{T} x, Ax \\leq b, x\n\\in \\{0,1\\}^n$, where $A \\in \\mathbb{R}^{m \\times n}$ and $c \\in \\mathbb{R}^n$\nhave independent Gaussian entries and the right-hand side $b \\in \\mathbb{R}^m$\nsatisfies that its negative coordinates have $\\ell_2$ norm at most $n/10$, we\nprove that the gap between the value of the linear programming relaxation and\nthe IP is upper bounded by $\\operatorname{poly}(m)(\\log n)^2 / n$ with\nprobability at least $1-2/n^7-2^{-\\operatorname{poly}(m)}$. Our results give a\nGaussian analogue of the classical integrality gap result of Dyer and Frieze\n(Math. of O.R., 1989) in the case of random packing IPs. In constrast to the\npacking case, our integrality gap depends only polynomially on $m$ instead of\nexponentially. Building upon recent breakthrough work of Dey, Dubey and\nMolinaro (SODA, 2021), we show that the integrality gap implies that\nbranch-and-bound requires $n^{\\operatorname{poly}(m)}$ time on random Gaussian\nIPs with good probability, which is polynomial when the number of constraints\n$m$ is fixed. We derive this result via a novel meta-theorem, which relates the\nsize of branch-and-bound trees and the integrality gap for random logconcave\nIPs.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 14:59:44 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 15:15:36 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Borst", "Sander", ""], ["Dadush", "Daniel", ""], ["Huiberts", "Sophie", ""], ["Tiwari", "Samarth", ""]]}, {"id": "2012.08379", "submitter": "Vladimir Shenmaier", "authors": "Vladimir Shenmaier", "title": "Efficient PTAS for the Maximum Traveling Salesman Problem in a Metric\n  Space of Fixed Doubling Dimension", "comments": null, "journal-ref": null, "doi": "10.1007/s11590-021-01769-2", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum traveling salesman problem (Max TSP) consists of finding a\nHamiltonian cycle with the maximum total weight of the edges in a given\ncomplete weighted graph. This problem is APX-hard in the general metric case\nbut admits polynomial-time approximation schemes in the geometric setting, when\nthe edge weights are induced by a vector norm in fixed-dimensional real space.\nWe propose the first approximation scheme for Max TSP in an arbitrary metric\nspace of fixed doubling dimension. The proposed algorithm implements an\nefficient PTAS which, for any fixed $\\varepsilon\\in(0,1)$, computes a\n$(1-\\varepsilon)$-approximate solution of the problem in cubic time.\nAdditionally, we suggest a cubic-time algorithm which finds asymptotically\noptimal solutions of the metric Max TSP in fixed and sublogarithmic doubling\ndimensions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 15:52:51 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 21:09:33 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 18:52:47 GMT"}, {"version": "v4", "created": "Mon, 8 Mar 2021 18:54:44 GMT"}, {"version": "v5", "created": "Tue, 8 Jun 2021 15:55:59 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Shenmaier", "Vladimir", ""]]}, {"id": "2012.08466", "submitter": "Dmitrii Avdiukhin", "authors": "Stanislav Naumov, Grigory Yaroslavtsev, Dmitrii Avdiukhin", "title": "Objective-Based Hierarchical Clustering of Deep Embedding Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We initiate a comprehensive experimental study of objective-based\nhierarchical clustering methods on massive datasets consisting of deep\nembedding vectors from computer vision and NLP applications. This includes a\nlarge variety of image embedding (ImageNet, ImageNetV2, NaBirds), word\nembedding (Twitter, Wikipedia), and sentence embedding (SST-2) vectors from\nseveral popular recent models (e.g. ResNet, ResNext, Inception V3, SBERT). Our\nstudy includes datasets with up to $4.5$ million entries with embedding\ndimensions up to $2048$.\n  In order to address the challenge of scaling up hierarchical clustering to\nsuch large datasets we propose a new practical hierarchical clustering\nalgorithm B++&C. It gives a 5%/20% improvement on average for the popular\nMoseley-Wang (MW) / Cohen-Addad et al. (CKMM) objectives (normalized) compared\nto a wide range of classic methods and recent heuristics. We also introduce a\ntheoretical algorithm B2SAT&C which achieves a $0.74$-approximation for the\nCKMM objective in polynomial time. This is the first substantial improvement\nover the trivial $2/3$-approximation achieved by a random binary tree. Prior to\nthis work, the best poly-time approximation of $\\approx 2/3 + 0.0004$ was due\nto Charikar et al. (SODA'19).\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:08:34 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Naumov", "Stanislav", ""], ["Yaroslavtsev", "Grigory", ""], ["Avdiukhin", "Dmitrii", ""]]}, {"id": "2012.08476", "submitter": "Lorenzo Balzotti", "authors": "Lorenzo Balzotti", "title": "A New Algorithm to Recognize Path Graphs", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Path Graph is the intersection graph of vertex paths in an undirected tree.\nWe present a new algorithm to recognize Path Graphs. It has the same worst-case\ntime complexity as the faster recognition algorithm known so far [A.A.\nSch\\\"affer, A faster algorithm to recognize undirected path graphs, Discrete\nApplied Mathematics, 43 (1993), pp. 261-295.] but it has an easier and more\nintuitive implementation based on a new characterization of Path Graphs [N.\nApollonio and L. Balzotti, A New Characterization of Path Graphs, CoRR,\nabs/1911.09069, (2019).].\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:19:17 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Balzotti", "Lorenzo", ""]]}, {"id": "2012.08589", "submitter": "Albert Tedja", "authors": "Albert Tedja", "title": "Sorting Lists with Equal Keys Using Mergesort in Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article introduces a new optimization method to improve mergesort's\nruntime complexity, when sorting sequences that have equal keys to $O(n log_2\nk)$, where $k$ is the number of distinct keys in the sequence. When $k$ is\nconstant, it is evident that mergesort is capable of achieving linear time by\nutilizing linked lists as its underlying data structure. Mergesort linked list\nimplementations can be optimized by introducing a new mechanism to group\nelements with equal keys together, thus allowing merge algorithm to achieve\nlinear time.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 20:00:01 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Tedja", "Albert", ""]]}, {"id": "2012.08665", "submitter": "Mark Huber", "authors": "Mark Huber", "title": "Generating from the Strauss Process using stitching", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The STrauss process is a point process with unnormalized density with respect\nto a Poisson point process, where each pair of points within a specified\ndistance $r$ of each other contributes a factor $\\lambda \\in (0, 1)$ to the\ndensity. Basic Acceptance Rejection works spectacularly poorly for this\nproblem, which is why several other perfect simulation methods have been\ndeveloped. these methods, however, also work poorly for reasonably large values\nof $\\lambda$. *Acceptance Rejection Stitching* is a new method that works much\nfaster, allowing the simulation of point processes with values of $\\lambda$\nmuch larger than ever before.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 23:17:43 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Huber", "Mark", ""]]}, {"id": "2012.08735", "submitter": "Li-Yang Tan", "authors": "Guy Blanc, Jane Lange, Li-Yang Tan", "title": "Testing and reconstruction via decision trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sublinear and local computation algorithms for decision trees,\nfocusing on testing and reconstruction. Our first result is a tester that runs\nin $\\mathrm{poly}(\\log s, 1/\\varepsilon)\\cdot n\\log n$ time, makes\n$\\mathrm{poly}(\\log s,1/\\varepsilon)\\cdot \\log n$ queries to an unknown\nfunction $f$, and:\n  $\\circ$ Accepts if $f$ is $\\varepsilon$-close to a size-$s$ decision tree;\n  $\\circ$ Rejects if $f$ is $\\Omega(\\varepsilon)$-far from decision trees of\nsize $s^{\\tilde{O}((\\log s)^2/\\varepsilon^2)}$.\n  Existing testers distinguish size-$s$ decision trees from those that are\n$\\varepsilon$-far from from size-$s$ decision trees in\n$\\mathrm{poly}(s^s,1/\\varepsilon)\\cdot n$ time with $\\tilde{O}(s/\\varepsilon)$\nqueries. We therefore solve an incomparable problem, but achieve\ndoubly-exponential-in-$s$ and exponential-in-$s$ improvements in time and query\ncomplexities respectively. We obtain our tester by designing a reconstruction\nalgorithm for decision trees: given query access to a function $f$ that is\nclose to a small decision tree, this algorithm provides fast query access to a\nsmall decision tree that is close to $f$. By known relationships, our results\nyield reconstruction algorithms for numerous other boolean function properties\n-- Fourier degree, randomized and quantum query complexities, certificate\ncomplexity, sensitivity, etc. -- which in turn yield new testers for these\nproperties. Finally, we give a hardness result for testing whether an unknown\nfunction is $\\varepsilon$-close-to or $\\Omega(\\varepsilon)$-far-from size-$s$\ndecision trees. We show that an efficient algorithm for this task would yield\nan efficient algorithm for properly learning decision trees, a central open\nproblem of learning theory. It has long been known that proper learning\nalgorithms for any class $\\mathcal{H}$ yield property testers for\n$\\mathcal{H}$; this provides an example of a converse.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 04:18:00 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Blanc", "Guy", ""], ["Lange", "Jane", ""], ["Tan", "Li-Yang", ""]]}, {"id": "2012.08874", "submitter": "Santiago Andr\\'es Azcoitia", "authors": "Santiago Andr\\'es Azcoitia, Nikolaos Laoutaris", "title": "Try Before You Buy: A practical data purchasing algorithm for real-world\n  data marketplaces", "comments": "20 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data trading is becoming increasingly popular, as evident by the appearance\nof scores of Data Marketplaces (DMs) in the last few years. Pricing digital\nassets is particularly complex since, unlike physical assets, digital ones can\nbe replicated at zero cost, stored, and transmitted almost for free, etc. In\nmost DMs, data sellers are invited to indicate a price, together with a\ndescription of their datasets. For data buyers, however, deciding whether\npaying the requested price makes sense, can only be done after having used the\ndata with their AI/ML algorithms. Theoretical works have analysed the problem\nof which datasets to buy, and at what price, in the context of full information\nmodels, in which the performance of algorithms over any of the O(2^N) possible\nsubsets of N datasets is known a priori, together with the value functions of\nbuyers. Such information is, however, difficult to compute, let alone be made\npublic in the context of real-world DMs.\n  In this paper, we show that if a DM provides to potential buyers a measure of\nthe performance of their AI/ML algorithm on individual datasets, then they can\nselect which datasets to buy with an efficacy that approximates that of a\ncomplete information model. We call the resulting algorithm Try Before You Buy\n(TBYB) and demonstrate over synthetic and real-world datasets how TBYB can lead\nto near optimal buying performance with only O(N) instead of O(2^N) information\nreleased by a marketplace.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 11:32:00 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Azcoitia", "Santiago Andr\u00e9s", ""], ["Laoutaris", "Nikolaos", ""]]}, {"id": "2012.08878", "submitter": "Bastien Cazaux", "authors": "Bastien Cazaux and Eric Rivals", "title": "Greedy-reduction from Shortest Linear Superstring to Shortest Circular\n  Superstring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A superstring of a set of strings correspond to a string which contains all\nthe other strings as substrings. The problem of finding the Shortest Linear\nSuperstring is a well-know and well-studied problem in stringology. We present\nhere a variant of this problem, the Shortest Circular Superstring problem where\nthe sought superstring is a circular string. We show a strong link between\nthese two problems and prove that the Shortest Circular Superstring problem is\nNP-complete. Moreover, we propose a new conjecture on the approximation ratio\nof the Shortest Circular Superstring problem.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 11:41:45 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Cazaux", "Bastien", ""], ["Rivals", "Eric", ""]]}, {"id": "2012.08897", "submitter": "David Schaller", "authors": "David Schaller, Manuel Lafond, Peter F. Stadler, Nicolas Wieseke and\n  Marc Hellmuth", "title": "Indirect Identification of Horizontal Gene Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DM cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Several implicit methods to infer Horizontal Gene Transfer (HGT) focus on\npairs of genes that have diverged only after the divergence of the two species\nin which the genes reside. This situation defines the edge set of a graph, the\nlater-divergence-time (LDT) graph, whose vertices correspond to genes colored\nby their species. We investigate these graphs in the setting of relaxed\nscenarios, i.e., evolutionary scenarios that encompass all commonly used\nvariants of duplication-transfer-loss scenarios in the literature. We\ncharacterize LDT graphs as a subclass of properly vertex-colored cographs, and\nprovide a polynomial-time recognition algorithm as well as an algorithm to\nconstruct a relaxed scenario that explains a given LDT. An edge in an LDT graph\nimplies that the two corresponding genes are separated by at least one HGT\nevent. The converse is not true, however. We show that the complete xenology\nrelation is described by an rs-Fitch graph, i.e., a complete multipartite graph\nsatisfying constraints on the vertex coloring. This class of vertex-colored\ngraphs is also recognizable in polynomial time. We finally address the question\n\"how much information about all HGT events is contained in LDT graphs\" with the\nhelp of simulations of evolutionary scenarios with a wide range of duplication,\nloss, and HGT events. In particular, we show that a simple greedy graph editing\nscheme can be used to efficiently detect HGT events that are implicitly\ncontained in LDT graphs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 12:18:42 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 15:24:27 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Schaller", "David", ""], ["Lafond", "Manuel", ""], ["Stadler", "Peter F.", ""], ["Wieseke", "Nicolas", ""], ["Hellmuth", "Marc", ""]]}, {"id": "2012.08909", "submitter": "Subhrangsu Mandal", "authors": "Subhrangsu Mandal, Arobinda Gupta", "title": "Maximum 0-1 Timed Matching on Temporal Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal graphs are graphs where the topology and/or other properties of the\ngraph change with time. They have been used to model applications with temporal\ninformation in various domains. Problems on static graphs become more\nchallenging to solve in temporal graphs because of dynamically changing\ntopology, and many recent works have explored graph problems on temporal\ngraphs. In this paper, we define a type of matching called {\\em 0-1 timed\nmatching} for temporal graphs, and investigate the problem of finding a {\\em\nmaximum 0-1 timed matching} for different classes of temporal graphs. We first\nprove that the problem is NP-Complete for rooted temporal trees when each edge\nis associated with two or more time intervals. We then propose an $O(n^3)$ time\nalgorithm for the problem on a rooted temporal tree with $n$ nodes when each\nedge is associated with exactly one time interval. The problem is then shown to\nbe NP-Complete also for bipartite temporal graphs even when each edge is\nassociated with a single time interval and degree of each node is bounded by a\nconstant $k \\geq 3$. We next investigate approximation algorithms for the\nproblem for temporal graphs where each edge is associated with more than one\ntime intervals. It is first shown that there is no\n$\\frac{1}{n^{1-\\epsilon}}$-factor approximation algorithm for the problem for\nany $\\epsilon > 0$ even on a rooted temporal tree with $n$ nodes unless NP =\nZPP. We then present a $\\frac{1}{\\mathcal{N}^* + 1}$-factor approximation\nalgorithm for the problem for general temporal graphs where $\\mathcal{N^*}$ is\nthe average number of edges overlapping in time with each edge in the temporal\ngraph. The same algorithm is also a constant-factor approximation algorithm for\ndegree bounded temporal graphs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 12:40:21 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Mandal", "Subhrangsu", ""], ["Gupta", "Arobinda", ""]]}, {"id": "2012.09063", "submitter": "Manish Bansal", "authors": "Manish Bansal and Parshin Shojaee", "title": "Planar Maximum Coverage Location Problem with Partial Coverage,\n  Continuous Spatial Demand, and Adjustable Quality of Service", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider a generalization of the classical planar maximum coverage\nlocation problem (PMCLP) in which partial coverage is allowed, facilities have\nadjustable quality of service (QoS) or service range, and demand zones and\nservice zone of each facility are represented by two-dimensional spatial\nobjects such as rectangles, circles, polygons, etc. We denote this\ngeneralization by PMCLP-PC-QoS. A key challenge in this problem is to\nsimultaneously decide position of the facilities on a continuous\ntwo-dimensional plane and their QoS. We present a greedy algorithm and a\npseudo-greedy algorithm for it, and provide their approximation ratios. We also\ninvestigate theoretical properties and propose exact algorithms for solving:\n(1) PMCLP-PC-QoS where demand and service zones are represented by\naxis-parallel rectangles (denoted by PMCLP-PCR-QoS), which also has\napplications in camera surveillance and satellite imaging; and (2) one\ndimensional PMCLP-PC-QoS which has applications in river cleanups. These\nresults extend and strengthen the only known exact algorithm for PMCLP-PCR-QoS\nwith fixed and same QoS by Bansal and Kianfar [INFORMS Journal on Computing\n29(1), 152-169, 2017]. We present results of our computational experiments\nconducted to evaluate the performance of our proposed exact and approximation\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 16:32:40 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Bansal", "Manish", ""], ["Shojaee", "Parshin", ""]]}, {"id": "2012.09116", "submitter": "Pasin Manurangsi", "authors": "Badih Ghazi, Ravi Kumar, Pasin Manurangsi", "title": "On Avoiding the Union Bound When Answering Multiple Differentially\n  Private Queries", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the problem of answering $k$ queries with $(\\epsilon,\n\\delta)$-differential privacy, where each query has sensitivity one. We give an\nalgorithm for this task that achieves an expected $\\ell_\\infty$ error bound of\n$O(\\frac{1}{\\epsilon}\\sqrt{k \\log \\frac{1}{\\delta}})$, which is known to be\ntight (Steinke and Ullman, 2016).\n  A very recent work by Dagan and Kur (2020) provides a similar result, albeit\nvia a completely different approach. One difference between our work and theirs\nis that our guarantee holds even when $\\delta < 2^{-\\Omega(k/(\\log k)^8)}$\nwhereas theirs does not apply in this case. On the other hand, the algorithm of\nDagan and Kur has a remarkable advantage that the $\\ell_{\\infty}$ error bound\nof $O(\\frac{1}{\\epsilon}\\sqrt{k \\log \\frac{1}{\\delta}})$ holds not only in\nexpectation but always (i.e., with probability one) while we can only get a\nhigh probability (or expected) guarantee on the error.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 17:58:45 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Ghazi", "Badih", ""], ["Kumar", "Ravi", ""], ["Manurangsi", "Pasin", ""]]}, {"id": "2012.09153", "submitter": "Tuukka Korhonen", "authors": "Tuukka Korhonen", "title": "Listing Small Minimal Separators of a Graph", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a graph and $a,b$ vertices of $G$. A minimal $a,b$-separator of\n$G$ is an inclusion-wise minimal vertex set of $G$ that separates $a$ and $b$.\nWe consider the problem of enumerating the minimal $a,b$-separators of $G$ that\ncontain at most $k$ vertices, given some integer $k$. We give an algorithm\nwhich enumerates such minimal separators, outputting the first $R$ minimal\nseparators in at most $poly(n) R \\cdot \\min(4^k, R)$ time for all $R$.\nTherefore, our algorithm can be classified as fixed-parameter-delay and\nincremental-polynomial time. To the best of our knowledge, no algorithms with\nnon-trivial time complexity have been published for this problem before. We\nalso discuss barriers for obtaining a polynomial-delay algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 18:45:03 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Korhonen", "Tuukka", ""]]}, {"id": "2012.09194", "submitter": "Yuan Su", "authors": "Yuan Su, Hsin-Yuan Huang, Earl T. Campbell", "title": "Nearly tight Trotterization of interacting electrons", "comments": "58 pages, 2 figures", "journal-ref": "Quantum 5, 495 (2021)", "doi": "10.22331/q-2021-07-05-495", "report-no": null, "categories": "quant-ph cond-mat.str-el cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider simulating quantum systems on digital quantum computers. We show\nthat the performance of quantum simulation can be improved by simultaneously\nexploiting commutativity of the target Hamiltonian, sparsity of interactions,\nand prior knowledge of the initial state. We achieve this using Trotterization\nfor a class of interacting electrons that encompasses various physical systems,\nincluding the plane-wave-basis electronic structure and the Fermi-Hubbard\nmodel. We estimate the simulation error by taking the transition amplitude of\nnested commutators of the Hamiltonian terms within the $\\eta$-electron\nmanifold. We develop multiple techniques for bounding the transition amplitude\nand expectation of general fermionic operators, which may be of independent\ninterest. We show that it suffices to use\n$\\left(\\frac{n^{5/3}}{\\eta^{2/3}}+n^{4/3}\\eta^{2/3}\\right)n^{o(1)}$ gates to\nsimulate electronic structure in the plane-wave basis with $n$ spin orbitals\nand $\\eta$ electrons, improving the best previous result in second quantization\nup to a negligible factor while outperforming the first-quantized simulation\nwhen $n=\\eta^{2-o(1)}$. We also obtain an improvement for simulating the\nFermi-Hubbard model. We construct concrete examples for which our bounds are\nalmost saturated, giving a nearly tight Trotterization of interacting\nelectrons.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 19:00:05 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 18:00:02 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Su", "Yuan", ""], ["Huang", "Hsin-Yuan", ""], ["Campbell", "Earl T.", ""]]}, {"id": "2012.09202", "submitter": "Pedro Felzenszwalb", "authors": "Pedro Felzenszwalb, Caroline Klivans, Alice Paul", "title": "Clustering with Iterated Linear Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel method for clustering using a semidefinite programming\n(SDP) relaxation of the Max k-Cut problem. The approach is based on a new\nmethodology for rounding the solution of an SDP using iterated linear\noptimization. We show the vertices of the Max k-Cut SDP relaxation correspond\nto partitions of the data into at most k sets. We also show the vertices are\nattractive fixed points of iterated linear optimization. We interpret the\nprocess of fixed point iteration with linear optimization as repeated\nrelaxations of the closest vertex problem. Our experiments show that using\nfixed point iteration for rounding the Max k-Cut SDP relaxation leads to\nsignificantly better results when compared to randomized rounding.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 19:01:13 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Felzenszwalb", "Pedro", ""], ["Klivans", "Caroline", ""], ["Paul", "Alice", ""]]}, {"id": "2012.09376", "submitter": "Qisheng Wang", "authors": "Qisheng Wang and Mingsheng Ying", "title": "Quantum Algorithm for Lexicographically Minimal String Rotation", "comments": "40 pages, 6 algorithms, minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexicographically minimal string rotation (LMSR) is a problem to find the\nminimal one among all rotations of a string in the lexicographical order, which\nis widely used in equality checking of graphs, polygons, automata and chemical\nstructures.\n  In this paper, we propose an $O(n^{3/4})$ quantum query algorithm for LMSR.\nIn particular, the algorithm has average-case query complexity $O(\\sqrt n \\log\nn)$, which is shown to be asymptotically optimal up to a polylogarithmic\nfactor, compared with its $\\Omega\\left(\\sqrt{n/\\log n}\\right)$ lower bound.\nFurthermore, we claim that our quantum algorithm outperforms any (classical)\nrandomized algorithms in both worst-case and average-case query complexities by\nshowing that every (classical) randomized algorithm for LMSR has worst-case\nquery complexity $\\Omega(n)$ and average-case query complexity $\\Omega(n/\\log\nn)$.\n  Our quantum algorithm for LMSR is developed in a framework of nested quantum\nalgorithms, based on two new results: (i) an $O(\\sqrt{n})$ (optimal) quantum\nminimum finding on bounded-error quantum oracles; and (ii) its $O\\left(\\sqrt{n\n\\log(1/\\varepsilon)}\\right)$ (optimal) error reduction. As a byproduct, we\nobtain some better upper bounds of independent interest: (i) $O(\\sqrt{N})$\n(optimal) for constant-depth MIN-MAX trees on $N$ variables; and (ii)\n$O(\\sqrt{n \\log m})$ for pattern matching which removes\n$\\operatorname{polylog}(n)$ factors.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 03:13:45 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 12:45:42 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Wang", "Qisheng", ""], ["Ying", "Mingsheng", ""]]}, {"id": "2012.09385", "submitter": "Daniel McKenzie", "authors": "Anna Little, Daniel McKenzie and James Murphy", "title": "Balancing Geometry and Density: Path Distances on High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  New geometric and computational analyses of power-weighted shortest-path\ndistances (PWSPDs) are presented. By illuminating the way these metrics balance\ndensity and geometry in the underlying data, we clarify their key parameters\nand discuss how they may be chosen in practice. Comparisons are made with\nrelated data-driven metrics, which illustrate the broader role of density in\nkernel-based unsupervised and semi-supervised machine learning.\nComputationally, we relate PWSPDs on complete weighted graphs to their\nanalogues on weighted nearest neighbor graphs, providing high probability\nguarantees on their equivalence that are near-optimal. Connections with\npercolation theory are developed to establish estimates on the bias and\nvariance of PWSPDs in the finite sample setting. The theoretical results are\nbolstered by illustrative experiments, demonstrating the versatility of PWSPDs\nfor a wide range of data settings. Throughout the paper, our results require\nonly that the underlying data is sampled from a low-dimensional manifold, and\ndepend crucially on the intrinsic dimension of this manifold, rather than its\nambient dimension.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 04:03:15 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 21:39:01 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Little", "Anna", ""], ["McKenzie", "Daniel", ""], ["Murphy", "James", ""]]}, {"id": "2012.09394", "submitter": "Kevin Rao", "authors": "Kevin Rao", "title": "Metrical Task Systems with Online Machine Learned Advice", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning algorithms are designed to make accurate predictions of the\nfuture based on existing data, while online algorithms seek to bound some\nperformance measure (typically the competitive ratio) without knowledge of the\nfuture. Lykouris and Vassilvitskii demonstrated that augmenting online\nalgorithms with a machine learned predictor can provably decrease the\ncompetitive ratio under as long as the predictor is suitably accurate.\n  In this work we apply this idea to the Online Metrical Task System problem,\nwhich was put forth by Borodin, Linial, and Saks as a general model for dynamic\nsystems processing tasks in an online fashion. We focus on the specific class\nof uniform task systems on $n$ tasks, for which the best deterministic\nalgorithm is $O(n)$ competitive and the best randomized algorithm is $O(\\log\nn)$ competitive.\n  By giving an online algorithms access to a machine learned oracle with\nabsolute predictive error bounded above by $\\eta_0$, we construct a\n$\\Theta(\\min(\\sqrt{\\eta_0}, \\log n))$ competitive algorithm for the uniform\ncase of the metrical task systems problem. We also give a $\\Theta(\\log\n\\sqrt{\\eta_0})$ lower bound on the competitive ratio of any randomized\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 04:56:51 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Rao", "Kevin", ""]]}, {"id": "2012.09451", "submitter": "Mingyu Xiao", "authors": "Zhenyu Guo, Mingyu Xiao, Yi Zhou, Dongxiang Zhang, Kian-Lee Tan", "title": "Enhancing Balanced Graph Edge Partition with Effective Local Search", "comments": "To appear in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Graph partition is a key component to achieve workload balance and reduce job\ncompletion time in parallel graph processing systems. Among the various\npartition strategies, edge partition has demonstrated more promising\nperformance in power-law graphs than vertex partition and thereby has been more\nwidely adopted as the default partition strategy by existing graph systems. The\ngraph edge partition problem, which is to split the edge set into multiple\nbalanced parts to minimize the total number of copied vertices, has been widely\nstudied from the view of optimization and algorithms. In this paper, we study\nlocal search algorithms for this problem to further improve the partition\nresults from existing methods. More specifically, we propose two novel\nconcepts, namely adjustable edges and blocks. Based on these, we develop a\ngreedy heuristic as well as an improved search algorithm utilizing the property\nof the max-flow model. To evaluate the performance of our algorithms, we first\nprovide adequate theoretical analysis in terms of the approximation quality. We\nsignificantly improve the previously known approximation ratio for this\nproblem. Then we conduct extensive experiments on a large number of benchmark\ndatasets and state-of-the-art edge partition strategies. The results show that\nour proposed local search framework can further improve the quality of graph\npartition by a wide margin.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 08:58:06 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Guo", "Zhenyu", ""], ["Xiao", "Mingyu", ""], ["Zhou", "Yi", ""], ["Zhang", "Dongxiang", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "2012.09475", "submitter": "Murilo Santos de Lima", "authors": "Magn\\'us M. Halld\\'orsson (1), Murilo S. de Lima (2) ((1) ICE-TCS,\n  Department of Computer Science, Reykjavik University, Iceland, (2) School of\n  Informatics, University of Leicester, United Kingdom)", "title": "Query-Competitive Sorting with Uncertainty", "comments": "Partially supported by Icelandic Research Fund grant 174484-051. A\n  preliminary version of this paper appeared in volume 138 of LIPICs, article\n  7, 2019. DOI: 10.4230/LIPIcs.MFCS.2019.7", "journal-ref": "Theoretical Computer Science 867 (2021) 50-67", "doi": "10.1016/j.tcs.2021.03.021", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of sorting under incomplete information, when queries\nare used to resolve uncertainties. Each of $n$ data items has an unknown value,\nwhich is known to lie in a given interval. We can pay a query cost to learn the\nactual value, and we may allow an error threshold in the sorting. The goal is\nto find a nearly-sorted permutation by performing a minimum-cost set of\nqueries.\n  We show that an offline optimum query set can be found in poly time, and that\nboth oblivious and adaptive problems have simple competitive algorithms. The\ncompetitive ratio for the oblivious problem is $n$ for uniform query costs, and\nunbounded for arbitrary costs; for the adaptive problem, the ratio is 2.\n  We present a unified adaptive strategy for uniform costs that yields the\nfollowing improved results: (1) a 3/2-competitive randomized algorithm; (2) a\n5/3-competitive deterministic algorithm if the dependency graph has no\n2-components after some preprocessing, which has competitive ratio\n$3/2+\\mathrm{O}(1/k)$ if the components obtained have size at least $k$; and\n(3) an exact algorithm for laminar families of intervals. The first two results\nhave matching lower bounds, and we have a lower bound of 7/5 for large\ncomponents.\n  We also give a randomized adaptive algorithm with competitive ratio\n$1+\\frac{4}{3\\sqrt{3}}\\approx 1.7698$ for arbitrary query costs, and we show\nthat the 2-competitive deterministic adaptive algorithm can be generalized for\nqueries returning intervals and for a more general vertex cover problem, by\nusing the local ratio technique. Moreover, we prove that the advice complexity\nof the adaptive problem is $\\lfloor n/2\\rfloor$ if no error threshold is\nallowed, and $\\lceil n/3\\cdot\\lg 3\\rceil$ for the general case.\n  Finally, we present some graph-theoretical results on co-threshold tolerance\ngraphs, and we discuss uncertainty variants of some classical interval\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 09:54:24 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 15:42:36 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Halld\u00f3rsson", "Magn\u00fas M.", ""], ["de Lima", "Murilo S.", ""]]}, {"id": "2012.09502", "submitter": "Nima Anari", "authors": "Nima Anari and Nathan Hu and Amin Saberi and Aaron Schild", "title": "Sampling Arborescences in Parallel", "comments": "To appear in ITCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of sampling a uniformly random directed rooted spanning\ntree, also known as an arborescence, from a possibly weighted directed graph.\nClassically, this problem has long been known to be polynomial-time solvable;\nthe exact number of arborescences can be computed by a determinant [Tut48], and\nsampling can be reduced to counting [JVV86, JS96]. However, the classic\nreduction from sampling to counting seems to be inherently sequential. This\nraises the question of designing efficient parallel algorithms for sampling. We\nshow that sampling arborescences can be done in RNC.\n  For several well-studied combinatorial structures, counting can be reduced to\nthe computation of a determinant, which is known to be in NC [Csa75]. These\ninclude arborescences, planar graph perfect matchings, Eulerian tours in\ndigraphs, and determinantal point processes. However, not much is known about\nefficient parallel sampling of these structures. Our work is a step towards\nresolving this mystery.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 11:01:40 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Anari", "Nima", ""], ["Hu", "Nathan", ""], ["Saberi", "Amin", ""], ["Schild", "Aaron", ""]]}, {"id": "2012.09725", "submitter": "Wenxin Li", "authors": "Wenxin Li", "title": "A Note on Optimizing the Ratio of Monotone Supermodular Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for the problem of minimizing (or maximizing) the ratio of two\nsupermodular functions, no bounded approximation ratio can be achieved via\npolynomial number of queries, if the two supermodular functions are both\nmonotone non-decreasing or non-increasing.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 04:00:21 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Li", "Wenxin", ""]]}, {"id": "2012.09814", "submitter": "Daniel Paulusma", "authors": "Petr A. Golovach and Dani\\\"el Paulusma and Erik Jan van Leeuwen", "title": "Induced Disjoint Paths in AT-free Graphs", "comments": "An extended abstract of this paper appeared in the proceedings of\n  SWAT 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paths $P_1,\\ldots,P_k$ in a graph $G=(V,E)$ are mutually induced if any two\ndistinct $P_i$ and $P_j$ have neither common vertices nor adjacent vertices\n(except perhaps their end-vertices). The Induced Disjoint Paths problem is to\ndecide if a graph $G$ with $k$ pairs of specified vertices $(s_i,t_i)$ contains\n$k$ mutually induced paths $P_i$ such that each $P_i$ connects $s_i$ and $t_i$.\nThis is a classical graph problem that is NP-complete even for $k=2$. We study\nit for AT-free graphs.\n  Unlike its subclasses of permutation graphs and cocomparability graphs, the\nclass of AT-free graphs has no geometric intersection model. However, by a new,\nstructural analysis of the behaviour of Induced Disjoint Paths for AT-free\ngraphs, we prove that it can be solved in polynomial time for AT-free graphs\neven when $k$ is part of the input. This is in contrast to the situation for\nother well-known graph classes, such as planar graphs, claw-free graphs, or\nmore recently, (theta,wheel)-free graphs, for which such a result only holds if\n$k$ is fixed.\n  As a consequence of our main result, the problem of deciding if a given\nAT-free graph contains a fixed graph $H$ as an induced topological minor admits\na polynomial-time algorithm. In addition, we show that such an algorithm is\nessentially optimal by proving that the problem is W[1]-hard with parameter\n$|V_H|$, even on a subclass of AT-free graph, namely cobipartite graphs. We\nalso show that the problems $k$-in-a-Path and $k$-in-a-Tree are polynomial-time\nsolvable on AT-free graphs even if $k$ is part of the input. These problems are\nto test if a graph has an induced path or induced tree, respectively, spanning\n$k$ given vertices.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:32:25 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Golovach", "Petr A.", ""], ["Paulusma", "Dani\u00ebl", ""], ["van Leeuwen", "Erik Jan", ""]]}, {"id": "2012.09990", "submitter": "Won-Yong Shin", "authors": "Cong Tran, Dung D. Vu, Won-Yong Shin", "title": "An Improved Approach for Estimating Social POI Boundaries With Textual\n  Attributes on Social Media", "comments": "13 pages, 6 figures, 5 tables; to appear in the Knowledge-Based\n  Systems (Please cite our journal version that will appear in an upcoming\n  issue.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.DS cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has been insufficiently explored how to perform density-based clustering\nby exploiting textual attributes on social media. In this paper, we aim at\ndiscovering a social point-of-interest (POI) boundary, formed as a convex\npolygon. More specifically, we present a new approach and algorithm, built upon\nour earlier work on social POI boundary estimation (SoBEst). This SoBEst\napproach takes into account both relevant and irrelevant records within a\ngeographic area, where relevant records contain a POI name or its variations in\ntheir text field. Our study is motivated by the following empirical\nobservation: a fixed representative coordinate of each POI that SoBEst\nbasically assumes may be far away from the centroid of the estimated social POI\nboundary for certain POIs. Thus, using SoBEst in such cases may possibly result\nin unsatisfactory performance on the boundary estimation quality (BEQ), which\nis expressed as a function of the $F$-measure. To solve this problem, we\nformulate a joint optimization problem of simultaneously finding the radius of\na circle and the POI's representative coordinate $c$ by allowing to update $c$.\nSubsequently, we design an iterative SoBEst (I-SoBEst) algorithm, which enables\nus to achieve a higher degree of BEQ for some POIs. The computational\ncomplexity of the proposed I-SoBEst algorithm is shown to scale linearly with\nthe number of records. We demonstrate the superiority of our algorithm over\ncompeting clustering methods including the original SoBEst.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 00:41:44 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Tran", "Cong", ""], ["Vu", "Dung D.", ""], ["Shin", "Won-Yong", ""]]}, {"id": "2012.10026", "submitter": "Zite Jiang", "authors": "Zite Jiang, Tao Liu, Shuai Zhang, Zhen Guan, Mengting Yuan, Haihang\n  You", "title": "Fast and Efficient Parallel Breadth-First Search with Power-law Graph\n  Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the big data era, graph computing is widely used to exploit the hidden\nvalue in real-world graphs in various scenarios such as social networks,\nknowledge graphs, web searching, and recommendation systems. However, the\nrandom memory accesses result in inefficient use of cache and the irregular\ndegree distribution leads to substantial load imbalance. Breadth-First Search\n(BFS) is frequently utilized as a kernel for many important and complex graph\nalgorithms. In this paper, we describe a preprocessing approach using Reverse\nCuthill-Mckee (RCM) algorithm to improve data locality and demonstrate how to\nachieve an efficient load balancing for BFS. Computations on RCM-reordered\ngraph data are also accelerated with SIMD executions. We evaluate the\nperformance of the graph preprocessing approach on Kronecker graphs of the\nGraph500 benchmark and real-world graphs. Our BFS implementation on\nRCM-reordered graph data achieves 326.48 MTEPS/W (mega TEPS per watt) on an\nARMv8 system, ranking 2nd on the Green Graph500 list in June 2020 (the 1st rank\nuses GPU acceleration).\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 02:58:00 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Jiang", "Zite", ""], ["Liu", "Tao", ""], ["Zhang", "Shuai", ""], ["Guan", "Zhen", ""], ["Yuan", "Mengting", ""], ["You", "Haihang", ""]]}, {"id": "2012.10092", "submitter": "Shunsuke Inenaga", "authors": "Noriki Fujisato, Yuto Nakashima, Shunsuke Inenaga, Hideo Bannai,\n  Masayuki Takeda", "title": "The Parameterized Suffix Tray", "comments": "Accepted for CIAC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $\\Sigma$ and $\\Pi$ be disjoint alphabets, respectively called the static\nalphabet and the parameterized alphabet. Two strings $x$ and $y$ over $\\Sigma\n\\cup \\Pi$ of equal length are said to parameterized match (p-match) if there\nexists a renaming bijection $f$ on $\\Sigma$ and $\\Pi$ which is identity on\n$\\Sigma$ and maps the characters of $x$ to those of $y$ so that the two strings\nbecome identical. The indexing version of the problem of finding p-matching\noccurrences of a given pattern in the text is a well-studied topic in string\nmatching. In this paper, we present a state-of-the-art indexing structure for\np-matching called the parameterized suffix tray of an input text $T$, denoted\nby $\\mathsf{PSTray}(T)$. We show that $\\mathsf{PSTray}(T)$ occupies $O(n)$\nspace and supports pattern matching queries in $O(m + \\log (\\sigma+\\pi) +\n\\mathit{occ})$ time, where $n$ is the length of $T$, $m$ is the length of a\nquery pattern $P$, $\\pi$ is the number of distinct symbols of $|\\Pi|$ in $T$,\n$\\sigma$ is the number of distinct symbols of $|\\Sigma|$ in $T$ and\n$\\mathit{occ}$ is the number of p-matching occurrences of $P$ in $T$. We also\npresent how to build $\\mathsf{PSTray}(T)$ in $O(n)$ time from the parameterized\nsuffix tree of $T$.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 07:53:48 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 03:26:22 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Fujisato", "Noriki", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "2012.10281", "submitter": "Ohad Trabelsi", "authors": "Amir Abboud, Robert Krauthgamer, Ohad Trabelsi", "title": "Subcubic Algorithms for Gomory-Hu Tree in Unweighted Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every undirected graph $G$ has a (weighted) cut-equivalent tree $T$, commonly\nnamed after Gomory and Hu who discovered it in 1961. Both $T$ and $G$ have the\nsame node set, and for every node pair $s,t$, the minimum $(s,t)$-cut in $T$ is\nalso an exact minimum $(s,t)$-cut in $G$.\n  We give the first subcubic-time algorithm that constructs such a tree for a\nsimple graph $G$ (unweighted with no parallel edges). Its time complexity is\n$\\tilde{O}(n^{2.5})$, for $n=|V(G)|$; previously, only $\\tilde{O}(n^3)$ was\nknown, except for restricted cases like sparse graphs. Consequently, we obtain\nthe first algorithm for All-Pairs Max-Flow in simple graphs that breaks the\ncubic-time barrier.\n  Gomory and Hu compute this tree using $n-1$ queries to (single-pair)\nMax-Flow; the new algorithm can be viewed as a fine-grained reduction to\n$\\tilde{O}(\\sqrt{n})$ Max-Flow computations on $n$-node graphs.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 14:57:34 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 20:55:13 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Abboud", "Amir", ""], ["Krauthgamer", "Robert", ""], ["Trabelsi", "Ohad", ""]]}, {"id": "2012.10307", "submitter": "Debasis Dwibedy", "authors": "Sheetal Swain, Rakesh Mohanty, Debasis Dwibedy", "title": "Results on Competitiveness of Online Shortest Remaining Processing\n  Time(SRPT) Scheduling with Special Classes of Inputs", "comments": "10 pages and 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Shortest Remaining Processing Time (SRPT) is a well known preemptive\nscheduling algorithm for uniprocessor and multiprocessor systems. SRPT finds\napplications in the emerging areas such as scheduling of client's requests that\nare submitted to a web server for accessing static web pages, managing the\naccess requests to files in multiuser database systems and routing of packets\nacross several links as per bandwidth availability in data communications. SRPT\nhas been proved to be optimal for the settings, where the objective is to\nminimize the mean response time of a list of jobs. According to our knowledge,\nthere is less attention on the study of online SRPT with respect to the\nminimization of makespan as a performance criterion. In this paper, we study\nthe SRPT algorithm for online scheduling in multiprocessor systems with\nmakespan minimization as an objective. We obtain improved constant\ncompetitiveness results for algorithm SRPT for special classes of online job\nsequences based on practical real life applications.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 15:52:31 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Swain", "Sheetal", ""], ["Mohanty", "Rakesh", ""], ["Dwibedy", "Debasis", ""]]}, {"id": "2012.10793", "submitter": "Jie Shen", "authors": "Jie Shen", "title": "On the Power of Localized Perceptron for Label-Optimal Learning of\n  Halfspaces with Adversarial Noise", "comments": "V2 and V3 polished writing; accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study {\\em online} active learning of homogeneous halfspaces in\n$\\mathbb{R}^d$ with adversarial noise where the overall probability of a noisy\nlabel is constrained to be at most $\\nu$. Our main contribution is a\nPerceptron-like online active learning algorithm that runs in polynomial time,\nand under the conditions that the marginal distribution is isotropic\nlog-concave and $\\nu = \\Omega(\\epsilon)$, where $\\epsilon \\in (0, 1)$ is the\ntarget error rate, our algorithm PAC learns the underlying halfspace with\nnear-optimal label complexity of $\\tilde{O}\\big(d \\cdot\npolylog(\\frac{1}{\\epsilon})\\big)$ and sample complexity of\n$\\tilde{O}\\big(\\frac{d}{\\epsilon} \\big)$. Prior to this work, existing online\nalgorithms designed for tolerating the adversarial noise are subject to either\nlabel complexity polynomial in $\\frac{1}{\\epsilon}$, or suboptimal noise\ntolerance, or restrictive marginal distributions. With the additional prior\nknowledge that the underlying halfspace is $s$-sparse, we obtain\nattribute-efficient label complexity of $\\tilde{O}\\big( s \\cdot polylog(d,\n\\frac{1}{\\epsilon}) \\big)$ and sample complexity of\n$\\tilde{O}\\big(\\frac{s}{\\epsilon} \\cdot polylog(d) \\big)$. As an immediate\ncorollary, we show that under the agnostic model where no assumption is made on\nthe noise rate $\\nu$, our active learner achieves an error rate of $O(OPT) +\n\\epsilon$ with the same running time and label and sample complexity, where\n$OPT$ is the best possible error rate achievable by any homogeneous halfspace.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 22:04:10 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 04:34:16 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 20:47:56 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Shen", "Jie", ""]]}, {"id": "2012.10884", "submitter": "Yishui Wang", "authors": "Yishui Wang, Rolf H. M\\\"ohring, Chenchen Wu, Dachuan Xu, Dongmei Zhang", "title": "Outliers Detection Is Not So Hard: Approximation Algorithms for Robust\n  Clustering Problems Using Local Search Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider two types of robust models of the\n$k$-median/$k$-means problems: the outlier-version ($k$-MedO/$k$-MeaO) and the\npenalty-version ($k$-MedP/$k$-MeaP), in which we can mark some points as\noutliers and discard them. In $k$-MedO/$k$-MeaO, the number of outliers is\nbounded by a given integer. In $k$-MedP/$k$-MeaP, we do not bound the number of\noutliers, but each outlier will incur a penalty cost. We develop a new\ntechnique to analyze the approximation ratio of local search algorithms for\nthese two problems by introducing an adapted cluster that can capture useful\ninformation about outliers in the local and the global optimal solution. For\n$k$-MeaP, we improve the best known approximation ratio based on local search\nfrom $25+\\varepsilon$ to $9+\\varepsilon$. For $k$-MedP, we obtain the best\nknown approximation ratio. For $k$-MedO/$k$-MeaO, there exists only two\nbi-criteria approximation algorithms based on local search. One violates the\noutlier constraint (the constraint on the number of outliers), while the other\nviolates the cardinality constraint (the constraint on the number of clusters).\nWe consider the former algorithm and improve its approximation ratios from\n$17+\\varepsilon$ to $3+\\varepsilon$ for $k$-MedO, and from $274+\\varepsilon$ to\n$9+\\varepsilon$ for $k$-MeaO.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 10:57:13 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 04:58:23 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Wang", "Yishui", ""], ["M\u00f6hring", "Rolf H.", ""], ["Wu", "Chenchen", ""], ["Xu", "Dachuan", ""], ["Zhang", "Dongmei", ""]]}, {"id": "2012.10919", "submitter": "Antoine Vigneron", "authors": "Corentin Allair and Antoine Vigneron", "title": "Pattern Matching in Doubling Spaces", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of matching a metric space $(X,d_X)$ of size $k$ with\na subspace of a metric space $(Y,d_Y)$ of size $n \\geq k$, assuming that these\ntwo spaces have constant doubling dimension $\\delta$. More precisely, given an\ninput parameter $\\rho \\geq 1$, the $\\rho$-distortion problem is to find a\none-to-one mapping from $X$ to $Y$ that distorts distances by a factor at most\n$\\rho$. We first show by a reduction from $k$-clique that, in doubling\ndimension $\\log_2 3$, this problem is NP-hard and W[1]-hard. Then we provide a\nnear-linear time approximation algorithm for fixed $k$: Given an approximation\nratio $0<\\varepsilon\\leq 1$, and a positive instance of the $\\rho$-distortion\nproblem, our algorithm returns a solution to the\n$(1+\\varepsilon)\\rho$-distortion problem in time $(\\rho/\\varepsilon)^{O(1)}n\n\\log n$. We also show how to extend these results to the minimum distortion\nproblem in doubling spaces: We prove the same hardness results, and for fixed\n$k$, we give a $(1+\\varepsilon)$-approximation algorithm running in time\n$($dist$(X,Y)/\\varepsilon)^{O(1)}n^2\\log n$, where dist$(X,Y)$ denotes the\nminimum distortion between $X$ and $Y$.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 13:33:41 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Allair", "Corentin", ""], ["Vigneron", "Antoine", ""]]}, {"id": "2012.10985", "submitter": "Ori Kelner", "authors": "Ori Kelner", "title": "Learning Halfspaces With Membership Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning is a subfield of machine learning, in which the learning\nalgorithm is allowed to choose the data from which it learns. In some cases, it\nhas been shown that active learning can yield an exponential gain in the number\nof samples the algorithm needs to see, in order to reach generalization error\n$\\leq \\epsilon$. In this work we study the problem of learning halfspaces with\nmembership queries. In the membership query scenario, we allow the learning\nalgorithm to ask for the label of every sample in the input space. We suggest a\nnew algorithm for this problem, and prove it achieves a near optimal label\ncomplexity in some cases. We also show that the algorithm works well in\npractice, and significantly outperforms uncertainty sampling.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 18:02:47 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Kelner", "Ori", ""]]}, {"id": "2012.11098", "submitter": "Ninh Pham", "authors": "Ninh Pham", "title": "Sublinear Maximum Inner Product Search using Concomitants of Extreme\n  Order Statistics", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel dimensionality reduction method for maximum inner product\nsearch (MIPS), named CEOs, based on the theory of concomitants of extreme order\nstatistics. Utilizing the asymptotic behavior of these concomitants, we show\nthat a few dimensions associated with the extreme values of the query signature\nare enough to estimate inner products. Since CEOs only uses the sign of a small\nsubset of the query signature for estimation, we can precompute all inner\nproduct estimators accurately before querying. These properties yield a\nsublinear MIPS algorithm with an exponential indexing space complexity. We show\nthat our exponential space is optimal for the $(1 + \\epsilon)$-approximate MIPS\non a unit sphere. The search recall of CEOs can be theoretically guaranteed\nunder a mild condition.\n  To deal with the exponential space complexity, we propose two practical\nvariants, including sCEOs-TA and coCEOs, that use linear space for solving\nMIPS. sCEOs-TA exploits the threshold algorithm (TA) and provides superior\nsearch recalls to competitive MIPS solvers. coCEOs is a data and dimension\nco-reduction technique and outperforms sCEOs-TA on high recall requirements.\nEmpirically, they are very simple to implement and achieve at least 100x\nspeedup compared to the bruteforce search while returning top-10 MIPS with\naccuracy at least 90% on many large-scale data sets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 03:24:22 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Pham", "Ninh", ""]]}, {"id": "2012.11188", "submitter": "Tom Tseng", "authors": "Tom Tseng, Laxman Dhulipala, Julian Shun", "title": "Parallel Index-Based Structural Graph Clustering and Its Approximation", "comments": null, "journal-ref": null, "doi": "10.1145/3448016.3457278", "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SCAN (Structural Clustering Algorithm for Networks) is a well-studied, widely\nused graph clustering algorithm. For large graphs, however, sequential SCAN\nvariants are prohibitively slow, and parallel SCAN variants do not effectively\nshare work among queries with different SCAN parameter settings. Since users of\nSCAN often explore many parameter settings to find good clusterings, it is\nworthwhile to precompute an index that speeds up queries.\n  This paper presents a practical and provably efficient parallel index-based\nSCAN algorithm based on GS*-Index, a recent sequential algorithm. Our parallel\nalgorithm improves upon the asymptotic work of the sequential algorithm by\nusing integer sorting. It is also highly parallel, achieving logarithmic span\n(parallel time) for both index construction and clustering queries.\nFurthermore, we apply locality-sensitive hashing (LSH) to design a novel\napproximate SCAN algorithm and prove guarantees for its clustering behavior.\n  We present an experimental evaluation of our algorithms on large real-world\ngraphs. On a 48-core machine with two-way hyper-threading, our parallel index\nconstruction achieves 50--151$\\times$ speedup over the construction of\nGS*-Index. In fact, even on a single thread, our index construction algorithm\nis faster than GS*-Index. Our parallel index query implementation achieves\n5--32$\\times$ speedup over GS*-Index queries across a range of SCAN parameter\nvalues, and our implementation is always faster than ppSCAN, a state-of-the-art\nparallel SCAN algorithm. Moreover, our experiments show that applying LSH\nresults in faster index construction while maintaining good clustering quality.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 09:07:44 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 21:51:50 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Tseng", "Tom", ""], ["Dhulipala", "Laxman", ""], ["Shun", "Julian", ""]]}, {"id": "2012.11394", "submitter": "Graham Campbell", "authors": "Graham Campbell and Brian Courtehoute and Detlef Plump", "title": "Fast Rule-Based Graph Programs", "comments": "47 pages, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Implementing graph algorithms efficiently in a rule-based language is\nchallenging because graph pattern matching is expensive. In this paper, we\npresent a number of linear-time implementations of graph algorithms in GP 2, an\nexperimental programming language based on graph transformation rules which\naims to facilitate program analysis and verification. We focus on two classes\nof rule-based graph programs: graph reduction programs which check some graph\nproperty, and programs using a depth-first search to test some property or\nperform an operation such as producing a 2-colouring or a topological sorting.\nPrograms of the first type run in linear time without any constraints on input\ngraphs while programs of the second type require input graphs of bounded degree\nto run in linear time. Essential for achieving the linear time complexity are\nso-called rooted rules in GP 2, which, in many situations, can be matched in\nconstant time. For each of our programs, we prove both correctness and\ncomplexity, and also give empirical evidence for their run time.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 14:50:31 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 01:33:19 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Campbell", "Graham", ""], ["Courtehoute", "Brian", ""], ["Plump", "Detlef", ""]]}, {"id": "2012.11702", "submitter": "Mehrnoosh Shafiee", "authors": "Mehrnoosh Shafiee, Javad Ghaderi", "title": "Scheduling Coflows with Dependency Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Applications in data-parallel computing typically consist of multiple stages.\nIn each stage, a set of intermediate parallel data flows (Coflow) is produced\nand transferred between servers to enable starting of next stage. While there\nhas been much research on scheduling isolated coflows, the dependency between\ncoflows in multi-stage jobs has been largely ignored. In this paper, we\nconsider scheduling coflows of multi-stage jobs represented by general DAGs\n(Directed Acyclic Graphs) in a shared data center network, so as to minimize\nthe total weighted completion time of jobs. This problem is significantly more\nchallenging than the traditional coflow scheduling, as scheduling even a single\nmulti-stage job to minimize its completion time is shown to be NP-hard.\n  In this paper, we propose a polynomial-time algorithm with approximation\nratio of $O(\\mu\\log(m)/\\log(\\log(m)))$, where $\\mu$ is the maximum number of\ncoflows in a job and $m$ is the number of servers. For the special case that\nthe jobs' underlying dependency graphs are rooted trees, we modify the\nalgorithm and improve its approximation ratio. To verify the performance of our\nalgorithms, we present simulation results using real traffic traces that show\nup to $53 \\%$ improvement over the prior approach. We conclude the paper by\nproviding a result concerning an optimality gap for scheduling coflows with\ngeneral DAGs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 21:59:24 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Shafiee", "Mehrnoosh", ""], ["Ghaderi", "Javad", ""]]}, {"id": "2012.11742", "submitter": "Micha{\\l} Pilipczuk", "authors": "Jana Cslovjecsek and Friedrich Eisenbrand and Micha{\\l} Pilipczuk and\n  Moritz Venzin and Robert Weismantel", "title": "Efficient sequential and parallel algorithms for multistage stochastic\n  integer programming using proximity", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of solving integer programs of the form $\\min\n\\{\\,c^\\intercal x\\ \\colon\\ Ax=b, x\\geq 0\\}$, where $A$ is a multistage\nstochastic matrix in the following sense: the primal treedepth of $A$ is\nbounded by a parameter $d$, which means that the columns of $A$ can be\norganized into a rooted forest of depth at most $d$ so that columns not bound\nby the ancestor/descendant relation in the forest do not have non-zero entries\nin the same row. We give an algorithm that solves this problem in\nfixed-parameter time $f(d,\\|A\\|_{\\infty})\\cdot n\\log^{O(2^d)} n$, where $f$ is\na computable function and $n$ is the number of rows of $A$. The algorithm works\nin the strong model, where the running time only measures unit arithmetic\noperations on the input numbers and does not depend on their bitlength. This is\nthe first fpt algorithm for multistage stochastic integer programming to\nachieve almost linear running time in the strong sense. For the case of\ntwo-stage stochastic integer programs, our algorithm works in time\n$2^{(2\\|A\\|_\\infty)^{O(r(r+s))}}\\cdot n\\log^{O(rs)} n$. The algorithm can be\nalso parallelized: we give an implementation in the PRAM model that achieves\nrunning time $f(d,\\|A\\|_{\\infty})\\cdot \\log^{O(2^d)} n$ using $n$ processors.\n  The main conceptual ingredient in our algorithms is a new proximity result\nfor multistage stochastic integer programs. We prove that if we consider an\ninteger program $P$, say with a constraint matrix $A$, then for every optimum\nsolution to the linear relaxation of $P$ there exists an optimum (integral)\nsolution to $P$ that lies, in the $\\ell_{\\infty}$-norm, within distance bounded\nby a function of $\\|A\\|_{\\infty}$ and the primal treedepth of $A$. On the way\nto achieve this result, we prove a generalization and considerable improvement\nof a structural result of Klein for multistage stochastic integer programs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 23:21:50 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Cslovjecsek", "Jana", ""], ["Eisenbrand", "Friedrich", ""], ["Pilipczuk", "Micha\u0142", ""], ["Venzin", "Moritz", ""], ["Weismantel", "Robert", ""]]}, {"id": "2012.11891", "submitter": "Christian Sohler", "authors": "Vincent Cohen-Addad and Silvio Lattanzi and Ashkan Norouzi-Fard and\n  Christian Sohler and Ola Svensson", "title": "Fast and Accurate $k$-means++ via Rejection Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $k$-means++ \\cite{arthur2007k} is a widely used clustering algorithm that is\neasy to implement, has nice theoretical guarantees and strong empirical\nperformance. Despite its wide adoption, $k$-means++ sometimes suffers from\nbeing slow on large data-sets so a natural question has been to obtain more\nefficient algorithms with similar guarantees. In this paper, we present a near\nlinear time algorithm for $k$-means++ seeding. Interestingly our algorithm\nobtains the same theoretical guarantees as $k$-means++ and significantly\nimproves earlier results on fast $k$-means++ seeding. Moreover, we show\nempirically that our algorithm is significantly faster than $k$-means++ and\nobtains solutions of equivalent quality.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 09:14:41 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Lattanzi", "Silvio", ""], ["Norouzi-Fard", "Ashkan", ""], ["Sohler", "Christian", ""], ["Svensson", "Ola", ""]]}, {"id": "2012.11965", "submitter": "Nikolaos Tziavelis", "authors": "Nofar Carmeli, Nikolaos Tziavelis, Wolfgang Gatterbauer, Benny\n  Kimelfeld, Mirek Riedewald", "title": "Tractable Orders for Direct Access to Ranked Answers of Conjunctive\n  Queries", "comments": "17 pages", "journal-ref": null, "doi": "10.1145/3452021.3458331", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question of when we can provide logarithmic-time direct access\nto the k-th answer to a Conjunctive Query (CQ) with a specified ordering over\nthe answers, following a preprocessing step that constructs a data structure in\ntime quasilinear in the size of the database. Specifically, we embark on the\nchallenge of identifying the tractable answer orderings that allow for ranked\ndirect access with such complexity guarantees. We begin with lexicographic\norderings and give a decidable characterization (under conventional complexity\nassumptions) of the class of tractable lexicographic orderings for every CQ\nwithout self-joins. We then continue to the more general orderings by the sum\nof attribute weights and show for it that ranked direct access is tractable\nonly in trivial cases. Hence, to better understand the computational challenge\nat hand, we consider the more modest task of providing access to only a single\nanswer (i.e., finding the answer at a given position) - a task that we refer to\nas the selection problem. We indeed achieve a quasilinear-time algorithm for a\nsubset of the class of full CQs without self-joins, by adopting a solution of\nFrederickson and Johnson to the classic problem of selection over sorted\nmatrices. We further prove that none of the other queries in this class admit\nsuch an algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 12:33:51 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 09:33:22 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 15:43:23 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Carmeli", "Nofar", ""], ["Tziavelis", "Nikolaos", ""], ["Gatterbauer", "Wolfgang", ""], ["Kimelfeld", "Benny", ""], ["Riedewald", "Mirek", ""]]}, {"id": "2012.12059", "submitter": "Sandra Kingan", "authors": "J. P. Costalonga, R. J. Kingan and S. R. Kingan", "title": "Constructing minimally 3-connected graphs", "comments": null, "journal-ref": "Algorithms 14, no. 1: 9 (2021)", "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A $3$-connected graph is minimally 3-connected if removal of any edge\ndestroys 3-connectivity. We present an algorithm for constructing minimally\n3-connected graphs based on the results in (Dawes, JCTB 40, 159-168, 1986)\nusing two operations: adding an edge between non-adjacent vertices and\nsplitting a vertex. In order to test sets of vertices and edges for\n3-compatibility, which depends on the cycles of the graph, we develop a method\nfor obtaining the cycles of $G'$ from the cycles of $G$, where $G'$ is obtained\nfrom $G$ by one of the two operations above. We eliminate isomorphs using\ncertificates generated by McKay's isomorphism checker nauty. The algorithm\nconsecutively constructs the non-isomorphic minimally 3-connected graphs with\n$n$ vertices and $m$ edges from the non-isomorphic minimally 3-connected graphs\nwith $n-1$ vertices and $m-2$ edges, $n-1$ vertices and $m-3$ edges, and $n-2$\nvertices and $m-3$ edges.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 14:51:03 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 14:17:07 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Costalonga", "J. P.", ""], ["Kingan", "R. J.", ""], ["Kingan", "S. R.", ""]]}, {"id": "2012.12138", "submitter": "Adrian Vladu", "authors": "Alina Ene, Huy L. Nguyen, Adrian Vladu", "title": "Projection-Free Bandit Optimization with Privacy Guarantees", "comments": "Appears in AAAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design differentially private algorithms for the bandit convex\noptimization problem in the projection-free setting. This setting is important\nwhenever the decision set has a complex geometry, and access to it is done\nefficiently only through a linear optimization oracle, hence Euclidean\nprojections are unavailable (e.g. matroid polytope, submodular base polytope).\nThis is the first differentially-private algorithm for projection-free bandit\noptimization, and in fact our bound of $\\widetilde{O}(T^{3/4})$ matches the\nbest known non-private projection-free algorithm (Garber-Kretzu, AISTATS `20)\nand the best known private algorithm, even for the weaker setting when\nprojections are available (Smith-Thakurta, NeurIPS `13).\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 16:19:29 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""], ["Vladu", "Adrian", ""]]}, {"id": "2012.12153", "submitter": "Joshua Grochow", "authors": "Yerim Song, Joshua A. Grochow", "title": "An Improved Algorithm for Coarse-Graining Cellular Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CG cond-mat.stat-mech cs.DS nlin.PS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In studying the predictability of emergent phenomena in complex systems,\nIsraeli & Goldenfeld (Phys. Rev. Lett., 2004; Phys. Rev. E, 2006) showed how to\ncoarse-grain (elementary) cellular automata (CA). Their algorithm for finding\ncoarse-grainings of supercell size $N$ took doubly-exponential $2^{2^N}$-time,\nand thus only allowed them to explore supercell sizes $N \\leq 4$. Here we\nintroduce a new, more efficient algorithm for finding coarse-grainings between\nany two given CA that allows us to systematically explore all elementary CA\nwith supercell sizes up to $N=7$, and to explore individual examples of even\nlarger supercell size. Our algorithm is based on a backtracking search, similar\nto the DPLL algorithm with unit propagation for the NP-complete problem of\nBoolean Satisfiability.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 16:42:20 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Song", "Yerim", ""], ["Grochow", "Joshua A.", ""]]}, {"id": "2012.12347", "submitter": "Kevin Thompson", "authors": "Ojas Parekh and Kevin Thompson", "title": "Beating Random Assignment for Approximating Quantum 2-Local Hamiltonian\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quantum k-Local Hamiltonian problem is a natural generalization of\nclassical constraint satisfaction problems (k-CSP) and is complete for QMA, a\nquantum analog of NP. Although the complexity of k-Local Hamiltonian problems\nhas been well studied, only a handful of approximation results are known. For\nMax 2-Local Hamiltonian where each term is a rank 3 projector, a natural\nquantum generalization of classical Max 2-SAT, the best known approximation\nalgorithm was the trivial random assignment, yielding a 0.75-approximation. We\npresent the first approximation algorithm beating this bound, a classical\npolynomial-time 0.764-approximation. For strictly quadratic instances, which\nare maximally entangled instances, we provide a 0.801 approximation algorithm,\nand numerically demonstrate that our algorithm is likely a 0.821-approximation.\nWe conjecture these are the hardest instances to approximate. We also give\nimproved approximations for quantum generalizations of other related classical\n2-CSPs. Finally, we exploit quantum connections to a generalization of the\nGrothendieck problem to obtain a classical constant-factor approximation for\nthe physically relevant special case of strictly quadratic traceless 2-Local\nHamiltonians on bipartite interaction graphs, where a inverse logarithmic\napproximation was the best previously known (for general interaction graphs).\nOur work employs recently developed techniques for analyzing classical\napproximations of CSPs and is intended to be accessible to both quantum\ninformation scientists and classical computer scientists.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 20:41:34 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Parekh", "Ojas", ""], ["Thompson", "Kevin", ""]]}, {"id": "2012.12369", "submitter": "Daniel Lemire", "authors": "Daniel Lemire, Colin Bartlett, Owen Kaser", "title": "Integer Division by Constants: Optimal Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": "TR-20-001, Dept of CS, UNB Saint John", "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The integer division of a numerator n by a divisor d gives a quotient q and a\nremainder r. Optimizing compilers accelerate software by replacing the division\nof n by d with the division of c * n (or c * n + c) by m for convenient\nintegers c and m chosen so that they approximate the reciprocal: c/m ~= 1/d.\nSuch techniques are especially advantageous when m is chosen to be a power of\ntwo and when d is a constant so that c and m can be precomputed. The literature\ncontains many bounds on the distance between c/m and the divisor d. Some of\nthese bounds are optimally tight, while others are not. We present optimally\ntight bounds for quotient and remainder computations.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 21:44:42 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 19:07:35 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 13:53:43 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Lemire", "Daniel", ""], ["Bartlett", "Colin", ""], ["Kaser", "Owen", ""]]}, {"id": "2012.12594", "submitter": "Christian Schulz", "authors": "Faisal Abu-Khzam, Sebastian Lamm, Matthias Mnich, Alexander Noe,\n  Christian Schulz, and Darren Strash", "title": "Recent Advances in Practical Data Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last two decades, significant advances have been made in the design\nand analysis of fixed-parameter algorithms for a wide variety of\ngraph-theoretic problems. This has resulted in an algorithmic toolbox that is\nby now well-established. However, these theoretical algorithmic ideas have\nreceived very little attention from the practical perspective. We survey recent\ntrends in data reduction engineering results for selected problems. Moreover,\nwe describe concrete techniques that may be useful for future implementations\nin the area and give open problems and research questions.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 10:52:29 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2020 10:11:13 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 07:57:30 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Abu-Khzam", "Faisal", ""], ["Lamm", "Sebastian", ""], ["Mnich", "Matthias", ""], ["Noe", "Alexander", ""], ["Schulz", "Christian", ""], ["Strash", "Darren", ""]]}, {"id": "2012.12607", "submitter": "Marcin Wrochna", "authors": "Bal\\'azs F. Mezei, Marcin Wrochna, Stanislav \\v{Z}ivn\\'y", "title": "PTAS for Sparse General-Valued CSPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study polynomial-time approximation schemes (PTASes) for constraint\nsatisfaction problems (CSPs) such as Maximum Independent Set or Minimum Vertex\nCover on sparse graph classes. Baker's approach gives a PTAS on planar graphs,\nexcluded-minor classes, and beyond. For Max-CSPs, and even more generally,\nmaximisation finite-valued CSPs (where constraints are arbitrary non-negative\nfunctions), Romero, Wrochna, and \\v{Z}ivn\\'y [SODA'21] showed that the\nSherali-Adams LP relaxation gives a simple PTAS for all\nfractionally-treewidth-fragile classes, which is the most general \"sparsity\"\ncondition for which a PTAS is known. We extend these results to general-valued\nCSPs, which include \"crisp\" (or \"strict\") constraints that have to be satisfied\nby every feasible assignment. The only condition on the crisp constraints is\nthat their domain contains an element which is at least as feasible as all the\nothers (but possibly less valuable). For minimisation general-valued CSPs with\ncrisp constraints, we present a PTAS for all Baker graph classes -- a\ndefinition by Dvo\\v{r}\\'ak [SODA'20] which encompasses all classes where\nBaker's technique is known to work, except possibly for\nfractionally-treewidth-fragile classes. While this is standard for problems\nsatisfying a certain monotonicity condition on crisp constraints, we show this\ncan be relaxed to diagonalisability -- a property of relational structures\nconnected to logics, statistical physics, and random CSPs.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 11:26:55 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 14:53:55 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Mezei", "Bal\u00e1zs F.", ""], ["Wrochna", "Marcin", ""], ["\u017divn\u00fd", "Stanislav", ""]]}, {"id": "2012.12697", "submitter": "Luana Silva", "authors": "Luana Silva", "title": "Library of efficient algorithms for phylogenetic analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary relationships between species are usually inferred through\nphylogenetic analysis, which provides phylogenetic trees computed from allelic\nprofiles built by sequencing specific regions of the sequences and abstracting\nthem to categorical indexes. With growing exchanges of people and merchandise,\nepidemics have become increasingly important, and combining information of\ncountry-specific datasets can now reveal unknown spreading patterns. The\nphylogenetic analysis workflow is mainly composed of four consecutive steps,\nthe distance calculation, distance correction, inference algorithm, and local\noptimization steps. There are many phylogenetic tools out there, however most\nimplement only some of these steps and serve only one single purpose, such as\none type of algorithm. Another problem with these is that they are often hard\nto use and integrate, since each of them has its own API. This project resulted\na library that implements the four steps of the workflow, and is highly\nperformant, extensible, reusable, and portable, while providing common APIs and\ndocumentation. It also differs from other tools in the sense that, it is able\nto stop and resume the workflow whenever the user desires, and it was built to\nbe continuously extended and not just serve a single purpose. The time\nbenchmarks conducted on this library show that its implementations of the\nalgorithms conform to their theoretical time complexity. Meanwhile, the memory\nbenchmarks showcase that the implementations of the NJ algorithms follow a\nlinear memory complexity, while the implementations of the MST and GCP\nalgorithms follow a quadratic memory complexity.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 14:23:41 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2020 22:25:04 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2020 23:38:23 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Silva", "Luana", ""]]}, {"id": "2012.12770", "submitter": "Dorothee Henke", "authors": "Christoph Buchheim, Dorothee Henke, Felix Hommelsheim", "title": "On the Complexity of the Bilevel Minimum Spanning Tree Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the bilevel minimum spanning tree (BMST) problem where the leader\nand the follower choose a spanning tree together, according to different\nobjective functions. By showing that this problem is NP-hard in general, we\nanswer an open question stated by Shi et al. We prove that BMST remains hard\neven in the special case where the follower only controls a matching. Moreover,\nby a polynomial reduction from the vertex-disjoint Steiner trees problem, we\ngive some evidence that BMST might even remain hard in case the follower\ncontrols only few edges.\n  On the positive side, we present a polynomial-time $(n-1)$-approximation\nalgorithm for BMST, where $n$ is the number of vertices in the input graph.\nMoreover, considering the number of edges controlled by the follower as\nparameter, we show that 2-approximating BMST is fixed-parameter tractable and\nthat, in case of uniform costs on leader's edges, even solving BMST exactly is\nfixed-parameter tractable. We finally consider bottleneck variants of BMST and\nsettle the complexity landscape of all combinations of sum or bottleneck\nobjective functions for the leader and follower, for the optimistic as well as\nthe pessimistic setting.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 16:23:57 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Buchheim", "Christoph", ""], ["Henke", "Dorothee", ""], ["Hommelsheim", "Felix", ""]]}, {"id": "2012.12803", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman, Audra McMillan, Kunal Talwar", "title": "Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy\n  Amplification by Shuffling", "comments": "Minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work of Erlingsson, Feldman, Mironov, Raghunathan, Talwar, and\nThakurta [EFMRTT19] demonstrates that random shuffling amplifies differential\nprivacy guarantees of locally randomized data. Such amplification implies\nsubstantially stronger privacy guarantees for systems in which data is\ncontributed anonymously [BEMMRLRKTS17] and has lead to significant interest in\nthe shuffle model of privacy [CSUZZ19,EFMRTT19].\n  We show that random shuffling of $n$ data records that are input to\n$\\varepsilon_0$-differentially private local randomizers results in an\n$(O((1-e^{-\\varepsilon_0})\\sqrt{\\frac{e^{\\varepsilon_0}\\log(1/\\delta)}{n}}),\n\\delta)$-differentially private algorithm. This significantly improves over\nprevious work and achieves the asymptotically optimal dependence in\n$\\varepsilon_0$. Our result is based on a new approach that is simpler than\nprevious work and extends to approximate differential privacy with nearly the\nsame guarantees. Our work also yields an empirical method to derive tighter\nbounds the resulting $\\varepsilon$ and we show that it gets to within a small\nconstant factor of the optimal bound. As a direct corollary of our analysis, we\nderive a simple and asymptotically optimal algorithm for discrete distribution\nestimation in the shuffle model of privacy. We also observe that our result\nimplies the first asymptotically optimal privacy analysis of noisy stochastic\ngradient descent that applies to sampling without replacement.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 17:07:26 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 06:55:45 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Feldman", "Vitaly", ""], ["McMillan", "Audra", ""], ["Talwar", "Kunal", ""]]}, {"id": "2012.13306", "submitter": "Makrand Sinha", "authors": "Sander Borst, Daniel Dadush, Neil Olver, Makrand Sinha", "title": "Majorizing Measures for the Optimizer", "comments": "37 pages. Extended Abstract to appear in ITCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of majorizing measures, extensively developed by Fernique,\nTalagrand and many others, provides one of the most general frameworks for\ncontrolling the behavior of stochastic processes. In particular, it can be\napplied to derive quantitative bounds on the expected suprema and the degree of\ncontinuity of sample paths for many processes.\n  One of the crowning achievements of the theory is Talagrand's tight\nalternative characterization of the suprema of Gaussian processes in terms of\nmajorizing measures. The proof of this theorem was difficult, and thus\nconsiderable effort was put into the task of developing both shorter and easier\nto understand proofs. A major reason for this difficulty was considered to be\ntheory of majorizing measures itself, which had the reputation of being opaque\nand mysterious. As a consequence, most recent treatments of the theory\n(including by Talagrand himself) have eschewed the use of majorizing measures\nin favor of a purely combinatorial approach (the generic chaining) where\nobjects based on sequences of partitions provide roughly matching upper and\nlower bounds on the desired expected supremum.\n  In this paper, we return to majorizing measures as a primary object of study,\nand give a viewpoint that we think is natural and clarifying from an\noptimization perspective. As our main contribution, we give an algorithmic\nproof of the majorizing measures theorem based on two parts: (1) We make the\nsimple (but apparently new) observation that finding the best majorizing\nmeasure can be cast as a convex program. This also allows for efficiently\ncomputing the measure using off-the-shelf methods from convex optimization. (2)\nWe obtain tree-based upper and lower bound certificates by rounding, in a\nseries of steps, the primal and dual solutions to this convex program.\n  [...]\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 16:04:23 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Borst", "Sander", ""], ["Dadush", "Daniel", ""], ["Olver", "Neil", ""], ["Sinha", "Makrand", ""]]}, {"id": "2012.13484", "submitter": "Tian An Wong", "authors": "Ivano Lodato, Snehal M. Shekatkar, and Tian An Wong", "title": "On partial information retrieval: the unconstrained 100 prisoner problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical 100 Prisoner problem and its variant, involving\nempty boxes, introduced by Gal and Miltersen. Unlike previous studies, here we\nfocus on the winning probabilities for an arbitrary number of winners and\nattempts, which we call the unconstrained problem. We introduce general classes\nof strategies, applicable to different settings and quantify how efficient they\nare. We make use of Monte Carlo simulations, whenever analytic results are not\navailable, to estimate with high accuracy the probabilities of winning.\nFinally, we also comment on the possible applications of our results in\nunderstanding processes of information retrieval, such as \"memory\" in living\norganisms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 02:05:18 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Lodato", "Ivano", ""], ["Shekatkar", "Snehal M.", ""], ["Wong", "Tian An", ""]]}, {"id": "2012.13698", "submitter": "Chen Avin", "authors": "Chen Avin", "title": "Arithmetic Binary Search Trees: Static Optimality in the Matching Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent developments in optical switching and reconfigurable\nnetwork design, we study dynamic binary search trees (BSTs) in the matching\nmodel. In the classical dynamic BST model, the cost of both link traversal and\nbasic reconfiguration (rotation) is $O(1)$. However, in the matching model, the\nBST is defined by two optical switches (that represent two matchings in an\nabstract way), and each switch (or matching) reconfiguration cost is $\\alpha$\nwhile a link traversal cost is still $O(1)$. In this work, we propose\nArithmetic BST (A-BST), a simple dynamic BST algorithm that is based on dynamic\nShannon-Fano-Elias coding, and show that A-BST is statically optimal for\nsequences of length $\\Omega(n \\alpha \\log \\alpha)$ where $n$ is the number of\nnodes (keys) in the tree.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 08:08:46 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Avin", "Chen", ""]]}, {"id": "2012.13764", "submitter": "Frank Gurski", "authors": "Frank Gurski and Dominique Komander and Marvin Lindemann", "title": "Efficient computation of the oriented chromatic number of recursively\n  defined digraphs", "comments": "25 pages. arXiv admin note: text overlap with arXiv:2006.13911", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider colorings of oriented graphs, i.e. digraphs without\ncycles of length 2. Given some oriented graph $G=(V,E)$, an oriented\n$r$-coloring for $G$ is a partition of the vertex set $V$ into $r$ independent\nsets, such that all the arcs between two of these sets have the same direction.\nThe oriented chromatic number of $G$ is the smallest integer $r$ such that $G$\npermits an oriented $r$-coloring.\n  In this paper we consider the Oriented Chromatic Number problem on classes of\nrecursively defined oriented graphs. Oriented co-graphs (short for oriented\ncomplement reducible graphs) can be recursively defined defined from the single\nvertex graph by applying the disjoint union and order composition. This\nrecursive structure allows to compute an optimal oriented coloring and the\noriented chromatic number in linear time. We generalize this result using the\nconcept of perfect orderable graphs. Therefore, we show that for acyclic\ntransitive digraphs every greedy coloring along a topological ordering leads to\nan optimal oriented coloring. Msp-digraphs (short for minimal series-parallel\ndigraphs) can be defined from the single vertex graph by applying the parallel\ncomposition and series composition. We prove an upper bound of $7$ for the\noriented chromatic number for msp-digraphs and we give an example to show that\nthis is bound best possible. We apply this bound and the recursive structure of\nmsp-digraphs to obtain a linear time solution for computing the oriented\nchromatic number of msp-digraphs.\n  In order to generalize the results on computing the oriented chromatic number\nof special graph classes, we consider the parameterized complexity of the\nOriented Chromatic Number problem by so-called structural parameters, which are\nmeasuring the difficulty of decomposing a graph into a special tree-structure\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 15:26:56 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 10:26:58 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Gurski", "Frank", ""], ["Komander", "Dominique", ""], ["Lindemann", "Marvin", ""]]}, {"id": "2012.13976", "submitter": "Raghavendra Addanki", "authors": "Raghavendra Addanki, Andrew McGregor, Cameron Musco", "title": "Intervention Efficient Algorithms for Approximate Learning of Causal\n  Graphs", "comments": "To appear, International Conference on Algorithmic Learning\n  Theory(ALT) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning the causal relationships between a set of\nobserved variables in the presence of latents, while minimizing the cost of\ninterventions on the observed variables. We assume access to an undirected\ngraph $G$ on the observed variables whose edges represent either all direct\ncausal relationships or, less restrictively, a superset of causal relationships\n(identified, e.g., via conditional independence tests or a domain expert). Our\ngoal is to recover the directions of all causal or ancestral relations in $G$,\nvia a minimum cost set of interventions. It is known that constructing an exact\nminimum cost intervention set for an arbitrary graph $G$ is NP-hard. We further\nargue that, conditioned on the hardness of approximate graph coloring, no\npolynomial time algorithm can achieve an approximation factor better than\n$\\Theta(\\log n)$, where $n$ is the number of observed variables in $G$. To\novercome this limitation, we introduce a bi-criteria approximation goal that\nlets us recover the directions of all but $\\epsilon n^2$ edges in $G$, for some\nspecified error parameter $\\epsilon > 0$. Under this relaxed goal, we give\npolynomial time algorithms that achieve intervention cost within a small\nconstant factor of the optimal. Our algorithms combine work on efficient\nintervention design and the design of low-cost separating set systems, with\nideas from the literature on graph property testing.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 17:08:46 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Addanki", "Raghavendra", ""], ["McGregor", "Andrew", ""], ["Musco", "Cameron", ""]]}, {"id": "2012.14079", "submitter": "Yuki Amano", "authors": "Yuki Amano and Kazuhisa Makino", "title": "A 3/4 Differential Approximation Algorithm for Traveling Salesman\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider differential approximability of the traveling\nsalesman problem (TSP). We show that TSP is $3/4$-differential approximable,\nwhich improves the currently best known bound $3/4 -O(1/n)$ due to Escoffier\nand Monnot in 2008, where $n$ denotes the number of vertices in the given\ngraph.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 03:31:00 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Amano", "Yuki", ""], ["Makino", "Kazuhisa", ""]]}, {"id": "2012.14169", "submitter": "Yannic Maus", "authors": "Magn\\'us M. Halld\\'orsson, Fabian Kuhn, Yannic Maus, Tigran Tonoyan", "title": "Efficient Randomized Distributed Coloring in CONGEST", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed vertex coloring is one of the classic problems and probably also\nthe most widely studied problems in the area of distributed graph algorithms.\nWe present a new randomized distributed vertex coloring algorithm for the\nstandard CONGEST model, where the network is modeled as an $n$-node graph $G$,\nand where the nodes of $G$ operate in synchronous communication rounds in which\nthey can exchange $O(\\log n)$-bit messages over all the edges of $G$. For\ngraphs with maximum degree $\\Delta$, we show that the $(\\Delta+1)$-list\ncoloring problem (and therefore also the standard $(\\Delta+1)$-coloring\nproblem) can be solved in $O(\\log^5\\log n)$ rounds. Previously such a result\nwas only known for the significantly more powerful LOCAL model, where in each\nround, neighboring nodes can exchange messages of arbitrary size. The best\nprevious $(\\Delta+1)$-coloring algorithm in the CONGEST model had a running\ntime of $O(\\log\\Delta + \\log^6\\log n)$ rounds. As a function of $n$ alone, the\nbest previous algorithm therefore had a round complexity of $O(\\log n)$, which\nis a bound that can also be achieved by a na\\\"{i}ve folklore algorithm. For\nlarge maximum degree $\\Delta$, our algorithm hence is an exponential\nimprovement over the previous state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 10:09:39 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 11:33:04 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Halld\u00f3rsson", "Magn\u00fas M.", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""], ["Tonoyan", "Tigran", ""]]}, {"id": "2012.14174", "submitter": "Mingyu Xiao", "authors": "Mingyu Xiao and Hiroshi Nagamochi", "title": "Bounded-Degree Cut is Fixed-Parameter Tractable", "comments": "ICALP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the bounded-degree cut problem, we are given a multigraph $G=(V,E)$, two\ndisjoint vertex subsets $A,B\\subseteq V$, two functions $\\mathrm{u}_A,\n\\mathrm{u}_B:V\\to \\{0,1,\\ldots,|E|\\}$ on $V$, and an integer $k\\geq 0$. The\ntask is to determine whether there is a minimal $(A,B)$-cut $(V_A,V_B)$ of size\nat most $k$ such that the degree of each vertex $v\\in V_A$ in the induced\nsubgraph $G[V_A]$ is at most $\\mathrm{u}_A(v)$ and the degree of each vertex\n$v\\in V_B$ in the induced subgraph $G[V_B]$ is at most $\\mathrm{u}_B(v)$. In\nthis paper, we show that the bounded-degree cut problem is fixed-parameter\ntractable by giving a $2^{18k}|G|^{O(1)}$-time algorithm. This is the first\nsingle exponential FPT algorithm for this problem. The core of the algorithm\nlies two new lemmas based on important cuts, which give some upper bounds on\nthe number of candidates for vertex subsets in one part of a minimal cut\nsatisfying some properties. These lemmas can be used to design fixed-parameter\ntractable algorithms for more related problems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 10:26:03 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Xiao", "Mingyu", ""], ["Nagamochi", "Hiroshi", ""]]}, {"id": "2012.14233", "submitter": "Hyung-Chan An", "authors": "Hyung-Chan An, Robert Kleinberg, David B. Shmoys", "title": "Approximation Algorithms for the Bottleneck Asymmetric Traveling\n  Salesman Problem", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present the first nontrivial approximation algorithm for the bottleneck\nasymmetric traveling salesman problem. Given an asymmetric metric cost between\nn vertices, the problem is to find a Hamiltonian cycle that minimizes its\nbottleneck (or maximum-length edge) cost. We achieve an O(log n / log log n)\napproximation performance guarantee by giving a novel algorithmic technique to\nshortcut Eulerian circuits while bounding the lengths of the shortcuts needed.\nThis allows us to build on a related result of Asadpour, Goemans, M\\k{a}dry,\nOveis Gharan, and Saberi to obtain this guarantee. Furthermore, we show how our\ntechnique yields stronger approximation bounds in some cases, such as the\nbounded orientable genus case studied by Oveis Gharan and Saberi. We also\nexplore the possibility of further improvement upon our main result through a\ncomparison to the symmetric counterpart of the problem.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 13:59:44 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["An", "Hyung-Chan", ""], ["Kleinberg", "Robert", ""], ["Shmoys", "David B.", ""]]}, {"id": "2012.14317", "submitter": "Heng Guo", "authors": "Heng Guo and Giorgos Mousa", "title": "Local-to-Global Contraction in Simplicial Complexes", "comments": "v2: improved presentation, updated reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give a local-to-global principle for relative entropy contraction in\nsimplicial complexes. This is similar to the local-to-global principle for\nvariances obtained by Alev and Lau (2020).\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 15:58:31 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 18:18:28 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Guo", "Heng", ""], ["Mousa", "Giorgos", ""]]}, {"id": "2012.14368", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Faeze Ebrahimian, Jerry Li, Dan Alistarh", "title": "Byzantine-Resilient Non-Convex Stochastic Gradient Descent", "comments": "V1.5 polishes writing and V2 rewrites the experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study adversary-resilient stochastic distributed optimization, in which\n$m$ machines can independently compute stochastic gradients, and cooperate to\njointly optimize over their local objective functions. However, an\n$\\alpha$-fraction of the machines are $\\textit{Byzantine}$, in that they may\nbehave in arbitrary, adversarial ways. We consider a variant of this procedure\nin the challenging $\\textit{non-convex}$ case. Our main result is a new\nalgorithm SafeguardSGD which can provably escape saddle points and find\napproximate local minima of the non-convex objective. The algorithm is based on\na new concentration filtering technique, and its sample and time complexity\nbounds match the best known theoretical bounds in the stochastic, distributed\nsetting when no Byzantine machines are present.\n  Our algorithm is very practical: it improves upon the performance of all\nprior methods when training deep neural networks, it is relatively lightweight,\nand it is the first method to withstand two recently-proposed Byzantine\nattacks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 17:19:32 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 17:25:48 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Ebrahimian", "Faeze", ""], ["Li", "Jerry", ""], ["Alistarh", "Dan", ""]]}, {"id": "2012.14512", "submitter": "Robi Bhattacharjee", "authors": "Robi Bhattacharjee and Michal Moshkovitz", "title": "No-substitution k-means Clustering with Adversarial Order", "comments": "accepted to ALT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate $k$-means clustering in the online no-substitution setting\nwhen the input arrives in \\emph{arbitrary} order. In this setting, points\narrive one after another, and the algorithm is required to instantly decide\nwhether to take the current point as a center before observing the next point.\nDecisions are irrevocable. The goal is to minimize both the number of centers\nand the $k$-means cost. Previous works in this setting assume that the input's\norder is random, or that the input's aspect ratio is bounded. It is known that\nif the order is arbitrary and there is no assumption on the input, then any\nalgorithm must take all points as centers. Moreover, assuming a bounded aspect\nratio is too restrictive -- it does not include natural input generated from\nmixture models.\n  We introduce a new complexity measure that quantifies the difficulty of\nclustering a dataset arriving in arbitrary order. We design a new random\nalgorithm and prove that if applied on data with complexity $d$, the algorithm\ntakes $O(d\\log(n) k\\log(k))$ centers and is an $O(k^3)$-approximation. We also\nprove that if the data is sampled from a ``natural\" distribution, such as a\nmixture of $k$ Gaussians, then the new complexity measure is equal to\n$O(k^2\\log(n))$. This implies that for data generated from those distributions,\nour new algorithm takes only $\\text{poly}(k\\log(n))$ centers and is a\n$\\text{poly}(k)$-approximation. In terms of negative results, we prove that the\nnumber of centers needed to achieve an $\\alpha$-approximation is at least\n$\\Omega\\left(\\frac{d}{k\\log(n\\alpha)}\\right)$.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 22:35:44 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Bhattacharjee", "Robi", ""], ["Moshkovitz", "Michal", ""]]}, {"id": "2012.14540", "submitter": "Leonard Schulman", "authors": "Spencer L. Gordon, Bijan Mazaheri, Yuval Rabani, Leonard J. Schulman", "title": "Source Identification for Mixtures of Product Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give an algorithm for source identification of a mixture of $k$ product\ndistributions on $n$ bits. This is a fundamental problem in machine learning\nwith many applications. Our algorithm identifies the source parameters of an\nidentifiable mixture, given, as input, approximate values of multilinear\nmoments (derived, for instance, from a sufficiently large sample), using\n$2^{O(k^2)} n^{O(k)}$ arithmetic operations. Our result is the first explicit\nbound on the computational complexity of source identification of such\nmixtures. The running time improves previous results by Feldman, O'Donnell, and\nServedio (FOCS 2005) and Chen and Moitra (STOC 2019) that guaranteed only\nlearning the mixture (without parametric identification of the source). Our\nanalysis gives a quantitative version of a qualitative characterization of\nidentifiable sources that is due to Tahmasebi, Motahari, and Maddah-Ali (ISIT\n2018).\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 00:21:11 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Gordon", "Spencer L.", ""], ["Mazaheri", "Bijan", ""], ["Rabani", "Yuval", ""], ["Schulman", "Leonard J.", ""]]}, {"id": "2012.14542", "submitter": "Ajay Singh", "authors": "Ajay Singh, Trevor Brown, Ali Mashtizadeh", "title": "NBR: Neutralization Based Reclamation", "comments": "Accepted in PPoPP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Safe memory reclamation (SMR) algorithms suffer from a trade-off between\nbounding unreclaimed memory and the speed of reclamation. Hazard pointer (HP)\nbased algorithms bound unreclaimed memory at all times, but tend to be slower\nthan other approaches. Epoch based reclamation (EBR) algorithms are faster, but\ndo not bound memory reclamation. Other algorithms follow hybrid approaches,\nrequiring special compiler or hardware support, changes to record layouts,\nand/or extensive code changes. Not all SMR algorithms can be used to reclaim\nmemory for all data structures.\n  We propose a new neutralization based reclamation (NBR) algorithm that is\nfaster than the best known EBR algorithms and achieves bounded unreclaimed\nmemory. It is non-blocking when used with a non-blocking operating system (OS)\nkernel, and only requires atomic read, write and CAS. NBR is straightforward to\nuse with many different data structures, and in most cases, require similar\nreasoning and programmer effort to two-phased locking. NBR is implemented using\nOS signals and a lightweight handshaking mechanism between participating\nthreads to determine when it is safe to reclaim a record. Experiments on a\nlock-based binary search tree and a lazy linked list show that NBR\nsignificantly outperforms many state of the art reclamation algorithms. In the\ntree NBR is faster than next best algorithm, DEBRA by upto 38% and HP by upto\n17%. And, in the list NBR is 15% and 243% faster than DEBRA and HP,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 00:32:48 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 21:20:03 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Singh", "Ajay", ""], ["Brown", "Trevor", ""], ["Mashtizadeh", "Ali", ""]]}, {"id": "2012.14632", "submitter": "Sutanu Gayen", "authors": "Arnab Bhattacharyya, Sutanu Gayen, Saravanan Kandasamy, N. V.\n  Vinodchandran", "title": "Testing Product Distributions: A Closer Look", "comments": "A version appears in ALT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problems of identity and closeness testing of $n$-dimensional\nproduct distributions. Prior works by Canonne, Diakonikolas, Kane and Stewart\n(COLT 2017) and Daskalakis and Pan (COLT 2017) have established tight sample\ncomplexity bounds for non-tolerant testing over a binary alphabet: given two\nproduct distributions $P$ and $Q$ over a binary alphabet, distinguish between\nthe cases $P = Q$ and $d_{\\mathrm{TV}}(P, Q) > \\epsilon$. We build on this\nprior work to give a more comprehensive map of the complexity of testing of\nproduct distributions by investigating tolerant testing with respect to several\nnatural distance measures and over an arbitrary alphabet. Our study gives a\nfine-grained understanding of how the sample complexity of tolerant testing\nvaries with the distance measures for product distributions. In addition, we\nalso extend one of our upper bounds on product distributions to bounded-degree\nBayes nets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 07:02:32 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 04:51:59 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Gayen", "Sutanu", ""], ["Kandasamy", "Saravanan", ""], ["Vinodchandran", "N. V.", ""]]}, {"id": "2012.15002", "submitter": "Spencer Compton", "authors": "Spencer Compton, Slobodan Mitrovi\\'c, Ronitt Rubinfeld", "title": "New Partitioning Techniques and Faster Algorithms for Approximate\n  Interval Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interval scheduling is a basic problem in the theory of algorithms and a\nclassical task in combinatorial optimization. We develop a set of techniques\nfor partitioning and grouping jobs based on their starting and ending times,\nthat enable us to view an instance of interval scheduling on many jobs as a\nunion of multiple interval scheduling instances, each containing only a few\njobs. Instantiating these techniques in dynamic and local settings of\ncomputation leads to several new results.\n  For $(1+\\varepsilon)$-approximation of job scheduling of $n$ jobs on a single\nmachine, we obtain a fully dynamic algorithm with\n$O(\\frac{\\log{n}}{\\varepsilon})$ update and $O(\\log{n})$ query worst-case time.\nFurther, we design a local computation algorithm that uses only\n$O(\\frac{\\log{n}}{\\varepsilon})$ queries. Our techniques are also applicable in\na setting where jobs have rewards/weights. For this case we obtain a fully\ndynamic algorithm whose worst-case update and query time has only polynomial\ndependence on $1/\\varepsilon$, which is an exponential improvement over the\nresult of Henzinger et al. [SoCG, 2020].\n  We extend our approaches for unweighted interval scheduling on a single\nmachine to the setting with $M$ machines, while achieving the same\napproximation factor and only $M$ times slower update time in the dynamic\nsetting. In addition, we provide a general framework for reducing the task of\ninterval scheduling on $M$ machines to that of interval scheduling on a single\nmachine. In the unweighted case this approach incurs a multiplicative\napproximation factor $2 - 1/M$.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 01:58:16 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 10:01:14 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Compton", "Spencer", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "2012.15056", "submitter": "Debarsho Sannyasi", "authors": "Debarsho Sannyasi", "title": "Improved Approximation Algorithms for Weighted Edge Coloring of Graphs", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study weighted edge coloring of graphs, where we are given an undirected\nedge-weighted general multi-graph $G := (V, E)$ with weights $w : E \\rightarrow\n[0, 1]$. The goal is to find a proper weighted coloring of the edges with as\nfew colors as possible. An edge coloring is called a proper weighted coloring\nif the sum of the weights of the edges incident to a vertex of any color is at\nmost one. In the online setting, the edges are revealed one by one and have to\nbe colored irrevocably as soon as they are revealed. We show that $3.39m+o(m)$\ncolors are enough when the maximum number of neighbors of a vertex over all the\nvertices is $o(m)$ and where $m$ is the maximum over all vertices of the\nminimum number of unit-sized bins needed to pack the weights of the incident\nedges to that vertex. We also prove the tightness of our analysis. This\nimproves upon the previous best upper bound of $5m$ by Correa and Goemans [STOC\n2004]. For the offline case, we show that for a simple graph with edge disjoint\ncycles, $m+1$ colors are sufficient and for a multi-graph tree, we show that\n$1.693m+12$ colors are sufficient.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 06:31:32 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Sannyasi", "Debarsho", ""]]}, {"id": "2012.15194", "submitter": "Dabeen Lee", "authors": "Dabeen Lee, Milan Vojnovic, Se-Young Yun", "title": "Test Score Algorithms for Budgeted Stochastic Utility Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent developments in designing algorithms based on individual\nitem scores for solving utility maximization problems, we study the framework\nof using test scores, defined as a statistic of observed individual item\nperformance data, for solving the budgeted stochastic utility maximization\nproblem. We extend an existing scoring mechanism, namely the replication test\nscores, to incorporate heterogeneous item costs as well as item values. We show\nthat a natural greedy algorithm that selects items solely based on their\nreplication test scores outputs solutions within a constant factor of the\noptimum for a broad class of utility functions. Our algorithms and\napproximation guarantees assume that test scores are noisy estimates of certain\nexpected values with respect to marginal distributions of individual item\nvalues, thus making our algorithms practical and extending previous work that\nassumes noiseless estimates. Moreover, we show how our algorithm can be adapted\nto the setting where items arrive in a streaming fashion while maintaining the\nsame approximation guarantee. We present numerical results, using synthetic\ndata and data sets from the Academia.StackExchange Q&A forum, which show that\nour test score algorithm can achieve competitiveness, and in some cases better\nperformance than a benchmark algorithm that requires access to a value oracle\nto evaluate function values.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 15:28:41 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Lee", "Dabeen", ""], ["Vojnovic", "Milan", ""], ["Yun", "Se-Young", ""]]}, {"id": "2012.15279", "submitter": "Shri Prakash Dwivedi", "authors": "Shri Prakash Dwivedi", "title": "Some Algorithms on Exact, Approximate and Error-Tolerant Graph Matching", "comments": "Ph.D. Thesis, Indian Institute of Technology (BHU), Varanasi, July\n  2019. (Adviser: Dr. R.S. Singh)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The graph is one of the most widely used mathematical structures in\nengineering and science because of its representational power and inherent\nability to demonstrate the relationship between objects. The objective of this\nwork is to introduce the novel graph matching techniques using the\nrepresentational power of the graph and apply it to structural pattern\nrecognition applications. We present an extensive survey of various exact and\ninexact graph matching techniques. Graph matching using the concept of\nhomeomorphism is presented. A category of graph matching algorithms is\npresented, which reduces the graph size by removing the less important nodes\nusing some measure of relevance. We present an approach to error-tolerant graph\nmatching using node contraction where the given graph is transformed into\nanother graph by contracting smaller degree nodes. We use this scheme to extend\nthe notion of graph edit distance, which can be used as a trade-off between\nexecution time and accuracy. We describe an approach to graph matching by\nutilizing the various node centrality information, which reduces the graph size\nby removing a fraction of nodes from both graphs based on a given centrality\nmeasure. The graph matching problem is inherently linked to the geometry and\ntopology of graphs. We introduce a novel approach to measure graph similarity\nusing geometric graphs. We define the vertex distance between two geometric\ngraphs using the position of their vertices and show it to be a metric over the\nset of all graphs with vertices only. We define edge distance between two\ngraphs based on the angular orientation, length and position of the edges. Then\nwe combine the notion of vertex distance and edge distance to define the graph\ndistance between two geometric graphs and show it to be a metric. Finally, we\nuse the proposed graph similarity framework to perform exact and error-tolerant\ngraph matching.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 18:51:06 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Dwivedi", "Shri Prakash", ""]]}, {"id": "2012.15381", "submitter": "Sergio Cabello", "authors": "Sergio Cabello", "title": "Faster Distance-Based Representative Skyline and $k$-Center Along Pareto\n  Front in the Plane", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DB cs.DS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of computing the \\emph{distance-based representative\nskyline} in the plane, a problem introduced by Tao, Ding, Lin and Pei [Proc.\n25th IEEE International Conference on Data Engineering (ICDE), 2009] and\nindependently considered by Dupin, Nielsen and Talbi [Optimization and Learning\n- Third International Conference, OLA 2020] in the context of multi-objective\noptimization. Given a set $P$ of $n$ points in the plane and a parameter $k$,\nthe task is to select $k$ points of the skyline defined by $P$ (also known as\nPareto front for $P$) to minimize the maximum distance from the points of the\nskyline to the selected points. We show that the problem can be solved in\n$O(n\\log h)$ time, where $h$ is the number of points in the skyline of $P$. We\nalso show that the decision problem can be solved in $O(n\\log k)$ time and the\noptimization problem can be solved in $O(n \\log k + n \\operatorname{loglog} n)$\ntime. This improves previous algorithms and is optimal for a large range of\nvalues of $k$.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 00:34:57 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Cabello", "Sergio", ""]]}, {"id": "2012.15438", "submitter": "Jacob Nelson", "authors": "Jacob Nelson, Ahmed Hassan, and Roberto Palmieri", "title": "Bundled References: An Abstraction for Highly-Concurrent Linearizable\n  Range Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present bundled references, a new building block to provide linearizable\nrange query operations for highly concurrent linked data structures. Bundled\nreferences allow range queries to traverse a path through the data structure\nthat is consistent with the target atomic snapshot and is made of the minimal\namount of nodes that should be accessed to preserve linearizability. We\nimplement our technique into a skip list, a binary search tree, and a linked\nlist data structure. Our evaluation reveals that in mixed workloads, our design\nimproves upon the state-of-the-art techniques by 3.9x for a skip list and 2.1x\nfor a binary search tree. We also integrate our bundled data structure into the\nDBx1000 in-memory database, yielding up to 20% gain over the same competitors.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 04:11:11 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Nelson", "Jacob", ""], ["Hassan", "Ahmed", ""], ["Palmieri", "Roberto", ""]]}, {"id": "2012.15584", "submitter": "Yuko Kuroki", "authors": "Yuko Kuroki, Junya Honda, Masashi Sugiyama", "title": "Combinatorial Pure Exploration with Full-bandit Feedback and Beyond:\n  Solving Combinatorial Optimization under Uncertainty with Limited Observation", "comments": "Preprint of an Invited Review Article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.DS cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial optimization is one of the fundamental research fields that has\nbeen extensively studied in theoretical computer science and operations\nresearch. When developing an algorithm for combinatorial optimization, it is\ncommonly assumed that parameters such as edge weights are exactly known as\ninputs. However, this assumption may not be fulfilled since input parameters\nare often uncertain or initially unknown in many applications such as\nrecommender systems, crowdsourcing, communication networks, and online\nadvertisement. To resolve such uncertainty, the problem of combinatorial pure\nexploration of multi-armed bandits (CPE) and its variants have recieved\nincreasing attention. Earlier work on CPE has studied the semi-bandit feedback\nor assumed that the outcome from each individual edge is always accessible at\nall rounds. However, due to practical constraints such as a budget ceiling or\nprivacy concern, such strong feedback is not always available in recent\napplications. In this article, we review recently proposed techniques for\ncombinatorial pure exploration problems with limited feedback.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 12:40:52 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Kuroki", "Yuko", ""], ["Honda", "Junya", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2012.15593", "submitter": "Michele Scquizzato", "authors": "Enoch Peserico and Michele Scquizzato", "title": "Matching on the line admits no $o(\\sqrt{\\log n})$-competitive algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple proof that the competitive ratio of any randomized online\nmatching algorithm for the line is at least $\\sqrt{\\log_2(n\\!+\\!1)}/12$ for all\n$n=2^i\\!-\\!1: i\\in\\mathbb{N}$.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 13:05:45 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Peserico", "Enoch", ""], ["Scquizzato", "Michele", ""]]}, {"id": "2012.15675", "submitter": "Xiaorui Sun", "authors": "Sebastian Forster and Gramoz Goranci and Yang P. Liu and Richard Peng\n  and Xiaorui Sun and Mingquan Ye", "title": "Minor Sparsifiers and the Distributed Laplacian Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed algorithms built around edge contraction based vertex\nsparsifiers, and give sublinear round algorithms in the $\\textsf{CONGEST}$\nmodel for exact mincost flow, negative weight shortest paths, maxflow, and\nbipartite matching on sparse graphs. For the maxflow problem, this is the first\nexact distributed algorithm that applies to directed graphs, while the previous\nwork by [Ghaffari et al. SICOMP'18] considered the approximate setting and\nworks only for undirected graphs. For the mincost flow and the negative weight\nshortest path problems, our results constitute the first exact distributed\nalgorithms running in a sublinear number of rounds. These algorithms follow the\ncelebrated Laplacian paradigm, which numerically solve combinatorial graph\nproblems via series of linear systems in graph Laplacian matrices.\n  To enable Laplacian based algorithms in the distributed setting, we develop a\nLaplacian solver based upon the subspace sparsifiers of [Li, Schild FOCS'18].\nWe give a parallel variant of their algorithm that avoids the sampling of\nrandom spanning trees, and analyze it using matrix martingales. Combining this\nvertex reduction recursively with both tree and elimination based\npreconditioners leads to an algorithm for solving Laplacian systems on $n$\nvertex graphs to high accuracy in $O(n^{o(1)}(\\sqrt{n}+D))$ rounds. The round\ncomplexity of this distributed solver almost matches the lower bound of\n$\\widetilde{\\Omega}(\\sqrt{n}+D)$.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 15:52:28 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 18:01:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Forster", "Sebastian", ""], ["Goranci", "Gramoz", ""], ["Liu", "Yang P.", ""], ["Peng", "Richard", ""], ["Sun", "Xiaorui", ""], ["Ye", "Mingquan", ""]]}, {"id": "2012.15775", "submitter": "Mateusz Wasylkiewicz", "authors": "Katarzyna Paluch (1) and Mateusz Wasylkiewicz (1) ((1) Institute of\n  Computer Science, University of Wroc{\\l}aw)", "title": "A simple combinatorial algorithm for restricted 2-matchings in subcubic\n  graphs -- via half-edges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider three variants of the problem of finding a maximum weight\nrestricted $2$-matching in a subcubic graph $G$. (A $2$-matching is any subset\nof the edges such that each vertex is incident to at most two of its edges.)\nDepending on the variant a restricted $2$-matching means a $2$-matching that is\neither triangle-free or square-free or both triangle- and square-free. While\nthere exist polynomial time algorithms for the first two types of\n$2$-matchings, they are quite complicated or use advanced methodology. For each\nof the three problems we present a simple reduction to the computation of a\nmaximum weight $b$-matching. The reduction is conducted with the aid of\nhalf-edges. A half-edge of edge $e$ is, informally speaking, a half of $e$\ncontaining exactly one of its endpoints. For a subset of triangles of $G$, we\nreplace each edge of such a triangle with two half-edges. Two half-edges of one\nedge $e$ of weight $w(e)$ may get different weights, not necessarily equal to\n$\\frac{1}{2}w(e)$. In the metric setting when the edge weights satisfy the\ntriangle inequality, this has a geometric interpretation connected to how an\nincircle partitions the edges of a triangle. Our algorithms are additionally\nfaster than those known before. The running time of each of them is\n$O(n^2\\log{n})$, where $n$ denotes the number of vertices in the graph.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 17:47:52 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Paluch", "Katarzyna", ""], ["Wasylkiewicz", "Mateusz", ""]]}, {"id": "2012.15843", "submitter": "Shabnam Daghaghi", "authors": "Shabnam Daghaghi, Tharun Medini, Nicholas Meisburger, Beidi Chen,\n  Mengnan Zhao, Anshumali Shrivastava", "title": "A Tale of Two Efficient and Informative Negative Sampling Distributions", "comments": "Published at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Softmax classifiers with a very large number of classes naturally occur in\nmany applications such as natural language processing and information\nretrieval. The calculation of full softmax is costly from the computational and\nenergy perspective. There have been various sampling approaches to overcome\nthis challenge, popularly known as negative sampling (NS). Ideally, NS should\nsample negative classes from a distribution that is dependent on the input\ndata, the current parameters, and the correct positive class. Unfortunately,\ndue to the dynamically updated parameters and data samples, there is no\nsampling scheme that is provably adaptive and samples the negative classes\nefficiently. Therefore, alternative heuristics like random sampling, static\nfrequency-based sampling, or learning-based biased sampling, which primarily\ntrade either the sampling cost or the adaptivity of samples per iteration are\nadopted. In this paper, we show two classes of distributions where the sampling\nscheme is truly adaptive and provably generates negative samples in\nnear-constant time. Our implementation in C++ on CPU is significantly superior,\nboth in terms of wall-clock time and accuracy, compared to the most optimized\nTensorFlow implementations of other popular negative sampling approaches on\npowerful NVIDIA V100 GPU.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:56:41 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 03:02:26 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Daghaghi", "Shabnam", ""], ["Medini", "Tharun", ""], ["Meisburger", "Nicholas", ""], ["Chen", "Beidi", ""], ["Zhao", "Mengnan", ""], ["Shrivastava", "Anshumali", ""]]}]