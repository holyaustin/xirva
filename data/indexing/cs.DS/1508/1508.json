[{"id": "1508.00060", "submitter": "Shankar Sastry", "authors": "Shankar Prasad Sastry", "title": "Snow Globe: An Advancing-Front 3D Delaunay Mesh Refinement Algorithm", "comments": "incorrect proofs; does not consider an important case because of\n  which the proofs are wrong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  <incorrect proofs; does not consider an important case because of which the\nproofs are wrong. The paper was withdrawn from submission> One of the\nobjectives of a Delaunay mesh refinement algorithm is to produce meshes with\ntetrahedral elements having a bounded aspect ratio, which is the ratio between\nthe radius of the circumscribing and inscribing spheres. The refinement is\ncarried out by inserting additional Steiner vertices inside the circumsphere of\na poor-quality tetrahedron (to remove the tetrahedron) at a sufficient distance\nfrom existing vertices to guarantee the termination and size optimality of the\nalgorithm. This technique eliminates tetrahedra whose ratio of the radius of\nthe circumscribing sphere and the shortest side, the radius-edge ratio, is\nlarge. Slivers, almost-planar tetrahedra, which have a small radius-edge ratio,\nbut a large aspect ratio, are avoided by small random perturbations of the\nSteiner vertices to improve the aspect ratio. Additionally, geometric\nconstraints, called \"petals\", have been shown to produce smaller high-quality\nmeshes in 2D Delaunay refinement algorithms. In this paper, we develop a\ndeterministic nondifferentiable optimization routine to place the Steiner\nvertex inside geometrical constraints that we call \"snow globes\" for 3D\nDelaunay refinement. We explore why the geometrical constraints and an ordering\non processing of poor-quality tetrahedra result in smaller meshes. The stricter\nanalysis provides an improved constant associated with the size optimality of\nthe generated meshes. Aided by the analysis, we present a modified algorithm to\nhandle boundary encroachment. The final algorithm behaves like an\nadvancing-front algorithms that are commonly used for quadrilateral and\nhexahedral meshing.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 00:33:12 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 05:28:17 GMT"}, {"version": "v3", "created": "Sat, 4 Aug 2018 22:50:20 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Sastry", "Shankar Prasad", ""]]}, {"id": "1508.00123", "submitter": "Anatoly Plotnikov D.", "authors": "Anatoly D. Plotnikov", "title": "On non-canonical solving the Satisfiability problem", "comments": "5 pages", "journal-ref": "International Journal of Automation, Control and Intelligent\n  Systems, Vol. 1, No. 3, September 2015, Pub. Date: Aug. 5, 2015, p.73-76", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study the non-canonical method for solving the Satisfiability problem\nwhich given by a formula in the form of the conjunctive normal form. The\nessence of this method consists in counting the number of tuples of Boolean\nvariables, on which at least one clause of the given formula is false. On this\nbasis the solution of the problem obtains in the form YES or NO without\nconstructing tuple, when the answer is YES. It is found that if the clause in\nthe given formula has pairwise contrary literals, then the problem can be\nsolved efficiently. However, when in the formula there are a long chain of\nclauses with pairwise non-contrary literals, the solution leads to an\nexponential calculations.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 13:42:31 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Plotnikov", "Anatoly D.", ""]]}, {"id": "1508.00142", "submitter": "Moran Feldman", "authors": "Moran Feldman, Ola Svensson and Rico Zenklusen", "title": "Online Contention Resolution Schemes", "comments": "33 pages. To appear in SODA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new rounding technique designed for online optimization\nproblems, which is related to contention resolution schemes, a technique\ninitially introduced in the context of submodular function maximization. Our\nrounding technique, which we call online contention resolution schemes (OCRSs),\nis applicable to many online selection problems, including Bayesian online\nselection, oblivious posted pricing mechanisms, and stochastic probing models.\nIt allows for handling a wide set of constraints, and shares many strong\nproperties of offline contention resolution schemes. In particular, OCRSs for\ndifferent constraint families can be combined to obtain an OCRS for their\nintersection. Moreover, we can approximately maximize submodular functions in\nthe online settings we consider.\n  We, thus, get a broadly applicable framework for several online selection\nproblems, which improves on previous approaches in terms of the types of\nconstraints that can be handled, the objective functions that can be dealt\nwith, and the assumptions on the strength of the adversary. Furthermore, we\nresolve two open problems from the literature; namely, we present the first\nconstant-factor constrained oblivious posted price mechanism for matroid\nconstraints, and the first constant-factor algorithm for weighted stochastic\nprobing with deadlines.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 16:25:53 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 08:22:04 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Feldman", "Moran", ""], ["Svensson", "Ola", ""], ["Zenklusen", "Rico", ""]]}, {"id": "1508.00292", "submitter": "John Ellis", "authors": "John Ellis and Ulrike Stege", "title": "A Provably, Linear Time, In-place and Stable Merge Algorithm via the\n  Perfect Shuffle", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reconsider a recently published algorithm (Dalkilic et al.) for merging\nlists by way of the perfect shuffle. The original publication gave only\nexperimental results which, although consistent with linear execution time on\nthe samples tested, provided no analysis. Here we prove that the time\ncomplexity, in the average case, is indeed linear, although there is an\nOmega(n^2) worst case. This is then the first provably linear time merge\nalgorithm based on the use of the perfect shuffle. We provide a proof of\ncorrectness, extend the algorithm to the general case where the lists are of\nunequal length and show how it can be made stable, all aspects not included in\nthe original presentation and we give a much more concise definition of the\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 23:39:58 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Ellis", "John", ""], ["Stege", "Ulrike", ""]]}, {"id": "1508.00514", "submitter": "Jieming Mao", "authors": "Mark Braverman, Ran Gelles, Jieming Mao, Rafail Ostrovsky", "title": "Coding for interactive communication correcting insertions and deletions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the question of interactive communication, in which two remote\nparties perform a computation while their communication channel is\n(adversarially) noisy. We extend here the discussion into a more general and\nstronger class of noise, namely, we allow the channel to perform insertions and\ndeletions of symbols. These types of errors may bring the parties \"out of\nsync\", so that there is no consensus regarding the current round of the\nprotocol.\n  In this more general noise model, we obtain the first interactive coding\nscheme that has a constant rate and resists noise rates of up to\n$1/18-\\varepsilon$. To this end we develop a novel primitive we name edit\ndistance tree code. The edit distance tree code is designed to replace the\nHamming distance constraints in Schulman's tree codes (STOC 93), with a\nstronger edit distance requirement. However, the straightforward generalization\nof tree codes to edit distance does not seem to yield a primitive that suffices\nfor communication in the presence of synchronization problems. Giving the\n\"right\" definition of edit distance tree codes is a main conceptual\ncontribution of this work.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 18:37:33 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 19:55:09 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Braverman", "Mark", ""], ["Gelles", "Ran", ""], ["Mao", "Jieming", ""], ["Ostrovsky", "Rafail", ""]]}, {"id": "1508.00625", "submitter": "Megasthenis Asteris", "authors": "Megasthenis Asteris, Dimitris Papailiopoulos, Anastasios Kyrillidis,\n  Alexandros G. Dimakis", "title": "Sparse PCA via Bipartite Matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following multi-component sparse PCA problem: given a set of\ndata points, we seek to extract a small number of sparse components with\ndisjoint supports that jointly capture the maximum possible variance. These\ncomponents can be computed one by one, repeatedly solving the single-component\nproblem and deflating the input data matrix, but as we show this greedy\nprocedure is suboptimal. We present a novel algorithm for sparse PCA that\njointly optimizes multiple disjoint components. The extracted features capture\nvariance that lies within a multiplicative factor arbitrarily close to 1 from\nthe optimal. Our algorithm is combinatorial and computes the desired components\nby solving multiple instances of the bipartite maximum weight matching problem.\nIts complexity grows as a low order polynomial in the ambient dimension of the\ninput data matrix, but exponentially in its rank. However, it can be\neffectively applied on a low-dimensional sketch of the data; this allows us to\nobtain polynomial-time approximation guarantees via spectral bounds. We\nevaluate our algorithm on real data-sets and empirically demonstrate that in\nmany cases it outperforms existing, deflation-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 00:12:35 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Asteris", "Megasthenis", ""], ["Papailiopoulos", "Dimitris", ""], ["Kyrillidis", "Anastasios", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1508.00690", "submitter": "G\\'abor Ivanyos", "authors": "G\\'abor Ivanyos, Youming Qiao, K. V. Subrahmanyam", "title": "Non-commutative Edmonds' problem and matrix semi-invariants", "comments": "24 pages; Significantly improved presentation; References to recent\n  developments added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.AC math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1967, Edmonds introduced the problem of computing the rank over the\nrational function field of an $n\\times n$ matrix $T$ with integral homogeneous\nlinear polynomials. In this paper, we consider the non-commutative version of\nEdmonds' problem: compute the rank of $T$ over the free skew field. It is known\nthat this problem relates to the ring of matrix semi-invariants. In particular,\nif the nullcone of matrix semi-invariants is defined by elements of degree\n$\\leq \\sigma$, then there follows a $\\mathrm{poly}(n, \\sigma)$-time randomized\nalgorithm to decide whether the non-commutative rank of $T$ is $<n$. To our\nknowledge, previously the best bound for $\\sigma$ was $O(n^2\\cdot 4^{n^2})$\nover algebraically closed fields of characteristic $0$ (Derksen, 2001).\n  In this article we prove the following results:\n  (1) We observe that by using an algorithm of Gurvits, and assuming the above\nbound $\\sigma$ for $R(n, m)$ over $\\mathbb{Q}$, deciding whether $T$ has\nnon-commutative rank $<n$ over $\\mathbb{Q}$ can be done deterministically in\ntime polynomial in the input size and $\\sigma$.\n  (2) When $\\mathbb{F}$ is large enough, we devise a deterministic algorithm\nfor non-commutative Edmonds' problem in time polynomial in $(n+1)!$, with the\nfollowing consequences.\n  (2.a) If the commutative rank and the non-commutative rank of $T$ differ by a\nconstant, then there exists a randomized efficient algorithm that computes the\nnon-commutative rank of $T$.\n  (2.b) We prove that $\\sigma\\leq (n+1)!$. This not only improves the bound\nobtained from Derksen's work over algebraically closed field of characteristic\n$0$ but, more importantly, also provides for the first time an explicit bound\non $\\sigma$ for matrix semi-invariants over fields of positive characteristics.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 07:39:16 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 09:24:44 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Ivanyos", "G\u00e1bor", ""], ["Qiao", "Youming", ""], ["Subrahmanyam", "K. V.", ""]]}, {"id": "1508.00731", "submitter": "Tatiana Starikovskaya", "authors": "Rapha\\\"el Clifford, Allyx Fontaine, Ely Porat, Benjamin Sach, Tatiana\n  Starikovskaya", "title": "The k-mismatch problem revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the complexity of one of the most basic problems in pattern\nmatching. In the k-mismatch problem we must compute the Hamming distance\nbetween a pattern of length m and every m-length substring of a text of length\nn, as long as that Hamming distance is at most k. Where the Hamming distance is\ngreater than k at some alignment of the pattern and text, we simply output\n\"No\".\n  We study this problem in both the standard offline setting and also as a\nstreaming problem. In the streaming k-mismatch problem the text arrives one\nsymbol at a time and we must give an output before processing any future\nsymbols. Our main results are as follows:\n  1) Our first result is a deterministic $O(n k^2\\log{k} / m+n \\text{polylog}\nm)$ time offline algorithm for k-mismatch on a text of length n. This is a\nfactor of k improvement over the fastest previous result of this form from SODA\n2000 by Amihood Amir et al.\n  2) We then give a randomised and online algorithm which runs in the same time\ncomplexity but requires only $O(k^2\\text{polylog} {m})$ space in total.\n  3) Next we give a randomised $(1+\\epsilon)$-approximation algorithm for the\nstreaming k-mismatch problem which uses $O(k^2\\text{polylog} m / \\epsilon^2)$\nspace and runs in $O(\\text{polylog} m / \\epsilon^2)$ worst-case time per\narriving symbol.\n  4) Finally we combine our new results to derive a randomised\n$O(k^2\\text{polylog} {m})$ space algorithm for the streaming k-mismatch problem\nwhich runs in $O(\\sqrt{k}\\log{k} + \\text{polylog} {m})$ worst-case time per\narriving symbol. This improves the best previous space complexity for streaming\nk-mismatch from FOCS 2009 by Benny Porat and Ely Porat by a factor of k. We\nalso improve the time complexity of this previous result by an even greater\nfactor to match the fastest known offline algorithm (up to logarithmic\nfactors).\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 11:07:50 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2015 17:39:45 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Clifford", "Rapha\u00ebl", ""], ["Fontaine", "Allyx", ""], ["Porat", "Ely", ""], ["Sach", "Benjamin", ""], ["Starikovskaya", "Tatiana", ""]]}, {"id": "1508.00851", "submitter": "Kyrill Winkler", "authors": "Manfred Schwarz, Kyrill Winkler, Ulrich Schmid", "title": "Fast Consensus under Eventually Stabilizing Message Adversaries", "comments": "13 pages, 5 figures, updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to deterministic consensus in synchronous dynamic\nnetworks with unidirectional links, which are under the control of an\nomniscient message adversary. Motivated by unpredictable node/system\ninitialization times and long-lasting periods of massive transient faults, we\nconsider message adversaries that guarantee periods of less erratic message\nloss only eventually: We present a tight bound of $2D+1$ for the termination\ntime of consensus under a message adversary that eventually guarantees a single\nvertex-stable root component with dynamic network diameter $D$, as well as a\nsimple algorithm that matches this bound. It effectively halves the termination\ntime $4D+1$ achieved by an existing consensus algorithm, which also works under\nour message adversary. We also introduce a generalized, considerably stronger\nvariant of our message adversary, and show that our new algorithm, unlike the\nexisting one, still works correctly under it.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 18:01:37 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2015 07:47:58 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Schwarz", "Manfred", ""], ["Winkler", "Kyrill", ""], ["Schmid", "Ulrich", ""]]}, {"id": "1508.01059", "submitter": "Gideon Blocq", "authors": "Noa Avigdor-Elgrabli, Gideon Blocq, Iftah Gamzu, Ariel Orda", "title": "Offline and Online Models of Budget Allocation for Maximizing Influence\n  Spread", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research of influence propagation in social networks via word-of-mouth\nprocesses has been given considerable attention in recent years. Arguably, the\nmost fundamental problem in this domain is influence maximization, where the\ngoal is to identify a seed set of individuals that can trigger a large cascade\nof influence in the network. While there has been significant progress\nregarding this problem and its variants, one basic shortcoming of the models is\nthat they lack the flexibility in the way the budget is allocated to\nindividuals. Indeed, budget allocation is a critical issue in advertising and\nviral marketing. Taking the other point of view, known models allowing flexible\nbudget allocation do not take into account the influence spread in the network.\nWe introduce a generalized model that captures both budgets and influence\npropagation simultaneously.\n  For the offline setting, we identify a large family of budget-based\npropagation functions that admit tight approximation guarantee. This family\nextends most of the previously studied influence models, including the\nwell-known Triggering model. We establish that any function in this family\nimplies an instance of a monotone submodular function maximization over the\ninteger lattice subject to a knapsack constraint. This problem is known to\nadmit an optimal (1-1/e)-approximation. We also study the price of anarchy of\nthe multi-player game that extends the model and establish tight results.\n  For the online setting, in which an unknown subset of agents arrive in a\nrandom order and the algorithm needs to make an irrevocable budget allocation\nin each step, we develop a 1/(15e)-competitive algorithm. This setting extends\nthe secretary problem, and its variant, the submodular knapsack secretary\nproblem. Notably, our algorithm improves over the best known approximation for\nthe latter problem, even though it applies to a more general setting.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 13:06:38 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 21:14:38 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 17:10:25 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Avigdor-Elgrabli", "Noa", ""], ["Blocq", "Gideon", ""], ["Gamzu", "Iftah", ""], ["Orda", "Ariel", ""]]}, {"id": "1508.01110", "submitter": "Vladimir Burichenko", "authors": "V.P. Burichenko", "title": "Symmetries of matrix multiplication algorithms. I", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work the algorithms of fast multiplication of matrices are\nconsidered. To any algorithm there associated a certain group of automorphisms.\nThese automorphism groups are found for some well-known algorithms, including\nalgorithms of Hopcroft, Laderman, and Pan. The automorphism group is isomorphic\nto $S_3\\times Z_2$ and $S_4$ for Hopcroft anf Laderman algorithms,\nrespectively. The studying of symmetry of algorithms may be a fruitful idea for\nfinding fast algorithms, by an analogy with well-known optimization problems\nfor codes, lattices, and graphs.\n  {\\em Keywords}: Strassen algorithm, symmetry, fast matrix multiplication.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 15:45:58 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Burichenko", "V. P.", ""]]}, {"id": "1508.01376", "submitter": "Abdolahad Noori Zehmakan", "authors": "Abdolahad Noori Zehmakan", "title": "Bin Packing Problem: Two Approximation Algorithms", "comments": "12 pages, 10 figures, 1 table in International Journal in Foundations\n  of Computer Science & Technology (IJFCST, July 2015, Volume 5, Number 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bin Packing Problem is one of the most important optimization problems.\nIn recent years, due to its NP-hard nature, several approximation algorithms\nhave been presented. It is proved that the best algorithm for the Bin Packing\nProblem has the approximation ratio 3/2 and the time order O(n), unless P=NP.\nIn this paper, first, a 3/2-approximation algorithm is presented, then a\nmodification to FFD algorithm is proposed to decrease time order to linear.\nFinally, these suggested approximation algorithms are compared with some other\napproximation algorithms. The experimental results show the suggested\nalgorithms perform efficiently. In summary, the main goal of the research is\npresenting methods which not only enjoy the best theoretical criteria, but also\nperform considerably efficient in practice.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 12:46:30 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Zehmakan", "Abdolahad Noori", ""]]}, {"id": "1508.01448", "submitter": "Rico Zenklusen", "authors": "Rico Zenklusen", "title": "An O(1)-Approximation for Minimum Spanning Tree Interdiction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network interdiction problems are a natural way to study the sensitivity of a\nnetwork optimization problem with respect to the removal of a limited set of\nedges or vertices. One of the oldest and best-studied interdiction problems is\nminimum spanning tree (MST) interdiction. Here, an undirected multigraph with\nnonnegative edge weights and positive interdiction costs on its edges is given,\ntogether with a positive budget B. The goal is to find a subset of edges R,\nwhose total interdiction cost does not exceed B, such that removing R leads to\na graph where the weight of an MST is as large as possible. Frederickson and\nSolis-Oba (SODA 1996) presented an O(log m)-approximation for MST interdiction,\nwhere m is the number of edges. Since then, no further progress has been made\nregarding approximations, and the question whether MST interdiction admits an\nO(1)-approximation remained open.\n  We answer this question in the affirmative, by presenting a 14-approximation\nthat overcomes two main hurdles that hindered further progress so far.\nMoreover, based on a well-known 2-approximation for the metric traveling\nsalesman problem (TSP), we show that our O(1)-approximation for MST\ninterdiction implies an O(1)-approximation for a natural interdiction version\nof metric TSP.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 16:11:24 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Zenklusen", "Rico", ""]]}, {"id": "1508.01504", "submitter": "Vijaya Ramachandran", "authors": "Richard Cole and Vijaya Ramachandran", "title": "Resource Oblivious Sorting on Multicores", "comments": "A version very similar to this appears in ACM Transactions on\n  Parallel Computing (TOPC), Vol. 3, No. 4, Article 23, 2017. The current\n  version adds some additional citations to earlier sorting algorithms, and a\n  comparison to Sharesort", "journal-ref": "ACM Transactions on Parallel Computing (TOPC), Vol. 3, No. 4,\n  Article 23, 2017", "doi": "10.1145/3040221", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic sorting algorithm, SPMS (Sample, Partition, and\nMerge Sort), that interleaves the partitioning of a sample sort with merging.\nSequentially, it sorts $n$ elements in $O(n \\log n)$ time cache-obliviously\nwith an optimal number of cache misses. The parallel complexity (or critical\npath length) of the algorithm is $O(\\log n \\cdot \\log\\log n)$, which improves\non previous bounds for optimal cache oblivious sorting. The algorithm also has\nlow false sharing costs. When scheduled by a work-stealing scheduler in a\nmulticore computing environment with a global shared memory and $p$ cores, each\nhaving a cache of size $M$ organized in blocks of size $B$, the costs of the\nadditional cache misses and false sharing misses due to this parallel execution\nare bounded by the cost of $O(S\\cdot M/B)$ and $O(S \\cdot B)$ cache misses\nrespectively, where $S$ is the number of steals performed during the execution.\nFinally, SPMS is resource oblivious in Athat the dependence on machine\nparameters appear only in the analysis of its performance, and not within the\nalgorithm itself.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 19:32:53 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 14:45:19 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Cole", "Richard", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1508.01657", "submitter": "Ren\\'e van Bevern", "authors": "Ren\\'e van Bevern and Rolf Niedermeier and Ond\\v{r}ej Such\\'y", "title": "A parameterized complexity view on non-preemptively scheduling\n  interval-constrained jobs: few machines, small looseness, and small slack", "comments": "Version accepted at Journal of Scheduling", "journal-ref": "Journal of Scheduling 20(3): 255-265 (2017)", "doi": "10.1007/s10951-016-0478-9", "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of non-preemptively scheduling $n$ jobs, each job $j$\nwith a release time $t_j$, a deadline $d_j$, and a processing time $p_j$, on\n$m$ parallel identical machines. Cieliebak et al. (2004) considered the two\nconstraints $|d_j-t_j|\\leq \\lambda p_j$ and $|d_j-t_j|\\leq p_j +\\sigma$ and\nshowed the problem to be NP-hard for any $\\lambda>1$ and for any $\\sigma\\geq\n2$. We complement their results by parameterized complexity studies: we show\nthat, for any $\\lambda>1$, the problem remains weakly NP-hard even for $m=2$\nand strongly W[1]-hard parameterized by $m$. We present a\npseudo-polynomial-time algorithm for constant $m$ and $\\lambda$ and a\nfixed-parameter tractability result for the parameter $m$ combined with\n$\\sigma$.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 10:52:31 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 16:52:18 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["van Bevern", "Ren\u00e9", ""], ["Niedermeier", "Rolf", ""], ["Such\u00fd", "Ond\u0159ej", ""]]}, {"id": "1508.01753", "submitter": "Martin Marinov Mr", "authors": "Martin Marinov, Nicholas Nash and David Gregg", "title": "Practical Algorithms for Finding Extremal Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimal sets within a collection of sets are defined as the ones which do\nnot have a proper subset within the collection, and the maximal sets are the\nones which do not have a proper superset within the collection. Identifying\nextremal sets is a fundamental problem with a wide-range of applications in SAT\nsolvers, data-mining and social network analysis. In this paper, we present two\nnovel improvements of the high-quality extremal set identification algorithm,\n\\textit{AMS-Lex}, described by Bayardo and Panda. The first technique uses\nmemoization to improve the execution time of the single-threaded variant of the\nAMS-Lex, whilst our second improvement uses parallel programming methods. In a\nsubset of the presented experiments our memoized algorithm executes more than\n$400$ times faster than the highly efficient publicly available implementation\nof AMS-Lex. Moreover, we show that our modified algorithm's speedup is not\nbounded above by a constant and that it increases as the length of the common\nprefixes in successive input \\textit{itemsets} increases. We provide\nexperimental results using both real-world and synthetic data sets, and show\nour multi-threaded variant algorithm out-performing AMS-Lex by $3$ to $6$\ntimes. We find that on synthetic input datasets when executed using $16$ CPU\ncores of a $32$-core machine, our multi-threaded program executes about as fast\nas the state of the art parallel GPU-based program using an NVIDIA GTX 580\ngraphics processing unit.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 16:33:54 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Marinov", "Martin", ""], ["Nash", "Nicholas", ""], ["Gregg", "David", ""]]}, {"id": "1508.01813", "submitter": "Kaarthik Sundar", "authors": "Kaarthik Sundar and Sivakumar Rathinam", "title": "Generalized multiple depot traveling salesmen problem - polyhedral study\n  and exact algorithm", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized multiple depot traveling salesmen problem (GMDTSP) is a\nvariant of the multiple depot traveling salesmen problem (MDTSP), where each\nsalesman starts at a distinct depot, the targets are partitioned into clusters\nand at least one target in each cluster is visited by some salesman. The GMDTSP\nis an NP-hard problem as it generalizes the MDTSP and has practical\napplications in design of ring networks, vehicle routing, flexible\nmanufacturing scheduling and postal routing. We present an integer programming\nformulation for the GMDTSP and valid inequalities to strengthen the linear\nprogramming relaxation. Furthermore, we present a polyhedral analysis of the\nconvex hull of feasible solutions to the GMDTSP and derive facet-defining\ninequalities that strengthen the linear programming relaxation of the GMDTSP.\nAll these results are then used to develop a branch-and-cut algorithm to obtain\noptimal solutions to the problem. The performance of the algorithm is evaluated\nthrough extensive computational experiments on several benchmark instances.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 20:42:18 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Sundar", "Kaarthik", ""], ["Rathinam", "Sivakumar", ""]]}, {"id": "1508.01907", "submitter": "John Wright", "authors": "Ryan O'Donnell and John Wright", "title": "Efficient quantum tomography", "comments": "25 pages. This version includes a new section on principal component\n  analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the quantum state tomography problem, one wishes to estimate an unknown\n$d$-dimensional mixed quantum state $\\rho$, given few copies. We show that\n$O(d/\\epsilon)$ copies suffice to obtain an estimate $\\hat{\\rho}$ that\nsatisfies $\\|\\hat{\\rho} - \\rho\\|_F^2 \\leq \\epsilon$ (with high probability). An\nimmediate consequence is that $O(\\mathrm{rank}(\\rho) \\cdot d/\\epsilon^2) \\leq\nO(d^2/\\epsilon^2)$ copies suffice to obtain an $\\epsilon$-accurate estimate in\nthe standard trace distance. This improves on the best known prior result of\n$O(d^3/\\epsilon^2)$ copies for full tomography, and even on the best known\nprior result of $O(d^2\\log(d/\\epsilon)/\\epsilon^2)$ copies for spectrum\nestimation. Our result is the first to show that nontrivial tomography can be\nobtained using a number of copies that is just linear in the dimension.\n  Next, we generalize these results to show that one can perform efficient\nprincipal component analysis on $\\rho$. Our main result is that $O(k\nd/\\epsilon^2)$ copies suffice to output a rank-$k$ approximation $\\hat{\\rho}$\nwhose trace distance error is at most $\\epsilon$ more than that of the best\nrank-$k$ approximator to $\\rho$. This subsumes our above trace distance\ntomography result and generalizes it to the case when $\\rho$ is not guaranteed\nto be of low rank. A key part of the proof is the analogous generalization of\nour spectrum-learning results: we show that the largest $k$ eigenvalues of\n$\\rho$ can be estimated to trace-distance error $\\epsilon$ using\n$O(k^2/\\epsilon^2)$ copies. In turn, this result relies on a new coupling\ntheorem concerning the Robinson-Schensted-Knuth algorithm that should be of\nindependent combinatorial interest.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 13:56:26 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2015 02:26:10 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["O'Donnell", "Ryan", ""], ["Wright", "John", ""]]}, {"id": "1508.01977", "submitter": "Sushant Sachdeva", "authors": "Sushant Sachdeva and Nisheeth K. Vishnoi", "title": "The Mixing Time of the Dikin Walk in a Polytope - A Simple Proof", "comments": "5 pages, published in Operations Research Letters", "journal-ref": "Operations Research Letters 2016 44 (5), 630-634", "doi": "10.1016/j.orl.2016.07.005", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the mixing time of the Dikin walk in a polytope - a random walk\nbased on the log-barrier from the interior point method literature. This walk,\nand a close variant, were studied by Narayanan (2016) and Kannan-Narayanan\n(2012). Bounds on its mixing time are important for algorithms for sampling and\noptimization over polytopes. Here, we provide a simple proof of their result\nthat this random walk mixes in time O(mn) for an n-dimensional polytope\ndescribed using m inequalities.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 02:55:52 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 03:39:37 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Sachdeva", "Sushant", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1508.02157", "submitter": "Niv Buchbinder", "authors": "Niv Buchbinder and Moran Feldman", "title": "Deterministic Algorithms for Submodular Maximization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomization is a fundamental tool used in many theoretical and practical\nareas of computer science. We study here the role of randomization in the area\nof submodular function maximization. In this area most algorithms are\nrandomized, and in almost all cases the approximation ratios obtained by\ncurrent randomized algorithms are superior to the best results obtained by\nknown deterministic algorithms. Derandomization of algorithms for general\nsubmodular function maximization seems hard since the access to the function is\ndone via a value oracle. This makes it hard, for example, to apply standard\nderandomization techniques such as conditional expectations. Therefore, an\ninteresting fundamental problem in this area is whether randomization is\ninherently necessary for obtaining good approximation ratios.\n  In this work we give evidence that randomization is not necessary for\nobtaining good algorithms by presenting a new technique for derandomization of\nalgorithms for submodular function maximization. Our high level idea is to\nmaintain explicitly a (small) distribution over the states of the algorithm,\nand carefully update it using marginal values obtained from an extreme point\nsolution of a suitable linear formulation. We demonstrate our technique on two\nrecent algorithms for unconstrained submodular maximization and for maximizing\nsubmodular function subject to a cardinality constraint. In particular, for\nunconstrained submodular maximization we obtain an optimal deterministic\n$1/2$-approximation showing that randomization is unnecessary for obtaining\noptimal results for this setting.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 08:02:44 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Buchbinder", "Niv", ""], ["Feldman", "Moran", ""]]}, {"id": "1508.02435", "submitter": "Tobias Weinzierl", "authors": "T. Weinzierl and B. Verleye and P. Henri and D. Roose", "title": "Two Particle-in-Grid Realisations on Spacetrees", "comments": null, "journal-ref": "Parallel Computing, 2016", "doi": "10.1016/j.parco.2015.12.007", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper studies two particle management strategies for dynamically\nadaptive Cartesian grids at hands of a particle-in-cell code. One holds the\nparticles within the grid cells, the other within the grid vertices. The\nfundamental challenge for the algorithmic strategies results from the fact that\nparticles may run through the grid without velocity constraints. To facilitate\nthis, we rely on multiscale grid representations. They allow us to lift and\ndrop particles between different spatial resolutions. We call this cell-based\nstrategy particle in tree (PIT). Our second approach assigns particles to\nvertices describing a dual grid (PIDT) and augments the lifts and drops with\nmultiscale linked cells.\n  Our experiments validate the two schemes at hands of an electrostatic\nparticle-in-cell code by retrieving the dispersion relation of Langmuir waves\nin a thermal plasma. They reveal that different particle and grid\ncharacteristics favour different realisations. The possibility that particles\ncan tunnel through an arbitrary number of grid cells implies that most data is\nexchanged between neighbouring ranks, while very few data is transferred\nnon-locally. This constraints the scalability as the code potentially has to\nrealise global communication. We show that the merger of an analysed tree\ngrammar with PIDT allows us to predict particle movements among several levels\nand to skip parts of this global communication a priori. It is capable to\noutperform several established implementations based upon trees and/or\nspace-filling curves.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 21:39:01 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2015 08:15:32 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Weinzierl", "T.", ""], ["Verleye", "B.", ""], ["Henri", "P.", ""], ["Roose", "D.", ""]]}, {"id": "1508.02439", "submitter": "Di Wang", "authors": "Di Wang, Satish Rao, Michael W. Mahoney", "title": "Unified Acceleration Method for Packing and Covering Problems via\n  Diameter Reduction", "comments": "Fixed typo in packing LP formulation (page 1), and wrong citation in\n  the discussion of earlier works on page 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear coupling method was introduced recently by Allen-Zhu and Orecchia\nfor solving convex optimization problems with first order methods, and it\nprovides a conceptually simple way to integrate a gradient descent step and\nmirror descent step in each iteration. The high-level approach of the linear\ncoupling method is very flexible, and it has shown initial promise by providing\nimproved algorithms for packing and covering linear programs. Somewhat\nsurprisingly, however, while the dependence of the convergence rate on the\nerror parameter $\\epsilon$ for packing problems was improved to\n$O(1/\\epsilon)$, which corresponds to what accelerated gradient methods are\ndesigned to achieve, the dependence for covering problems was only improved to\n$O(1/\\epsilon^{1.5})$, and even that required a different more complicated\nalgorithm. Given the close connections between packing and covering problems\nand since previous algorithms for these very related problems have led to the\nsame $\\epsilon$ dependence, this discrepancy is surprising, and it leaves open\nthe question of the exact role that the linear coupling is playing in\ncoordinating the complementary gradient and mirror descent step of the\nalgorithm. In this paper, we clarify these issues for linear coupling\nalgorithms for packing and covering linear programs, illustrating that the\nlinear coupling method can lead to improved $O(1/\\epsilon)$ dependence for both\npacking and covering problems in a unified manner, i.e., with the same\nalgorithm and almost identical analysis. Our main technical result is a novel\ndiameter reduction method for covering problems that is of independent interest\nand that may be useful in applying the accelerated linear coupling method to\nother combinatorial problems.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 21:56:20 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 06:41:38 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Wang", "Di", ""], ["Rao", "Satish", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1508.02440", "submitter": "Carlo Comin", "authors": "Carlo Comin", "title": "Energy Structure of Optimal Positional Strategies in Mean Payoff Games", "comments": "arXiv admin note: substantial text overlap with arXiv:1609.01517", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note studies structural aspects concerning Optimal Positional Strategies\n(OPSs) in Mean Payoff Games (MPGs), it is a contribution to understanding the\nrelationship between OPSs in MPGs and Small Energy-Progress Measures (SEPMs) in\nreweighted Energy Games (EGs). Firstly, it is observed that the space of all\nOPSs, $\\texttt{opt}_{\\Gamma}\\Sigma^M_0$, admits a unique complete decomposition\nin terms of so-called extremal-SEPM{s} in reweighted EG{s}; this points out\nwhat we called the \"Energy-Lattice $\\mathcal{X}^*_{\\Gamma}$ of\n$\\texttt{opt}_{\\Gamma}\\Sigma^M_0$\". Secondly, it is offered a pseudo-polynomial\ntotal-time recursive procedure for enumerating (w/o repetitions) all the\nelements of $\\mathcal{X}^*_{\\Gamma}$, and for computing the corresponding\npartitioning of $\\texttt{opt}_{\\Gamma}\\Sigma^M_0$. It is observed that the\ncorresponding recursion tree defines an additional lattice\n$\\mathcal{B}^*_{\\Gamma}$, whose elements are certain subgames $\\Gamma'\\subseteq\n\\Gamma$ that we call basic subgames. The extremal-SEPMs of a given \\MPG\n$\\Gamma$ coincide with the least-SEPMs of the basic subgames of $\\Gamma$; so,\n$\\mathcal{X}^*_{\\Gamma}$ is the energy-lattice comprising all and only the\nleast-SEPMs of the \\emph{basic} subgames of $\\Gamma$. The complexity of the\nproposed enumeration for both $\\mathcal{B}^*_{\\Gamma}$ and\n$\\mathcal{X}^*_{\\Gamma}$ is $O(|V|^3|E|W |\\mathcal{B}^*_{\\Gamma}|)$ total time\nand $O(|V||E|)+\\Theta\\big(|E| \\mathcal{B}^*_{\\Gamma}|\\big)$ working space.\nFinally, it is constructed an \\MPG $\\Gamma$ for which $|\\mathcal{B}^*_{\\Gamma}|\n> |\\mathcal{X}^*_\\Gamma|$, this proves that $\\mathcal{B}^*_{\\Gamma}$ and\n$\\mathcal{X}^*_\\Gamma$ are not isomorphic.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 22:02:56 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 13:47:33 GMT"}, {"version": "v3", "created": "Thu, 1 Dec 2016 22:56:07 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Comin", "Carlo", ""]]}, {"id": "1508.02477", "submitter": "Indranil Banerjee", "authors": "Indranil Banerjee, Dana Richards", "title": "Computing Maximal Layers Of Points in $E^{f(n)}$", "comments": "13 pages, submitted to LATIN 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a randomized algorithm for computing the collection\nof maximal layers for a point set in $E^{k}$ ($k = f(n)$). The input to our\nalgorithm is a point set $P = \\{p_1,...,p_n\\}$ with $p_i \\in E^{k}$. The\nproposed algorithm achieves a runtime of $O\\left(kn^{2 - {1 \\over \\log{k}} +\n\\log_k{\\left(1 + {2 \\over {k+1}}\\right)}}\\log{n}\\right)$ when $P$ is a random\norder and a runtime of $O(k^2 n^{3/2 + (\\log_{k}{(k-1)})/2}\\log{n})$ for an\narbitrary $P$. Both bounds hold in expectation. Additionally, the run time is\nbounded by $O(kn^2)$ in the worst case. This is the first non-trivial algorithm\nwhose run-time remains polynomial whenever $f(n)$ is bounded by some polynomial\nin $n$ while remaining sub-quadratic in $n$ for constant $k$. The algorithm is\nimplemented using a new data-structure for storing and answering dominance\nqueries over the set of incomparable points.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 03:21:06 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 21:45:18 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Banerjee", "Indranil", ""], ["Richards", "Dana", ""]]}, {"id": "1508.02550", "submitter": "Jouni Sir\\'en", "authors": "Andrea Farruggia, Travis Gagie, Gonzalo Navarro, Simon J. Puglisi,\n  Jouni Sir\\'en", "title": "Relative Suffix Trees", "comments": "Accepted to The Computer Journal. The implementation is available at\n  https://github.com/jltsiren/relative-fm", "journal-ref": null, "doi": "10.1093/comjnl/bxx108", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suffix trees are one of the most versatile data structures in stringology,\nwith many applications in bioinformatics. Their main drawback is their size,\nwhich can be tens of times larger than the input sequence. Much effort has been\nput into reducing the space usage, leading ultimately to compressed suffix\ntrees. These compressed data structures can efficiently simulate the suffix\ntree, while using space proportional to a compressed representation of the\nsequence. In this work, we take a new approach to compressed suffix trees for\nrepetitive sequence collections, such as collections of individual genomes. We\ncompress the suffix trees of individual sequences relative to the suffix tree\nof a reference sequence. These relative data structures provide competitive\ntime/space trade-offs, being almost as small as the smallest compressed suffix\ntrees for repetitive collections, and competitive in time with the largest and\nfastest compressed suffix trees.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 10:34:50 GMT"}, {"version": "v2", "created": "Sun, 14 May 2017 15:08:38 GMT"}, {"version": "v3", "created": "Fri, 15 Dec 2017 14:14:30 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Farruggia", "Andrea", ""], ["Gagie", "Travis", ""], ["Navarro", "Gonzalo", ""], ["Puglisi", "Simon J.", ""], ["Sir\u00e9n", "Jouni", ""]]}, {"id": "1508.02598", "submitter": "Steven Kelk", "authors": "Steven Kelk and Georgios Stamoulis", "title": "A note on convex characters, Fibonacci numbers and exponential-time\n  algorithms", "comments": "added a significant number of new results to the previous version (on\n  dynamic programming, g-spectra and so on)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic trees are used to model evolution: leaves are labelled to\nrepresent contemporary species (\"taxa\") and interior vertices represent extinct\nancestors. Informally, convex characters are measurements on the contemporary\nspecies in which the subset of species (both contemporary and extinct) that\nshare a given state, form a connected subtree. Given an unrooted, binary\nphylogenetic tree T on a set of n >= 2 taxa, a closed (but fairly opaque)\nexpression for the number of convex characters on T has been known since 1992,\nand this is independent of the exact topology of T. In this note we prove that\nthis number is actually equal to the (2n-1)th Fibonacci number. Next, we define\ng_k(T) to be the number of convex characters on T in which each state appears\non at least k taxa. We show that, somewhat curiously, g_2(T) is also\nindependent of the topology of T, and is equal to to the (n-1)th Fibonacci\nnumber. As we demonstrate, this topological neutrality subsequently breaks down\nfor k >= 3. However, we show that for each fixed k >= 1, g_k(T) can be computed\nin O(n) time and the set of characters thus counted can be efficiently listed\nand sampled. We use these insights to give a simple but effective exact\nalgorithm for the NP-hard maximum parsimony distance problem that runs in time\n$\\Theta( \\phi^{n} \\cdot n^2 )$, where $\\phi \\approx 1.618...$ is the golden\nratio, and an exact algorithm which computes the tree bisection and\nreconnection distance (equivalently, a maximum agreement forest) in time\n$\\Theta( \\phi^{2n}\\cdot \\text{poly}(n))$, where $\\phi^2 \\approx 2.619$.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 13:59:46 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 18:00:05 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2016 14:46:44 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Kelk", "Steven", ""], ["Stamoulis", "Georgios", ""]]}, {"id": "1508.02759", "submitter": "Thibaut Vidal", "authors": "Thibaut Vidal", "title": "Technical Note: Split Algorithm in O(n) for the Capacitated Vehicle\n  Routing Problem", "comments": null, "journal-ref": null, "doi": "10.1016/j.cor.2015.11.012", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Split algorithm is an essential building block of route-first\ncluster-second heuristics and modern genetic algorithms for vehicle routing\nproblems. The algorithm is used to partition a solution, represented as a giant\ntour without occurrences of the depot, into separate routes with minimum cost.\nAs highlighted by the recent survey of [Prins, Lacomme and Prodhon, Transport\nRes. C (40), 179-200], no less than 70 recent articles use this technique. In\nthe vehicle routing literature, Split is usually assimilated to the search for\na shortest path in a directed acyclic graph $\\mathcal{G}$ and solved in $O(nB)$\nusing Bellman's algorithm, where $n$ is the number of delivery points and $B$\nis the average number of feasible routes that start with a given customer in\nthe giant tour. Some linear-time algorithms are also known for this problem as\na consequence of a Monge property of $\\mathcal{G}$. In this article, we\nhighlight a stronger property of this graph, leading to a simple alternative\nalgorithm in $O(n)$. Experimentally, we observe that the approach is faster\nthan the classical Split for problem instances of practical size. We also\nextend the method to deal with a limited fleet and soft capacity constraints.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 21:41:37 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2015 23:51:21 GMT"}, {"version": "v3", "created": "Sat, 5 May 2018 17:26:07 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Vidal", "Thibaut", ""]]}, {"id": "1508.02773", "submitter": "Daniel Paulusma", "authors": "Konrad K. Dabrowski, Petr A. Golovach, Pim van 't Hof, Daniel\n  Paulusma, Dimitrios M. Thilikos", "title": "Editing to a Planar Graph of Given Degrees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following graph modification problem. Let the input consist\nof a graph $G=(V,E)$, a weight function $w\\colon V\\cup E\\rightarrow\n\\mathbb{N}$, a cost function $c\\colon V\\cup E\\rightarrow \\mathbb{N}$ and a\ndegree function $\\delta\\colon V\\rightarrow \\mathbb{N}_0$, together with three\nintegers $k_v, k_e$ and $C$. The question is whether we can delete a set of\nvertices of total weight at most $k_v$ and a set of edges of total weight at\nmost $k_e$ so that the total cost of the deleted elements is at most $C$ and\nevery non-deleted vertex $v$ has degree $\\delta(v)$ in the resulting graph\n$G'$. We also consider the variant in which $G'$ must be connected. Both\nproblems are known to be NP-complete and W[1]-hard when parameterized by\n$k_v+k_e$. We prove that, when restricted to planar graphs, they stay\nNP-complete but have polynomial kernels when parameterized by $k_v+k_e$.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 23:30:20 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Dabrowski", "Konrad K.", ""], ["Golovach", "Petr A.", ""], ["Hof", "Pim van 't", ""], ["Paulusma", "Daniel", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1508.02968", "submitter": "Djamal Belazzougui", "authors": "Djamal Belazzougui and Fabio Cunial", "title": "Space-efficient detection of unusual words", "comments": "arXiv admin note: text overlap with arXiv:1502.06370", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting all the strings that occur in a text more frequently or less\nfrequently than expected according to an IID or a Markov model is a basic\nproblem in string mining, yet current algorithms are based on data structures\nthat are either space-inefficient or incur large slowdowns, and current\nimplementations cannot scale to genomes or metagenomes in practice. In this\npaper we engineer an algorithm based on the suffix tree of a string to use just\na small data structure built on the Burrows-Wheeler transform, and a stack of\n$O(\\sigma^2\\log^2 n)$ bits, where $n$ is the length of the string and $\\sigma$\nis the size of the alphabet. The size of the stack is $o(n)$ except for very\nlarge values of $\\sigma$. We further improve the algorithm by removing its time\ndependency on $\\sigma$, by reporting only a subset of the maximal repeats and\nof the minimal rare words of the string, and by detecting and scoring candidate\nunder-represented strings that $\\textit{do not occur}$ in the string. Our\nalgorithms are practical and work directly on the BWT, thus they can be\nimmediately applied to a number of existing datasets that are available in this\nform, returning this string mining problem to a manageable scale.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 16:01:21 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Belazzougui", "Djamal", ""], ["Cunial", "Fabio", ""]]}, {"id": "1508.03064", "submitter": "Yves Lucet", "authors": "Yasha Pushak, Warren Hare, Yves Lucet", "title": "Multiple-Path Selection for new Highway Alignments using Discrete\n  Algorithms", "comments": "to be published in European Journal of Operational Research", "journal-ref": null, "doi": "10.1016/j.ejor.2015.07.039", "report-no": null, "categories": "cs.DS cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of finding multiple near-optimal,\nspatially-dissimilar paths that can be considered as alternatives in the\ndecision making process, for finding optimal corridors in which to construct a\nnew road. We further consider combinations of techniques for reducing the costs\nassociated with the computation and increasing the accuracy of the cost\nformulation. Numerical results for five algorithms to solve the dissimilar\nmultipath problem show that a \"bidirectional approach\" yields the fastest\nrunning times and the most robust algorithm. Further modifications of the\nalgorithms to reduce the running time were tested and it is shown that running\ntime can be reduced by an average of 56 percent without compromising the\nquality of the results.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 05:12:47 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Pushak", "Yasha", ""], ["Hare", "Warren", ""], ["Lucet", "Yves", ""]]}, {"id": "1508.03167", "submitter": "J\\'er\\'emie Lumbroso", "authors": "Axel Bacher, Olivier Bodini, Alexandros Hollender, J\\'er\\'emie\n  Lumbroso", "title": "MergeShuffle: A Very Fast, Parallel Random Permutation Algorithm", "comments": "Preliminary draft. 12 pages, 1 figure, 3 algorithms, implementation\n  code at https://github.com/axel-bacher/mergeshuffle", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces an algorithm, MergeShuffle, which is an extremely\nefficient algorithm to generate random permutations (or to randomly permute an\nexisting array). It is easy to implement, runs in $n\\log_2 n + O(1)$ time, is\nin-place, uses $n\\log_2 n + \\Theta(n)$ random bits, and can be parallelized\naccross any number of processes, in a shared-memory PRAM model. Finally, our\npreliminary simulations using OpenMP suggest it is more efficient than the\nRao-Sandelius algorithm, one of the fastest existing random permutation\nalgorithms.\n  We also show how it is possible to further reduce the number of random bits\nconsumed, by introducing a second algorithm BalancedShuffle, a variant of the\nRao-Sandelius algorithm which is more conservative in the way it recursively\npartitions arrays to be shuffled. While this algorithm is of lesser practical\ninterest, we believe it may be of theoretical value.\n  Our full code is available at: https://github.com/axel-bacher/mergeshuffle\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 10:37:54 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Bacher", "Axel", ""], ["Bodini", "Olivier", ""], ["Hollender", "Alexandros", ""], ["Lumbroso", "J\u00e9r\u00e9mie", ""]]}, {"id": "1508.03261", "submitter": "He Sun", "authors": "Yin Tat Lee and He Sun", "title": "Constructing Linear-Sized Spectral Sparsification in Almost-Linear Time", "comments": "22 pages. A preliminary version of this paper is to appear in\n  proceedings of the 56th Annual IEEE Symposium on Foundations of Computer\n  Science (FOCS 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first almost-linear time algorithm for constructing\nlinear-sized spectral sparsification for graphs. This improves all previous\nconstructions of linear-sized spectral sparsification, which requires\n$\\Omega(n^2)$ time.\n  A key ingredient in our algorithm is a novel combination of two techniques\nused in literature for constructing spectral sparsification: Random sampling by\neffective resistance, and adaptive constructions based on barrier functions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 16:24:28 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Lee", "Yin Tat", ""], ["Sun", "He", ""]]}, {"id": "1508.03337", "submitter": "Kimon Fountoulakis", "authors": "Kimon Fountoulakis, Abhisek Kundu, Eugenia-Maria Kontopoulou and\n  Petros Drineas", "title": "A Randomized Rounding Algorithm for Sparse PCA", "comments": "28 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and analyze a simple, two-step algorithm to approximate the\noptimal solution of the sparse PCA problem. Our approach first solves a L1\npenalized version of the NP-hard sparse PCA optimization problem and then uses\na randomized rounding strategy to sparsify the resulting dense solution. Our\nmain theoretical result guarantees an additive error approximation and provides\na tradeoff between sparsity and accuracy. Our experimental evaluation indicates\nthat our approach is competitive in practice, even compared to state-of-the-art\ntoolboxes such as Spasm.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 20:06:59 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 08:38:17 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2016 01:34:47 GMT"}, {"version": "v4", "created": "Sat, 26 Mar 2016 21:09:19 GMT"}, {"version": "v5", "created": "Tue, 22 Nov 2016 20:05:10 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Fountoulakis", "Kimon", ""], ["Kundu", "Abhisek", ""], ["Kontopoulou", "Eugenia-Maria", ""], ["Drineas", "Petros", ""]]}, {"id": "1508.03428", "submitter": "Dimitris Papamichail", "authors": "Dimitris Papamichail, Hongmei Liu, Vitor Machado, Nathan Gould, J.\n  Robert Coleman, Georgios Papamichail", "title": "Codon Context Optimization in Synthetic Gene Design", "comments": "9 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE q-bio.GN", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Advances in de novo synthesis of DNA and computational gene design methods\nmake possible the customization of genes by direct manipulation of features\nsuch as codon bias and mRNA secondary structure. Codon context is another\nfeature significantly affecting mRNA translational efficiency, but existing\nmethods and tools for evaluating and designing novel optimized protein coding\nsequences utilize untested heuristics and do not provide quantifiable\nguarantees on design quality. In this study we examine statistical properties\nof codon context measures in an effort to better understand the phenomenon. We\nanalyze the computational complexity of codon context optimization and design\nexact and efficient heuristic gene recoding algorithms under reasonable\nconstraint models. We also present a web-based tool for evaluating codon\ncontext bias in the appropriate context.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 07:00:07 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Papamichail", "Dimitris", ""], ["Liu", "Hongmei", ""], ["Machado", "Vitor", ""], ["Gould", "Nathan", ""], ["Coleman", "J. Robert", ""], ["Papamichail", "Georgios", ""]]}, {"id": "1508.03473", "submitter": "Fabrizio Frati", "authors": "Fabrizio Frati", "title": "A Lower Bound on the Diameter of the Flip Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The flip graph is the graph whose nodes correspond to non-isomorphic\ncombinatorial triangulations and whose edges connect pairs of triangulations\nthat can be obtained one from the other by flipping a single edge. In this note\nwe show that the diameter of the flip graph is at least $\\frac{7n}{3} +\n\\Theta(1)$, improving upon the previous $2n + \\Theta(1)$ lower bound.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 12:00:19 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Frati", "Fabrizio", ""]]}, {"id": "1508.03566", "submitter": "Paul Renaud-Goud", "authors": "Aras Atalar and Paul Renaud-Goud and Philippas Tsigas", "title": "Analyzing the Performance of Lock-Free Data Structures: A Conflict-based\n  Model", "comments": "Short version to appear in DISC'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the modeling and the analysis of the performance of\nlock-free concurrent data structures. Lock-free designs employ an optimistic\nconflict control mechanism, allowing several processes to access the shared\ndata object at the same time. They guarantee that at least one concurrent\noperation finishes in a finite number of its own steps regardless of the state\nof the operations. Our analysis considers such lock-free data structures that\ncan be represented as linear combinations of fixed size retry loops. Our main\ncontribution is a new way of modeling and analyzing a general class of\nlock-free algorithms, achieving predictions of throughput that are close to\nwhat we observe in practice. We emphasize two kinds of conflicts that shape the\nperformance: (i) hardware conflicts, due to concurrent calls to atomic\nprimitives; (ii) logical conflicts, caused by simultaneous operations on the\nshared data structure. We show how to deal with these hardware and logical\nconflicts separately, and how to combine them, so as to calculate the\nthroughput of lock-free algorithms. We propose also a common framework that\nenables a fair comparison between lock-free implementations by covering the\nwhole contention domain, together with a better understanding of the\nperformance impacting factors. This part of our analysis comes with a method\nfor calculating a good back-off strategy to finely tune the performance of a\nlock-free algorithm. Our experimental results, based on a set of widely used\nconcurrent data structures and on abstract lock-free designs, show that our\nanalysis follows closely the actual code behavior.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 16:43:56 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Atalar", "Aras", ""], ["Renaud-Goud", "Paul", ""], ["Tsigas", "Philippas", ""]]}, {"id": "1508.03572", "submitter": "Lukasz Kowalik", "authors": "Andreas Bj\\\"orklund, Petteri Kaski, {\\L}ukasz Kowalik", "title": "Fast Witness Extraction Using a Decision Oracle", "comments": "Journal version, 16 pages. Extended abstract presented at ESA'14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gist of many (NP-)hard combinatorial problems is to decide whether a\nuniverse of $n$ elements contains a witness consisting of $k$ elements that\nmatch some prescribed pattern. For some of these problems there are known\nadvanced algebra-based FPT algorithms which solve the decision problem but do\nnot return the witness. We investigate techniques for turning such a\nYES/NO-decision oracle into an algorithm for extracting a single witness, with\nan objective to obtain practical scalability for large values of $n$. By\nrelying on techniques from combinatorial group testing, we demonstrate that a\nwitness may be extracted with $O(k\\log n)$ queries to either a deterministic or\na randomized set inclusion oracle with one-sided probability of error.\nFurthermore, we demonstrate through implementation and experiments that the\nalgebra-based FPT algorithms are practical, in particular in the setting of the\n$k$-path problem. Also discussed are engineering issues such as optimizing\nfinite field arithmetic.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 17:13:13 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""], ["Kaski", "Petteri", ""], ["Kowalik", "\u0141ukasz", ""]]}, {"id": "1508.03593", "submitter": "Justin Hsu", "authors": "Sepehr Assadi, Justin Hsu, Shahin Jabbari", "title": "Online Assignment of Heterogeneous Tasks in Crowdsourcing Markets", "comments": "Extended version of paper in HCOMP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of heterogeneous task assignment in crowdsourcing\nmarkets from the point of view of the requester, who has a collection of tasks.\nWorkers arrive online one by one, and each declare a set of feasible tasks they\ncan solve, and desired payment for each feasible task. The requester must\ndecide on the fly which task (if any) to assign to the worker, while assigning\nworkers only to feasible tasks. The goal is to maximize the number of assigned\ntasks with a fixed overall budget.\n  We provide an online algorithm for this problem and prove an upper bound on\nthe competitive ratio of this algorithm against an arbitrary (possibly\nworst-case) sequence of workers who want small payments relative to the\nrequester's total budget. We further show an almost matching lower bound on the\ncompetitive ratio of any algorithm in this setting. Finally, we propose a\ndifferent algorithm that achieves an improved competitive ratio in the random\npermutation model, where the order of arrival of the workers is chosen\nuniformly at random. Apart from these strong theoretical guarantees, we carry\nout experiments on simulated data which demonstrates the practical\napplicability of our algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 18:24:37 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Assadi", "Sepehr", ""], ["Hsu", "Justin", ""], ["Jabbari", "Shahin", ""]]}, {"id": "1508.03600", "submitter": "Yusu Wang", "authors": "Anastasios Sidiropoulos and Yusu Wang", "title": "Metric embedding with outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of metric embeddings with \\emph{outliers}. Given some\nmetric space $(X,\\rho)$ we wish to find a small set of outlier points $K\n\\subset X$ and either an isometric or a low-distortion embedding of\n$(X\\setminus K,\\rho)$ into some target metric space. This is a natural problem\nthat captures scenarios where a small fraction of points in the input\ncorresponds to noise.\n  For the case of isometric embeddings we derive polynomial-time approximation\nalgorithms for minimizing the number of outliers when the target space is an\nultrametric, a tree metric, or constant-dimensional Euclidean space. The\napproximation factors are 3, 4 and 2, respectively. For the case of embedding\ninto an ultrametric or tree metric, we further improve the running time to\n$O(n^2)$ for an $n$-point input metric space, which is optimal. We complement\nthese upper bounds by showing that outlier embedding into ultrametrics, trees,\nand $d$-dimensional Euclidean space for any $d\\geq 2$ are all NP-hard, as well\nas NP-hard to approximate within a factor better than 2 assuming the Unique\nGame Conjecture.\n  For the case of non-isometries we consider embeddings with small\n$\\ell_{\\infty}$ distortion. We present polynomial-time \\emph{bi-criteria}\napproximation algorithms. Specifically, given some $\\epsilon > 0$, let\n$k_\\epsilon$ denote the minimum number of outliers required to obtain an\nembedding with distortion $\\epsilon$. For the case of embedding into\nultrametrics we obtain a polynomial-time algorithm which computes a set of at\nmost $3k_{\\epsilon}$ outliers and an embedding of the remaining points into an\nultrametric with distortion $O(\\epsilon \\log n)$. For embedding a metric of\nunit diameter into constant-dimensional Euclidean space we present a\npolynomial-time algorithm which computes a set of at most $2k_{\\epsilon}$\noutliers and an embedding of the remaining points with distortion\n$O(\\sqrt{\\epsilon})$.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 18:38:05 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Sidiropoulos", "Anastasios", ""], ["Wang", "Yusu", ""]]}, {"id": "1508.03619", "submitter": "Scott Beamer", "authors": "Scott Beamer, Krste Asanovi\\'c, David Patterson", "title": "The GAP Benchmark Suite", "comments": "small revisions to correspond to v1.0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a graph processing benchmark suite with the goal of helping to\nstandardize graph processing evaluations. Fewer differences between graph\nprocessing evaluations will make it easier to compare different research\nefforts and quantify improvements. The benchmark not only specifies graph\nkernels, input graphs, and evaluation methodologies, but it also provides\noptimized baseline implementations. These baseline implementations are\nrepresentative of state-of-the-art performance, and thus new contributions\nshould outperform them to demonstrate an improvement.\n  The input graphs are sized appropriately for shared memory platforms, but any\nimplementation on any platform that conforms to the benchmark's specifications\ncould be compared. This benchmark suite can be used in a variety of settings.\nGraph framework developers can demonstrate the generality of their programming\nmodel by implementing all of the benchmark's kernels and delivering competitive\nperformance on all of the benchmark's graphs. Algorithm designers can use the\ninput graphs and the baseline implementations to demonstrate their\ncontribution. Platform designers and performance analysts can use the suite as\na workload representative of graph processing.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 19:46:48 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 23:40:36 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2015 17:51:24 GMT"}, {"version": "v4", "created": "Tue, 16 May 2017 19:14:14 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Beamer", "Scott", ""], ["Asanovi\u0107", "Krste", ""], ["Patterson", "David", ""]]}, {"id": "1508.03657", "submitter": "Geir Agnarsson", "authors": "Geir Agnarsson, Raymond Greenlaw, Sanpawat Kantabutra", "title": "The complexity of cyber attacks in a new layered-security model and the\n  maximum-weight, rooted-subtree problem", "comments": "18 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our cyber security model we define the concept of {\\em penetration cost},\nwhich is the cost that must be paid in order to break into the next layer of\nsecurity. Given a tree $T$ rooted at a vertex $r$, a {\\em penetrating cost}\nedge function $c$ on $T$, a {\\em target-acquisition} vertex function $p$ on\n$T$, the attacker's {\\em budget} and the {\\em game-over threshold} $B,G \\in\n{\\mathbb{Q}}^{+}$ respectively, we consider the problem of determining the\nexistence of a rooted subtree $T'$ of $T$ within the attacker's budget (that\nis, the sum of the costs of the edges in $T'$ is less than or equal to $B$)\nwith total acquisition value more than the game-over threshold (that is, the\nsum of the target values of the nodes in $T'$ is greater than or equal to $G$).\nWe prove that the general version of this problem is intractable, but does\nadmit a polynomial time approximation scheme. We also analyze the complexity of\nthree restricted versions of the problems, where the penetration cost is the\nconstant function, integer-valued, and rational-valued among a given fixed\nnumber of distinct values.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 16:00:30 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Agnarsson", "Geir", ""], ["Greenlaw", "Raymond", ""], ["Kantabutra", "Sanpawat", ""]]}, {"id": "1508.03679", "submitter": "Yu Cheng", "authors": "Yu Cheng, Ho Yee Cheung, Shaddin Dughmi, Ehsan Emamjomeh-Zadeh, Li Han\n  and Shang-Hua Teng", "title": "Mixture Selection, Mechanism Design, and Signaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We pose and study a fundamental algorithmic problem which we term mixture\nselection, arising as a building block in a number of game-theoretic\napplications: Given a function $g$ from the $n$-dimensional hypercube to the\nbounded interval $[-1,1]$, and an $n \\times m$ matrix $A$ with bounded entries,\nmaximize $g(Ax)$ over $x$ in the $m$-dimensional simplex. This problem arises\nnaturally when one seeks to design a lottery over items for sale in an auction,\nor craft the posterior beliefs for agents in a Bayesian game through the\nprovision of information (a.k.a. signaling).\n  We present an approximation algorithm for this problem when $g$\nsimultaneously satisfies two smoothness properties: Lipschitz continuity with\nrespect to the $L^\\infty$ norm, and noise stability. The latter notion, which\nwe define and cater to our setting, controls the degree to which\nlow-probability errors in the inputs of $g$ can impact its output. When $g$ is\nboth $O(1)$-Lipschitz continuous and $O(1)$-stable, we obtain an (additive)\nPTAS for mixture selection. We also show that neither assumption suffices by\nitself for an additive PTAS, and both assumptions together do not suffice for\nan additive FPTAS.\n  We apply our algorithm to different game-theoretic applications from\nmechanism design and optimal signaling. We make progress on a number of open\nproblems suggested in prior work by easily reducing them to mixture selection:\nwe resolve an important special case of the small-menu lottery design problem\nposed by Dughmi, Han, and Nisan; we resolve the problem of revenue-maximizing\nsignaling in Bayesian second-price auctions posed by Emek et al. and Miltersen\nand Sheffet; we design a quasipolynomial-time approximation scheme for the\noptimal signaling problem in normal form games suggested by Dughmi; and we\ndesign an approximation algorithm for the optimal signaling problem in the\nvoting model of Alonso and C\\^{a}mara.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 23:52:14 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Cheng", "Yu", ""], ["Cheung", "Ho Yee", ""], ["Dughmi", "Shaddin", ""], ["Emamjomeh-Zadeh", "Ehsan", ""], ["Han", "Li", ""], ["Teng", "Shang-Hua", ""]]}, {"id": "1508.03698", "submitter": "Indranil Banerjee", "authors": "Indranil Banerjee, Dana Richards", "title": "Sorting Under 1-$\\infty$ Cost Model", "comments": "12 pages, 1 figure, submitted to STOC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of sorting under non-uniform comparison\ncosts, where costs are either 1 or $\\infty$. If comparing a pair has an\nassociated cost of $\\infty$ then we say that such a pair cannot be compared\n(forbidden pairs). Along with the set of elements $V$ the input to our problem\nis a graph $G(V, E)$, whose edges represents the pairs that we can compare\nincurring an unit of cost. Given a graph with $n$ vertices and $q$ forbidden\nedges we propose the first non-trivial deterministic algorithm which makes\n$O((q + n)\\log{n})$ comparisons with a total complexity of $O(n^2 +\nq^{\\omega/2})$, where $\\omega$ is the exponent in the complexity of matrix\nmultiplication. We also propose a simple randomized algorithm for the problem\nwhich makes $\\widetilde{O}(n^2/\\sqrt{q + n} + n\\sqrt{q})$ probes with high\nprobability. When the input graph is random we show that\n$\\widetilde{O}(\\min{(n^{3/2}, pn^2)})$ probes suffice, where $p$ is the edge\nprobability.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 05:46:59 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2015 05:17:13 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2015 02:06:16 GMT"}, {"version": "v4", "created": "Tue, 10 Nov 2015 21:34:40 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Banerjee", "Indranil", ""], ["Richards", "Dana", ""]]}, {"id": "1508.03735", "submitter": "Zhiwei Steven Wu", "authors": "Rachel Cummings, Katrina Ligett, Jaikumar Radhakrishnan, Aaron Roth,\n  Zhiwei Steven Wu", "title": "Coordination Complexity: Small Information Coordinating Large\n  Populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of a quantity that we call coordination complexity. In\na distributed optimization problem, the information defining a problem instance\nis distributed among $n$ parties, who need to each choose an action, which\njointly will form a solution to the optimization problem. The coordination\ncomplexity represents the minimal amount of information that a centralized\ncoordinator, who has full knowledge of the problem instance, needs to broadcast\nin order to coordinate the $n$ parties to play a nearly optimal solution.\n  We show that upper bounds on the coordination complexity of a problem imply\nthe existence of good jointly differentially private algorithms for solving\nthat problem, which in turn are known to upper bound the price of anarchy in\ncertain games with dynamically changing populations.\n  We show several results. We fully characterize the coordination complexity\nfor the problem of computing a many-to-one matching in a bipartite graph by\ngiving almost matching lower and upper bounds.Our upper bound in fact extends\nmuch more generally, to the problem of solving a linearly separable convex\nprogram. We also give a different upper bound technique, which we use to bound\nthe coordination complexity of coordinating a Nash equilibrium in a routing\ngame, and of computing a stable matching.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 14:02:53 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 09:04:49 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Cummings", "Rachel", ""], ["Ligett", "Katrina", ""], ["Radhakrishnan", "Jaikumar", ""], ["Roth", "Aaron", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1508.03769", "submitter": "Alan Roytman", "authors": "Lachlan L. H. Andrew, Siddharth Barman, Katrina Ligett, Minghong Lin,\n  Adam Meyerson, Alan Roytman, Adam Wierman", "title": "A Tale of Two Metrics: Simultaneous Bounds on Competitiveness and Regret", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider algorithms for \"smoothed online convex optimization\" problems, a\nvariant of the class of online convex optimization problems that is strongly\nrelated to metrical task systems. Prior literature on these problems has\nfocused on two performance metrics: regret and the competitive ratio. There\nexist known algorithms with sublinear regret and known algorithms with constant\ncompetitive ratios; however, no known algorithm achieves both simultaneously.\nWe show that this is due to a fundamental incompatibility between these two\nmetrics - no algorithm (deterministic or randomized) can achieve sublinear\nregret and a constant competitive ratio, even in the case when the objective\nfunctions are linear. However, we also exhibit an algorithm that, for the\nimportant special case of one-dimensional decision spaces, provides sublinear\nregret while maintaining a competitive ratio that grows arbitrarily slowly.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 20:53:06 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Andrew", "Lachlan L. H.", ""], ["Barman", "Siddharth", ""], ["Ligett", "Katrina", ""], ["Lin", "Minghong", ""], ["Meyerson", "Adam", ""], ["Roytman", "Alan", ""], ["Wierman", "Adam", ""]]}, {"id": "1508.03859", "submitter": "Calvin Newport", "authors": "Seth Gilbert and Calvin Newport", "title": "The Computational Power of Beeps", "comments": "Extended abstract to appear in the Proceedings of the International\n  Symposium on Distributed Computing (DISC 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the quantity of computational resources (state\nmachine states and/or probabilistic transition precision) needed to solve\nspecific problems in a single hop network where nodes communicate using only\nbeeps. We begin by focusing on randomized leader election. We prove a lower\nbound on the states required to solve this problem with a given error bound,\nprobability precision, and (when relevant) network size lower bound. We then\nshow the bound tight with a matching upper bound. Noting that our optimal upper\nbound is slow, we describe two faster algorithms that trade some state\noptimality to gain efficiency. We then turn our attention to more general\nclasses of problems by proving that once you have enough states to solve leader\nelection with a given error bound, you have (within constant factors) enough\nstates to simulate correctly, with this same error bound, a logspace TM with a\nconstant number of unary input tapes: allowing you to solve a large and\nexpressive set of problems. These results identify a key simplicity threshold\nbeyond which useful distributed computation is possible in the beeping model.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 19:58:37 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Gilbert", "Seth", ""], ["Newport", "Calvin", ""]]}, {"id": "1508.03931", "submitter": "Michael Goodrich", "authors": "Michael T. Goodrich and Timothy Johnson and Manuel Torres", "title": "Knuthian Drawings of Series-Parallel Flowcharts", "comments": "Full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by a classic paper by Knuth, we revisit the problem of drawing\nflowcharts of loop-free algorithms, that is, degree-three series-parallel\ndigraphs. Our drawing algorithms show that it is possible to produce Knuthian\ndrawings of degree-three series-parallel digraphs with good aspect ratios and\nsmall numbers of edge bends.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 05:59:27 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Goodrich", "Michael T.", ""], ["Johnson", "Timothy", ""], ["Torres", "Manuel", ""]]}, {"id": "1508.03992", "submitter": "Eduardo Xavier", "authors": "Andrew Twigg and Eduardo C. Xavier", "title": "Locality-preserving allocations Problems and coloured Bin Packing", "comments": null, "journal-ref": null, "doi": "10.1016/j.tcs.2015.06.036", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following problem, introduced by Chung et al. in 2006. We are\ngiven, online or offline, a set of coloured items of different sizes, and wish\nto pack them into bins of equal size so that we use few bins in total (at most\n$\\alpha$ times optimal), and that the items of each colour span few bins (at\nmost $\\beta$ times optimal). We call such allocations $(\\alpha,\n\\beta)$-approximate. As usual in bin packing problems, we allow additive\nconstants and consider $(\\alpha,\\beta)$ as the asymptotic performance ratios.\nWe prove that for $\\eps>0$, if we desire small $\\alpha$, no scheme can beat\n$(1+\\eps, \\Omega(1/\\eps))$-approximate allocations and similarly as we desire\nsmall $\\beta$, no scheme can beat $(1.69103, 1+\\eps)$-approximate allocations.\nWe give offline schemes that come very close to achieving these lower bounds.\nFor the online case, we prove that no scheme can even achieve\n$(O(1),O(1))$-approximate allocations. However, a small restriction on item\nsizes permits a simple online scheme that computes $(2+\\eps, 1.7)$-approximate\nallocations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 12:08:08 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Twigg", "Andrew", ""], ["Xavier", "Eduardo C.", ""]]}, {"id": "1508.04095", "submitter": "Omar Fawzi", "authors": "Siddharth Barman and Omar Fawzi", "title": "Algorithmic Aspects of Optimal Channel Coding", "comments": "v2: 16 pages. Added alternate proof of main result with random coding", "journal-ref": "IEEE Transactions on Information Theory ( Volume: 64, Issue: 2,\n  Feb. 2018 )", "doi": "10.1109/TIT.2017.2696963", "report-no": null, "categories": "cs.IT cs.DS math.IT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central question in information theory is to determine the maximum success\nprobability that can be achieved in sending a fixed number of messages over a\nnoisy channel. This was first studied in the pioneering work of Shannon who\nestablished a simple expression characterizing this quantity in the limit of\nmultiple independent uses of the channel. Here we consider the general setting\nwith only one use of the channel. We observe that the maximum success\nprobability can be expressed as the maximum value of a submodular function.\nUsing this connection, we establish the following results:\n  1. There is a simple greedy polynomial-time algorithm that computes a code\nachieving a (1-1/e)-approximation of the maximum success probability. Moreover,\nfor this problem it is NP-hard to obtain an approximation ratio strictly better\nthan (1-1/e).\n  2. Shared quantum entanglement between the sender and the receiver can\nincrease the success probability by a factor of at most 1/(1-1/e). In addition,\nthis factor is tight if one allows an arbitrary non-signaling box between the\nsender and the receiver.\n  3. We give tight bounds on the one-shot performance of the meta-converse of\nPolyanskiy-Poor-Verdu.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 17:41:47 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 15:44:34 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Barman", "Siddharth", ""], ["Fawzi", "Omar", ""]]}, {"id": "1508.04234", "submitter": "Amitabh Trehan", "authors": "Armando Castaneda, Danny Dolev and Amitabh Trehan", "title": "Compact Routing Messages in Self-Healing Trees", "comments": "Under Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing compact routing schemes, e.g., Thorup and Zwick [SPAA 2001] and\nChechik [PODC 2013], often have no means to tolerate failures, once the system\nhas been setup and started. This paper presents, to our knowledge, the first\nself-healing compact routing scheme. Besides, our schemes are developed for low\nmemory nodes, i.e., nodes need only $O(\\log^2 n)$ memory, and are thus, compact\nschemes.\n  We introduce two algorithms of independent interest: The first is CompactFT,\na novel compact version (using only $O(\\log n)$ local memory) of the\nself-healing algorithm Forgiving Tree of Hayes et al. [PODC 2008]. The second\nalgorithm (CompactFTZ) combines CompactFT with Thorup-Zwick's tree-based\ncompact routing scheme [SPAA 2001] to produce a fully compact self-healing\nrouting scheme. In the self-healing model, the adversary deletes nodes one at a\ntime with the affected nodes self-healing locally by adding few edges.\nCompactFT recovers from each attack in only $O(1)$ time and $\\Delta$ messages,\nwith only +3 degree increase and $O(log \\Delta)$ graph diameter increase, over\nany sequence of deletions ($\\Delta$ is the initial maximum degree).\n  Additionally, CompactFTZ guarantees delivery of a packet sent from sender s\nas long as the receiver t has not been deleted, with only an additional $O(y\n\\log \\Delta)$ latency, where $y$ is the number of nodes that have been deleted\non the path between $s$ and $t$. If $t$ has been deleted, $s$ gets informed and\nthe packet removed from the network.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 07:47:55 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Castaneda", "Armando", ""], ["Dolev", "Danny", ""], ["Trehan", "Amitabh", ""]]}, {"id": "1508.04250", "submitter": "Salman Fadaei", "authors": "Salman Fadaei", "title": "Mechanism Design via Dantzig-Wolfe Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In random allocation rules, typically first an optimal fractional point is\ncalculated via solving a linear program. The calculated point represents a\nfractional assignment of objects or more generally packages of objects to\nagents. In order to implement an expected assignment, the mechanism designer\nmust decompose the fractional point into integer solutions, each satisfying\nunderlying constraints. The resulting convex combination can then be viewed as\na probability distribution over feasible assignments out of which a random\nassignment can be sampled. This approach has been successfully employed in\ncombinatorial optimization as well as mechanism design with or without money.\n  In this paper, we show that both finding the optimal fractional point as well\nas its decomposition into integer solutions can be done at once. We propose an\nappropriate linear program which provides the desired solution. We show that\nthe linear program can be solved via Dantzig-Wolfe decomposition. Dantzig-Wolfe\ndecomposition is a direct implementation of the revised simplex method which is\nwell known to be highly efficient in practice. We also show how to use the\nBenders decomposition as an alternative method to solve the problem. The\nproposed method can also find a decomposition into integer solutions when the\nfractional point is readily present perhaps as an outcome of other algorithms\nrather than linear programming. The resulting convex decomposition in this case\nis tight in terms of the number of integer points according to the\nCarath{\\'e}odory's theorem.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 08:59:53 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 19:43:00 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Fadaei", "Salman", ""]]}, {"id": "1508.04278", "submitter": "Fabian Fuchs", "authors": "Fabian Fuchs, Matthias Wolf", "title": "On the Distributed Computation of Fractional Connected Dominating Set\n  Packings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most fundamental problems in wireless networks is to achieve high\nthroughput. Fractional Connected Dominating Set (FCDS) Packings can achieve a\nthroughput of ${\\Theta}(k/\\log n)$ messages for networks with node connectivity\n$k$, which is optimal regarding routing-based message transmission. FCDS were\nproposed by Censor-Hillel \\emph{et al.} [SODA'14,PODC'14] and are a natural\ngeneralization to Connected Dominating Sets (CDS), allowing each node to\nparticipate with a fraction of its weight in multiple FCDS. Thus, $\\Omega(k)$\nco-existing transmission backbones are established, taking full advantage of\nthe networks connectivity. We propose a modified distributed algorithm that\nimproves upon previous algorithms for $k\\Delta \\in o(\\min\\{\\frac{n \\log n}{k}\n,D,\\sqrt{n \\log n} \\log^* n\\}\\log n)$, where $\\Delta$ is the maximum node\ndegree, $D$ the diameter and $n$ the number of nodes in the network. We achieve\nthis by explicitly computing connections between tentative dominating sets.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 11:28:39 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Fuchs", "Fabian", ""], ["Wolf", "Matthias", ""]]}, {"id": "1508.04522", "submitter": "Palash Dey", "authors": "Arnab Bhattacharyya and Palash Dey", "title": "Fishing out Winners from Vote Streams", "comments": "Adding Acknowledgement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DM cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of winner determination from computational social\nchoice theory in the data stream model. Specifically, we consider the task of\nsummarizing an arbitrarily ordered stream of $n$ votes on $m$ candidates into a\nsmall space data structure so as to be able to obtain the winner determined by\npopular voting rules. As we show, finding the exact winner requires storing\nessentially all the votes. So, we focus on the problem of finding an {\\em\n$\\eps$-winner}, a candidate who could win by a change of at most $\\eps$\nfraction of the votes. We show non-trivial upper and lower bounds on the space\ncomplexity of $\\eps$-winner determination for several voting rules, including\n$k$-approval, $k$-veto, scoring rules, approval, maximin, Bucklin, Copeland,\nand plurality with run off.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 04:09:03 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 04:27:41 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Dey", "Palash", ""]]}, {"id": "1508.04747", "submitter": "Fabian Kuhn", "authors": "Mohsen Ghaffari and Andreas Karrenbauer and Fabian Kuhn and Christoph\n  Lenzen and Boaz Patt-Shamir", "title": "Near-Optimal Distributed Maximum Flow", "comments": "34 pages, 5 figures, conference version appeared in ACM Symp. on\n  Principles of Distributed Computing (PODC) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a near-optimal distributed algorithm for $(1+o(1))$-approximation\nof single-commodity maximum flow in undirected weighted networks that runs in\n$(D+ \\sqrt{n})\\cdot n^{o(1)}$ communication rounds in the \\Congest model. Here,\n$n$ and $D$ denote the number of nodes and the network diameter, respectively.\nThis is the first improvement over the trivial bound of $O(n^2)$, and it nearly\nmatches the $\\tilde{\\Omega}(D+ \\sqrt{n})$ round complexity lower bound.\n  The development of the algorithm contains two results of independent\ninterest:\n  (i) A $(D+\\sqrt{n})\\cdot n^{o(1)}$-round distributed construction of a\nspanning tree of average stretch $n^{o(1)}$.\n  (ii) A $(D+\\sqrt{n})\\cdot n^{o(1)}$-round distributed construction of an\n$n^{o(1)}$-congestion approximator consisting of the cuts induced by $O(\\log\nn)$ virtual trees. The distributed representation of the cut approximator\nallows for evaluation in $(D+\\sqrt{n})\\cdot n^{o(1)}$ rounds.\n  All our algorithms make use of randomization and succeed with high\nprobability.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 19:17:55 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Karrenbauer", "Andreas", ""], ["Kuhn", "Fabian", ""], ["Lenzen", "Christoph", ""], ["Patt-Shamir", "Boaz", ""]]}, {"id": "1508.04783", "submitter": "Aida Ouangraoua", "authors": "Fran\\c{c}ois B\\'elanger and A\\\"ida Ouangraoua", "title": "Alignment of protein-coding sequences with frameshift extension\n  penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an algorithm for the alignment of protein- coding sequences\naccounting for frameshifts. The main specificity of this algorithm as compared\nto previously published protein-coding sequence alignment methods is the\nintroduction of a penalty cost for frameshift ex- tensions. Previous algorithms\nhave only used constant frameshift penal- ties. This is similar to the use of\nscoring schemes with affine gap penalties in classical sequence alignment\nalgorithms. However, the overall penalty of a frameshift portion in an\nalignment cannot be formulated as an affine function, because it should also\nincorporate varying codon substitution scores. The second specificity of the\nalgorithm is its search space being the set of all possible alignments between\ntwo coding sequences, under the classical definition of an alignment between\ntwo DNA sequences. Previous algorithms have introduced constraints on the\nlength of the alignments, and additional symbols for the representation of\nframeshift openings in an alignment. The algorithm has the same asymptotic\nspace and time complexity as the classical Needleman-Wunsch algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 20:26:38 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["B\u00e9langer", "Fran\u00e7ois", ""], ["Ouangraoua", "A\u00efda", ""]]}, {"id": "1508.04816", "submitter": "Nikesh Dattani", "authors": "Richard Tanburn (Oxford University), Emile Okada (Cambridge\n  University), Nike Dattani (Kyoto University)", "title": "Reducing multi-qubit interactions in adiabatic quantum computation\n  without adding auxiliary qubits. Part 1: The \"deduc-reduc\" method and its\n  application to quantum factorization of numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DM cs.DS math.NT", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Adiabatic quantum computing has recently been used to factor 56153 [Dattani &\nBryans, arXiv:1411.6758] at room temperature, which is orders of magnitude\nlarger than any number attempted yet using Shor's algorithm (circuit-based\nquantum computation). However, this number is still vastly smaller than RSA-768\nwhich is the largest number factored thus far on a classical computer. We\naddress a major issue arising in the scaling of adiabatic quantum factorization\nto much larger numbers. Namely, the existence of many 4-qubit, 3-qubit and\n2-qubit interactions in the Hamiltonians. We showcase our method on various\nexamples, one of which shows that we can remove 94% of the 4-qubit interactions\nand 83% of the 3-qubit interactions in the factorization of a 25-digit number\nwith almost no effort, without adding any auxiliary qubits. Our method is not\nlimited to quantum factoring. Its importance extends to the wider field of\ndiscrete optimization. Any CSP (constraint-satisfiability problem),\npsuedo-boolean optimization problem, or QUBO (quadratic unconstrained Boolean\noptimization) problem can in principle benefit from the \"deduction-reduction\"\nmethod which we introduce in this paper. We provide an open source code which\ntakes in a Hamiltonian (or a discrete discrete function which needs to be\noptimized), and returns a Hamiltonian that has the same unique ground state(s),\nno new auxiliary variables, and as few multi-qubit (multi-variable) terms as\npossible with deduc-reduc.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 22:20:29 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 23:48:10 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Tanburn", "Richard", "", "Oxford University"], ["Okada", "Emile", "", "Cambridge\n  University"], ["Dattani", "Nike", "", "Kyoto University"]]}, {"id": "1508.04874", "submitter": "Sam Chiu-wai Wong", "authors": "Yin Tat Lee, Aaron Sidford and Sam Chiu-wai Wong", "title": "A Faster Cutting Plane Method and its Implications for Combinatorial and\n  Convex Optimization", "comments": "111 pages, FOCS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve upon the running time for finding a point in a convex set given a\nseparation oracle. In particular, given a separation oracle for a convex set\n$K\\subset \\mathbb{R}^n$ contained in a box of radius $R$, we show how to either\nfind a point in $K$ or prove that $K$ does not contain a ball of radius\n$\\epsilon$ using an expected $O(n\\log(nR/\\epsilon))$ oracle evaluations and\nadditional time $O(n^3\\log^{O(1)}(nR/\\epsilon))$. This matches the oracle\ncomplexity and improves upon the $O(n^{\\omega+1}\\log(nR/\\epsilon))$ additional\ntime of the previous fastest algorithm achieved over 25 years ago by Vaidya for\nthe current matrix multiplication constant $\\omega<2.373$ when\n$R/\\epsilon=n^{O(1)}$.\n  Using a mix of standard reductions and new techniques, our algorithm yields\nimproved runtimes for solving classic problems in continuous and combinatorial\noptimization:\n  Submodular Minimization: Our weakly and strongly polynomial time algorithms\nhave runtimes of $O(n^2\\log nM\\cdot\\text{EO}+n^3\\log^{O(1)}nM)$ and\n$O(n^3\\log^2 n\\cdot\\text{EO}+n^4\\log^{O(1)}n)$, improving upon the previous\nbest of $O((n^4\\text{EO}+n^5)\\log M)$ and $O(n^5\\text{EO}+n^6)$.\n  Matroid Intersection: Our runtimes are $O(nrT_{\\text{rank}}\\log n\\log (nM)\n+n^3\\log^{O(1)}(nM))$ and $O(n^2\\log (nM) T_{\\text{ind}}+n^3 \\log^{O(1)}\n(nM))$, achieving the first quadratic bound on the query complexity for the\nindependence and rank oracles. In the unweighted case, this is the first\nimprovement since 1986 for independence oracle.\n  Submodular Flow: Our runtime is $O(n^2\\log\nnCU\\cdot\\text{EO}+n^3\\log^{O(1)}nCU)$, improving upon the previous bests from\n15 years ago roughly by a factor of $O(n^4)$.\n  Semidefinite Programming: Our runtime is $\\tilde{O}(n(n^2+m^{\\omega}+S))$,\nimproving upon the previous best of $\\tilde{O}(n(n^{\\omega}+m^{\\omega}+S))$ for\nthe regime where the number of nonzeros $S$ is small.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 04:44:51 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 07:04:31 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Lee", "Yin Tat", ""], ["Sidford", "Aaron", ""], ["Wong", "Sam Chiu-wai", ""]]}, {"id": "1508.05013", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh", "title": "Message Passing and Combinatorial Optimization", "comments": "Ravanbakhsh, S. (2015), Message Passing and Combinatorial\n  Optimization, PhD thesis, University of Alberta", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.DS math.AC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models use the intuitive and well-studied methods of graph theory\nto implicitly represent dependencies between variables in large systems. They\ncan model the global behaviour of a complex system by specifying only local\nfactors. This thesis studies inference in discrete graphical models from an\nalgebraic perspective and the ways inference can be used to express and\napproximate NP-hard combinatorial problems.\n  We investigate the complexity and reducibility of various inference problems,\nin part by organizing them in an inference hierarchy. We then investigate\ntractable approximations for a subset of these problems using distributive law\nin the form of message passing. The quality of the resulting message passing\nprocedure, called Belief Propagation (BP), depends on the influence of loops in\nthe graphical model. We contribute to three classes of approximations that\nimprove BP for loopy graphs A) loop correction techniques; B) survey\npropagation, another message passing technique that surpasses BP in some\nsettings; and C) hybrid methods that interpolate between deterministic message\npassing and Markov Chain Monte Carlo inference.\n  We then review the existing message passing solutions and provide novel\ngraphical models and inference techniques for combinatorial problems under\nthree broad classes: A) constraint satisfaction problems such as\nsatisfiability, coloring, packing, set / clique-cover and dominating /\nindependent set and their optimization counterparts; B) clustering problems\nsuch as hierarchical clustering, K-median, K-clustering, K-center and\nmodularity optimization; C) problems over permutations including assignment,\ngraph morphisms and alignment, finding symmetries and traveling salesman\nproblem. In many cases we show that message passing is able to find solutions\nthat are either near optimal or favourably compare with today's\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 15:50:45 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Ravanbakhsh", "Siamak", ""]]}, {"id": "1508.05117", "submitter": "Federico Ricci-Tersenghi", "authors": "Raffaele Marino, Giorgio Parisi and Federico Ricci-Tersenghi", "title": "The backtracking survey propagation algorithm for solving random K-SAT\n  problems", "comments": "11 pages, 10 figures. v2: data largely improved and manuscript\n  rewritten", "journal-ref": "Nature Communications 7, 12996 (2016)", "doi": "10.1038/ncomms12996", "report-no": null, "categories": "cs.CC cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete combinatorial optimization has a central role in many scientific\ndisciplines, however, for hard problems we lack linear time algorithms that\nwould allow us to solve very large instances. Moreover, it is still unclear\nwhat are the key features that make a discrete combinatorial optimization\nproblem hard to solve. Here we study random K-satisfiability problems with\n$K=3,4$, which are known to be very hard close to the SAT-UNSAT threshold,\nwhere problems stop having solutions. We show that the backtracking survey\npropagation algorithm, in a time practically linear in the problem size, is\nable to find solutions very close to the threshold, in a region unreachable by\nany other algorithm. All solutions found have no frozen variables, thus\nsupporting the conjecture that only unfrozen solutions can be found in linear\ntime, and that a problem becomes impossible to solve in linear time when all\nsolutions contain frozen variables.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 20:41:29 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 17:00:45 GMT"}, {"version": "v3", "created": "Tue, 19 Apr 2016 22:26:58 GMT"}, {"version": "v4", "created": "Thu, 6 Oct 2016 07:37:19 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Marino", "Raffaele", ""], ["Parisi", "Giorgio", ""], ["Ricci-Tersenghi", "Federico", ""]]}, {"id": "1508.05143", "submitter": "Haris Aziz", "authors": "Haris Aziz and Simon Mackenzie", "title": "A Discrete and Bounded Envy-free Cake Cutting Protocol for Four Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the well-studied cake cutting problem in which the goal is to\nidentify a fair allocation based on a minimal number of queries from the\nagents. The problem has attracted considerable attention within various\nbranches of computer science, mathematics, and economics. Although, the elegant\nSelfridge-Conway envy-free protocol for three agents has been known since 1960,\nit has been a major open problem for the last fifty years to obtain a bounded\nenvy-free protocol for more than three agents. We propose a discrete and\nbounded envy-free protocol for four agents.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 22:51:21 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 21:00:07 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Aziz", "Haris", ""], ["Mackenzie", "Simon", ""]]}, {"id": "1508.05282", "submitter": "Marek Cygan", "authors": "Ivan Bliznets, Marek Cygan, Pawel Komosa, Lukas Mach, Michal Pilipczuk", "title": "Lower bounds for the parameterized complexity of Minimum Fill-in and\n  other completion problems", "comments": "Accepted to SODA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we focus on several completion problems for subclasses of\nchordal graphs: Minimum Fill-In, Interval Completion, Proper Interval\nCompletion, Threshold Completion, and Trivially Perfect Completion. In these\nproblems, the task is to add at most k edges to a given graph in order to\nobtain a chordal, interval, proper interval, threshold, or trivially perfect\ngraph, respectively. We prove the following lower bounds for all these\nproblems, as well as for the related Chain Completion problem: Assuming the\nExponential Time Hypothesis, none of these problems can be solved in time\n2^O(n^(1/2) / log^c n) or 2^O(k^(1/4) / log^c k) n^O(1), for some integer c.\nAssuming the non-existence of a subexponential-time approximation scheme for\nMin Bisection on d-regular graphs, for some constant d, none of these problems\ncan be solved in time 2^o(n) or 2^o(sqrt(k)) n^O(1).\n  For all the aforementioned completion problems, apart from Proper Interval\nCompletion, FPT algorithms with running time of the form 2^O(sqrt(k) log k)\nn^O(1) are known. Thus, the second result proves that a significant improvement\nof any of these algorithms would lead to a surprising breakthrough in the\ndesign of approximation algorithms for Min Bisection.\n  To prove our results, we use a reduction methodology based on combining the\nclassic approach of starting with a sparse instance of 3-Sat, prepared using\nthe Sparsification Lemma, with the existence of almost linear-size\nProbabilistically Checkable Proofs (PCPs). Apart from our main results, we also\nobtain lower bounds excluding the existence of subexponential algorithms for\nthe Optimum Linear Arrangement problem, as well as improved, yet still not\ntight, lower bounds for Feedback Arc Set in Tournaments.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 14:29:21 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2015 09:09:34 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Bliznets", "Ivan", ""], ["Cygan", "Marek", ""], ["Komosa", "Pawel", ""], ["Mach", "Lukas", ""], ["Pilipczuk", "Michal", ""]]}, {"id": "1508.05424", "submitter": "Charles Semple", "authors": "Magnus Bordewich and Charles Semple", "title": "Reticulation-visible networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X$ be a finite set, $\\mathcal N$ be a reticulation-visible network on\n$X$, and $\\mathcal T$ be a rooted binary phylogenetic tree. We show that there\nis a polynomial-time algorithm for deciding whether or not $\\mathcal N$\ndisplays $\\mathcal T$. Furthermore, for all $|X|\\ge 1$, we show that $\\mathcal\nN$ has at most $8|X|-7$ vertices in total and at most $3|X|-3$ reticulation\nvertices, and that these upper bounds are sharp.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 22:03:35 GMT"}, {"version": "v2", "created": "Sun, 25 Jun 2017 02:56:06 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Bordewich", "Magnus", ""], ["Semple", "Charles", ""]]}, {"id": "1508.05515", "submitter": "Zhao Zhang", "authors": "Yishuo Shi, Zhao Zhang, Ding-Zhu Du", "title": "Approximation Algorithm for Minimum Weight $(k,m)$-CDS Problem in Unit\n  Disk Graph", "comments": null, "journal-ref": "IEEE/ACM Transactions on Networking, 25(2) (2017.4) 925-933", "doi": "10.1109/TNET.2016.2607723", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wireless sensor network, the virtual backbone plays an important role.\nDue to accidental damage or energy depletion, it is desirable that the virtual\nbackbone is fault-tolerant. A fault-tolerant virtual backbone can be modeled as\na $k$-connected $m$-fold dominating set ($(k,m)$-CDS for short). In this paper,\nwe present a constant approximation algorithm for the minimum weight\n$(k,m)$-CDS problem in unit disk graphs under the assumption that $k$ and $m$\nare two fixed constants with $m\\geq k$. Prior to this work, constant\napproximation algorithms are known for $k=1$ with weight and $2\\leq k\\leq 3$\nwithout weight. Our result is the first constant approximation algorithm for\nthe $(k,m)$-CDS problem with general $k,m$ and with weight. The performance\nratio is $(\\alpha+2.5k\\rho)$ for $k\\geq 3$ and $(\\alpha+2.5\\rho)$ for $k=2$,\nwhere $\\alpha$ is the performance ratio for the minimum weight $m$-fold\ndominating set problem and $\\rho$ is the performance ratio for the subset\n$k$-connected subgraph problem (both problems are known to have constant\nperformance ratios.)\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 13:57:44 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2019 06:24:30 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Shi", "Yishuo", ""], ["Zhang", "Zhao", ""], ["Du", "Ding-Zhu", ""]]}, {"id": "1508.05538", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Vladimir Nikishkin", "title": "Optimal Algorithms and Lower Bounds for Testing Closeness of Structured\n  Distributions", "comments": "27 pages, to appear in FOCS'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a general unified method that can be used for $L_1$ {\\em closeness\ntesting} of a wide range of univariate structured distribution families. More\nspecifically, we design a sample optimal and computationally efficient\nalgorithm for testing the equivalence of two unknown (potentially arbitrary)\nunivariate distributions under the $\\mathcal{A}_k$-distance metric: Given\nsample access to distributions with density functions $p, q: I \\to \\mathbb{R}$,\nwe want to distinguish between the cases that $p=q$ and\n$\\|p-q\\|_{\\mathcal{A}_k} \\ge \\epsilon$ with probability at least $2/3$. We show\nthat for any $k \\ge 2, \\epsilon>0$, the {\\em optimal} sample complexity of the\n$\\mathcal{A}_k$-closeness testing problem is $\\Theta(\\max\\{\nk^{4/5}/\\epsilon^{6/5}, k^{1/2}/\\epsilon^2 \\})$. This is the first $o(k)$\nsample algorithm for this problem, and yields new, simple $L_1$ closeness\ntesters, in most cases with optimal sample complexity, for broad classes of\nstructured distributions.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 18:49:50 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Nikishkin", "Vladimir", ""]]}, {"id": "1508.05553", "submitter": "Xiaodong Wang", "authors": "Daxin Zhu, Lei Wang, Yingjie Wu and Xiaodong Wang", "title": "A Practical O(R\\log\\log n+n) time Algorithm for Computing the Longest\n  Common Subsequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revisit the much studied LCS problem for two given\nsequences. Based on the algorithm of Iliopoulos and Rahman for solving the LCS\nproblem, we have suggested 3 new improved algorithms. We first reformulate the\nproblem in a very succinct form. The problem LCS is abstracted to an abstract\ndata type DS on an ordered positive integer set with a special operation\nUpdate(S,x). For the two input sequences X and Y of equal length n, the first\nimproved algorithm uses a van Emde Boas tree for DS and its time and space\ncomplexities are O(R\\log\\log n+n) and O(R), where R is the number of matched\npairs of the two input sequences. The second algorithm uses a balanced binary\nsearch tree for DS and its time and space complexities are O(R\\log L+n) and\nO(R), where L is the length of the longest common subsequence of X and Y. The\nthird algorithm uses an ordered vector for DS and its time and space\ncomplexities are O(nL) and O(R).\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 00:56:03 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Zhu", "Daxin", ""], ["Wang", "Lei", ""], ["Wu", "Yingjie", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1508.05567", "submitter": "Benjamin Grimmer", "authors": "Benjamin Grimmer", "title": "Dual-Based Approximation Algorithms for Cut-Based Network Connectivity\n  Problems", "comments": "7/20/2017: Changed Title to be more accurate. Improved presentation\n  and clarity throughout the document (i.e. adding references and fixing typos)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variety of NP-Complete network connectivity problems. We\nintroduce a novel dual-based approach to approximating network design problems\nwith cut-based linear programming relaxations. This approach gives a\n$3/2$-approximation to Minimum 2-Edge-Connected Spanning Subgraph that is\nequivalent to a previously proposed algorithm. One well-studied branch of\nnetwork design models ad hoc networks where each node can either operate at\nhigh or low power. If we allow unidirectional links, we can formalize this into\nthe problem Dual Power Assignment (DPA). Our dual-based approach gives a\n$3/2$-approximation to DPA, improving the previous best approximation known of\n$11/7\\approx 1.57$.\n  Another standard network design problem is Minimum Strongly Connected\nSpanning Subgraph (MSCS). We propose a new problem generalizing MSCS and DPA\ncalled Star Strong Connectivity (SSC). Then we show that our dual-based\napproach achieves a 1.6-approximation ratio on SSC. As a consequence of our\ndual-based approximations, we prove new upper bounds on the integrality gaps of\nthese problems.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 04:00:20 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 18:57:42 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Grimmer", "Benjamin", ""]]}, {"id": "1508.05786", "submitter": "Rafal Kapelko", "authors": "Rafal Kapelko and Evangelos Kranakis", "title": "On the Displacement for Covering a $d-$dimensional Cube with Randomly\n  Placed Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider $n$ sensors placed randomly and independently with the uniform\ndistribution in a $d-$dimensional unit cube ($d\\ge 2$). The sensors have\nidentical sensing range equal to $r$, for some $r >0$. We are interested in\nmoving the sensors from their initial positions to new positions so as to\nensure that the $d-$dimensional unit cube is completely covered, i.e., every\npoint in the $d-$dimensional cube is within the range of a sensor. If the\n$i$-th sensor is displaced a distance $d_i$, what is a displacement of minimum\ncost? As cost measure for the displacement of the team of sensors we consider\nthe $a$-total movement defined as the sum $M_a:= \\sum_{i=1}^n d_i^a$, for some\nconstant $a>0$. We assume that $r$ and $n$ are chosen so as to allow full\ncoverage of the $d-$dimensional unit cube and $a > 0$.\n  The main contribution of the paper is to show the existence of a tradeoff\nbetween the $d-$dimensional cube, sensing radius and $a$-total movement. The\nmain results can be summarized as follows for the case of the $d-$dimensional\ncube.\n  If the $d-$dimensional cube sensing radius is $\\frac{1}{2n^{1/d}}$ and\n$n=m^d$, for some $m\\in N$, then we present an algorithm that uses\n$O\\left(n^{1-\\frac{a}{2d}}\\right)$ total expected movement (see Algorithm 2 and\nTheorem 5).\n  If the $d-$dimensional cube sensing radius is greater than\n$\\frac{3^{3/d}}{(3^{1/d}-1)(3^{1/d}-1)}\\frac{1}{2n^{1/d}}$ and $n$ is a natural\nnumber then the total expected movement is\n$O\\left(n^{1-\\frac{a}{2d}}\\left(\\frac{\\ln n}{n}\\right)^{\\frac{a}{2d}}\\right)$\n(see Algorithm 3 and Theorem 7).\n  In addition, we simulate Algorithm 2 and discuss the results of our\nsimulations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 12:49:51 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Kapelko", "Rafal", ""], ["Kranakis", "Evangelos", ""]]}, {"id": "1508.05968", "submitter": "Kaarthik Sundar", "authors": "Kaarthik Sundar, Saravanan Venkatachalam, Sivakumar Rathinam", "title": "Formulations and algorithms for the multiple depot, fuel-constrained,\n  multiple vehicle routing problem", "comments": "6 pages, 2 figures, submitted to American Control Conference 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multiple depot, multiple vehicle routing problem with fuel\nconstraints. We are given a set of targets, a set of depots and a set of\nhomogeneous vehicles, one for each depot. The depots are also allowed to act as\nrefueling stations. The vehicles are allowed to refuel at any depot, and our\nobjective is to determine a route for each vehicle with a minimum total cost\nsuch that each target is visited at least once by some vehicle, and the\nvehicles never run out fuel as it traverses its route. We refer this problem as\nMultiple Depot, Fuel-Constrained, Multiple Vehicle Routing Problem (FCMVRP).\nThis paper presents four new mixed integer linear programming formulations to\ncompute an optimal solution for the problem. Extensive computational results\nfor a large set of instances are also presented.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 20:48:33 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Sundar", "Kaarthik", ""], ["Venkatachalam", "Saravanan", ""], ["Rathinam", "Sivakumar", ""]]}, {"id": "1508.06019", "submitter": "Jesper Nederlof", "authors": "Per Austrin, Mikko Koivisto, Petteri Kaski, Jesper Nederlof", "title": "Dense Subset Sum may be the hardest", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Subset Sum problem asks whether a given set of $n$ positive integers\ncontains a subset of elements that sum up to a given target $t$. It is an\noutstanding open question whether the $O^*(2^{n/2})$-time algorithm for Subset\nSum by Horowitz and Sahni [J. ACM 1974] can be beaten in the worst-case setting\nby a \"truly faster\", $O^*(2^{(0.5-\\delta)n})$-time algorithm, with some\nconstant $\\delta > 0$. Continuing an earlier work [STACS 2015], we study Subset\nSum parameterized by the maximum bin size $\\beta$, defined as the largest\nnumber of subsets of the $n$ input integers that yield the same sum. For every\n$\\epsilon > 0$ we give a truly faster algorithm for instances with $\\beta \\leq\n2^{(0.5-\\epsilon)n}$, as well as instances with $\\beta \\geq 2^{0.661n}$.\nConsequently, we also obtain a characterization in terms of the popular density\nparameter $n/\\log_2 t$: if all instances of density at least $1.003$ admit a\ntruly faster algorithm, then so does every instance. This goes against the\ncurrent intuition that instances of density 1 are the hardest, and therefore is\na step toward answering the open question in the affirmative. Our results stem\nfrom novel combinations of earlier algorithms for Subset Sum and a study of an\nextremal question in additive combinatorics connected to the problem of\nUniquely Decodable Code Pairs in information theory.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 03:14:53 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Austrin", "Per", ""], ["Koivisto", "Mikko", ""], ["Kaski", "Petteri", ""], ["Nederlof", "Jesper", ""]]}, {"id": "1508.06182", "submitter": "Gili Rosenberg", "authors": "Gili Rosenberg, Poya Haghnegahdar, Phil Goddard, Peter Carr, Kesheng\n  Wu and Marcos L\\'opez de Prado", "title": "Solving the Optimal Trading Trajectory Problem Using a Quantum Annealer", "comments": "7 pages; expanded and updated", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing (JSTSP),\n  Volume 10, Issue 6, 2016, and Proc. of the 8th Workshop on High Performance\n  Computational Finance (WHPCF), p. 7, ACM, 2015", "doi": "10.1109/JSTSP.2016.2574703", "report-no": null, "categories": "q-fin.CP cs.DS math.OC q-fin.PM quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve a multi-period portfolio optimization problem using D-Wave Systems'\nquantum annealer. We derive a formulation of the problem, discuss several\npossible integer encoding schemes, and present numerical examples that show\nhigh success rates. The formulation incorporates transaction costs (including\npermanent and temporary market impact), and, significantly, the solution does\nnot require the inversion of a covariance matrix. The discrete multi-period\nportfolio optimization problem we solve is significantly harder than the\ncontinuous variable problem. We present insight into how results may be\nimproved using suitable software enhancements, and why current quantum\nannealing technology limits the size of problem that can be successfully solved\ntoday. The formulation presented is specifically designed to be scalable, with\nthe expectation that as quantum annealing technology improves, larger problems\nwill be solvable using the same techniques.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 18:52:49 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 21:09:04 GMT"}, {"version": "v3", "created": "Thu, 11 Aug 2016 15:26:21 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Rosenberg", "Gili", ""], ["Haghnegahdar", "Poya", ""], ["Goddard", "Phil", ""], ["Carr", "Peter", ""], ["Wu", "Kesheng", ""], ["de Prado", "Marcos L\u00f3pez", ""]]}, {"id": "1508.06216", "submitter": "Aviv Yehezkel", "authors": "Reuven Cohen, Liran Katzir and Aviv Yehezkel", "title": "Cardinality Estimation Meets Good-Turing", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardinality estimation algorithms receive a stream of elements whose order\nmight be arbitrary, with possible repetitions, and return the number of\ndistinct elements. Such algorithms usually seek to minimize the required\nstorage and processing at the price of inaccuracy in their output. Real-world\napplications of these algorithms are required to process large volumes of\nmonitored data, making it impractical to collect and analyze the entire input\nstream. In such cases, it is common practice to sample and process only a small\npart of the stream elements. This paper presents and analyzes a generic\nalgorithm for combining every cardinality estimation algorithm with a sampling\nprocess. We show that the proposed sampling algorithm does not affect the\nestimator's asymptotic unbiasedness, and we analyze the sampling effect on the\nestimator's variance.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 16:35:36 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Cohen", "Reuven", ""], ["Katzir", "Liran", ""], ["Yehezkel", "Aviv", ""]]}, {"id": "1508.06329", "submitter": "Agnieszka Lupinska", "authors": "Agnieszka Lupinska", "title": "A Parallel Algorithm to Test Chordality of Graphs", "comments": "MSc thesis, promoter: dr Maciej \\'Slusarek", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple parallel algorithm to test chordality of graphs which is\nbased on the parallel Lexicographical Breadth-First Search algorithm. In total,\nthe algorithm takes time O(N ) on N-threads machine and it performs work O(N 2\n) , where N is the number of vertices in a graph. Our implementation of the\nalgorithm uses a GPU environment Nvidia CUDA C. The algorithm is implemented in\nCUDA 4.2 and it has been tested on Nvidia GeForce GTX 560 Ti of compute\ncapability 2.1. At the end of the thesis we present the results achieved by our\nimplementation and compare them with the results achieved by the sequential\nalgorithm\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 23:51:04 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Lupinska", "Agnieszka", ""]]}, {"id": "1508.06420", "submitter": "Daniel Paulusma", "authors": "P\\'eter Bir\\'o and Walter Kern and Dani\\\"el Paulusma and P\\'eter\n  Wojuteczky", "title": "The Stable Fixtures Problem with Payments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize two well-known game-theoretic models by introducing multiple\npartners matching games, defined by a graph $G=(N,E)$, with an integer vertex\ncapacity function $b$ and an edge weighting $w$. The set $N$ consists of a\nnumber of players that are to form a set $M\\subseteq E$ of 2-player coalitions\n$ij$ with value $w(ij)$, such that each player $i$ is in at most $b(i)$\ncoalitions. A payoff vector is a mapping $p: N \\times N \\rightarrow {\\mathbb\nR}$ with $p(i,j)+p(j,i)=w(ij)$ if $ij\\in M$ and $p(i,j)=p(j,i)=0$ if $ij\\notin\nM$. The pair $(M,p)$ is called a solution. A pair of players $i,j$ with $ij\\in\nE\\setminus M$ blocks a solution $(M,p)$ if $i,j$ can form, possibly only after\nwithdrawing from one of their existing 2-player coalitions, a new 2-player\ncoalition in which they are mutually better off. A solution is stable if it has\nno blocking pairs.\n  We give a polynomial-time algorithm that either finds that a given multiple\npartners matching game has no stable solution, or obtains a stable solution for\nit. We characterize the set of stable solutions of a multiple partners matching\ngame in two different ways and show how this leads to simple proofs for a\nnumber of known results of Sotomayor (1992,1999,2007) for multiple partners\nssignment games and to generalizations of some of these results to multiple\npartners matching games. We also perform a study on the core of the\ncorresponding cooperative game, where coalitions of any size may be formed. In\nparticular we show that the standard relation between the existence of a stable\nsolution and the non-emptiness of the core, which holds in the other models\nwith payments, is no longer valid for our (most general) model. We also prove\nthat the problem of deciding if an allocation belongs to the core jumps from\nbeing polynomial-time solvable for $b\\leq 2$ to NP-complete for $b\\equiv 3$.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 09:25:17 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 16:37:44 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Bir\u00f3", "P\u00e9ter", ""], ["Kern", "Walter", ""], ["Paulusma", "Dani\u00ebl", ""], ["Wojuteczky", "P\u00e9ter", ""]]}, {"id": "1508.06460", "submitter": "Andrzej Pelc", "authors": "Kokouvi Hounkanli, Andrzej Pelc", "title": "Deterministic Broadcasting and Gossiping with Beeps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Broadcasting and gossiping are fundamental communication tasks in networks.\nIn broadcasting,one node of a network has a message that must be learned by all\nother nodes. In gossiping, every node has a (possibly different) message, and\nall messages must be learned by all nodes. We study these well-researched tasks\nin a very weak communication model, called the {\\em beeping model}.\nCommunication proceeds in synchronous rounds. In each round, a node can either\nlisten, i.e., stay silent, or beep, i.e., emit a signal. A node hears a beep in\na round, if it listens in this round and if one or more adjacent nodes beep in\nthis round. All nodes have different labels from the set $\\{0,\\dots , L-1\\}$.\n  Our aim is to provide fast deterministic algorithms for broadcasting and\ngossiping in the beeping model. Let $N$ be an upper bound on the size of the\nnetwork and $D$ its diameter. Let $m$ be the size of the message in\nbroadcasting, and $M$ an upper bound on the size of all input messages in\ngossiping. For the task of broadcasting we give an algorithm working in time\n$O(D+m)$ for arbitrary networks, which is optimal. For the task of gossiping we\ngive an algorithm working in time $O(N(M+D\\log L))$ for arbitrary networks.\n  At the time of writing this paper we were unaware of the paper: A. Czumaj, P.\nDavis, Communicating with Beeps, arxiv:1505.06107 [cs.DC] which contains the\nsame results for broadcasting and a stronger upper bound for gossiping in a\nslightly different model.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 12:10:38 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2015 18:56:56 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Hounkanli", "Kokouvi", ""], ["Pelc", "Andrzej", ""]]}, {"id": "1508.06610", "submitter": "Aleksander Cis{\\l}ak", "authors": "Aleksander Cis{\\l}ak", "title": "Full-text and Keyword Indexes for String Searching", "comments": "Master's thesis, 107 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a literature review for full-text and keyword\nindexes as well as our contributions (which are mostly practice-oriented).\n  The first contribution is the FM-bloated index, which is a modification of\nthe well-known FM-index (a compressed, full-text index) that trades space for\nspeed. In our approach, the count table and the occurrence lists store\ninformation about selected $q$-grams in addition to the individual characters.\nTwo variants are described, namely one using $O(n \\log^2 n)$ bits of space with\n$O(m + \\log m \\log \\log n)$ average query time, and one with linear space and\n$O(m \\log \\log n)$ average query time, where $n$ is the input text length and\n$m$ is the pattern length. We experimentally show that a significant speedup\ncan be achieved by operating on $q$-grams (albeit at the cost of very high\nspace requirements, hence the name \"bloated\").\n  In the category of keyword indexes we present the so-called split index,\nwhich can efficiently solve the $k$-mismatches problem, especially for 1 error.\nOur implementation in the C++ language is focused mostly on data compaction,\nwhich is beneficial for the search speed (by being cache friendly). We compare\nour solution with other algorithms and we show that it is faster when the\nHamming distance is used. Query times in the order of 1 microsecond were\nreported for one mismatch for a few-megabyte natural language dictionary on a\nmedium-end PC.\n  A minor contribution includes string sketches which aim to speed up\napproximate string comparison at the cost of additional space ($O(1)$ per\nstring). They can be used in the context of keyword indexes in order to deduce\nthat two strings differ by at least $k$ mismatches with the use of fast bitwise\noperations rather than an explicit verification.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 19:11:09 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Cis\u0142ak", "Aleksander", ""]]}, {"id": "1508.06802", "submitter": "Johannes Lengler", "authors": "Carola Doerr, Johannes Lengler", "title": "Introducing Elitist Black-Box Models: When Does Elitist Selection Weaken\n  the Performance of Evolutionary Algorithms?", "comments": "A short version of this work has been presented at the GECCO\n  conference 2015 in Madrid, Spain. Available at\n  http://dl.acm.org/citation.cfm?doid=2739480.2754654", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box complexity theory provides lower bounds for the runtime of\nblack-box optimizers like evolutionary algorithms and serves as an inspiration\nfor the design of new genetic algorithms. Several black-box models covering\ndifferent classes of algorithms exist, each highlighting a different aspect of\nthe algorithms under considerations. In this work we add to the existing\nblack-box notions a new \\emph{elitist black-box model}, in which algorithms are\nrequired to base all decisions solely on (a fixed number of) the best search\npoints sampled so far. Our model combines features of the ranking-based and the\nmemory-restricted black-box models with elitist selection.\n  We provide several examples for which the elitist black-box complexity is\nexponentially larger than that the respective complexities in all previous\nblack-box models, thus showing that the elitist black-box complexity can be\nmuch closer to the runtime of typical evolutionary algorithms.\n  We also introduce the concept of $p$-Monte Carlo black-box complexity, which\nmeasures the time it takes to optimize a problem with failure probability at\nmost $p$. Even for small~$p$, the $p$-Monte Carlo black-box complexity of a\nfunction class $\\mathcal F$ can be smaller by an exponential factor than its\ntypically regarded Las Vegas complexity (which measures the \\emph{expected}\ntime it takes to optimize $\\mathcal F$).\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 11:17:34 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Doerr", "Carola", ""], ["Lengler", "Johannes", ""]]}, {"id": "1508.06829", "submitter": "Gregory Gutin", "authors": "Gregory Gutin and Magnus Wahlstrom", "title": "Tight Lower Bounds for the Workflow Satisfiability Problem Based on the\n  Strong Exponential Time Hypothesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Workflow Satisfiability Problem (WSP) asks whether there exists an\nassignment of authorized users to the steps in a workflow specification,\nsubject to certain constraints on the assignment. The problem is NP-hard even\nwhen restricted to just not equals constraints. Since the number of steps $k$\nis relatively small in practice, Wang and Li (2010) introduced a\nparametrisation of WSP by $k$. Wang and Li (2010) showed that, in general, the\nWSP is W[1]-hard, i.e., it is unlikely that there exists a fixed-parameter\ntractable (FPT) algorithm for solving the WSP. Crampton et al. (2013) and Cohen\net al. (2014) designed FPT algorithms of running time $O^*(2^{k})$ and\n$O^*(2^{k\\log_2 k})$ for the WSP with so-called regular and user-independent\nconstraints, respectively. In this note, we show that there are no algorithms\nof running time $O^*(2^{ck})$ and $O^*(2^{ck\\log_2 k})$ for the two\nrestrictions of WSP, respectively, with any $c<1$, unless the Strong\nExponential Time Hypothesis fails.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 12:32:42 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Gutin", "Gregory", ""], ["Wahlstrom", "Magnus", ""]]}, {"id": "1508.07065", "submitter": "Hiroshi Hirai", "authors": "Hiroshi Hirai", "title": "A dual descent algorithm for node-capacitated multiflow problems and its\n  applications", "comments": "To appear in ACM Transactions on Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop an $O((m \\log k) {\\rm MSF} (n,m,1))$-time algorithm\nto find a half-integral node-capacitated multiflow of the maximum total\nflow-value in a network with $n$ nodes, $m$ edges, and $k$ terminals, where\n${\\rm MSF} (n',m',\\gamma)$ denotes the time complexity of solving the maximum\nsubmodular flow problem in a network with $n'$ nodes, $m'$ edges, and the\ncomplexity $\\gamma$ of computing the exchange capacity of the submodular\nfunction describing the problem. By using Fujishige-Zhang algorithm for\nsubmodular flow, we can find a maximum half-integral multiflow in $O(m n^3 \\log\nk)$ time. This is the first combinatorial strongly polynomial time algorithm\nfor this problem. Our algorithm is built on a developing theory of discrete\nconvex functions on certain graph structures. Applications include\n\"ellipsoid-free\" combinatorial implementations of a 2-approximation algorithm\nfor the minimum node-multiway cut problem by Garg, Vazirani, and Yannakakis.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 01:05:23 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 05:03:13 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 01:44:17 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Hirai", "Hiroshi", ""]]}, {"id": "1508.07230", "submitter": "Svante Janson", "authors": "Svante Janson", "title": "On the tails of the limiting Quicksort distribution", "comments": "8 pages. v2: Typos corrected and some formulations improved", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give asymptotics for the left and right tails of the limiting Quicksort\ndistribution. The results agree with, but are less precise than, earlier\nnon-rigorous results by Knessl and Spankowski.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 14:31:09 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2015 06:15:14 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Janson", "Svante", ""]]}, {"id": "1508.07338", "submitter": "Sevag Gharibian", "authors": "Niel de Beaudrap, Sevag Gharibian", "title": "A linear time algorithm for quantum 2-SAT", "comments": "21 pages", "journal-ref": "Proceedings of 31st Conference on Computational Complexity (CCC),\n  volume 50 of Leibniz International Proceedings in Informatics (LIPIcs), pages\n  27:1-27:21, 2016", "doi": "10.4230/LIPIcs.CCC.2016.27", "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Boolean constraint satisfaction problem 3-SAT is arguably the canonical\nNP-complete problem. In contrast, 2-SAT can not only be decided in polynomial\ntime, but in fact in deterministic linear time. In 2006, Bravyi proposed a\nphysically motivated generalization of k-SAT to the quantum setting, defining\nthe problem \"quantum k-SAT\". He showed that quantum 2-SAT is also solvable in\npolynomial time on a classical computer, in particular in deterministic time\nO(n^4), assuming unit-cost arithmetic over a field extension of the rational\nnumbers, where n is number of variables. In this paper, we present an algorithm\nfor quantum 2-SAT which runs in linear time, i.e. deterministic time O(n+m) for\nn and m the number of variables and clauses, respectively. Our approach\nexploits the transfer matrix techniques of Laumann et al. [QIC, 2010] used in\nthe study of phase transitions for random quantum 2-SAT, and bears similarities\nwith both the linear time 2-SAT algorithms of Even, Itai, and Shamir (based on\nbacktracking) [SICOMP, 1976] and Aspvall, Plass, and Tarjan (based on strongly\nconnected components) [IPL, 1979].\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 20:32:36 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["de Beaudrap", "Niel", ""], ["Gharibian", "Sevag", ""]]}, {"id": "1508.07372", "submitter": "Vijay Gadepally", "authors": "Vijay Gadepally, Jake Bolewski, Dan Hook, Dylan Hutchison, Ben Miller,\n  Jeremy Kepner", "title": "Graphulo: Linear Algebra Graph Kernels for NoSQL Databases", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/IPDPSW.2015.19", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data and the Internet of Things era continue to challenge computational\nsystems. Several technology solutions such as NoSQL databases have been\ndeveloped to deal with this challenge. In order to generate meaningful results\nfrom large datasets, analysts often use a graph representation which provides\nan intuitive way to work with the data. Graph vertices can represent users and\nevents, and edges can represent the relationship between vertices. Graph\nalgorithms are used to extract meaningful information from these very large\ngraphs. At MIT, the Graphulo initiative is an effort to perform graph\nalgorithms directly in NoSQL databases such as Apache Accumulo or SciDB, which\nhave an inherently sparse data storage scheme. Sparse matrix operations have a\nhistory of efficient implementations and the Graph Basic Linear Algebra\nSubprogram (GraphBLAS) community has developed a set of key kernels that can be\nused to develop efficient linear algebra operations. However, in order to use\nthe GraphBLAS kernels, it is important that common graph algorithms be recast\nusing the linear algebra building blocks. In this article, we look at common\nclasses of graph algorithms and recast them into linear algebra operations\nusing the GraphBLAS building blocks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 23:03:10 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 03:23:10 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Gadepally", "Vijay", ""], ["Bolewski", "Jake", ""], ["Hook", "Dan", ""], ["Hutchison", "Dylan", ""], ["Miller", "Ben", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1508.07380", "submitter": "Ananya Christman", "authors": "Hamza Alsarhan, Davin Chia, Ananya Christman, Shannia Fu, Tony Jin", "title": "Colored Bin Packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Colored Bin Packing Problem: we are given a set of items where\neach item has a weight and color. We must pack the items in bins of uniform\ncapacity such that no two items of the same color may be adjacent within in a\nbin. The goal is to perform this packing using the fewest number of bins. We\nconsider a version of the problem where reordering is allowed. We first\nconsider the zero-weight and unit weight versions of this problem, i.e. where\nthe items have weight zero and one, respectively. We present linear time\noptimal algorithms for both versions.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2015 00:08:46 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Alsarhan", "Hamza", ""], ["Chia", "Davin", ""], ["Christman", "Ananya", ""], ["Fu", "Shannia", ""], ["Jin", "Tony", ""]]}, {"id": "1508.07504", "submitter": "Joseph Cheriyan", "authors": "Joe Cheriyan, Zhihan Gao", "title": "Approximating (Unweighted) Tree Augmentation via Lift-and-Project, Part\n  I: Stemless TAP", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Part I, we study a special case of the unweighted Tree Augmentation\nProblem (TAP) via the Lasserre (Sum of Squares) system. In the special case, we\nforbid so-called stems; these are a particular type of subtree configuration.\nFor stemless TAP, we prove that the integrality ratio of an SDP relaxation (the\nLasserre tightening of an LP relaxation) is $\\leq \\frac{3}{2}+\\epsilon$, where\n$\\epsilon>0$ can be any small constant. We obtain this result by designing a\npolynomial-time algorithm for stemless TAP that achieves an approximation\nguarantee of ($\\frac32+\\epsilon$) relative to the SDP relaxation. The algorithm\nis combinatorial and does not solve the SDP relaxation, but our analysis relies\non the SDP relaxation.\n  We generalize the combinatorial analysis of integral solutions from the\nprevious literature to fractional solutions by identifying some properties of\nfractional solutions of the Lasserre system via the decomposition result of\nKarlin, Mathieu and Nguyen (IPCO 2011).\n  Also, we present an example of stemless TAP such that the approximation\nguarantee of $\\frac32$ is tight for the algorithm.\n  In Part II of this paper, we extend the methods of Part I to prove the same\nresults relative to the same SDP relaxation for TAP.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2015 20:48:09 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Cheriyan", "Joe", ""], ["Gao", "Zhihan", ""]]}, {"id": "1508.07557", "submitter": "Giordano Da Lozzo", "authors": "Patrizio Angelini, Giordano Da Lozzo, Giuseppe Di Battista, Fabrizio\n  Frati, Maurizio Patrignani, and Ignaz Rutter", "title": "Intersection-Link Representations of Graphs", "comments": "15 pages, 8 figures, extended version of 'Intersection-Link\n  Representations of Graphs' (23rd International Symposium on Graph Drawing,\n  2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider drawings of graphs that contain dense subgraphs. We introduce\nintersection-link representations for such graphs, in which each vertex $u$ is\nrepresented by a geometric object $R(u)$ and in which each edge $(u,v)$ is\nrepresented by the intersection between $R(u)$ and $R(v)$ if it belongs to a\ndense subgraph or by a curve connecting the boundaries of $R(u)$ and $R(v)$\notherwise. We study a notion of planarity, called Clique Planarity, for\nintersection-link representations of graphs in which the dense subgraphs are\ncliques.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 11:46:18 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Angelini", "Patrizio", ""], ["Da Lozzo", "Giordano", ""], ["Di Battista", "Giuseppe", ""], ["Frati", "Fabrizio", ""], ["Patrignani", "Maurizio", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1508.07771", "submitter": "Marek Adamczyk", "authors": "Marek Adamczyk", "title": "Non-negative submodular stochastic probing via stochastic contention\n  resolution schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abstract model of stochastic probing was presented by Gupta and Nagarajan\n(IPCO'13), and provides a unified view of a number of problems. Adamczyk,\nSviridenko, Ward (STACS'14) gave better approximation for matroid environments\nand linear objectives. At the same time this method was easily extendable to\nsettings, where the objective function was monotone submodular. However, the\ncase of non-negative submodular function could not be handled by previous\ntechniques. In this paper we address this problem, and our results are twofold.\nFirst, we adapt the notion of contention resolution schemes of Chekuri,\nVondr\\'ak, Zenklusen (SICOMP'14) to show that we can optimize non-negative\nsubmodular functions in this setting with a constant factor loss with respect\nto the deterministic setting. Second, we show a new contention resolution\nscheme for transversal matroids, which yields better approximations in the\nstochastic probing setting than the previously known tools. The rounding\nprocedure underlying the scheme can be of independent interest --- Bansal,\nGupta, Li, Mestre, Nagarajan, Rudra (Algorithmica'12) gave two seemingly\ndifferent algorithms for stochastic matching and stochastic $k$-set packing\nproblems with two different analyses, but we show that our single technique can\nbe used to analyze both their algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 12:05:17 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2015 13:16:13 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2015 02:31:21 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Adamczyk", "Marek", ""]]}, {"id": "1508.07820", "submitter": "Daniel Valenzuela", "authors": "Veli M\\\"akinen, Valeria Staneva, Alexandru Tomescu, Daniel Valenzuela", "title": "Interval scheduling maximizing minimum coverage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical interval scheduling type of problems, a set of $n$ jobs,\ncharacterized by their start and end time, need to be executed by a set of\nmachines, under various constraints. In this paper we study a new variant in\nwhich the jobs need to be assigned to at most $k$ identical machines, such that\nthe minimum number of machines that are busy at the same time is maximized.\nThis is relevant in the context of genome sequencing and haplotyping,\nspecifically when a set of DNA reads aligned to a genome needs to be pruned so\nthat no more than $k$ reads overlap, while maintaining as much read coverage as\npossible across the entire genome. We show that the problem can be solved in\ntime $\\min\\left(O(n^2\\log k / \\log n),O(nk\\log k)\\right)$ by using max-flows.\nWe also give an $O(n\\log n)$-time approximation algorithm with approximation\nratio $\\rho =\\frac{k}{\\lfloor k/2 \\rfloor}$.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 13:33:36 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 13:17:39 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["M\u00e4kinen", "Veli", ""], ["Staneva", "Valeria", ""], ["Tomescu", "Alexandru", ""], ["Valenzuela", "Daniel", ""]]}, {"id": "1508.07902", "submitter": "Alexander Shekhovtsov", "authors": "Alexander Shekhovtsov, Paul Swoboda, Bogdan Savchynskyy", "title": "Maximum Persistency via Iterative Relaxed Inference with Graphical\n  Models", "comments": "Reworked version, submitted to PAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the NP-hard problem of MAP-inference for undirected discrete\ngraphical models. We propose a polynomial time and practically efficient\nalgorithm for finding a part of its optimal solution. Specifically, our\nalgorithm marks some labels of the considered graphical model either as (i)\noptimal, meaning that they belong to all optimal solutions of the inference\nproblem; (ii) non-optimal if they provably do not belong to any solution. With\naccess to an exact solver of a linear programming relaxation to the\nMAP-inference problem, our algorithm marks the maximal possible (in a specified\nsense) number of labels. We also present a version of the algorithm, which has\naccess to a suboptimal dual solver only and still can ensure the\n(non-)optimality for the marked labels, although the overall number of the\nmarked labels may decrease. We propose an efficient implementation, which runs\nin time comparable to a single run of a suboptimal dual solver. Our method is\nwell-scalable and shows state-of-the-art results on computational benchmarks\nfrom machine learning and computer vision.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 16:28:55 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2015 23:30:53 GMT"}, {"version": "v3", "created": "Fri, 3 Feb 2017 13:21:01 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Shekhovtsov", "Alexander", ""], ["Swoboda", "Paul", ""], ["Savchynskyy", "Bogdan", ""]]}, {"id": "1508.07921", "submitter": "Fabrizio Frati", "authors": "Fabrizio Frati, Michael Hoffmann, Vincent Kusters", "title": "Simultaneous Embeddings with Few Bends and Crossings", "comments": "Full version of the paper \"Simultaneous Embeddings with Few Bends and\n  Crossings\" accepted at GD '15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simultaneous embedding with fixed edges (SEFE) of two planar graphs $R$ and\n$B$ is a pair of plane drawings of $R$ and $B$ that coincide when restricted to\nthe common vertices and edges of $R$ and $B$. We show that whenever $R$ and $B$\nadmit a SEFE, they also admit a SEFE in which every edge is a polygonal curve\nwith few bends and every pair of edges has few crossings. Specifically: (1) if\n$R$ and $B$ are trees then one bend per edge and four crossings per edge pair\nsuffice (and one bend per edge is sometimes necessary), (2) if $R$ is a planar\ngraph and $B$ is a tree then six bends per edge and eight crossings per edge\npair suffice, and (3) if $R$ and $B$ are planar graphs then six bends per edge\nand sixteen crossings per edge pair suffice. Our results improve on a paper by\nGrilli et al. (GD'14), which proves that nine bends per edge suffice, and on a\npaper by Chan et al. (GD'14), which proves that twenty-four crossings per edge\npair suffice.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 17:12:21 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Frati", "Fabrizio", ""], ["Hoffmann", "Michael", ""], ["Kusters", "Vincent", ""]]}]