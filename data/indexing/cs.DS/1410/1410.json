[{"id": "1410.0260", "submitter": "William March", "authors": "William B. March, Bo Xiao, George Biros", "title": "ASKIT: Approximate Skeletonization Kernel-Independent Treecode in High\n  Dimensions", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast algorithm for kernel summation problems in high-dimensions.\nThese problems appear in computational physics, numerical approximation,\nnon-parametric statistics, and machine learning. In our context, the sums\ndepend on a kernel function that is a pair potential defined on a dataset of\npoints in a high-dimensional Euclidean space. A direct evaluation of the sum\nscales quadratically with the number of points. Fast kernel summation methods\ncan reduce this cost to linear complexity, but the constants involved do not\nscale well with the dimensionality of the dataset.\n  The main algorithmic components of fast kernel summation algorithms are the\nseparation of the kernel sum between near and far field (which is the basis for\npruning) and the efficient and accurate approximation of the far field.\n  We introduce novel methods for pruning and approximating the far field. Our\nfar field approximation requires only kernel evaluations and does not use\nanalytic expansions. Pruning is not done using bounding boxes but rather\ncombinatorially using a sparsified nearest-neighbor graph of the input. The\ntime complexity of our algorithm depends linearly on the ambient dimension. The\nerror in the algorithm depends on the low-rank approximability of the far\nfield, which in turn depends on the kernel function and on the intrinsic\ndimensionality of the distribution of the points. The error of the far field\napproximation does not depend on the ambient dimension.\n  We present the new algorithm along with experimental results that demonstrate\nits performance. We report results for Gaussian kernel sums for 100 million\npoints in 64 dimensions, for one million points in 1000 dimensions, and for\nproblems in which the Gaussian kernel has a variable bandwidth. To the best of\nour knowledge, all of these experiments are impossible or prohibitively\nexpensive with existing fast kernel summation methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 15:41:11 GMT"}, {"version": "v2", "created": "Fri, 23 Jan 2015 22:38:05 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2015 17:31:21 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["March", "William B.", ""], ["Xiao", "Bo", ""], ["Biros", "George", ""]]}, {"id": "1410.0276", "submitter": "Julia Chuzhoy", "authors": "Julia Chuzhoy", "title": "Improved Bounds for the Flat Wall Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Flat Wall Theorem of Robertson and Seymour states that there is some\nfunction $f$, such that for all integers $w,t>1$, every graph $G$ containing a\nwall of size $f(w,t)$, must contain either (i) a $K_t$-minor; or (ii) a small\nsubset $A\\subset V(G)$ of vertices, and a flat wall of size $w$ in $G\\setminus\nA$. Kawarabayashi, Thomas and Wollan recently showed a self-contained proof of\nthis theorem with the following two sets of parameters: (1)\n$f(w,t)=\\Theta(t^{24}(t^2+w))$ with $|A|=O(t^{24})$, and (2)\n$f(w,t)=w^{2^{\\Theta(t^{24})}}$ with $|A|\\leq t-5$. The latter result gives the\nbest possible bound on $|A|$. In this paper we improve their bounds to\n$f(w,t)=\\Theta(t(t+w))$ with $|A|\\leq t-5$. For the special case where the\nmaximum vertex degree in $G$ is bounded by $D$, we show that, if $G$ contains a\nwall of size $\\Omega(Dt(t+w))$, then either $G$ contains a $K_t$-minor, or\nthere is a flat wall of size $w$ in $G$. This setting naturally arises in\nalgorithms for the Edge-Disjoint Paths problem, with $D\\leq 4$. Like the proof\nof Kawarabayashi et al., our proof is self-contained, except for using a\nwell-known theorem on routing pairs of disjoint paths. We also provide\nefficient algorithms that return either a model of the $K_t$-minor, or a vertex\nset $A$ and a flat wall of size $w$ in $G\\setminus A$.\n  We complement our result for the low-degree scenario by proving an almost\nmatching lower bound: namely, for all integers $w,t>1$, there is a graph $G$,\ncontaining a wall of size $\\Omega(wt)$, such that the maximum vertex degree in\n$G$ is 5, and $G$ contains no flat wall of size $w$, and no $K_t$-minor.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 16:29:21 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Chuzhoy", "Julia", ""]]}, {"id": "1410.0462", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Jesper Larsson Tr\\\"aff, Martin Wimmer", "title": "An improved, easily computable combinatorial lower bound for weighted\n  graph bipartitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has recently been much progress on exact algorithms for the\n(un)weighted graph (bi)partitioning problem using branch-and-bound and related\nmethods. In this note we present and improve an easily computable, purely\ncombinatorial lower bound for the weighted bipartitioning problem. The bound is\ncomputable in $O(n\\log n+m)$ time steps for weighted graphs with $n$ vertices\nand $m$ edges. In the branch-and-bound setting, the bound for each new\nsubproblem can be updated in $O(n+(m/n)\\log n)$ time steps amortized over a\nseries of $n$ branching steps; a rarely triggered tightening of the bound\nrequires search on the graph of unassigned vertices and can take from $O(n+m)$\nto $O(nm+n^2\\log n)$ steps depending on implementation and possible bound\nquality. Representing a subproblem uses $O(n)$ space. Although the bound is\nweak, we believe that it can be advantageous in a parallel setting to be able\nto generate many subproblems fast, possibly out-weighting the advantages of\ntighter, but much more expensive (algebraic, spectral, flow) lower bounds.\n  We use a recent priority task-scheduling framework for giving a parallel\nimplementation, and show the relative improvements in bound quality and\nsolution speed by the different contributions of the lower bound. A detailed\ncomparison with standardized input graphs to other lower bounds and frameworks\nis pending. Detailed investigations of branching and subproblem selection rules\nare likewise not the focus here, but various options are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 07:32:12 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Tr\u00e4ff", "Jesper Larsson", ""], ["Wimmer", "Martin", ""]]}, {"id": "1410.0553", "submitter": "Vincent Cohen-Addad", "authors": "Vincent Cohen-Addad and Claire Mathieu", "title": "The Unreasonable Success of Local Search: Geometric Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the effectiveness of local search algorithms for geometric problems\nin the plane? We prove that local search with neighborhoods of magnitude\n$1/\\epsilon^c$ is an approximation scheme for the following problems in the\nEuclidian plane: TSP with random inputs, Steiner tree with random inputs,\nfacility location (with worst case inputs), and bicriteria $k$-median (also\nwith worst case inputs). The randomness assumption is necessary for TSP.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 13:56:25 GMT"}, {"version": "v2", "created": "Thu, 16 Oct 2014 18:23:31 GMT"}, {"version": "v3", "created": "Thu, 15 Jan 2015 08:37:07 GMT"}, {"version": "v4", "created": "Wed, 9 Sep 2015 07:36:27 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Mathieu", "Claire", ""]]}, {"id": "1410.0562", "submitter": "Jacopo Pantaleoni", "authors": "Jacopo Pantaleoni", "title": "A massively parallel algorithm for constructing the BWT of large string\n  sets", "comments": null, "journal-ref": null, "doi": null, "report-no": "NVR-2014-002", "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new scalable, lightweight algorithm to incrementally construct\nthe BWT and FM-index of large string sets such as those produced by Next\nGeneration Sequencing. The algorithm is designed for massive parallelism and\ncan effectively exploit the combination of low capacity high bandwidth memory\nand slower external system memory typical of GPU accelerated systems.\nParticularly, for a string set of n characters from an alphabet with \\sigma\nsymbols, it uses a constant amount of high-bandwidth memory and at most 3n\nlog(\\sigma) bits of system memory. Given that deep memory hierarchies are\nbecoming a pervasive trait of high performance computing architectures, we\nbelieve this to be a relevant feature. The implementation can handle reads of\narbitrary length and is up to 2 and respectively 6.5 times faster than\nstate-of-the-art for short and long genomic reads\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 14:25:51 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Pantaleoni", "Jacopo", ""]]}, {"id": "1410.0571", "submitter": "Liudmila Ostroumova Prokhorenkova", "authors": "Konstantin Avrachenkov, Nelly Litvak, Liudmila Ostroumova\n  Prokhorenkova, Eugenia Suyargulova", "title": "Quick Detection of High-degree Entities in Large Directed Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of quick detection of high-degree\nentities in large online social networks. Practical importance of this problem\nis attested by a large number of companies that continuously collect and update\nstatistics about popular entities, usually using the degree of an entity as an\napproximation of its popularity. We suggest a simple, efficient, and easy to\nimplement two-stage randomized algorithm that provides highly accurate\nsolutions for this problem. For instance, our algorithm needs only one thousand\nAPI requests in order to find the top-100 most followed users in Twitter, a\nnetwork with approximately a billion of registered users, with more than 90%\nprecision. Our algorithm significantly outperforms existing methods and serves\nmany different purposes, such as finding the most popular users or the most\npopular interest groups in social networks. An important contribution of this\nwork is the analysis of the proposed algorithm using Extreme Value Theory -- a\nbranch of probability that studies extreme events and properties of largest\norder statistics in random samples. Using this theory, we derive an accurate\nprediction for the algorithm's performance and show that the number of API\nrequests for finding the top-k most popular entities is sublinear in the number\nof entities. Moreover, we formally show that the high variability among the\nentities, expressed through heavy-tailed distributions, is the reason for the\nalgorithm's efficiency. We quantify this phenomenon in a rigorous mathematical\nway.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 14:36:23 GMT"}, {"version": "v2", "created": "Thu, 23 Oct 2014 09:25:56 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Avrachenkov", "Konstantin", ""], ["Litvak", "Nelly", ""], ["Prokhorenkova", "Liudmila Ostroumova", ""], ["Suyargulova", "Eugenia", ""]]}, {"id": "1410.0589", "submitter": "Mateus de Oliveira Oliveira", "authors": "Mateus de Oliveira Oliveira", "title": "An Algorithmic Metatheorem for Directed Treewidth", "comments": "41 pages, 6 figures, Accepted to Discrete Applied Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of directed treewidth was introduced by Johnson, Robertson,\nSeymour and Thomas [Journal of Combinatorial Theory, Series B, Vol 82, 2001] as\na first step towards an algorithmic metatheory for digraphs. They showed that\nsome NP-complete properties such as Hamiltonicity can be decided in polynomial\ntime on digraphs of constant directed treewidth. Nevertheless, despite more\nthan one decade of intensive research, the list of hard combinatorial problems\nthat are known to be solvable in polynomial time when restricted to digraphs of\nconstant directed treewidth has remained scarce. In this work we enrich this\nlist by providing for the first time an algorithmic metatheorem connecting the\nmonadic second order logic of graphs to directed treewidth. We show that most\nof the known positive algorithmic results for digraphs of constant directed\ntreewidth can be reformulated in terms of our metatheorem. Additionally, we\nshow how to use our metatheorem to provide polynomial time algorithms for two\nclasses of combinatorial problems that have not yet been studied in the context\nof directed width measures. More precisely, for each fixed $k,w \\in\n\\mathbb{N}$, we show how to count in polynomial time on digraphs of directed\ntreewidth $w$, the number of minimum spanning strong subgraphs that are the\nunion of $k$ directed paths, and the number of maximal subgraphs that are the\nunion of $k$ directed paths and satisfy a given minor closed property. To prove\nour metatheorem we devise two technical tools which we believe to be of\nindependent interest. First, we introduce the notion of tree-zig-zag number of\na digraph, a new directed width measure that is at most a constant times\ndirected treewidth. Second, we introduce the notion of $z$-saturated tree slice\nlanguage, a new formalism for the specification and manipulation of infinite\nsets of digraphs.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 15:27:00 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2015 07:16:23 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Oliveira", "Mateus de Oliveira", ""]]}, {"id": "1410.0717", "submitter": "George Ovchinnikov", "authors": "I.V. Oseledets and G.V. Ovchinnikov", "title": "Fast, memory efficient low-rank approximation of SimRank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SimRank is a well-known similarity measure between graph vertices.\n  In this paper novel low-rank approximation of SimRank is proposed.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 21:31:12 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Oseledets", "I. V.", ""], ["Ovchinnikov", "G. V.", ""]]}, {"id": "1410.0768", "submitter": "Michael Elkin", "authors": "Michael Elkin, Ofer Neiman, Christian Wulff-Nilsen", "title": "Space-Efficient Path-Reporting Approximate Distance Oracles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider approximate {\\em path-reporting} distance oracles, distance\nlabeling and labeled routing with extremely low space requirement, for general\nundirected graphs. For distance oracles, we show how to break the n\\log n space\nbound of Thorup and Zwick if approximate {\\em paths} rather than distances need\nto be reported. For approximate distance labeling and labeled routing, we break\nthe previously best known space bound of O(log n) words per vertex. The cost\nfor such space efficiency is an increased stretch.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 07:42:38 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Elkin", "Michael", ""], ["Neiman", "Ofer", ""], ["Wulff-Nilsen", "Christian", ""]]}, {"id": "1410.0773", "submitter": "Moran Feldman", "authors": "Niv Buchbinder, Moran Feldman and Roy Schwartz", "title": "Comparing Apples and Oranges: Query Tradeoff in Submodular Maximization", "comments": "29 pages, accepted to SODA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast algorithms for submodular maximization problems have a vast potential\nuse in applicative settings, such as machine learning, social networks, and\neconomics. Though fast algorithms were known for some special cases, only\nrecently Badanidiyuru and Vondr\\'{a}k (2014) were the first to explicitly look\nfor such algorithms in the general case of maximizing a monotone submodular\nfunction subject to a matroid independence constraint. The algorithm of\nBadanidiyuru and Vondr\\'{a}k matches the best possible approximation guarantee,\nwhile trying to reduce the number of value oracle queries the algorithm\nperforms.\n  Our main result is a new algorithm for this general case which establishes a\nsurprising tradeoff between two seemingly unrelated quantities: the number of\nvalue oracle queries and the number of matroid independence queries performed\nby the algorithm. Specifically, one can decrease the former by increasing the\nlatter and vice versa, while maintaining the best possible approximation\nguarantee. Such a tradeoff is very useful since various applications might\nincur significantly different costs in querying the value and matroid\nindependence oracles. Furthermore, in case the rank of the matroid is $O(n^c)$,\nwhere $n$ is the size of the ground set and $c$ is an absolute constant smaller\nthan $1$, the total number of oracle queries our algorithm uses can be made to\nhave a smaller magnitude compared to that needed by Badanidiyuru and\nVondr\\'{a}k. We also provide even faster algorithms for the well studied\nspecial cases of a cardinality constraint and a partition matroid independence\nconstraint, both of which capture many real-world applications and have been\nwidely studied both theorically and in practice.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 08:21:39 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Buchbinder", "Niv", ""], ["Feldman", "Moran", ""], ["Schwartz", "Roy", ""]]}, {"id": "1410.0833", "submitter": "J\\\"urgen Koslowski", "authors": "Krishnendu Chatterjee, Monika Henzinger, Veronika Loitzenbauer", "title": "Improved Algorithms for Parity and Streett objectives", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 13, Issue 3 (September\n  26, 2017) lmcs:3953", "doi": "10.23638/LMCS-13(3:26)2017", "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of the winning set for parity objectives and for Streett\nobjectives in graphs as well as in game graphs are central problems in\ncomputer-aided verification, with application to the verification of closed\nsystems with strong fairness conditions, the verification of open systems,\nchecking interface compatibility, well-formedness of specifications, and the\nsynthesis of reactive systems. We show how to compute the winning set on $n$\nvertices for (1) parity-3 (aka one-pair Streett) objectives in game graphs in\ntime $O(n^{5/2})$ and for (2) k-pair Streett objectives in graphs in time\n$O(n^2 + nk \\log n)$. For both problems this gives faster algorithms for dense\ngraphs and represents the first improvement in asymptotic running time in 15\nyears.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 12:24:33 GMT"}, {"version": "v2", "created": "Tue, 7 Oct 2014 09:12:00 GMT"}, {"version": "v3", "created": "Mon, 10 Apr 2017 09:01:29 GMT"}, {"version": "v4", "created": "Fri, 22 Sep 2017 19:01:16 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Henzinger", "Monika", ""], ["Loitzenbauer", "Veronika", ""]]}, {"id": "1410.0855", "submitter": "Bart M. P. Jansen", "authors": "Bart M. P. Jansen and D\\'aniel Marx", "title": "Characterizing the easy-to-find subgraphs from the viewpoint of\n  polynomial-time algorithms, kernels, and Turing kernels", "comments": "69 pages, extended abstract to appear in the proceedings of SODA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two fundamental problems related to finding subgraphs: (1) given\ngraphs G and H, Subgraph Test asks if H is isomorphic to a subgraph of G, (2)\ngiven graphs G, H, and an integer t, Packing asks if G contains t\nvertex-disjoint subgraphs isomorphic to H. For every graph class F, let\nF-Subgraph Test and F-Packing be the special cases of the two problems where H\nis restricted to be in F. Our goal is to study which classes F make the two\nproblems tractable in one of the following senses:\n  * (randomized) polynomial-time solvable,\n  * admits a polynomial (many-one) kernel, or\n  * admits a polynomial Turing kernel (that is, has an adaptive polynomial-time\nprocedure that reduces the problem to a polynomial number of instances, each of\nwhich has size bounded polynomially by the size of the solution).\n  We identify a simple combinatorial property such that if a hereditary class F\nhas this property, then F-Packing admits a polynomial kernel, and has no\npolynomial (many-one) kernel otherwise, unless the polynomial hierarchy\ncollapses. Furthermore, if F does not have this property, then F-Packing is\neither WK[1]-hard, W[1]-hard, or Long Path-hard, giving evidence that it does\nnot admit polynomial Turing kernels either.\n  For F-Subgraph Test, we show that if every graph of a hereditary class F\nsatisfies the property that it is possible to delete a bounded number of\nvertices such that every remaining component has size at most two, then\nF-Subgraph Test is solvable in randomized polynomial time and it is NP-hard\notherwise. We introduce a combinatorial property called (a,b,c,d)-splittability\nand show that if every graph in a hereditary class F has this property, then\nF-Subgraph Test admits a polynomial Turing kernel and it is WK[1]-hard,\nW[1]-hard, or Long Path-hard, otherwise.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 14:04:19 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1410.1006", "submitter": "Fabrizio Frati", "authors": "Giuseppe Di Battista and Fabrizio Frati", "title": "A Survey on Small-Area Planar Graph Drawing", "comments": "Preliminary version appeared in \"Thirty Essays on Geometric Graph\n  Theory\", J. Pach (ed.), 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey algorithms and bounds for constructing planar drawings of graphs in\nsmall area.\n", "versions": [{"version": "v1", "created": "Sat, 4 Oct 2014 01:44:39 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Di Battista", "Giuseppe", ""], ["Frati", "Fabrizio", ""]]}, {"id": "1410.1016", "submitter": "Chandra Chekuri", "authors": "Chandra Chekuri and Julia Chuzhoy", "title": "Degree-3 Treewidth Sparsifiers", "comments": "Extended abstract to appear in Proceedings of ACM-SIAM SODA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study treewidth sparsifiers. Informally, given a graph $G$ of treewidth\n$k$, a treewidth sparsifier $H$ is a minor of $G$, whose treewidth is close to\n$k$, $|V(H)|$ is small, and the maximum vertex degree in $H$ is bounded.\nTreewidth sparsifiers of degree $3$ are of particular interest, as routing on\nnode-disjoint paths, and computing minors seems easier in sub-cubic graphs than\nin general graphs.\n  In this paper we describe an algorithm that, given a graph $G$ of treewidth\n$k$, computes a topological minor $H$ of $G$ such that (i) the treewidth of $H$\nis $\\Omega(k/\\text{polylog}(k))$; (ii) $|V(H)| = O(k^4)$; and (iii) the maximum\nvertex degree in $H$ is $3$. The running time of the algorithm is polynomial in\n$|V(G)|$ and $k$. Our result is in contrast to the known fact that unless $NP\n\\subseteq coNP/{\\sf poly}$, treewidth does not admit polynomial-size kernels.\nOne of our key technical tools, which is of independent interest, is a\nconstruction of a small minor that preserves node-disjoint routability between\ntwo pairs of vertex subsets. This is closely related to the open question of\ncomputing small good-quality vertex-cut sparsifiers that are also minors of the\noriginal graph.\n", "versions": [{"version": "v1", "created": "Sat, 4 Oct 2014 03:37:29 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Chekuri", "Chandra", ""], ["Chuzhoy", "Julia", ""]]}, {"id": "1410.1228", "submitter": "Jonathan Ullman", "authors": "Thomas Steinke and Jonathan Ullman", "title": "Interactive Fingerprinting Codes and the Hardness of Preventing False\n  Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show an essentially tight bound on the number of adaptively chosen\nstatistical queries that a computationally efficient algorithm can answer\naccurately given $n$ samples from an unknown distribution. A statistical query\nasks for the expectation of a predicate over the underlying distribution, and\nan answer to a statistical query is accurate if it is \"close\" to the correct\nexpectation over the distribution. This question was recently studied by Dwork\net al., who showed how to answer $\\tilde{\\Omega}(n^2)$ queries efficiently, and\nalso by Hardt and Ullman, who showed that answering $\\tilde{O}(n^3)$ queries is\nhard. We close the gap between the two bounds and show that, under a standard\nhardness assumption, there is no computationally efficient algorithm that,\ngiven $n$ samples from an unknown distribution, can give valid answers to\n$O(n^2)$ adaptively chosen statistical queries. An implication of our results\nis that computationally efficient algorithms for answering arbitrary,\nadaptively chosen statistical queries may as well be differentially private.\n  We obtain our results using a new connection between the problem of answering\nadaptively chosen statistical queries and a combinatorial object called an\ninteractive fingerprinting code. In order to optimize our hardness result, we\ngive a new Fourier-analytic approach to analyzing fingerprinting codes that is\nsimpler, more flexible, and yields better parameters than previous\nconstructions.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 23:55:22 GMT"}, {"version": "v2", "created": "Fri, 20 Feb 2015 19:29:47 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Steinke", "Thomas", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1410.1409", "submitter": "Pierre Le Bodic", "authors": "Karen Aardal and Pierre Le Bodic", "title": "Approximation algorithms for the Transportation Problem with Market\n  Choice and related models", "comments": null, "journal-ref": null, "doi": "10.1016/j.orl.2014.09.008", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given facilities with capacities and clients with penalties and demands, the\ntransportation problem with market choice consists in finding the minimum-cost\nway to partition the clients into unserved clients, paying the penalties, and\ninto served clients, paying the transportation cost to serve them. We give\npolynomial-time reductions from this problem and variants to the\n(un)capacitated facility location problem, directly yielding approximation\nalgorithms, two with constant factors in the metric case, one with a\nlogarithmic factor in the general case.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 20:57:10 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Aardal", "Karen", ""], ["Bodic", "Pierre Le", ""]]}, {"id": "1410.1594", "submitter": "Marco Winkler", "authors": "Marco Winkler and Joerg Reichardt", "title": "Node-Specific Triad Pattern Mining for Complex-Network Analysis", "comments": "Published in IEEE ICDMW 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mining of graphs in terms of their local substructure is a\nwell-established methodology to analyze networks. It was hypothesized that\nmotifs - subgraph patterns which appear significantly more often than expected\nat random - play a key role for the ability of a system to perform its task.\nYet the framework commonly used for motif-detection averages over the local\nenvironments of all nodes. Therefore, it remains unclear whether motifs are\noverrepresented in the whole system or only in certain regions.\n  In this contribution, we overcome this limitation by mining node-specific\ntriad patterns. For every vertex, the abundance of each triad pattern is\nconsidered only in triads it participates in. We investigate systems of various\nfields and find that motifs are distributed highly heterogeneously. In\nparticular we focus on the feed-forward loop motif which has been alleged to\nplay a key role in biological networks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Oct 2014 23:52:11 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 14:44:34 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Winkler", "Marco", ""], ["Reichardt", "Joerg", ""]]}, {"id": "1410.1653", "submitter": "S\\'andor Kisfaludi-Bak", "authors": "Zolt\\'an Kir\\'aly, S\\'andor Kisfaludi-Bak", "title": "Notes on dual-critical graphs", "comments": "10 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define dual-critical graphs as graphs having an acyclic orientation, where\nthe indegrees are odd except for the unique source. We have very limited\nknowledge about the complexity of dual-criticality testing. By the definition\nthe problem is in NP, and a result of Bal\\'azs and Christian Szegedy provides a\nrandomized polynomial algorithm, which relies on formal matrix rank computing.\nIt is unknown whether dual-criticality test can be done in deterministic\npolynomial time. Moreover, the question of being in co-NP is also open.\n  We give equivalent descriptions for dual-critical graphs in the general case,\nand further equivalent descriptions in the special cases of planar graphs and\n3-regular graphs. These descriptions provide polynomial algorithms for these\nspecial classes. We also give an FPT algorithm for a relaxed version of\ndual-criticality called $k$-dual-criticality.\n", "versions": [{"version": "v1", "created": "Tue, 7 Oct 2014 09:44:06 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Kir\u00e1ly", "Zolt\u00e1n", ""], ["Kisfaludi-Bak", "S\u00e1ndor", ""]]}, {"id": "1410.1703", "submitter": "Salman Fadaei", "authors": "Salman Fadaei and Martin Bichler", "title": "A Truthful Mechanism for the Generalized Assignment Problem", "comments": "18 pages, Earlier version accepted at WINE 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a truthful-in-expectation, $(1-1/e)$-approximation mechanism for a\nstrategic variant of the generalized assignment problem (GAP). In GAP, a set of\nitems has to be optimally assigned to a set of bins without exceeding the\ncapacity of any singular bin. In the strategic variant of the problem we study,\nvalues for assigning items to bins are the private information of bidders and\nthe mechanism should provide bidders with incentives to truthfully report their\nvalues. The approximation ratio of the mechanism is a significant improvement\nover the approximation ratio of the existing truthful mechanism for GAP.\n  The proposed mechanism comprises a novel convex optimization program as the\nallocation rule as well as an appropriate payment rule. To implement the convex\nprogram in polynomial time, we propose a fractional local search algorithm\nwhich approximates the optimal solution within an arbitrarily small error\nleading to an approximately truthful-in-expectation mechanism. The presented\nalgorithm improves upon the existing optimization algorithms for GAP in terms\nof simplicity and runtime while the approximation ratio closely matches the\nbest approximation ratio given for GAP when all inputs are publicly known.\n", "versions": [{"version": "v1", "created": "Tue, 7 Oct 2014 12:29:43 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 20:05:49 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Fadaei", "Salman", ""], ["Bichler", "Martin", ""]]}, {"id": "1410.1839", "submitter": "Syamantak Das", "authors": "Anamitra Roy Choudhury, Syamantak Das, Naveen Garg, Amit Kumar", "title": "Rejecting Jobs to Minimize Load and Maximum Flow-time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online algorithms are usually analyzed using the notion of competitive ratio\nwhich compares the solution obtained by the algorithm to that obtained by an\nonline adversary for the worst possible input sequence. Often this measure\nturns out to be too pessimistic, and one popular approach especially for\nscheduling problems has been that of \"resource augmentation\" which was first\nproposed by Kalyanasundaram and Pruhs. Although resource augmentation has been\nvery successful in dealing with a variety of objective functions, there are\nproblems for which even a (arbitrary) constant speedup cannot lead to a\nconstant competitive algorithm. In this paper we propose a \"rejection model\"\nwhich requires no resource augmentation but which permits the online algorithm\nto not serve an epsilon-fraction of the requests.\n  The problems considered in this paper are in the restricted assignment\nsetting where each job can be assigned only to a subset of machines. For the\nload balancing problem where the objective is to minimize the maximum load on\nany machine, we give $O(\\log^2 1/\\eps)$-competitive algorithm which rejects at\nmost an $\\eps$-fraction of the jobs. For the problem of minimizing the maximum\nweighted flow-time, we give an $O(1/\\eps^4)$-competitive algorithm which can\nreject at most an $\\eps$-fraction of the jobs by weight. We also extend this\nresult to a more general setting where the weights of a job for measuring its\nweighted flow-time and its contribution towards total allowed rejection weight\nare different. This is useful, for instance, when we consider the objective of\nminimizing the maximum stretch. We obtain an $O(1/\\eps^6)$-competitive\nalgorithm in this case.\n  Our algorithms are immediate dispatch, though they may not be immediate\nreject. All these problems have very strong lower bounds in the speed\naugmentation model.\n", "versions": [{"version": "v1", "created": "Tue, 7 Oct 2014 18:32:57 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Choudhury", "Anamitra Roy", ""], ["Das", "Syamantak", ""], ["Garg", "Naveen", ""], ["Kumar", "Amit", ""]]}, {"id": "1410.2209", "submitter": "Alexander Golovnev", "authors": "Alexander Golovnev, Alexander S. Kulikov and Ivan Mihajlin", "title": "Families with infants: speeding up algorithms for NP-hard problems using\n  FFT", "comments": "18 pages. arXiv admin note: substantial text overlap with\n  arXiv:1311.2456", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assume that a group of people is going to an excursion and our task is to\nseat them into buses with several constraints each saying that a pair of people\ndoes not want to see each other in the same bus. This is a well-known coloring\nproblem and it can be solved in $O^*(2^n)$ time by the inclusion-exclusion\nprinciple as shown by Bj\\\"{o}rklund, Husfeldt, and Koivisto in 2009.Another\napproach to solve this problem in $O^*(2^n)$ time is to use the fast Fourier\ntransform. A graph is $k$-colorable if and only if the $k$-th power of a\npolynomial containing a monomial $\\prod_{i=1}^n x_i^{[i \\in I]}$ for each\nindependent set $I \\subseteq [n]$ of the graph, contains the monomial\n$x_1x_2... x_n$.\n  Assume now that we have additional constraints: the group of people contains\nseveral infants and these infants should be accompanied by their relatives in a\nbus. We show that if the number of infants is linear then the problem can be\nsolved in $O^*((2-\\varepsilon)^n)$ time. We use this approach to improve known\nbounds for several NP-hard problems (the traveling salesman problem, the graph\ncoloring problem, the problem of counting perfect matchings) on graphs of\nbounded average degree, as well as to simplify the proofs of several known\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 18:27:38 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Golovnev", "Alexander", ""], ["Kulikov", "Alexander S.", ""], ["Mihajlin", "Ivan", ""]]}, {"id": "1410.2231", "submitter": "David Eppstein", "authors": "Brad Ballinger and Mirela Damian and David Eppstein and Robin Flatland\n  and Jessica Ginepro and Thomas Hull", "title": "Minimum Forcing Sets for Miura Folding Patterns", "comments": "20 pages, 16 figures. To appear at the ACM/SIAM Symp. on Discrete\n  Algorithms (SODA 2015)", "journal-ref": "ACM-SIAM Symposium on Discrete Algorithms (SODA15), (2015),\n  136-147", "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the study of forcing sets in mathematical origami. The origami\nmaterial folds flat along straight line segments called creases, each of which\nis assigned a folding direction of mountain or valley. A subset $F$ of creases\nis forcing if the global folding mountain/valley assignment can be deduced from\nits restriction to $F$. In this paper we focus on one particular class of\nfoldable patterns called Miura-ori, which divide the plane into congruent\nparallelograms using horizontal lines and zig-zag vertical lines. We develop\nefficient algorithms for constructing a minimum forcing set of a Miura-ori map,\nand for deciding whether a given set of creases is forcing or not. We also\nprovide tight bounds on the size of a forcing set, establishing that the\nstandard mountain-valley assignment for the Miura-ori is the one that requires\nthe most creases in its forcing sets. Additionally, given a partial\nmountain/valley assignment to a subset of creases of a Miura-ori map, we\ndetermine whether the assignment domain can be extended to a locally\nflat-foldable pattern on all the creases. At the heart of our results is a\nnovel correspondence between flat-foldable Miura-ori maps and $3$-colorings of\ngrid graphs.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 19:46:21 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Ballinger", "Brad", ""], ["Damian", "Mirela", ""], ["Eppstein", "David", ""], ["Flatland", "Robin", ""], ["Ginepro", "Jessica", ""], ["Hull", "Thomas", ""]]}, {"id": "1410.2266", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Vladimir Nikishkin", "title": "Testing Identity of Structured Distributions", "comments": "21 pages, to appear in SODA'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question of identity testing for structured distributions. More\nprecisely, given samples from a {\\em structured} distribution $q$ over $[n]$\nand an explicit distribution $p$ over $[n]$, we wish to distinguish whether\n$q=p$ versus $q$ is at least $\\epsilon$-far from $p$, in $L_1$ distance. In\nthis work, we present a unified approach that yields new, simple testers, with\nsample complexity that is information-theoretically optimal, for broad classes\nof structured distributions, including $t$-flat distributions, $t$-modal\ndistributions, log-concave distributions, monotone hazard rate (MHR)\ndistributions, and mixtures thereof.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 20:31:20 GMT"}], "update_date": "2014-10-10", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Nikishkin", "Vladimir", ""]]}, {"id": "1410.2470", "submitter": "Jalaj Upadhyay", "authors": "Jalaj Upadhyay", "title": "Randomness Efficient Fast-Johnson-Lindenstrauss Transform with\n  Applications in Differential Privacy and Compressed Sensing", "comments": "Corrected a mistake in the proof and few small typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Johnson-Lindenstrauss property ({\\sf JLP}) of random matrices has immense\napplication in computer science ranging from compressed sensing, learning\ntheory, numerical linear algebra, to privacy. This paper explores the\nproperties and applications of a distribution of random matrices. Our\ndistribution satisfies {\\sf JLP} with desirable properties like fast\nmatrix-vector multiplication, sparsity, and optimal subspace embedding. We can\nsample a random matrix from this distribution using exactly $2n+n \\log n$\nrandom bits. We show that a random matrix picked from this distribution\npreserves differential privacy under the condition that the input private\nmatrix satisfies certain spectral property. This improves the run-time of\nvarious differentially private mechanisms like Blocki {\\it et al.} (FOCS 2012)\nand Upadhyay (ASIACRYPT 13). Our final construction has a bounded column\nsparsity. Therefore, this answers an open problem stated in Blocki {\\it et al.}\n(FOCS 2012). Using the results of Baranuik {\\it et al.} (Constructive\nApproximation: 28(3)), our result implies a randomness efficient matrices that\nsatisfies the Restricted-Isometry Property of optimal order for small sparsity\nwith exactly linear random bits.\n  We also show that other known distributions of sparse random matrices with\nthe {\\sf JLP} does not preserves differential privacy; thereby, answering one\nof the open problem posed by Blocki {\\it et al.} (FOCS 2012). Extending on the\nworks of Kane and Nelson (JACM: 61(1)), we also give unified analysis of some\nof the known Johnson-Lindenstrauss transform. We also present a self-contained\nsimplified proof of an inequality on quadratic form of Gaussian variables that\nwe use in all our proofs.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 14:04:26 GMT"}, {"version": "v2", "created": "Tue, 21 Oct 2014 15:42:47 GMT"}, {"version": "v3", "created": "Wed, 22 Oct 2014 00:42:07 GMT"}, {"version": "v4", "created": "Wed, 5 Nov 2014 13:39:49 GMT"}, {"version": "v5", "created": "Thu, 2 Apr 2015 19:24:35 GMT"}, {"version": "v6", "created": "Thu, 16 Jul 2015 16:09:50 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Upadhyay", "Jalaj", ""]]}, {"id": "1410.2480", "submitter": "Rami Puzis", "authors": "Shlomi Dolev, Jonathan Goldfeld, Rami Puzis", "title": "Efficient On-line Detection of Temporal Patterns", "comments": "withdrawn due to submission policy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying a temporal pattern of events is a fundamental task of on-line\n(real-time) verification. We present efficient schemes for on-line monitoring\nof events for identifying desired/undesired patterns of events. The schemes use\npreprocessing to ensure that the number of comparisons during run-time is\nminimized. In particular, the first comparison following the time point when an\nexecution sub-sequence cannot be further extended to satisfy the temporal\nrequirements, halts the process that monitors the sub-sequence.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 14:19:52 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 18:07:49 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Dolev", "Shlomi", ""], ["Goldfeld", "Jonathan", ""], ["Puzis", "Rami", ""]]}, {"id": "1410.2595", "submitter": "Piyush Srivastava", "authors": "Alistair Sinclair, Piyush Srivastava, Daniel \\v{S}tefankovi\\v{c},\n  Yitong Yin", "title": "Spatial mixing and the connective constant: Optimal bounds", "comments": "This paper supersedes arxiv:1308.1762, in which weaker versions of\n  some of the results in this paper appeared. The current paper strengthens the\n  main result of 1308.1762 (Theorem 1.3) to obtain an optimal setting of the\n  parameters, and also adds new results for the monomer-dimer model", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of deterministic approximate counting of matchings and\nindependent sets in graphs of bounded connective constant. More generally, we\nconsider the problem of evaluating the partition functions of the monomer-dimer\nmodel (which is defined as a weighted sum over all matchings where each\nmatching is given a weight $\\gamma^{|V| - 2 |M|}$ in terms of a fixed parameter\ngamma called the monomer activity) and the hard core model (which is defined as\na weighted sum over all independent sets where an independent set I is given a\nweight $\\lambda^{|I|}$ in terms of a fixed parameter lambda called the vertex\nactivity). The connective constant is a natural measure of the average degree\nof a graph which has been studied extensively in combinatorics and mathematical\nphysics, and can be bounded by a constant even for certain unbounded degree\ngraphs such as those sampled from the sparse Erd\\H{o}s-R\\'enyi model $G(n,\nd/n)$.\n  Our main technical contribution is to prove the best possible rates of decay\nof correlations in the natural probability distributions induced by both the\nhard core model and the monomer-dimer model in graphs with a given bound on the\nconnective constant. These results on decay of correlations are obtained using\na new framework based on the so-called message approach that has been\nextensively used recently to prove such results for bounded degree graphs. We\nthen use these optimal decay of correlations results to obtain FPTASs for the\ntwo problems on graphs of bounded connective constant.\n  Our techniques also allow us to improve upon known bounds for decay of\ncorrelations for the hard core model on various regular lattices, including\nthose obtained by Restrepo, Shin, Vigoda and Tetali (2011) for the special case\nof Z^2 using sophisticated numerically intensive methods tailored to that\nspecial case.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 04:14:36 GMT"}], "update_date": "2014-10-10", "authors_parsed": [["Sinclair", "Alistair", ""], ["Srivastava", "Piyush", ""], ["\u0160tefankovi\u010d", "Daniel", ""], ["Yin", "Yitong", ""]]}, {"id": "1410.2628", "submitter": "Andrew King", "authors": "Andrew D. King and Catherine C. McGeoch", "title": "Algorithm engineering for a quantum annealing platform", "comments": "16 pages. V2: minor edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.ET quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances bring within reach the viability of solving combinatorial\nproblems using a quantum annealing algorithm implemented on a purpose-built\nplatform that exploits quantum properties. However, the question of how to tune\nthe algorithm for most effective use in this framework is not well understood.\nIn this paper we describe some operational parameters that drive performance,\ndiscuss approaches for mitigating sources of error, and present experimental\nresults from a D-Wave Two quantum annealing processor.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 21:09:09 GMT"}, {"version": "v2", "created": "Sat, 18 Oct 2014 00:28:13 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["King", "Andrew D.", ""], ["McGeoch", "Catherine C.", ""]]}, {"id": "1410.2640", "submitter": "Eric Price", "authors": "Eric Price", "title": "Optimal Lower Bound for Itemset Frequency Indicator Sketches", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Given a database, a common problem is to find the pairs or $k$-tuples of\nitems that frequently co-occur. One specific problem is to create a small space\n\"sketch\" of the data that records which $k$-tuples appear in more than an\n$\\epsilon$ fraction of rows of the database.\n  We improve the lower bound of Liberty, Mitzenmacher, and Thaler [LMT14],\nshowing that $\\Omega(\\frac{1}{\\epsilon}d \\log (\\epsilon d))$ bits are necessary\neven in the case of $k=2$. This matches the sampling upper bound for all\n$\\epsilon \\geq 1/d^{.99}$, and (in the case of $k=2$) another trivial upper\nbound for $\\epsilon = 1/d$.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 22:07:01 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Price", "Eric", ""]]}, {"id": "1410.2645", "submitter": "Anudhyan Boral", "authors": "Anudhyan Boral, Michael Mitzenmacher", "title": "Multi-Party Set Reconciliation Using Characteristic Polynomials", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the standard set reconciliation problem, there are two parties $A_1$ and\n$A_2$, each respectively holding a set of elements $S_1$ and $S_2$. The goal is\nfor both parties to obtain the union $S_1 \\cup S_2$. In many distributed\ncomputing settings the sets may be large but the set difference\n$|S_1-S_2|+|S_2-S_1|$ is small. In these cases one aims to achieve\nreconciliation efficiently in terms of communication; ideally, the\ncommunication should depend on the size of the set difference, and not on the\nsize of the sets.\n  Recent work has considered generalizations of the reconciliation problem to\nmulti-party settings, using a framework based on a specific type of linear\nsketch called an Invertible Bloom Lookup Table. Here, we consider multi-party\nset reconciliation using the alternative framework of characteristic\npolynomials, which have previously been used for efficient pairwise set\nreconciliation protocols, and compare their performance with Invertible Bloom\nLookup Tables for these problems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 22:45:50 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Boral", "Anudhyan", ""], ["Mitzenmacher", "Michael", ""]]}, {"id": "1410.2721", "submitter": "Jean-Philippe Hamiez", "authors": "Jean-Philippe Hamiez and Jin-Kao Hao", "title": "A note on a sports league scheduling problem", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sports league scheduling is a difficult task in the general case. In this\nshort note, we report two improvements to an existing enumerative search\nalgorithm for a NP-hard sports league scheduling problem known as \"prob026\" in\nCSPLib. These improvements are based on additional rules to constraint and\naccelerate the enumeration process. The proposed approach is able to find a\nsolution (schedule) for all prob026 instances for a number T of teams ranging\nfrom 12 to 70, including several T values for which a solution is reported for\nthe first time.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 09:50:26 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Hamiez", "Jean-Philippe", ""], ["Hao", "Jin-Kao", ""]]}, {"id": "1410.2736", "submitter": "Mike M\\\"uller", "authors": "Thorsten Ehlers and Mike M\\\"uller", "title": "Faster Sorting Networks for $17$, $19$ and $20$ Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new parallel sorting networks for $17$ to $20$ inputs. For $17,\n19,$ and $20$ inputs these new networks are faster (i.e., they require less\ncomputation steps) than the previously known best networks. Therefore, we\nimprove upon the known upper bounds for minimal depth sorting networks on $17,\n19,$ and $20$ channels. The networks were obtained using a combination of\nhand-crafted first layers and a SAT encoding of sorting networks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 10:55:38 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Ehlers", "Thorsten", ""], ["M\u00fcller", "Mike", ""]]}, {"id": "1410.2803", "submitter": "Ali Shoker", "authors": "Paulo S\\'ergio Almeida, Ali Shoker, and Carlos Baquero", "title": "Efficient State-based CRDTs by Delta-Mutation", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CRDTs are distributed data types that make eventual consistency of a\ndistributed object possible and non ad-hoc. Specifically, state-based CRDTs\nensure convergence through disseminating the en- tire state, that may be large,\nand merging it to other replicas; whereas operation-based CRDTs disseminate\noperations (i.e., small states) assuming an exactly-once reliable dissemination\nlayer. We introduce Delta State Conflict-Free Replicated Datatypes\n({\\delta}-CRDT) that can achieve the best of both worlds: small messages with\nan incremental nature, as in operation-based CRDTs, disseminated over\nunreliable communication channels, as in traditional state-based CRDTs. This is\nachieved by defining {\\delta}-mutators to return a delta-state, typically with\na much smaller size than the full state, that is joined to both: local and\nremote states. We introduce the {\\delta}-CRDT framework, and we explain it\nthrough establishing a correspondence to current state-based CRDTs. In\naddition, we present an anti-entropy algorithm that ensures causal consistency,\nand we introduce two {\\delta}-CRDT specifications of well-known replicated\ndatatypes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 15:16:23 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 12:32:20 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Almeida", "Paulo S\u00e9rgio", ""], ["Shoker", "Ali", ""], ["Baquero", "Carlos", ""]]}, {"id": "1410.2847", "submitter": "Patrick Nicholson", "authors": "Pawel Gawrychowski and Patrick K. Nicholson", "title": "Encodings of Range Maximum-Sum Segment Queries and Applications", "comments": "19 pages + 2 page appendix, 4 figures. A shortened version of this\n  paper will appear in CPM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an array A containing arbitrary (positive and negative) numbers, we\nconsider the problem of supporting range maximum-sum segment queries on A:\ni.e., given an arbitrary range [i,j], return the subrange [i' ,j' ] \\subseteq\n[i,j] such that the sum of the numbers in A[i'..j'] is maximized. Chen and Chao\n[Disc. App. Math. 2007] presented a data structure for this problem that\noccupies {\\Theta}(n) words, can be constructed in {\\Theta}(n) time, and\nsupports queries in {\\Theta}(1) time. Our first result is that if only the\nindices [i',j'] are desired (rather than the maximum sum achieved in that\nsubrange), then it is possible to reduce the space to {\\Theta}(n) bits,\nregardless the numbers stored in A, while retaining the same construction and\nquery time. We also improve the best known space lower bound for any data\nstructure that supports range maximum-sum segment queries from n bits to\n1.89113n - {\\Theta}(lg n) bits, for sufficiently large values of n. Finally, we\nprovide a new application of this data structure which simplifies a previously\nknown linear time algorithm for finding k-covers: i.e., given an array A of n\nnumbers and a number k, find k disjoint subranges [i_1 ,j_1 ],...,[i_k ,j_k ],\nsuch that the total sum of all the numbers in the subranges is maximized.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 17:36:52 GMT"}, {"version": "v2", "created": "Mon, 24 Nov 2014 19:39:04 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2015 08:55:46 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Gawrychowski", "Pawel", ""], ["Nicholson", "Patrick K.", ""]]}, {"id": "1410.3247", "submitter": "Tomasz Krawczyk", "authors": "Bart{\\l}omiej Bosek, Hal A. Kierstead, Tomasz Krawczyk, Grzegorz\n  Matecki, Matthew E. Smith", "title": "An easy subexponential bound for online chain partitioning", "comments": "23 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bosek and Krawczyk exhibited an online algorithm for partitioning an online\nposet of width $w$ into $w^{14\\lg w}$ chains. We improve this to $w^{6.5 \\lg w\n+ 7}$ with a simpler and shorter proof by combining the work of Bosek &\nKrawczyk with work of Kierstead & Smith on First-Fit chain partitioning of\nladder-free posets. We also provide examples illustrating the limits of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 10:13:43 GMT"}, {"version": "v2", "created": "Sun, 15 Apr 2018 13:13:10 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Bosek", "Bart\u0142omiej", ""], ["Kierstead", "Hal A.", ""], ["Krawczyk", "Tomasz", ""], ["Matecki", "Grzegorz", ""], ["Smith", "Matthew E.", ""]]}, {"id": "1410.3386", "submitter": "Jayadev Acharya", "authors": "Jayadev Acharya and Constantinos Daskalakis", "title": "Testing Poisson Binomial Distributions", "comments": "To appear in ACM-SIAM Symposium on Discrete Algorithms (SODA) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Poisson Binomial distribution over $n$ variables is the distribution of the\nsum of $n$ independent Bernoullis. We provide a sample near-optimal algorithm\nfor testing whether a distribution $P$ supported on $\\{0,...,n\\}$ to which we\nhave sample access is a Poisson Binomial distribution, or far from all Poisson\nBinomial distributions. The sample complexity of our algorithm is $O(n^{1/4})$\nto which we provide a matching lower bound. We note that our sample complexity\nimproves quadratically upon that of the naive \"learn followed by tolerant-test\"\napproach, while instance optimal identity testing [VV14] is not applicable\nsince we are looking to simultaneously test against a whole family of\ndistributions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 16:36:10 GMT"}, {"version": "v2", "created": "Tue, 14 Oct 2014 00:27:21 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Acharya", "Jayadev", ""], ["Daskalakis", "Constantinos", ""]]}, {"id": "1410.3408", "submitter": "Fatemeh Rajabi-Alni", "authors": "Fatemeh Rajabi-Alni, Alireza Bagheri, Behrouz Minaei-Bidgoli", "title": "An O(n^3) time algorithm for the maximum weight b-matching problem on\n  bipartite graphs", "comments": "12 pages, 2 figures, submitted. arXiv admin note: text overlap with\n  arXiv:1303.4031", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected bipartite graph G= (A U B,E), the b-matching of G matches\neach vertex v in A (B) to at least 1 and at most b(v) vertices in B (A), where\nb(v) denotes the capacity of v. In this paper, we present an O(n^3) time\nalgorithm for finding a maximum weight b-matching of G, where |A|+|B|=O(n). Our\nalgorithm improves the previous best time complexity of O(n^3 log n) for this\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 17:44:37 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2019 06:09:31 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Rajabi-Alni", "Fatemeh", ""], ["Bagheri", "Alireza", ""], ["Minaei-Bidgoli", "Behrouz", ""]]}, {"id": "1410.3438", "submitter": "Alberto Ord\\'o\\~nez Pereira", "authors": "Travis Gagie, Gonzalo Navarro, Yakov Nekrich, Alberto Ord\\'o\\~nez", "title": "Efficient and Compact Representations of Prefix Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the attention in statistical compression is given to the space used\nby the compressed sequence, a problem completely solved with optimal prefix\ncodes. However, in many applications, the storage space used to represent the\nprefix code itself can be an issue. In this paper we introduce and compare\nseveral techniques to store prefix codes. Let $N$ be the sequence length and\n$n$ be the alphabet size. Then a naive storage of an optimal prefix code uses\n$O(n\\log n)$ bits. Our first technique shows how to use $O(n\\log\\log(N/n))$\nbits to store the optimal prefix code. Then we introduce an approximate\ntechnique that, for any $0<\\epsilon<1/2$, takes $O(n \\log \\log (1 / \\epsilon))$\nbits to store a prefix code with average codeword length within an additive\n$\\epsilon$ of the minimum. Finally, a second approximation takes, for any\nconstant $c > 1$, $O(n^{1 / c} \\log n)$ bits to store a prefix code with\naverage codeword length at most $c$ times the minimum. In all cases, our data\nstructures allow encoding and decoding of any symbol in $O(1)$ time. We\nexperimentally compare our new techniques with the state of the art, showing\nthat we achieve 6--8-fold space reductions, at the price of a slower encoding\n(2.5--8 times slower) and decoding (12--24 times slower). The approximations\nfurther reduce this space and improve the time significantly, up to recovering\nthe speed of classical implementations, for a moderate penalty in the average\ncode length. As a byproduct, we compare various heuristic, approximate, and\noptimal algorithms to generate length-restricted codes, showing that the\noptimal ones are clearly superior and practical enough to be implemented.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 19:02:40 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2015 13:25:45 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Gagie", "Travis", ""], ["Navarro", "Gonzalo", ""], ["Nekrich", "Yakov", ""], ["Ord\u00f3\u00f1ez", "Alberto", ""]]}, {"id": "1410.3764", "submitter": "Grzegorz Matecki", "authors": "Jakub Kozik and Grzegorz Matecki", "title": "A lazy approach to on-line bipartite matching", "comments": null, "journal-ref": "The Electronic Journal of Combinatorics, Volume 25(2), Paper\n  P2.24, 2018", "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach, called a lazy matching, to the problem of on-line\nmatching on bipartite graphs. Imagine that one side of a graph is given and the\nvertices of the other side are arriving on-line. Originally, incoming vertex is\neither irrevocably matched to an another element or stays forever unmatched. A\nlazy algorithm is allowed to match a new vertex to a group of elements\n(possibly empty) and afterwords, forced against next vertices, may give up\nparts of the group. The restriction is that all the time each element is in at\nmost one group. We present an optimal lazy algorithm (deterministic) and prove\nthat its competitive ratio equals $1-\\pi/\\cosh(\\frac{\\sqrt{3}}{2}\\pi)\\approx\n0.588$. The lazy approach allows us to break the barrier of $1/2$, which is the\nbest competitive ratio that can be guaranteed by any deterministic algorithm in\nthe classical on-line matching.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 16:56:58 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 09:19:23 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Kozik", "Jakub", ""], ["Matecki", "Grzegorz", ""]]}, {"id": "1410.3886", "submitter": "Srinadh Bhojanapalli", "authors": "Srinadh Bhojanapalli, Prateek Jain, Sujay Sanghavi", "title": "Tighter Low-rank Approximation via Sampling the Leveraged Element", "comments": "36 pages, 3 figures, Extended abstract to appear in the proceedings\n  of ACM-SIAM Symposium on Discrete Algorithms (SODA15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new randomized algorithm for computing a low-rank\napproximation to a given matrix. Taking an approach different from existing\nliterature, our method first involves a specific biased sampling, with an\nelement being chosen based on the leverage scores of its row and column, and\nthen involves weighted alternating minimization over the factored form of the\nintended low-rank matrix, to minimize error only on these samples. Our method\ncan leverage input sparsity, yet produce approximations in {\\em spectral} (as\nopposed to the weaker Frobenius) norm; this combines the best aspects of\notherwise disparate current results, but with a dependence on the condition\nnumber $\\kappa = \\sigma_1/\\sigma_r$. In particular we require $O(nnz(M) +\n\\frac{n\\kappa^2 r^5}{\\epsilon^2})$ computations to generate a rank-$r$\napproximation to $M$ in spectral norm. In contrast, the best existing method\nrequires $O(nnz(M)+ \\frac{nr^2}{\\epsilon^4})$ time to compute an approximation\nin Frobenius norm. Besides the tightness in spectral norm, we have a better\ndependence on the error $\\epsilon$. Our method is naturally and highly\nparallelizable.\n  Our new approach enables two extensions that are interesting on their own.\nThe first is a new method to directly compute a low-rank approximation (in\nefficient factored form) to the product of two given matrices; it computes a\nsmall random set of entries of the product, and then executes weighted\nalternating minimization (as before) on these. The sampling strategy is\ndifferent because now we cannot access leverage scores of the product matrix\n(but instead have to work with input matrices). The second extension is an\nimproved algorithm with smaller communication complexity for the distributed\nPCA setting (where each server has small set of rows of the matrix, and want to\ncompute low rank approximation with small amount of communication with other\nservers).\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 22:41:20 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Bhojanapalli", "Srinadh", ""], ["Jain", "Prateek", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1410.3889", "submitter": "Tal Wagner", "authors": "Robert Krauthgamer and Tal Wagner", "title": "Cheeger-type approximation for sparsest $st$-cut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the $st$-cut version the Sparsest-Cut problem, where the goal is\nto find a cut of minimum sparsity among those separating two distinguished\nvertices $s,t\\in V$. Clearly, this problem is at least as hard as the usual\n(non-$st$) version. Our main result is a polynomial-time algorithm for the\nproduct-demands setting, that produces a cut of sparsity $O(\\sqrt{\\OPT})$,\nwhere $\\OPT$ denotes the optimum, and the total edge capacity and the total\ndemand are assumed (by normalization) to be $1$.\n  Our result generalizes the recent work of Trevisan [arXiv, 2013] for the\nnon-$st$ version of the same problem (Sparsest-Cut with product demands), which\nin turn generalizes the bound achieved by the discrete Cheeger inequality, a\ncornerstone of Spectral Graph Theory that has numerous applications. Indeed,\nCheeger's inequality handles graph conductance, the special case of product\ndemands that are proportional to the vertex (capacitated) degrees. Along the\nway, we obtain an $O(\\log n)$-approximation, where $n=\\card{V}$, for the\ngeneral-demands setting of Sparsest $st$-Cut.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 23:05:57 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 19:10:28 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Krauthgamer", "Robert", ""], ["Wagner", "Tal", ""]]}, {"id": "1410.4161", "submitter": "Donglei Du", "authors": "Chenchen Wu, Dachuan Xu, Donglei Du, Yishui Wang", "title": "An improved approximation algorithm for k-median problem using a new\n  factor-revealing LP", "comments": "This paper has been withdrawn by the authors due to a critical error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-median problem is a well-known strongly NP-hard combinatorial\noptimization problem of both theoretical and practical significance. The\nprevious best approximation ratio for this problem is 2.611+\\epsilon (Bryka et\nal. 2014) based on an (1, 1.95238219) bi-factor approximation algorithm for the\nclassical facility location problem (FLP). This work offers an improved\nalgorithm with an approximation ratio 2.592 +\\epsilon based on a new (1,\n1.93910094) bi-factor approximation algorithm for the FLP.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 18:41:14 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 14:30:01 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Wu", "Chenchen", ""], ["Xu", "Dachuan", ""], ["Du", "Donglei", ""], ["Wang", "Yishui", ""]]}, {"id": "1410.4209", "submitter": "Leah Epstein", "authors": "Ron Adar and Leah Epstein", "title": "Models for the k-metric dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an undirected graph G=(V,E), a vertex x \\in V separates vertices u and v\n(where u,v \\in V, u \\neq v) if their distances to x are not equal. Given an\ninteger parameter k \\geq 1, a set of vertices L \\subseteq V is a feasible\nsolution if for every pair of distinct vertices, u,v, there are at least k\ndistinct vertices x_1,x_2,...,x_k \\in L each separating u and v. Such a\nfeasible solution is called a \"landmark set\", and the k-metric dimension of a\ngraph is the minimal cardinality of a landmark set for the parameter k. The\ncase k=1 is a classic problem, where in its weighted version, each vertex v has\na non-negative weight, and the goal is to find a landmark set with minimal\ntotal weight. We generalize the problem for k \\geq 2, introducing two models,\nand we seek for solutions to both the weighted version and the unweighted\nversion of this more general problem. In the model of all-pairs (AP), k\nseparations are needed for every pair of distinct vertices of V, while in the\nnon-landmarks model (NL), such separations are required only for pairs of\ndistinct vertices in V \\setminus L.\n  We study the weighted and unweighted versions for both models (AP and NL),\nfor path graphs, complete graphs, complete bipartite graphs, and complete wheel\ngraphs, for all values of k \\geq 2. We present algorithms for these cases, thus\ndemonstrating the difference between the two new models, and the differences\nbetween the cases k=1 and k \\geq 2.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 20:06:20 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Adar", "Ron", ""], ["Epstein", "Leah", ""]]}, {"id": "1410.4240", "submitter": "Seeun Umboh", "authors": "Seeun Umboh", "title": "Online Network Design Algorithms via Hierarchical Decompositions", "comments": "Accepted to SODA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new approach for online network design and obtain improved\ncompetitive ratios for several problems. Our approach gives natural\ndeterministic algorithms and simple analyses. At the heart of our work is a\nnovel application of embeddings into hierarchically well-separated trees (HSTs)\nto the analysis of online network design algorithms --- we charge the cost of\nthe algorithm to the cost of the optimal solution on any HST embedding of the\nterminals. This analysis technique is widely applicable to many problems and\ngives a unified framework for online network design.\n  In a sense, our work brings together two of the main approaches to online\nnetwork design. The first uses greedy-like algorithms and analyzes them using\ndual-fitting. The second uses tree embeddings and results in randomized $O(\\log\nn)$-competitive algorithms, where $n$ is the total number of vertices in the\ngraph. Our approach uses deterministic greedy-like algorithms but analyzes them\nvia HST embeddings of the terminals. Our proofs are simpler as we do not need\nto carefully construct dual solutions and we get $O(\\log k)$ competitive\nratios, where $k$ is the number of terminals.\n  In this paper, we apply our approach to obtain deterministic $O(\\log\nk)$-competitive online algorithms for the following problems.\n  - Steiner network with edge duplication. Previously, only a randomized\n$O(\\log n)$-competitive algorithm was known.\n  - Rent-or-buy. Previously, only deterministic $O(\\log^2 k)$-competitive and\nrandomized $O(\\log k)$-competitive algorithms by Awerbuch, Azar and Bartal\n(2004) were known.\n  - Connected facility location. Previously, only a randomized $O(\\log^2\nk)$-competitive algorithm by San Felice, Williamson and Lee (2014) was known.\n  - Prize-collecting Steiner forest. We match the competitive ratio first\nachieved by Qian and Williamson (2011) and give a simpler analysis.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 21:43:04 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Umboh", "Seeun", ""]]}, {"id": "1410.4266", "submitter": "Bernhard Haeupler", "authors": "Bernhard Haeupler, Mark Manasse, Kunal Talwar", "title": "Consistent Weighted Sampling Made Fast, Small, and Easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document sketching using Jaccard similarity has been a workable effective\ntechnique in reducing near-duplicates in Web page and image search results, and\nhas also proven useful in file system synchronization, compression and learning\napplications.\n  Min-wise sampling can be used to derive an unbiased estimator for Jaccard\nsimilarity and taking a few hundred independent consistent samples leads to\ncompact sketches which provide good estimates of pairwise-similarity.\nSubsequent works extended this technique to weighted sets and show how to\nproduce samples with only a constant number of hash evaluations for any\nelement, independent of its weight. Another improvement by Li et al. shows how\nto speedup sketch computations by computing many (near-)independent samples in\none shot. Unfortunately this latter improvement works only for the unweighted\ncase.\n  In this paper we give a simple, fast and accurate procedure which reduces\nweighted sets to unweighted sets with small impact on the Jaccard similarity.\nThis leads to compact sketches consisting of many (near-)independent weighted\nsamples which can be computed with just a small constant number of hash\nfunction evaluations per weighted element. The size of the produced unweighted\nset is furthermore a tunable parameter which enables us to run the unweighted\nscheme of Li et al. in the regime where it is most efficient. Even when the\nsets involved are unweighted, our approach gives a simple solution to the\ndensification problem that other works attempted to address.\n  Unlike previously known schemes, ours does not result in an unbiased\nestimator. However, we prove that the bias introduced by our reduction is\nnegligible and that the standard deviation is comparable to the unweighted\ncase. We also empirically evaluate our scheme and show that it gives\nsignificant gains in computational efficiency, without any measurable loss in\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 00:44:52 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Manasse", "Mark", ""], ["Talwar", "Kunal", ""]]}, {"id": "1410.4273", "submitter": "David Anderson", "authors": "David G. Anderson, Ming Gu, and Christopher Melgaard", "title": "An Efficient Algorithm for Unweighted Spectral Graph Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral graph sparsification has emerged as a powerful tool in the analysis\nof large-scale networks by reducing the overall number of edges, while\nmaintaining a comparable graph Laplacian matrix. In this paper, we present an\nefficient algorithm for the construction of a new type of spectral sparsifier,\nthe unweighted spectral sparsifier. Given a general undirected and unweighted\ngraph $G = (V, E)$ and an integer $\\ell < |E|$ (the number of edges in $E$), we\ncompute an unweighted graph $H = (V, F)$ with $F \\subset E$ and $|F| = \\ell$\nsuch that for every $x \\in \\mathbb{R}^{V}$ \\[ {\\displaystyle \\frac{x^T L_G\nx}{\\kappa} \\leq x^T L_H x \\leq x^T L_G x,} \\] where $L_G$ and $L_H$ are the\nLaplacian matrices for $G$ and $H$, respectively, and $\\kappa \\geq 1$ is a\nslowly-varying function of $|V|, |E|$ and $\\ell$. This work addresses the open\nquestion of the existence of unweighted graph sparsifiers for unweighted\ngraphs. Additionally, our algorithm can efficiently compute unweighted graph\nsparsifiers for weighted graphs, leading to sparsified graphs that retain the\nweights of the original graphs.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 01:53:01 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 08:09:32 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Anderson", "David G.", ""], ["Gu", "Ming", ""], ["Melgaard", "Christopher", ""]]}, {"id": "1410.4395", "submitter": "Alexander Setzer", "authors": "Martina Eikel, Christian Scheideler, Alexander Setzer", "title": "Minimum Linear Arrangement of Series-Parallel Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a factor $14D^2$ approximation algorithm for the minimum linear\narrangement problem on series-parallel graphs, where $D$ is the maximum degree\nin the graph. Given a suitable decomposition of the graph, our algorithm runs\nin time $O(|E|)$ and is very easy to implement. Its divide-and-conquer approach\nallows for an effective parallelization. Note that a suitable decomposition can\nalso be computed in time $O(|E|\\log{|E|})$ (or even $O(\\log{|E|}\\log^*{|E|})$\non an EREW PRAM using $O(|E|)$ processors).\n  For the proof of the approximation ratio, we use a sophisticated charging\nmethod that uses techniques similar to amortized analysis in advanced data\nstructures.\n  On general graphs, the minimum linear arrangement problem is known to be\nNP-hard. To the best of our knowledge, the minimum linear arrangement problem\non series-parallel graphs has not been studied before.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 12:37:33 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Eikel", "Martina", ""], ["Scheideler", "Christian", ""], ["Setzer", "Alexander", ""]]}, {"id": "1410.4429", "submitter": "Daniel Hsu", "authors": "Daniel Hsu", "title": "Weighted sampling of outer products", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note gives a simple analysis of the randomized approximation scheme for\nmatrix multiplication of Drineas et al (2006) with a particular sampling\ndistribution over outer products. The result follows from a matrix version of\nBernstein's inequality. To approximate the matrix product $AB^\\top$ to spectral\nnorm error $\\varepsilon\\|A\\|\\|B\\|$, it suffices to sample on the order of\n$(\\mathrm{sr}(A) \\vee \\mathrm{sr}(B)) \\log(\\mathrm{sr}(A) \\wedge\n\\mathrm{sr}(B)) / \\varepsilon^2$ outer products, where $\\mathrm{sr}(M)$ is the\nstable rank of a matrix $M$.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 13:58:09 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Hsu", "Daniel", ""]]}, {"id": "1410.4701", "submitter": "Patrick Hagge Cording", "authors": "Patrick Hagge Cording", "title": "Optimal Time Random Access to Grammar-Compressed Strings in Small Space", "comments": "Withdrawn because of errors in proofs. Fixed versions will be\n  incorporated into a paper by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random access problem for compressed strings is to build a data structure\nthat efficiently supports accessing the character in position $i$ of a string\ngiven in compressed form. Given a grammar of size $n$ compressing a string of\nsize $N$, we present a data structure using $O(n\\Delta \\log_\\Delta \\frac N n\n\\log N)$ bits of space that supports accessing position $i$ in $O(\\log_\\Delta\nN)$ time for $\\Delta \\leq \\log^{O(1)} N$. The query time is optimal for\npolynomially compressible strings, i.e., when $n=O(N^{1-\\epsilon})$.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 11:59:45 GMT"}, {"version": "v2", "created": "Mon, 20 Oct 2014 12:51:55 GMT"}, {"version": "v3", "created": "Fri, 31 Oct 2014 14:27:34 GMT"}, {"version": "v4", "created": "Mon, 26 Jan 2015 13:56:24 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Cording", "Patrick Hagge", ""]]}, {"id": "1410.4885", "submitter": "Ilya Safro", "authors": "William W. Hager and James T. Hungerford and Ilya Safro", "title": "A Multilevel Bilinear Programming Algorithm For the Vertex Separator\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Vertex Separator Problem for a graph is to find the smallest collection\nof vertices whose removal breaks the graph into two disconnected subsets that\nsatisfy specified size constraints. In the paper 10.1016/j.ejor.2014.05.042,\nthe Vertex Separator Problem was formulated as a continuous\n(non-concave/non-convex) bilinear quadratic program. In this paper, we develop\na more general continuous bilinear program which incorporates vertex weights,\nand which applies to the coarse graphs that are generated in a multilevel\ncompression of the original Vertex Separator Problem. A Mountain Climbing\nAlgorithm is used to find a stationary point of the continuous bilinear\nquadratic program, while second-order optimality conditions and perturbation\ntechniques are used to escape from either a stationary point or a local\nmaximizer. The algorithms for solving the continuous bilinear program are\nemployed during the solution and refinement phases in a multilevel scheme.\nComputational results and comparisons demonstrate the advantage of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 23:07:50 GMT"}, {"version": "v2", "created": "Sun, 17 Jul 2016 20:21:07 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Hager", "William W.", ""], ["Hungerford", "James T.", ""], ["Safro", "Ilya", ""]]}, {"id": "1410.4955", "submitter": "Wenbin Chen", "authors": "Wenbin Chen", "title": "Settling the Randomized k-sever Conjecture on Some Special Metrics", "comments": "This paper has been withdrawn by the author due to a crucial error in\n  the LP formulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we settle the randomized $k$-sever conjecture for the\nfollowing metric spaces: line, circle, Hierarchically well-separated tree\n(HST). Specially, we show that there are $O(\\log k)$-competitive randomized\n$k$-sever algorithms for above metric spaces. For any general metric space with\n$n$ points, we show that there is an $O( \\log k \\log n)$-competitive randomized\n$k$-sever algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 18 Oct 2014 13:11:47 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2015 06:40:39 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2015 04:55:22 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Chen", "Wenbin", ""]]}, {"id": "1410.4963", "submitter": "Rajeev Raman", "authors": "Pooya Davoodi and Rajeev Raman and Srinivasa Rao Satti", "title": "On Succinct Representations of Binary Trees", "comments": "Journal version of part of COCOON 2012 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe that a standard transformation between \\emph{ordinal} trees\n(arbitrary rooted trees with ordered children) and binary trees leads to\ninteresting succinct binary tree representations. There are four symmetric\nversions of these transformations. Via these transformations we get four\nsuccinct representations of $n$-node binary trees that use $2n + n/(\\log\nn)^{O(1)}$ bits and support (among other operations) navigation, inorder\nnumbering, one of pre- or post-order numbering, subtree size and lowest common\nancestor (LCA) queries. The ability to support inorder numbering is crucial for\nthe well-known range-minimum query (RMQ) problem on an array $A$ of $n$ ordered\nvalues. While this functionality, and more, is also supported in $O(1)$ time\nusing $2n + o(n)$ bits by Davoodi et al.'s (\\emph{Phil. Trans. Royal Soc. A}\n\\textbf{372} (2014)) extension of a representation by Farzan and Munro\n(\\emph{Algorithmica} \\textbf{6} (2014)), their \\emph{redundancy}, or the $o(n)$\nterm, is much larger, and their approach may not be suitable for practical\nimplementations.\n  One of these transformations is related to the Zaks' sequence (S.~Zaks,\n\\emph{Theor. Comput. Sci.} \\textbf{10} (1980)) for encoding binary trees, and\nwe thus provide the first succinct binary tree representation based on Zaks'\nsequence. Another of these transformations is equivalent to Fischer and Heun's\n(\\emph{SIAM J. Comput.} \\textbf{40} (2011)) \\minheap\\ structure for this\nproblem. Yet another variant allows an encoding of the Cartesian tree of $A$ to\nbe constructed from $A$ using only $O(\\sqrt{n} \\log n)$ bits of working space.\n", "versions": [{"version": "v1", "created": "Sat, 18 Oct 2014 14:43:10 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Davoodi", "Pooya", ""], ["Raman", "Rajeev", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "1410.5053", "submitter": "Yuichi Yoshida", "authors": "Yuichi Yoshida", "title": "Gowers Norm, Function Limits, and Parameter Estimation", "comments": "arXiv admin note: text overlap with arXiv:1212.3849, arXiv:1308.4108\n  by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\{f_i:\\mathbb{F}_p^i \\to \\{0,1\\}\\}$ be a sequence of functions, where\n$p$ is a fixed prime and $\\mathbb{F}_p$ is the finite field of order $p$. The\nlimit of the sequence can be syntactically defined using the notion of\nultralimit. Inspired by the Gowers norm, we introduce a metric over limits of\nfunction sequences, and study properties of it. One application of this metric\nis that it provides a characterization of affine-invariant parameters of\nfunctions that are constant-query estimable. Using this characterization, we\nshow that the property of being a function of a constant number of low-degree\npolynomials and a constant number of factored polynomials (of arbitrary\ndegrees) is constant-query testable if it is closed under blowing-up. Examples\nof this property include the property of having a constant spectral norm and\ndegree-structural properties with rank conditions.\n", "versions": [{"version": "v1", "created": "Sun, 19 Oct 2014 09:50:04 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 05:28:07 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Yoshida", "Yuichi", ""]]}, {"id": "1410.5062", "submitter": "Meirav Zehavi", "authors": "Meirav Zehavi", "title": "Mixing Color Coding-Related Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Narrow sieves, representative sets and divide-and-color are three\nbreakthrough color coding-related techniques, which led to the design of\nextremely fast parameterized algorithms. We present a novel family of\nstrategies for applying mixtures of them. This includes: (a) a mix of\nrepresentative sets and narrow sieves; (b) a faster computation of\nrepresentative sets under certain separateness conditions, mixed with\ndivide-and-color and a new technique, \"balanced cutting\"; (c) two mixtures of\nrepresentative sets, iterative compression and a new technique, \"unbalanced\ncutting\". We demonstrate our strategies by obtaining, among other results,\nsignificantly faster algorithms for $k$-Internal Out-Branching and Weighted\n3-Set $k$-Packing, and a framework for speeding-up the previous best\ndeterministic algorithms for $k$-Path, $k$-Tree, $r$-Dimensional $k$-Matching,\nGraph Motif and Partial Cover.\n", "versions": [{"version": "v1", "created": "Sun, 19 Oct 2014 11:54:00 GMT"}, {"version": "v2", "created": "Sun, 23 Nov 2014 20:27:26 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2015 15:23:40 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Zehavi", "Meirav", ""]]}, {"id": "1410.5105", "submitter": "Chaitanya Swamy", "authors": "Guru Guruganesh, Laura Sanita, and Chaitanya Swamy", "title": "Improved Region-Growing and Combinatorial Algorithms for $k$-Route Cut\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the {\\em $k$-route} generalizations of various cut problems, the\nmost general of which is \\emph{$k$-route multicut} ($k$-MC) problem, wherein we\nhave $r$ source-sink pairs and the goal is to delete a minimum-cost set of\nedges to reduce the edge-connectivity of every source-sink pair to below $k$.\nThe $k$-route extensions of multiway cut ($k$-MWC), and the minimum $s$-$t$ cut\nproblem ($k$-$(s,t)$-cut), are similarly defined. We present various\napproximation and hardness results for these $k$-route cut problems that\nimprove the state-of-the-art for these problems in several cases. (i) For {\\em\n$k$-route multiway cut}, we devise simple, but surprisingly effective,\ncombinatorial algorithms that yield bicriteria approximation guarantees that\nmarkedly improve upon the previous-best guarantees. (ii) For {\\em $k$-route\nmulticut}, we design algorithms that improve upon the previous-best\napproximation factors by roughly an $O(\\sqrt{\\log r})$-factor, when $k=2$, and\nfor general $k$ and unit costs and any fixed violation of the connectivity\nthreshold $k$. The main technical innovation is the definition of a new,\npowerful \\emph{region growing} lemma that allows us to perform region-growing\nin a recursive fashion even though the LP solution yields a {\\em different\nmetric} for each source-sink pair. (iii) We complement these results by showing\nthat the {\\em $k$-route $s$-$t$ cut} problem is at least as hard to approximate\nas the {\\em densest-$k$-subgraph} (DkS) problem on uniform hypergraphs.\n", "versions": [{"version": "v1", "created": "Sun, 19 Oct 2014 19:23:24 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Guruganesh", "Guru", ""], ["Sanita", "Laura", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "1410.5152", "submitter": "Shanghua Teng", "authors": "Christian Borgs and Jennifer Chayes and Adrian Marple and Shang-Hua\n  Teng", "title": "Fixed-Points of Social Choice: An Axiomatic Approach to Network\n  Communities", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CC cs.DS cs.GT cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first social choice theory approach to the question of what\nconstitutes a community in a social network. Inspired by the classic\npreferences models in social choice theory, we start from an abstract social\nnetwork framework, called preference networks; these consist of a finite set of\nmembers where each member has a total-ranking preference of all members in the\nset.\n  Within this framework, we develop two complementary approaches to\naxiomatically study the formation and structures of communities. (1) We apply\nsocial choice theory and define communities indirectly by postulating that they\nare fixed points of a preference aggregation function obeying certain desirable\naxioms. (2) We directly postulate desirable axioms for communities without\nreference to preference aggregation, leading to eight natural community axioms.\n  These approaches allow us to formulate and analyze community rules. We prove\na taxonomy theorem that provides a structural characterization of the family of\ncommunity rules that satisfies all eight axioms. The structure is actually\nquite beautiful: these community rules form a bounded lattice under the natural\nintersection and union operations. Our structural theorem is complemented with\na complexity result: while identifying a community by the most selective rule\nof the lattice is in P, deciding if a subset is a community by the most\ncomprehensive rule of the lattice is coNP-complete. Our studies also shed light\non the limitations of defining community rules solely based on preference\naggregation: any aggregation function satisfying Arrow's IIA axiom, or based on\ncommonly used aggregation schemes like the Borda count or generalizations\nthereof, lead to communities which violate at least one of our community\naxioms. Finally, we give a polynomial-time rule consistent with seven axioms\nand weakly satisfying the eighth axiom.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 04:37:23 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Borgs", "Christian", ""], ["Chayes", "Jennifer", ""], ["Marple", "Adrian", ""], ["Teng", "Shang-Hua", ""]]}, {"id": "1410.5191", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, Mark Jones, and Magnus Wahlstrom", "title": "Structural Parameterizations of the Mixed Chinese Postman Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Mixed Chinese Postman Problem (MCPP), given a weighted mixed graph $G$\n($G$ may have both edges and arcs), our aim is to find a minimum weight closed\nwalk traversing each edge and arc at least once. The MCPP parameterized by the\nnumber of edges in $G$ or the number of arcs in $G$ is fixed-parameter\ntractable as proved by van Bevern {\\em et al.} (in press) and Gutin, Jones and\nSheng (ESA 2014), respectively. In this paper, we consider the unweighted\nversion of MCPP. Solving an open question of van Bevern {\\em et al.} (in\npress), we show that somewhat unexpectedly MCPP parameterized by the\n(undirected) treewidth of $G$ is W[1]-hard. In fact, we prove that even the\nMCPP parameterized by the pathwidth of $G$ is W[1]-hard. On the positive side,\nwe show that the unweighted version of MCPP parameterized by tree-depth is\nfixed-parameter tractable. We are unaware of any natural graph parameters\nbetween pathwidth and tree-depth and so our results provide a dichotomy of the\ncomplexity of MCPP. Furthermore, we believe that MCPP is the first problem\nknown to be W[1]-hard with respect to treewidth but FPT with respect to\ntree-depth.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 08:37:11 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 19:12:30 GMT"}, {"version": "v3", "created": "Sun, 25 Jan 2015 10:35:57 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Gutin", "Gregory", ""], ["Jones", "Mark", ""], ["Wahlstrom", "Magnus", ""]]}, {"id": "1410.5355", "submitter": "Dominik Kaaser", "authors": "Robert Els\\\"asser and Dominik Kaaser", "title": "On the Influence of Graph Density on Randomized Gossiping", "comments": "Full version of paper submitted to IPDPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information dissemination is a fundamental problem in parallel and\ndistributed computing. In its simplest variant, the broadcasting problem, a\nmessage has to be spread among all nodes of a graph. A prominent communication\nprotocol for this problem is based on the random phone call model (Karp et al.,\nFOCS 2000). In each step, every node opens a communication channel to a\nrandomly chosen neighbor for bi-directional communication.\n  Motivated by replicated databases and peer-to-peer networks, Berenbrink et\nal., ICALP 2010, considered the gossiping problem in the random phone call\nmodel. There, each node starts with its own message and all messages have to be\ndisseminated to all nodes in the network. They showed that any $O(\\log n)$-time\nalgorithm in complete graphs requires $\\Omega(\\log n)$ message transmissions\nper node to complete gossiping, w.h.p, while for broadcasting the average\nnumber of transmissions per node is $O(\\log\\log n)$.\n  It is known that the $O(n\\log\\log n)$ bound on the number of transmissions\nrequired for randomized broadcasting in complete graphs cannot be achieved in\nsparse graphs even if they have best expansion and connectivity properties. In\nthis paper, we analyze whether a similar influence of the graph density also\nholds w.r.t. the performance of gossiping. We study analytically and\nempirically the communication overhead generated by randomized gossiping in\nrandom graphs and consider simple modifications of the random phone call model\nin these graphs. Our results indicate that, unlike in broadcasting, there is no\nsignificant difference between the performance of randomized gossiping in\ncomplete graphs and sparse random graphs. Furthermore, our simulations indicate\nthat by tuning the parameters of our algorithms, we can significantly reduce\nthe communication overhead compared to the traditional push-pull approach in\nthe graphs we consider.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 17:10:32 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 19:23:45 GMT"}, {"version": "v3", "created": "Tue, 9 Dec 2014 09:10:55 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Els\u00e4sser", "Robert", ""], ["Kaaser", "Dominik", ""]]}, {"id": "1410.5392", "submitter": "Yu Cheng", "authors": "Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng and Shang-Hua Teng", "title": "Scalable Parallel Factorizations of SDD Matrices and Efficient Sampling\n  for Gaussian Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a sampling problem basic to computational statistical inference,\nwe develop a nearly optimal algorithm for a fundamental problem in spectral\ngraph theory and numerical analysis. Given an $n\\times n$ SDDM matrix ${\\bf\n\\mathbf{M}}$, and a constant $-1 \\leq p \\leq 1$, our algorithm gives efficient\naccess to a sparse $n\\times n$ linear operator $\\tilde{\\mathbf{C}}$ such that\n$${\\mathbf{M}}^{p} \\approx \\tilde{\\mathbf{C}} \\tilde{\\mathbf{C}}^\\top.$$ The\nsolution is based on factoring ${\\bf \\mathbf{M}}$ into a product of simple and\nsparse matrices using squaring and spectral sparsification. For ${\\mathbf{M}}$\nwith $m$ non-zero entries, our algorithm takes work nearly-linear in $m$, and\npolylogarithmic depth on a parallel machine with $m$ processors. This gives the\nfirst sampling algorithm that only requires nearly linear work and $n$ i.i.d.\nrandom univariate Gaussian samples to generate i.i.d. random samples for\n$n$-dimensional Gaussian random fields with SDDM precision matrices. For\nsampling this natural subclass of Gaussian random fields, it is optimal in the\nrandomness and nearly optimal in the work and parallel complexity. In addition,\nour sampling algorithm can be directly extended to Gaussian random fields with\nSDD precision matrices.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 18:59:58 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Cheng", "Dehua", ""], ["Cheng", "Yu", ""], ["Liu", "Yan", ""], ["Peng", "Richard", ""], ["Teng", "Shang-Hua", ""]]}, {"id": "1410.5410", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "Improved Asymmetric Locality Sensitive Hashing (ALSH) for Maximum Inner\n  Product Search (MIPS)", "comments": "arXiv admin note: text overlap with arXiv:1405.5869", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it was shown that the problem of Maximum Inner Product Search (MIPS)\nis efficient and it admits provably sub-linear hashing algorithms. Asymmetric\ntransformations before hashing were the key in solving MIPS which was otherwise\nhard. In the prior work, the authors use asymmetric transformations which\nconvert the problem of approximate MIPS into the problem of approximate near\nneighbor search which can be efficiently solved using hashing. In this work, we\nprovide a different transformation which converts the problem of approximate\nMIPS into the problem of approximate cosine similarity search which can be\nefficiently solved using signed random projections. Theoretical analysis show\nthat the new scheme is significantly better than the original scheme for MIPS.\nExperimental evaluations strongly support the theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 19:54:58 GMT"}, {"version": "v2", "created": "Thu, 13 Nov 2014 20:48:36 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1410.5415", "submitter": "Antoine Thomas", "authors": "Antoine Thomas", "title": "Rearrangement Problems with Duplicated Genomic Markers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the dynamics of genome rearrangements is a major issue of\nphylogenetics. Phylogenetics is the study of species evolution. A major goal of\nthe field is to establish evolutionary relationships within groups of species,\nin order to infer the topology of an evolutionary tree formed by this group and\ncommon ancestors to some of these species. In this context, having means to\nevaluate relative evolutionary distances between species, or to infer common\nancestor genomes to a group of species would be of great help. This work, in\nthe vein of other studies from the past, aims at designing such means, here in\nthe particular case where genomes present multiple occurrencies of genes, which\nmakes things more complex. Several hypotheses accounting for the presence of\nduplications were considered. Distances formulae as well as scenario computing\nalgorithms were established, along with their complexity proofs.\n", "versions": [{"version": "v1", "created": "Sat, 18 Oct 2014 14:11:20 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Thomas", "Antoine", ""]]}, {"id": "1410.5420", "submitter": "Russell Brown", "authors": "Russell A. Brown", "title": "Building a Balanced k-d Tree in O(kn log n) Time", "comments": "11 pages, 9 figures, published at\n  http://jcgt.org/published/0004/01/03/", "journal-ref": "Journal of Computer Graphics Techniques (JCGT), vol. 4, no. 1,\n  50-68, 2015", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original description of the k-d tree recognized that rebalancing\ntechniques, such as are used to build an AVL tree or a red-black tree, are not\napplicable to a k-d tree. Hence, in order to build a balanced k-d tree, it is\nnecessary to find the median of the data for each recursive subdivision of\nthose data. The sort or selection that is used to find the median for each\nsubdivision strongly influences the computational complexity of building a k-d\ntree. This paper discusses an alternative algorithm that builds a balanced k-d\ntree by presorting the data in each of k dimensions prior to building the tree.\nIt then preserves the order of these k sorts during tree construction and\nthereby avoids the requirement for any further sorting. Moreover, this\nalgorithm is amenable to parallel execution via multiple threads. Compared to\nan algorithm that finds the median for each recursive subdivision, this\npresorting algorithm has equivalent performance for four dimensions and better\nperformance for three or fewer dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 16:08:40 GMT"}, {"version": "v10", "created": "Sun, 30 Nov 2014 02:35:50 GMT"}, {"version": "v11", "created": "Wed, 10 Dec 2014 05:47:22 GMT"}, {"version": "v12", "created": "Sun, 14 Dec 2014 20:11:46 GMT"}, {"version": "v13", "created": "Mon, 29 Dec 2014 15:23:32 GMT"}, {"version": "v14", "created": "Tue, 30 Dec 2014 20:27:00 GMT"}, {"version": "v15", "created": "Fri, 2 Jan 2015 17:49:54 GMT"}, {"version": "v16", "created": "Wed, 7 Jan 2015 16:25:12 GMT"}, {"version": "v17", "created": "Sat, 10 Jan 2015 02:33:07 GMT"}, {"version": "v18", "created": "Tue, 10 Feb 2015 07:07:24 GMT"}, {"version": "v19", "created": "Sun, 15 Feb 2015 15:12:58 GMT"}, {"version": "v2", "created": "Wed, 22 Oct 2014 02:22:55 GMT"}, {"version": "v20", "created": "Thu, 2 Apr 2015 17:16:05 GMT"}, {"version": "v21", "created": "Sun, 5 Apr 2015 14:47:36 GMT"}, {"version": "v22", "created": "Thu, 9 Apr 2015 04:13:16 GMT"}, {"version": "v23", "created": "Sun, 12 Apr 2015 22:07:16 GMT"}, {"version": "v24", "created": "Tue, 14 Apr 2015 19:43:47 GMT"}, {"version": "v25", "created": "Mon, 20 Apr 2015 02:42:12 GMT"}, {"version": "v26", "created": "Tue, 28 Apr 2015 04:20:00 GMT"}, {"version": "v27", "created": "Thu, 7 May 2015 14:07:35 GMT"}, {"version": "v28", "created": "Thu, 13 Feb 2020 04:20:11 GMT"}, {"version": "v29", "created": "Mon, 2 Mar 2020 00:39:21 GMT"}, {"version": "v3", "created": "Thu, 23 Oct 2014 05:05:42 GMT"}, {"version": "v4", "created": "Fri, 24 Oct 2014 17:53:47 GMT"}, {"version": "v5", "created": "Mon, 27 Oct 2014 04:50:47 GMT"}, {"version": "v6", "created": "Wed, 29 Oct 2014 14:59:24 GMT"}, {"version": "v7", "created": "Mon, 10 Nov 2014 15:10:01 GMT"}, {"version": "v8", "created": "Mon, 17 Nov 2014 04:46:47 GMT"}, {"version": "v9", "created": "Tue, 25 Nov 2014 20:48:43 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Brown", "Russell A.", ""]]}, {"id": "1410.5518", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Nathan Srebro", "title": "On Symmetric and Asymmetric LSHs for Inner Product Search", "comments": "11 pages, 3 figures, In Proceedings of The 32nd International\n  Conference on Machine Learning (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing locality sensitive hashes (LSH) for\ninner product similarity, and of the power of asymmetric hashes in this\ncontext. Shrivastava and Li argue that there is no symmetric LSH for the\nproblem and propose an asymmetric LSH based on different mappings for query and\ndatabase points. However, we show there does exist a simple symmetric LSH that\nenjoys stronger guarantees and better empirical performance than the asymmetric\nLSH they suggest. We also show a variant of the settings where asymmetry is\nin-fact needed, but there a different asymmetric LSH is required.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 02:00:34 GMT"}, {"version": "v2", "created": "Fri, 24 Oct 2014 21:31:06 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2015 19:30:35 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Srebro", "Nathan", ""]]}, {"id": "1410.5607", "submitter": "Amihood Amir", "authors": "Amihood Amir, Oren Kapah, Ely Porat, and Amir Rothschild", "title": "Polynomials: a new tool for length reduction in binary discrete\n  convolutions", "comments": "arXiv admin note: substantial text overlap with arXiv:0802.0017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient handling of sparse data is a key challenge in Computer Science.\nBinary convolutions, such as polynomial multiplication or the Walsh Transform\nare a useful tool in many applications and are efficiently solved.\n  In the last decade, several problems required efficient solution of sparse\nbinary convolutions. both randomized and deterministic algorithms were\ndeveloped for efficiently computing the sparse polynomial multiplication. The\nkey operation in all these algorithms was length reduction. The sparse data is\nmapped into small vectors that preserve the convolution result. The reduction\nmethod used to-date was the modulo function since it preserves location (of the\n\"1\" bits) up to cyclic shift.\n  To date there is no known efficient algorithm for computing the sparse Walsh\ntransform. Since the modulo function does not preserve the Walsh transform a\nnew method for length reduction is needed. In this paper we present such a new\nmethod - polynomials. This method enables the development of an efficient\nalgorithm for computing the binary sparse Walsh transform. To our knowledge,\nthis is the first such algorithm. We also show that this method allows a faster\ndeterministic computation of sparse polynomial multiplication than currently\nknown in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 10:19:07 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Amir", "Amihood", ""], ["Kapah", "Oren", ""], ["Porat", "Ely", ""], ["Rothschild", "Amir", ""]]}, {"id": "1410.5775", "submitter": "A. B. Dieker", "authors": "A. B. Dieker and Santosh Vempala", "title": "Stochastic billiards for sampling from the boundary of a convex set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic billiards can be used for approximate sampling from the boundary\nof a bounded convex set through the Markov Chain Monte Carlo (MCMC) paradigm.\nThis paper studies how many steps of the underlying Markov chain are required\nto get samples (approximately) from the uniform distribution on the boundary of\nthe set, for sets with an upper bound on the curvature of the boundary. Our\nmain theorem implies a polynomial-time algorithm for sampling from the boundary\nof such sets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 18:28:31 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Dieker", "A. B.", ""], ["Vempala", "Santosh", ""]]}, {"id": "1410.5778", "submitter": "Sergio Cabello", "authors": "Sergio Cabello, David Gajser", "title": "Simple PTAS's for families of graphs excluding a minor", "comments": "To appear in Discrete Applied Mathematics", "journal-ref": "Discrete Applied Mathematics, 189, p. 41-48, 2015", "doi": "10.1016/j.dam.2015.03.004", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that very simple algorithms based on local search are polynomial-time\napproximation schemes for Maximum Independent Set, Minimum Vertex Cover and\nMinimum Dominating Set, when the input graphs have a fixed forbidden minor.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 18:32:28 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 01:15:32 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Cabello", "Sergio", ""], ["Gajser", "David", ""]]}, {"id": "1410.5850", "submitter": "Fabio D'Andreagiovanni", "authors": "Fabio D'Andreagiovanni, Jonatan Krolikowski, Jonad Pulaj", "title": "A Fast Hybrid Primal Heuristic for Multiband Robust Capacitated Network\n  Design with Multiple Time Periods", "comments": "This is the authors' final version of the paper published in Applied\n  Soft Computing 26, 497-507, 2015, DOI: 10.1016/j.asoc.2014.10.016. The final\n  publication is available at Elsevier ScienceDirect via\n  http://dx.doi.org/10.1016/j.asoc.2014.10.016", "journal-ref": "Applied Soft Computing 26 (2015) 497-507", "doi": "10.1016/j.asoc.2014.10.016", "report-no": null, "categories": "math.OC cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the Robust Multiperiod Network Design Problem, a\ngeneralization of the Capacitated Network Design Problem (CNDP) that, besides\nestablishing flow routing and network capacity installation as in a canonical\nCNDP, also considers a planning horizon made up of multiple time periods and\nprotection against fluctuations in traffic volumes. As a remedy against traffic\nvolume uncertainty, we propose a Robust Optimization model based on Multiband\nRobustness (B\\\"using and D'Andreagiovanni, 2012), a refinement of classical\nGamma-Robustness by Bertsimas and Sim that uses a system of multiple deviation\nbands. Since the resulting optimization problem may prove very challenging even\nfor instances of moderate size solved by a state-of-the-art optimization\nsolver, we propose a hybrid primal heuristic that combines a randomized fixing\nstrategy inspired by ant colony optimization, which exploits information coming\nfrom linear relaxations of the problem, and an exact large neighbourhood\nsearch. Computational experiments on a set of realistic instances from the\nSNDlib show that our original heuristic can run fast and produce solutions of\nextremely high quality associated with low optimality gaps.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 20:40:58 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 21:13:50 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["D'Andreagiovanni", "Fabio", ""], ["Krolikowski", "Jonatan", ""], ["Pulaj", "Jonad", ""]]}, {"id": "1410.5967", "submitter": "Svante Janson", "authors": "Svante Janson and Alfredo Viola", "title": "A unified approach to linear probing hashing with buckets", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a unified analysis of linear probing hashing with a general bucket\nsize. We use both a combinatorial approach, giving exact formulas for\ngenerating functions, and a probabilistic approach, giving simple derivations\nof asymptotic results. Both approaches complement nicely, and give a good\ninsight in the relation between linear probing and random walks. A key\nmethodological contribution, at the core of Analytic Combinatorics, is the use\nof the symbolic method (based on q-calculus) to directly derive the generating\nfunctions to analyze.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 09:39:49 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Janson", "Svante", ""], ["Viola", "Alfredo", ""]]}, {"id": "1410.6030", "submitter": "Toshimasa Ishii", "authors": "Toshimasa Ishii, Kazuhisa Makino", "title": "Posimodular Function Optimization", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a posimodular function $f: 2^V \\to \\mathbb{R}$ on a finite set $V$, we\nconsider the problem of finding a nonempty subset $X$ of $V$ that minimizes\n$f(X)$. Posimodular functions often arise in combinatorial optimization such as\nundirected cut functions. In this paper, we show that any algorithm for the\nproblem requires $\\Omega(2^{\\frac{n}{7.54}})$ oracle calls to $f$, where\n$n=|V|$. It contrasts to the fact that the submodular function minimization,\nwhich is another generalization of cut functions, is polynomially solvable.\n  When the range of a given posimodular function is restricted to be\n$D=\\{0,1,...,d\\}$ for some nonnegative integer $d$, we show that\n$\\Omega(2^{\\frac{d}{15.08}})$ oracle calls are necessary, while we propose an\n$O(n^dT_f+n^{2d+1})$-time algorithm for the problem. Here, $T_f$ denotes the\ntime needed to evaluate the function value $f(X)$ for a given $X \\subseteq V$.\n  We also consider the problem of maximizing a given posimodular function. We\nshow that $\\Omega(2^{n-1})$ oracle calls are necessary for solving the problem,\nand that the problem has time complexity $\\Theta(n^{d-1}T_f) $ when\n$D=\\{0,1,..., d\\}$ is the range of $f$ for some constant $d$.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 13:14:57 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Ishii", "Toshimasa", ""], ["Makino", "Kazuhisa", ""]]}, {"id": "1410.6220", "submitter": "Aran Nayebi", "authors": "Aran Nayebi and Virginia Vassilevska Williams", "title": "Quantum algorithms for shortest paths problems in structured instances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the quantum time complexity of the all pairs shortest paths\n(APSP) problem and some of its variants. The trivial classical algorithm for\nAPSP and most all pairs path problems runs in $O(n^3)$ time, while the trivial\nalgorithm in the quantum setting runs in $\\tilde{O}(n^{2.5})$ time, using\nGrover search. A major open problem in classical algorithms is to obtain a\ntruly subcubic time algorithm for APSP, i.e. an algorithm running in\n$O(n^{3-\\varepsilon})$ time for constant $\\varepsilon>0$. To approach this\nproblem, many truly subcubic time classical algorithms have been devised for\nAPSP and its variants for structured inputs. Some examples of such problems are\nAPSP in geometrically weighted graphs, graphs with small integer edge weights\nor a small number of weights incident to each vertex, and the all pairs\nearliest arrivals problem. In this paper we revisit these problems in the\nquantum setting and obtain the first nontrivial (i.e. $O(n^{2.5-\\varepsilon})$\ntime) quantum algorithms for the problems.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 01:09:06 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Nayebi", "Aran", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "1410.6400", "submitter": "Tobias Friedrich", "authors": "Nikolaos Fountoulakis, Tobias Friedrich, Danny Hermelin", "title": "On the Average-case Complexity of Parameterized Clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-Clique problem is a fundamental combinatorial problem that plays a\nprominent role in classical as well as in parameterized complexity theory. It\nis among the most well-known NP-complete and W[1]-complete problems. Moreover,\nits average-case complexity analysis has created a long thread of research\nalready since the 1970s. Here, we continue this line of research by studying\nthe dependence of the average-case complexity of the k-Clique problem on the\nparameter k. To this end, we define two natural parameterized analogs of\nefficient average-case algorithms. We then show that k-Clique admits both\nanalogues for Erd\\H{o}s-R\\'{e}nyi random graphs of arbitrary density. We also\nshow that k-Clique is unlikely to admit neither of these analogs for some\nspecific computable input distribution.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 15:33:01 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Fountoulakis", "Nikolaos", ""], ["Friedrich", "Tobias", ""], ["Hermelin", "Danny", ""]]}, {"id": "1410.6433", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Pawe{\\l} Gawrychowski and Przemys{\\l}aw Uzna\\'nski", "title": "Tight tradeoffs for approximating palindromes in streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider computing the longest palindrome in a text of length $n$ in the\nstreaming model, where the characters arrive one-by-one, and we do not have\nrandom access to the input. While computing the answer exactly using sublinear\nmemory is not possible in such a setting, one can still hope for a good\napproximation guarantee.\n  We focus on the two most natural variants, where we aim for either additive\nor multiplicative approximation of the length of the longest palindrome. We\nfirst show that there is no point in considering Las Vegas algorithms in such a\nsetting, as they cannot achieve sublinear space complexity. For Monte Carlo\nalgorithms, we provide a lowerbound of $\\Omega(\\frac{n}{E})$ bits for\napproximating the answer with additive error $E$, and $\\Omega(\\frac{\\log\nn}{\\log(1+\\varepsilon)})$ bits for approximating the answer with multiplicative\nerror $(1+\\varepsilon)$ for the binary alphabet. Then, we construct a generic\nMonte Carlo algorithm, which by choosing the parameters appropriately achieves\nspace complexity matching up to a logarithmic factor for both variants. This\nsubstantially improves the previous results by Berenbrink et al. (STACS 2014)\nand essentially settles the space complexity.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 17:54:10 GMT"}, {"version": "v2", "created": "Sun, 4 Jan 2015 23:30:24 GMT"}, {"version": "v3", "created": "Fri, 20 Feb 2015 19:53:39 GMT"}, {"version": "v4", "created": "Thu, 25 Feb 2016 16:57:32 GMT"}, {"version": "v5", "created": "Thu, 7 Apr 2016 10:28:22 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1410.6455", "submitter": "Yong Kong", "authors": "Yong Kong", "title": "Btrim: A fast, lightweight adapter and quality trimming program for\n  next-generation sequencing technologies", "comments": "8 pages, 1 figure", "journal-ref": "Genomics, 98, 152-153 (2001)", "doi": "10.1016/j.ygeno.2011.05.009", "report-no": null, "categories": "q-bio.GN cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Btrim is a fast and lightweight software to trim adapters and low quality\nregions in reads from ultra high-throughput next-generation sequencing\nmachines. It also can reliably identify barcodes and assign the reads to the\noriginal samples. Based on a modified Myers's bit-vector dynamic programming\nalgorithm, Btrim can handle indels in adapters and barcodes. It removes low\nquality regions and trims off adapters at both or either end of the reads. A\ntypical trimming of 30M reads with two sets of adapter pairs can be done in\nabout a minute with a small memory footprint. Btrim is a versatile stand-alone\ntool that can be used as the first step in virtually all next-generation\nsequence analysis pipelines. The program is available at\n\\url{http://graphics.med.yale.edu/trim/}.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 18:56:10 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Kong", "Yong", ""]]}, {"id": "1410.6621", "submitter": "Narasimhan Sadagopan", "authors": "M. Kavin, K. Keerthana, N. Sadagopan, Sangeetha. S, R. Vinothini", "title": "Some Combinatorial Problems on Halin Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $T$ be a tree with no degree 2 vertices and $L(T)=\\{l_1,\\ldots,l_r\\}, r\n\\geq 2$ denote the set of leaves in $T$. An Halin graph $G$ is a graph obtained\nfrom $T$ such that $V(G)=V(T)$ and $E(G)=E(T) \\cup \\{\\{l_i,l_{i+1}\\} ~|~ 1 \\leq\ni \\leq r-1\\} \\cup \\{l_1,l_r\\}$. In this paper, we investigate combinatorial\nproblems such as, testing whether a given graph is Halin or not, chromatic\nbounds, an algorithm to color Halin graphs with the minimum number of colors.\nFurther, we present polynomial-time algorithms for testing and coloring\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 08:55:12 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Kavin", "M.", ""], ["Keerthana", "K.", ""], ["Sadagopan", "N.", ""], ["S", "Sangeetha.", ""], ["Vinothini", "R.", ""]]}, {"id": "1410.6726", "submitter": "Jaroslav Opatrny", "authors": "J. Czyzowicz, E. Kranakis, D. Krizanc, L. Narayanan, J. Opatrny", "title": "Optimal online and offline algorithms for robot-assisted restoration of\n  barrier coverage", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperation between mobile robots and wireless sensor networks is a line of\nresearch that is currently attracting a lot of attention. In this context, we\nstudy the following problem of barrier coverage by stationary wireless sensors\nthat are assisted by a mobile robot with the capacity to move sensors. Assume\nthat $n$ sensors are initially arbitrarily distributed on a line segment\nbarrier. Each sensor is said to cover the portion of the barrier that\nintersects with its sensing area. Owing to incorrect initial position, or the\ndeath of some of the sensors, the barrier is not completely covered by the\nsensors. We employ a mobile robot to move the sensors to final positions on the\nbarrier such that barrier coverage is guaranteed. We seek algorithms that\nminimize the length of the robot's trajectory, since this allows the\nrestoration of barrier coverage as soon as possible. We give an optimal\nlinear-time offline algorithm that gives a minimum-length trajectory for a\nrobot that starts at one end of the barrier and achieves the restoration of\nbarrier coverage. We also study two different online models: one in which the\nonline robot does not know the length of the barrier in advance, and the other\nin which the online robot knows the length of the barrier. For the case when\nthe online robot does not know the length of the barrier, we prove a tight\nbound of $3/2$ on the competitive ratio, and we give a tight lower bound of\n$5/4$ on the competitive ratio in the other case. Thus for each case we give an\noptimal online algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 16:13:54 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Czyzowicz", "J.", ""], ["Kranakis", "E.", ""], ["Krizanc", "D.", ""], ["Narayanan", "L.", ""], ["Opatrny", "J.", ""]]}, {"id": "1410.6754", "submitter": "Michael Axtmann", "authors": "Michael Axtmann, Timo Bingmann, Peter Sanders, and Christian Schulz", "title": "Practical Massively Parallel Sorting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous parallel sorting algorithms do not scale to the largest available\nmachines, since they either have prohibitive communication volume or\nprohibitive critical path length. We describe algorithms that are a viable\ncompromise and overcome this gap both in theory and practice. The algorithms\nare multi-level generalizations of the known algorithms sample sort and\nmultiway mergesort. In particular our sample sort variant turns out to be very\nscalable. Some tools we develop may be of independent interest -- a simple,\npractical, and flexible sorting algorithm for small inputs working in\nlogarithmic time, a near linear time optimal algorithm for solving a\nconstrained bin packing problem, and an algorithm for data delivery, that\nguarantees a small number of message startups on each processor.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 17:56:48 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 10:53:28 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Axtmann", "Michael", ""], ["Bingmann", "Timo", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1410.6801", "submitter": "Christopher Musco", "authors": "Michael B. Cohen, Sam Elder, Cameron Musco, Christopher Musco,\n  Madalina Persu", "title": "Dimensionality Reduction for k-Means Clustering and Low Rank\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to approximate a data matrix $\\mathbf{A}$ with a much smaller\nsketch $\\mathbf{\\tilde A}$ that can be used to solve a general class of\nconstrained k-rank approximation problems to within $(1+\\epsilon)$ error.\nImportantly, this class of problems includes $k$-means clustering and\nunconstrained low rank approximation (i.e. principal component analysis). By\nreducing data points to just $O(k)$ dimensions, our methods generically\naccelerate any exact, approximate, or heuristic algorithm for these ubiquitous\nproblems.\n  For $k$-means dimensionality reduction, we provide $(1+\\epsilon)$ relative\nerror results for many common sketching techniques, including random row\nprojection, column selection, and approximate SVD. For approximate principal\ncomponent analysis, we give a simple alternative to known algorithms that has\napplications in the streaming setting. Additionally, we extend recent work on\ncolumn-based matrix reconstruction, giving column subsets that not only `cover'\na good subspace for $\\bv{A}$, but can be used directly to compute this\nsubspace.\n  Finally, for $k$-means clustering, we show how to achieve a $(9+\\epsilon)$\napproximation by Johnson-Lindenstrauss projecting data points to just $O(\\log\nk/\\epsilon^2)$ dimensions. This gives the first result that leverages the\nspecific structure of $k$-means to achieve dimension independent of input size\nand sublinear in $k$.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 19:43:16 GMT"}, {"version": "v2", "created": "Tue, 4 Nov 2014 03:42:52 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2015 02:33:24 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Cohen", "Michael B.", ""], ["Elder", "Sam", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""], ["Persu", "Madalina", ""]]}, {"id": "1410.6937", "submitter": "Aleksandr Cariow", "authors": "Aleksandr Cariow, Galina Cariowa", "title": "A Hardware-oriented Algorithm for Complex-valued Constant Matrix-vector\n  Multiplication", "comments": "4 pages, 3 fgures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a hardware-oriented algorithm for constant\nmatrix-vector product calculating, when the all elements of vector and matrix\nare complex numbers. The proposed algorithm versus the naive method of\nanalogous calculations drastically reduces the number of multipliers required\nfor FPGA implementation of complex-valued constant matrix-vector\nmultiplication.If the fully parallel hardware implementation of naive\n(schoolbook) method for complex-valued matrix-vector multiplication requires\n4MN multipliers, 2M N-inputs adders and 2MN two-input adders, the proposed\nalgorithm requires only 3N(M+1)/2 multipliers and 3M(N+2)+1,5N+2 two-input\nadders and 3(M+1) N/2-input adders.\n", "versions": [{"version": "v1", "created": "Sat, 25 Oct 2014 17:11:21 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Cariow", "Aleksandr", ""], ["Cariowa", "Galina", ""]]}, {"id": "1410.7050", "submitter": "Amit Daniely", "authors": "Amit Daniely", "title": "A PTAS for Agnostically Learning Halfspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a PTAS for agnostically learning halfspaces w.r.t. the uniform\ndistribution on the $d$ dimensional sphere. Namely, we show that for every\n$\\mu>0$ there is an algorithm that runs in time\n$\\mathrm{poly}(d,\\frac{1}{\\epsilon})$, and is guaranteed to return a classifier\nwith error at most $(1+\\mu)\\mathrm{opt}+\\epsilon$, where $\\mathrm{opt}$ is the\nerror of the best halfspace classifier. This improves on Awasthi, Balcan and\nLong [ABL14] who showed an algorithm with an (unspecified) constant\napproximation ratio. Our algorithm combines the classical technique of\npolynomial regression (e.g. [LMN89, KKMS05]), together with the new\nlocalization technique of [ABL14].\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 15:41:37 GMT"}, {"version": "v2", "created": "Tue, 28 Oct 2014 09:59:55 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2015 06:28:49 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Daniely", "Amit", ""]]}, {"id": "1410.7092", "submitter": "Marek Chrobak", "authors": "Marek Chrobak, Mordecai Golin, Tak-Wah Lam, Dorian Nogneng", "title": "Scheduling with Gaps: New Models and Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider scheduling problems for unit jobs with release times, where the\nnumber or size of the gaps in the schedule is taken into consideration, either\nin the objective function or as a constraint. Except for a few papers on energy\nminimization, there is no work in the scheduling literature that uses\nperformance metrics depending on the gap structure of a schedule. One of our\nobjectives is to initiate the study of such scheduling problems with gaps. We\nshow that such problems often lead to interesting algorithmic problems, with\nconnections to other areas of algorithmics. We focus on the model with unit\njobs. First we examine scheduling problems with deadlines, where we consider\nvariants of minimum-gap scheduling, including maximizing throughput with a\nbudget for gaps or minimizing the number of gaps with a throughput requirement.\nWe then turn to other objective functions. For example, in some scenarios, gaps\nin a schedule may be actually desirable, leading to the problem of maximizing\nthe number of gaps. Other versions we study include minimizing maximum gap or\nmaximizing minimum gap. The second part of the paper examines the model without\ndeadlines, where we focus on the tradeoff between the number of gaps and the\ntotal or maximum flow time. For all these problems we provide polynomial time\nalgorithms, with running times ranging from $O(n \\log n)$ for some problems, to\n$O(n^7)$ for other. The solutions involve a spectrum of algorithmic techniques,\nincluding different dynamic programming formulations, speed-up techniques based\non searching Monge arrays, searching X + Y matrices, or implicit binary search.\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 22:07:25 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 14:44:02 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 16:25:25 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2020 18:56:33 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Chrobak", "Marek", ""], ["Golin", "Mordecai", ""], ["Lam", "Tak-Wah", ""], ["Nogneng", "Dorian", ""]]}, {"id": "1410.7171", "submitter": "Reza Eghbali", "authors": "Reza Eghbali, Jon Swenson, Maryam Fazel", "title": "Exponentiated Subgradient Algorithm for Online Optimization under the\n  Random Permutation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online optimization problems arise in many resource allocation tasks, where\nthe future demands for each resource and the associated utility functions\nchange over time and are not known apriori, yet resources need to be allocated\nat every point in time despite the future uncertainty. In this paper, we\nconsider online optimization problems with general concave utilities. We modify\nand extend an online optimization algorithm proposed by Devanur et al. for\nlinear programming to this general setting. The model we use for the arrival of\nthe utilities and demands is known as the random permutation model, where a\nfixed collection of utilities and demands are presented to the algorithm in\nrandom order. We prove that under this model the algorithm achieves a\ncompetitive ratio of $1-O(\\epsilon)$ under a near-optimal assumption that the\nbid to budget ratio is $O (\\frac{\\epsilon^2}{\\log({m}/{\\epsilon})})$, where $m$\nis the number of resources, while enjoying a significantly lower computational\ncost than the optimal algorithm proposed by Kesselheim et al. We draw a\nconnection between the proposed algorithm and subgradient methods used in\nconvex optimization. In addition, we present numerical experiments that\ndemonstrate the performance and speed of this algorithm in comparison to\nexisting algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 10:28:12 GMT"}, {"version": "v2", "created": "Thu, 5 Feb 2015 07:58:42 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Eghbali", "Reza", ""], ["Swenson", "Jon", ""], ["Fazel", "Maryam", ""]]}, {"id": "1410.7237", "submitter": "Tobias Jacobs", "authors": "Tobias Jacobs and Salvatore Longo", "title": "A New Perspective on the Windows Scheduling Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Windows Scheduling Problem, also known as the Pinwheel Problem, is to\nschedule periodic jobs subject to their processing frequency demands. Instances\nare given as a set of jobs that have to be processed infinitely often such that\nthe time interval between two consecutive executions of the same job j is no\nlonger than the job's given period $p_j$.\n  The key contribution of this work is a new interpretation of the problem\nvariant with exact periods, where the time interval between consecutive\nexecutions must be strictly $p_j$. We show that this version is equivalent to a\nnatural combinatorial problem we call Partial Coding. Reductions in both\ndirections can be realized in polynomial time, so that both hardness proofs and\nalgorithms for Partial Coding transfer to Windows Scheduling.\n  Applying this new perspective, we obtain a number of new results regarding\nthe computational complexity of various Windows Scheduling Problem variants. We\nprove that even the case of one processor and unit-length jobs does not admit a\npseudo-polynomial time algorithm unless SAT can be solved by a randomized\nmethod in expected quasi-polynomial time. This result also extends to the case\nof inexact periods, which answers a question that has remained open for more\nthan two decades. Furthermore, we report an error found in a hardness proof\npreviously given for the multi-machine case without machine migration, and we\nshow that this variant reduces to the single-machine case. Finally, we prove\nthat even with unit-length jobs the problem is co-NP-hard when jobs are allowed\nto migrate between machines.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 13:49:35 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Jacobs", "Tobias", ""], ["Longo", "Salvatore", ""]]}, {"id": "1410.7506", "submitter": "Deeparnab Chakrabarty", "authors": "Deeparnab Chakrabarty and Sanjeev Khanna and Shi Li", "title": "On $(1,\\epsilon)$-Restricted Assignment Makespan Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Makespan minimization on unrelated machines is a classic problem in\napproximation algorithms. No polynomial time $(2-\\delta)$-approximation\nalgorithm is known for the problem for constant $\\delta> 0$. This is true even\nfor certain special cases, most notably the restricted assignment problem where\neach job has the same load on any machine but can be assigned to one from a\nspecified subset. Recently in a breakthrough result, Svensson [Svensson, 2011]\nproved that the integrality gap of a certain configuration LP relaxation is\nupper bounded by $1.95$ for the restricted assignment problem; however, the\nrounding algorithm is not known to run in polynomial time.\n  In this paper we consider the $(1,\\varepsilon)$-restricted assignment problem\nwhere each job is either heavy ($p_j = 1$) or light ($p_j = \\varepsilon$), for\nsome parameter $\\varepsilon > 0$. Our main result is a $(2-\\delta)$-approximate\npolynomial time algorithm for the $(1,\\epsilon)$-restricted assignment problem\nfor a fixed constant $\\delta> 0$. Even for this special case, the best\npolynomial-time approximation factor known so far is 2. We obtain this result\nby rounding the configuration LP relaxation for this problem. A simple\nreduction from vertex cover shows that this special case remains NP-hard to\napproximate to within a factor better than 7/6.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 03:40:59 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Khanna", "Sanjeev", ""], ["Li", "Shi", ""]]}, {"id": "1410.7530", "submitter": "Thomas Dueholm Hansen", "authors": "Oliver Friedmann and Thomas Dueholm Hansen and Uri Zwick", "title": "Random-Facet and Random-Bland require subexponential time even for\n  shortest paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Random-Facet algorithm of Kalai and of Matousek, Sharir and Welzl is an\nelegant randomized algorithm for solving linear programs and more general\nLP-type problems. Its expected subexponential time of\n$2^{\\tilde{O}(\\sqrt{m})}$, where $m$ is the number of inequalities, makes it\nthe fastest known combinatorial algorithm for solving linear programs. We\npreviously showed that Random-Facet performs an expected number of\n$2^{\\tilde{\\Omega}(\\sqrt[3]{m})}$ pivoting steps on some LPs with $m$\ninequalities that correspond to $m$-action Markov Decision Processes (MDPs). We\nalso showed that Random-Facet-1P, a one permutation variant of Random-Facet,\nperforms an expected number of $2^{\\tilde{O}(\\sqrt{m})}$ pivoting steps on\nthese examples. Here we show that the same results can be obtained using LPs\nthat correspond to instances of the classical shortest paths problem. This\nshows that the stochasticity of the MDPs, which is essential for obtaining\nlower bounds for Random-Edge, is not needed in order to obtain lower bounds for\nRandom-Facet. We also show that our new $2^{\\tilde{\\Omega}(\\sqrt{m})}$ lower\nbound applies to Random-Bland, a randomized variant of the classical\nanti-cycling rule suggested by Bland.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 06:47:55 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Friedmann", "Oliver", ""], ["Hansen", "Thomas Dueholm", ""], ["Zwick", "Uri", ""]]}, {"id": "1410.7534", "submitter": "Krzysztof Ciebiera", "authors": "Krzysztof Ciebiera, Piotr Godlewski, Piotr Sankowski, Piotr Wygocki", "title": "Approximation Algorithms for Steiner Tree Problems Based on Universal\n  Solution Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes the work on implementing few solutions for the Steiner\nTree problem which we undertook in the PAAL project. The main focus of the\nproject is the development of generic implementations of approximation\nalgorithms together with universal solution frameworks. In particular, we have\nimplemented Zelikovsky 11/6-approximation using local search framework, and\n1.39-approximation by Byrka et al. using iterative rounding framework. These\ntwo algorithms are experimentally compared with greedy 2-approximation, with\nexact but exponential time Dreyfus-Wagner algorithm, as well as with results\ngiven by a state-of-the-art local search techniques by Uchoa and Werneck. The\nresults of this paper are twofold. On one hand, we demonstrate that high level\nalgorithmic concepts can be designed and efficiently used in C++. On the other\nhand, we show that the above algorithms with good theoretical guarantees, give\ndecent results in practice, but are inferior to state-of-the-art heuristical\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 07:11:47 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Ciebiera", "Krzysztof", ""], ["Godlewski", "Piotr", ""], ["Sankowski", "Piotr", ""], ["Wygocki", "Piotr", ""]]}, {"id": "1410.7596", "submitter": "Shipra Agrawal", "authors": "Shipra Agrawal, Nikhil R. Devanur", "title": "Fast Algorithms for Online Stochastic Convex Programming", "comments": "To appear in SODA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the online stochastic Convex Programming (CP) problem, a very\ngeneral version of stochastic online problems which allows arbitrary concave\nobjectives and convex feasibility constraints. Many well-studied problems like\nonline stochastic packing and covering, online stochastic matching with concave\nreturns, etc. form a special case of online stochastic CP. We present fast\nalgorithms for these problems, which achieve near-optimal regret guarantees for\nboth the i.i.d. and the random permutation models of stochastic inputs. When\napplied to the special case online packing, our ideas yield a simpler and\nfaster primal-dual algorithm for this well studied problem, which achieves the\noptimal competitive ratio. Our techniques make explicit the connection of\nprimal-dual paradigm and online learning to online stochastic CP.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 11:57:54 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Agrawal", "Shipra", ""], ["Devanur", "Nikhil R.", ""]]}, {"id": "1410.7724", "submitter": "Andreas Pavlogiannis", "authors": "Krishnendu Chatterjee, Rasmus Ibsen-Jensen, Andreas Pavlogiannis,\n  Prateesh Goyal", "title": "Faster Algorithms for Algebraic Path Properties in RSMs with Constant\n  Treewidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interprocedural analysis is at the heart of numerous applications in\nprogramming languages, such as alias analysis, constant propagation, etc.\nRecursive state machines (RSMs) are standard models for interprocedural\nanalysis. We consider a general framework with RSMs where the transitions are\nlabeled from a semiring, and path properties are algebraic with semiring\noperations. RSMs with algebraic path properties can model interprocedural\ndataflow analysis problems, the shortest path problem, the most probable path\nproblem, etc. The traditional algorithms for interprocedural analysis focus on\npath properties where the starting point is \\emph{fixed} as the entry point of\na specific method. In this work, we consider possible multiple queries as\nrequired in many applications such as in alias analysis. The study of multiple\nqueries allows us to bring in a very important algorithmic distinction between\nthe resource usage of the \\emph{one-time} preprocessing vs for \\emph{each\nindividual} query. The second aspect that we consider is that the control flow\ngraphs for most programs have constant treewidth.\n  Our main contributions are simple and implementable algorithms that support\nmultiple queries for algebraic path properties for RSMs that have constant\ntreewidth. Our theoretical results show that our algorithms have small\nadditional one-time preprocessing, but can answer subsequent queries\nsignificantly faster as compared to the current best-known solutions for\nseveral important problems, such as interprocedural reachability and shortest\npath. We provide a prototype implementation for interprocedural reachability\nand intraprocedural shortest path that gives a significant speed-up on several\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 18:27:10 GMT"}, {"version": "v2", "created": "Tue, 25 Nov 2014 21:40:40 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Ibsen-Jensen", "Rasmus", ""], ["Pavlogiannis", "Andreas", ""], ["Goyal", "Prateesh", ""]]}, {"id": "1410.7871", "submitter": "Thomas Dueholm Hansen", "authors": "Oliver Friedmann and Thomas Dueholm Hansen and Uri Zwick", "title": "Errata for: A subexponential lower bound for the Random Facet algorithm\n  for Parity Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Friedmann, Hansen, and Zwick (2011) we claimed that the expected number of\npivoting steps performed by the Random-Facet algorithm of Kalai and of\nMatousek, Sharir, and Welzl is equal to the expected number of pivoting steps\nperformed by Random-Facet^*, a variant of Random-Facet that bases its random\ndecisions on one random permutation. We then obtained a lower bound on the\nexpected number of pivoting steps performed by Random-Facet^* and claimed that\nthe same lower bound holds also for Random-Facet. Unfortunately, the claim that\nthe expected numbers of steps performed by Random-Facet and Random-Facet^* are\nthe same is false. We provide here simple examples that show that the expected\nnumbers of steps performed by the two algorithms are not the same.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 04:17:29 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Friedmann", "Oliver", ""], ["Hansen", "Thomas Dueholm", ""], ["Zwick", "Uri", ""]]}, {"id": "1410.7912", "submitter": "Manuel Malatyali", "authors": "Alexander M\\\"acker, Manuel Malatyali, Friedhelm Meyer auf der Heide", "title": "Online Top-k-Position Monitoring of Distributed Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider n nodes connected to a single coordinator. Each node receives an\nindividual online data stream of numbers and, at any point in time, the\ncoordinator has to know the k nodes currently observing the largest values, for\na given k between 1 and n. We design and analyze an algorithm that solves this\nproblem while bounding the amount of messages exchanged between the nodes and\nthe coordinator. Our algorithm employs the idea of using filters which,\nintuitively speaking, leads to few messages to be sent, if the new input is\n\"similar\" to the previous ones. The algorithm uses a number of messages that is\non expectation by a factor of O((log {\\Delta} + k) log n) larger than that of\nan offline algorithm that sets filters in an optimal way, where {\\Delta} is\nupper bounded by the largest value observed by any node.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 09:38:22 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 12:21:20 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["M\u00e4cker", "Alexander", ""], ["Malatyali", "Manuel", ""], ["der Heide", "Friedhelm Meyer auf", ""]]}, {"id": "1410.7923", "submitter": "Jesper W. Mikkelsen", "authors": "Jesper W. Mikkelsen", "title": "Optimal Online Edge Coloring of Planar Graphs with Advice", "comments": "CIAC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the framework of advice complexity, we study the amount of knowledge\nabout the future that an online algorithm needs to color the edges of a graph\noptimally, i.e., using as few colors as possible. For graphs of maximum degree\n$\\Delta$, it follows from Vizing's Theorem that $O(m\\log \\Delta)$ bits of\nadvice suffice to achieve optimality, where $m$ is the number of edges. We show\nthat for graphs of bounded degeneracy (a class of graphs including e.g. trees\nand planar graphs), only $O(m)$ bits of advice are needed to compute an optimal\nsolution online, independently of how large $\\Delta$ is. On the other hand, we\nshow that $\\Omega (m)$ bits of advice are necessary just to achieve a\ncompetitive ratio better than that of the best deterministic online algorithm\nwithout advice. Furthermore, we consider algorithms which use a fixed number of\nadvice bits per edge (our algorithm for graphs of bounded degeneracy belongs to\nthis class of algorithms). We show that for bipartite graphs, any such\nalgorithm must use at least $\\Omega(m\\log \\Delta)$ bits of advice to achieve\noptimality.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 10:34:01 GMT"}, {"version": "v2", "created": "Thu, 12 Feb 2015 08:20:06 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Mikkelsen", "Jesper W.", ""]]}, {"id": "1410.8205", "submitter": "Fabrizio Frati", "authors": "Timothy M. Chan and Fabrizio Frati and Carsten Gutwenger and Anna\n  Lubiw and Petra Mutzel and Marcus Schaefer", "title": "Drawing Partially Embedded and Simultaneously Planar Graphs", "comments": "Preliminary version appeared at the 22nd International Symposium on\n  Graph Drawing (GD '14)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of constructing planar drawings with few bends for\ntwo related problems, the partially embedded graph problem---to extend a\nstraight-line planar drawing of a subgraph to a planar drawing of the whole\ngraph---and the simultaneous planarity problem---to find planar drawings of two\ngraphs that coincide on shared vertices and edges. In both cases we show that\nif the required planar drawings exist, then there are planar drawings with a\nlinear number of bends per edge and, in the case of simultaneous planarity, a\nconstant number of crossings between every pair of edges. Our proofs provide\nefficient algorithms if the combinatorial embedding of the drawing is given.\nOur result on partially embedded graph drawing generalizes a classic result by\nPach and Wenger which shows that any planar graph can be drawn with a linear\nnumber of bends per edge if the location of each vertex is fixed.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 00:17:58 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Chan", "Timothy M.", ""], ["Frati", "Fabrizio", ""], ["Gutwenger", "Carsten", ""], ["Lubiw", "Anna", ""], ["Mutzel", "Petra", ""], ["Schaefer", "Marcus", ""]]}, {"id": "1410.8336", "submitter": "Marthe Bonamy", "authors": "Marthe Bonamy, Lukasz Kowalik", "title": "A 13k-kernel for Planar Feedback Vertex Set via Region Decomposition", "comments": "22 pages, short version accepted to IPEC'14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a kernel of at most $13k$ vertices for the Planar Feedback Vertex Set\nproblem restricted to planar graphs, i.e., a polynomial-time algorithm that\ntransforms an input instance $(G,k)$ to an equivalent instance with at most\n$13k$ vertices. To this end we introduce a few new reduction rules. However,\nour main contribution is an application of the region decomposition technique\nin the analysis of the kernel size. We show that our analysis is tight, up to a\nconstant additive term.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 11:50:36 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Bonamy", "Marthe", ""], ["Kowalik", "Lukasz", ""]]}, {"id": "1410.8664", "submitter": "Yishi Lin", "authors": "Yishi Lin, John C.S. Lui", "title": "Algorithmic Design for Competitive Influence Maximization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the popularity of the viral marketing campaign in online social\nnetworks, finding an effective method to identify a set of most influential\nnodes so to compete well with other viral marketing competitors is of upmost\nimportance. We propose a \"General Competitive Independent Cascade (GCIC)\" model\nto describe the general influence propagation of two competing sources in the\nsame network. We formulate the \"Competitive Influence Maximization (CIM)\"\nproblem as follows: Under a prespecified influence propagation model and that\nthe competitor's seed set is known, how to find a seed set of $k$ nodes so as\nto trigger the largest influence cascade? We propose a general algorithmic\nframework TCIM for the CIM problem under the GCIC model. TCIM returns a\n$(1-1/e-\\epsilon)$-approximate solution with probability at least\n$1-n^{-\\ell}$, and has an efficient time complexity of $O(c(k+\\ell)(m+n)\\log\nn/\\epsilon^2)$, where $c$ depends on specific propagation model and may also\ndepend on $k$ and underlying network $G$. To the best of our knowledge, this is\nthe first general algorithmic framework that has both $(1-1/e-\\epsilon)$\nperformance guarantee and practical efficiency. We conduct extensive\nexperiments on real-world datasets under three specific influence propagation\nmodels, and show the efficiency and accuracy of our framework. In particular,\nwe achieve up to four orders of magnitude speedup as compared to the previous\nstate-of-the-art algorithms with the approximate guarantee.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 08:16:20 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Lin", "Yishi", ""], ["Lui", "John C. S.", ""]]}, {"id": "1410.8774", "submitter": "Konrad Dabrowski", "authors": "Konrad K. Dabrowski, Dominique de Werra, Vadim V. Lozin, Viktor\n  Zamaraev", "title": "Combinatorics and algorithms for augmenting graphs", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of augmenting graphs generalizes Berge's idea of augmenting\nchains, which was used by Edmonds in his celebrated solution of the maximum\nmatching problem. This problem is a special case of the more general maximum\nindependent set (MIS) problem. Recently, the augmenting graph approach has been\nsuccessfully applied to solve MIS in various other special cases. However, our\nknowledge of augmenting graphs is still very limited, and we do not even know\nwhat the minimal infinite classes of augmenting graphs are. In the present\npaper, we find an answer to this question and apply it to extend the area of\npolynomial-time solvability of the maximum independent set problem.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 15:27:06 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Dabrowski", "Konrad K.", ""], ["de Werra", "Dominique", ""], ["Lozin", "Vadim V.", ""], ["Zamaraev", "Viktor", ""]]}]