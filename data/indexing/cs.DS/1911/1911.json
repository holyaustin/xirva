[{"id": "1911.00038", "submitter": "Ziteng Sun", "authors": "Jayadev Acharya, Keith Bonawitz, Peter Kairouz, Daniel Ramage, Ziteng\n  Sun", "title": "Context-Aware Local Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local differential privacy (LDP) is a strong notion of privacy for individual\nusers that often comes at the expense of a significant drop in utility. The\nclassical definition of LDP assumes that all elements in the data domain are\nequally sensitive. However, in many applications, some symbols are more\nsensitive than others. This work proposes a context-aware framework of local\ndifferential privacy that allows a privacy designer to incorporate the\napplication's context into the privacy definition. For binary data domains, we\nprovide a universally optimal privatization scheme and highlight its\nconnections to Warner's randomized response (RR) and Mangat's improved\nresponse. Motivated by geolocation and web search applications, for $k$-ary\ndata domains, we consider two special cases of context-aware LDP:\nblock-structured LDP and high-low LDP. We study discrete distribution\nestimation and provide communication-efficient, sample-optimal schemes and\ninformation-theoretic lower bounds for both models. We show that using\ncontextual information can require fewer samples than classical LDP to achieve\nthe same accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 18:15:33 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 23:00:22 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Acharya", "Jayadev", ""], ["Bonawitz", "Keith", ""], ["Kairouz", "Peter", ""], ["Ramage", "Daniel", ""], ["Sun", "Ziteng", ""]]}, {"id": "1911.00044", "submitter": "Uwe Baier", "authors": "Uwe Baier, Thomas B\\\"uchler, Enno Ohlebusch, Pascal Weber", "title": "Edge minimization in de Bruijn graphs", "comments": "Accepted for Data Compression Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces the de Bruijn graph edge minimization problem, which is\nrelated to the compression of de Bruijn graphs: find the order-k de Bruijn\ngraph with minimum edge count among all orders. We describe an efficient\nalgorithm that solves this problem. Since the edge minimization problem is\nconnected to the BWT compression technique called \"tunneling\", the paper also\ndescribes a way to minimize the length of a tunneled BWT in such a way that\nuseful properties for sequence analysis are preserved. Although being a\nrestriction, this is significant progress towards a solution to the open\nproblem of finding optimal disjoint blocks that minimize space, as stated in\nAlanko et al. (DCC 2019).\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 18:19:55 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 17:02:26 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 16:14:20 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Baier", "Uwe", ""], ["B\u00fcchler", "Thomas", ""], ["Ohlebusch", "Enno", ""], ["Weber", "Pascal", ""]]}, {"id": "1911.00223", "submitter": "Huanbiao Zhu", "authors": "Huanbiao Zhu and Werner Stuetzle", "title": "A Simple and Efficient Method to Compute a Single Linkage Dendrogram", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of computing a single linkage dendrogram. A possible\napproach is to: (i) Form an edge weighted graph $G$ over the data, with edge\nweights reflecting dissimilarities. (ii) Calculate the MST $T$ of $G$. (iii)\nBreak the longest edge of $T$ thereby splitting it into subtrees $T_L$, $T_R$.\n(iv) Apply the splitting process recursively to the subtrees. This approach has\nthe attractive feature that Prim's algorithm for MST construction calculates\ndistances as needed, and hence there is no need to ever store the inter-point\ndistance matrix. The recursive partitioning algorithm requires us to determine\nthe vertices (and edges) of $T_L$ and $T_R$. We show how this can be done\neasily and efficiently using information generated by Prim's algorithm without\nany additional computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 06:36:51 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Zhu", "Huanbiao", ""], ["Stuetzle", "Werner", ""]]}, {"id": "1911.00336", "submitter": "Roni Mateless", "authors": "Roni Mateless, Michael Segal and Robert Moskovitch", "title": "THAAD: Efficient Matching Queries under Temporal Abstraction for Anomaly\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel algorithm and efficient data structure for\nanomaly detection based on temporal data. Time-series data are represented by a\nsequence of symbolic time intervals, describing increasing and decreasing\ntrends, in a compact way using gradient temporal abstraction technique. Then we\nidentify unusual subsequences in the resulting sequence using dynamic data\nstructure based on the geometric observations supporting polylogarithmic update\nand query times. Moreover, we introduce a new parameter to control the pairwise\ndifference between the corresponding symbols in addition to a distance metric\nbetween the subsequences. Experimental results on a public DNS network traffic\ndataset show the superiority of our approach compared to the baselines.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 12:33:34 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 06:53:53 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Mateless", "Roni", ""], ["Segal", "Michael", ""], ["Moskovitch", "Robert", ""]]}, {"id": "1911.00586", "submitter": "Micha{\\l}  Karpi\\'nski", "authors": "Micha{\\l} Karpi\\'nski", "title": "CNF Encodings of Cardinality Constraints Based on Comparator Networks", "comments": "Phd thesis. Defended 09.09.2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean Satisfiability Problem (SAT) is one of the core problems in computer\nscience. As one of the fundamental NP-complete problems, it can be used - by\nknown reductions - to represent instances of variety of hard decision problems.\nAdditionally, those representations can be passed to a program for finding\nsatisfying assignments to Boolean formulas, for example, to a program called\nMiniSat. Those programs (called SAT-solvers) have been intensively developed\nfor many years and - despite their worst-case exponential time complexity - are\nable to solve a multitude of hard practical instances. A drawback of this\napproach is that clauses are neither expressive, nor compact, and using them to\ndescribe decision problems can pose a big challenge on its own.\n  We can improve this by using high-level constraints as a bridge between a\nproblem at hand and SAT. Such constraints are then automatically translated to\neqisatisfiable Boolean formulas. The main theme of this thesis revolves around\none type of such constraints, namely Boolean Cardinality Constraints (or simply\ncardinality constraints). Cardinality constraints state that at most (at least,\nor exactly) k out of n propositional literals can be true. Such cardinality\nconstraints appear naturally in formulations of different real-world problems\nincluding cumulative scheduling, timetabling or formal hardware verification.\n  The goal of this thesis is to propose and analyze new and efficient methods\nto encode (translate) cardinality constraints into equisatisfiable proposition\nformulas in CNF, such that the resulting SAT instances are small and that the\nSAT-solver runtime is as short as possible.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 20:43:36 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Karpi\u0144ski", "Micha\u0142", ""]]}, {"id": "1911.00595", "submitter": "Young-Hyun Oh", "authors": "Young-Hyun Oh, Hamed Mohammadbagherpoor, Patrick Dreher, Anand Singh,\n  Xianqing Yu, Andy J. Rindos", "title": "Solving Multi-Coloring Combinatorial Optimization Problems Using Hybrid\n  Quantum Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of a good algorithm to solve NP-hard combinatorial approximation\nproblems requires specific domain knowledge about the problems and often needs\na trial-and-error problem solving approach. Graph coloring is one of the\nessential fields to provide an efficient solution for combinatorial\napplications such as flight scheduling, frequency allocation in networking, and\nregister allocation. In particular, some optimization algorithms have been\nproposed to solve the multi-coloring graph problems but most of the cases a\nsimple searching method would be the best approach to find an optimal solution\nfor graph coloring problems. However, this naive approach can increase the\ncomputation cost exponentially as the graph size and the number of colors\nincrease. To mitigate such intolerable overhead, we investigate the methods to\ntake the advantages of quantum computing properties to find a solution for\nmulti-coloring graph problems in polynomial time. We utilize the variational\nquantum eigensolver (VQE) technique and quantum approximate optimization\nalgorithm (QAOA) to find solutions for three combinatorial applications by both\ntransferring each problem model to the corresponding Ising model and by using\nthe calculated Hamiltonian matrices. Our results demonstrate that VQE and QAOA\nalgorithms can find one of the best solutions for each application. Therefore,\nour modeling approach with hybrid quantum algorithms can be applicable for\ncombinatorial problems in various fields to find an optimal solution in\npolynomial time.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 21:27:49 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 20:43:05 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Oh", "Young-Hyun", ""], ["Mohammadbagherpoor", "Hamed", ""], ["Dreher", "Patrick", ""], ["Singh", "Anand", ""], ["Yu", "Xianqing", ""], ["Rindos", "Andy J.", ""]]}, {"id": "1911.00612", "submitter": "Sally Dong", "authors": "Sally Dong, Yin Tat Lee, Kent Quanrud", "title": "Computing Circle Packing Representations of Planar Graphs", "comments": "19 pages, 10 figures. SODA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Circle Packing Theorem states that every planar graph can be represented\nas the tangency graph of a family of internally-disjoint circles. A well-known\ngeneralization is the Primal-Dual Circle Packing Theorem for 3-connected planar\ngraphs. The existence of these representations has widespread applications in\ntheoretical computer science and mathematics; however, the algorithmic aspect\nhas received relatively little attention. In this work, we present an algorithm\nbased on convex optimization for computing a primal-dual circle packing\nrepresentation of maximal planar graphs, i.e. triangulations. This in turn\ngives an algorithm for computing a circle packing representation of any planar\ngraph. Both take $\\widetilde{O}(n \\log(R/\\varepsilon))$ expected run-time to\nproduce a solution that is $\\varepsilon$ close to a true representation, where\n$R$ is the ratio between the maximum and minimum circle radius in the true\nrepresentation.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 23:08:01 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Dong", "Sally", ""], ["Lee", "Yin Tat", ""], ["Quanrud", "Kent", ""]]}, {"id": "1911.00675", "submitter": "Otmar Ertl", "authors": "Otmar Ertl", "title": "ProbMinHash -- A Class of Locality-Sensitive Hash Algorithms for the\n  (Probability) Jaccard Similarity", "comments": "to be published in TKDE, source code available at\n  https://github.com/oertl/probminhash", "journal-ref": null, "doi": "10.1109/TKDE.2020.3021176", "report-no": null, "categories": "cs.DS cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probability Jaccard similarity was recently proposed as a natural\ngeneralization of the Jaccard similarity to measure the proximity of sets whose\nelements are associated with relative frequencies or probabilities. In\ncombination with a hash algorithm that maps those weighted sets to compact\nsignatures which allow fast estimation of pairwise similarities, it constitutes\na valuable method for big data applications such as near-duplicate detection,\nnearest neighbor search, or clustering. This paper introduces a class of\none-pass locality-sensitive hash algorithms that are orders of magnitude faster\nthan the original approach. The performance gain is achieved by calculating\nsignature components not independently, but collectively. Four different\nalgorithms are proposed based on this idea. Two of them are statistically\nequivalent to the original approach and can be used as drop-in replacements.\nThe other two may even improve the estimation error by introducing statistical\ndependence between signature components. Moreover, the presented techniques can\nbe specialized for the conventional Jaccard similarity, resulting in highly\nefficient algorithms that outperform traditional minwise hashing and that are\nable to compete with the state of the art.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 07:58:10 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 21:16:52 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 10:29:53 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Ertl", "Otmar", ""]]}, {"id": "1911.00864", "submitter": "Barton E. Lee", "authors": "Haris Aziz and Barton E. Lee", "title": "Proportionally Representative Participatory Budgeting with Ordinal\n  Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Participatory budgeting (PB) is a democratic paradigm whereby voters decide\non a set of projects to fund with a limited budget. We consider PB in a setting\nwhere voters report ordinal preferences over projects and have (possibly)\nasymmetric weights. We propose proportional representation axioms and clarify\nhow they fit into other preference aggregation settings, such as multi-winner\nvoting and approval-based multi-winner voting. As a result of our study, we\nalso discover a new solution concept for approval-based multi-winner voting,\nwhich we call Inclusion PSC (IPSC). IPSC is stronger than proportional\njustified representation (PJR), incomparable to extended justified\nrepresentation (EJR), and yet compatible with EJR. The well-studied\nProportional Approval Voting (PAV) rule produces a committee that satisfies\nboth EJR and IPSC; however, both these axioms can also be satisfied by an\nalgorithm that runs in polynomial-time.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 11:06:50 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 02:03:29 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Aziz", "Haris", ""], ["Lee", "Barton E.", ""]]}, {"id": "1911.00911", "submitter": "Anindya De", "authors": "Xue Chen, Anindya De and Rocco A. Servedio", "title": "Testing noisy linear functions for sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following basic inference problem: there is an unknown\nhigh-dimensional vector $w \\in \\mathbb{R}^n$, and an algorithm is given access\nto labeled pairs $(x,y)$ where $x \\in \\mathbb{R}^n$ is a measurement and $y = w\n\\cdot x + \\mathrm{noise}$. What is the complexity of deciding whether the\ntarget vector $w$ is (approximately) $k$-sparse? The recovery analogue of this\nproblem --- given the promise that $w$ is sparse, find or approximate the\nvector $w$ --- is the famous sparse recovery problem, with a rich body of work\nin signal processing, statistics, and computer science.\n  We study the decision version of this problem (i.e.~deciding whether the\nunknown $w$ is $k$-sparse) from the vantage point of property testing. Our\nfocus is on answering the following high-level question: when is it possible to\nefficiently test whether the unknown target vector $w$ is sparse versus\nfar-from-sparse using a number of samples which is completely independent of\nthe dimension $n$? We consider the natural setting in which $x$ is drawn from a\ni.i.d.~product distribution $\\mathcal{D}$ over $\\mathbb{R}^n$ and the\n$\\mathrm{noise}$ process is independent of the input $x$. As our main result,\nwe give a general algorithm which solves the above-described testing problem\nusing a number of samples which is completely independent of the ambient\ndimension $n$, as long as $\\mathcal{D}$ is not a Gaussian. In fact, our\nalgorithm is fully noise tolerant, in the sense that for an arbitrary $w$, it\napproximately computes the distance of $w$ to the closest $k$-sparse vector. To\ncomplement this algorithmic result, we show that weakening any of our condition\nmakes it information-theoretically impossible for any algorithm to solve the\ntesting problem with fewer than essentially $\\log n$ samples.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 15:40:47 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Chen", "Xue", ""], ["De", "Anindya", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1911.01074", "submitter": "Alexander V. Hopp", "authors": "Yann Disser, Oliver Friedmann, Alexander V. Hopp", "title": "An Exponential Lower Bound for Zadeh's pivot rule", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question whether the Simplex Algorithm admits an efficient pivot rule\nremains one of the most important open questions in discrete optimization.\nWhile many natural, deterministic pivot rules are known to yield exponential\nrunning times, the random-facet rule was shown to have a subexponential running\ntime. For a long time, Zadeh's rule remained the most prominent candidate for\nthe first deterministic pivot rule with subexponential running time. We present\na lower bound construction that shows that Zadeh's rule is in fact exponential\nin the worst case. Our construction is based on a close relation to the\nStrategy Improvement Algorithm for Parity Games and the Policy Iteration\nAlgorithm for Markov Decision Processes, and we also obtain exponential lower\nbounds for Zadeh's rule in these contexts.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 08:48:37 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 12:03:47 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Disser", "Yann", ""], ["Friedmann", "Oliver", ""], ["Hopp", "Alexander V.", ""]]}, {"id": "1911.01145", "submitter": "Oren Weimann", "authors": "Pawe{\\l} Gawrychowski, Shay Mozes, Oren Weimann", "title": "Minimum Cut in $O(m\\log^2 n)$ Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a randomized algorithm that finds a minimum cut in an undirected\nweighted $m$-edge $n$-vertex graph $G$ with high probability in $O(m \\log^2 n)$\ntime. This is the first improvement to Karger's celebrated $O(m \\log^3 n)$ time\nalgorithm from 1996. Our main technical contribution is a deterministic $O(m\n\\log n)$ time algorithm that, given a spanning tree $T$ of $G$, finds a minimum\ncut of $G$ that 2-respects (cuts two edges of) $T$.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 11:53:33 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 08:48:43 GMT"}, {"version": "v3", "created": "Mon, 16 Dec 2019 10:25:38 GMT"}, {"version": "v4", "created": "Mon, 23 Mar 2020 06:13:03 GMT"}, {"version": "v5", "created": "Mon, 3 Aug 2020 09:18:18 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Mozes", "Shay", ""], ["Weimann", "Oren", ""]]}, {"id": "1911.01169", "submitter": "Omri Ben-Eliezer", "authors": "Omri Ben-Eliezer and Shoham Letzter and Erik Waingarten", "title": "Optimal Adaptive Detection of Monotone Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate adaptive sublinear algorithms for detecting monotone patterns\nin an array. Given fixed $2 \\leq k \\in \\mathbb{N}$ and $\\varepsilon > 0$,\nconsider the problem of finding a length-$k$ increasing subsequence in an array\n$f \\colon [n] \\to \\mathbb{R}$, provided that $f$ is $\\varepsilon$-far from free\nof such subsequences. Recently, it was shown that the non-adaptive query\ncomplexity of the above task is $\\Theta((\\log n)^{\\lfloor \\log_2 k \\rfloor})$.\nIn this work, we break the non-adaptive lower bound, presenting an adaptive\nalgorithm for this problem which makes $O(\\log n)$ queries. This is optimal,\nmatching the classical $\\Omega(\\log n)$ adaptive lower bound by Fischer [2004]\nfor monotonicity testing (which corresponds to the case $k=2$), and implying in\nparticular that the query complexity of testing whether the longest increasing\nsubsequence (LIS) has constant length is $\\Theta(\\log n)$.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 12:45:25 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Ben-Eliezer", "Omri", ""], ["Letzter", "Shoham", ""], ["Waingarten", "Erik", ""]]}, {"id": "1911.01319", "submitter": "Weiming Feng", "authors": "Weiming Feng, Heng Guo, Yitong Yin, Chihao Zhang", "title": "Fast sampling and counting k-SAT solutions in the local lemma regime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new algorithms based on Markov chains to sample and approximately\ncount satisfying assignments to $k$-uniform CNF formulas where each variable\nappears at most $d$ times. For any $k$ and $d$ satisfying $kd<n^{o(1)}$ and\n$k\\ge 20\\log k + 20\\log d + 60$, the new sampling algorithm runs in close to\nlinear time, and the counting algorithm runs in close to quadratic time.\n  Our approach is inspired by Moitra (JACM, 2019) which remarkably utilizes the\nLov\\'{a}sz local lemma in approximate counting. Our main technical contribution\nis to use the local lemma to bypass the connectivity barrier in traditional\nMarkov chain approaches, which makes the well developed MCMC method applicable\non disconnected state spaces such as SAT solutions. The benefit of our approach\nis to avoid the enumeration of local structures and obtain fixed polynomial\nrunning times, even if $k=\\omega(1)$ or $d=\\omega(1)$.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 16:31:39 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Feng", "Weiming", ""], ["Guo", "Heng", ""], ["Yin", "Yitong", ""], ["Zhang", "Chihao", ""]]}, {"id": "1911.01330", "submitter": "Daniel Diroff", "authors": "Daniel J. Diroff (Akvelon, Inc.)", "title": "Bitcoin Coin Selection with Leverage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new Bitcoin coin selection algorithm, \"coin selection with\nleverage\", which aims to improve upon cost savings than that of standard\nknapsack like approaches. Parameters to the new algorithm are available to be\ntuned at the users discretion to address other goals of coin selection. Our\napproach naturally fits as a replacement for the standard knapsack ingredient\nof full coin selection procedures.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 16:48:45 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 23:11:33 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Diroff", "Daniel J.", "", "Akvelon, Inc."]]}, {"id": "1911.01348", "submitter": "Huacheng Yu", "authors": "Huacheng Yu", "title": "Nearly Optimal Static Las Vegas Succinct Dictionary", "comments": "preliminary version appeared in STOC'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $S$ of $n$ (distinct) keys from key space $[U]$, each associated\nwith a value from $\\Sigma$, the \\emph{static dictionary} problem asks to\npreprocess these (key, value) pairs into a data structure, supporting\nvalue-retrieval queries: for any given $x\\in [U]$, $\\mathtt{valRet}(x)$ must\nreturn the value associated with $x$ if $x\\in S$, or return $\\bot$ if $x\\notin\nS$. The special case where $|\\Sigma|=1$ is called the \\emph{membership}\nproblem. The \"textbook\" solution is to use a hash table, which occupies linear\nspace and answers each query in constant time. On the other hand, the minimum\npossible space to encode all (key, value) pairs is only $\\mathtt{OPT}:=\n\\lceil\\lg_2\\binom{U}{n}+n\\lg_2|\\Sigma|\\rceil$ bits, which could be much less.\n  In this paper, we design a randomized dictionary data structure using\n$\\mathtt{OPT}+\\mathrm{poly}\\lg n+O(\\lg\\lg\\lg\\lg\\lg U)$ bits of space, and it\nhas \\emph{expected constant} query time, assuming the query algorithm can\naccess an external lookup table of size $n^{0.001}$. The lookup table depends\nonly on $U$, $n$ and $|\\Sigma|$, and not the input. Previously, even for\nmembership queries and $U\\leq n^{O(1)}$, the best known data structure with\nconstant query time requires $\\mathtt{OPT}+n/\\mathrm{poly}\\lg n$ bits of space\n(Pagh [Pag01] and P\\v{a}tra\\c{s}cu [Pat08]); the best-known using\n$\\mathtt{OPT}+n^{0.999}$ space has query time $O(\\lg n)$; the only known\nnon-trivial data structure with $\\mathtt{OPT}+n^{0.001}$ space has $O(\\lg n)$\nquery time and requires a lookup table of size $\\geq n^{2.99}$ (!). Our new\ndata structure answers open questions by P\\v{a}tra\\c{s}cu and Thorup\n[Pat08,Tho13].\n  We also present a scheme that compresses a sequence $X\\in\\Sigma^n$ to its\nzeroth order (empirical) entropy up to $|\\Sigma|\\cdot\\mathrm{poly}\\lg n$ extra\nbits, supporting decoding each $X_i$ in $O(\\lg |\\Sigma|)$ expected time.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 17:18:44 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 15:06:40 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Yu", "Huacheng", ""]]}, {"id": "1911.01351", "submitter": "Huacheng Yu", "authors": "Josh Alman, Huacheng Yu", "title": "Faster Update Time for Turnstile Streaming Algorithms", "comments": "To appear in SODA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new algorithm for maintaining linear sketches in\nturnstile streams with faster update time. As an application, we show that\n$\\log n$ \\texttt{Count} sketches or \\texttt{CountMin} sketches with a constant\nnumber of columns (i.e., buckets) can be implicitly maintained in\n\\emph{worst-case} $O(\\log^{0.582} n)$ update time using $O(\\log n)$ words of\nspace, on a standard word RAM with word-size $w=\\Theta(\\log n)$. The exponent\n$0.582\\approx 2\\omega/3-1$, where $\\omega$ is the current matrix multiplication\nexponent. Due to the numerous applications of linear sketches, our algorithm\nimproves the update time for many streaming problems in turnstile streams, in\nthe high success probability setting, without using more space, including\n$\\ell_2$ norm estimation, $\\ell_2$ heavy hitters, point query with $\\ell_1$ or\n$\\ell_2$ error, etc. Our algorithm generalizes, with the same update time and\nspace, to maintaining $\\log n$ linear sketches, where each sketch partitions\nthe coordinates into $k<\\log^{o(1)} n$ buckets using a $c$-wise independent\nhash function for constant $c$, and maintains the sum of coordinates for each\nbucket. Moreover, if arbitrary word operations are allowed, the update time can\nbe further improved to $O(\\log^{0.187} n)$, where $0.187\\approx \\omega/2-1$.\nOur update algorithm is adaptive, and it circumvents the non-adaptive\ncell-probe lower bounds for turnstile streaming algorithms by Larsen, Nelson\nand Nguy{\\^{e}}n (STOC'15).\n  On the other hand, our result also shows that proving unconditional\ncell-probe lower bound for the update time seems very difficult, even if the\nspace is restricted to be (nearly) the optimum. If $\\omega=2$, the cell-probe\nupdate time of our algorithm would be $\\log^{o(1)} n$. Hence, proving any\nhigher lower bound would imply $\\omega>2$.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 17:28:06 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Alman", "Josh", ""], ["Yu", "Huacheng", ""]]}, {"id": "1911.01374", "submitter": "Tanmay Inamdar", "authors": "Chandra Chekuri, Tanmay Inamdar", "title": "Algorithms for Intersection Graphs of Multiple Intervals and Pseudo\n  Disks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intersection graphs of planar geometric objects such as intervals, disks,\nrectangles and pseudo-disks are well studied. Motivated by various\napplications, Butman et al. in SODA 2007 considered algorithmic questions in\nintersection graphs of $t$-intervals. A $t$-interval is a union of at most $t$\ndistinct intervals (here $t$ is a parameter) -- these graphs are referred to as\nMultiple-Interval Graphs. Subsequent work by Kammer et al. in Approx 2010 also\nconsidered $t$-disks and other geometric shapes. In this paper we revisit some\nof these algorithmic questions via more recent developments in computational\ngeometry. For the minimum weight dominating set problem, we give a simple $O(t\n\\log t)$ approximation for Multiple-Interval Graphs, improving on the\npreviously known bound of $t^2$ . We also show that it is NP-hard to obtain an\n$o(t)$-approximation in this case. In fact, our results hold for the\nintersection graph of a set of t-pseudo-disks which is a much larger class. We\nobtain an ${\\Omega}(1/t)$-approximation for the maximum weight independent set\nin the intersection graph of $t$-pseudo-disks. Our results are based on simple\nreductions to existing algorithms by appropriately bounding the union\ncomplexity of the objects under consideration.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 18:03:21 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Chekuri", "Chandra", ""], ["Inamdar", "Tanmay", ""]]}, {"id": "1911.01402", "submitter": "Xiaolan Gu", "authors": "Xiaolan Gu, Ming Li, Li Xiong, Yang Cao", "title": "Providing Input-Discriminative Protection for Local Differential Privacy", "comments": "This is a full version of our paper that appears in the 36th IEEE\n  International Conference on Data Engineering (ICDE 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local Differential Privacy (LDP) provides provable privacy protection for\ndata collection without the assumption of the trusted data server. In the\nreal-world scenario, different data have different privacy requirements due to\nthe distinct sensitivity levels. However, LDP provides the same protection for\nall data. In this paper, we tackle the challenge of providing\ninput-discriminative protection to reflect the distinct privacy requirements of\ndifferent inputs. We first present the Input-Discriminative LDP (ID-LDP)\nprivacy notion and focus on a specific version termed MinID-LDP, which is shown\nto be a fine-grained version of LDP. Then, we focus on the application of\nfrequency estimation and develop the IDUE mechanism based on Unary Encoding for\nsingle-item input and the extended mechanism IDUE-PS (with Padding-and-Sampling\nprotocol) for item-set input. The results on both synthetic and real-world\ndatasets validate the correctness of our theoretical analysis and show that the\nproposed mechanisms satisfying MinID-LDP have better utility than the\nstate-of-the-art mechanisms satisfying LDP due to the input-discriminative\nprotection.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 18:48:04 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 18:42:41 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Gu", "Xiaolan", ""], ["Li", "Ming", ""], ["Xiong", "Li", ""], ["Cao", "Yang", ""]]}, {"id": "1911.01411", "submitter": "Sidhanth Mohanty", "authors": "Sidhanth Mohanty, Prasad Raghavendra, Jeff Xu", "title": "Lifting Sum-of-Squares Lower Bounds: Degree-$2$ to Degree-$4$", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The degree-$4$ Sum-of-Squares (SoS) SDP relaxation is a powerful algorithm\nthat captures the best known polynomial time algorithms for a broad range of\nproblems including MaxCut, Sparsest Cut, all MaxCSPs and tensor PCA. Despite\nbeing an explicit algorithm with relatively low computational complexity, the\nlimits of degree-$4$ SoS SDP are not well understood. For example, existing\nintegrality gaps do not rule out a $(2-\\varepsilon)$-algorithm for Vertex Cover\nor a $(0.878+\\varepsilon)$-algorithm for MaxCut via degree-$4$ SoS SDPs, each\nof which would refute the notorious Unique Games Conjecture.\n  We exhibit an explicit mapping from solutions for degree-$2$ Sum-of-Squares\nSDP (Goemans-Williamson SDP) to solutions for the degree-$4$ Sum-of-Squares SDP\nrelaxation on boolean variables. By virtue of this mapping, one can lift lower\nbounds for degree-$2$ SoS SDP relaxation to corresponding lower bounds for\ndegree-$4$ SoS SDPs. We use this approach to obtain degree-$4$ SoS SDP lower\nbounds for MaxCut on random $d$-regular graphs, Sherington-Kirkpatrick model\nfrom statistical physics and PSD Grothendieck problem.\n  Our constructions use the idea of pseudocalibration towards candidate SDP\nvectors, while it was previously only used to produce the candidate matrix\nwhich one would show is PSD using much technical work. In addition, we develop\na different technique to bound the spectral norms of _graphical matrices_ that\narise in the context of SoS SDPs. The technique is much simpler and yields\nbetter bounds in many cases than the _trace method_ -- which was the sole\ntechnique for this purpose.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 18:53:35 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Mohanty", "Sidhanth", ""], ["Raghavendra", "Prasad", ""], ["Xu", "Jeff", ""]]}, {"id": "1911.01414", "submitter": "Chaim Even-Zohar", "authors": "Chaim Even-Zohar, Calvin Leng", "title": "Counting Small Permutation Patterns", "comments": null, "journal-ref": "SODA 2021", "doi": null, "report-no": null, "categories": "cs.DS math.CO stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sample of n generic points in the xy-plane defines a permutation that\nrelates their ranks along the two axes. Every subset of k points similarly\ndefines a pattern, which occurs in that permutation. The number of occurrences\nof small patterns in a large permutation arises in many areas, including\nnonparametric statistics. It is therefore desirable to count them more\nefficiently than the straightforward ~O(n^k) time algorithm.\n  This work proposes new algorithms for counting patterns. We show that all\npatterns of order 2 and 3, as well as eight patterns of order 4, can be counted\nin nearly linear time. To that end, we develop an algebraic framework that we\ncall corner tree formulas. Our approach generalizes the existing methods and\nallows a systematic study of their scope.\n  Using the machinery of corner trees, we find twenty-three independent linear\ncombinations of order-4 patterns, that can be computed in time ~O(n). We also\ndescribe an algorithm that counts one of the remaining 4-patterns, and hence\nall 4-patterns, in time ~O(n^(3/2)).\n  As a practical application, we provide a nearly linear time computation of a\nstatistic by Yanagimoto (1970), Bergsma and Dassios (2010). This statistic\nyields a natural and strongly consistent variant of Hoeffding's test for\nindependence of X and Y, given a random sample as above. This improves upon the\nso far most efficient ~O(n^2) algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 18:57:04 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 18:55:43 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2020 18:38:43 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Even-Zohar", "Chaim", ""], ["Leng", "Calvin", ""]]}, {"id": "1911.01420", "submitter": "Neeraj Bokde", "authors": "Marc Agenis-Nevers, Neeraj Dhanraj Bokde, Zaher Mundher Yaseen, Mayur\n  Shende", "title": "An empirical estimation for time and memory algorithm complexities:\n  newly developed R package", "comments": null, "journal-ref": "Multimedia Tools and Applications (2020)", "doi": "10.1007/s11042-020-09471-8", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article introduces GuessCompx which is an R package that performs an\nempirical estimation on the time and memory complexities of an algorithm or a\nfunction. It tests multiple increasing-sizes samples of the user's data and\nattempts to fit one of seven complexity functions: O(N), O(N^2), O(log(N)),\netc. Based on a best fit procedure using LOO-MSE (leave one out-mean squared\nerror), it also predicts the full computation time and memory usage on the\nwhole dataset. Conceptually, it relies on the base R functions system.time and\nmemory.size, the latter being only suitable for Windows users. Together with\nthis results, a plot and a significance test are returned. Complexity is\nassessed with regard to the user's actual dataset through its size (and no\nother parameter). This article provides several examples demonstrating several\ncases (e.g., distance function, time series and custom function) and optimal\nparameters tuning. The subject of the empirical computational complexity has\nbeen relatively little studied in computer sciences, and such a package\nprovides a reliable, convenient and simple procedure for estimation process.\nFurther, the package does not require to have the code of the target function.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 12:51:09 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 08:39:40 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Agenis-Nevers", "Marc", ""], ["Bokde", "Neeraj Dhanraj", ""], ["Yaseen", "Zaher Mundher", ""], ["Shende", "Mayur", ""]]}, {"id": "1911.01452", "submitter": "Matthew Joseph", "authors": "Kareem Amin, Matthew Joseph, and Jieming Mao", "title": "Pan-Private Uniformity Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A centrally differentially private algorithm maps raw data to differentially\nprivate outputs. In contrast, a locally differentially private algorithm may\nonly access data through public interaction with data holders, and this\ninteraction must be a differentially private function of the data. We study the\nintermediate model of pan-privacy. Unlike a locally private algorithm, a\npan-private algorithm receives data in the clear. Unlike a centrally private\nalgorithm, the algorithm receives data one element at a time and must maintain\na differentially private internal state while processing this stream.\n  First, we show that pure pan-privacy against multiple intrusions on the\ninternal state is equivalent to sequentially interactive local privacy. Next,\nwe contextualize pan-privacy against a single intrusion by analyzing the sample\ncomplexity of uniformity testing over domain $[k]$. Focusing on the dependence\non $k$, centrally private uniformity testing has sample complexity\n$\\Theta(\\sqrt{k})$, while noninteractive locally private uniformity testing has\nsample complexity $\\Theta(k)$. We show that the sample complexity of pure\npan-private uniformity testing is $\\Theta(k^{2/3})$. By a new $\\Omega(k)$ lower\nbound for the sequentially interactive setting, we also separate pan-private\nfrom sequentially interactive locally private and multi-intrusion pan-private\nuniformity testing.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 19:06:29 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 18:16:33 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 13:45:25 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Amin", "Kareem", ""], ["Joseph", "Matthew", ""], ["Mao", "Jieming", ""]]}, {"id": "1911.01462", "submitter": "Surbhi Goel", "authors": "Surbhi Goel, Sushrut Karmalkar, Adam Klivans", "title": "Time/Accuracy Tradeoffs for Learning a ReLU with respect to Gaussian\n  Marginals", "comments": "To appear in NeurIPS 2019 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing the best-fitting ReLU with respect to\nsquare-loss on a training set when the examples have been drawn according to a\nspherical Gaussian distribution (the labels can be arbitrary). Let\n$\\mathsf{opt} < 1$ be the population loss of the best-fitting ReLU. We prove:\n  1. Finding a ReLU with square-loss $\\mathsf{opt} + \\epsilon$ is as hard as\nthe problem of learning sparse parities with noise, widely thought to be\ncomputationally intractable. This is the first hardness result for learning a\nReLU with respect to Gaussian marginals, and our results imply -{\\emph\nunconditionally}- that gradient descent cannot converge to the global minimum\nin polynomial time.\n  2. There exists an efficient approximation algorithm for finding the\nbest-fitting ReLU that achieves error $O(\\mathsf{opt}^{2/3})$. The algorithm\nuses a novel reduction to noisy halfspace learning with respect to $0/1$ loss.\n  Prior work due to Soltanolkotabi [Sol17] showed that gradient descent can\nfind the best-fitting ReLU with respect to Gaussian marginals, if the training\nset is exactly labeled by a ReLU.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 19:35:49 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Goel", "Surbhi", ""], ["Karmalkar", "Sushrut", ""], ["Klivans", "Adam", ""]]}, {"id": "1911.01465", "submitter": "Iyad Kanj", "authors": "Eduard Eiben, Robert Ganian, Iyad Kanj, Sebastian Ordyniak, Stefan\n  Szeider", "title": "The Parameterized Complexity of Clustering Incomplete Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study fundamental clustering problems for incomplete data. Specifically,\ngiven a set of incomplete d-dimensional vectors (representing rows of a\nmatrix), the goal is to complete the missing vector entries in a way that\nadmits a partitioning of the vectors into at most $k$ clusters with radius or\ndiameter at most r. We give tight characterizations of the parameterized\ncomplexity of these problems with respect to the parameters k, r, and the\nminimum number of rows and columns needed to cover all the missing entries. We\nshow that the considered problems are fixed-parameter tractable when\nparameterized by the three parameters combined, and that dropping any of the\nthree parameters results in parameterized intractability. A byproduct of our\nresults is that, for the complete data setting, all problems under\nconsideration are fixed-parameter tractable parameterized by k+r.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 19:43:47 GMT"}, {"version": "v2", "created": "Sun, 10 Nov 2019 13:10:03 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 17:01:27 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Eiben", "Eduard", ""], ["Ganian", "Robert", ""], ["Kanj", "Iyad", ""], ["Ordyniak", "Sebastian", ""], ["Szeider", "Stefan", ""]]}, {"id": "1911.01469", "submitter": "Andre Wibisono", "authors": "Andre Wibisono", "title": "Proximal Langevin Algorithm: Rapid Convergence Under Isoperimetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Proximal Langevin Algorithm (PLA) for sampling from a\nprobability distribution $\\nu = e^{-f}$ on $\\mathbb{R}^n$ under isoperimetry.\nWe prove a convergence guarantee for PLA in Kullback-Leibler (KL) divergence\nwhen $\\nu$ satisfies log-Sobolev inequality (LSI) and $f$ has bounded second\nand third derivatives. This improves on the result for the Unadjusted Langevin\nAlgorithm (ULA), and matches the fastest known rate for sampling under LSI\n(without Metropolis filter) with a better dependence on the LSI constant. We\nalso prove convergence guarantees for PLA in R\\'enyi divergence of order $q >\n1$ when the biased limit satisfies either LSI or Poincar\\'e inequality.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 19:57:38 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Wibisono", "Andre", ""]]}, {"id": "1911.01503", "submitter": "Gregory Herschlag", "authors": "Eric Autrey and Daniel Carter and Gregory Herschlag and Zach Hunter\n  and Jonathan C. Mattingly", "title": "Metropolized Forest Recombination for Monte Carlo Sampling of Graph\n  Partitions", "comments": "29 pages; 14 figures; 3 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new Markov chain on graph partitions that makes relatively\nglobal moves yet is computationally feasible to be used as the proposal in the\nMetropolis-Hastings method. Our resulting algorithm can be made reversible and\nable to sample from a specified measure on partitions. Both of these properties\nare critical to some important applications and computational Bayesian\nstatistics in general. Our proposal chain modifies the recently developed\nmethod called Recombination (ReCom), which draws spanning trees on joined\npartitions and then randomly cuts them to repartition. We improve the\ncomputational efficiency by augmenting the state space from partitions to\nspanning forests. The extra information accelerates the computation of the\nforward and backward proposal probabilities. We demonstrate this method by\nsampling redistricting plans and find promising convergence results on several\nkey observables of interest.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 16:41:15 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 15:32:06 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Autrey", "Eric", ""], ["Carter", "Daniel", ""], ["Herschlag", "Gregory", ""], ["Hunter", "Zach", ""], ["Mattingly", "Jonathan C.", ""]]}, {"id": "1911.01504", "submitter": "Ewan Davies", "authors": "Matthew Coulson, Ewan Davies, Alexandra Kolla, Viresh Patel, and Guus\n  Regts", "title": "Statistical physics approaches to Unique Games", "comments": "26 pages", "journal-ref": null, "doi": "10.4230/LIPIcs.CCC.2020.13", "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how two techniques from statistical physics can be adapted to solve a\nvariant of the notorious Unique Games problem, potentially opening new avenues\ntowards the Unique Games Conjecture. The variant, which we call Count Unique\nGames, is a promise problem in which the \"yes\" case guarantees a certain number\nof highly satisfiable assignments to the Unique Games instance. In the standard\nUnique Games problem, the \"yes\" case only guarantees at least one such\nassignment. We exhibit efficient algorithms for Count Unique Games based on\napproximating a suitable partition function for the Unique Games instance via\n(i) a zero-free region and polynomial interpolation, and (ii) the cluster\nexpansion. We also show that a modest improvement to the parameters for which\nwe give results would refute the Unique Games Conjecture.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 21:58:35 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Coulson", "Matthew", ""], ["Davies", "Ewan", ""], ["Kolla", "Alexandra", ""], ["Patel", "Viresh", ""], ["Regts", "Guus", ""]]}, {"id": "1911.01508", "submitter": "Siddharth Krishna", "authors": "Siddharth Krishna, Michael Emmi, Constantin Enea, Dejan Jovanovic", "title": "Verifying Visibility-Based Weak Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multithreaded programs generally leverage efficient and thread-safe\nconcurrent objects like sets, key-value maps, and queues. While some\nconcurrent-object operations are designed to behave atomically, each witnessing\nthe atomic effects of predecessors in a linearization order, others forego such\nstrong consistency to avoid complex control and synchronization bottlenecks.\nFor example, contains (value) methods of key-value maps may iterate through\nkey-value entries without blocking concurrent updates, to avoid unwanted\nperformance bottlenecks, and consequently overlook the effects of some\nlinearization-order predecessors. While such weakly-consistent operations may\nnot be atomic, they still offer guarantees, e.g., only observing values that\nhave been present.\n  In this work we develop a methodology for proving that concurrent object\nimplementations adhere to weak-consistency specifications. In particular, we\nconsider (forward) simulation-based proofs of implementations against\nrelaxed-visibility specifications, which allow designated operations to\noverlook some of their linearization-order predecessors, i.e., behaving as if\nthey never occurred. Besides annotating implementation code to identify\nlinearization points, i.e., points at which operations' logical effects occur,\nwe also annotate code to identify visible operations, i.e., operations whose\neffects are observed; in practice this annotation can be done automatically by\ntracking the writers to each accessed memory location. We formalize our\nmethodology over a general notion of transition systems, agnostic to any\nparticular programming language or memory model, and demonstrate its\napplication, using automated theorem provers, by verifying models of Java\nconcurrent object implementations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 22:13:43 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Krishna", "Siddharth", ""], ["Emmi", "Michael", ""], ["Enea", "Constantin", ""], ["Jovanovic", "Dejan", ""]]}, {"id": "1911.01592", "submitter": "Marcin Bienkowski", "authors": "Marcin Bienkowski, Jaros{\\l}aw Byrka, Christian Coester, {\\L}ukasz\n  Je\\.z", "title": "Unbounded lower bound for k-server against weak adversaries", "comments": "To appear in STOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the resource augmented version of the $k$-server problem, also known\nas the $k$-server problem against weak adversaries or the $(h,k)$-server\nproblem. In this setting, an online algorithm using $k$ servers is compared to\nan offline algorithm using $h$ servers, where $h\\le k$. For uniform metrics, it\nhas been known since the seminal work of Sleator and Tarjan (1985) that for any\n$\\epsilon>0$, the competitive ratio drops to a constant if $k=(1+\\epsilon)\n\\cdot h$. This result was later generalized to weighted stars (Young 1994) and\ntrees of bounded depth (Bansal et al. 2017). The main open problem for this\nsetting is whether a similar phenomenon occurs on general metrics.\n  We resolve this question negatively. With a simple recursive construction, we\nshow that the competitive ratio is at least $\\Omega(\\log \\log h)$, even as\n$k\\to\\infty$. Our lower bound holds for both deterministic and randomized\nalgorithms. It also disproves the existence of a competitive algorithm for the\ninfinite server problem on general metrics.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 03:20:15 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 23:17:47 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Bienkowski", "Marcin", ""], ["Byrka", "Jaros\u0142aw", ""], ["Coester", "Christian", ""], ["Je\u017c", "\u0141ukasz", ""]]}, {"id": "1911.01626", "submitter": "Jason Li", "authors": "Jason Li", "title": "Faster Parallel Algorithm for Approximate Shortest Path", "comments": "53 pages, STOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the first $m\\,\\text{polylog}(n)$ work, $\\text{polylog}(n)$ time\nalgorithm in the PRAM model that computes $(1+\\epsilon)$-approximate\nsingle-source shortest paths on weighted, undirected graphs. This improves upon\nthe breakthrough result of Cohen~[JACM'00] that achieves $O(m^{1+\\epsilon_0})$\nwork and $\\text{polylog}(n)$ time. While most previous approaches, including\nCohen's, leveraged the power of hopsets, our algorithm builds upon the recent\ndevelopments in \\emph{continuous optimization}, studying the shortest path\nproblem from the lens of the closely-related \\emph{minimum transshipment}\nproblem. To obtain our algorithm, we demonstrate a series of near-linear work,\npolylogarithmic-time reductions between the problems of approximate shortest\npath, approximate transshipment, and $\\ell_1$-embeddings, and establish a\nrecursive algorithm that cycles through the three problems and reduces the\ngraph size on each cycle. As a consequence, we also obtain faster parallel\nalgorithms for approximate transshipment and $\\ell_1$-embeddings with\npolylogarithmic distortion. The minimum transshipment algorithm in particular\nimproves upon the previous best $m^{1+o(1)}$ work sequential algorithm of\nSherman~[SODA'17].\n  To improve readability, the paper is almost entirely self-contained, save for\nseveral staple theorems in algorithms and combinatorics.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 05:28:08 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 06:39:59 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 07:48:00 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2020 07:47:12 GMT"}, {"version": "v5", "created": "Thu, 10 Jun 2021 05:47:12 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Li", "Jason", ""]]}, {"id": "1911.01632", "submitter": "Evangelia Gergatsouli", "authors": "Shuchi Chawla, Evangelia Gergatsouli, Yifeng Teng, Christos Tzamos,\n  Ruimin Zhang", "title": "Pandora's Box with Correlations: Learning and Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Pandora's Box problem and its extensions capture optimization problems\nwith stochastic input where the algorithm can obtain instantiations of input\nrandom variables at some cost. To our knowledge, all previous work on this\nclass of problems assumes that different random variables in the input are\ndistributed independently. As such it does not capture many real-world\nsettings. In this paper, we provide the first approximation algorithms for\nPandora's Box-type problems with correlations. We assume that the algorithm has\naccess to samples drawn from the joint distribution on input.\n  Algorithms for these problems must determine an order in which to probe\nrandom variables, as well as when to stop and return the best solution found so\nfar. In general, an optimal algorithm may make both decisions adaptively based\non instantiations observed previously. Such fully adaptive (FA) strategies\ncannot be efficiently approximated to within any sublinear factor with sample\naccess. We therefore focus on the simpler objective of approximating partially\nadaptive (PA) strategies that probe random variables in a fixed predetermined\norder but decide when to stop based on the instantiations observed. We consider\na number of different feasibility constraints and provide simple PA strategies\nthat are approximately optimal with respect to the best PA strategy for each\ncase. All of our algorithms have polynomial sample complexity. We further show\nthat our results are tight within constant factors: better factors cannot be\nachieved even using the full power of FA strategies.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 05:59:01 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 06:09:11 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Chawla", "Shuchi", ""], ["Gergatsouli", "Evangelia", ""], ["Teng", "Yifeng", ""], ["Tzamos", "Christos", ""], ["Zhang", "Ruimin", ""]]}, {"id": "1911.01644", "submitter": "Geonmo Gu", "authors": "Geonmo Gu, Siwoo Song, Simone Faro, Thierry Lecroq, Kunsoo Park", "title": "Fast Multiple Pattern Cartesian Tree Matching", "comments": "Submitted to WALCOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cartesian tree matching is the problem of finding all substrings in a given\ntext which have the same Cartesian trees as that of a given pattern. In this\npaper, we deal with Cartesian tree matching for the case of multiple patterns.\nWe present two fingerprinting methods, i.e., the parent-distance encoding and\nthe binary encoding. By combining an efficient fingerprinting method and a\nconventional multiple string matching algorithm, we can efficiently solve\nmultiple pattern Cartesian tree matching. We propose three practical algorithms\nfor multiple pattern Cartesian tree matching based on the Wu-Manber algorithm,\nthe Rabin-Karp algorithm, and the Alpha Skip Search algorithm, respectively. In\nthe experiments we compare our solutions against the previous algorithm [18].\nOur solutions run faster than the previous algorithm as the pattern lengths\nincrease. Especially, our algorithm based on Wu-Manber runs up to 33 times\nfaster.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 06:59:56 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Gu", "Geonmo", ""], ["Song", "Siwoo", ""], ["Faro", "Simone", ""], ["Lecroq", "Thierry", ""], ["Park", "Kunsoo", ""]]}, {"id": "1911.01651", "submitter": "Sagnik Mukhopadhyay", "authors": "Sagnik Mukhopadhyay, Danupon Nanongkai", "title": "Weighted Min-Cut: Sequential, Cut-Query and Streaming Algorithms", "comments": "Updates on this version: (1) Minor corrections in Section 5.1, 5.2;\n  (2) Reference to newer results by GMW SOSA21 (arXiv:2008.02060v2), DEMN\n  STOC21 (arXiv:2004.09129v2) and LMN 21 (arXiv:2102.06565v1)", "journal-ref": null, "doi": "10.1145/3357713.3384334", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Consider the following 2-respecting min-cut problem. Given a weighted graph\n$G$ and its spanning tree $T$, find the minimum cut among the cuts that contain\nat most two edges in $T$. This problem is an important subroutine in Karger's\ncelebrated randomized near-linear-time min-cut algorithm [STOC'96]. We present\na new approach for this problem which can be easily implemented in many\nsettings, leading to the following randomized min-cut algorithms for weighted\ngraphs.\n  * An $O(m\\frac{\\log^2 n}{\\log\\log n} + n\\log^6 n)$-time sequential algorithm:\nThis improves Karger's $O(m \\log^3 n)$ and $O(m\\frac{(\\log^2 n)\\log\n(n^2/m)}{\\log\\log n} + n\\log^6 n)$ bounds when the input graph is not extremely\nsparse or dense. Improvements over Karger's bounds were previously known only\nunder a rather strong assumption that the input graph is simple [Henzinger et\nal. SODA'17; Ghaffari et al. SODA'20]. For unweighted graphs with parallel\nedges, our bound can be improved to $O(m\\frac{\\log^{1.5} n}{\\log\\log n} +\nn\\log^6 n)$.\n  * An algorithm requiring $\\tilde O(n)$ cut queries to compute the min-cut of\na weighted graph: This answers an open problem by Rubinstein et al. ITCS'18,\nwho obtained a similar bound for simple graphs.\n  * A streaming algorithm that requires $\\tilde O(n)$ space and $O(\\log n)$\npasses to compute the min-cut: The only previous non-trivial exact min-cut\nalgorithm in this setting is the 2-pass $\\tilde O(n)$-space algorithm on simple\ngraphs [Rubinstein et al., ITCS'18] (observed by Assadi et al. STOC'19).\n  In contrast to Karger's 2-respecting min-cut algorithm which deploys\nsophisticated dynamic programming techniques, our approach exploits some cute\nstructural properties so that it only needs to compute the values of $\\tilde\nO(n)$ cuts corresponding to removing $\\tilde O(n)$ pairs of tree edges, an\noperation that can be done quickly in many settings.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 07:38:26 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 12:36:06 GMT"}, {"version": "v3", "created": "Mon, 4 May 2020 15:02:59 GMT"}, {"version": "v4", "created": "Tue, 5 May 2020 06:54:19 GMT"}, {"version": "v5", "created": "Thu, 18 Feb 2021 09:20:58 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Mukhopadhyay", "Sagnik", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1911.01663", "submitter": "Yang Li", "authors": "Yang Li, Junbin Gao, Mingyuan Bai, Chengjun Li and Gang Liu", "title": "A Heuristic Algorithm Based on Tour Rebuilding Operator for the\n  Traveling Salesman Problem", "comments": "The value of Euclidean distance is not rounded, so there is some\n  error in the result. This paper is currently under revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TSP (Traveling Salesman Problem), a classic NP-complete problem in\ncombinatorial optimization, is of great significance in multiple fields. Exact\nalgorithms for TSP are not practical due to their exponential time cost. Thus,\napproximate algorithms become the research focus and can be further divided\ninto two types, tour construction algorithms and tour improvement algorithms.\nResearches show that the latter type of algorithms can obtain better solutions\nthan the former one. However, traditional tour improvement algorithms have\nshortcomings. They converge very slowly and tend to be trapped in local optima.\nIn practice, tour construction algorithms are often used in initialization of\ntour improvement algorithms to speed up convergence. Nevertheless, such a\ncombination leads to no improvement on quality of solutions. In this paper, a\nheuristic algorithm based on the new tour rebuilding operator is proposed. The\nalgorithm features rapid convergence and powerful global search. In the\nexperiments based on 40 instances in TSPLIB, the best known solutions of 22\ninstances are refreshed by the proposed method. Meanwhile, the best known\nsolutions of the other 18 instances are obtained.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 08:15:58 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 07:00:07 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Li", "Yang", ""], ["Gao", "Junbin", ""], ["Bai", "Mingyuan", ""], ["Li", "Chengjun", ""], ["Liu", "Gang", ""]]}, {"id": "1911.01763", "submitter": "Rahat Yeasin Emon", "authors": "Rahat Yeasin Emon, Sharmistha Chanda Tista", "title": "An Efficient Word Lookup System by using Improved Trie Algorithm", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently word storing and searching is an important task in computer\nscience. An application space complexity, time complexity, and overall\nperformance depend on this string data. Many word searching data structures and\nalgorithms exist in the current world but few of them have space compress\nability. Trie is a popular data structure for word searching for its linear\nsearching capability. It is the basic and important part of various computer\napplications such as information retrieval, natural language processing,\ndatabase system, compiler, and computer network. But currently, the available\nversion of trie tree cannot be used widely because of its high memory\nrequirement. This paper proposes a new Radix trie based data structure for word\nstoring and searching which can share not only just prefix but also infix and\nsuffix and thus reduces memory requirement. We propose a new emptiness property\nto Radix trie. Proposed trie has character cell reduction capability and it can\ndramatically reduce any application runtime memory size. Using it as data tank\nto an operating system the overall main memory requirement of a device can be\nreduced to a large extent.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 13:36:15 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Emon", "Rahat Yeasin", ""], ["Tista", "Sharmistha Chanda", ""]]}, {"id": "1911.01839", "submitter": "Soheil Behnezhad", "authors": "Soheil Behnezhad, Jakub {\\L}\\k{a}cki, Vahab Mirrokni", "title": "Fully Dynamic Matching: Beating 2-Approximation in $\\Delta^\\epsilon$\n  Update Time", "comments": "A preliminary version of this paper is to appear in proceedings of\n  SODA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fully dynamic graphs, we know how to maintain a 2-approximation of maximum\nmatching extremely fast, that is, in polylogarithmic update time or better. In\na sharp contrast and despite extensive studies, all known algorithms that\nmaintain a $2-\\Omega(1)$ approximate matching are much slower. Understanding\nthis gap and, in particular, determining the best possible update time for\nalgorithms providing a better-than-2 approximate matching is a major open\nquestion.\n  In this paper, we show that for any constant $\\epsilon > 0$, there is a\nrandomized algorithm that with high probability maintains a $2-\\Omega(1)$\napproximate maximum matching of a fully-dynamic general graph in worst-case\nupdate time $O(\\Delta^{\\epsilon}+\\text{polylog } n)$, where $\\Delta$ is the\nmaximum degree.\n  Previously, the fastest fully dynamic matching algorithm providing a\nbetter-than-2 approximation had $O(m^{1/4})$ update-time [Bernstein and Stein,\nSODA 2016]. A faster algorithm with update-time $O(n^\\epsilon)$ was known, but\nworked only for maintaining the size (and not the edges) of the matching in\nbipartite graphs [Bhattacharya, Henzinger, and Nanongkai, STOC 2016].\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 14:57:26 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Behnezhad", "Soheil", ""], ["\u0141\u0105cki", "Jakub", ""], ["Mirrokni", "Vahab", ""]]}, {"id": "1911.01931", "submitter": "Hanbaek Lyu", "authors": "Hanbaek Lyu, Deanna Needell, and Laura Balzano", "title": "Online matrix factorization for Markovian data and applications to\n  Network Dictionary Learning", "comments": "39 pages, 13 figures", "journal-ref": "Journal of Machine Learning Research 21 (2020)", "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Matrix Factorization (OMF) is a fundamental tool for dictionary\nlearning problems, giving an approximate representation of complex data sets in\nterms of a reduced number of extracted features. Convergence guarantees for\nmost of the OMF algorithms in the literature assume independence between data\nmatrices, and the case of dependent data streams remains largely unexplored. In\nthis paper, we show that a non-convex generalization of the well-known OMF\nalgorithm for i.i.d. stream of data in \\citep{mairal2010online} converges\nalmost surely to the set of critical points of the expected loss function, even\nwhen the data matrices are functions of some underlying Markov chain satisfying\na mild mixing condition. This allows one to extract features more efficiently\nfrom dependent data streams, as there is no need to subsample the data sequence\nto approximately satisfy the independence assumption. As the main application,\nby combining online non-negative matrix factorization and a recent MCMC\nalgorithm for sampling motifs from networks, we propose a novel framework of\nNetwork Dictionary Learning, which extracts ``network dictionary patches' from\na given network in an online manner that encodes main features of the network.\nWe demonstrate this technique and its application to network denoising problems\non real-world network data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 16:47:28 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 01:32:44 GMT"}, {"version": "v3", "created": "Sat, 9 Nov 2019 05:52:03 GMT"}, {"version": "v4", "created": "Thu, 16 Apr 2020 01:01:38 GMT"}, {"version": "v5", "created": "Wed, 14 Oct 2020 03:27:15 GMT"}, {"version": "v6", "created": "Sat, 7 Nov 2020 22:41:18 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Lyu", "Hanbaek", ""], ["Needell", "Deanna", ""], ["Balzano", "Laura", ""]]}, {"id": "1911.01951", "submitter": "George Manoussakis", "authors": "George Manoussakis", "title": "The Bron-Kerbosch Algorithm with Vertex Ordering is Output-Sensitive", "comments": "bug lemma11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bron-Kerbosch algorithm is a well known maximal clique enumeration\nalgorithm. So far it was unknown whether it was output sensitive or not. In\nthis paper we partially answer this question by proving that the Bron-Kerbosch\nAlgorithm with vertex ordering, first introduced and studied by Eppstein,\nL\\\"offler and Strash in \"Listing all maximal cliques in sparse graphs in\nnear-optimal time. International Symposium on Algorithms and Computation.\nSpringer, Berlin, Heidelberg, 2010\" is output sensitive.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 17:21:28 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 16:03:22 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Manoussakis", "George", ""]]}, {"id": "1911.01956", "submitter": "Peilin Zhong", "authors": "Alexandr Andoni, Clifford Stein, Peilin Zhong", "title": "Parallel Approximate Undirected Shortest Paths Via Low Hop Emulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a $(1+\\varepsilon)$-approximate parallel algorithm for computing\nshortest paths in undirected graphs, achieving $\\mathrm{poly}(\\log n)$ depth\nand $m\\mathrm{poly}(\\log n)$ work for $n$-nodes $m$-edges graphs. Although\nsequential algorithms with (nearly) optimal running time have been known for\nseveral decades, near-optimal parallel algorithms have turned out to be a much\ntougher challenge. For $(1+\\varepsilon)$-approximation, all prior algorithms\nwith $\\mathrm{poly}(\\log n)$ depth perform at least $\\Omega(mn^{c})$ work for\nsome constant $c>0$. Improving this long-standing upper bound obtained by Cohen\n(STOC'94) has been open for $25$ years.\n  We develop several new tools of independent interest. One of them is a new\nnotion beyond hopsets --- low hop emulator --- a $\\mathrm{poly}(\\log\nn)$-approximate emulator graph in which every shortest path has at most\n$O(\\log\\log n)$ hops (edges). Direct applications of the low hop emulators are\nparallel algorithms for $\\mathrm{poly}(\\log n)$-approximate single source\nshortest path (SSSP), Bourgain's embedding, metric tree embedding, and low\ndiameter decomposition, all with $\\mathrm{poly}(\\log n)$ depth and\n$m\\mathrm{poly}(\\log n)$ work.\n  To boost the approximation ratio to $(1+\\varepsilon)$, we introduce\ncompressible preconditioners and apply it inside Sherman's framework (SODA'17)\nto solve the more general problem of uncapacitated minimum cost flow (a.k.a.,\ntransshipment problem). Our algorithm computes a $(1+\\varepsilon)$-approximate\nuncapacitated minimum cost flow in $\\mathrm{poly}(\\log n)$ depth using\n$m\\mathrm{poly}(\\log n)$ work. As a consequence, it also improves the\nstate-of-the-art sequential running time from $m\\cdot 2^{O(\\sqrt{\\log n})}$ to\n$m\\mathrm{poly}(\\log n)$.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 17:33:02 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Andoni", "Alexandr", ""], ["Stein", "Clifford", ""], ["Zhong", "Peilin", ""]]}, {"id": "1911.01960", "submitter": "Sidhanth Mohanty", "authors": "Jess Banks, Sidhanth Mohanty, Prasad Raghavendra", "title": "Local Statistics, Semidefinite Programming, and Community Detection", "comments": "83 pages. Paper completely rewritten. Results for the stochastic\n  block model were added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new hierarchy of semidefinite programming relaxations for\ninference problems. As test cases, we consider the problem of community\ndetection in block models. The vertices are partitioned into $k$ communities,\nand a graph is sampled conditional on a prescribed number of inter- and\nintra-community edges. The problem of detection, where we are to decide with\nhigh probability whether a graph was drawn from this model or the uniform\ndistribution on regular graphs, is conjectured to undergo a computational phase\ntransition at a point called the Kesten-Stigum (KS) threshold.\n  In this work, we consider two models of random graphs namely the well-studied\n(irregular) stochastic block model and a distribution over random regular\ngraphs we'll call the Degree Regular Block Model. For both these models, we\nshow that sufficiently high constant levels of our hierarchy can perform\ndetection arbitrarily close to the KS threshold and that our algorithm is\nrobust to up to a linear number of adversarial edge perturbations. Furthermore,\nin the case of Degree Regular Block Model (DRBM), we show that below the\nKesten-Stigum threshold no constant level can do so.\n  In the case of the (irregular) Stochastic Block Model, it is known that\nefficient algorithms exist all the way down to this threshold, although none\nare robust to a linear number of adversarial perturbations of the graph when\nthe average degree is small. More importantly, there is little\ncomplexity-theoretic evidence that detection is hard below the threshold. In\nthe DRBM with more than two groups, it has not to our knowledge been proven\nthat any algorithm succeeds down to the KS threshold, let alone that one can do\nso robustly, and there is a similar dearth of evidence for hardness below this\npoint.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 17:40:13 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 04:45:44 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Banks", "Jess", ""], ["Mohanty", "Sidhanth", ""], ["Raghavendra", "Prasad", ""]]}, {"id": "1911.02035", "submitter": "Sitan Chen", "authors": "Sitan Chen, Jerry Li, Ankur Moitra", "title": "Efficiently Learning Structured Distributions from Untrusted Batches", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem, introduced by Qiao and Valiant, of learning from\nuntrusted batches. Here, we assume $m$ users, all of whom have samples from\nsome underlying distribution $p$ over $1, \\ldots, n$. Each user sends a batch\nof $k$ i.i.d. samples from this distribution; however an $\\epsilon$-fraction of\nusers are untrustworthy and can send adversarially chosen responses. The goal\nis then to learn $p$ in total variation distance. When $k = 1$ this is the\nstandard robust univariate density estimation setting and it is well-understood\nthat $\\Omega (\\epsilon)$ error is unavoidable. Suprisingly, Qiao and Valiant\ngave an estimator which improves upon this rate when $k$ is large.\nUnfortunately, their algorithms run in time exponential in either $n$ or $k$.\n  We first give a sequence of polynomial time algorithms whose estimation error\napproaches the information-theoretically optimal bound for this problem. Our\napproach is based on recent algorithms derived from the sum-of-squares\nhierarchy, in the context of high-dimensional robust estimation. We show that\nalgorithms for learning from untrusted batches can also be cast in this\nframework, but by working with a more complicated set of test functions.\n  It turns out this abstraction is quite powerful and can be generalized to\nincorporate additional problem specific constraints. Our second and main result\nis to show that this technology can be leveraged to build in prior knowledge\nabout the shape of the distribution. Crucially, this allows us to reduce the\nsample complexity of learning from untrusted batches to polylogarithmic in $n$\nfor most natural classes of distributions, which is important in many\napplications. To do so, we demonstrate that these sum-of-squares algorithms for\nrobust mean estimation can be made to handle complex combinatorial constraints\n(e.g. those arising from VC theory), which may be of independent technical\ninterest.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 19:01:46 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Chen", "Sitan", ""], ["Li", "Jerry", ""], ["Moitra", "Ankur", ""]]}, {"id": "1911.02146", "submitter": "Yang Cai", "authors": "Johaness Brustle, Yang Cai, Constantinos Daskalakis", "title": "Multi-Item Mechanisms without Item-Independence: Learnability via\n  Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the sample complexity of learning revenue-optimal multi-item\nauctions. We obtain the first set of positive results that go beyond the\nstandard but unrealistic setting of item-independence. In particular, we\nconsider settings where bidders' valuations are drawn from correlated\ndistributions that can be captured by Markov Random Fields or Bayesian Networks\n-- two of the most prominent graphical models. We establish parametrized sample\ncomplexity bounds for learning an up-to-$\\varepsilon$ optimal mechanism in both\nmodels, which scale polynomially in the size of the model, i.e.~the number of\nitems and bidders, and only exponential in the natural complexity measure of\nthe model, namely either the largest in-degree (for Bayesian Networks) or the\nsize of the largest hyper-edge (for Markov Random Fields).\n  We obtain our learnability results through a novel and modular framework that\ninvolves first proving a robustness theorem. We show that, given only\n``approximate distributions'' for bidder valuations, we can learn a mechanism\nwhose revenue is nearly optimal simultaneously for all ``true distributions''\nthat are close to the ones we were given in Prokhorov distance. Thus, to learn\na good mechanism, it suffices to learn approximate distributions. When item\nvalues are independent, learning in Prokhorov distance is immediate, hence our\nframework directly implies the main result of Gonczarowski and Weinberg. When\nitem values are sampled from more general graphical models, we combine our\nrobustness theorem with novel sample complexity results for learning Markov\nRandom Fields or Bayesian Networks in Prokhorov distance, which may be of\nindependent interest. Finally, in the single-item case, our robustness result\ncan be strengthened to hold under an even weaker distribution distance, the\nL\\'evy distance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 00:05:26 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 16:25:00 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Brustle", "Johaness", ""], ["Cai", "Yang", ""], ["Daskalakis", "Constantinos", ""]]}, {"id": "1911.02212", "submitter": "Max Simchowitz", "authors": "Mark Braverman, Elad Hazan, Max Simchowitz, Blake Woodworth", "title": "The gradient complexity of linear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the computational complexity of several basic linear algebra\nprimitives, including largest eigenvector computation and linear regression, in\nthe computational model that allows access to the data via a matrix-vector\nproduct oracle. We show that for polynomial accuracy, $\\Theta(d)$ calls to the\noracle are necessary and sufficient even for a randomized algorithm.\n  Our lower bound is based on a reduction to estimating the least eigenvalue of\na random Wishart matrix. This simple distribution enables a concise proof,\nleveraging a few key properties of the random Wishart ensemble.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 05:45:05 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 01:06:45 GMT"}, {"version": "v3", "created": "Sun, 23 May 2021 05:12:54 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Braverman", "Mark", ""], ["Hazan", "Elad", ""], ["Simchowitz", "Max", ""], ["Woodworth", "Blake", ""]]}, {"id": "1911.02259", "submitter": "Jaroslaw Byrka", "authors": "Jaros{\\l}aw Byrka, Fabrizio Grandoni, and Afrouz Jabal Ameli", "title": "Breaching the 2-Approximation Barrier for Connectivity Augmentation: a\n  Reduction to Steiner Tree", "comments": "Corrected a typo in the abstract (in metadata)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basic goal of survivable network design is to build a cheap network that\nmaintains the connectivity between given sets of nodes despite the failure of a\nfew edges/nodes. The Connectivity Augmentation Problem (CAP) is arguably one of\nthe most basic problems in this area: given a $k$(-edge)-connected graph $G$\nand a set of extra edges (links), select a minimum cardinality subset $A$ of\nlinks such that adding $A$ to $G$ increases its edge connectivity to $k+1$.\nIntuitively, one wants to make an existing network more reliable by augmenting\nit with extra edges. The best known approximation factor for this NP-hard\nproblem is $2$, and this can be achieved with multiple approaches (the first\nsuch result is in [Frederickson and J\\'aj\\'a'81]).\n  It is known [Dinitz et al.'76] that CAP can be reduced to the case $k=1$,\na.k.a. the Tree Augmentation Problem (TAP), for odd $k$, and to the case $k=2$,\na.k.a. the Cactus Augmentation Problem (CacAP), for even $k$. Several better\nthan $2$ approximation algorithms are known for TAP, culminating with a recent\n$1.458$ approximation [Grandoni et al.'18]. However, for CacAP the best known\napproximation is $2$.\n  In this paper we breach the $2$ approximation barrier for CacAP, hence for\nCAP, by presenting a polynomial-time $2\\ln(4)-\\frac{967}{1120}+\\epsilon<1.91$\napproximation. Previous approaches exploit properties of TAP that do not seem\nto generalize to CacAP. We instead use a reduction to the Steiner tree problem\nwhich was previously used in parameterized algorithms [Basavaraju et al.'14].\nThis reduction is not approximation preserving, and using the current best\napproximation factor for Steiner tree [Byrka et al.'13] as a black-box would\nnot be good enough to improve on $2$. To achieve the latter goal, we ``open the\nbox'' and exploit the specific properties of the instances of Steiner tree\narising from CacAP.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 08:55:43 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 14:43:19 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Byrka", "Jaros\u0142aw", ""], ["Grandoni", "Fabrizio", ""], ["Ameli", "Afrouz Jabal", ""]]}, {"id": "1911.02356", "submitter": "Naga Venkata Chaitanya Gudapati", "authors": "Naga V. C. Gudapati, Enrico Malaguti and Michele Monaci", "title": "In Search of Dense Subgraphs: How Good is Greedy Peeling?", "comments": "15 pages", "journal-ref": "Technical Report OR-19-9, DEI - University of Bologna, 2019", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding the densest subgraph in a given graph has several\napplications in graph mining, particularly in areas like social network\nanalysis, protein and gene analyses etc. Depending on the application, finding\ndense subgraphs can be used to determine regions of high importance, similar\ncharacteristics or enhanced interaction. The densest subgraph extraction\nproblem is a fundamentally a non-linear optimization problem. Nevertheless, it\ncan be solved in polynomial time by an exact algorithm based on the iterative\nsolution of a series of maximum flow sub-problems. Despite its polynomial time\ncomplexity, the computing time required by the exact algorithms on very large\ngraphs could be prohibitive. Thus, to approach graphs with millions of vertices\nand edges, one has to resort to heuristic algorithms. We provide an efficient\nimplementation of a greedy heuristic from the literature that is extremely fast\nand has some nice theoretical properties. We also introduce a new heurisitic\nalgorithm that is built on top of the greedy and the exact methods. An\nextensive computational study is presented to evaluate the performance of\nvarious solution methods on a benchmark composed of 86 instances taken from the\nliterature. This analysis shows that the proposed heuristic algorithm proved\nvery effective on a large number of test instances, often providing either the\noptimal solution or near-optimal solution within short computing times.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 13:10:54 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Gudapati", "Naga V. C.", ""], ["Malaguti", "Enrico", ""], ["Monaci", "Michele", ""]]}, {"id": "1911.02363", "submitter": "Chi-Ning Chou", "authors": "Chi-Ning Chou, Mien Brabeeba Wang", "title": "ODE-Inspired Analysis for the Biological Version of Oja's Rule in\n  Solving Streaming PCA", "comments": "Accepted for presentation at the Conference on Learning Theory (COLT)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oja's rule [Oja, Journal of mathematical biology 1982] is a well-known\nbiologically-plausible algorithm using a Hebbian-type synaptic update rule to\nsolve streaming principal component analysis (PCA). Computational\nneuroscientists have known that this biological version of Oja's rule converges\nto the top eigenvector of the covariance matrix of the input in the limit.\nHowever, prior to this work, it was open to prove any convergence rate\nguarantee.\n  In this work, we give the first convergence rate analysis for the biological\nversion of Oja's rule in solving streaming PCA. Moreover, our convergence rate\nmatches the information theoretical lower bound up to logarithmic factors and\noutperforms the state-of-the-art upper bound for streaming PCA. Furthermore, we\ndevelop a novel framework inspired by ordinary differential equations (ODE) to\nanalyze general stochastic dynamics. The framework abandons the traditional\nstep-by-step analysis and instead analyzes a stochastic dynamic in one-shot by\ngiving a closed-form solution to the entire dynamic. The one-shot framework\nallows us to apply stopping time and martingale techniques to have a flexible\nand precise control on the dynamic. We believe that this general framework is\npowerful and should lead to effective yet simple analysis for a large class of\nproblems with stochastic dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 16:01:32 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 21:32:51 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Chou", "Chi-Ning", ""], ["Wang", "Mien Brabeeba", ""]]}, {"id": "1911.02440", "submitter": "Alexander Golovnev", "authors": "Divesh Aggarwal, Huck Bennett, Alexander Golovnev, Noah\n  Stephens-Davidowitz", "title": "Fine-grained hardness of CVP(P) -- Everything that we can prove (and\n  nothing else)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a number of fine-grained hardness results for the Closest Vector\nProblem in the $\\ell_p$ norm ($\\mathrm{CVP}_p$), and its approximate and\nnon-uniform variants. First, we show that $\\mathrm{CVP}_p$ cannot be solved in\n$2^{(1-\\varepsilon)n}$ time for all $p \\notin 2\\mathbb{Z}$ and $\\varepsilon >\n0$, assuming the Strong Exponential Time Hypothesis (SETH). Second, we extend\nthis by showing that there is no $2^{(1-\\varepsilon)n}$-time algorithm for\napproximating $\\mathrm{CVP}_p$ to within a constant factor $\\gamma$ for such\n$p$ assuming a \"gap\" version of SETH, with an explicit relationship between\n$\\gamma$, $p$, and the arity $k = k(\\varepsilon)$ of the underlying hard CSP.\nThird, we show the same hardness result for (exact) $\\mathrm{CVP}_p$ with\npreprocessing (assuming non-uniform SETH). For exact \"plain\" $\\mathrm{CVP}_p$,\nthe same hardness result was shown in [Bennett, Golovnev, and\nStephens-Davidowitz FOCS 2017] for all but finitely many $p \\notin\n2\\mathbb{Z}$, where the set of exceptions depended on $\\varepsilon$ and was not\nexplicit. For the approximate and preprocessing problems, only very weak bounds\nwere known prior to this work. We also show that the restriction to $p \\notin\n2\\mathbb{Z}$ is in some sense inherent. In particular, we show that no\n\"natural\" reduction can rule out even a $2^{3n/4}$-time algorithm for\n$\\mathrm{CVP}_2$ under SETH. For this, we prove that the possible sets of\nclosest lattice vectors to a target in the $\\ell_2$ norm have quite rigid\nstructure, which essentially prevents them from being as expressive as\n$3$-CNFs. We prove these results using techniques from many different fields,\nincluding complex analysis, functional analysis, additive combinatorics, and\ndiscrete Fourier analysis. E.g., along the way, we give a new (and tighter)\nproof of Szemer\\'{e}di's cube lemma for the boolean cube.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 15:34:28 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 16:28:27 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Aggarwal", "Divesh", ""], ["Bennett", "Huck", ""], ["Golovnev", "Alexander", ""], ["Stephens-Davidowitz", "Noah", ""]]}, {"id": "1911.02453", "submitter": "Katrin Casel", "authors": "Lukas Behrendt, Katrin Casel, Tobias Friedrich, J.A. Gregor\n  Lagodzinski, Alexander L\\\"oser, Marcus Wilhelm", "title": "From Symmetry to Asymmetry: Generalizing TSP Approximations by\n  Parametrization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the tree doubling and Christofides algorithm, the two most\ncommon approximations for TSP, to parameterized approximations for ATSP. The\nparameters we consider for the respective parameterizations are upper bounded\nby the number of asymmetric distances in the given instance, which yields\nalgorithms to efficiently compute constant factor approximations also for\nmoderately asymmetric TSP instances. As generalization of the Christofides\nalgorithm, we derive a parameterized 2.5-approximation, where the parameter is\nthe size of a vertex cover for the subgraph induced by the asymmetric edges.\nOur generalization of the tree doubling algorithm gives a parameterized\n3-approximation, where the parameter is the number of asymmetric edges in a\ngiven minimum spanning arborescence. Both algorithms are also stated in the\nform of additive lossy kernelizations, which allows to combine them with known\npolynomial time approximations for ATSP. Further, we combine them with a notion\nof symmetry relaxation which allows to trade approximation guarantee for\nruntime. We complement our results by experimental evaluations, which show that\ngeneralized tree-doubling frequently outperforms generalized Christofides with\nrespect to parameter size.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 16:06:02 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 13:18:39 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Behrendt", "Lukas", ""], ["Casel", "Katrin", ""], ["Friedrich", "Tobias", ""], ["Lagodzinski", "J. A. Gregor", ""], ["L\u00f6ser", "Alexander", ""], ["Wilhelm", "Marcus", ""]]}, {"id": "1911.02506", "submitter": "Haotian Jiang", "authors": "Haotian Jiang, Jian Li, Daogao Liu, Sahil Singla", "title": "Algorithms and Adaptivity Gaps for Stochastic $k$-TSP", "comments": "ITCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a metric $(V,d)$ and a $\\textsf{root} \\in V$, the classic\n$\\textsf{$k$-TSP}$ problem is to find a tour originating at the $\\textsf{root}$\nof minimum length that visits at least $k$ nodes in $V$. In this work,\nmotivated by applications where the input to an optimization problem is\nuncertain, we study two stochastic versions of $\\textsf{$k$-TSP}$.\n  In Stoch-Reward $k$-TSP, originally defined by Ene-Nagarajan-Saket [ENS17],\neach vertex $v$ in the given metric $(V,d)$ contains a stochastic reward $R_v$.\nThe goal is to adaptively find a tour of minimum expected length that collects\nat least reward $k$; here \"adaptively\" means our next decision may depend on\nprevious outcomes. Ene et al. give an $O(\\log k)$-approximation adaptive\nalgorithm for this problem, and left open if there is an $O(1)$-approximation\nalgorithm. We totally resolve their open question and even give an\n$O(1)$-approximation \\emph{non-adaptive} algorithm for this problem.\n  We also introduce and obtain similar results for the Stoch-Cost $k$-TSP\nproblem. In this problem each vertex $v$ has a stochastic cost $C_v$, and the\ngoal is to visit and select at least $k$ vertices to minimize the expected\n\\emph{sum} of tour length and cost of selected vertices. This problem\ngeneralizes the Price of Information framework [Singla18] from deterministic\nprobing costs to metric probing costs.\n  Our techniques are based on two crucial ideas: \"repetitions\" and \"critical\nscaling\". We show using Freedman's and Jogdeo-Samuels' inequalities that for\nour problems, if we truncate the random variables at an ideal threshold and\nrepeat, then their expected values form a good surrogate. Unfortunately, this\nideal threshold is adaptive as it depends on how far we are from achieving our\ntarget $k$, so we truncate at various different scales and identify a\n\"critical\" scale.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 17:46:13 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Jiang", "Haotian", ""], ["Li", "Jian", ""], ["Liu", "Daogao", ""], ["Singla", "Sahil", ""]]}, {"id": "1911.02628", "submitter": "Nguyen Pham", "authors": "Soumyottam Chatterjee, Gopal Pandurangan, Nguyen Dinh Pham", "title": "Distributed MST: A Smoothed Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study smoothed analysis of distributed graph algorithms, focusing on the\nfundamental minimum spanning tree (MST) problem. With the goal of studying the\ntime complexity of distributed MST as a function of the \"perturbation\" of the\ninput graph, we posit a {\\em smoothing model} that is parameterized by a\nsmoothing parameter $0 \\leq \\epsilon(n) \\leq 1$ which controls the amount of\n{\\em random} edges that can be added to an input graph $G$ per round.\nInformally, $\\epsilon(n)$ is the probability (typically a small function of\n$n$, e.g., $n^{-\\frac{1}{4}}$) that a random edge can be added to a node per\nround. The added random edges, once they are added, can be used (only) for\ncommunication.\n  We show upper and lower bounds on the time complexity of distributed MST in\nthe above smoothing model. We present a distributed algorithm that, with high\nprobability,\\footnote{Throughout, with high probability (whp) means with\nprobability at least $1 - n^{-c}$, for some fixed, positive constant $c$.}\ncomputes an MST and runs in $\\tilde{O}(\\min\\{\\frac{1}{\\sqrt{\\epsilon(n)}}\n2^{O(\\sqrt{\\log n})}, D + \\sqrt{n}\\})$ rounds\\footnote{The notation $\\tilde{O}$\nhides a $\\polylog(n)$ factor and $\\tilde{\\Omega}$ hides a\n$\\frac{1}{\\polylog{(n)}}$ factor, where $n$ is the number of nodes of the\ngraph.} where $\\epsilon$ is the smoothing parameter, $D$ is the network\ndiameter and $n$ is the network size. To complement our upper bound, we also\nshow a lower bound of $\\tilde{\\Omega}(\\min\\{\\frac{1}{\\sqrt{\\epsilon(n)}},\nD+\\sqrt{n}\\})$. We note that the upper and lower bounds essentially match\nexcept for a multiplicative $2^{O(\\sqrt{\\log n})} \\polylog(n)$ factor.\n  Our work can be considered as a first step in understanding the smoothed\ncomplexity of distributed graph algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 21:05:52 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Chatterjee", "Soumyottam", ""], ["Pandurangan", "Gopal", ""], ["Pham", "Nguyen Dinh", ""]]}, {"id": "1911.02653", "submitter": "Ariel Kulik", "authors": "Ariel Kulik and Hadas Shachnai", "title": "Analysis of Two-variable Recurrence Relations with Application to\n  Parameterized Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce randomized branching as a tool for parameterized\napproximation and develop the mathematical machinery for its analysis. Our\nalgorithms improve the best known running times of parameterized approximation\nalgorithms for Vertex Cover and $3$-Hitting Set for a wide range of\napproximation ratios. One notable example is a simple parameterized random\n$1.5$-approximation algorithm for Vertex Cover, whose running time of\n$O^*(1.01657^k)$ substantially improves the best known runnning time of\n$O^*(1.0883^k)$ [Brankovic and Fernau, 2013]. For $3$-Hitting Set we present a\nparameterized random $2$-approximation algorithm with running time of\n$O^*(1.0659^k)$, improving the best known $O^*(1.29^k)$ algorithm of [Brankovic\nand Fernau, 2012].\n  The running times of our algorithms are derived from an asymptotic analysis\nof a wide class of two-variable recurrence relations of the form: $$p(b,k) =\n\\min_{1\\leq j \\leq N} \\sum_{i=1}^{r_j} \\bar{\\gamma}_i^j \\cdot p(b-\\bar{b}^j_i,\nk-\\bar{k}_i^j),$$ where $\\bar{b}^j$ and $\\bar{k}^j$ are vectors of natural\nnumbers, and $\\bar{\\gamma}^j$ is a probability distribution over $r_j$\nelements, for $1\\leq j \\leq N$. Our main theorem asserts that for any\n$\\alpha>0$, $$\\lim_{k \\rightarrow \\infty } \\frac{1}{k} \\log p(\\alpha k,k) =\n-\\max_{1\\leq j \\leq N} M_j,$$\n  where $M_j$ depends only on $\\alpha$, $\\bar{\\gamma}^j$, $\\bar{b}^j$ and\n$\\bar{k}^j$, and can be efficiently calculated by solving a simple numerical\noptimization problem. To this end, we show an equivalence between the\nrecurrence and a stochastic process. We analyze this process using the Method\nof Types, by introducing an adaptation of Sanov's theorem to our setting. We\nbelieve our novel analysis of recurrence relations which is of independent\ninterest is a main contribution of this paper.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 22:19:24 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 15:18:22 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Kulik", "Ariel", ""], ["Shachnai", "Hadas", ""]]}, {"id": "1911.02696", "submitter": "Daniel Lemire", "authors": "Marcus D. R. Klarqvist and Wojciech Mu{\\l}a and Daniel Lemire", "title": "Efficient Computation of Positional Population Counts Using SIMD\n  Instructions", "comments": null, "journal-ref": null, "doi": "10.1002/cpe.6304", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several fields such as statistics, machine learning, and bioinformatics,\ncategorical variables are frequently represented as one-hot encoded vectors.\nFor example, given 8 distinct values, we map each value to a byte where only a\nsingle bit has been set. We are motivated to quickly compute statistics over\nsuch encodings. Given a stream of k-bit words, we seek to compute k distinct\nsums corresponding to bit values at indexes 0, 1, 2, ..., k-1. If the k-bit\nwords are one-hot encoded then the sums correspond to a frequency histogram.\nThis multiple-sum problem is a generalization of the population-count problem\nwhere we seek the sum of all bit values. Accordingly, we refer to the\nmultiple-sum problem as a positional population-count. Using SIMD (Single\nInstruction, Multiple Data) instructions from recent Intel processors, we\ndescribe algorithms for computing the 16-bit position population count using\nless than half of a CPU cycle per 16-bit word. Our best approach uses up to 400\ntimes fewer instructions and is up to 50 times faster than baseline code using\nonly regular (non-SIMD) instructions, for sufficiently large inputs.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 01:06:38 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 17:01:46 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 21:39:54 GMT"}, {"version": "v4", "created": "Wed, 23 Dec 2020 20:55:55 GMT"}, {"version": "v5", "created": "Wed, 17 Mar 2021 21:39:19 GMT"}, {"version": "v6", "created": "Tue, 11 May 2021 21:15:54 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Klarqvist", "Marcus D. R.", ""], ["Mu\u0142a", "Wojciech", ""], ["Lemire", "Daniel", ""]]}, {"id": "1911.02716", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi and Sahil Singla", "title": "Improved Truthful Mechanisms for Combinatorial Auctions with Submodular\n  Bidders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A longstanding open problem in Algorithmic Mechanism Design is to design\ncomputationally-efficient truthful mechanisms for (approximately) maximizing\nwelfare in combinatorial auctions with submodular bidders. The first such\nmechanism was obtained by Dobzinski, Nisan, and Schapira [STOC'06] who gave an\n$O(\\log^2{m})$-approximation where $m$ is the number of items. This problem has\nbeen studied extensively since, culminating in an\n$O(\\sqrt{\\log{m}})$-approximation mechanism by Dobzinski [STOC'16].\n  We present a computationally-efficient truthful mechanism with approximation\nratio that improves upon the state-of-the-art by an exponential factor. In\nparticular, our mechanism achieves an $O((\\log\\log{m})^3)$-approximation in\nexpectation, uses only $O(n)$ demand queries, and has universal truthfulness\nguarantee. This settles an open question of Dobzinski on whether\n$\\Theta(\\sqrt{\\log{m}})$ is the best approximation ratio in this setting in\nnegative.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 02:09:19 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Assadi", "Sepehr", ""], ["Singla", "Sahil", ""]]}, {"id": "1911.02780", "submitter": "Hongchao Qin", "authors": "Hongchao Qin, Rong-Hua Li, Guoren Wang, Lu Qin, Ye Yuan, Zhiwei Zhang", "title": "Mining Bursting Communities in Temporal Graphs", "comments": "Submitted to ICDE at October 15, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal graphs are ubiquitous. Mining communities that are bursting in a\nperiod of time is essential to seek emergency events in temporal graphs.\nUnfortunately, most previous studies for community mining in temporal networks\nignore the bursting patterns of communities. In this paper, we are the first to\nstudy a problem of seeking bursting communities in a temporal graph. We propose\na novel model, called (l, {\\delta})-maximal dense core, to represent a bursting\ncommunity in a temporal graph. Specifically, an (l, {\\delta})-maximal dense\ncore is a temporal subgraph in which each node has average degree no less than\n{\\delta} in a time segment with length no less than l. To compute the (l,\n{\\delta})-maximal dense core, we first develop a novel dynamic programming\nalgorithm which can calculate the segment density efficiently. Then, we propose\nan improved algorithm with several novel pruning techniques to further improve\nthe efficiency. In addition, we also develop an efficient algorithm to\nenumerate all (l, {\\delta})-maximal dense cores that are not dominated by the\nothers in terms of the parameters l and {\\delta}. The results of extensive\nexperiments on 9 real-life datasets demonstrate the effectiveness, efficiency\nand scalability of our algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 07:25:13 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Qin", "Hongchao", ""], ["Li", "Rong-Hua", ""], ["Wang", "Guoren", ""], ["Qin", "Lu", ""], ["Yuan", "Ye", ""], ["Zhang", "Zhiwei", ""]]}, {"id": "1911.02786", "submitter": "Kei Kimura", "authors": "Kei Kimura and Akira Suzuki", "title": "Trichotomy for the reconfiguration problem of integer linear systems", "comments": "Accepted by WALCOM2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the reconfiguration problem of integer linear\nsystems. In this problem, we are given an integer linear system $I$ and two\nfeasible solutions $\\boldsymbol{s}$ and $\\boldsymbol{t}$ of $I$, and then asked\nto transform $\\boldsymbol{s}$ to $\\boldsymbol{t}$ by changing a value of only\none variable at a time, while maintaining a feasible solution of $I$\nthroughout. $Z(I)$ for $I$ is the complexity index introduced by Kimura and\nMakino (Discrete Applied Mathematics 200:67--78, 2016), which is defined by the\nsign pattern of the input matrix. We analyze the complexity of the\nreconfiguration problem of integer linear systems based on the complexity index\n$Z(I)$ of given $I$. We then show that the problem is (i) solvable in constant\ntime if $Z(I)$ is less than one, (ii) weakly coNP-complete and\npseudo-polynomially solvable if $Z(I)$ is exactly one, and (iii)\nPSPACE-complete if $Z(I)$ is greater than one. Since the complexity indices of\nHorn and two-variable-par-inequality integer linear systems are at most one,\nour results imply that the reconfiguration of these systems are in coNP and\npseudo-polynomially solvable. Moreover, this is the first result that reveals\ncoNP-completeness for a reconfiguration problem, to the best of our knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 07:44:50 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Kimura", "Kei", ""], ["Suzuki", "Akira", ""]]}, {"id": "1911.02889", "submitter": "Micha{\\l} Ga\\'nczorz", "authors": "Micha{\\l} Ga\\'nczorz", "title": "Towards Better Compressed Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of computing a parsing where each phrase is of\nlength at most $m$ and which minimizes the zeroth order entropy of parsing.\nBased on the recent theoretical results we devise a heuristic for this problem.\nThe solution has straightforward application in succinct text representations\nand gives practical improvements. Moreover the proposed heuristic yields\nstructure whose size can be bounded both by $|S|H_{m-1}(S)$ and by\n$|S|/m(H_0(S) + \\cdots + H_{m-1})$, where $H_{k}(S)$ is the $k$-th order\nempirical entropy of $S$. We also consider a similar problem in which the\nfirst-order entropy is minimized.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 13:24:50 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 11:28:31 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Ga\u0144czorz", "Micha\u0142", ""]]}, {"id": "1911.02904", "submitter": "Haider Al Kim", "authors": "Haider Al Kim, Sven Puchinger, Antonia Wachter-Zeh", "title": "Error Correction for Partially Stuck Memory Cells", "comments": "6 pages, 4 theorems, XVI International Symposium Problems of\n  Redundancy in Information and Control Systems (Redundancy 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present code constructions for masking $u$ partially stuck memory cells\nwith $q$ levels and correcting additional random errors. The results are\nachieved by combining the methods for masking and error correction for stuck\ncells in [1] with the masking-only results for partially stuck cells in [2]. We\npresent two constructions for masking $u<q$ cells and error correction: one is\ngeneral and based on a generator matrix of a specific form. The second\nconstruction uses cyclic codes and allows to efficiently bound the\nerror-correction capability using the BCH bound. Furthermore, we extend the\nresults to masking $u\\geq q$ cells. For $u>1$ and $q>2$, all new constructions\nrequire less redundancy for masking partially stuck cells than previous work on\nstuck cells, which in turn can result in higher code rates at the same masking\nand error correction capability.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 13:49:27 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Kim", "Haider Al", ""], ["Puchinger", "Sven", ""], ["Wachter-Zeh", "Antonia", ""]]}, {"id": "1911.02911", "submitter": "Jonah Brown-Cohen", "authors": "Jonah Brown-Cohen and Prasad Raghavendra", "title": "Extended Formulation Lower Bounds for Refuting Random CSPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random constraint satisfaction problems (CSPs) such as random $3$-SAT are\nconjectured to be computationally intractable. The average case hardness of\nrandom $3$-SAT and other CSPs has broad and far-reaching implications on\nproblems in approximation, learning theory and cryptography.\n  In this work, we show subexponential lower bounds on the size of linear\nprogramming relaxations for refuting random instances of constraint\nsatisfaction problems. Formally, suppose $P : \\{0,1\\}^k \\to \\{0,1\\}$ is a\npredicate that supports a $t-1$-wise uniform distribution on its satisfying\nassignments. Consider the distribution of random instances of CSP $P$ with $m =\n\\Delta n$ constraints. We show that any linear programming extended formulation\nthat can refute instances from this distribution with constant probability must\nhave size at least\n$\\Omega\\left(\\exp\\left(\\left(\\frac{n^{t-2}}{\\Delta^2}\\right)^{\\frac{1-\\nu}{k}}\\right)\\right)$\nfor all $\\nu > 0$. For example, this yields a lower bound of size\n$\\exp(n^{1/3})$ for random $3$-SAT with a linear number of clauses.\n  We use the technique of pseudocalibration to directly obtain extended\nformulation lower bounds from the planted distribution. This approach bypasses\nthe need to construct Sherali-Adams integrality gaps in proving general LP\nlower bounds. As a corollary, one obtains a self-contained proof of\nsubexponential Sherali-Adams LP lower bounds for these problems. We believe the\nresult sheds light on the technique of pseudocalibration, a promising but\nconjectural approach to LP/SDP lower bounds.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 14:02:19 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Brown-Cohen", "Jonah", ""], ["Raghavendra", "Prasad", ""]]}, {"id": "1911.02921", "submitter": "Enrique Fita Sanmartin", "authors": "Enrique Fita Sanmartin, Sebastian Damrich, Fred A. Hamprecht (HCI/IWR\n  at Heidelberg University)", "title": "Probabilistic Watershed: Sampling all spanning forests for seeded\n  segmentation and semi-supervised learning", "comments": "To be published in NeurIPS2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seeded Watershed algorithm / minimax semi-supervised learning on a graph\ncomputes a minimum spanning forest which connects every pixel / unlabeled node\nto a seed / labeled node. We propose instead to consider all possible spanning\nforests and calculate, for every node, the probability of sampling a forest\nconnecting a certain seed with that node. We dub this approach \"Probabilistic\nWatershed\". Leo Grady (2006) already noted its equivalence to the Random Walker\n/ Harmonic energy minimization. We here give a simpler proof of this\nequivalence and establish the computational feasibility of the Probabilistic\nWatershed with Kirchhoff's matrix tree theorem. Furthermore, we show a new\nconnection between the Random Walker probabilities and the triangle inequality\nof the effective resistance. Finally, we derive a new and intuitive\ninterpretation of the Power Watershed.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 13:26:32 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Sanmartin", "Enrique Fita", "", "HCI/IWR\n  at Heidelberg University"], ["Damrich", "Sebastian", "", "HCI/IWR\n  at Heidelberg University"], ["Hamprecht", "Fred A.", "", "HCI/IWR\n  at Heidelberg University"]]}, {"id": "1911.03026", "submitter": "Tsuyoshi Yagita", "authors": "Duc A. Hoang, Akira Suzuki, Tsuyoshi Yagita", "title": "Reconfiguring k-path vertex covers", "comments": "29 pages, 4 figures, to appear in WALCOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vertex subset $I$ of a graph $G$ is called a $k$-path vertex cover if every\npath on $k$ vertices in $G$ contains at least one vertex from $I$. The\n\\textsc{$k$-Path Vertex Cover Reconfiguration ($k$-PVCR)} problem asks if one\ncan transform one $k$-path vertex cover into another via a sequence of $k$-path\nvertex covers where each intermediate member is obtained from its predecessor\nby applying a given reconfiguration rule exactly once. We investigate the\ncomputational complexity of \\textsc{$k$-PVCR} from the viewpoint of graph\nclasses under the well-known reconfiguration rules: $\\mathsf{TS}$,\n$\\mathsf{TJ}$, and $\\mathsf{TAR}$. The problem for $k=2$, known as the\n\\textsc{Vertex Cover Reconfiguration (VCR)} problem, has been well-studied in\nthe literature. We show that certain known hardness results for \\textsc{VCR} on\ndifferent graph classes including planar graphs, bounded bandwidth graphs,\nchordal graphs, and bipartite graphs, can be extended for \\textsc{$k$-PVCR}. In\nparticular, we prove a complexity dichotomy for \\textsc{$k$-PVCR} on general\ngraphs: on those whose maximum degree is $3$ (and even planar), the problem is\n$\\mathtt{PSPACE}$-complete, while on those whose maximum degree is $2$ (i.e.,\npaths and cycles), the problem can be solved in polynomial time. Additionally,\nwe also design polynomial-time algorithms for \\textsc{$k$-PVCR} on trees under\neach of $\\mathsf{TJ}$ and $\\mathsf{TAR}$. Moreover, on paths, cycles, and\ntrees, we describe how one can construct a reconfiguration sequence between two\ngiven $k$-path vertex covers in a yes-instance. In particular, on paths, our\nconstructed reconfiguration sequence is shortest.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 03:49:14 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Hoang", "Duc A.", ""], ["Suzuki", "Akira", ""], ["Yagita", "Tsuyoshi", ""]]}, {"id": "1911.03028", "submitter": "Robert Kelly", "authors": "Robert Kelly, Barak A. Pearlmutter, Phil Maguire", "title": "Lock-Free Hopscotch Hashing", "comments": "15 pages, to appear in APOCS20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a lock-free version of Hopscotch Hashing. Hopscotch\nHashing is an open addressing algorithm originally proposed by Herlihy, Shavit,\nand Tzafrir, which is known for fast performance and excellent cache locality.\nThe algorithm allows users of the table to skip or jump over irrelevant\nentries, allowing quick search, insertion, and removal of entries. Unlike\ntraditional linear probing, Hopscotch Hashing is capable of operating under a\nhigh load factor, as probe counts remain small. Our lock-free version improves\non both speed, cache locality, and progress guarantees of the original, being a\nchimera of two concurrent hash tables. We compare our data structure to various\nother lock-free and blocking hashing algorithms and show that its performance\nis in many cases superior to existing strategies. The proposed lock-free\nversion overcomes some of the drawbacks associated with the original blocking\nversion, leading to a substantial boost in scalability while maintaining\nattractive features like physical deletion or probe-chain compression.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 03:55:54 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Kelly", "Robert", ""], ["Pearlmutter", "Barak A.", ""], ["Maguire", "Phil", ""]]}, {"id": "1911.03035", "submitter": "Daniel Gibney", "authors": "Jason Bentley, Daniel Gibney, Sharma V. Thankachan", "title": "On the Complexity of BWT-runs Minimization via Alphabet Reordering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Burrows-Wheeler Transform (BWT) has been an essential tool in text\ncompression and indexing. First introduced in 1994, it went on to provide the\nbackbone for the first encoding of the classic suffix tree data structure in\nspace close to the entropy-based lower bound. Recently, there has been the\ndevelopment of compact suffix trees in space proportional to \"$r$\", the number\nof runs in the BWT, as well as the appearance of $r$ in the time complexity of\nnew algorithms. Unlike other popular measures of compression, the parameter $r$\nis sensitive to the lexicographic ordering given to the text's alphabet.\nDespite several past attempts to exploit this, a provably efficient algorithm\nfor finding, or approximating, an alphabet ordering which minimizes $r$ has\nbeen open for years.\n  We present the first set of results on the computational complexity of\nminimizing BWT-runs via alphabet reordering. We prove that the decision version\nof this problem is NP-complete and cannot be solved in time $2^{o(\\sigma +\n\\sqrt{n})}$ unless the Exponential Time Hypothesis fails, where $\\sigma$ is the\nsize of the alphabet and $n$ is the length of the text. We also show that the\noptimization problem is APX-hard. In doing so, we relate two previously\ndisparate topics: the optimal traveling salesperson path and the number of runs\nin the BWT of a text, providing a surprising connection between problems on\ngraphs and text compression. Also, by relating recent results in the field of\ndictionary compression, we illustrate that an arbitrary alphabet ordering\nprovides a $O(\\log^2 n)$-approximation.\n  We provide an optimal linear-time algorithm for the problem of finding a run\nminimizing ordering on a subset of symbols (occurring only once) under ordering\nconstraints, and prove a generalization of this problem to a class of graphs\nwith BWT like properties called Wheeler graphs is NP-complete.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 04:03:40 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 18:21:38 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Bentley", "Jason", ""], ["Gibney", "Daniel", ""], ["Thankachan", "Sharma V.", ""]]}, {"id": "1911.03043", "submitter": "Holden Lee", "authors": "Rong Ge, Holden Lee, Jianfeng Lu", "title": "Estimating Normalizing Constants for Log-Concave Distributions:\n  Algorithms and Lower Bounds", "comments": "46 pages", "journal-ref": "Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of\n  Computing. 2020. p. 579-586", "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the normalizing constant of an unnormalized probability\ndistribution has important applications in computer science, statistical\nphysics, machine learning, and statistics. In this work, we consider the\nproblem of estimating the normalizing constant $Z=\\int_{\\mathbb{R}^d}\ne^{-f(x)}\\,\\mathrm{d}x$ to within a multiplication factor of $1 \\pm\n\\varepsilon$ for a $\\mu$-strongly convex and $L$-smooth function $f$, given\nquery access to $f(x)$ and $\\nabla f(x)$. We give both algorithms and\nlowerbounds for this problem. Using an annealing algorithm combined with a\nmultilevel Monte Carlo method based on underdamped Langevin dynamics, we show\nthat $\\widetilde{\\mathcal{O}}\\Bigl(\\frac{d^{4/3}\\kappa +\nd^{7/6}\\kappa^{7/6}}{\\varepsilon^2}\\Bigr)$ queries to $\\nabla f$ are\nsufficient, where $\\kappa= L / \\mu$ is the condition number. Moreover, we\nprovide an information theoretic lowerbound, showing that at least\n$\\frac{d^{1-o(1)}}{\\varepsilon^{2-o(1)}}$ queries are necessary. This provides\na first nontrivial lowerbound for the problem.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 04:32:11 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 19:22:32 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Ge", "Rong", ""], ["Lee", "Holden", ""], ["Lu", "Jianfeng", ""]]}, {"id": "1911.03071", "submitter": "Fredrik S\\\"avje", "authors": "Christopher Harshaw and Fredrik S\\\"avje and Daniel Spielman and Peng\n  Zhang", "title": "Balancing covariates in randomized experiments with the Gram-Schmidt\n  Walk design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of experiments involves a compromise between covariate balance and\nrobustness. This paper introduces an experimental design that admits precise\ncontrol over this trade-off. The design is specified by a parameter that bounds\nthe worst-case mean square error of an estimator of the average treatment\neffect. Subject to the experimenter's desired level of robustness, the design\naims to simultaneously balance all linear functions of the covariates. The\nachieved level of balance is considerably better than what a fully random\nassignment would produce, and it is close to optimal given the desired level of\nrobustness. We show that the mean square error of the estimator is bounded by\nthe minimum of the loss function of a ridge regression of the potential\noutcomes on the covariates. One may thus interpret the approach as regression\nadjustment by design. Finally, we provide non-asymptotic tail bounds for the\nestimator, which facilitate the construction of conservative confidence\nintervals.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 06:09:36 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 21:10:43 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 02:32:30 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Harshaw", "Christopher", ""], ["S\u00e4vje", "Fredrik", ""], ["Spielman", "Daniel", ""], ["Zhang", "Peng", ""]]}, {"id": "1911.03195", "submitter": "Lu\\'is M. S. Russo", "authors": "Miguel E. Coimbra and Alexandre P. Francisco and Lu\\'is M. S. Russo\n  and Guillermo de Bernardo and Susana Ladra and Gonzalo Navarro", "title": "On dynamic succinct graph representations", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of representing dynamic graphs using $k^2$-trees. The\n$k^2$-tree data structure is one of the succinct data structures proposed for\nrepresenting static graphs, and binary relations in general. It relies on\ncompact representations of bit vectors. Hence, by relying on compact\nrepresentations of dynamic bit vectors, we can also represent dynamic graphs.\nIn this paper we follow instead the ideas by Munro {\\em et al.}, and we present\nan alternative implementation for representing dynamic graphs using\n$k^2$-trees. Our experimental results show that this new implementation is\ncompetitive in practice.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 11:35:29 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 12:12:28 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Coimbra", "Miguel E.", ""], ["Francisco", "Alexandre P.", ""], ["Russo", "Lu\u00eds M. S.", ""], ["de Bernardo", "Guillermo", ""], ["Ladra", "Susana", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1911.03204", "submitter": "Marcin Wrochna", "authors": "Miguel Romero, Marcin Wrochna, Stanislav \\v{Z}ivn\\'y", "title": "Treewidth-Pliability and PTAS for Max-CSPs", "comments": "Full version of a SODA'21 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify a sufficient condition, treewidth-pliability, that gives a\npolynomial-time approximation scheme (PTAS) for a large class of Max-2-CSPs\nparametrised by the class of allowed constraint graphs (with arbitrary\nconstraints on an unbounded alphabet). Our result applies more generally to the\nmaximum homomorphism problem between two rational-valued structures.\n  The condition unifies the two main approaches for designing PTASes. One is\nBaker's layering technique, which applies to sparse graphs such as planar or\nexcluded-minor graphs. The other is based on Szemer\\'{e}di's regularity lemma\nand applies to dense graphs. We extend the applicability of both techniques to\nnew classes of Max-CSPs.\n  Treewidth-pliability turns out to be a robust notion that can be defined in\nseveral equivalent ways, including characterisations via size, treedepth, or\nthe Hadwiger number. We show connections to the notions of\nfractional-treewidth-fragility from structural graph theory, hyperfiniteness\nfrom the area of property testing, and regularity partitions from the theory of\ndense graph limits. These may be of independent interest. In particular we show\nthat a monotone class of graphs is hyperfinite if and only if it is\nfractionally-treewidth-fragile and has bounded degree.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 11:53:17 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 17:40:52 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 11:23:37 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Romero", "Miguel", ""], ["Wrochna", "Marcin", ""], ["\u017divn\u00fd", "Stanislav", ""]]}, {"id": "1911.03297", "submitter": "Guilherme de Castro Mendes Gomes", "authors": "Guilherme C. M. Gomes, Matheus R. Guedes, Vinicius F. dos Santos", "title": "Structural Parameterizations for Equitable Coloring", "comments": "34 pages, 7 figures. Partial results were published in the\n  proceedings of LATIN2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $n$-vertex graph is equitably $k$-colorable if there is a proper coloring\nof its vertices such that each color is used either $\\left\\lfloor n/k\n\\right\\rfloor$ or $\\left\\lceil n/k \\right\\rceil$ times. While classic Vertex\nColoring is fixed parameter tractable under well established parameters such as\npathwidth and feedback vertex set, Equitable Coloring is\n$\\mathsf{W}[1]$-$\\mathsf{hard}$. We present an extensive study of structural\nparameterizations of Equitable Coloring, tackling both tractability and\nkernelization questions. We begin by showing that the problem is fixed\nparameter tractable when parameterized by distance to cluster or by distance to\nco-cluster -- improving on the $\\mathsf{FPT}$ algorithm of Fiala et al.\n[Theoretical Computer Science, 2011] parameterized by vertex cover -- and also\nwhen parameterized by distance to disjoint paths of bounded length. To justify\nthe latter result, we adapt a proof of Fellows et al. [Information and\nComputation, 2011] to show that Equitable Coloring is\n$\\mathsf{W}[1]$-$\\mathsf{hard}$ when simultaneously parameterized by distance\nto disjoint paths and number of colors. In terms of kernelization, on the\npositive side we present a linear kernel for the distance to clique parameter\nand a cubic kernel when parameterized by the maximum leaf number; on the other\nhand, we show that, unlike Vertex Coloring, Equitable Coloring does not admit a\npolynomial kernel when jointly parameterized by vertex cover and number of\ncolors, unless $\\mathsf{NP} \\subseteq \\mathsf{coNP}/\\mathsf{poly}$. We also\nrevisit the literature and derive other results on the parameterized complexity\nof the problem through minor reductions or other observations.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 14:49:35 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 20:53:48 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Gomes", "Guilherme C. M.", ""], ["Guedes", "Matheus R.", ""], ["Santos", "Vinicius F. dos", ""]]}, {"id": "1911.03360", "submitter": "Eugenio Angriman", "authors": "Eugenio Angriman, Alexander van der Grinten, Henning Meyerhenke", "title": "Local Search for Group Closeness Maximization on Big Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In network analysis and graph mining, closeness centrality is a popular\nmeasure to infer the importance of a vertex. Computing closeness efficiently\nfor individual vertices received considerable attention. The NP-hard problem of\ngroup closeness maximization, in turn, is more challenging: the objective is to\nfind a vertex group that is central as a whole and state-of-the-art heuristics\nfor it do not scale to very big graphs yet.\n  In this paper, we present new local search heuristics for group closeness\nmaximization. By using randomized approximation techniques and dynamic data\nstructures, our algorithms are often able to perform locally optimal decisions\nefficiently. The final result is a group with high (but not optimal) closeness\ncentrality.\n  We compare our algorithms to the current state-of-the-art greedy heuristic\nboth on weighted and on unweighted real-world graphs. For graphs with hundreds\nof millions of edges, our local search algorithms take only around ten minutes,\nwhile greedy requires more than ten hours. Overall, our new algorithms are\nbetween one and two orders of magnitude faster, depending on the desired group\nsize and solution quality. For example, on weighted graphs and $k = 10$, our\nalgorithms yield solutions of $12,4\\%$ higher quality, while also being\n$793,6\\times$ faster. For unweighted graphs and $k = 10$, we achieve solutions\nwithin $99,4\\%$ of the state-of-the-art quality while being $127,8\\times$\nfaster.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 16:32:02 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Angriman", "Eugenio", ""], ["van der Grinten", "Alexander", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1911.03449", "submitter": "Jacob Holm", "authors": "Jacob Holm and Eva Rotenberg", "title": "Fully-dynamic Planarity Testing in Polylogarithmic Time", "comments": "Updated version of paper submitted to STOC'20. This version features\n  a complete rewrite of section 4.4 (do-separation-flips). The new version\n  fixes an overlooked case in the previous version (the two fundamental cycles\n  we find do not necessarily share an edge) and contains a detailed\n  case-by-case proof of correctness", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a dynamic graph subject to insertions and deletions of edges, a natural\nquestion is whether the graph presently admits a planar embedding. We give a\ndeterministic fully-dynamic algorithm for general graphs, running in amortized\n$O(\\log^3 n)$ time per edge insertion or deletion, that maintains a bit\nindicating whether or not the graph is presently planar. This is an exponential\nimprovement over the previous best algorithm [Eppstein, Galil, Italiano,\nSpencer, 1996] which spends amortized $O(\\sqrt{n})$ time per update.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 18:59:12 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 13:39:32 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Holm", "Jacob", ""], ["Rotenberg", "Eva", ""]]}, {"id": "1911.03456", "submitter": "Gabriele D'Angelo", "authors": "Moreno Marzolla, Gabriele D'Angelo", "title": "Parallel Data Distribution Management on Shared-Memory Multiprocessors", "comments": "arXiv admin note: text overlap with arXiv:1703.06680", "journal-ref": "ACM Transactions on Modeling and Computer Simulation (TOMACS),\n  Vol. 30, No. 1, Article 5. ACM, February 2020. ISSN: 1049-3301", "doi": "10.1145/3369759", "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of identifying intersections between two sets of d-dimensional\naxis-parallel rectangles appears frequently in the context of agent-based\nsimulation studies. For this reason, the High Level Architecture (HLA)\nspecification -- a standard framework for interoperability among simulators --\nincludes a Data Distribution Management (DDM) service whose responsibility is\nto report all intersections between a set of subscription and update regions.\nThe algorithms at the core of the DDM service are CPU-intensive, and could\ngreatly benefit from the large computing power of modern multi-core processors.\nIn this paper we propose two parallel solutions to the DDM problem that can\noperate effectively on shared-memory multiprocessors. The first solution is\nbased on a data structure (the Interval Tree) that allows concurrent\ncomputation of intersections between subscription and update regions. The\nsecond solution is based on a novel parallel extension of the Sort Based\nMatching algorithm, whose sequential version is considered among the most\nefficient solutions to the DDM problem. Extensive experimental evaluation of\nthe proposed algorithms confirm their effectiveness on taking advantage of\nmultiple execution units in a shared-memory architecture.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 13:18:14 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 10:24:12 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Marzolla", "Moreno", ""], ["D'Angelo", "Gabriele", ""]]}, {"id": "1911.03542", "submitter": "Jonas Ellert", "authors": "Philip Bille, Jonas Ellert, Johannes Fischer, Inge Li G{\\o}rtz,\n  Florian Kurpicz, Ian Munro, Eva Rotenberg", "title": "Space Efficient Construction of Lyndon Arrays in Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first linear time algorithm to construct the $2n$-bit version\nof the Lyndon array for a string of length $n$ using only $o(n)$ bits of\nworking space. A simpler variant of this algorithm computes the plain ($n\\lg\nn$-bit) version of the Lyndon array using only $\\mathcal{O}(1)$ words of\nadditional working space. All previous algorithms are either not linear, or use\nat least $n\\lg n$ bits of additional working space. Also in practice, our new\nalgorithms outperform the previous best ones by an order of magnitude, both in\nterms of time and space.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 21:15:21 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 13:59:09 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Bille", "Philip", ""], ["Ellert", "Jonas", ""], ["Fischer", "Johannes", ""], ["G\u00f8rtz", "Inge Li", ""], ["Kurpicz", "Florian", ""], ["Munro", "Ian", ""], ["Rotenberg", "Eva", ""]]}, {"id": "1911.03605", "submitter": "Justin Chen", "authors": "Justin Y. Chen, Gregory Valiant, Paul Valiant", "title": "Worst-Case Analysis for Randomly Collected Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for statistical estimation that leverages knowledge\nof how samples are collected but makes no distributional assumptions on the\ndata values. Specifically, we consider a population of elements\n$[n]={1,\\ldots,n}$ with corresponding data values $x_1,\\ldots,x_n$. We observe\nthe values for a \"sample\" set $A \\subset [n]$ and wish to estimate some\nstatistic of the values for a \"target\" set $B \\subset [n]$ where $B$ could be\nthe entire set. Crucially, we assume that the sets $A$ and $B$ are drawn\naccording to some known distribution $P$ over pairs of subsets of $[n]$. A\ngiven estimation algorithm is evaluated based on its \"worst-case, expected\nerror\" where the expectation is with respect to the distribution $P$ from which\nthe sample $A$ and target sets $B$ are drawn, and the worst-case is with\nrespect to the data values $x_1,\\ldots,x_n$. Within this framework, we give an\nefficient algorithm for estimating the target mean that returns a weighted\ncombination of the sample values--where the weights are functions of the\ndistribution $P$ and the sample and target sets $A$, $B$--and show that the\nworst-case expected error achieved by this algorithm is at most a\nmultiplicative $\\pi/2$ factor worse than the optimal of such algorithms. The\nalgorithm and proof leverage a surprising connection to the Grothendieck\nproblem. This framework, which makes no distributional assumptions on the data\nvalues but rather relies on knowledge of the data collection process, is a\nsignificant departure from typical estimation and introduces a uniform\nalgorithmic analysis for the many natural settings where membership in a sample\nmay be correlated with data values, such as when sampling probabilities vary as\nin \"importance sampling\", when individuals are recruited into a sample via a\nsocial network as in \"snowball sampling\", or when samples have chronological\nstructure as in \"selective prediction\".\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 03:35:14 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 16:05:01 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Chen", "Justin Y.", ""], ["Valiant", "Gregory", ""], ["Valiant", "Paul", ""]]}, {"id": "1911.03620", "submitter": "Hossein Esfandiari", "authors": "Hossein Esfandiari, Amin Karbasi, Vahab Mirrokni", "title": "Adaptivity in Adaptive Submodularity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive sequential decision making is one of the central challenges in\nmachine learning and artificial intelligence. In such problems, the goal is to\ndesign an interactive policy that plans for an action to take, from a finite\nset of $n$ actions, given some partial observations. It has been shown that in\nmany applications such as active learning, robotics, sequential experimental\ndesign, and active detection, the utility function satisfies adaptive\nsubmodularity, a notion that generalizes the notion of diminishing returns to\npolicies. In this paper, we revisit the power of adaptivity in maximizing an\nadaptive monotone submodular function. We propose an efficient semi adaptive\npolicy that with $O(\\log n \\times\\log k)$ adaptive rounds of observations can\nachieve an almost tight $1-1/e-\\epsilon$ approximation guarantee with respect\nto an optimal policy that carries out $k$ actions in a fully sequential manner.\nTo complement our results, we also show that it is impossible to achieve a\nconstant factor approximation with $o(\\log n)$ adaptive rounds. We also extend\nour result to the case of adaptive stochastic minimum cost coverage where the\ngoal is to reach a desired utility $Q$ with the cheapest policy. We first prove\nthe conjecture of the celebrated work of Golovin and Krause by showing that the\ngreedy policy achieves the asymptotically tight logarithmic approximation\nguarantee without resorting to stronger notions of adaptivity. We then propose\na semi adaptive policy that provides the same guarantee in polylogarithmic\nadaptive rounds through a similar information-parallelism scheme. Our results\nshrink the adaptivity gap in adaptive submodular maximization by an exponential\nfactor.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 06:31:14 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 06:32:51 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Esfandiari", "Hossein", ""], ["Karbasi", "Amin", ""], ["Mirrokni", "Vahab", ""]]}, {"id": "1911.03683", "submitter": "William Lochet", "authors": "Eduard Eiben, William Lochet and Saket Saurabh", "title": "A Polynomial Kernel for Paw-Free Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a fixed graph $H$, the $H$-free-editing problem asks whether we can\nmodify a given graph $G$ by adding or deleting at most $k$ edges such that the\nresulting graph does not contain $H$ as an induced subgraph. The problem is\nknown to be NP-complete for all fixed $H$ with at least $3$ vertices and it\nadmits a $2^{O(k)}n^{O(1)}$ algorithm. Cai and Cai showed that the\n$H$-free-editing problem does not admit a polynomial kernel whenever $H$ or its\ncomplement is a path or a cycle with at least $4$ edges or a $3$-connected\ngraph with at least $1$ edge missing. Their results suggest that if $H$ is not\nindependent set or a clique, then $H$-free-editing admits polynomial kernels\nonly for few small graphs $H$, unless $\\textsf{coNP} \\in \\textsf{NP/poly}$.\nTherefore, resolving the kernelization of $H$-free-editing for small graphs $H$\nplays a crucial role in obtaining a complete dichotomy for this problem. In\nthis paper, we positively answer the question of compressibility for one of the\nlast two unresolved graphs $H$ on $4$ vertices. Namely, we give the first\npolynomial kernel for paw-free editing with $O(k^{6})$vertices.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 13:09:05 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Eiben", "Eduard", ""], ["Lochet", "William", ""], ["Saurabh", "Saket", ""]]}, {"id": "1911.03757", "submitter": "Nathaniel Harms", "authors": "Nathaniel Harms", "title": "Universal Communication, Universal Graphs, and Graph Labeling", "comments": "26 pages, 1 figure. To appear in ITCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a communication model called universal SMP, in which Alice and\nBob receive a function $f$ belonging to a family $\\mathcal{F}$, and inputs $x$\nand $y$. Alice and Bob use shared randomness to send a message to a third party\nwho cannot see $f, x, y$, or the shared randomness, and must decide $f(x,y)$.\nOur main application of universal SMP is to relate communication complexity to\ngraph labeling, where the goal is to give a short label to each vertex in a\ngraph, so that adjacency or other functions of two vertices $x$ and $y$ can be\ndetermined from the labels $\\ell(x),\\ell(y)$. We give a universal SMP protocol\nusing $O(k^2)$ bits of communication for deciding whether two vertices have\ndistance at most $k$ on distributive lattices (generalizing the $k$-Hamming\nDistance problem in communication complexity), and explain how this implies an\n$O(k^2\\log n)$ labeling scheme for determining $\\mathrm{dist}(x,y) \\leq k$ on\ndistributive lattices with size $n$; in contrast, we show that a universal SMP\nprotocol for determining $\\mathrm{dist}(x,y) \\leq 2$ in modular lattices (a\nsuperset of distributive lattices) has super-constant $\\Omega(n^{1/4})$\ncommunication cost. On the other hand, we demonstrate that many graph families\nknown to have efficient adjacency labeling schemes, such as trees,\nlow-arboricity graphs, and planar graphs, admit constant-cost communication\nprotocols for adjacency. Trees also have an $O(k)$ protocol for deciding\n$\\mathrm{dist}(x,y) \\leq k$ and planar graphs have an $O(1)$ protocol for\n$\\mathrm{dist}(x,y) \\leq 2$, which implies a new $O(\\log n)$ labeling scheme\nfor the same problem on planar graphs.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 19:11:08 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Harms", "Nathaniel", ""]]}, {"id": "1911.03858", "submitter": "Andrii Riazanov", "authors": "Venkatesan Guruswami, Andrii Riazanov, Min Ye", "title": "Ar{\\i}kan meets Shannon: Polar codes with near-optimal convergence to\n  channel capacity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $W$ be a binary-input memoryless symmetric (BMS) channel with Shannon\ncapacity $I(W)$ and fix any $\\alpha > 0$. We construct, for any sufficiently\nsmall $\\delta > 0$, binary linear codes of block length\n$O(1/\\delta^{2+\\alpha})$ and rate $I(W)-\\delta$ that enable reliable\ncommunication on $W$ with quasi-linear time encoding and decoding. Shannon's\nnoisy coding theorem established the \\emph{existence} of such codes (without\nefficient constructions or decoding) with block length $O(1/\\delta^2)$. This\nquadratic dependence on the gap $\\delta$ to capacity is known to be best\npossible. Our result thus yields a constructive version of Shannon's theorem\nwith near-optimal convergence to capacity as a function of the block length.\nThis resolves a central theoretical challenge associated with the attainment of\nShannon capacity. Previously such a result was only known for the erasure\nchannel.\n  Our codes are a variant of Ar{\\i}kan's polar codes based on multiple\ncarefully constructed local kernels, one for each intermediate channel that\narises in the decoding. A crucial ingredient in the analysis is a strong\nconverse of the noisy coding theorem when communicating using random linear\ncodes on arbitrary BMS channels. Our converse theorem shows extreme\nunpredictability of even a single message bit for random coding at rates\nslightly above capacity.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 05:45:33 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 20:51:11 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Riazanov", "Andrii", ""], ["Ye", "Min", ""]]}, {"id": "1911.04014", "submitter": "Yuval Dagan", "authors": "Yuval Dagan, Vitaly Feldman", "title": "Interaction is necessary for distributed learning with privacy or\n  communication constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local differential privacy (LDP) is a model where users send privatized data\nto an untrusted central server whose goal it to solve some data analysis task.\nIn the non-interactive version of this model the protocol consists of a single\nround in which a server sends requests to all users then receives their\nresponses. This version is deployed in industry due to its practical advantages\nand has attracted significant research interest. Our main result is an\nexponential lower bound on the number of samples necessary to solve the\nstandard task of learning a large-margin linear separator in the\nnon-interactive LDP model. Via a standard reduction this lower bound implies an\nexponential lower bound for stochastic convex optimization and specifically,\nfor learning linear models with a convex, Lipschitz and smooth loss. These\nresults answer the questions posed in \\citep{SmithTU17,DanielyF18}. Our lower\nbound relies on a new technique for constructing pairs of distributions with\nnearly matching moments but whose supports can be nearly separated by a large\nmargin hyperplane. These lower bounds also hold in the model where\ncommunication from each user is limited and follow from a lower bound on\nlearning using non-adaptive \\emph{statistical queries}.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 00:06:17 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 09:04:56 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Dagan", "Yuval", ""], ["Feldman", "Vitaly", ""]]}, {"id": "1911.04198", "submitter": "Adri\\'an G\\'omez-Brand\\'on", "authors": "Nieves R. Brisaboa, Adri\\'an G\\'omez-Brand\\'on, Gonzalo Navarro,\n  Jos\\'e R. Param\\'a", "title": "GraCT: A Grammar-based Compressed Index for Trajectory Data", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "Information Sciences, 2019, vol. 483, p. 106-135", "doi": "10.1016/j.ins.2019.01.035", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a compressed data structure for the storage of free trajectories\nof moving objects (such as ships and planes) that efficiently supports various\nspatio-temporal queries. Our structure, dubbed GraCT, stores the absolute\npositions of all the objects at regular time intervals (snapshots) using a\n$k^2$-tree, which is a space- and time-efficient version of a region quadtree.\nPositions between snapshots are represented as logs of relative movements and\ncompressed using Re-Pair, a grammar-based compressor. The nonterminals of this\ngrammar are enhanced with MBR information to enable fast queries.\n  The GraCT structure of a dataset occupies less than the raw data compressed\nwith a powerful traditional compressor such as p7zip. Further, instead of\nrequiring full decompression to access the data like a traditional compressor,\nGraCT supports direct access to object trajectories or to their position at\nspecific time instants, as well as spatial range and nearest-neighbor queries\non time instants and/or time intervals.\n  Compared to traditional methods for storing and indexing spatio-temporal\ndata, GraCT requires two orders of magnitude less space, and is competitive in\nquery times. In particular, thanks to its compressed representation, the GraCT\nstructure may reside in main memory in situations where any classical\nuncompressed index must resort to disk, thereby being one or two orders of\nmagnitude faster.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 11:54:20 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["G\u00f3mez-Brand\u00f3n", "Adri\u00e1n", ""], ["Navarro", "Gonzalo", ""], ["Param\u00e1", "Jos\u00e9 R.", ""]]}, {"id": "1911.04202", "submitter": "Adri\\'an G\\'omez-Brand\\'on", "authors": "Nieves R. Brisaboa, Antonio Fari\\~na, Adri\\'an G\\'omez-Brand\\'on,\n  Gonzalo Navarro, Tirso V. Rodeiro", "title": "Dv2v: A Dynamic Variable-to-Variable Compressor", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "Dv2v: A Dynamic Variable-to-Variable Compressor. In 2019 Data\n  Compression Conference (DCC) (pp. 83-92). IEEE", "doi": "10.1109/DCC.2019.00016", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Dv2v, a new dynamic (one-pass) variable-to-variable compressor.\nVariable-to-variable compression aims at using a modeler that gathers\nvariable-length input symbols and a variable-length statistical coder that\nassigns shorter codewords to the more frequent symbols. In Dv2v, we process the\ninput text word-wise to gather variable-length symbols that can be either\nterminals (new words) or non-terminals, subsequences of words seen before in\nthe input text. Those input symbols are set in a vocabulary that is kept sorted\nby frequency. Therefore, those symbols can be easily encoded with dense codes.\nOur Dv2v permits real-time transmission of data, i.e. compression/transmission\ncan begin as soon as data become available. Our experiments show that Dv2v is\nable to overcome the compression ratios of the v2vDC, the state-of-the-art\nsemi-static variable-to-variable compressor, and to almost reach p7zip values.\nIt also draws a competitive performance at both compression and decompression.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 12:05:37 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Fari\u00f1a", "Antonio", ""], ["G\u00f3mez-Brand\u00f3n", "Adri\u00e1n", ""], ["Navarro", "Gonzalo", ""], ["Rodeiro", "Tirso V.", ""]]}, {"id": "1911.04249", "submitter": "Jungho Ahn", "authors": "Jungho Ahn, Eduard Eiben, O-joung Kwon, and Sang-il Oum", "title": "A polynomial kernel for $3$-leaf power deletion", "comments": "33 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a non-negative integer $\\ell$, a graph $G$ is an $\\ell$-leaf power of a\ntree $T$ if $V(G)$ is equal to the set of leaves of $T$, and distinct vertices\n$v$ and $w$ of $G$ are adjacent if and only if the distance between $v$ and $w$\nin $T$ is at most $\\ell$. Given a graph $G$, 3-Leaf Power Deletion asks whether\nthere is a set $S\\subseteq V(G)$ of size at most $k$ such that $G\\setminus S$\nis a $3$-leaf power of some tree $T$. We provide a polynomial kernel for this\nproblem. More specifically, we present a polynomial-time algorithm for an input\ninstance $(G,k)$ to output an equivalent instance $(G',k')$ such that $k'\\leq\nk$ and $G'$ has at most $O(k^{14})$ vertices.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 13:33:13 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 17:13:09 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 01:09:16 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Ahn", "Jungho", ""], ["Eiben", "Eduard", ""], ["Kwon", "O-joung", ""], ["Oum", "Sang-il", ""]]}, {"id": "1911.04372", "submitter": "Vladan Majerech", "authors": "Vladan Majerech", "title": "Information carefull worstcase DecreaseKey heaps with simple nonMeld\n  variant", "comments": "10 pages including one figure and 4 tables + references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze priority queues including DecreaseKey method in its interface. The\npaper is inspired by Strict Fibonacci Heaps [2], where G. S. Brodal, G.\nLagogiannis, and R. E. Tarjan implemented the heap with DecreaseKey and Meld\ninterface in assymptotically optimal worst case times (based on key\ncomparisons). At the end of the paper there are mentioned possible variants of\nother structural properties an violations than they have used in the analysis.\nIn the main variant a lot of information is wasted during violation reduction\nsteps. Our goal is to concentrate on other variants and to invent natural\nstrategy not losing that much in the information value. In other words we try\nto choose among them one which corresponds to superexpensive comparision\nprinciple as much as possible. The principle was described in [5] of myself,\nbut after publication I have found these ideas in [4] of H. Kaplan, R. E.\nTarjan, and U. Zwick.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 16:28:22 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Majerech", "Vladan", ""]]}, {"id": "1911.04382", "submitter": "Zhuo Feng", "authors": "Zhuo Feng", "title": "GRASS: Graph Spectral Sparsification Leveraging Scalable Spectral\n  Perturbation Analysis", "comments": "14 pages, 13 figures. arXiv admin note: substantial text overlap with\n  arXiv:1711.05135", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA cs.SI math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral graph sparsification aims to find ultra-sparse subgraphs whose\nLaplacian matrix can well approximate the original Laplacian eigenvalues and\neigenvectors. In recent years, spectral sparsification techniques have been\nextensively studied for accelerating various numerical and graph-related\napplications. Prior nearly-linear-time spectral sparsification methods first\nextract low-stretch spanning tree from the original graph to form the backbone\nof the sparsifier, and then recover small portions of spectrally-critical\noff-tree edges to the spanning tree to significantly improve the approximation\nquality. However, it is not clear how many off-tree edges should be recovered\nfor achieving a desired spectral similarity level within the sparsifier.\nMotivated by recent graph signal processing techniques, this paper proposes a\nsimilarity-aware spectral graph sparsification framework that leverages\nefficient spectral off-tree edge embedding and filtering schemes to construct\nspectral sparsifiers with guaranteed spectral similarity (relative condition\nnumber) level. An iterative graph densification scheme is also introduced to\nfacilitate efficient and effective filtering of off-tree edges for highly\nill-conditioned problems. The proposed method has been validated using various\nkinds of graphs obtained from public domain sparse matrix collections relevant\nto VLSI CAD, finite element analysis, as well as social and data networks\nfrequently studied in many machine learning and data mining applications. For\ninstance, a sparse SDD matrix with 40 million unknowns and 180 million nonzeros\ncan be solved (1E-3 accuracy level) within two minutes using a single CPU core\nand about 6GB memory.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 00:47:36 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 12:33:59 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 01:17:42 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Feng", "Zhuo", ""]]}, {"id": "1911.04415", "submitter": "Cyrille W. Combettes", "authors": "Cyrille W. Combettes and Sebastian Pokutta", "title": "Revisiting the Approximate Carath\\'eodory Problem via the Frank-Wolfe\n  Algorithm", "comments": "21 pages and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approximate Carath\\'eodory theorem states that given a compact convex set\n$\\mathcal{C}\\subset\\mathbb{R}^n$ and $p\\in\\left[2,+\\infty\\right[$, each point\n$x^*\\in\\mathcal{C}$ can be approximated to $\\epsilon$-accuracy in the\n$\\ell_p$-norm as the convex combination of $\\mathcal{O}(pD_p^2/\\epsilon^2)$\nvertices of $\\mathcal{C}$, where $D_p$ is the diameter of $\\mathcal{C}$ in the\n$\\ell_p$-norm. A solution satisfying these properties can be built using\nprobabilistic arguments or by applying mirror descent to the dual problem. We\nrevisit the approximate Carath\\'eodory problem by solving the primal problem\nvia the Frank-Wolfe algorithm, providing a simplified analysis and leading to\nan efficient practical method. Furthermore, improved cardinality bounds are\nderived naturally using existing convergence rates of the Frank-Wolfe algorithm\nin different scenarios, when $x^*$ is in the (relative) interior of\n$\\mathcal{C}$, when $x^*$ is the convex combination of a subset of vertices\nwith small diameter, or when $\\mathcal{C}$ is uniformly convex. We also propose\ncardinality bounds when $p\\in\\left[1,2\\right[\\cup\\{+\\infty\\}$ via a nonsmooth\nvariant of the algorithm. Lastly, we address the problem of finding sparse\napproximate projections onto $\\mathcal{C}$ in the $\\ell_p$-norm,\n$p\\in\\left[1,+\\infty\\right]$.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 17:41:58 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 04:12:32 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 08:53:28 GMT"}, {"version": "v4", "created": "Tue, 13 Apr 2021 17:53:30 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Combettes", "Cyrille W.", ""], ["Pokutta", "Sebastian", ""]]}, {"id": "1911.04629", "submitter": "Quan Nguyen Hoang", "authors": "Quan Nguyen, Andre Cronje, Michael Kong", "title": "Fast Stochastic Peer Selection in Proof-of-Stake Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of peer selection, which randomly selects a peer from a set, is\ncommonplace in Proof-of-Stake (PoS) protocols. In PoS, peers are chosen\nrandomly with probability proportional to the amount of stake that they\npossess. This paper presents an approach that relates PoS peer selection to\nRoulette-wheel selection, which is frequently used in genetic and evolutionary\nalgorithms or complex network modelling. In particular, we introduce the use of\nstochastic acceptance algorithm [6] for fast peer selection. The roulette-wheel\nselection algorithm [6] achieves O(1) complexity based on stochastic\nacceptance, whereas searching based algorithms may take O(N ) or O(logN )\ncomplexity in a network of N peers.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 01:24:39 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Nguyen", "Quan", ""], ["Cronje", "Andre", ""], ["Kong", "Michael", ""]]}, {"id": "1911.04676", "submitter": "Indraneel Patil", "authors": "Indraneel Patil, B.K. Rout, V. Kalaichelvi", "title": "Prediction of Bottleneck Points for Manipulation Planning in Cluttered\n  Environment using a 3D Convolutional Neural Network", "comments": "7 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.DS cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latest research in industrial robotics is aimed at making human robot\ncollaboration possible seamlessly. For this purpose, industrial robots are\nexpected to work on the fly in unstructured and cluttered environments and\nhence the subject of perception driven motion planning plays a vital role.\nSampling based motion planners are proven to be the most effective for such\nhigh dimensional planning problems with real time constraints. Unluckily random\nstochastic samplers suffer from the phenomenon of 'narrow passages' or\nbottleneck regions which need targeted sampling to improve their convergence\nrate. Also identifying these bottleneck regions in a diverse set of planning\nproblems is a challenge. In this paper an attempt has been made to address\nthese two problems by designing an intelligent 'bottleneck guided' heuristic\nfor a Rapidly Exploring Random Tree Star (RRT*) planner which is based on\nrelevant context extracted from the planning scenario using a 3D Convolutional\nNeural Network and it is also proven that the proposed technique generalises to\nunseen problem instances. This paper benchmarks the technique (bottleneck\nguided RRT*) against a 10% Goal biased RRT star planner, shows significant\nimprovement in planning time and memory requirement and uses ABB 1410\nindustrial manipulator as a platform for implantation and validation of the\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 05:16:40 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Patil", "Indraneel", ""], ["Rout", "B. K.", ""], ["Kalaichelvi", "V.", ""]]}, {"id": "1911.04681", "submitter": "Abhratanu Dutta", "authors": "Pranjal Awasthi, Abhratanu Dutta and Aravindan Vijayaraghavan", "title": "On Robustness to Adversarial Examples and Polynomial Optimization", "comments": "To appear at NeurIPS2019. 30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the design of computationally efficient algorithms with provable\nguarantees, that are robust to adversarial (test time) perturbations. While\nthere has been an proliferation of recent work on this topic due to its\nconnections to test time robustness of deep networks, there is limited\ntheoretical understanding of several basic questions like (i) when and how can\none design provably robust learning algorithms? (ii) what is the price of\nachieving robustness to adversarial examples in a computationally efficient\nmanner?\n  The main contribution of this work is to exhibit a strong connection between\nachieving robustness to adversarial examples, and a rich class of polynomial\noptimization problems, thereby making progress on the above questions. In\nparticular, we leverage this connection to (a) design computationally efficient\nrobust algorithms with provable guarantees for a large class of hypothesis,\nnamely linear classifiers and degree-2 polynomial threshold functions (PTFs),\n(b) give a precise characterization of the price of achieving robustness in a\ncomputationally efficient manner for these classes, (c) design efficient\nalgorithms to certify robustness and generate adversarial attacks in a\nprincipled manner for 2-layer neural networks. We empirically demonstrate the\neffectiveness of these attacks on real data.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 05:33:06 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Dutta", "Abhratanu", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1911.04686", "submitter": "Kangning Wang", "authors": "Nick Gravin, Zhihao Gavin Tang and Kangning Wang", "title": "Online Stochastic Matching with Edge Arrivals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online bipartite matching with edge arrivals remained a major open question\nfor a long time until a recent negative result by [Gamlath et al. FOCS 2019],\nwho showed that no online policy is better than the straightforward greedy\nalgorithm, i.e., no online algorithm has a worst-case competitive ratio better\nthan $0.5$. In this work, we consider the bipartite matching problem with edge\narrivals in a natural stochastic framework, i.e., Bayesian setting where each\nedge of the graph is independently realized according to a known probability\ndistribution.\n  We focus on a natural class of prune & greedy online policies motivated by\npractical considerations from a multitude of online matching platforms. Any\nprune & greedy algorithm consists of two stages: first, it decreases the\nprobabilities of some edges in the stochastic instance and then runs greedy\nalgorithm on the pruned graph. We propose prune & greedy algorithms that are\n$0.552$-competitive on the instances that can be pruned to a $2$-regular\nstochastic bipartite graph, and $0.503$-competitive on arbitrary bipartite\ngraphs. The algorithms and our analysis significantly deviate from the prior\nwork. We first obtain analytically manageable lower bound on the size of the\nmatching, which leads to a non linear optimization problem. We further reduce\nthis problem to a continuous optimization with a constant number of parameters\nthat can be solved using standard software tools.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 05:49:39 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 18:35:25 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 19:01:21 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Gravin", "Nick", ""], ["Tang", "Zhihao Gavin", ""], ["Wang", "Kangning", ""]]}, {"id": "1911.04931", "submitter": "Mohammad Mahdi Kamani", "authors": "Mohammad Mahdi Kamani, Farzin Haddadpour, Rana Forsati, Mehrdad\n  Mahdavi", "title": "Efficient Fair Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that dimension reduction methods such as PCA may be\ninherently prone to unfairness and treat data from different sensitive groups\nsuch as race, color, sex, etc., unfairly. In pursuit of fairness-enhancing\ndimensionality reduction, using the notion of Pareto optimality, we propose an\nadaptive first-order algorithm to learn a subspace that preserves fairness,\nwhile slightly compromising the reconstruction loss. Theoretically, we provide\nsufficient conditions that the solution of the proposed algorithm belongs to\nthe Pareto frontier for all sensitive groups; thereby, the optimal trade-off\nbetween overall reconstruction loss and fairness constraints is guaranteed. We\nalso provide the convergence analysis of our algorithm and show its efficacy\nthrough empirical studies on different datasets, which demonstrates superior\nperformance in comparison with state-of-the-art algorithms. The proposed\nfairness-aware PCA algorithm can be efficiently generalized to multiple group\nsensitive features and effectively reduce the unfairness decisions in\ndownstream tasks such as classification.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 15:29:05 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 01:31:11 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Kamani", "Mohammad Mahdi", ""], ["Haddadpour", "Farzin", ""], ["Forsati", "Rana", ""], ["Mahdavi", "Mehrdad", ""]]}, {"id": "1911.05032", "submitter": "Tom\\'a\\v{s} Masa\\v{r}\\'ik", "authors": "Julien Baste, Lars Jaffke, Tom\\'a\\v{s} Masa\\v{r}\\'ik, Geevarghese\n  Philip, G\\\"unter Rote", "title": "FPT Algorithms for Diverse Collections of Hitting Sets", "comments": "17 pages, 3 figures", "journal-ref": "Algorithms 12(12):254 (2019)", "doi": "10.3390/a12120254", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the $d$-Hitting Set and Feedback Vertex Set problems\nthrough the paradigm of finding diverse collections of $r$ solutions of size at\nmost $k$ each, which has recently been introduced to the field of parameterized\ncomplexity [Baste et al., 2019]. This paradigm is aimed at addressing the loss\nof important side information which typically occurs during the abstraction\nprocess which models real-world problems as computational problems. We use two\nmeasures for the diversity of such a collection: the sum of all pairwise\nHamming distances, and the minimum pairwise Hamming distance. We show that both\nproblems are FPT in $k + r$ for both diversity measures. A key ingredient in\nour algorithms is a (problem independent) network flow formulation that, given\na set of `base' solutions, computes a maximally diverse collection of\nsolutions. We believe that this could be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 17:49:43 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 16:17:37 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Baste", "Julien", ""], ["Jaffke", "Lars", ""], ["Masa\u0159\u00edk", "Tom\u00e1\u0161", ""], ["Philip", "Geevarghese", ""], ["Rote", "G\u00fcnter", ""]]}, {"id": "1911.05060", "submitter": "Ioana O. Bercea", "authors": "Ioana O. Bercea and Guy Even", "title": "Fully-Dynamic Space-Efficient Dictionaries and Filters with Constant\n  Number of Memory Accesses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fully-dynamic dictionary is a data structure for maintaining sets that\nsupports insertions, deletions and membership queries. A filter approximates\nmembership queries with a one-sided error. We present two designs:\n  1. The first space-efficient fully-dynamic dictionary that maintains both\nsets and random multisets and supports queries, insertions, and deletions with\na constant number of memory accesses in the worst case with high probability.\nThe comparable dictionary of Arbitman, Naor, and Segev [FOCS 2010] works only\nfor sets.\n  2. By a reduction from our dictionary for random multisets, we obtain a\nspace-efficient fully-dynamic filter that supports queries, insertions, and\ndeletions with a constant number of memory accesses in the worst case with high\nprobability (as long as the false positive probability is $2^{-O(w)}$, where\n$w$ denotes the word length). This is the first in-memory space-efficient\nfully-dynamic filter design that provably achieves these properties.\n  We also present an application of the techniques used to design our\ndictionary to the static Retrieval Problem.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 18:35:28 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Bercea", "Ioana O.", ""], ["Even", "Guy", ""]]}, {"id": "1911.05333", "submitter": "Yuan Tang", "authors": "Yuan Tang", "title": "Nested Dataflow Algorithms for Dynamic Programming Recurrences with more\n  than O(1) Dependency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic programming problems have wide applications in real world and have\nbeen studied extensively in both serial and parallel settings. In 1994, Galil\nand Park developed work-efficient and sublinear-time algorithms for several\nimportant dynamic programming problems based on the closure method and matrix\nproduct method. However, in the same paper, they raised an open question\nwhether such an algorithm exists for the general GAP problem. % In this paper,\nwe answer their question by developing the first work-efficient and\nsublinear-time GAP algorithm based on the closure method and Nested Dataflow\nmethod. % We also improve the time bounds of classic work-efficient,\ncache-oblivious and cache-efficient algorithms for the 1D problem and GAP\nproblem, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 07:24:05 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Tang", "Yuan", ""]]}, {"id": "1911.05416", "submitter": "Warut Suksompong", "authors": "Paul W. Goldberg, Alexandros Hollender, Warut Suksompong", "title": "Contiguous Cake Cutting: Hardness Results and Approximation Algorithms", "comments": "Appears in the 34th AAAI Conference on Artificial Intelligence\n  (AAAI), 2020", "journal-ref": "Journal of Artificial Intelligence Research, 69:109-141 (2020)", "doi": "10.1613/jair.1.12222", "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fair allocation of a cake, which serves as a metaphor for a\ndivisible resource, under the requirement that each agent should receive a\ncontiguous piece of the cake. While it is known that no finite envy-free\nalgorithm exists in this setting, we exhibit efficient algorithms that produce\nallocations with low envy among the agents. We then establish NP-hardness\nresults for various decision problems on the existence of envy-free\nallocations, such as when we fix the ordering of the agents or constrain the\npositions of certain cuts. In addition, we consider a discretized setting where\nindivisible items lie on a line and show a number of hardness results extending\nand strengthening those from prior work. Finally, we investigate connections\nbetween approximate and exact envy-freeness, as well as between continuous and\ndiscrete cake cutting.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 12:06:48 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 21:36:24 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Goldberg", "Paul W.", ""], ["Hollender", "Alexandros", ""], ["Suksompong", "Warut", ""]]}, {"id": "1911.05520", "submitter": "Marcelo Garlet Milani", "authors": "Marcelo Garlet Milani", "title": "A Polynomial Kernel for Funnel Arc Deletion Set", "comments": "Accepted at IPEC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Directed Feeback Arc Set (DFAS) we search for a set of at most $k$ arcs\nwhich intersect every cycle in the input digraph. It is a well-known open\nproblem in parameterized complexity to decide if DFAS admits a kernel of\npolynomial size. We consider $\\mathcal{C}$-Arc Deletion Set\n($\\mathcal{C}$-ADS), a variant of DFAS where we want to remove at most $k$ arcs\nfrom the input digraph in order to turn it into a digraph of a class\n$\\mathcal{C}$. In this work, we choose $\\mathcal{C}$ to be the class of\nfunnels. Funnel-Arc Deletion Set is NP-hard even if the input is a DAG, but is\nfixed-parameter tractable with respect to $k$. So far no polynomial kernels for\nthis problem were known. Our main result is a kernel for Funnel-Arc Deletion\nSet with $\\mathcal{O}(k^6)$ many vertices and $\\mathcal{O}(k^7)$ many arcs,\ncomputable in $\\mathcal{O}(nm)$ time, where $n$ is the number of vertices and\n$m$ the number of arcs in the input digraph.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 14:56:19 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 14:08:29 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Milani", "Marcelo Garlet", ""]]}, {"id": "1911.05545", "submitter": "David Wajc", "authors": "David Wajc", "title": "Rounding Dynamic Matchings Against an Adaptive Adversary", "comments": "Full version of STOC 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new dynamic matching sparsification scheme. From this scheme we\nderive a framework for dynamically rounding fractional matchings against\n\\emph{adaptive adversaries}. Plugging in known dynamic fractional matching\nalgorithms into our framework, we obtain numerous randomized dynamic matching\nalgorithms which work against adaptive adversaries (the first such algorithms,\nas all previous randomized algorithms for this problem assumed an\n\\emph{oblivious} adversary). In particular, for any constant $\\epsilon>0$, our\nframework yields $(2+\\epsilon)$-approximate algorithms with constant update\ntime or polylog worst-case update time, as well as $(2-\\delta)$-approximate\nalgorithms in bipartite graphs with arbitrarily-small polynomial update time,\nwith all these algorithms' guarantees holding against adaptive adversaries. All\nthese results achieve \\emph{polynomially} better update time to approximation\ntradeoffs than previously known to be achievable against adaptive adversaries.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 15:27:43 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 13:18:35 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Wajc", "David", ""]]}, {"id": "1911.05582", "submitter": "Nikolaos Tziavelis", "authors": "Nikolaos Tziavelis, Deepak Ajwani, Wolfgang Gatterbauer, Mirek\n  Riedewald, Xiaofeng Yang", "title": "Optimal Algorithms for Ranked Enumeration of Answers to Full Conjunctive\n  Queries", "comments": "50 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study ranked enumeration of join-query results according to very general\norders defined by selective dioids. Our main contribution is a framework for\nranked enumeration over a class of dynamic programming problems that\ngeneralizes seemingly different problems that had been studied in isolation. To\nthis end, we extend classic algorithms that find the k-shortest paths in a\nweighted graph. For full conjunctive queries, including cyclic ones, our\napproach is optimal in terms of the time to return the top result and the delay\nbetween results. These optimality properties are derived for the widely used\nnotion of data complexity, which treats query size as a constant. By performing\na careful cost analysis, we are able to uncover a previously unknown tradeoff\nbetween two incomparable enumeration approaches: one has lower complexity when\nthe number of returned results is small, the other when the number is very\nlarge. We theoretically and empirically demonstrate the superiority of our\ntechniques over batch algorithms, which produce the full result and then sort\nit. Our technique is not only faster for returning the first few results, but\non some inputs beats the batch algorithm even when all results are produced.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 16:21:36 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 17:49:09 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2020 19:21:22 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Tziavelis", "Nikolaos", ""], ["Ajwani", "Deepak", ""], ["Gatterbauer", "Wolfgang", ""], ["Riedewald", "Mirek", ""], ["Yang", "Xiaofeng", ""]]}, {"id": "1911.05624", "submitter": "Mohammad Khosravi", "authors": "Mohammadjavad Khosravi, Hamid Saeedi, Hossein Pishro-Nik", "title": "Multi-Purpose Aerial Drones for Network Coverage and Package Delivery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned aerial vehicles (UAVs) have become important in many applications\nincluding last-mile deliveries, surveillance and monitoring, and wireless\nnetworks. This paper aims to design UAV trajectories that simultaneously\nperform multiple tasks. We aim to design UAV trajectories that minimize package\ndelivery time, and at the same time provide uniform coverage over a\nneighborhood area which is needed for applications such as network coverage or\nsurveillance. We first consider multi-task UAVs for a simplified scenario where\nthe neighborhood area is a circular region with the post office located at its\ncenter and the houses are assumed to be uniformly distributed on the circle\nboundary. We propose a trajectory process such that if according to which the\ndrones move, a uniform coverage can be achieved while the delivery efficiency\nis still preserved. We then consider a more practical scenario in which the\ndelivery destinations are arbitrarily distributed in an arbitrarily-shaped\nregion. We also do not assume any restrictions on the package arrivals. We show\nthat simultaneous uniform coverage and efficient package delivery is possible\nfor such realistic scenarios. This is shown using both rigorous analyses as\nwell as simulations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 17:04:13 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Khosravi", "Mohammadjavad", ""], ["Saeedi", "Hamid", ""], ["Pishro-Nik", "Hossein", ""]]}, {"id": "1911.05656", "submitter": "Aditi Laddha", "authors": "Aditi Laddha, Yin Tat Lee, Santosh Vempala", "title": "Strong Self-Concordance and Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the Dikin walk, we develop aspects of an interior-point theory\nfor sampling in high dimension. Specifically, we introduce a symmetric\nparameter and the notion of strong self-concordance. These properties imply\nthat the corresponding Dikin walk mixes in $\\tilde{O}(n\\bar{\\nu})$ steps from a\nwarm start in a convex body in $\\mathbb{R}^{n}$ using a strongly\nself-concordant barrier with symmetric self-concordance parameter $\\bar{\\nu}$.\nFor many natural barriers, $\\bar{\\nu}$ is roughly bounded by $\\nu$, the\nstandard self-concordance parameter. We show that this property and strong\nself-concordance hold for the Lee-Sidford barrier. As a consequence, we obtain\nthe first walk to mix in $\\tilde{O}(n^{2})$ steps for an arbitrary polytope in\n$\\mathbb{R}^{n}$. Strong self-concordance for other barriers leads to an\ninteresting (and unexpected) connection -- for the universal and entropic\nbarriers, it is implied by the KLS conjecture.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 17:34:28 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 22:25:04 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Laddha", "Aditi", ""], ["Lee", "Yin Tat", ""], ["Vempala", "Santosh", ""]]}, {"id": "1911.05676", "submitter": "M. O\\u{g}uzhan K\\\"ulekci", "authors": "M. O\\u{g}uzhan K\\\"ulekci, Yasin \\\"Ozt\\\"urk, Elif Altunok, Can\n  Alt{\\i}ni\\u{g}ne", "title": "Enumerative Data Compression with Non-Uniquely Decodable Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-uniquely decodable codes can be defined as the codes that cannot be\nuniquely decoded without additional disambiguation information. These are\nmainly the class of non-prefix-free codes, where a codeword can be a prefix of\nother(s), and thus, the codeword boundary information is essential for correct\ndecoding. Although the codeword bit stream consumes significantly less space\nwhen compared to prefix--free codes, the additional disambiguation information\nmakes it difficult to catch the performance of prefix-free codes in total.\nPrevious studies considered compression with non-prefix-free codes by\nintegrating rank/select dictionaries or wavelet trees to mark the code-word\nboundaries. In this study we focus on another dimension with a block--wise\nenumeration scheme that improves the compression ratios of the previous studies\nsignificantly. Experiments conducted on a known corpus showed that the proposed\nscheme successfully represents a source within its entropy, even performing\nbetter than the Huffman and arithmetic coding in some cases. The non-uniquely\ndecodable codes also provides an intrinsic security feature due to lack of\nunique-decodability. We investigate this dimension as an opportunity to provide\ncompressed data security without (or with less) encryption, and discuss various\npossible practical advantages supported by such codes.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 17:55:06 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["K\u00fclekci", "M. O\u011fuzhan", ""], ["\u00d6zt\u00fcrk", "Yasin", ""], ["Altunok", "Elif", ""], ["Alt\u0131ni\u011fne", "Can", ""]]}, {"id": "1911.05896", "submitter": "Noujan Pashanasangi", "authors": "Suman K. Bera, Noujan Pashanasangi, C. Seshadhri", "title": "Linear Time Subgraph Counting, Graph Degeneracy, and the Chasm at Size\n  Six", "comments": "The previous version did not handle the case of k=8. We corrected\n  that in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of counting all $k$-vertex subgraphs in an input\ngraph, for any constant $k$. This problem (denoted sub-cnt$_k$) has been\nstudied extensively in both theory and practice. In a classic result, Chiba and\nNishizeki (SICOMP 85) gave linear time algorithms for clique and 4-cycle\ncounting for bounded degeneracy graphs. This is a rich class of sparse graphs\nthat contains, for example, all minor-free families and preferential attachment\ngraphs. The techniques from this result have inspired a number of recent\npractical algorithms for sub-cnt$_k$. Towards a better understanding of the\nlimits of these techniques, we ask: for what values of $k$ can sub-cnt$_k$ be\nsolved in linear time?\n  We discover a chasm at $k=6$. Specifically, we prove that for $k < 6$,\nsub-cnt$_k$ can be solved in linear time. Assuming a standard conjecture in\nfine-grained complexity, we prove that for all $k \\geq 6$, sub-cnt$_k$ cannot\nbe solved even in near-linear time.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 02:15:15 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 21:10:53 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Bera", "Suman K.", ""], ["Pashanasangi", "Noujan", ""], ["Seshadhri", "C.", ""]]}, {"id": "1911.05911", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane", "title": "Recent Advances in Algorithmic High-Dimensional Robust Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in the presence of outliers is a fundamental problem in statistics.\nUntil recently, all known efficient unsupervised learning algorithms were very\nsensitive to outliers in high dimensions. In particular, even for the task of\nrobust mean estimation under natural distributional assumptions, no efficient\nalgorithm was known. Recent work in theoretical computer science gave the first\nefficient robust estimators for a number of fundamental statistical tasks,\nincluding mean and covariance estimation. Since then, there has been a flurry\nof research activity on algorithmic high-dimensional robust estimation in a\nrange of settings. In this survey article, we introduce the core ideas and\nalgorithmic techniques in the emerging area of algorithmic high-dimensional\nrobust statistics with a focus on robust mean estimation. We also provide an\noverview of the approaches that have led to computationally efficient robust\nestimators for a range of broader statistical tasks and discuss new directions\nand opportunities for future work.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 02:56:56 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""]]}, {"id": "1911.05949", "submitter": "Haoyu Zhao", "authors": "Haoyu Zhao, Wei Chen", "title": "Online Second Price Auction with Semi-bandit Feedback Under the\n  Non-Stationary Setting", "comments": "Accepted to AAAI-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the non-stationary online second price auction\nproblem. We assume that the seller is selling the same type of items in $T$\nrounds by the second price auction, and she can set the reserve price in each\nround. In each round, the bidders draw their private values from a joint\ndistribution unknown to the seller. Then, the seller announced the reserve\nprice in this round. Next, bidders with private values higher than the\nannounced reserve price in that round will report their values to the seller as\ntheir bids. The bidder with the highest bid larger than the reserved price\nwould win the item and she will pay to the seller the price equal to the\nsecond-highest bid or the reserve price, whichever is larger. The seller wants\nto maximize her total revenue during the time horizon $T$ while learning the\ndistribution of private values over time. The problem is more challenging than\nthe standard online learning scenario since the private value distribution is\nnon-stationary, meaning that the distribution of bidders' private values may\nchange over time, and we need to use the \\emph{non-stationary regret} to\nmeasure the performance of our algorithm. To our knowledge, this paper is the\nfirst to study the repeated auction in the non-stationary setting\ntheoretically. Our algorithm achieves the non-stationary regret upper bound\n$\\tilde{\\mathcal{O}}(\\min\\{\\sqrt{\\mathcal S T},\n\\bar{\\mathcal{V}}^{\\frac{1}{3}}T^{\\frac{2}{3}}\\})$, where $\\mathcal S$ is the\nnumber of switches in the distribution, and $\\bar{\\mathcal{V}}$ is the sum of\ntotal variation, and $\\mathcal S$ and $\\bar{\\mathcal{V}}$ are not needed to be\nknown by the algorithm. We also prove regret lower bounds\n$\\Omega(\\sqrt{\\mathcal S T})$ in the switching case and\n$\\Omega(\\bar{\\mathcal{V}}^{\\frac{1}{3}}T^{\\frac{2}{3}})$ in the dynamic case,\nshowing that our algorithm has nearly optimal \\emph{non-stationary regret}.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 05:46:42 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Zhao", "Haoyu", ""], ["Chen", "Wei", ""]]}, {"id": "1911.05991", "submitter": "Taisuke Yasuda", "authors": "Manuel Fernandez, David P. Woodruff, Taisuke Yasuda", "title": "Graph Spanners in the Message-Passing Model", "comments": "ITCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph spanners are sparse subgraphs which approximately preserve all pairwise\nshortest-path distances in an input graph. The notion of approximation can be\nadditive, multiplicative, or both, and many variants of this problem have been\nextensively studied. We study the problem of computing a graph spanner when the\nedges of the input graph are distributed across two or more sites in an\narbitrary, possibly worst-case partition, and the goal is for the sites to\nminimize the communication used to output a spanner. We assume the\nmessage-passing model of communication, for which there is a point-to-point\nlink between all pairs of sites as well as a coordinator who is responsible for\nproducing the output. We stress that the subset of edges that each site has is\nnot related to the network topology, which is fixed to be point-to-point. While\nthis model has been extensively studied for related problems such as graph\nconnectivity, it has not been systematically studied for graph spanners. We\npresent the first tradeoffs for total communication versus the quality of the\nspanners computed, for two or more sites, as well as for additive and\nmultiplicative notions of distortion. We show separations in the communication\ncomplexity when edges are allowed to occur on multiple sites, versus when each\nedge occurs on at most one site. We obtain nearly tight bounds (up to polylog\nfactors) for the communication of additive $2$-spanners in both the with and\nwithout duplication models, multiplicative $(2k-1)$-spanners in the with\nduplication model, and multiplicative $3$ and $5$-spanners in the without\nduplication model. Our lower bound for multiplicative $3$-spanners employs\nbiregular bipartite graphs rather than the usual Erd\\H{o}s girth conjecture\ngraphs and may be of wider interest.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 08:36:12 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 14:00:36 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Fernandez", "Manuel", ""], ["Woodruff", "David P.", ""], ["Yasuda", "Taisuke", ""]]}, {"id": "1911.06132", "submitter": "Tsvi Kopelowitz", "authors": "Hodaya Barr, Tsvi Kopelowitz, Ely Porat, Liam Roditty", "title": "$\\{-1,0,1\\}$-APSP and (min,max)-Product Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $\\{-1,0,1\\}$-APSP problem the goal is to compute all-pairs shortest\npaths (APSP) on a directed graph whose edge weights are all from $\\{-1,0,1\\}$.\nIn the (min,max)-product problem the input is two $n\\times n$ matrices $A$ and\n$B$, and the goal is to output the (min,max)-product of $A$ and $B$.\n  This paper provides a new algorithm for the $\\{-1,0,1\\}$-APSP problem via a\nsimple reduction to the target-(min,max)-product problem where the input is\nthree $n\\times n$ matrices $A,B$, and $T$, and the goal is to output a Boolean\n$n\\times n$ matrix $C$ such that the $(i,j)$ entry of $C$ is 1 if and only if\nthe $(i,j)$ entry of the (min,max)-product of $A$ and $B$ is exactly the\n$(i,j)$ entry of the target matrix $T$. If (min,max)-product can be solved in\n$T_{MM}(n) = \\Omega(n^2)$ time then it is straightforward to solve\ntarget-(min,max)-product in $O(T_{MM}(n))$ time. Thus, given the recent result\nof Bringmann, K\\\"unnemann, and Wegrzycki [STOC 2019], the $\\{-1,0,1\\}$-APSP\nproblem can be solved in the same time needed for solving approximate APSP on\ngraphs with positive weights.\n  Moreover, we design a simple algorithm for target-(min,max)-product when the\ninputs are restricted to the family of inputs generated by our reduction. Using\nfast rectangular matrix multiplication, the new algorithm is faster than the\ncurrent best known algorithm for (min,max)-product.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 14:39:27 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Barr", "Hodaya", ""], ["Kopelowitz", "Tsvi", ""], ["Porat", "Ely", ""], ["Roditty", "Liam", ""]]}, {"id": "1911.06347", "submitter": "Nikita Koval", "authors": "Dan Alistarh, Alexander Fedorov, and Nikita Koval", "title": "In Search of the Fastest Concurrent Union-Find Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Union-Find (or Disjoint-Set Union) is one of the fundamental problems in\ncomputer science; it has been well-studied from both theoretical and practical\nperspectives in the sequential case. Recently, there has been mounting interest\nin analyzing this problem in the concurrent scenario, and several\nasymptotically-efficient algorithms have been proposed. Yet, to date, there is\nvery little known about the practical performance of concurrent Union-Find.\n  This work addresses this gap. We evaluate and analyze the performance of\nseveral concurrent Union-Find algorithms and optimization strategies across a\nwide range of platforms (Intel, AMD, and ARM) and workloads (social, random,\nand road networks, as well as integrations into more complex algorithms). We\nfirst observe that, due to the limited computational cost, the number of\ninduced cache misses is the critical determining factor for the performance of\nexisting algorithms. We introduce new techniques to reduce this cost by storing\nnode priorities implicitly and by using plain reads and writes in a way that\ndoes not affect the correctness of the algorithms. Finally, we show that\nUnion-Find implementations are an interesting application for Transactional\nMemory (TM): one of the fastest algorithm variants we discovered is a\nsequential one that uses coarse-grained locking with the lock elision\noptimization to reduce synchronization cost and increase scalability.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 19:07:20 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Alistarh", "Dan", ""], ["Fedorov", "Alexander", ""], ["Koval", "Nikita", ""]]}, {"id": "1911.06436", "submitter": "Yusuke Kobayashi", "authors": "Yusuke Kobayashi", "title": "Weighted Triangle-free 2-matching Problem with Edge-disjoint Forbidden\n  Triangles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The weighted $\\mathcal{T}$-free $2$-matching problem is the following\nproblem: given an undirected graph $G$, a weight function on its edge set, and\na set $\\mathcal{T}$ of triangles in $G$, find a maximum weight $2$-matching\ncontaining no triangle in $\\mathcal{T}$. When $\\mathcal{T}$ is the set of all\ntriangles in $G$, this problem is known as the weighted triangle-free\n$2$-matching problem, which is a long-standing open problem. A main\ncontribution of this paper is to give a first polynomial-time algorithm for the\nweighted $\\mathcal{T}$-free $2$-matching problem under the assumption that\n$\\mathcal{T}$ is a set of edge-disjoint triangles. In our algorithm, a key\ningredient is to give an extended formulation representing the solution set,\nthat is, we introduce new variables and represent the convex hull of the\nfeasible solutions as a projection of another polytope in a higher dimensional\nspace. Although our extended formulation has exponentially many inequalities,\nwe show that the separation problem can be solved in polynomial time, which\nleads to a polynomial-time algorithm for the weighted $\\mathcal{T}$-free\n$2$-matching problem.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 01:06:11 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Kobayashi", "Yusuke", ""]]}, {"id": "1911.06511", "submitter": "Can Lu", "authors": "Can Lu, Jeffrey Xu Yu, Zhiwei Zhang, Hong Cheng", "title": "Graph Iso/Auto-morphism: A Divide-&-Conquer Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph isomorphism is to determine whether two graphs are isomorphic. A\nclosely related problem is automorphism detection, where an isomorphism between\ntwo graphs is a bijection between their vertex sets that preserves adjacency,\nand an automorphism is an isomorphism from a graph to itself. Applications of\ngraph isomorphism/automorphism include database indexing, network\nsimplification, network anonymization. By graph automorphism, we deal with\nsymmetric subgraph matching (SSM), which is to find all subgraphs that are\nsymmetric to a given subgraph in G. An application of SSM is to identify\nmultiple seed sets that have the same influence power as a seed set found by\ninfluence maximization in a social network. To test two graphs for isomorphism,\ncanonical labeling has been studied to relabel a graph in such a way that\nisomorphic graphs are identical after relabeling. Efficient canonical labeling\nalgorithms have been designed by individualization-refinement. They enumerate\nall permutations using a search tree, and select the minimum as the canonical\nlabeling, which prunes candidates during enumeration. Despite high performance\nin benchmark graphs, these algorithms face difficulties in handling massive\ngraphs. In this paper, we design a new efficient canonical labeling algorithm\nDviCL. Different from previous algorithms, we take a divide-&-conquer approach\nto partition G. By partitioning G, an AutoTree is constructed, which preserves\nsymmetric structures and the automorphism group of G. The canonical labeling\nfor a tree node can be obtained by the canonical labeling of its child nodes,\nand the canonical labeling for the root is the one for G. Such AutoTree can\nalso be effectively used to answer the automorphism group, symmetric subgraphs.\nWe conducted extensive performance studies using 22 large graphs, and confirmed\nthat DviCL is much more efficient and robust than the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 08:25:13 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Lu", "Can", ""], ["Yu", "Jeffrey Xu", ""], ["Zhang", "Zhiwei", ""], ["Cheng", "Hong", ""]]}, {"id": "1911.06790", "submitter": "Samson Zhou", "authors": "Mohammad Hassan Ameri, Jeremiah Blocki, Samson Zhou", "title": "Computationally Data-Independent Memory Hard Functions", "comments": "To appear at ITCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory hard functions (MHFs) are an important cryptographic primitive that\nare used to design egalitarian proofs of work and in the construction of\nmoderately expensive key-derivation functions resistant to brute-force attacks.\nBroadly speaking, MHFs can be divided into two categories: data-dependent\nmemory hard functions (dMHFs) and data-independent memory hard functions\n(iMHFs). iMHFs are resistant to certain side-channel attacks as the memory\naccess pattern induced by the honest evaluation algorithm is independent of the\npotentially sensitive input e.g., password. While dMHFs are potentially\nvulnerable to side-channel attacks (the induced memory access pattern might\nleak useful information to a brute-force attacker), they can achieve higher\ncumulative memory complexity (CMC) in comparison than an iMHF. In this paper,\nwe introduce the notion of computationally data-independent memory hard\nfunctions (ciMHFs). Intuitively, we require that memory access pattern induced\nby the (randomized) ciMHF evaluation algorithm appears to be independent from\nthe standpoint of a computationally bounded eavesdropping attacker --- even if\nthe attacker selects the initial input. We then ask whether it is possible to\ncircumvent known upper bound for iMHFs and build a ciMHF with CMC\n$\\Omega(N^2)$. Surprisingly, we answer the question in the affirmative when the\nciMHF evaluation algorithm is executed on a two-tiered memory architecture\n(RAM/Cache).\n  See paper for the full abstract.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 18:17:56 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Ameri", "Mohammad Hassan", ""], ["Blocki", "Jeremiah", ""], ["Zhou", "Samson", ""]]}, {"id": "1911.06791", "submitter": "Francesco Quinzan", "authors": "Vanja Dosko\\v{c} and Tobias Friedrich and Andreas G\\\"obel and Frank\n  Neumann and Aneta Neumann and Francesco Quinzan", "title": "Non-Monotone Submodular Maximization with Multiple Knapsacks in Static\n  and Dynamic Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of maximizing a non-monotone submodular function under\nmultiple knapsack constraints. We propose a simple discrete greedy algorithm to\napproach this problem, and prove that it yields strong approximation guarantees\nfor functions with bounded curvature. In contrast to other heuristics, this\nrequires no problem relaxation to continuous domains and it maintains a\nconstant-factor approximation guarantee in the problem size. In the case of a\nsingle knapsack, our analysis suggests that the standard greedy can be used in\nnon-monotone settings.\n  Additionally, we study this problem in a dynamic setting, by which knapsacks\nchange during the optimization process. We modify our greedy algorithm to avoid\na complete restart at each constraint update. This modification retains the\napproximation guarantees of the static case.\n  We evaluate our results experimentally on a video summarization and sensor\nplacement task. We show that our proposed algorithm competes with the\nstate-of-the-art in static settings. Furthermore, we show that in dynamic\nsettings with tight computational time budget, our modified greedy yields\nsignificant improvements over starting the greedy from scratch, in terms of the\nsolution quality achieved.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 18:22:46 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 20:20:10 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 10:55:31 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Dosko\u010d", "Vanja", ""], ["Friedrich", "Tobias", ""], ["G\u00f6bel", "Andreas", ""], ["Neumann", "Frank", ""], ["Neumann", "Aneta", ""], ["Quinzan", "Francesco", ""]]}, {"id": "1911.06889", "submitter": "Tristan Pollner", "authors": "Andrei Graur, Tristan Pollner, Vidhya Ramaswamy, and S. Matthew\n  Weinberg", "title": "New Query Lower Bounds for Submodular Function MInimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider submodular function minimization in the oracle model: given\nblack-box access to a submodular set function $f:2^{[n]}\\rightarrow\n\\mathbb{R}$, find an element of $\\arg\\min_S \\{f(S)\\}$ using as few queries to\n$f(\\cdot)$ as possible. State-of-the-art algorithms succeed with\n$\\tilde{O}(n^2)$ queries [LeeSW15], yet the best-known lower bound has never\nbeen improved beyond $n$ [Harvey08].\n  We provide a query lower bound of $2n$ for submodular function minimization,\na $3n/2-2$ query lower bound for the non-trivial minimizer of a symmetric\nsubmodular function, and a $\\binom{n}{2}$ query lower bound for the non-trivial\nminimizer of an asymmetric submodular function.\n  Our $3n/2-2$ lower bound results from a connection between SFM lower bounds\nand a novel concept we term the cut dimension of a graph. Interestingly, this\nyields a $3n/2-2$ cut-query lower bound for finding the global mincut in an\nundirected, weighted graph, but we also prove it cannot yield a lower bound\nbetter than $n+1$ for $s$-$t$ mincut, even in a directed, weighted graph.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 21:45:14 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Graur", "Andrei", ""], ["Pollner", "Tristan", ""], ["Ramaswamy", "Vidhya", ""], ["Weinberg", "S. Matthew", ""]]}, {"id": "1911.06895", "submitter": "Mark Blanco", "authors": "Upasana Sridhar, Mark Blanco, Rahul Mayuranath, Daniele G. Spampinato,\n  Tze Meng Low, and Scott McMillan", "title": "Delta-stepping SSSP: from Vertices and Edges to GraphBLAS\n  Implementations", "comments": "10 pages, 4 figures, IPDPSW GRAPL 2019 Workshop", "journal-ref": "IEEE International Parallel and Distributed Processing Symposium\n  Workshops, 2019, pp 241 to 250", "doi": "10.1109/IPDPSW.2019.00047", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GraphBLAS is an interface for implementing graph algorithms. Algorithms\nimplemented using the GraphBLAS interface are cast in terms of linear\nalgebra-like operations. However, many graph algorithms are canonically\ndescribed in terms of operations on vertices and/or edges. Despite the known\nduality between these two representations, the differences in the way\nalgorithms are described using the two approaches can pose considerable\ndifficulties in the adoption of the GraphBLAS as standard interface for\ndevelopment. This paper investigates a systematic approach for translating a\ngraph algorithm described in the canonical vertex and edge representation into\nan implementation that leverages the GraphBLAS interface. We present a two-step\napproach to this problem. First, we express common vertex- and edge-centric\ndesign patterns using a linear algebraic language. Second, we map this\nintermediate representation to the GraphBLAS interface. We illustrate our\napproach by translating the delta-stepping single source shortest path\nalgorithm from its canonical description to a GraphBLAS implementation, and\nhighlight lessons learned when implementing using GraphBLAS.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 22:11:01 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 18:26:19 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Sridhar", "Upasana", ""], ["Blanco", "Mark", ""], ["Mayuranath", "Rahul", ""], ["Spampinato", "Daniele G.", ""], ["Low", "Tze Meng", ""], ["McMillan", "Scott", ""]]}, {"id": "1911.06907", "submitter": "Greg Bodwin", "authors": "Greg Bodwin and Ofer Grossman", "title": "Strategy-Stealing is Non-Constructive", "comments": "ITCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many combinatorial games, one can prove that the first player wins under\nbest play using a simple but non-constructive argument called\nstrategy-stealing. This work is about the complexity behind these proofs: how\nhard is it to actually find a winning move in a game, when you know by\nstrategy-stealing that one exists? We prove that this problem is PSPACE-hard\nalready for Minimum Poset Games and Symmetric Maker-Maker Games, which are\nsimple classes of games that capture two of the main types of strategy-stealing\narguments in the current literature.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 23:19:11 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Bodwin", "Greg", ""], ["Grossman", "Ofer", ""]]}, {"id": "1911.06924", "submitter": "Ramesh Krishnan S. Pallavoor", "authors": "Ramesh Krishnan S. Pallavoor, Sofya Raskhodnikova and Erik Waingarten", "title": "Approximating the Distance to Monotonicity of Boolean Functions", "comments": "To be published in Random Structures & Algorithms", "journal-ref": null, "doi": "10.4230/LIPIcs.ITCS.2021.80", "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We design a nonadaptive algorithm that, given oracle access to a function $f:\n\\{0,1\\}^n \\to \\{0,1\\}$ which is $\\alpha$-far from monotone, makes poly$(n,\n1/\\alpha)$ queries and returns an estimate that, with high probability, is an\n$\\widetilde{O}(\\sqrt{n})$-approximation to the distance of $f$ to monotonicity.\nThe analysis of our algorithm relies on an improvement to the directed\nisoperimetric inequality of Khot, Minzer, and Safra (SIAM J. Comput., 2018).\nFurthermore, we rule out a poly$(n, 1/\\alpha)$-query nonadaptive algorithm that\napproximates the distance to monotonicity significantly better by showing that,\nfor all constant $\\kappa > 0,$ every nonadaptive $n^{1/2 -\n\\kappa}$-approximation algorithm for this problem requires $2^{n^\\kappa}$\nqueries. This answers a question of Seshadhri (Property Testing Review, 2014)\nfor the case of nonadaptive algorithms. We obtain our lower bound by proving an\nanalogous bound for erasure-resilient (and tolerant) testers. Our method also\nyields the same lower bounds for unateness and being a $k$-junta.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 00:43:43 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 08:35:43 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Pallavoor", "Ramesh Krishnan S.", ""], ["Raskhodnikova", "Sofya", ""], ["Waingarten", "Erik", ""]]}, {"id": "1911.06951", "submitter": "Samson Zhou", "authors": "Zaoxing Liu, Samson Zhou, Ori Rottenstreich, Vladimir Braverman,\n  Jennifer Rexford", "title": "Memory-Efficient Performance Monitoring on Programmable Switches with\n  Lean Algorithms", "comments": "To appear at APoCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network performance problems are notoriously difficult to diagnose. Prior\nprofiling systems collect performance statistics by keeping information about\neach network flow, but maintaining per-flow state is not scalable on\nresource-constrained NIC and switch hardware. Instead, we propose sketch-based\nperformance monitoring using memory that is sublinear in the number of flows.\nExisting sketches estimate flow monitoring metrics based on flow sizes. In\ncontrast, performance monitoring typically requires combining information\nacross pairs of packets, such as matching a data packet with its acknowledgment\nto compute a round-trip time. We define a new class of \\emph{lean} algorithms\nthat use memory sublinear in both the size of input data and the number of\nflows. We then introduce lean algorithms for a set of important statistics,\nsuch as identifying flows with high latency, loss, out-of-order, or\nretransmitted packets. We implement prototypes of our lean algorithms on a\ncommodity programmable switch using the P4 language. Our experiments show that\nlean algorithms detect $\\sim$82\\% of top 100 problematic flows among real-world\npacket traces using just 40KB memory.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 04:01:37 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Liu", "Zaoxing", ""], ["Zhou", "Samson", ""], ["Rottenstreich", "Ori", ""], ["Braverman", "Vladimir", ""], ["Rexford", "Jennifer", ""]]}, {"id": "1911.06958", "submitter": "Frank Ban", "authors": "Frank Ban, David Woodruff, Qiuyi Zhang", "title": "Regularized Weighted Low Rank Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical low rank approximation problem is to find a rank $k$ matrix\n$UV$ (where $U$ has $k$ columns and $V$ has $k$ rows) that minimizes the\nFrobenius norm of $A - UV$. Although this problem can be solved efficiently, we\nstudy an NP-hard variant of this problem that involves weights and\nregularization. A previous paper of [Razenshteyn et al. '16] derived a\npolynomial time algorithm for weighted low rank approximation with constant\nrank. We derive provably sharper guarantees for the regularized version by\nobtaining parameterized complexity bounds in terms of the statistical dimension\nrather than the rank, allowing for a rank-independent runtime that can be\nsignificantly faster. Our improvement comes from applying sharper matrix\nconcentration bounds, using a novel conditioning technique, and proving\nstructural theorems for regularized low rank problems.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 05:14:45 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 03:54:41 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Ban", "Frank", ""], ["Woodruff", "David", ""], ["Zhang", "Qiuyi", ""]]}, {"id": "1911.06985", "submitter": "Dominik K\\\"oppl", "authors": "Hideo Bannai and Juha K\\\"arkk\\\"ainen and Dominik K\\\"oppl and Marcin\n  Pi\\c{a}tkowski", "title": "Constructing the Bijective and the Extended Burrows-Wheeler Transform in\n  Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Burrows-Wheeler transform (BWT) is a permutation whose applications are\nprevalent in data compression and text indexing. The bijective BWT (BBWT) is a\nbijective variant of it. Although it is known that the BWT can be constructed\nin linear time for integer alphabets by using a linear time suffix array\nconstruction algorithm, it was up to now only conjectured that the BBWT can\nalso be constructed in linear time. We confirm this conjecture by proposing a\nconstruction algorithm that is based on SAIS, improving the best known result\nof $O(n \\lg n /\\lg \\lg n)$ time to linear.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 08:04:25 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 06:48:12 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Bannai", "Hideo", ""], ["K\u00e4rkk\u00e4inen", "Juha", ""], ["K\u00f6ppl", "Dominik", ""], ["Picatkowski", "Marcin", ""]]}, {"id": "1911.07020", "submitter": "Andreas Galanis", "authors": "Andreas Galanis and Leslie Ann Goldberg and Heng Guo and Kuan Yang", "title": "Counting solutions to random CNF formulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first efficient algorithm to approximately count the number of\nsolutions in the random $k$-SAT model when the density of the formula scales\nexponentially with $k$. The best previous counting algorithm for the permissive\nversion of the model was due to Montanari and Shah and was based on the\ncorrelation decay method, which works up to densities $(1+o_k(1))\\frac{2\\log\nk}{k}$, the Gibbs uniqueness threshold for the model. Instead, our algorithm\nharnesses a recent technique by Moitra to work for random formulas. The main\nchallenge in our setting is to account for the presence of high-degree\nvariables whose marginal distributions are hard to control and which cause\nsignificant correlations within the formula.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 12:13:11 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 12:06:18 GMT"}, {"version": "v3", "created": "Wed, 15 Jan 2020 19:34:03 GMT"}, {"version": "v4", "created": "Mon, 24 May 2021 07:27:02 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Galanis", "Andreas", ""], ["Goldberg", "Leslie Ann", ""], ["Guo", "Heng", ""], ["Yang", "Kuan", ""]]}, {"id": "1911.07124", "submitter": "Matthew Groff S.", "authors": "Matt Groff", "title": "Faster Integer Multiplication Using Preprocessing", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.NT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A New Number Theoretic Transform(NTT), which is a form of FFT, is introduced,\nthat is faster than FFTs. Also, a multiplication algorithm is introduced that\nuses this to perform integer multiplication faster than O(n log n). It uses\npreprocessing to achieve an upper bounds of (n log n/(log log n/ log log log\nn).\n  Also, we explore the possibility of O(n) time multiplication via NTTs that\nrequire only O(n) operations, using preprocessing.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 01:18:03 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 00:15:07 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Groff", "Matt", ""]]}, {"id": "1911.07151", "submitter": "Siddharth Dawar", "authors": "Siddharth Dawar, Vikram Goyal, Debajyoti Bera", "title": "A one-phase tree-based algorithm for mining high-utility itemsets from a\n  transaction database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-utility itemset mining finds itemsets from a transaction database with\nutility no less than a fixed user-defined threshold. The utility of an itemset\nis defined as the sum of the utilities of its item. Several algorithms were\nproposed to mine high-utility itemsets. However, no state-of-the-art algorithm\nperforms consistently good across dense and sparse datasets. In this paper, we\npropose a novel data structure called Utility-Tree, and a tree-based algorithm\ncalled UT-Miner that mines high-utility itemsets in one-phase only without\ngenerating any candidates and uses a lightweight construction method to reduce\nthe cost of creating projected databases during the search space exploration.\nThe transaction information is stored compactly with every node of the\nUtility-Tree, and the information is computed efficiently during the recursive\ninvocation of the algorithm. Experimental results on several real-life dense\nand sparse datasets reveal that UT-Miner is among the top-performing efficient\nalgorithms across different datasets.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 04:47:16 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Dawar", "Siddharth", ""], ["Goyal", "Vikram", ""], ["Bera", "Debajyoti", ""]]}, {"id": "1911.07154", "submitter": "Yasamin Nazari", "authors": "Yasamin Nazari", "title": "Sparse Hopsets in Congested Clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give the first Congested Clique algorithm that computes a sparse hopset\nwith polylogarithmic hopbound in polylogarithmic time. Given a graph $G=(V,E)$,\na $(\\beta,\\epsilon)$-hopset $H$ with \"hopbound\" $\\beta$, is a set of edges\nadded to $G$ such that for any pair of nodes $u$ and $v$ in $G$ there is a path\nwith at most $\\beta$ hops in $G \\cup H$ with length within $(1+\\epsilon)$ of\nthe shortest path between $u$ and $v$ in $G$.\n  Our hopsets are significantly sparser than the recent construction of\nCensor-Hillel et al. [6], that constructs a hopset of size\n$\\tilde{O}(n^{3/2})$, but with a smaller polylogarithmic hopbound. On the other\nhand, the previously known constructions of sparse hopsets with polylogarithmic\nhopbound in the Congested Clique model, proposed by Elkin and Neiman\n[10],[11],[12], all require polynomial rounds.\n  One tool that we use is an efficient algorithm that constructs an\n$\\ell$-limited neighborhood cover, that may be of independent interest.\n  Finally, as a side result, we also give a hopset construction in a variant of\nthe low-memory Massively Parallel Computation model, with improved running time\nover existing algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 05:10:25 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 00:04:08 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Nazari", "Yasamin", ""]]}, {"id": "1911.07232", "submitter": "Ali \\c{C}ivril", "authors": "Ali \\c{C}ivril", "title": "5/4-Approximation of Minimum 2-Edge-Connected Spanning Subgraph", "comments": "The claims about the nature of an optimal solution are all wrong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a $5/4$-approximation algorithm for the minimum 2-edge-connected\nspanning subgraph problem. This improves upon the previous best ratio of $4/3$.\nThe algorithm is based on applying local improvement steps on a starting\nsolution provided by a standard ear decomposition together with the idea of\nrunning several iterations on residual graphs by excluding certain edges that\ndo not belong to an optimum solution. The latter idea is a novel one, which\nallows us to bypass $3$-ears with no loss in approximation ratio, the\nbottleneck for obtaining a performance guarantee below $3/2$. Our algorithm\nalso implies a simpler $7/4$-approximation algorithm for the matching\naugmentation problem, which was recently treated.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 13:23:11 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 12:39:11 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 10:29:43 GMT"}, {"version": "v4", "created": "Tue, 4 May 2021 08:52:20 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["\u00c7ivril", "Ali", ""]]}, {"id": "1911.07234", "submitter": "Ali \\c{C}ivril", "authors": "Ali \\c{C}ivril", "title": "Approximation of Steiner Forest via the Bidirected Cut Relaxation", "comments": "15 pages, 5 figures", "journal-ref": "Journal of Combinatorial Optimization, 38(4):1196-1212, 2019", "doi": "10.1007/s10878-019-00444-8", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical algorithm of Agrawal, Klein and Ravi [SIAM J. Comput., 24\n(1995), pp. 440-456], stated in the setting of the primal-dual schema by\nGoemans and Williamson [SIAM J. Comput., 24 (1995), pp. 296-317] uses the\nundirected cut relaxation for the Steiner forest problem. Its approximation\nratio is $2-\\frac{1}{k}$, where $k$ is the number of terminal pairs. A variant\nof this algorithm more recently proposed by K\\\"onemann et al. [SIAM J. Comput.,\n37 (2008), pp. 1319-1341] is based on the lifted cut relaxation. In this paper,\nwe continue this line of work and consider the bidirected cut relaxation for\nthe Steiner forest problem, which lends itself to a novel algorithmic idea\nyielding the same approximation ratio as the classical algorithm. In doing so,\nwe introduce an extension of the primal-dual schema in which we run two\ndifferent phases to satisfy connectivity requirements in both directions. This\nreveals more about the combinatorial structure of the problem. In particular,\nthere are examples on which the classical algorithm fails to give a good\napproximation, but the new algorithm finds a near-optimal solution.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 13:34:00 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["\u00c7ivril", "Ali", ""]]}, {"id": "1911.07306", "submitter": "Simon Apers", "authors": "Simon Apers and Ronald de Wolf", "title": "Quantum Speedup for Graph Sparsification, Cut Approximation and\n  Laplacian Solving", "comments": "v2: several small improvements to the text. An extended abstract will\n  appear in FOCS'20; v3: corrected a minor mistake in Appendix A", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph sparsification underlies a large number of algorithms, ranging from\napproximation algorithms for cut problems to solvers for linear systems in the\ngraph Laplacian. In its strongest form, \"spectral sparsification\" reduces the\nnumber of edges to near-linear in the number of nodes, while approximately\npreserving the cut and spectral structure of the graph. In this work we\ndemonstrate a polynomial quantum speedup for spectral sparsification and many\nof its applications. In particular, we give a quantum algorithm that, given a\nweighted graph with $n$ nodes and $m$ edges, outputs a classical description of\nan $\\epsilon$-spectral sparsifier in sublinear time\n$\\tilde{O}(\\sqrt{mn}/\\epsilon)$. This contrasts with the optimal classical\ncomplexity $\\tilde{O}(m)$. We also prove that our quantum algorithm is optimal\nup to polylog-factors. The algorithm builds on a string of existing results on\nsparsification, graph spanners, quantum algorithms for shortest paths, and\nefficient constructions for $k$-wise independent random strings. Our algorithm\nimplies a quantum speedup for solving Laplacian systems and for approximating a\nrange of cut problems such as min cut and sparsest cut.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 17:29:40 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 17:36:12 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 12:29:02 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Apers", "Simon", ""], ["de Wolf", "Ronald", ""]]}, {"id": "1911.07324", "submitter": "Maryam Aliakbarpour", "authors": "Maryam Aliakbarpour, Sandeep Silwal", "title": "Testing Properties of Multiple Distributions with Few Samples", "comments": "ITCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new setting for testing properties of distributions while\nreceiving samples from several distributions, but few samples per distribution.\nGiven samples from $s$ distributions, $p_1, p_2, \\ldots, p_s$, we design\ntesters for the following problems: (1) Uniformity Testing: Testing whether all\nthe $p_i$'s are uniform or $\\epsilon$-far from being uniform in\n$\\ell_1$-distance (2) Identity Testing: Testing whether all the $p_i$'s are\nequal to an explicitly given distribution $q$ or $\\epsilon$-far from $q$ in\n$\\ell_1$-distance, and (3) Closeness Testing: Testing whether all the $p_i$'s\nare equal to a distribution $q$ which we have sample access to, or\n$\\epsilon$-far from $q$ in $\\ell_1$-distance. By assuming an additional natural\ncondition about the source distributions, we provide sample optimal testers for\nall of these problems.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 19:44:13 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Aliakbarpour", "Maryam", ""], ["Silwal", "Sandeep", ""]]}, {"id": "1911.07352", "submitter": "Goran \\v{Z}u\\v{z}i\\'c", "authors": "Domagoj Bradac, Anupam Gupta, Sahil Singla, Goran Zuzic", "title": "Robust Algorithms for the Secretary Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classical secretary problems, a sequence of $n$ elements arrive in a\nuniformly random order, and we want to choose a single item, or a set of size\n$K$. The random order model allows us to escape from the strong lower bounds\nfor the adversarial order setting, and excellent algorithms are known in this\nsetting. However, one worrying aspect of these results is that the algorithms\noverfit to the model: they are not very robust. Indeed, if a few \"outlier\"\narrivals are adversarially placed in the arrival sequence, the algorithms\nperform poorly. E.g., Dynkin's popular $1/e$-secretary algorithm fails with\neven a single adversarial arrival.\n  We investigate a robust version of the secretary problem. In the Byzantine\nSecretary model, we have two kinds of elements: green (good) and red (rogue).\nThe values of all elements are chosen by the adversary. The green elements\narrive at times uniformly randomly drawn from $[0,1]$. The red elements,\nhowever, arrive at adversarially chosen times. Naturally, the algorithm does\nnot see these colors: how well can it solve secretary problems?\n  We give algorithms which get value comparable to the value of the optimal\ngreen set minus the largest green item. Specifically, we give an algorithm to\npick $K$ elements that gets within $(1-\\varepsilon)$ factor of the above\nbenchmark, as long as $K \\geq \\mathrm{poly}(\\varepsilon^{-1} \\log n)$. We\nextend this to the knapsack secretary problem, for large knapsack size $K$.\n  For the single-item case, an analogous benchmark is the value of the\nsecond-largest green item. For value-maximization, we give a $\\mathrm{poly}\n\\log^* n$-competitive algorithm, using a multi-layered bucketing scheme that\nadaptively refines our estimates of second-max over time. For\nprobability-maximization, we show the existence of a good randomized algorithm,\nusing the minimax principle.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 21:48:23 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 19:36:07 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Bradac", "Domagoj", ""], ["Gupta", "Anupam", ""], ["Singla", "Sahil", ""], ["Zuzic", "Goran", ""]]}, {"id": "1911.07357", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne, Xi Chen, Gautam Kamath, Amit Levi, Erik\n  Waingarten", "title": "Random Restrictions of High-Dimensional Distributions and Uniformity\n  Testing with Subcube Conditioning", "comments": "Added Remark 4.4, which discusses the time complexity (the algorithms\n  are polynomial-time, based on an observation from [CJLW20]); removing log log\n  log n factor for the Gaussian testing algorithm. These changes reflect those\n  included in the conference version (SODA'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a nearly-optimal algorithm for testing uniformity of distributions\nsupported on $\\{-1,1\\}^n$, which makes $\\tilde O (\\sqrt{n}/\\varepsilon^2)$\nqueries to a subcube conditional sampling oracle (Bhattacharyya and Chakraborty\n(2018)). The key technical component is a natural notion of random restriction\nfor distributions on $\\{-1,1\\}^n$, and a quantitative analysis of how such a\nrestriction affects the mean vector of the distribution. Along the way, we\nconsider the problem of mean testing with independent samples and provide a\nnearly-optimal algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 22:39:17 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 23:30:41 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["Chen", "Xi", ""], ["Kamath", "Gautam", ""], ["Levi", "Amit", ""], ["Waingarten", "Erik", ""]]}, {"id": "1911.07375", "submitter": "Li-Yang Tan", "authors": "Guy Blanc, Jane Lange, Li-Yang Tan", "title": "Top-down induction of decision trees: rigorous guarantees and inherent\n  limitations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following heuristic for building a decision tree for a function\n$f : \\{0,1\\}^n \\to \\{\\pm 1\\}$. Place the most influential variable $x_i$ of $f$\nat the root, and recurse on the subfunctions $f_{x_i=0}$ and $f_{x_i=1}$ on the\nleft and right subtrees respectively; terminate once the tree is an\n$\\varepsilon$-approximation of $f$. We analyze the quality of this heuristic,\nobtaining near-matching upper and lower bounds:\n  $\\circ$ Upper bound: For every $f$ with decision tree size $s$ and every\n$\\varepsilon \\in (0,\\frac1{2})$, this heuristic builds a decision tree of size\nat most $s^{O(\\log(s/\\varepsilon)\\log(1/\\varepsilon))}$.\n  $\\circ$ Lower bound: For every $\\varepsilon \\in (0,\\frac1{2})$ and $s \\le\n2^{\\tilde{O}(\\sqrt{n})}$, there is an $f$ with decision tree size $s$ such that\nthis heuristic builds a decision tree of size $s^{\\tilde{\\Omega}(\\log s)}$.\n  We also obtain upper and lower bounds for monotone functions:\n$s^{O(\\sqrt{\\log s}/\\varepsilon)}$ and $s^{\\tilde{\\Omega}(\\sqrt[4]{\\log s } )}$\nrespectively. The lower bound disproves conjectures of Fiat and Pechyony (2004)\nand Lee (2009).\n  Our upper bounds yield new algorithms for properly learning decision trees\nunder the uniform distribution. We show that these algorithms---which are\nmotivated by widely employed and empirically successful top-down decision tree\nlearning heuristics such as ID3, C4.5, and CART---achieve provable guarantees\nthat compare favorably with those of the current fastest algorithm (Ehrenfeucht\nand Haussler, 1989). Our lower bounds shed new light on the limitations of\nthese heuristics.\n  Finally, we revisit the classic work of Ehrenfeucht and Haussler. We extend\nit to give the first uniform-distribution proper learning algorithm that\nachieves polynomial sample and memory complexity, while matching its\nstate-of-the-art quasipolynomial runtime.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 00:25:31 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Blanc", "Guy", ""], ["Lange", "Jane", ""], ["Tan", "Li-Yang", ""]]}, {"id": "1911.07378", "submitter": "Roie Levin", "authors": "Parikshit Gopalan, Roie Levin and Udi Wieder", "title": "Finding Skewed Subcubes Under a Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Say that we are given samples from a distribution $\\psi$ over an\n$n$-dimensional space. We expect or desire $\\psi$ to behave like a product\ndistribution (or a $k$-wise independent distribution over its marginals for\nsmall $k$). We propose the problem of enumerating/list-decoding all large\nsubcubes where the distribution $\\psi$ deviates markedly from what we expect;\nwe refer to such subcubes as skewed subcubes. Skewed subcubes are certificates\nof dependencies between small subsets of variables in $\\psi$. We motivate this\nproblem by showing that it arises naturally in the context of algorithmic\nfairness and anomaly detection.\n  In this work we focus on the special but important case where the space is\nthe Boolean hypercube, and the expected marginals are uniform. We show that the\nobvious definition of skewed subcubes can lead to intractable list sizes, and\npropose a better definition of a minimal skewed subcube, which are subcubes\nwhose skew cannot be attributed to a larger subcube that contains it. Our main\ntechnical contribution is a list-size bound for this definition and an\nalgorithm to efficiently find all such subcubes. Both the bound and the\nalgorithm rely on Fourier-analytic techniques, especially the powerful\nhypercontractive inequality.\n  On the lower bounds side, we show that finding skewed subcubes is as hard as\nthe sparse noisy parity problem, and hence our algorithms cannot be improved on\nsubstantially without a breakthrough on this problem which is believed to be\nintractable. Motivated by this, we study alternate models allowing query access\nto $\\psi$ where finding skewed subcubes might be easier.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 00:32:58 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 02:13:20 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Gopalan", "Parikshit", ""], ["Levin", "Roie", ""], ["Wieder", "Udi", ""]]}, {"id": "1911.07417", "submitter": "Michael Whitmeyer", "authors": "Michael Whitmeyer, Jonathan Liu", "title": "Algorithmic Discrepancy Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report will be a literature review on a result in algorithmic\ndiscrepancy theory. We will begin by providing a quick overview on discrepancy\ntheory and some major results in the field, and then focus on an important\nresult by Shachar Lovett and Raghu Meka. We restate the main algorithm and\nideas of the paper, and rewrite proofs for some of the major results in the\npaper.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 04:10:43 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Whitmeyer", "Michael", ""], ["Liu", "Jonathan", ""]]}, {"id": "1911.07465", "submitter": "Yu Nakahata", "authors": "Yu Nakahata, Jun Kawahara, Takashi Horiyama, Shin-ichi Minato", "title": "Implicit Enumeration of Topological-Minor-Embeddings and Its Application\n  to Planar Subgraph Enumeration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given graphs $G$ and $H$, we propose a method to implicitly enumerate\ntopological-minor-embeddings of $H$ in $G$ using decision diagrams. We show a\nuseful application of our method to enumerating subgraphs characterized by\nforbidden topological minors, that is, planar, outerplanar, series-parallel,\nand cactus subgraphs. Computational experiments show that our method can find\nall planar subgraphs in a given graph at most five orders of magnitude faster\nthan a naive backtracking-based method.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 07:31:53 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Nakahata", "Yu", ""], ["Kawahara", "Jun", ""], ["Horiyama", "Takashi", ""], ["Minato", "Shin-ichi", ""]]}, {"id": "1911.07945", "submitter": "Jack Wang", "authors": "Aviad Rubinstein, Jack Z. Wang, S. Matthew Weinberg", "title": "Optimal Single-Choice Prophet Inequalities from Samples", "comments": "Appears in Innovations in Theoretical Computer Science (ITCS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the single-choice Prophet Inequality problem when the gambler is\ngiven access to samples. We show that the optimal competitive ratio of $1/2$\ncan be achieved with a single sample from each distribution. When the\ndistributions are identical, we show that for any constant $\\varepsilon > 0$,\n$O(n)$ samples from the distribution suffice to achieve the optimal competitive\nratio ($\\approx 0.745$) within $(1+\\varepsilon)$, resolving an open problem of\nCorrea, D\\\"utting, Fischer, and Schewior.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 20:48:12 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Rubinstein", "Aviad", ""], ["Wang", "Jack Z.", ""], ["Weinberg", "S. Matthew", ""]]}, {"id": "1911.07972", "submitter": "Mohammad Hajiesmaili", "authors": "Russell Lee and Mohammad H. Hajiesmaili and Jian Li", "title": "Learning-Assisted Competitive Algorithms for Peak-Aware Energy\n  Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the peak-aware energy scheduling problem using the\ncompetitive framework with machine learning prediction. With the uncertainty of\nenergy demand as the fundamental challenge, the goal is to schedule the energy\noutput of local generation units such that the electricity bill is minimized.\nWhile this problem has been tackled using classic competitive design with\nworst-case guarantee, the goal of this paper is to develop learning-assisted\ncompetitive algorithms to improve the performance in a provable manner. We\ndevelop two deterministic and randomized algorithms that are provably robust\nagainst the poor performance of learning prediction, however, achieve the\noptimal performance as the error of prediction goes to zero. Extensive\nexperiments using real data traces verify our theoretical observations and show\n15.13% improved performance against pure online algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 21:49:42 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Lee", "Russell", ""], ["Hajiesmaili", "Mohammad H.", ""], ["Li", "Jian", ""]]}, {"id": "1911.07976", "submitter": "Sourbh Bhadane", "authors": "Jayadev Acharya, Sourbh Bhadane, Piotr Indyk, Ziteng Sun", "title": "Estimating Entropy of Distributions in Constant Space", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of estimating the entropy of $k$-ary distributions from\nsamples in the streaming model, where space is limited. Our main contribution\nis an algorithm that requires $O\\left(\\frac{k \\log\n(1/\\varepsilon)^2}{\\varepsilon^3}\\right)$ samples and a constant $O(1)$ memory\nwords of space and outputs a $\\pm\\varepsilon$ estimate of $H(p)$. Without space\nlimitations, the sample complexity has been established as\n$S(k,\\varepsilon)=\\Theta\\left(\\frac k{\\varepsilon\\log k}+\\frac{\\log^2\nk}{\\varepsilon^2}\\right)$, which is sub-linear in the domain size $k$, and the\ncurrent algorithms that achieve optimal sample complexity also require\nnearly-linear space in $k$.\n  Our algorithm partitions $[0,1]$ into intervals and estimates the entropy\ncontribution of probability values in each interval. The intervals are designed\nto trade off the bias and variance of these estimates.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 21:54:59 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Acharya", "Jayadev", ""], ["Bhadane", "Sourbh", ""], ["Indyk", "Piotr", ""], ["Sun", "Ziteng", ""]]}, {"id": "1911.08004", "submitter": "Dana Yang", "authors": "Jian Ding, Yihong Wu, Jiaming Xu, and Dana Yang", "title": "Consistent recovery threshold of hidden nearest neighbor graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.SI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications such as discovering strong ties in social networks\nand assembling genome subsequences in biology, we study the problem of\nrecovering a hidden $2k$-nearest neighbor (NN) graph in an $n$-vertex complete\ngraph, whose edge weights are independent and distributed according to $P_n$\nfor edges in the hidden $2k$-NN graph and $Q_n$ otherwise. The special case of\nBernoulli distributions corresponds to a variant of the Watts-Strogatz\nsmall-world graph. We focus on two types of asymptotic recovery guarantees as\n$n\\to \\infty$: (1) exact recovery: all edges are classified correctly with\nprobability tending to one; (2) almost exact recovery: the expected number of\nmisclassified edges is $o(nk)$. We show that the maximum likelihood estimator\nachieves (1) exact recovery for $2 \\le k \\le n^{o(1)}$ if $ \\liminf\n\\frac{2\\alpha_n}{\\log n}>1$; (2) almost exact recovery for $ 1 \\le k \\le\no\\left( \\frac{\\log n}{\\log \\log n} \\right)$ if $\\liminf\n\\frac{kD(P_n||Q_n)}{\\log n}>1$, where $\\alpha_n \\triangleq -2 \\log \\int \\sqrt{d\nP_n d Q_n}$ is the R\\'enyi divergence of order $\\frac{1}{2}$ and $D(P_n||Q_n)$\nis the Kullback-Leibler divergence. Under mild distributional assumptions,\nthese conditions are shown to be information-theoretically necessary for any\nalgorithm to succeed. A key challenge in the analysis is the enumeration of\n$2k$-NN graphs that differ from the hidden one by a given number of edges.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 23:44:54 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Ding", "Jian", ""], ["Wu", "Yihong", ""], ["Xu", "Jiaming", ""], ["Yang", "Dana", ""]]}, {"id": "1911.08015", "submitter": "Hannah Lawrence", "authors": "Hannah Lawrence, Jerry Li, Cameron Musco, and Christopher Musco", "title": "Low-Rank Toeplitz Matrix Estimation via Random Ultra-Sparse Rulers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to estimate a nearly low-rank Toeplitz covariance matrix $T$\nfrom compressed measurements. Recent work of Qiao and Pal addresses this\nproblem by combining sparse rulers (sparse linear arrays) with frequency\nfinding (sparse Fourier transform) algorithms applied to the Vandermonde\ndecomposition of $T$. Analytical bounds on the sample complexity are shown,\nunder the assumption of sufficiently large gaps between the frequencies in this\ndecomposition. In this work, we introduce random ultra-sparse rulers and\npropose an improved approach based on these objects. Our random rulers\neffectively apply a random permutation to the frequencies in $T$'s Vandermonde\ndecomposition, letting us avoid frequency gap assumptions and leading to\nimproved sample complexity bounds. In the special case when $T$ is circulant,\nwe theoretically analyze the performance of our method when combined with\nsparse Fourier transform algorithms based on random hashing. We also show\nexperimentally that our ultra-sparse rulers give significantly more robust and\nsample efficient estimation then baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 00:31:27 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Lawrence", "Hannah", ""], ["Li", "Jerry", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""]]}, {"id": "1911.08043", "submitter": "Bas Lodewijks", "authors": "Bas Lodewijks", "title": "Mapping NP-hard and NP-complete optimisation problems to Quadratic\n  Unconstrained Binary Optimisation problems", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cond-mat.stat-mech cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss several mappings from well-known NP-hard problems to Quadratic\nUnconstrained Binary Optimisation problems which are treated incorrectly by\nLucas. We provide counterexamples and correct the mappings. We also extend the\nbody of QUBO formulations of NP-complete and NP-hard optimisation problems by\ndiscussing additional problems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 17:14:44 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 22:56:51 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2020 15:35:20 GMT"}, {"version": "v4", "created": "Mon, 3 Aug 2020 08:22:43 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Lodewijks", "Bas", ""]]}, {"id": "1911.08085", "submitter": "Sushrut Karmalkar", "authors": "Ilias Diakonikolas, Sushrut Karmalkar, Daniel Kane, Eric Price,\n  Alistair Stewart", "title": "Outlier-Robust High-Dimensional Sparse Estimation via Iterative\n  Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study high-dimensional sparse estimation tasks in a robust setting where a\nconstant fraction of the dataset is adversarially corrupted. Specifically, we\nfocus on the fundamental problems of robust sparse mean estimation and robust\nsparse PCA. We give the first practically viable robust estimators for these\nproblems. In more detail, our algorithms are sample and computationally\nefficient and achieve near-optimal robustness guarantees. In contrast to prior\nprovable algorithms which relied on the ellipsoid method, our algorithms use\nspectral techniques to iteratively remove outliers from the dataset. Our\nexperimental evaluation on synthetic data shows that our algorithms are\nscalable and significantly outperform a range of previous approaches, nearly\nmatching the best error rate without corruptions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 04:12:54 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Karmalkar", "Sushrut", ""], ["Kane", "Daniel", ""], ["Price", "Eric", ""], ["Stewart", "Alistair", ""]]}, {"id": "1911.08130", "submitter": "Alberto Paoluzzi", "authors": "Alberto Paoluzzi, Vadim Shapiro, Antonio DiCarlo, Francesco Furiani,\n  Giulio Martella, Giorgio Scorzelli", "title": "Topological computing of arrangements with (co)chains", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many areas of applied geometric/numeric computational mathematics,\nincluding geo-mapping, computer vision, computer graphics, finite element\nanalysis, medical imaging, geometric design, and solid modeling, one has to\ncompute incidences, adjacencies and ordering of cells, generally using\ndisparate and often incompatible data structures and algorithms. This paper\nintroduces computational topology algorithms to discover the 2D/3D space\npartition induced by a collection of geometric objects of dimension 1D/2D,\nrespectively. Methods and language are those of basic geometric and algebraic\ntopology. Only sparse vectors and matrices are used to compute both spaces and\nmaps, i.e., the chain complex, from dimension zero to three.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 07:21:03 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Paoluzzi", "Alberto", ""], ["Shapiro", "Vadim", ""], ["DiCarlo", "Antonio", ""], ["Furiani", "Francesco", ""], ["Martella", "Giulio", ""], ["Scorzelli", "Giorgio", ""]]}, {"id": "1911.08275", "submitter": "Manodeep Sinha", "authors": "Manodeep Sinha and Lehman H. Garrison", "title": "Corrfunc: Blazing fast correlation functions with AVX512F SIMD\n  Intrinsics", "comments": "Paper II for the Corrfunc software package, paper I is on arXiv here:\n  arXiv:1911.03545. Appeared in the refereed proceedings for the \"Second\n  Workshop on Software Challenges to Exascale Computing\"", "journal-ref": null, "doi": "10.1007/978-981-13-7729-7_1", "report-no": null, "categories": "astro-ph.IM astro-ph.CO astro-ph.GA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation functions are widely used in extra-galactic astrophysics to\nextract insights into how galaxies occupy dark matter halos and in cosmology to\nplace stringent constraints on cosmological parameters. A correlation function\nfundamentally requires computing pair-wise separations between two sets of\npoints and then computing a histogram of the separations. Corrfunc is an\nexisting open-source, high-performance software package for efficiently\ncomputing a multitude of correlation functions. In this paper, we will discuss\nthe SIMD AVX512F kernels within Corrfunc, capable of processing 16 floats or 8\ndoubles at a time. The latest manually implemented Corrfunc AVX512F kernels\nshow a speedup of up to $\\sim 4\\times$ relative to compiler-generated code for\ndouble-precision calculations. The AVX512F kernels show $\\sim 1.6\\times$\nspeedup relative to the AVX kernels and compare favorably to a theoretical\nmaximum of $2\\times$. In addition, by pruning pairs with too large of a minimum\npossible separation, we achieve a $\\sim 5-10\\%$ speedup across all the SIMD\nkernels. Such speedups highlight the importance of programming explicitly with\nSIMD vector intrinsics for complex calculations that can not be efficiently\nvectorized by compilers. Corrfunc is publicly available at\nhttps://github.com/manodeep/Corrfunc/.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 20:52:09 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Sinha", "Manodeep", ""], ["Garrison", "Lehman H.", ""]]}, {"id": "1911.08320", "submitter": "Sandeep Silwal", "authors": "Rogers Epstein, Sandeep Silwal", "title": "Property Testing of LP-Type Problems", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given query access to a set of constraints $S$, we wish to quickly check if\nsome objective function $\\varphi$ subject to these constraints is at most a\ngiven value $k$. We approach this problem using the framework of property\ntesting where our goal is to distinguish the case $\\varphi(S) \\le k$ from the\ncase that at least an $\\epsilon$ fraction of the constraints in $S$ need to be\nremoved for $\\varphi(S) \\le k$ to hold. We restrict our attention to the case\nwhere $(S, \\varphi)$ are LP-Type problems which is a rich family of\ncombinatorial optimization problems with an inherent geometric structure. By\nutilizing a simple sampling procedure which has been used previously to study\nthese problems, we are able to create property testers for any LP-Type problem\nwhose query complexities are independent of the number of constraints. To the\nbest of our knowledge, this is the first work that connects the area of LP-Type\nproblems and property testing in a systematic way. Among our results is a tight\nupper bound on the query complexity of testing clusterability with one cluster\nconsidered by Alon, Dar, Parnas, and Ron (FOCS 2000). We also supply a\ncorresponding tight lower bound for this problem and other LP-Type problems\nusing geometric constructions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 14:52:50 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Epstein", "Rogers", ""], ["Silwal", "Sandeep", ""]]}, {"id": "1911.08339", "submitter": "Jonathan Ullman", "authors": "Alexander Edmonds and Aleksandar Nikolov and Jonathan Ullman", "title": "The Power of Factorization Mechanisms in Local and Central Differential\n  Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new characterizations of the sample complexity of answering linear\nqueries (statistical queries) in the local and central models of differential\nprivacy:\n  *In the non-interactive local model, we give the first approximate\ncharacterization of the sample complexity. Informally our bounds are tight to\nwithin polylogarithmic factors in the number of queries and desired accuracy.\nOur characterization extends to agnostic learning in the local model.\n  *In the central model, we give a characterization of the sample complexity in\nthe high-accuracy regime that is analogous to that of Nikolov, Talwar, and\nZhang (STOC 2013), but is both quantitatively tighter and has a dramatically\nsimpler proof.\n  Our lower bounds apply equally to the empirical and population estimation\nproblems. In both cases, our characterizations show that a particular\nfactorization mechanism is approximately optimal, and the optimal sample\ncomplexity is bounded from above and below by well studied factorization norms\nof a matrix associated with the queries.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 15:17:18 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Edmonds", "Alexander", ""], ["Nikolov", "Aleksandar", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1911.08372", "submitter": "Guillermo de Bernardo", "authors": "Nieves R. Brisaboa, Ana Cerdeira-Pena, Guillermo de Bernardo, Gonzalo\n  Navarro", "title": "Improved Compressed String Dictionaries", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "Proc. 28th ACM International Conference on Information and\n  Knowledge Management (CIKM 2019)", "doi": "10.1145/3357384.3357972", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new family of compressed data structures to efficiently store\nand query large string dictionaries in main memory. Our main technique is a\ncombination of hierarchical Front-coding with ideas from longest-common-prefix\ncomputation in suffix arrays. Our data structures yield relevant space-time\ntradeoffs in real-world dictionaries. We focus on two domains where string\ndictionaries are extensively used and efficient compression is required: URL\ncollections, a key element in Web graphs and applications such as Web mining;\nand collections of URIs and literals, the basic components of RDF datasets. Our\nexperiments show that our data structures achieve better compression than the\nstate-of-the-art alternatives while providing very competitive query times.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 16:07:49 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Cerdeira-Pena", "Ana", ""], ["de Bernardo", "Guillermo", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1911.08374", "submitter": "Tobias Maier", "authors": "Tobias Maier, Peter Sanders, Robert Williger", "title": "Concurrent Expandable AMQs on the Basis of Quotient Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A quotient filter is a cache efficient AMQ data structure. Depending on the\nfill degree of the filter most insertions and queries only need to access one\nor two consecutive cache lines. This makes quotient filters fast compared to\nthe more commonly used Bloom filters that incur multiple cache misses. However,\nconcurrent Bloom filters are easy to implement and can be implemented lock-free\nwhile concurrent quotient filters are not as simple. Usually concurrent\nquotient filters work by using an external array of locks -- each protecting a\nregion of the table. Accessing this array incurs one additional cache miss per\noperation. We propose a new locking scheme that has no memory overhead. Using\nthis new locking scheme we achieve 1.8 times higher speedups than with the\ncommon external locking scheme.\n  Another advantage of quotient filters over Bloom filters is that a quotient\nfilter can change its size when it is becoming full. We implement this growing\ntechnique for our concurrent quotient filters and adapt it in a way that allows\nunbounded growing while keeping a bounded false positive rate. We call the\nresulting data structure a fully expandable quotient filter. Its design is\nsimilar to scalable Bloom filters, but we exploit some concepts inherent to\nquotient filters to improve the space efficiency and the query speed.\n  We also propose quotient filter variants that are aimed to reduce the number\nof status bits (2-status-bit variant) or to simplify concurrent implementations\n(linear probing quotient filter). The linear probing quotient filter even leads\nto a lock-free concurrent filter implementation. This is especially\ninteresting, since we show that any lock-free implementation of another common\nquotient filter variant would incur significant overheads in the form of\nadditional data fields or multiple passes over the accessed data.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 16:13:41 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Maier", "Tobias", ""], ["Sanders", "Peter", ""], ["Williger", "Robert", ""]]}, {"id": "1911.08376", "submitter": "Guillermo de Bernardo", "authors": "Nieves R. Brisaboa, Ana Cerdeira-Pena, Guillermo de Bernardo, Gonzalo\n  Navarro, Oscar Pedreira", "title": "Extending General Compact Querieable Representations to GIS Applications", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941,", "journal-ref": "Information Sciences 2020", "doi": "10.1016/j.ins.2019.08.007", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The raster model is commonly used for the representation of images in many\ndomains, and is especially useful in Geographic Information Systems (GIS) to\nstore information about continuous variables of the space (elevation,\ntemperature, etc.). Current representations of raster data are usually designed\nfor external memory or, when stored in main memory, lack efficient query\ncapabilities. In this paper we propose compact representations to efficiently\nstore and query raster datasets in main memory. We present different\nrepresentations for binary raster data, general raster data and time-evolving\nraster data. We experimentally compare our proposals with traditional storage\nmechanisms such as linear quadtrees or compressed GeoTIFF files. Results show\nthat our structures are up to 10 times smaller than classical linear quadtrees,\nand even comparable in space to non-querieable representations of raster data,\nwhile efficiently answering a number of typical queries.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 16:18:15 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Cerdeira-Pena", "Ana", ""], ["de Bernardo", "Guillermo", ""], ["Navarro", "Gonzalo", ""], ["Pedreira", "Oscar", ""]]}, {"id": "1911.08529", "submitter": "Ahmad Biniaz", "authors": "Ahmad Biniaz", "title": "Euclidean Bottleneck Bounded-Degree Spanning Tree Ratios", "comments": "SODA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the seminal works of Khuller et al. (STOC 1994) and Chan (SoCG\n2003) we study the bottleneck version of the Euclidean bounded-degree spanning\ntree problem. A bottleneck spanning tree is a spanning tree whose largest\nedge-length is minimum, and a bottleneck degree-$K$ spanning tree is a\ndegree-$K$ spanning tree whose largest edge-length is minimum. Let $\\beta_K$ be\nthe supremum ratio of the largest edge-length of the bottleneck degree-$K$\nspanning tree to the largest edge-length of the bottleneck spanning tree, over\nall finite point sets in the Euclidean plane. It is known that $\\beta_5=1$, and\nit is easy to verify that $\\beta_2\\ge 2$, $\\beta_3\\ge \\sqrt{2}$, and\n$\\beta_4>1.175$.\n  It is implied by the Hamiltonicity of the cube of the bottleneck spanning\ntree that $\\beta_2\\le 3$. The degree-3 spanning tree algorithm of Ravi et al.\n(STOC 1993) implies that $\\beta_3\\le 2$. Andersen and Ras (Networks,\n68(4):302-314, 2016) showed that $\\beta_4\\le \\sqrt{3}$. We present the\nfollowing improved bounds: $\\beta_2\\ge\\sqrt{7}$, $\\beta_3\\le \\sqrt{3}$, and\n$\\beta_4\\le \\sqrt{2}$. As a result, we obtain better approximation algorithms\nfor Euclidean bottleneck degree-3 and degree-4 spanning trees. As parts of our\nproofs of these bounds we present some structural properties of the Euclidean\nminimum spanning tree which are of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 19:48:27 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Biniaz", "Ahmad", ""]]}, {"id": "1911.08600", "submitter": "Artem Kaznatcheev", "authors": "David A. Cohen, Martin C. Cooper, Artem Kaznatcheev, Mark Wallace", "title": "Steepest ascent can be exponential in bounded treewidth problems", "comments": "8 pages main text, 4 pages appendix, 1 page references; fixed error\n  in f(a,b) to match code", "journal-ref": "Operations Research Letters 48 (2020) 217-224", "doi": "10.1016/j.orl.2020.02.010", "report-no": null, "categories": "cs.DM cs.DS cs.NE q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the complexity of local search based on steepest ascent. We\nshow that even when all variables have domains of size two and the underlying\nconstraint graph of variable interactions has bounded treewidth (in our\nconstruction, treewidth 7), there are fitness landscapes for which an\nexponential number of steps may be required to reach a local optimum. This is\nan improvement on prior recursive constructions of long steepest ascents, which\nwe prove to need constraint graphs of unbounded treewidth.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 21:42:08 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 14:42:33 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Cohen", "David A.", ""], ["Cooper", "Martin C.", ""], ["Kaznatcheev", "Artem", ""], ["Wallace", "Mark", ""]]}, {"id": "1911.08689", "submitter": "Thodoris Lykouris", "authors": "Thodoris Lykouris, Max Simchowitz, Aleksandrs Slivkins, Wen Sun", "title": "Corruption robust exploration in episodic reinforcement learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of multi-stage episodic reinforcement learning under\nadversarial corruptions in both the rewards and the transition probabilities of\nthe underlying system extending recent results for the special case of\nstochastic bandits. We provide a framework which modifies the aggressive\nexploration enjoyed by existing reinforcement learning approaches based on\n\"optimism in the face of uncertainty\", by complementing them with principles\nfrom \"action elimination\". Importantly, our framework circumvents the major\nchallenges posed by naively applying action elimination in the RL setting, as\nformalized by a lower bound we demonstrate. Our framework yields efficient\nalgorithms which (a) attain near-optimal regret in the absence of corruptions\nand (b) adapt to unknown levels corruption, enjoying regret guarantees which\ndegrade gracefully in the total corruption encountered. To showcase the\ngenerality of our approach, we derive results for both tabular settings (where\nstates and actions are finite) as well as linear-function-approximation\nsettings (where the dynamics and rewards admit a linear underlying\nrepresentation). Notably, our work provides the first sublinear regret\nguarantee which accommodates any deviation from purely i.i.d. transitions in\nthe bandit-feedback model for episodic reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 03:49:13 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 21:20:40 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Lykouris", "Thodoris", ""], ["Simchowitz", "Max", ""], ["Slivkins", "Aleksandrs", ""], ["Sun", "Wen", ""]]}, {"id": "1911.08800", "submitter": "Masataka Gohda", "authors": "Masataka Gohda and Naonori Kakimura", "title": "Online Spectral Approximation in Random Order Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies spectral approximation for a positive semidefinite matrix\nin the online setting. It is known in [Cohen et al. APPROX 2016] that we can\nconstruct a spectral approximation of a given $n \\times d$ matrix in the online\nsetting if an additive error is allowed. In this paper, we propose an online\nalgorithm that avoids an additive error with the same time and space\ncomplexities as the algorithm of Cohen et al., and provides a better upper\nbound on the approximation size when a given matrix has small rank. In\naddition, we consider the online random order setting where a row of a given\nmatrix arrives uniformly at random. In this setting, we propose time and space\nefficient algorithms to find a spectral approximation. Moreover, we reveal that\na lower bound on the approximation size in the online random order setting is\n$\\Omega (d \\epsilon^{-2} \\log n)$, which is larger than the one in the offline\nsetting by an $\\mathrm{O}\\left( \\log n \\right)$ factor.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 10:19:03 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Gohda", "Masataka", ""], ["Kakimura", "Naonori", ""]]}, {"id": "1911.08832", "submitter": "Christian Konrad", "authors": "Christian Konrad", "title": "Frequent Elements with Witnesses in Data Streams", "comments": "Fixed the statement of Lemma 5.1, introduction updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting frequent elements is among the oldest and most-studied problems in\nthe area of data streams. Given a stream of $m$ data items in $\\{1, 2, \\dots,\nn\\}$, the objective is to output items that appear at least $d$ times, for some\nthreshold parameter $d$, and provably optimal algorithms are known today.\nHowever, in many applications, knowing only the frequent elements themselves is\nnot enough: For example, an Internet router may not only need to know the most\nfrequent destination IP addresses of forwarded packages, but also the\ntimestamps of when these packages appeared or any other meta-data that\n\"arrived\" with the packages, e.g., their source IP addresses.\n  In this paper, we introduce the witness version of the frequent elements\nproblem: Given a desired approximation guarantee $\\alpha \\ge 1$ and a desired\nfrequency $d \\le \\Delta$, where $\\Delta$ is the frequency of the most frequent\nitem, the objective is to report an item together with at least $d / \\alpha$\ntimestamps of when the item appeared in the stream (or any other meta-data that\narrived with the items). We give provably optimal algorithms for both the\ninsertion-only and insertion-deletion stream settings: In insertion-only\nstreams, we show that space $\\tilde{O}(n + d \\cdot n^{\\frac{1}{\\alpha}})$ is\nnecessary and sufficient for every integral $1 \\le \\alpha \\le \\log n$. In\ninsertion-deletion streams, we show that space $\\tilde{O}(\\frac{n \\cdot\nd}{\\alpha^2})$ is necessary and sufficient, for every $\\alpha \\le \\sqrt{n}$.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 11:22:17 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 16:15:40 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2021 15:20:54 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Konrad", "Christian", ""]]}, {"id": "1911.08924", "submitter": "Sayan Bandyapadhyay", "authors": "Sayan Bandyapadhyay and Aritra Banik and Sujoy Bhore and Martin\n  N\\\"ollenburg", "title": "Geometric Planar Networks on Bichromatic Points", "comments": "Accepted in CALDAM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study four classical graph problems -- Hamiltonian path, Traveling\nsalesman, Minimum spanning tree, and Minimum perfect matching on geometric\ngraphs induced by bichromatic (red and blue) points. These problems have been\nwidely studied for points in the Euclidean plane, and many of them are NP-hard.\nIn this work, we consider these problems in two restricted settings: (i)\ncollinear points and (ii) equidistant points on a circle. We show that almost\nall of these problems can be solved in linear time in these constrained, yet\nnon-trivial settings.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 14:16:24 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Bandyapadhyay", "Sayan", ""], ["Banik", "Aritra", ""], ["Bhore", "Sujoy", ""], ["N\u00f6llenburg", "Martin", ""]]}, {"id": "1911.08959", "submitter": "Ben Hermans", "authors": "Ben Hermans, Roel Leus, Jannik Matuschke", "title": "Exact and approximation algorithms for the expanding search problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose a target is hidden in one of the vertices of an edge-weighted graph\naccording to a known probability distribution. The expanding search problem\nasks for a search sequence of the vertices so as to minimize the expected time\nfor finding the target, where the time for reaching the next vertex is\ndetermined by its distance to the region that was already searched. This\nproblem has numerous applications, such as searching for hidden explosives,\nmining coal, and disaster relief. In this paper, we develop exact algorithms\nand heuristics, including a branch-and-cut procedure, a greedy algorithm with a\nconstant-factor approximation guarantee, and a novel local search procedure\nbased on a spanning tree neighborhood. Computational experiments show that our\nbranch-and-cut procedure outperforms all existing methods for general instances\nand both heuristics compute near-optimal solutions with little computational\neffort.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 15:18:24 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Hermans", "Ben", ""], ["Leus", "Roel", ""], ["Matuschke", "Jannik", ""]]}, {"id": "1911.08964", "submitter": "Louis Dublois", "authors": "Louis Dublois, Michael Lampis, and Vangelis Th. Paschos", "title": "New Algorithms for Mixed Dominating Set", "comments": "This paper has been accepted to IPEC 2020", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 23 no.\n  1, Discrete Algorithms (April 30, 2021) dmtcs:7407", "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixed dominating set is a collection of vertices and edges that dominates\nall vertices and edges of a graph. We study the complexity of exact and\nparameterized algorithms for \\textsc{Mixed Dominating Set}, resolving some open\nquestions. In particular, we settle the problem's complexity parameterized by\ntreewidth and pathwidth by giving an algorithm running in time $O^*(5^{tw})$\n(improving the current best $O^*(6^{tw})$), as well as a lower bound showing\nthat our algorithm cannot be improved under the Strong Exponential Time\nHypothesis (SETH), even if parameterized by pathwidth (improving a lower bound\nof $O^*((2 - \\varepsilon)^{pw})$). Furthermore, by using a simple but so far\noverlooked observation on the structure of minimal solutions, we obtain\nbranching algorithms which improve both the best known FPT algorithm for this\nproblem, from $O^*(4.172^k)$ to $O^*(3.510^k)$, and the best known\nexponential-time exact algorithm, from $O^*(2^n)$ and exponential space, to\n$O^*(1.912^n)$ and polynomial space.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 15:24:47 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 13:38:43 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 10:50:58 GMT"}, {"version": "v4", "created": "Wed, 14 Apr 2021 14:18:12 GMT"}, {"version": "v5", "created": "Mon, 26 Apr 2021 14:57:44 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Dublois", "Louis", ""], ["Lampis", "Michael", ""], ["Paschos", "Vangelis Th.", ""]]}, {"id": "1911.08971", "submitter": "Guillermo de Bernardo", "authors": "Diego Arroyuelo, Guillermo de Bernardo, Travis Gagie, Gonzalo Navarro", "title": "Faster Dynamic Compressed d-ary Relations", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "Proc. SPIRE 2019", "doi": "10.1007/978-3-030-32686-9_30", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k^2$-tree is a successful compact representation of binary relations\nthat exhibit sparseness and/or clustering properties. It can be extended to $d$\ndimensions, where it is called a $k^d$-tree. The representation boils down to a\nlong bitvector. We show that interpreting the $k^d$-tree as a dynamic trie on\nthe Morton codes of the points, instead of as a dynamic representation of the\nbitvector as done in previous work, yields operation times that are below the\nlower bound of dynamic bitvectors and offers improved time performance in\npractice.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 15:35:20 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Arroyuelo", "Diego", ""], ["de Bernardo", "Guillermo", ""], ["Gagie", "Travis", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1911.09044", "submitter": "Daniil Galaktionov", "authors": "Nieves R. Brisaboa, Antonio Fari\\~na, Daniil Galaktionov, Tirso V.\n  Rodeiro, M. Andrea Rodr\\'iguez", "title": "New structures to solve aggregated queries for trips over public\n  transportation networks", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "Proc. of the 25th International Symposium on String Processing and\n  Information Retrieval (SPIRE), Lima, Peru, October 9-11th, pp 85-101 (2018)", "doi": "10.1007/978-3-030-00479-8_8", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing the trajectories of mobile objects is a hot topic from the\nwidespread use of smartphones and other GPS devices. However, few works have\nfocused on representing trips over public transportation networks (buses,\nsubway, and trains) where a user's trips can be seen as a sequence of stages\nperformed within a vehicle shared with many other users. In this context,\nrepresenting vehicle journeys reduces the redundancy because all the passengers\ninside a vehicle share the same arrival time for each stop. In addition, each\nvehicle journey follows exactly the sequence of stops corresponding to its\nline, which makes it unnecessary to represent that sequence for each journey.\n  To solve data management for transportation systems, we designed a conceptual\nmodel that gave us a better insight into this data domain and allowed us the\ndefinition of relevant terms and the detection of redundancy sources among\nthose data. Then, we designed two compact representations focused on users'\ntrips (TTCTR) and on vehicle trips (AcumM), respectively. Each approach owns\nsome strengths and is able to answer some queries efficiently.\n  We include experimental results over synthetic trips generated from accurate\nschedules obtained from a real network description (from the bus transportation\nsystem of Madrid) to show the space/time trade-off of both approaches. We\nconsidered a wide range of different queries about the use of the\ntransportation network such as counting-based or aggregate queries regarding\nthe load of any line of the network at different times.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 17:18:11 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Fari\u00f1a", "Antonio", ""], ["Galaktionov", "Daniil", ""], ["Rodeiro", "Tirso V.", ""], ["Rodr\u00edguez", "M. Andrea", ""]]}, {"id": "1911.09077", "submitter": "Alberto Ordonez Pereira", "authors": "Alberto Ord\\'o\\~nez, Gonzalo Navarro and Nieves R. Brisaboa", "title": "Grammar Compressed Sequences with Rank/Select Support", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "Journal of Discrete Algorithms 43, pp. 54-71 (2017)", "doi": "10.1016/j.jda.2016.10.001", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence representations supporting not only direct access to their symbols,\nbut also rank/select operations, are a fundamental building block in many\ncompressed data structures. Several recent applications need to represent\nhighly repetitive sequences, and classical statistical compression proves\nineffective. We introduce, instead, grammar-based representations for\nrepetitive sequences, which use up to 6% of the space needed by statistically\ncompressed representations, and support direct access and rank/select\noperations within tens of microseconds. We demonstrate the impact of our\nstructures in text indexing applications.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 18:29:30 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 21:32:34 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Ord\u00f3\u00f1ez", "Alberto", ""], ["Navarro", "Gonzalo", ""], ["Brisaboa", "Nieves R.", ""]]}, {"id": "1911.09133", "submitter": "Harro Wimmel", "authors": "Harro Wimmel", "title": "Synthesis of Reduced Asymmetric Choice Petri Nets", "comments": "27 pages, 10 figures, V2 due to font problem with ulsy.sty (one font\n  symbol had been erroneously replaced with a greek Psi by LiveTeX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Petri net is choice-free if any place has at most one transition in its\npostset (consuming its tokens) and it is (extended) free-choice (EFC) if the\npostsets of any two places are either equal or disjoint. Asymmetric choice (AC)\nextends EFC such that two places may also have postsets where one is contained\nin the other. In reduced AC nets this containment is limited: If the postsets\nare neither disjoint nor equal, one is a singleton and the other has exactly\ntwo transitions. The aim of Petri net synthesis is to find an unlabelled Petri\nnet in some target class with a reachability graph isomorphic to a given finite\nlabelled transition system (lts). Choice-free nets have strong properties,\nallowing to often easily detect when synthesis will fail or at least to quicken\nthe synthesis. With EFC as the target class, only few properties can be checked\nahead and there seem to be no short cuts lowering the complexity of the\nsynthesis (compared to arbitrary Petri nets). For AC nets no synthesis\nprocedure is known at all. We show here how synthesis to a superclass of\nreduced AC nets (not containing the full AC net class) can be done.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 19:07:10 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 10:28:05 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Wimmel", "Harro", ""]]}, {"id": "1911.09150", "submitter": "Bundit Laekhanukit", "authors": "Chun-Hsiang Chan, Bundit Laekhanukit, Hao-Ting Wei, Yuhao Zhang", "title": "Polylogarithmic Approximation Algorithm for k-Connected Directed Steiner\n  Tree on Quasi-Bipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the k-Connected Directed Steiner Tree problem (k-DST), we are given a\ndirected graph G=(V, E) with edge (or vertex) costs, a root vertex r, a set of\nq terminals T, and a connectivity requirement k>0; the goal is to find a\nminimum-cost subgraph H of G such that H has k internally disjoint paths from\nthe root r to each terminal t . The k-DST problem is a natural generalization\nof the classical Directed Steiner Tree problem (DST) in the fault-tolerant\nsetting in which the solution subgraph is required to have an r,t-path, for\nevery terminal t, even after removing k-1 edges or vertices.\n  Despite being a classical problem, there are not many positive results on the\nproblem, especially for the case k >= 3. In this paper, we will present an\nO(log k log q)-approximation algorithm for k-DST when an input graph is\nquasi-bipartite, i.e., when there is no edge joining two non-terminal vertices.\nTo the best of our knowledge, our algorithm is the only known non-trivial\napproximation algorithm for k-DST, for k >= 3, that runs in polynomial-time\nregardless of the structure of the optimal solution. In addition, our algorithm\nis tight for every constant k, due to the hardness result inherited from the\nSet Cover problem.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 20:02:45 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Chan", "Chun-Hsiang", ""], ["Laekhanukit", "Bundit", ""], ["Wei", "Hao-Ting", ""], ["Zhang", "Yuhao", ""]]}, {"id": "1911.09165", "submitter": "Jason Li", "authors": "Anupam Gupta, Euiwoong Lee, Jason Li", "title": "The Karger-Stein Algorithm is Optimal for $k$-cut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $k$-cut problem, we are given an edge-weighted graph and want to find\nthe least-weight set of edges whose deletion breaks the graph into $k$\nconnected components. Algorithms due to Karger-Stein and Thorup showed how to\nfind such a minimum $k$-cut in time approximately $O(n^{2k-2})$. The best lower\nbounds come from conjectures about the solvability of the $k$-clique problem\nand a reduction from $k$-clique to $k$-cut, and show that solving $k$-cut is\nlikely to require time $\\Omega(n^k)$. Our recent results have given\nspecial-purpose algorithms that solve the problem in time $n^{1.98k + O(1)}$,\nand ones that have better performance for special classes of graphs (e.g., for\nsmall integer weights). In this work, we resolve the problem for general\ngraphs, by showing that for any fixed $k \\geq 2$, the Karger-Stein algorithm\noutputs any fixed minimum $k$-cut with probability at least $\\hat{O}(n^{-k})$,\nwhere $\\hat{O}(\\cdot)$ hides a $2^{O(\\ln \\ln n)^2}$ factor. This also gives an\nextremal bound of $\\hat{O}(n^k)$ on the number of minimum $k$-cuts in an\n$n$-vertex graph and an algorithm to compute a minimum $k$-cut in similar\nruntime. Both are tight up to $\\hat{O}(1)$ factors. The first main ingredient\nin our result is a fine-grained analysis of how the graph shrinks---and how the\naverage degree evolves---under the Karger-Stein process. The second ingredient\nis an extremal result bounding the number of cuts of size at most $(2-\\delta)\nOPT/k$, using the Sunflower lemma.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 20:38:24 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Gupta", "Anupam", ""], ["Lee", "Euiwoong", ""], ["Li", "Jason", ""]]}, {"id": "1911.09176", "submitter": "Luowen Qian", "authors": "Kai-Min Chung, Tai-Ning Liao, Luowen Qian", "title": "Lower Bounds for Function Inversion with Quantum Advice", "comments": "ITC full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.CR cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Function inversion is the problem that given a random function $f: [M] \\to\n[N]$, we want to find pre-image of any image $f^{-1}(y)$ in time $T$. In this\nwork, we revisit this problem under the preprocessing model where we can\ncompute some auxiliary information or advice of size $S$ that only depends on\n$f$ but not on $y$. It is a well-studied problem in the classical settings,\nhowever, it is not clear how quantum algorithms can solve this task any better\nbesides invoking Grover's algorithm, which does not leverage the power of\npreprocessing.\n  Nayebi et al. proved a lower bound $ST^2 \\ge \\tilde\\Omega(N)$ for quantum\nalgorithms inverting permutations, however, they only consider algorithms with\nclassical advice. Hhan et al. subsequently extended this lower bound to fully\nquantum algorithms for inverting permutations. In this work, we give the same\nasymptotic lower bound to fully quantum algorithms for inverting functions for\nfully quantum algorithms under the regime where $M = O(N)$.\n  In order to prove these bounds, we generalize the notion of quantum random\naccess code, originally introduced by Ambainis et al., to the setting where we\nare given a list of (not necessarily independent) random variables, and we wish\nto compress them into a variable-length encoding such that we can retrieve a\nrandom element just using the encoding with high probability. As our main\ntechnical contribution, we give a nearly tight lower bound (for a wide\nparameter range) for this generalized notion of quantum random access codes,\nwhich may be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 21:13:26 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 05:23:57 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Chung", "Kai-Min", ""], ["Liao", "Tai-Ning", ""], ["Qian", "Luowen", ""]]}, {"id": "1911.09221", "submitter": "Hugo Rosado", "authors": "Lehilton Lelis Chaves Pedrosa, Hugo Kooki Kasuya Rosado", "title": "A 2-approximation for the $k$-prize-collecting Steiner tree problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the $k$-prize-collecting Steiner tree problem. An instance is\ncomposed of an integer $k$ and a graph $G$ with costs on edges and penalties on\nvertices. The objective is to find a tree spanning at least $k$ vertices which\nminimizes the cost of the edges in the tree plus the penalties of vertices not\nin the tree. This is one of the most fundamental network design problems and is\na common generalization of the prize-collecting Steiner tree and the\n$k$-minimum spanning tree problems. Our main result is a 2-approximation\nalgorithm, which improves on the currently best known approximation factor of\n3.96 and has a faster running time. The algorithm builds on a modification of\nthe primal-dual framework of Goemans and Williamson, and reveals interesting\nproperties that can be applied to other similar problems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 23:52:03 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Pedrosa", "Lehilton Lelis Chaves", ""], ["Rosado", "Hugo Kooki Kasuya", ""]]}, {"id": "1911.09370", "submitter": "Susana Ladra", "authors": "Jos\\'e Fuentes-Sep\\'ulveda and Susana Ladra", "title": "Energy consumption in compact integer vectors: A study case", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "IEEE Access 7, pp. 155625-155636 (2019)", "doi": "10.1109/ACCESS.2019.2949655", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the field of algorithms and data structures analysis and design, most of\nthe researchers focus only on the space/time trade-off, and little attention\nhas been paid to energy consumption. Moreover, most of the efforts in the field\nof Green Computing have been devoted to hardware-related issues, being green\nsoftware in its infancy. Optimizing the usage of computing resources,\nminimizing power consumption or increasing battery life are some of the goals\nof this field of research.\n  As an attempt to address the most recent sustainability challenges, we must\nincorporate the energy consumption as a first-class constraint when designing\nnew compact data structures. Thus, as a preliminary work to reach that goal, we\nfirst need to understand the factors that impact on the energy consumption and\ntheir relation with compression. In this work, we study the energy consumption\nrequired by several integer vector representations. We execute typical\noperations over datasets of different nature. We can see that, as commonly\nbelieved, energy consumption is highly related to the time required by the\nprocess, but not always. We analyze other parameters, such as number of\ninstructions, number of CPU cycles, memory loads, among others.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 09:56:37 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Fuentes-Sep\u00falveda", "Jos\u00e9", ""], ["Ladra", "Susana", ""]]}, {"id": "1911.09447", "submitter": "Gregor Ulm", "authors": "Gregor Ulm, Simon Smith, Adrian Nilsson, Emil Gustavsson, Mats\n  Jirstrand", "title": "S-RASTER: Contraction Clustering for Evolving Data Streams", "comments": "24 pages, 5 figures, 2 tables", "journal-ref": "Journal of Big Data (Springer) Vol. 7, Article number: 62 (2020)", "doi": "10.1186/s40537-020-00336-3", "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contraction Clustering (RASTER) is a single-pass algorithm for density-based\nclustering of 2D data. It can process arbitrary amounts of data in linear time\nand in constant memory, quickly identifying approximate clusters. It also\nexhibits good scalability in the presence of multiple CPU cores. RASTER\nexhibits very competitive performance compared to standard clustering\nalgorithms, but at the cost of decreased precision. Yet, RASTER is limited to\nbatch processing and unable to identify clusters that only exist temporarily.\nIn contrast, S-RASTER is an adaptation of RASTER to the stream processing\nparadigm that is able to identify clusters in evolving data streams. This\nalgorithm retains the main benefits of its parent algorithm, i.e. single-pass\nlinear time cost and constant memory requirements for each discrete time step\nwithin a sliding window. The sliding window is efficiently pruned, and\nclustering is still performed in linear time. Like RASTER, S-RASTER trades off\nan often negligible amount of precision for speed. Our evaluation shows that\ncompeting algorithms are at least 50% slower. Furthermore, S-RASTER shows good\nqualitative results, based on standard metrics. It is very well suited to\nreal-world scenarios where clustering does not happen continually but only\nperiodically.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 13:01:43 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 12:57:20 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 22:48:06 GMT"}, {"version": "v4", "created": "Mon, 6 Jul 2020 19:38:21 GMT"}, {"version": "v5", "created": "Wed, 16 Sep 2020 12:43:59 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Ulm", "Gregor", ""], ["Smith", "Simon", ""], ["Nilsson", "Adrian", ""], ["Gustavsson", "Emil", ""], ["Jirstrand", "Mats", ""]]}, {"id": "1911.09498", "submitter": "Jos\\'e Fuentes", "authors": "Jos\\'e Fuentes-Sep\\'ulveda and Gonzalo Navarro and Diego Seco", "title": "Implementing the Topological Model Succinctly", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. Conference version\n  presented at SPIRE 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32686-9_35", "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the topological model, a semantically rich standard to represent\nGIS data, can be encoded succinctly while efficiently answering a number of\ntopology-related queries. We build on recent succinct planar graph\nrepresentations so as to encode a model with $m$ edges within $4m+o(m)$ bits\nand answer various queries relating nodes, edges, and faces in $o(\\log\\log m)$\ntime, or any time in $\\omega(\\log m)$ for a few complex ones.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 14:42:08 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Fuentes-Sep\u00falveda", "Jos\u00e9", ""], ["Navarro", "Gonzalo", ""], ["Seco", "Diego", ""]]}, {"id": "1911.09650", "submitter": "Rajesh Chitnis", "authors": "Rajesh Chitnis, Graham Cormode", "title": "Towards a Theory of Parameterized Streaming Algorithms", "comments": "Extended abstract in IPEC 2019. arXiv admin note: text overlap with\n  arXiv:1603.05715 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameterized complexity attempts to give a more fine-grained analysis of the\ncomplexity of problems: instead of measuring the running time as a function of\nonly the input size, we analyze the running time with respect to additional\nparameters. This approach has proven to be highly successful in delineating our\nunderstanding of \\NP-hard problems. Given this success with the TIME resource,\nit seems but natural to use this approach for dealing with the SPACE resource.\nFirst attempts in this direction have considered a few individual problems,\nwith some success: Fafianie and Kratsch [MFCS'14] and Chitnis et al. [SODA'15]\nintroduced the notions of streaming kernels and parameterized streaming\nalgorithms respectively. For example, the latter shows how to refine the\n$\\Omega(n^2)$ bit lower bound for finding a minimum Vertex Cover (VC) in the\nstreaming setting by designing an algorithm for the parameterized $k$-VC\nproblem which uses $O(k^{2}\\log n)$ bits.\n  In this paper, we initiate a systematic study of graph problems from the\nparadigm of parameterized streaming algorithms. We first define a natural\nhierarchy of space complexity classes of FPS, SubPS, SemiPS, SupPS and BrutePS,\nand then obtain tight classifications for several well-studied graph problems\nsuch as Longest Path, Feedback Vertex Set, Dominating Set, Girth, Treewidth,\netc. into this hierarchy.\n  (see paper for full abstract)\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 14:11:12 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Chitnis", "Rajesh", ""], ["Cormode", "Graham", ""]]}, {"id": "1911.09810", "submitter": "Xiaoyuan Liu", "authors": "Xiaoyuan Liu, Hayato Ushijima-Mwesigwa, Avradip Mandal, Sarvagya\n  Upadhyay, Ilya Safro, Arnab Roy", "title": "Leveraging Special-Purpose Hardware for Local Search Heuristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we approach the physical limits predicted by Moore's law, a variety of\nspecialized hardware is emerging to tackle specialized tasks in different\ndomains. Within combinatorial optimization, adiabatic quantum computers, CMOS\nannealers, and optical parametric oscillators are few of the emerging\nspecialized hardware technology aimed at solving optimization problems. In\nterms of mathematical framework, the Ising optimization model unifies all of\nthese emerging special-purpose hardware. In other words, they are all designed\nto solve optimization problems expressed in the Ising model or equivalently as\na quadratic unconstrained binary optimization model. Due to variety of\nconstraints specific to each type of hardware, they usually suffer from a major\nchallenge: the number of variables that the hardware can manage to solve is\nvery limited. Given that large-scale practical problems, including problems in\noperations research, combinatorial scientific computing, data science and\nnetwork science require significantly more variables to model than these\ndevices provide, we are likely to witness that cloud-based deployments of these\ndevices will be available for parallel and shared access. Thus hybrid\ntechniques in combination with both hardware and software must be developed to\nutilize these technologies. Local search meta-heuristics is one of the\napproaches to tackle large scale problems. However, a general optimization step\nwithin local search is not traditionally formulated in the Ising form. In this\nwork, we propose a new meta-heuristic to model local search in the Ising form\nfor the special-purpose hardware devices. As such, we demonstrate that our\nmethod takes the limitations of the Ising model and current hardware into\naccount, utilizes a given hardware more efficiently compared to previous\napproaches, while also producing high quality solutions compared to other\nwell-known meta-heuristics.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 02:05:11 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 02:44:04 GMT"}, {"version": "v3", "created": "Sat, 28 Nov 2020 17:34:08 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Liu", "Xiaoyuan", ""], ["Ushijima-Mwesigwa", "Hayato", ""], ["Mandal", "Avradip", ""], ["Upadhyay", "Sarvagya", ""], ["Safro", "Ilya", ""], ["Roy", "Arnab", ""]]}, {"id": "1911.09852", "submitter": "Haris Aziz", "authors": "Haris Aziz", "title": "Developments in Multi-Agent Fair Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness is becoming an increasingly important concern when designing\nmarkets, allocation procedures, and computer systems. I survey some recent\ndevelopments in the field of multi-agent fair allocation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 04:36:28 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 04:21:03 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Aziz", "Haris", ""]]}, {"id": "1911.09890", "submitter": "Matthias Mnich", "authors": "Krist\\'of B\\'erczi and Andr\\'e Berger and Matthias Mnich and Roland\n  Vincze", "title": "Degree-Bounded Generalized Polymatroids and Approximating the Metric\n  Many-Visits TSP", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Bounded Degree Matroid Basis Problem, we are given a matroid and a\nhypergraph on the same ground set, together with costs for the elements of that\nset as well as lower and upper bounds $f(\\varepsilon)$ and $g(\\varepsilon)$ for\neach hyperedge $\\varepsilon$. The objective is to find a minimum-cost basis $B$\nsuch that $f(\\varepsilon) \\leq |B \\cap \\varepsilon| \\leq g(\\varepsilon)$ for\neach hyperedge $\\varepsilon$. Kir\\'aly et al. (Combinatorica, 2012) provided an\nalgorithm that finds a basis of cost at most the optimum value which violates\nthe lower and upper bounds by at most $2 \\Delta-1$, where $\\Delta$ is the\nmaximum degree of the hypergraph. When only lower or only upper bounds are\npresent for each hyperedge, this additive error is decreased to $\\Delta-1$.\n  We consider an extension of the matroid basis problem to generalized\npolymatroids, or g-polymatroids, and additionally allow element multiplicities.\nThe Bounded Degree g-polymatroid Element Problem with Multiplicities takes as\ninput a g-polymatroid $Q(p,b)$ instead of a matroid, and besides the lower and\nupper bounds, each hyperedge $\\varepsilon$ has element multiplicities\n$m_\\varepsilon$. Building on the approach of Kir\\'aly et al., we provide an\nalgorithm for finding a solution of cost at most the optimum value, having the\nsame additive approximation guarantee.\n  As an application, we develop a $1.5$-approximation for the metric\nMany-Visits TSP, where the goal is to find a minimum-cost tour that visits each\ncity $v$ a positive $r(v)$ number of times. Our approach combines our algorithm\nfor the Bounded Degree g-polymatroid Element Problem with Multiplicities with\nthe principle of Christofides' algorithm from 1976 for the (single-visit)\nmetric TSP, whose approximation guarantee it matches.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 07:06:57 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2019 11:27:09 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["B\u00e9rczi", "Krist\u00f3f", ""], ["Berger", "Andr\u00e9", ""], ["Mnich", "Matthias", ""], ["Vincze", "Roland", ""]]}, {"id": "1911.10099", "submitter": "Marijn Heule", "authors": "Sean Weaver and Marijn Heule", "title": "Constructing Minimal Perfect Hash Functions Using SAT Technology", "comments": "Accepted for AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimal perfect hash functions (MPHFs) are used to provide efficient access\nto values of large dictionaries (sets of key-value pairs). Discovering new\nalgorithms for building MPHFs is an area of active research, especially from\nthe perspective of storage efficiency. The information-theoretic limit for\nMPHFs is 1/(ln 2) or roughly 1.44 bits per key. The current best practical\nalgorithms range between 2 and 4 bits per key. In this article, we propose two\nSAT-based constructions of MPHFs. Our first construction yields MPHFs near the\ninformation-theoretic limit. For this construction, current state-of-the-art\nSAT solvers can handle instances where the dictionaries contain up to 40\nelements, thereby outperforming the existing (brute-force) methods. Our second\nconstruction uses XOR-SAT filters to realize a practical approach with\nlong-term storage of approximately 1.83 bits per key.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 15:52:01 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Weaver", "Sean", ""], ["Heule", "Marijn", ""]]}, {"id": "1911.10137", "submitter": "Uri Stemmer", "authors": "Haim Kaplan, Katrina Ligett, Yishay Mansour, Moni Naor, Uri Stemmer", "title": "Privately Learning Thresholds: Closing the Exponential Gap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the sample complexity of learning threshold functions under the\nconstraint of differential privacy. It is assumed that each labeled example in\nthe training data is the information of one individual and we would like to\ncome up with a generalizing hypothesis $h$ while guaranteeing differential\nprivacy for the individuals. Intuitively, this means that any single labeled\nexample in the training data should not have a significant effect on the choice\nof the hypothesis. This problem has received much attention recently; unlike\nthe non-private case, where the sample complexity is independent of the domain\nsize and just depends on the desired accuracy and confidence, for private\nlearning the sample complexity must depend on the domain size $X$ (even for\napproximate differential privacy). Alon et al. (STOC 2019) showed a lower bound\nof $\\Omega(\\log^*|X|)$ on the sample complexity and Bun et al. (FOCS 2015)\npresented an approximate-private learner with sample complexity\n$\\tilde{O}\\left(2^{\\log^*|X|}\\right)$. In this work we reduce this gap\nsignificantly, almost settling the sample complexity. We first present a new\nupper bound (algorithm) of $\\tilde{O}\\left(\\left(\\log^*|X|\\right)^2\\right)$ on\nthe sample complexity and then present an improved version with sample\ncomplexity $\\tilde{O}\\left(\\left(\\log^*|X|\\right)^{1.5}\\right)$.\n  Our algorithm is constructed for the related interior point problem, where\nthe goal is to find a point between the largest and smallest input elements. It\nis based on selecting an input-dependent hash function and using it to embed\nthe database into a domain whose size is reduced logarithmically; this results\nin a new database, an interior point of which can be used to generate an\ninterior point in the original database in a differentially private manner.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 16:58:25 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Kaplan", "Haim", ""], ["Ligett", "Katrina", ""], ["Mansour", "Yishay", ""], ["Naor", "Moni", ""], ["Stemmer", "Uri", ""]]}, {"id": "1911.10172", "submitter": "Mingfei Zhao", "authors": "Yang Cai, Argyris Oikonomou, Grigoris Velegkas, Mingfei Zhao", "title": "An Efficient $\\varepsilon$-BIC to BIC Transformation and Its Application\n  to Black-Box Reduction in Revenue Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the black-box reduction from multi-dimensional revenue\nmaximization to virtual welfare maximization. Cai et al. show a polynomial-time\napproximation-preserving reduction, however, the mechanism produced by their\nreduction is only approximately Bayesian incentive compatible\n($\\varepsilon$-BIC). We provide two new polynomial time transformations that\nconvert any $\\varepsilon$-BIC mechanism to an exactly BIC mechanism with only a\nnegligible revenue loss.\n  Our first transformation applies to any mechanism design setting with\ndownward-closed outcome space and only requires sample access to the agents'\ntype distributions. Our second transformation applies to the fully general\noutcome space, removing the downward-closed assumption, but requires full\naccess to the agents' type distributions. Both transformations only require\nquery access to the original $\\varepsilon$-BIC mechanism. Other\n$\\varepsilon$-BIC to BIC transformations for revenue exist in the literature\nbut all require exponential time to run in both of the settings we consider. As\nan application of our transformations, we improve the reduction by Cai et al.\nto generate an exactly BIC mechanism.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 18:11:49 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 22:43:58 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 00:50:20 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Cai", "Yang", ""], ["Oikonomou", "Argyris", ""], ["Velegkas", "Grigoris", ""], ["Zhao", "Mingfei", ""]]}, {"id": "1911.10217", "submitter": "Jacopo Pantaleoni", "authors": "Jacopo Pantaleoni", "title": "Importance Sampling of Many Lights with Reinforcement Lightcuts Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we introduce a novel technique for sampling and\nintegrating direct illumination in the presence of many lights. Unlike previous\nwork, the presented technique importance samples the product distribution of\nradiance and visibility while using bounded memory footprint and very low\nsampling overhead. This is achieved by learning a compact approximation of the\ntarget distributions over both space and time, allowing to reuse and adapt the\nlearnt distributions both spatially, within a frame, and temporally, across\nmultiple frames. Finally, the technique is amenable to massive parallelization\non GPUs and suitable for both offline and real-time rendering.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 19:02:02 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 09:00:31 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 11:46:20 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Pantaleoni", "Jacopo", ""]]}, {"id": "1911.10262", "submitter": "Sofiat Olaosebikan", "authors": "Sofiat Olaosebikan and David Manlove", "title": "An Algorithm for Strong Stability in the Student-Project Allocation\n  Problem with Ties", "comments": "28 pages (including References). To appear in Proceedings of CALDAM\n  2020: 6th Annual International Conference on Algorithms and Discrete Applied\n  Mathematics. arXiv admin note: text overlap with arXiv:1805.09887", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a variant of the Student-Project Allocation problem with lecturer\npreferences over Students where ties are allowed in the preference lists of\nstudents and lecturers (SPA-ST). We investigate the concept of strong stability\nin this context. Informally, a matching is strongly stable if there is no\nstudent and lecturer $l$ such that if they decide to form a private arrangement\noutside of the matching via one of $l$'s proposed projects, then neither party\nwould be worse off and at least one of them would strictly improve. We describe\nthe first polynomial-time algorithm to find a strongly stable matching or to\nreport that no such matching exists, given an instance of SPA-ST. Our algorithm\nruns in $O(m^2)$ time, where $m$ is the total length of the students'\npreference lists.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 18:18:43 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Olaosebikan", "Sofiat", ""], ["Manlove", "David", ""]]}, {"id": "1911.10304", "submitter": "Samuel Hopkins", "authors": "Samuel B. Hopkins, Tselil Schramm, Luca Trevisan", "title": "Subexponential LPs Approximate Max-Cut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for every $\\varepsilon > 0$, the degree-$n^\\varepsilon$\nSherali-Adams linear program (with $\\exp(\\tilde{O}(n^\\varepsilon))$ variables\nand constraints) approximates the maximum cut problem within a factor of\n$(\\frac{1}{2}+\\varepsilon')$, for some $\\varepsilon'(\\varepsilon) > 0$. Our\nresult provides a surprising converse to known lower bounds against all linear\nprogramming relaxations of Max-Cut, and hence resolves the extension complexity\nof approximate Max-Cut for approximation factors close to $\\frac{1}{2}$ (up to\nthe function $\\varepsilon'(\\varepsilon)$). Previously, only semidefinite\nprograms and spectral methods were known to yield approximation factors better\nthan $\\frac 12$ for Max-Cut in time $2^{o(n)}$. We also show that\nconstant-degree Sherali-Adams linear programs (with $\\text{poly}(n)$ variables\nand constraints) can solve Max-Cut with approximation factor close to $1$ on\ngraphs of small threshold rank: this is the first connection of which we are\naware between threshold rank and linear programming-based algorithms.\n  Our results separate the power of Sherali-Adams versus Lov\\'asz-Schrijver\nhierarchies for approximating Max-Cut, since it is known that\n$(\\frac{1}{2}+\\varepsilon)$ approximation of Max Cut requires\n$\\Omega_\\varepsilon (n)$ rounds in the Lov\\'asz-Schrijver hierarchy.\n  We also provide a subexponential time approximation for Khot's Unique Games\nproblem: we show that for every $\\varepsilon > 0$ the degree-$(n^\\varepsilon\n\\log q)$ Sherali-Adams linear program distinguishes instances of Unique Games\nof value $\\geq 1-\\varepsilon'$ from instances of value $\\leq \\varepsilon'$, for\nsome $\\varepsilon'( \\varepsilon) >0$, where $q$ is the alphabet size. Such\nguarantees are qualitatively similar to those of previous subexponential-time\nalgorithms for Unique Games but our algorithm does not rely on semidefinite\nprogramming or subspace enumeration techniques.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 03:20:04 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 17:09:19 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Hopkins", "Samuel B.", ""], ["Schramm", "Tselil", ""], ["Trevisan", "Luca", ""]]}, {"id": "1911.10381", "submitter": "Emmanouil Vasileios Vlatakis Gkaragkounis", "authors": "Xi Chen, Chenghao Guo, Emmanouil-Vasileios Vlatakis-Gkaragkounis,\n  Mihalis Yannakakis and Xinzhi Zhang", "title": "Smoothed complexity of local Max-Cut and binary Max-CSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the smoothed complexity of the FLIP algorithm for local Max-Cut\nis at most $\\smash{\\phi n^{O(\\sqrt{\\log n})}}$, where $n$ is the number of\nnodes in the graph and $\\phi$ is a parameter that measures the magnitude of\nperturbations applied on its edge weights. This improves the previously best\nupper bound of $\\phi n^{O(\\log n)}$ by Etscheid and R\\\"{o}glin. Our result is\nbased on an analysis of long sequences of flips, which shows~that~it is very\nunlikely for every flip in a long sequence to incur a positive but small\nimprovement in the cut weight. We also extend the same upper bound on the\nsmoothed complexity of FLIP to all binary Maximum Constraint Satisfaction\nProblems.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 16:15:11 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Chen", "Xi", ""], ["Guo", "Chenghao", ""], ["Vlatakis-Gkaragkounis", "Emmanouil-Vasileios", ""], ["Yannakakis", "Mihalis", ""], ["Zhang", "Xinzhi", ""]]}, {"id": "1911.10541", "submitter": "Yuval Dagan", "authors": "Yuval Dagan and Vitaly Feldman", "title": "PAC learning with stable and private predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study binary classification algorithms for which the prediction on any\npoint is not too sensitive to individual examples in the dataset. Specifically,\nwe consider the notions of uniform stability (Bousquet and Elisseeff, 2001) and\nprediction privacy (Dwork and Feldman, 2018). Previous work on these notions\nshows how they can be achieved in the standard PAC model via simple aggregation\nof models trained on disjoint subsets of data. Unfortunately, this approach\nleads to a significant overhead in terms of sample complexity. Here we\ndemonstrate several general approaches to stable and private prediction that\neither eliminate or significantly reduce the overhead. Specifically, we\ndemonstrate that for any class $C$ of VC dimension $d$ there exists a\n$\\gamma$-uniformly stable algorithm for learning $C$ with excess error $\\alpha$\nusing $\\tilde O(d/(\\alpha\\gamma) + d/\\alpha^2)$ samples. We also show that this\nbound is nearly tight. For $\\epsilon$-differentially private prediction we give\ntwo new algorithms: one using $\\tilde O(d/(\\alpha^2\\epsilon))$ samples and\nanother one using $\\tilde O(d^2/(\\alpha\\epsilon) + d/\\alpha^2)$ samples. The\nbest previously known bounds for these problems are $O(d/(\\alpha^2\\gamma))$ and\n$O(d/(\\alpha^3\\epsilon))$, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 14:48:29 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 09:11:58 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Dagan", "Yuval", ""], ["Feldman", "Vitaly", ""]]}, {"id": "1911.10616", "submitter": "Noujan Pashanasangi", "authors": "Noujan Pashanasangi, C. Seshadhri", "title": "Efficiently Counting Vertex Orbits of All 5-vertex Subgraphs, by EVOKE", "comments": "We replaced the previous version with the full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph counting is a fundamental task in network analysis. Typically,\nalgorithmic work is on total counting, where we wish to count the total\nfrequency of a (small) pattern subgraph in a large input data set. But many\napplications require local counts (also called vertex orbit counts) wherein,\nfor every vertex $v$ of the input graph, one needs the count of the pattern\nsubgraph involving $v$. This provides a rich set of vertex features that can be\nused in machine learning tasks, especially classification and clustering. But\ngetting local counts is extremely challenging. Even the easier problem of\ngetting total counts has received much research attention. Local counts require\nalgorithms that get much finer grained information, and the sheer output size\nmakes it difficult to design scalable algorithms.\n  We present EVOKE, a scalable algorithm that can determine vertex orbits\ncounts for all 5-vertex pattern subgraphs. In other words, EVOKE exactly\ndetermines, for every vertex $v$ of the input graph and every 5-vertex subgraph\n$H$, the number of copies of $H$ that $v$ participates in. EVOKE can process\ngraphs with tens of millions of edges, within an hour on a commodity machine.\nEVOKE is typically hundreds of times faster than previous state of the art\nalgorithms, and gets results on datasets beyond the reach of previous methods.\n  Theoretically, we generalize a recent \"graph cutting\" framework to get vertex\norbit counts. This framework generate a collection of polynomial equations\nrelating vertex orbit counts of larger subgraphs to those of smaller subgraphs.\nEVOKE carefully exploits the structure among these equations to rapidly count.\nWe prove and empirically validate that EVOKE only has a small constant factor\noverhead over the best (total) 5-vertex subgraph counter.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 21:48:42 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 07:30:04 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Pashanasangi", "Noujan", ""], ["Seshadhri", "C.", ""]]}, {"id": "1911.10765", "submitter": "Sahil Singla", "authors": "Deeparnab Chakrabarty, Yin Tat Lee, Aaron Sidford, Sahil Singla, and\n  Sam Chiu-wai Wong", "title": "Faster Matroid Intersection", "comments": "38 pages. Preliminary version appeared in FOCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the classic matroid intersection problem: given two\nmatroids $\\M_{1}=(V,\\I_{1})$ and $\\M_{2}=(V,\\I_{2})$ defined over a common\nground set $V$, compute a set $S\\in\\I_{1}\\cap\\I_{2}$ of largest possible\ncardinality, denoted by $r$. We consider this problem both in the setting where\neach $\\M_{i}$ is accessed through an independence oracle, i.e. a routine which\nreturns whether or not a set $S\\in\\I_{i}$ in $\\indep$ time, and the setting\nwhere each $\\M_{i}$ is accessed through a rank oracle, i.e. a routine which\nreturns the size of the largest independent subset of $S$ in $\\M_{i}$ in\n$\\rank$ time.\n  In each setting we provide faster exact and approximate algorithms. Given an\nindependence oracle, we provide an exact $O(nr\\log r \\indep)$ time algorithm.\nThis improves upon the running time of $O(nr^{1.5} \\indep)$ due to Cunningham\nin 1986 and $\\tilde{O}(n^{2} \\indep+n^{3})$ due to Lee, Sidford, and Wong in\n2015. We also provide two algorithms which compute a $(1-\\epsilon)$-approximate\nsolution to matroid intersection running in times $\\tilde{O}(n^{1.5}/\\eps^{1.5}\n\\indep)$ and $\\tilde{O}((n^{2}r^{-1}\\epsilon^{-2}+r^{1.5}\\epsilon^{-4.5})\n\\indep)$, respectively. These results improve upon the $O(nr/\\eps \\indep)$-time\nalgorithm of Cunningham as noted recently by Chekuri and Quanrud.\n  Given a rank oracle, we provide algorithms with even better dependence on $n$\nand $r$. We provide an $O(n\\sqrt{r}\\log n \\rank)$-time exact algorithm and an\n$O(n\\epsilon^{-1}\\log n \\rank)$-time algorithm which obtains a\n$(1-\\eps)$-approximation to the matroid intersection problem. The former result\nimproves over the $\\tilde{O}(nr \\rankt+n^{3})$-time algorithm by Lee, Sidford,\nand Wong. The rank oracle is of particular interest as the matroid intersection\nproblem with this oracle is a special case of the submodular function\nminimization problem with an evaluation oracle.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 08:37:19 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Lee", "Yin Tat", ""], ["Sidford", "Aaron", ""], ["Singla", "Sahil", ""], ["Wong", "Sam Chiu-wai", ""]]}, {"id": "1911.10833", "submitter": "Xiaojin Zhang", "authors": "Xiaojin Zhang", "title": "Near-Optimal Algorithm for Distribution-Free Junta Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an adaptive algorithm with one-sided error for the problem of\njunta testing for Boolean function under the challenging distribution-free\nsetting, the query complexity of which is $\\tilde O(k)/\\epsilon$. This improves\nthe upper bound of $\\tilde O(k^2)/\\epsilon$ by \\cite{liu2019distribution}. From\nthe $\\Omega(k\\log k)$ lower bound for junta testing under the uniform\ndistribution by \\cite{sauglam2018near}, our algorithm is nearly optimal. In the\nstandard uniform distribution, the optimal junta testing algorithm is mainly\ndesigned by bridging between relevant variables and relevant blocks. At the\nheart of the analysis is the Efron-Stein orthogonal decomposition. However, it\nis not clear how to generalize this tool to the general setting. Surprisingly,\nwe find that junta could be tested in a very simple and efficient way even in\nthe distribution-free setting. It is interesting that the analysis does not\nrely on Fourier tools directly which are commonly used in junta testing.\nFurther, we present a simpler algorithm with the same query complexity.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 11:19:01 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 14:48:33 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 01:24:14 GMT"}, {"version": "v4", "created": "Tue, 24 Nov 2020 13:38:52 GMT"}, {"version": "v5", "created": "Tue, 13 Jul 2021 12:05:48 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zhang", "Xiaojin", ""]]}, {"id": "1911.10863", "submitter": "Michael Bekos", "authors": "Michael A. Bekos, Carla Binucci, Michael Kaufmann, Chrysanthi\n  Raftopoulou, Antonios Symvonis, Alessandra Tappini", "title": "Coloring outerplanar graphs and planar 3-trees with small monochromatic\n  components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we continue the study of vertex colorings of graphs, in which\nadjacent vertices are allowed to be of the same color as long as each\nmonochromatic connected component is of relatively small cardinality. We focus\non colorings with two and three available colors and present improved bounds on\nthe size of the monochromatic connected components for two meaningful\nsubclasses of planar graphs, namely maximal outerplanar graphs and complete\nplanar 3-trees.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 12:26:18 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Bekos", "Michael A.", ""], ["Binucci", "Carla", ""], ["Kaufmann", "Michael", ""], ["Raftopoulou", "Chrysanthi", ""], ["Symvonis", "Antonios", ""], ["Tappini", "Alessandra", ""]]}, {"id": "1911.10871", "submitter": "Tobias M\\\"omke", "authors": "Tobias M\\\"omke and Andreas Wiese", "title": "Breaking the Barrier of 2 for the Storage Allocation Problem", "comments": "54 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Packing problems are an important class of optimization problems. The\nprobably most well-known problem if this type is knapsack and many\ngeneralizations of it have been studied in the literature like Two-dimensional\nGeometric Knapsack (2DKP) and Unsplittable Flow on a Path (UFP). For the latter\ntwo problems, recently the first polynomial time approximation algorithms with\nbetter approximation ratios than 2 were presented [G\\'alvez et al., FOCS\n2017][Grandoni et al., STOC 2018].\n  In this paper we break the barrier of 2 for the Storage Allocation Problem\n(SAP) which is a natural intermediate problem between 2DKP and UFP. We are\ngiven a path with capacitated edges and a set of tasks where each task has a\nstart vertex, an end vertex, a size, and a profit. We seek to select the most\nprofitable set of tasks that we can draw as non-overlapping rectangles\nunderneath the capacity profile of the edges where the height of each rectangle\nequals the size of the corresponding task.\n  This problem is motivated by settings of allocation resources like memory,\nbandwidths, etc. where each request needs a contiguous portion of the resource.\nThe best known polynomial time approximation algorithm for SAP has an\napproximation ratio of 2+epsilon$ [M\\\"omke and Wiese, ICALP 2015] and no better\nquasi-polynomial time algorithm is known. We present a polynomial time (63/32)\n< 1.969-approximation algorithm for the case of uniform edge capacities and a\nquasi-polynomial time (1.997)-approximation algorithm for non-uniform\nquasi-polynomially bounded edge capacities. Finally, we show that under slight\nresource augmentation we can obtain approximation ratios of 3/2 + epsilon in\npolynomial time and 1 + epsilon in quasi-polynomial time, both for arbitrary\nedge capacities.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 12:39:15 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["M\u00f6mke", "Tobias", ""], ["Wiese", "Andreas", ""]]}, {"id": "1911.11048", "submitter": "Mathias Weller", "authors": "Mathias Weller", "title": "Listing Conflicting Triples in Optimal Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different sources of information might tell different stories about the\nevolutionary history of a given set of species. This leads to (rooted)\nphylogenetic trees that \"disagree\" on triples of species, which we call\n\"conflict triples\". An important subtask of computing consensus trees which is\ninteresting in its own regard is the enumeration of all conflicts exhibited by\na pair of phylogenetic trees (on the same set of $n$ taxa). As it is possible\nthat a significant part of the $n^3$ triples are in conflict, the trivial\n${\\Theta}(n^3)$-time algorithm that checks for each triple whether it\nconstitutes a conflict, was considered optimal. It turns out, however, that we\ncan do way better in the case that there are only few conflicts. In particular,\nwe show that we can enumerate all d conflict triples between a pair of\nphylogenetic trees in $O(n + d)$ time. Since any deterministic algorithm has to\nspend ${\\Theta}(n)$ time reading the input and ${\\Theta}(d)$ time writing the\noutput, no deterministic algorithm can solve this task faster than we do (up to\nconstant factors).\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 17:00:13 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Weller", "Mathias", ""]]}, {"id": "1911.11190", "submitter": "Max Alekseyev", "authors": "Sergey Aganezov, Pavel Avdeyev, Nikita Alexeev, Yongwu Rong, Max A.\n  Alekseyev", "title": "Orienting Ordered Scaffolds: Complexity and Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent progress in genome sequencing and assembly, many of the\ncurrently available assembled genomes come in a draft form. Such draft genomes\nconsist of a large number of genomic fragments (scaffolds), whose order and/or\norientation (i.e., strand) in the genome are unknown. There exist various\nscaffold assembly methods, which attempt to determine the order and orientation\nof scaffolds along the genome chromosomes. Some of these methods (e.g., based\non FISH physical mapping, chromatin conformation capture, etc.) can infer the\norder of scaffolds, but not necessarily their orientation. This leads to a\nspecial case of the scaffold orientation problem (i.e., deducing the\norientation of each scaffold) with a known order of the scaffolds.\n  We address the problem of orientating ordered scaffolds as an optimization\nproblem based on given weighted orientations of scaffolds and their pairs\n(e.g., coming from pair-end sequencing reads, long reads, or homologous\nrelations). We formalize this problem using notion of a scaffold graph (i.e., a\ngraph, where vertices correspond to the assembled contigs or scaffolds and\nedges represent connections between them). We prove that this problem is\nNP-hard, and present a polynomial-time algorithm for solving its special case,\nwhere orientation of each scaffold is imposed relatively to at most two other\nscaffolds. We further develop an FPT algorithm for the general case of the OOS\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 19:45:23 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Aganezov", "Sergey", ""], ["Avdeyev", "Pavel", ""], ["Alexeev", "Nikita", ""], ["Rong", "Yongwu", ""], ["Alekseyev", "Max A.", ""]]}, {"id": "1911.11229", "submitter": "Da Qi Chen", "authors": "Hassene Aissi, Da Qi Chen, R. Ravi", "title": "Downgrading to Minimize Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of interdicting a directed graph by deleting nodes with\nthe goal of minimizing the local edge connectivity of the remaining graph from\na given source to a sink. We show hardness of obtaining strictly unicriterion\napproximations for this basic vertex interdiction problem. We also introduce\nand study a general downgrading variant of the interdiction problem where the\ncapacity of an arc is a function of the subset of its endpoints that are\ndowngraded, and the goal is to minimize the downgraded capacity of a minimum\nsource-sink cut subject to a node downgrading budget. This models the case when\nboth ends of an arc must be downgraded to remove it, for example.\n  For this generalization, we provide a bicriteria $(4,4)$-approximation that\ndowngrades nodes with total weight at most 4 times the budget and provides a\nsolution where the downgraded connectivity from the source to the sink is at\nmost 4 times that in an optimal solution. WE accomplish this with an LP\nrelaxation and round using a ball-growing algorithm based on the LP values. We\nfurther generalize the downgrading problem to one where each vertex can be\ndowngraded to one of $k$ levels, and the arc capacities are functions of the\npairs of levels to which its ends are downgraded. We generalize our LP rounding\nto get $(4k,4k)$-approximation for this case.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 21:02:22 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Aissi", "Hassene", ""], ["Chen", "Da Qi", ""], ["Ravi", "R.", ""]]}, {"id": "1911.11257", "submitter": "Daniel Wiebking", "authors": "Daniel Wiebking", "title": "Graph isomorphism in quasipolynomial time parameterized by treewidth", "comments": "52 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend Babai's quasipolynomial-time graph isomorphism test (STOC 2016) and\ndevelop a quasipolynomial-time algorithm for the multiple-coset isomorphism\nproblem. The algorithm for the multiple-coset isomorphism problem allows to\nexploit graph decompositions of the given input graphs within Babai's\ngroup-theoretic framework.\n  We use it to develop a graph isomorphism test that runs in time\n$n^{\\operatorname{polylog}(k)}$ where $n$ is the number of vertices and $k$ is\nthe minimum treewidth of the given graphs and $\\operatorname{polylog}(k)$ is\nsome polynomial in $\\operatorname{log}(k)$. Our result generalizes Babai's\nquasipolynomial-time graph isomorphism test.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 22:15:37 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 13:57:44 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Wiebking", "Daniel", ""]]}, {"id": "1911.11354", "submitter": "Mingtao Lei", "authors": "Mingtao Lei and Xi Zhang and Lingyang Chu and Zhefeng Wang and Philip\n  S. Yu and Binxing Fang", "title": "Finding Route Hotspots in Large Labeled Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many advanced network analysis applications, like social networks,\ne-commerce, and network security, hotspots are generally considered as a group\nof vertices that are tightly connected owing to the similar characteristics,\nsuch as common habits and location proximity. In this paper, we investigate the\nformation of hotspots from an alternative perspective that considers the routes\nalong the network paths as the auxiliary information, and attempt to find the\nroute hotspots in large labeled networks. A route hotspot is a cohesive\nsubgraph that is covered by a set of routes, and these routes correspond to the\nsame sequential pattern consisting of vertices' labels. To the best of our\nknowledge, the problem of Finding Route Hotspots in Large Labeled Networks has\nnot been tackled in the literature. However, it is challenging as counting the\nnumber of hotspots in a network is #P-hard. Inspired by the observation that\nthe sizes of hotspots decrease with the increasing lengths of patterns, we\nprove several anti-monotonicity properties of hotspots, and then develop a\nscalable algorithm called FastRH that can use these properties to effectively\nprune the patterns that cannot form any hotspots. In addition, to avoid the\nduplicate computation overhead, we judiciously design an effective index\nstructure called RH-Index for storing the hotspot and pattern information\ncollectively, which also enables incremental updating and efficient query\nprocessing. Our experimental results on real-world datasets clearly demonstrate\nthe effectiveness and scalability of our proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 05:54:43 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Lei", "Mingtao", ""], ["Zhang", "Xi", ""], ["Chu", "Lingyang", ""], ["Wang", "Zhefeng", ""], ["Yu", "Philip S.", ""], ["Fang", "Binxing", ""]]}, {"id": "1911.11368", "submitter": "Sidhanth Mohanty", "authors": "Shafi Goldwasser, Ofer Grossman, Sidhanth Mohanty, David P. Woodruff", "title": "Pseudo-deterministic Streaming", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pseudo-deterministic algorithm is a (randomized) algorithm which, when run\nmultiple times on the same input, with high probability outputs the same result\non all executions. Classic streaming algorithms, such as those for finding\nheavy hitters, approximate counting, $\\ell_2$ approximation, finding a nonzero\nentry in a vector (for turnstile algorithms) are not pseudo-deterministic. For\nexample, in the instance of finding a nonzero entry in a vector, for any known\nlow-space algorithm $A$, there exists a stream $x$ so that running $A$ twice on\n$x$ (using different randomness) would with high probability result in two\ndifferent entries as the output.\n  In this work, we study whether it is inherent that these algorithms output\ndifferent values on different executions. That is, we ask whether these\nproblems have low-memory pseudo-deterministic algorithms. For instance, we show\nthat there is no low-memory pseudo-deterministic algorithm for finding a\nnonzero entry in a vector (given in a turnstile fashion), and also that there\nis no low-dimensional pseudo-deterministic sketching algorithm for $\\ell_2$\nnorm estimation. We also exhibit problems which do have low memory\npseudo-deterministic algorithms but no low memory deterministic algorithm, such\nas outputting a nonzero row of a matrix, or outputting a basis for the row-span\nof a matrix.\n  We also investigate multi-pseudo-deterministic algorithms: algorithms which\nwith high probability output one of a few options. We show the first lower\nbounds for such algorithms. This implies that there are streaming problems such\nthat every low space algorithm for the problem must have inputs where there are\nmany valid outputs, all with a significant probability of being outputted.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 06:50:28 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Goldwasser", "Shafi", ""], ["Grossman", "Ofer", ""], ["Mohanty", "Sidhanth", ""], ["Woodruff", "David P.", ""]]}, {"id": "1911.11637", "submitter": "Vladan Majerech Dr.", "authors": "Vladan Majerech", "title": "Fast Fibonacci heaps with worst case extensions", "comments": "15 pages at all, 4+2/2 pages of tables, 1 figure. arXiv admin note:\n  text overlap with arXiv:1911.04372", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concentrating on reducing overhead of heaps based on comparisons with\noptimal worstcase behaviour. The paper is inspired by Strict Fibonacci Heaps\n[1], where G. S. Brodal, G. Lagogiannis, and R. E. Tarjan implemented the heap\nwith DecreaseKey and Meld interface in assymptotically optimal worst case times\n(based on key comparisons). In the paper [2], the ideas were elaborated and it\nwas shown that the same asymptotical times could be achieved with a strategy\nloosing much less information from previous comparisons. There is big overhead\nwith maintainance of violation lists in these heaps. We propose simple\nalternative reducing this overhead. It allows us to implement fast amortized\nFibonacci heaps, where user could call some methods in variants guaranting\nworst case time. If he does so, the heaps are not guaranted to be Fibonacci\nuntil an amortized version of a method is called. Of course we could call worst\ncase versions all the time, but as there is an overhead with the guarantee,\ncalling amortized versions is prefered choice if we are not concentrated on\ncomplexity of the separate operation.\n  We have shown, we could implement full DecreaseKey-Meld interface, but Meld\ninterface is not natural for these heaps, so if Meld is not needed, much\nsimpler implementation suffices. As I don't know application requiring Meld, we\nwould concentrate on noMeld variant, but we will show the changes could be\napplied on Meld including variant as well. The papers [1], [2] shown the heaps\ncould be implemented on pointer machine model. For fast practical\nimplementations we would rather use arrays. Our goal is to reduce number of\npointer manipulations. Maintainance of ranks by pointers to rank lists would be\nunnecessary overhead.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 08:42:50 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Majerech", "Vladan", ""]]}, {"id": "1911.11838", "submitter": "He Jia", "authors": "He Jia, Santosh Vempala", "title": "Robustly Clustering a Mixture of Gaussians", "comments": "Some of our proofs were not SoS proofs. Turning them into SoS proofs\n  requires substantial changes and leads to very similar arguments (in fact\n  special cases for k=2) as those given for k-GMMs by Bakshi-Kothari\n  [arXiv2020] and Diakonikolas-Hopkins-Kane-Karmalkar [arXiv2020]. Hence we\n  withdraw the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an efficient algorithm for robustly clustering of a mixture of two\narbitrary Gaussians, a central open problem in the theory of computationally\nefficient robust estimation, assuming only that the the means of the component\nGaussians are well-separated or their covariances are well-separated. Our\nalgorithm and analysis extend naturally to robustly clustering mixtures of\nwell-separated strongly logconcave distributions. The mean separation required\nis close to the smallest possible to guarantee that most of the measure of each\ncomponent can be separated by some hyperplane (for covariances, it is the same\ncondition in the second degree polynomial kernel). We also show that for\nGaussian mixtures, separation in total variation distance suffices to achieve\nrobust clustering. Our main tools are a new identifiability criterion based on\nisotropic position and the Fisher discriminant, and a corresponding\nSum-of-Squares convex programming relaxation, of fixed degree.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 21:16:17 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 16:15:16 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2020 16:16:11 GMT"}, {"version": "v4", "created": "Fri, 1 May 2020 19:04:57 GMT"}, {"version": "v5", "created": "Thu, 7 May 2020 17:56:26 GMT"}, {"version": "v6", "created": "Sun, 31 May 2020 16:06:58 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Jia", "He", ""], ["Vempala", "Santosh", ""]]}, {"id": "1911.11847", "submitter": "Hassan Aissi", "authors": "Hassene Aissi, S. Thomas McCormick, Maurice Queyranne", "title": "Faster Algorithms for Parametric Global Minimum Cut Problems", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parametric global minimum cut problem concerns a graph $G = (V,E)$ where\nthe cost of each edge is an affine function of a parameter $\\mu \\in\n\\mathbb{R}^d$ for some fixed dimension $d$. We consider the problems of finding\nthe next breakpoint in a given direction, and finding a parameter value with\nmaximum minimum cut value. We develop strongly polynomial algorithms for these\nproblems that are faster than a naive application of Megiddo's parametric\nsearch technique. Our results indicate that the next breakpoint problem is\neasier than the max value problem.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 21:46:41 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Aissi", "Hassene", ""], ["McCormick", "S. Thomas", ""], ["Queyranne", "Maurice", ""]]}, {"id": "1911.11868", "submitter": "Greg Bodwin", "authors": "Greg Bodwin and Santosh Vempala", "title": "A Unified View of Graph Regularity via Matrix Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove algorithmic weak and \\Szemeredi{} regularity lemmas for several\nclasses of sparse graphs in the literature, for which only weak regularity\nlemmas were previously known. These include core-dense graphs, low threshold\nrank graphs, and (a version of) $L^p$ upper regular graphs. More precisely, we\ndefine \\emph{cut pseudorandom graphs}, we prove our regularity lemmas for these\ngraphs, and then we show that cut pseudorandomness captures all of the above\ngraph classes as special cases.\n  The core of our approach is an abstracted matrix decomposition, roughly\nfollowing Frieze and Kannan [Combinatorica '99] and \\Lovasz{} and Szegedy\n[Geom.\\ Func.\\ Anal.\\ '07], which can be computed by a simple algorithm by\nCharikar [AAC0 '00]. This gives rise to the class of cut pseudorandom graphs,\nand using work of Oveis Gharan and Trevisan [TOC '15], it also implies new\nPTASes for MAX-CUT, MAX-BISECTION, MIN-BISECTION for a significantly expanded\nclass of input graphs. (It is NP Hard to get PTASes for these graphs in\ngeneral.)\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 22:31:42 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 23:33:47 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 22:36:56 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2021 01:21:17 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Bodwin", "Greg", ""], ["Vempala", "Santosh", ""]]}, {"id": "1911.11962", "submitter": "Zhihan Jin", "authors": "Zhengfeng Ji, Zhihan Jin, Pinyan Lu", "title": "Approximating Permanent of Random Matrices with Vanishing Mean: Made\n  Better and Simpler", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The algorithm and complexity of approximating the permanent of a matrix is an\nextensively studied topic. Recently, its connection with quantum supremacy and\nmore specifically BosonSampling draws special attention to the average-case\napproximation problem of the permanent of random matrices with zero or small\nmean value for each entry. Eldar and Mehraban (FOCS 2018) gave a\nquasi-polynomial time algorithm for random matrices with mean at least\n$1/\\mathbf{\\mathrm{polyloglog}} (n)$. In this paper, we improve the result by\ndesigning a deterministic quasi-polynomial time algorithm and a PTAS for random\nmatrices with mean at least $1/\\mathbf{\\mathrm{polylog}}(n)$. We note that if\nit can be further improved to $1/\\mathbf{\\mathrm{poly}}(n)$, it will disprove a\ncentral conjecture for quantum supremacy.\n  Our algorithm is also much simpler and has a better and flexible trade-off\nfor running time. The running time can be quasi-polynomial in both $n$ and\n$1/\\epsilon$, or PTAS (polynomial in $n$ but exponential in $1/\\epsilon$),\nwhere $\\epsilon$ is the approximation parameter.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 05:36:03 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ji", "Zhengfeng", ""], ["Jin", "Zhihan", ""], ["Lu", "Pinyan", ""]]}, {"id": "1911.12003", "submitter": "Shu-Chuan Chen", "authors": "Justie Su-Tzu Juan, Yi-Ching Chen, Chen-Hui Lin, Shu-Chuan (Grace)\n  Chen", "title": "Measuring similarity between two mixture trees using mixture distance\n  metric and algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ancestral mixture model, proposed by Chen and Lindsay (2006), is an important\nmodel to build a hierarchical tree from high dimensional binary sequences.\nMixture trees created from ancestral mixture models involve in the inferred\nevolutionary relationships among various biological species. Moreover, it\ncontains the information of time when the species mutates. Tree comparison\nmetric, an essential issue in bioinformatics, is to measure the similarity\nbetween trees. However, to our knowledge, the approach to the comparison\nbetween two mixture trees is still under development. In this paper, we propose\na new metric, named mixture distance metric, to measure the similarity of two\nmixture trees. It uniquely considers the factor of evolutionary times between\ntrees. In addition, we also further develop two algorithms to compute the\nmixture distance between two mixture trees. One requires O(n^2) and the other\nrequires O(nh) computation time with O(n) preprocessing time, where n denotes\nthe number of leaves in the two mixture trees, and h denotes the minimum height\nof these two trees.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 07:58:01 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Juan", "Justie Su-Tzu", "", "Grace"], ["Chen", "Yi-Ching", "", "Grace"], ["Lin", "Chen-Hui", "", "Grace"], ["Shu-Chuan", "", "", "Grace"], ["Chen", "", ""]]}, {"id": "1911.12125", "submitter": "Zhenyu Weng", "authors": "Zhenyu Weng, Yuesheng Zhu", "title": "Online Hashing with Efficient Updating of Binary Codes", "comments": "9 pages, accepted by AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online hashing methods are efficient in learning the hash functions from the\nstreaming data. However, when the hash functions change, the binary codes for\nthe database have to be recomputed to guarantee the retrieval accuracy.\nRecomputing the binary codes by accumulating the whole database brings a\ntimeliness challenge to the online retrieval process. In this paper, we propose\na novel online hashing framework to update the binary codes efficiently without\naccumulating the whole database. In our framework, the hash functions are fixed\nand the projection functions are introduced to learn online from the streaming\ndata. Therefore, inefficient updating of the binary codes by accumulating the\nwhole database can be transformed to efficient updating of the binary codes by\nprojecting the binary codes into another binary space. The queries and the\nbinary code database are projected asymmetrically to further improve the\nretrieval accuracy. The experiments on two multi-label image databases\ndemonstrate the effectiveness and the efficiency of our method for multi-label\nimage retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 03:03:48 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 03:49:31 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Weng", "Zhenyu", ""], ["Zhu", "Yuesheng", ""]]}, {"id": "1911.12138", "submitter": "Krist\\'of B\\'erczi", "authors": "Krist\\'of B\\'erczi and Tam\\'as Kir\\'aly and Simon Omlor", "title": "Scheduling with Non-Renewable Resources: Minimizing the Sum of\n  Completion Times", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers single-machine scheduling problems with a non-renewable\nresource. In this setting, we are given a set jobs, each of which is\ncharacterized by a processing time, a weight, and the job also has some\nresource requirement. At fixed points in time, a certain amount of the resource\nis made available to be consumed by the jobs. The goal is to assign the jobs\nnon-preemptively to time slots on the machine, so that at any time their\nresource requirement does not exceed the available amounts of resources. The\nobjective that we consider here is the minimization of the sum of weighted\ncompletion times.\n  We give polynomial approximation algorithms and complexity results for single\nscheduling machine problems. In particular, we show strong NP-hardness of the\ncase of unit resource requirements and weights ($1|rm=1,a_j=1|\\sum C_j$), thus\nanswering an open question of Gy\\\"orgyi and Kis. We also prove that the\nschedule corresponding to the Shortest Processing Time First ordering provides\na $3/2$-approximation for the same problem. We give simple constant factor\napproximations and a more complicated PTAS for the case of $0$ processing times\n($1|rm=1,p_j=0|\\sum w_jC_j$). We close the paper by proposing a new variant of\nthe problem in which the resource arrival times are unknown. A\n$4$-approximation is presented for this variant, together with an\n$(4-\\varepsilon)$-inapproximability result.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 05:03:39 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["B\u00e9rczi", "Krist\u00f3f", ""], ["Kir\u00e1ly", "Tam\u00e1s", ""], ["Omlor", "Simon", ""]]}, {"id": "1911.12350", "submitter": "Matthias Mnich", "authors": "Danny Hermelin and Matthias Mnich and Simon Omlor", "title": "Single Machine Batch Scheduling to Minimize the Weighted Number of Tardy\n  Jobs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $1|B,r_j|\\sum w_jU_j$ scheduling problem takes as input a batch setup\ntime $\\Delta$ and a set of $n$ jobs, each having a processing time, a release\ndate, a weight, and a due date; the task is to find a sequence of batches that\nminimizes the weighted number of tardy jobs. This problem was introduced by\nHochbaum and Landy in 1994; as a wide generalization of {\\sc Knapsack}, it is\n$\\mathsf{NP}$-hard.\n  In this work we provide a multivariate complexity analysis of the\n$1|B,r_j|\\sum w_jU_j$ problem with respect to several natural parameters. That\nis, we establish a thorough classification into fixed-parameter tractable and\n$\\mathsf{W}[1]$-hard problems, for parameter combinations of (i) $\\#p$ =\ndistinct number of processing times, (ii) $\\#w$ = number of distinct weights,\n(iii) $\\#d$ = number of distinct due dates, (iv) $\\#r$ = number of distinct\nrelease dates, and (v) $b$ = batch sizes. Thereby, we significantly extend the\nwork of Hermelin et al. (2018) who analyzed the parameterized complexity of the\nnon-batch variant of this problem without release dates.\n  As one of our key results, we prove that $1|B,r_j|\\sum w_jU_j$ is\n$\\mathsf{W}[1]$-hard parameterized by the number of distinct processing times\nand distinct due dates. To the best of our knowledge, these are the first\nparameterized intractability results for scheduling problems with few distinct\nprocessing times. Further, we show that $1|B,r_j|\\sum w_jU_j$ is\nfixed-parameter tractable with respect to parameter $\\#p+\\#d+\\#r$ and with\nrespect to parameter $\\#w+\\#d$ if there is just a single release date. Both\nresults hold even if the number of jobs per batch is limited by some integer\n$b$.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 18:51:24 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Hermelin", "Danny", ""], ["Mnich", "Matthias", ""], ["Omlor", "Simon", ""]]}, {"id": "1911.12411", "submitter": "Ran Duan", "authors": "Ruoxu Cen, Ran Duan, Yong Gu", "title": "Roundtrip Spanners with $(2k-1)$ Stretch", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A roundtrip spanner of a directed graph $G$ is a subgraph of $G$ preserving\nroundtrip distances approximately for all pairs of vertices. Despite extensive\nresearch, there is still a small stretch gap between roundtrip spanners in\ndirected graphs and undirected graphs. For a directed graph with real edge\nweights in $[1,W]$, we first propose a new deterministic algorithm that\nconstructs a roundtrip spanner with $(2k-1)$ stretch and $O(k n^{1+1/k}\\log\n(nW))$ edges for every integer $k> 1$, then remove the dependence of size on\n$W$ to give a roundtrip spanner with $(2k-1)$ stretch and $O(k n^{1+1/k}\\log\nn)$ edges. While keeping the edge size small, our result improves the previous\n$2k+\\epsilon$ stretch roundtrip spanners in directed graphs [Roditty, Thorup,\nZwick'02; Zhu, Lam'18], and almost matches the undirected $(2k-1)$-spanner with\n$O(n^{1+1/k})$ edges [Alth\\\"ofer et al. '93] when $k$ is a constant, which is\noptimal under Erd\\\"os conjecture.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 20:41:26 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 08:15:10 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Cen", "Ruoxu", ""], ["Duan", "Ran", ""], ["Gu", "Yong", ""]]}, {"id": "1911.12427", "submitter": "Luc Libralesso", "authors": "Luc Libralesso (G-SCOP_ROSP), Abdel-Malik Bouhassoun (G-SCOP_ROSP),\n  Hadrien Cambazard (G-SCOP_ROSP), Vincent Jost (G-SCOP_ROSP)", "title": "Tree search algorithms for the Sequential Ordering Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a study of several generic tree search techniques applied to the\nSequential Ordering Problem. This study enables us to propose a simple and\ncompetitive tree search algorithm. It consists of an iterative Beam Search\nalgorithm that favors search over inference and integrates dynamic programming\ninspired cuts. It proves optimality on half of the SOPLIB instances and finds\nnew best known solutions on 6 among 7 open instances of the benchmark in a\nsmall amount of time.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 14:22:10 GMT"}], "update_date": "2020-01-26", "authors_parsed": [["Libralesso", "Luc", "", "G-SCOP_ROSP"], ["Bouhassoun", "Abdel-Malik", "", "G-SCOP_ROSP"], ["Cambazard", "Hadrien", "", "G-SCOP_ROSP"], ["Jost", "Vincent", "", "G-SCOP_ROSP"]]}, {"id": "1911.12737", "submitter": "Romain Edelmann", "authors": "Romain Edelmann, Jad Hamza, Viktor Kun\\v{c}ak", "title": "LL(1) Parsing with Derivatives and Zippers", "comments": "Appeared at PLDI'20 under the title \"Zippy LL(1) Parsing with\n  Derivatives\"", "journal-ref": null, "doi": "10.1145/3385412.3385992", "report-no": null, "categories": "cs.FL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an efficient, functional, and formally verified\nparsing algorithm for LL(1) context-free expressions based on the concept of\nderivatives of formal languages. Parsing with derivatives is an elegant parsing\ntechnique, which, in the general case, suffers from cubic worst-case time\ncomplexity and slow performance in practice. We specialise the parsing with\nderivatives algorithm to LL(1) context-free expressions, where alternatives can\nbe chosen given a single token of lookahead. We formalise the notion of LL(1)\nexpressions and show how to efficiently check the LL(1) property. Next, we\npresent a novel linear-time parsing with derivatives algorithm for LL(1)\nexpressions operating on a zipper-inspired data structure. We prove the\nalgorithm correct in Coq and present an implementation as a parser combinators\nframework in Scala, with enumeration and pretty printing capabilities.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 14:57:03 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 12:54:49 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Edelmann", "Romain", ""], ["Hamza", "Jad", ""], ["Kun\u010dak", "Viktor", ""]]}, {"id": "1911.12778", "submitter": "Sai Sandeep", "authors": "Varun Gupta, Ravishankar Krishnaswamy, Sai Sandeep", "title": "PERMUTATION Strikes Back: The Power of Recourse in Online Metric\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical Online Metric Matching problem, we are given a metric space\nwith $k$ servers. A collection of clients arrive in an online fashion, and upon\narrival, a client should irrevocably be matched to an as-yet-unmatched server.\nThe goal is to find an online matching which minimizes the total cost, i.e.,\nthe sum of distances between each client and the server it is matched to. We\nknow deterministic algorithms~\\cite{KP93,khuller1994line} that achieve a\ncompetitive ratio of $2k-1$, and this bound is tight for deterministic\nalgorithms. The problem has also long been considered in specialized metrics\nsuch as the line metric or metrics of bounded doubling dimension, with the\ncurrent best result on a line metric being a deterministic $O(\\log k)$\ncompetitive algorithm~\\cite{raghvendra2018optimal}. Obtaining (or refuting)\n$O(\\log k)$-competitive algorithms in general metrics and constant-competitive\nalgorithms on the line metric have been long-standing open questions in this\narea.\n  In this paper, we investigate the robustness of these lower bounds by\nconsidering the Online Metric Matching with Recourse problem where we are\nallowed to change a small number of previous assignments upon arrival of a new\nclient. Indeed, we show that a small logarithmic amount of recourse can\nsignificantly improve the quality of matchings we can maintain. For general\nmetrics, we show a simple \\emph{deterministic} $O(\\log k)$-competitive\nalgorithm with $O(\\log k)$-amortized recourse, an exponential improvement over\nthe $2k-1$ lower bound when no recourse is allowed. We next consider the line\nmetric, and present a deterministic algorithm which is $3$-competitive and has\n$O(\\log k)$-recourse, again a substantial improvement over the best known\n$O(\\log k)$-competitive algorithm when no recourse is allowed.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 16:26:39 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Gupta", "Varun", ""], ["Krishnaswamy", "Ravishankar", ""], ["Sandeep", "Sai", ""]]}, {"id": "1911.12959", "submitter": "Alina Ene", "authors": "Naor Alaluf, Alina Ene, Moran Feldman, Huy L. Nguyen, Andrew Suh", "title": "Optimal Streaming Algorithms for Submodular Maximization with\n  Cardinality Constraints", "comments": "This paper is a merger of arXiv:1906.11237 and arXiv:1911.12959", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of maximizing a non-monotone submodular function subject\nto a cardinality constraint in the streaming model. Our main contribution is a\nsingle-pass (semi-)streaming algorithm that uses roughly $O(k / \\varepsilon^2)$\nmemory, where $k$ is the size constraint. At the end of the stream, our\nalgorithm post-processes its data structure using any offline algorithm for\nsubmodular maximization, and obtains a solution whose approximation guarantee\nis $\\frac{\\alpha}{1+\\alpha}-\\varepsilon$, where $\\alpha$ is the approximation\nof the offline algorithm. If we use an exact (exponential time) post-processing\nalgorithm, this leads to $\\frac{1}{2}-\\varepsilon$ approximation (which is\nnearly optimal). If we post-process with the algorithm of Buchbinder and\nFeldman (Math of OR 2019), that achieves the state-of-the-art offline\napproximation guarantee of $\\alpha=0.385$, we obtain $0.2779$-approximation in\npolynomial time, improving over the previously best polynomial-time\napproximation of $0.1715$ due to Feldman et al. (NeurIPS 2018). It is also\nworth mentioning that our algorithm is combinatorial and deterministic, which\nis rare for an algorithm for non-monotone submodular maximization, and enjoys a\nfast update time of $O(\\frac{\\log k + \\log (1/\\alpha)}{\\varepsilon^2})$ per\nelement.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 05:59:58 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 18:25:55 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2020 11:56:50 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Alaluf", "Naor", ""], ["Ene", "Alina", ""], ["Feldman", "Moran", ""], ["Nguyen", "Huy L.", ""], ["Suh", "Andrew", ""]]}, {"id": "1911.12995", "submitter": "Robert Ganian", "authors": "Robert Ganian and Neha Lodha and Sebastian Ordyniak and Stefan Szeider", "title": "SAT-Encodings for Treecut Width and Treedepth", "comments": "Presented at ALENEX 2019; this version corrects a minor issue in one\n  of the tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose, implement, and test the first practical\ndecomposition algorithms for the width parameters treecut width and treedepth.\nThese two parameters have recently gained a lot of attention in the theoretical\nresearch community as they offer the algorithmic advantage over treewidth by\nsupporting so-called fixed-parameter algorithms for certain problems that are\nnot fixed-parameter tractable with respect to treewidth. However, the existing\nresearch has mostly been theoretical. A main obstacle for any practical or\nexperimental use of these two width parameters is the lack of any practical or\nimplemented algorithm for actually computing the associated decompositions. We\naddress this obstacle by providing the first practical decomposition\nalgorithms.\n  Our approach for computing treecut width and treedepth decompositions is\nbased on efficient encodings of these decomposition methods to the\npropositional satisfiability problem (SAT). Once an encoding is generated, any\nsatisfiability solver can be used to find the decomposition. Moreover, we\npropose new characterisations for treecut width and treedepth that are based on\nsequences of partitions of the vertex set, a method that was pioneered for\nclique-width. We implemented and systematically tested our encodings on various\nbenchmark instances, including famous named graphs and random graphs of various\ndensity. It turned out that for the considered width parameters, our\npartition-based SAT encoding even outperforms the best existing SAT encoding\nfor treewidth.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 08:22:40 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ganian", "Robert", ""], ["Lodha", "Neha", ""], ["Ordyniak", "Sebastian", ""], ["Szeider", "Stefan", ""]]}, {"id": "1911.13014", "submitter": "Andreas Kipf", "authors": "Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons\n  Kemper, Tim Kraska, Thomas Neumann", "title": "SOSD: A Benchmark for Learned Indexes", "comments": "NeurIPS 2019 Workshop on Machine Learning for Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A groundswell of recent work has focused on improving data management systems\nwith learned components. Specifically, work on learned index structures has\nproposed replacing traditional index structures, such as B-trees, with learned\nmodels. Given the decades of research committed to improving index structures,\nthere is significant skepticism about whether learned indexes actually\noutperform state-of-the-art implementations of traditional structures on\nreal-world data. To answer this question, we propose a new benchmarking\nframework that comes with a variety of real-world datasets and baseline\nimplementations to compare against. We also show preliminary results for\nselected index structures, and find that learned models indeed often outperform\nstate-of-the-art implementations, and are therefore a promising direction for\nfuture research.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 09:35:04 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Kipf", "Andreas", ""], ["Marcus", "Ryan", ""], ["van Renen", "Alexander", ""], ["Stoian", "Mihail", ""], ["Kemper", "Alfons", ""], ["Kraska", "Tim", ""], ["Neumann", "Thomas", ""]]}, {"id": "1911.13085", "submitter": "Alexander Eckl", "authors": "Alexander Eckl and Luisa Peter and Maximilian Schiffer and Susanne\n  Albers", "title": "Minimization of Weighted Completion Times in Path-based Coflow\n  Scheduling", "comments": "25 pages, 2 pages references, 4 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coflow scheduling models communication requests in parallel computing\nframeworks where multiple data flows between shared resources need to be\ncompleted before computation can continue. In this paper, we introduce\nPath-based Coflow Scheduling, a generalized problem variant that considers\ncoflows as collections of flows along fixed paths on general network topologies\nwith node capacity restrictions. For this problem, we minimize the coflows'\ntotal weighted completion time. We show that flows on paths in the original\nnetwork can be interpreted as hyperedges in a hypergraph and transform the\npath-based scheduling problem into an edge scheduling problem on this\nhypergraph. We present a $(2\\lambda + 1)$-approximation algorithm when node\ncapacities are set to one, where $\\lambda$ is the maximum number of nodes in a\npath. For the special case of simultaneous release times for all flows, our\nresult improves to a $(2\\lambda)$-approximation. Furthermore, we generalize the\nresult to arbitrary node constraints and obtain a $(2\\lambda\\Delta + 1)$- and a\n$(2\\lambda\\Delta)$-approximation in the case of general and zero release times,\nwhere $\\Delta$ captures the capacity disparity between nodes.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 12:54:02 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 13:59:19 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Eckl", "Alexander", ""], ["Peter", "Luisa", ""], ["Schiffer", "Maximilian", ""], ["Albers", "Susanne", ""]]}, {"id": "1911.13144", "submitter": "Alane Lima", "authors": "Alane M. de Lima, Murilo V. G. da Silva, Andr\\'e L. Vignatti", "title": "Shortest Path Centrality and the APSP problem via VC-dimension and\n  Rademacher Averages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are interested in a version of the All-pairs Shortest Paths\nproblem (APSP) that fits neither in the exact nor in the approximate case. We\ndefine a measure of centrality of a shortest path, related to the\n``importance'' of such shortest path in the graph, and propose an algorithm\nbased on the idea of progressive sampling that, for {\\it any fixed constants}\n$0 < \\epsilon$, $ \\delta < 1$, given an undirected graph $G$ with non-negative\nedge weights, outputs with probability $1 - \\delta$ a data structure of size $n\n\\cdot \\textrm{Diam}_V(G)$, where $\\textrm{Diam}_V(G)$ is the vertex diameter of\n$G$, in expected time $\\mathcal{O}(\\lg n \\max(m + n \\log n, n \\cdot\n\\textrm{Diam}_V(G)))$ containing the (exact) distance and the shortest path\nbetween every pair of vertices $(u,v)$ that has centrality at least $\\epsilon$.\nThe progressive sampling technique is sensitive to the probability distribution\nof the input (if we assume that $G$ is chosen from a prescribed random\ndistribution), but even in the case where we take no assumption about such\ndistribution, we show an upper bound for the sample size using VC-dimension\ntheory that is tighter than the bound given by standard Hoeffding and union\nbounds, since VC-dimension theory captures the combinatorial structure of the\ninput graph.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 15:12:27 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 20:29:43 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 18:48:12 GMT"}, {"version": "v4", "created": "Mon, 4 May 2020 19:10:40 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["de Lima", "Alane M.", ""], ["da Silva", "Murilo V. G.", ""], ["Vignatti", "Andr\u00e9 L.", ""]]}, {"id": "1911.13161", "submitter": "Rajesh Chitnis", "authors": "Rajesh Chitnis, Andreas Emil Feldmann, MohammadTaghi Hajiaghayi,\n  D\\'aniel Marx", "title": "Tight Bounds for Planar Strongly Connected Steiner Subgraph with Fixed\n  Number of Terminals (and Extensions)", "comments": "To appear in SICOMP. Extended abstract in SODA 2014. This version has\n  a new author (Andreas Emil Feldmann), and the algorithm is faster and\n  considerably simplified as compared to conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (see paper for full abstract)\n  Given a vertex-weighted directed graph $G=(V,E)$ and a set $T=\\{t_1, t_2,\n\\ldots t_k\\}$ of $k$ terminals, the objective of the SCSS problem is to find a\nvertex set $H\\subseteq V$ of minimum weight such that $G[H]$ contains a\n$t_{i}\\rightarrow t_j$ path for each $i\\neq j$. The problem is NP-hard, but\nFeldman and Ruhl [FOCS '99; SICOMP '06] gave a novel $n^{O(k)}$ algorithm for\nthe SCSS problem, where $n$ is the number of vertices in the graph and $k$ is\nthe number of terminals. We explore how much easier the problem becomes on\nplanar directed graphs:\n  - Our main algorithmic result is a $2^{O(k)}\\cdot n^{O(\\sqrt{k})}$ algorithm\nfor planar SCSS, which is an improvement of a factor of $O(\\sqrt{k})$ in the\nexponent over the algorithm of Feldman and Ruhl.\n  - Our main hardness result is a matching lower bound for our algorithm: we\nshow that planar SCSS does not have an $f(k)\\cdot n^{o(\\sqrt{k})}$ algorithm\nfor any computable function $f$, unless the Exponential Time Hypothesis (ETH)\nfails.\n  The following additional results put our upper and lower bounds in context:\n  - In general graphs, we cannot hope for such a dramatic improvement over the\n$n^{O(k)}$ algorithm of Feldman and Ruhl: assuming ETH, SCSS in general graphs\ndoes not have an $f(k)\\cdot n^{o(k/\\log k)}$ algorithm for any computable\nfunction $f$.\n  - Feldman and Ruhl generalized their $n^{O(k)}$ algorithm to the more general\nDirected Steiner Network (DSN) problem; here the task is to find a subgraph of\nminimum weight such that for every source $s_i$ there is a path to the\ncorresponding terminal $t_i$. We show that, assuming ETH, there is no\n$f(k)\\cdot n^{o(k)}$ time algorithm for DSN on acyclic planar graphs.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 15:49:44 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Chitnis", "Rajesh", ""], ["Feldmann", "Andreas Emil", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1911.13268", "submitter": "Xue Chen", "authors": "Pranjal Awasthi, Vaggos Chatziafratis, Xue Chen, Aravindan\n  Vijayaraghavan", "title": "Adversarially Robust Low Dimensional Representations", "comments": "68 pages including references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning systems are vulnerable to small perturbations made to\nthe input either at test time or at training time. This has received much\nrecent interest on the empirical front due to several applications where\nreliability and security are critical, and the emergence of paradigms such as\nlow precision machine learning. However our theoretical understanding of the\ndesign of adversarially robust algorithms for the above settings is limited.\n  In this work we focus on Principal Component Analysis (PCA), a ubiquitous\nalgorithmic primitive in machine learning. We formulate a natural robust\nvariant of PCA, where the goal is to find a low dimensional subspace to\nrepresent the given data with minimum projection error, and that is in addition\nrobust to small perturbations measured in $\\ell_q$ norm (say $q=\\infty$).\nUnlike PCA which is solvable in polynomial time, our formulation is\ncomputationally intractable to optimize as it captures the well-studied sparse\nPCA objective as a special case. We show various algorithmic and statistical\nresults including:\n  - Polynomial time algorithm that is constant factor competitive in the\nworst-case, with respect to the best subspace both in terms of the projection\nerror and the robustness criterion. We also show that our algorithmic\ntechniques can be made robust to corruptions in the training data as well, in\naddition to yielding representations that are robust at test time.\n  - We prove that our formulation (and algorithms) also enjoy significant\nstatistical benefits in terms of sample complexity over standard PCA on account\nof a ``regularization effect'', that is formalized using the well-studied\nspiked covariance model.\n  - We illustrate the broad applicability of our algorithmic techniques in\naddressing robustness to adversarial perturbations, both at training-time and\ntest-time.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 18:06:29 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 03:46:13 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Chatziafratis", "Vaggos", ""], ["Chen", "Xue", ""], ["Vijayaraghavan", "Aravindan", ""]]}]