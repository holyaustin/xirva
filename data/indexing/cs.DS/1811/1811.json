[{"id": "1811.00139", "submitter": "Nathaniel Harms", "authors": "Nathaniel Harms", "title": "Testing Halfspaces over Rotation-Invariant Distributions", "comments": "36 pages, 2 figures, to appear in SODA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for testing halfspaces over arbitrary, unknown\nrotation-invariant distributions. Using $\\tilde O(\\sqrt{n}\\epsilon^{-7})$\nrandom examples of an unknown function $f$, the algorithm determines with high\nprobability whether $f$ is of the form $f(x) = sign(\\sum_i w_ix_i-t)$ or is\n$\\epsilon$-far from all such functions. This sample size is significantly\nsmaller than the well-known requirement of $\\Omega(n)$ samples for learning\nhalfspaces, and known lower bounds imply that our sample size is optimal (in\nits dependence on $n$) up to logarithmic factors. The algorithm is\ndistribution-free in the sense that it requires no knowledge of the\ndistribution aside from the promise of rotation invariance. To prove the\ncorrectness of this algorithm we present a theorem relating the distance\nbetween a function and a halfspace to the distance between their centers of\nmass, that applies to arbitrary distributions.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 22:25:27 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Harms", "Nathaniel", ""]]}, {"id": "1811.00148", "submitter": "Hongyang Zhang", "authors": "Hongyang Zhang, Vatsal Sharan, Moses Charikar, Yingyu Liang", "title": "Recovery Guarantees for Quadratic Tensors with Limited Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the tensor completion problem of predicting the missing entries\nof a tensor. The commonly used CP model has a triple product form, but an\nalternate family of quadratic models which are the sum of pairwise products\ninstead of a triple product have emerged from applications such as\nrecommendation systems. Non-convex methods are the method of choice for\nlearning quadratic models, and this work examines their sample complexity and\nerror guarantee. Our main result is that with the number of samples being only\nlinear in the dimension, all local minima of the mean squared error objective\nare global minima and recover the original tensor accurately. The techniques\nlead to simple proofs showing that convex relaxation can recover quadratic\ntensors provided with linear number of samples. We substantiate our theoretical\nresults with experiments on synthetic and real-world data, showing that\nquadratic models have better performance than CP models in scenarios where\nthere are limited amount of observations available.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 23:05:22 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Zhang", "Hongyang", ""], ["Sharan", "Vatsal", ""], ["Charikar", "Moses", ""], ["Liang", "Yingyu", ""]]}, {"id": "1811.00414", "submitter": "Ewin Tang", "authors": "Ewin Tang", "title": "Quantum-inspired classical algorithms for principal component analysis\n  and supervised clustering", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe classical analogues to Lloyd et al.'s quantum algorithms for\nprincipal component analysis and nearest-centroid clustering. We introduce a\nclassical algorithm model that assumes we can efficiently perform $\\ell^2$-norm\nsamples of input data, a natural analogue to quantum algorithms assuming\nefficient state preparation. In this model, our classical algorithms run in\ntime polylogarithmic in input size, matching the runtime of the quantum\nalgorithms with only polynomial slowdown. These algorithms indicate that their\ncorresponding problems do not yield exponential quantum speedups.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 03:23:52 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 10:22:32 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Tang", "Ewin", ""]]}, {"id": "1811.00487", "submitter": "Tu Nguyen", "authors": "Ngoc-Tu Nguyen, Bing-Hong Liu, and Shih-Yuan Wang", "title": "On New Approaches of Maximum Weighted Target Coverage and Sensor\n  Connectivity: Hardness and Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mobile wireless sensor networks (MWSNs), each sensor has the ability not\nonly to sense and transmit data but also to move to some specific location.\nBecause the movement of sensors consumes much more power than that in sensing\nand communication, the problem of scheduling mobile sensors to cover all\ntargets and maintain network connectivity such that the total movement distance\nof mobile sensors is minimized has received a great deal of attention. However,\nin reality, due to a limited budget or numerous targets, mobile sensors may be\nnot enough to cover all targets or form a connected network. Therefore, targets\nmust be weighted by their importance. The more important a target, the higher\nthe weight of the target. A more general problem for target coverage and\nnetwork connectivity, termed the Maximum Weighted Target Coverage and Sensor\nConnectivity with Limited Mobile Sensors (MWTCSCLMS) problem, is studied. In\nthis paper, an approximation algorithm, termed the\nweighted-maximum-coverage-based algorithm (WMCBA), is proposed for the\nsubproblem of the MWTCSCLMS problem. Based on the WMCBA, the Steiner-tree-based\nalgorithm (STBA) is proposed for the MWTCSCLMS problem. Simulation results\ndemonstrate that the STBA provides better performance than the other methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 04:34:40 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Nguyen", "Ngoc-Tu", ""], ["Liu", "Bing-Hong", ""], ["Wang", "Shih-Yuan", ""]]}, {"id": "1811.00573", "submitter": "Emmanouil Theodosis", "authors": "Emmanouil Theodosis and Petros Maragos", "title": "Tropical Modeling of Weighted Transducer Algorithms on Graphs", "comments": "Under review for the International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.RA cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted Finite State Transducers (WFSTs) are versatile data structures that\ncan model a great number of problems, ranging from Automatic Speech Recognition\nto DNA sequencing. Traditional computer science algorithms are employed when\nworking with these structures in order to optimise their size, but also the\nruntime of decoding algorithms. However, these algorithms are not unified under\na common framework that would allow for their treatment as a whole. Moreover,\nthe inherent geometrical representation of WFSTs, coupled with the\ntopology-preserving algorithms that operate on them make the structures ideal\nfor tropical analysis. The benefits of such analysis have a twofold nature;\nfirst, matrix operations offer a connection to nonlinear vector space and\nspectral theory, and, second, tropical algebra offers a connection to tropical\ngeometry. In this work we model some of the most frequently used algorithms in\nWFSTs by using tropical algebra; this provides a theoretical unification and\nallows us to also analyse aspects of their tropical geometry. Further, we\nprovide insights via numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 18:19:38 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Theodosis", "Emmanouil", ""], ["Maragos", "Petros", ""]]}, {"id": "1811.00619", "submitter": "David Bryant", "authors": "David Bryant and Celine Scornavacca", "title": "An $O(n \\log n)$ time Algorithm for computing the Path-length Distance\n  between Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree comparison metrics have proven to be an invaluable aide in the\nreconstruction and analysis of phylogenetic (evolutionary) trees. The\npath-length distance between trees is a particularly attractive measure as it\nreflects differences in tree shape as well as differences between branch\nlengths. The distance equals the sum, over all pairs of taxa, of the squared\ndifferences between the lengths of the unique path connecting them in each\ntree. We describe an $O(n \\log n)$ time for computing this distance, making\nextensive use of tree decomposition techniques introduced by Brodal et al.\n(2004).\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 20:14:43 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Bryant", "David", ""], ["Scornavacca", "Celine", ""]]}, {"id": "1811.00710", "submitter": "Bundit Laekhanukit", "authors": "Marek Cygan, Guy Kortsarz, Bundit Laekhanukit", "title": "On subexponential running times for approximating directed Steiner tree\n  and related problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns proving almost tight (super-polynomial) running times,\nfor achieving desired approximation ratios for various problems. To illustrate,\nthe question we study, let us consider the Set-Cover problem with n elements\nand m sets. Now we specify our goal to approximate Set-Cover to a factor of\n(1-d)ln n, for a given parameter 0<d<1. What is the best possible running time\nfor achieving such approximation? This question was answered implicitly in the\nwork of Moshkovitz [Theory of Computing, 2015]: Assuming both the Projection\nGames Conjecture (PGC) and the Exponential-Time Hypothesis (ETH), any ((1-d) ln\nn)-approximation algorithm for Set-Cover must run in time >= 2^{n^{c d}}, for\nsome constant 0<d<1.\n  We study the questions along this line. First, we show that under ETH and PGC\nany ((1-d) \\ln n)-approximation for Set-Cover requires 2^{n^{d}}-time. This\n(almost) matches the running time of 2^{O(n^d)} for approximating Set-Cover to\na factor (1-d) ln n by Cygan et al. [IPL, 2009]. Our result is tight up to the\nconstant multiplying the n^{d} terms in the exponent. This lower bound applies\nto all of its generalizations, e.g., Group Steiner Tree (GST), Directed Steiner\n(DST), Covering Steiner Tree (CST), Connected Polymatroid (CP). We also show\nthat in almost exponential time, these problems reduce to Set-Cover: We show\n(1-d)ln n approximation algorithms for all these problems that run in time\n2^{n^{d \\log n } poly(m).\n  We also study log^{2-d}n approximation for GST. Chekuri-Pal [FOCS, 2005]\nshowed that GST admits (log^{2-d}n)-approximation in time\nexp(2^{log^{d+o(1)}n}), for any 0 < d < 1. We show the lower bound of GST: any\n(log^{2-d}n)-approximation for GST must run in time >=\nexp((1+o(1)){log^{d-c}n}), for any c>0, unless the ETH is false. Our result\nfollows by analyzing the work of Halperin and Krauthgamer [STOC, 2003]. The\nsame lower and upper bounds hold for CST.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 02:24:40 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Cygan", "Marek", ""], ["Kortsarz", "Guy", ""], ["Laekhanukit", "Bundit", ""]]}, {"id": "1811.00785", "submitter": "Marcel Radermacher", "authors": "Tamara Mchedlidze, Marcel Radermacher, Ignaz Rutter, Nina Zimbel", "title": "Drawing Clustered Graphs on Disk Arrangements", "comments": "Preliminary work appeared in the Proceedings of the 13th\n  International Conference and Workshops on Algorithms and Computation (WALCOM\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V, E)$ be a planar graph and let $\\mathcal{C}$ be a partition of $V$.\nWe refer to the graphs induced by the vertex sets in $\\mathcal{C}$ as Clusters.\nLet $D_{\\mathcal C}$ be an arrangement of disks with a bijection between the\ndisks and the clusters. Akitaya et al. give an algorithm to test whether $(G,\n\\mathcal{C})$ can be embedded onto $D_{\\mathcal C}$ with the additional\nconstraint that edges are routed through a set of pipes between the disks.\nBased on such an embedding, we prove that every clustered graph and every disk\narrangement without pipe-disk intersections has a planar straight-line drawing\nwhere every vertex is embedded in the disk corresponding to its cluster. This\nresult can be seen as an extension of the result by Alam et al. who solely\nconsider biconnected clusters. Moreover, we prove that it is NP-hard to decide\nwhether a clustered graph has such a straight-line drawing, if we permit\npipe-disk intersections.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 09:05:43 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Mchedlidze", "Tamara", ""], ["Radermacher", "Marcel", ""], ["Rutter", "Ignaz", ""], ["Zimbel", "Nina", ""]]}, {"id": "1811.00816", "submitter": "Fabrizio Montecchiani", "authors": "Michael A. Bekos, Henry F\\\"orster, Martin Gronemann, Tamara\n  Mchedlidze, Fabrizio Montecchiani, Chrysanthi Raftopoulou, Torsten Ueckerdt", "title": "Planar Graphs of Bounded Degree have Constant Queue Number", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A \\emph{queue layout} of a graph consists of a \\emph{linear order} of its\nvertices and a partition of its edges into \\emph{queues}, so that no two\nindependent edges of the same queue are nested. The \\emph{queue number} of a\ngraph is the minimum number of queues required by any of its queue layouts. A\nlong-standing conjecture by Heath, Leighton and Rosenberg states that the queue\nnumber of planar graphs is bounded. This conjecture has been partially settled\nin the positive for several subfamilies of planar graphs (most of which have\nbounded treewidth). In this paper, we make a further important step towards\nsettling this conjecture. We prove that planar graphs of bounded degree (which\nmay have unbounded treewidth) have bounded queue number.\n  A notable implication of this result is that every planar graph of bounded\ndegree admits a three-dimensional straight-line grid drawing in linear volume.\nFurther implications are that every planar graph of bounded degree has bounded\ntrack number, and that every $k$-planar graph (i.e., every graph that can be\ndrawn in the plane with at most $k$ crossings per edge) of bounded degree has\nbounded queue number.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 10:58:17 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 14:59:04 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2019 08:19:53 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Bekos", "Michael A.", ""], ["F\u00f6rster", "Henry", ""], ["Gronemann", "Martin", ""], ["Mchedlidze", "Tamara", ""], ["Montecchiani", "Fabrizio", ""], ["Raftopoulou", "Chrysanthi", ""], ["Ueckerdt", "Torsten", ""]]}, {"id": "1811.00833", "submitter": "Armin Wei{\\ss}", "authors": "Stefan Edelkamp, Armin Wei{\\ss}", "title": "Worst-Case Efficient Sorting with QuickMergesort", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two most prominent solutions for the sorting problem are Quicksort and\nMergesort. While Quicksort is very fast on average, Mergesort additionally\ngives worst-case guarantees, but needs extra space for a linear number of\nelements. Worst-case efficient in-place sorting, however, remains a challenge:\nthe standard solution, Heapsort, suffers from a bad cache behavior and is also\nnot overly fast for in-cache instances.\n  In this work we present median-of-medians QuickMergesort (MoMQuickMergesort),\na new variant of QuickMergesort, which combines Quicksort with Mergesort\nallowing the latter to be implemented in place. Our new variant applies the\nmedian-of-medians algorithm for selecting pivots in order to circumvent the\nquadratic worst case. Indeed, we show that it uses at most $n \\log n + 1.6n$\ncomparisons for $n$ large enough.\n  We experimentally confirm the theoretical estimates and show that the new\nalgorithm outperforms Heapsort by far and is only around 10% slower than\nIntrosort (std::sort implementation of stdlibc++), which has a rather poor\nguarantee for the worst case. We also simulate the worst case, which is only\naround 10% slower than the average case. In particular, the new algorithm is a\nnatural candidate to replace Heapsort as a worst-case stopper in Introsort.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 12:18:23 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Edelkamp", "Stefan", ""], ["Wei\u00df", "Armin", ""]]}, {"id": "1811.00887", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck and Yin Tat Lee and Yuanzhi Li and Mark Sellke", "title": "Competitively Chasing Convex Bodies", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathcal{F}$ be a family of sets in some metric space. In the\n$\\mathcal{F}$-chasing problem, an online algorithm observes a request sequence\nof sets in $\\mathcal{F}$ and responds (online) by giving a sequence of points\nin these sets. The movement cost is the distance between consecutive such\npoints. The competitive ratio is the worst case ratio (over request sequences)\nbetween the total movement of the online algorithm and the smallest movement\none could have achieved by knowing in advance the request sequence. The family\n$\\mathcal{F}$ is said to be chaseable if there exists an online algorithm with\nfinite competitive ratio. In 1991, Linial and Friedman conjectured that the\nfamily of convex sets in Euclidean space is chaseable. We prove this\nconjecture.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 14:30:54 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Lee", "Yin Tat", ""], ["Li", "Yuanzhi", ""], ["Sellke", "Mark", ""]]}, {"id": "1811.00944", "submitter": "Alexander Wein", "authors": "Ankur Moitra and Alexander S. Wein", "title": "Spectral Methods from Tensor Networks", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tensor network is a diagram that specifies a way to \"multiply\" a collection\nof tensors together to produce another tensor (or matrix). Many existing\nalgorithms for tensor problems (such as tensor decomposition and tensor PCA),\nalthough they are not presented this way, can be viewed as spectral methods on\nmatrices built from simple tensor networks. In this work we leverage the full\npower of this abstraction to design new algorithms for certain continuous\ntensor decomposition problems.\n  An important and challenging family of tensor problems comes from orbit\nrecovery, a class of inference problems involving group actions (inspired by\napplications such as cryo-electron microscopy). Orbit recovery problems over\nfinite groups can often be solved via standard tensor methods. However, for\ninfinite groups, no general algorithms are known. We give a new spectral\nalgorithm based on tensor networks for one such problem: continuous\nmulti-reference alignment over the infinite group SO(2). Our algorithm extends\nto the more general heterogeneous case.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 15:52:35 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Moitra", "Ankur", ""], ["Wein", "Alexander S.", ""]]}, {"id": "1811.00950", "submitter": "Lars Rohwedder", "authors": "Klaus Jansen, Alexandra Lassota and Lars Rohwedder", "title": "Near-Linear Time Algorithm for n-fold ILPs via Color Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an important case of ILPs $\\max\\{c^Tx \\ \\vert\\ \\mathcal Ax = b, l\n\\leq x \\leq u,\\, x \\in \\mathbb{Z}^{n t} \\} $ with $n\\cdot t$ variables and\nlower and upper bounds $\\ell, u\\in\\mathbb Z^{nt}$. In $n$-fold ILPs non-zero\nentries only appear in the first $r$ rows of the matrix $\\mathcal A$ and in\nsmall blocks of size $s\\times t$ along the diagonal underneath. Despite this\nrestriction many optimization problems can be expressed in this form. It is\nknown that $n$-fold ILPs can be solved in FPT time regarding the parameters $s,\nr,$ and $\\Delta$, where $\\Delta$ is the greatest absolute value of an entry in\n$\\mathcal A$. The state-of-the-art technique is a local search algorithm that\nsubsequently moves in an improving direction. Both, the number of iterations\nand the search for such an improving direction take time $\\Omega(n)$, leading\nto a quadratic running time in $n$. We introduce a technique based on Color\nCoding, which allows us to compute these improving directions in logarithmic\ntime after a single initialization step. This leads to the first algorithm for\n$n$-fold ILPs with a running time that is near-linear in the number $nt$ of\nvariables, namely $(rs\\Delta)^{O(r^2s + s^2)} L^2 \\cdot nt \\log^{O(1)}(nt)$,\nwhere $L$ is the encoding length of the largest integer in the input. In\ncontrast to the algorithms in recent literature, we do not need to solve the LP\nrelaxation in order to handle unbounded variables. Instead, we give a\nstructural lemma to introduce appropriate bounds. If, on the other hand, we are\ngiven such an LP solution, the running time can be decreased by a factor of\n$L$.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 15:59:32 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Jansen", "Klaus", ""], ["Lassota", "Alexandra", ""], ["Rohwedder", "Lars", ""]]}, {"id": "1811.00955", "submitter": "Lars Rohwedder", "authors": "Klaus Jansen and Lars Rohwedder", "title": "Local search breaks 1.75 for Graph Balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Balancing is the problem of orienting the edges of a weighted\nmultigraph so as to minimize the maximum weighted in-degree. Since the\nintroduction of the problem the best algorithm known achieves an approximation\nratio of $1.75$ and it is based on rounding a linear program with this exact\nintegrality gap. It is also known that there is no $(1.5 -\n\\epsilon)$-approximation algorithm, unless $\\mathrm{P}=\\mathrm{NP}$. Can we do\nbetter than $1.75$? We prove that a different LP formulation, the configuration\nLP, has a strictly smaller integrality gap. Graph Balancing was the last one in\na group of related problems from literature, for which it was open whether the\nconfiguration LP is stronger than previous, simple LP relaxations. We base our\nproof on a local search approach that has been applied successfully to the more\ngeneral Restricted Assignment problem, which in turn is a prominent special\ncase of makespan minimization on unrelated machines. With a number of technical\nnovelties we are able to obtain a bound of $1.749$ for the case of Graph\nBalancing. It is not clear whether the local search algorithm we present\nterminates in polynomial time, which means that the bound is non-constructive.\nHowever, it is a strong evidence that a better approximation algorithm is\npossible using the configuration LP and it allows the optimum to be estimated\nwithin a factor better than $1.75$. A particularly interesting aspect of our\ntechniques is the way we handle small edges in the local search. We manage to\nexploit the configuration constraints enforced on small edges in the LP. This\nmay be of interest to other problems such as Restricted Assignment as well.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 16:08:29 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Jansen", "Klaus", ""], ["Rohwedder", "Lars", ""]]}, {"id": "1811.00999", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck, Bo'az Klartag, Yin Tat Lee, Yuanzhi Li, Mark\n  Sellke", "title": "Chasing Nested Convex Bodies Nearly Optimally", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convex body chasing problem, introduced by Friedman and Linial, is a\ncompetitive analysis problem on any normed vector space. In convex body\nchasing, for each timestep $t\\in\\mathbb N$, a convex body $K_t\\subseteq \\mathbb\nR^d$ is given as a request, and the player picks a point $x_t\\in K_t$. The\nplayer aims to ensure that the total distance $\\sum_{t=0}^{T-1}||x_t-x_{t+1}||$\nis within a bounded ratio of the smallest possible offline solution.\n  In this work, we consider the nested version of the problem, in which the\nsequence $(K_t)$ must be decreasing. For Euclidean spaces, we consider a\nmemoryless algorithm which moves to the so-called Steiner point, and show that\nin a certain sense it is exactly optimal among memoryless algorithms. For\ngeneral finite dimensional normed spaces, we combine the Steiner point and our\nrecent previous algorithm to obtain a new algorithm which is nearly optimal for\nall $\\ell^p_d$ spaces with $p\\geq 1$, closing a polynomial gap.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 17:32:16 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 18:13:44 GMT"}, {"version": "v3", "created": "Fri, 28 Jun 2019 21:24:57 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Klartag", "Bo'az", ""], ["Lee", "Yin Tat", ""], ["Li", "Yuanzhi", ""], ["Sellke", "Mark", ""]]}, {"id": "1811.01077", "submitter": "Jinglong Zhao", "authors": "Will Ma, David Simchi-Levi, Jinglong Zhao", "title": "Dynamic Pricing (and Assortment) under a Static Calendar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by our collaboration with a large consumer packaged\ngoods (CPG) company. We have found that while the company appreciates the\nadvantages of dynamic pricing, they deem it operationally much easier to plan\nout a static price calendar in advance.\n  We investigate the efficacy of static control policies for revenue management\nproblems whose optimal solution is inherently dynamic. In these problems, a\nfirm has limited inventory to sell over a finite time horizon, over which\nheterogeneous customers stochastically arrive. We consider both pricing and\nassortment controls, and derive simple static policies in the form of a price\ncalendar or a planned sequence of assortments, respectively. In the assortment\nplanning problem, we also differentiate between the static vs. dynamic\nsubstitution models of customer demand. We show that our policies are within\n1-1/e (approximately 0.63) of the optimum under stationary (IID) demand, and\n1/2 of the optimum under non-stationary demand, with both guarantees\napproaching 1 if the starting inventories are large.\n  We adapt the technique of prophet inequalities from optimal stopping theory\nto pricing and assortment problems, and our results are relative to the linear\nprogramming relaxation. Under the special case of IID single-item pricing, our\nresults improve the understanding of irregular and discrete demand curves, by\nshowing that a static calendar can be (1-1/e)-approximate if the prices are\nsorted high-to-low.\n  Finally, we demonstrate on both data from the CPG company and synthetic data\nfrom the literature that our simple price and assortment calendars are\neffective.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 20:27:01 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 17:34:10 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 19:39:53 GMT"}, {"version": "v4", "created": "Mon, 6 Apr 2020 03:52:21 GMT"}, {"version": "v5", "created": "Sun, 22 Nov 2020 01:24:47 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ma", "Will", ""], ["Simchi-Levi", "David", ""], ["Zhao", "Jinglong", ""]]}, {"id": "1811.01121", "submitter": "Arun Ganesh", "authors": "Arun Ganesh, Qiuyi Zhang", "title": "Optimal Sequence Length Requirements for Phylogenetic Tree\n  Reconstruction with Indels", "comments": "Update: Many minor edits to improve clarity and presentation as\n  suggested by STOC reviewers. The results and overall structure of the paper\n  are unaffected. To appear in STOC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the phylogenetic tree reconstruction problem with insertions and\ndeletions (indels). Phylogenetic algorithms proceed under a model where\nsequences evolve down the model tree, and given sequences at the leaves, the\nproblem is to reconstruct the model tree with high probability. Traditionally,\nsequences mutate by substitution-only processes, although some recent work\nconsiders evolutionary processes with insertions and deletions. In this paper,\nwe improve on previous work by giving a reconstruction algorithm that\nsimultaneously has $O(\\text{poly} \\log n)$ sequence length and tolerates\nconstant indel probabilities on each edge. Our recursively-reconstructed\ndistance-based technique provably outputs the model tree when the model tree\nhas $O(\\text{poly} \\log n)$ diameter and discretized branch lengths, allowing\nfor the probability of insertion and deletion to be non-uniform and asymmetric\non each edge. Our polylogarithmic sequence length bounds improve significantly\nover previous polynomial sequence length bounds and match sequence length\nbounds in the substitution-only models of phylogenetic evolution, thereby\nchallenging the idea that many global misalignments caused by insertions and\ndeletions when $p_{indel}$ is large are a fundamental obstruction to\nreconstruction with short sequences.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 23:17:17 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 21:14:10 GMT"}, {"version": "v3", "created": "Wed, 20 Feb 2019 23:08:56 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Ganesh", "Arun", ""], ["Zhang", "Qiuyi", ""]]}, {"id": "1811.01129", "submitter": "Sam Safavi", "authors": "Bei Jia, Surjyendu Ray, Sam Safavi, Jos\\'e Bento", "title": "Efficient Projection onto the Perfect Phylogeny Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several algorithms build on the perfect phylogeny model to infer evolutionary\ntrees. This problem is particularly hard when evolutionary trees are inferred\nfrom the fraction of genomes that have mutations in different positions, across\ndifferent samples. Existing algorithms might do extensive searches over the\nspace of possible trees. At the center of these algorithms is a projection\nproblem that assigns a fitness cost to phylogenetic trees. In order to perform\na wide search over the space of the trees, it is critical to solve this\nprojection problem fast. In this paper, we use Moreau's decomposition for\nproximal operators, and a tree reduction scheme, to develop a new algorithm to\ncompute this projection. Our algorithm terminates with an exact solution in a\nfinite number of steps, and is extremely fast. In particular, it can search\nover all evolutionary trees with fewer than 11 nodes, a size relevant for\nseveral biological problems (more than 2 billion trees) in about 2 hours.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 00:03:01 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 21:27:21 GMT"}, {"version": "v3", "created": "Fri, 28 Dec 2018 17:25:51 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Jia", "Bei", ""], ["Ray", "Surjyendu", ""], ["Safavi", "Sam", ""], ["Bento", "Jos\u00e9", ""]]}, {"id": "1811.01162", "submitter": "Guohui Lin", "authors": "An Zhang, Yong Chen, Zhi-Zhong Chen, and Guohui Lin", "title": "Improved approximation algorithms for path vertex covers in regular\n  graphs", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a simple graph $G = (V, E)$ and a constant integer $k \\ge 2$, the\n$k$-path vertex cover problem ({\\sc P$k$VC}) asks for a minimum subset $F\n\\subseteq V$ of vertices such that the induced subgraph $G[V - F]$ does not\ncontain any path of order $k$. When $k = 2$, this turns out to be the classic\nvertex cover ({\\sc VC}) problem, which admits a $\\left(2 - {\\rm\n\\Theta}\\left(\\frac 1{\\log|V|}\\right)\\right)$-approximation. The general {\\sc\nP$k$VC} admits a trivial $k$-approximation; when $k = 3$ and $k = 4$, the best\nknown approximation results for {\\sc P$3$VC} and {\\sc P$4$VC} are a\n$2$-approximation and a $3$-approximation, respectively. On $d$-regular graphs,\nthe approximation ratios can be reduced to $\\min\\left\\{2 - \\frac 5{d+3} +\n\\epsilon, 2 - \\frac {(2 - o(1))\\log\\log d}{\\log d}\\right\\}$ for {\\sc VC} ({\\it\ni.e.}, {\\sc P$2$VC}), $2 - \\frac 1d + \\frac {4d - 2}{3d |V|}$ for {\\sc P$3$VC},\n$\\frac {\\lfloor d/2\\rfloor (2d - 2)}{(\\lfloor d/2\\rfloor + 1) (d - 2)}$ for\n{\\sc P$4$VC}, and $\\frac {2d - k + 2}{d - k + 2}$ for {\\sc P$k$VC} when $1 \\le\nk-2 < d \\le 2(k-2)$. By utilizing an existing algorithm for graph defective\ncoloring, we first present a $\\frac {\\lfloor d/2\\rfloor (2d - k + 2)}{(\\lfloor\nd/2\\rfloor + 1) (d - k + 2)}$-approximation for {\\sc P$k$VC} on $d$-regular\ngraphs when $1 \\le k - 2 < d$. This beats all the best known approximation\nresults for {\\sc P$k$VC} on $d$-regular graphs for $k \\ge 3$, except for {\\sc\nP$4$VC} it ties with the best prior work and in particular they tie at $2$ on\ncubic graphs and $4$-regular graphs. We then propose a $1.875$-approximation\nand a $1.852$-approximation for {\\sc P$4$VC} on cubic graphs and $4$-regular\ngraphs, respectively. We also present a better approximation algorithm for {\\sc\nP$4$VC} on $d$-regular bipartite graphs.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 05:05:51 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Zhang", "An", ""], ["Chen", "Yong", ""], ["Chen", "Zhi-Zhong", ""], ["Lin", "Guohui", ""]]}, {"id": "1811.01177", "submitter": "Tillmann Miltzow", "authors": "Michael Gene Dobbins, Andreas Holmsen, Tillmann Miltzow", "title": "Smoothed Analysis of the Art Gallery Problem", "comments": "24 pages, 12 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Art Gallery Problem we are given a polygon $P\\subset [0,L]^2$ on $n$\nvertices and a number $k$. We want to find a guard set $G$ of size $k$, such\nthat each point in $P$ is seen by a guard in $G$. Formally, a guard $g$ sees a\npoint $p \\in P$ if the line segment $pg$ is fully contained inside the polygon\n$P$. The history and practical findings indicate that irrational coordinates\nare a \"very rare\" phenomenon. We give a theoretical explanation. Next to worst\ncase analysis, Smoothed Analysis gained popularity to explain the practical\nperformance of algorithms, even if they perform badly in the worst case. The\nidea is to study the expected performance on small perturbations of the worst\ninput. The performance is measured in terms of the magnitude $\\delta$ of the\nperturbation and the input size. We consider four different models of\nperturbation. We show that the expected number of bits to describe optimal\nguard positions per guard is logarithmic in the input and the magnitude of the\nperturbation. This shows from a theoretical perspective that rational guards\nwith small bit-complexity are typical. Note that describing the guard position\nis the bottleneck to show NP-membership. The significance of our results is\nthat algebraic methods are not needed to solve the Art Gallery Problem in\ntypical instances. This is the first time an $\\exists\\mathbb{R}$-complete\nproblem was analyzed by Smoothed Analysis.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 08:45:12 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Dobbins", "Michael Gene", ""], ["Holmsen", "Andreas", ""], ["Miltzow", "Tillmann", ""]]}, {"id": "1811.01209", "submitter": "Nicola Prezza", "authors": "Nicola Prezza", "title": "Optimal Rank and Select Queries on Dictionary-Compressed Text", "comments": "improved select bound with reduction to psum. Added lower bounds on\n  trees", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of supporting queries on a string $S$ of length $n$\nwithin a space bounded by the size $\\gamma$ of a string attractor for $S$.\nRecent works showed that random access on $S$ can be supported in optimal\n$O(\\log(n/\\gamma)/\\log\\log n)$ time within $O\\left (\\gamma\\ \\rm{polylog}\\ n\n\\right)$ space. In this paper, we extend this result to \\emph{rank} and\n\\emph{select} queries and provide lower bounds matching our upper bounds on\nalphabets of polylogarithmic size. Our solutions are given in the form of a\nspace-time trade-off that is more general than the one previously known for\ngrammars and that improves existing bounds on LZ77-compressed text by a\n$\\log\\log n$ time-factor in \\emph{select} queries. We also provide matching\nlower and upper bounds for \\emph{partial sum} and \\emph{predecessor} queries\nwithin attractor-bounded space, and extend our lower bounds to encompass\nnavigation of dictionary-compressed tree representations.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 13:09:57 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 11:44:22 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 18:00:33 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Prezza", "Nicola", ""]]}, {"id": "1811.01216", "submitter": "Anindya De", "authors": "Anindya De, Ryan O'Donnell and Rocco Servedio", "title": "Learning sparse mixtures of rankings from noisy information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning an unknown mixture of $k$ rankings over $n$\nelements, given access to noisy samples drawn from the unknown mixture. We\nconsider a range of different noise models, including natural variants of the\n\"heat kernel\" noise framework and the Mallows model. For each of these noise\nmodels we give an algorithm which, under mild assumptions, learns the unknown\nmixture to high accuracy and runs in $n^{O(\\log k)}$ time. The best previous\nalgorithms for closely related problems have running times which are\nexponential in $k$.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 13:36:12 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["De", "Anindya", ""], ["O'Donnell", "Ryan", ""], ["Servedio", "Rocco", ""]]}, {"id": "1811.01226", "submitter": "Nabil Ibtehaz", "authors": "Nabil Ibtehaz, M. Kaykobad, M. Sohel Rahman", "title": "Multidimensional segment trees can do range updates in poly-logarithmic\n  time", "comments": null, "journal-ref": null, "doi": "10.1016/j.tcs.2020.11.034", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Updating and querying on a range is a classical algorithmic problem with a\nmultitude of applications. The Segment Tree data structure is particularly\nnotable in handling the range query and update operations. A Segment Tree\ndivides the range into disjoint segments and merges them together to perform\nrange queries and range updates elegantly. Although this data structure is\nremarkably potent for 1-dimensional problems, it falls short in higher\ndimensions. Lazy Propagation enables the operations to be computed in $O(logn)$\ntime in a single dimension. However, the concept of lazy propagation could not\nbe translated to higher-dimensional cases, which imposes a time complexity of\n$O(n^{k-1} \\; logn)$ for operations on $k$-dimensional data. In this work, we\nhave made an attempt to emulate the idea of lazy propagation differently so\nthat it can be applied for 2-dimensional cases. Moreover, the proposed\nmodification should be capable of performing most general aggregate functions\nsimilar to the original Segment Tree, and can also be extended to even higher\ndimensions. Our proposed algorithm manages to perform range sum queries and\nupdates in $O(\\log^2 n)$ time for a 2-dimensional problem, which becomes\n$O(\\log^d n)$ for a $d$-dimensional situation.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 14:21:14 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 19:58:47 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Ibtehaz", "Nabil", ""], ["Kaykobad", "M.", ""], ["Rahman", "M. Sohel", ""]]}, {"id": "1811.01248", "submitter": "Sivukhin Nikita", "authors": "Dmitry Kosolobov, Nikita Sivukhin", "title": "Compressed Multiple Pattern Matching", "comments": "14 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given $d$ strings over the alphabet $\\{0,1,\\ldots,\\sigma{-}1\\}$, the\nclassical Aho--Corasick data structure allows us to find all $occ$ occurrences\nof the strings in any text $T$ in $O(|T| + occ)$ time using $O(m\\log m)$ bits\nof space, where $m$ is the number of edges in the trie containing the strings.\nFix any constant $\\varepsilon \\in (0, 2)$. We describe a compressed solution\nfor the problem that, provided $\\sigma \\le m^\\delta$ for a constant $\\delta <\n1$, works in $O(|T| \\frac{1}{\\varepsilon} \\log\\frac{1}{\\varepsilon} + occ)$\ntime, which is $O(|T| + occ)$ since $\\varepsilon$ is constant, and occupies\n$mH_k + 1.443 m + \\varepsilon m + O(d\\log\\frac{m}{d})$ bits of space, for all\n$0 \\le k \\le \\max\\{0,\\alpha\\log_\\sigma m - 2\\}$ simultaneously, where $\\alpha\n\\in (0,1)$ is an arbitrary constant and $H_k$ is the $k$th-order empirical\nentropy of the trie. Hence, we reduce the $3.443m$ term in the space bounds of\npreviously best succinct solutions to $(1.443 + \\varepsilon)m$, thus solving an\nopen problem posed by Belazzougui. Further, we notice that $L =\n\\log\\binom{\\sigma (m+1)}{m} - O(\\log(\\sigma m))$ is a worst-case space lower\nbound for any solution of the problem and, for $d = o(m)$ and constant\n$\\varepsilon$, our approach allows to achieve $L + \\varepsilon m$ bits of\nspace, which gives an evidence that, for $d = o(m)$, the space of our data\nstructure is theoretically optimal up to the $\\varepsilon m$ additive term and\nit is hardly possible to eliminate the term $1.443m$. In addition, we refine\nthe space analysis of previous works by proposing a more appropriate definition\nfor $H_k$. We also simplify the construction for practice adapting the fixed\nblock compression boosting technique, then implement our data structure, and\nconduct a number of experiments showing that it is comparable to the state of\nthe art in terms of time and is superior in space.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 17:06:18 GMT"}, {"version": "v2", "created": "Sun, 31 Mar 2019 11:33:54 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Kosolobov", "Dmitry", ""], ["Sivukhin", "Nikita", ""]]}, {"id": "1811.01259", "submitter": "Sebastian Wild", "authors": "Stefan Edelkamp, Armin Wei{\\ss}, Sebastian Wild", "title": "QuickXsort - A Fast Sorting Scheme in Theory and Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  QuickXsort is a highly efficient in-place sequential sorting scheme that\nmixes Hoare's Quicksort algorithm with X, where X can be chosen from a wider\nrange of other known sorting algorithms, like Heapsort, Insertionsort and\nMergesort. Its major advantage is that QuickXsort can be in-place even if X is\nnot. In this work we provide general transfer theorems expressing the number of\ncomparisons of QuickXsort in terms of the number of comparisons of X. More\nspecifically, if pivots are chosen as medians of (not too fast) growing size\nsamples, the average number of comparisons of QuickXsort and X differ only by\n$o(n)$-terms. For median-of-$k$ pivot selection for some constant $k$, the\ndifference is a linear term whose coefficient we compute precisely. For\ninstance, median-of-three QuickMergesort uses at most $n \\lg n - 0.8358n +\nO(\\log n)$ comparisons.\n  Furthermore, we examine the possibility of sorting base cases with some other\nalgorithm using even less comparisons. By doing so the average-case number of\ncomparisons can be reduced down to $n \\lg n- 1.4106n + o(n)$ for a remaining\ngap of only $0.0321n$ comparisons to the known lower bound (while using only\n$O(\\log n)$ additional space and $O(n \\log n)$ time overall).\n  Implementations of these sorting strategies show that the algorithms\nchallenge well-established library implementations like Musser's Introsort.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 18:00:29 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Edelkamp", "Stefan", ""], ["Wei\u00df", "Armin", ""], ["Wild", "Sebastian", ""]]}, {"id": "1811.01296", "submitter": "Marcin Wrochna", "authors": "Du\\v{s}an Knop, Micha{\\l} Pilipczuk, Marcin Wrochna", "title": "Tight complexity lower bounds for integer linear programming with few\n  constraints", "comments": "Added Corollary 2, extended Conclusions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the ILP Feasibility problem: given an integer linear program\n$\\{Ax = b, x\\geq 0\\}$, where $A$ is an integer matrix with $k$ rows and $\\ell$\ncolumns and $b$ is a vector of $k$ integers, we ask whether there exists\n$x\\in\\mathbb{N}^\\ell$ that satisfies $Ax = b$. Our goal is to study the\ncomplexity of ILP Feasibility when both $k$, the number of constraints (rows of\n$A$), and $\\|A\\|_\\infty$, the largest absolute value in $A$, are small.\n  Papadimitriou [J. ACM, 1981] was the first to give a fixed-parameter\nalgorithm for ILP Feasibility in this setting, with running time\n$\\left((\\|A\\mid b\\|_\\infty) \\cdot k\\right)^{O(k^2)}$. This was very recently\nimproved by Eisenbrand and Weismantel [SODA 2018], who used the Steinitz lemma\nto design an algorithm with running time $(k\\|A\\|_\\infty)^{O(k)}\\cdot\n\\|b\\|_\\infty^2$, and subsequently by Jansen and Rohwedder [2018] to\n$O(k\\|A\\|_\\infty)^{k}\\cdot \\log \\|b\\|_\\infty$. We prove that for\n$\\{0,1\\}$-matrices $A$, the dependency on $k$ is probably optimal: an algorithm\nwith running time $2^{o(k\\log k)}\\cdot (\\ell+\\|b\\|_\\infty)^{o(k)}$ would\ncontradict ETH. This improves previous non-tight lower bounds of Fomin et al.\n[ESA 2018].\n  We then consider ILPs with many constraints, but structured in a shallow way.\nPrecisely, we consider the dual treedepth of the matrix $A$, which is the\ntreedepth of the graph over the rows of $A$, with two rows adjacent if in some\ncolumn they both contain a non-zero entry. It was recently shown by\nKouteck\\'{y} et al. [ICALP 2018] that ILP Feasibility can be solved in time\n$\\|A\\|_\\infty^{2^{O(td(A))}}\\cdot (k+\\ell+\\log \\|b\\|_\\infty)^{O(1)}$. We\npresent a streamlined proof of this fact and prove optimality: even assuming\nthat all entries of $A$ and $b$ are in $\\{-1,0,1\\}$, the existence of an\nalgorithm with running time $2^{2^{o(td(A))}}\\cdot (k+\\ell)^{O(1)}$ would\ncontradict ETH.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 22:54:45 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 11:06:16 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2019 10:53:14 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Knop", "Du\u0161an", ""], ["Pilipczuk", "Micha\u0142", ""], ["Wrochna", "Marcin", ""]]}, {"id": "1811.01313", "submitter": "Kasper Green Larsen", "authors": "Alireza Farhadi, MohammadTaghi Hajiaghayi, Kasper Green Larsen, Elaine\n  Shi", "title": "Lower Bounds for External Memory Integer Sorting via Network Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting extremely large datasets is a frequently occuring task in practice.\nThese datasets are usually much larger than the computer's main memory; thus\nexternal memory sorting algorithms, first introduced by Aggarwal and Vitter\n(1988), are often used. The complexity of comparison based external memory\nsorting has been understood for decades by now, however the situation remains\nelusive if we assume the keys to be sorted are integers. In internal memory,\none can sort a set of $n$ integer keys of $\\Theta(\\lg n)$ bits each in $O(n)$\ntime using the classic Radix Sort algorithm, however in external memory, there\nare no faster integer sorting algorithms known than the simple comparison based\nones. In this paper, we present a tight conditional lower bound on the\ncomplexity of external memory sorting of integers. Our lower bound is based on\na famous conjecture in network coding by Li and Li, who conjectured that\nnetwork coding cannot help anything beyond the standard multicommodity flow\nrate in undirected graphs. The only previous work connecting the Li and Li\nconjecture to lower bounds for algorithms is due to Adler et al. Adler et al.\nindeed obtain relatively simple lower bounds for oblivious algorithms (the\nmemory access pattern is fixed and independent of the input data).\nUnfortunately obliviousness is a strong limitations, especially for integer\nsorting: we show that the Li and Li conjecture implies an $\\Omega(n \\log n)$\nlower bound for internal memory oblivious sorting when the keys are $\\Theta(\\lg\nn)$ bits. This is in sharp contrast to the classic (non-oblivious) Radix Sort\nalgorithm. Indeed going beyond obliviousness is highly non-trivial; we need to\nintroduce several new methods and involved techniques, which are of their own\ninterest, to obtain our tight lower bound for external memory integer sorting.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 02:41:43 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Farhadi", "Alireza", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Larsen", "Kasper Green", ""], ["Shi", "Elaine", ""]]}, {"id": "1811.01442", "submitter": "Zhao Song", "authors": "Zhao Song, David P. Woodruff, Peilin Zhong", "title": "Towards a Zero-One Law for Column Subset Selection", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are a number of approximation algorithms for NP-hard versions of low\nrank approximation, such as finding a rank-$k$ matrix $B$ minimizing the sum of\nabsolute values of differences to a given $n$-by-$n$ matrix $A$,\n$\\min_{\\textrm{rank-}k~B}\\|A-B\\|_1$, or more generally finding a rank-$k$\nmatrix $B$ which minimizes the sum of $p$-th powers of absolute values of\ndifferences, $\\min_{\\textrm{rank-}k~B}\\|A-B\\|_p^p$. Many of these algorithms\nare linear time columns subset selection algorithms, returning a subset of\n$\\mathrm{poly}(k \\log n)$ columns whose cost is no more than a\n$\\mathrm{poly}(k)$ factor larger than the cost of the best rank-$k$ matrix. The\nabove error measures are special cases of the following general entrywise low\nrank approximation problem: given an arbitrary function $g:\\mathbb{R}\n\\rightarrow \\mathbb{R}_{\\geq 0}$, find a rank-$k$ matrix $B$ which minimizes\n$\\|A-B\\|_g = \\sum_{i,j}g(A_{i,j}-B_{i,j})$. A natural question is which\nfunctions $g$ admit efficient approximation algorithms? Indeed, this is a\ncentral question of recent work studying generalized low rank models. In this\nwork we give approximation algorithms for $\\textit{every}$ function $g$ which\nis approximately monotone and satisfies an approximate triangle inequality, and\nwe show both of these conditions are necessary. Further, our algorithm is\nefficient if the function $g$ admits an efficient approximate regression\nalgorithm. Our approximation algorithms handle functions which are not even\nscale-invariant, such as the Huber loss function, which we show have very\ndifferent structural properties than $\\ell_p$-norms, e.g., one can show the\nlack of scale-invariance causes any column subset selection algorithm to\nprovably require a $\\sqrt{\\log n}$ factor larger number of columns than\n$\\ell_p$-norms; nevertheless we design the first efficient column subset\nselection algorithms for such error measures.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 21:43:55 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 23:53:54 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Song", "Zhao", ""], ["Woodruff", "David P.", ""], ["Zhong", "Peilin", ""]]}, {"id": "1811.01472", "submitter": "Tomohiro I", "authors": "Kensuke Sakai, Tatsuya Ohno, Keisuke Goto, Yoshimasa Takabatake,\n  Tomohiro I, Hiroshi Sakamoto", "title": "RePair in Compressed Space and Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a string $T$ of length $N$, the goal of grammar compression is to\nconstruct a small context-free grammar generating only $T$. Among existing\ngrammar compression methods, RePair (recursive paring) [Larsson and Moffat,\n1999] is notable for achieving good compression ratios in practice. Although\nthe original paper already achieved a time-optimal algorithm to compute the\nRePair grammar RePair($T$) in expected $O(N)$ time, the study to reduce its\nworking space is still active so that it is applicable to large-scale data. In\nthis paper, we propose the first RePair algorithm working in compressed space,\ni.e., potentially $o(N)$ space for highly compressible texts. The key idea is\nto give a new way to restructure an arbitrary grammar $S$ for $T$ into\nRePair($T$) in compressed space and time. Based on the recompression technique,\nwe propose an algorithm for RePair($T$) in $O(\\min(N, nm \\log N))$ space and\nexpected $O(\\min(N, nm \\log N) m)$ time or $O(\\min(N, nm \\log N) \\log \\log N)$\ntime, where $n$ is the size of $S$ and $m$ is the number of variables in\nRePair($T$). We implemented our algorithm running in $O(\\min(N, nm \\log N) m)$\ntime and show it can actually run in compressed space. We also present a new\napproach to reduce the peak memory usage of existing RePair algorithms\ncombining with our algorithms, and show that the new approach outperforms, both\nin computation time and space, the most space efficient linear-time RePair\nimplementation to date.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 01:24:30 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Sakai", "Kensuke", ""], ["Ohno", "Tatsuya", ""], ["Goto", "Keisuke", ""], ["Takabatake", "Yoshimasa", ""], ["I", "Tomohiro", ""], ["Sakamoto", "Hiroshi", ""]]}, {"id": "1811.01537", "submitter": "Simon Mauras", "authors": "Claire Mathieu and Simon Mauras", "title": "How to aggregate Top-lists: Approximation algorithms via scores and\n  average ranks", "comments": "To appear in SODA'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A top-list is a possibly incomplete ranking of elements: only a subset of the\nelements are ranked, with all unranked elements tied for last. Top-list\naggregation, a generalization of the well-known rank aggregation problem, takes\nas input a collection of top-lists and aggregates them into a single complete\nranking, aiming to minimize the number of upsets (pairs ranked in opposite\norder in the input and in the output). In this paper, we give simple\napproximation algorithms for top-list aggregation.\n  * We generalize the footrule algorithm for rank aggregation.\n  * Using inspiration from approval voting, we define the score of an element\nas the frequency with which it is ranked, i.e. appears in an input top-list. We\nreinterpret Ailon's RepeatChoice algorithm for top-list aggregation using the\nscore of an element and its average rank given that it is ranked.\n  * Using average ranks, we generalize and analyze Borda's algorithm for rank\naggregation.\n  * We design a simple 2-phase variant of the Generalized Borda's algorithm,\nroughly sorting by scores and breaking ties by average ranks.\n  * We then design another 2-phase variant in which in order to break ties we\nuse, as a black box, the Mathieu-Schudy PTAS for rank aggregation, yielding a\nPTAS for top-list aggregation.\n  * Finally, we discuss the special case in which all input lists have constant\nlength.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 07:24:14 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 12:47:33 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Mathieu", "Claire", ""], ["Mauras", "Simon", ""]]}, {"id": "1811.01551", "submitter": "Oren Weimann", "authors": "Panagiotis Charalampopoulos, Pawe{\\l} Gawrychowski, Shay Mozes, Oren\n  Weimann", "title": "Almost Optimal Distance Oracles for Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new tradeoffs between space and query-time for exact distance\noracles in directed weighted planar graphs. These tradeoffs are almost optimal\nin the sense that they are within polylogarithmic, sub-polynomial or\narbitrarily small polynomial factors from the na\\\"{\\i}ve linear space, constant\nquery-time lower bound. These tradeoffs include: (i) an oracle with space\n$\\tilde{O}(n^{1+\\epsilon})$ and query-time $\\tilde{O}(1)$ for any constant\n$\\epsilon>0$, (ii) an oracle with space $\\tilde{O}(n)$ and query-time\n$\\tilde{O}(n^{\\epsilon})$ for any constant $\\epsilon>0$, and (iii) an oracle\nwith space $n^{1+o(1)}$ and query-time $n^{o(1)}$.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 08:37:57 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Charalampopoulos", "Panagiotis", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Mozes", "Shay", ""], ["Weimann", "Oren", ""]]}, {"id": "1811.01597", "submitter": "Nikhil Bansal", "authors": "Nikhil Bansal", "title": "On a generalization of iterated and randomized rounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a general method for rounding linear programs that combines the\ncommonly used iterated rounding and randomized rounding techniques. In\nparticular, we show that whenever iterated rounding can be applied to a problem\nwith some slack, there is a randomized procedure that returns an integral\nsolution that satisfies the guarantees of iterated rounding and also has\nconcentration properties. We use this to give new results for several classic\nproblems where iterated rounding has been useful.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 10:42:44 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 14:55:39 GMT"}, {"version": "v3", "created": "Thu, 18 Jul 2019 12:13:22 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Bansal", "Nikhil", ""]]}, {"id": "1811.01661", "submitter": "Stanislaw Gorlow", "authors": "Pedro J. Villasana T. and Stanislaw Gorlow", "title": "Exact multiplicative updates for convolutional $\\beta$-NMF in 2D", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend the $\\beta$-CNMF to two dimensions and derive exact\nmultiplicative updates for its factors. The new updates generalize and correct\nthe nonnegative matrix factor deconvolution previously proposed by Schmidt and\nM{\\o}rup. We show by simulation that the updates lead to a monotonically\ndecreasing $\\beta$-divergence in terms of the mean and the standard deviation\nand that the corresponding convergence curves are consistent across the most\ncommon values for $\\beta$.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 13:17:55 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["T.", "Pedro J. Villasana", ""], ["Gorlow", "Stanislaw", ""]]}, {"id": "1811.01672", "submitter": "Alkida Balliu", "authors": "Alkida Balliu, Sebastian Brandt, Yi-Jun Chang, Dennis Olivetti,\n  Mika\\\"el Rabie, Jukka Suomela", "title": "The distributed complexity of locally checkable problems on paths is\n  decidable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a computer network that consists of a path with $n$ nodes. The nodes\nare labeled with inputs from a constant-sized set, and the task is to find\noutput labels from a constant-sized set subject to some local\nconstraints---more formally, we have an LCL (locally checkable labeling)\nproblem. How many communication rounds are needed (in the standard LOCAL model\nof computing) to solve this problem?\n  It is well known that the answer is always either $O(1)$ rounds, or\n$\\Theta(\\log^* n)$ rounds, or $\\Theta(n)$ rounds. In this work we show that\nthis question is decidable (albeit PSPACE-hard): we present an algorithm that,\ngiven any LCL problem defined on a path, outputs the distributed computational\ncomplexity of this problem and the corresponding asymptotically optimal\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 13:37:30 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 13:55:30 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Balliu", "Alkida", ""], ["Brandt", "Sebastian", ""], ["Chang", "Yi-Jun", ""], ["Olivetti", "Dennis", ""], ["Rabie", "Mika\u00ebl", ""], ["Suomela", "Jukka", ""]]}, {"id": "1811.01816", "submitter": "Kuikui Liu", "authors": "Nima Anari and Kuikui Liu and Shayan Oveis Gharan and Cynthia Vinzant", "title": "Log-Concave Polynomials II: High-Dimensional Walks and an FPRAS for\n  Counting Bases of a Matroid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.PR math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design an FPRAS to count the number of bases of any matroid given by an\nindependent set oracle, and to estimate the partition function of the random\ncluster model of any matroid in the regime where $0<q<1$. Consequently, we can\nsample random spanning forests in a graph and (approximately) compute the\nreliability polynomial of any matroid. We also prove the thirty year old\nconjecture of Mihail and Vazirani that the bases exchange graph of any matroid\nhas expansion at least 1. One of our key observations is a close connection\nbetween pure simplicial complexes and multiaffine homogeneous polynomials.\nSpecifically, if $X$ is a pure simplicial complex with positive weights on its\nmaximal faces, we can associate with $X$ a multiaffine homogeneous polynomial\n$p_{X}$ such that the eigenvalues of the localized random walks on $X$\ncorrespond to the eigenvalues of the Hessian of derivatives of $p_{X}$.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 15:56:47 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 18:53:18 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2019 20:54:55 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Anari", "Nima", ""], ["Liu", "Kuikui", ""], ["Gharan", "Shayan Oveis", ""], ["Vinzant", "Cynthia", ""]]}, {"id": "1811.01885", "submitter": "Ainesh Bakshi", "authors": "Ainesh Bakshi, Rajesh Jayaram and David P. Woodruff", "title": "Learning Two Layer Rectified Neural Networks in Polynomial Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following fundamental learning problem: given input examples $x\n\\in \\mathbb{R}^d$ and their vector-valued labels, as defined by an underlying\ngenerative neural network, recover the weight matrices of this network. We\nconsider two-layer networks, mapping $\\mathbb{R}^d$ to $\\mathbb{R}^m$, with $k$\nnon-linear activation units $f(\\cdot)$, where $f(x) = \\max \\{x , 0\\}$ is the\nReLU. Such a network is specified by two weight matrices, $\\mathbf{U}^* \\in\n\\mathbb{R}^{m \\times k}, \\mathbf{V}^* \\in \\mathbb{R}^{k \\times d}$, such that\nthe label of an example $x \\in \\mathbb{R}^{d}$ is given by $\\mathbf{U}^*\nf(\\mathbf{V}^* x)$, where $f(\\cdot)$ is applied coordinate-wise. Given $n$\nsamples as a matrix $\\mathbf{X} \\in \\mathbb{R}^{d \\times n}$ and the (possibly\nnoisy) labels $\\mathbf{U}^* f(\\mathbf{V}^* \\mathbf{X}) + \\mathbf{E}$ of the\nnetwork on these samples, where $\\mathbf{E}$ is a noise matrix, our goal is to\nrecover the weight matrices $\\mathbf{U}^*$ and $\\mathbf{V}^*$.\n  In this work, we develop algorithms and hardness results under varying\nassumptions on the input and noise. Although the problem is NP-hard even for\n$k=2$, by assuming Gaussian marginals over the input $\\mathbf{X}$ we are able\nto develop polynomial time algorithms for the approximate recovery of\n$\\mathbf{U}^*$ and $\\mathbf{V}^*$. Perhaps surprisingly, in the noiseless case\nour algorithms recover $\\mathbf{U}^*,\\mathbf{V}^*$ exactly, i.e., with no\nerror. To the best of the our knowledge, this is the first algorithm to\naccomplish exact recovery. For the noisy case, we give the first polynomial\ntime algorithm that approximately recovers the weights in the presence of\nmean-zero noise $\\mathbf{E}$. Our algorithms generalize to a larger class of\nrectified activation functions, $f(x) = 0$ when $x\\leq 0$, and $f(x) > 0$\notherwise.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 18:03:56 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Bakshi", "Ainesh", ""], ["Jayaram", "Rajesh", ""], ["Woodruff", "David P.", ""]]}, {"id": "1811.01903", "submitter": "Jelena Diakonikolas", "authors": "Jelena Diakonikolas and Crist\\'obal Guzm\\'an", "title": "Lower Bounds for Parallel and Randomized Convex Optimization", "comments": "In Proc. COLT'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question of whether parallelization in the exploration of the\nfeasible set can be used to speed up convex optimization, in the local oracle\nmodel of computation. We show that the answer is negative for both\ndeterministic and randomized algorithms applied to essentially any of the\ninteresting geometries and nonsmooth, weakly-smooth, or smooth objective\nfunctions. In particular, we show that it is not possible to obtain a\npolylogarithmic (in the sequential complexity of the problem) number of\nparallel rounds with a polynomial (in the dimension) number of queries per\nround. In the majority of these settings and when the dimension of the space is\npolynomial in the inverse target accuracy, our lower bounds match the oracle\ncomplexity of sequential convex optimization, up to at most a logarithmic\nfactor in the dimension, which makes them (nearly) tight. Prior to our work,\nlower bounds for parallel convex optimization algorithms were only known in a\nsmall fraction of the settings considered in this paper, mainly applying to\nEuclidean ($\\ell_2$) and $\\ell_\\infty$ spaces. Our work provides a more general\napproach for proving lower bounds in the setting of parallel convex\noptimization.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 18:32:06 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 14:29:38 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2019 21:41:52 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Diakonikolas", "Jelena", ""], ["Guzm\u00e1n", "Crist\u00f3bal", ""]]}, {"id": "1811.01997", "submitter": "Ami Paz", "authors": "Keren Censor-Hillel, Ami Paz, Noam Ravid", "title": "The Sparsest Additive Spanner via Multiple Weighted BFS Trees", "comments": "Preliminary versions appeared in OPODIS 2018 conference and in TCS\n  journal", "journal-ref": null, "doi": "10.1016/j.tcs.2020.05.035", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spanners are fundamental graph structures that sparsify graphs at the cost of\nsmall stretch. In particular, in recent years, many sequential algorithms\nconstructing additive all-pairs spanners were designed, providing very sparse\nsmall-stretch subgraphs. Remarkably, it was then shown that the known\n(+6)-spanner constructions are essentially the sparsest possible, that is, a\nlarger additive stretch cannot guarantee a sparser spanner, which brought the\nstretch-sparsity trade-off to its limit. Distributed constructions of spanners\nare also abundant. However, for additive spanners, while there were algorithms\nconstructing (+2) and (+4)-all-pairs spanners, the sparsest case of\n(+6)-spanners remained elusive.\n  We remedy this by designing a new sequential algorithm for constructing a\n(+6)-spanner with the essentially-optimal sparsity of roughly O(n^{4/3}) edges.\nWe then show a distributed implementation of our algorithm, answering an open\nproblem in [Censor-Hillel et al., DISC 2016].\n  A main ingredient in our distributed algorithm is an efficient construction\nof multiple weighted BFS trees. A weighted BFS tree is a BFS tree in a weighted\ngraph, that consists of the lightest among all shortest paths from the root to\neach node. We present a distributed algorithm in the CONGEST model, that\nconstructs multiple weighted BFS trees in |S|+D-1 rounds, where S is the set of\nsources and D is the diameter of the network graph.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 19:44:50 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 14:20:52 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Paz", "Ami", ""], ["Ravid", "Noam", ""]]}, {"id": "1811.02009", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Aaron Bernstein", "title": "Towards a Unified Theory of Sparsification for Matching Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a construction of a `matching sparsifier', that is,\na sparse subgraph of the given graph that preserves large matchings\napproximately and is robust to modifications of the graph. We use this matching\nsparsifier to obtain several new algorithmic results for the maximum matching\nproblem:\n  * An almost $(3/2)$-approximation one-way communication protocol for the\nmaximum matching problem, significantly simplifying the $(3/2)$-approximation\nprotocol of Goel, Kapralov, and Khanna (SODA 2012) and extending it from\nbipartite graphs to general graphs.\n  * An almost $(3/2)$-approximation algorithm for the stochastic matching\nproblem, improving upon and significantly simplifying the previous\n$1.999$-approximation algorithm of Assadi, Khanna, and Li (EC 2017).\n  * An almost $(3/2)$-approximation algorithm for the fault-tolerant matching\nproblem, which, to our knowledge, is the first non-trivial algorithm for this\nproblem.\n  Our matching sparsifier is obtained by proving new properties of the\nedge-degree constrained subgraph (EDCS) of Bernstein and Stein (ICALP 2015;\nSODA 2016)---designed in the context of maintaining matchings in dynamic\ngraphs---that identifies EDCS as an excellent choice for a matching sparsifier.\nThis leads to surprisingly simple and non-technical proofs of the above results\nin a unified way. Along the way, we also provide a much simpler proof of the\nfact that an EDCS is guaranteed to contain a large matching, which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 19:57:33 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 12:26:30 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Assadi", "Sepehr", ""], ["Bernstein", "Aaron", ""]]}, {"id": "1811.02023", "submitter": "Omri Ben-Eliezer", "authors": "Omri Ben-Eliezer, Eldar Fischer, Amit Levi, Yuichi Yoshida", "title": "Limits of Ordered Graphs and their Applications", "comments": "Added a new application: An Alon-Stav type result on the furthest\n  ordered graph from a hereditary property; Fixed and extended proof sketch of\n  the removal lemma application", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging theory of graph limits exhibits an analytic perspective on\ngraphs, showing that many important concepts and tools in graph theory and its\napplications can be described more naturally (and sometimes proved more easily)\nin analytic language. We extend the theory of graph limits to the ordered\nsetting, presenting a limit object for dense vertex-ordered graphs, which we\ncall an \\emph{orderon}. As a special case, this yields limit objects for\nmatrices whose rows and columns are ordered, and for dynamic graphs that expand\n(via vertex insertions) over time. Along the way, we devise an ordered\nlocality-preserving variant of the cut distance between ordered graphs, showing\nthat two graphs are close with respect to this distance if and only if they are\nsimilar in terms of their ordered subgraph frequencies. We show that the space\nof orderons is compact with respect to this distance notion, which is key to a\nsuccessful analysis of combinatorial objects through their limits.\n  We derive several applications of the ordered limit theory in extremal\ncombinatorics, sampling, and property testing in ordered graphs. In particular,\nwe prove a new ordered analogue of the well-known result by Alon and Stav\n[RS\\&A'08] on the furthest graph from a hereditary property; this is the first\nknown result of this type in the ordered setting. Unlike the unordered regime,\nhere the random graph model $G(n, p)$ with an ordering over the vertices is\n\\emph{not} always asymptotically the furthest from the property for some $p$.\nHowever, using our ordered limit theory, we show that random graphs generated\nby a stochastic block model, where the blocks are consecutive in the vertex\nordering, are (approximately) the furthest. Additionally, we describe an\nalternative analytic proof of the ordered graph removal lemma [Alon et al.,\nFOCS'17].\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 20:33:19 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 17:24:22 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Ben-Eliezer", "Omri", ""], ["Fischer", "Eldar", ""], ["Levi", "Amit", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1811.02043", "submitter": "Timon Knigge", "authors": "Timon E. Knigge, Rob H. Bisseling", "title": "An improved exact algorithm and an NP-completeness proof for sparse\n  matrix bipartitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate sparse matrix bipartitioning -- a problem where we minimize\nthe communication volume in parallel sparse matrix-vector multiplication. We\nprove, by reduction from graph bisection, that this problem is\n$\\mathcal{NP}$-complete in the case where each side of the bipartitioning must\ncontain a linear fraction of the nonzeros.\n  We present an improved exact branch-and-bound algorithm which finds the\nminimum communication volume for a given matrix and maximum allowed imbalance.\nThe algorithm is based on a maximum-flow bound and a packing bound, which\nextend previous matching and packing bounds.\n  We implemented the algorithm in a new program called MP (Matrix Partitioner),\nwhich solved 839 matrices from the SuiteSparse collection to optimality, each\nwithin 24 hours of CPU-time. Furthermore, MP solved the difficult problem of\nthe matrix cage6 in about 3 days. The new program is on average more than ten\ntimes faster than the previous program MondriaanOpt.\n  Benchmark results using the set of 839 optimally solved matrices show that\ncombining the medium-grain/iterative refinement methods of the Mondriaan\npackage with the hypergraph bipartitioner of the PaToH package produces sparse\nmatrix bipartitionings on average within 10% of the optimal solution.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 21:38:11 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 08:38:02 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Knigge", "Timon E.", ""], ["Bisseling", "Rob H.", ""]]}, {"id": "1811.02078", "submitter": "Huacheng Yu", "authors": "Huacheng Yu", "title": "Optimal Succinct Rank Data Structure via Approximate Nonnegative Tensor\n  Decomposition", "comments": "A preliminary version of this paper will appear in STOC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an $n$-bit array $A$, the succinct rank data structure problem asks to\nconstruct a data structure using space $n+r$ bits for $r\\ll n$, supporting rank\nqueries of form $\\mathtt{rank}(x)=\\sum_{i=0}^{x-1} A[i]$. In this paper, we\ndesign a new succinct rank data structure with $r=n/(\\log\nn)^{\\Omega(t)}+n^{1-c}$ and query time $O(t)$ for some constant $c>0$,\nimproving the previous best-known by Patrascu [Pat08], which has\n$r=n/(\\frac{\\log n}{t})^{\\Omega(t)}+\\tilde{O}(n^{3/4})$ bits of redundancy. For\n$r>n^{1-c}$, our space-time tradeoff matches the cell-probe lower bound by\nPatrascu and Viola [PV10], which asserts that $r$ must be at least $n/(\\log\nn)^{O(t)}$. Moreover, one can avoid an $n^{1-c}$-bit lookup table when the data\nstructure is implemented in the cell-probe model, achieving $r=\\lceil n/(\\log\nn)^{\\Omega(t)}\\rceil$. It matches the lower bound for the full range of\nparameters.\n  En route to our new data structure design, we establish an interesting\nconnection between succinct data structures and approximate nonnegative tensor\ndecomposition. Our connection shows that for specific problems, to construct a\nspace-efficient data structure, it suffices to approximate a particular tensor\nby a sum of (few) nonnegative rank-$1$ tensors. For the rank problem, we\nexplicitly construct such an approximation, which yields an explicit\nconstruction of the data structure.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 23:05:04 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 02:47:30 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Yu", "Huacheng", ""]]}, {"id": "1811.02089", "submitter": "Pan Li", "authors": "Pan Li, Gregory J. Puleo, Olgica Milenkovic", "title": "Motif and Hypergraph Correlation Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in social and biological network analysis, we\nintroduce a new form of agnostic clustering termed~\\emph{motif correlation\nclustering}, which aims to minimize the cost of clustering errors associated\nwith both edges and higher-order network structures. The problem may be\nsuccinctly described as follows: Given a complete graph $G$, partition the\nvertices of the graph so that certain predetermined `important' subgraphs\nmostly lie within the same cluster, while `less relevant' subgraphs are allowed\nto lie across clusters. Our contributions are as follows: We first introduce\nseveral variants of motif correlation clustering and then show that these\nclustering problems are NP-hard. We then proceed to describe polynomial-time\nclustering algorithms that provide constant approximation guarantees for the\nproblems at hand. Despite following the frequently used LP relaxation and\nrounding procedure, the algorithms involve a sophisticated and carefully\ndesigned neighborhood growing step that combines information about both edge\nand motif structures. We conclude with several examples illustrating the\nperformance of the developed algorithms on synthetic and real networks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 23:40:03 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Li", "Pan", ""], ["Puleo", "Gregory J.", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1811.02177", "submitter": "Yuval Dagan", "authors": "Yuval Dagan, Yuval Filmus, Daniel Kane, Shay Moran", "title": "The entropy of lies: playing twenty questions with a liar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  `Twenty questions' is a guessing game played by two players: Bob thinks of an\ninteger between $1$ and $n$, and Alice's goal is to recover it using a minimal\nnumber of Yes/No questions. Shannon's entropy has a natural interpretation in\nthis context. It characterizes the average number of questions used by an\noptimal strategy in the distributional variant of the game: let $\\mu$ be a\ndistribution over $[n]$, then the average number of questions used by an\noptimal strategy that recovers $x\\sim \\mu$ is between $H(\\mu)$ and $H(\\mu)+1$.\nWe consider an extension of this game where at most $k$ questions can be\nanswered falsely. We extend the classical result by showing that an optimal\nstrategy uses roughly $H(\\mu) + k H_2(\\mu)$ questions, where $H_2(\\mu) = \\sum_x\n\\mu(x)\\log\\log\\frac{1}{\\mu(x)}$. This also generalizes a result by Rivest et\nal. for the uniform distribution. Moreover, we design near optimal strategies\nthat only use comparison queries of the form `$x \\leq c$?' for $c\\in[n]$. The\nusage of comparison queries lends itself naturally to the context of sorting,\nwhere we derive sorting algorithms in the presence of adversarial noise.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 06:05:14 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Dagan", "Yuval", ""], ["Filmus", "Yuval", ""], ["Kane", "Daniel", ""], ["Moran", "Shay", ""]]}, {"id": "1811.02259", "submitter": "Frank Gurski", "authors": "Frank Gurski and Carolin Rehs and Jochen Rethmann", "title": "Characterizations and Directed Path-Width of Sequence Digraphs", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the directed path-width of a directed graph is an NP-hard problem.\nEven for digraphs of maximum semi-degree 3 the problem remains hard. We propose\na decomposition of an input digraph G=(V,A) by a number k of sequences with\nentries from V, such that (u,v) in A if and only if in one of the sequences\nthere is an occurrence of u appearing before an occurrence of v. We present\nseveral graph theoretical properties of these digraphs. Among these we give\nforbidden subdigraphs of digraphs which can be defined by k=1 sequence, which\nis a subclass of semicomplete digraphs. Given the decomposition of digraph G,\nwe show an algorithm which computes the directed path-width of G in time\nO(k\\cdot (1+N)^k), where N denotes the maximum sequence length. This leads to\nan XP-algorithm w.r.t. k for the directed path-width problem. Our result\nimproves the algorithms of Kitsunai et al. for digraphs of large directed\npath-width which can be decomposed by a small number of sequence.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 09:50:49 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Gurski", "Frank", ""], ["Rehs", "Carolin", ""], ["Rethmann", "Jochen", ""]]}, {"id": "1811.02457", "submitter": "Travis Gagie", "authors": "Jarno Alanko, Travis Gagie, Gonzalo Navarro and Louisa Seelbach\n  Benkner", "title": "Tunneling on Wheeler Graphs", "comments": "11 Pages, 1 figure. This research has received funding from the\n  European Union's Horizon 2020 research and innovation programme under the\n  Marie Sk{\\l}odowska-Curie Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Burrows-Wheeler Transform (BWT) is an important technique both in data\ncompression and in the design of compact indexing data structures. It has been\ngeneralized from single strings to collections of strings and some classes of\nlabeled directed graphs, such as tries and de Bruijn graphs. The BWTs of\nrepetitive datasets are often compressible using run-length compression, but\nrecently Baier (CPM 2018) described how they could be even further compressed\nusing an idea he called tunneling. In this paper we show that tunneled BWTs can\nstill be used for indexing and extend tunneling to the BWTs of Wheeler graphs,\na framework that includes all the generalizations mentioned above.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 16:08:32 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 12:39:32 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Alanko", "Jarno", ""], ["Gagie", "Travis", ""], ["Navarro", "Gonzalo", ""], ["Benkner", "Louisa Seelbach", ""]]}, {"id": "1811.02527", "submitter": "Ran Gelles", "authors": "Ran Gelles, Siddharth Iyer", "title": "Interactive coding resilient to an unknown number of erasures", "comments": "28 pages; small changes and corrections from previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider distributed computations between two parties carried out over a\nnoisy channel that may erase messages. Following a noise model proposed by Dani\net al. (2018), the noise level observed by the parties during the computation\nin our setting is arbitrary and a priori unknown to the parties.\n  We develop interactive coding schemes that adapt to the actual level of noise\nand correctly execute any two-party computation. Namely, in case the channel\nerases $T$ transmissions, the coding scheme will take $N+2T$ transmissions\nusing an alphabet of size $4$ (alternatively, using $2N+4T$ transmissions over\na binary channel) to correctly simulate any binary protocol that takes $N$\ntransmissions assuming a noiseless channel. We can further reduce the\ncommunication to $N+T$ by relaxing the communication model and allowing parties\nto remain silent rather than forcing them to communicate in every round of the\ncoding scheme.\n  Our coding schemes are efficient, deterministic, have linear overhead both in\ntheir communication and round complexity, and succeed (with probability 1)\nregardless of the number of erasures $T$.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 17:52:58 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 14:26:38 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Gelles", "Ran", ""], ["Iyer", "Siddharth", ""]]}, {"id": "1811.02599", "submitter": "Mehdi Khosravian Ghadikolaei", "authors": "Kaveh Khoshkhah, Mehdi Khosravian Ghadikolaei, Jerome Monnot and\n  Florian Sikora", "title": "Weighted Upper Edge Cover: Complexity and Approximability", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization problems consist of either maximizing or minimizing an objective\nfunction. Instead of looking for a maximum solution (resp. minimum solution),\none can find a minimum maximal solution (resp. maximum minimal solution). Such\n\"flipping\" of the objective function was done for many classical optimization\nproblems. For example, Minimum Vertex Cover becomes Maximum Minimal Vertex\nCover, Maximum Independent Set becomes Minimum Maximal Independent Set and so\non. In this paper, we propose to study the weighted version of Maximum Minimal\nEdge Cover called Upper Edge Cover, a problem having application in the genomic\nsequence alignment. It is well-known that Minimum Edge Cover is polynomial-time\nsolvable and the \"flipped\" version is NP-hard, but constant approximable. We\nshow that the weighted Upper Edge Cover is much more difficult than Upper Edge\nCover because it is not $O(\\frac{1}{n^{1/2-\\varepsilon}})$ approximable, nor\n$O(\\frac{1}{\\Delta^{1-\\varepsilon}})$ in edge-weighted graphs of size $n$ and\nmaximum degree $\\Delta$ respectively. Indeed, we give some hardness of\napproximation results for some special restricted graph classes such as\nbipartite graphs, split graphs and $k$-trees. We counter-balance these negative\nresults by giving some positive approximation results in specific graph\nclasses.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 19:13:00 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Khoshkhah", "Kaveh", ""], ["Ghadikolaei", "Mehdi Khosravian", ""], ["Monnot", "Jerome", ""], ["Sikora", "Florian", ""]]}, {"id": "1811.02676", "submitter": "Avah Banerjee", "authors": "Avah Banerjee and Dana Richards", "title": "Oblivious Set-maxima for Intersection of Convex Polygons", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit the well known set-maxima problem in the oblivious\nsetting. Let $X=\\{x_1,\\ldots, x_n\\}$ be a set of $n$ elements with an\nunderlying total order. Let $\\mathcal{S}=\\{S_1,\\ldots,S_m\\}$ be a collection of\n$m$ distinct subsets of $X$. The set-maxima problem asks to determine the\nmaxima of all the sets in the collection. In the comparison tree model we are\ninterested in determining the number of comparisons necessary and sufficient to\nsolve the problem. We present an oblivious algorithm based on the lattice\nstructure of the input set system. Our algorithm is simple and yet for many set\nsystems gives a non-trivial improvement over known deterministic algorithms. We\napply our algorithm to a special $\\cal S$ which is determined by an\nintersection structure of convex polygons and show that $O(n)$ comparisons\nsuffice.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 21:52:58 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 21:57:32 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Banerjee", "Avah", ""], ["Richards", "Dana", ""]]}, {"id": "1811.02685", "submitter": "Havana Rika", "authors": "Robert Krauthgamer, James R. Lee, Havana Rika", "title": "Flow-Cut Gaps and Face Covers in Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relationship between the sparsest cut and the maximum concurrent\nmulti-flow in graphs has been studied extensively. For general graphs with $k$\nterminal pairs, the flow-cut gap is $O(\\log k)$, and this is tight. But when\ntopological restrictions are placed on the flow network, the situation is far\nless clear. In particular, it has been conjectured that the flow-cut gap in\nplanar networks is $O(1)$, while the known bounds place the gap somewhere\nbetween $2$ (Lee and Raghavendra, 2003) and $O(\\sqrt{\\log k})$ (Rao, 1999).\n  A seminal result of Okamura and Seymour (1981) shows that when all the\nterminals of a planar network lie on a single face, the flow-cut gap is exactly\n$1$. This setting can be generalized by considering planar networks where the\nterminals lie on $\\gamma>1$ faces in some fixed planar drawing. Lee and\nSidiropoulos (2009) proved that the flow-cut gap is bounded by a function of\n$\\gamma$, and Chekuri, Shepherd, and Weibel (2013) showed that the gap is at\nmost $3\\gamma$. We prove that the flow-cut gap is $O(\\log\\gamma)$, by showing\nthat the edge-weighted shortest-path metric induced on the terminals admits a\nstochastic embedding into trees with distortion $O(\\log\\gamma)$, which is\ntight.\n  The preceding results refer to the setting of edge-capacitated networks. For\nvertex-capacitated networks, it can be significantly more challenging to\ncontrol flow-cut gaps. While there is no exact vertex-capacitated version of\nthe Okamura-Seymour Theorem, an approximate version holds; Lee, Mendel, and\nMoharrami (2015) showed that the vertex-capacitated flow-cut gap is $O(1)$ on\nplanar networks whose terminals lie on a single face. We prove that the\nflow-cut gap is $O(\\gamma)$ for vertex-capacitated instances when the terminals\nlie on at most $\\gamma$ faces. In fact, this result holds in the more general\nsetting of submodular vertex capacities.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 22:04:28 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Krauthgamer", "Robert", ""], ["Lee", "James R.", ""], ["Rika", "Havana", ""]]}, {"id": "1811.02725", "submitter": "Alexander Golovnev", "authors": "Zeev Dvir, Alexander Golovnev, Omri Weinstein", "title": "Static Data Structure Lower Bounds Imply Rigidity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that static data structure lower bounds in the group (linear) model\nimply semi-explicit lower bounds on matrix rigidity. In particular, we prove\nthat an explicit lower bound of $t \\geq \\omega(\\log^2 n)$ on the cell-probe\ncomplexity of linear data structures in the group model, even against\narbitrarily small linear space $(s= (1+\\varepsilon)n)$, would already imply a\nsemi-explicit ($\\bf P^{NP}\\rm$) construction of rigid matrices with\nsignificantly better parameters than the current state of art (Alon, Panigrahy\nand Yekhanin, 2009). Our results further assert that polynomial ($t\\geq\nn^{\\delta}$) data structure lower bounds against near-optimal space, would\nimply super-linear circuit lower bounds for log-depth linear circuits (a\nfour-decade open question). In the succinct space regime $(s=n+o(n))$, we show\nthat any improvement on current cell-probe lower bounds in the linear model\nwould also imply new rigidity bounds. Our results rely on a new connection\nbetween the \"inner\" and \"outer\" dimensions of a matrix (Paturi and Pudlak,\n2006), and on a new reduction from worst-case to average-case rigidity, which\nis of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 02:11:39 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 19:38:23 GMT"}, {"version": "v3", "created": "Wed, 13 Feb 2019 23:40:39 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Dvir", "Zeev", ""], ["Golovnev", "Alexander", ""], ["Weinstein", "Omri", ""]]}, {"id": "1811.02760", "submitter": "Slobodan Mitrovi\\'c", "authors": "Buddhima Gamlath, Sagar Kale, Slobodan Mitrovi\\'c, Ola Svensson", "title": "Weighted Matchings via Unweighted Augmentations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a generic method for reducing the task of finding weighted\nmatchings to that of finding short augmenting paths in unweighted graphs. This\nmethod enables us to provide efficient implementations for approximating\nweighted matchings in the streaming model and in the massively parallel\ncomputation (MPC) model.\n  In the context of streaming with random edge arrivals, our techniques yield a\n$(1/2+c)$-approximation algorithm thus breaking the natural barrier of $1/2$.\nFor multi-pass streaming and the MPC model, we show that any algorithm\ncomputing a $(1-\\delta)$-approximate unweighted matching in bipartite graphs\ncan be translated into an algorithm that computes a\n$(1-\\varepsilon(\\delta))$-approximate maximum weighted matching. Furthermore,\nthis translation incurs only a constant factor (that depends on $\\varepsilon>\n0$) overhead in the complexity. Instantiating this with the current best\nmulti-pass streaming and MPC algorithms for unweighted matchings yields the\nfollowing results for maximum weighted matchings:\n  * A $(1-\\varepsilon)$-approximation streaming algorithm that uses\n$O_\\varepsilon(1)$ passes and $O_\\varepsilon(n\\, \\text{poly} (\\log n))$ memory.\nThis is the first $(1-\\varepsilon)$-approximation streaming algorithm for\nweighted matchings that uses a constant number of passes (only depending on\n$\\varepsilon$).\n  * A $(1 - \\varepsilon)$-approximation algorithm in the MPC model that uses\n$O_\\varepsilon(\\log \\log n)$ rounds, $O(m/n)$ machines per round, and\n$O_\\varepsilon(n\\, \\text{poly}(\\log n))$ memory per machine. This improves upon\nthe previous best approximation guarantee of $(1/2-\\varepsilon)$ for weighted\ngraphs.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 04:48:57 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Gamlath", "Buddhima", ""], ["Kale", "Sagar", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Svensson", "Ola", ""]]}, {"id": "1811.02822", "submitter": "Rosario Scatamacchia", "authors": "Federico Della Croce, Rosario Scatamacchia", "title": "A new exact approach for the Bilevel Knapsack with Interdiction\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Bilevel Knapsack with Interdiction Constraints, an extension\nof the classic 0-1 knapsack problem formulated as a Stackelberg game with two\nagents, a leader and a follower, that choose items from a common set and hold\ntheir own private knapsacks. First, the leader selects some items to be\ninterdicted for the follower while satisfying a capacity constraint. Then the\nfollower packs a set of the remaining items according to his knapsack\nconstraint in order to maximize the profits. The goal of the leader is to\nminimize the follower's profits. The presence of two decision levels makes this\nproblem very difficult to solve in practice: the current state-of-the-art\nalgorithms can solve to optimality instances with 50-55 items at most. We\nderive effective lower bounds and present a new exact approach that exploits\nthe structure of the induced follower's problem. The approach successfully\nsolves all benchmark instances within one second in the worst case and larger\ninstances with up to 500 items within 60 seconds.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 10:39:13 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 08:17:58 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Della Croce", "Federico", ""], ["Scatamacchia", "Rosario", ""]]}, {"id": "1811.02933", "submitter": "Nima Anari", "authors": "Nima Anari and Alireza Rezaei", "title": "A Tight Analysis of Bethe Approximation for Permanent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT math-ph math.CO math.IT math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the permanent of nonnegative matrices can be deterministically\napproximated within a factor of $\\sqrt{2}^n$ in polynomial time, improving upon\nthe previous deterministic approximations. We show this by proving that the\nBethe approximation of the permanent, a quantity computable in polynomial time,\nis at least as large as the permanent divided by $\\sqrt{2}^{n}$. This resolves\na conjecture of Gurvits. Our bound is tight, and when combined with previously\nknown inequalities lower bounding the permanent, fully resolves the quality of\nBethe approximation for permanent. As an additional corollary of our methods,\nwe resolve a conjecture of Chertkov and Yedidia, proving that fractional belief\npropagation with fractional parameter $\\gamma=-1/2$ yields an upper bound on\nthe permanent.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 15:28:26 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 23:27:48 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Anari", "Nima", ""], ["Rezaei", "Alireza", ""]]}, {"id": "1811.02937", "submitter": "Hendrik Fichtenberger", "authors": "Hendrik Fichtenberger, Pan Peng, Christian Sohler", "title": "Every Testable (Infinite) Property of Bounded-Degree Graphs Contains an\n  Infinite Hyperfinite Subproperty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most fundamental questions in graph property testing is to\ncharacterize the combinatorial structure of properties that are testable with a\nconstant number of queries. We work towards an answer to this question for the\nbounded-degree graph model introduced in [Goldreich, Ron, 2002], where the\ninput graphs have maximum degree bounded by a constant $d$. In this model, it\nis known (among other results) that every \\emph{hyperfinite} property is\nconstant-query testable [Newman, Sohler, 2013], where, informally, a graph\nproperty is hyperfinite, if for every $\\delta >0$ every graph in the property\ncan be partitioned into small connected components by removing $\\delta n$\nedges.\n  In this paper we show that hyperfiniteness plays a role in \\emph{every}\ntestable property, i.e. we show that every testable property is either finite\n(which trivially implies hyperfiniteness and testability) or contains an\ninfinite hyperfinite subproperty. A simple consequence of our result is that no\ninfinite graph property that only consists of expander graphs is constant-query\ntestable.\n  Based on the above findings, one could ask if every infinite testable\nnon-hyperfinite property might contain an infinite family of expander (or\nnear-expander) graphs. We show that this is not true. Motivated by our\ncounter-example we develop a theorem that shows that we can partition the set\nof vertices of every bounded degree graph into a constant number of subsets and\na separator set, such that the separator set is small and the distribution of\n$k$-disks on every subset of a partition class, is roughly the same as that of\nthe partition class if the subset has small expansion.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 15:34:41 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Fichtenberger", "Hendrik", ""], ["Peng", "Pan", ""], ["Sohler", "Christian", ""]]}, {"id": "1811.02944", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Florent Capelli, Mika\\\"el Monet, Pierre Senellart", "title": "Connecting Knowledge Compilation Classes and Width Parameters", "comments": "46 pages. Extended version of arXiv:1709.06188. Up to the stylesheet,\n  page/environment numbering, minor formatting, and publisher-induced changes,\n  this is the exact content of the paper in Theory of Computing Systems\n  <https://link.springer.com/article/10.1007%2Fs00224-019-09930-2>. The\n  difference in the titles (missing \"and\") is an error introduced by the\n  publisher", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of knowledge compilation establishes the tractability of many tasks\nby studying how to compile them to Boolean circuit classes obeying some\nrequirements such as structuredness, decomposability, and determinism. However,\nin other settings such as intensional query evaluation on databases, we obtain\nBoolean circuits that satisfy some width bounds, e.g., they have bounded\ntreewidth or pathwidth. In this work, we give a systematic picture of many\ncircuit classes considered in knowledge compilation and show how they can be\nsystematically connected to width measures, through upper and lower bounds. Our\nupper bounds show that bounded-treewidth circuits can be constructively\nconverted to d-SDNNFs, in time linear in the circuit size and singly\nexponential in the treewidth; and that bounded-pathwidth circuits can similarly\nbe converted to uOBDDs. We show matching lower bounds on the compilation of\nmonotone DNF or CNF formulas to structured targets, assuming a constant bound\non the arity (size of clauses) and degree (number of occurrences of each\nvariable): any d-SDNNF (resp., SDNNF) for such a DNF (resp., CNF) must be of\nexponential size in its treewidth, and the same holds for uOBDDs (resp.,\nn-OBDDs) when considering pathwidth. Unlike most previous work, our bounds\napply to any formula of this class, not just a well-chosen family. Hence, we\nshow that pathwidth and treewidth respectively characterize the efficiency of\ncompiling monotone DNFs to uOBDDs and d-SDNNFs with compilation being singly\nexponential in the corresponding width parameter. We also show that our lower\nbounds on CNFs extend to unstructured compilation targets, with an exponential\nlower bound in the treewidth (resp., pathwidth) when compiling monotone CNFs of\nconstant arity and degree to DNNFs (resp., nFBDDs).\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 15:45:43 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 09:13:04 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Amarilli", "Antoine", ""], ["Capelli", "Florent", ""], ["Monet", "Mika\u00ebl", ""], ["Senellart", "Pierre", ""]]}, {"id": "1811.03019", "submitter": "Rajendra Kumar", "authors": "Shashank K Mehta, Mahesh Sreekumar Rajasree and Rajendra Kumar", "title": "Maximum Distance Sub-Lattice Problem", "comments": "17 pages, No figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we define a problem on lattices called the Maximum Distance\nSub-lattice Problem (MDSP). The decision version of this problem is shown to be\nin NP. We prove that MDSP is isomorphic to a well-known problem called closest\nvector problem (CVP). We give an exact and a heuristic algorithm for MDSP.\nUsing experimental results we show that the LLL algorithm can be accelerated\nwhen it is combined with the heuristic algorithm for MDSP.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 17:12:02 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Mehta", "Shashank K", ""], ["Rajasree", "Mahesh Sreekumar", ""], ["Kumar", "Rajendra", ""]]}, {"id": "1811.03020", "submitter": "Shi Li", "authors": "Fabrizio Grandoni, Bundit Laekhanukit, Shi Li", "title": "$O(\\log^2k/\\log\\log{k})$-Approximation Algorithm for Directed Steiner\n  Tree: A Tight Quasi-Polynomial-Time Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Directed Steiner Tree (DST) problem we are given an $n$-vertex\ndirected edge-weighted graph, a root $r$, and a collection of $k$ terminal\nnodes. Our goal is to find a minimum-cost arborescence that contains a directed\npath from $r$ to every terminal. We present an $O(\\log^2\nk/\\log\\log{k})$-approximation algorithm for DST that runs in\nquasi-polynomial-time. By adjusting the parameters in the hardness result of\nHalperin and Krauthgamer, we show the matching lower bound of\n$\\Omega(\\log^2{k}/\\log\\log{k})$ for the class of quasi-polynomial-time\nalgorithms. This is the first improvement on the DST problem since the\nclassical quasi-polynomial-time $O(\\log^3 k)$ approximation algorithm by\nCharikar et al. (The paper erroneously claims an $O(\\log^2k)$ approximation due\nto a mistake in prior work.)\n  Our approach is based on two main ingredients. First, we derive an\napproximation preserving reduction to the Label-Consistent Subtree (LCST)\nproblem. The LCST instance has quasi-polynomial size and logarithmic height. We\nremark that, in contrast, Zelikovsky's heigh-reduction theorem used in all\nprior work on DST achieves a reduction to a tree instance of the related Group\nSteiner Tree (GST) problem of similar height, however losing a logarithmic\nfactor in the approximation ratio. Our second ingredient is an LP-rounding\nalgorithm to approximately solve LCST instances, which is inspired by the\nframework developed by Rothvo{\\ss}. We consider a Sherali-Adams lifting of a\nproper LP relaxation of LCST. Our rounding algorithm proceeds level by level\nfrom the root to the leaves, rounding and conditioning each time on a proper\nsubset of label variables. A small enough (namely, polylogarithmic) number of\nSherali-Adams lifting levels is sufficient to condition up to the leaves.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 17:12:46 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Grandoni", "Fabrizio", ""], ["Laekhanukit", "Bundit", ""], ["Li", "Shi", ""]]}, {"id": "1811.03031", "submitter": "Aleksandr Maksimenko", "authors": "Aleksandr Maksimenko", "title": "Branch and bound algorithm for the traveling salesman problem is not a\n  direct type algorithm", "comments": "14 pages, in Russian, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the notion of a direct type algorithm introduced\nby V.A. Bondarenko in 1983. A direct type algorithm is a linear decision tree\nwith some special properties. Until recently, it was thought that the class of\ndirect type algorithms is wide and includes many classical combinatorial\nalgorithms, including the branch and bound algorithm for the traveling salesman\nproblem, proposed by J.D.C. Little, K.G. Murty, D.W. Sweeney, C. Karel in 1963.\nWe show that this algorithm is not a direct type algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 17:33:26 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Maksimenko", "Aleksandr", ""]]}, {"id": "1811.03093", "submitter": "Eric Balkanski", "authors": "Eric Balkanski, Aviad Rubinstein, Yaron Singer", "title": "An Optimal Approximation for Submodular Maximization under a Matroid\n  Constraint in the Adaptive Complexity Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study submodular maximization under a matroid constraint in\nthe adaptive complexity model. This model was recently introduced in the\ncontext of submodular optimization in [BS18a] to quantify the information\ntheoretic complexity of black-box optimization in a parallel computation model.\nInformally, the adaptivity of an algorithm is the number of sequential rounds\nit makes when each round can execute polynomially-many function evaluations in\nparallel. Since submodular optimization is regularly applied on large datasets\nwe seek algorithms with low adaptivity to enable speedups via parallelization.\nConsequently, a recent line of work has been devoted to designing constant\nfactor approximation algorithms for maximizing submodular functions under\nvarious constraints in the adaptive complexity model [BS18a, BS18b, BBS18,\nBRS19, EN19, FMZ19, CQ19, ENV18, FMZ18].\n  Despite the burst in work on submodular maximization in the adaptive\ncomplexity model the fundamental problem of maximizing a monotone submodular\nfunction under a matroid constraint has remained elusive. In particular, all\nknown techniques fail for this problem and there are no known constant factor\napproximation algorithms whose adaptivity is sublinear in the rank of the\nmatroid $k$ or in the worst case sublinear in the size of the ground set $n$.\n  In this paper we present an approximation algorithm for the problem of\nmaximizing a monotone submodular function under a matroid constraint in the\nadaptive complexity model. The approximation guarantee of the algorithm is\narbitrarily close to the optimal $1-1/e$ and it has near optimal adaptivity of\n$O(\\log(n)\\log(k))$. This result is obtained using a novel technique of\nadaptive sequencing which departs from previous techniques for submodular\nmaximization in the adaptive complexity model.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 18:20:03 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Balkanski", "Eric", ""], ["Rubinstein", "Aviad", ""], ["Singer", "Yaron", ""]]}, {"id": "1811.03158", "submitter": "Lin Chen", "authors": "Lin Chen, Lei Xu, Shouhuai Xu, Zhimin Gao, Weidong Shi", "title": "Election with Bribed Voter Uncertainty: Hardness and Approximation\n  Algorithm", "comments": "Accepted at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bribery in election (or computational social choice in general) is an\nimportant problem that has received a considerable amount of attention. In the\nclassic bribery problem, the briber (or attacker) bribes some voters in\nattempting to make the briber's designated candidate win an election. In this\npaper, we introduce a novel variant of the bribery problem, \"Election with\nBribed Voter Uncertainty\" or BVU for short, accommodating the uncertainty that\nthe vote of a bribed voter may or may not be counted. This uncertainty occurs\neither because a bribed voter may not cast its vote in fear of being caught, or\nbecause a bribed voter is indeed caught and therefore its vote is discarded. As\na first step towards ultimately understanding and addressing this important\nproblem, we show that it does not admit any multiplicative $O(1)$-approximation\nalgorithm modulo standard complexity assumptions. We further show that there is\nan approximation algorithm that returns a solution with an additive-$\\epsilon$\nerror in FPT time for any fixed $\\epsilon$.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 21:49:00 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Chen", "Lin", ""], ["Xu", "Lei", ""], ["Xu", "Shouhuai", ""], ["Gao", "Zhimin", ""], ["Shi", "Weidong", ""]]}, {"id": "1811.03195", "submitter": "Ilya Razenshteyn", "authors": "Konstantin Makarychev, Yury Makarychev, Ilya Razenshteyn", "title": "Performance of Johnson-Lindenstrauss Transform for k-Means and k-Medians\n  Clustering", "comments": "31 pages, an extended abstract appeared in the proceedings of STOC\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an instance of Euclidean $k$-means or $k$-medians clustering. We\nshow that the cost of the optimal solution is preserved up to a factor of\n$(1+\\varepsilon)$ under a projection onto a random $O(\\log(k / \\varepsilon) /\n\\varepsilon^2)$-dimensional subspace. Further, the cost of every clustering is\npreserved within $(1+\\varepsilon)$. More generally, our result applies to any\ndimension reduction map satisfying a mild sub-Gaussian-tail condition. Our\nbound on the dimension is nearly optimal. Additionally, our result applies to\nEuclidean $k$-clustering with the distances raised to the $p$-th power for any\nconstant $p$.\n  For $k$-means, our result resolves an open problem posed by Cohen, Elder,\nMusco, Musco, and Persu (STOC 2015); for $k$-medians, it answers a question\nraised by Kannan.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 00:24:23 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 23:48:43 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""], ["Razenshteyn", "Ilya", ""]]}, {"id": "1811.03197", "submitter": "Hassan Jameel Asghar", "authors": "Victor Perrier and Hassan Jameel Asghar and Dali Kaafar", "title": "Private Continual Release of Real-Valued Data Streams", "comments": "Accepted for publication at NDSS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a differentially private mechanism to display statistics (e.g.,\nthe moving average) of a stream of real valued observations where the bound on\neach observation is either too conservative or unknown in advance. This is\nparticularly relevant to scenarios of real-time data monitoring and reporting,\ne.g., energy data through smart meters. Our focus is on real-world data streams\nwhose distribution is light-tailed, meaning that the tail approaches zero at\nleast as fast as the exponential distribution. For such data streams,\nindividual observations are expected to be concentrated below an unknown\nthreshold. Estimating this threshold from the data can potentially violate\nprivacy as it would reveal particular events tied to individuals [1]. On the\nother hand an overly conservative threshold may impact accuracy by adding more\nnoise than necessary. We construct a utility optimizing differentially private\nmechanism to release this threshold based on the input stream. Our main\nadvantage over the state-of-the-art algorithms is that the resulting noise\nadded to each observation of the stream is scaled to the threshold instead of a\npossibly much larger bound; resulting in considerable gain in utility when the\ndifference is significant. Using two real-world datasets, we demonstrate that\nour mechanism, on average, improves the utility by a factor of 3.5 on the first\ndataset, and 9 on the other. While our main focus is on continual release of\nstatistics, our mechanism for releasing the threshold can be used in various\nother applications where a (privacy-preserving) measure of the scale of the\ninput distribution is required.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 00:26:33 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Perrier", "Victor", ""], ["Asghar", "Hassan Jameel", ""], ["Kaafar", "Dali", ""]]}, {"id": "1811.03204", "submitter": "Brian Axelrod", "authors": "Brian Axelrod, Gregory Valiant", "title": "An Efficient Algorithm for High-Dimensional Log-Concave Maximum\n  Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The log-concave maximum likelihood estimator (MLE) problem answers: for a set\nof points $X_1,...X_n \\in \\mathbb R^d$, which log-concave density maximizes\ntheir likelihood? We present a characterization of the log-concave MLE that\nleads to an algorithm with runtime $poly(n,d, \\frac 1 \\epsilon,r)$ to compute a\nlog-concave distribution whose log-likelihood is at most $\\epsilon$ less than\nthat of the MLE, and $r$ is parameter of the problem that is bounded by the\n$\\ell_2$ norm of the vector of log-likelihoods the MLE evaluated at\n$X_1,...,X_n$.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 01:04:51 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Axelrod", "Brian", ""], ["Valiant", "Gregory", ""]]}, {"id": "1811.03224", "submitter": "Soheil Behnezhad", "authors": "Soheil Behnezhad and Alireza Farhadi and MohammadTaghi Hajiaghayi and\n  Nima Reyhani", "title": "Stochastic Matching with Few Queries: New Algorithms and Tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following stochastic matching problem on both weighted and\nunweighted graphs: A graph $G(V, E)$ along with a parameter $p \\in (0, 1)$ is\ngiven in the input. Each edge of $G$ is realized independently with probability\n$p$. The goal is to select a degree bounded (dependent only on $p$) subgraph\n$H$ of $G$ such that the expected size/weight of maximum realized matching of\n$H$ is close to that of $G$.\n  This model of stochastic matching has attracted significant attention over\nthe recent years due to its various applications. The most fundamental open\nquestion is the best approximation factor achievable for such algorithms that,\nin the literature, are referred to as non-adaptive algorithms. Prior work has\nidentified breaking (near) half-approximation as a barrier for both weighted\nand unweighted graphs. Our main results are as follows:\n  -- We analyze a simple and clean algorithm and show that for unweighted\ngraphs, it finds an (almost) $4\\sqrt{2}-5$ ($\\approx 0.6568$) approximation by\nquerying $O(\\frac{\\log (1/p)}{p})$ edges per vertex. This improves over the\nstate-of-the-art $0.5001$ approximate algorithm of Assadi et al. [EC'17].\n  -- We show that the same algorithm achieves a $0.501$ approximation for\nweighted graphs by querying $O(\\frac{\\log (1/p)}{p})$ edges per vertex. This is\nthe first algorithm to break $0.5$ approximation barrier for weighted graphs.\nIt also improves the per-vertex queries of the state-of-the-art by Yamaguchi\nand Maehara [SODA'18] and Behnezhad and Reyhani [EC'18].\n  Our algorithms are fundamentally different from prior works, yet are very\nsimple and natural. For the analysis, we introduce a number of procedures that\nconstruct heavy fractional matchings. We consider the new algorithms and our\nanalytical tools to be the main contributions of this paper.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 02:23:48 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Behnezhad", "Soheil", ""], ["Farhadi", "Alireza", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Reyhani", "Nima", ""]]}, {"id": "1811.03337", "submitter": "Danupon Nanongkai", "authors": "Aaron Bernstein, Danupon Nanongkai", "title": "Distributed Exact Weighted All-Pairs Shortest Paths in Near-Linear Time", "comments": "Full version of STOC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the {\\em distributed all-pairs shortest paths} problem (APSP), every node\nin the weighted undirected distributed network (the CONGEST model) needs to\nknow the distance from every other node using least number of communication\nrounds (typically called {\\em time complexity}). The problem admits\n$(1+o(1))$-approximation $\\tilde\\Theta(n)$-time algorithm and a nearly-tight\n$\\tilde \\Omega(n)$ lower bound [Nanongkai, STOC'14; Lenzen and Patt-Shamir\nPODC'15]\\footnote{$\\tilde \\Theta$, $\\tilde O$ and $\\tilde \\Omega$ hide\npolylogarithmic factors. Note that the lower bounds also hold even in the\nunweighted case and in the weighted case with polynomial approximation\nratios~\\cite{LenzenP_podc13,HolzerW12,PelegRT12,Nanongkai-STOC14}.}. For the\nexact case, Elkin [STOC'17] presented an $O(n^{5/3} \\log^{2/3} n)$ time bound,\nwhich was later improved to $\\tilde O(n^{5/4})$ [Huang, Nanongkai, Saranurak\nFOCS'17]. It was shown that any super-linear lower bound (in $n$) requires a\nnew technique [Censor-Hillel, Khoury, Paz, DISC'17], but otherwise it remained\nwidely open whether there exists a $\\tilde O(n)$-time algorithm for the exact\ncase, which would match the best possible approximation algorithm.\n  This paper resolves this question positively: we present a randomized (Las\nVegas) $\\tilde O(n)$-time algorithm, matching the lower bound up to\npolylogarithmic factors. Like the previous $\\tilde O(n^{5/4})$ bound, our\nresult works for directed graphs with zero (and even negative) edge weights. In\naddition to the improved running time, our algorithm works in a more general\nsetting than that required by the previous $\\tilde O(n^{5/4})$ bound; in our\nsetting (i) the communication is only along edge directions (as opposed to\nbidirectional), and (ii) edge weights are arbitrary (as opposed to integers in\n{1, 2, ... poly(n)}). ...\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 10:00:29 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 23:00:01 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Bernstein", "Aaron", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1811.03491", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane", "title": "Degree-$d$ Chow Parameters Robustly Determine Degree-$d$ PTFs (and\n  Algorithmic Applications)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The degree-$d$ Chow parameters of a Boolean function $f: \\{-1,1\\}^n \\to\n\\mathbb{R}$ are its degree at most $d$ Fourier coefficients. It is well-known\nthat degree-$d$ Chow parameters uniquely characterize degree-$d$ polynomial\nthreshold functions (PTFs) within the space of all bounded functions. In this\npaper, we prove a robust version of this theorem: For $f$ any Boolean\ndegree-$d$ PTF and $g$ any bounded function, if the degree-$d$ Chow parameters\nof $f$ are close to the degree-$d$ Chow parameters of $g$ in $\\ell_2$-norm,\nthen $f$ is close to $g$ in $\\ell_1$-distance. Notably, our bound relating the\ntwo distances is completely independent of the dimension $n$. That is, we show\nthat Boolean degree-$d$ PTFs are {\\em robustly identifiable} from their\ndegree-$d$ Chow parameters. Results of this form had been shown for the $d=1$\ncase~\\cite{OS11:chow, DeDFS14}, but no non-trivial bound was previously known\nfor $d >1$.\n  Our robust identifiability result gives the following algorithmic\napplications: First, we show that Boolean degree-$d$ PTFs can be efficiently\napproximately reconstructed from approximations to their degree-$d$ Chow\nparameters. This immediately implies that degree-$d$ PTFs are efficiently\nlearnable in the uniform distribution $d$-RFA\nmodel~\\cite{BenDavidDichterman:98}. As a byproduct of our approach, we also\nobtain the first low integer-weight approximations of degree-$d$ PTFs, for\n$d>1$. As our second application, our robust identifiability result gives the\nfirst efficient algorithm, with dimension-independent error guarantees, for\nmalicious learning of Boolean degree-$d$ PTFs under the uniform distribution.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 17:59:16 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""]]}, {"id": "1811.03591", "submitter": "Ilya Razenshteyn", "authors": "Sepideh Mahabadi, Konstantin Makarychev, Yury Makarychev, Ilya\n  Razenshteyn", "title": "Nonlinear Dimension Reduction via Outer Bi-Lipschitz Extensions", "comments": "27 pages, 6 figures; an extended abstract appeared in the proceedings\n  of STOC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.LG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study the notion of an outer bi-Lipschitz extension of a map\nbetween Euclidean spaces. The notion is a natural analogue of the notion of a\nLipschitz extension of a Lipschitz map. We show that for every map $f$ there\nexists an outer bi-Lipschitz extension $f'$ whose distortion is greater than\nthat of $f$ by at most a constant factor. This result can be seen as a\ncounterpart of the classic Kirszbraun theorem for outer bi-Lipschitz\nextensions. We also study outer bi-Lipschitz extensions of near-isometric maps\nand show upper and lower bounds for them. Then, we present applications of our\nresults to prioritized and terminal dimension reduction problems.\n  * We prove a prioritized variant of the Johnson-Lindenstrauss lemma: given a\nset of points $X\\subset \\mathbb{R}^d$ of size $N$ and a permutation (\"priority\nranking\") of $X$, there exists an embedding $f$ of $X$ into $\\mathbb{R}^{O(\\log\nN)}$ with distortion $O(\\log \\log N)$ such that the point of rank $j$ has only\n$O(\\log^{3 + \\varepsilon} j)$ non-zero coordinates - more specifically, all but\nthe first $O(\\log^{3+\\varepsilon} j)$ coordinates are equal to $0$; the\ndistortion of $f$ restricted to the first $j$ points (according to the ranking)\nis at most $O(\\log\\log j)$. The result makes a progress towards answering an\nopen question by Elkin, Filtser, and Neiman about prioritized dimension\nreductions.\n  * We prove that given a set $X$ of $N$ points in $\\mathbb{R}^d$, there exists\na terminal dimension reduction embedding of $\\mathbb{R}^d$ into\n$\\mathbb{R}^{d'}$, where $d' = O\\left(\\frac{\\log N}{\\varepsilon^4}\\right)$,\nwhich preserves distances $\\|x-y\\|$ between points $x\\in X$ and $y \\in\n\\mathbb{R}^{d}$, up to a multiplicative factor of $1 \\pm \\varepsilon$. This\nimproves a recent result by Elkin, Filtser, and Neiman.\n  The dimension reductions that we obtain are nonlinear, and this nonlinearity\nis necessary.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 18:23:56 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Mahabadi", "Sepideh", ""], ["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""], ["Razenshteyn", "Ilya", ""]]}, {"id": "1811.03592", "submitter": "Dekel Tsur", "authors": "Dekel Tsur", "title": "An O^*(2.619^k) algorithm for 4-path vertex cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the 4-path vertex cover problem, the input is an undirected graph $G$ and\nan integer $k$. The goal is to decide whether there is a set of vertices $S$ of\nsize at most $k$ such that every path with 4 vertices in $G$ contains at least\none vertex of $S$. In this paper we give a parameterized algorithm for 4-path\nvertex cover whose time complexity is $O^*(2.619^k)$.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 18:26:26 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 05:26:53 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Tsur", "Dekel", ""]]}, {"id": "1811.03744", "submitter": "Anindya De", "authors": "Anindya De, Philip M. Long and Rocco A. Servedio", "title": "Density estimation for shift-invariant multidimensional distributions", "comments": "Appears in the Proceedings of ITCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study density estimation for classes of shift-invariant distributions over\n$\\mathbb{R}^d$. A multidimensional distribution is \"shift-invariant\" if,\nroughly speaking, it is close in total variation distance to a small shift of\nit in any direction. Shift-invariance relaxes smoothness assumptions commonly\nused in non-parametric density estimation to allow jump discontinuities. The\ndifferent classes of distributions that we consider correspond to different\nrates of tail decay.\n  For each such class we give an efficient algorithm that learns any\ndistribution in the class from independent samples with respect to total\nvariation distance. As a special case of our general result, we show that\n$d$-dimensional shift-invariant distributions which satisfy an exponential tail\nbound can be learned to total variation distance error $\\epsilon$ using\n$\\tilde{O}_d(1/ \\epsilon^{d+2})$ examples and $\\tilde{O}_d(1/ \\epsilon^{2d+2})$\ntime. This implies that, for constant $d$, multivariate log-concave\ndistributions can be learned in $\\tilde{O}_d(1/\\epsilon^{2d+2})$ time using\n$\\tilde{O}_d(1/\\epsilon^{d+2})$ samples, answering a question of [Diakonikolas,\nKane and Stewart, 2016] All of our results extend to a model of noise-tolerant\ndensity estimation using Huber's contamination model, in which the target\ndistribution to be learned is a $(1-\\epsilon,\\epsilon)$ mixture of some unknown\ndistribution in the class with some other arbitrary and unknown distribution,\nand the learning algorithm must output a hypothesis distribution with total\nvariation distance error $O(\\epsilon)$ from the target distribution. We show\nthat our general results are close to best possible by proving a simple\n$\\Omega\\left(1/\\epsilon^d\\right)$ information-theoretic lower bound on sample\ncomplexity even for learning bounded distributions that are shift-invariant.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 02:29:43 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["De", "Anindya", ""], ["Long", "Philip M.", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1811.03763", "submitter": "Aleksandar Nikolov", "authors": "Jaroslaw Blasiok, Mark Bun, Aleksandar Nikolov, Thomas Steinke", "title": "Towards Instance-Optimal Private Query Release", "comments": "To appear in SODA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study efficient mechanisms for the query release problem in differential\nprivacy: given a workload of $m$ statistical queries, output approximate\nanswers to the queries while satisfying the constraints of differential\nprivacy. In particular, we are interested in mechanisms that optimally adapt to\nthe given workload. Building on the projection mechanism of Nikolov, Talwar,\nand Zhang, and using the ideas behind Dudley's chaining inequality, we propose\nnew efficient algorithms for the query release problem, and prove that they\nachieve optimal sample complexity for the given workload (up to constant\nfactors, in certain parameter regimes) with respect to the class of mechanisms\nthat satisfy concentrated differential privacy. We also give variants of our\nalgorithms that satisfy local differential privacy, and prove that they also\nachieve optimal sample complexity among all local sequentially interactive\nprivate mechanisms.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 03:52:26 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Blasiok", "Jaroslaw", ""], ["Bun", "Mark", ""], ["Nikolov", "Aleksandar", ""], ["Steinke", "Thomas", ""]]}, {"id": "1811.03836", "submitter": "Joshua Lau", "authors": "Serge Gaspers and Joshua Lau", "title": "Minimizing and Computing the Inverse Geodesic Length on Trees", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any fixed measure $H$ that maps graphs to real numbers, the MinH problem\nis defined as follows: given a graph $G$, an integer $k$, and a target $\\tau$,\nis there a set $S$ of $k$ vertices that can be deleted, so that $H(G - S)$ is\nat most $\\tau$? In this paper, we consider the MinH problem on trees.\n  We call $H$ \"balanced on trees\" if, whenever $G$ is a tree, there is an\noptimal choice of $S$ such that the components of $G-S$ have sizes bounded by a\npolynomial in $n/k$. We show that MinH on trees is FPT for parameter $n/k$, and\nfurthermore, can be solved in subexponential time, and polynomial space, if $H$\nis additive, balanced on trees, and computable in polynomial time.\n  A measure of interest is the Inverse Geodesic Length (IGL), which is used to\ngauge the connectedness of a graph. It is defined as the sum of inverse\ndistances between every two vertices: $IGL(G)=\\sum_{\\{u,v\\} \\subseteq V}\n\\frac{1}{d_G(u,v)}$. While MinIGL is W[1]-hard for parameter treewidth, and\ncannot be solved in $2^{o(k+n+m)}$ time, even on bipartite graphs with $n$\nvertices and $m$ edges, the complexity status of the problem remains open on\ntrees. We show that IGL is balanced on trees, to give a $2^{O((n\\log\nn)^{5/6})}$ time, polynomial space algorithm.\n  The distance distribution of $G$ is the sequence $\\{a_i\\}$ describing the\nnumber of vertex pairs distance $i$ apart in $G$: $a_i=|\\{\\{u, v\\}: d_G(u,\nv)=i\\}|$. We show that the distance distribution of a tree can be computed in\n$O(n\\log^2 n)$ time by reduction to polynomial multiplication. We extend our\nresult to graphs with small treewidth by showing that the first $p$ values of\nthe distance distribution can be computed in $2^{O(tw(G))} n^{1+\\varepsilon}\n\\sqrt{p}$ time, and the entire distance distribution can be computed in\n$2^{O(tw(G))} n^{1+\\varepsilon}$ time, when the diameter of $G$ is\n$O(n^{\\varepsilon'})$ for every $\\varepsilon'>0$.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 09:48:46 GMT"}, {"version": "v2", "created": "Sat, 28 Sep 2019 04:49:39 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Gaspers", "Serge", ""], ["Lau", "Joshua", ""]]}, {"id": "1811.03841", "submitter": "John Fearnley", "authors": "John Fearnley, Spencer Gordon, Ruta Mehta, Rahul Savani", "title": "Unique End of Potential Line", "comments": "This paper substantially revises and extends the work described in\n  our previous preprint \"End of Potential Line'' (arXiv:1804.03450). The\n  abstract has been shortened to meet the arXiv character limit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the complexity of problems in PPAD $\\cap$ PLS that have\nunique solutions. Three well-known examples of such problems are the problem of\nfinding a fixpoint of a contraction map, finding the unique sink of a Unique\nSink Orientation (USO), and solving the P-matrix Linear Complementarity Problem\n(P-LCP). Each of these are promise-problems, and when the promise holds, they\nalways possess unique solutions.\n  We define the complexity class UEOPL to capture problems of this type. We\nfirst define a class that we call EOPL, which consists of all problems that can\nbe reduced to End-of-Potential-Line. This problem merges the canonical\nPPAD-complete problem End-of-Line, with the canonical PLS-complete problem\nSink-of-Dag, and so EOPL captures problems that can be solved by a\nline-following algorithm that also simultaneously decreases a potential\nfunction.\n  Promise-UEOPL is a promise-subclass of EOPL in which the line in the\nEnd-of-Potential-Line instance is guaranteed to be unique via a promise. We\nturn this into a non-promise class UEOPL, by adding an extra solution type to\nEOPL that captures any pair of points that are provably on two different lines.\n  We show that UEOPL $\\subseteq$ EOPL $\\subseteq$ CLS, and that all of our\nmotivating problems are contained in UEOPL: specifically USO, P-LCP, and\nfinding a fixpoint of a Piecewise-Linear Contraction under an $\\ell_p$-norm all\nlie in UEOPL. Our results also imply that parity games, mean-payoff games,\ndiscounted games, and simple-stochastic games lie in UEOPL.\n  All of our containment results are proved via a reduction to a problem that\nwe call One-Permutation Discrete Contraction (OPDC). This problem is motivated\nby a discretized version of contraction, but it is also closely related to the\nUSO problem. We show that OPDC lies in UEOPL, and we are also able to show that\nOPDC is UEOPL-complete.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 10:06:17 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Fearnley", "John", ""], ["Gordon", "Spencer", ""], ["Mehta", "Ruta", ""], ["Savani", "Rahul", ""]]}, {"id": "1811.03962", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li, Zhao Song", "title": "A Convergence Theory for Deep Learning via Over-Parameterization", "comments": "V2 adds citation and V3/V4/V5 polish writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have demonstrated dominating performance in many\nfields; since AlexNet, networks used in practice are going wider and deeper. On\nthe theoretical side, a long line of works has been focusing on training neural\nnetworks with one hidden layer. The theory of multi-layer networks remains\nlargely unsettled.\n  In this work, we prove why stochastic gradient descent (SGD) can find\n$\\textit{global minima}$ on the training objective of DNNs in\n$\\textit{polynomial time}$. We only make two assumptions: the inputs are\nnon-degenerate and the network is over-parameterized. The latter means the\nnetwork width is sufficiently large: $\\textit{polynomial}$ in $L$, the number\nof layers and in $n$, the number of samples.\n  Our key technique is to derive that, in a sufficiently large neighborhood of\nthe random initialization, the optimization landscape is almost-convex and\nsemi-smooth even with ReLU activations. This implies an equivalence between\nover-parameterized neural networks and neural tangent kernel (NTK) in the\nfinite (and polynomial) width setting.\n  As concrete examples, starting from randomly initialized weights, we prove\nthat SGD can attain 100% training accuracy in classification tasks, or minimize\nregression loss in linear convergence speed, with running time polynomial in\n$n,L$. Our theory applies to the widely-used but non-smooth ReLU activation,\nand to any smooth and possibly non-convex loss functions. In terms of network\narchitectures, our theory at least applies to fully-connected neural networks,\nconvolutional neural networks (CNN), and residual neural networks (ResNet).\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 15:16:13 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 18:54:20 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 11:44:07 GMT"}, {"version": "v4", "created": "Mon, 4 Feb 2019 03:57:59 GMT"}, {"version": "v5", "created": "Mon, 17 Jun 2019 06:39:04 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""], ["Song", "Zhao", ""]]}, {"id": "1811.03966", "submitter": "Lars Jaffke", "authors": "Lars Jaffke and Paloma T. Lima", "title": "A Complexity Dichotomy for Critical Values of the b-Chromatic Number of\n  Graphs", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $b$-coloring of a graph $G$ is a proper coloring of its vertices such that\neach color class contains a vertex that has at least one neighbor in all the\nother color classes. The b-Coloring problem asks whether a graph $G$ has a\n$b$-coloring with $k$ colors. The $b$-chromatic number of a graph $G$, denoted\nby $\\chi_b(G)$, is the maximum number $k$ such that $G$ admits a $b$-coloring\nwith $k$ colors. We consider the complexity of the b-Coloring problem, whenever\nthe value of $k$ is close to one of two upper bounds on $\\chi_b(G)$: The\nmaximum degree $\\Delta(G)$ plus one, and the $m$-degree, denoted by $m(G)$,\nwhich is defined as the maximum number $i$ such that $G$ has $i$ vertices of\ndegree at least $i-1$. We obtain a dichotomy result stating that for fixed $k\n\\in \\{\\Delta(G) + 1 - p, m(G) - p\\}$, the problem is polynomial-time solvable\nwhenever $p \\in \\{0, 1\\}$ and, even when $k = 3$, it is NP-complete whenever $p\n\\ge 2$. We furthermore consider parameterizations of the b-Coloring problem\nthat involve the maximum degree $\\Delta(G)$ of the input graph $G$ and give two\nFPT-algorithms. First, we show that deciding whether a graph $G$ has a\n$b$-coloring with $m(G)$ colors is FPT parameterized by $\\Delta(G)$. Second, we\nshow that b-Coloring is FPT parameterized by $\\Delta(G) + \\ell_k(G)$, where\n$\\ell_k(G)$ denotes the number of vertices of degree at least $k$.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 15:22:35 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 15:04:17 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Jaffke", "Lars", ""], ["Lima", "Paloma T.", ""]]}, {"id": "1811.04014", "submitter": "Service IES Inria Episciences IAM", "authors": "Alexander B\\\"uchel, Ulrich Gille{\\ss}en, Kurt-Ulrich Witt", "title": "An output-sensitive Algorithm to partition a Sequence of Integers into\n  Subsets with equal Sums", "comments": null, "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 20 no.\n  2, Discrete Algorithms (January 24, 2019) dmtcs:5122", "doi": "10.23638/DMTCS-20-2-18", "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a polynomial time algorithm, which solves a nonstandard Variation\nof the well-known PARTITION-problem: Given positive integers $n, k$ and $t$\nsuch that $t \\geq n$ and $k \\cdot t = {n+1 \\choose 2}$, the algorithm\npartitions the elements of the set $I_n = \\{1, \\ldots, n\\}$ into $k$ mutually\ndisjoint subsets $T_j$ such that $\\cup_{j=1}^k T_j = I_n$ and $\\sum_{x \\in\nT_{j}} x = t$ for each $j \\in \\{1,2, \\ldots, k\\}$. The algorithm needs\n$\\mathcal{O}(n \\cdot ( \\frac{n}{2k} + \\log \\frac{n(n+1)}{2k} ))$ steps to\ninsert the $n$ elements of $I_n$ into the $k$ sets $T_j$.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 17:08:35 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 14:25:34 GMT"}, {"version": "v3", "created": "Wed, 23 Jan 2019 15:45:23 GMT"}, {"version": "v4", "created": "Thu, 14 Feb 2019 07:33:33 GMT"}, {"version": "v5", "created": "Mon, 18 Feb 2019 14:17:03 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["B\u00fcchel", "Alexander", ""], ["Gille\u00dfen", "Ulrich", ""], ["Witt", "Kurt-Ulrich", ""]]}, {"id": "1811.04037", "submitter": "Alexander Prolubnikov", "authors": "Alexander Prolubnikov", "title": "An estimation of the greedy algorithm's accuracy for a set cover problem\n  instance", "comments": "10 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering the set cover problem, by modifying the approach that gives a\nlogarithmic approximation guarantee for the greedy algorithm, we obtain an\nestimation of the greedy algorithm's accuracy for a particular input. We\ncompare the presented estimation to another estimations of this type. We give\nsuch examples of the set cover problem instances that the presented estimation\nsagnificantly improves over linear programming relaxation based estimation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 19:11:58 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 05:16:21 GMT"}, {"version": "v3", "created": "Tue, 12 Feb 2019 10:27:58 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Prolubnikov", "Alexander", ""]]}, {"id": "1811.04052", "submitter": "Alireza Farhadi", "authors": "MohammadHossein Bateni, Alireza Farhadi and MohammadTaghi Hajiaghayi", "title": "Polynomial-time Approximation Scheme for Minimum k-cut in Planar and\n  Minor-free Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-cut problem asks, given a connected graph $G$ and a positive integer\n$k$, to find a minimum-weight set of edges whose removal splits $G$ into $k$\nconnected components. We give the first polynomial-time algorithm with\napproximation factor $2-\\epsilon$ (with constant $\\epsilon > 0$) for the\n$k$-cut problem in planar and minor-free graphs. Applying more complex\ntechniques, we further improve our method and give a polynomial-time\napproximation scheme for the $k$-cut problem in both planar and minor-free\ngraphs. Despite persistent effort, to the best of our knowledge, this is the\nfirst improvement for the $k$-cut problem over standard approximation factor of\n$2$ in any major class of graphs.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 18:15:26 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Bateni", "MohammadHossein", ""], ["Farhadi", "Alireza", ""], ["Hajiaghayi", "MohammadTaghi", ""]]}, {"id": "1811.04065", "submitter": "Alexandr Andoni", "authors": "Alexandr Andoni, Tal Malkin, Negev Shekel Nosatzki", "title": "Two Party Distribution Testing: Communication and Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of discrete distribution testing in the two-party\nsetting. For example, in the standard closeness testing problem, Alice and Bob\neach have $t$ samples from, respectively, distributions $a$ and $b$ over $[n]$,\nand they need to test whether $a=b$ or $a,b$ are $\\epsilon$-far for some fixed\n$\\epsilon>0$. This is in contrast to the well-studied one-party case, where the\ntester has unrestricted access to samples of both distributions, for which\noptimal bounds are known for a number of variations. Despite being a natural\nconstraint in applications, the two-party setting has evaded attention so far.\n  We address two fundamental aspects: 1) what is the communication complexity,\nand 2) can it be accomplished securely, without Alice and Bob learning extra\ninformation about each other's input. Besides closeness testing, we also study\nthe independence testing problem, where Alice and Bob have $t$ samples from\ndistributions $a$ and $b$ respectively, which may be correlated; the question\nis whether $a,b$ are independent of $\\epsilon$-far from being independent.\n  Our contribution is three-fold:\n  1) Communication: we show how to gain communication efficiency with more\nsamples, beyond the information-theoretic bound on $t$. The gain is\npolynomially better than what one obtains by adapting standard algorithms.\n  2) Lower bounds: we prove tightness of our protocols for the closeness\ntesting, and for the independence testing when the number of samples is\nunbounded. These lower bounds are of independent interest as these are the\nfirst 2-party communication lower bounds for testing problems.\n  3) Security: we define secure distribution testing and argue that it must\nleak at least some minimal information. We then provide secure versions of the\nabove protocols with an overhead that is only polynomial in the security\nparameter.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 18:52:03 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Andoni", "Alexandr", ""], ["Malkin", "Tal", ""], ["Nosatzki", "Negev Shekel", ""]]}, {"id": "1811.04150", "submitter": "Daniel Ting", "authors": "Daniel Ting", "title": "Count-Min: Optimal Estimation and Tight Error Bounds using Empirical\n  Error Distributions", "comments": "Long version of a KDD 2018 paper of the same name", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Count-Min sketch is an important and well-studied data summarization\nmethod. It allows one to estimate the count of any item in a stream using a\nsmall, fixed size data sketch. However, the accuracy of the sketch depends on\ncharacteristics of the underlying data. This has led to a number of count\nestimation procedures which work well in one scenario but perform poorly in\nothers. A practitioner is faced with two basic, unanswered questions. Which\nvariant should be chosen when the data is unknown? Given an estimate, is its\nerror sufficiently small to be trustworthy?\n  We provide answers to these questions. We derive new count estimators,\nincluding a provably optimal estimator, which best or match previous estimators\nin all scenarios. We also provide practical, tight error bounds at query time\nfor both new and existing estimators. These error estimates also yield\nprocedures to choose the sketch tuning parameters optimally, as they can\nextrapolate the error to different choices of sketch width and depth.\n  The key observation is that the distribution of errors in each counter can be\nempirically estimated from the sketch itself. By first estimating this\ndistribution, count estimation becomes a statistical estimation and inference\nproblem with a known error distribution. This provides both a principled way to\nderive new and optimal estimators as well as a way to study the error and\nproperties of existing estimators.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 22:11:56 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Ting", "Daniel", ""]]}, {"id": "1811.04177", "submitter": "Shunji Umetani", "authors": "Naoya Uematsu, Shunji Umetani and Yoshinobu Kawahara", "title": "An efficient branch-and-bound algorithm for submodular function\n  maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The submodular function maximization is an attractive optimization model that\nappears in many real applications. Although a variety of greedy algorithms\nquickly find good feasible solutions for many instances while guaranteeing\n(1-1/e)-approximation ratio, we still encounter many real applications that ask\noptimal or better feasible solutions within reasonable computation time. In\nthis paper, we present an efficient branch-and-bound algorithm for the\nnon-decreasing submodular function maximization problem based on its binary\ninteger programming (BIP) formulation with a huge number of constraints.\nNemhauser and Wolsey developed an exact algorithm called the constraint\ngeneration algorithm that starts from a reduced BIP problem with a small subset\nof constraints taken from the constraints and repeats solving a reduced BIP\nproblem while adding a new constraint at each iteration. However, their\nalgorithm is still computationally expensive due to many reduced BIP problems\nto be solved. To overcome this, we propose an improved constraint generation\nalgorithm to add a promising set of constraints at each iteration. We\nincorporate it into a branch-and-bound algorithm to attain good upper bounds\nwhile solving a smaller number of reduced BIP problems. According to\ncomputational results for well-known benchmark instances, our algorithm\nachieved better performance than the state-of-the-art exact algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 02:55:47 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Uematsu", "Naoya", ""], ["Umetani", "Shunji", ""], ["Kawahara", "Yoshinobu", ""]]}, {"id": "1811.04300", "submitter": "William Kuszmaul", "authors": "William Kuszmaul", "title": "Efficiently Approximating Edit Distance Between Pseudorandom Strings", "comments": "SODA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for approximating the edit distance\n$\\operatorname{ed}(x, y)$ between two strings $x$ and $y$ in time parameterized\nby the degree to which one of the strings $x$ satisfies a natural\npseudorandomness property. The pseudorandomness model is asymmetric in that no\nrequirements are placed on the second string $y$, which may be constructed by\nan adversary with full knowledge of $x$.\n  We say that $x$ is \\emph{$(p, B)$-pseudorandom} if all pairs $a$ and $b$ of\ndisjoint $B$-letter substrings of $x$ satisfy $\\operatorname{ed}(a, b) \\ge pB$.\nGiven parameters $p$ and $B$, our algorithm computes the edit distance between\na $(p, B)$-pseudorandom string $x$ and an arbitrary string $y$ within a factor\nof $O(1/p)$ in time $\\tilde{O}(nB)$, with high probability.\n  Our algorithm is robust in the sense that it can handle a small portion of\n$x$ being adversarial (i.e., not satisfying the pseudorandomness property). In\nthis case, the algorithm incurs an additive approximation error proportional to\nthe fraction of $x$ which behaves maliciously.\n  The asymmetry of our pseudorandomness model has particular appeal for the\ncase where $x$ is a \\emph{source string}, meaning that $\\operatorname{ed}(x,\ny)$ will be computed for many strings $y$. Suppose that one wishes to achieve\nan $O(\\alpha)$-approximation for each $\\operatorname{ed}(x, y)$ computation,\nand that $B$ is the smallest block-size for which the string $x$ is $(1/\\alpha,\nB)$-pseudorandom. We show that without knowing $B$ beforehand, $x$ may be\npreprocessed in time $\\tilde{O}(n^{1.5}\\sqrt{B})$, so that all future\ncomputations of the form $\\operatorname{ed}(x, y)$ may be\n$O(\\alpha)$-approximated in time $\\tilde{O}(nB)$. Furthermore, for the special\ncase where only a single $\\operatorname{ed}(x, y)$ computation will be\nperformed, we show how to achieve an $O(\\alpha)$-approximation in time\n$\\tilde{O}(n^{4/3}B^{2/3})$.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 20:06:19 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Kuszmaul", "William", ""]]}, {"id": "1811.04331", "submitter": "Lorenzo Severini", "authors": "Gianlorenzo D'Angelo, Martin Olsen, Lorenzo Severini", "title": "Coverage Centrality Maximization in Undirected Networks", "comments": "Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Centrality metrics are among the main tools in social network analysis. Being\ncentral for a user of a network leads to several benefits to the user: central\nusers are highly influential and play key roles within the network. Therefore,\nthe optimization problem of increasing the centrality of a network user\nrecently received considerable attention. Given a network and a target user\n$v$, the centrality maximization problem consists in creating $k$ new links\nincident to $v$ in such a way that the centrality of $v$ is maximized,\naccording to some centrality metric. Most of the algorithms proposed in the\nliterature are based on showing that a given centrality metric is monotone and\nsubmodular with respect to link addition. However, this property does not hold\nfor several shortest-path based centrality metrics if the links are undirected.\nIn this paper we study the centrality maximization problem in undirected\nnetworks for one of the most important shortest-path based centrality measures,\nthe coverage centrality. We provide several hardness and approximation results.\nWe first show that the problem cannot be approximated within a factor greater\nthan $1-1/e$, unless $P=NP$, and, under the stronger gap-ETH hypothesis, the\nproblem cannot be approximated within a factor better than $1/n^{o(1)}$, where\n$n$ is the number of users. We then propose two greedy approximation\nalgorithms, and show that, by suitably combining them, we can guarantee an\napproximation factor of $\\Omega(1/\\sqrt{n})$. We experimentally compare the\nsolutions provided by our approximation algorithm with optimal solutions\ncomputed by means of an exact IP formulation. We show that our algorithm\nproduces solutions that are very close to the optimum.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 01:19:02 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["D'Angelo", "Gianlorenzo", ""], ["Olsen", "Martin", ""], ["Severini", "Lorenzo", ""]]}, {"id": "1811.04425", "submitter": "Talya Eden", "authors": "Talya Eden and Dana Ron and C. Seshadhri", "title": "Faster sublinear approximations of $k$-cliques for low arboricity graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given query access to an undirected graph $G$, we consider the problem of\ncomputing a $(1\\pm\\epsilon)$-approximation of the number of $k$-cliques in $G$.\nThe standard query model for general graphs allows for degree queries, neighbor\nqueries, and pair queries. Let $n$ be the number of vertices, $m$ be the number\nof edges, and $n_k$ be the number of $k$-cliques. Previous work by Eden, Ron\nand Seshadhri (STOC 2018) gives an $O^*(\\frac{n}{n^{1/k}_k} +\n\\frac{m^{k/2}}{n_k})$-time algorithm for this problem (we use $O^*(\\cdot)$ to\nsuppress $\\poly(\\log n, 1/\\epsilon, k^k)$ dependencies). Moreover, this bound\nis nearly optimal when the expression is sublinear in the size of the graph.\n  Our motivation is to circumvent this lower bound, by parameterizing the\ncomplexity in terms of \\emph{graph arboricity}. The arboricity of $G$ is a\nmeasure for the graph density \"everywhere\". We design an algorithm for the\nclass of graphs with arboricity at most $\\alpha$, whose running time is\n$O^*(\\min\\{\\frac{n\\alpha^{k-1}}{n_k},\\, \\frac{n}{n_k^{1/k}}+\\frac{m\n\\alpha^{k-2}}{n_k} \\})$. We also prove a nearly matching lower bound. For all\ngraphs, the arboricity is $O(\\sqrt m)$, so this bound subsumes all previous\nresults on sublinear clique approximation.\n  As a special case of interest, consider minor-closed families of graphs,\nwhich have constant arboricity. Our result implies that for any minor-closed\nfamily of graphs, there is a $(1\\pm\\epsilon)$-approximation algorithm for $n_k$\nthat has running time $O^*(\\frac{n}{n_k})$. Such a bound was not known even for\nthe special (classic) case of triangle counting in planar graphs.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 14:43:33 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Eden", "Talya", ""], ["Ron", "Dana", ""], ["Seshadhri", "C.", ""]]}, {"id": "1811.04449", "submitter": "Anthony Bonato", "authors": "Anthony Bonato, Shahin Kamali", "title": "Approximation Algorithms for Graph Burning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerous approaches study the vulnerability of networks against social\ncontagion. Graph burning studies how fast a contagion, modeled as a set of\nfires, spreads in a graph. The burning process takes place in synchronous,\ndiscrete rounds. In each round, a fire breaks out at a vertex, and the fire\nspreads to all vertices that are adjacent to a burning vertex. The selection of\nvertices where fires start defines a schedule that indicates the number of\nrounds required to burn all vertices. Given a graph, the objective of an\nalgorithm is to find a schedule that minimizes the number of rounds to burn\ngraph. Finding the optimal schedule is known to be NP-hard, and the problem\nremains NP-hard when the graph is a tree or a set of disjoint paths. The only\nknown algorithm is an approximation algorithm for disjoint paths, which has an\napproximation ratio of 1.5.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 18:58:48 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 02:58:34 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Bonato", "Anthony", ""], ["Kamali", "Shahin", ""]]}, {"id": "1811.04465", "submitter": "Andrei Lissovoi", "authors": "Andrei Lissovoi, Pietro S. Oliveto", "title": "Computational Complexity Analysis of Genetic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic programming (GP) is an evolutionary computation technique to solve\nproblems in an automated, domain-independent way. Rather than identifying the\noptimum of a function as in more traditional evolutionary optimization, the aim\nof GP is to evolve computer programs with a given functionality. While many GP\napplications have produced human competitive results, the theoretical\nunderstanding of what problem characteristics and algorithm properties allow GP\nto be effective is comparatively limited. Compared with traditional\nevolutionary algorithms for function optimization, GP applications are further\ncomplicated by two additional factors: the variable-length representation of\ncandidate programs, and the difficulty of evaluating their quality efficiently.\nSuch difficulties considerably impact the runtime analysis of GP, where space\ncomplexity also comes into play. As a result, initial complexity analyses of GP\nhave focused on restricted settings such as the evolution of trees with given\nstructures or the estimation of solution quality using only a small polynomial\nnumber of input/output examples. However, the first computational complexity\nanalyses of GP for evolving proper functions with defined input/output behavior\nhave recently appeared. In this chapter, we present an overview of the state of\nthe art.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 19:54:28 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 13:35:41 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Lissovoi", "Andrei", ""], ["Oliveto", "Pietro S.", ""]]}, {"id": "1811.04560", "submitter": "Diptapriyo Majumdar", "authors": "Diptapriyo Majumdar, Rian Neogi, Venkatesh Raman, S. Vaishali", "title": "Tractability of Konig Edge Deletion Problems", "comments": "Accepted for publication in Theoretical Computer Science. Major\n  revisions from the previous version were incorporated based on the comments\n  from anonymous reviewers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is said to be a Konig graph if the size of its maximum matching is\nequal to the size of its minimum vertex cover. The Konig Edge Deletion problem\nasks if in a given graph there exists a set of at most k edges whose deletion\nresults in a Konig graph. While the vertex version of the problem (Konig vertex\ndeletion) has been shown to be fixed-parameter tractable more than a decade\nago, the fixed-parameter-tractability of the Konig Edge Deletion problem has\nbeen open since then, and has been conjectured to be W[1]-hard in several\npapers. In this paper, we settle the conjecture by proving it W[1]-hard. We\nprove that a variant of this problem, where we are given a graph G and a\nmaximum matching M and we want a k-sized Konig edge deletion set that is\ndisjoint from M, is fixed-parameter-tractable.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 05:31:39 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 22:45:13 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Majumdar", "Diptapriyo", ""], ["Neogi", "Rian", ""], ["Raman", "Venkatesh", ""], ["Vaishali", "S.", ""]]}, {"id": "1811.04596", "submitter": "Isamu Furuya", "authors": "Isamu Furuya, Takuya Takagi, Yuto Nakashima, Shunsuke Inenaga, Hideo\n  Bannai, Takuya Kida", "title": "MR-RePair: Grammar Compression based on Maximal Repeats", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the grammar generation algorithm of the RePair compression\nalgorithm and show the relation between a grammar generated by RePair and\nmaximal repeats. We reveal that RePair replaces step by step the most frequent\npairs within the corresponding most frequent maximal repeats. Then, we design a\nnovel variant of RePair, called MR-RePair, which substitutes the most frequent\nmaximal repeats at once instead of substituting the most frequent pairs\nconsecutively. We implemented MR-RePair and compared the size of the grammar\ngenerated by MR-RePair to that by RePair on several text corpus. Our\nexperiments show that MR-RePair generates more compact grammars than RePair\ndoes, especially for highly repetitive texts.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 08:11:54 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 05:29:30 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Furuya", "Isamu", ""], ["Takagi", "Takuya", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Kida", "Takuya", ""]]}, {"id": "1811.04607", "submitter": "Runzhou Tao", "authors": "Venkatesan Guruswami, Runzhou Tao", "title": "Streaming Hardness of Unique Games", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.APPROX-RANDOM.2019.5", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of approximating the value of a Unique Game instance in\nthe streaming model. A simple count of the number of constraints divided by\n$p$, the alphabet size of the Unique Game, gives a trivial $p$-approximation\nthat can be computed in $O(\\log n)$ space. Meanwhile, with high probability, a\nsample of $\\tilde{O}(n)$ constraints suffices to estimate the optimal value to\n$(1+\\epsilon)$ accuracy. We prove that any single-pass streaming algorithm that\nachieves a $(p-\\epsilon)$-approximation requires $\\Omega_\\epsilon(\\sqrt{n})$\nspace. Our proof is via a reduction from lower bounds for a communication\nproblem that is a $p$-ary variant of the Boolean Hidden Matching problem\nstudied in the literature. Given the utility of Unique Games as a starting\npoint for reduction to other optimization problems, our strong hardness for\napproximating Unique Games could lead to down\\emph{stream} hardness results for\nstreaming approximability for other CSP-like problems.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 08:56:28 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Tao", "Runzhou", ""]]}, {"id": "1811.04633", "submitter": "Wei Wu", "authors": "Wei Wu, Bin Li, Ling Chen, Junbin Gao, Chengqi Zhang", "title": "A Review for Weighted MinHash Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data similarity (or distance) computation is a fundamental research topic\nwhich underpins many high-level applications based on similarity measures in\nmachine learning and data mining. However, in large-scale real-world scenarios,\nthe exact similarity computation has become daunting due to \"3V\" nature\n(volume, velocity and variety) of big data. In such cases, the hashing\ntechniques have been verified to efficiently conduct similarity estimation in\nterms of both theory and practice. Currently, MinHash is a popular technique\nfor efficiently estimating the Jaccard similarity of binary sets and\nfurthermore, weighted MinHash is generalized to estimate the generalized\nJaccard similarity of weighted sets. This review focuses on categorizing and\ndiscussing the existing works of weighted MinHash algorithms. In this review,\nwe mainly categorize the Weighted MinHash algorithms into quantization-based\napproaches, \"active index\"-based ones and others, and show the evolution and\ninherent connection of the weighted MinHash algorithms, from the integer\nweighted MinHash algorithms to real-valued weighted MinHash ones (particularly\nthe Consistent Weighted Sampling scheme). Also, we have developed a python\ntoolbox for the algorithms, and released it in our github. Based on the\ntoolbox, we experimentally conduct a comprehensive comparative study of the\nstandard MinHash algorithm and the weighted MinHash ones.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 09:55:53 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Wu", "Wei", ""], ["Li", "Bin", ""], ["Chen", "Ling", ""], ["Gao", "Junbin", ""], ["Zhang", "Chengqi", ""]]}, {"id": "1811.04753", "submitter": "Hendrik Molter", "authors": "George B. Mertzios and Hendrik Molter and Viktor Zamaraev", "title": "Sliding Window Temporal Graph Coloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph coloring is one of the most famous computational problems with\napplications in a wide range of areas such as planning and scheduling, resource\nallocation, and pattern matching. So far coloring problems are mostly studied\non static graphs, which often stand in stark contrast to practice where data is\ninherently dynamic and subject to discrete changes over time. A temporal graph\nis a graph whose edges are assigned a set of integer time labels, indicating at\nwhich discrete time steps the edge is active. In this paper we present a\nnatural temporal extension of the classical graph coloring problem. Given a\ntemporal graph and a natural number $\\Delta$, we ask for a coloring sequence\nfor each vertex such that (i) in every sliding time window of $\\Delta$\nconsecutive time steps, in which an edge is active, this edge is properly\ncolored (i.e. its endpoints are assigned two different colors) at least once\nduring that time window, and (ii) the total number of different colors is\nminimized. This sliding window temporal coloring problem abstractly captures\nmany realistic graph coloring scenarios in which the underlying network changes\nover time, such as dynamically assigning communication channels to moving\nagents. We present a thorough investigation of the computational complexity of\nthis temporal coloring problem. More specifically, we prove strong\ncomputational hardness results, complemented by efficient exact and\napproximation algorithms. Some of our algorithms are linear-time\nfixed-parameter tractable with respect to appropriate parameters, while others\nare asymptotically almost optimal under the Exponential Time Hypothesis (ETH).\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 14:51:54 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 18:23:28 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Mertzios", "George B.", ""], ["Molter", "Hendrik", ""], ["Zamaraev", "Viktor", ""]]}, {"id": "1811.04852", "submitter": "Chunhao Wang", "authors": "Nai-Hui Chia, Han-Hsuan Lin, Chunhao Wang", "title": "Quantum-inspired sublinear classical algorithms for solving low-rank\n  linear systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present classical sublinear-time algorithms for solving low-rank linear\nsystems of equations. Our algorithms are inspired by the HHL quantum algorithm\nfor solving linear systems and the recent breakthrough by Tang of dequantizing\nthe quantum algorithm for recommendation systems. Let $A \\in \\mathbb{C}^{m\n\\times n}$ be a rank-$k$ matrix, and $b \\in \\mathbb{C}^m$ be a vector. We\npresent two algorithms: a \"sampling\" algorithm that provides a sample from\n$A^{-1}b$ and a \"query\" algorithm that outputs an estimate of an entry of\n$A^{-1}b$, where $A^{-1}$ denotes the Moore-Penrose pseudo-inverse. Both of our\nalgorithms have query and time complexity $O(\\mathrm{poly}(k, \\kappa, \\|A\\|_F,\n1/\\epsilon)\\,\\mathrm{polylog}(m, n))$, where $\\kappa$ is the condition number\nof $A$ and $\\epsilon$ is the precision parameter. Note that the algorithms we\nconsider are sublinear time, so they cannot write and read the whole matrix or\nvectors. In this paper, we assume that $A$ and $b$ come with well-known\nlow-overhead data structures such that entries of $A$ and $b$ can be sampled\naccording to some natural probability distributions. Alternatively, when $A$ is\npositive semidefinite, our algorithms can be adapted so that the sampling\nassumption on $b$ is not required.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 16:57:33 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Chia", "Nai-Hui", ""], ["Lin", "Han-Hsuan", ""], ["Wang", "Chunhao", ""]]}, {"id": "1811.04909", "submitter": "Andr\\'as Gily\\'en", "authors": "Andr\\'as Gily\\'en, Seth Lloyd, Ewin Tang", "title": "Quantum-inspired low-rank stochastic regression with logarithmic\n  dependence on the dimension", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct an efficient classical analogue of the quantum matrix inversion\nalgorithm (HHL) for low-rank matrices. Inspired by recent work of Tang,\nassuming length-square sampling access to input data, we implement the\npseudoinverse of a low-rank matrix and sample from the solution to the problem\n$Ax=b$ using fast sampling techniques. We implement the pseudo-inverse by\nfinding an approximate singular value decomposition of $A$ via subsampling,\nthen inverting the singular values. In principle, the approach can also be used\nto apply any desired \"smooth\" function to the singular values. Since many\nquantum algorithms can be expressed as a singular value transformation problem,\nour result suggests that more low-rank quantum algorithms can be effectively\n\"dequantised\" into classical length-square sampling algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:49:42 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Gily\u00e9n", "Andr\u00e1s", ""], ["Lloyd", "Seth", ""], ["Tang", "Ewin", ""]]}, {"id": "1811.04918", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Yuanzhi Li and Yingyu Liang", "title": "Learning and Generalization in Overparameterized Neural Networks, Going\n  Beyond Two Layers", "comments": "V1/V2/V3/V4 polish writing, V5 adds experiments, V6 reflects our\n  camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental learning theory behind neural networks remains largely open.\nWhat classes of functions can neural networks actually learn? Why doesn't the\ntrained network overfit when it is overparameterized?\n  In this work, we prove that overparameterized neural networks can learn some\nnotable concept classes, including two and three-layer networks with fewer\nparameters and smooth activations. Moreover, the learning can be simply done by\nSGD (stochastic gradient descent) or its variants in polynomial time using\npolynomially many samples. The sample complexity can also be almost independent\nof the number of parameters in the network.\n  On the technique side, our analysis goes beyond the so-called NTK (neural\ntangent kernel) linearization of neural networks in prior works. We establish a\nnew notion of quadratic approximation of the neural network (that can be viewed\nas a second-order variant of NTK), and connect it to the SGD theory of escaping\nsaddle points.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:57:02 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 15:56:01 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 04:10:51 GMT"}, {"version": "v4", "created": "Fri, 29 Mar 2019 17:09:46 GMT"}, {"version": "v5", "created": "Tue, 28 May 2019 10:25:09 GMT"}, {"version": "v6", "created": "Mon, 1 Jun 2020 17:11:51 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""], ["Liang", "Yingyu", ""]]}, {"id": "1811.05022", "submitter": "Deeparnab Chakrabarty", "authors": "Deeparnab Chakrabarty, Chaitanya Swamy", "title": "Approximation Algorithms for Minimum Norm and Ordered Optimization\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $ $In many optimization problems, a feasible solution induces a\nmulti-dimensional cost vector. For example, in load-balancing a schedule\ninduces a load vector across the machines. In $k$-clustering, opening $k$\nfacilities induces an assignment cost vector across the clients. In this paper\nwe consider the following minimum norm optimization problem : Given an\narbitrary monotone, symmetric norm, find a solution which minimizes the norm of\nthe induced cost-vector. This generalizes many fundamental NP-hard problems.\n  We give a general framework to tackle the minimum norm problem, and\nillustrate its efficacy in the unrelated machine load balancing and\n$k$-clustering setting. Our concrete results are the following.\n  $\\bullet$ We give constant factor approximation algorithms for the minimum\nnorm load balancing problem in unrelated machines, and the minimum norm\n$k$-clustering problem. To our knowledge, our results constitute the first\nconstant-factor approximations for such a general suite of objectives.\n  $\\bullet$ In load balancing with unrelated machines, we give a\n$2$-approximation for the problem of finding an assignment minimizing the sum\nof the largest $\\ell$ loads, for any $\\ell$. We give a\n$(2+\\varepsilon)$-approximation for the so-called ordered load-balancing\nproblem.\n  $\\bullet$ For $k$-clustering, we give a $(5+\\varepsilon)$-approximation for\nthe ordered $k$-median problem significantly improving the constant factor\napproximations from Byrka, Sornat, and Spoerhase (STOC 2018) and Chakrabarty\nand Swamy (ICALP 2018).\n  $\\bullet$ Our techniques also imply $O(1)$ approximations to the best\nsimultaneous optimization factor for any instance of the unrelated machine\nload-balancing and the $k$-clustering setting. To our knowledge, these are the\nfirst positive simultaneous optimization results in these settings.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 22:10:29 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "1811.05100", "submitter": "Karthik Abinav Sankararaman", "authors": "John P. Dickerson and Karthik Abinav Sankararaman and Aravind\n  Srinivasan and Pan Xu", "title": "Balancing Relevance and Diversity in Online Bipartite Matching via\n  Submodularity", "comments": "To appear in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In bipartite matching problems, vertices on one side of a bipartite graph are\npaired with those on the other. In its online variant, one side of the graph is\navailable offline, while the vertices on the other side arrive online. When a\nvertex arrives, an irrevocable and immediate decision should be made by the\nalgorithm; either match it to an available vertex or drop it. Examples of such\nproblems include matching workers to firms, advertisers to keywords, organs to\npatients, and so on. Much of the literature focuses on maximizing the total\nrelevance---modeled via total weight---of the matching. However, in many\nreal-world problems, it is also important to consider contributions of\ndiversity: hiring a diverse pool of candidates, displaying a relevant but\ndiverse set of ads, and so on. In this paper, we propose the Online Submodular\nBipartite Matching (\\osbm) problem, where the goal is to maximize a submodular\nfunction $f$ over the set of matched edges. This objective is general enough to\ncapture the notion of both diversity (\\emph{e.g.,} a weighted coverage\nfunction) and relevance (\\emph{e.g.,} the traditional linear function)---as\nwell as many other natural objective functions occurring in practice\n(\\emph{e.g.,} limited total budget in advertising settings). We propose novel\nalgorithms that have provable guarantees and are essentially optimal when\nrestricted to various special cases. We also run experiments on real-world and\nsynthetic datasets to validate our algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 04:43:05 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Dickerson", "John P.", ""], ["Sankararaman", "Karthik Abinav", ""], ["Srinivasan", "Aravind", ""], ["Xu", "Pan", ""]]}, {"id": "1811.05160", "submitter": "Krist\\'of B\\'erczi", "authors": "Krist\\'of B\\'erczi, Endre Boros, Ond\\v{r}ej \\v{C}epek, Petr\n  Ku\\v{c}era, Kazuhisa Makino", "title": "Approximating minimum representations of key Horn functions", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Horn functions form a subclass of Boolean functions and appear in many\ndifferent areas of computer science and mathematics as a general tool to\ndescribe implications and dependencies. Finding minimum sized representations\nfor such functions with respect to most commonly used measures is a\ncomputationally hard problem that remains hard even for the important subclass\nof key Horn functions. In this paper we provide logarithmic factor\napproximation algorithms for key Horn functions with respect to all measures\nstudied in the literature for which the problem is known to be hard.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 08:30:17 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 15:06:51 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["B\u00e9rczi", "Krist\u00f3f", ""], ["Boros", "Endre", ""], ["\u010cepek", "Ond\u0159ej", ""], ["Ku\u010dera", "Petr", ""], ["Makino", "Kazuhisa", ""]]}, {"id": "1811.05652", "submitter": "Junzhou Zhao", "authors": "Junzhou Zhao, Shuo Shang, Pinghui Wang, John C.S. Lui, Xiangliang\n  Zhang", "title": "Submodular Optimization Over Streams with Inhomogeneous Decays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardinality constrained submodular function maximization, which aims to\nselect a subset of size at most $k$ to maximize a monotone submodular utility\nfunction, is the key in many data mining and machine learning applications such\nas data summarization and maximum coverage problems. When data is given as a\nstream, streaming submodular optimization (SSO) techniques are desired.\nExisting SSO techniques can only apply to insertion-only streams where each\nelement has an infinite lifespan, and sliding-window streams where each element\nhas a same lifespan (i.e., window size). However, elements in some data streams\nmay have arbitrary different lifespans, and this requires addressing SSO over\nstreams with inhomogeneous-decays (SSO-ID). This work formulates the SSO-ID\nproblem and presents three algorithms: BasicStreaming is a basic streaming\nalgorithm that achieves an $(1/2-\\epsilon)$ approximation factor; HistApprox\nimproves the efficiency significantly and achieves an $(1/3-\\epsilon)$\napproximation factor; HistStreaming is a streaming version of HistApprox and\nuses heuristics to further improve the efficiency. Experiments conducted on\nreal data demonstrate that HistStreaming can find high quality solutions and is\nup to two orders of magnitude faster than the naive Greedy algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 05:34:16 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Zhao", "Junzhou", ""], ["Shang", "Shuo", ""], ["Wang", "Pinghui", ""], ["Lui", "John C. S.", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "1811.05808", "submitter": "Ludovic Stephan", "authors": "Ludovic Stephan, Laurent Massouli\\'e", "title": "Robustness of spectral methods for community detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work is concerned with community detection. Specifically, we\nconsider a random graph drawn according to the stochastic block model~: its\nvertex set is partitioned into blocks, or communities, and edges are placed\nrandomly and independently of each other with probability depending only on the\ncommunities of their two endpoints. In this context, our aim is to recover the\ncommunity labels better than by random guess, based only on the observation of\nthe graph.\n  In the sparse case, where edge probabilities are in $O(1/n)$, we introduce a\nnew spectral method based on the distance matrix $D^{(l)}$, where $D^{(l)}_{ij}\n= 1$ iff the graph distance between $i$ and $j$, noted $d(i, j)$ is equal to\n$\\ell$. We show that when $\\ell \\sim c\\log(n)$ for carefully chosen $c$, the\neigenvectors associated to the largest eigenvalues of $D^{(l)}$ provide enough\ninformation to perform non-trivial community recovery with high probability,\nprovided we are above the so-called Kesten-Stigum threshold. This yields an\nefficient algorithm for community detection, since computation of the matrix\n$D^{(l)}$ can be done in $O(n^{1+\\kappa})$ operations for a small constant\n$\\kappa$.\n  We then study the sensitivity of the eigendecomposition of $D^{(l)}$ when we\nallow an adversarial perturbation of the edges of $G$. We show that when the\nconsidered perturbation does not affect more than $O(n^\\varepsilon)$ vertices\nfor some small $\\varepsilon > 0$, the highest eigenvalues and their\ncorresponding eigenvectors incur negligible perturbations, which allows us to\nstill perform efficient recovery.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 14:37:20 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 00:53:21 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Stephan", "Ludovic", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "1811.05863", "submitter": "Xu Han", "authors": "Xu Han, Laurent Albera, Amar Kachenoura, Huazhong Shu, Lotfi Senhadji", "title": "Robust low-rank multilinear tensor approximation for a joint estimation\n  of the multilinear rank and the loading matrices", "comments": "it needs to be improved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to compute the best low-rank tensor approximation using the\nMultilinear Tensor Decomposition (MTD) model, it is essential to estimate the\nrank of the underlying multilinear tensor from the noisy observation tensor. In\nthis paper, we propose a Robust MTD (R-MTD) method, which jointly estimates the\nmultilinear rank and the loading matrices. Based on the low-rank property and\nan over-estimation of the core tensor, this joint estimation problem is solved\nby promoting (group) sparsity of the over-estimated core tensor. Group sparsity\nis promoted using mixed-norms. Then we establish a link between the mixed-norms\nand the nuclear norm, showing that mixed-norms are better candidates for a\nconvex envelope of the rank. After several iterations of the Alternating\nDirection Method of Multipliers (ADMM), the Minimum Description Length (MDL)\ncriterion computed from the eigenvalues of the unfolding matrices of the\nestimated core tensor is minimized in order to estimate the multilinear rank.\nThe latter is then used to estimate more accurately the loading matrices. We\nfurther develop another R-MTD method, called R-OMTD, by imposing an\northonormality constraint on each loading matrix in order to decrease the\ncomputation complexity. A series of simulated noisy tensor and real-world data\nare used to show the effectiveness of the proposed methods compared with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 15:46:34 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 15:32:29 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Han", "Xu", ""], ["Albera", "Laurent", ""], ["Kachenoura", "Amar", ""], ["Shu", "Huazhong", ""], ["Senhadji", "Lotfi", ""]]}, {"id": "1811.05916", "submitter": "Neil Olver", "authors": "Neil Olver, Frans Schalekamp, Suzanne van der Ster, Leen Stougie, Anke\n  van Zuylen", "title": "A Duality Based 2-Approximation Algorithm for Maximum Agreement Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a 2-approximation algorithm for the Maximum Agreement Forest problem\non two rooted binary trees. This NP-hard problem has been studied extensively\nin the past two decades, since it can be used to compute the rooted Subtree\nPrune-and-Regraft (rSPR) distance between two phylogenetic trees. Our algorithm\nis combinatorial and its running time is quadratic in the input size. To prove\nthe approximation guarantee, we construct a feasible dual solution for a novel\nlinear programming formulation. In addition, we show this linear program is\nstronger than previously known formulations, and we give a compact formulation,\nshowing that it can be solved in polynomial time\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 17:22:16 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Olver", "Neil", ""], ["Schalekamp", "Frans", ""], ["van der Ster", "Suzanne", ""], ["Stougie", "Leen", ""], ["van Zuylen", "Anke", ""]]}, {"id": "1811.05974", "submitter": "Andreas Henelius", "authors": "Kai Puolam\\\"aki, Andreas Henelius, Antti Ukkonen", "title": "Randomisation Algorithms for Large Sparse Matrices", "comments": null, "journal-ref": "Phys. Rev. E 99, 053311 (2019)", "doi": "10.1103/PhysRevE.99.053311", "report-no": null, "categories": "cs.DS physics.data-an q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many domains it is necessary to generate surrogate networks, e.g., for\nhypothesis testing of different properties of a network. Furthermore,\ngenerating surrogate networks typically requires that different properties of\nthe network is preserved, e.g., edges may not be added or deleted and the edge\nweights may be restricted to certain intervals. In this paper we introduce a\nnovel efficient property-preserving Markov Chain Monte Carlo method termed\nCycleSampler for generating surrogate networks in which (i) edge weights are\nconstrained to an interval and node weights are preserved exactly, and (ii)\nedge and node weights are both constrained to intervals. These two types of\nconstraints cover a wide variety of practical use-cases. The method is\napplicable to both undirected and directed graphs. We empirically demonstrate\nthe efficiency of the CycleSampler method on real-world datasets. We provide an\nimplementation of CycleSampler in R, with parts implemented in C.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 09:05:11 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Puolam\u00e4ki", "Kai", ""], ["Henelius", "Andreas", ""], ["Ukkonen", "Antti", ""]]}, {"id": "1811.06011", "submitter": "Alexandru Paler", "authors": "Alexandru Paler, Austin Fowler, Robert Wille", "title": "Faster manipulation of large quantum circuits using wire label reference\n  diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale quantum computing is highly anticipated, and quantum circuit\ndesign automation needs to keep up with the transition from small scale to\nlarge scale problems. Methods to support fast quantum circuit manipulations\n(e.g.~gate replacement, wire reordering, etc.) or specific circuit analysis\noperations have not been considered important and have been often implemented\nin a naive manner thus far. For example, quantum circuits are usually\nrepresented in term of one-dimensional gate lists or as directed acyclic\ngraphs. Although implementations for quantum circuit manipulations are often\nonly of polynomial complexity, the sheer number of possibilities to consider\nwith increasing scales of quantum computations make these representations\nhighly inefficient -- constituting a serious bottleneck. At the same time,\nquantum circuits have structural characteristics, which allow for more specific\nand faster approaches. This work utilises these characteristics by introducing\na dedicated representation for large quantum circuits, namely wire label\nreference diagrams. We apply the representation to a set of very common circuit\ntransformations, and develop corresponding solutions which achieve orders of\nmagnitude performance improvements for circuits which include up to 80 000\nqubits and 200 000 gates. The implementation of the proposed method is\navailable online.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 19:11:00 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Paler", "Alexandru", ""], ["Fowler", "Austin", ""], ["Wille", "Robert", ""]]}, {"id": "1811.06026", "submitter": "Jieming Mao", "authors": "Nicole Immorlica, Jieming Mao, Aleksandrs Slivkins, Zhiwei Steven Wu", "title": "Incentivizing Exploration with Selective Data Disclosure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the design of rating systems that incentivize efficient social\nlearning. Agents arrive sequentially and choose actions, each of which yields a\nreward drawn from an unknown distribution. A policy maps the rewards of\npreviously-chosen actions to messages for arriving agents. The regret of a\npolicy is the difference, over all rounds, between the expected reward of the\nbest action and the reward induced by the policy. Prior work proposes policies\nthat recommend a single action to each agent, obtaining optimal regret under\nstandard rationality assumptions. We instead assume a frequentist behavioral\nmodel and, accordingly, restrict attention to disclosure policies that use\nmessages consisting of the actions and rewards from a subsequence of past\nagents, chosen ex ante. We design a policy with optimal regret in the worst\ncase over reward distributions. Our research suggests three components of\neffective polices: independent focus groups, group aggregators, and interlaced\ninformation structures.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 19:29:16 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 19:26:09 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 15:58:06 GMT"}, {"version": "v4", "created": "Tue, 29 Dec 2020 16:41:07 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Immorlica", "Nicole", ""], ["Mao", "Jieming", ""], ["Slivkins", "Aleksandrs", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1811.06072", "submitter": "Chunjiang Zhu", "authors": "Chun Jiang Zhu, Tan Zhu, Kam-Yiu Lam, Song Han, Jinbo Bi", "title": "Communication-Optimal Distributed Dynamic Graph Clustering", "comments": "Accepted and to appear in AAAI'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering graph nodes over large-scale dynamic\ngraphs, such as citation networks, images and web networks, when graph updates\nsuch as node/edge insertions/deletions are observed distributively. We propose\ncommunication-efficient algorithms for two well-established communication\nmodels namely the message passing and the blackboard models. Given a graph with\n$n$ nodes that is observed at $s$ remote sites over time $[1,t]$, the two\nproposed algorithms have communication costs $\\tilde{O}(ns)$ and\n$\\tilde{O}(n+s)$ ($\\tilde{O}$ hides a polylogarithmic factor), almost matching\ntheir lower bounds, $\\Omega(ns)$ and $\\Omega(n+s)$, respectively, in the\nmessage passing and the blackboard models. More importantly, we prove that at\neach time point in $[1,t]$ our algorithms generate clustering quality nearly as\ngood as that of centralizing all updates up to that time and then applying a\nstandard centralized clustering algorithm. We conducted extensive experiments\non both synthetic and real-life datasets which confirmed the communication\nefficiency of our approach over baseline algorithms while achieving comparable\nclustering results.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 21:33:45 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Zhu", "Chun Jiang", ""], ["Zhu", "Tan", ""], ["Lam", "Kam-Yiu", ""], ["Han", "Song", ""], ["Bi", "Jinbo", ""]]}, {"id": "1811.06114", "submitter": "Kevin Schewior", "authors": "Jos\\'e R. Correa, Paul D\\\"utting, Felix Fischer, Kevin Schewior", "title": "Prophet Inequalities for I.I.D. Random Variables from an Unknown\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central object in optimal stopping theory is the single-choice prophet\ninequality for independent, identically distributed random variables: Given a\nsequence of random variables $X_1,\\dots,X_n$ drawn independently from a\ndistribution $F$, the goal is to choose a stopping time $\\tau$ so as to\nmaximize $\\alpha$ such that for all distributions $F$ we have\n$\\mathbb{E}[X_\\tau] \\geq \\alpha \\cdot \\mathbb{E}[\\max_tX_t]$. What makes this\nproblem challenging is that the decision whether $\\tau=t$ may only depend on\nthe values of the random variables $X_1,\\dots,X_t$ and on the distribution $F$.\nFor quite some time the best known bound for the problem was\n$\\alpha\\geq1-1/e\\approx0.632$ [Hill and Kertz, 1982]. Only recently this bound\nwas improved by Abolhassani et al. [2017], and a tight bound of\n$\\alpha\\approx0.745$ was obtained by Correa et al. [2017]. The case where $F$\nis unknown, such that the decision whether $\\tau=t$ may depend only on the\nvalues of the first $t$ random variables but not on $F$, is equally well\nmotivated (e.g., [Azar et al., 2014]) but has received much less attention. A\nstraightforward guarantee for this case of $\\alpha\\geq1/e\\approx0.368$ can be\nderived from the solution to the secretary problem. Our main result is that\nthis bound is tight. Motivated by this impossibility result we investigate the\ncase where the stopping time may additionally depend on a limited number of\nsamples from~$F$. An extension of our main result shows that even with $o(n)$\nsamples $\\alpha\\leq 1/e$, so that the interesting case is the one with\n$\\Omega(n)$ samples. Here we show that $n$ samples allow for a significant\nimprovement over the secretary problem, while $O(n^2)$ samples are equivalent\nto knowledge of the distribution: specifically, with $n$ samples\n$\\alpha\\geq1-1/e\\approx0.632$ and $\\alpha\\leq\\ln(2)\\approx0.693$, and with\n$O(n^2)$ samples $\\alpha\\geq0.745-\\epsilon$ for any $\\epsilon>0$.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 23:29:46 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 14:49:13 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Correa", "Jos\u00e9 R.", ""], ["D\u00fctting", "Paul", ""], ["Fischer", "Felix", ""], ["Schewior", "Kevin", ""]]}, {"id": "1811.06127", "submitter": "Roman Snytsar", "authors": "Roman Snytsar", "title": "Vectorized Character Counting for Faster Pattern Matching", "comments": null, "journal-ref": null, "doi": "10.5220/0007258201490154", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many modern sequence alignment tools implement fast string matching using the\nspace efficient data structure called FM-index. The succinct nature of this\ndata structure presents unique challenges for the algorithm designers. In this\npaper, we explore the opportunities for parallelization of the exact and\ninexact matches and present an efficient SIMD solution for the Occ portion of\nthe algorithm. Our implementation computes all eight Occ values required for\nthe inexact match algorithm step in a single pass. We showcase the algorithm\nperformance in a multi-core genome aligner and discuss effects of the memory\nprefetch.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 00:40:26 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 00:48:06 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Snytsar", "Roman", ""]]}, {"id": "1811.06244", "submitter": "Bartlomiej Dudek", "authors": "Bart{\\l}omiej Dudek and Pawe{\\l} Gawrychowski", "title": "Computing Quartet Distance is Equivalent to Counting 4-Cycles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quartet distance is a measure of similarity used to compare two unrooted\nphylogenetic trees on the same set of $n$ leaves, defined as the number of\nsubsets of four leaves related by a different topology in both trees. After a\nseries of previous results, Brodal et al. [SODA 2013] presented an algorithm\nthat computes this number in $\\mathcal{O}(nd\\log n)$ time, where $d$ is the\nmaximum degree of a node. Our main contribution is a two-way reduction\nestablishing that the complexity of computing the quartet distance between two\ntrees on $n$ leaves is the same, up to polylogarithmic factors, as the\ncomplexity of counting 4-cycles in an undirected simple graph with $m$ edges.\nThe latter problem has been extensively studied, and the fastest known\nalgorithm by Vassilevska Williams [SODA 2015] works in $\\mathcal{O}(m^{1.48})$\ntime. In fact, even for the seemingly simpler problem of detecting a 4-cycle,\nthe best known algorithm works in $\\mathcal{O}(m^{4/3})$ time, and a conjecture\nof Yuster and Zwick implies that this might be optimal. In particular, an\nalmost-linear time for computing the quartet distance would imply a\nsurprisingly efficient algorithm for counting 4-cycles. In the other direction,\nby plugging in the state-of-the-art algorithms for counting 4-cycles, our\nreduction allows us to significantly decrease the complexity of computing the\nquartet distance. For trees with unbounded degrees we obtain an\n$\\mathcal{O}(n^{1.48})$ time algorithm, which is a substantial improvement on\nthe previous bound of $\\mathcal{O}(n^{2}\\log n)$. For trees with degrees\nbounded by $d$, by analysing the reduction more carefully, we are able to\nobtain an $\\mathcal{\\tilde O}(nd^{0.77})$ time algorithm, which is again a\nnontrivial improvement on the previous bound of $\\mathcal{O}(nd\\log n)$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 09:14:56 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 13:31:26 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Dudek", "Bart\u0142omiej", ""], ["Gawrychowski", "Pawe\u0142", ""]]}, {"id": "1811.06444", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Eric Balkanski, Renato Paes Leme", "title": "Secretary Ranking with Minimal Inversions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a twist on the classic secretary problem, which we term the\nsecretary ranking problem: elements from an ordered set arrive in random order\nand instead of picking the maximum element, the algorithm is asked to assign a\nrank, or position, to each of the elements. The rank assigned is irrevocable\nand is given knowing only the pairwise comparisons with elements previously\narrived. The goal is to minimize the distance of the rank produced to the true\nrank of the elements measured by the Kendall-Tau distance, which corresponds to\nthe number of pairs that are inverted with respect to the true order.\n  Our main result is a matching upper and lower bound for the secretary ranking\nproblem. We present an algorithm that ranks $n$ elements with only $O(n^{3/2})$\ninversions in expectation, and show that any algorithm necessarily suffers\n$\\Omega(n^{3/2})$ inversions when there are $n$ available positions. In terms\nof techniques, the analysis of our algorithm draws connections to linear\nprobing in the hashing literature, while our lower bound result relies on a\ngeneral anti-concentration bound for a generic balls and bins sampling process.\nWe also consider the case where the number of positions $m$ can be larger than\nthe number of secretaries $n$ and provide an improved bound by showing a\nconnection of this problem with random binary trees.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 15:59:28 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Assadi", "Sepehr", ""], ["Balkanski", "Eric", ""], ["Leme", "Renato Paes", ""]]}, {"id": "1811.06494", "submitter": "Shreyas Pai", "authors": "Tanmay Inamdar, Shreyas Pai, Sriram V. Pemmaraju", "title": "Large-Scale Distributed Algorithms for Facility Location with Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents fast, distributed, $O(1)$-approximation algorithms for\nmetric facility location problems with outliers in the Congested Clique model,\nMassively Parallel Computation (MPC) model, and in the $k$-machine model. The\npaper considers Robust Facility Location and Facility Location with Penalties,\ntwo versions of the facility location problem with outliers proposed by\nCharikar et al. (SODA 2001). The paper also considers two alternatives for\nspecifying the input: the input metric can be provided explicitly (as an $n\n\\times n$ matrix distributed among the machines) or implicitly as the shortest\npath metric of a given edge-weighted graph. The results in the paper are:\n  - Implicit metric: For both problems, $O(1)$-approximation algorithms running\nin $O(\\mbox{poly}(\\log n))$ rounds in the Congested Clique and the MPC model\nand $O(1)$-approximation algorithms running in $\\tilde{O}(n/k)$ rounds in the\n$k$-machine model.\n  - Explicit metric: For both problems, $O(1)$-approximation algorithms running\nin $O(\\log\\log\\log n)$ rounds in the Congested Clique and the MPC model and\n$O(1)$-approximation algorithms running in $\\tilde{O}(n/k)$ rounds in the\n$k$-machine model.\n  Our main contribution is to show the existence of Mettu-Plaxton-style\n$O(1)$-approximation algorithms for both Facility Location with outlier\nproblems. As shown in our previous work (Berns et al., ICALP 2012,\nBandyapadhyay et al., ICDCN 2018) Mettu-Plaxton style algorithms are more\neasily amenable to being implemented efficiently in distributed and large-scale\nmodels of computation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 17:40:30 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Inamdar", "Tanmay", ""], ["Pai", "Shreyas", ""], ["Pemmaraju", "Sriram V.", ""]]}, {"id": "1811.06603", "submitter": "Lin Chen", "authors": "Lin Chen, Moran Feldman, Amin Karbasi", "title": "Unconstrained Submodular Maximization with Constant Adaptive Complexity", "comments": "Authors are listed in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the unconstrained submodular maximization problem.\nWe propose the first algorithm for this problem that achieves a tight\n$(1/2-\\varepsilon)$-approximation guarantee using $\\tilde{O}(\\varepsilon^{-1})$\nadaptive rounds and a linear number of function evaluations. No previously\nknown algorithm for this problem achieves an approximation ratio better than\n$1/3$ using less than $\\Omega(n)$ rounds of adaptivity, where $n$ is the size\nof the ground set. Moreover, our algorithm easily extends to the maximization\nof a non-negative continuous DR-submodular function subject to a box constraint\nand achieves a tight $(1/2-\\varepsilon)$-approximation guarantee for this\nproblem while keeping the same adaptive and query complexities.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 21:56:33 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 01:39:47 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Chen", "Lin", ""], ["Feldman", "Moran", ""], ["Karbasi", "Amin", ""]]}, {"id": "1811.06719", "submitter": "Adam Kasperski", "authors": "Mmikita Hradovich, Adam Kasperski, Pawel Zielinski", "title": "Robust recoverable 0-1 optimization problems under polyhedral\n  uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with a robust recoverable approach to 0-1 programming\nproblems. It is assumed that a solution constructed in the first stage can be\nmodified to some extent in the second stage. This modification consists in\nchoosing a solution in some prescribed neighborhood of the current solution.\nThe second stage solution cost can be uncertain and a polyhedral structure of\nuncertainty is used. The resulting robust recoverable problem is a min-max-min\nproblem, which can be hard to solve when the number of variables is large. In\nthis paper we provide a framework for solving robust recoverable 0-1\nprogramming problems with a specified polyhedral uncertainty and propose\nseveral lower bounds and approximate solutions, which can be used for a wide\nclass of 0-1 optimization problems. The results of computational tests for two\nproblems, namely the assignment and the knapsack ones, are also presented.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 09:27:42 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Hradovich", "Mmikita", ""], ["Kasperski", "Adam", ""], ["Zielinski", "Pawel", ""]]}, {"id": "1811.06749", "submitter": "Jakub T\\v{e}tek", "authors": "Tom\\'a\\v{s} Gaven\\v{c}iak, Jakub T\\v{e}tek", "title": "Compact I/O-Efficient Representation of Separable Graphs and Optimal\n  Tree Layouts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compact and I/O-efficient data representations play an important role in\nefficient algorithm design, as memory bandwidth and latency can present a\nsignificant performance bottleneck, slowing the computation by orders of\nmagnitude. While this problem is very well explored in e.g. uniform numerical\ndata processing, structural data applications (e.g. on huge graphs) require\ndifferent algorithm-dependent approaches. Separable graph classes (i.e. graph\nclasses with balanced separators of size $\\mathcal{O}(n^c)$ with $c < 1$)\ninclude planar graphs, bounded genus graphs, and minor-free graphs.\n  In this article we present two generalizations of the separator theorem, to\npartitions with small regions only on average and to weighted graphs. Then we\npropose I/O-efficient succinct representation and memory layout for random\nwalks in(weighted) separable graphs in the pointer machine model, including an\nefficient algorithm to compute them. Finally, we present a worst-case\nI/O-optimal tree layout algorithm for root-leaf path traversal, show an\nadditive (+1)-approximation of optimal compact layout and contrast this with\nNP-completeness proof of finding an optimal compact layout.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 11:01:06 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Gaven\u010diak", "Tom\u00e1\u0161", ""], ["T\u011btek", "Jakub", ""]]}, {"id": "1811.06803", "submitter": "Markus Blumenstock", "authors": "Markus Blumenstock and Frank Fischer", "title": "A Constructive Arboricity Approximation Scheme", "comments": "v4: Better runtime with a different union-find structure, notes on\n  near-exact algorithm, minor improvements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The arboricity $\\Gamma$ of a graph is the minimum number of forests its edge\nset can be partitioned into. Previous approximation schemes were\nnonconstructive, i.e., they only approximated the arboricity as a value without\ncomputing a corresponding forest partition. This is because they operate on the\nrelated pseudoforest partitions or the dual problem of finding dense subgraphs.\n  We propose an algorithm for converting a partition of $k$ pseudoforests into\na partition of $k+1$ forests in $O(mk\\log k + m \\log n)$ time with a data\nstructure by Brodal and Fagerberg that stores graphs of arboricity $k$. A\nslightly better bound can be given when perfect hashing is used. When applied\nto a pseudoforest partition obtained from Kowalik's approximation scheme, our\nconversion implies a constructive $(1+\\epsilon)$-approximation algorithm with\nruntime $O(m \\log n \\log \\Gamma\\, \\epsilon^{-1})$ for every $\\epsilon > 0$. For\nfixed $\\epsilon$, the runtime can be reduced to $O(m \\log n)$.\n  Our conversion also implies a near-exact algorithm that computes a partition\ninto at most $\\Gamma+2$ forests in $O(m\\log n \\,\\Gamma \\log^* \\Gamma)$ time. It\nmight also pave the way to faster exact arboricity algorithms.\n  We also make several remarks on approximation algorithms for the\npseudoarboricity and the equivalent graph orientations with smallest maximum\nindegree, and correct some mistakes made in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 13:57:32 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 17:33:31 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 13:17:14 GMT"}, {"version": "v4", "created": "Thu, 5 Sep 2019 11:50:12 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Blumenstock", "Markus", ""], ["Fischer", "Frank", ""]]}, {"id": "1811.06871", "submitter": "Sandor Kisfaludi-Bak", "authors": "S\\'andor Kisfaludi-Bak, Jesper Nederlof and Erik Jan van Leeuwen", "title": "Nearly ETH-Tight Algorithms for Planar Steiner Tree with Terminals on\n  Few Faces", "comments": "32 pages, 8 figures, accepted at SODA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Planar Steiner Tree problem is one of the most fundamental NP-complete\nproblems as it models many network design problems. Recall that an instance of\nthis problem consists of a graph with edge weights, and a subset of vertices\n(often called terminals); the goal is to find a subtree of the graph of minimum\ntotal weight that connects all terminals. A seminal paper by Erickson et al.\n[Math. Oper. Res., 1987] considers instances where the underlying graph is\nplanar and all terminals can be covered by the boundary of $k$ faces. Erickson\net al. show that the problem can be solved by an algorithm using $n^{O(k)}$\ntime and $n^{O(k)}$ space, where $n$ denotes the number of vertices of the\ninput graph. In the past 30 years there has been no significant improvement of\nthis algorithm, despite several efforts.\n  In this work, we give an algorithm for Planar Steiner Tree with running time\n$2^{O(k)} n^{O(\\sqrt{k})}$ using only polynomial space. Furthermore, we show\nthat the running time of our algorithm is almost tight: we prove that there is\nno $f(k)n^{o(\\sqrt{k})}$ algorithm for Planar Steiner Tree for any computable\nfunction $f$, unless the Exponential Time Hypothesis fails.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 15:47:38 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Kisfaludi-Bak", "S\u00e1ndor", ""], ["Nederlof", "Jesper", ""], ["van Leeuwen", "Erik Jan", ""]]}, {"id": "1811.06892", "submitter": "Steven Kelk", "authors": "Steven Kelk and Simone Linz", "title": "A tight kernel for computing the tree bisection and reconnection\n  distance between two phylogenetic trees", "comments": "One figure added, two small typos fixed. This version to appear in\n  SIDMA (SIAM Journal on Discrete Mathematics)", "journal-ref": "SIAM Journal on Discrete Mathematics, 33:1556-1574, 2019", "doi": "10.1137/18M122724X", "report-no": null, "categories": "cs.DS math.CO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2001 Allen and Steel showed that, if subtree and chain reduction rules\nhave been applied to two unrooted phylogenetic trees, the reduced trees will\nhave at most 28k taxa where k is the TBR (Tree Bisection and Reconnection)\ndistance between the two trees. Here we reanalyse Allen and Steel's\nkernelization algorithm and prove that the reduced instances will in fact have\nat most 15k-9 taxa. Moreover we show, by describing a family of instances which\nhave exactly 15k-9 taxa after reduction, that this new bound is tight. These\ninstances also have no common clusters, showing that a third\ncommonly-encountered reduction rule, the cluster reduction, cannot further\nreduce the size of the kernel in the worst case. To achieve these results we\nintroduce and use \"unrooted generators\" which are analogues of rooted\nstructures that have appeared earlier in the phylogenetic networks literature.\nUsing similar argumentation we show that, for the minimum hybridization problem\non two rooted trees, 9k-2 is a tight bound (when subtree and chain reduction\nrules have been applied) and 9k-4 is a tight bound (when, additionally, the\ncluster reduction has been applied) on the number of taxa, where k is the\nhybridization number of the two trees.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 16:29:16 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 16:44:37 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Kelk", "Steven", ""], ["Linz", "Simone", ""]]}, {"id": "1811.06933", "submitter": "Travis Gagie", "authors": "Alan Kuhnle, Taher Mun, Christina Boucher, Travis Gagie, Ben Langmead\n  and Giovanni Manzini", "title": "Efficient Construction of a Complete Index for Pan-Genomics Read\n  Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While short read aligners, which predominantly use the FM-index, are able to\neasily index one or a few human genomes, they do not scale well to indexing\ndatabases containing thousands of genomes. To understand why, it helps to\nexamine the main components of the FM-index in more detail, which is a rank\ndata structure over the Burrows-Wheeler Transform (BWT) of the string that will\nallow us to find the interval in the string's suffix array (SA) containing\npointers to starting positions of occurrences of a given pattern; second, a\nsample of the SA that --- when used with the rank data structure --- allows us\naccess the SA. The rank data structure can be kept small even for large genomic\ndatabases, by run-length compressing the BWT, but until recently there was no\nmeans known to keep the SA sample small without greatly slowing down access to\nthe SA. Now that Gagie et al. (SODA 2018) have defined an SA sample that takes\nabout the same space as the run-length compressed BWT --- we have the design\nfor efficient FM-indexes of genomic databases but are faced with the problem of\nbuilding them. In 2018 we showed how to build the BWT of large genomic\ndatabases efficiently (WABI 2018) but the problem of building Gagie et al.'s SA\nsample efficiently was left open. We compare our approach to state-of-the-art\nmethods for constructing the SA sample, and demonstrate that it is the fastest\nand most space-efficient method on highly repetitive genomic databases. Lastly,\nwe apply our method for indexing partial and whole human genomes, and show that\nit improves over Bowtie with respect to both memory and time.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 17:33:22 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Kuhnle", "Alan", ""], ["Mun", "Taher", ""], ["Boucher", "Christina", ""], ["Gagie", "Travis", ""], ["Langmead", "Ben", ""], ["Manzini", "Giovanni", ""]]}, {"id": "1811.07327", "submitter": "Zolt\\'an Kir\\'aly", "authors": "Dehia Ait Ferhat, Zolt\\'an Kir\\'aly, Andr\\'as Seb\\H{o} and Gautier\n  Stauffer", "title": "How many matchings cover the nodes of a graph?", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph, are there $k$ matchings whose union covers all of\nits nodes, that is, a matching-$k$-cover? A first, easy polynomial solution\nfrom matroid union is possible, as already observed by Wang, Song and Yuan\n(Mathematical Programming, 2014). However, it was not satisfactory neither from\nthe algorithmic viewpoint nor for proving graphic theorems, since the\ncorresponding matroid ignores the edges of the graph.\n  We prove here, simply and algorithmically: all nodes of a graph can be\ncovered with $k\\ge 2$ matchings if and only if for every stable set $S$ we have\n$|S|\\le k\\cdot|N(S)|$. When $k=1$, an exception occurs: this condition is not\nenough to guarantee the existence of a matching-$1$-cover, that is, the\nexistence of a perfect matching, in this case Tutte's famous matching theorem\n(J. London Math. Soc., 1947) provides the right `good' characterization. The\ncondition above then guarantees only that a perfect $2$-matching exists, as\nknown from another theorem of Tutte (Proc. Amer. Math. Soc., 1953).\n  Some results are then deduced as consequences with surprisingly simple\nproofs, using only the level of difficulty of bipartite matchings. We give some\ngeneralizations, as well as a solution for minimization if the edge-weights are\nnon-negative, while the edge-cardinality maximization of matching-$2$-covers\nturns out to be already NP-hard.\n  We have arrived at this problem as the line graph special case of a model\narising for manufacturing integrated circuits with the technology called\n`Directed Self Assembly'.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 13:34:27 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 19:03:40 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Ferhat", "Dehia Ait", ""], ["Kir\u00e1ly", "Zolt\u00e1n", ""], ["Seb\u0151", "Andr\u00e1s", ""], ["Stauffer", "Gautier", ""]]}, {"id": "1811.07381", "submitter": "Lukas Graf", "authors": "Lukas Graf and Tobias Harks and Leon Sering", "title": "Dynamic Flows with Adaptive Route Choice", "comments": "40 pages, shorter version published in the \"Proceedings of the 20th\n  Conference on Integer Programming and Combinatorial Optimization, 2019\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study dynamic network flows and introduce a notion of instantaneous\ndynamic equilibrium (IDE) requiring that for any positive inflow into an edge,\nthis edge must lie on a currently shortest path towards the respective sink. We\nmeasure current shortest path length by current waiting times in queues plus\nphysical travel times. As our main results, we show: 1. existence and\nconstructive computation of IDE flows for single-source single-sink networks\nassuming constant network inflow rates, 2. finite termination of IDE flows for\nmulti-source single-sink networks assuming bounded and finitely lasting inflow\nrates, 3. the existence of IDE flows for multi-source multi-sink instances\nassuming general measurable network inflow rates, 4. the existence of a complex\nsingle-source multi-sink instance in which any IDE flow is caught in cycles and\nflow remains forever in the network.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 19:08:50 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 11:53:23 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 20:23:35 GMT"}, {"version": "v4", "created": "Sat, 14 Mar 2020 21:27:37 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Graf", "Lukas", ""], ["Harks", "Tobias", ""], ["Sering", "Leon", ""]]}, {"id": "1811.07413", "submitter": "Kanthi Sarpatwar", "authors": "Kanthi Sarpatwar and Baruch Schieber and Hadas Shachnai", "title": "The Preemptive Resource Allocation Problem", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit a classical scheduling model to incorporate modern trends in data\ncenter networks and cloud services. Addressing some key challenges in the\nallocation of shared resources to user requests (jobs) in such settings, we\nconsider the following variants of the classic {\\em resource allocation\nproblem} (\\textsf{RAP}). The input to our problems is a set $J$ of jobs and a\nset $M$ of homogeneous hosts, each has an available amount of some resource. A\njob is associated with a release time, a due date, a weight, and a given\nlength, as well as its resource requirement. A \\emph{feasible} schedule is an\nallocation of the resource to a subset of the jobs, satisfying the job release\ntimes/due dates as well as the resource constraints. A crucial distinction\nbetween classic {\\textsf{RAP}} and our problems is that we allow preemption and\nmigration of jobs, motivated by virtualization techniques.\n  We consider two natural objectives: {\\em throughput maximization}\n(\\textsf{MaxT}), which seeks a maximum weight subset of the jobs that can be\nfeasibly scheduled on the hosts in $M$, and {\\em resource minimization}\n(\\textsf{MinR}), that is finding the minimum number of (homogeneous) hosts\nneeded to feasibly schedule all jobs. Both problems are known to be NP-hard.\n  We first present a $\\Omega(1)$-approximation algorithm for \\textsf{MaxT}\ninstances where time-windows form a laminar family of intervals. We then extend\nthe algorithm to handle instances with arbitrary time-windows, assuming there\nis sufficient slack for each job to be completed.\n  For \\textsf{MinR} we study a more general setting with $d$ resources and\nderive an $O(\\log d)$-approximation for any fixed $d \\geq 1$, under the\nassumption that time-windows are not too small. This assumption can be removed\nleading to a slightly worse ratio of $O(\\log d\\log^* T)$, where $T$ is the\nmaximum due date of any job.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 22:19:09 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Sarpatwar", "Kanthi", ""], ["Schieber", "Baruch", ""], ["Shachnai", "Hadas", ""]]}, {"id": "1811.07443", "submitter": "Bhadrachalam Chitturi", "authors": "Bhadrachalam Chitturi and Indulekha T S", "title": "Sorting permutations with a transposition tree", "comments": "13 pages. 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The set of all permutations with $n$ symbols is a symmetric group denoted by\n$S_n$. A transposition tree, $T$, is a spanning tree over its $n$ vertices\n$V_T=${$1, 2, 3, \\ldots n$} where the vertices are the positions of a\npermutation $\\pi$ and $\\pi$ is in $S_n$. $T$ is the operation and the edge set\n$E_T$ denotes the corresponding generator set. The goal is to sort a given\npermutation $\\pi$ with $T$. The number of generators of $E_T$ that suffices to\nsort any $\\pi \\in S_n$ constitutes an upper bound. It is an upper bound, on the\ndiameter of the corresponding Cayley graph $\\Gamma$ i.e. $diam(\\Gamma)$. A\nprecise upper bound equals $diam(\\Gamma)$. Such bounds are known only for a few\ntrees. Jerrum showed that computing $diam(\\Gamma)$ is intractable in general if\nthe number of generators is two or more whereas $T$ has $n-1$ generators. For\nseveral operations computing a tight upper bound is of theoretical interest.\nSuch bounds have applications in evolutionary biology to compute the\nevolutionary relatedness of species and parallel/distributed computing for\nlatency estimation. The earliest algorithm computed an upper bound $f(\\Gamma)$\nin a $\\Omega(n!)$ time by examining all $\\pi$ in $S_n$. Subsequently,\npolynomial time algorithms were designed to compute upper bounds or their\nestimates. We design an upper bound $\\delta^*$ whose cumulative value for all\ntrees of a given size $n$ is shown to be the tightest for $n \\leq 15$. We show\nthat $\\delta^*$ is tightest known upper bound for full binary trees.\n  Keywords: Transposition trees, Cayley graphs, permutations, sorting, upper\nbound, diameter, greedy algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 00:54:05 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Chitturi", "Bhadrachalam", ""], ["S", "Indulekha T", ""]]}, {"id": "1811.07448", "submitter": "Omri Ben-Eliezer", "authors": "Omri Ben-Eliezer", "title": "Testing local properties of arrays", "comments": "ITCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study testing of local properties in one-dimensional and multi-dimensional\narrays. A property of $d$-dimensional arrays $f:[n]^d \\to \\Sigma$ is $k$-local\nif it can be defined by a family of $k \\times \\ldots \\times k$ forbidden\nconsecutive patterns. This definition captures numerous interesting properties.\nFor example, monotonicity, Lipschitz continuity and submodularity are\n$2$-local; convexity is (usually) $3$-local; and many typical problems in\ncomputational biology and computer vision involve $o(n)$-local properties.\n  In this work, we present a generic approach to test all local properties of\narrays over any finite (and not necessarily bounded size) alphabet. We show\nthat any $k$-local property of $d$-dimensional arrays is testable by a simple\ncanonical one-sided error non-adaptive $\\varepsilon$-test, whose query\ncomplexity is $O(\\epsilon^{-1}k \\log{\\frac{\\epsilon n}{k}})$ for $d = 1$ and\n$O(c_d \\epsilon^{-1/d} k \\cdot n^{d-1})$ for $d > 1$. The queries made by the\ncanonical test constitute sphere-like structures of varying sizes, and are\ncompletely independent of the property and the alphabet $\\Sigma$. The query\ncomplexity is optimal for a wide range of parameters: For $d=1$, this matches\nthe query complexity of many previously investigated local properties, while\nfor $d > 1$ we design and analyze new constructions of $k$-local properties\nwhose one-sided non-adaptive query complexity matches our upper bounds. For\nsome previously studied properties, our method provides the first known\nsublinear upper bound on the query complexity.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 01:24:51 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Ben-Eliezer", "Omri", ""]]}, {"id": "1811.07464", "submitter": "Alina Ene", "authors": "Alina Ene, Huy L. Nguyen", "title": "Towards Nearly-linear Time Algorithms for Submodular Maximization with a\n  Matroid Constraint", "comments": "There is text overlap with an earlier version arXiv:1709.09767v2.\n  That version has been replaced by a paper with only the result for a knapsack\n  constraint, and this paper has the results for matroid constraints", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider fast algorithms for monotone submodular maximization subject to a\nmatroid constraint. We assume that the matroid is given as input in an explicit\nform, and the goal is to obtain the best possible running times for important\nmatroids. We develop a new algorithm for a \\emph{general matroid constraint}\nwith a $1 - 1/e - \\epsilon$ approximation that achieves a fast running time\nprovided we have a fast data structure for maintaining a maximum weight base in\nthe matroid through a sequence of decrease weight operations. We construct such\ndata structures for graphic matroids and partition matroids, and we obtain the\n\\emph{first algorithms} for these classes of matroids that achieve a\nnearly-optimal, $1 - 1/e - \\epsilon$ approximation, using a nearly-linear\nnumber of function evaluations and arithmetic operations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 02:22:48 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "1811.07515", "submitter": "Ruosong Wang", "authors": "Lijie Chen, Ruosong Wang", "title": "Classical Algorithms from Quantum and Arthur-Merlin Communication\n  Protocols", "comments": "To appear in ITCS 2019. Abstract is shortened to meet the constraint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The polynomial method from circuit complexity has been applied to several\nfundamental problems and obtains the state-of-the-art running times. As\nobserved in [Alman and Williams, STOC 2017], almost all applications of the\npolynomial method in algorithm design ultimately rely on certain low-rank\ndecompositions of the computation matrices corresponding to key subroutines.\nThey suggest that making use of low-rank decompositions directly could lead to\nmore powerful algorithms, as the polynomial method is just one way to derive\nsuch a decomposition. Inspired by their observation, in this paper, we study\nanother way of systematically constructing low-rank decompositions of matrices\nwhich could be used by algorithms. It is known that various types of\ncommunication protocols lead to certain low-rank decompositions (e.g.,\n$\\mathsf{P}$ protocols/rank, $\\mathsf{BQP}$ protocols/approximate rank). These\nare usually interpreted as approaches for proving communication lower bounds,\nwhile in this work we explore the other direction.\n  We have the two generic algorithmic applications of communication protocols.\nThe first connection is that a fast $\\mathsf{BQP}$ communication protocol for a\nfunction $f$ implies a fast deterministic additive approximate counting\nalgorithm for a related pair counting problem. The second connection is that a\nfast $\\mathsf{AM}^{\\mathsf{cc}}$ protocol for a function $f$ implies a\nfaster-than-bruteforce algorithm for $f\\textsf{-Satisfying-Pair}$.\n  We also apply our second connection to shed some light on long-standing open\nproblems in communication complexity. We show that if the Longest Common\nSubsequence problem admits an efficient $\\mathsf{AM}^{\\mathsf{cc}}$ protocol,\nthen polynomial-size Formula-$\\textsf{SAT}$ admits a $2^{n - n^{1-\\delta}}$\ntime algorithm for any constant $\\delta > 0$.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 06:05:54 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Chen", "Lijie", ""], ["Wang", "Ruosong", ""]]}, {"id": "1811.07624", "submitter": "Cristian Rusu", "authors": "Cristian Rusu", "title": "Approximate Eigenvalue Decompositions of Linear Transformations with a\n  Few Householder Reflectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to decompose a signal in an orthonormal basis (a set of\northogonal components, each normalized to have unit length) using a fast\nnumerical procedure rests at the heart of many signal processing methods and\napplications. The classic examples are the Fourier and wavelet transforms that\nenjoy numerically efficient implementations (FFT and FWT, respectively).\nUnfortunately, orthonormal transformations are in general unstructured, and\ntherefore they do not enjoy low computational complexity properties. In this\npaper, based on Householder reflectors, we introduce a class of orthonormal\nmatrices that are numerically efficient to manipulate: we control the\ncomplexity of matrix-vector multiplications with these matrices using a given\nparameter. We provide numerical algorithms that approximate any orthonormal or\nsymmetric transform with a new orthonormal or symmetric structure made up of\nproducts of a given number of Householder reflectors. We show analyses and\nnumerical evidence to highlight the accuracy of the proposed approximations and\nprovide an application to the case of learning fast Mahanalobis distance metric\ntransformations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 11:34:16 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 12:19:05 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Rusu", "Cristian", ""]]}, {"id": "1811.07673", "submitter": "Hyo-Sang Shin PhD", "authors": "Teng Li and Hyo-Sang Shin and Antonios Tsourdos", "title": "Fast submodular maximization subject to k-extendible system constraints", "comments": "14 pages with no figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the scales of data sets expand rapidly in some application scenarios,\nincreasing efforts have been made to develop fast submodular maximization\nalgorithms. This paper presents a currently the most efficient algorithm for\nmaximizing general non-negative submodular objective functions subject to\n$k$-extendible system constraints. Combining the sampling process and the\ndecreasing threshold strategy, our algorithm Sample Decreasing Threshold Greedy\nAlgorithm (SDTGA) obtains an expected approximation guarantee of ($p-\\epsilon$)\nfor monotone submodular functions and of ($p(1-p)-\\epsilon$) for non-monotone\ncases with expected computational complexity of only\n$O(\\frac{pn}{\\epsilon}\\ln\\frac{r}{\\epsilon})$, where $r$ is the largest size of\nthe feasible solutions, $0<p \\leq \\frac{1}{1+k}$ is the sampling probability\nand $0< \\epsilon < p$. If we fix the sampling probability $p$ as\n$\\frac{1}{1+k}$, we get the best approximation ratios for both monotone and\nnon-monotone submodular functions which are $(\\frac{1}{1+k}-\\epsilon)$ and\n$(\\frac{k}{(1+k)^2}-\\epsilon)$ respectively. While the parameter $\\epsilon$\nexists for the trade-off between the approximation ratio and the time\ncomplexity. Therefore, our algorithm can handle larger scale of submodular\nmaximization problems than existing algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 13:32:37 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Li", "Teng", ""], ["Shin", "Hyo-Sang", ""], ["Tsourdos", "Antonios", ""]]}, {"id": "1811.07765", "submitter": "Aaron Roth", "authors": "Seth Neel, Aaron Roth, Zhiwei Steven Wu", "title": "How to Use Heuristics for Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop theory for using heuristics to solve computationally hard problems\nin differential privacy. Heuristic approaches have enjoyed tremendous success\nin machine learning, for which performance can be empirically evaluated.\nHowever, privacy guarantees cannot be evaluated empirically, and must be proven\n--- without making heuristic assumptions. We show that learning problems over\nbroad classes of functions can be solved privately and efficiently, assuming\nthe existence of a non-private oracle for solving the same problem. Our first\nalgorithm yields a privacy guarantee that is contingent on the correctness of\nthe oracle. We then give a reduction which applies to a class of heuristics\nwhich we call certifiable, which allows us to convert oracle-dependent privacy\nguarantees to worst-case privacy guarantee that hold even when the heuristic\nstanding in for the oracle might fail in adversarial ways. Finally, we consider\na broad class of functions that includes most classes of simple boolean\nfunctions studied in the PAC learning literature, including conjunctions,\ndisjunctions, parities, and discrete halfspaces. We show that there is an\nefficient algorithm for privately constructing synthetic data for any such\nclass, given a non-private learning oracle. This in particular gives the first\noracle-efficient algorithm for privately generating synthetic data for\ncontingency tables. The most intriguing question left open by our work is\nwhether or not every problem that can be solved differentially privately can be\nprivately solved with an oracle-efficient algorithm. While we do not resolve\nthis, we give a barrier result that suggests that any generic oracle-efficient\nreduction must fall outside of a natural class of algorithms (which includes\nthe algorithms given in this paper).\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 15:57:08 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Neel", "Seth", ""], ["Roth", "Aaron", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1811.07779", "submitter": "Marcin Pilipczuk", "authors": "Marcin Pilipczuk and Micha{\\l} Ziobro", "title": "Experimental Evaluation of Parameterized Algorithms for Graph Separation\n  Problems: Half-Integral Relaxations and Matroid-based Kernelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years we have witnessed a rapid development of new algorithmic\ntechniques for parameterized algorithms for graph separation problems. We\npresent experimental evaluation of two cornerstone theoretical results in this\narea: linear-time branching algorithms guided by half-integral relaxations and\nkernelization (preprocessing) routines based on representative sets in\nmatroids. A side contribution is a new set of benchmark instances of\n(unweighted, vertex-deletion) Multiway Cut.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 16:28:55 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Pilipczuk", "Marcin", ""], ["Ziobro", "Micha\u0142", ""]]}, {"id": "1811.07780", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Michael Kapralov, Sanjeev Khanna", "title": "A Simple Sublinear-Time Algorithm for Counting Arbitrary Subgraphs via\n  Edge Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the subgraph counting problem, we are given a input graph $G(V, E)$ and a\ntarget graph $H$; the goal is to estimate the number of occurrences of $H$ in\n$G$. Our focus here is on designing sublinear-time algorithms for approximately\ncounting occurrences of $H$ in $G$ in the setting where the algorithm is given\nquery access to $G$. This problem has been studied in several recent papers\nwhich primarily focused on specific families of graphs $H$ such as triangles,\ncliques, and stars. However, not much is known about approximate counting of\narbitrary graphs $H$. This is in sharp contrast to the closely related subgraph\nenumeration problem that has received significant attention in the database\ncommunity as the database join problem. The AGM bound shows that the maximum\nnumber of occurrences of any arbitrary subgraph $H$ in a graph $G$ with $m$\nedges is $O(m^{\\rho(H)})$, where $\\rho(H)$ is the fractional edge-cover of $H$,\nand enumeration algorithms with matching runtime are known for any $H$.\n  We bridge this gap between subgraph counting and subgraph enumeration by\ndesigning a sublinear-time algorithm that can estimate the number of any\narbitrary subgraph $H$ in $G$, denoted by $\\#H$, to within a $(1\\pm\n\\epsilon)$-approximation w.h.p. in $O(\\frac{m^{\\rho(H)}}{\\#H}) \\cdot\npoly(\\log{n},1/\\epsilon)$ time. Our algorithm is allowed the standard set of\nqueries for general graphs, namely degree queries, pair queries and neighbor\nqueries, plus an additional edge-sample query that returns an edge chosen\nuniformly at random. The performance of our algorithm matches those of Eden\net.al. [FOCS 2015, STOC 2018] for counting triangles and cliques and extend\nthem to all choices of subgraph $H$ under the additional assumption of\nedge-sample queries. We further show that our algorithm works for the more\ngeneral database join size estimation problem and prove a matching lower bound\nfor this problem.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 16:29:08 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Assadi", "Sepehr", ""], ["Kapralov", "Michael", ""], ["Khanna", "Sanjeev", ""]]}, {"id": "1811.07806", "submitter": "Vahid Roostapour", "authors": "Vahid Roostapour, Aneta Neumann, Frank Neumann, Tobias Friedrich", "title": "Pareto Optimization for Subset Selection with Dynamic Cost Constraints", "comments": "A preliminary version of this article has been presented at the\n  Thirty-Third AAAI Conference on Artificial Intelligence (AAAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the subset selection problem for function $f$ with constraint\nbound $B$ that changes over time. Within the area of submodular optimization,\nvarious greedy approaches are commonly used. For dynamic environments we\nobserve that the adaptive variants of these greedy approaches are not able to\nmaintain their approximation quality. Investigating the recently introduced\nPOMC Pareto optimization approach, we show that this algorithm efficiently\ncomputes a $\\phi= (\\alpha_f/2)(1-\\frac{1}{e^{\\alpha_f}})$-approximation, where\n$\\alpha_f$ is the submodularity ratio of $f$, for each possible constraint\nbound $b \\leq B$. Furthermore, we show that POMC is able to adapt its set of\nsolutions quickly in the case that $B$ increases. Our experimental\ninvestigations for the influence maximization in social networks show the\nadvantage of POMC over generalized greedy algorithms. We also consider EAMC, a\nnew evolutionary algorithm with polynomial expected time guarantee to maintain\n$\\phi$ approximation ratio, and NSGA-II as an advanced multi-objective\noptimization algorithm, to demonstrate their challenges in optimizing the\nmaximum coverage problem. Our empirical analysis shows that, within the same\nnumber of evaluations, POMC is able to outperform NSGA-II under linear\nconstraint, while EAMC performs significantly worse than all considered\nalgorithms in most cases.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 07:03:18 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 11:52:28 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Roostapour", "Vahid", ""], ["Neumann", "Aneta", ""], ["Neumann", "Frank", ""], ["Friedrich", "Tobias", ""]]}, {"id": "1811.07821", "submitter": "Yihong Wu", "authors": "Jian Ding, Zongming Ma, Yihong Wu, Jiaming Xu", "title": "Efficient random graph matching via degree profiles", "comments": "Proof of Theorem 4 expanded and revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random graph matching refers to recovering the underlying vertex\ncorrespondence between two random graphs with correlated edges; a prominent\nexample is when the two random graphs are given by Erd\\H{o}s-R\\'{e}nyi graphs\n$G(n,\\frac{d}{n})$. This can be viewed as an average-case and noisy version of\nthe graph isomorphism problem. Under this model, the maximum likelihood\nestimator is equivalent to solving the intractable quadratic assignment\nproblem. This work develops an $\\tilde{O}(n d^2+n^2)$-time algorithm which\nperfectly recovers the true vertex correspondence with high probability,\nprovided that the average degree is at least $d = \\Omega(\\log^2 n)$ and the two\ngraphs differ by at most $\\delta = O( \\log^{-2}(n) )$ fraction of edges. For\ndense graphs and sparse graphs, this can be improved to $\\delta = O(\n\\log^{-2/3}(n) )$ and $\\delta = O( \\log^{-2}(d) )$ respectively, both in\npolynomial time. The methodology is based on appropriately chosen distance\nstatistics of the degree profiles (empirical distribution of the degrees of\nneighbors). Before this work, the best known result achieves $\\delta=O(1)$ and\n$n^{o(1)} \\leq d \\leq n^c$ for some constant $c$ with an $n^{O(\\log n)}$-time\nalgorithm \\cite{barak2018nearly} and $\\delta=\\tilde O((d/n)^4)$ and $d =\n\\tilde{\\Omega}(n^{4/5})$ with a polynomial-time algorithm\n\\cite{dai2018performance}.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 17:33:48 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 18:55:50 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ding", "Jian", ""], ["Ma", "Zongming", ""], ["Wu", "Yihong", ""], ["Xu", "Jiaming", ""]]}, {"id": "1811.07831", "submitter": "Wojciech Nadara", "authors": "Wojciech Nadara", "title": "Experimental evaluation of kernelization algorithms to Dominating Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theoretical notions of graph classes with bounded expansion and that are\nnowhere dense are meant to capture structural sparsity of real world networks\nthat can be used to design efficient algorithms. In the area of sparse graphs,\nthe flagship problems are Dominating Set and its generalization r-Dominating\nSet. They have been precursors for model checking of first order logic on\nsparse graph classes. On class of graphs of bounded expansions the r-Dominating\nSet problem admits a constant factor approximation, a fixed-parameter\nalgorithm, and an efficient preprocessing routine: the so-called linear kernel.\nThis should be put in constrast with general graphs where Dominating Set is\nAPX-hard and W[2]-complete. In this paper we provide an experimental evaluation\nof kernelization algorithm for Dominating Set in sparse graph classes and\ncompare it with previous approaches designed to the preprocessing for\nDominating Set.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 17:42:28 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Nadara", "Wojciech", ""]]}, {"id": "1811.07863", "submitter": "Manuel Gomez Rodriguez", "authors": "Khashayar Gatmiry and Manuel Gomez-Rodriguez", "title": "Non-submodular Function Maximization subject to a Matroid Constraint,\n  with Applications", "comments": "Added missing citations and changed strong submodularity ratio to\n  generalized curvature", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard greedy algorithm has been recently shown to enjoy approximation\nguarantees for constrained non-submodular nondecreasing set function\nmaximization. While these recent results allow to better characterize the\nempirical success of the greedy algorithm, they are only applicable to simple\ncardinality constraints. In this paper, we study the problem of maximizing a\nnon-submodular nondecreasing set function subject to a general matroid\nconstraint. We first show that the standard greedy algorithm offers an\napproximation factor of $\\frac{0.4 {\\gamma}^{2}}{\\sqrt{\\gamma r} + 1}$, where\n$\\gamma$ is the submodularity ratio of the function and $r$ is the rank of the\nmatroid. Then, we show that the same greedy algorithm offers a constant\napproximation factor of $(1 + 1/(1-\\alpha))^{-1}$, where $\\alpha$ is the\ngeneralized curvature of the function. In addition, we demonstrate that these\napproximation guarantees are applicable to several real-world applications in\nwhich the submodularity ratio and the generalized curvature can be bounded.\nFinally, we show that our greedy algorithm does achieve a competitive\nperformance in practice using a variety of experiments on synthetic and\nreal-world data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 18:31:21 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 22:23:02 GMT"}, {"version": "v3", "created": "Mon, 20 May 2019 10:09:26 GMT"}, {"version": "v4", "created": "Tue, 21 May 2019 06:27:47 GMT"}, {"version": "v5", "created": "Tue, 8 Oct 2019 15:57:22 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Gatmiry", "Khashayar", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "1811.07971", "submitter": "Jingcheng Liu", "authors": "Jingcheng Liu and Kunal Talwar", "title": "Private Selection from Private Candidates", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentially Private algorithms often need to select the best amongst many\ncandidate options. Classical works on this selection problem require that the\ncandidates' goodness, measured as a real-valued score function, does not change\nby much when one person's data changes. In many applications such as\nhyperparameter optimization, this stability assumption is much too strong. In\nthis work, we consider the selection problem under a much weaker stability\nassumption on the candidates, namely that the score functions are\ndifferentially private. Under this assumption, we present algorithms that are\nnear-optimal along the three relevant dimensions: privacy, utility and\ncomputational efficiency.\n  Our result can be seen as a generalization of the exponential mechanism and\nits existing generalizations. We also develop an online version of our\nalgorithm, that can be seen as a generalization of the sparse vector technique\nto this weaker stability assumption. We show how our results imply better\nalgorithms for hyperparameter selection in differentially private machine\nlearning, as well as for adaptive data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 20:48:42 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Liu", "Jingcheng", ""], ["Talwar", "Kunal", ""]]}, {"id": "1811.08097", "submitter": "Akinori Hosoyamada", "authors": "Akinori Hosoyamada, Yu Sasaki, Seiichiro Tani, Keita Xagawa", "title": "Improved Quantum Multicollision-Finding Algorithm", "comments": "To appear at PQCrypto 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CC cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current paper improves the number of queries of the previous quantum\nmulti-collision finding algorithms presented by Hosoyamada et al. at Asiacrypt\n2017. Let an $l$-collision be a tuple of $l$ distinct inputs that result in the\nsame output of a target function. In cryptology, it is important to study how\nmany queries are required to find $l$-collisions for random functions of which\ndomains are larger than ranges. The previous algorithm finds an $l$-collision\nfor a random function by recursively calling the algorithm for finding\n$(l-1)$-collisions, and it achieves the average quantum query complexity of\n$O(N^{(3^{l-1}-1) / (2 \\cdot 3^{l-1})})$, where $N$ is the range size of target\nfunctions. The new algorithm removes the redundancy of the previous recursive\nalgorithm so that different recursive calls can share a part of computations.\nThe new algorithm finds an $l$-collision for random functions with the average\nquantum query complexity of $O(N^{(2^{l-1}-1) / (2^{l}-1)})$, which improves\nthe previous bound for all $l\\ge 3$ (the new and previous algorithms achieve\nthe optimal bound for $l=2$). More generally, the new algorithm achieves the\naverage quantum query complexity of $O\\left(c^{3/2}_N N^{\\frac{2^{l-1}-1}{\n2^{l}-1}}\\right)$ for a random function $f\\colon X\\to Y$ such that $|X| \\geq l\n\\cdot |Y| / c_N$ for any $1\\le c_N \\in o(N^{\\frac{1}{2^l - 1}})$. With the same\nquery complexity, it also finds a multiclaw for random functions, which is\nharder to find than a multicollision.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 07:10:45 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 11:56:56 GMT"}, {"version": "v3", "created": "Mon, 28 Jan 2019 09:37:23 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Hosoyamada", "Akinori", ""], ["Sasaki", "Yu", ""], ["Tani", "Seiichiro", ""], ["Xagawa", "Keita", ""]]}, {"id": "1811.08185", "submitter": "Zhao Zhang", "authors": "Yishuo Shi and Yingli Ran and Zhao Zhang and James Willson and Guangmo\n  Tong and Ding-Zhu Du", "title": "Approximation Algorithm for the Partial Set Multi-Cover Problem", "comments": null, "journal-ref": "Journal of Global Optimization, 2019", "doi": "10.1007/s10898-019-00804-y", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial set cover problem and set multi-cover problem are two generalizations\nof set cover problem. In this paper, we consider the partial set multi-cover\nproblem which is a combination of them: given an element set $E$, a collection\nof sets $\\mathcal S\\subseteq 2^E$, a total covering ratio $q$ which is a\nconstant between 0 and 1, each set $S\\in\\mathcal S$ is associated with a cost\n$c_S$, each element $e\\in E$ is associated with a covering requirement $r_e$,\nthe goal is to find a minimum cost sub-collection $\\mathcal S'\\subseteq\\mathcal\nS$ to fully cover at least $q|E|$ elements, where element $e$ is fully covered\nif it belongs to at least $r_e$ sets of $\\mathcal S'$. Denote by\n$r_{\\max}=\\max\\{r_e\\colon e\\in E\\}$ the maximum covering requirement. We\npresent an $(O(\\frac{r_{\\max}\\log^2n}{\\varepsilon}),1-\\varepsilon)$-bicriteria\napproximation algorithm, that is, the output of our algorithm has cost at most\n$O(\\frac{r_{\\max}\\log^2 n}{\\varepsilon})$ times of the optimal value while the\nnumber of fully covered elements is at least $(1-\\varepsilon)q|E|$.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 11:25:54 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Shi", "Yishuo", ""], ["Ran", "Yingli", ""], ["Zhang", "Zhao", ""], ["Willson", "James", ""], ["Tong", "Guangmo", ""], ["Du", "Ding-Zhu", ""]]}, {"id": "1811.08205", "submitter": "Ce Jin", "authors": "Ce Jin", "title": "Simulating Random Walks on Graphs in the Streaming Model", "comments": "To appear in ITCS 2019", "journal-ref": null, "doi": "10.4230/LIPIcs.ITCS.2019.46", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of approximately simulating a $t$-step random walk on a\ngraph where the input edges come from a single-pass stream. The straightforward\nalgorithm using reservoir sampling needs $O(nt)$ words of memory. We show that\nthis space complexity is near-optimal for directed graphs. For undirected\ngraphs, we prove an $\\Omega(n\\sqrt{t})$-bit space lower bound, and give a\nnear-optimal algorithm using $O(n\\sqrt{t})$ words of space with\n$2^{-\\Omega(\\sqrt{t})}$ simulation error (defined as the $\\ell_1$-distance\nbetween the output distribution of the simulation algorithm and the\ndistribution of perfect random walks). We also discuss extending the algorithms\nto the turnstile model, where both insertion and deletion of edges can appear\nin the input stream.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 12:23:49 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Jin", "Ce", ""]]}, {"id": "1811.08238", "submitter": "Franziska Eberle", "authors": "Lin Chen, Franziska Eberle, Nicole Megow, Kevin Schewior, Cliff Stein", "title": "A general framework for handling commitment in online throughput\n  maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a fundamental online job admission problem where jobs with deadlines\narrive online over time at their release dates, and the task is to determine a\npreemptive single-server schedule which maximizes the number of jobs that\ncomplete on time. To circumvent known impossibility results, we make a standard\nslackness assumption by which the feasible time window for scheduling a job is\nat least $1+\\varepsilon$ times its processing time, for some $\\varepsilon>0$.\nWe quantify the impact that different provider commitment requirements have on\nthe performance of online algorithms. Our main contribution is one universal\nalgorithmic framework for online job admission both with and without\ncommitments. Without commitment, our algorithm with a competitive ratio of\n$O(1/\\varepsilon)$ is the best possible (deterministic) for this problem. For\ncommitment models, we give the first non-trivial performance bounds. If the\ncommitment decisions must be made before a job's slack becomes less than a\n$\\delta$-fraction of its size, we prove a competitive ratio of\n$O(\\varepsilon/((\\varepsilon-\\delta)\\delta^2))$, for $0<\\delta<\\varepsilon$.\nWhen a provider must commit upon starting a job, our bound is\n$O(1/\\varepsilon^2)$. Finally, we observe that for scheduling with commitment\nthe restriction to the `unweighted' throughput model is essential; if jobs have\nindividual weights, we rule out competitive deterministic algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 13:29:27 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Chen", "Lin", ""], ["Eberle", "Franziska", ""], ["Megow", "Nicole", ""], ["Schewior", "Kevin", ""], ["Stein", "Cliff", ""]]}, {"id": "1811.08372", "submitter": "Mohammad-Ali Javidian", "authors": "Mohammad Ali Javidian, Linyuan Lu, Marco Valtorta, Zhiyu Wang", "title": "On a hypergraph probabilistic graphical model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a directed acyclic hypergraph framework for a probabilistic\ngraphical model that we call Bayesian hypergraphs. The space of directed\nacyclic hypergraphs is much larger than the space of chain graphs. Hence\nBayesian hypergraphs can model much finer factorizations than Bayesian networks\nor LWF chain graphs and provide simpler and more computationally efficient\nprocedures for factorizations and interventions. Bayesian hypergraphs also\nallow a modeler to represent causal patterns of interaction such as Noisy-OR\ngraphically (without additional annotations). We introduce global, local and\npairwise Markov properties of Bayesian hypergraphs and prove under which\nconditions they are equivalent. We define a projection operator, called shadow,\nthat maps Bayesian hypergraphs to chain graphs, and show that the Markov\nproperties of a Bayesian hypergraph are equivalent to those of its\ncorresponding chain graph. We extend the causal interpretation of LWF chain\ngraphs to Bayesian hypergraphs and provide corresponding formulas and a\ngraphical criterion for intervention.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 17:11:47 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Javidian", "Mohammad Ali", ""], ["Lu", "Linyuan", ""], ["Valtorta", "Marco", ""], ["Wang", "Zhiyu", ""]]}, {"id": "1811.08393", "submitter": "Kush Bhatia", "authors": "Kush Bhatia, Aldo Pacchiano, Nicolas Flammarion, Peter L. Bartlett,\n  Michael I. Jordan", "title": "Gen-Oja: A Two-time-scale approach for Streaming CCA", "comments": "Accepted at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problems of principal Generalized Eigenvector\ncomputation and Canonical Correlation Analysis in the stochastic setting. We\npropose a simple and efficient algorithm, Gen-Oja, for these problems. We prove\nthe global convergence of our algorithm, borrowing ideas from the theory of\nfast-mixing Markov chains and two-time-scale stochastic approximation, showing\nthat it achieves the optimal rate of convergence. In the process, we develop\ntools for understanding stochastic processes with Markovian noise which might\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 17:57:13 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 01:19:06 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Bhatia", "Kush", ""], ["Pacchiano", "Aldo", ""], ["Flammarion", "Nicolas", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1811.08506", "submitter": "Jan Marcinkowski", "authors": "Szymon Dudycz, Mateusz Lewandowski, and Jan Marcinkowski (University\n  of Wroc{\\l}aw, Poland)", "title": "Tight Approximation Ratio for Minimum Maximal Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a combinatorial problem called Minimum Maximal Matching, where we\nare asked to find in a general graph the smallest that can not be extended. We\nshow that this problem is hard to approximate with a constant smaller than 2,\nassuming the Unique Games Conjecture.\n  As a corollary we show, that Minimum Maximal Matching in bipartite graphs is\nhard to approximate with constant smaller than $\\frac{4}{3}$, with the same\nassumption. With a stronger variant of the Unique Games Conjecture --- that is\nSmall Set Expansion Hypothesis --- we are able to improve the hardness result\nup to the factor of $\\frac{3}{2}$.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 22:04:59 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Dudycz", "Szymon", "", "University\n  of Wroc\u0142aw, Poland"], ["Lewandowski", "Mateusz", "", "University\n  of Wroc\u0142aw, Poland"], ["Marcinkowski", "Jan", "", "University\n  of Wroc\u0142aw, Poland"]]}, {"id": "1811.08532", "submitter": "Christoph Hunkenschr\\\"oder", "authors": "Christoph Hunkenschr\\\"oder and Gina Reuland and Matthias Schymura", "title": "On compact representations of Voronoi cells of lattices", "comments": "Final version, online published in Math. Prog", "journal-ref": "Math. Program. (2020)", "doi": "10.1007/s10107-019-01463-3", "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a seminal work, Micciancio & Voulgaris (2013) described a deterministic\nsingle-exponential time algorithm for the Closest Vector Problem (CVP) on\nlattices. It is based on the computation of the Voronoi cell of the given\nlattice and thus may need exponential space as well. We address the major open\nquestion whether there exists such an algorithm that requires only polynomial\nspace.\n  To this end, we define a lattice basis to be $c$-compact if every facet\nnormal of the Voronoi cell is a linear combination of the basis vectors using\ncoefficients that are bounded by $c$ in absolute value. Given such a basis, we\nget a polynomial space algorithm for CVP whose running time naturally depends\non $c$. Thus, our main focus is the behavior of the smallest possible value of\n$c$, with the following results: There always exist $c$-compact bases, where\n$c$ is bounded by $n^2$ for an $n$-dimension lattice; there are lattices not\nadmitting a $c$-compact basis with $c$ growing sublinearly with the dimension;\nand every lattice with a zonotopal Voronoi cell has a $1$-compact basis.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 23:50:23 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 13:01:23 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 16:25:25 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Hunkenschr\u00f6der", "Christoph", ""], ["Reuland", "Gina", ""], ["Schymura", "Matthias", ""]]}, {"id": "1811.08539", "submitter": "Victor Verdugo", "authors": "Victor Verdugo and Jos\\'e Verschae and Andreas Wiese", "title": "Breaking symmetries to rescue Sum of Squares in the case of makespan\n  scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Sum of Squares (\\sos{}) hierarchy gives an automatized technique to\ncreate a family of increasingly tight convex relaxations for binary programs.\nThere are several problems for which a constant number of rounds of this\nhierarchy give integrality gaps matching the best known approximation\nalgorithms. For many other problems, however, ad-hoc techniques give better\napproximation ratios than \\sos{} in the worst case, as shown by corresponding\nlower bound instances. Notably, in many cases these instances are invariant\nunder the action of a large permutation group. This yields the question how\nsymmetries in a formulation degrade the performance of the relaxation obtained\nby the \\sos{} hierarchy. In this paper, we study this for the case of the\nminimum makespan problem on identical machines. Our first result is to show\nthat $\\Omega(n)$ rounds of \\sos{} applied over the \\emph{configuration linear\nprogram} yields an integrality gap of at least $1.0009$, where $n$ is the\nnumber of jobs. Our result is based on tools from representation theory of\nsymmetric groups. Then, we consider the weaker \\emph{assignment linear program}\nand add a well chosen set of symmetry breaking inequalities that removes a\nsubset of the machine permutation symmetries. We show that applying\n$2^{\\tilde{O}(1/\\varepsilon^2)}$ rounds of the SA hierarchy to this stronger\nlinear program reduces the integrality gap to $1+\\varepsilon$, which yields a\nlinear programming based polynomial time approximation scheme. Our results\nsuggest that for this classical problem, symmetries were the main barrier\npreventing the \\sos{}/ SA hierarchies to give relaxations of polynomial\ncomplexity with an integrality gap of~$1+\\varepsilon$. We leave as an open\nquestion whether this phenomenon occurs for other symmetric problems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 01:23:48 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 01:54:10 GMT"}, {"version": "v3", "created": "Sat, 16 May 2020 13:31:43 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Verdugo", "Victor", ""], ["Verschae", "Jos\u00e9", ""], ["Wiese", "Andreas", ""]]}, {"id": "1811.08811", "submitter": "Mayuri Sridhar", "authors": "Mayuri Sridhar and Ronald L. Rivest", "title": "$k$-Cut: A Simple Approximately-Uniform Method for Sampling Ballots in\n  Post-Election Audits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approximate sampling framework and discuss how risk-limiting\naudits can compensate for these approximations, while maintaining their\n\"risk-limiting\" properties. Our framework is general and can compensate for\ncounting mistakes made during audits.\n  Moreover, we present and analyze a simple approximate sampling\nmethod,\"$k$-cut\", for picking a ballot randomly from a stack, without counting.\nOur method involves doing $k$ \"cuts\", each involving moving a random portion of\nballots from the top to the bottom of the stack, and then picking the ballot on\ntop. Unlike conventional methods of picking a ballot at random, $k$-cut does\nnot require identification numbers on the ballots or counting many ballots per\ndraw. We analyze how close the distribution of chosen ballots is to the uniform\ndistribution, and design different mitigation procedures. We show that $k=6$\ncuts is enough for an risk-limiting election audit, based on empirical data,\nwhich would provide a significant increase in efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 16:26:33 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 01:13:45 GMT"}, {"version": "v3", "created": "Wed, 2 Jan 2019 23:46:57 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Sridhar", "Mayuri", ""], ["Rivest", "Ronald L.", ""]]}, {"id": "1811.08850", "submitter": "Thorsten Wi{\\ss}mann", "authors": "Hans-Peter Deifel, Stefan Milius, Lutz Schr\\\"oder, Thorsten\n  Wi{\\ss}mann", "title": "Generic Partition Refinement and Weighted Tree Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partition refinement is a method for minimizing automata and transition\nsystems of various types. Recently, we have developed a partition refinement\nalgorithm that is generic in the transition type of the given system and\nmatches the run time of the best known algorithms for many concrete types of\nsystems, e.g. deterministic automata as well as ordinary, weighted, and\nprobabilistic (labelled) transition systems. Genericity is achieved by\nmodelling transition types as functors on sets, and systems as coalgebras. In\nthe present work, we refine the run time analysis of our algorithm to cover\nadditional instances, notably weighted automata and, more generally, weighted\ntree automata. For weights in a cancellative monoid we match, and for\nnon-cancellative monoids such as (the additive monoid of) the tropical semiring\neven substantially improve, the asymptotic run time of the best known\nalgorithms. We have implemented our algorithm in a generic tool that is easily\ninstantiated to concrete system types by implementing a simple refinement\ninterface. Moreover, the algorithm and the tool are modular, and partition\nrefiners for new types of systems are obtained easily by composing\npre-implemented basic functors. Experiments show that even for complex system\ntypes, the tool is able to handle systems with millions of transitions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 17:54:19 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 12:41:19 GMT"}, {"version": "v3", "created": "Wed, 10 Jul 2019 13:43:04 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Deifel", "Hans-Peter", ""], ["Milius", "Stefan", ""], ["Schr\u00f6der", "Lutz", ""], ["Wi\u00dfmann", "Thorsten", ""]]}, {"id": "1811.08918", "submitter": "Gerhard Woeginger", "authors": "Alexander Grigoriev, Tim A. Hartmann, Stefan Lendl, Gerhard J.\n  Woeginger", "title": "Dispersing obnoxious facilities on a graph", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a continuous facility location problem on a graph where all edges\nhave unit length and where the facilities may also be positioned in the\ninterior of the edges. The goal is to position as many facilities as possible\nsubject to the condition that any two facilities have at least distance\n$\\delta$ from each other. We investigate the complexity of this problem in\nterms of the rational parameter $\\delta$. The problem is polynomially solvable,\nif the numerator of $\\delta$ is $1$ or $2$, while all other cases turn out to\nbe NP-hard.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 19:23:48 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Grigoriev", "Alexander", ""], ["Hartmann", "Tim A.", ""], ["Lendl", "Stefan", ""], ["Woeginger", "Gerhard J.", ""]]}, {"id": "1811.09027", "submitter": "Andr\\'e Linhares", "authors": "Andr\\'e Linhares, Neil Olver, Chaitanya Swamy, Rico Zenklusen", "title": "Approximate Multi-Matroid Intersection via Iterative Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new iterative rounding technique to round a point in a matroid\npolytope subject to further matroid constraints. This technique returns an\nindependent set in one matroid with limited violations of the other ones. On\ntop of the classical steps of iterative relaxation approaches, we iteratively\nrefine/split involved matroid constraints to obtain a more restrictive\nconstraint system, that is amenable to iterative relaxation techniques. Hence,\nthroughout the iterations, we both tighten constraints and later relax them by\ndropping constrains under certain conditions. Due to the refinement step, we\ncan deal with considerably more general constraint classes than existing\niterative relaxation/rounding methods, which typically round on one matroid\npolytope with additional simple cardinality constraints that do not overlap too\nmuch.\n  We show how our rounding method, combined with an application of a matroid\nintersection algorithm, yields the first $2$-approximation for finding a\nmaximum-weight common independent set in $3$ matroids. Moreover, our\n$2$-approximation is LP-based, and settles the integrality gap for the natural\nrelaxation of the problem. Prior to our work, no better upper bound than $3$\nwas known for the integrality gap, which followed from the greedy algorithm. We\nalso discuss various other applications of our techniques, including an\nextension that allows us to handle a mixture of matroid and knapsack\nconstraints.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 05:41:07 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Linhares", "Andr\u00e9", ""], ["Olver", "Neil", ""], ["Swamy", "Chaitanya", ""], ["Zenklusen", "Rico", ""]]}, {"id": "1811.09045", "submitter": "Yutaro Yamaguchi", "authors": "Yuval Filmus, Yasushi Kawase, Yusuke Kobayashi, Yutaro Yamaguchi", "title": "Tight Approximation for Unconstrained XOS Maximization", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set function is called XOS if it can be represented by the maximum of\nadditive functions. When such a representation is fixed, the number of additive\nfunctions required to define the XOS function is called the width.\n  In this paper, we study the problem of maximizing XOS functions in the value\noracle model. The problem is trivial for the XOS functions of width $1$ because\nthey are just additive, but it is already nontrivial even when the width is\nrestricted to $2$. We show two types of tight bounds on the polynomial-time\napproximability for this problem. First, in general, the approximation bound is\nbetween $O(n)$ and $\\Omega(n / \\log n)$, and exactly $\\Theta(n / \\log n)$ if\nrandomization is allowed, where $n$ is the ground set size. Second, when the\nwidth of the input XOS functions is bounded by a constant $k \\geq 2$, the\napproximation bound is between $k - 1$ and $k - 1 - \\epsilon$ for any $\\epsilon\n> 0$. In particular, we give a linear-time algorithm to find an exact maximizer\nof a given XOS function of width $2$, while we show that any exact algorithm\nrequires an exponential number of value oracle calls even when the width is\nrestricted to $3$.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 07:27:57 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 07:27:41 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 01:24:41 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Filmus", "Yuval", ""], ["Kawase", "Yasushi", ""], ["Kobayashi", "Yusuke", ""], ["Yamaguchi", "Yutaro", ""]]}, {"id": "1811.09126", "submitter": "Peng Jia", "authors": "Pinghui Wang, Peng Jia, Xiangliang Zhang, Jing Tao, Xiaohong Guan, Don\n  Towsley", "title": "Utilizing Dynamic Properties of Sharing Bits and Registers to Estimate\n  User Cardinalities over Time", "comments": "Accepted in ICDE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online monitoring user cardinalities (or degrees) in graph streams is\nfundamental for many applications. For example in a bipartite graph\nrepresenting user-website visiting activities, user cardinalities (the number\nof distinct visited websites) are monitored to report network anomalies. These\nreal-world graph streams may contain user-item duplicates and have a huge\nnumber of distinct user-item pairs, therefore, it is infeasible to exactly\ncompute user cardinalities when memory and computation resources are\nlimited.Existing methods are designed to approximately estimate user\ncardinalities, whose accuracy highly depends on parameters that are not easy to\nset. Moreover, these methods cannot provide anytime-available estimation, as\nthe user cardinalities are computed at the end of the data stream. Real-time\napplications such as anomaly detection require that user cardinalities are\nestimated on the fly. To address these problems, we develop novel bit and\nregister sharing algorithms, which use a bit array and a register array to\nbuild a compact sketch of all users' connected items respectively. Compared\nwith previous bit and register sharing methods, our algorithms exploit the\ndynamic properties of the bit and register arrays (e.g., the fraction of zero\nbits in the bit array at each time) to significantly improve the estimation\naccuracy, and have low time complexity (O(1)) to update the estimations each\ntime they observe a new user-item pair. In addition, our algorithms are simple\nand easy to use, without requirements to tune any parameter. We evaluate the\nperformance of our methods on real-world datasets. The experimental results\ndemonstrate that our methods are several times more accurate and faster than\nstate-of-the-art methods using the same amount of memory.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 11:49:16 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 02:10:08 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Wang", "Pinghui", ""], ["Jia", "Peng", ""], ["Zhang", "Xiangliang", ""], ["Tao", "Jing", ""], ["Guan", "Xiaohong", ""], ["Towsley", "Don", ""]]}, {"id": "1811.09136", "submitter": "Peng Jia", "authors": "Pinghui Wang, Peng Jia, Yiyan Qi, Yu Sun, Jing Tao, Xiaohong Guan", "title": "REPT: A Streaming Algorithm of Approximating Global and Local Triangle\n  Counts in Parallel", "comments": "Accepted in ICDE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, considerable efforts have been devoted to approximately computing\nthe global and local (i.e., incident to each node) triangle counts of a large\ngraph stream represented as a sequence of edges. Existing approximate triangle\ncounting algorithms rely on sampling techniques to reduce the computational\ncost. However, their estimation errors are significantly determined by the\ncovariance between sampled triangles. Moreover, little attention has been paid\nto developing parallel one-pass streaming algorithms that can be used to fast\nand approximately count triangles on a multi-core machine or a cluster of\nmachines. To solve these problems, we develop a novel parallel method REPT to\nsignificantly reduce the covariance (even completely eliminate the covariance\nfor some cases) between sampled triangles. We theoretically prove that REPT is\nmore accurate than parallelizing existing triangle count estimation algorithms\nin a direct manner. In addition, we also conduct extensive experiments on a\nvariety of real-world graphs, and the results demonstrate that our method REPT\nis several times more accurate than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 12:28:29 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 02:06:42 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Wang", "Pinghui", ""], ["Jia", "Peng", ""], ["Qi", "Yiyan", ""], ["Sun", "Yu", ""], ["Tao", "Jing", ""], ["Guan", "Xiaohong", ""]]}, {"id": "1811.09233", "submitter": "Marek Chrobak", "authors": "Marcin Bienkowski, Jaros{\\l}aw Byrka, Marek Chrobak, Christian\n  Coester, {\\L}ukasz Je\\.z, Elias Koutsoupias", "title": "Better Bounds for Online Line Chasing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online competitive algorithms for the \\emph{line chasing problem} in\nEuclidean spaces $\\reals^d$, where the input consists of an initial point $P_0$\nand a sequence of lines $X_1,X_2,...,X_m$, revealed one at a time. At each step\n$t$, when the line $X_t$ is revealed, the algorithm must determine a point\n$P_t\\in X_t$. An online algorithm is called $c$-competitive if for any input\nsequence the path $P_0, P_1,...,P_m$ it computes has length at most $c$ times\nthe optimum path. The line chasing problem is a variant of a more general\nconvex body chasing problem, where the sets $X_t$ are arbitrary convex sets.\n  To date, the best competitive ratio for the line chasing problem was $28.1$,\neven in the plane. We significantly improve this bound, by providing\na~$3$-competitive algorithm for any dimension $d$. We also improve the lower\nbound on the competitive ratio, from $1.412$ to $1.5358$.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 16:45:38 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 00:39:12 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Bienkowski", "Marcin", ""], ["Byrka", "Jaros\u0142aw", ""], ["Chrobak", "Marek", ""], ["Coester", "Christian", ""], ["Je\u017c", "\u0141ukasz", ""], ["Koutsoupias", "Elias", ""]]}, {"id": "1811.09380", "submitter": "Yu Cheng", "authors": "Yu Cheng, Ilias Diakonikolas, Rong Ge", "title": "High-Dimensional Robust Mean Estimation in Nearly-Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problem of high-dimensional mean estimation in a\nrobust model where a constant fraction of the samples are adversarially\ncorrupted. Recent work gave the first polynomial time algorithms for this\nproblem with dimension-independent error guarantees for several families of\nstructured distributions.\n  In this work, we give the first nearly-linear time algorithms for\nhigh-dimensional robust mean estimation. Specifically, we focus on\ndistributions with (i) known covariance and sub-gaussian tails, and (ii)\nunknown bounded covariance. Given $N$ samples on $\\mathbb{R}^d$, an\n$\\epsilon$-fraction of which may be arbitrarily corrupted, our algorithms run\nin time $\\tilde{O}(Nd) / \\mathrm{poly}(\\epsilon)$ and approximate the true mean\nwithin the information-theoretically optimal error, up to constant factors.\nPrevious robust algorithms with comparable error guarantees have running times\n$\\tilde{\\Omega}(N d^2)$, for $\\epsilon = \\Omega(1)$.\n  Our algorithms rely on a natural family of SDPs parameterized by our current\nguess $\\nu$ for the unknown mean $\\mu^\\star$. We give a win-win analysis\nestablishing the following: either a near-optimal solution to the primal SDP\nyields a good candidate for $\\mu^\\star$ -- independent of our current guess\n$\\nu$ -- or the dual SDP yields a new guess $\\nu'$ whose distance from\n$\\mu^\\star$ is smaller by a constant factor. We exploit the special structure\nof the corresponding SDPs to show that they are approximately solvable in\nnearly-linear time. Our approach is quite general, and we believe it can also\nbe applied to obtain nearly-linear time algorithms for other high-dimensional\nrobust learning problems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 07:51:35 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Cheng", "Yu", ""], ["Diakonikolas", "Ilias", ""], ["Ge", "Rong", ""]]}, {"id": "1811.09411", "submitter": "Niels Gr\\\"uttemeier", "authors": "Laurent Bulteau, Niels Gr\\\"uttemeier, Christian Komusiewicz, Manuel\n  Sorge", "title": "Your Rugby Mates Don't Need to Know your Colleagues: Triadic Closure\n  with Edge Colors", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given an undirected graph $G=(V,E)$ the NP-hard Strong Triadic Closure (STC)\nproblem asks for a labeling of the edges as \\emph{weak} and \\emph{strong} such\nthat at most $k$ edges are weak and for each induced $P_3$ in $G$ at least one\nedge is weak. In this work, we study the following generalizations of STC with\n$c$ different strong edge colors. In Multi-STC an induced $P_3$ may receive two\nstrong labels as long as they are different. In Edge-List Multi-STC and\nVertex-List Multi-STC we may additionally restrict the set of permitted colors\nfor each edge of $G$. We show that, under the Exponential Time Hypothesis\n(ETH), Edge-List Multi-STC and Vertex-List Multi-STC cannot be solved in time\n$2^{o(|V|^2)}$. We then proceed with a parameterized complexity analysis in\nwhich we extend previous fixed-parameter tractability results and\nkernelizations for STC [Golovach et al., Algorithmica '20, Gr\\\"uttemeier and\nKomusiewicz, Algorithmica '20] to the three variants with multiple edge colors\nor outline the limits of such an extension.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 09:58:37 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 13:38:16 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Bulteau", "Laurent", ""], ["Gr\u00fcttemeier", "Niels", ""], ["Komusiewicz", "Christian", ""], ["Sorge", "Manuel", ""]]}, {"id": "1811.09429", "submitter": "Lars Jaffke", "authors": "Michael R. Fellows, Lars Jaffke, Aliz Izabella Kir\\'aly, Frances A.\n  Rosamond, and Mathias Weller", "title": "What is known about Vertex Cover Kernelization?", "comments": "25 pages, 10 figures. Appeared in volume 11011 of LNCS, pages\n  330-356, see Reference [29] in the text. Compared to [29], this arXiv-upload\n  contains a fixed version of Reduction R.8, the order of presentation of\n  Reductions R.6 and R.7 has been switched, and a few observations have been\n  added in Section 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are pleased to dedicate this survey on kernelization of the Vertex Cover\nproblem, to Professor Juraj Hromkovi\\v{c} on the occasion of his 60th birthday.\nThe Vertex Cover problem is often referred to as the Drosophila of\nparameterized complexity. It enjoys a long history. New and worthy perspectives\nwill always be demonstrated first with concrete results here. This survey\ndiscusses several research directions in Vertex Cover kernelization. The\nBarrier Degree of Vertex Cover kernelization is discussed. We have reduction\nrules that kernelize vertices of small degree, including in this paper new\nresults that reduce graphs almost to minimum degree five. Can this process go\non forever? What is the minimum vertex-degree barrier for polynomial-time\nkernelization? Assuming the Exponential-Time Hypothesis, there is a minimum\ndegree barrier. The idea of automated kernelization is discussed. We here\nreport the first experimental results of an AI-guided branching algorithm for\nVertex Cover whose logic seems amenable for application in finding reduction\nrules to kernelize small-degree vertices. The survey highlights a central open\nproblem in parameterized complexity. Happy Birthday, Juraj!\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 11:13:10 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 08:58:33 GMT"}, {"version": "v3", "created": "Thu, 14 Feb 2019 08:15:14 GMT"}, {"version": "v4", "created": "Tue, 14 May 2019 01:16:22 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Fellows", "Michael R.", ""], ["Jaffke", "Lars", ""], ["Kir\u00e1ly", "Aliz Izabella", ""], ["Rosamond", "Frances A.", ""], ["Weller", "Mathias", ""]]}, {"id": "1811.09573", "submitter": "Leah Epstein", "authors": "Leah Epstein", "title": "A lower bound for online rectangle packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We slightly improve the known lower bound on the asymptotic competitive ratio\nfor online bin packing of rectangles. We present a complete proof for the new\nlower bound, whose value is above 1.91.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 17:36:02 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Epstein", "Leah", ""]]}, {"id": "1811.09863", "submitter": "Anton Belyy", "authors": "Anton Belyy, Aleksei Sholokhov", "title": "MEMOIR: Multi-class Extreme Classification with Inexact Margin", "comments": "11 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-class classification with a very large number of classes, or extreme\nclassification, is a challenging problem from both statistical and\ncomputational perspectives. Most of the classical approaches to multi-class\nclassification, including one-vs-rest or multi-class support vector machines,\nrequire the exact estimation of the classifier's margin, at both the training\nand the prediction steps making them intractable in extreme classification\nscenarios. In this paper, we study the impact of computing an approximate\nmargin using nearest neighbor (ANN) search structures combined with\nlocality-sensitive hashing (LSH). This approximation allows to dramatically\nreduce both the training and the prediction time without a significant loss in\nperformance. We theoretically prove that this approximation does not lead to a\nsignificant loss of the risk of the model and provide empirical evidence over\nfive publicly available large scale datasets, showing that the proposed\napproach is highly competitive with respect to state-of-the-art approaches on\ntime, memory and performance measures.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 17:26:20 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 22:08:23 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Belyy", "Anton", ""], ["Sholokhov", "Aleksei", ""]]}, {"id": "1811.09906", "submitter": "Arash Haddadan", "authors": "Arash Haddadan and Alantha Newman", "title": "Efficient constructions of convex combinations for 2-edge-connected\n  subgraphs on fundamental classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the exact integrality gap $\\alpha$ for the LP relaxation of the\n2-edge-connected spanning multigraph problem (2EC) is closely related to the\nsame problem for the Held-Karp relaxation of the metric traveling salesman\nproblem (TSP). While the former problem seems easier than the latter, since it\nis less constrained, currently the upper bounds on the respective integrality\ngaps for the two problems are the same.\n  An approach to proving integrality gaps for both of these problems is to\nconsider fundamental classes of extreme points. For 2EC, better bounds on the\nintegrality gap are known for certain important special cases of these\nfundamental points. For example, for half-integer square points, the\nintegrality gap is between $\\frac{6}{5}$ and $\\frac{4}{3}$. Our main result is\nto improve the approximation factor to $\\frac{9}{7}$ for 2EC for these points.\nOur approach is based on constructing convex combinations and our key tool is\nthe top-down coloring framework for tree augmentation, whose flexibility we\nemploy to exploit beneficial properties in both the initial spanning tree and\nin the input graph. We also show how these tools can be tailored to the closely\nrelated problem of uniform covers for which the proofs of the best-known bounds\ndo not yield polynomial-time algorithms. Another key ingredient is to use a\nrainbow spanning tree decomposition, which allows us to obtain a convex\ncombination of spanning trees with particular properties\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 23:09:13 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 18:35:02 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Haddadan", "Arash", ""], ["Newman", "Alantha", ""]]}, {"id": "1811.10074", "submitter": "Roman Snytsar", "authors": "Roman Snytsar and Yatish Turakhia", "title": "Parallel approach to sliding window sums", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliding window sums are widely used in bioinformatics applications, including\nsequence assembly, k-mer generation, hashing and compression. New vector\nalgorithms which utilize the advanced vector extension (AVX) instructions\navailable on modern processors, or the parallel compute units on GPUs and\nFPGAs, would provide a significant performance boost for the bioinformatics\napplications. We develop a generic vectorized sliding sum algorithm with\nspeedup for window size w and number of processors P is O(P/w) for a generic\nsliding sum. For a sum with commutative operator the speedup is improved to\nO(P/log(w)). When applied to the genomic application of minimizer based k-mer\ntable generation using AVX instructions, we obtain a speedup of over 5X.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 19:09:47 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 14:39:58 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Snytsar", "Roman", ""], ["Turakhia", "Yatish", ""]]}, {"id": "1811.10161", "submitter": "Adil Erzin I", "authors": "Adil Erzin and Natalya Lagutkina", "title": "FPTAS for barrier covering problem with equal circles in 2D", "comments": null, "journal-ref": null, "doi": "10.1007/s11590-020-01650-8", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a problem of covering a straight line segment by\nequal circles that are initially arbitrarily placed on a plane by moving their\ncenters on a segment or on a straight line containing a segment so that the\nsegment is completely covered, the neighboring circles in the cover are\ntouching each other and the total length of the paths traveled by circles is\nminimal. The complexity status of the problem is not known. We propose a\n$O(n^{2+c}/\\varepsilon^2)$--time FPTAS for this problem, where $n$ is the\nnumber of circles and $c>0$ is arbitrarily small real.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 03:24:42 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 04:29:10 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Erzin", "Adil", ""], ["Lagutkina", "Natalya", ""]]}, {"id": "1811.10319", "submitter": "Daniel Schmidt", "authors": "Ioana O. Bercea, Martin Gro{\\ss}, Samir Khuller, Aounon Kumar, Clemens\n  R\\\"osner, Daniel R. Schmidt, Melanie Schmidt", "title": "On the cost of essentially fair clusterings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a fundamental tool in data mining. It partitions points into\ngroups (clusters) and may be used to make decisions for each point based on its\ngroup. However, this process may harm protected (minority) classes if the\nclustering algorithm does not adequately represent them in desirable clusters\n-- especially if the data is already biased.\n  At NIPS 2017, Chierichetti et al. proposed a model for fair clustering\nrequiring the representation in each cluster to (approximately) preserve the\nglobal fraction of each protected class. Restricting to two protected classes,\nthey developed both a 4-approximation for the fair $k$-center problem and a\n$O(t)$-approximation for the fair $k$-median problem, where $t$ is a parameter\nfor the fairness model. For multiple protected classes, the best known result\nis a 14-approximation for fair $k$-center.\n  We extend and improve the known results. Firstly, we give a 5-approximation\nfor the fair $k$-center problem with multiple protected classes. Secondly, we\npropose a relaxed fairness notion under which we can give bicriteria\nconstant-factor approximations for all of the classical clustering objectives\n$k$-center, $k$-supplier, $k$-median, $k$-means and facility location. The\nlatter approximations are achieved by a framework that takes an arbitrary\nexisting unfair (integral) solution and a fair (fractional) LP solution and\ncombines them into an essentially fair clustering with a weakly supervised\nrounding scheme. In this way, a fair clustering can be established belatedly,\nin a situation where the centers are already fixed.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 12:25:35 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Bercea", "Ioana O.", ""], ["Gro\u00df", "Martin", ""], ["Khuller", "Samir", ""], ["Kumar", "Aounon", ""], ["R\u00f6sner", "Clemens", ""], ["Schmidt", "Daniel R.", ""], ["Schmidt", "Melanie", ""]]}, {"id": "1811.10356", "submitter": "Yunyou Huang", "authors": "Yunyou Huang, Jianfeng Zhan, Nana Wang, Chunjie Luo, Lei Wang and Rui\n  Ren", "title": "Clustering Residential Electricity Load Curves via Community Detection\n  in Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing analytic of household load curves (LCs) has significant value in\npredicting individual electricity consumption patterns, and hence facilitate\ndeveloping demand-response strategy, and finally achieve energy efficiency\nimprovement and emission reduction. LC clustering is a widely used analytic\ntechnology, which discovers electricity consumption patterns by grouping\nsimilar LCs into same sub-groups. However, previous clustering methods treat\neach LC in the data set as an individual time series, ignoring the inherent\nrelationship among different LCs, restraining the performance of the clustering\nmethods. What's more, due to the significant volatility and uncertainty of LCs,\nthe previous LC clustering approaches inevitably result in either lager number\nof clusters or huge variances within a cluster, which is unacceptable for\nactual application needs. In this paper, we proposed an integrated approach to\naddress this issue. First, we converted the LC clustering task into the\ncommunity detection task in network. Second, we proposed a clustering approach\nincorporated with community detection to improve the performance of LC\nclustering, which includes network construction, community detection and\ntypical load profile extraction. The method convert the data set into a network\nin which the inherent relationship among LCs is represented by the edges of the\nnetwork. Third, we construct a multi-layer typical load profile directory to\nmake the trade-off between variances within a cluster and the number of the\nclusters, enabling the researchers to assess the LCs and the customers in\ndifferent layers according to practical application requirements. The\nexperiments demonstrate that our integrated approach outperform the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 13:24:03 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Huang", "Yunyou", ""], ["Zhan", "Jianfeng", ""], ["Wang", "Nana", ""], ["Luo", "Chunjie", ""], ["Wang", "Lei", ""], ["Ren", "Rui", ""]]}, {"id": "1811.10470", "submitter": "Hannu Reittu", "authors": "Hannu Reittu, Lasse Leskel\\\"a, Tomi R\\\"aty, Marco Fiorucci", "title": "Analysis of large sparse graphs using regular decomposition of graph\n  distance matrices", "comments": "IEEE BigData 2018 Conference Workshop, Advances in High Dimensional\n  Big Data, 10.-13.12. 2018, Seattle USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical analysis of large and sparse graphs is a challenging problem in\ndata science due to the high dimensionality and nonlinearity of the problem.\nThis paper presents a fast and scalable algorithm for partitioning such graphs\ninto disjoint groups based on observed graph distances from a set of reference\nnodes. The resulting partition provides a low-dimensional approximation of the\nfull distance matrix which helps to reveal global structural properties of the\ngraph using only small samples of the distance matrix. The presented algorithm\nis inspired by the information-theoretic minimum description principle. We\ninvestigate the performance of this algorithm for selected real data sets and\nfor synthetic graph data sets generated using stochastic block models and\npower-law random graphs, together with analytical considerations for sparse\nstochastic block models with bounded average degrees.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 16:06:17 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 14:38:44 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Reittu", "Hannu", ""], ["Leskel\u00e4", "Lasse", ""], ["R\u00e4ty", "Tomi", ""], ["Fiorucci", "Marco", ""]]}, {"id": "1811.10498", "submitter": "Vajira Thambawita", "authors": "Vajira Thambawita and Roshan G. Ragel and Dhammike Elkaduwe", "title": "An optimized Parallel Failure-less Aho-Corasick algorithm for DNA\n  sequence matching", "comments": "6 pages, 3 figures, 4 tables, 5 graphs, 2016 IEEE International\n  Conference on Information and Automation for Sustainability (ICIAfS)", "journal-ref": null, "doi": "10.1109/ICIAFS.2016.7946533", "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Aho-Corasick algorithm is multiple patterns searching algorithm running\nsequentially in various applications like network intrusion detection and\nbioinformatics for finding several input strings within a given large input\nstring. The parallel version of the Aho-Corasick algorithm is called as\nParallel Failure-less Aho-Corasick algorithm because it doesn't need failure\nlinks like in the original Aho-Corasick algorithm. In this research, we\nimplemented an application specific parallel failureless Aho-Corasick algorithm\nto the general purpose graphics processing unit by applying several cache\noptimization techniques for matching DNA sequences. Our parallel Aho-Corasick\nalgorithm shows better performance than the available parallel Aho-Corasick\nalgorithm library due to its simplicity and optimized cache memory usage of\ngraphics processing units for matching DNA sequences.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 16:45:30 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Thambawita", "Vajira", ""], ["Ragel", "Roshan G.", ""], ["Elkaduwe", "Dhammike", ""]]}, {"id": "1811.10722", "submitter": "Rasmus J Kyng", "authors": "Michael B. Cohen, Jonathan Kelner, Rasmus Kyng, John Peebles, Richard\n  Peng, Anup B. Rao, Aaron Sidford", "title": "Solving Directed Laplacian Systems in Nearly-Linear Time through Sparse\n  LU Factorizations", "comments": "Appeared in FOCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to solve directed Laplacian systems in nearly-linear time. Given\na linear system in an $n \\times n$ Eulerian directed Laplacian with $m$ nonzero\nentries, we show how to compute an $\\epsilon$-approximate solution in time $O(m\n\\log^{O(1)} (n) \\log (1/\\epsilon))$. Through reductions from [Cohen et al.\nFOCS'16] , this gives the first nearly-linear time algorithms for computing\n$\\epsilon$-approximate solutions to row or column diagonally dominant linear\nsystems (including arbitrary directed Laplacians) and computing\n$\\epsilon$-approximations to various properties of random walks on directed\ngraphs, including stationary distributions, personalized PageRank vectors,\nhitting times, and escape probabilities. These bounds improve upon the recent\nalmost-linear algorithms of [Cohen et al. STOC'17], which gave an algorithm to\nsolve Eulerian Laplacian systems in time $O((m+n2^{O(\\sqrt{\\log n \\log \\log\nn})})\\log^{O(1)}(n \\epsilon^{-1}))$.\n  To achieve our results, we provide a structural result that we believe is of\nindependent interest. We show that Laplacians of all strongly connected\ndirected graphs have sparse approximate LU-factorizations. That is, for every\nsuch directed Laplacian $ {\\mathbf{L}}$, there is a lower triangular matrix\n$\\boldsymbol{\\mathit{{\\mathfrak{L}}}}$ and an upper triangular matrix\n$\\boldsymbol{\\mathit{{\\mathfrak{U}}}}$, each with at most $\\tilde{O}(n)$\nnonzero entries, such that their product $\\boldsymbol{\\mathit{{\\mathfrak{L}}}}\n\\boldsymbol{\\mathit{{\\mathfrak{U}}}}$ spectrally approximates $ {\\mathbf{L}}$\nin an appropriate norm. This claim can be viewed as an analogue of recent work\non sparse Cholesky factorizations of Laplacians of undirected graphs. We show\nhow to construct such factorizations in nearly-linear time and prove that, once\nconstructed, they yield nearly-linear time algorithms for solving directed\nLaplacian systems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 22:33:44 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Cohen", "Michael B.", ""], ["Kelner", "Jonathan", ""], ["Kyng", "Rasmus", ""], ["Peebles", "John", ""], ["Peng", "Richard", ""], ["Rao", "Anup B.", ""], ["Sidford", "Aaron", ""]]}, {"id": "1811.10767", "submitter": "Juan C. Martinez Mori", "authors": "Juan C. Mart\\'inez Mori and Samitha Samaranayake", "title": "The Batched Set Cover Problem", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the batched set cover problem, which is a generalization of the\nonline set cover problem. In this problem, the elements of the ground set that\nneed to be covered arrive in batches. Our main technical contribution is a\ntight $\\Omega(H_{m - 2^z + 1})$ lower bound on the competitive ratio of any\nfractional batched algorithm given an adversary that is required to produce\nbatches of VC-dimension at least $z$, for some $z \\in \\mathbb{N}^0$. This\nrestriction on the adversary is motivated by the fact that, in some real world\napplications, decisions are made after collecting batches of data of\nnon-trivial VC-dimension. In particular, ridesharing systems rely on the batch\nassignment of trip requests to vehicles, and some related problems such as that\nof optimal congregation points for passenger pickups and dropoffs can be\nmodeled as a batched set cover problem with VC-dimension greater than or equal\nto two. Furthermore, we note that while any online algorithm may be used to\nsolve the batched set cover problem by artificially sequencing the elements in\na batch, this procedure may neglect the rich information encoded in the complex\ninteractions between the elements of a batch and the sets that contain them.\nTherefore, we propose a minor modification to an online algorithm found in [8]\nto obtain an algorithm that attempts to exploit such information.\nUnfortunately, we are unable to improve its analysis in a way that reflects\nthis intuition. However, we present computational experiments that provide\nempirical evidence of a constant factor improvement in the competitive ratio.\nTo the best of our knowledge, we are the first to use the VC-dimension in the\ncontext of online (batched) covering problems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 01:52:13 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Mori", "Juan C. Mart\u00ednez", ""], ["Samaranayake", "Samitha", ""]]}, {"id": "1811.10834", "submitter": "Aaron Schild", "authors": "Aaron Schild", "title": "A Schur Complement Cheeger Inequality", "comments": "ITCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cheeger's inequality shows that any undirected graph $G$ with minimum nonzero\nnormalized Laplacian eigenvalue $\\lambda_G$ has a cut with conductance at most\n$O(\\sqrt{\\lambda_G})$. Qualitatively, Cheeger's inequality says that if the\nrelaxation time of a graph is high, there is a cut that certifies this.\nHowever, there is a gap in this relationship, as cuts can have conductance as\nlow as $\\Theta(\\lambda_G)$.\n  To better approximate the relaxation time of a graph, we consider a more\ngeneral object. Instead of bounding the mixing time with cuts, we bound it with\ncuts in graphs obtained by Schur complementing out vertices from the graph $G$.\nCombinatorially, these Schur complements describe random walks in $G$\nrestricted to a subset of its vertices. As a result, all Schur complement cuts\nhave conductance at least $\\Omega(\\lambda_G)$. We show that unlike with cuts,\nthis inequality is tight up to a constant factor. Specifically, there is a\nSchur complement cut with conductance at most $O(\\lambda_G)$.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 06:29:24 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Schild", "Aaron", ""]]}, {"id": "1811.10866", "submitter": "Neha Gupta", "authors": "Neha Gupta and Aaron Sidford", "title": "Exploiting Numerical Sparsity for Efficient Learning : Faster\n  Eigenvector Computation and Regression", "comments": "To appear in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we obtain improved running times for regression and top\neigenvector computation for numerically sparse matrices. Given a data matrix $A\n\\in \\mathbb{R}^{n \\times d}$ where every row $a \\in \\mathbb{R}^d$ has\n$\\|a\\|_2^2 \\leq L$ and numerical sparsity at most $s$, i.e. $\\|a\\|_1^2 /\n\\|a\\|_2^2 \\leq s$, we provide faster algorithms for these problems in many\nparameter settings.\n  For top eigenvector computation, we obtain a running time of $\\tilde{O}(nd +\nr(s + \\sqrt{r s}) / \\mathrm{gap}^2)$ where $\\mathrm{gap} > 0$ is the relative\ngap between the top two eigenvectors of $A^\\top A$ and $r$ is the stable rank\nof $A$. This running time improves upon the previous best unaccelerated running\ntime of $O(nd + r d / \\mathrm{gap}^2)$ as it is always the case that $r \\leq d$\nand $s \\leq d$.\n  For regression, we obtain a running time of $\\tilde{O}(nd + (nL / \\mu)\n\\sqrt{s nL / \\mu})$ where $\\mu > 0$ is the smallest eigenvalue of $A^\\top A$.\nThis running time improves upon the previous best unaccelerated running time of\n$\\tilde{O}(nd + n L d / \\mu)$. This result expands the regimes where regression\ncan be solved in nearly linear time from when $L/\\mu = \\tilde{O}(1)$ to when $L\n/ \\mu = \\tilde{O}(d^{2/3} / (sn)^{1/3})$.\n  Furthermore, we obtain similar improvements even when row norms and numerical\nsparsities are non-uniform and we show how to achieve even faster running times\nby accelerating using approximate proximal point [Frostig et. al. 2015] /\ncatalyst [Lin et. al. 2015]. Our running times depend only on the size of the\ninput and natural numerical measures of the matrix, i.e. eigenvalues and\n$\\ell_p$ norms, making progress on a key open problem regarding optimal running\ntimes for efficient large-scale learning.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 08:22:54 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Gupta", "Neha", ""], ["Sidford", "Aaron", ""]]}, {"id": "1811.10879", "submitter": "Michael Kapralov", "authors": "Michael Kapralov and Dmitry Krachun", "title": "An Optimal Space Lower Bound for Approximating MAX-CUT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the value of MAX-CUT in a graph in the\nstreaming model of computation. At one extreme, there is a trivial\n$2$-approximation for this problem that uses only $O(\\log n)$ space, namely,\ncount the number of edges and output half of this value as the estimate for the\nsize of the MAX-CUT. On the other extreme, for any fixed $\\epsilon > 0$, if one\nallows $\\tilde{O}(n)$ space, a $(1+\\epsilon)$-approximate solution to the\nMAX-CUT value can be obtained by storing an $\\tilde{O}(n)$-size sparsifier that\nessentially preserves MAX-CUT value.\n  Our main result is that any (randomized) single pass streaming algorithm that\nbreaks the $2$-approximation barrier requires $\\Omega(n)$-space, thus resolving\nthe space complexity of any non-trivial approximations of the MAX-CUT value to\nwithin polylogarithmic factors in the single pass streaming model. We achieve\nthe result by presenting a tight analysis of the Implicit Hidden Partition\nProblem introduced by Kapralov et al.[SODA'17] for an arbitrarily large number\nof players. In this problem a number of players receive random matchings of\n$\\Omega(n)$ size together with random bits on the edges, and their task is to\ndetermine whether the bits correspond to parities of some hidden bipartition,\nor are just uniformly random.\n  Unlike all previous Fourier analytic communication lower bounds, our analysis\ndoes not directly use bounds on the $\\ell_2$ norm of Fourier coefficients of a\ntypical message at any given weight level that follow from hypercontractivity.\nInstead, we use the fact that graphs received by players are sparse (matchings)\nto obtain strong upper bounds on the $\\ell_1$ norm of the Fourier coefficients\nof the messages of individual players, and then argue, using the convolution\ntheorem, that similar strong bounds on the $\\ell_1$ norm are essentially\npreserved once messages of different players are combined.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 09:04:16 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Kapralov", "Michael", ""], ["Krachun", "Dmitry", ""]]}, {"id": "1811.10909", "submitter": "He Sun", "authors": "Huan Li and He Sun and Luca Zanetti", "title": "Hermitian Laplacians and a Cheeger inequality for the Max-2-Lin problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study spectral approaches for the MAX-2-LIN(k) problem, in which we are\ngiven a system of $m$ linear equations of the form $x_i - x_j \\equiv c_{ij}\\mod\nk$, and required to find an assignment to the $n$ variables $\\{x_i\\}$ that\nmaximises the total number of satisfied equations.\n  We consider Hermitian Laplacians related to this problem, and prove a Cheeger\ninequality that relates the smallest eigenvalue of a Hermitian Laplacian to the\nmaximum number of satisfied equations of a MAX-2-LIN(k) instance $\\mathcal{I}$.\nWe develop an $\\widetilde{O}(kn^2)$ time algorithm that, for any\n$(1-\\varepsilon)$-satisfiable instance, produces an assignment satisfying a\n$\\left(1 - O(k)\\sqrt{\\varepsilon}\\right)$-fraction of equations. We also\npresent a subquadratic-time algorithm that, when the graph associated with\n$\\mathcal{I}$ is an expander, produces an assignment satisfying a $\\left(1-\nO(k^2)\\varepsilon \\right)$-fraction of the equations. Our Cheeger inequality\nand first algorithm can be seen as generalisations of the Cheeger inequality\nand algorithm for MAX-CUT developed by Trevisan.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 11:02:36 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Li", "Huan", ""], ["Sun", "He", ""], ["Zanetti", "Luca", ""]]}, {"id": "1811.11007", "submitter": "Christopher Whidden", "authors": "Chris Whidden, Brian C. Claywell, Thayer Fisher, Andrew F. Magee,\n  Mathieu Fourment, Frederick A. Matsen IV", "title": "Systematic Exploration of the High Likelihood Set of Phylogenetic Tree\n  Topologies", "comments": "25 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Markov chain Monte Carlo explores tree space slowly, in part because\nit frequently returns to the same tree topology. An alternative strategy would\nbe to explore tree space systematically, and never return to the same topology.\nIn this paper, we present an efficient parallelized method to map out the high\nlikelihood set of phylogenetic tree topologies via systematic search, which we\nshow to be a good approximation of the high posterior set of tree topologies.\nHere `likelihood' of a topology refers to the tree likelihood for the\ncorresponding tree with optimized branch lengths. We call this method\n`phylogenetic topographer' (PT). The PT strategy is very simple: starting in a\nnumber of local topology maxima (obtained by hill-climbing from random starting\npoints), explore out using local topology rearrangements, only continuing\nthrough topologies that are better than than some likelihood threshold below\nthe best observed topology. We show that the normalized topology likelihoods\nare a useful proxy for the Bayesian posterior probability of those topologies.\nBy using a non-blocking hash table keyed on unique representations of tree\ntopologies, we avoid visiting topologies more than once across all concurrent\nthreads exploring tree space. We demonstrate that PT can be used directly to\napproximate a Bayesian consensus tree topology. When combined with an accurate\nmeans of evaluating per-topology marginal likelihoods, PT gives an alternative\nprocedure for obtaining Bayesian posterior distributions on phylogenetic tree\ntopologies.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 14:19:08 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Whidden", "Chris", ""], ["Claywell", "Brian C.", ""], ["Fisher", "Thayer", ""], ["Magee", "Andrew F.", ""], ["Fourment", "Mathieu", ""], ["Matsen", "Frederick A.", "IV"]]}, {"id": "1811.11076", "submitter": "J\\\"org Bachmann", "authors": "J\\\"org P. Bachmann, Johann-Christoph Freytag", "title": "An Analytical Approach to Improving Time Warping on Multidimensional\n  Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic time warping ($\\texttt{DTW}$) is one of the most used distance\nfunctions to compare time series, e.$\\,$g. in nearest neighbor classifiers.\nYet, fast state of the art algorithms only compare 1-dimensional time series\nefficiently. One of these state of the art algorithms uses a lower bound\n($\\texttt{LB}_\\texttt{Keogh}$) introduced by E. Keogh to prune $\\texttt{DTW}$\ncomputations. We introduce $\\texttt{LB}_\\texttt{Box}$ as a canonical extension\nto $\\texttt{LB}_\\texttt{Keogh}$ on multi-dimensional time series. We evaluate\nits performance conceptually and experimentally and show that an alternative to\n$\\texttt{LB}_\\texttt{Box}$ is necessary for multi-dimensional time series. We\nalso propose a new algorithm for the dog-keeper distance ($\\texttt{DK}$) which\nis an alternative distance function to $\\texttt{DTW}$ and show that it\noutperforms $\\texttt{DTW}$ with $\\texttt{LB}_\\texttt{Box}$ by more than one\norder of magnitude on multi-dimensional time series.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 16:19:05 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Bachmann", "J\u00f6rg P.", ""], ["Freytag", "Johann-Christoph", ""]]}, {"id": "1811.11087", "submitter": "Hang Hu", "authors": "Hayoung Choi, Jinglian He, Hang Hu, and Yuanming Shi", "title": "Fast computation of von Neumann entropy for large-scale graphs via\n  quadratic approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.SI eess.SP math.IT physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The von Neumann graph entropy (VNGE) can be used as a measure of graph\ncomplexity, which can be the measure of information divergence and distance\nbetween graphs. However, computing VNGE is extensively demanding for a\nlarge-scale graph. We propose novel quadratic approximations for fast computing\nVNGE. Various inequalities for error between the quadratic approximations and\nthe exact VNGE are found. Our methods reduce the cubic complexity of VNGE to\nlinear complexity. Computational simulations on random graph models and various\nreal network datasets demonstrate superior performance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 16:02:08 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 02:26:47 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Choi", "Hayoung", ""], ["He", "Jinglian", ""], ["Hu", "Hang", ""], ["Shi", "Yuanming", ""]]}, {"id": "1811.11148", "submitter": "Gautam Kamath", "authors": "Cl\\'ement L. Canonne, Gautam Kamath, Audra McMillan, Adam Smith,\n  Jonathan Ullman", "title": "The Structure of Optimal Private Tests for Simple Hypotheses", "comments": "To appear in STOC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis testing plays a central role in statistical inference, and is used\nin many settings where privacy concerns are paramount. This work answers a\nbasic question about privately testing simple hypotheses: given two\ndistributions $P$ and $Q$, and a privacy level $\\varepsilon$, how many i.i.d.\nsamples are needed to distinguish $P$ from $Q$ subject to\n$\\varepsilon$-differential privacy, and what sort of tests have optimal sample\ncomplexity? Specifically, we characterize this sample complexity up to constant\nfactors in terms of the structure of $P$ and $Q$ and the privacy level\n$\\varepsilon$, and show that this sample complexity is achieved by a certain\nrandomized and clamped variant of the log-likelihood ratio test. Our result is\nan analogue of the classical Neyman-Pearson lemma in the setting of private\nhypothesis testing. We also give an application of our result to the private\nchange-point detection. Our characterization applies more generally to\nhypothesis tests satisfying essentially any notion of algorithmic stability,\nwhich is known to imply strong generalization bounds in adaptive data analysis,\nand thus our results have applications even when privacy is not a primary\nconcern.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:21:33 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 21:46:45 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["Kamath", "Gautam", ""], ["McMillan", "Audra", ""], ["Smith", "Adam", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1811.11197", "submitter": "Richard Garcia-Lebron", "authors": "Richard Garcia-Lebron, David J. Myers, Shouhuai Xu and Jie Sun", "title": "Node Diversification in Complex Networks by Decentralized Coloring", "comments": null, "journal-ref": "Journal of Complex Networks (2018)", "doi": "10.1093/comnet/cny031", "report-no": null, "categories": "cs.SI cs.CR cs.DS cs.MA physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a decentralized coloring approach to diversify the nodes in a\ncomplex network. The key is the introduction of a local conflict index that\nmeasures the color conflicts arising at each node which can be efficiently\ncomputed using only local information. We demonstrate via both synthetic and\nreal-world networks that the proposed approach significantly outperforms random\ncoloring as measured by the size of the largest color-induced connected\ncomponent. Interestingly, for scale-free networks further improvement of\ndiversity can be achieved by tuning a degree-biasing weighting parameter in the\nlocal conflict index.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:02:36 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Garcia-Lebron", "Richard", ""], ["Myers", "David J.", ""], ["Xu", "Shouhuai", ""], ["Sun", "Jie", ""]]}, {"id": "1811.11237", "submitter": "Yue Wu", "authors": "Yue Wu", "title": "A Note on Random Sampling for Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the framework of randomised matrix multiplication to a\ncoarser partition and proposes an algorithm as a complement to the classical\nalgorithm, especially when the optimal probability distribution of the latter\none is closed to uniform. The new algorithm increases the likelihood of getting\na small approximation error in 2-norm and has the squared approximation error\nin Frobenious norm bounded by that from the classical algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 20:16:23 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 18:46:09 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Wu", "Yue", ""]]}, {"id": "1811.11259", "submitter": "Francesco Fraternali", "authors": "Francesco Fraternali, Bharathan Balaji, Rajesh Gupta", "title": "Scaling Configuration of Energy Harvesting Sensors with Reinforcement\n  Learning", "comments": "7 pages, 5 figures", "journal-ref": "ENSsys '18: International Workshop on Energy Harvesting &\n  Energy-Neutral Sensing Systems}{November 4, 2018}{Shenzhen, China", "doi": "10.1145/3279755.3279760", "report-no": null, "categories": "cs.LG cs.AI cs.DS cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of the Internet of Things (IoT), an increasing number of\nenergy harvesting methods are being used to supplement or supplant battery\nbased sensors. Energy harvesting sensors need to be configured according to the\napplication, hardware, and environmental conditions to maximize their\nusefulness. As of today, the configuration of sensors is either manual or\nheuristics based, requiring valuable domain expertise. Reinforcement learning\n(RL) is a promising approach to automate configuration and efficiently scale\nIoT deployments, but it is not yet adopted in practice. We propose solutions to\nbridge this gap: reduce the training phase of RL so that nodes are operational\nwithin a short time after deployment and reduce the computational requirements\nto scale to large deployments. We focus on configuration of the sampling rate\nof indoor solar panel based energy harvesting sensors. We created a simulator\nbased on 3 months of data collected from 5 sensor nodes subject to different\nlighting conditions. Our simulation results show that RL can effectively learn\nenergy availability patterns and configure the sampling rate of the sensor\nnodes to maximize the sensing data while ensuring that energy storage is not\ndepleted. The nodes can be operational within the first day by using our\nmethods. We show that it is possible to reduce the number of RL policies by\nusing a single policy for nodes that share similar lighting conditions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 21:05:43 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Fraternali", "Francesco", ""], ["Balaji", "Bharathan", ""], ["Gupta", "Rajesh", ""]]}, {"id": "1811.11538", "submitter": "Gary Kochenberger Dr.", "authors": "Fred Glover, Gary Kochenberger, Yu Du", "title": "A Tutorial on Formulating and Using QUBO Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Quadratic Unconstrained Binary Optimization (QUBO) model has gained\nprominence in recent years with the discovery that it unifies a rich variety of\ncombinatorial optimization problems. By its association with the Ising problem\nin physics, the QUBO model has emerged as an underpinning of the quantum\ncomputing area known as quantum annealing and has become a subject of study in\nneuromorphic computing. Through these connections, QUBO models lie at the heart\nof experimentation carried out with quantum computers developed by D-Wave\nSystems and neuromorphic computers developed by IBM. Computational experience\nis being amassed by both the classical and the quantum computing communities\nthat highlights not only the potential of the QUBO model but also its\neffectiveness as an alternative to traditional modeling and solution\nmethodologies. This tutorial discloses the basic features of the QUBO model\nthat give it the power and flexibility to encompass the range of applications\nthat have thrust it onto center stage of the optimization field. We show how\nmany different types of constraining relationships arising in practice can be\nembodied within the \"unconstrained\" QUBO formulation in a very natural manner\nusing penalty functions, yielding exact model representations in contrast to\nthe approximate representations produced by customary uses of penalty\nfunctions. Each step of generating such models is illustrated in detail by\nsimple numerical examples, to highlight the convenience of using QUBO models in\nnumerous settings. We also describe recent innovations for solving QUBO models\nthat offer a fertile avenue for integrating classical and quantum computing and\nfor applying these models in machine learning.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 18:38:34 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2018 03:00:35 GMT"}, {"version": "v3", "created": "Sat, 5 Jan 2019 02:44:10 GMT"}, {"version": "v4", "created": "Sun, 10 Feb 2019 15:53:08 GMT"}, {"version": "v5", "created": "Fri, 14 Jun 2019 17:54:38 GMT"}, {"version": "v6", "created": "Mon, 4 Nov 2019 20:50:57 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Glover", "Fred", ""], ["Kochenberger", "Gary", ""], ["Du", "Yu", ""]]}, {"id": "1811.11549", "submitter": "Eli Chien", "authors": "I Chien, Huozhi Zhou, Pan Li", "title": "$HS^2$: Active Learning over Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hypergraph-based active learning scheme which we term $HS^2$,\n$HS^2$ generalizes the previously reported algorithm $S^2$ originally proposed\nfor graph-based active learning with pointwise queries [Dasarathy et al., COLT\n2015]. Our $HS^2$ method can accommodate hypergraph structures and allows one\nto ask both pointwise queries and pairwise queries. Based on a novel parametric\nsystem particularly designed for hypergraphs, we derive theoretical results on\nthe query complexity of $HS^2$ for the above described generalized settings.\nBoth the theoretical and empirical results show that $HS^2$ requires a\nsignificantly fewer number of queries than $S^2$ when one uses $S^2$ over a\ngraph obtained from the corresponding hypergraph via clique expansion.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 18:00:56 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Chien", "I", ""], ["Zhou", "Huozhi", ""], ["Li", "Pan", ""]]}, {"id": "1811.11593", "submitter": "Nikolaos Bikakis", "authors": "Nikos Bikakis, Vana Kalogeraki, Dimitrios Gunupulos", "title": "Attendance Maximization for Successful Social Event Planning", "comments": "This paper appears in 22nd Intl. Conf. on Extending Database\n  Technology (EDBT 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social event planning has received a great deal of attention in recent years\nwhere various entities, such as event planners and marketing companies,\norganizations, venues, or users in Event-based Social Networks, organize\nnumerous social events (e.g., festivals, conferences, promotion parties).\nRecent studies show that \"attendance\" is the most common metric used to capture\nthe success of social events, since the number of attendees has great impact on\nthe event's expected gains (e.g., revenue, artist/brand publicity). In this\nwork, we study the Social Event Scheduling (SES) problem which aims at\nidentifying and assigning social events to appropriate time slots, so that the\nnumber of events attendees is maximized. We show that, even in highly\nrestricted instances, the SES problem is NP-hard to be approximated over a\nfactor. To solve the SES problem, we design three efficient and scalable\nalgorithms. These algorithms exploit several novel schemes that we design. We\nconduct extensive experiments using several real and synthetic datasets, and\ndemonstrate that the proposed algorithms perform on average half the\ncomputations compared to the existing solution and, in several cases, are 3-5\ntimes faster.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 14:35:22 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Bikakis", "Nikos", ""], ["Kalogeraki", "Vana", ""], ["Gunupulos", "Dimitrios", ""]]}, {"id": "1811.11595", "submitter": "Yulia Kovalenko", "authors": "Alexander Kononov and Yulia Kovalenko", "title": "Approximate Schedules for Non-Migratory Parallel Jobs in Speed-Scaled\n  Multiprocessor Systems", "comments": "10 pages, Siberian Electronic Mathematical Reports (on review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of scheduling rigid parallel jobs on variable speed\nprocessors so as to minimize the total energy consumption. Each job is\nspecified by its processing volume and the required number of processors. We\npropose new constant factor approximation algorithms for the non-migratory\ncases when all jobs have a common release time and/or a common deadline.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 14:38:07 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kononov", "Alexander", ""], ["Kovalenko", "Yulia", ""]]}, {"id": "1811.11635", "submitter": "Ellis Hershkowitz", "authors": "D Ellis Hershkowitz and R. Ravi and Sahil Singla", "title": "Prepare for the Expected Worst: Algorithms for Reconfigurable Resources\n  Under Uncertainty", "comments": "Corrected several typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study how to optimally balance cheap inflexible resources\nwith more expensive, reconfigurable resources despite uncertainty in the input\nproblem. Specifically, we introduce the MinEMax model to study \"build versus\nrent\" problems. In our model different scenarios appear independently. Before\nknowing which scenarios appear, we may build rigid resources that cannot be\nchanged for different scenarios. Once we know which scenarios appear, we are\nallowed to rent reconfigurable but expensive resources to use across scenarios.\nAlthough computing the objective in our model might seem to require enumerating\nexponentially-many possibilities, we show it is well-estimated by a surrogate\nobjective which is representable by a polynomial-size LP. In this surrogate\nobjective we pay for each scenario only to the extent that it exceeds a certain\nthreshold. Using this objective we design algorithms that\napproximately-optimally balance inflexible and reconfigurable resources for\nseveral NP-hard covering problems. For example, we study minimum spanning and\nSteiner trees, minimum cuts and facility location variants. Up to constants our\napproximation guarantees match those of previous algorithms for the\npreviously-studied demand-robust and stochastic two-stage models. Lastly, we\ndemonstrate that our problem is sufficiently general to smoothly interpolate\nbetween previous demand-robust and stochastic two-stage problems.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 15:43:01 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 14:04:21 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Hershkowitz", "D Ellis", ""], ["Ravi", "R.", ""], ["Singla", "Sahil", ""]]}, {"id": "1811.11700", "submitter": "Richard Spence", "authors": "Faryad Darabi Sahneh, Alon Efrat, Stephen Kobourov, Spencer Krieger,\n  Richard Spence", "title": "Approximation algorithms for the vertex-weighted grade-of-service\n  Steiner tree problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G = (V,E)$ and a subset $T \\subseteq V$ of terminals, a\n\\emph{Steiner tree} of $G$ is a tree that spans $T$. In the vertex-weighted\nSteiner tree (VST) problem, each vertex is assigned a non-negative weight, and\nthe goal is to compute a minimum weight Steiner tree of $G$.\n  We study a natural generalization of the VST problem motivated by multi-level\ngraph construction, the \\emph{vertex-weighted grade-of-service Steiner tree\nproblem} (V-GSST), which can be stated as follows: given a graph $G$ and\nterminals $T$, where each terminal $v \\in T$ requires a facility of a minimum\ngrade of service $R(v)\\in \\{1,2,\\ldots\\ell\\}$, compute a Steiner tree $G'$ by\ninstalling facilities on a subset of vertices, such that any two vertices\nrequiring a certain grade of service are connected by a path in $G'$ with the\nminimum grade of service or better. Facilities of higher grade are more costly\nthan facilities of lower grade. Multi-level variants such as this one can be\nuseful in network design problems where vertices may require facilities of\nvarying priority.\n  While similar problems have been studied in the edge-weighted case, they have\nnot been studied as well in the more general vertex-weighted case. We first\ndescribe a simple heuristic for the V-GSST problem whose approximation ratio\ndepends on $\\ell$, the number of grades of service. We then generalize the\ngreedy algorithm of [Klein \\& Ravi, 1995] to show that the V-GSST problem\nadmits a $(2 \\ln |T|)$-approximation, where $T$ is the set of terminals\nrequiring some facility. This result is surprising, as it shows that the\n(seemingly harder) multi-grade problem can be approximated as well as the VST\nproblem, and that the approximation ratio does not depend on the number of\ngrades of service.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 17:37:13 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 23:02:41 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Sahneh", "Faryad Darabi", ""], ["Efrat", "Alon", ""], ["Kobourov", "Stephen", ""], ["Krieger", "Spencer", ""], ["Spence", "Richard", ""]]}, {"id": "1811.11746", "submitter": "Rui Portocarrero Sarmento MSc", "authors": "Rui Portocarrero Sarmento and Pavel Brazdil", "title": "Incremental Sparse TFIDF & Incremental Similarity with Bipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we experimented with several concepts regarding text streams\nanalysis.\n  We tested an implementation of Incremental Sparse TF-IDF (IS-TFIDF) and\nIncremental Cosine Similarity (ICS) with the use of bipartite graphs.\n  We are using bipartite graphs - one type of node are documents, and the other\ntype of nodes are words - to know what documents are affected with a word\narrival at the stream (the neighbors of the word in the graph). Thus, with this\ninformation, we leverage optimized algorithms used for graph-based\napplications. The concept is similar to, for example, the use of hash tables or\nother computer science concepts used for fast access to information in memory.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 15:20:32 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Sarmento", "Rui Portocarrero", ""], ["Brazdil", "Pavel", ""]]}, {"id": "1811.11856", "submitter": "J\\\"org Bachmann", "authors": "J\\\"org P. Bachmann, Johann-Christoph Freytag", "title": "Efficient Measuring of Congruence on High Dimensional Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A time series is a sequence of data items; typical examples are streams of\ntemperature measurements, stock ticker data, or gestures recorded with modern\nvirtual reality motion controllers. Quite some research has been devoted to\ncomparing and indexing time series. Especially, when the comparison should not\nbe affected by time warping, the ubiquitous Dynamic Time Warping distance\nfunction ($\\texttt{DTW}$) is one of the most analyzed time series distance\nfunctions. The Dog-Keeper distance ($\\texttt{DK}$) is another example for a\ndistance function on time series which is truely invariant under time warping.\n  For many application scenarios (e.$\\,$g. motion gesture recognition in\nvirtual reality), the invariance under isometric spatial transformations\n(i.$\\,$e. rotation, translation, and mirroring) is as important as the\ninvariance under time warping. Distance functions on time series which are\ninvariant under isometric transformations can be seen as measurements for the\ncongruency of two time series. The congruence distance ($\\texttt{CD}$) is an\nexample for such a distance function. However, it is very hard to compute and\nit is not invariant under time warpings.\n  In this work, we are taking one step towards developing a feasable distance\nfunction which is invariant under isometric spatial transformations and time\nwarping: We develop four approximations for $\\texttt{CD}$. Two of these even\nsatisfy the triangle inequality and can thus be used with metric indexing\nstructures. We show that all approximations serve as a lower bound to\n$\\texttt{CD}$. Our evaluation shows that they achieve remarkable tightness\nwhile providing a speedup of more than two orders of magnitude to the\ncongruence distance.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 16:42:21 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Bachmann", "J\u00f6rg P.", ""], ["Freytag", "Johann-Christoph", ""]]}, {"id": "1811.11881", "submitter": "Karthik Abinav Sankararaman", "authors": "Nicole Immorlica and Karthik Abinav Sankararaman and Robert Schapire\n  and Aleksandrs Slivkins", "title": "Adversarial Bandits with Knapsacks", "comments": "Extended abstract appeared in FOCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bandits with Knapsacks (henceforth, BwK), a general model for\nmulti-armed bandits under supply/budget constraints. In particular, a bandit\nalgorithm needs to solve a well-known knapsack problem: find an optimal packing\nof items into a limited-size knapsack. The BwK problem is a common\ngeneralization of numerous motivating examples, which range from dynamic\npricing to repeated auctions to dynamic ad allocation to network routing and\nscheduling. While the prior work on BwK focused on the stochastic version, we\npioneer the other extreme in which the outcomes can be chosen adversarially.\nThis is a considerably harder problem, compared to both the stochastic version\nand the \"classic\" adversarial bandits, in that regret minimization is no longer\nfeasible. Instead, the objective is to minimize the competitive ratio: the\nratio of the benchmark reward to the algorithm's reward.\n  We design an algorithm with competitive ratio O(log T) relative to the best\nfixed distribution over actions, where T is the time horizon; we also prove a\nmatching lower bound. The key conceptual contribution is a new perspective on\nthe stochastic version of the problem. We suggest a new algorithm for the\nstochastic version, which builds on the framework of regret minimization in\nrepeated games and admits a substantially simpler analysis compared to prior\nwork. We then analyze this algorithm for the adversarial version and use it as\na subroutine to solve the latter.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 23:43:11 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 02:13:00 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2019 17:12:51 GMT"}, {"version": "v4", "created": "Fri, 22 Mar 2019 22:17:04 GMT"}, {"version": "v5", "created": "Sun, 13 Oct 2019 05:01:32 GMT"}, {"version": "v6", "created": "Fri, 6 Nov 2020 19:18:05 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Immorlica", "Nicole", ""], ["Sankararaman", "Karthik Abinav", ""], ["Schapire", "Robert", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "1811.12017", "submitter": "Lijie Chen", "authors": "Lijie Chen, Ryan Williams", "title": "An Equivalence Class for Orthogonal Vectors", "comments": "To appear in SODA 2019. The abstract is shortened to meet the\n  constraint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Orthogonal Vectors problem ($\\textsf{OV}$) asks: given $n$ vectors in\n$\\{0,1\\}^{O(\\log n)}$, are two of them orthogonal? $\\textsf{OV}$ is easily\nsolved in $O(n^2 \\log n)$ time, and it is a central problem in fine-grained\ncomplexity: dozens of conditional lower bounds are based on the popular\nhypothesis that $\\textsf{OV}$ cannot be solved in (say) $n^{1.99}$ time.\nHowever, unlike the APSP problem, few other problems are known to be\nnon-trivially equivalent to $\\textsf{OV}$.\n  We show $\\textsf{OV}$ is truly-subquadratic equivalent to several fundamental\nproblems, all of which (a priori) look harder than $\\textsf{OV}$. A partial\nlist is given below:\n  ($\\textsf{Min-IP}/\\textsf{Max-IP}$) Find a red-blue pair of vectors with\nminimum (respectively, maximum) inner product, among $n$ vectors in\n$\\{0,1\\}^{O(\\log n)}$.\n  ($\\textsf{Exact-IP}$) Find a red-blue pair of vectors with inner product\nequal to a given target integer, among $n$ vectors in $\\{0,1\\}^{O(\\log n)}$.\n  ($\\textsf{Apx-Min-IP}/\\textsf{Apx-Max-IP}$) Find a red-blue pair of vectors\nthat is a 100-approximation to the minimum (resp. maximum) inner product, among\n$n$ vectors in $\\{0,1\\}^{O(\\log n)}$.\n  (Approx. $\\textsf{Bichrom.-$\\ell_p$-Closest-Pair}$) Compute a $(1 +\n\\Omega(1))$-approximation to the $\\ell_p$-closest red-blue pair (for a constant\n$p \\in [1,2]$), among $n$ points in $\\mathbb{R}^d$, $d \\le n^{o(1)}$.\n  (Approx. $\\textsf{$\\ell_p$-Furthest-Pair}$) Compute a $(1 +\n\\Omega(1))$-approximation to the $\\ell_p$-furthest pair (for a constant $p \\in\n[1,2]$), among $n$ points in $\\mathbb{R}^d$, $d \\le n^{o(1)}$.\n  We also show that there is a $\\text{poly}(n)$ space, $n^{1-\\epsilon}$ query\ntime data structure for Partial Match with vectors from $\\{0,1\\}^{O(\\log n)}$\nif and only if such a data structure exists for $1+\\Omega(1)$ Approximate\nNearest Neighbor Search in Euclidean space.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 08:48:06 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Chen", "Lijie", ""], ["Williams", "Ryan", ""]]}, {"id": "1811.12040", "submitter": "Brendan Avent", "authors": "Brendan Avent, Yatharth Dubey, Aleksandra Korolova", "title": "The Power of The Hybrid Model for Mean Estimation", "comments": "Proceedings on Privacy Enhancing Technologies 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the power of the hybrid model of differential privacy (DP), in\nwhich some users desire the guarantees of the local model of DP and others are\ncontent with receiving the trusted-curator model guarantees. In particular, we\nstudy the utility of hybrid model estimators that compute the mean of arbitrary\nreal-valued distributions with bounded support. When the curator knows the\ndistribution's variance, we design a hybrid estimator that, for realistic\ndatasets and parameter settings, achieves a constant factor improvement over\nnatural baselines. We then analytically characterize how the estimator's\nutility is parameterized by the problem setting and parameter choices. When the\ndistribution's variance is unknown, we design a heuristic hybrid estimator and\nanalyze how it compares to the baselines. We find that it often performs better\nthan the baselines, and sometimes almost as well as the known-variance\nestimator. We then answer the question of how our estimator's utility is\naffected when users' data are not drawn from the same distribution, but rather\nfrom distributions dependent on their trust model preference. Concretely, we\nexamine the implications of the two groups' distributions diverging and show\nthat in some cases, our estimators maintain fairly high utility. We then\ndemonstrate how our hybrid estimator can be incorporated as a sub-component in\nmore complex, higher-dimensional applications. Finally, we propose a new\nprivacy amplification notion for the hybrid model that emerges due to\ninteraction between the groups, and derive corresponding amplification results\nfor our hybrid estimators.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 09:52:17 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 21:14:29 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Avent", "Brendan", ""], ["Dubey", "Yatharth", ""], ["Korolova", "Aleksandra", ""]]}, {"id": "1811.12094", "submitter": "Oylum \\c{S}eker", "authors": "Oylum \\c{S}eker, T{\\i}naz Ekim, Z. Caner Ta\\c{s}k{\\i}n", "title": "An Exact Cutting Plane Algorithm to Solve the Selective Graph Coloring\n  Problem in Perfect Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the selective graph coloring problem, which is a generalization\nof the classical graph coloring problem. Given a graph together with a\npartition of its vertex set into clusters, we want to choose exactly one vertex\nper cluster so that the number of colors needed to color the selected set of\nvertices is minimized. This problem is known to be NP-hard. In this study, we\nfocus on an exact cutting plane algorithm for selective graph coloring in\nperfect graphs. Since there exists no suite of perfect graph instances to the\nbest of our knowledge, we also propose an algorithm to randomly (but not\nuniformly) generate perfect graphs, and provide a large collection of instances\navailable online. We conduct computational experiments to test our method on\ngraphs with varying size and densities, and compare our results with a\nstate-of-the-art algorithm from the literature and with solving an integer\nprogramming formulation of the problem by CPLEX. Our experiments demonstrate\nthat our solution strategy significantly improves the solvability of the\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 12:20:05 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 19:11:39 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 08:43:56 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["\u015eeker", "Oylum", ""], ["Ekim", "T\u0131naz", ""], ["Ta\u015fk\u0131n", "Z. Caner", ""]]}, {"id": "1811.12280", "submitter": "Nate Veldt", "authors": "Nate Veldt and Christine Klymko and David Gleich", "title": "Flow-Based Local Graph Clustering with Better Seed Set Inclusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow-based methods for local graph clustering have received significant\nrecent attention for their theoretical cut improvement and runtime guarantees.\nIn this work we present two improvements for using flow-based methods in\nreal-world semi-supervised clustering problems. Our first contribution is a\ngeneralized objective function that allows practitioners to place strict and\nsoft penalties on excluding specific seed nodes from the output set. This\nfeature allows us to avoid the tendency, often exhibited by previous flow-based\nmethods, to contract a large seed set into a small set of nodes that does not\ncontain all or even most of the seed nodes. Our second contribution is a fast\nalgorithm for minimizing our generalized objective function, based on a variant\nof the push-relabel algorithm for computing preflows. We make our approach very\nfast in practice by implementing a global relabeling heuristic and employing a\nwarm-start procedure to quickly solve related cut problems. In practice our\nalgorithm is faster than previous related flow-based methods, and is also more\nrobust in detecting ground truth target regions in a graph, thanks to its\nability to better incorporate semi-supervised information about target\nclusters.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 16:14:10 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 17:51:45 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Veldt", "Nate", ""], ["Klymko", "Christine", ""], ["Gleich", "David", ""]]}, {"id": "1811.12361", "submitter": "Aravindan Vijayaraghavan", "authors": "Aditya Bhaskara, Aidao Chen, Aidan Perreault and Aravindan\n  Vijayaraghavan", "title": "Smoothed Analysis in Unsupervised Learning via Decoupling", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoothed analysis is a powerful paradigm in overcoming worst-case\nintractability in unsupervised learning and high-dimensional data analysis.\nWhile polynomial time smoothed analysis guarantees have been obtained for\nworst-case intractable problems like tensor decompositions and learning\nmixtures of Gaussians, such guarantees have been hard to obtain for several\nother important problems in unsupervised learning. A core technical challenge\nin analyzing algorithms is obtaining lower bounds on the least singular value\nfor random matrix ensembles with dependent entries, that are given by\nlow-degree polynomials of a few base underlying random variables.\n  In this work, we address this challenge by obtaining high-confidence lower\nbounds on the least singular value of new classes of structured random matrix\nensembles of the above kind. We then use these bounds to design algorithms with\npolynomial time smoothed analysis guarantees for the following three important\nproblems in unsupervised learning:\n  1. Robust subspace recovery, when the fraction $\\alpha$ of inliers in the\nd-dimensional subspace $T \\subset \\mathbb{R}^n$ is at least $\\alpha >\n(d/n)^\\ell$ for any constant integer $\\ell>0$. This contrasts with the known\nworst-case intractability when $\\alpha< d/n$, and the previous smoothed\nanalysis result which needed $\\alpha > d/n$ (Hardt and Moitra, 2013).\n  2. Learning overcomplete hidden markov models, where the size of the state\nspace is any polynomial in the dimension of the observations. This gives the\nfirst polynomial time guarantees for learning overcomplete HMMs in a smoothed\nanalysis model.\n  3. Higher order tensor decompositions, where we generalize the so-called\nFOOBI algorithm of Cardoso to find order-$\\ell$ rank-one tensors in a subspace.\nThis allows us to obtain polynomially robust decomposition algorithms for\n$2\\ell$'th order tensors with rank $O(n^{\\ell})$.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 18:12:32 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 02:12:23 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Bhaskara", "Aditya", ""], ["Chen", "Aidao", ""], ["Perreault", "Aidan", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1811.12369", "submitter": "Moti Medina", "authors": "Johannes Bund, Christoph Lenzen, Moti Medina", "title": "Small Hazard-free Transducers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, an unconditional exponential separation between the hazard-free\ncomplexity and (standard) circuit complexity of explicit functions has been\nshown~\\cite{ikenmeyer18complexity}. This raises the question: which classes of\nfunctions permit efficient hazard-free circuits?\n  Our main result is as follows. A \\emph{transducer} is a finite state machine\nthat transcribes, symbol by symbol, an input string of length $n$ into an\noutput string of length $n$. We prove that any function arising from a\ntransducer with $s$ states receiving input symbols encoded by $\\ell$ bits has a\nhazard-free circuit of size $2^{O(s+\\ell)}\\cdot n$ and depth $O(\\ell+ s\\cdot\n\\log n)$; in particular, if $s, \\ell\\in O(1)$, size and depth are\nasymptotically optimal.\n  We utilize our main result to derive efficient circuits for\n\\emph{$k$-recoverable addition}. Informally speaking, a code is\n\\emph{$k$-recoverable} if it does not increase uncertainty regarding the\nencoded value, so long as it is guaranteed that it is from\n$\\{x,x+1,\\ldots,x+k\\}$ for some $x\\in \\mathbb{N}_0$. We provide an\nasymptotically optimal $k$-recoverable code. We also realize a transducer with\n$O(k)$ states that adds two codewords from this $k$-recoverable code. Combined\nwith our main result, we obtain a hazard-free adder circuit of size $2^{O(k)}n$\nand depth $O(k\\log n)$ with respect to this code, i.e., a $k$-recoverable adder\ncircuit that adds two codewords of $n$ bits each. In other words,\n$k$-recoverable addition is fixed-parameter tractable with respect to $k$. We\nthen reduce the maximum size of the state machines involved to $O(1)$,\nresulting in a circuit for $k$-recoverable addition of size $O(n+k\\log k)$ and\ndepth $O(\\log n)$. Thus, if the uncertainties of each of the addends span\nintervals of length $O(n/\\log n)$, there is an \\emph{asymptotically optimal}\nadder that attains the best possible output uncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 18:27:25 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 10:16:58 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Bund", "Johannes", ""], ["Lenzen", "Christoph", ""], ["Medina", "Moti", ""]]}, {"id": "1811.12469", "submitter": "Vitaly Feldman", "authors": "\\'Ulfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan,\n  Kunal Talwar, Abhradeep Thakurta", "title": "Amplification by Shuffling: From Local to Central Differential Privacy\n  via Anonymity", "comments": "Stated amplification bounds for epsilon > 1 explicitly and also\n  stated the bounds for for Renyi DP. Fixed an incorrect statement in one of\n  the proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensitive statistics are often collected across sets of users, with repeated\ncollection of reports done over time. For example, trends in users' private\npreferences or software usage may be monitored via such reports. We study the\ncollection of such statistics in the local differential privacy (LDP) model,\nand describe an algorithm whose privacy cost is polylogarithmic in the number\nof changes to a user's value.\n  More fundamentally---by building on anonymity of the users' reports---we also\ndemonstrate how the privacy cost of our LDP algorithm can actually be much\nlower when viewed in the central model of differential privacy. We show, via a\nnew and general privacy amplification technique, that any permutation-invariant\nalgorithm satisfying $\\varepsilon$-local differential privacy will satisfy\n$(O(\\varepsilon \\sqrt{\\log(1/\\delta)/n}), \\delta)$-central differential\nprivacy. By this, we explain how the high noise and $\\sqrt{n}$ overhead of LDP\nprotocols is a consequence of them being significantly more private in the\ncentral model. As a practical corollary, our results imply that several\nLDP-based industrial deployments may have much lower privacy cost than their\nadvertised $\\varepsilon$ would indicate---at least if reports are anonymized.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 20:24:45 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 01:37:51 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Erlingsson", "\u00dalfar", ""], ["Feldman", "Vitaly", ""], ["Mironov", "Ilya", ""], ["Raghunathan", "Ananth", ""], ["Talwar", "Kunal", ""], ["Thakurta", "Abhradeep", ""]]}, {"id": "1811.12527", "submitter": "Nicole Wein", "authors": "Bertie Ancona, Monika Henzinger, Liam Roditty, Virginia Vassilevska\n  Williams, Nicole Wein", "title": "Algorithms and Hardness for Diameter in Dynamic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diameter, radius and eccentricities are natural graph parameters. While\nthese problems have been studied extensively, there are no known dynamic\nalgorithms for them beyond the ones that follow from trivial recomputation\nafter each update or from solving dynamic All-Pairs Shortest Paths (APSP),\nwhich is very computationally intensive. This is the situation for dynamic\napproximation algorithms as well, and even if only edge insertions or edge\ndeletions need to be supported.\n  This paper provides a comprehensive study of the dynamic approximation of\nDiameter, Radius and Eccentricities, providing both conditional lower bounds,\nand new algorithms whose bounds are optimal under popular hypotheses in\nfine-grained complexity. Some of the highlights include:\n  - Under popular hardness hypotheses, there can be no significantly better\nfully dynamic approximation algorithms than recomputing the answer after each\nupdate, or maintaining full APSP.\n  - Nearly optimal partially dynamic (incremental/decremental) algorithms can\nbe achieved via efficient reductions to (incremental/decremental) maintenance\nof Single-Source Shortest Paths. For instance, a nearly\n$(3/2+\\epsilon)$-approximation to Diameter in directed or undirected graphs can\nbe maintained decrementally in total time $m^{1+o(1)}\\sqrt{n}/\\epsilon^2$. This\nnearly matches the static $3/2$-approximation algorithm for the problem that is\nknown to be conditionally optimal.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 22:52:05 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 02:53:07 GMT"}, {"version": "v3", "created": "Tue, 17 Dec 2019 18:30:11 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Ancona", "Bertie", ""], ["Henzinger", "Monika", ""], ["Roditty", "Liam", ""], ["Williams", "Virginia Vassilevska", ""], ["Wein", "Nicole", ""]]}, {"id": "1811.12547", "submitter": "Sergio Cabello", "authors": "\\'Edouard Bonnet, Sergio Cabello, Bojan Mohar, Hebert P\\'erez-Ros\\'es", "title": "The inverse Voronoi problem in graphs", "comments": "46 pages, 18 figures; several changes with respect to the previous\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the inverse Voronoi diagram problem in graphs: given a graph $G$\nwith positive edge-lengths and a collection $\\mathbb{U}$ of subsets of vertices\nof $V(G)$, decide whether $\\mathbb{U}$ is a Voronoi diagram in $G$ with respect\nto the shortest-path metric. We show that the problem is NP-hard, even for\nplanar graphs where all the edges have unit length. We also study the\nparameterized complexity of the problem and show that the problem is W[1]-hard\nwhen parameterized by the number of Voronoi cells or by the pathwidth of the\ngraph. For trees we show that the problem can be solved in $O(N+n \\log^2 n)$\ntime, where $n$ is the number of vertices in the tree and $N=n+\\sum_{U\\in\n\\mathbb{U}}|U|$ is the size of the description of the input. We also provide a\nlower bound of $\\Omega(n \\log n)$ time for trees with $n$ vertices.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 00:05:37 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 20:29:01 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Cabello", "Sergio", ""], ["Mohar", "Bojan", ""], ["P\u00e9rez-Ros\u00e9s", "Hebert", ""]]}, {"id": "1811.12568", "submitter": "Kent Quanrud", "authors": "Chandra Chekuri and Kent Quanrud", "title": "Parallelizing greedy for submodular set function maximization in\n  matroids and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parallel, or low adaptivity, algorithms for submodular function\nmaximization. This line of work was recently initiated by Balkanski and Singer\nand has already led to several interesting results on the cardinality\nconstraint and explicit packing constraints. An important open problem is the\nclassical setting of matroid constraint, which has been instrumental for\ndevelopments in submodular function maximization. In this paper we develop a\ngeneral strategy to parallelize the well-studied greedy algorithm and use it to\nobtain a randomized $\\left(\\frac{1}{2} - \\epsilon\\right)$-approximation in\n$\\operatorname{O}\\left( \\frac{\\log^2 n}{\\epsilon^2} \\right)$ rounds of\nadaptivity. We rely on this algorithm, and an elegant amplification approach\ndue to Badanidiyuru and Vondr\\'ak to obtain a fractional solution that yields a\nnear-optimal randomized $\\left( 1 - 1/e - \\epsilon \\right)$-approximation in\n$O\\left( {\\frac{\\log^2 n}{\\epsilon^3}} \\right) $ rounds of adaptivity. For\nnon-negative functions we obtain a $\\left( {3-2\\sqrt{2}}\\right)$-approximation\nand a fractional solution that yields a $\\left( {\\frac{1}{e} -\n\\epsilon}\\right)$-approximation. Our approach for parallelizing greedy yields\napproximations for intersections of matroids and matchoids, and the\napproximation ratios are comparable to those known for sequential greedy.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 01:15:35 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Chekuri", "Chandra", ""], ["Quanrud", "Kent", ""]]}, {"id": "1811.12657", "submitter": "Roman Rischke", "authors": "Lin Chen, Nicole Megow, Roman Rischke, Leen Stougie, Jos\\'e Verschae", "title": "Optimal Algorithms for Scheduling under Time-of-Use Tariffs", "comments": "17 pages; A preliminary version of this paper with a subset of\n  results appeared in the Proceedings of MFCS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a natural generalization of classical scheduling problems in\nwhich using a time unit for processing a job causes some time-dependent cost\nwhich must be paid in addition to the standard scheduling cost. We study the\nscheduling objectives of minimizing the makespan and the sum of (weighted)\ncompletion times. It is not difficult to derive a polynomial-time algorithm for\npreemptive scheduling to minimize the makespan on unrelated machines. The\nproblem of minimizing the total (weighted) completion time is considerably\nharder, even on a single machine. We present a polynomial-time algorithm that\ncomputes for any given sequence of jobs an optimal schedule, i.e., the optimal\nset of time-slots to be used for scheduling jobs according to the given\nsequence. This result is based on dynamic programming using a subtle analysis\nof the structure of optimal solutions and a potential function argument. With\nthis algorithm, we solve the unweighted problem optimally in polynomial time.\nFor the more general problem, in which jobs may have individual weights, we\ndevelop a polynomial-time approximation scheme (PTAS) based on a dual\nscheduling approach introduced for scheduling on a machine of varying speed. As\nthe weighted problem is strongly NP-hard, our PTAS is the best possible\napproximation we can hope for.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 07:57:40 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Chen", "Lin", ""], ["Megow", "Nicole", ""], ["Rischke", "Roman", ""], ["Stougie", "Leen", ""], ["Verschae", "Jos\u00e9", ""]]}, {"id": "1811.12779", "submitter": "Gonzalo Navarro", "authors": "Anders Roy Christiansen and Mikko Berggren Ettienne and Tomasz\n  Kociumaka and Gonzalo Navarro and Nicola Prezza", "title": "Optimal-Time Dictionary-Compressed Indexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the first self-indexes able to count and locate pattern\noccurrences in optimal time within a space bounded by the size of the most\npopular dictionary compressors. To achieve this result we combine several\nrecent findings, including \\emph{string attractors} --- new combinatorial\nobjects encompassing most known compressibility measures for highly repetitive\ntexts ---, and grammars based on \\emph{locally-consistent parsing}.\n  More in detail, let $\\gamma$ be the size of the smallest attractor for a text\n$T$ of length $n$. The measure $\\gamma$ is an (asymptotic) lower bound to the\nsize of dictionary compressors based on Lempel--Ziv, context-free grammars, and\nmany others. The smallest known text representations in terms of attractors use\nspace $O(\\gamma\\log(n/\\gamma))$, and our lightest indexes work within the same\nasymptotic space. Let $\\epsilon>0$ be a suitably small constant fixed at\nconstruction time, $m$ be the pattern length, and $occ$ be the number of its\ntext occurrences. Our index counts pattern occurrences in\n$O(m+\\log^{2+\\epsilon}n)$ time, and locates them in $O(m+(occ+1)\\log^\\epsilon\nn)$ time. These times already outperform those of most dictionary-compressed\nindexes, while obtaining the least asymptotic space for any index searching\nwithin $O((m+occ)\\,\\textrm{polylog}\\,n)$ time. Further, by increasing the space\nto $O(\\gamma\\log(n/\\gamma)\\log^\\epsilon n)$, we reduce the locating time to the\noptimal $O(m+occ)$, and within $O(\\gamma\\log(n/\\gamma)\\log n)$ space we can\nalso count in optimal $O(m)$ time. No dictionary-compressed index had obtained\nthis time before. All our indexes can be constructed in $O(n)$ space and\n$O(n\\log n)$ expected time.\n  As a byproduct of independent interest...\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 13:16:24 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 21:20:10 GMT"}, {"version": "v3", "created": "Thu, 9 May 2019 15:13:57 GMT"}, {"version": "v4", "created": "Fri, 7 Jun 2019 21:16:31 GMT"}, {"version": "v5", "created": "Fri, 30 Aug 2019 22:50:32 GMT"}, {"version": "v6", "created": "Wed, 4 Sep 2019 18:48:37 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Christiansen", "Anders Roy", ""], ["Ettienne", "Mikko Berggren", ""], ["Kociumaka", "Tomasz", ""], ["Navarro", "Gonzalo", ""], ["Prezza", "Nicola", ""]]}, {"id": "1811.12824", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr, Carsten Witt, Jing Yang", "title": "Runtime Analysis for Self-adaptive Mutation Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a self-adaptive version of the $(1,\\lambda)$\nevolutionary algorithm in which the current mutation rate is part of the\nindividual and thus also subject to mutation. A rigorous runtime analysis on\nthe OneMax benchmark function reveals that a simple local mutation scheme for\nthe rate leads to an expected optimization time (number of fitness evaluations)\nof $O(n\\lambda/\\log\\lambda+n\\log n)$ when $\\lambda$ is at least $C \\ln n$ for\nsome constant $C > 0$. For all values of $\\lambda \\ge C \\ln n$, this\nperformance is asymptotically best possible among all $\\lambda$-parallel\nmutation-based unbiased black-box algorithms.\n  Our result shows that self-adaptation in evolutionary computation can find\ncomplex optimal parameter settings on the fly. At the same time, it proves that\na relatively complicated self-adjusting scheme for the mutation rate proposed\nby Doerr, Gie{\\ss}en, Witt, and Yang~(GECCO~2017) can be replaced by our simple\nendogenous scheme.\n  On the technical side, the paper contributes new tools for the analysis of\ntwo-dimensional drift processes arising in the analysis of dynamic parameter\nchoices in EAs, including bounds on occupation probabilities in processes with\nnon-constant drift.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 14:38:05 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Doerr", "Benjamin", ""], ["Witt", "Carsten", ""], ["Yang", "Jing", ""]]}]