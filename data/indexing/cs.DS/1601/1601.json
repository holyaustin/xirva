[{"id": "1601.00163", "submitter": "Mingyu Xiao", "authors": "Mingyu Xiao", "title": "A Parameterized Algorithm for Bounded-Degree Vertex Deletion", "comments": null, "journal-ref": "COCOON 2016, LNCS 9797, 79-91", "doi": "10.1007/978-3-319-42634-1_7", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $d$-bounded-degree vertex deletion problem, to delete at most $k$\nvertices in a given graph to make the maximum degree of the remaining graph at\nmost $d$, finds applications in computational biology, social network analysis\nand some others. It can be regarded as a special case of the $(d+2)$-hitting\nset problem and generates the famous vertex cover problem. The\n$d$-bounded-degree vertex deletion problem is NP-hard for each fixed $d\\geq 0$.\nIn terms of parameterized complexity, the problem parameterized by $k$ is\nW[2]-hard for unbounded $d$ and fixed-parameter tractable for each fixed $d\\geq\n0$. Previously, (randomized) parameterized algorithms for this problem with\nrunning time bound $O^*((d+1)^k)$ are only known for $d\\leq2$. In this paper,\nwe give a uniform parameterized algorithm deterministically solving this\nproblem in $O^*((d+1)^k)$ time for each $d\\geq 3$. Note that it is an open\nproblem whether the $d'$-hitting set problem can be solved in $O^*((d'-1)^k)$\ntime for $d'\\geq 3$. Our result answers this challenging open problem\naffirmatively for a special case. Furthermore, our algorithm also gets a\nrunning time bound of $O^*(3.0645^k)$ for the case that $d=2$, improving the\nprevious deterministic bound of $O^*(3.24^k)$.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 11:09:23 GMT"}, {"version": "v2", "created": "Sat, 20 Aug 2016 09:35:42 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Xiao", "Mingyu", ""]]}, {"id": "1601.00164", "submitter": "Mingyu Xiao", "authors": "Mingyu Xiao", "title": "On a generalization of Nemhauser and Trotter's local optimization\n  theorem", "comments": null, "journal-ref": "Journal of Computer and System Sciences 84: 97-106 (2017)", "doi": "10.1016/j.jcss.2016.08.003", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fellows, Guo, Moser and Niedermeier~[JCSS2011] proved a generalization of\nNemhauser and Trotter's theorem, which applies to \\textsc{Bounded-Degree Vertex\nDeletion} (for a fixed integer $d\\geq 0$, to delete $k$ vertices of the input\ngraph to make the maximum degree of it $\\leq d$) and gets a linear-vertex\nkernel for $d=0$ and $1$, and a superlinear-vertex kernel for each $d\\geq 2$.\nIt is still left as an open problem whether \\textsc{Bounded-Degree Vertex\nDeletion} admits a linear-vertex kernel for each $d\\geq 3$. In this paper, we\nrefine the generalized Nemhauser and Trotter's theorem and get a linear-vertex\nkernel for each $d\\geq 0$.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 11:24:58 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Xiao", "Mingyu", ""]]}, {"id": "1601.00271", "submitter": "Rico Zenklusen", "authors": "David Adjiashvili and Andrea Baggio and Rico Zenklusen", "title": "Firefighting on Trees Beyond Integrality Gaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Firefighter problem and a variant of it, known as Resource Minimization\nfor Fire Containment (RMFC), are natural models for optimal inhibition of\nharmful spreading processes. Despite considerable progress on several fronts,\nthe approximability of these problems is still badly understood. This is the\ncase even when the underlying graph is a tree, which is one of the most-studied\ngraph structures in this context and the focus of this paper. In their simplest\nversion, a fire spreads from one fixed vertex step by step from burning to\nadjacent non-burning vertices, and at each time step, $B$ many non-burning\nvertices can be protected from catching fire. The Firefighter problem asks, for\na given $B$, to maximize the number of vertices that will not catch fire,\nwhereas RMFC (on a tree) asks to find the smallest $B$ that allows for saving\nall leaves of the tree. Prior to this work, the best known approximation ratios\nwere an $O(1)$-approximation for the Firefighter problem and an $O(\\log^*\nn)$-approximation for RMFC, both being LP-based and essentially matching the\nintegrality gaps of two natural LP relaxations.\n  We improve on both approximations by presenting a PTAS for the Firefighter\nproblem and an $O(1)$-approximation for RMFC, both qualitatively matching the\nknown hardness results. Our results are obtained through a combination of the\nknown LPs with several new techniques, which allow for efficiently enumerating\nsubsets of super-constant size of a good solution to obtain stronger LPs.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 10:29:56 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 15:03:39 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Adjiashvili", "David", ""], ["Baggio", "Andrea", ""], ["Zenklusen", "Rico", ""]]}, {"id": "1601.00479", "submitter": "Dennis Kraft", "authors": "Susanne Albers and Dennis Kraft", "title": "Motivating Time-Inconsistent Agents: A Computational Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the computational complexity of motivating\ntime-inconsistent agents to complete long term projects. We resort to an\nelegant graph-theoretic model, introduced by Kleinberg and Oren, which consists\nof a task graph $G$ with $n$ vertices, including a source $s$ and target $t$,\nand an agent that incrementally constructs a path from $s$ to $t$ in order to\ncollect rewards. The twist is that the agent is present-biased and discounts\nfuture costs and rewards by a factor $\\beta\\in [0,1]$. Our design objective is\nto ensure that the agent reaches $t$ i.e.\\ completes the project, for as little\nreward as possible. Such graphs are called motivating. We consider two\nstrategies. First, we place a single reward $r$ at $t$ and try to guide the\nagent by removing edges from $G$. We prove that deciding the existence of such\nmotivating subgraphs is NP-complete if $r$ is fixed. More importantly, we\ngeneralize our reduction to a hardness of approximation result for computing\nthe minimum $r$ that admits a motivating subgraph. In particular, we show that\nno polynomial-time approximation to within a ratio of $\\sqrt{n}/4$ or less is\npossible, unless ${\\rm P}={\\rm NP}$. Furthermore, we develop a\n$(1+\\sqrt{n})$-approximation algorithm and thus settle the approximability of\ncomputing motivating subgraphs. Secondly, we study motivating reward\nconfigurations, where non-negative rewards $r(v)$ may be placed on arbitrary\nvertices $v$ of $G$. The agent only receives the rewards of visited vertices.\nAgain we give an NP-completeness result for deciding the existence of a\nmotivating reward configuration within a fixed budget $b$. This result even\nholds if $b=0$, which in turn implies that no efficient approximation of a\nminimum $b$ within a ration grater or equal to $1$ is possible, unless ${\\rm\nP}={\\rm NP}$.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 12:27:54 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Albers", "Susanne", ""], ["Kraft", "Dennis", ""]]}, {"id": "1601.00839", "submitter": "Christian Wulff-Nilsen", "authors": "Christian Wulff-Nilsen", "title": "Approximate Distance Oracles for Planar Graphs with Improved Query\n  Time-Space Tradeoff", "comments": "20 pages, 9 figures of which 2 illustrate pseudo-code. This is the\n  SODA 2016 version but with the definition of C_i in Phase I fixed and the\n  analysis slightly modified accordingly. The main change is in the subsection\n  bounding query time and stretch for Phase I", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider approximate distance oracles for edge-weighted n-vertex\nundirected planar graphs. Given fixed epsilon > 0, we present a\n(1+epsilon)-approximate distance oracle with O(n(loglog n)^2) space and\nO((loglog n)^3) query time. This improves the previous best product of query\ntime and space of the oracles of Thorup (FOCS 2001, J. ACM 2004) and Klein\n(SODA 2002) from O(n log n) to O(n(loglog n)^5).\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 14:30:48 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Wulff-Nilsen", "Christian", ""]]}, {"id": "1601.00847", "submitter": "David Breuer", "authors": "David Breuer, Zoran Nikoloski", "title": "DeFiNe: an optimisation-based method for robust disentangling of\n  filamentous networks", "comments": null, "journal-ref": "Sci Rep, 2015, 5:18267", "doi": "10.1038/srep18267", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thread-like structures are pervasive across scales, from polymeric proteins\nto root systems to galaxy filaments, and their characteristics can be readily\ninvestigated in the network formalism. Yet, network links usually represent\nonly parts of filaments, which, when neglected, may lead to erroneous\nconclusions from network-based analyses. The existing alternatives to detect\nfilaments in network representations require tuning of parameters over a large\nrange of values and treat all filaments equally, thus, precluding automated\nanalysis of diverse filamentous systems. Here, we propose a fully automated and\nrobust optimisation-based approach to detect filaments of consistent\nintensities and angles in a given network. We test and demonstrate the accuracy\nof our solution with contrived, biological, and cosmic filamentous structures.\nIn particular, we show that the proposed approach provides powerful automated\nmeans to study properties of individual actin filaments in their network\ncontext. Our solution is made publicly available as an open-source tool,\nDeFiNe, facilitating decomposition of any given network into individual\nfilaments.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 10:23:08 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Breuer", "David", ""], ["Nikoloski", "Zoran", ""]]}, {"id": "1601.01336", "submitter": "Matthew  Johnson", "authors": "Foad Lotfifar and Matthew Johnson", "title": "A Serial Multilevel Hypergraph Partitioning Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph partitioning problem has many applications in scientific computing\nsuch as computer aided design, data mining, image compression and other\napplications with sparse-matrix vector multiplications as a kernel operation.\nIn many cases it is advantageous to use hypergraphs as they, compared to\ngraphs, have a more general structure and can be used to model more complex\nrelationships between groups of objects. This motivates our focus on the\nless-studied hypergraph partitioning problem.\n  In this paper, we propose a serial multi-level bipartitioning algorithm. One\nimportant step in current heuristics for hypergraph partitioning is clustering\nduring which similar vertices must be recognized. This can be particularly\ndifficult in irregular hypergraphs with high variation of vertex degree and\nhyperedge size; heuristics that rely on local vertex clustering decisions often\ngive poor partitioning quality. A novel feature of the proposed algorithm is to\nuse the techniques of rough set clustering to address this problem. We show\nthat our proposed algorithm gives on average between 18.8 per cent and 71.1 per\ncent better quality on these irregular hypergraphs by comparing it to\nstate-of-the-art hypergraph partitioning algorithms on benchmarks taken from\nreal applications.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 21:58:17 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Lotfifar", "Foad", ""], ["Johnson", "Matthew", ""]]}, {"id": "1601.01372", "submitter": "Anastasios Sidiropoulos", "authors": "Daniel Marx, Ario Salmasi, Anastasios Sidiropoulos", "title": "Constant-factor approximations for asymmetric TSP on nearly-embeddable\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Asymmetric Traveling Salesperson Problem (ATSP) the goal is to find a\nclosed walk of minimum cost in a directed graph visiting every vertex. We\nconsider the approximability of ATSP on topologically restricted graphs. It has\nbeen shown by [Oveis Gharan and Saberi 2011] that there exists polynomial-time\nconstant-factor approximations on planar graphs and more generally graphs of\nconstant orientable genus. This result was extended to non-orientable genus by\n[Erickson and Sidiropoulos 2014].\n  We show that for any class of \\emph{nearly-embeddable} graphs, ATSP admits a\npolynomial-time constant-factor approximation. More precisely, we show that for\nany fixed $k\\geq 0$, there exist $\\alpha, \\beta>0$, such that ATSP on\n$n$-vertex $k$-nearly-embeddable graphs admits a $\\alpha$-approximation in time\n$O(n^\\beta)$. The class of $k$-nearly-embeddable graphs contains graphs with at\nmost $k$ apices, $k$ vortices of width at most $k$, and an underlying surface\nof either orientable or non-orientable genus at most $k$. Prior to our work,\neven the case of graphs with a single apex was open. Our algorithm combines\ntools from rounding the Held-Karp LP via thin trees with dynamic programming.\n  We complement our upper bounds by showing that solving ATSP exactly on graphs\nof pathwidth $k$ (and hence on $k$-nearly embeddable graphs) requires time\n$n^{\\Omega(k)}$, assuming the Exponential-Time Hypothesis (ETH). This is\nsurprising in light of the fact that both TSP on undirected graphs and Minimum\nCost Hamiltonian Cycle on directed graphs are FPT parameterized by treewidth.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 02:09:22 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Marx", "Daniel", ""], ["Salmasi", "Ario", ""], ["Sidiropoulos", "Anastasios", ""]]}, {"id": "1601.01396", "submitter": "arXiv Admin", "authors": "George Tsatsanifos", "title": "On the Computation of the Optimal Connecting Points in Road Networks", "comments": "This submission has been withdrawn by arXiv administrators due to\n  disputed authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a set of travelers, starting from likely different\nlocations towards a common destination within a road network, and propose\nsolutions to find the optimal connecting points for them. A connecting point is\na vertex of the network where a subset of the travelers meet and continue\ntraveling together towards the next connecting point or the destination. The\nnotion of optimality is with regard to a given aggregated travel cost, e.g.,\ntravel distance or shared fuel cost. This problem by itself is new and we make\nit even more interesting (and complex) by considering affinity factors among\nthe users, i.e., how much a user likes to travel together with another one.\nThis plays a fundamental role in determining where the connecting points are\nand how subsets of travelers are formed. We propose three methods for\naddressing this problem, one that relies on a fast and greedy approach that\nfinds a sub-optimal solution, and two others that yield globally optimal\nsolution. We evaluate all proposed approaches through experiments, where\ncollections of real datasets are used to assess the trade-offs, behavior and\ncharacteristics of each method.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 04:25:27 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 12:15:03 GMT"}, {"version": "v3", "created": "Fri, 6 May 2016 17:30:25 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Tsatsanifos", "George", ""]]}, {"id": "1601.01465", "submitter": "Bing Yao", "authors": "Bing Yao, Xia Liu, Jin Xu", "title": "Maximum Leaf Spanning Trees of Growing Sierpinski Networks Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamical phenomena of complex networks are very difficult to predict\nfrom local information due to the rich microstructures and corresponding\ncomplex dynamics. On the other hands, it is a horrible job to compute some\nstochastic parameters of a large network having thousand and thousand nodes. We\ndesign several recursive algorithms for finding spanning trees having maximal\nleaves (MLS-trees) in investigation of topological structures of Sierpinski\ngrowing network models, and use MLS-trees to determine the kernels, dominating\nand balanced sets of the models. We propose a new stochastic method for the\nmodels, called the edge-cumulative distribution, and show that it obeys a power\nlaw distribution.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 10:20:06 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Yao", "Bing", ""], ["Liu", "Xia", ""], ["Xu", "Jin", ""]]}, {"id": "1601.01478", "submitter": "Anton Wijs", "authors": "Jan Friso Groote and Anton Wijs", "title": "An O(m log n) Algorithm for Stuttering Equivalence and Branching\n  Bisimulation", "comments": "A shortened version of this technical report has been published in\n  the proceedings of TACAS 2016", "journal-ref": null, "doi": null, "report-no": "CSR-15-06", "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new algorithm to determine stuttering equivalence with time\ncomplexity $O(m \\log n)$, where $n$ is the number of states and $m$ is the\nnumber of transitions of a Kripke structure. This algorithm can also be used to\ndetermine branching bisimulation in $O(m(\\log |\\mathit{Act}|+ \\log n))$ time\nwhere $\\mathit{Act}$ is the set of actions in a labelled transition system.\nTheoretically, our algorithm substantially improves upon existing algorithms\nwhich all have time complexity $O(m n)$ at best. Moreover, it has better or\nequal space complexity. Practical results confirm these findings showing that\nour algorithm can outperform existing algorithms with orders of magnitude,\nespecially when the sizes of the Kripke structures are large. The importance of\nour algorithm stretches far beyond stuttering equivalence and branching\nbisimulation. The known $O(m n)$ algorithms were already far more efficient\n(both in space and time) than most other algorithms to determine behavioural\nequivalences (including weak bisimulation) and therefore it was often used as\nan essential preprocessing step. This new algorithm makes this use of\nstuttering equivalence and branching bisimulation even more attractive.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 10:58:36 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Groote", "Jan Friso", ""], ["Wijs", "Anton", ""]]}, {"id": "1601.01549", "submitter": "Tenindra Abeywickrama", "authors": "Tenindra Abeywickrama, Muhammad Aamir Cheema, David Taniar", "title": "k-Nearest Neighbors on Road Networks: A Journey in Experimentation and\n  In-Memory Implementation", "comments": "Added reference to conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A k nearest neighbor (kNN) query on road networks retrieves the k closest\npoints of interest (POIs) by their network distances from a given location.\nToday, in the era of ubiquitous mobile computing, this is a highly pertinent\nquery. While Euclidean distance has been used as a heuristic to search for the\nclosest POIs by their road network distance, its efficacy has not been\nthoroughly investigated. The most recent methods have shown significant\nimprovement in query performance. Earlier studies, which proposed disk-based\nindexes, were compared to the current state-of-the-art in main memory. However,\nrecent studies have shown that main memory comparisons can be challenging and\nrequire careful adaptation. This paper presents an extensive experimental\ninvestigation in main memory to settle these and several other issues. We use\nefficient and fair memory-resident implementations of each method to reproduce\npast experiments and conduct additional comparisons for several overlooked\nevaluations. Notably we revisit a previously discarded technique (IER) showing\nthat, through a simple improvement, it is often the best performing technique.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 14:51:07 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 13:58:37 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Abeywickrama", "Tenindra", ""], ["Cheema", "Muhammad Aamir", ""], ["Taniar", "David", ""]]}, {"id": "1601.01597", "submitter": "Jonathan S Turner", "authors": "Jonathan Turner", "title": "Grafalgo - A Library of Graph Algorithms and Supporting Data Structures\n  (revised)", "comments": null, "journal-ref": null, "doi": null, "report-no": "WUCSE-2016-01", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report provides an (updated) overview of {\\sl Grafalgo}, an open-source\nlibrary of graph algorithms and the data structures used to implement them. The\nprograms in this library were originally written to support a graduate class in\nadvanced data structures and algorithms at Washington University. Because the\ncode's primary purpose was pedagogical, it was written to be as straightforward\nas possible, while still being highly efficient. Grafalgo is implemented in C++\nand incorporates some features of C++11.\n  The library is available on an open-source basis and may be downloaded from\nhttps://code.google.com/p/grafalgo/. Source code documentation is at\nwww.arl.wustl.edu/\\textasciitilde jst/doc/grafalgo. While not designed as\nproduction code, the library is suitable for use in larger systems, so long as\nits limitations are understood. The readability of the code also makes it\nrelatively straightforward to extend it for other purposes.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 16:57:17 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Turner", "Jonathan", ""]]}, {"id": "1601.01736", "submitter": "Elod Pal Csirmaz", "authors": "Elod Pal Csirmaz", "title": "Algebraic File Synchronization: Adequacy and Completeness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With distributed computing and mobile applications, synchronizing diverging\nreplicas of data structures is a more and more common problem. We use algebraic\nmethods to reason about filesystem operations, and introduce a simplified\ndefinition of conflicting updates to filesystems. We also define algorithms for\nupdate detection and reconciliation and present rigorous proofs that they not\nonly work as intended, but also cannot be improved on.\n  To achieve this, we introduce a novel, symmetric set of filesystem commands\nwith higher information content, which removes edge cases and increases the\npredictive powers of our algebraic model. We also present a number of generally\nuseful classes and properties of sequences of commands.\n  While these results are often intuitive, providing exact proofs for them is\nfar from trivial. They contribute to our understanding of this special type of\nalgebraic model, and toward building more complete algebras of filesystem trees\nand extending algebraic approaches to other data storage protocols. They also\nform a theoretical basis for specifying and guaranteeing the error-free\noperation of applications that implement an algebraic approach to\nsynchronization.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 01:01:55 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 23:44:04 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Csirmaz", "Elod Pal", ""]]}, {"id": "1601.01744", "submitter": "Yechao Zhu", "authors": "Cedric Yen-Yu Lin, Yechao Zhu", "title": "Performance of QAOA on Typical Instances of Constraint Satisfaction\n  Problems with Bounded Degree", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": "MIT-CTP-4751", "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider constraint satisfaction problems of bounded degree, with a good\nnotion of \"typicality\", e.g. the negation of the variables in each constraint\nis taken independently at random. Using the quantum approximate optimization\nalgorithm (QAOA), we show that $ \\mu+\\Omega(1/\\sqrt{D}) $ fraction of the\nconstraints can be satisfied for typical instances, with the assignment\nefficiently produced by QAOA. We do so by showing that the averaged fraction of\nconstraints being satisfied is $ \\mu+\\Omega(1/\\sqrt{D}) $, with small variance.\nHere $ \\mu $ is the fraction that would be satisfied by a uniformly random\nassignment, and $ D $ is the number of constraints that each variable can\nappear. CSPs with typicality include Max-$ k $XOR and Max-$ k $SAT. We point\nout how it can be applied to determine the typical ground-state energy of some\nlocal Hamiltonians. We also give a similar result for instances with \"no\noverlapping constraints\", using the quantum algorithm. We sketch how the\nclassical algorithm might achieve some partial result.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 01:44:13 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Lin", "Cedric Yen-Yu", ""], ["Zhu", "Yechao", ""]]}, {"id": "1601.01746", "submitter": "Yiyang Zhou", "authors": "Shoujian Yu, Yiyang Zhou", "title": "A Prefixed-Itemset-Based Improvement For Apriori Algorithm", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Association rules is a very important part of data mining. It is used to find\nthe interesting patterns from transaction databases. Apriori algorithm is one\nof the most classical algorithms of association rules, but it has the\nbottleneck in efficiency. In this article, we proposed a prefixed-itemset-based\ndata structure for candidate itemset generation, with the help of the structure\nwe managed to improve the efficiency of the classical Apriori algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 02:06:05 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Yu", "Shoujian", ""], ["Zhou", "Yiyang", ""]]}, {"id": "1601.01958", "submitter": "Guillaume Ducoffe", "authors": "Guillaume Ducoffe and Sylvain Legay and Nicolas Nisse", "title": "On computing tree and path decompositions with metric constraints on the\n  bags", "comments": "50 pages, 39 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We here investigate on the complexity of computing the \\emph{tree-length} and\nthe \\emph{tree-breadth} of any graph $G$, that are respectively the best\npossible upper-bounds on the diameter and the radius of the bags in a tree\ndecomposition of $G$. \\emph{Path-length} and \\emph{path-breadth} are similarly\ndefined and studied for path decompositions. So far, it was already known that\ntree-length is NP-hard to compute. We here prove it is also the case for\ntree-breadth, path-length and path-breadth. Furthermore, we provide a more\ndetailed analysis on the complexity of computing the tree-breadth. In\nparticular, we show that graphs with tree-breadth one are in some sense the\nhardest instances for the problem of computing the tree-breadth. We give new\nproperties of graphs with tree-breadth one. Then we use these properties in\norder to recognize in polynomial-time all graphs with tree-breadth one that are\nplanar or bipartite graphs. On the way, we relate tree-breadth with the notion\nof \\emph{$k$-good} tree decompositions (for $k=1$), that have been introduced\nin former work for routing. As a byproduct of the above relation, we prove that\ndeciding on the existence of a $k$-good tree decomposition is NP-complete (even\nif $k=1$). All this answers open questions from the literature.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 17:57:40 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Ducoffe", "Guillaume", ""], ["Legay", "Sylvain", ""], ["Nisse", "Nicolas", ""]]}, {"id": "1601.02034", "submitter": "Ayush Jain", "authors": "Ayush Jain, Joon Young Seo, Karan Goel, Andrew Kuznetsov, Aditya\n  Parameswaran, Hari Sundaram", "title": "It's just a matter of perspective(s): Crowd-Powered Consensus\n  Organization of Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of organizing a collection of objects - images, videos -\ninto clusters, using crowdsourcing. This problem is notoriously hard for\ncomputers to do automatically, and even with crowd workers, is challenging to\norchestrate: (a) workers may cluster based on different latent hierarchies or\nperspectives; (b) workers may cluster at different granularities even when\nclustering using the same perspective; and (c) workers may only see a small\nportion of the objects when deciding how to cluster them (and therefore have\nlimited understanding of the \"big picture\"). We develop cost-efficient,\naccurate algorithms for identifying the consensus organization (i.e., the\norganizing perspective most workers prefer to employ), and incorporate these\nalgorithms into a cost-effective workflow for organizing a collection of\nobjects, termed ORCHESTRA. We compare our algorithms with other algorithms for\nclustering, on a variety of real-world datasets, and demonstrate that ORCHESTRA\norganizes items better and at significantly lower costs.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 21:31:56 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Jain", "Ayush", ""], ["Seo", "Joon Young", ""], ["Goel", "Karan", ""], ["Kuznetsov", "Andrew", ""], ["Parameswaran", "Aditya", ""], ["Sundaram", "Hari", ""]]}, {"id": "1601.02225", "submitter": "Hamid Mansouri", "authors": "Hamid Mansouri (Machine Vision Lab., Computer Engineering Department,\n  Ferdowsi University of Mashhad, Mashhad, Iran) and Hamid-Reza Pourreza\n  (Machine Vision Lab., Computer Engineering Department, Ferdowsi University of\n  Mashhad, Mashhad, Iran)", "title": "Parallel Stroked Multi Line: a model-based method for compressing large\n  fingerprint databases", "comments": "26 pages, 10 figures, submitted to Computer Vision and Image\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing usage of fingerprints as an important biometric data, the\nneed to compress the large fingerprint databases has become essential. The most\nrecommended compression algorithm, even by standards, is JPEG2K. But at high\ncompression rates, this algorithm is ineffective. In this paper, a model is\nproposed which is based on parallel lines with same orientations, arbitrary\nwidths and same gray level values located on rectangle with constant gray level\nvalue as background. We refer to this algorithm as Parallel Stroked Multi Line\n(PSML). By using Adaptive Geometrical Wavelet and employing PSML, a compression\nalgorithm is developed. This compression algorithm can preserve fingerprint\nstructure and minutiae. The exact algorithm of computing the PSML model take\nexponential time. However, we have proposed an alternative approximation\nalgorithm, which reduces the time complexity to $O(n^3)$. The proposed PSML\nalg. has significant advantage over Wedgelets Transform in PSNR value and\nvisual quality in compressed images. The proposed method, despite the lower\nPSNR values than JPEG2K algorithm in common range of compression rates, in all\ncompression rates have nearly equal or greater advantage over JPEG2K when used\nby Automatic Fingerprint Identification Systems (AFIS). At high compression\nrates, according to PSNR values, mean EER rate and visual quality, the encoded\nimages with JPEG2K can not be identified from each other after compression.\nBut, images encoded by the PSML alg. retained the sufficient information to\nmaintain fingerprint identification performances similar to the ones obtained\nby raw images without compression. One the U.are.U 400 database, the mean EER\nrate for uncompressed images is 4.54%, while at 267:1 compression ratio, this\nvalue becomes 49.41% and 6.22% for JPEG2K and PSML, respectively. This result\nshows a significant improvement over the standard JPEG2K algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2016 15:01:10 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Mansouri", "Hamid", "", "Machine Vision Lab., Computer Engineering Department,\n  Ferdowsi University of Mashhad, Mashhad, Iran"], ["Pourreza", "Hamid-Reza", "", "Machine Vision Lab., Computer Engineering Department, Ferdowsi University of\n  Mashhad, Mashhad, Iran"]]}, {"id": "1601.02323", "submitter": "Chi-Kin Chau", "authors": "Majid Khonji, Chi-Kin Chau, Khaled Elbassioni", "title": "Optimal Power Flow with Inelastic Demands for Demand Response in Radial\n  Distribution Networks", "comments": "Extended version of the journal paper appears in IEEE Transactions on\n  Control of Network Systems", "journal-ref": "IEEE Transactions on Control of Network Systems (pp513 - 524,\n  Volume: 5, Issue: 1, March 2018)", "doi": "10.1109/TCNS.2016.2622362", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical optimal power flow problem optimizes the power flow in a power\nnetwork considering the associated flow and operating constraints. In this\npaper, we investigate optimal power flow in the context of utility-maximizing\ndemand response management in distribution networks, in which customers'\ndemands are satisfied subject to the operating constraints of voltage and\ntransmission power capacity. The prior results concern only elastic demands\nthat can be partially satisfied, whereas power demands in practice can be\ninelastic with binary control decisions, which gives rise to a mixed integer\nprogramming problem. We shed light on the hardness and approximability by\npolynomial-time algorithms for optimal power flow problem with inelastic\ndemands. We show that this problem is inapproximable for general power network\ntopology with upper and lower bounds of nodal voltage. Then, we propose an\nefficient algorithm for a relaxed problem in radial networks with bounded\ntransmission power loss and upper bound of nodal voltage. We derive an\napproximation ratio between the proposed algorithm and the exact optimal\nsolution. Simulations show that the proposed algorithm can produce\nclose-to-optimal solutions in practice.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 05:10:31 GMT"}, {"version": "v2", "created": "Sun, 31 Jul 2016 07:20:30 GMT"}, {"version": "v3", "created": "Thu, 3 Nov 2016 13:10:44 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Khonji", "Majid", ""], ["Chau", "Chi-Kin", ""], ["Elbassioni", "Khaled", ""]]}, {"id": "1601.02388", "submitter": "Daniel Vaz", "authors": "Parinya Chalermsook and Daniel Vaz", "title": "New Integrality Gap Results for the Firefighters Problem on Trees", "comments": "22 pages", "journal-ref": null, "doi": "10.1007/978-3-319-51741-4_6", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The firefighter problem is NP-hard and admits a $(1-1/e)$ approximation based\non rounding the canonical LP. In this paper, we first show a matching\nintegrality gap of $(1-1/e+\\epsilon)$ on the canonical LP. This result relies\non a powerful combinatorial gadget that can be used to prove integrality gap\nresults for many problem settings. We also consider the canonical LP augmented\nwith simple additional constraints (as suggested by Hartke). We provide several\nevidences that these constraints improve the integrality gap of the canonical\nLP: (i) Extreme points of the new LP are integral for some known tractable\ninstances and (ii) A natural family of instances that are bad for the canonical\nLP admits an improved approximation algorithm via the new LP. We conclude by\npresenting a $5/6$ integrality gap instance for the new LP.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 10:39:32 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2016 10:56:51 GMT"}, {"version": "v3", "created": "Fri, 29 Jul 2016 15:22:24 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Chalermsook", "Parinya", ""], ["Vaz", "Daniel", ""]]}, {"id": "1601.02415", "submitter": "Hans Bodlaender", "authors": "Hans L. Bodlaender and Jesper Nederlof", "title": "Subexponential time algorithms for finding small tree and path\n  decompositions", "comments": "Extended abstract appeared in proceedings of ESA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Size Tree Decomposition (MSTD) and Minimum Size Path\nDecomposition (MSPD) problems ask for a given n-vertex graph G and integer k,\nwhat is the minimum number of bags of a tree decomposition (respectively, path\ndecomposition) of G of width at most k. The problems are known to be\nNP-complete for each fixed $k\\geq 4$. We present algorithms that solve both\nproblems for fixed k in $2^{O(n/ \\log n)}$ time and show that they cannot be\nsolved in $2^{o(n / \\log n)}$ time, assuming the Exponential Time Hypothesis.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 12:15:45 GMT"}, {"version": "v2", "created": "Wed, 4 May 2016 14:29:15 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Bodlaender", "Hans L.", ""], ["Nederlof", "Jesper", ""]]}, {"id": "1601.02481", "submitter": "Mateusz Lewandowski", "authors": "Jaros{\\l}aw Byrka, Mateusz Lewandowski, Carsten Moldenhauer", "title": "Approximation algorithms for node-weighted prize-collecting Steiner tree\n  problems on planar graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the prize-collecting version of the Node-weighted Steiner Tree\nproblem (NWPCST) restricted to planar graphs. We give a new primal-dual\nLagrangian-multiplier-preserving (LMP) 3-approximation algorithm for planar\nNWPCST. We then show a ($2.88 + \\epsilon$)-approximation which establishes a\nnew best approximation guarantee for planar NWPCST. This is done by combining\nour LMP algorithm with a threshold rounding technique and utilizing the\n2.4-approximation of Berman and Yaroslavtsev for the version without penalties.\nWe also give a primal-dual 4-approximation algorithm for the more general\nforest version using techniques introduced by Hajiaghay and Jain.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 15:39:41 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Byrka", "Jaros\u0142aw", ""], ["Lewandowski", "Mateusz", ""], ["Moldenhauer", "Carsten", ""]]}, {"id": "1601.02522", "submitter": "Nathanael Perraudin N. P.", "authors": "Nathana\\\"el Perraudin, Pierre Vandergheynst", "title": "Stationary signal processing on graphs", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2690388", "report-no": null, "categories": "cs.DS stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are a central tool in machine learning and information processing as\nthey allow to conveniently capture the structure of complex datasets. In this\ncontext, it is of high importance to develop flexible models of signals defined\nover graphs or networks. In this paper, we generalize the traditional concept\nof wide sense stationarity to signals defined over the vertices of arbitrary\nweighted undirected graphs. We show that stationarity is expressed through the\ngraph localization operator reminiscent of translation. We prove that\nstationary graph signals are characterized by a well-defined Power Spectral\nDensity that can be efficiently estimated even for large graphs. We leverage\nthis new concept to derive Wiener-type estimation procedures of noisy and\npartially observed signals and illustrate the performance of this new model for\ndenoising and regression.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 16:58:45 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 16:42:30 GMT"}, {"version": "v3", "created": "Thu, 21 Apr 2016 16:34:34 GMT"}, {"version": "v4", "created": "Fri, 8 Jul 2016 21:25:26 GMT"}, {"version": "v5", "created": "Fri, 21 Apr 2017 18:30:15 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Perraudin", "Nathana\u00ebl", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1601.02603", "submitter": "Marian-Andrei Rizoiu", "authors": "Marian-Andrei Rizoiu, Julien Velcin, St\\'ephane Lallich", "title": "How to Use Temporal-Driven Constrained Clustering to Detect Typical\n  Evolutions", "comments": null, "journal-ref": "Int. J. Artif. Intell. Tools 23, 1460013 (2014) [26 pages]", "doi": "10.1142/S0218213014600136", "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new time-aware dissimilarity measure that takes\ninto account the temporal dimension. Observations that are close in the\ndescription space, but distant in time are considered as dissimilar. We also\npropose a method to enforce the segmentation contiguity, by introducing, in the\nobjective function, a penalty term inspired from the Normal Distribution\nFunction. We combine the two propositions into a novel time-driven constrained\nclustering algorithm, called TDCK-Means, which creates a partition of coherent\nclusters, both in the multidimensional space and in the temporal space. This\nalgorithm uses soft semi-supervised constraints, to encourage adjacent\nobservations belonging to the same entity to be assigned to the same cluster.\nWe apply our algorithm to a Political Studies dataset in order to detect\ntypical evolution phases. We adapt the Shannon entropy in order to measure the\nentity contiguity, and we show that our proposition consistently improves\ntemporal cohesion of clusters, without any significant loss in the\nmultidimensional variance.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 01:20:26 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Rizoiu", "Marian-Andrei", ""], ["Velcin", "Julien", ""], ["Lallich", "St\u00e9phane", ""]]}, {"id": "1601.02712", "submitter": "Damian Straszak", "authors": "Damian Straszak and Nisheeth K. Vishnoi", "title": "IRLS and Slime Mold: Equivalence and Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.ET cs.IT math.IT math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a connection between two dynamical systems arising\nin entirely different contexts: one in signal processing and the other in\nbiology. The first is the famous Iteratively Reweighted Least Squares (IRLS)\nalgorithm used in compressed sensing and sparse recovery while the second is\nthe dynamics of a slime mold (Physarum polycephalum). Both of these dynamics\nare geared towards finding a minimum l1-norm solution in an affine subspace.\nDespite its simplicity the convergence of the IRLS method has been shown only\nfor a certain regularization of it and remains an important open problem. Our\nfirst result shows that the two dynamics are projections of the same dynamical\nsystem in higher dimensions. As a consequence, and building on the recent work\non Physarum dynamics, we are able to prove convergence and obtain complexity\nbounds for a damped version of the IRLS algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 02:24:18 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Straszak", "Damian", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1601.02867", "submitter": "Denis Kurz", "authors": "Denis Kurz, Petra Mutzel", "title": "A Sidetrack-Based Algorithm for Finding the k Shortest Simple Paths in a\n  Directed Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for the k shortest simple path problem on weighted\ndirected graphs (kSSP) that is based on Eppstein's algorithm for a similar\nproblem in which paths are allowed to contain cycles. In contrast to most other\nalgorithms for kSSP, ours is not based on Yen's algorithm and does not solve\nreplacement path problems. Its worst-case running time is on par with\nstate-of-the-art algorithms for kSSP. Using our algorithm, one may find O(m)\nsimple paths with a single shortest path tree computation and O(n + m)\nadditional time per path in well-behaved cases, where n is the number of nodes\nand m is the number of edges. Our computational results show that on random\ngraphs and large road networks, these well-behaved cases are quite common and\nour algorithm is faster than existing algorithms by an order of magnitude.\nFurther, the running time is far better predictable due to very small\ndispersion.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 14:10:26 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Kurz", "Denis", ""], ["Mutzel", "Petra", ""]]}, {"id": "1601.02932", "submitter": "Paul Medvedev", "authors": "Alexandru I. Tomescu and Paul Medvedev", "title": "Safe and complete contig assembly via omnitigs", "comments": "Full version of the paper in the proceedings of RECOMB 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.DM cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contig assembly is the first stage that most assemblers solve when\nreconstructing a genome from a set of reads. Its output consists of contigs --\na set of strings that are promised to appear in any genome that could have\ngenerated the reads. From the introduction of contigs 20 years ago, assemblers\nhave tried to obtain longer and longer contigs, but the following question was\nnever solved: given a genome graph $G$ (e.g. a de Bruijn, or a string graph),\nwhat are all the strings that can be safely reported from $G$ as contigs? In\nthis paper we finally answer this question, and also give a polynomial time\nalgorithm to find them. Our experiments show that these strings, which we call\nomnitigs, are 66% to 82% longer on average than the popular unitigs, and 29% of\ndbSNP locations have more neighbors in omnitigs than in unitigs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 16:02:04 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 20:41:42 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Tomescu", "Alexandru I.", ""], ["Medvedev", "Paul", ""]]}, {"id": "1601.02939", "submitter": "Andrew Gainer-Dewar", "authors": "Andrew Gainer-Dewar and Paola Vera-Licona", "title": "The minimal hitting set generation problem: algorithms and computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding inclusion-minimal \"hitting sets\" for a given collection of sets is a\nfundamental combinatorial problem with applications in domains as diverse as\nBoolean algebra, computational biology, and data mining. Much of the\nalgorithmic literature focuses on the problem of *recognizing* the collection\nof minimal hitting sets; however, in many of the applications, it is more\nimportant to *generate* these hitting sets. We survey twenty algorithms from\nacross a variety of domains, considering their history, classification, useful\nfeatures, and computational performance on a variety of synthetic and\nreal-world inputs. We also provide a suite of implementations of these\nalgorithms with a ready-to-use, platform-agnostic interface based on Docker\ncontainers and the AlgoRun framework, so that interested computational\nscientists can easily perform similar tests with inputs from their own research\nareas on their own computers or through a convenient Web interface.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 19:24:25 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Gainer-Dewar", "Andrew", ""], ["Vera-Licona", "Paola", ""]]}, {"id": "1601.03030", "submitter": "Elizabeth Crosson", "authors": "Elizabeth Crosson, Aram W. Harrow", "title": "Simulated Quantum Annealing Can Be Exponentially Faster than Classical\n  Simulated Annealing", "comments": "21 pages, revised analysis includes worldline updates and spikes with\n  polynomial width", "journal-ref": "Proc of FOCS 2016, pp. 714-723", "doi": "10.1109/FOCS.2016.81", "report-no": "MIT-CTP/4760", "categories": "quant-ph cond-mat.stat-mech cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulated Quantum Annealing (SQA) is a Markov Chain Monte-Carlo algorithm\nthat samples the equilibrium thermal state of a Quantum Annealing (QA)\nHamiltonian. In addition to simulating quantum systems, SQA has also been\nproposed as another physics-inspired classical algorithm for combinatorial\noptimization, alongside classical simulated annealing. However, in many cases\nit remains an open challenge to determine the performance of both QA and SQA.\nOne piece of evidence for the strength of QA over classical simulated annealing\ncomes from an example by Farhi, Goldstone and Gutmann . There a bit-symmetric\ncost function with a thin, high energy barrier was designed to show an\nexponential seperation between classical simulated annealing, for which thermal\nfluctuations take exponential time to climb the barrier, and quantum annealing\nwhich passes through the barrier and reaches the global minimum in poly time,\narguably by taking advantage of quantum tunneling. In this work we apply a\ncomparison method to rigorously show that the Markov chain underlying SQA\nefficiently samples the target distribution and finds the global minimum of\nthis spike cost function in polynomial time. Our work provides evidence for the\ngrowing consensus that SQA inherits at least some of the advantages of\ntunneling in QA, and so QA is unlikely to achieve exponential speedups over\nclassical computing solely by the use of quantum tunneling. Since we analyze\nonly a particular model this evidence is not decisive. However, techniques\napplied here---including warm starts from the adiabatic path and the use of the\nquantum ground state probability distribution to understand the stationary\ndistribution of SQA---may be valuable for future studies of the performance of\nSQA on cost functions for which QA is efficient.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 20:48:20 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 18:31:17 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Crosson", "Elizabeth", ""], ["Harrow", "Aram W.", ""]]}, {"id": "1601.03038", "submitter": "Thi Ha Duong Phan", "authors": "Phan Thi Ha Duong", "title": "Linear time algorithm for computing the rank of divisors on cactus\n  graphs", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank of divisor on graph was introduced in 2007 and it quickly attracts many\nattentions. Recently, in 2015 the problem for computing this quantity was\nproved to be NP-hard. In this paper, we describe a linear time algorithm for\nthis problem limited on cactus graphs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 10:54:17 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Duong", "Phan Thi Ha", ""]]}, {"id": "1601.03095", "submitter": "Yaron Singer", "authors": "Avinatan Hassidim and Yaron Singer", "title": "Submodular Optimization under Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing a monotone submodular function under\nnoise. There has been a great deal of work on optimization of submodular\nfunctions under various constraints, resulting in algorithms that provide\ndesirable approximation guarantees. In many applications, however, we do not\nhave access to the submodular function we aim to optimize, but rather to some\nerroneous or noisy version of it. This raises the question of whether provable\nguarantees are obtainable in presence of error and noise. We provide initial\nanswers, by focusing on the question of maximizing a monotone submodular\nfunction under a cardinality constraint when given access to a noisy oracle of\nthe function. We show that:\n  - For a cardinality constraint $k \\geq 2$, there is an approximation\nalgorithm whose approximation ratio is arbitrarily close to $1-1/e$;\n  - For $k=1$ there is an algorithm whose approximation ratio is arbitrarily\nclose to $1/2$. No randomized algorithm can obtain an approximation ratio\nbetter than $1/2+o(1)$;\n  -If the noise is adversarial, no non-trivial approximation guarantee can be\nobtained.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 23:05:24 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 22:24:46 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 21:33:37 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Hassidim", "Avinatan", ""], ["Singer", "Yaron", ""]]}, {"id": "1601.03174", "submitter": "Petr Golovach", "authors": "Petr A. Golovach and George B. Mertzios", "title": "Graph Editing to a Given Degree Sequence", "comments": "arXiv admin note: text overlap with arXiv:1311.4768", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the parameterized complexity of the graph editing problem\ncalled Editing to a Graph with a Given Degree Sequence, where the aim is to\nobtain a graph with a given degree sequence \\sigma by at most k vertex or edge\ndeletions and edge additions. We show that the problem is W[1]-hard when\nparameterized by k for any combination of the allowed editing operations. From\nthe positive side, we show that the problem can be solved in time\n2^{O(k(\\Delta+k)^2)}n^2 log n for n-vertex graphs, where \\Delta=max \\sigma,\ni.e., the problem is FPT when parameterized by k+\\Delta. We also show that\nEditing to a Graph with a Given Degree Sequence has a polynomial kernel when\nparameterized by k+\\Delta if only edge additions are allowed, and there is no\npolynomial kernel unless NP\\subseteq coNP/poly for all other combinations of\nallowed editing operations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 09:09:43 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Golovach", "Petr A.", ""], ["Mertzios", "George B.", ""]]}, {"id": "1601.03202", "submitter": "Alberto Molinari", "authors": "A. Molinari, A. Montanari, A. Peron", "title": "Complexity of ITL model checking: some well-behaved fragments of the\n  interval logic HS", "comments": null, "journal-ref": "In proceedings of 22nd TIME, Pages 90-100, 2015 IEEE", "doi": "10.1109/TIME.2015.12", "report-no": null, "categories": "cs.LO cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model checking has been successfully used in many computer science fields,\nincluding artificial intelligence, theoretical computer science, and databases.\nMost of the proposed solutions make use of classical, point-based temporal\nlogics, while little work has been done in the interval temporal logic setting.\nRecently, a non-elementary model checking algorithm for Halpern and Shoham's\nmodal logic of time intervals HS over finite Kripke structures (under the\nhomogeneity assumption) and an EXPSPACE model checking procedure for two\nmeaningful fragments of it have been proposed. In this paper, we show that more\nefficient model checking procedures can be developed for some expressive enough\nfragments of HS.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 11:17:36 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Molinari", "A.", ""], ["Montanari", "A.", ""], ["Peron", "A.", ""]]}, {"id": "1601.03311", "submitter": "Shashwat Garg", "authors": "Nikhil Bansal, Shashwat Garg", "title": "Improved Algorithmic Bounds for Discrepancy of Sparse Set Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding a low discrepancy coloring for sparse set\nsystems where each element lies in at most $t$ sets. We give an algorithm that\nfinds a coloring with discrepancy $O((t \\log n \\log s)^{1/2})$ where $s$ is the\nmaximum cardinality of a set. This improves upon the previous constructive\nbound of $O(t^{1/2} \\log n)$ based on algorithmic variants of the partial\ncoloring method, and for small $s$ (e.g.$s=\\textrm{poly}(t)$) comes close to\nthe non-constructive $O((t \\log n)^{1/2})$ bound due to Banaszczyk. Previously,\nno algorithmic results better than $O(t^{1/2}\\log n)$ were known even for $s =\nO(t^2)$. Our method is quite robust and we give several refinements and\nextensions. For example, the coloring we obtain satisfies the stronger\nsize-sensitive property that each set $S$ in the set system incurs an $O((t\n\\log n \\log |S|)^{1/2})$ discrepancy. Another variant can be used to\nessentially match Banaszczyk's bound for a wide class of instances even where\n$s$ is arbitrarily large. Finally, these results also extend directly to the\nmore general Koml\\'{o}s setting.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 16:48:59 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 12:46:42 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Bansal", "Nikhil", ""], ["Garg", "Shashwat", ""]]}, {"id": "1601.03316", "submitter": "Atsushi Miyauchi", "authors": "Yasushi Kawase, Tomomi Matsui, Atsushi Miyauchi", "title": "Additive Approximation Algorithms for Modularity Maximization", "comments": "23 pages, 4 figures", "journal-ref": "Journal of Computer and System Sciences, 117 (2021), 182-201", "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modularity is a quality function in community detection, which was\nintroduced by Newman and Girvan (2004). Community detection in graphs is now\noften conducted through modularity maximization: given an undirected graph\n$G=(V,E)$, we are asked to find a partition $\\mathcal{C}$ of $V$ that maximizes\nthe modularity. Although numerous algorithms have been developed to date, most\nof them have no theoretical approximation guarantee. Recently, to overcome this\nissue, the design of modularity maximization algorithms with provable\napproximation guarantees has attracted significant attention in the computer\nscience community.\n  In this study, we further investigate the approximability of modularity\nmaximization. More specifically, we propose a polynomial-time\n$\\left(\\cos\\left(\\frac{3-\\sqrt{5}}{4}\\pi\\right) -\n\\frac{1+\\sqrt{5}}{8}\\right)$-additive approximation algorithm for the\nmodularity maximization problem. Note here that\n$\\cos\\left(\\frac{3-\\sqrt{5}}{4}\\pi\\right) - \\frac{1+\\sqrt{5}}{8} < 0.42084$\nholds. This improves the current best additive approximation error of $0.4672$,\nwhich was recently provided by Dinh, Li, and Thai (2015). Interestingly, our\nanalysis also demonstrates that the proposed algorithm obtains a nearly-optimal\nsolution for any instance with a very high modularity value. Moreover, we\npropose a polynomial-time $0.16598$-additive approximation algorithm for the\nmaximum modularity cut problem. It should be noted that this is the first\nnon-trivial approximability result for the problem. Finally, we demonstrate\nthat our approximation algorithm can be extended to some related problems.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 17:11:16 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kawase", "Yasushi", ""], ["Matsui", "Tomomi", ""], ["Miyauchi", "Atsushi", ""]]}, {"id": "1601.03411", "submitter": "Andrew MacFie", "authors": "Andrew MacFie", "title": "Analysis of Algorithms and Partial Algorithms", "comments": null, "journal-ref": "Artificial General Intelligence 2016, New York, USA, July 16-19,\n  2016, Proceedings, 284-293", "doi": "10.1007/978-3-319-41649-6_29", "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an alternative methodology for the analysis of algorithms, based\non the concept of expected discounted reward. This methodology naturally\nhandles algorithms that do not always terminate, so it can (theoretically) be\nused with partial algorithms for undecidable problems, such as those found in\nartificial general intelligence (AGI) and automated theorem proving. We mention\nan approach to self-improving AGI enabled by this methodology.\n  Aug 2017 addendum: This article was originally written with multiple\naudiences in mind. It is really best put in the following terms. Goertzel,\nHutter, Legg, and others have developed a definition of an intelligence score\nfor a general abstract agent: expected lifetime reward in a random environment.\nAIXI is generally the optimal agent according to this score, but there may be\nreasons to analyze other agents and compare score values. If we want to use\nthis definition of intelligence in practice, perhaps we can start by analyzing\nsome simple agents. Common algorithms can be thought of as simple agents\n(environment is input, reward is based on running time) so we take the goal of\napplying the agent intelligence score to algorithms. That is, we want to find,\nwhat are the IQ scores of algorithms? We can do some very simple analysis, but\nthe real answer is that even for simple algorithms, the intelligence score is\ntoo difficult to work with in practice.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 21:17:42 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2016 18:30:51 GMT"}, {"version": "v3", "created": "Tue, 1 Mar 2016 19:21:41 GMT"}, {"version": "v4", "created": "Sun, 8 May 2016 00:52:51 GMT"}, {"version": "v5", "created": "Mon, 7 Aug 2017 01:30:46 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["MacFie", "Andrew", ""]]}, {"id": "1601.03428", "submitter": "Veit Elser", "authors": "Veit Elser", "title": "The complexity of bit retrieval", "comments": "42 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bit retrieval is the problem of reconstructing a binary sequence from its\nperiodic autocorrelation, with applications in cryptography and x-ray\ncrystallography. After defining the problem, with and without noise, we\ndescribe and compare various algorithms for solving it. A geometrical\nconstraint satisfaction algorithm, relaxed-reflect-reflect, is currently the\nbest algorithm for noisy bit retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 22:19:20 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Elser", "Veit", ""]]}, {"id": "1601.03458", "submitter": "Tomomi Matsui", "authors": "Tomomi Matsui and Takayoshi Hamaguchi", "title": "Characterizing a Set of Popular Matchings Defined by Preference Lists\n  with Ties", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give a simple characterization of a set of popular\nmatchings defined by preference lists with ties. By employing our\ncharacterization, we propose a polynomial time algorithm for finding a minimum\ncost popular matching.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 01:17:36 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Matsui", "Tomomi", ""], ["Hamaguchi", "Takayoshi", ""]]}, {"id": "1601.03523", "submitter": "Ashish Shrivastava", "authors": "Ashish Shrivastava, C. Pandu Rangan", "title": "Stable Marriage Problem with Ties and Incomplete bounded length\n  preference list under social stability", "comments": "This paper has been withdrawn by the author due to a change in\n  algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variant of socially stable marriage problem where preference\nlists may be incomplete, may contain ties and may have bounded length. In real\nworld application like NRMP and Scottish medical matching scheme such\nrestrictions arise very frequently where set of agents (man/woman) is very\nlarge and providing a complete and strict order preference list is practically\nin-feasible. In presence of ties in preference lists, the most common solution\nis weakly socially stable matching. It is a fact that in an instance, weakly\nstable matching can have different sizes. This motivates the problem of finding\na maximum cardinality weakly socially stable matching.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 09:22:58 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 20:37:56 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Shrivastava", "Ashish", ""], ["Rangan", "C. Pandu", ""]]}, {"id": "1601.03603", "submitter": "Jannik Matuschke", "authors": "Jannik Matuschke, S. Thomas McCormick, Gianpaolo Oriolo, Britta Peis,\n  Martin Skutella", "title": "Protection of flows under targeted attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the importance of robustness in many real-world optimization problems,\nthe field of robust optimization has gained a lot of attention over the past\ndecade. We concentrate on maximum flow problems and introduce a novel robust\noptimization model which, compared to known models from the literature,\nfeatures several advantageous properties: (i) We consider a general class of\npath-based flow problems which can be used to model a large variety of network\nrouting problems (and other packing problems). (ii) We aim at solutions that\nare robust against targeted attacks by a potent adversary who may attack any\nflow path of his choice on any edge of the network. (iii) In contrast to\nprevious robust maximum flow models, for which no efficient algorithms are\nknown, optimal robust flows for the most important basic variants of our model\ncan be found in polynomial time.\n  We also consider generalizations where the flow player can spend a budget to\nprotect the network against the interdictor. Here, we show that the problem can\nbe solved efficiently when the interdiction costs are determined by the flow\nplayer from scratch. However, the problem becomes hard to approximate when the\nflow player has to improve an initial protection infrastructure.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 14:31:10 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Matuschke", "Jannik", ""], ["McCormick", "S. Thomas", ""], ["Oriolo", "Gianpaolo", ""], ["Peis", "Britta", ""], ["Skutella", "Martin", ""]]}, {"id": "1601.03676", "submitter": "Jazm\\'in Romero", "authors": "Alejandro L\\'opez-Ortiz and Jazm\\'in Romero", "title": "Arbitrary Overlap Constraints in Graph Packing Problems", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In earlier versions of the community discovering problem, the overlap between\ncommunities was restricted by a simple count upper-bound [17,5,11,8]. In this\npaper, we introduce the $\\Pi$-Packing with $\\alpha()$-Overlap problem to allow\nfor more complex constraints in the overlap region than those previously\nstudied. Let $\\mathcal{V}^r$ be all possible subsets of vertices of $V(G)$ each\nof size at most $r$, and $\\alpha: \\mathcal{V}^r \\times \\mathcal{V}^r \\to\n\\{0,1\\}$ be a function. The $\\Pi$-Packing with $\\alpha()$-Overlap problem seeks\nat least $k$ induced subgraphs in a graph $G$ subject to: (i) each subgraph has\nat most $r$ vertices and obeys a property $\\Pi$, and (ii) for any pair\n$H_i,H_j$, with $i\\neq j$, $\\alpha(H_i, H_j) = 0$ (i.e., $H_i,H_j$ do not\nconflict). We also consider a variant that arises in clustering applications:\neach subgraph of a solution must contain a set of vertices from a given\ncollection of sets $\\mathcal{C}$, and no pair of subgraphs may share vertices\nfrom the sets of $\\mathcal{C}$. In addition, we propose similar formulations\nfor packing hypergraphs. We give an $O(r^{rk} k^{(r+1)k} n^{cr})$ algorithm for\nour problems where $k$ is the parameter and $c$ and $r$ are constants, provided\nthat: i) $\\Pi$ is computable in polynomial time in $n$ and ii) the function\n$\\alpha()$ satisfies specific conditions. Specifically, $\\alpha()$ is\nhereditary, applicable only to overlapping subgraphs, and computable in\npolynomial time in $n$. Motivated by practical applications we give several\nexamples of $\\alpha()$ functions which meet those conditions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 17:31:55 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["L\u00f3pez-Ortiz", "Alejandro", ""], ["Romero", "Jazm\u00edn", ""]]}, {"id": "1601.03754", "submitter": "Ryan Curtin", "authors": "Ryan R. Curtin", "title": "Dual-tree $k$-means with bounded iteration runtime", "comments": "supplementary material included; submitted to ICML '16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  k-means is a widely used clustering algorithm, but for $k$ clusters and a\ndataset size of $N$, each iteration of Lloyd's algorithm costs $O(kN)$ time.\nAlthough there are existing techniques to accelerate single Lloyd iterations,\nnone of these are tailored to the case of large $k$, which is increasingly\ncommon as dataset sizes grow. We propose a dual-tree algorithm that gives the\nexact same results as standard $k$-means; when using cover trees, we use\nadaptive analysis techniques to, under some assumptions, bound the\nsingle-iteration runtime of the algorithm as $O(N + k log k)$. To our knowledge\nthese are the first sub-$O(kN)$ bounds for exact Lloyd iterations. We then show\nthat this theoretically favorable algorithm performs competitively in practice,\nespecially for large $N$ and $k$ in low dimensions. Further, the algorithm is\ntree-independent, so any type of tree may be used.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 21:18:06 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Curtin", "Ryan R.", ""]]}, {"id": "1601.03800", "submitter": "Sang-il Oum", "authors": "Sang-il Oum", "title": "Rank-width: Algorithmic and structural results", "comments": "14 pages; minor revision", "journal-ref": "Discrete Applied Math., 231(November 2017), pp. 15-24", "doi": "10.1016/j.dam.2016.08.006", "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank-width is a width parameter of graphs describing whether it is possible\nto decompose a graph into a tree-like structure by `simple' cuts. This survey\naims to summarize known algorithmic and structural results on rank-width of\ngraphs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 02:37:52 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 13:53:11 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Oum", "Sang-il", ""]]}, {"id": "1601.03892", "submitter": "Massimo Cafaro", "authors": "Massimo Cafaro, Marco Pulimeno, Italo Epicoco and Giovanni Aloisio", "title": "Mining frequent items in the time fading model", "comments": "To appear in Information Sciences, Elsevier", "journal-ref": "Information Sciences, Elsevier, 2016, Volume 370-371, pp.221-238", "doi": "10.1016/j.ins.2016.07.077", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FDCMSS, a new sketch-based algorithm for mining frequent items in\ndata streams. The algorithm cleverly combines key ideas borrowed from forward\ndecay, the Count-Min and the Space Saving algorithms. It works in the time\nfading model, mining data streams according to the cash register model. We\nformally prove its correctness and show, through extensive experimental\nresults, that our algorithm outperforms $\\lambda$-HCount, a recently developed\nalgorithm, with regard to speed, space used, precision attained and error\ncommitted on both synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 12:21:47 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 10:00:22 GMT"}, {"version": "v3", "created": "Tue, 2 Aug 2016 08:24:12 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Cafaro", "Massimo", ""], ["Pulimeno", "Marco", ""], ["Epicoco", "Italo", ""], ["Aloisio", "Giovanni", ""]]}, {"id": "1601.03928", "submitter": "Krasimir Yordzhev", "authors": "Krasimir Yordzhev", "title": "On an application of multidimensional arrays", "comments": null, "journal-ref": "British Journal of Mathematics & Computer Science, ISSN:\n  2231-0851, 11(4): 1-7, 2015", "doi": "10.9734/BJMCS/2015/20372", "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article discusses some difficulties in the implementation of\ncombinatorial algorithms associated with the choice of all elements with\ncertain properties among the elements of a set with great cardinality.The\nproblem has been resolved by using multidimensional arrays. Illustration of the\nmethod is a solution of the problem of obtaining one representative from each\nequivalence class with respect to the described in the article equivalence\nrelation in the set of all $m\\sim n$ binary matrices. This equivalence relation\nhas an application in the mathematical modeling in the textile industry.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 14:17:27 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Yordzhev", "Krasimir", ""]]}, {"id": "1601.04017", "submitter": "Tobias Maier", "authors": "Tobias Maier, Peter Sanders and Roman Dementiev", "title": "Concurrent Hash Tables: Fast and General?(!)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrent hash tables are one of the most important concurrent data\nstructures with numerous applications. Since hash table accesses can dominate\nthe execution time of the overall application, we need implementations that\nachieve good speedup. Unfortunately, currently available concurrent hashing\nlibraries turn out to be far away from this requirement in particular when\ncontention on some elements occurs.\n  Our starting point for better performing data structures is a fast and simple\nlock-free concurrent hash table based on linear probing that is limited to\nword-sized key-value types and does not support dynamic size adaptation. We\nexplain how to lift these limitations in a provably scalable way and\ndemonstrate that dynamic growing has a performance overhead comparable to the\nsame generalization in sequential hash tables.\n  We perform extensive experiments comparing the performance of our\nimplementations with six of the most widely used concurrent hash tables. Ours\nare considerably faster than the best algorithms with similar restrictions and\nan order of magnitude faster than the best more general tables. In some extreme\ncases, the difference even approaches four orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 17:46:53 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 17:57:28 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Maier", "Tobias", ""], ["Sanders", "Peter", ""], ["Dementiev", "Roman", ""]]}, {"id": "1601.04169", "submitter": "Stefano Leucci", "authors": "Davide Bil\\`o, Luciano Gual\\`a, Stefano Leucci, Guido Proietti", "title": "Multiple-Edge-Fault-Tolerant Approximate Shortest-Path Trees", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Let $G$ be an $n$-node and $m$-edge positively real-weighted undirected\ngraph. For any given integer $f \\ge 1$, we study the problem of designing a\nsparse \\emph{f-edge-fault-tolerant} ($f$-EFT) $\\sigma${\\em -approximate\nsingle-source shortest-path tree} ($\\sigma$-ASPT), namely a subgraph of $G$\nhaving as few edges as possible and which, following the failure of a set $F$\nof at most $f$ edges in $G$, contains paths from a fixed source that are\nstretched at most by a factor of $\\sigma$. To this respect, we provide an\nalgorithm that efficiently computes an $f$-EFT $(2|F|+1)$-ASPT of size $O(f\nn)$. Our structure improves on a previous related construction designed for\n\\emph{unweighted} graphs, having the same size but guaranteeing a larger\nstretch factor of $3(f+1)$, plus an additive term of $(f+1) \\log n$.\n  Then, we show how to convert our structure into an efficient $f$-EFT\n\\emph{single-source distance oracle} (SSDO), that can be built in\n$\\widetilde{O}(f m)$ time, has size $O(fn \\log^2 n)$, and is able to report,\nafter the failure of the edge set $F$, in $O(|F|^2 \\log^2 n)$ time a\n$(2|F|+1)$-approximate distance from the source to any node, and a\ncorresponding approximate path in the same amount of time plus the path's size.\nSuch an oracle is obtained by handling another fundamental problem, namely that\nof updating a \\emph{minimum spanning forest} (MSF) of $G$ after that a\n\\emph{batch} of $k$ simultaneous edge modifications (i.e., edge insertions,\ndeletions and weight changes) is performed. For this problem, we build in $O(m\n\\log^3 n)$ time a \\emph{sensitivity} oracle of size $O(m \\log^2 n)$, that\nreports in $O(k^2 \\log^2 n)$ time the (at most $2k$) edges either exiting from\nor entering into the MSF. [...]\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 14:44:58 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 11:24:45 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Bil\u00f2", "Davide", ""], ["Gual\u00e0", "Luciano", ""], ["Leucci", "Stefano", ""], ["Proietti", "Guido", ""]]}, {"id": "1601.04203", "submitter": "Christopher Musco", "authors": "Christopher Musco, Maxim Sviridenko, Justin Thaler", "title": "Determining Tournament Payout Structures for Daily Fantasy Sports", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an exploding global market and the recent introduction of online cash\nprize tournaments, fantasy sports contests are quickly becoming a central part\nof the social gaming and sports industries. For sports fans and online media\ncompanies, fantasy sports contests are an opportunity for large financial\ngains. However, they present a host of technical challenges that arise from the\ncomplexities involved in running a web-scale, prize driven fantasy sports\nplatform.\n  We initiate the study of these challenges by examining one concrete problem\nin particular: how to algorithmically generate contest payout structures that\nare 1) economically motivating and appealing to contestants and 2) reasonably\nstructured and succinctly representable. We formalize this problem and present\na general two-staged approach for producing satisfying payout structures given\nconstraints on contest size, entry fee, prize bucketing, etc.\n  We then propose and evaluate several potential algorithms for solving the\npayout problem efficiently, including methods based on dynamic programming,\ninteger programming, and heuristic techniques. Experimental results show that a\ncarefully designed heuristic scales very well, even to contests with over\n100,000 prize winners.\n  Our approach extends beyond fantasy sports -- it is suitable for generating\nengaging payout structures for any contest with a large number of entrants and\na large number of prize winners, including other massive online games, poker\ntournaments, and real-life sports tournaments.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 19:56:53 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 15:50:36 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Musco", "Christopher", ""], ["Sviridenko", "Maxim", ""], ["Thaler", "Justin", ""]]}, {"id": "1601.04213", "submitter": "Junichiro Fukuyama", "authors": "Junichiro Fukuyama", "title": "Partial-Match Queries with Random Wildcards: In Tries and Distributed\n  Hash Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an $m$-bit query $q$ to a bitwise trie $T$. A wildcard $*$ is an\nunspecified bit in $q$ for which the query asks the membership for both cases\n$*=0$ and $*=1$. It is common that such partial-match queries with wildcards\nare issued in tries. With uniformly random occurrences of $w$ wildcards in $q$\nassumed, the obvious upper bound on the average number of traversal steps in\n$T$ is $2^w m$. We show that the average does not exceed \\[ \\frac{m+1}{w+1}\n\\left( 2^{w+2} - 2 w - 4 \\right) + m = O \\left( \\frac{2^w m}{w} \\right), \\] and\nequals the value exactly when $T$ includes all the $m$-bit keys as the worst\ncase. Here the query $q$ performs with the naive backtracking algorithm in $T$.\nIt is similarly shown that the average is $O \\left( \\frac{k^w m}{w} \\right)$ in\na general trie of maximum out-degree $k$. Our analysis for tries is extended to\na distributed hash table (DHT), which is among the most frequently used\ndecentralized data structures in networking. We show, under a natural\nprobabilistic assumption for the largest class of DHTs, that the average number\nof hops required by an $m$-bit query $q$ to a DHT $D$ with random $w$ wildcards\nmeets the same asymptotic bound. As a result, $q$ is answered with average $O\n\\left( \\frac{2^w m}{w} \\right)$ hops rather than $\\Theta \\left( 2^w m \\right)$\nin the four major DHTs Chord, Pastry, Tapestry and Kademlia. In addition, with\na uniform key distribution for sufficiently many entries, we prove that a\nlookup request to the DHT Chord is answered correctly with $O(m)$ hops and\nprobability $1 - 2^{-\\Omega (m)}$. To the author's knowledge, the probability\n$1 - 2^{-\\Omega (m)}$ of correct lookup in Chord has not been identified so\nfar.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 21:35:36 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 22:22:31 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Fukuyama", "Junichiro", ""]]}, {"id": "1601.04233", "submitter": "Maryam Aliakbarpour", "authors": "Maryam Aliakbarpour, Amartya Shankha Biswas, Themistoklis Gouleakis,\n  John Peebles, Ronitt Rubinfeld, Anak Yodpinyanee", "title": "Sublinear-Time Algorithms for Counting Star Subgraphs with Applications\n  to Join Selectivity Estimation", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the value of sums of the form $S_p\n\\triangleq \\sum \\binom{x_i}{p}$ when one has the ability to sample $x_i \\geq 0$\nwith probability proportional to its magnitude. When $p=2$, this problem is\nequivalent to estimating the selectivity of a self-join query in database\nsystems when one can sample rows randomly. We also study the special case when\n$\\{x_i\\}$ is the degree sequence of a graph, which corresponds to counting the\nnumber of $p$-stars in a graph when one has the ability to sample edges\nrandomly.\n  Our algorithm for a $(1 \\pm \\varepsilon)$-multiplicative approximation of\n$S_p$ has query and time complexities $\\O(\\frac{m \\log \\log n}{\\epsilon^2\nS_p^{1/p}})$. Here, $m=\\sum x_i/2$ is the number of edges in the graph, or\nequivalently, half the number of records in the database table. Similarly, $n$\nis the number of vertices in the graph and the number of unique values in the\ndatabase table. We also provide tight lower bounds (up to polylogarithmic\nfactors) in almost all cases, even when $\\{x_i\\}$ is a degree sequence and one\nis allowed to use the structure of the graph to try to get a better estimate.\nWe are not aware of any prior lower bounds on the problem of join selectivity\nestimation.\n  For the graph problem, prior work which assumed the ability to sample only\n\\emph{vertices} uniformly gave algorithms with matching lower bounds [Gonen,\nRon, and Shavitt. \\textit{SIAM J. Comput.}, 25 (2011), pp. 1365-1411]. With the\nability to sample edges randomly, we show that one can achieve faster\nalgorithms for approximating the number of star subgraphs, bypassing the lower\nbounds in this prior work. For example, in the regime where $S_p\\leq n$, and\n$p=2$, our upper bound is $\\tilde{O}(n/S_p^{1/2})$, in contrast to their\n$\\Omega(n/S_p^{1/3})$ lower bound when no random edge queries are available.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2016 00:02:37 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Aliakbarpour", "Maryam", ""], ["Biswas", "Amartya Shankha", ""], ["Gouleakis", "Themistoklis", ""], ["Peebles", "John", ""], ["Rubinfeld", "Ronitt", ""], ["Yodpinyanee", "Anak", ""]]}, {"id": "1601.04248", "submitter": "Tejeswini Sundaram", "authors": "Tejeswini Sundaram and Vyom Chabbra", "title": "Word Existence Algorithm", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The current scenario in the field of computing is largely affected by the\nspeed at which data can be accessed and recalled. In this paper, we present the\nword existence algorithm which is used to check if the word given as an input\nis part of a particular database or not. We have taken the English language as\nan example here. This algorithm tries to solve the problem of lookup by using a\nuniformly distributed hash function. We have also addressed the problem of\nclustering and collision. A further contribution is that we follow a direct\nhashed model where each hash value is linked to another table if the continuity\nfor the function holds true. The core of the algorithm lies in the data model\nbeing used during preordering. Our focus lies on the formation of a continuity\nseries and validating the words that exists in the database. This algorithm can\nbe used in applications where we there is a requirement to search for just the\nexistence of a word, example Artificial Intelligence responding to input ,look\nup for neural networks and dictionary lookups and more. We have observed that\nthis algorithm provides a faster search time\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 12:37:22 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Sundaram", "Tejeswini", ""], ["Chabbra", "Vyom", ""]]}, {"id": "1601.04448", "submitter": "Manuel Malatyali", "authors": "Alexander M\\\"acker, Manuel Malatyali, Friedhelm Meyer auf der Heide", "title": "On Competitive Algorithms for Approximations of Top-k-Position\n  Monitoring of Distributed Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the continuous distributed monitoring model in which $n$ distributed\nnodes, receiving individual data streams, are connected to a designated server.\nThe server is asked to continuously monitor a function defined over the values\nobserved across all streams while minimizing the communication. We study a\nvariant in which the server is equipped with a broadcast channel and is\nsupposed to keep track of an approximation of the set of nodes currently\nobserving the $k$ largest values. Such an approximate set is exact except for\nsome imprecision in an $\\varepsilon$-neighborhood of the $k$-th largest value.\nThis approximation of the Top-$k$-Position Monitoring Problem is of interest in\ncases where marginal changes (e.g. due to noise) in observed values can be\nignored so that monitoring an approximation is sufficient and can reduce\ncommunication.\n  This paper extends our results from [IPDPS'15], where we have developed a\nfilter-based online algorithm for the (exact) Top-k-Position Monitoring\nProblem. There we have presented a competitive analysis of our algorithm\nagainst an offline adversary that also is restricted to filter-based\nalgorithms. Our new algorithms as well as their analyses use new methods. We\nanalyze their competitiveness against adversaries that use both exact and\napproximate filter-based algorithms, and observe severe differences between the\nrespective powers of these adversaries.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 10:10:59 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 10:45:00 GMT"}, {"version": "v3", "created": "Thu, 27 Oct 2016 12:21:27 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["M\u00e4cker", "Alexander", ""], ["Malatyali", "Manuel", ""], ["der Heide", "Friedhelm Meyer auf", ""]]}, {"id": "1601.04458", "submitter": "Frank T.  Bergmann", "authors": "Christoph Zimmer and Frank T. Bergmann and Sven Sahle", "title": "Reducing local minima in fitness landscapes of parameter estimation by\n  using piecewise evaluation and state estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.DS cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary differential equations (ODE) are widely used for modeling in Systems\nBiology. As most commonly only some of the kinetic parameters are measurable or\nprecisely known, parameter estimation techniques are applied to parametrize the\nmodel to experimental data. A main challenge for the parameter estimation is\nthe complexity of the parameter space, especially its high dimensionality and\nlocal minima.\n  Parameter estimation techniques consist of an objective function, measuring\nhow well a certain parameter set describes the experimental data, and an\noptimization algorithm that optimizes this objective function. A lot of effort\nhas been spent on developing highly sophisticated optimization algorithms to\ncope with the complexity in the parameter space, but surprisingly few articles\naddress the influence of the objective function on the computational complexity\nin finding global optima. We extend a recently developed multiple shooting for\nstochastic systems (MSS) objective function for parameter estimation of\nstochastic models and apply it to parameter estimation of ODE models. This MSS\nobjective function treats the intervals between measurement points separately.\nThis separate treatment allows the ODE trajectory to stay closer to the data\nand we show that it reduces the complexity of the parameter space.\n  We use examples from Systems Biology, namely a Lotka-Volterra model, a\nFitzHugh-Nagumo oscillator and a Calcium oscillation model, to demonstrate the\npower of the MSS approach for reducing the complexity and the number of local\nminima in the parameter space. The approach is fully implemented in the COPASI\nsoftware package and, therefore, easily accessible for a wide community of\nresearchers.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 10:38:52 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Zimmer", "Christoph", ""], ["Bergmann", "Frank T.", ""], ["Sahle", "Sven", ""]]}, {"id": "1601.04692", "submitter": "Jean Gallier", "authors": "Jean Gallier", "title": "Spectral Theory of Unsigned and Signed Graphs. Applications to Graph\n  Clustering: a Survey", "comments": "122 pages. arXiv admin note: substantial text overlap with\n  arXiv:1311.2492", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a survey of the method of graph cuts and its applications to graph\nclustering of weighted unsigned and signed graphs. I provide a fairly thorough\ntreatment of the method of normalized graph cuts, a deeply original method due\nto Shi and Malik, including complete proofs. The main thrust of this paper is\nthe method of normalized cuts. I give a detailed account for K = 2 clusters,\nand also for K > 2 clusters, based on the work of Yu and Shi. I also show how\nboth graph drawing and normalized cut K-clustering can be easily generalized to\nhandle signed graphs, which are weighted graphs in which the weight matrix W\nmay have negative coefficients. Intuitively, negative coefficients indicate\ndistance or dissimilarity. The solution is to replace the degree matrix by the\nmatrix in which absolute values of the weights are used, and to replace the\nLaplacian by the Laplacian with the new degree matrix of absolute values. As\nfar as I know, the generalization of K-way normalized clustering to signed\ngraphs is new. Finally, I show how the method of ratio cuts, in which a cut is\nnormalized by the size of the cluster rather than its volume, is just a special\ncase of normalized cuts.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 20:57:41 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Gallier", "Jean", ""]]}, {"id": "1601.04814", "submitter": "Gianmarco De Francisci Morales", "authors": "Gianmarco De Francisci Morales and Aristides Gionis", "title": "Streaming Similarity Self-Join", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study the problem of computing the similarity self-join in a\nstreaming context (SSSJ), where the input is an unbounded stream of items\narriving continuously. The goal is to find all pairs of items in the stream\nwhose similarity is greater than a given threshold. The simplest formulation of\nthe problem requires unbounded memory, and thus, it is intractable. To make the\nproblem feasible, we introduce the notion of time-dependent similarity: the\nsimilarity of two items decreases with the difference in their arrival time. By\nleveraging the properties of this time-dependent similarity function, we design\ntwo algorithmic frameworks to solve the sssj problem. The first one, MiniBatch\n(MB), uses existing index-based filtering techniques for the static version of\nthe problem, and combines them in a pipeline. The second framework, Streaming\n(STR), adds time filtering to the existing indexes, and integrates new\ntime-based bounds deeply in the working of the algorithms. We also introduce a\nnew indexing technique (L2), which is based on an existing state-of-the-art\nindexing technique (L2AP), but is optimized for the streaming case. Extensive\nexperiments show that the STR algorithm, when instantiated with the L2 index,\nis the most scalable option across a wide array of datasets and parameters.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 07:34:17 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 09:14:27 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Morales", "Gianmarco De Francisci", ""], ["Gionis", "Aristides", ""]]}, {"id": "1601.04974", "submitter": "Leo van Iersel", "authors": "Laura Jetten and Leo van Iersel", "title": "Nonbinary tree-based phylogenetic networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rooted phylogenetic networks are used to describe evolutionary histories that\ncontain non-treelike evolutionary events such as hybridization and horizontal\ngene transfer. In some cases, such histories can be described by a phylogenetic\nbase-tree with additional linking arcs, which can for example represent gene\ntransfer events. Such phylogenetic networks are called tree-based. Here, we\nconsider two possible generalizations of this concept to nonbinary networks,\nwhich we call tree-based and strictly-tree-based nonbinary phylogenetic\nnetworks. We give simple graph-theoretic characterizations of tree-based and\nstrictly-tree-based nonbinary phylogenetic networks. Moreover, we show for each\nof these two classes that it can be decided in polynomial time whether a given\nnetwork is contained in the class. Our approach also provides a new view on\ntree-based binary phylogenetic networks. Finally, we discuss two examples of\nnonbinary phylogenetic networks in biology and show how our results can be\napplied to them.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 16:13:31 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 08:11:59 GMT"}, {"version": "v3", "created": "Fri, 30 Sep 2016 05:34:13 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Jetten", "Laura", ""], ["van Iersel", "Leo", ""]]}, {"id": "1601.05003", "submitter": "Florent Foucaud", "authors": "Florent Foucaud and Ralf Klasing", "title": "Parameterized and approximation complexity of the detection pair problem\n  in graphs", "comments": "13 pages", "journal-ref": "Journal of Graph Algorithms and Applications 21(6):1039-1056, 2017", "doi": "10.7155/jgaa.00449", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the problem DETECTION PAIR. A detection pair of a\ngraph $G$ is a pair $(W,L)$ of sets of detectors with $W\\subseteq V(G)$, the\nwatchers, and $L\\subseteq V(G)$, the listeners, such that for every pair $u,v$\nof vertices that are not dominated by a watcher of $W$, there is a listener of\n$L$ whose distances to $u$ and to $v$ are different. The goal is to minimize\n$|W|+|L|$. This problem generalizes the two classic problems DOMINATING SET and\nMETRIC DIMENSION, that correspond to the restrictions $L=\\emptyset$ and\n$W=\\emptyset$, respectively. DETECTION PAIR was recently introduced by Finbow,\nHartnell and Young [A. S. Finbow, B. L. Hartnell and J. R. Young. The\ncomplexity of monitoring a network with both watchers and listeners.\nManuscript, 2015], who proved it to be NP-complete on trees, a surprising\nresult given that both DOMINATING SET and METRIC DIMENSION are known to be\nlinear-time solvable on trees. It follows from an existing reduction by Hartung\nand Nichterlein for METRIC DIMENSION that even on bipartite subcubic graphs of\narbitrarily large girth, DETECTION PAIR is NP-hard to approximate within a\nsub-logarithmic factor and W[2]-hard (when parameterized by solution size). We\nshow, using a reduction to SET COVER, that DETECTION PAIR is approximable\nwithin a factor logarithmic in the number of vertices of the input graph. Our\ntwo main results are a linear-time $2$-approximation algorithm and an FPT\nalgorithm for DETECTION PAIR on trees.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 17:20:40 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 07:00:24 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Foucaud", "Florent", ""], ["Klasing", "Ralf", ""]]}, {"id": "1601.05020", "submitter": "German Tischler", "authors": "German Tischler", "title": "Low Space External Memory Construction of the Succinct Permuted Longest\n  Common Prefix Array", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The longest common prefix (LCP) array is a versatile auxiliary data structure\nin indexed string matching. It can be used to speed up searching using the\nsuffix array (SA) and provides an implicit representation of the topology of an\nunderlying suffix tree. The LCP array of a string of length $n$ can be\nrepresented as an array of length $n$ words, or, in the presence of the SA, as\na bit vector of $2n$ bits plus asymptotically negligible support data\nstructures. External memory construction algorithms for the LCP array have been\nproposed, but those proposed so far have a space requirement of $O(n)$ words\n(i.e. $O(n \\log n)$ bits) in external memory. This space requirement is in some\npractical cases prohibitively expensive. We present an external memory\nalgorithm for constructing the $2n$ bit version of the LCP array which uses\n$O(n \\log \\sigma)$ bits of additional space in external memory when given a\n(compressed) BWT with alphabet size $\\sigma$ and a sampled inverse suffix array\nat sampling rate $O(\\log n)$. This is often a significant space gain in\npractice where $\\sigma$ is usually much smaller than $n$ or even constant. We\nalso consider the case of computing succinct LCP arrays for circular strings.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 17:54:14 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 16:03:59 GMT"}, {"version": "v3", "created": "Tue, 8 Mar 2016 15:43:48 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Tischler", "German", ""]]}, {"id": "1601.05047", "submitter": "Justin Hsu", "authors": "Gilles Barthe, Marco Gaboardi, Benjamin Gr\\'egoire, Justin Hsu,\n  Pierre-Yves Strub", "title": "Proving Differential Privacy via Probabilistic Couplings", "comments": null, "journal-ref": null, "doi": "10.1145/2933575.2934554", "report-no": null, "categories": "cs.LO cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop compositional methods for formally verifying\ndifferential privacy for algorithms whose analysis goes beyond the composition\ntheorem. Our methods are based on the observation that differential privacy has\ndeep connections with a generalization of probabilistic couplings, an\nestablished mathematical tool for reasoning about stochastic processes. Even\nwhen the composition theorem is not helpful, we can often prove privacy by a\ncoupling argument.\n  We demonstrate our methods on two algorithms: the Exponential mechanism and\nthe Above Threshold algorithm, the critical component of the famous Sparse\nVector algorithm. We verify these examples in a relational program logic\napRHL+, which can construct approximate couplings. This logic extends the\nexisting apRHL logic with more general rules for the Laplace mechanism and the\none-sided Laplace mechanism, and new structural rules enabling pointwise\nreasoning about privacy; all the rules are inspired by the connection with\ncoupling. While our paper is presented from a formal verification perspective,\nwe believe that its main insight is of independent interest for the\ndifferential privacy community.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 19:37:19 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 09:27:19 GMT"}, {"version": "v3", "created": "Sun, 19 Jun 2016 18:57:10 GMT"}, {"version": "v4", "created": "Thu, 9 Mar 2017 16:00:41 GMT"}, {"version": "v5", "created": "Sun, 14 Mar 2021 14:06:26 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Barthe", "Gilles", ""], ["Gaboardi", "Marco", ""], ["Gr\u00e9goire", "Benjamin", ""], ["Hsu", "Justin", ""], ["Strub", "Pierre-Yves", ""]]}, {"id": "1601.05480", "submitter": "Yasushi Kawase", "authors": "Yasushi Kawase, Kazuhisa Makino, Kento Seimi", "title": "Optimal Composition Ordering Problems for Piecewise Linear Functions", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce maximum composition ordering problems. The input\nis $n$ real functions $f_1,\\dots,f_n:\\mathbb{R}\\to\\mathbb{R}$ and a constant\n$c\\in\\mathbb{R}$. We consider two settings: total and partial compositions. The\nmaximum total composition ordering problem is to compute a permutation\n$\\sigma:[n]\\to[n]$ which maximizes $f_{\\sigma(n)}\\circ\nf_{\\sigma(n-1)}\\circ\\dots\\circ f_{\\sigma(1)}(c)$, where $[n]=\\{1,\\dots,n\\}$.\nThe maximum partial composition ordering problem is to compute a permutation\n$\\sigma:[n]\\to[n]$ and a nonnegative integer $k~(0\\le k\\le n)$ which maximize\n$f_{\\sigma(k)}\\circ f_{\\sigma(k-1)}\\circ\\dots\\circ f_{\\sigma(1)}(c)$.\n  We propose $O(n\\log n)$ time algorithms for the maximum total and partial\ncomposition ordering problems for monotone linear functions $f_i$, which\ngeneralize linear deterioration and shortening models for the time-dependent\nscheduling problem. We also show that the maximum partial composition ordering\nproblem can be solved in polynomial time if $f_i$ is of form\n$\\max\\{a_ix+b_i,c_i\\}$ for some constants $a_i\\,(\\ge 0)$, $b_i$ and $c_i$. We\nfinally prove that there exists no constant-factor approximation algorithm for\nthe problems, even if $f_i$'s are monotone, piecewise linear functions with at\nmost two pieces, unless P=NP.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 00:52:00 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Kawase", "Yasushi", ""], ["Makino", "Kazuhisa", ""], ["Seimi", "Kento", ""]]}, {"id": "1601.05527", "submitter": "Emmanuel John Emmanuel John", "authors": "Emmanuel John and Ilya Safro", "title": "Single- and Multi-level Network Sparsification by Algebraic Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network sparsification methods play an important role in modern network\nanalysis when fast estimation of computationally expensive properties (such as\nthe diameter, centrality indices, and paths) is required. We propose a method\nof network sparsification that preserves a wide range of structural properties.\nDepending on the analysis goals, the method allows to distinguish between local\nand global range edges that can be filtered out during the sparsification.\nFirst we rank edges by their algebraic distances and then we sample them. We\nalso introduce a multilevel framework for sparsification that can be used to\ncontrol the sparsification process at various coarse-grained resolutions. Based\nprimarily on the matrix-vector multiplications, our method is easily\nparallelized for different architectures.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 07:09:21 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["John", "Emmanuel", ""], ["Safro", "Ilya", ""]]}, {"id": "1601.05557", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane", "title": "A New Approach for Testing Properties of Discrete Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we give a novel general approach for distribution testing. We\ndescribe two techniques: our first technique gives sample-optimal testers,\nwhile our second technique gives matching sample lower bounds. As a\nconsequence, we resolve the sample complexity of a wide variety of testing\nproblems.\n  Our upper bounds are obtained via a modular reduction-based approach. Our\napproach yields optimal testers for numerous problems by using a standard\n$\\ell_2$-identity tester as a black-box. Using this recipe, we obtain simple\nestimators for a wide range of problems, encompassing most problems previously\nstudied in the TCS literature, namely: (1) identity testing to a fixed\ndistribution, (2) closeness testing between two unknown distributions (with\nequal/unequal sample sizes), (3) independence testing (in any number of\ndimensions), (4) closeness testing for collections of distributions, and (5)\ntesting histograms. For all of these problems, our testers are sample-optimal,\nup to constant factors. With the exception of (1), ours are the {\\em first\nsample-optimal testers for the corresponding problems.} Moreover, our\nestimators are significantly simpler to state and analyze compared to previous\nresults.\n  As an application of our reduction-based technique, we obtain the first {\\em\nnearly instance-optimal} algorithm for testing equivalence between two {\\em\nunknown} distributions. Moreover, our technique naturally generalizes to other\nmetrics beyond the $\\ell_1$-distance.\n  Our lower bounds are obtained via a direct information-theoretic approach:\nGiven a candidate hard instance, our proof proceeds by bounding the mutual\ninformation between appropriate random variables. While this is a classical\nmethod in information theory, prior to our work, it had not been used in\ndistribution property testing.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 09:06:17 GMT"}, {"version": "v2", "created": "Mon, 9 May 2016 06:55:09 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""]]}, {"id": "1601.05962", "submitter": "Samuele Giraudo", "authors": "Samuele Giraudo and St\\'ephane Vialette", "title": "Unshuffling Permutations", "comments": "13 pages", "journal-ref": "Latin American Theoretical Informatics Symposium, LNCS 9644,\n  509--521, 2016", "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A permutation is said to be a square if it can be obtained by shuffling two\norder-isomorphic patterns. The definition is intended to be the natural\ncounterpart to the ordinary shuffle of words and languages. In this paper, we\ntackle the problem of recognizing square permutations from both the point of\nview of algebra and algorithms. On the one hand, we present some algebraic and\ncombinatorial properties of the shuffle product of permutations. We follow an\nunusual line consisting in defining the shuffle of permutations by means of an\nunshuffling operator, known as a coproduct. This strategy allows to obtain easy\nproofs for algebraic and combinatorial properties of our shuffle product. We\nbesides exhibit a bijection between square $(213,231)$-avoiding permutations\nand square binary words. On the other hand, by using a pattern avoidance\ncriterion on oriented perfect matchings, we prove that recognizing square\npermutations is $\\mathbf{NP}$-complete.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 11:24:54 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2016 09:12:02 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Giraudo", "Samuele", ""], ["Vialette", "St\u00e9phane", ""]]}, {"id": "1601.06291", "submitter": "C.S. Rahul", "authors": "N S Narayanaswamy and C S Rahul", "title": "A Characterization for the Existence of Connected $f$-Factors of\n  $\\textit{ Large}$ Minimum Degree", "comments": "10 pages, Presented in 9th International colloquium on graph theory\n  and combinatorics, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is well known that when $f(v)$ is a constant for each vertex $v$, the\nconnected $f$-factor problem is NP-Complete. In this note we consider the case\nwhen $f(v) \\geq \\lceil \\frac{n}{2.5}\\rceil$ for each vertex $v$, where $n$ is\nthe number of vertices. We present a diameter based characterization of graphs\nhaving a connected $f$-factor (for such $f$). We show that if a graph $G$ has a\nconnected $f$-factor and an $f$-factor with 2 connected components, then it has\na connected $f$-factor of diameter at least 3. This result yields a polynomial\ntime algorithm which first executes the Tutte's $f$-factor algorithm, and if\nthe output has 2 connected components, our algorithm searches for a connected\n$f$-factor of diameter at least 3.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 17:14:24 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Narayanaswamy", "N S", ""], ["Rahul", "C S", ""]]}, {"id": "1601.06311", "submitter": "Apurba Das", "authors": "Apurba Das, Michael Svendsen, Srikanta Tirthapura", "title": "Incremental Maintenance of Maximal Cliques in a Dynamic Graph", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the maintenance of the set of all maximal cliques in a dynamic\ngraph that is changing through the addition or deletion of edges. We present\nnearly tight bounds on the magnitude of change in the set of maximal cliques,\nas well as the first change-sensitive algorithms for clique maintenance, whose\nruntime is proportional to the magnitude of the change in the set of maximal\ncliques. We present experimental results showing these algorithms are efficient\nin practice and are faster than prior work by two to three orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 21:15:26 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 03:23:59 GMT"}, {"version": "v3", "created": "Sat, 17 Mar 2018 22:50:56 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Das", "Apurba", ""], ["Svendsen", "Michael", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1601.06319", "submitter": "Rohit Gurjar", "authors": "Stephen A. Fenner, Rohit Gurjar, Thomas Thierauf", "title": "Bipartite Perfect Matching is in quasi-NC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the bipartite perfect matching problem is in quasi-NC$^2$. That\nis, it has uniform circuits of quasi-polynomial size $n^{O(\\log n)}$, and\n$O(log^2 n)$ depth. Previously, only an exponential upper bound was known on\nthe size of such circuits with poly-logarithmic depth.\n  We obtain our result by an almost complete derandomization of the famous\nIsolation Lemma when applied to yield an efficient randomized parallel\nalgorithm for the bipartite perfect matching problem.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 22:50:22 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 17:47:47 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Fenner", "Stephen A.", ""], ["Gurjar", "Rohit", ""], ["Thierauf", "Thomas", ""]]}, {"id": "1601.06693", "submitter": "Maritza Hernandez", "authors": "Maritza Hernandez, Arman Zaribafiyan, Maliheh Aramon, and Mohammad\n  Naghibi", "title": "A Novel Graph-based Approach for Determining Molecular Similarity", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.QM quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of measuring similarity among graphs\nthat represent real objects with noisy data. To account for noise, we relax the\ndefinition of similarity using the maximum weighted co-$k$-plex relaxation\nmethod, which allows dissimilarities among graphs up to a predetermined level.\nWe then formulate the problem as a novel quadratic unconstrained binary\noptimization problem that can be solved by a quantum annealer. The context of\nour study is molecular similarity where the presence of noise might be due to\nregular errors in measuring molecular features. We develop a similarity measure\nand use it to predict the mutagenicity of a molecule. Our results indicate that\nthe relaxed similarity measure, designed to accommodate the regular errors,\nyields a higher prediction accuracy than the measure that ignores the noise.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 17:43:38 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Hernandez", "Maritza", ""], ["Zaribafiyan", "Arman", ""], ["Aramon", "Maliheh", ""], ["Naghibi", "Mohammad", ""]]}, {"id": "1601.06834", "submitter": "Toma\\v{z} Ho\\v{c}evar", "authors": "Toma\\v{z} Ho\\v{c}evar, Janez Dem\\v{s}ar", "title": "Combinatorial algorithm for counting small induced graphs and orbits", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0171428", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphlet analysis is an approach to network analysis that is particularly\npopular in bioinformatics. We show how to set up a system of linear equations\nthat relate the orbit counts and can be used in an algorithm that is\nsignificantly faster than the existing approaches based on direct enumeration\nof graphlets. The algorithm requires existence of a vertex with certain\nproperties; we show that such vertex exists for graphlets of arbitrary size,\nexcept for complete graphs and $C_4$, which are treated separately. Empirical\nanalysis of running time agrees with the theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 22:17:05 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Ho\u010devar", "Toma\u017e", ""], ["Dem\u0161ar", "Janez", ""]]}, {"id": "1601.06915", "submitter": "Bader AlBdaiwi", "authors": "Bader AlBdaiwi, Zaid Hussain, Anton Cerny, and Robert Aldred", "title": "Edge-Disjoint Node-Independent Spanning Trees in Dense Gaussian Networks", "comments": null, "journal-ref": "The Journal of Supercomputing, December 2016, Vol. 72, No. 12, pp.\n  4718-4736", "doi": "10.1007/s11227-016-1768-x", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent trees are used in building secure and/or fault-tolerant network\ncommunication protocols. They have been investigated for different network\ntopologies including tori. Dense Gaussian networks are potential alternatives\nfor 2-dimensional tori. They have similar topological properties; however, they\nare superiors in carrying communications due to their node-distance\ndistributions and smaller diameters. In this paper, we present constructions of\nedge-disjoint node-independent spanning trees in dense Gaussian networks. Based\non the constructed trees, we design algorithms that could be used in\nfault-tolerant routing or secure message distribution.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 07:57:16 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 20:09:21 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["AlBdaiwi", "Bader", ""], ["Hussain", "Zaid", ""], ["Cerny", "Anton", ""], ["Aldred", "Robert", ""]]}, {"id": "1601.06939", "submitter": "Gonzalo Navarro", "authors": "Joshimar Cordova and Gonzalo Navarro", "title": "Simple and Efficient Fully-Functional Succinct Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fully-functional succinct tree representation of Navarro and Sadakane\n(ACM Transactions on Algorithms, 2014) supports a large number of operations in\nconstant time using $2n+o(n)$ bits. However, the full idea is hard to\nimplement. Only a simplified version with $O(\\log n)$ operation time has been\nimplemented and shown to be practical and competitive. We describe a new\nvariant of the original idea that is much simpler to implement and has\nworst-case time $O(\\log\\log n)$ for the operations. An implementation based on\nthis version is experimentally shown to be superior to existing\nimplementations.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 09:19:15 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 13:26:34 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Cordova", "Joshimar", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1601.07473", "submitter": "Stephen Chestnut", "authors": "Vladimir Braverman, Stephen R. Chestnut, David P. Woodruff, Lin F.\n  Yang", "title": "Streaming Space Complexity of Nearly All Functions of One Variable on\n  Frequency Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in the theory of algorithms for data streams is to\ndetermine which functions on a stream can be approximated in sublinear, and\nespecially sub-polynomial or poly-logarithmic, space. Given a function $g$, we\nstudy the space complexity of approximating $\\sum_{i=1}^n g(|f_i|)$, where\n$f\\in\\mathbb{Z}^n$ is the frequency vector of a turnstile stream. This is a\ngeneralization of the well-known frequency moments problem, and previous\nresults apply only when $g$ is monotonic or has a special functional form. Our\ncontribution is to give a condition such that, except for a narrow class of\nfunctions $g$, there is a space-efficient approximation algorithm for the sum\nif and only if $g$ satisfies the condition. The functions $g$ that we are able\nto characterize include all convex, concave, monotonic, polynomial, and\ntrigonometric functions, among many others, and is the first such\ncharacterization for non-monotonic functions. Thus, for nearly all functions of\none variable, we answer the open question from the celebrated paper of Alon,\nMatias and Szegedy (1996).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 18:04:05 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Braverman", "Vladimir", ""], ["Chestnut", "Stephen R.", ""], ["Woodruff", "David P.", ""], ["Yang", "Lin F.", ""]]}, {"id": "1601.07518", "submitter": "Alexander Barvinok", "authors": "Alexander Barvinok", "title": "Approximating permanents and hafnians", "comments": "The article number (for \"Discrete Analysis\") is corrected", "journal-ref": "Discrete Analysis, 2017:2, 34 pp", "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove that the logarithm of the permanent of an nxn real matrix A and the\nlogarithm of the hafnian of a 2nx2n real symmetric matrix A can be approximated\nwithin an additive error 1 > epsilon > 0 by a polynomial p in the entries of A\nof degree O(ln n - ln epsilon) provided the entries a_ij of A satisfy delta <\na_ij < 1 for an arbitrarily small delta > 0, fixed in advance. Moreover, the\npolynomial p can be computed in n^{O(ln n - ln epsilon)} time. We also improve\nbounds for approximating ln per A, ln haf A and logarithms of multi-dimensional\npermanents for complex matrices and tensors A.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 19:46:45 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 17:49:10 GMT"}, {"version": "v3", "created": "Tue, 10 May 2016 18:13:26 GMT"}, {"version": "v4", "created": "Thu, 12 Jan 2017 17:14:19 GMT"}, {"version": "v5", "created": "Fri, 13 Jan 2017 15:01:10 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Barvinok", "Alexander", ""]]}, {"id": "1601.07670", "submitter": "Hideo Bannai", "authors": "Yuka Tanimura and Tomohiro I and Hideo Bannai and Shunsuke Inenaga and\n  Simon J. Puglisi and Masayuki Takeda", "title": "Deterministic sub-linear space LCE data structures with efficient\n  construction", "comments": "updated title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a string $S$ of $n$ symbols, a longest common extension query\n$\\mathsf{LCE}(i,j)$ asks for the length of the longest common prefix of the\n$i$th and $j$th suffixes of $S$. LCE queries have several important\napplications in string processing, perhaps most notably to suffix sorting.\nRecently, Bille et al. (J. Discrete Algorithms 25:42-50, 2014, Proc. CPM 2015:\n65-76) described several data structures for answering LCE queries that offers\na space-time trade-off between data structure size and query time. In\nparticular, for a parameter $1 \\leq \\tau \\leq n$, their best deterministic\nsolution is a data structure of size $O(n/\\tau)$ which allows LCE queries to be\nanswered in $O(\\tau)$ time. However, the construction time for all\ndeterministic versions of their data structure is quadratic in $n$. In this\npaper, we propose a deterministic solution that achieves a similar space-time\ntrade-off of $O(\\tau\\min\\{\\log\\tau,\\log\\frac{n}{\\tau}\\})$ query time using\n$O(n/\\tau)$ space, but significantly improve the construction time to\n$O(n\\tau)$.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 07:14:19 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 08:30:02 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Tanimura", "Yuka", ""], ["I", "Tomohiro", ""], ["Bannai", "Hideo", ""], ["Inenaga", "Shunsuke", ""], ["Puglisi", "Simon J.", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1601.08051", "submitter": "Tomasz Kociumaka", "authors": "Tomasz Kociumaka", "title": "Minimal Suffix and Rotation of a Substring in Optimal Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a text given in advance, the substring minimal suffix queries ask to\ndetermine the lexicographically minimal non-empty suffix of a substring\nspecified by the location of its occurrence in the text. We develop a data\nstructure answering such queries optimally: in constant time after linear-time\npreprocessing. This improves upon the results of Babenko et al. (CPM 2014),\nwhose trade-off solution is characterized by $\\Theta(n\\log n)$ product of these\ntime complexities. Next, we extend our queries to support concatenations of\n$O(1)$ substrings, for which the construction and query time is preserved. We\napply these generalized queries to compute lexicographically minimal and\nmaximal rotations of a given substring in constant time after linear-time\npreprocessing.\n  Our data structures mainly rely on properties of Lyndon words and Lyndon\nfactorizations. We combine them with further algorithmic and combinatorial\ntools, such as fusion trees and the notion of order isomorphism of strings.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 10:58:24 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Kociumaka", "Tomasz", ""]]}, {"id": "1601.08067", "submitter": "Zhuolun Xiang", "authors": "Zhuolun Xiang, Nitin H.Vaidya", "title": "Relaxed Byzantine Vector Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact Byzantine consensus problem requires that non-faulty processes reach\nagreement on a decision (or output) that is in the convex hull of the inputs at\nthe non-faulty processes. It is well-known that exact consensus is impossible\nin an asynchronous system in presence of faults, and in a synchronous system,\nn>=3f+1 is tight on the number of processes to achieve exact Byzantine\nconsensus with scalar inputs, in presence of up to f Byzantine faulty\nprocesses. Recent work has shown that when the inputs are d-dimensional vectors\nof reals, n>=max(3f+1,(d+1)f+1) is tight to achieve exact Byzantine consensus\nin synchronous systems, and n>= (d+2)f+1 for approximate Byzantine consensus in\nasynchronous systems.\n  Due to the dependence of the lower bound on vector dimension d, the number of\nprocesses necessary becomes large when the vector dimension is large. With the\nhope of reducing the lower bound on n, we consider two relaxed versions of\nByzantine vector consensus: k-Relaxed Byzantine vector consensus and\n(delta,p)-Relaxed Byzantine vector consensus. In k-relaxed consensus, the\nvalidity condition requires that the output must be in the convex hull of\nprojection of the inputs onto any subset of k-dimensions of the vectors. For\n(delta,p)-consensus the validity condition requires that the output must be\nwithin distance delta of the convex hull of the inputs of the non-faulty\nprocesses, where L_p norm is used as the distance metric. For\n(delta,p)-consensus, we consider two versions: in one version, delta is a\nconstant, and in the second version, delta is a function of the inputs\nthemselves.\n  We show that for k-relaxed consensus and (delta,p)-consensus with constant\ndelta>=0, the bound on n is identical to the bound stated above for the\noriginal vector consensus problem. On the other hand, when delta depends on the\ninputs, we show that the bound on n is smaller when d>=3.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 11:53:24 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Xiang", "Zhuolun", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1601.08111", "submitter": "Martin B\\\"ohm", "authors": "Martin B\\\"ohm, Ji\\v{r}\\'i Sgall, Rob van Stee, Pavel Vesel\\'y", "title": "A Two-Phase Algorithm for Bin Stretching with Stretching Factor 1.5", "comments": "Preprint of a journal version; updated title and some minor edits.\n  The conference version can be found at arXiv:1404.5569 version 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Bin Stretching is a semi-online variant of bin packing in which the\nalgorithm has to use the same number of bins as an optimal packing, but is\nallowed to slightly overpack the bins. The goal is to minimize the amount of\noverpacking, i.e., the maximum size packed into any bin.\n  We give an algorithm for Online Bin Stretching with a stretching factor of\n1.5 for any number of bins. We build on previous algorithms and use a two-phase\napproach. However, our analysis is technically more complicated and uses\namortization over the bins with the help of two weight functions.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 14:04:12 GMT"}, {"version": "v2", "created": "Sat, 20 Aug 2016 12:43:42 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["B\u00f6hm", "Martin", ""], ["Sgall", "Ji\u0159\u00ed", ""], ["van Stee", "Rob", ""], ["Vesel\u00fd", "Pavel", ""]]}, {"id": "1601.08189", "submitter": "Jiahao Chen", "authors": "Jiahao Chen and Weijian Zhang", "title": "The Right Way to Search Evolving Graphs", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolving graphs arise in problems where interrelations between data change\nover time. We present a breadth first search (BFS) algorithm for evolving\ngraphs that computes the most direct influences between nodes at two different\ntimes. Using simple examples, we show that naive unfoldings of adjacency\nmatrices miscount the number of temporal paths. By mapping an evolving graph to\nan adjacency matrix of an equivalent static graph, we prove that our\ngeneralization of the BFS algorithm correctly accounts for paths that traverse\nboth space and time. Finally, we demonstrate how the BFS over evolving graphs\ncan be applied to mine citation networks.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 16:48:55 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 20:24:02 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Chen", "Jiahao", ""], ["Zhang", "Weijian", ""]]}]