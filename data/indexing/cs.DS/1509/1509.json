[{"id": "1509.00092", "submitter": "Raghu Meka", "authors": "Raghu Meka", "title": "Explicit resilient functions matching Ajtai-Linial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Boolean function on n variables is q-resilient if for any subset of at most\nq variables, the function is very likely to be determined by a uniformly random\nassignment to the remaining n-q variables; in other words, no coalition of at\nmost q variables has significant influence on the function. Resilient functions\nhave been extensively studied with a variety of applications in cryptography,\ndistributed computing, and pseudorandomness. The best known balanced resilient\nfunction on n variables due to Ajtai and Linial ([AL93]) is Omega(n/(log^2\nn))-resilient. However, the construction of Ajtai and Linial is by the\nprobabilistic method and does not give an efficiently computable function.\n  In this work we give an explicit monotone depth three almost-balanced Boolean\nfunction on n bits that is Omega(n/(log^2 n))-resilient matching the work of\nAjtai and Linial. The best previous explicit construction due to Meka [Meka09]\n(which only gives a logarithmic depth function) and Chattopadhyay and\nZuckermman [CZ15] were only n^{1-c}-resilient for any constant c < 1. Our\nconstruction and analysis are motivated by (and simplifies parts of) the recent\nbreakthrough of [CZ15] giving explicit two-sources extractors for\npolylogarithmic min-entropy; a key ingredient in their result was the\nconstruction of explicit constant-depth resilient functions.\n  An important ingredient in our construction is a new randomness optimal\noblivious sampler which preserves moment generating functions of sums of\nvariables and could be useful elsewhere.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 23:27:06 GMT"}, {"version": "v2", "created": "Sun, 17 Apr 2016 21:03:47 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Meka", "Raghu", ""]]}, {"id": "1509.00099", "submitter": "T\\'omas Magn\\'usson", "authors": "Bjarki \\'Ag\\'ust Gu{\\dh}mundsson, T\\'omas Ken Magn\\'usson, Bj\\\"orn\n  Orri S{\\ae}mundsson", "title": "Bounds and Fixed-Parameter Algorithms for Weighted Improper Coloring\n  (Extended Version)", "comments": "18 pages, 5 figures, extended version for additional proofs in\n  appendix for 16th Italian Conference on Theoretical Computer Science 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the weighted improper coloring problem, a generalization of\ndefective coloring. We present some hardness results and in particular we show\nthat weighted improper coloring is not fixed-parameter tractable when\nparameterized by pathwidth. We generalize bounds for defective coloring to\nweighted improper coloring and give a bound for weighted improper coloring in\nterms of the sum of edge weights. Finally we give fixed-parameter algorithms\nfor weighted improper coloring both when parameterized by treewidth and maximum\ndegree and when parameterized by treewidth and precision of edge weights. In\nparticular, we obtain a linear-time algorithm for weighted improper coloring of\ninterval graphs of bounded degree.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 00:15:32 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Gu\u00f0mundsson", "Bjarki \u00c1g\u00fast", ""], ["Magn\u00fasson", "T\u00f3mas Ken", ""], ["S\u00e6mundsson", "Bj\u00f6rn Orri", ""]]}, {"id": "1509.00118", "submitter": "Ali Vakilian", "authors": "Sariel Har-Peled and Piotr Indyk and Sepideh Mahabadi and Ali Vakilian", "title": "Towards Tight Bounds for the Streaming Set Cover Problem", "comments": "A preliminary version of this paper is to appear in PODS 2016", "journal-ref": null, "doi": "10.1145/2902251.2902287", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classic Set Cover problem in the data stream model. For $n$\nelements and $m$ sets ($m\\geq n$) we give a $O(1/\\delta)$-pass algorithm with a\nstrongly sub-linear $\\tilde{O}(mn^{\\delta})$ space and logarithmic\napproximation factor. This yields a significant improvement over the earlier\nalgorithm of Demaine et al. [DIMV14] that uses exponentially larger number of\npasses. We complement this result by showing that the tradeoff between the\nnumber of passes and space exhibited by our algorithm is tight, at least when\nthe approximation factor is equal to $1$. Specifically, we show that any\nalgorithm that computes set cover exactly using $({1 \\over 2\\delta}-1)$ passes\nmust use $\\tilde{\\Omega}(mn^{\\delta})$ space in the regime of $m=O(n)$.\nFurthermore, we consider the problem in the geometric setting where the\nelements are points in $\\mathbb{R}^2$ and sets are either discs, axis-parallel\nrectangles, or fat triangles in the plane, and show that our algorithm (with a\nslight modification) uses the optimal $\\tilde{O}(n)$ space to find a\nlogarithmic approximation in $O(1/\\delta)$ passes.\n  Finally, we show that any randomized one-pass algorithm that distinguishes\nbetween covers of size 2 and 3 must use a linear (i.e., $\\Omega(mn)$) amount of\nspace. This is the first result showing that a randomized, approximate\nalgorithm cannot achieve a space bound that is sublinear in the input size.\n  This indicates that using multiple passes might be necessary in order to\nachieve sub-linear space bounds for this problem while guaranteeing small\napproximation factors.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 02:16:50 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 16:58:19 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Har-Peled", "Sariel", ""], ["Indyk", "Piotr", ""], ["Mahabadi", "Sepideh", ""], ["Vakilian", "Ali", ""]]}, {"id": "1509.00442", "submitter": "Darren Strash", "authors": "Irina Kostitsyna, Martin N\\\"ollenburg, Valentin Polishchuk, Andr\\'e\n  Schulz, and Darren Strash", "title": "On Minimizing Crossings in Storyline Visualizations", "comments": "6 pages, 4 figures. To appear at the 23rd International Symposium on\n  Graph Drawing and Network Visualization (GD 2015)", "journal-ref": null, "doi": "10.1007/978-3-319-27261-0_16", "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a storyline visualization, we visualize a collection of interacting\ncharacters (e.g., in a movie, play, etc.) by $x$-monotone curves that converge\nfor each interaction, and diverge otherwise. Given a storyline with $n$\ncharacters, we show tight lower and upper bounds on the number of crossings\nrequired in any storyline visualization for a restricted case. In particular,\nwe show that if (1) each meeting consists of exactly two characters and (2) the\nmeetings can be modeled as a tree, then we can always find a storyline\nvisualization with $O(n\\log n)$ crossings. Furthermore, we show that there\nexist storylines in this restricted case that require $\\Omega(n\\log n)$\ncrossings. Lastly, we show that, in the general case, minimizing the number of\ncrossings in a storyline visualization is fixed-parameter tractable, when\nparameterized on the number of characters $k$. Our algorithm runs in time\n$O(k!^2k\\log k + k!^2m)$, where $m$ is the number of meetings.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 19:03:34 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Kostitsyna", "Irina", ""], ["N\u00f6llenburg", "Martin", ""], ["Polishchuk", "Valentin", ""], ["Schulz", "Andr\u00e9", ""], ["Strash", "Darren", ""]]}, {"id": "1509.00630", "submitter": "Pierre-Louis Poirion", "authors": "Ky Vu, Pierre-Louis Poirion, Leo Liberti", "title": "Gaussian random projections for Euclidean membership problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the application of random projections to the fundamental problem\nof deciding whether a given point in a Euclidean space belongs to a given set.\nWe show that, under a number of different assumptions, the feasibility and\ninfeasibility of this problem are preserved with high probability when the\nproblem data is projected to a lower dimensional space. Our results are\napplicable to any algorithmic setting which needs to solve Euclidean membership\nproblems in a high-dimensional space.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 10:23:23 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 12:45:00 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Vu", "Ky", ""], ["Poirion", "Pierre-Louis", ""], ["Liberti", "Leo", ""]]}, {"id": "1509.00669", "submitter": "Ross Atkins", "authors": "Ross Atkins and Colin McDiarmid", "title": "Extremal Distances for Subtree Transfer Operations in Binary Trees", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three standard subtree transfer operations for binary trees, used in\nparticular for phylogenetic trees, are: tree bisection and reconnection\n($TBR$), subtree prune and regraft ($SPR$) and rooted subtree prune and regraft\n($rSPR$). For a pair of leaf-labelled binary trees with $n$ leaves, the maximum\nnumber of such moves required to transform one into the other is\n$n-\\Theta(\\sqrt{n})$, extending a result of Ding, Grunewald and Humphries. We\nshow that if the pair is chosen uniformly at random, then the expected number\nof moves required to transfer one into the other is $n-\\Theta(n^{2/3})$. These\nresults may be phrased in terms of agreement forests: we also give extensions\nfor more than two binary trees.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 12:49:53 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Atkins", "Ross", ""], ["McDiarmid", "Colin", ""]]}, {"id": "1509.00684", "submitter": "Giordano Da Lozzo", "authors": "Patrizio Angelini, Giordano Da Lozzo, Marco Di Bartolomeo, Valentino\n  Di Donato, Maurizio Patrignani, Vincenzo Roselli, Ioannis G. Tollis", "title": "L-Drawings of Directed Graphs", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce L-drawings, a novel paradigm for representing directed graphs\naiming at combining the readability features of orthogonal drawings with the\nexpressive power of matrix representations. In an L-drawing, vertices have\nexclusive $x$- and $y$-coordinates and edges consist of two segments, one\nexiting the source vertically and one entering the destination horizontally.\n  We study the problem of computing L-drawings using minimum ink. We prove its\nNP-completeness and provide a heuristics based on a polynomial-time algorithm\nthat adds a vertex to a drawing using the minimum additional ink. We performed\nan experimental analysis of the heuristics which confirms its effectiveness.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 13:19:58 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Angelini", "Patrizio", ""], ["Da Lozzo", "Giordano", ""], ["Di Bartolomeo", "Marco", ""], ["Di Donato", "Valentino", ""], ["Patrignani", "Maurizio", ""], ["Roselli", "Vincenzo", ""], ["Tollis", "Ioannis G.", ""]]}, {"id": "1509.00757", "submitter": "Dimitrios Thilikos", "authors": "Petr A. Golovach and Cl\\'ement Requil\\'e and Dimitrios M. Thilikos", "title": "Variants of Plane Diameter Completion", "comments": "Accepted in IPEC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The {\\sc Plane Diameter Completion} problem asks, given a plane graph $G$ and\na positive integer $d$, if it is a spanning subgraph of a plane graph $H$ that\nhas diameter at most $d$. We examine two variants of this problem where the\ninput comes with another parameter $k$. In the first variant, called BPDC, $k$\nupper bounds the total number of edges to be added and in the second, called\nBFPDC, $k$ upper bounds the number of additional edges per face. We prove that\nboth problems are {\\sf NP}-complete, the first even for 3-connected graphs of\nface-degree at most 4 and the second even when $k=1$ on 3-connected graphs of\nface-degree at most 5. In this paper we give parameterized algorithms for both\nproblems that run in $O(n^{3})+2^{2^{O((kd)^2\\log d)}}\\cdot n$ steps.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 15:52:35 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Golovach", "Petr A.", ""], ["Requil\u00e9", "Cl\u00e9ment", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1509.00764", "submitter": "Darren Strash", "authors": "Sebastian Lamm, Peter Sanders, Christian Schulz, Darren Strash, and\n  Renato F. Werneck", "title": "Finding Near-Optimal Independent Sets at Scale", "comments": "17 pages, 1 figure, 8 tables. arXiv admin note: text overlap with\n  arXiv:1502.01687", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The independent set problem is NP-hard and particularly difficult to solve in\nlarge sparse graphs. In this work, we develop an advanced evolutionary\nalgorithm, which incorporates kernelization techniques to compute large\nindependent sets in huge sparse networks. A recent exact algorithm has shown\nthat large networks can be solved exactly by employing a branch-and-reduce\ntechnique that recursively kernelizes the graph and performs branching.\nHowever, one major drawback of their algorithm is that, for huge graphs,\nbranching still can take exponential time. To avoid this problem, we\nrecursively choose vertices that are likely to be in a large independent set\n(using an evolutionary approach), then further kernelize the graph. We show\nthat identifying and removing vertices likely to be in large independent sets\nopens up the reduction space---which not only speeds up the computation of\nlarge independent sets drastically, but also enables us to compute high-quality\nindependent sets on much larger instances than previously reported in the\nliterature.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 16:04:59 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Lamm", "Sebastian", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""], ["Strash", "Darren", ""], ["Werneck", "Renato F.", ""]]}, {"id": "1509.00824", "submitter": "Afonso S. Bandeira", "authors": "Afonso S. Bandeira", "title": "A note on Probably Certifiably Correct algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many optimization problems of interest are known to be intractable, and while\nthere are often heuristics that are known to work on typical instances, it is\nusually not easy to determine a posteriori whether the optimal solution was\nfound. In this short note, we discuss algorithms that not only solve the\nproblem on typical instances, but also provide a posteriori certificates of\noptimality, probably certifiably correct (PCC) algorithms. As an illustrative\nexample, we present a fast PCC algorithm for minimum bisection under the\nstochastic block model and briefly discuss other examples.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 19:10:00 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Bandeira", "Afonso S.", ""]]}, {"id": "1509.00864", "submitter": "Jonathan Sorenson", "authors": "Jonathan P. Sorenson and Jonathan Webster", "title": "Strong Pseudoprimes to Twelve Prime Bases", "comments": null, "journal-ref": null, "doi": "10.1090/mcom/3134", "report-no": null, "categories": "math.NT cs.DS cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\psi_m$ be the smallest strong pseudoprime to the first $m$ prime bases.\nThis value is known for $1 \\leq m \\leq 11$. We extend this by finding\n$\\psi_{12}$ and $\\psi_{13}$. We also present an algorithm to find all integers\n$n\\le B$ that are strong pseudoprimes to the first $m$ prime bases; with a\nreasonable heuristic assumption we can show that it takes at most\n$B^{2/3+o(1)}$ time.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 20:23:07 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Sorenson", "Jonathan P.", ""], ["Webster", "Jonathan", ""]]}, {"id": "1509.00930", "submitter": "Yuichi Yoshida", "authors": "Kenta Oono and Yuichi Yoshida", "title": "Testing Properties of Functions on Finite Groups", "comments": "Accepted to Random Structures and Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study testing properties of functions on finite groups. First we consider\nfunctions of the form $f:G \\to \\mathbb{C}$, where $G$ is a finite group. We\nshow that conjugate invariance, homomorphism, and the property of being\nproportional to an irreducible character is testable with a constant number of\nqueries to $f$, where a character is a crucial notion in representation theory.\nOur proof relies on representation theory and harmonic analysis on finite\ngroups. Next we consider functions of the form $f: G \\to M_d(\\mathbb{C})$,\nwhere $d$ is a fixed constant and $M_d(\\mathbb{C})$ is the family of $d$ by $d$\nmatrices with each element in $\\mathbb{C}$. For a function $g:G \\to\nM_d(\\mathbb{C})$, we show that the unitary isomorphism to $g$ is testable with\na constant number of queries to $f$, where we say that $f$ and $g$ are unitary\nisomorphic if there exists a unitary matrix $U$ such that $f(x) = Ug(x)U^{-1}$\nfor any $x \\in G$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 03:27:55 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Oono", "Kenta", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1509.01014", "submitter": "Yoichi Iwata", "authors": "Yoichi Iwata and Yuichi Yoshida", "title": "On the Equivalence among Problems of Bounded Width", "comments": "accepted to ESA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a methodology, called decomposition-based\nreductions, for showing the equivalence among various problems of\nbounded-width.\n  First, we show that the following are equivalent for any $\\alpha > 0$:\n  * SAT can be solved in $O^*(2^{\\alpha \\mathrm{tw}})$ time,\n  * 3-SAT can be solved in $O^*(2^{\\alpha \\mathrm{tw}})$ time,\n  * Max 2-SAT can be solved in $O^*(2^{\\alpha \\mathrm{tw}})$ time,\n  * Independent Set can be solved in $O^*(2^{\\alpha \\mathrm{tw}})$ time, and\n  * Independent Set can be solved in $O^*(2^{\\alpha \\mathrm{cw}})$ time, where\ntw and cw are the tree-width and clique-width of the instance, respectively.\n  Then, we introduce a new parameterized complexity class EPNL, which includes\nSet Cover and Directed Hamiltonicity, and show that SAT, 3-SAT, Max 2-SAT, and\nIndependent Set parameterized by path-width are EPNL-complete. This implies\nthat if one of these EPNL-complete problems can be solved in $O^*(c^k)$ time,\nthen any problem in EPNL can be solved in $O^*(c^k)$ time.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 09:58:50 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Iwata", "Yoichi", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1509.01018", "submitter": "Dmitry Kosolobov", "authors": "Dmitry Kosolobov", "title": "Finding the Leftmost Critical Factorization on Unordered Alphabet", "comments": "13 pages, 13 figures (accepted to Theor. Comp. Sci.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a linear time and space algorithm computing the leftmost critical\nfactorization of a given string on an unordered alphabet.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 10:11:12 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 19:52:42 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Kosolobov", "Dmitry", ""]]}, {"id": "1509.01190", "submitter": "Christian Schulz", "authors": "Peter Sanders and Christian Schulz", "title": "Advanced Multilevel Node Separator Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A node separator of a graph is a subset S of the nodes such that removing S\nand its incident edges divides the graph into two disconnected components of\nabout equal size. In this work, we introduce novel algorithms to find small\nnode separators in large graphs. With focus on solution quality, we introduce\nnovel flow-based local search algorithms which are integrated in a multilevel\nframework. In addition, we transfer techniques successfully used in the graph\npartitioning field. This includes the usage of edge ratings tailored to our\nproblem to guide the graph coarsening algorithm as well as highly localized\nlocal search and iterated multilevel cycles to improve solution quality even\nfurther. Experiments indicate that flow-based local search algorithms on its\nown in a multilevel framework are already highly competitive in terms of\nseparator quality. Adding additional local search algorithms further improves\nsolution quality. Our strongest configuration almost always outperforms\ncompeting systems while on average computing 10% and 62% smaller separators\nthan Metis and Scotch, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 18:10:17 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1509.01630", "submitter": "Yael Mordechai", "authors": "Yael Mordechai", "title": "Optimization and Reoptimization in Scheduling Problems", "comments": "Thesis work. arXiv admin note: text overlap with arXiv:1311.4021 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel machine scheduling has been extensively studied in the past decades,\nwith applications ranging from production planning to job processing in large\ncomputing clusters. In this work we study some of these fundamental\noptimization problems, as well as their parameterized and reoptimization\nvariants.\n  We first present improved bounds for job scheduling on unrelated parallel\nmachines, with the objective of minimizing the latest completion time\n(makespan) of the schedule. We consider the subclass of fully-feasible\ninstances, in which the processing time of each job, on any machine, does not\nexceed the minimum makespan. The problem is known to be hard to approximate\nwithin factor 4/3 already in this subclass. Although fully-feasible instances\nare hard to identify, we give a polynomial time algorithm that yields for such\ninstances a schedule whose makespan is better than twice the optimal, the best\nknown ratio for general instances. Moreover, we show that our result is robust\nunder small violations of feasibility constraints.\n  We further study the power of parameterization. We show that makespan\nminimization on unrelated machines admits a parameterized approximation scheme,\nwhere the parameter used is the number of processing times that are large\nrelative to the latest completion time of the schedule. We also present an FPT\nalgorithm for the graph-balancing problem, which corresponds to the instances\nof the restricted assignment problem where each job can be processed on at most\n2 machines.\n  Finally, motivated by practical scenarios, we initiate the study of\nreoptimization in job scheduling on identical and uniform machines, with the\nobjective of minimizing the makespan. We develop reapproximation algorithms\nthat yield in both models the best possible approximation ratio of\n$(1+\\epsilon)$, for any $\\epsilon >0$, with respect to the minimum makespan.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 23:03:47 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Mordechai", "Yael", ""]]}, {"id": "1509.01675", "submitter": "Lukasz Kowalik", "authors": "Marthe Bonamy, {\\L}ukasz Kowalik, Micha{\\l} Pilipczuk, Arkadiusz\n  Soca{\\l}a", "title": "Linear kernels for outbranching problems in sparse digraphs", "comments": "Extended abstract accepted for IPEC'15, 27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $k$-Leaf Out-Branching and $k$-Internal Out-Branching problems we are\ngiven a directed graph $D$ with a designated root $r$ and a nonnegative integer\n$k$. The question is to determine the existence of an outbranching rooted at\n$r$ that has at least $k$ leaves, or at least $k$ internal vertices,\nrespectively. Both these problems were intensively studied from the points of\nview of parameterized complexity and kernelization, and in particular for both\nof them kernels with $O(k^2)$ vertices are known on general graphs. In this\nwork we show that $k$-Leaf Out-Branching admits a kernel with $O(k)$ vertices\non $\\mathcal{H}$-minor-free graphs, for any fixed family of graphs\n$\\mathcal{H}$, whereas $k$-Internal Out-Branching admits a kernel with $O(k)$\nvertices on any graph class of bounded expansion.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2015 08:24:41 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Bonamy", "Marthe", ""], ["Kowalik", "\u0141ukasz", ""], ["Pilipczuk", "Micha\u0142", ""], ["Soca\u0142a", "Arkadiusz", ""]]}, {"id": "1509.01844", "submitter": "Arnold Filtser", "authors": "Arnold Filtser and Robert Krauthgamer", "title": "Sparsification of Two-Variable Valued CSPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A valued constraint satisfaction problem (VCSP) instance $(V,\\Pi,w)$ is a set\nof variables $V$ with a set of constraints $\\Pi$ weighted by $w$. Given a VCSP\ninstance, we are interested in a re-weighted sub-instance $(V,\\Pi'\\subset\n\\Pi,w')$ such that preserves the value of the given instance (under every\nassignment to the variables) within factor $1\\pm\\epsilon$. A well-studied\nspecial case is cut sparsification in graphs, which has found various\napplications.\n  We show that a VCSP instance consisting of a single boolean predicate\n$P(x,y)$ (e.g., for cut, $P=\\mbox{XOR}$) can be sparsified into\n$O(|V|/\\epsilon^2)$ constraints if and only if the number of inputs that\nsatisfy $P$ is anything but one (i.e., $|P^{-1}(1)| \\neq 1$). Furthermore, this\nsparsity bound is tight unless $P$ is a relatively trivial predicate. We\nconclude that also systems of 2SAT (or 2LIN) constraints can be sparsified.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2015 19:24:24 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Filtser", "Arnold", ""], ["Krauthgamer", "Robert", ""]]}, {"id": "1509.01866", "submitter": "Richard Taylor Dr", "authors": "Richard Taylor", "title": "Approximation of the Quadratic Knapsack Problem", "comments": "8 pages one figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any given $\\epsilon>0$ we provide an algorithm for the Quadratic Knapsack\nProblem that has an approximation ratio within $O(n^{2/5+\\epsilon})$ and a run\ntime within $O(n^{9/\\epsilon})$.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2015 23:21:39 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 01:12:25 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Taylor", "Richard", ""]]}, {"id": "1509.01867", "submitter": "Attila De\\'ak", "authors": "Endre Cs\\'oka, Attila De\\'ak", "title": "A macro placer algorithm for chip design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a set of rectangular macros with given dimensions, and there are\nwires connecting some pairs (or sets) of them. We have a placement area where\nthese macros should be placed without overlaps in order to minimize the total\nlength of wires. We present a heuristic algorithm which utilizes a special data\nstructure for representing two dimensional stepfunctions. This results in fast\nintegral computation and function modification over rectangles. Our heuristics,\nespecially our data structure for two-dimensional functions, may be useful in\nother applications, as well.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2015 23:33:23 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Cs\u00f3ka", "Endre", ""], ["De\u00e1k", "Attila", ""]]}, {"id": "1509.01988", "submitter": "Nikos Leonardos", "authors": "Varun Kanade, Nikos Leonardos and Fr\\'ed\\'eric Magniez", "title": "Stable Matching with Evolving Preferences", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of stable matching with dynamic preference lists. At\neach time step, the preference list of some player may change by swapping\nrandom adjacent members. The goal of a central agency (algorithm) is to\nmaintain an approximately stable matching (in terms of number of blocking\npairs) at all times. The changes in the preference lists are not reported to\nthe algorithm, but must instead be probed explicitly by the algorithm. We\ndesign an algorithm that in expectation and with high probability maintains a\nmatching that has at most $O((log (n))^2)$ blocking pairs.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 11:28:42 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 10:27:06 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Kanade", "Varun", ""], ["Leonardos", "Nikos", ""], ["Magniez", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1509.01990", "submitter": "Moritz von Looz-Corswarem", "authors": "Moritz von Looz and Henning Meyerhenke", "title": "Querying Probabilistic Neighborhoods in Spatial Data Sets Efficiently", "comments": "The final publication is available at Springer via\n  http://dx.doi.org/10.1007/978-3-319-44543-4_35", "journal-ref": "LNCS 9843 (2016), pp 449-460", "doi": "10.1007/978-3-319-44543-4_35", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $\\newcommand{\\dist}{\\operatorname{dist}}$ In this paper we define the notion\nof a probabilistic neighborhood in spatial data: Let a set $P$ of $n$ points in\n$\\mathbb{R}^d$, a query point $q \\in \\mathbb{R}^d$, a distance metric $\\dist$,\nand a monotonically decreasing function $f : \\mathbb{R}^+ \\rightarrow [0,1]$ be\ngiven. Then a point $p \\in P$ belongs to the probabilistic neighborhood $N(q,\nf)$ of $q$ with respect to $f$ with probability $f(\\dist(p,q))$. We envision\napplications in facility location, sensor networks, and other scenarios where a\nconnection between two entities becomes less likely with increasing distance. A\nstraightforward query algorithm would determine a probabilistic neighborhood in\n$\\Theta(n\\cdot d)$ time by probing each point in $P$.\n  To answer the query in sublinear time for the planar case, we augment a\nquadtree suitably and design a corresponding query algorithm. Our theoretical\nanalysis shows that -- for certain distributions of planar $P$ -- our algorithm\nanswers a query in $O((|N(q,f)| + \\sqrt{n})\\log n)$ time with high probability\n(whp). This matches up to a logarithmic factor the cost induced by\nquadtree-based algorithms for deterministic queries and is asymptotically\nfaster than the straightforward approach whenever $|N(q,f)| \\in o(n / \\log n)$.\n  As practical proofs of concept we use two applications, one in the Euclidean\nand one in the hyperbolic plane. In particular, our results yield the first\ngenerator for random hyperbolic graphs with arbitrary temperatures in\nsubquadratic time. Moreover, our experimental data show the usefulness of our\nalgorithm even if the point distribution is unknown or not uniform: The running\ntime savings over the pairwise probing approach constitute at least one order\nof magnitude already for a modest number of points and queries.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 11:31:09 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 14:09:45 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["von Looz", "Moritz", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1509.02140", "submitter": "Miguel Mosteiro", "authors": "Alessia Milani and Miguel A. Mosteiro", "title": "A Faster Counting Protocol for Anonymous Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of counting the number of nodes in a slotted-time\ncommunication network, under the challenging assumption that nodes do not have\nidentifiers and the network topology changes frequently. That is, for each time\nslot links among nodes can change arbitrarily provided that the network is\nalways connected. Tolerating dynamic topologies is crucial in face of mobility\nand unreliable communication whereas, even if identifiers are available, it\nmight be convenient to ignore them in massive networks with changing topology.\nCounting is a fundamental task in distributed computing since knowing the size\nof the system often facilitates the design of solutions for more complex\nproblems. Currently, the best upper bound proved on the running time to compute\nthe exact network size is double-exponential. However, only linear complexity\nlower bounds are known, leaving open the question of whether efficient Counting\nprotocols for Anonymous Dynamic Networks exist or not. In this paper we make a\nsignificant step towards answering this question by presenting a distributed\nCounting protocol for Anonymous Dynamic Networks which has exponential time\ncomplexity. Our algorithm ensures that eventually every node knows the exact\nsize of the system and stops executing the algorithm. Previous Counting\nprotocols have either double-exponential time complexity, or they are\nexponential but do not terminate, or terminate but do not provide running-time\nguarantees, or guarantee only an exponential upper bound on the network size.\nOther protocols are heuristic and do not guarantee the correct count.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 18:46:18 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Milani", "Alessia", ""], ["Mosteiro", "Miguel A.", ""]]}, {"id": "1509.02235", "submitter": "Anton Malakhov", "authors": "Anton Malakhov", "title": "Per-bucket concurrent rehashing algorithms", "comments": "The author is one of core developers of Intel Threading Building\n  Blocks (TBB) and the algorithm discussed in the paper is implemented as\n  tbb::concurrent_hash_map since TBB version 2.2. The paper compares it with\n  tbb::concurrent_hash_map implementation available in TBB 2.1. This paper was\n  written in 2011 for the last of 3 attempts to be accepted for a conference\n  (PPoPP10, PPoPP11, and SPAA11)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a generic algorithm for concurrent resizing and\non-demand per-bucket rehashing for an extensible hash table. In contrast to\nknown lock-based hash table algorithms, the proposed algorithm separates the\nresizing and rehashing stages so that they neither invalidate existing buckets\nnor block any concurrent operations. Instead, the rehashing work is deferred\nand split across subsequent operations with the table. The rehashing operation\nuses bucket-level synchronization only and therefore allows a race condition\nbetween lookup and moving operations running in different threads. Instead of\nusing explicit synchronization, the algorithm detects the race condition and\nrestarts the lookup operation. In comparison with other lock-based algorithms,\nthe proposed algorithm reduces high-level synchronization on the hot path,\nimproving performance, concurrency, and scalability of the table. The response\ntime of the operations is also more predictable. The algorithm is compatible\nwith cache friendly data layouts for buckets and does not depend on any memory\nreclamation techniques thus potentially achieving additional performance gain\nwith corresponding implementations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 00:59:04 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Malakhov", "Anton", ""]]}, {"id": "1509.02374", "submitter": "Ashley Montanaro", "authors": "Ashley Montanaro", "title": "Quantum walk speedup of backtracking algorithms", "comments": "23 pages; v2: minor changes to presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a general method to obtain quantum speedups of classical\nalgorithms which are based on the technique of backtracking, a standard\napproach for solving constraint satisfaction problems (CSPs). Backtracking\nalgorithms explore a tree whose vertices are partial solutions to a CSP in an\nattempt to find a complete solution. Assume there is a classical backtracking\nalgorithm which finds a solution to a CSP on n variables, or outputs that none\nexists, and whose corresponding tree contains T vertices, each vertex\ncorresponding to a test of a partial solution. Then we show that there is a\nbounded-error quantum algorithm which completes the same task using O(sqrt(T)\nn^(3/2) log n) tests. In particular, this quantum algorithm can be used to\nspeed up the DPLL algorithm, which is the basis of many of the most efficient\nSAT solvers used in practice. The quantum algorithm is based on the use of a\nquantum walk algorithm of Belovs to search in the backtracking tree. We also\ndiscuss how, for certain distributions on the inputs, the algorithm can lead to\nan exponential reduction in expected runtime.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 14:02:43 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2016 16:59:50 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Montanaro", "Ashley", ""]]}, {"id": "1509.02487", "submitter": "Ahmad Mahmoody", "authors": "Ahmad Mahmoody, Evgenios M. Kornaropoulos, and Eli Upfal", "title": "Optimizing Static and Adaptive Probing Schedules for Rapid Event\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate and study a fundamental search and detection problem, Schedule\nOptimization, motivated by a variety of real-world applications, ranging from\nmonitoring content changes on the web, social networks, and user activities to\ndetecting failure on large systems with many individual machines.\n  We consider a large system consists of many nodes, where each node has its\nown rate of generating new events, or items. A monitoring application can probe\na small number of nodes at each step, and our goal is to compute a probing\nschedule that minimizes the expected number of undiscovered items at the\nsystem, or equivalently, minimizes the expected time to discover a new item in\nthe system.\n  We study the Schedule Optimization problem both for deterministic and\nrandomized memoryless algorithms. We provide lower bounds on the cost of an\noptimal schedule and construct close to optimal schedules with rigorous\nmathematical guarantees. Finally, we present an adaptive algorithm that starts\nwith no prior information on the system and converges to the optimal memoryless\nalgorithms by adapting to observed data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 18:28:24 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 02:22:51 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Mahmoody", "Ahmad", ""], ["Kornaropoulos", "Evgenios M.", ""], ["Upfal", "Eli", ""]]}, {"id": "1509.02533", "submitter": "Michael Mathioudakis", "authors": "Charalampos Mavroforakis, Michael Mathioudakis and Aristides Gionis", "title": "Absorbing random-walk centrality: Theory and algorithms", "comments": "11 pages, 11 figures, short paper to appear at ICDM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a new notion of graph centrality based on absorbing random walks.\nGiven a graph $G=(V,E)$ and a set of query nodes $Q\\subseteq V$, we aim to\nidentify the $k$ most central nodes in $G$ with respect to $Q$. Specifically,\nwe consider central nodes to be absorbing for random walks that start at the\nquery nodes $Q$. The goal is to find the set of $k$ central nodes that\nminimizes the expected length of a random walk until absorption. The proposed\nmeasure, which we call $k$ absorbing random-walk centrality, favors diverse\nsets, as it is beneficial to place the $k$ absorbing nodes in different parts\nof the graph so as to \"intercept\" random walks that start from different query\nnodes.\n  Although similar problem definitions have been considered in the literature,\ne.g., in information-retrieval settings where the goal is to diversify\nweb-search results, in this paper we study the problem formally and prove some\nof its properties. We show that the problem is NP-hard, while the objective\nfunction is monotone and supermodular, implying that a greedy algorithm\nprovides solutions with an approximation guarantee. On the other hand, the\ngreedy algorithm involves expensive matrix operations that make it prohibitive\nto employ on large datasets. To confront this challenge, we develop more\nefficient algorithms based on spectral clustering and on personalized PageRank.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 20:10:04 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Mavroforakis", "Charalampos", ""], ["Mathioudakis", "Michael", ""], ["Gionis", "Aristides", ""]]}, {"id": "1509.02841", "submitter": "Charis Papadopoulos", "authors": "Loukas Georgiadis, Giuseppe F. Italiano, Charis Papadopoulos, and\n  Nikos Parotsidis", "title": "Approximating the Smallest Spanning Subgraph for 2-Edge-Connectivity in\n  Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a strongly connected directed graph. We consider the following\nthree problems, where we wish to compute the smallest strongly connected\nspanning subgraph of $G$ that maintains respectively: the $2$-edge-connected\nblocks of $G$ (\\textsf{2EC-B}); the $2$-edge-connected components of $G$\n(\\textsf{2EC-C}); both the $2$-edge-connected blocks and the $2$-edge-connected\ncomponents of $G$ (\\textsf{2EC-B-C}). All three problems are NP-hard, and thus\nwe are interested in efficient approximation algorithms. For \\textsf{2EC-C} we\ncan obtain a $3/2$-approximation by combining previously known results. For\n\\textsf{2EC-B} and \\textsf{2EC-B-C}, we present new $4$-approximation\nalgorithms that run in linear time. We also propose various heuristics to\nimprove the size of the computed subgraphs in practice, and conduct a thorough\nexperimental study to assess their merits in practical scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 16:31:24 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Georgiadis", "Loukas", ""], ["Italiano", "Giuseppe F.", ""], ["Papadopoulos", "Charis", ""], ["Parotsidis", "Nikos", ""]]}, {"id": "1509.02897", "submitter": "Ilya Razenshteyn", "authors": "Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn,\n  Ludwig Schmidt", "title": "Practical and Optimal LSH for Angular Distance", "comments": "22 pages, an extended abstract is to appear in the proceedings of the\n  29th Annual Conference on Neural Information Processing Systems (NIPS 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show the existence of a Locality-Sensitive Hashing (LSH) family for the\nangular distance that yields an approximate Near Neighbor Search algorithm with\nthe asymptotically optimal running time exponent. Unlike earlier algorithms\nwith this property (e.g., Spherical LSH [Andoni, Indyk, Nguyen, Razenshteyn\n2014], [Andoni, Razenshteyn 2015]), our algorithm is also practical, improving\nupon the well-studied hyperplane LSH [Charikar, 2002] in practice. We also\nintroduce a multiprobe version of this algorithm, and conduct experimental\nevaluation on real and synthetic data sets.\n  We complement the above positive results with a fine-grained lower bound for\nthe quality of any LSH family for angular distance. Our lower bound implies\nthat the above LSH family exhibits a trade-off between evaluation time and\nquality that is close to optimal for a natural class of LSH functions.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 19:24:33 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Andoni", "Alexandr", ""], ["Indyk", "Piotr", ""], ["Laarhoven", "Thijs", ""], ["Razenshteyn", "Ilya", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1509.02972", "submitter": "Jared Lichtman", "authors": "Jared D. Lichtman", "title": "On the Multidimensional Stable Marriage Problem", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a problem definition of the stable marriage problem for a general\nnumber of parties $p$ under a natural preference scheme in which each person\nhas simple lists for the other parties. We extend the notion of stability in a\nnatural way and present so called elemental and compound algorithms to generate\nmatchings for a problem instance. We demonstrate the stability of matchings\ngenerated by both algorithms, as well as show that the former runs in $O(pn^2)$\ntime.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 23:03:38 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Lichtman", "Jared D.", ""]]}, {"id": "1509.03046", "submitter": "Roland Mark\\'o", "authors": "Marek Karpinski and Roland Mark\\'o", "title": "Explicit Bounds for Nondeterministically Testable Hypergraph Parameters", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we give a new effective proof method for the equivalence of the\nnotions of testability and nondeterministic testability for uniform hypergraph\nparameters. We provide the first effective upper bound on the sample complexity\nof any nondeterministically testable $r$-uniform hypergraph parameter as a\nfunction of the sample complexity of its witness parameter for arbitrary $r$.\nThe dependence is of the form of an exponential tower function with the height\nlinear in $r$. Our argument depends crucially on the new upper bounds for the\n$r$-cut norm of sampled $r$-uniform hypergraphs. We employ also our approach\nfor some other restricted classes of hypergraph parameters, and present some\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 07:46:55 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Karpinski", "Marek", ""], ["Mark\u00f3", "Roland", ""]]}, {"id": "1509.03147", "submitter": "Ilkka Kivim\\\"aki", "authors": "Ilkka Kivim\\\"aki, Bertrand Lebichot, Jari Saram\\\"aki, Marco Saerens", "title": "Two betweenness centrality measures based on Randomized Shortest Paths", "comments": "Minor updates; published in Scientific Reports", "journal-ref": "Scientific Reports 6, Article number: 19668 (2016)", "doi": "10.1038/srep19668", "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces two new closely related betweenness centrality measures\nbased on the Randomized Shortest Paths (RSP) framework, which fill a gap\nbetween traditional network centrality measures based on shortest paths and\nmore recent methods considering random walks or current flows. The framework\ndefines Boltzmann probability distributions over paths of the network which\nfocus on the shortest paths, but also take into account longer paths depending\non an inverse temperature parameter. RSP's have previously proven to be useful\nin defining distance measures on networks. In this work we study their utility\nin quantifying the importance of the nodes of a network. The proposed RSP\nbetweenness centralities combine, in an optimal way, the ideas of using the\nshortest and purely random paths for analysing the roles of network nodes,\navoiding issues involving these two paradigms. We present the derivations of\nthese measures and how they can be computed in an efficient way. In addition,\nwe show with real world examples the potential of the RSP betweenness\ncentralities in identifying interesting nodes of a network that more\ntraditional methods might fail to notice.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 13:37:45 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 12:33:05 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Kivim\u00e4ki", "Ilkka", ""], ["Lebichot", "Bertrand", ""], ["Saram\u00e4ki", "Jari", ""], ["Saerens", "Marco", ""]]}, {"id": "1509.03165", "submitter": "Ben Strasser", "authors": "Julian Dibbelt, Ben Strasser, Dorothea Wagner", "title": "Fast Exact Shortest Path and Distance Queries on Road Networks with\n  Parametrized Costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a scenario for route planning in road networks, where the objective\nto be optimized may change between every shortest path query. Since this\ninvalidates many of the known speedup techniques for road networks that are\nbased on preprocessing of shortest path structures, we investigate\noptimizations exploiting solely the topological structure of networks. We\nexperimentally evaluate our technique on a large set of real-world road\nnetworks of various data sources. With lightweight preprocessing our technique\nanswers long-distance queries across continental networks significantly faster\nthan previous approaches towards the same problem formulation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 14:04:06 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Dibbelt", "Julian", ""], ["Strasser", "Ben", ""], ["Wagner", "Dorothea", ""]]}, {"id": "1509.03212", "submitter": "Alina Ene", "authors": "Deeparnab Chakrabarty, Alina Ene, Ravishankar Krishnaswamy, Debmalya\n  Panigrahi", "title": "Online Buy-at-Bulk Network Design", "comments": "24 pages, longer version of a FOCS 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first non-trivial online algorithms for the non-uniform,\nmulticommodity buy-at-bulk (MC-BB) network design problem in undirected and\ndirected graphs. Our competitive ratios qualitatively match the best known\napproximation factors for the corresponding offline problems. The main engine\nfor our results is an online reduction theorem of MC-BB problems to their\nsingle-sink (SS-BB) counterparts. We use the concept of junction-tree solutions\n(Chekuri et al., FOCS 2006) that play an important role in solving the offline\nversions of the problem via a greedy subroutine -- an inherently offline\nprocedure. Our main technical contribution is in designing an online algorithm\nusing only the existence of good junction-trees to reduce an MC-BB instance to\nmultiple SS-BB sub-instances. Along the way, we also give the first non-trivial\nonline node-weighted/directed single-sink buy-at-bulk algorithms. In addition\nto the new results, our generic reduction also yields new proofs of recent\nresults for the online node-weighted Steiner forest and online group Steiner\nforest problems.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 16:16:21 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Ene", "Alina", ""], ["Krishnaswamy", "Ravishankar", ""], ["Panigrahi", "Debmalya", ""]]}, {"id": "1509.03389", "submitter": "Piotr Skowron", "authors": "Jerome Lang and Piotr Skowron", "title": "Multi-Attribute Proportional Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem in which a given number of items has to be\nchosen from a predefined set. Each item is described by a vector of attributes\nand for each attribute there is a desired distribution that the selected set\nshould have. We look for a set that fits as much as possible the desired\ndistributions on all attributes. Examples of applications include choosing\nmembers of a representative committee, where candidates are described by\nattributes such as sex, age and profession, and where we look for a committee\nthat for each attribute offers a certain representation, i.e., a single\ncommittee that contains a certain number of young and old people, certain\nnumber of men and women, certain number of people with different professions,\netc. With a single attribute the problem collapses to the apportionment problem\nfor party-list proportional representation systems (in such case the value of\nthe single attribute would be a political affiliation of a candidate). We study\nthe properties of the associated subset selection rules, as well as their\ncomputation complexity.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 05:01:17 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 22:04:34 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Lang", "Jerome", ""], ["Skowron", "Piotr", ""]]}, {"id": "1509.03484", "submitter": "Shenggong Ji", "authors": "Yanqing Hu, Shenggong Ji, Yuliang Jin, Ling Feng, H. Eugene Stanley,\n  Shlomo Havlin", "title": "Local structure can identify and quantify influential global spreaders\n  in large scale social networks", "comments": "6 pages, 5 figures, Proceedings of the National Academy of Sciences\n  of the United States of America (PNAS), July 3, 2018", "journal-ref": null, "doi": "10.1073/pnas.1710547115", "report-no": null, "categories": "physics.soc-ph cs.CY cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring and optimizing the influence of nodes in big-data online social\nnetworks are important for many practical applications, such as the viral\nmarketing and the adoption of new products. As the viral spreading on social\nnetwork is a global process, it is commonly believed that measuring the\ninfluence of nodes inevitably requires the knowledge of the entire network.\nEmploying percolation theory, we show that the spreading process displays a\nnucleation behavior: once a piece of information spread from the seeds to more\nthan a small characteristic number of nodes, it reaches a point of no return\nand will quickly reach the percolation cluster, regardless of the entire\nnetwork structure, otherwise the spreading will be contained locally. Thus, we\nfind that, without the knowledge of entire network, any nodes' global influence\ncan be accurately measured using this characteristic number, which is\nindependent of the network size. This motivates an efficient algorithm with\nconstant time complexity on the long standing problem of best seed spreaders\nselection, with performance remarkably close to the true optimum.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 12:51:10 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 14:42:17 GMT"}, {"version": "v3", "created": "Tue, 6 Mar 2018 13:49:16 GMT"}, {"version": "v4", "created": "Wed, 23 May 2018 00:23:48 GMT"}, {"version": "v5", "created": "Mon, 30 Jul 2018 05:15:12 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Hu", "Yanqing", ""], ["Ji", "Shenggong", ""], ["Jin", "Yuliang", ""], ["Feng", "Ling", ""], ["Stanley", "H. Eugene", ""], ["Havlin", "Shlomo", ""]]}, {"id": "1509.03503", "submitter": "Marco Winkler", "authors": "Marco Winkler", "title": "NoSPaM Manual - A Tool for Node-Specific Triad Pattern Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV cs.DS physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of triadic subgraph motifs is a common methodology in\ncomplex-networks research. The procedure usually applied in order to detect\nmotifs evaluates whether a certain subgraph pattern is overrepresented in a\nnetwork as a whole. However, motifs do not necessarily appear frequently in\nevery region of a graph. For this reason, we recently introduced the framework\nof Node-Specific Pattern Mining (NoSPaM). This work is a manual for an\nimplementation of NoSPaM which can be downloaded from www.mwinkler.eu.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 14:57:08 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Winkler", "Marco", ""]]}, {"id": "1509.03530", "submitter": "Ravi Kumar Yadav Dega", "authors": "Ravi Kumar Yadav Dega and Gunes Ercal", "title": "A comparative analysis of progressive multiple sequence alignment\n  approaches using UPGMA and neighbor joining based guide trees", "comments": "9 Pages", "journal-ref": "International Journal of Computer Science, Engineering and\n  Information Technology (IJCSEIT), Vol. 5,No.3/4, August 2015", "doi": null, "report-no": null, "categories": "cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple sequence alignment is increasingly important to bioinformatics, with\nseveral applications ranging from phylogenetic analyses to domain\nidentification. There are several ways to perform multiple sequence alignment,\nan important way of which is the progressive alignment approach studied in this\nwork. Progressive alignment involves three steps: find the distance between\neach pair of sequences; construct a guide tree based on the distance matrix;\nfinally based on the guide tree align sequences using the concept of aligned\nprofiles. Our contribution is in comparing two main methods of guide tree\nconstruction in terms of both efficiency and accuracy of the overall alignment:\nUPGMA and Neighbor Join methods. Our experimental results indicate that the\nNeighbor Join method is both more efficient in terms of performance and more\naccurate in terms of overall cost minimization.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 14:23:16 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Dega", "Ravi Kumar Yadav", ""], ["Ercal", "Gunes", ""]]}, {"id": "1509.03600", "submitter": "Satyen Kale", "authors": "Satyen Kale and Chansoo Lee and D\\'avid P\\'al", "title": "Hardness of Online Sleeping Combinatorial Optimization Problems", "comments": "A version of this paper was published in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that several online combinatorial optimization problems that admit\nefficient no-regret algorithms become computationally hard in the sleeping\nsetting where a subset of actions becomes unavailable in each round.\nSpecifically, we show that the sleeping versions of these problems are at least\nas hard as PAC learning DNF expressions, a long standing open problem. We show\nhardness for the sleeping versions of Online Shortest Paths, Online Minimum\nSpanning Tree, Online $k$-Subsets, Online $k$-Truncated Permutations, Online\nMinimum Cut, and Online Bipartite Matching. The hardness result for the\nsleeping version of the Online Shortest Paths problem resolves an open problem\npresented at COLT 2015 (Koolen et al., 2015).\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 18:27:42 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 18:12:37 GMT"}, {"version": "v3", "created": "Mon, 19 Dec 2016 22:28:15 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Kale", "Satyen", ""], ["Lee", "Chansoo", ""], ["P\u00e1l", "D\u00e1vid", ""]]}, {"id": "1509.03753", "submitter": "Petr Golovach", "authors": "Petr A. Golovach, Pinar Heggernes, Mamadou Moustapha Kant\\'e, Dieter\n  Kratsch, Sigve H. S{\\ae}ther, Yngve Villanger", "title": "Output-Polynomial Enumeration on Graphs of Bounded (Local) Linear\n  MIM-Width", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear induced matching width (LMIM-width) of a graph is a width\nparameter defined by using the notion of branch-decompositions of a set\nfunction on ternary trees. In this paper we study output-polynomial enumeration\nalgorithms on graphs of bounded LMIM-width and graphs of bounded local\nLMIM-width. In particular, we show that all 1-minimal and all 1-maximal\n(\\sigma,\\rho)-dominating sets, and hence all minimal dominating sets, of graphs\nof bounded LMIM-width can be enumerated with polynomial (linear) delay using\npolynomial space. Furthermore, we show that all minimal dominating sets of a\nunit square graph can be enumerated in incremental polynomial time.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2015 15:00:20 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 18:11:51 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Golovach", "Petr A.", ""], ["Heggernes", "Pinar", ""], ["Kant\u00e9", "Mamadou Moustapha", ""], ["Kratsch", "Dieter", ""], ["S\u00e6ther", "Sigve H.", ""], ["Villanger", "Yngve", ""]]}, {"id": "1509.03817", "submitter": "Mircea Parpalea", "authors": "Mircea Parpalea, Nicoleta Avesalon, Eleonor Ciurea", "title": "Minimum parametric flow over time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a dynamic solution method for dynamic minimum parametric\nnetworks flow. The solution method solves the problem for a special parametric\ndynamic network with linear lower bound functions of a single parameter.\nInstead directly work on the original network, the method implements a\nlabelling algorithm in the parametric dynamic residual network and uses\nquickest paths from the source node to the sink node in the time-space network\nalong which repeatedly decreases the dynamic flow for a sequence of parameter\nvalues, in their increasing order. In each iteration, the algorithm computes\nboth the minimum flow for a certain subinterval of the parameter values, and\nthe new breakpoint for the maximum parametric dynamic flow value function.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2015 06:30:00 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Parpalea", "Mircea", ""], ["Avesalon", "Nicoleta", ""], ["Ciurea", "Eleonor", ""]]}, {"id": "1509.03915", "submitter": "Haris Aziz", "authors": "Haris Aziz", "title": "An Impossibility Result for Housing Markets with Fractional Endowments", "comments": "removed an incorrect claim in the previous version regarding an\n  algorithm satisfying SD-core stability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The housing market setting constitutes a fundamental model of exchange\neconomies of goods. Most of the work concerning housing markets does not cater\nfor randomized assignments or allocation of time-shares. House allocation with\nfractional endowments of houses was considered by Athanassoglou and Sethuraman\n(2011) who posed the open problem whether individual rationality, weak\nstrategyproofness, and efficiency are compatible for the setting. We show that\nthe three axioms are incompatible.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 00:22:54 GMT"}, {"version": "v2", "created": "Sat, 5 Mar 2016 22:24:40 GMT"}, {"version": "v3", "created": "Wed, 30 May 2018 09:27:08 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Aziz", "Haris", ""]]}, {"id": "1509.03917", "submitter": "Anastasios Kyrillidis", "authors": "Srinadh Bhojanapalli, Anastasios Kyrillidis, Sujay Sanghavi", "title": "Dropping Convexity for Faster Semi-definite Optimization", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG cs.NA math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the minimization of a convex function $f(X)$ over the set of\n$n\\times n$ positive semi-definite matrices, but when the problem is recast as\n$\\min_U g(U) := f(UU^\\top)$, with $U \\in \\mathbb{R}^{n \\times r}$ and $r \\leq\nn$. We study the performance of gradient descent on $g$---which we refer to as\nFactored Gradient Descent (FGD)---under standard assumptions on the original\nfunction $f$.\n  We provide a rule for selecting the step size and, with this choice, show\nthat the local convergence rate of FGD mirrors that of standard gradient\ndescent on the original $f$: i.e., after $k$ steps, the error is $O(1/k)$ for\nsmooth $f$, and exponentially small in $k$ when $f$ is (restricted) strongly\nconvex. In addition, we provide a procedure to initialize FGD for (restricted)\nstrongly convex objectives and when one only has access to $f$ via a\nfirst-order oracle; for several problem instances, such proper initialization\nleads to global convergence guarantees.\n  FGD and similar procedures are widely used in practice for problems that can\nbe posed as matrix factorization. To the best of our knowledge, this is the\nfirst paper to provide precise convergence rate guarantees for general convex\nfunctions under standard convex assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 00:40:11 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 01:45:02 GMT"}, {"version": "v3", "created": "Sat, 16 Apr 2016 03:17:46 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Bhojanapalli", "Srinadh", ""], ["Kyrillidis", "Anastasios", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1509.03976", "submitter": "Mikael Gast", "authors": "Mikael Gast, Mathias Hauptmann and Marek Karpinski", "title": "Approximability of TSP on Power Law Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the special case of Graphic TSP where the underlying\ngraph is a power law graph (PLG). We give a refined analysis of some of the\ncurrent best approximation algorithms and show that an improved approximation\nratio can be achieved for certain ranges of the power law exponent $\\beta$. For\nthe value of power law exponent $\\beta=1.5$ we obtain an approximation ratio of\n$1.34$ for Graphic TSP. Moreover we study the $(1,2)$-TSP with the underlying\ngraph of $1$-edges being a PLG. We show improved approximation ratios in the\ncase of underlying deterministic PLGs for $\\beta$ greater than $1.666$. For\nunderlying random PLGs we further improve the analysis and show even better\nexpected approximation ratio for the range of $\\beta$ between $1$ and $3.5$. On\nthe other hand we prove the first explicit inapproximability bounds for\n$(1,2)$-TSP for an underlying power law graph.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 07:50:27 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Gast", "Mikael", ""], ["Hauptmann", "Mathias", ""], ["Karpinski", "Marek", ""]]}, {"id": "1509.03979", "submitter": "Chun-Shien Lu", "authors": "Sung-Hsien Hsieh and Chun-Shien Lu and Soo-Chang Pei", "title": "Fast Greedy Approaches for Compressive Sensing of Large-Scale Signals", "comments": "10 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cost-efficient compressive sensing is challenging when facing large-scale\ndata, {\\em i.e.}, data with large sizes. Conventional compressive sensing\nmethods for large-scale data will suffer from low computational efficiency and\nmassive memory storage. In this paper, we revisit well-known solvers called\ngreedy algorithms, including Orthogonal Matching Pursuit (OMP), Subspace\nPursuit (SP), Orthogonal Matching Pursuit with Replacement (OMPR). Generally,\nthese approaches are conducted by iteratively executing two main steps: 1)\nsupport detection and 2) solving least square problem.\n  To reduce the cost of Step 1, it is not hard to employ the sensing matrix\nthat can be implemented by operator-based strategy instead of matrix-based one\nand can be speeded by fast Fourier Transform (FFT). Step 2, however, requires\nmaintaining and calculating a pseudo-inverse of a sub-matrix, which is random\nand not structural, and, thus, operator-based matrix does not work. To overcome\nthis difficulty, instead of solving Step 2 by a closed-form solution, we\npropose a fast and cost-effective least square solver, which combines a\nConjugate Gradient (CG) method with our proposed weighted least square problem\nto iteratively approximate the ground truth yielded by a greedy algorithm.\nExtensive simulations and theoretical analysis validate that the proposed\nmethod is cost-efficient and is readily incorporated with the existing greedy\nalgorithms to remarkably improve the performance for large-scale problems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 07:55:51 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 05:22:08 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Hsieh", "Sung-Hsien", ""], ["Lu", "Chun-Shien", ""], ["Pei", "Soo-Chang", ""]]}, {"id": "1509.03990", "submitter": "Geevarghese Philip", "authors": "Shivam Garg and Geevarghese Philip", "title": "Raising The Bar For Vertex Cover: Fixed-parameter Tractability Above A\n  Higher Guarantee", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the following above-guarantee parameterization of the\nclassical Vertex Cover problem: Given a graph $G$ and $k\\in\\mathbb{N}$ as\ninput, does $G$ have a vertex cover of size at most $(2LP-MM)+k$? Here $MM$ is\nthe size of a maximum matching of $G$, $LP$ is the value of an optimum solution\nto the relaxed (standard) LP for Vertex Cover on $G$, and $k$ is the parameter.\nSince $(2LP-MM)\\geq{LP}\\geq{MM}$, this is a stricter parameterization than\nthose---namely, above-$MM$, and above-$LP$---which have been studied so far.\n  We prove that Vertex Cover is fixed-parameter tractable for this stricter\nparameter $k$: We derive an algorithm which solves Vertex Cover in time\n$O^{*}(3^{k})$, pushing the envelope further on the parameterized tractability\nof Vertex Cover.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 08:38:20 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Garg", "Shivam", ""], ["Philip", "Geevarghese", ""]]}, {"id": "1509.04344", "submitter": "Sushmita Gupta", "authors": "Sushmita Gupta and Kazuo Iwama and Shuichi Miyazaki", "title": "Stable Nash Equilibria in the Gale-Shapley Matching Game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we study the stable marriage game induced by the\nmen-proposing Gale-Shapley algorithm. Our setting is standard: all the lists\nare complete and the matching mechanism is the men-proposing Gale-Shapley\nalgorithm. It is well known that in this setting, men cannot cheat, but women\ncan. In fact, Teo, Sethuraman and Tan \\cite{TST01}, show that there is a\npolynomial time algorithm to obtain, for a given strategy (the set of all\nlists) $Q$ and a woman $w$, the best partner attainable by changing her list.\nHowever, what if the resulting matching is not stable with respect to $Q$?\nObviously, such a matching would be vulnerable to further manipulation, but is\nnot mentioned in \\cite{TST01}. In this paper, we consider (safe) manipulation\nthat implies a stable matching in a most general setting. Specifically, our\ngoal is to decide for a given $Q$, if w can manipulate her list to obtain a\nstrictly better partner with respect to the true strategy $P$ (which may be\ndifferent from $Q$), and also the outcome is a stable matching for $P$.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 22:11:40 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Gupta", "Sushmita", ""], ["Iwama", "Kazuo", ""], ["Miyazaki", "Shuichi", ""]]}, {"id": "1509.04549", "submitter": "Mikkel Thorup", "authors": "Mikkel Thorup", "title": "Linear Probing with 5-Independent Hashing", "comments": "arXiv admin note: text overlap with arXiv:1505.01523", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These lecture notes show that linear probing takes expected constant time if\nthe hash function is 5-independent. This result was first proved by Pagh et al.\n[STOC'07,SICOMP'09]. The simple proof here is essentially taken from [Patrascu\nand Thorup ICALP'10]. We will also consider a smaller space version of linear\nprobing that may have false positives like Bloom filters.\n  These lecture notes illustrate the use of higher moments in data structures,\nand could be used in a course on randomized algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 13:48:59 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 14:11:51 GMT"}, {"version": "v3", "created": "Thu, 11 May 2017 06:58:46 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Thorup", "Mikkel", ""]]}, {"id": "1509.04863", "submitter": "Chun-Shien Lu", "authors": "Sung-Hsien Hsieh and Chun-Shien Lu and and Soo-Chang Pei", "title": "Fast Template Matching by Subsampled Circulant Matrix", "comments": "7 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Template matching is widely used for many applications in image and signal\nprocessing and usually is time-critical. Traditional methods usually focus on\nhow to reduce the search locations by coarse-to-fine strategy or full search\ncombined with pruning strategy. However, the computation cost of those methods\nis easily dominated by the size of signal N instead of that of template K. This\npaper proposes a probabilistic and fast matching scheme, which computation\ncosts requires O(N) additions and O(K \\log K) multiplications, based on\ncross-correlation. The nuclear idea is to first downsample signal, which size\nbecomes O(K), and then subsequent operations only involves downsampled signals.\nThe probability of successful match depends on cross-correlation between signal\nand the template. We show the sufficient condition for successful match and\nprove that the probability is high for binary signals with K^2/log K >= O(N).\nThe experiments shows this proposed scheme is fast and efficient and supports\nthe theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 09:38:29 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Hsieh", "Sung-Hsien", ""], ["Lu", "Chun-Shien", ""], ["Pei", "and Soo-Chang", ""]]}, {"id": "1509.04880", "submitter": "Ignasi Sau", "authors": "Eunjung Kim, Sang-il Oum, Christophe Paul, Ignasi Sau, Dimitrios M.\n  Thilikos", "title": "An FPT 2-Approximation for Tree-Cut Decomposition", "comments": "17 pages, 3 figures", "journal-ref": "Algorithmica, 80(1)(January 2018), pp. 116-135", "doi": "10.1007/s00453-016-0245-5", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tree-cut width of a graph is a graph parameter defined by Wollan [J.\nComb. Theory, Ser. B, 110:47-66, 2015] with the help of tree-cut\ndecompositions. In certain cases, tree-cut width appears to be more adequate\nthan treewidth as an invariant that, when bounded, can accelerate the\nresolution of intractable problems. While designing algorithms for problems\nwith bounded tree-cut width, it is important to have a parametrically tractable\nway to compute the exact value of this parameter or, at least, some constant\napproximation of it. In this paper we give a parameterized 2-approximation\nalgorithm for the computation of tree-cut width; for an input $n$-vertex graph\n$G$ and an integer $w$, our algorithm either confirms that the tree-cut width\nof $G$ is more than $w$ or returns a tree-cut decomposition of $G$ certifying\nthat its tree-cut width is at most $2w$, in time $2^{O(w^2\\log w)} \\cdot n^2$.\nPrior to this work, no constructive parameterized algorithms, even approximated\nones, existed for computing the tree-cut width of a graph. As a consequence of\nthe Graph Minors series by Robertson and Seymour, only the existence of a\ndecision algorithm was known.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 10:50:00 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Kim", "Eunjung", ""], ["Oum", "Sang-il", ""], ["Paul", "Christophe", ""], ["Sau", "Ignasi", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1509.04927", "submitter": "Norbert Blum", "authors": "Norbert Blum", "title": "Maximum Matching in General Graphs Without Explicit Consideration of\n  Blossoms Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reduce the problem of finding an augmenting path in a general graph to a\nreachability problem in a directed bipartite graph. A slight modification of\ndepth-first search leads to an algorithm for finding such paths. Although this\nsetting is equivalent to the traditional terminology of blossoms due to\nEdmonds, there are some advantages. Mainly, this point of view enables the\ndescription of algorithms for the solution of matching problems without\nexplicit analysis of blossoms, nested blossoms, and so on. Exemplary, we\ndescribe an efficient realization of the Hopcroft-Karp approach for the\ncomputation of a maximum cardinality matching in general graphs and a variant\nof Edmonds' primal-dual algorithm for the maximum weighted matching problem.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 14:11:38 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Blum", "Norbert", ""]]}, {"id": "1509.05053", "submitter": "Pat Morin", "authors": "Paul-Virak Khuong and Pat Morin", "title": "Array Layouts for Comparison-Based Searching", "comments": "46 pages; 24 figures; updated after reviewing by ACM JEA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We attempt to determine the best order and search algorithm to store $n$\ncomparable data items in an array, $A$, of length $n$ so that we can, for any\nquery value, $x$, quickly find the smallest value in $A$ that is greater than\nor equal to $x$. In particular, we consider the important case where there are\nmany such queries to the same array, $A$, which resides entirely in RAM. In\naddition to the obvious sorted order/binary search combination we consider the\nEytzinger (BFS) layout normally used for heaps, an implicit B-tree layout that\ngeneralizes the Eytzinger layout, and the van Emde Boas layout commonly used in\nthe cache-oblivious algorithms literature.\n  After extensive testing and tuning on a wide variety of modern hardware, we\narrive at the conclusion that, for small values of $n$, sorted order, combined\nwith a good implementation of binary search is best. For larger values of $n$,\nwe arrive at the surprising conclusion that the Eytzinger layout is usually the\nfastest. The latter conclusion is unexpected and goes counter to earlier\nexperimental work by Brodal, Fagerberg, and Jacob (SODA~2003), who concluded\nthat both the B-tree and van Emde Boas layouts were faster than the Eytzinger\nlayout for large values of $n$. Our fastest C++ implementations, when compiled,\nuse conditional moves to avoid branch mispredictions and prefetching to reduce\ncache latency.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 20:20:47 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 13:08:49 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Khuong", "Paul-Virak", ""], ["Morin", "Pat", ""]]}, {"id": "1509.05065", "submitter": "Fernando Brandao", "authors": "Fernando G.S.L. Brandao, Aram W. Harrow", "title": "Estimating operator norms using covering nets", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present several polynomial- and quasipolynomial-time approximation schemes\nfor a large class of generalized operator norms. Special cases include the\n$2\\rightarrow q$ norm of matrices for $q>2$, the support function of the set of\nseparable quantum states, finding the least noisy output of\nentanglement-breaking quantum channels, and approximating the injective tensor\nnorm for a map between two Banach spaces whose factorization norm through\n$\\ell_1^n$ is bounded.\n  These reproduce and in some cases improve upon the performance of previous\nalgorithms by Brand\\~ao-Christandl-Yard and followup work, which were based on\nthe Sum-of-Squares hierarchy and whose analysis used techniques from quantum\ninformation such as the monogamy principle of entanglement. Our algorithms, by\ncontrast, are based on brute force enumeration over carefully chosen covering\nnets. These have the advantage of using less memory, having much simpler proofs\nand giving new geometric insights into the problem. Net-based algorithms for\nsimilar problems were also presented by Shi-Wu and Barak-Kelner-Steurer, but in\neach case with a run-time that is exponential in the rank of some matrix. We\nachieve polynomial or quasipolynomial runtimes by using the much smaller nets\nthat exist in $\\ell_1$ spaces. This principle has been used in learning theory,\nwhere it is known as Maurey's empirical method.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 21:08:30 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Brandao", "Fernando G. S. L.", ""], ["Harrow", "Aram W.", ""]]}, {"id": "1509.05305", "submitter": "Carlo Albert", "authors": "Carlo Albert, Simone Ulzega, Ruedi Stoop", "title": "Boosting Bayesian Parameter Inference of Nonlinear Stochastic\n  Differential Equation Models by Hamiltonian Scale Separation", "comments": "15 pages, 8 figures", "journal-ref": "Phys. Rev. E 93, 043313, 15 April 2016", "doi": "10.1103/PhysRevE.93.043313", "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter inference is a fundamental problem in data-driven modeling. Given\nobserved data that is believed to be a realization of some parameterized model,\nthe aim is to find parameter values that are able to explain the observed data.\nIn many situations, the dominant sources of uncertainty must be included into\nthe model, for making reliable predictions. This naturally leads to stochastic\nmodels. Stochastic models render parameter inference much harder, as the aim\nthen is to find a distribution of likely parameter values. In Bayesian\nstatistics, which is a consistent framework for data-driven learning, this\nso-called posterior distribution can be used to make probabilistic predictions.\nWe propose a novel, exact and very efficient approach for generating posterior\nparameter distributions, for stochastic differential equation models calibrated\nto measured time-series. The algorithm is inspired by re-interpreting the\nposterior distribution as a statistical mechanics partition function of an\nobject akin to a polymer, where the measurements are mapped on heavier beads\ncompared to those of the simulated data. To arrive at distribution samples, we\nemploy a Hamiltonian Monte Carlo approach combined with a multiple time-scale\nintegration. A separation of time scales naturally arises if either the number\nof measurement points or the number of simulation points becomes large.\nFurthermore, at least for 1D problems, we can decouple the harmonic modes\nbetween measurement points and solve the fastest part of their dynamics\nanalytically. Our approach is applicable to a wide range of inference problems\nand is highly parallelizable.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 15:52:46 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2016 09:04:31 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Albert", "Carlo", ""], ["Ulzega", "Simone", ""], ["Stoop", "Ruedi", ""]]}, {"id": "1509.05377", "submitter": "Jingru Zhang", "authors": "Haitao Wang and Jingru Zhang", "title": "Computing the Rectilinear Center of Uncertain Points in the Plane", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the rectilinear one-center problem on uncertain\npoints in the plane. In this problem, we are given a set $P$ of $n$ (weighted)\nuncertain points in the plane and each uncertain point has $m$ possible\nlocations each associated with a probability for the point appearing at that\nlocation. The goal is to find a point $q^*$ in the plane which minimizes the\nmaximum expected rectilinear distance from $q^*$ to all uncertain points of\n$P$, and $q^*$ is called a rectilinear center. We present an algorithm that\nsolves the problem in $O(mn)$ time. Since the input size of the problem is\n$\\Theta(mn)$, our algorithm is optimal.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 19:14:43 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Wang", "Haitao", ""], ["Zhang", "Jingru", ""]]}, {"id": "1509.05445", "submitter": "Ferdinando Cicalese", "authors": "Wilfredo Bardales Roncalla and Eduardo Laber and Ferdinando Cicalese", "title": "Searching for a superlinear lower bounds for the Maximum Consecutive\n  Subsums Problem and the (min,+)-convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a sequence of n numbers, the Maximum Consecutive Subsums Problem (MCSP)\nasks for the maximum consecutive sum of lengths l for each l = 1,...,n. No\nalgorithm is known for this problem which is significantly better than the\nnaive quadratic solution. Nor a super linear lower bound is known. The best\nknown bound for the MCSP is based on the the computation of the\n(min,+)-convolution, another problem for which neither an O(n^{2-{\\epsilon}})\nupper bound is known nor a super linear lower bound. We show that the two\nproblems are in fact computationally equivalent by providing linear reductions\nbetween them. Then, we concentrate on the problem of finding super linear lower\nbounds and provide empirical evidence for an {\\Omega}(nlogn) lower bounds for\nboth problems in the decision tree model.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 21:16:38 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Roncalla", "Wilfredo Bardales", ""], ["Laber", "Eduardo", ""], ["Cicalese", "Ferdinando", ""]]}, {"id": "1509.05494", "submitter": "Kuan Yang", "authors": "Pinyan Lu, Kuan Yang, Chihao Zhang", "title": "FPTAS for Hardcore and Ising Models on Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardcore and Ising models are two most important families of two state spin\nsystems in statistic physics. Partition function of spin systems is the center\nconcept in statistic physics which connects microscopic particles and their\ninteractions with their macroscopic and statistical properties of materials\nsuch as energy, entropy, ferromagnetism, etc. If each local interaction of the\nsystem involves only two particles, the system can be described by a graph. In\nthis case, fully polynomial-time approximation scheme (FPTAS) for computing the\npartition function of both hardcore and anti-ferromagnetic Ising model was\ndesigned up to the uniqueness condition of the system. These result are the\nbest possible since approximately computing the partition function beyond this\nthreshold is NP-hard. In this paper, we generalize these results to general\nphysics systems, where each local interaction may involves multiple particles.\nSuch systems are described by hypergraphs. For hardcore model, we also provide\nFPTAS up to the uniqueness condition, and for anti-ferromagnetic Ising model,\nwe obtain FPTAS where a slightly stronger condition holds.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 03:08:55 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Lu", "Pinyan", ""], ["Yang", "Kuan", ""], ["Zhang", "Chihao", ""]]}, {"id": "1509.05514", "submitter": "Samira Daruki", "authors": "Samira Daruki and Justin Thaler and Suresh Venkatasubramanian", "title": "Streaming Verification in Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming interactive proofs (SIPs) are a framework to reason about\noutsourced computation, where a data owner (the verifier) outsources a\ncomputation to the cloud (the prover), but wishes to verify the correctness of\nthe solution provided by the cloud service. In this paper we present streaming\ninteractive proofs for problems in data analysis. We present protocols for\nclustering and shape fitting problems, as well as an improved protocol for\nrectangular matrix multiplication. The latter can in turn be used to verify $k$\neigenvectors of a (streamed) $n \\times n$ matrix. In general our solutions use\npolylogarithmic rounds of communication and polylogarithmic total communication\nand verifier space. For special cases (when optimality certificates can be\nverified easily), we present constant round protocols with similar costs. For\nrectangular matrix multiplication and eigenvector verification, our protocols\nwork in the more restricted annotated data streaming model, and use sublinear\n(but not polylogarithmic) communication.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 06:24:16 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Daruki", "Samira", ""], ["Thaler", "Justin", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1509.05559", "submitter": "Junjie Ye", "authors": "Leizhen Cai, Junjie Ye", "title": "Finding Two Edge-Disjoint Paths with Length Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding, for two pairs $(s_1,t_1)$ and $(s_2,t_2)$\nof vertices in an undirected graphs, an $(s_1,t_1)$-path $P_1$ and an\n$(s_2,t_2)$-path $P_2$ such that $P_1$ and $P_2$ share no edges and the length\nof each $P_i$ satisfies $L_i$, where $L_i \\in \\{ \\le k_i, \\; = k_i, \\; \\ge k_i,\n\\; \\le \\infty\\}$.\n  We regard $k_1$ and $k_2$ as parameters and investigate the parameterized\ncomplexity of the above problem when at least one of $P_1$ and $P_2$ has a\nlength constraint (note that $L_i = \"\\le \\infty\"$ indicates that $P_i$ has no\nlength constraint). For the nine different cases of $(L_1, L_2)$, we obtain FPT\nalgorithms for seven of them. Our algorithms uses random partition backed by\nsome structural results. On the other hand, we prove that the problem admits no\npolynomial kernel for all nine cases unless $NP \\subseteq coNP/poly$.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 09:28:23 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Cai", "Leizhen", ""], ["Ye", "Junjie", ""]]}, {"id": "1509.05572", "submitter": "Kitty Meeks", "authors": "Kitty Meeks", "title": "Randomised enumeration of small witnesses using a decision oracle", "comments": "To appear in Algorithmica. Author final version, incorporating\n  reviewer comments. An extended abstract of part of this work appeared in proc\n  IPEC '16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many combinatorial problems involve determining whether a universe of $n$\nelements contains a witness consisting of $k$ elements which have some\nspecified property. In this paper we investigate the relationship between the\ndecision and enumeration versions of such problems: efficient methods are known\nfor transforming a decision algorithm into a search procedure that finds a\nsingle witness, but even finding a second witness is not so straightforward in\ngeneral. We show that, if the decision version of the problem can be solved in\ntime $f(k) \\cdot poly(n)$, there is a randomised algorithm which enumerates all\nwitnesses in time $e^{k + o(k)} \\cdot f(k) \\cdot poly(n) \\cdot N$, where $N$ is\nthe total number of witnesses. If the decision version of the problem is solved\nby a randomised algorithm which may return false negatives, then the same\nmethod allows us to output a list of witnesses in which any given witness will\nbe included with high probability. The enumeration algorithm also gives rise to\nan efficient algorithm to count the total number of witnesses when this number\nis small.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 10:20:24 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 09:54:51 GMT"}, {"version": "v3", "created": "Thu, 25 May 2017 11:04:55 GMT"}, {"version": "v4", "created": "Thu, 4 Jan 2018 10:46:20 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Meeks", "Kitty", ""]]}, {"id": "1509.05612", "submitter": "Ashutosh Rai", "authors": "Ashutosh Rai, M. S. Ramanujan, Saket Saurabh", "title": "A Parameterized Algorithm for Mixed Cut", "comments": "16 pages. arXiv admin note: substantial text overlap with\n  arXiv:1207.4079 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical Menger's theorem states that in any undirected (or directed)\ngraph $G$, given a pair of vertices $s$ and $t$, the maximum number of vertex\n(edge) disjoint paths is equal to the minimum number of vertices (edges) needed\nto disconnect from $s$ and $t$. This min-max result can be turned into a\npolynomial time algorithm to find the maximum number of vertex (edge) disjoint\npaths as well as the minimum number of vertices (edges) needed to disconnect\n$s$ from $t$. In this paper we study a mixed version of this problem, called\nMixed-Cut, where we are given an undirected graph $G$, vertices $s$ and $t$,\npositive integers $k$ and $l$ and the objective is to test whether there exist\na $k$ sized vertex set $S \\subseteq V(G)$ and an $l$ sized edge set $F\n\\subseteq E(G)$ such that deletion of $S$ and $F$ from $G$ disconnects from $s$\nand $t$. We start with a small observation that this problem is NP-complete and\nthen study this problem, in fact a much stronger generalization of this, in the\nrealm of parameterized complexity. In particular we study the Mixed-Multiway\nCut-Uncut problem where along with a set of terminals $T$, we are also given an\nequivalence relation $\\mathcal{R}$ on $T$, and the question is whether we can\ndelete at most $k$ vertices and at most $l$ edges such that connectivity of the\nterminals in the resulting graph respects $\\mathcal{R}$. Our main results is a\nfixed parameter algorithm for Mixed-Multiway Cut-Uncut using the method of\nrecursive understanding introduced by Chitnis et al. (FOCS 2012).\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 13:03:51 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Rai", "Ashutosh", ""], ["Ramanujan", "M. S.", ""], ["Saurabh", "Saket", ""]]}, {"id": "1509.05809", "submitter": "Marcin Pilipczuk", "authors": "Marek Cygan and Daniel Lokshtanov and Marcin Pilipczuk and Micha{\\l}\n  Pilipczuk and Saket Saurabh", "title": "Lower bounds for approximation schemes for Closest String", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Closest String problem one is given a family $\\mathcal S$ of\nequal-length strings over some fixed alphabet, and the task is to find a string\n$y$ that minimizes the maximum Hamming distance between $y$ and a string from\n$\\mathcal S$. While polynomial-time approximation schemes (PTASes) for this\nproblem are known for a long time [Li et al., J. ACM'02], no efficient\npolynomial-time approximation scheme (EPTAS) has been proposed so far. In this\npaper, we prove that the existence of an EPTAS for Closest String is in fact\nunlikely, as it would imply that $\\mathrm{FPT}=\\mathrm{W}[1]$, a highly\nunexpected collapse in the hierarchy of parameterized complexity classes. Our\nproof also shows that the existence of a PTAS for Closest String with running\ntime $f(\\varepsilon)\\cdot n^{o(1/\\varepsilon)}$, for any computable function\n$f$, would contradict the Exponential Time Hypothesis.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 21:54:26 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Cygan", "Marek", ""], ["Lokshtanov", "Daniel", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""], ["Saurabh", "Saket", ""]]}, {"id": "1509.05828", "submitter": "Francisco Soulignac", "authors": "Francisco J. Soulignac", "title": "A certifying and dynamic algorithm for the recognition of proper\n  circular-arc graphs", "comments": "44 pages, 8 figures, appendix with 11 pages and many figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dynamic algorithm for the recognition of proper circular-arc\n(PCA) graphs, that supports the insertion and removal of vertices (together\nwith its incident edges). The main feature of the algorithm is that it outputs\na minimally non-PCA induced subgraph when the insertion of a vertex fails. Each\noperation cost $O(\\log n + d)$ time, where $n$ is the number vertices and $d$\nis the degree of the modified vertex. When removals are disallowed, each\ninsertion is processed in $O(d)$ time. The algorithm also provides two\nconstant-time operations to query if the dynamic graph is proper Helly (PHCA)\nor proper interval (PIG). When the dynamic graph is not PHCA (resp. PIG), a\nminimally non-PHCA (resp. non-PIG) induced subgraph is obtained.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2015 00:06:19 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Soulignac", "Francisco J.", ""]]}, {"id": "1509.05831", "submitter": "Alexander Lozovskiy", "authors": "Alexander Lozovskiy", "title": "A greedy algorithm for the minimization of a ratio of same-index element\n  sums from two positive arrays", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider two ordered positive real number arrays of equal size. The problem\nis to find such set of indices of given size that the ratio of the sums of the\narray elements with those indices is minimized. In this work, in order to\nmitigate the exponential complexity of the brute force search, we present a\ngreedy algorithm applied to the search of such an index set. The main result of\nthe paper is the theorem that states that the algorithm eliminates from\ncandidates all index sets that do not contain any elements from the greedily\nselected set. We additionally prove exactness for a particular case of a ratio\nof the sums of only two elements.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2015 00:38:34 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Lozovskiy", "Alexander", ""]]}, {"id": "1509.05870", "submitter": "Yi Fan", "authors": "Yi Fan, Chengqian Li, Zongjie Ma, LjiLjana Brankovic, Vladimir\n  Estivill-Castro, Abdul Sattar", "title": "Exploiting Reduction Rules and Data Structures: Local Search for Minimum\n  Vertex Cover in Massive Graphs", "comments": "7 pages, 3 figures, 2 tables, 6 algorithms, submitted to AAAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Vertex Cover (MinVC) problem is a well-known NP-hard problem.\nRecently there has been great interest in solving this problem on real-world\nmassive graphs. For such graphs, local search is a promising approach to\nfinding optimal or near-optimal solutions. In this paper we propose a local\nsearch algorithm that exploits reduction rules and data structures to solve the\nMinVC problem in such graphs. Experimental results on a wide range of real-word\nmassive graphs show that our algorithm finds better covers than\nstate-of-the-art local search algorithms for MinVC. Also we present interesting\nresults about the complexities of some well-known heuristics.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2015 10:48:31 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Fan", "Yi", ""], ["Li", "Chengqian", ""], ["Ma", "Zongjie", ""], ["Brankovic", "LjiLjana", ""], ["Estivill-Castro", "Vladimir", ""], ["Sattar", "Abdul", ""]]}, {"id": "1509.05896", "submitter": "Marcin Wrochna", "authors": "Micha{\\l} Pilipczuk, Marcin Wrochna", "title": "On space efficiency of algorithms working on structural decompositions\n  of graphs", "comments": "An extended abstract appeared in the proceedings of STACS'16. The new\n  version is augmented with a space-efficient algorithm for Dominating Set\n  using the Chinese remainder theorem", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic programming on path and tree decompositions of graphs is a technique\nthat is ubiquitous in the field of parameterized and exponential-time\nalgorithms. However, one of its drawbacks is that the space usage is\nexponential in the decomposition's width. Following the work of Allender et al.\n[Theory of Computing, '14], we investigate whether this space complexity\nexplosion is unavoidable. Using the idea of reparameterization of Cai and\nJuedes [J. Comput. Syst. Sci., '03], we prove that the question is closely\nrelated to a conjecture that the Longest Common Subsequence problem\nparameterized by the number of input strings does not admit an algorithm that\nsimultaneously uses XP time and FPT space. Moreover, we complete the complexity\nlandscape sketched for pathwidth and treewidth by Allender et al. by\nconsidering the parameter tree-depth. We prove that computations on tree-depth\ndecompositions correspond to a model of non-deterministic machines that work in\npolynomial time and logarithmic space, with access to an auxiliary stack of\nmaximum height equal to the decomposition's depth. Together with the results of\nAllender et al., this describes a hierarchy of complexity classes for\npolynomial-time non-deterministic machines with different restrictions on the\naccess to working space, which mirrors the classic relations between treewidth,\npathwidth, and tree-depth.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2015 14:20:05 GMT"}, {"version": "v2", "created": "Thu, 12 May 2016 09:12:34 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Pilipczuk", "Micha\u0142", ""], ["Wrochna", "Marcin", ""]]}, {"id": "1509.06048", "submitter": "Abdolahad Noori Zehmakan", "authors": "Abdolahad Noori Zehmakan and Mojtaba Eslahi", "title": "A linear approximation algorithm for the BPP with the best possible\n  absolute approximation ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bin Packing Problem is one of the most important Combinatorial\nOptimization problems in optimization and has a lot of real-world applications.\nMany approximation algorithms have been presented for this problem because of\nits NP-hard nature. In this article also a new creative approximation algorithm\nis presented for this important problem. It has been proven that the best\napproximation ratio and the best time order for the Bin Packing Problem are 3/2\nand O(n), respectively unless P=NP. The presented algorithm in this article has\nthe best possible factors, O(n) and 3/2.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 20:18:46 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Zehmakan", "Abdolahad Noori", ""], ["Eslahi", "Mojtaba", ""]]}, {"id": "1509.06167", "submitter": "Matev\\v{z} Jekovec", "authors": "Matev\\v{z} Jekovec, Andrej Brodnik", "title": "Parallel Query in the Suffix Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the query string of length $m$, we explore a parallel query in a static\nsuffix tree based data structure for $p \\ll n$, where $p$ is the number of\nprocessors and $n$ is the length of the text. We present three results on CREW\nPRAM. The parallel query in the suffix trie requires $O(m + p)$ work, $O(m/p +\n\\lg p)$ time and $O(n^2)$ space in the worst case. We extend the same technique\nto the suffix tree where we show it is, by design, inherently sequential in the\nworst case. Finally we perform the parallel query using an interleaved approach\nand achieve $O(m \\lg p)$ work, $O(\\frac{m}{p} \\lg p)$ time and $O(n \\lg p)$\nspace in the worst case.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 10:02:39 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2015 11:08:42 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Jekovec", "Matev\u017e", ""], ["Brodnik", "Andrej", ""]]}, {"id": "1509.06215", "submitter": "Lukas Fleischer", "authors": "Lukas Fleischer, Manfred Kufleitner", "title": "Efficient Algorithms for Morphisms over Omega-Regular Languages", "comments": "Full version of a paper accepted to FSTTCS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphisms to finite semigroups can be used for recognizing omega-regular\nlanguages. The so-called strongly recognizing morphisms can be seen as a\ndeterministic computation model which provides minimal objects (known as the\nsyntactic morphism) and a trivial complementation procedure. We give a\nquadratic-time algorithm for computing the syntactic morphism from any given\nstrongly recognizing morphism, thereby showing that minimization is easy as\nwell. In addition, we give algorithms for efficiently solving various decision\nproblems for weakly recognizing morphisms. Weakly recognizing morphism are\noften smaller than their strongly recognizing counterparts. Finally, we\ndescribe the language operations needed for converting formulas in monadic\nsecond-order logic (MSO) into strongly recognizing morphisms, and we give some\nexperimental results.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 13:15:41 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 12:58:46 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Fleischer", "Lukas", ""], ["Kufleitner", "Manfred", ""]]}, {"id": "1509.06257", "submitter": "Tim Roughgarden", "authors": "Tim Roughgarden", "title": "Communication Complexity (for Algorithm Designers)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document collects the lecture notes from my course \"Communication\nComplexity (for Algorithm Designers),'' taught at Stanford in the winter\nquarter of 2015. The two primary goals of the course are: 1. Learn several\ncanonical problems that have proved the most useful for proving lower bounds\n(Disjointness, Index, Gap-Hamming, etc.). 2. Learn how to reduce lower bounds\nfor fundamental algorithmic problems to communication complexity lower bounds.\nAlong the way, we'll also: 3. Get exposure to lots of cool computational models\nand some famous results about them --- data streams and linear sketches,\ncompressive sensing, space-query time trade-offs in data structures,\nsublinear-time algorithms, and the extension complexity of linear programs. 4.\nScratch the surface of techniques for proving communication complexity lower\nbounds (fooling sets, corruption bounds, etc.).\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 14:59:05 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Roughgarden", "Tim", ""]]}, {"id": "1509.06332", "submitter": "Pooja Pandey", "authors": "Pooja Pandey", "title": "A note on linear fractional set packing problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we point out various errors in the paper by Rashmi Gupta and R.\nR. Saxena, Set packing problem with linear fractional objective function,\nInternational Journal of Mathematics and Computer Applications Research\n(IJMCAR), 4 (2014) 9 - 18. We also provide some additional results.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 18:29:57 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2015 21:41:03 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Pandey", "Pooja", ""]]}, {"id": "1509.06357", "submitter": "Daniel Paulusma", "authors": "Paul Bonsma, Daniel Paulusma", "title": "Using Contracted Solution Graphs for Solving Reconfiguration Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce in a general setting a dynamic programming method for solving\nreconfiguration problems. Our method is based on contracted solution graphs,\nwhich are obtained from solution graphs by performing an appropriate series of\nedge contractions that decrease the graph size without losing any critical\ninformation needed to solve the reconfiguration problem under consideration.\nOur general framework captures the approach behind known reconfiguration\nresults of Bonsma (2012) and Hatanaka, Ito and Zhou (2014). As a third example,\nwe apply the method to the following problem: given two $k$-colorings $\\alpha$\nand $\\beta$ of a graph $G$, can $\\alpha$ be modified into $\\beta$ by recoloring\none vertex of $G$ at a time, while maintaining a $k$-coloring throughout? This\nproblem is known to be PSPACE-hard even for bipartite planar graphs and $k=4$.\nBy applying our method in combination with a thorough exploitation of the graph\nstructure we obtain a polynomial time algorithm for $(k-2)$-connected chordal\ngraphs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 19:35:26 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 18:29:57 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Bonsma", "Paul", ""], ["Paulusma", "Daniel", ""]]}, {"id": "1509.06430", "submitter": "David Harris", "authors": "Bernhard Haeupler, David G. Harris", "title": "Parallel algorithms and concentration bounds for the Lovasz Local Lemma\n  via witness DAGs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lov\\'{a}sz Local Lemma (LLL) is a cornerstone principle in the\nprobabilistic method of combinatorics, and a seminal algorithm of Moser &\nTardos (2010) provides an efficient randomized algorithm to implement it. This\ncan be parallelized to give an algorithm that uses polynomially many processors\nand runs in $O(\\log^3 n)$ time on an EREW PRAM, stemming from $O(\\log n)$\nadaptive computations of a maximal independent set (MIS). Chung et al. (2014)\ndeveloped faster local and parallel algorithms, potentially running in time\n$O(\\log^2 n)$, but these algorithms require more stringent conditions than the\nLLL.\n  We give a new parallel algorithm that works under essentially the same\nconditions as the original algorithm of Moser & Tardos but uses only a single\nMIS computation, thus running in $O(\\log^2 n)$ time on an EREW PRAM. This can\nbe derandomized to give an NC algorithm running in time $O(\\log^2 n)$ as well,\nspeeding up a previous NC LLL algorithm of Chandrasekaran et al. (2013).\n  We also provide improved and tighter bounds on the run-times of the\nsequential and parallel resampling-based algorithms originally developed by\nMoser & Tardos. These apply to any problem instance in which the tighter\nShearer LLL criterion is satisfied.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 23:49:56 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2015 02:56:28 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2015 13:14:55 GMT"}, {"version": "v4", "created": "Thu, 20 Oct 2016 15:53:58 GMT"}, {"version": "v5", "created": "Wed, 5 Jul 2017 13:19:14 GMT"}, {"version": "v6", "created": "Fri, 8 Sep 2017 23:16:50 GMT"}, {"version": "v7", "created": "Thu, 28 Sep 2017 18:13:50 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Harris", "David G.", ""]]}, {"id": "1509.06464", "submitter": "Valerie King", "authors": "David Gibb, Bruce Kapron, Valerie King, Nolan Thorn", "title": "Dynamic graph connectivity with improved worst case update time and\n  sublinear space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers fully dynamic graph algorithms with both faster worst\ncase update time and sublinear space. The fully dynamic graph connectivity\nproblem is the following: given a graph on a fixed set of n nodes, process an\nonline sequence of edge insertions, edge deletions, and queries of the form \"Is\nthere a path between nodes a and b?\" In 2013, the first data structure was\npresented with worst case time per operation which was polylogarithmic in n. In\nthis paper, we shave off a factor of log n from that time, to O(log^4 n) per\nupdate. For sequences which are polynomial in length, our algorithm answers\nqueries in O(log n/\\log\\log n) time correctly with high probability and using\nO(n \\log^2 n) words (of size log n). This matches the amount of space used by\nthe most space-efficient graph connectivity streaming algorithm. We also show\nthat 2-edge connectivity can be maintained using O(n log^2 n) words with an\namortized update time of O(log^6 n).\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 04:56:11 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Gibb", "David", ""], ["Kapron", "Bruce", ""], ["King", "Valerie", ""], ["Thorn", "Nolan", ""]]}, {"id": "1509.06712", "submitter": "Deepak Mehta", "authors": "Hadrien Cambazard, Deepak Mehta, Barry O'Sullivan, Helmut Simonis", "title": "Bin Packing with Linear Usage Costs", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bin packing is a well studied problem involved in many applications. The\nclassical bin packing problem is about minimising the number of bins and\nignores how the bins are utilised. We focus in this paper, on a variant of bin\npacking that is at the heart of efficient management of data centres. In this\ncontext, servers can be viewed as bins and virtual machines as items. The\nefficient management of a data-centre involves minimising energy costs while\nensuring service quality. The assignment of virtual machines on servers and how\nthese servers are utilised has a huge impact on the energy consumption. We\nfocus on a bin packing problem where linear costs are associated to the use of\nbins to model the energy consumption. We study lower bounds based on Linear\nProgramming and extend the bin packing global constraint with cost information.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 18:20:41 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Cambazard", "Hadrien", ""], ["Mehta", "Deepak", ""], ["O'Sullivan", "Barry", ""], ["Simonis", "Helmut", ""]]}, {"id": "1509.06849", "submitter": "Sungsoo Ahn", "authors": "Sungsoo Ahn (1), Sejun Park (1), Michael Chertkov (2), Jinwoo Shin (1)\n  ((1) Korea Advanced Institute of Science and Technology (2) Los Alamos\n  National Laboratory)", "title": "Minimum Weight Perfect Matching via Blossom Belief Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-product Belief Propagation (BP) is a popular message-passing algorithm\nfor computing a Maximum-A-Posteriori (MAP) assignment over a distribution\nrepresented by a Graphical Model (GM). It has been shown that BP can solve a\nnumber of combinatorial optimization problems including minimum weight\nmatching, shortest path, network flow and vertex cover under the following\ncommon assumption: the respective Linear Programming (LP) relaxation is tight,\ni.e., no integrality gap is present. However, when LP shows an integrality gap,\nno model has been known which can be solved systematically via sequential\napplications of BP. In this paper, we develop the first such algorithm, coined\nBlossom-BP, for solving the minimum weight matching problem over arbitrary\ngraphs. Each step of the sequential algorithm requires applying BP over a\nmodified graph constructed by contractions and expansions of blossoms, i.e.,\nodd sets of vertices. Our scheme guarantees termination in O(n^2) of BP runs,\nwhere n is the number of vertices in the original graph. In essence, the\nBlossom-BP offers a distributed version of the celebrated Edmonds' Blossom\nalgorithm by jumping at once over many sub-steps with a single BP. Moreover,\nour result provides an interpretation of the Edmonds' algorithm as a sequence\nof LPs.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 05:49:53 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Ahn", "Sungsoo", ""], ["Park", "Sejun", ""], ["Chertkov", "Michael", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1509.06948", "submitter": "Konrad Kulakowski", "authors": "Konrad Ku{\\l}akowski", "title": "Dynamic concurrent van Emde Boas array", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing popularity of shared-memory multiprocessor machines has caused\nsignificant changes in the design of concurrent software. In this approach, the\nconcurrently running threads communicate and synchronize with each other\nthrough data structures in shared memory. Hence, the efficiency of these\nstructures is essential for the performance of concurrent applications. The\nneed to find new concurrent data structures prompted the author some time ago\nto propose the cvEB array modeled on the van Emde Boas Tree structure as a\ndynamic set alternative. This paper describes an improved version of that\nstructure - the dcvEB array (Dynamic Concurrent van Emde Boas Array). One of\nthe improvements involves memory usage optimization. This enhancement required\nthe design of a tree which grows and shrinks at both: the top (root) and the\nbottom (leaves) level. Another enhancement concerns the successor (and\npredecessor) search strategy. The tests performed seem to confirm the high\nperformance of the dcvEB array. They are especially visible when the range of\nkeys is significantly larger than the number of elements in the collection.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 12:53:59 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Ku\u0142akowski", "Konrad", ""]]}, {"id": "1509.06957", "submitter": "Ville Hyv\\\"onen", "authors": "Ville Hyv\\\"onen, Teemu Pitk\\\"anen, Sotiris Tasoulis, Elias\n  J\\\"a\\\"asaari, Risto Tuomainen, Liang Wang, Jukka Corander, Teemu Roos", "title": "Fast k-NN search", "comments": null, "journal-ref": "IEEE International Conference on Big Data 2016, p. 881-888", "doi": "10.1109/BigData.2016.7840682", "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient index structures for fast approximate nearest neighbor queries are\nrequired in many applications such as recommendation systems. In\nhigh-dimensional spaces, many conventional methods suffer from excessive usage\nof memory and slow response times. We propose a method where multiple random\nprojection trees are combined by a novel voting scheme. The key idea is to\nexploit the redundancy in a large number of candidate sets obtained by\nindependently generated random projections in order to reduce the number of\nexpensive exact distance evaluations. The method is straightforward to\nimplement using sparse projections which leads to a reduced memory footprint\nand fast index construction. Furthermore, it enables grouping of the required\ncomputations into big matrix multiplications, which leads to additional savings\ndue to cache effects and low-level parallelization. We demonstrate by extensive\nexperiments on a wide variety of data sets that the method is faster than\nexisting partitioning tree or hashing based approaches, making it the fastest\navailable technique on high accuracy levels.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 13:10:36 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 12:54:40 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Hyv\u00f6nen", "Ville", ""], ["Pitk\u00e4nen", "Teemu", ""], ["Tasoulis", "Sotiris", ""], ["J\u00e4\u00e4saari", "Elias", ""], ["Tuomainen", "Risto", ""], ["Wang", "Liang", ""], ["Corander", "Jukka", ""], ["Roos", "Teemu", ""]]}, {"id": "1509.06984", "submitter": "Max Bannach", "authors": "Max Bannach, Christoph Stockhusen, Till Tantau", "title": "Fast Parallel Fixed-Parameter Algorithms via Color Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fixed-parameter algorithms have been successfully applied to solve numerous\ndifficult problems within acceptable time bounds on large inputs. However, most\nfixed-parameter algorithms are inherently \\emph{sequential} and, thus, make no\nuse of the parallel hardware present in modern computers. We show that parallel\nfixed-parameter algorithms do not only exist for numerous parameterized\nproblems from the literature -- including vertex cover, packing problems,\ncluster editing, cutting vertices, finding embeddings, or finding matchings --\nbut that there are parallel algorithms working in \\emph{constant} time or at\nleast in time \\emph{depending only on the parameter} (and not on the size of\nthe input) for these problems. Phrased in terms of complexity classes, we place\nnumerous natural parameterized problems in parameterized versions of AC$^0$. On\na more technical level, we show how the \\emph{color coding} method can be\nimplemented in constant time and apply it to embedding problems for graphs of\nbounded tree-width or tree-depth and to model checking first-order formulas in\ngraphs of bounded degree.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 13:55:36 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Bannach", "Max", ""], ["Stockhusen", "Christoph", ""], ["Tantau", "Till", ""]]}, {"id": "1509.07007", "submitter": "Chidambaram Annamalai", "authors": "Chidambaram Annamalai", "title": "Finding Perfect Matchings in Bipartite Hypergraphs", "comments": "added a figure; some clarifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haxell's condition is a natural hypergraph analog of Hall's condition, which\nis a well-known necessary and sufficient condition for a bipartite graph to\nadmit a perfect matching. That is, when Haxell's condition holds it forces the\nexistence of a perfect matching in the bipartite hypergraph. Unlike in graphs,\nhowever, there is no known polynomial time algorithm to find the hypergraph\nperfect matching that is guaranteed to exist when Haxell's condition is\nsatisfied.\n  We prove the existence of an efficient algorithm to find perfect matchings in\nbipartite hypergraphs whenever a stronger version of Haxell's condition holds.\nOur algorithm can be seen as a generalization of the classical Hungarian\nalgorithm for finding perfect matchings in bipartite graphs. The techniques we\nuse to achieve this result could be of use more generally in other\ncombinatorial problems on hypergraphs where disjointness structure is crucial,\ne.g. Set Packing.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 14:30:18 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 19:22:34 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Annamalai", "Chidambaram", ""]]}, {"id": "1509.07053", "submitter": "Jakob Gruber", "authors": "Jakob Gruber", "title": "Practical Concurrent Priority Queues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Priority queues are abstract data structures which store a set of key/value\npairs and allow efficient access to the item with the minimal (maximal) key.\nSuch queues are an important element in various areas of computer science such\nas algorithmics (i.e. Dijkstra's shortest path algorithm) and operating system\n(i.e. priority schedulers).\n  The recent trend towards multiprocessor computing requires new\nimplementations of basic data structures which are able to be used concurrently\nand scale well to a large number of threads. In particular, lock-free\nstructures promise superior scalability by avoiding the use of blocking\nsynchronization primitives.\n  Concurrent priority queues have been extensively researched over the past\ndecades. In this paper, we discuss three major ideas within the field:\nfine-grained locking employs multiple locks to avoid a single bottleneck within\nthe queue; SkipLists are search structures which use randomization and\ntherefore do not require elaborate reorganization schemes; and relaxed data\nstructures trade semantic guarantees for improved scalability.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 16:35:38 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Gruber", "Jakob", ""]]}, {"id": "1509.07278", "submitter": "Frank Gurski", "authors": "Frank Gurski, Jochen Rethmann, Egon Wanke", "title": "Integer Programming Models and Parameterized Algorithms for Controlling\n  Palletizers", "comments": "27 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1307.1915", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the combinatorial FIFO Stack-Up problem, where bins have to be\nstacked-up from conveyor belts onto pallets. Given k sequences of labeled bins\nand a positive integer p, the goal is to stack-up the bins by iteratively\nremoving the first bin of one of the k sequences and put it onto a pallet\nlocated at one of p stack-up places. The FIFO Stack-Up problem asks whether\nthere is some processing of the sequences of bins such that at most p stack-up\nplaces are used. In this paper we strengthen the hardness of the FIFO Stack-Up\nby considering practical cases and the distribution of the pallets onto the\nsequences. We introduce a digraph model for this problem, the so called\ndecision graph, which allows us to give a breadth first search solution.\nFurther we apply methods to solve hard problems to the FIFO Stack-Up problem.\nIn order to evaluate our algorithms, we introduce a method to generate random,\nbut realistic instances for the FIFO Stack-Up problem. Our experimental study\nof running times shows that the breadth first search solution on the decision\ngraph combined with a cutting technique can be used to solve practical\ninstances on several thousands of bins of the FIFO Stack-Up problem. Further we\nanalyze two integer programming approaches implemented in CPLEX and GLPK. As\nexpected CPLEX can solve the instances much faster than GLPK and our pallet\nsolution approach is much better than the bin solution approach.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 09:03:49 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 15:45:15 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Gurski", "Frank", ""], ["Rethmann", "Jochen", ""], ["Wanke", "Egon", ""]]}, {"id": "1509.07404", "submitter": "Dimitrios Thilikos", "authors": "Eunjung Kim and Christophe Paul and Ignasi Sau and Dimitrios M.\n  Thilikos", "title": "Parameterized Algorithms for Min-Max Multiway Cut and List Digraph\n  Homomorphism", "comments": "An extended abstract of this work will appear in the Proceedings of\n  the 10th International Symposium on Parameterized and Exact Computation\n  (IPEC), Patras, Greece, September 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we design {\\sf FPT}-algorithms for two parameterized problems.\nThe first is \\textsc{List Digraph Homomorphism}: given two digraphs $G$ and $H$\nand a list of allowed vertices of $H$ for every vertex of $G$, the question is\nwhether there exists a homomorphism from $G$ to $H$ respecting the list\nconstraints. The second problem is a variant of \\textsc{Multiway Cut}, namely\n\\textsc{Min-Max Multiway Cut}: given a graph $G$, a non-negative integer\n$\\ell$, and a set $T$ of $r$ terminals, the question is whether we can\npartition the vertices of $G$ into $r$ parts such that (a) each part contains\none terminal and (b) there are at most $\\ell$ edges with only one endpoint in\nthis part. We parameterize \\textsc{List Digraph Homomorphism} by the number $w$\nof edges of $G$ that are mapped to non-loop edges of $H$ and we give a time\n$2^{O(\\ell\\cdot\\log h+\\ell^2\\cdot \\log \\ell)}\\cdot n^{4}\\cdot \\log n$\nalgorithm, where $h$ is the order of the host graph $H$. We also prove that\n\\textsc{Min-Max Multiway Cut} can be solved in time $2^{O((\\ell r)^2\\log \\ell\nr)}\\cdot n^{4}\\cdot \\log n$. Our approach introduces a general problem, called\n{\\sc List Allocation}, whose expressive power permits the design of\nparameterized reductions of both aforementioned problems to it. Then our\nresults are based on an {\\sf FPT}-algorithm for the {\\sc List Allocation}\nproblem that is designed using a suitable adaptation of the {\\em randomized\ncontractions} technique (introduced by [Chitnis, Cygan, Hajiaghayi, Pilipczuk,\nand Pilipczuk, FOCS 2012]).\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 15:16:06 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Kim", "Eunjung", ""], ["Paul", "Christophe", ""], ["Sau", "Ignasi", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1509.07417", "submitter": "Dominik K\\\"oppl", "authors": "Johannes Fischer, Tomohiro I, Dominik K\\\"oppl", "title": "Deterministic Sparse Suffix Sorting in the Restore Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a text $T$ of length $n$, we propose a deterministic online algorithm\ncomputing the sparse suffix array and the sparse longest common prefix array of\n$T$ in $O(c \\sqrt{\\lg n} + m \\lg m \\lg n \\lg^* n)$ time with $O(m)$ words of\nspace under the premise that the space of $T$ is rewritable, where $m \\le n$ is\nthe number of suffixes to be sorted (provided online and arbitrarily), and $c$\nis the number of characters with $m \\le c \\le n$ that must be compared for\ndistinguishing the designated suffixes.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 16:05:25 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 14:05:57 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Fischer", "Johannes", ""], ["I", "Tomohiro", ""], ["K\u00f6ppl", "Dominik", ""]]}, {"id": "1509.07422", "submitter": "Craig Wilson", "authors": "Craig Wilson and Venugopal V. Veeravalli", "title": "Adaptive Sequential Optimization with Applications to Machine Learning", "comments": "submitted to ICASSP 2016, extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework is introduced for solving a sequence of slowly changing\noptimization problems, including those arising in regression and classification\napplications, using optimization algorithms such as stochastic gradient descent\n(SGD). The optimization problems change slowly in the sense that the minimizers\nchange at either a fixed or bounded rate. A method based on estimates of the\nchange in the minimizers and properties of the optimization algorithm is\nintroduced for adaptively selecting the number of samples needed from the\ndistributions underlying each problem in order to ensure that the excess risk,\ni.e., the expected gap between the loss achieved by the approximate minimizer\nproduced by the optimization algorithm and the exact minimizer, does not exceed\na target level. Experiments with synthetic and real data are used to confirm\nthat this approach performs well.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 16:19:43 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Wilson", "Craig", ""], ["Veeravalli", "Venugopal V.", ""]]}, {"id": "1509.07437", "submitter": "Astrid Pieterse", "authors": "Bart M.P. Jansen and Astrid Pieterse", "title": "Sparsification Upper and Lower Bounds for Graph Problems and\n  Not-All-Equal SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present several sparsification lower and upper bounds for classic problems\nin graph theory and logic. For the problems 4-Coloring, (Directed) Hamiltonian\nCycle, and (Connected) Dominating Set, we prove that there is no\npolynomial-time algorithm that reduces any n-vertex input to an equivalent\ninstance, of an arbitrary problem, with bitsize O(n^{2-e}) for e > 0, unless NP\nis in coNP/poly and the polynomial-time hierarchy collapses. These results\nimply that existing linear-vertex kernels for k-Nonblocker and k-Max Leaf\nSpanning Tree (the parametric duals of (Connected) Dominating Set) cannot be\nimproved to have O(k^{2-e}) edges, unless NP is in coNP/poly. We also present a\npositive result and exhibit a non-trivial sparsification algorithm for\nd-Not-All-Equal SAT. We give an algorithm that reduces an n-variable input with\nclauses of size at most d to an equivalent input with O(n^{d-1}) clauses, for\nany fixed d. Our algorithm is based on a linear-algebraic proof of Lovasz that\nbounds the number of hyperedges in critically 3-chromatic d-uniform n-vertex\nhypergraphs by n choose d-1. We show that our kernel is tight under the\nassumption that NP is not a subset of coNP/poly.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 17:02:31 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["Pieterse", "Astrid", ""]]}, {"id": "1509.07445", "submitter": "Edith Cohen", "authors": "Edith Cohen", "title": "Multi-Objective Weighted Sampling", "comments": "14 pages; full version of a HotWeb 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  {\\em Multi-objective samples} are powerful and versatile summaries of large\ndata sets. For a set of keys $x\\in X$ and associated values $f_x \\geq 0$, a\nweighted sample taken with respect to $f$ allows us to approximate {\\em\nsegment-sum statistics} $\\text{Sum}(f;H) = \\text{sum}_{x\\in H} f_x$, for any\nsubset $H$ of the keys, with statistically-guaranteed quality that depends on\nsample size and the relative weight of $H$. When estimating $\\text{Sum}(g;H)$\nfor $g\\not=f$, however, quality guarantees are lost. A multi-objective sample\nwith respect to a set of functions $F$ provides for each $f\\in F$ the same\nstatistical guarantees as a dedicated weighted sample while minimizing the\nsummary size.\n  We analyze properties of multi-objective samples and present sampling schemes\nand meta-algortithms for estimation and optimization while showcasing two\nimportant application domains. The first are key-value data sets, where\ndifferent functions $f\\in F$ applied to the values correspond to different\nstatistics such as moments, thresholds, capping, and sum. A multi-objective\nsample allows us to approximate all statistics in $F$. The second is metric\nspaces, where keys are points, and each $f\\in F$ is defined by a set of points\n$C$ with $f_x$ being the service cost of $x$ by $C$, and $\\text{Sum}(f;X)$\nmodels centrality or clustering cost of $C$. A multi-objective sample allows us\nto estimate costs for each $f\\in F$. In these domains, multi-objective samples\nare often of small size, are efficiently to construct, and enable scalable\nestimation and optimization. We aim here to facilitate further applications of\nthis powerful technique.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 17:27:13 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2015 15:04:46 GMT"}, {"version": "v3", "created": "Tue, 19 Apr 2016 16:35:48 GMT"}, {"version": "v4", "created": "Fri, 23 Dec 2016 09:42:08 GMT"}, {"version": "v5", "created": "Sat, 29 Apr 2017 19:18:49 GMT"}, {"version": "v6", "created": "Tue, 13 Jun 2017 09:00:05 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Cohen", "Edith", ""]]}, {"id": "1509.07600", "submitter": "Yuya Higashikawa", "authors": "Yuya Higashikawa, Siu-Wing Cheng, Tsunehiko Kameda, Naoki Katoh, and\n  Shun Saburi", "title": "Minimax Regret 1-Median Problem in Dynamic Path Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the minimax regret 1-median problem in dynamic path\nnetworks. In our model, we are given a dynamic path network consisting of an\nundirected path with positive edge lengths, uniform positive edge capacity, and\nnonnegative vertex supplies. Here, each vertex supply is unknown but only an\ninterval of supply is known. A particular assignment of supply to each vertex\nis called a scenario. Given a scenario s and a sink location x in a dynamic\npath network, let us consider the evacuation time to x of a unit supply given\non a vertex by s. The cost of x under s is defined as the sum of evacuation\ntimes to x for all supplies given by s, and the median under s is defined as a\nsink location which minimizes this cost. The regret for x under s is defined as\nthe cost of x under s minus the cost of the median under s. Then, the problem\nis to find a sink location such that the maximum regret for all possible\nscenarios is minimized. We propose an O(n^3) time algorithm for the minimax\nregret 1-median problem in dynamic path networks with uniform capacity, where n\nis the number of vertices in the network.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 06:46:36 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Higashikawa", "Yuya", ""], ["Cheng", "Siu-Wing", ""], ["Kameda", "Tsunehiko", ""], ["Katoh", "Naoki", ""], ["Saburi", "Shun", ""]]}, {"id": "1509.07680", "submitter": "Zhentao Li", "authors": "Ken-ichi Kawarabayashi, Zhentao Li, Bruce Reed", "title": "Connectivity Preserving Iterative Compaction and Finding 2 Disjoint\n  Rooted Paths in Linear Time", "comments": "83 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how to combine two algorithmic techniques to obtain\nlinear time algorithms for various optimization problems on graphs, and present\na subroutine which will be useful in doing so.\n  The first technique is iterative shrinking. In the first phase of an\niterative shrinking algorithm, we construct a sequence of graphs of decreasing\nsize $G_1,\\ldots,G_\\ell$ where $G_1$ is the initial input, $G_\\ell$ is a graph\non which the problem is easy, and $G_i$ is obtained from $G_{i+1}$ via some\nshrinking algorithm. In the second phase we work through the sequence in\nreverse, repeatedly constructing a solution for a graph from the solution for\nits successor. In an iterative compaction algorithm, we insist that the graphs\ndecrease by a constant fraction of the entire graph.\n  Another approach to solving optimization problems is to exploit the\nstructural properties implied by the connectivity of the input graph. This\napproach can be used on graphs which are not highly connected by decomposing an\ninput graph into its highly connected pieces, solving subproblems on these\nspecially structured pieces and then combining their solutions.\n  We combine these two techniques by developing compaction algorithms which\nwhen applied to the highly connected pieces preserve their connectivity\nproperties. The structural properties this connectivity implies can be helpful\nboth in finding further compactions in later iterations and when we are\nmanipulating solutions in the second phase of an iterative compaction\nalgorithm.\n  To illustrate how this compaction algorithm can be used as a subroutine, we\npresent a linear time algorithm that given four vertices $\\{s_1,s_2,t_1,t_2\\}$\nof a graph $G$, either finds a pair of disjoint paths $P_1$ and $P_2$ of $G$\nsuch that $P_i$ has endpoints $s_i$ and $t_i$, or returns a planar embedding of\nan auxiliary graph which shows that no such pair exists.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 11:13:38 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Kawarabayashi", "Ken-ichi", ""], ["Li", "Zhentao", ""], ["Reed", "Bruce", ""]]}, {"id": "1509.07715", "submitter": "Yixuan Li", "authors": "Yixuan Li, Kun He, David Bindel and John Hopcroft", "title": "Uncovering the Small Community Structure in Large Networks: A Local\n  Spectral Approach", "comments": "10pages, published in WWW2015 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large graphs arise in a number of contexts and understanding their structure\nand extracting information from them is an important research area. Early\nalgorithms on mining communities have focused on the global structure, and\noften run in time functional to the size of the entire graph. Nowadays, as we\noften explore networks with billions of vertices and find communities of size\nhundreds, it is crucial to shift our attention from macroscopic structure to\nmicroscopic structure when dealing with large networks. A growing body of work\nhas been adopting local expansion methods in order to identify the community\nfrom a few exemplary seed members.\n  In this paper, we propose a novel approach for finding overlapping\ncommunities called LEMON (Local Expansion via Minimum One Norm). Different from\nPageRank-like diffusion methods, LEMON finds the community by seeking a sparse\nvector in the span of the local spectra such that the seeds are in its support.\nWe show that LEMON can achieve the highest detection accuracy among\nstate-of-the-art proposals. The running time depends on the size of the\ncommunity rather than that of the entire graph. The algorithm is easy to\nimplement, and is highly parallelizable.\n  Moreover, given that networks are not all similar in nature, a comprehensive\nanalysis on how the local expansion approach is suited for uncovering\ncommunities in different networks is still lacking. We thoroughly evaluate our\napproach using both synthetic and real-world datasets across different domains,\nand analyze the empirical variations when applying our method to inherently\ndifferent networks in practice. In addition, the heuristics on how the quality\nand quantity of the seed set would affect the performance are provided.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 13:50:34 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Li", "Yixuan", ""], ["He", "Kun", ""], ["Bindel", "David", ""], ["Hopcroft", "John", ""]]}, {"id": "1509.07808", "submitter": "Thomas Rothvoss", "authors": "Elaine Levey and Thomas Rothvoss", "title": "A (1+epsilon)-Approximation for Makespan Scheduling with Precedence\n  Constraints using LP Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a classical problem in scheduling, one has $n$ unit size jobs with a\nprecedence order and the goal is to find a schedule of those jobs on $m$\nidentical machines as to minimize the makespan. It is one of the remaining four\nopen problems from the book of Garey & Johnson whether or not this problem is\n$\\mathbf{NP}$-hard for $m=3$.\n  We prove that for any fixed $\\varepsilon$ and $m$, an LP-hierarchy lift of\nthe time-indexed LP with a slightly super poly-logarithmic number of $r =\n(\\log(n))^{\\Theta(\\log \\log n)}$ rounds provides a $(1 +\n\\varepsilon)$-approximation. For example Sherali-Adams suffices as hierarchy.\nThis implies an algorithm that yields a $(1+\\varepsilon)$-approximation in time\n$n^{O(r)}$. The previously best approximation algorithms guarantee a $2 -\n\\frac{7}{3m+1}$-approximation in polynomial time for $m \\geq 4$ and\n$\\frac{4}{3}$ for $m=3$. Our algorithm is based on a recursive scheduling\napproach where in each step we reduce the correlation in form of long chains.\nOur method adds to the rather short list of examples where hierarchies are\nactually useful to obtain better approximation algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 17:43:05 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 01:15:58 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Levey", "Elaine", ""], ["Rothvoss", "Thomas", ""]]}, {"id": "1509.07935", "submitter": "Weidong Li", "authors": "Weidong Li, Xi Liu, Xiaolu Zhang, Xuejie Zhang", "title": "A note on the dynamic dominant resource fairness mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-resource fair allocation has beena hot topic of resource allocation.\nMost recently, a dynamic dominant resource fairness (DRF) mechanism is proposed\nfor dynamic multi-resource fair allocation. In this paper, we prove that the\ncompetitive ratio of the dynamic DRF mechanism is the reciprocal of the number\nof resource types, for two different objectives. Moreover, we develop a\nlinear-time algorithm to find a dynamic DRF solution at each step.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2015 01:36:54 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 03:33:03 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Li", "Weidong", ""], ["Liu", "Xi", ""], ["Zhang", "Xiaolu", ""], ["Zhang", "Xuejie", ""]]}, {"id": "1509.07952", "submitter": "Petr Hlin\\v{e}n\\'y", "authors": "Markus Chimani, Petr Hlin\\v{e}n\\'y", "title": "Inserting Multiple Edges into a Planar Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a connected planar (but not yet embedded) graph and $F$ a set of\nadditional edges not yet in $G$. The {multiple edge insertion} problem (MEI)\nasks for a drawing of $G+F$ with the minimum number of pairwise edge crossings,\nsuch that the subdrawing of $G$ is plane. An optimal solution to this problem\napproximates the crossing number of the graph $G+F$.\n  Finding an exact solution to MEI is NP-hard for general $F$, but linear time\nsolvable for the special case of $|F|=1$ (SODA01, Algorithmica) or when all of\n$F$ are incident to a new vertex (SODA09).\n  The complexity for general $F$ but with constant $k=|F|$ was open, but\nalgorithms both with relative and absolute approximation guarantees have been\npresented (SODA11, ICALP11). We show that the problem is fixed parameter\ntractable (FPT) in $k$ for biconnected $G$, or if the cut vertices of $G$ have\ndegrees bounded by a constant. We give the first exact algorithm for this\nproblem; it requires only $O(|V(G)|)$ time for any constant $k$.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2015 09:02:35 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Chimani", "Markus", ""], ["Hlin\u011bn\u00fd", "Petr", ""]]}, {"id": "1509.07983", "submitter": "Soledad Villar", "authors": "Takayuki Iguchi, Dustin G. Mixon, Jesse Peterson, Soledad Villar", "title": "Probably certifiably correct k-means clustering", "comments": "Major revision from previous version. This paper is a extension of\n  and improvement to the authors' preprint [arXiv:1505.04778]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Bandeira [arXiv:1509.00824] introduced a new type of algorithm (the\nso-called probably certifiably correct algorithm) that combines fast solvers\nwith the optimality certificates provided by convex relaxations. In this paper,\nwe devise such an algorithm for the problem of k-means clustering. First, we\nprove that Peng and Wei's semidefinite relaxation of k-means is tight with high\nprobability under a distribution of planted clusters called the stochastic ball\nmodel. Our proof follows from a new dual certificate for integral solutions of\nthis semidefinite program. Next, we show how to test the optimality of a\nproposed k-means solution using this dual certificate in quasilinear time.\nFinally, we analyze a version of spectral clustering from Peng and Wei that is\ndesigned to solve k-means in the case of two clusters. In particular, we show\nthat this quasilinear-time method typically recovers planted clusters under the\nstochastic ball model.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2015 14:09:56 GMT"}, {"version": "v2", "created": "Sat, 23 Apr 2016 19:55:12 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Iguchi", "Takayuki", ""], ["Mixon", "Dustin G.", ""], ["Peterson", "Jesse", ""], ["Villar", "Soledad", ""]]}, {"id": "1509.07996", "submitter": "Yixuan Li", "authors": "Yixuan Li, Kun He, David Bindel and John Hopcroft", "title": "Overlapping Community Detection via Local Spectral Clustering", "comments": "Extended version to the conference proceeding in WWW'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large graphs arise in a number of contexts and understanding their structure\nand extracting information from them is an important research area. Early\nalgorithms on mining communities have focused on the global structure, and\noften run in time functional to the size of the entire graph. Nowadays, as we\noften explore networks with billions of vertices and find communities of size\nhundreds, it is crucial to shift our attention from macroscopic structure to\nmicroscopic structure in large networks. A growing body of work has been\nadopting local expansion methods in order to identify the community members\nfrom a few exemplary seed members.\n  In this paper, we propose a novel approach for finding overlapping\ncommunities called LEMON (Local Expansion via Minimum One Norm). The algorithm\nfinds the community by seeking a sparse vector in the span of the local spectra\nsuch that the seeds are in its support. We show that LEMON can achieve the\nhighest detection accuracy among state-of-the-art proposals. The running time\ndepends on the size of the community rather than that of the entire graph. The\nalgorithm is easy to implement, and is highly parallelizable. We further\nprovide theoretical analysis on the local spectral properties, bounding the\nmeasure of tightness of extracted community in terms of the eigenvalues of\ngraph Laplacian.\n  Moreover, given that networks are not all similar in nature, a comprehensive\nanalysis on how the local expansion approach is suited for uncovering\ncommunities in different networks is still lacking. We thoroughly evaluate our\napproach using both synthetic and real-world datasets across different domains,\nand analyze the empirical variations when applying our method to inherently\ndifferent networks in practice. In addition, the heuristics on how the seed set\nquality and quantity would affect the performance are provided.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2015 15:27:38 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Li", "Yixuan", ""], ["He", "Kun", ""], ["Bindel", "David", ""], ["Hopcroft", "John", ""]]}, {"id": "1509.08123", "submitter": "Dana Moshkovitz", "authors": "Ofer Grossman, Dana Moshkovitz", "title": "Amplification and Derandomization Without Slowdown", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present techniques for decreasing the error probability of randomized\nalgorithms and for converting randomized algorithms to deterministic\n(non-uniform) algorithms. Unlike most existing techniques that involve\nrepetition of the randomized algorithm and hence a slowdown, our techniques\nproduce algorithms with a similar run-time to the original randomized\nalgorithms. The amplification technique is related to a certain stochastic\nmulti-armed bandit problem. The derandomization technique - which is the main\ncontribution of this work - points to an intriguing connection between\nderandomization and sketching/sparsification.\n  We demonstrate the techniques by showing applications to Max-Cut on dense\ngraphs, approximate clique on graphs that contain a large clique, constraint\nsatisfaction problems on dense bipartite graphs and the list decoding to unique\ndecoding problem for the Reed-Muller code.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 19:01:47 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Grossman", "Ofer", ""], ["Moshkovitz", "Dana", ""]]}, {"id": "1509.08216", "submitter": "William Kuszmaul", "authors": "William Kuszmaul", "title": "Fast Algorithms for Finding Pattern Avoiders and Counting Pattern\n  Occurrences in Permutations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $\\Pi$ of permutation patterns of length at most $k$, we present\nan algorithm for building $S_{\\le n}(\\Pi)$, the set of permutations of length\nat most $n$ avoiding the patterns in $\\Pi$, in time $O(|S_{\\le n - 1}(\\Pi)|\n\\cdot k + |S_{n}(\\Pi)|)$. Additionally, we present an $O(n!k)$-time algorithm\nfor counting the number of copies of patterns from $\\Pi$ in each permutation in\n$S_n$. Surprisingly, when $|\\Pi| = 1$, this runtime can be improved to $O(n!)$,\nspending only constant time per permutation. Whereas the previous best\nalgorithms, based on generate-and-check, take exponential time per permutation\nanalyzed, all of our algorithms take time at most polynomial per outputted\npermutation.\n  If we want to solve only the enumerative variant of each problem, computing\n$|S_{\\le n}(\\Pi)|$ or tallying permutations according to $\\Pi$-patterns, rather\nthan to store information about every permutation, then all of our algorithms\ncan be implemented in $O(n^{k+1}k)$ space.\n  Using our algorithms, we generated $|S_5(\\Pi)|, \\ldots, |S_{16}(\\Pi)|$ for\neach $\\Pi \\subseteq S_4$ with $|\\Pi| > 4$, and analyzed OEIS matches. We\nobtained a number of potentially novel pattern-avoidance conjectures.\n  Our algorithms extend to considering permutations in any set closed under\nstandardization of subsequences. Our algorithms also partially adapt to\nconsidering vincular patterns.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 07:08:27 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2015 17:18:05 GMT"}, {"version": "v3", "created": "Sun, 30 Oct 2016 04:11:26 GMT"}, {"version": "v4", "created": "Fri, 17 Mar 2017 02:21:24 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Kuszmaul", "William", ""]]}, {"id": "1509.08240", "submitter": "Gerth St{\\o}lting Brodal", "authors": "Gerth St{\\o}lting Brodal", "title": "External Memory Three-Sided Range Reporting and Top-$k$ Queries with\n  Sublogarithmic Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An external memory data structure is presented for maintaining a dynamic set\nof $N$ two-dimensional points under the insertion and deletion of points, and\nsupporting 3-sided range reporting queries and top-$k$ queries, where top-$k$\nqueries report the $k$~points with highest $y$-value within a given $x$-range.\nFor any constant $0<\\varepsilon\\leq \\frac{1}{2}$, a data structure is\nconstructed that supports updates in amortized $O(\\frac{1}{\\varepsilon\nB^{1-\\varepsilon}}\\log_B N)$ IOs and queries in amortized\n$O(\\frac{1}{\\varepsilon}\\log_B N+K/B)$ IOs, where $B$ is the external memory\nblock size, and $K$ is the size of the output to the query (for top-$k$ queries\n$K$ is the minimum of $k$ and the number of points in the query interval). The\ndata structure uses linear space. The update bound is a significant factor\n$B^{1-\\varepsilon}$ improvement over the previous best update bounds for the\ntwo query problems, while staying within the same query and space bounds.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 09:02:54 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Brodal", "Gerth St\u00f8lting", ""]]}, {"id": "1509.08251", "submitter": "Paul Bonsma", "authors": "Christoph Berkholz, Paul Bonsma, Martin Grohe", "title": "Tight Lower and Upper Bounds for the Complexity of Canonical Colour\n  Refinement", "comments": "An extended abstract of this paper appeared in the proceedings of\n  ESA'13, LNCS 8125, pp. 145-156", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An assignment of colours to the vertices of a graph is stable if any two\nvertices of the same colour have identically coloured neighbourhoods. The goal\nof colour refinement is to find a stable colouring that uses a minimum number\nof colours. This is a widely used subroutine for graph isomorphism testing\nalgorithms, since any automorphism needs to be colour preserving. We give an\n$O((m+n)\\log n)$ algorithm for finding a canonical version of such a stable\ncolouring, on graphs with $n$ vertices and $m$ edges. We show that no faster\nalgorithm is possible, under some modest assumptions about the type of\nalgorithm, which captures all known colour refinement algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 09:36:08 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Berkholz", "Christoph", ""], ["Bonsma", "Paul", ""], ["Grohe", "Martin", ""]]}, {"id": "1509.08608", "submitter": "Sudip Biswas", "authors": "Sharma V. Thankachan, Manish Patil, Rahul Shah, and Sudip Biswas", "title": "Probabilistic Threshold Indexing for Uncertain Strings", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strings form a fundamental data type in computer systems. String searching\nhas been extensively studied since the inception of computer science.\nIncreasingly many applications have to deal with imprecise strings or strings\nwith fuzzy information in them. String matching becomes a probabilistic event\nwhen a string contains uncertainty, i.e. each position of the string can have\ndifferent probable characters with associated probability of occurrence for\neach character. Such uncertain strings are prevalent in various applications\nsuch as biological sequence data, event monitoring and automatic ECG\nannotations. We explore the problem of indexing uncertain strings to support\nefficient string searching. In this paper we consider two basic problems of\nstring searching, namely substring searching and string listing. In substring\nsearching, the task is to find the occurrences of a deterministic string in an\nuncertain string. We formulate the string listing problem for uncertain\nstrings, where the objective is to output all the strings from a collection of\nstrings, that contain probable occurrence of a deterministic query string.\nIndexing solution for both these problems are significantly more challenging\nfor uncertain strings than for deterministic strings. Given a construction time\nprobability value $\\tau$, our indexes can be constructed in linear space and\nsupports queries in near optimal time for arbitrary values of probability\nthreshold parameter greater than $\\tau$. To the best of our knowledge, this is\nthe first indexing solution for searching in uncertain strings that achieves\nstrong theoretical bound and supports arbitrary values of probability threshold\nparameter. We also propose an approximate substring search index that can\nanswer substring search queries with an additive error in optimal time. We\nconduct experiments to evaluate the performance of our indexes.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 06:49:32 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Thankachan", "Sharma V.", ""], ["Patil", "Manish", ""], ["Shah", "Rahul", ""], ["Biswas", "Sudip", ""]]}, {"id": "1509.08639", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Tuned and GPU-accelerated parallel data mining from comparable corpora", "comments": "Machine translation, comparable corpora, Machine learning, NLP,\n  Knowledge-free learning, Unsupervised bi-lingual data mining", "journal-ref": "Lecture Notes in Artificial Intelligence, p. 32-40, ISBN:\n  978-3-319-24032-9, Springer, 2015", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multilingual nature of the world makes translation a crucial requirement\ntoday. Parallel dictionaries constructed by humans are a widely-available\nresource, but they are limited and do not provide enough coverage for good\nquality translation purposes, due to out-of-vocabulary words and neologisms.\nThis motivates the use of statistical translation systems, which are\nunfortunately dependent on the quantity and quality of training data. Such has\na very limited availability especially for some languages and very narrow text\ndomains. Is this research we present our improvements to Yalign mining\nmethodology by reimplementing the comparison algorithm, introducing a tuning\nscripts and by improving performance using GPU computing acceleration. The\nexperiments are conducted on various text domains and bi-data is extracted from\nthe Wikipedia dumps.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 08:44:14 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.08671", "submitter": "Samin Aref", "authors": "Iman Kazemian and Samin Aref", "title": "A green perspective on capacitated time-dependent vehicle routing\n  problem with time windows", "comments": "17 pages, accepted pre-print (author copy)", "journal-ref": "Int. J. Supply Chain and Inventory Management, Vol. 2, No. 1,\n  pp.20-38 (2017)", "doi": "10.1504/IJSCIM.2017.086372", "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a novel approach to the vehicle routing problem by\nfocusing on greenhouse gas emissions and fuel consumption aiming to mitigate\nadverse environmental effects of transportation. A time-dependent model with\ntime windows is developed to incorporate speed and schedule in transportation.\nThe model considers speed limits for different times of the day in a realistic\ndelivery context. Due to the complexity of solving the model, a simulated\nannealing algorithm is proposed to find solutions with high quality in a timely\nmanner. Our method can be used in practice to lower fuel consumption and\ngreenhouse gas emissions while total route cost is also controlled to some\nextent. The capability of method is depicted by numerical examples productively\nsolved within 3.5% to the exact optimal for small and mid-sized problems.\nMoreover, comparatively appropriate solutions are obtained for large problems\nin averagely one tenth of the exact method restricted computation time.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 10:05:29 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 02:48:51 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Kazemian", "Iman", ""], ["Aref", "Samin", ""]]}, {"id": "1509.08807", "submitter": "R.B. Sandeep", "authors": "N. R. Aravind, R. B. Sandeep, Naveen Sivadasan", "title": "Parameterized Lower Bounds and Dichotomy Results for the NP-completeness\n  of $H$-free Edge Modification Problems", "comments": "16 pages. arXiv admin note: substantial text overlap with\n  arXiv:1507.06341", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph $H$, the $H$-free Edge Deletion problem asks whether there exist\nat most $k$ edges whose deletion from the input graph $G$ results in a graph\nwithout any induced copy of $H$. $H$-free Edge Completion and $H$-free Edge\nEditing are defined similarly where only completion (addition) of edges are\nallowed in the former and both completion and deletion are allowed in the\nlatter. We completely settle the classical complexities of these problems by\nproving that $H$-free Edge Deletion is NP-complete if and only if $H$ is a\ngraph with at least two edges, $H$-free Edge Completion is NP-complete if and\nonly if $H$ is a graph with at least two non-edges and $H$-free Edge Editing is\nNP-complete if and only if $H$ is a graph with at least three vertices.\nAdditionally, we prove that, these NP-complete problems cannot be solved in\nparameterized subexponential time, i.e., in time $2^{o(k)}\\cdot |G|^{O(1)}$,\nunless Exponential Time Hypothesis fails. Furthermore, we obtain implications\non the incompressibility of these problems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 15:33:06 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Aravind", "N. R.", ""], ["Sandeep", "R. B.", ""], ["Sivadasan", "Naveen", ""]]}, {"id": "1509.08937", "submitter": "Nikos Bikakis", "authors": "Nikos Bikakis, Karim Benouaret, Dimitris Sacharidis", "title": "Finding Desirable Objects under Group Categorical Preferences", "comments": "To appear in Knowledge and Information Systems Journal (KAIS),\n  Springer 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering a group of users, each specifying individual preferences over\ncategorical attributes, the problem of determining a set of objects that are\nobjectively preferable by all users is challenging on two levels. First, we\nneed to determine the preferable objects based on the categorical preferences\nfor each user, and second we need to reconcile possible conflicts among users'\npreferences. A naive solution would first assign degrees of match between each\nuser and each object, by taking into account all categorical attributes, and\nthen for each object combine these matching degrees across users to compute the\ntotal score of an object. Such an approach, however, performs two series of\naggregation, among categorical attributes and then across users, which\ncompletely obscure and blur individual preferences. Our solution, instead of\ncombining individual matching degrees, is to directly operate on categorical\nattributes, and define an objective Pareto-based aggregation for group\npreferences. Building on our interpretation, we tackle two distinct but\nrelevant problems: finding the Pareto-optimal objects, and objectively ranking\nobjects with respect to the group preferences. To increase the efficiency when\ndealing with categorical attributes, we introduce an elegant transformation of\ncategorical attribute values into numerical values, which exhibits certain nice\nproperties and allows us to use well-known index structures to accelerate the\nsolutions to the two problems. In fact, experiments on real and synthetic data\nshow that our index-based techniques are an order of magnitude faster than\nbaseline approaches, scaling up to millions of objects and thousands of users.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 20:15:46 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Bikakis", "Nikos", ""], ["Benouaret", "Karim", ""], ["Sacharidis", "Dimitris", ""]]}, {"id": "1509.09147", "submitter": "Yun Kuen Cheung", "authors": "Yun Kuen Cheung and Monika Henzinger and Martin Hoefer and Martin\n  Starnberger", "title": "Combinatorial Auctions with Conflict-Based Externalities", "comments": "This is the full version of our WINE 2015 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial auctions (CA) are a well-studied area in algorithmic mechanism\ndesign. However, contrary to the standard model, empirical studies suggest that\na bidder's valuation often does not depend solely on the goods assigned to him.\nFor instance, in adwords auctions an advertiser might not want his ads to be\ndisplayed next to his competitors' ads. In this paper, we propose and analyze\nseveral natural graph-theoretic models that incorporate such negative\nexternalities, in which bidders form a directed conflict graph with maximum\nout-degree $\\Delta$. We design algorithms and truthful mechanisms for social\nwelfare maximization that attain approximation ratios depending on $\\Delta$.\n  For CA, our results are twofold: (1) A lottery that eliminates conflicts by\ndiscarding bidders/items independent of the bids. It allows to apply any\ntruthful $\\alpha$-approximation mechanism for conflict-free valuations and\nyields an $\\mathcal{O}(\\alpha\\Delta)$-approximation mechanism. (2) For\nfractionally sub-additive valuations, we design a rounding algorithm via a\nnovel combination of a semi-definite program and a linear program, resulting in\na cone program; the approximation ratio is $\\mathcal{O}((\\Delta \\log \\log\n\\Delta)/\\log \\Delta)$. The ratios are almost optimal given existing hardness\nresults.\n  For the prominent application of adwords auctions, we present several\nalgorithms for the most relevant scenario when the number of items is small. In\nparticular, we design a truthful mechanism with approximation ratio $o(\\Delta)$\nwhen the number of items is only logarithmic in the number of bidders.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 12:26:08 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Cheung", "Yun Kuen", ""], ["Henzinger", "Monika", ""], ["Hoefer", "Martin", ""], ["Starnberger", "Martin", ""]]}, {"id": "1509.09228", "submitter": "Srikrishnan Divakaran", "authors": "Srikrishnan Divakaran", "title": "Fast Algorithms for Exact String Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a pattern string $P$ of length $n$ and a query string $T$ of length\n$m$, where the characters of $P$ and $T$ are drawn from an alphabet of size\n$\\Delta$, the {\\em exact string matching} problem consists of finding all\noccurrences of $P$ in $T$. For this problem, we present algorithms that in\n$O(n\\Delta^2)$ time pre-process $P$ to essentially identify $sparse(P)$, a\nrarely occurring substring of $P$, and then use it to find occurrences of $P$\nin $T$ efficiently. Our algorithms require a worst case search time of $O(m)$,\nand expected search time of $O(m/min(|sparse(P)|, \\Delta))$, where\n$|sparse(P)|$ is at least $\\delta$ (i.e. the number of distinct characters in\n$P$), and for most pattern strings it is observed to be $\\Omega(n^{1/2})$.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 15:42:42 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Divakaran", "Srikrishnan", ""]]}, {"id": "1509.09237", "submitter": "Dominik K\\\"oppl", "authors": "Pawe{\\l} Gawrychowski and Tomohiro I and Shunsuke Inenaga and Dominik\n  K\\\"oppl and Florin Manea", "title": "Efficiently Finding All Maximal $\\alpha$-gapped Repeats", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For $\\alpha\\geq 1$, an $\\alpha$-gapped repeat in a word $w$ is a factor $uvu$\nof $w$ such that $|uv|\\leq \\alpha |u|$; the two factors $u$ in such a repeat\nare called arms, while the factor $v$ is called gap. Such a repeat is called\nmaximal if its arms cannot be extended simultaneously with the same symbol to\nthe right or, respectively, to the left. In this paper we show that the number\nof maximal $\\alpha$-gapped repeats that may occur in a word is upper bounded by\n$18\\alpha n$. This allows us to construct an algorithm finding all the maximal\n$\\alpha$-gapped repeats of a word in $O(\\alpha n)$; this is optimal, in the\nworst case, as there are words that have $\\Theta(\\alpha n)$ maximal\n$\\alpha$-gapped repeats. Our techniques can be extended to get comparable\nresults in the case of $\\alpha$-gapped palindromes, i.e., factors\n$uvu^\\mathrm{T}$ with $|uv|\\leq \\alpha |u|$.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 16:13:06 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["I", "Tomohiro", ""], ["Inenaga", "Shunsuke", ""], ["K\u00f6ppl", "Dominik", ""], ["Manea", "Florin", ""]]}, {"id": "1509.09271", "submitter": "Andrew M. Childs", "authors": "Andrew M. Childs, Wim van Dam, Shih-Han Hung, Igor E. Shparlinski", "title": "Optimal quantum algorithm for polynomial interpolation", "comments": "17 pages, minor improvements, added conjecture about multivariate\n  interpolation", "journal-ref": "Proceedings of the 43rd International Colloquium on Automata,\n  Languages, and Programming (ICALP 2016), pp. 16:1-16:13 (2016)", "doi": "10.4230/LIPIcs.ICALP.2016.16", "report-no": null, "categories": "quant-ph cs.CC cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the number of quantum queries required to determine the\ncoefficients of a degree-d polynomial over GF(q). A lower bound shown\nindependently by Kane and Kutin and by Meyer and Pommersheim shows that d/2+1/2\nquantum queries are needed to solve this problem with bounded error, whereas an\nalgorithm of Boneh and Zhandry shows that d quantum queries are sufficient. We\nshow that the lower bound is achievable: d/2+1/2 quantum queries suffice to\ndetermine the polynomial with bounded error. Furthermore, we show that d/2+1\nqueries suffice to achieve probability approaching 1 for large q. These upper\nbounds improve results of Boneh and Zhandry on the insecurity of cryptographic\nprotocols against quantum attacks. We also show that our algorithm's success\nprobability as a function of the number of queries is precisely optimal.\nFurthermore, the algorithm can be implemented with gate complexity poly(log q)\nwith negligible decrease in the success probability. We end with a conjecture\nabout the quantum query complexity of multivariate polynomial interpolation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 17:47:39 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 18:36:30 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Childs", "Andrew M.", ""], ["van Dam", "Wim", ""], ["Hung", "Shih-Han", ""], ["Shparlinski", "Igor E.", ""]]}]