[{"id": "2107.00068", "submitter": "Zixiu Wang", "authors": "Zixiu Wang, Yiwen Guo and Hu Ding", "title": "Robust Coreset for Continuous-and-Bounded Learning (with Outliers)", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this big data era, we often confront large-scale data in many machine\nlearning tasks. A common approach for dealing with large-scale data is to build\na small summary, {\\em e.g.,} coreset, that can efficiently represent the\noriginal input. However, real-world datasets usually contain outliers and most\nexisting coreset construction methods are not resilient against outliers (in\nparticular, the outliers can be located arbitrarily in the space by an\nadversarial attacker). In this paper, we propose a novel robust coreset method\nfor the {\\em continuous-and-bounded learning} problem (with outliers) which\nincludes a broad range of popular optimization objectives in machine learning,\nlike logistic regression and $ k $-means clustering. Moreover, our robust\ncoreset can be efficiently maintained in fully-dynamic environment. To the best\nof our knowledge, this is the first robust and fully-dynamic coreset\nconstruction method for these optimization problems. We also conduct the\nexperiments to evaluate the effectiveness of our robust coreset in practice.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 19:24:20 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Wang", "Zixiu", ""], ["Guo", "Yiwen", ""], ["Ding", "Hu", ""]]}, {"id": "2107.00072", "submitter": "David Schaller", "authors": "David Schaller, Marc Hellmuth, Peter F. Stadler", "title": "A Linear-Time Algorithm for the Common Refinement of Rooted Phylogenetic\n  Trees on a Common Leaf Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding a common refinement of a set of rooted trees with\ncommon leaf set $L$ appears naturally in mathematical phylogenetics whenever\npoorly resolved information on the same taxa from different sources is to be\nreconciled. This constitutes a special case of the well-studied supertree\nproblem, where the leaf sets of the input trees may differ. Algorithms that\nsolve the rooted tree compatibility problem are of course applicable to this\nspecial case. However, they require sophisticated auxiliary data structures and\nhave a running time of at least $O(k|L|\\log^2(k|L|))$ for $k$ input trees.\nHere, we show that the problem can be solved in $O(k|L|)$ time using a simple\nbottom-up algorithm called LinCR. An implementation of LinCR in Python is\nfreely available at https://github.com/david-schaller/tralda.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 19:45:18 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Schaller", "David", ""], ["Hellmuth", "Marc", ""], ["Stadler", "Peter F.", ""]]}, {"id": "2107.00145", "submitter": "Marcin Bienkowski", "authors": "Marcin Bienkowski, Martin B\\\"ohm, Martin Kouteck\\'y, Thomas\n  Rothvo{\\ss}, Ji\\v{r}\\'i Sgall, Pavel Vesel\\'y", "title": "Improved Analysis of Online Balanced Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the online balanced graph repartitioning problem, one has to maintain a\nclustering of $n$ nodes into $\\ell$ clusters, each having $k = n / \\ell$ nodes.\nDuring runtime, an online algorithm is given a stream of communication requests\nbetween pairs of nodes: an inter-cluster communication costs one unit, while\nthe intra-cluster communication is free. An algorithm can change the\nclustering, paying unit cost for each moved node.\n  This natural problem admits a simple $O(\\ell^2 \\cdot k^2)$-competitive\nalgorithm COMP, whose performance is far apart from the best known lower bound\nof $\\Omega(\\ell \\cdot k)$. One of open questions is whether the dependency on\n$\\ell$ can be made linear; this question is of practical importance as in the\ntypical datacenter application where virtual machines are clustered on physical\nservers, $\\ell$ is of several orders of magnitude larger than $k$. We answer\nthis question affirmatively, proving that a simple modification of COMP is\n$(\\ell \\cdot 2^{O(k)})$-competitive.\n  On the technical level, we achieve our bound by translating the problem to a\nsystem of linear integer equations and using Graver bases to show the existence\nof a ``small'' solution.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 23:07:14 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Bienkowski", "Marcin", ""], ["B\u00f6hm", "Martin", ""], ["Kouteck\u00fd", "Martin", ""], ["Rothvo\u00df", "Thomas", ""], ["Sgall", "Ji\u0159\u00ed", ""], ["Vesel\u00fd", "Pavel", ""]]}, {"id": "2107.00251", "submitter": "Quentin Stout", "authors": "Quentin F. Stout", "title": "$L_p$ Isotonic Regression Algorithms Using an $L_0$ Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Significant advances in maximum flow algorithms have changed the relative\nperformance of various approaches to isotonic regression. If the transitive\nclosure is given then the standard approach used for $L_0$ (Hamming distance)\nisotonic regression (finding anti-chains in the transitive closure of the\nviolator graph), combined with new flow algorithms, gives an $L_1$ algorithm\ntaking $\\tilde{\\Theta}(n^2+n^\\frac{3}{2} \\log U )$ time, where $U$ is the\nmaximum vertex weight. The previous fastest was $\\Theta(n^3)$. Similar results\nare obtained for $L_2$ and for $L_p$ approximations, $1 < p < \\infty$. For\nweighted points in $d$-dimensional space with coordinate-wise ordering, $d \\geq\n3$, $L_0, L_1$ and $L_2$ regressions can be found in only $o(n^\\frac{3}{2}\n\\log^d n \\log U)$ time, improving on the previous best of $\\tilde{\\Theta}(n^2\n\\log^d n)$, and for unweighted points the time is $O(n^{\\frac{4}{3}+o(1)}\n\\log^d n)$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 07:04:58 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Stout", "Quentin F.", ""]]}, {"id": "2107.00314", "submitter": "Daan Van Den Berg", "authors": "Joeri Sleegers and Daan van den Berg", "title": "Backtracking (the) Algorithms on the Hamiltonian Cycle Problem", "comments": "Not yet peer-reviewed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Even though the Hamiltonian cycle problem is NP-complete, many of its problem\ninstances aren't. In fact, almost all the hard instances reside in one area:\nnear the Koml\\'os-Szemer\\'edi bound, of $\\frac{1}{2}\\ v\\cdot ln(v) +\n\\frac{1}{2}\\ v\\cdot ln( ln(v))$ edges, where randomly generated graphs have an\napproximate 50\\% chance of being Hamiltonian. If the number of edges is either\nmuch higher or much lower, the problem is not hard -- most backtracking\nalgorithms decide such instances in (near) polynomial time. Recently however,\ntargeted search efforts have identified very hard Hamiltonian cycle problem\ninstances very far away from the Koml\\'os-Szemer\\'edi bound. In that study, the\nused backtracking algorithm was Vandegriend-Culberson's, which was supposedly\nthe most efficient of all Hamiltonian backtracking algorithms.\n  In this paper, we make a unified large scale quantitative comparison for the\nbest known backtracking algorithms described between 1877 and 2016. We confirm\nthe suspicion that the Koml\\'os-Szemer\\'edi bound is a hard area for all\nbacktracking algorithms, but also that Vandegriend-Culberson is indeed the most\nefficient algorithm, when expressed in consumed computing time. When measured\nin recursive effectiveness however, the algorithm by Frank Rubin, almost half a\ncentury old, performs best. In a more general algorithmic assessment, we\nconjecture that edge pruning and non-Hamiltonicity checks might be largely\nresponsible for these recursive savings. When expressed in system time however,\ndenser problem instances require much more time per recursion. This is most\nlikely due to the costliness of the extra search pruning procedures, which are\nrelatively elaborate. We supply large amounts of experimental data, and a\nunified single-program implementation for all six algorithms. All data and\nalgorithmic source code is made public for further use by our colleagues.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 09:07:51 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Sleegers", "Joeri", ""], ["Berg", "Daan van den", ""]]}, {"id": "2107.00378", "submitter": "Jan-Hendrik Lorenz", "authors": "Florian W\\\"orz and Jan-Hendrik Lorenz", "title": "Evidence for Long-Tails in SLS Algorithms", "comments": "To appear at ESA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic local search (SLS) is a successful paradigm for solving the\nsatisfiability problem of propositional logic. A recent development in this\narea involves solving not the original instance, but a modified, yet logically\nequivalent one. Empirically, this technique was found to be promising as it\nimproves the performance of state-of-the-art SLS solvers.\n  Currently, there is only a shallow understanding of how this modification\ntechnique affects the runtimes of SLS solvers. Thus, we model this modification\nprocess and conduct an empirical analysis of the hardness of logically\nequivalent formulas. Our results are twofold. First, if the modification\nprocess is treated as a random process, a lognormal distribution perfectly\ncharacterizes the hardness; implying that the hardness is long-tailed. This\nmeans that the modification technique can be further improved by implementing\nan additional restart mechanism. Thus, as a second contribution, we\ntheoretically prove that all algorithms exhibiting this long-tail property can\nbe further improved by restarts. Consequently, all SAT solvers employing this\nmodification technique can be enhanced.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 11:31:39 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["W\u00f6rz", "Florian", ""], ["Lorenz", "Jan-Hendrik", ""]]}, {"id": "2107.00403", "submitter": "Rajni Dabas", "authors": "Rajni Dabas and Neelima Gupta", "title": "On Variants of Facility Location Problem with Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the extension of two variants of the facility location\nproblem (FL) to make them robust towards a few distantly located clients.\nFirst, $k$-facility location problem ($k$FL), a common generalization of FL and\n$k$ median problems, is a well studied problem in literature. In the second\nvariant, lower bounded facility location (LBFL), we are given a bound on the\nminimum number of clients that an opened facility must serve. Lower bounds are\nrequired in many applications like profitability in commerce and load balancing\nin transportation problem. In both the cases, the cost of the solution may be\nincreased grossly by a few distantly located clients, called the outliers.\nThus, in this work, we extend $k$FL and LBFL to make them robust towards the\noutliers. For $k$FL with outliers ($k$FLO) we present the first (constant)\nfactor approximation violating the cardinality requirement by +1. As a\nby-product, we also obtain the first approximation for FLO based on\nLP-rounding. For LBFLO, we present a tri-criteria solution with a trade-off\nbetween the violations in lower bounds and the number of outliers. With a\nviolation of $1/2$ in lower bounds, we get a violation of $2$ in outliers.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 12:22:16 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Dabas", "Rajni", ""], ["Gupta", "Neelima", ""]]}, {"id": "2107.00446", "submitter": "Moses Ganardi", "authors": "Moses Ganardi", "title": "Compression by Contracting Straight-Line Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In grammar-based compression a string is represented by a context-free\ngrammar, also called a straight-line program (SLP), that generates only that\nstring. We refine a recent balancing result stating that one can transform an\nSLP of size $g$ in linear time into an equivalent SLP of size $O(g)$ so that\nthe height of the unique derivation tree is $O(\\log N)$ where $N$ is the length\nof the represented string (FOCS 2019). We introduce a new class of balanced\nSLPs, called contracting SLPs, where for every rule $A \\to \\beta_1 \\dots\n\\beta_k$ the string length of every variable $\\beta_i$ on the right-hand side\nis smaller by a constant factor than the string length of $A$. In particular,\nthe derivation tree of a contracting SLP has the property that every subtree\nhas logarithmic height in its leaf size. We show that a given SLP of size $g$\ncan be transformed in linear time into an equivalent contracting SLP of size\n$O(g)$ with rules of constant length.\n  We present an application to the navigation problem in compressed unranked\ntrees, represented by forest straight-line programs (FSLPs). We extend a linear\nspace data structure by Reh and Sieber (2020) by the operation of moving to the\n$i$-th child in time $O(\\log d)$ where $d$ is the degree of the current node.\nContracting SLPs are also applied to the finger search problem over\nSLP-compressed strings where one wants to access positions near to a\npre-specified finger position, ideally in $O(\\log d)$ time where $d$ is the\ndistance between the accessed position and the finger. We give a linear space\nsolution where one can access symbols or move the finger in time $O(\\log d +\n\\log^{(t)} N)$ for any constant $t$ where $\\log^{(t)} N$ is the $t$-fold\nlogarithm of $N$. This improves a previous solution by Bille, Christiansen,\nCording, and G{\\o}rtz (2018) with access/move time $O(\\log d + \\log \\log N)$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 13:45:48 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ganardi", "Moses", ""]]}, {"id": "2107.00526", "submitter": "Alexander Braun", "authors": "Alexander Braun, Matthias Buttkus and Thomas Kesselheim", "title": "Asymptotically Optimal Welfare of Posted Pricing for Multiple Items with\n  MHR Distributions", "comments": "To appear at the 29th Annual European Symposium on Algorithms (ESA\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of posting prices for unit-demand buyers if all $n$\nbuyers have identically distributed valuations drawn from a distribution with\nmonotone hazard rate. We show that even with multiple items asymptotically\noptimal welfare can be guaranteed.\n  Our main results apply to the case that either a buyer's value for different\nitems are independent or that they are perfectly correlated. We give mechanisms\nusing dynamic prices that obtain a $1 - \\Theta \\left( \\frac{1}{\\log\nn}\\right)$-fraction of the optimal social welfare in expectation. Furthermore,\nwe devise mechanisms that only use static item prices and are $1 - \\Theta\n\\left( \\frac{\\log\\log\\log n}{\\log n}\\right)$-competitive compared to the\noptimal social welfare. As we show, both guarantees are asymptotically optimal,\neven for a single item and exponential distributions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 15:11:49 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Braun", "Alexander", ""], ["Buttkus", "Matthias", ""], ["Kesselheim", "Thomas", ""]]}, {"id": "2107.00572", "submitter": "Thomas Erlebach", "authors": "Evripidis Bampis, Christoph D\\\"urr, Thomas Erlebach, Murilo S. de\n  Lima, Nicole Megow, Jens Schl\\\"oter", "title": "Orienting (hyper)graphs under explorable stochastic uncertainty", "comments": "An extended abstract appears in the proceedings of the 29th Annual\n  European Symposium on Algorithms (ESA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a hypergraph with uncertain node weights following known probability\ndistributions, we study the problem of querying as few nodes as possible until\nthe identity of a node with minimum weight can be determined for each\nhyperedge. Querying a node has a cost and reveals the precise weight of the\nnode, drawn from the given probability distribution. Using competitive\nanalysis, we compare the expected query cost of an algorithm with the expected\ncost of an optimal query set for the given instance. For the general case, we\ngive a polynomial-time $f(\\alpha)$-competitive algorithm, where $f(\\alpha)\\in\n[1.618+\\epsilon,2]$ depends on the approximation ratio $\\alpha$ for an\nunderlying vertex cover problem. We also show that no algorithm using a similar\napproach can be better than $1.5$-competitive. Furthermore, we give\npolynomial-time $4/3$-competitive algorithms for bipartite graphs with\narbitrary query costs and for hypergraphs with a single hyperedge and uniform\nquery costs, with matching lower bounds.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 16:10:30 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Bampis", "Evripidis", ""], ["D\u00fcrr", "Christoph", ""], ["Erlebach", "Thomas", ""], ["de Lima", "Murilo S.", ""], ["Megow", "Nicole", ""], ["Schl\u00f6ter", "Jens", ""]]}, {"id": "2107.00629", "submitter": "Radu Curticapean", "authors": "Radu Curticapean, Holger Dell, Thore Husfeldt", "title": "Modular counting of subgraphs: Matchings, matching-splittable graphs,\n  and paths", "comments": "23 pages, to appear at ESA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We systematically investigate the complexity of counting subgraph patterns\nmodulo fixed integers. For example, it is known that the parity of the number\nof $k$-matchings can be determined in polynomial time by a simple reduction to\nthe determinant. We generalize this to an $n^{f(t,s)}$-time algorithm to\ncompute modulo $2^t$ the number of subgraph occurrences of patterns that are\n$s$ vertices away from being matchings. This shows that the known\npolynomial-time cases of subgraph detection (Jansen and Marx, SODA 2015) carry\nover into the setting of counting modulo $2^t$.\n  Complementing our algorithm, we also give a simple and self-contained proof\nthat counting $k$-matchings modulo odd integers $q$ is Mod_q-W[1]-complete and\nprove that counting $k$-paths modulo $2$ is Parity-W[1]-complete, answering an\nopen question by Bj\\\"orklund, Dell, and Husfeldt (ICALP 2015).\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:38:02 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Curticapean", "Radu", ""], ["Dell", "Holger", ""], ["Husfeldt", "Thore", ""]]}, {"id": "2107.00684", "submitter": "Csaba D. Toth", "authors": "Sujoy Bhore and Csaba D. T\\'oth", "title": "Online Euclidean Spanners", "comments": "22 pages, 8 figures. An extended abstract of this paper will appear\n  in the Proceedings of ESA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the online Euclidean spanners problem for points in\n$\\mathbb{R}^d$. Suppose we are given a sequence of $n$ points $(s_1,s_2,\\ldots,\ns_n)$ in $\\mathbb{R}^d$, where point $s_i$ is presented in step~$i$ for\n$i=1,\\ldots, n$. The objective of an online algorithm is to maintain a\ngeometric $t$-spanner on $S_i=\\{s_1,\\ldots, s_i\\}$ for each step~$i$.\n  First, we establish a lower bound of $\\Omega(\\varepsilon^{-1}\\log n / \\log\n\\varepsilon^{-1})$ for the competitive ratio of any online\n$(1+\\varepsilon)$-spanner algorithm, for a sequence of $n$ points in\n1-dimension. We show that this bound is tight, and there is an online algorithm\nthat can maintain a $(1+\\varepsilon)$-spanner with competitive ratio\n$O(\\varepsilon^{-1}\\log n / \\log \\varepsilon^{-1})$. Next, we design online\nalgorithms for sequences of points in $\\mathbb{R}^d$, for any constant $d\\ge\n2$, under the $L_2$ norm. We show that previously known incremental algorithms\nachieve a competitive ratio $O(\\varepsilon^{-(d+1)}\\log n)$. However, if the\nalgorithm is allowed to use additional points (Steiner points), then it is\npossible to substantially improve the competitive ratio in terms of\n$\\varepsilon$. We describe an online Steiner $(1+\\varepsilon)$-spanner\nalgorithm with competitive ratio $O(\\varepsilon^{(1-d)/2} \\log n)$. As a\ncounterpart, we show that the dependence on $n$ cannot be eliminated in\ndimensions $d \\ge 2$. In particular, we prove that any online spanner algorithm\nfor a sequence of $n$ points in $\\mathbb{R}^d$ under the $L_2$ norm has\ncompetitive ratio $\\Omega(f(n))$, where $\\lim_{n\\rightarrow\n\\infty}f(n)=\\infty$. Finally, we provide improved lower bounds under the $L_1$\nnorm: $\\Omega(\\varepsilon^{-2}/\\log \\varepsilon^{-1})$ in the plane and\n$\\Omega(\\varepsilon^{-d})$ in $\\mathbb{R}^d$ for $d\\geq 3$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 18:18:52 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Bhore", "Sujoy", ""], ["T\u00f3th", "Csaba D.", ""]]}, {"id": "2107.00761", "submitter": "Francesco Silvestri", "authors": "Elia Costa and Francesco Silvestri", "title": "On the Bike Spreading Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE cs.LG cs.SI math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A free-floating bike-sharing system (FFBSS) is a dockless rental system where\nan individual can borrow a bike and returns it everywhere, within the service\narea. To improve the rental service, available bikes should be distributed over\nthe entire service area: a customer leaving from any position is then more\nlikely to find a near bike and then to use the service. Moreover, spreading\nbikes among the entire service area increases urban spatial equity since the\nbenefits of FFBSS are not a prerogative of just a few zones. For guaranteeing\nsuch distribution, the FFBSS operator can use vans to manually relocate bikes,\nbut it incurs high economic and environmental costs. We propose a novel\napproach that exploits the existing bike flows generated by customers to\ndistribute bikes. More specifically, by envisioning the problem as an Influence\nMaximization problem, we show that it is possible to position batches of bikes\non a small number of zones, and then the daily use of FFBSS will efficiently\nspread these bikes on a large area. We show that detecting these areas is\nNP-complete, but there exists a simple and efficient $1-1/e$ approximation\nalgorithm; our approach is then evaluated on a dataset of rides from the\nfree-floating bike-sharing system of the city of Padova.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 22:14:31 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Costa", "Elia", ""], ["Silvestri", "Francesco", ""]]}, {"id": "2107.00774", "submitter": "Shyam Narayanan", "authors": "Hossein Esfandiari, Vahab Mirrokni, Shyam Narayanan", "title": "Almost Tight Approximation Algorithms for Explainable Clustering", "comments": "27 pages. Added references to independent work, as well as a table of\n  results, pseudocode, and improved introduction. Note: first version was\n  uploaded on July 1, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, due to an increasing interest for transparency in artificial\nintelligence, several methods of explainable machine learning have been\ndeveloped with the simultaneous goal of accuracy and interpretability by\nhumans. In this paper, we study a recent framework of explainable clustering\nfirst suggested by Dasgupta et al.~\\cite{dasgupta2020explainable}.\nSpecifically, we focus on the $k$-means and $k$-medians problems and provide\nnearly tight upper and lower bounds.\n  First, we provide an $O(\\log k \\log \\log k)$-approximation algorithm for\nexplainable $k$-medians, improving on the best known algorithm of\n$O(k)$~\\cite{dasgupta2020explainable} and nearly matching the known\n$\\Omega(\\log k)$ lower bound~\\cite{dasgupta2020explainable}. In addition, in\nlow-dimensional spaces $d \\ll \\log k$, we show that our algorithm also provides\nan $O(d \\log^2 d)$-approximate solution for explainable $k$-medians. This\nimproves over the best known bound of $O(d \\log k)$ for low\ndimensions~\\cite{laber2021explainable}, and is a constant for constant\ndimensional spaces. To complement this, we show a nearly matching $\\Omega(d)$\nlower bound. Next, we study the $k$-means problem in this context and provide\nan $O(k \\log k)$-approximation algorithm for explainable $k$-means, improving\nover the $O(k^2)$ bound of Dasgupta et al. and the $O(d k \\log k)$ bound of\n\\cite{laber2021explainable}. To complement this we provide an almost tight\n$\\Omega(k)$ lower bound, improving over the $\\Omega(\\log k)$ lower bound of\nDasgupta et al. Given an approximate solution to the classic $k$-means and\n$k$-medians, our algorithm for $k$-medians runs in time $O(kd \\log^2 k )$ and\nour algorithm for $k$-means runs in time $ O(k^2 d)$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 23:49:23 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 16:39:17 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Esfandiari", "Hossein", ""], ["Mirrokni", "Vahab", ""], ["Narayanan", "Shyam", ""]]}, {"id": "2107.00798", "submitter": "Liren Shan", "authors": "Konstantin Makarychev, Liren Shan", "title": "Near-optimal Algorithms for Explainable k-Medians and k-Means", "comments": "28 pages, 4 figures, ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of explainable $k$-medians and $k$-means introduced\nby Dasgupta, Frost, Moshkovitz, and Rashtchian~(ICML 2020). In this problem,\nour goal is to find a \\emph{threshold decision tree} that partitions data into\n$k$ clusters and minimizes the $k$-medians or $k$-means objective. The obtained\nclustering is easy to interpret because every decision node of a threshold tree\nsplits data based on a single feature into two groups. We propose a new\nalgorithm for this problem which is $\\tilde O(\\log k)$ competitive with\n$k$-medians with $\\ell_1$ norm and $\\tilde O(k)$ competitive with $k$-means.\nThis is an improvement over the previous guarantees of $O(k)$ and $O(k^2)$ by\nDasgupta et al (2020). We also provide a new algorithm which is $O(\\log^{3/2}\nk)$ competitive for $k$-medians with $\\ell_2$ norm. Our first algorithm is\nnear-optimal: Dasgupta et al (2020) showed a lower bound of $\\Omega(\\log k)$\nfor $k$-medians; in this work, we prove a lower bound of $\\tilde\\Omega(k)$ for\n$k$-means. We also provide a lower bound of $\\Omega(\\log k)$ for $k$-medians\nwith $\\ell_2$ norm.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 02:07:12 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Shan", "Liren", ""]]}, {"id": "2107.00819", "submitter": "Mingda Qiao", "authors": "Guy Blanc, Jane Lange, Mingda Qiao, Li-Yang Tan", "title": "Decision tree heuristics can fail, even in the smoothed setting", "comments": "To appear in RANDOM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Greedy decision tree learning heuristics are mainstays of machine learning\npractice, but theoretical justification for their empirical success remains\nelusive. In fact, it has long been known that there are simple target functions\nfor which they fail badly (Kearns and Mansour, STOC 1996).\n  Recent work of Brutzkus, Daniely, and Malach (COLT 2020) considered the\nsmoothed analysis model as a possible avenue towards resolving this disconnect.\nWithin the smoothed setting and for targets $f$ that are $k$-juntas, they\nshowed that these heuristics successfully learn $f$ with depth-$k$ decision\ntree hypotheses. They conjectured that the same guarantee holds more generally\nfor targets that are depth-$k$ decision trees.\n  We provide a counterexample to this conjecture: we construct targets that are\ndepth-$k$ decision trees and show that even in the smoothed setting, these\nheuristics build trees of depth $2^{\\Omega(k)}$ before achieving high accuracy.\nWe also show that the guarantees of Brutzkus et al. cannot extend to the\nagnostic setting: there are targets that are very close to $k$-juntas, for\nwhich these heuristics build trees of depth $2^{\\Omega(k)}$ before achieving\nhigh accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 04:24:55 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Blanc", "Guy", ""], ["Lange", "Jane", ""], ["Qiao", "Mingda", ""], ["Tan", "Li-Yang", ""]]}, {"id": "2107.00850", "submitter": "Yeganeh Alimohammadi", "authors": "Yeganeh Alimohammadi, Persi Diaconis, Mohammad Roghani, Amin Saberi", "title": "Sequential importance sampling for estimating expectations over the\n  space of perfect matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper makes three contributions to estimating the number of perfect\nmatching in bipartite graphs. First, we prove that the popular sequential\nimportance sampling algorithm works in polynomial time for dense bipartite\ngraphs. More carefully, our algorithm gives a $(1-\\epsilon)$-approximation for\nthe number of perfect matchings of a $\\lambda$-dense bipartite graph, using\n$O(n^{\\frac{1-2\\lambda}{8\\lambda}+\\epsilon^{-2}})$ samples. With size $n$ on\neach side and for $\\frac{1}{2}>\\lambda>0$, a $\\lambda$-dense bipartite graph\nhas all degrees greater than $(\\lambda+\\frac{1}{2})n$.\n  Second, practical applications of the algorithm requires many calls to\nmatching algorithms. A novel preprocessing step is provided which makes\nsignificant improvements.\n  Third, three applications are provided. The first is for counting Latin\nsquares, the second is a practical way of computing the greedy algorithm for a\ncard guessing game with feedback, and the third is for stochastic block models.\nIn all three examples, sequential importance sampling allows treating practical\nproblems of reasonably large sizes.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 05:48:40 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Alimohammadi", "Yeganeh", ""], ["Diaconis", "Persi", ""], ["Roghani", "Mohammad", ""], ["Saberi", "Amin", ""]]}, {"id": "2107.00995", "submitter": "Flore Sentenac", "authors": "Nathan Noiry, Flore Sentenac, Vianney Perchet", "title": "Online Matching in Sparse Random Graphs: Non-Asymptotic Performances of\n  Greedy Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by sequential budgeted allocation problems, we investigate online\nmatching problems where connections between vertices are not i.i.d., but they\nhave fixed degree distributions -- the so-called configuration model. We\nestimate the competitive ratio of the simplest algorithm, GREEDY, by\napproximating some relevant stochastic discrete processes by their continuous\ncounterparts, that are solutions of an explicit system of partial differential\nequations. This technique gives precise bounds on the estimation errors, with\narbitrarily high probability as the problem size increases. In particular, it\nallows the formal comparison between different configuration models. We also\nprove that, quite surprisingly, GREEDY can have better performance guarantees\nthan RANKING, another celebrated algorithm for online matching that usually\noutperforms the former.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 12:18:19 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Noiry", "Nathan", ""], ["Sentenac", "Flore", ""], ["Perchet", "Vianney", ""]]}, {"id": "2107.01133", "submitter": "Faisal Abu-Khzam", "authors": "Faisal N. Abu-Khzam, Norma Makarem and Maryam Shehab", "title": "An Improved Fixed-Parameter Algorithm for 2-Club Cluster Edge Deletion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A 2-club is a graph of diameter at most two. In the decision version of the\nparametrized {\\sc 2-Club Cluster Edge Deletion} problem, an undirected graph\n$G$ is given along with an integer $k\\geq 0$ as parameter, and the question is\nwhether $G$ can be transformed into a disjoint union of 2-clubs by deleting at\nmost $k$ edges. A simple fixed-parameter algorithm solves the problem in\n$\\mathcal{O}^*(3^k)$, and a decade-old algorithm was claimed to have an\nimproved running time of $\\mathcal{O}^*(2.74^k)$ via a sophisticated case\nanalysis. Unfortunately, this latter algorithm suffers from a flawed branching\nscenario. In this paper, an improved fixed-parameter algorithm is presented\nwith a running time in $\\mathcal{O}^*(2.695^k)$.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 15:22:25 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Abu-Khzam", "Faisal N.", ""], ["Makarem", "Norma", ""], ["Shehab", "Maryam", ""]]}, {"id": "2107.01250", "submitter": "William Kuszmaul", "authors": "Michael A. Bender, Bradley C. Kuszmaul, William Kuszmaul", "title": "Linear Probing Revisited: Tombstones Mark the Death of Primary\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First introduced in 1954, linear probing is one of the oldest data structures\nin computer science, and due to its unrivaled data locality, it continues to be\none of the fastest hash tables in practice. It is widely believed and taught,\nhowever, that linear probing should never be used at high load factors; this is\nbecause primary-clustering effects cause insertions at load factor $1 - 1 /x$\nto take expected time $\\Theta(x^2)$ (rather than the ideal $\\Theta(x)$). The\ndangers of primary clustering, first discovered by Knuth in 1963, have been\ntaught to generations of computer scientists, and have influenced the design of\nsome of many widely used hash tables.\n  We show that primary clustering is not a foregone conclusion. We demonstrate\nthat small design decisions in how deletions are implemented have dramatic\neffects on the asymptotic performance of insertions, so that, even if a hash\ntable operates continuously at a load factor $1 - \\Theta(1/x)$, the expected\namortized cost per operation is $\\tilde{O}(x)$. This is because tombstones\ncreated by deletions actually cause an anti-clustering effect that combats\nprimary clustering.\n  We also present a new variant of linear probing (which we call graveyard\nhashing) that completely eliminates primary clustering on \\emph{any} sequence\nof operations: if, when an operation is performed, the current load factor is\n$1 - 1/x$ for some $x$, then the expected cost of the operation is $O(x)$. One\ncorollary is that, in the external-memory model with a data blocks of size $B$,\ngraveyard hashing offers the following remarkable guarantee: at any load factor\n$1 - 1/x$ satisfying $x = o(B)$, graveyard hashing achieves $1 + o(1)$ expected\nblock transfers per operation. Past external-memory hash tables have only been\nable to offer a $1 + o(1)$ guarantee when the block size $B$ is at least\n$\\Omega(x^2)$.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 19:51:09 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Bender", "Michael A.", ""], ["Kuszmaul", "Bradley C.", ""], ["Kuszmaul", "William", ""]]}, {"id": "2107.01335", "submitter": "Cyrus Rashtchian", "authors": "Cyrus Rashtchian, David P. Woodruff, Peng Ye, Hanlin Zhu", "title": "Average-Case Communication Complexity of Statistical Problems", "comments": "28 pages. Conference on Learning Theory (COLT), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical problems, such as planted clique, its variants, and\nsparse principal component analysis in the context of average-case\ncommunication complexity. Our motivation is to understand the\nstatistical-computational trade-offs in streaming, sketching, and query-based\nmodels. Communication complexity is the main tool for proving lower bounds in\nthese models, yet many prior results do not hold in an average-case setting. We\nprovide a general reduction method that preserves the input distribution for\nproblems involving a random graph or matrix with planted structure. Then, we\nderive two-party and multi-party communication lower bounds for detecting or\nfinding planted cliques, bipartite cliques, and related problems. As a\nconsequence, we obtain new bounds on the query complexity in the edge-probe,\nvector-matrix-vector, matrix-vector, linear sketching, and\n$\\mathbb{F}_2$-sketching models. Many of these results are nearly tight, and we\nuse our techniques to provide simple proofs of some known lower bounds for the\nedge-probe model.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 03:31:37 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Rashtchian", "Cyrus", ""], ["Woodruff", "David P.", ""], ["Ye", "Peng", ""], ["Zhu", "Hanlin", ""]]}, {"id": "2107.01350", "submitter": "Marvin Williams", "authors": "Marvin Williams, Peter Sanders, and Roman Dementiev", "title": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Priority queues with parallel access are an attractive data structure for\napplications like prioritized online scheduling, discrete event simulation, or\ngreedy algorithms. However, a classical priority queue constitutes a severe\nbottleneck in this context, leading to very small throughput. Hence, there has\nbeen significant interest in concurrent priority queues with relaxed semantics.\nWe investigate the complementary quality criteria rank error (how close are\ndeleted elements to the global minimum) and delay (for each element x, how many\nelements with lower priority are deleted before x). In this paper, we introduce\nMultiQueues as a natural approach to relaxed priority queues based on multiple\nsequential priority queues. Their naturally high theoretical scalability is\nfurther enhanced by using three orthogonal ways of batching operations on the\nsequential queues. Experiments indicate that MultiQueues present a very good\nperformance-quality tradeoff and considerably outperform competing approaches\nin at least one of these aspects. We employ a seemingly paradoxical technique\nof \"wait-free locking\" that might be of more general interest to convert\nsequential data structures to relaxed concurrent data structures.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 05:57:44 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 17:02:19 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Williams", "Marvin", ""], ["Sanders", "Peter", ""], ["Dementiev", "Roman", ""]]}, {"id": "2107.01391", "submitter": "Peeyush Kumar", "authors": "Peeyush Kumar, Ayushe Gangal, Sunita Kumari and Sunita Tiwari", "title": "Recombinant Sort: N-Dimensional Cartesian Spaced Algorithm Designed from\n  Synergetic Combination of Hashing, Bucket, Counting and Radix Sort", "comments": null, "journal-ref": "Ing. des Sys. dInfo. 2020; 25 Number 5 655-688", "doi": "10.18280/isi.250513", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Sorting is an essential operation which is widely used and is fundamental to\nsome very basic day to day utilities like searches, databases, social networks\nand much more. Optimizing this basic operation in terms of complexity as well\nas efficiency is cardinal. Optimization is achieved with respect to space and\ntime complexities of the algorithm. In this paper, a novel left-field\nN-dimensional cartesian spaced sorting method is proposed by combining the best\ncharacteristics of bucket sort, counting sort and radix sort, in addition to\nemploying hashing and dynamic programming for making the method more efficient.\nComparison between the proposed sorting method and various existing sorting\nmethods like bubble sort, insertion sort, selection sort, merge sort, heap\nsort, counting sort, bucket sort, etc., has also been performed. The time\ncomplexity of the proposed model is estimated to be linear i.e. O(n) for the\nbest, average and worst cases, which is better than every sorting algorithm\nintroduced till date.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 09:47:09 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Kumar", "Peeyush", ""], ["Gangal", "Ayushe", ""], ["Kumari", "Sunita", ""], ["Tiwari", "Sunita", ""]]}, {"id": "2107.01428", "submitter": "Konrad Dabrowski", "authors": "Konrad K. Dabrowski and Peter Jonsson and Sebastian Ordyniak and\n  George Osipov", "title": "Solving Infinite-Domain CSPs Using the Patchwork Property", "comments": "34 pages, 2 figures. Parts of this article appeared in the\n  proceedings of the 35th AAAI Conference on Artificial Intelligence (AAAI\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constraint satisfaction problem (CSP) has important applications in\ncomputer science and AI. In particular, infinite-domain CSPs have been\nintensively used in subareas of AI such as spatio-temporal reasoning. Since\nconstraint satisfaction is a computationally hard problem, much work has been\ndevoted to identifying restricted problems that are efficiently solvable. One\nway of doing this is to restrict the interactions of variables and constraints,\nand a highly successful approach is to bound the treewidth of the underlying\nprimal graph. Bodirsky & Dalmau [J. Comput. System. Sci. 79(1), 2013] and Huang\net al. [Artif. Intell. 195, 2013] proved that CSP$(\\Gamma)$ can be solved in\n$n^{f(w)}$ time (where $n$ is the size of the instance, $w$ is the treewidth of\nthe primal graph and $f$ is a computable function) for certain classes of\nconstraint languages $\\Gamma$. We improve this bound to $f(w) \\cdot n^{O(1)}$,\nwhere the function $f$ only depends on the language $\\Gamma$, for CSPs whose\nbasic relations have the patchwork property. Hence, such problems are\nfixed-parameter tractable and our algorithm is asymptotically faster than the\nprevious ones. Additionally, our approach is not restricted to binary\nconstraints, so it is applicable to a strictly larger class of problems than\nthat of Huang et al. However, there exist natural problems that are covered by\nBodirsky & Dalmau's algorithm but not by ours, and we begin investigating ways\nof generalising our results to larger families of languages. We also analyse\nour algorithm with respect to its running time and show that it is optimal\n(under the Exponential Time Hypothesis) for certain languages such as Allen's\nInterval Algebra.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 13:04:41 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Dabrowski", "Konrad K.", ""], ["Jonsson", "Peter", ""], ["Ordyniak", "Sebastian", ""], ["Osipov", "George", ""]]}, {"id": "2107.01471", "submitter": "Hanyu Li", "authors": "Zhaohua Chen, Xiaotie Deng, Wenhan Huang, Hanyu Li and Yuhao Li", "title": "On Tightness of the Tsaknakis-Spirakis Algorithm for Approximate Nash\n  Equilibrium", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the minimum approximate ratio for Nash equilibrium of bi-matrix games\nhas derived a series of studies, started with 3/4, followed by 1/2, 0.38 and\n0.36, finally the best approximate ratio of 0.3393 by Tsaknakis and Spirakis\n(TS algorithm for short). Efforts to improve the results remain not successful\nin the past 14 years. This work makes the first progress to show that the bound\nof 0.3393 is indeed tight for the TS algorithm. Next, we characterize all\npossible tight game instances for the TS algorithm. It allows us to conduct\nextensive experiments to study the nature of the TS algorithm and to compare it\nwith other algorithms. We find that this lower bound is not smoothed for the TS\nalgorithm in that any perturbation on the initial point may deviate away from\nthis tight bound approximate solution. Other approximate algorithms such as\nFictitious Play and Regret Matching also find better approximate solutions.\nHowever, the new distributed algorithm for approximate Nash equilibrium by\nCzumaj et al. performs consistently at the same bound of 0.3393. This proves\nour lower bound instances generated against the TS algorithm can serve as a\nbenchmark in design and analysis of approximate Nash equilibrium algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 17:49:52 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 13:52:05 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Chen", "Zhaohua", ""], ["Deng", "Xiaotie", ""], ["Huang", "Wenhan", ""], ["Li", "Hanyu", ""], ["Li", "Yuhao", ""]]}, {"id": "2107.01607", "submitter": "Fabio Henrique Viduani Martinez", "authors": "Eloi Araujo and Luiz Rozante and Diego P. Rubert and Fabio V. Martinez", "title": "Algorithms for normalized multiple sequence alignments", "comments": "26 pages, 2 figures, 5 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequence alignment supports numerous tasks in bioinformatics, natural\nlanguage processing, pattern recognition, social sciences, and others fields.\nWhile the alignment of two sequences may be performed swiftly in many\napplications, the simultaneous alignment of multiple sequences proved to be\nnaturally more intricate. Although most multiple sequence alignment (MSA)\nformulations are NP-hard, several approaches have been developed, as they can\noutperform pairwise alignment methods or are necessary for some applications.\n  Taking into account not only similarities but also the lengths of the\ncompared sequences (i.e. normalization) can provide better alignment results\nthan both unnormalized or post-normalized approaches. While some normalized\nmethods have been developed for pairwise sequence alignment, none have been\nproposed for MSA. This work is a first effort towards the development of\nnormalized methods for MSA.\n  We discuss multiple aspects of normalized multiple sequence alignment (NMSA).\nWe define three new criteria for computing normalized scores when aligning\nmultiple sequences, showing the NP-hardness and exact algorithms for solving\nthe NMSA using those criteria. In addition, we provide approximation algorithms\nfor MSA and NMSA for some classes of scoring matrices.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 12:45:20 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Araujo", "Eloi", ""], ["Rozante", "Luiz", ""], ["Rubert", "Diego P.", ""], ["Martinez", "Fabio V.", ""]]}, {"id": "2107.01613", "submitter": "Malin Rau", "authors": "Klaus Jansen and Malin Rau", "title": "Closing the gap for single resource constraint scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In the problem called single resource constraint scheduling, we are given $m$\nidentical machines and a set of jobs, each needing one machine to be processed\nas well as a share of a limited renewable resource $R$. A schedule of these\njobs is feasible if, at each point in the schedule, the number of machines and\nresources required by jobs processed at this time is not exceeded. It is\nNP-hard to approximate this problem with a ratio better than $3/2$. On the\nother hand, the best algorithm so far has an absolute approximation ratio of\n$2+\\varepsilon$. This paper presents an algorithm with absolute approximation\nratio~$(3/2+\\varepsilon)$, which closes the gap between inapproximability and\nbest algorithm except for a negligible small~$\\varepsilon$.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 12:58:45 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Jansen", "Klaus", ""], ["Rau", "Malin", ""]]}, {"id": "2107.01673", "submitter": "Arindam Biswas", "authors": "Arindam Biswas, Venkatesh Raman", "title": "Sublinear-Space Approximation Algorithms for Max r-SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the Max $r$-SAT problem, the input is a CNF formula with $n$ variables\nwhere each clause is a disjunction of at most $r$ literals. The objective is to\ncompute an assignment which satisfies as many of the clauses as possible. While\nthere are a large number of polynomial-time approximation algorithms for this\nproblem, we take the viewpoint of space complexity following [Biswas et al.,\nAlgorithmica 2021] and design sublinear-space approximation algorithms for the\nproblem.\n  We show that the classical algorithm of [Lieberherr and Specker, JACM 1981]\ncan be implemented to run in $n^{O(1)}$ time while using $(\\log{n})$ bits of\nspace. The more advanced algorithms use linear or semi-definite programming,\nand seem harder to carry out in sublinear space. We show that a more recent\nalgorithm with approximation ratio $\\sqrt{2}/2$ [Chou et al., FOCS 2020],\ndesigned for the streaming model, can be implemented to run in time $n^{O(r)}$\nusing $O(r \\log{n})$ bits of space. While known streaming algorithms for the\nproblem approximate optimum values and use randomization, our algorithms are\ndeterministic and can output the approximately optimal assignments in sublinear\nspace.\n  For instances of Max $r$-SAT with planar incidence graphs, we devise a\nfactor-$(1 - \\epsilon)$ approximation scheme which computes assignments in time\n$n^{O(r / \\epsilon)}$ and uses $\\max\\{\\sqrt{n} \\log{n}, (r / \\epsilon)\n\\log^2{n}\\}$ bits of space.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 16:13:33 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Biswas", "Arindam", ""], ["Raman", "Venkatesh", ""]]}, {"id": "2107.01721", "submitter": "Alejandro Cassis", "authors": "Karl Bringmann, Alejandro Cassis, Nick Fischer, Marvin K\\\"unnemann", "title": "Fine-Grained Completeness for Optimization in P", "comments": "Full version of APPROX'21 paper, abstract shortened to fit ArXiv\n  requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of fine-grained completeness theorems for exact and\napproximate optimization in the polynomial-time regime. Inspired by the first\ncompleteness results for decision problems in P (Gao, Impagliazzo, Kolokolova,\nWilliams, TALG 2019) as well as the classic class MaxSNP and\nMaxSNP-completeness for NP optimization problems (Papadimitriou, Yannakakis,\nJCSS 1991), we define polynomial-time analogues MaxSP and MinSP, which contain\na number of natural optimization problems in P, including Maximum Inner\nProduct, general forms of nearest neighbor search and optimization variants of\nthe $k$-XOR problem. Specifically, we define MaxSP as the class of problems\ndefinable as $\\max_{x_1,\\dots,x_k} \\#\\{ (y_1,\\dots,y_\\ell) :\n\\phi(x_1,\\dots,x_k, y_1,\\dots,y_\\ell) \\}$, where $\\phi$ is a quantifier-free\nfirst-order property over a given relational structure (with MinSP defined\nanalogously). On $m$-sized structures, we can solve each such problem in time\n$O(m^{k+\\ell-1})$. Our results are:\n  - We determine (a sparse variant of) the Maximum/Minimum Inner Product\nproblem as complete under *deterministic* fine-grained reductions: A strongly\nsubquadratic algorithm for Maximum/Minimum Inner Product would beat the\nbaseline running time of $O(m^{k+\\ell-1})$ for *all* problems in MaxSP/MinSP by\na polynomial factor.\n  - This completeness transfers to approximation: Maximum/Minimum Inner Product\nis also complete in the sense that a strongly subquadratic $c$-approximation\nwould give a $(c+\\varepsilon)$-approximation for all MaxSP/MinSP problems in\ntime $O(m^{k+\\ell-1-\\delta})$, where $\\varepsilon > 0$ can be chosen\narbitrarily small. Combining our completeness with~(Chen, Williams, SODA 2019),\nwe obtain the perhaps surprising consequence that refuting the OV Hypothesis is\n*equivalent* to giving a $O(1)$-approximation for all MinSP problems in\nfaster-than-$O(m^{k+\\ell-1})$ time.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 20:08:52 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Bringmann", "Karl", ""], ["Cassis", "Alejandro", ""], ["Fischer", "Nick", ""], ["K\u00fcnnemann", "Marvin", ""]]}, {"id": "2107.01752", "submitter": "Max Little", "authors": "Max A. Little and Ugur Kayas", "title": "Polymorphic dynamic programming by algebraic shortcut fusion", "comments": "Updated v9 with 2 additional figures and descriptions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.RA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Dynamic programming (DP) is a broadly applicable algorithmic design paradigm\nfor the efficient, exact solution of otherwise intractable, combinatorial\nproblems. However, the design of such algorithms is often presented informally\nin an ad-hoc manner, and as a result is often difficult to apply correctly. In\nthis paper, we present a rigorous algebraic formalism for systematically\nderiving novel DP algorithms, either from existing DP algorithms or from simple\nfunctional recurrences. These derivations lead to algorithms which are provably\ncorrect and polymorphic over any semiring, which means that they can be applied\nto the full scope of combinatorial problems expressible in terms of semirings.\nThis includes, for example: optimization, optimal probability and Viterbi\ndecoding, probabilistic marginalization, logical inference, fuzzy sets,\ndifferentiable softmax, and relational and provenance queries. The approach,\nbuilding on many ideas from the existing literature on constructive\nalgorithmics, exploits generic properties of (semiring) polymorphic functions,\ntupling and formal sums (lifting), and algebraic simplifications arising from\nconstraint algebras. We demonstrate the effectiveness of this formalism for\nsome example applications arising in signal processing, bioinformatics and\nreliability engineering.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 00:51:02 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 22:37:35 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Little", "Max A.", ""], ["Kayas", "Ugur", ""]]}, {"id": "2107.01804", "submitter": "Shyam Narayanan", "authors": "Shyam Narayanan, Sandeep Silwal, Piotr Indyk, Or Zamir", "title": "Randomized Dimensionality Reduction for Facility Location and\n  Single-Linkage Clustering", "comments": "25 pages. Published as a conference paper in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random dimensionality reduction is a versatile tool for speeding up\nalgorithms for high-dimensional problems. We study its application to two\nclustering problems: the facility location problem, and the single-linkage\nhierarchical clustering problem, which is equivalent to computing the minimum\nspanning tree. We show that if we project the input pointset $X$ onto a random\n$d = O(d_X)$-dimensional subspace (where $d_X$ is the doubling dimension of\n$X$), then the optimum facility location cost in the projected space\napproximates the original cost up to a constant factor. We show an analogous\nstatement for minimum spanning tree, but with the dimension $d$ having an extra\n$\\log \\log n$ term and the approximation factor being arbitrarily close to $1$.\nFurthermore, we extend these results to approximating solutions instead of just\ntheir costs. Lastly, we provide experimental results to validate the quality of\nsolutions and the speedup due to the dimensionality reduction. Unlike several\nprevious papers studying this approach in the context of $k$-means and\n$k$-medians, our dimension bound does not depend on the number of clusters but\nonly on the intrinsic dimensionality of $X$.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 05:55:26 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Narayanan", "Shyam", ""], ["Silwal", "Sandeep", ""], ["Indyk", "Piotr", ""], ["Zamir", "Or", ""]]}, {"id": "2107.02018", "submitter": "Finn Stutzenstein", "authors": "Markus Chimani, Finn Stutzenstein", "title": "Spanner Approximations in Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A multiplicative $\\alpha$-spanner $H$ is a subgraph of $G=(V,E)$ with the\nsame vertices and fewer edges that preserves distances up to the factor\n$\\alpha$, i.e., $d_H(u,v)\\leq\\alpha\\cdot d_G(u,v)$ for all vertices $u$, $v$.\nWhile many algorithms have been developed to find good spanners in terms of\napproximation guarantees, no experimental studies comparing different\napproaches exist. We implemented a rich selection of those algorithms and\nevaluate them on a variety of instances regarding, e.g., their running time,\nsparseness, lightness, and effective stretch.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 13:40:02 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Chimani", "Markus", ""], ["Stutzenstein", "Finn", ""]]}, {"id": "2107.02058", "submitter": "Jiashuo Jiang", "authors": "Jiashuo Jiang, Will Ma, Jiawei Zhang", "title": "Tight Guarantees for Multi-unit Prophet Inequalities and Online\n  Stochastic Knapsack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prophet inequalities are a useful tool for designing online allocation\nprocedures and comparing their performance to the optimal offline allocation.\nIn the basic setting of $k$-unit prophet inequalities, the magical procedure of\nAlaei (2011) with its celebrated performance guarantee of\n$1-\\frac{1}{\\sqrt{k+3}}$ has found widespread adoption in mechanism design and\nother online allocation problems in online advertising, healthcare scheduling,\nand revenue management. Despite being commonly used for implementing online\nallocation, the tightness of Alaei's procedure for a given $k$ has remained\nunknown. In this paper we resolve this question, characterizing the tight bound\nby identifying the structure of the optimal online implementation, and\nconsequently improving the best-known guarantee for $k$-unit prophet\ninequalities for all $k>1$. We also consider a more general online stochastic\nknapsack problem where each individual allocation can consume an arbitrary\nfraction of the initial capacity. We introduce a new \"best-fit\" procedure for\nimplementing a fractionally-feasible knapsack solution online, with a\nperformance guarantee of $\\frac{1}{3+e^{-2}}\\approx0.319$, which we also show\nis tight. This improves the previously best-known guarantee of 0.2 for online\nknapsack. Our analysis differs from existing ones by eschewing the need to\nsplit items into \"large\" or \"small\" based on capacity consumption, using\ninstead an invariant for the overall utilization on different sample paths.\nFinally, we refine our technique for the unit-density special case of knapsack,\nand improve the guarantee from 0.321 to 0.3557 in the multi-resource\nappointment scheduling application of Stein et al. (2020). All in all, our\nresults imply \\textit{tight} Online Contention Resolution Schemes for\n$k$-uniform matroids and the knapsack polytope, respectively, which has further\nimplications in mechanism design.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 14:42:29 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 03:43:05 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Jiang", "Jiashuo", ""], ["Ma", "Will", ""], ["Zhang", "Jiawei", ""]]}, {"id": "2107.02205", "submitter": "Linas Stripinis Dr.", "authors": "Linas Stripinis and Remigijus Paulavi\\v{c}ius", "title": "DGO: A new DIRECT-type MATLAB toolbox for derivative-free global\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we introduce DGO, a new MATLAB toolbox for derivative-free\nglobal optimization. DGO collects various deterministic derivative-free\nDIRECT-type algorithms for box-constrained, generally-constrained, and problems\nwith hidden constraints. Each sequential algorithm is implemented in two\ndifferent ways: using static and dynamic data structures for more efficient\ninformation storage and organization. Furthermore, parallel schemes are applied\nto some promising algorithms within DGO. The toolbox is equipped with a\ngraphical user interface (GUI), which ensures the user-friendly use of all\nfunctionalities available in DGO. Available features are demonstrated in\ndetailed computational studies using a created comprehensive library of global\noptimization problems. Additionally, eleven classical engineering design\nproblems are used to illustrate the potential of DGO to solve challenging\nreal-world problems.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 18:13:21 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Stripinis", "Linas", ""], ["Paulavi\u010dius", "Remigijus", ""]]}, {"id": "2107.02318", "submitter": "William Kuszmaul", "authors": "Michael A. Bender, Tsvi Kopelowitz, William Kuszmaul, Ely Porat, and\n  Clifford Stein", "title": "Incremental Edge Orientation in Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any forest $G = (V, E)$ it is possible to orient the edges $E$ so that no\nvertex in $V$ has out-degree greater than $1$. This paper considers the\nincremental edge-orientation problem, in which the edges $E$ arrive over time\nand the algorithm must maintain a low-out-degree edge orientation at all times.\nWe give an algorithm that maintains a maximum out-degree of $3$ while flipping\nat most $O(\\log \\log n)$ edge orientations per edge insertion, with high\nprobability in $n$. The algorithm requires worst-case time $O(\\log n \\log \\log\nn)$ per insertion, and takes amortized time $O(1)$. The previous state of the\nart required up to $O(\\log n / \\log \\log n)$ edge flips per insertion.\n  We then apply our edge-orientation results to the problem of dynamic Cuckoo\nhashing. The problem of designing simple families $\\mathcal{H}$ of hash\nfunctions that are compatible with Cuckoo hashing has received extensive\nattention. These families $\\mathcal{H}$ are known to satisfy \\emph{static\nguarantees}, but do not come typically with \\emph{dynamic guarantees} for the\nrunning time of inserts and deletes. We show how to transform static guarantees\n(for $1$-associativity) into near-state-of-the-art dynamic guarantees (for\n$O(1)$-associativity) in a black-box fashion. Rather than relying on the family\n$\\mathcal{H}$ to supply randomness, as in past work, we instead rely on\nrandomness within our table-maintenance algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 23:29:42 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Bender", "Michael A.", ""], ["Kopelowitz", "Tsvi", ""], ["Kuszmaul", "William", ""], ["Porat", "Ely", ""], ["Stein", "Clifford", ""]]}, {"id": "2107.02432", "submitter": "Deeksha Adil", "authors": "Deeksha Adil, Brian Bullins, Sushant Sachdeva", "title": "Unifying Width-Reduced Methods for Quasi-Self-Concordant Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide several algorithms for constrained optimization of a large class\nof convex problems, including softmax, $\\ell_p$ regression, and logistic\nregression. Central to our approach is the notion of width reduction, a\ntechnique which has proven immensely useful in the context of maximum flow\n[Christiano et al., STOC'11] and, more recently, $\\ell_p$ regression [Adil et\nal., SODA'19], in terms of improving the iteration complexity from $O(m^{1/2})$\nto $\\tilde{O}(m^{1/3})$, where $m$ is the number of rows of the design matrix,\nand where each iteration amounts to a linear system solve. However, a\nconsiderable drawback is that these methods require both problem-specific\npotentials and individually tailored analyses.\n  As our main contribution, we initiate a new direction of study by presenting\nthe first unified approach to achieving $m^{1/3}$-type rates. Notably, our\nmethod goes beyond these previously considered problems to more broadly capture\nquasi-self-concordant losses, a class which has recently generated much\ninterest and includes the well-studied problem of logistic regression, among\nothers. In order to do so, we develop a unified width reduction method for\ncarefully handling these losses based on a more general set of potentials.\nAdditionally, we directly achieve $m^{1/3}$-type rates in the constrained\nsetting without the need for any explicit acceleration schemes, thus naturally\ncomplementing recent work based on a ball-oracle approach [Carmon et al.,\nNeurIPS'20].\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 07:12:26 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Adil", "Deeksha", ""], ["Bullins", "Brian", ""], ["Sachdeva", "Sushant", ""]]}, {"id": "2107.02503", "submitter": "Dominik K\\\"oppl", "authors": "Jacqueline W. Daykin and Dominik K\\\"oppl and David K\\\"ubel and Florian\n  Stober", "title": "On Arithmetically Progressed Suffix Arrays and related Burrows-Wheeler\n  Transforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize those strings whose suffix arrays are based on arithmetic\nprogressions, in particular, arithmetically progressed permutations where all\npairs of successive entries of the permutation have the same difference modulo\nthe respective string length. We show that an arithmetically progressed\npermutation $P$ coincides with the suffix array of a unary, binary, or ternary\nstring. We further analyze the conditions of a given $P$ under which we can\nfind a uniquely defined string over either a binary or ternary alphabet having\n$P$ as its suffix array. For the binary case, we show its connection to lower\nChristoffel words, balanced words, and Fibonacci words. In addition to solving\nthe arithmetically progressed suffix array problem, we give the shape of the\nBurrows-Wheeler transform of those strings solving this problem. These results\ngive rise to numerous future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 09:48:41 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Daykin", "Jacqueline W.", ""], ["K\u00f6ppl", "Dominik", ""], ["K\u00fcbel", "David", ""], ["Stober", "Florian", ""]]}, {"id": "2107.02554", "submitter": "Micha{\\l} W{\\l}odarczyk", "authors": "Bart M. P. Jansen, Shivesh K. Roy, Micha{\\l} W{\\l}odarczyk", "title": "On the Hardness of Compressing Weights", "comments": "To appear at MFCS'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate computational problems involving large weights through the\nlens of kernelization, which is a framework of polynomial-time preprocessing\naimed at compressing the instance size. Our main focus is the weighted Clique\nproblem, where we are given an edge-weighted graph and the goal is to detect a\nclique of total weight equal to a prescribed value. We show that the weighted\nvariant, parameterized by the number of vertices $n$, is significantly harder\nthan the unweighted problem by presenting an $O(n^{3 - \\varepsilon})$ lower\nbound on the size of the kernel, under the assumption that NP $\\not \\subseteq$\ncoNP/poly. This lower bound is essentially tight: we show that we can reduce\nthe problem to the case with weights bounded by $2^{O(n)}$, which yields a\nrandomized kernel of $O(n^3)$ bits.\n  We generalize these results to the weighted $d$-Uniform Hyperclique problem,\nSubset Sum, and weighted variants of Boolean Constraint Satisfaction Problems\n(CSPs). We also study weighted minimization problems and show that weight\ncompression is easier when we only want to preserve the collection of optimal\nsolutions. Namely, we show that for node-weighted Vertex Cover on bipartite\ngraphs it is possible to maintain the set of optimal solutions using integer\nweights from the range $[1, n]$, but if we want to maintain the ordering of the\nweights of all inclusion-minimal solutions, then weights as large as\n$2^{\\Omega(n)}$ are necessary.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 11:52:51 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["Roy", "Shivesh K.", ""], ["W\u0142odarczyk", "Micha\u0142", ""]]}, {"id": "2107.02573", "submitter": "Francisco Lazaro", "authors": "Francisco L\\'azaro, Bal\\'azs Matuz", "title": "Irregular Invertible Bloom Look-Up Tables", "comments": "Accepted for presentation at ISTC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DB cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider invertible Bloom lookup tables (IBLTs) which are probabilistic\ndata structures that allow to store keyvalue pairs. An IBLT supports insertion\nand deletion of key-value pairs, as well as the recovery of all key-value pairs\nthat have been inserted, as long as the number of key-value pairs stored in the\nIBLT does not exceed a certain number. The recovery operation on an IBLT can be\nrepresented as a peeling process on a bipartite graph. We present a density\nevolution analysis of IBLTs which allows to predict the maximum number of\nkey-value pairs that can be inserted in the table so that recovery is still\nsuccessful with high probability. This analysis holds for arbitrary irregular\ndegree distributions and generalizes results in the literature. We complement\nour analysis by numerical simulations of our own IBLT design which allows to\nrecover a larger number of key-value pairs as state-of-the-art IBLTs of same\nsize.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 12:21:50 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["L\u00e1zaro", "Francisco", ""], ["Matuz", "Bal\u00e1zs", ""]]}, {"id": "2107.02578", "submitter": "Samson Zhou", "authors": "Michael Kapralov, Amulya Musipatla, Jakab Tardos, David P. Woodruff,\n  Samson Zhou", "title": "Noisy Boolean Hidden Matching with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Boolean Hidden Matching (BHM) problem, introduced in a seminal paper of\nGavinsky et. al. [STOC'07], has played an important role in the streaming lower\nbounds for graph problems such as triangle and subgraph counting, maximum\nmatching, MAX-CUT, Schatten $p$-norm approximation, maximum acyclic subgraph,\ntesting bipartiteness, $k$-connectivity, and cycle-freeness. The one-way\ncommunication complexity of the Boolean Hidden Matching problem on a universe\nof size $n$ is $\\Theta(\\sqrt{n})$, resulting in $\\Omega(\\sqrt{n})$ lower bounds\nfor constant factor approximations to several of the aforementioned graph\nproblems. The related (and, in fact, more general) Boolean Hidden Hypermatching\n(BHH) problem introduced by Verbin and Yu [SODA'11] provides an approach to\nproving higher lower bounds of $\\Omega(n^{1-1/t})$ for integer $t\\geq 2$.\nReductions based on Boolean Hidden Hypermatching generate distributions on\ngraphs with connected components of diameter about $t$, and basically show that\nlong range exploration is hard in the streaming model of computation with\nadversarial arrivals.\n  In this paper we introduce a natural variant of the BHM problem, called noisy\nBHM (and its natural noisy BHH variant), that we use to obtain higher than\n$\\Omega(\\sqrt{n})$ lower bounds for approximating several of the aforementioned\nproblems in graph streams when the input graphs consist only of components of\ndiameter bounded by a fixed constant. We also use the noisy BHM problem to show\nthat the problem of classifying whether an underlying graph is isomorphic to a\ncomplete binary tree in insertion-only streams requires $\\Omega(n)$ space,\nwhich seems challenging to show using BHM or BHH alone.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 12:34:46 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Kapralov", "Michael", ""], ["Musipatla", "Amulya", ""], ["Tardos", "Jakab", ""], ["Woodruff", "David P.", ""], ["Zhou", "Samson", ""]]}, {"id": "2107.02581", "submitter": "Niklas Troost", "authors": "Markus Chimani and Niklas Troost and Tilo Wiedera", "title": "A General Approach to Approximate Multistage Subgraph Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a Subgraph Problem we are given some graph and want to find a feasible\nsubgraph that optimizes some measure. We consider Multistage Subgraph Problems\n(MSPs), where we are given a sequence of graph instances (stages) and are asked\nto find a sequence of subgraphs, one for each stage, such that each is optimal\nfor its respective stage and the subgraphs for subsequent stages are as similar\nas possible. We present a framework that provides a\n$(1/\\sqrt{2\\chi})$-approximation algorithm for the $2$-stage restriction of an\nMSP if the similarity of subsequent solutions is measured as the intersection\ncardinality and said MSP is preficient, i.e., we can efficiently find a\nsingle-stage solution that prefers some given subset. The approximation factor\nis dependent on the instance's intertwinement $\\chi$, a similarity measure for\nmultistage graphs. We also show that for any MSP, independent of similarity\nmeasure and preficiency, given an exact or approximation algorithm for a\nconstant number of stages, we can approximate the MSP for an unrestricted\nnumber of stages. Finally, we combine and apply these results and show that the\nabove restrictions describe a very rich class of MSPs and that proving\nmembership for this class is mostly straightforward. As examples, we explicitly\nstate these proofs for natural multistage versions of Perfect Matching,\nShortest s-t-Path, Minimum s-t-Cut and further classical problems on bipartite\nor planar graphs, namely Maximum Cut, Vertex Cover, Independent Set, and\nBiclique.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 12:46:27 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chimani", "Markus", ""], ["Troost", "Niklas", ""], ["Wiedera", "Tilo", ""]]}, {"id": "2107.02605", "submitter": "Hyung-Chan An", "authors": "Yongho Shin, Hyung-Chan An", "title": "Making Three Out of Two: Three-Way Online Correlated Selection", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Two-way online correlated selection (two-way OCS) is an online algorithm\nthat, at each timestep, takes a pair of elements from the ground set and\nirrevocably chooses one of the two elements, while ensuring negative\ncorrelation in the algorithm's choices. Whilst OCS was initially invented by\nFahrbach, Huang, Tao, and Zadimoghaddam to solve the edge-weighted online\nbipartite matching problem, it is an interesting technique on its own due to\nits capability of introducing a powerful algorithmic tool, namely negative\ncorrelation, to online algorithms. As such, Fahrbach et al. posed two\ntantalizing open questions in their paper, one of which was the following: Can\nwe obtain n-way OCS for n>2, in which the algorithm can be given n>2 elements\nto choose from at each timestep?\n  In this paper, we affirmatively answer this open question by presenting a\nthree-way OCS. Our algorithm uses two-way OCS as its building block and is\nsimple to describe; however, as it internally runs two instances of two-way\nOCS, one of which is fed with the output of the other, the final output\nprobability distribution becomes highly elusive. We tackle this difficulty by\napproximating the output distribution of OCS by a flat, less correlated\nfunction and using it as a safe \"surrogate\" of the real distribution. Our\nthree-way OCS also yields a 0.5093-competitive algorithm for edge-weighted\nonline matching, demonstrating its usefulness.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 13:27:15 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Shin", "Yongho", ""], ["An", "Hyung-Chan", ""]]}, {"id": "2107.02666", "submitter": "Gopinath Mishra", "authors": "Arijit Bishnu, Arijit Ghosh, Gopinath Mishra", "title": "Distance Estimation Between Unknown Matrices Using Sublinear Projections\n  on Hamming Cube", "comments": "30 pages. Accepted in RANDOM'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Using geometric techniques like projection and dimensionality reduction, we\nshow that there exists a randomized sub-linear time algorithm that can estimate\nthe Hamming distance between two matrices. Consider two matrices ${\\bf A}$ and\n${\\bf B}$ of size $n \\times n$ whose dimensions are known to the algorithm but\nthe entries are not. The entries of the matrix are real numbers. The access to\nany matrix is through an oracle that computes the projection of a row (or a\ncolumn) of the matrix on a vector in $\\{0,1\\}^n$. We call this query oracle to\nbe an {\\sc Inner Product} oracle (shortened as {\\sc IP}). We show that our\nalgorithm returns a $(1\\pm \\epsilon)$ approximation to ${{\\bf D}}_{\\bf M} ({\\bf\nA},{\\bf B})$ with high probability by making ${\\cal\nO}\\left(\\frac{n}{\\sqrt{{{\\bf D}}_{\\bf M} ({\\bf A},{\\bf\nB})}}\\mbox{poly}\\left(\\log n, \\frac{1}{\\epsilon}\\right)\\right)$ oracle queries,\nwhere ${{\\bf D}}_{\\bf M} ({\\bf A},{\\bf B})$ denotes the Hamming distance (the\nnumber of corresponding entries in which ${\\bf A}$ and ${\\bf B}$ differ)\nbetween two matrices ${\\bf A}$ and ${\\bf B}$ of size $n \\times n$. We also show\na matching lower bound on the number of such {\\sc IP} queries needed. Though\nour main result is on estimating ${{\\bf D}}_{\\bf M} ({\\bf A},{\\bf B})$ using\n{\\sc IP}, we also compare our results with other query models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 15:12:34 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Bishnu", "Arijit", ""], ["Ghosh", "Arijit", ""], ["Mishra", "Gopinath", ""]]}, {"id": "2107.02736", "submitter": "Matti Karppa", "authors": "Matti Karppa and Martin Aum\\\"uller and Rasmus Pagh", "title": "DEANN: Speeding up Kernel-Density Estimation using Approximate Nearest\n  Neighbor Search", "comments": "24 pages, 1 figure. Submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel Density Estimation (KDE) is a nonparametric method for estimating the\nshape of a density function, given a set of samples from the distribution.\nRecently, locality-sensitive hashing, originally proposed as a tool for nearest\nneighbor search, has been shown to enable fast KDE data structures. However,\nthese approaches do not take advantage of the many other advances that have\nbeen made in algorithms for nearest neighbor algorithms. We present an\nalgorithm called Density Estimation from Approximate Nearest Neighbors (DEANN)\nwhere we apply Approximate Nearest Neighbor (ANN) algorithms as a black box\nsubroutine to compute an unbiased KDE. The idea is to find points that have a\nlarge contribution to the KDE using ANN, compute their contribution exactly,\nand approximate the remainder with Random Sampling (RS). We present a\ntheoretical argument that supports the idea that an ANN subroutine can speed up\nthe evaluation. Furthermore, we provide a C++ implementation with a Python\ninterface that can make use of an arbitrary ANN implementation as a subroutine\nfor KDE evaluation. We show empirically that our implementation outperforms\nstate of the art implementations in all high dimensional datasets we\nconsidered, and matches the performance of RS in cases where the ANN yield no\ngains in performance.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 17:11:28 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Karppa", "Matti", ""], ["Aum\u00fcller", "Martin", ""], ["Pagh", "Rasmus", ""]]}, {"id": "2107.02738", "submitter": "Lee Cohen", "authors": "Lee Cohen, Ulrike Schmidt-Kraepelin, Yishay Mansour", "title": "Dueling Bandits with Team Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the dueling teams problem, a new online-learning setting in\nwhich the learner observes noisy comparisons of disjoint pairs of $k$-sized\nteams from a universe of $n$ players. The goal of the learner is to minimize\nthe number of duels required to identify, with high probability, a Condorcet\nwinning team, i.e., a team which wins against any other disjoint team (with\nprobability at least $1/2$). Noisy comparisons are linked to a total order on\nthe teams. We formalize our model by building upon the dueling bandits setting\n(Yue et al.2012) and provide several algorithms, both for stochastic and\ndeterministic settings. For the stochastic setting, we provide a reduction to\nthe classical dueling bandits setting, yielding an algorithm that identifies a\nCondorcet winning team within $\\mathcal{O}((n + k \\log (k)) \\frac{\\max(\\log\\log\nn, \\log k)}{\\Delta^2})$ duels, where $\\Delta$ is a gap parameter. For\ndeterministic feedback, we additionally present a gap-independent algorithm\nthat identifies a Condorcet winning team within $\\mathcal{O}(nk\\log(k)+k^5)$\nduels.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 17:12:17 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Cohen", "Lee", ""], ["Schmidt-Kraepelin", "Ulrike", ""], ["Mansour", "Yishay", ""]]}, {"id": "2107.02743", "submitter": "Rajan Udwani", "authors": "Rajan Udwani", "title": "Submodular Order Functions and Assortment Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a new class of set functions that in addition to being monotone and\nsubadditive, also admit a very limited form of submodularity defined over a\npermutation of the ground set. We refer to this permutation as a submodular\norder. This class of functions includes monotone submodular functions as a\nsub-family. To understand the importance of this structure in optimization\nproblems we consider the problem of maximizing function value under various\ntypes of constraints.\n  To demonstrate the modeling power of submodular order functions we show\napplications in two different settings. First, we apply our results to the\nextensively studied problem of assortment optimization. While the objectives in\nassortment optimization are known to be non-submodular (and non-monotone) even\nfor simple choice models, we show that they are compatible with the notion of\nsubmodular order. Consequently, we obtain new and in some cases the first\nconstant factor guarantee for constrained assortment optimization in\nfundamental choice models. As a second application of submodular order\nfunctions, we show an intriguing algorithmic connection to the maximization of\nmonotone submodular functions in the streaming model. We recover some best\nknown guarantees for this problem as a corollary of our results.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 17:20:16 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Udwani", "Rajan", ""]]}, {"id": "2107.02748", "submitter": "Shyan Akmal", "authors": "Shyan Akmal and Ryan Williams", "title": "MAJORITY-3SAT (and Related Problems) in Polynomial Time", "comments": "Abstract shortened to fit arXiv requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Majority-SAT is the problem of determining whether an input $n$-variable\nformula in conjunctive normal form (CNF) has at least $2^{n-1}$ satisfying\nassignments. Majority-SAT and related problems have been studied extensively in\nvarious AI communities interested in the complexity of probabilistic planning\nand inference. Although Majority-SAT has been known to be PP-complete for over\n40 years, the complexity of a natural variant has remained open:\nMajority-$k$SAT, where the input CNF formula is restricted to have clause width\nat most $k$.\n  We prove that for every $k$, Majority-$k$SAT is in P. In fact, for any\npositive integer $k$ and rational $\\rho \\in (0,1)$ with bounded denominator, we\ngive an algorithm that can determine whether a given $k$-CNF has at least $\\rho\n\\cdot 2^n$ satisfying assignments, in deterministic linear time (whereas the\nprevious best-known algorithm ran in exponential time). Our algorithms have\ninteresting positive implications for counting complexity and the complexity of\ninference, significantly reducing the known complexities of related problems\nsuch as E-MAJ-$k$SAT and MAJ-MAJ-$k$SAT. At the heart of our approach is an\nefficient method for solving threshold counting problems by extracting\nsunflowers found in the corresponding set system of a $k$-CNF.\n  We also show that the tractability of Majority-$k$SAT is somewhat fragile.\nFor the closely related GtMajority-SAT problem (where we ask whether a given\nformula has greater than $2^{n-1}$ satisfying assignments) which is known to be\nPP-complete, we show that GtMajority-$k$SAT is in P for $k\\le 3$, but becomes\nNP-complete for $k\\geq 4$. These results are counterintuitive, because the\n``natural'' classifications of these problems would have been PP-completeness,\nand because there is a stark difference in the complexity of GtMajority-$k$SAT\nand Majority-$k$SAT for all $k\\ge 4$.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 17:24:04 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Akmal", "Shyan", ""], ["Williams", "Ryan", ""]]}, {"id": "2107.02787", "submitter": "Younan Gao", "authors": "Younan Gao and Meng He", "title": "Space Efficient Two-Dimensional Orthogonal Colored Range Counting", "comments": "full version of an ESA 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the two-dimensional orthogonal colored range counting problem, we\npreprocess a set, $P$, of $n$ colored points on the plane, such that given an\northogonal query rectangle, the number of distinct colors of the points\ncontained in this rectangle can be computed efficiently.\n  For this problem, we design three new solutions, and the bounds of each can\nbe expressed in some form of time-space tradeoff.\n  By setting appropriate parameter values for these solutions, we can achieve\nnew specific results with (the space are in words and $\\epsilon$ is an\narbitrary constant in $(0,1)$):\n  ** $O(n\\lg^3 n)$ space and $O(\\sqrt{n}\\lg^{5/2} n \\lg \\lg n)$ query time;\n  ** $O(n\\lg^2 n)$ space and $O(\\sqrt{n}\\lg^{4+\\epsilon} n)$ query time;\n  ** $O(n\\frac{\\lg^2 n}{\\lg \\lg n})$ space and $O(\\sqrt{n}\\lg^{5+\\epsilon} n)$\nquery time;\n  ** $O(n\\lg n)$ space and $O(n^{1/2+\\epsilon})$ query time.\n  A known conditional lower bound to this problem based on Boolean matrix\nmultiplication gives some evidence on the difficulty of achieving near-linear\nspace solutions with query time better than $\\sqrt{n}$ by more than a\npolylogarithmic factor using purely combinatorial approaches. Thus the time and\nspace bounds in all these results are efficient.\n  Previously, among solutions with similar query times, the most\nspace-efficient solution uses $O(n\\lg^4 n)$ space to answer queries in\n$O(\\sqrt{n}\\lg^8 n)$ time (SIAM. J. Comp.~2008).\n  Thus the new results listed above all achieve improvements in space\nefficiency, while all but the last result achieve speed-up in query time as\nwell.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 17:50:19 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Gao", "Younan", ""], ["He", "Meng", ""]]}, {"id": "2107.02866", "submitter": "Samuel McCauley", "authors": "David J. Lee, Samuel McCauley, Shikha Singh, Max Stein", "title": "Telescoping Filter: A Practical Adaptive Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Filters are fast, small and approximate set membership data structures. They\nare often used to filter out expensive accesses to a remote set S for negative\nqueries (that is, a query x not in S). Filters have one-sided errors: on a\nnegative query, a filter may say \"present\" with a tunable false-positve\nprobability of epsilon. Correctness is traded for space: filters only use log\n(1/\\epsilon) + O(1) bits per element.\n  The false-positive guarantees of most filters, however, hold only for a\nsingle query. In particular, if x is a false positive of a filter, a subsequent\nquery to x is a false positive with probability 1, not epsilon. With this in\nmind, recent work has introduced the notion of an adaptive filter. A filter is\nadaptive if each query has false positive epsilon, regardless of what queries\nwere made in the past. This requires \"fixing\" false positives as they occur.\n  Adaptive filters not only provide strong false positive guarantees in\nadversarial environments but also improve performance on query practical\nworkloads by eliminating repeated false positives.\n  Existing work on adaptive filters falls into two categories. First, there are\npractical filters based on cuckoo filters that attempt to fix false positives\nheuristically, without meeting the adaptivity guarantee. Meanwhile, the broom\nfilter is a very complex adaptive filter that meets the optimal theoretical\nbounds.\n  In this paper, we bridge this gap by designing a practical, provably adaptive\nfilter: the telescoping adaptive filter. We provide theoretical false-positive\nand space guarantees of our filter, along with empirical results where we\ncompare its false positive performance against state-of-the-art filters. We\nalso test the throughput of our filters, showing that they achieve comparable\nperformance to similar non-adaptive filters.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 19:57:03 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Lee", "David J.", ""], ["McCauley", "Samuel", ""], ["Singh", "Shikha", ""], ["Stein", "Max", ""]]}, {"id": "2107.02882", "submitter": "\\'Edouard Bonnet", "authors": "\\'Edouard Bonnet, Eun Jung Kim, Amadeus Reinald, St\\'ephan Thomass\\'e,\n  R\\'emi Watrigant", "title": "Twin-width and polynomial kernels", "comments": "32 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the existence of polynomial kernels, for parameterized problems\nwithout a polynomial kernel on general graphs, when restricted to graphs of\nbounded twin-width. Our main result is that a polynomial kernel for\n$k$-Dominating Set on graphs of twin-width at most 4 would contradict a\nstandard complexity-theoretic assumption. The reduction is quite involved,\nespecially to get the twin-width upper bound down to 4, and can be tweaked to\nwork for Connected $k$-Dominating Set and Total $k$-Dominating Set (albeit with\na worse upper bound on the twin-width). The $k$-Independent Set problem admits\nthe same lower bound by a much simpler argument, previously observed [ICALP\n'21], which extends to $k$-Independent Dominating Set, $k$-Path, $k$-Induced\nPath, $k$-Induced Matching, etc. On the positive side, we obtain a simple\nquadratic vertex kernel for Connected $k$-Vertex Cover and Capacitated\n$k$-Vertex Cover on graphs of bounded twin-width. Interestingly the kernel\napplies to graphs of Vapnik-Chervonenkis density 1, and does not require a\nwitness sequence. We also present a more intricate $O(k^{1.5})$ vertex kernel\nfor Connected $k$-Vertex Cover. Finally we show that deciding if a graph has\ntwin-width at most 1 can be done in polynomial time, and observe that most\noptimization/decision graph problems can be solved in polynomial time on graphs\nof twin-width at most 1.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 20:46:27 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Kim", "Eun Jung", ""], ["Reinald", "Amadeus", ""], ["Thomass\u00e9", "St\u00e9phan", ""], ["Watrigant", "R\u00e9mi", ""]]}, {"id": "2107.02922", "submitter": "Pooya Nikbakht", "authors": "Shahin Kamali, Pooya Nikbakht", "title": "On the Fault-Tolerant Online Bin Packing Problem", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fault-tolerant variant of the online bin packing problem.\nSimilar to the classic bin packing problem, an online sequence of items of\nvarious sizes should be packed into a minimum number of bins of uniform\ncapacity. For applications such as server consolidation, where bins represent\nservers and items represent jobs of different loads, it is required to maintain\nfault-tolerant solutions. In a fault-tolerant packing, any job is replicated\ninto f+1 servers, for some integer f > 1, so that the failure of up to f\nservers does not interrupt service. We build over a practical model introduced\nby Li and Tang [SPAA 2017] in which each job of load $x$ has a primary replica\nof load $x$ and $f$ standby replicas, each of load $x/\\eta$, where $\\eta >1$ is\na parameter of the problem. Upon failure of up to $f$ servers, any primary\nreplica in a failed bin should be replaced by one of its standby replicas so\nthat the extra load of the new primary replica does not cause an overflow in\nits bin. We study a general setting in which bins might fail while the input is\nstill being revealed. Our main contribution is an algorithm, named\nHarmonic-Stretch, which maintains fault-tolerant packings under this general\nsetting. We prove that Harmonic-Stretch has an asymptotic competitive ratio of\nat most 1.75. This is an improvement over the best existing asymptotic\ncompetitive ratio 2 of an algorithm by Li and Tang [TPDS 2020], which works\nunder a model in which bins fail only after all items are packed.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 22:09:53 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Kamali", "Shahin", ""], ["Nikbakht", "Pooya", ""]]}, {"id": "2107.02956", "submitter": "Silvia Butti", "authors": "Silvia Butti, Victor Dalmau", "title": "Fractional homomorphism, Weisfeiler-Leman invariance, and the\n  Sherali-Adams hierarchy for the Constraint Satisfaction Problem", "comments": "Full version of a MFCS'21 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a pair of graphs $\\textbf{A}$ and $\\textbf{B}$, the problems of\ndeciding whether there exists either a homomorphism or an isomorphism from\n$\\textbf{A}$ to $\\textbf{B}$ have received a lot of attention. While graph\nhomomorphism is known to be NP-complete, the complexity of the graph\nisomorphism problem is not fully understood. A well-known combinatorial\nheuristic for graph isomorphism is the Weisfeiler-Leman test together with its\nhigher order variants. On the other hand, both problems can be reformulated as\ninteger programs and various LP methods can be applied to obtain high-quality\nrelaxations that can still be solved efficiently. We study so-called fractional\nrelaxations of these programs in the more general context where $\\textbf{A}$\nand $\\textbf{B}$ are not graphs but arbitrary relational structures. We give a\ncombinatorial characterization of the Sherali-Adams hierarchy applied to the\nhomomorphism problem in terms of fractional isomorphism. Collaterally, we also\nextend a number of known results from graph theory to give a characterization\nof the notion of fractional isomorphism for relational structures in terms of\nthe Weisfeiler-Leman test, equitable partitions, and counting homomorphisms\nfrom trees. As a result, we obtain a description of the families of CSPs that\nare closed under Weisfeiler-Leman invariance in terms of their polymorphisms as\nwell as decidability by the first level of the Sherali-Adams hierarchy.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 00:35:53 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Butti", "Silvia", ""], ["Dalmau", "Victor", ""]]}, {"id": "2107.03020", "submitter": "N.S Narayanaswamy", "authors": "Keerti Choudhary, Avi Cohen, N. S. Narayanaswamy, David Peleg, R.\n  Vijayaragunathan", "title": "Budgeted Dominating Sets in Uncertain Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the {\\em Budgeted Dominating Set} (BDS) problem on uncertain graphs,\nnamely, graphs with a probability distribution $p$ associated with the edges,\nsuch that an edge $e$ exists in the graph with probability $p(e)$. The input to\nthe problem consists of a vertex-weighted uncertain graph $\\G=(V, E, p,\n\\omega)$ and an integer {\\em budget} (or {\\em solution size}) $k$, and the\nobjective is to compute a vertex set $S$ of size $k$ that maximizes the\nexpected total domination (or total weight) of vertices in the closed\nneighborhood of $S$. We refer to the problem as the {\\em Probabilistic Budgeted\nDominating Set}~(PBDS) problem and present the following results.\n\\begin{enumerate} \\dnsitem We show that the PBDS problem is NP-complete even\nwhen restricted to uncertain {\\em trees} of diameter at most four. This is in\nsharp contrast with the well-known fact that the BDS problem is solvable in\npolynomial time in trees. We further show that PBDS is \\wone-hard for the\nbudget parameter $k$, and under the {\\em Exponential time hypothesis} it cannot\nbe solved in $n^{o(k)}$ time.\n  \\item We show that if one is willing to settle for $(1-\\epsilon)$\napproximation, then there exists a PTAS for PBDS on trees. Moreover, for the\nscenario of uniform edge-probabilities, the problem can be solved optimally in\npolynomial time.\n  \\item We consider the parameterized complexity of the PBDS problem, and show\nthat Uni-PBDS (where all edge probabilities are identical) is \\wone-hard for\nthe parameter pathwidth. On the other hand, we show that it is FPT in the\ncombined parameters of the budget $k$ and the treewidth.\n  \\item Finally, we extend some of our parameterized results to planar and\napex-minor-free graphs. \\end{enumerate}\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 05:23:19 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Choudhary", "Keerti", ""], ["Cohen", "Avi", ""], ["Narayanaswamy", "N. S.", ""], ["Peleg", "David", ""], ["Vijayaragunathan", "R.", ""]]}, {"id": "2107.03076", "submitter": "Yu Yokoi Dr.", "authors": "Yu Yokoi", "title": "An Approximation Algorithm for Maximum Stable Matching with Ties and\n  Constraints", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a polynomial-time $\\frac{3}{2}$-approximation algorithm for the\nproblem of finding a maximum-cardinality stable matching in a many-to-many\nmatching model with ties and laminar constraints on both sides. We formulate\nour problem using a bipartite multigraph whose vertices are called workers and\nfirms, and edges are called contracts. Our algorithm is described as the\ncomputation of a stable matching in an auxiliary instance, in which each\ncontract is replaced with three of its copies and all agents have strict\npreferences on the copied contracts. The construction of this auxiliary\ninstance is symmetric for the two sides, which facilitates a simple symmetric\nanalysis. We use the notion of matroid-kernel for computation in the auxiliary\ninstance and exploit the base-orderability of laminar matroids to show the\napproximation ratio.\n  In a special case in which each worker is assigned at most one contract and\neach firm has a strict preference, our algorithm defines a\n$\\frac{3}{2}$-approximation mechanism that is strategy-proof for workers.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 08:46:03 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Yokoi", "Yu", ""]]}, {"id": "2107.03092", "submitter": "Yasuaki Kobayashi", "authors": "Takehiro Ito, Yuni Iwamasa, Yasuaki Kobayashi, Yu Nakahata, Yota\n  Otachi, Kunihiro Wasa", "title": "Reconfiguring Directed Trees in a Digraph", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the computational complexity of subgraph\nreconfiguration problems in directed graphs. More specifically, we focus on the\nproblem of determining whether, given two directed trees in a digraph, there is\na (reconfiguration) sequence of directed trees such that for every pair of two\nconsecutive trees in the sequence, one of them is obtained from the other by\nremoving an arc and then adding another arc. We show that this problem can be\nsolved in polynomial time, whereas the problem is PSPACE-complete when we\nrestrict directed trees in a reconfiguration sequence to form directed paths.\nWe also show that there is a polynomial-time algorithm for finding a shortest\nreconfiguration sequence between two directed spanning trees.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 09:18:00 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Ito", "Takehiro", ""], ["Iwamasa", "Yuni", ""], ["Kobayashi", "Yasuaki", ""], ["Nakahata", "Yu", ""], ["Otachi", "Yota", ""], ["Wasa", "Kunihiro", ""]]}, {"id": "2107.03123", "submitter": "Koki Hamada", "authors": "Koki Hamada (1 and 2), Shuichi Miyazaki (3) ((1) NTT Corporation, (2)\n  Graduate School of Informatics, Kyoto University, (3) Academic Center for\n  Computing and Media Studies, Kyoto University)", "title": "Refined Computational Complexities of Hospitals/Residents Problem with\n  Regional Caps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hospitals/Residents problem (HR) is a many-to-one matching problem whose\nsolution concept is stability. It is widely used in assignment systems such as\nassigning medical students (residents) to hospitals. To resolve imbalance in\nthe number of residents assigned to hospitals, an extension called HR with\nregional caps (HRRC) was introduced. In this problem, a positive integer\n(called a regional cap) is associated with a subset of hospitals (called a\nregion), and the total number of residents assigned to hospitals in a region\nmust be at most its regional cap. Kamada and Kojima defined strong stability\nfor HRRC and demonstrated that a strongly stable matching does not necessarily\nexist. Recently, Aziz et al. proved that the problem of determining if a\nstrongly stable matching exists is NP-complete in general. In this paper, we\nrefine Aziz et al.'s result by investigating the computational complexity of\nthe problem in terms of the length of preference lists, the size of regions,\nand whether or not regions can overlap, and completely classify tractable and\nintractable cases.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 10:12:40 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Hamada", "Koki", "", "1 and 2"], ["Miyazaki", "Shuichi", ""]]}, {"id": "2107.03133", "submitter": "Moreno Marzolla", "authors": "Luca Calderoni, Luciano Margara, Moreno Marzolla", "title": "A Heuristic for Direct Product Graph Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a heuristic for decomposing a directed graph into\nfactors according to the direct product (also known as Kronecker, cardinal or\ntensor product). Given a directed, unweighted graph~$G$ with adjacency matrix\nAdj($G$), our heuristic searches for a pair of graphs~$G_1$ and~$G_2$ such that\n$G = G_1 \\otimes G_2$, where $G_1 \\otimes G_2$ is the direct product of~$G_1$\nand~$G_2$. For undirected, connected graphs it has been shown that graph\ndecomposition is \"at least as difficult\" as graph isomorphism; therefore,\npolynomial-time algorithms for decomposing a general directed graph into\nfactors are unlikely to exist. Although graph factorization is a problem that\nhas been extensively investigated, the heuristic proposed in this paper\nrepresents -- to the best of our knowledge -- the first computational approach\nfor general directed, unweighted graphs. We have implemented our algorithm\nusing the MATLAB environment; we report on a set of experiments that show that\nthe proposed heuristic solves reasonably-sized instances in a few seconds on\ngeneral-purpose hardware.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 10:34:56 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Calderoni", "Luca", ""], ["Margara", "Luciano", ""], ["Marzolla", "Moreno", ""]]}, {"id": "2107.03193", "submitter": "Thore Thie{\\ss}en", "authors": "Thore Thie{\\ss}en and Jan Vahrenhold", "title": "Oblivious Median Slope Selection", "comments": "14 pages, to appear in Proceedings of CCCG 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the median slope selection problem in the oblivious RAM model. In\nthis model memory accesses have to be independent of the data processed, i.e.,\nan adversary cannot use observed access patterns to derive additional\ninformation about the input. We show how to modify the randomized algorithm of\nMatou\\v{s}ek (1991) to obtain an oblivious version with O(n log^2 n) expected\ntime for n points in R^2. This complexity matches a theoretical upper bound\nthat can be obtained through general oblivious transformation. In addition,\nresults from a proof-of-concept implementation show that our algorithm is also\npractically efficient.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 13:10:13 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Thie\u00dfen", "Thore", ""], ["Vahrenhold", "Jan", ""]]}, {"id": "2107.03290", "submitter": "Ani Kristo", "authors": "Ani Kristo, Kapil Vaidya, Tim Kraska", "title": "Defeating duplicates: A re-design of the LearnedSort algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  LearnedSort is a novel sorting algorithm that, unlike traditional methods,\nuses fast ML models to boost the sorting speed. The models learn to estimate\nthe input's distribution and arrange the keys in sorted order by predicting\ntheir empirical cumulative distribution function (eCDF) values. LearnedSort has\nshown outstanding performance compared to state-of-the-art sorting algorithms\non several datasets, both synthetic and real. However, given the nature of the\neCDF model, its performance is affected in the cases when the input data\ncontains a large number of repeated keys (i.e., duplicates). This work analyzes\nthis scenario in depth and introduces LearnedSort 2.0: a re-design of the\nalgorithm that addresses this issue and enables the algorithm to maintain the\nleading edge even for high-duplicate inputs. Our extensive benchmarks on a\nlarge set of diverse datasets demonstrate that the new design performs at much\nhigher sorting rates than the original version: an average of 4.78x improvement\nfor high-duplicate datasets, and 1.60x for low-duplicate datasets while taking\nthe lead among sorting algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 21:38:48 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Kristo", "Ani", ""], ["Vaidya", "Kapil", ""], ["Kraska", "Tim", ""]]}, {"id": "2107.03341", "submitter": "Simona Rombo", "authors": "Ylenia Galluzzo, Raffaele Giancarlo, Mario Randazzo, Simona E. Rombo", "title": "Burrows Wheeler Transform on a Large Scale: Algorithms Implemented in\n  Apache Spark", "comments": "11 pages, 2 figures, 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:2007.10095", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of Next Generation Sequencing (NGS) technologies, large\namounts of \"omics\" data are daily collected and need to be processed. Indexing\nand compressing large sequences datasets are some of the most important tasks\nin this context. Here we propose algorithms for the computation of Burrows\nWheeler transform relying on Big Data technologies, i.e., Apache Spark and\nHadoop. Our algorithms are the first ones that distribute the index computation\nand not only the input dataset, allowing to fully benefit of the available\ncloud resources.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 16:34:01 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Galluzzo", "Ylenia", ""], ["Giancarlo", "Raffaele", ""], ["Randazzo", "Mario", ""], ["Rombo", "Simona E.", ""]]}, {"id": "2107.03485", "submitter": "Martin Schirneck", "authors": "Davide Bil\\`o, Sarel Cohen, Tobias Friedrich, and Martin Schirneck", "title": "Space-Efficient Fault-Tolerant Diameter Oracles", "comments": "Full version of a paper to appear at MFCS'21. Abstract shortened to\n  meet ArXiv requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We design $f$-edge fault-tolerant diameter oracles ($f$-FDOs). We preprocess\na given graph $G$ on $n$ vertices and $m$ edges, and a positive integer $f$, to\nconstruct a data structure that, when queried with a set $F$ of $|F| \\leq f$\nedges, returns the diameter of $G-F$.\n  For a single failure ($f=1$) in an unweighted directed graph of diameter $D$,\nthere exists an approximate FDO by Henzinger et al. [ITCS 2017] with stretch\n$(1+\\varepsilon)$, constant query time, space $O(m)$, and a combinatorial\npreprocessing time of $\\widetilde{O}(mn + n^{1.5} \\sqrt{Dm/\\varepsilon})$.We\npresent an FDO for directed graphs with the same stretch, query time, and\nspace. It has a preprocessing time of $\\widetilde{O}(mn + n^2/\\varepsilon)$.\nThe preprocessing time nearly matches a conditional lower bound for\ncombinatorial algorithms, also by Henzinger et al. With fast matrix\nmultiplication, we achieve a preprocessing time of $\\widetilde{O}(n^{2.5794} +\nn^2/\\varepsilon)$. We further prove an information-theoretic lower bound\nshowing that any FDO with stretch better than $3/2$ requires $\\Omega(m)$ bits\nof space.\n  For multiple failures ($f>1$) in undirected graphs with non-negative edge\nweights, we give an $f$-FDO with stretch $(f+2)$, query time $O(f^2\\log^2{n})$,\n$\\widetilde{O}(fn)$ space, and preprocessing time $\\widetilde{O}(fm)$. We\ncomplement this with a lower bound excluding any finite stretch in $o(fn)$\nspace. We show that for unweighted graphs with polylogarithmic diameter and up\nto $f = o(\\log n/ \\log\\log n)$ failures, one can swap approximation for query\ntime and space. We present an exact combinatorial $f$-FDO with preprocessing\ntime $mn^{1+o(1)}$, query time $n^{o(1)}$, and space $n^{2+o(1)}$. When using\nfast matrix multiplication instead, the preprocessing time can be improved to\n$n^{\\omega+o(1)}$, where $\\omega < 2.373$ is the matrix multiplication\nexponent.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 21:19:02 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Bil\u00f2", "Davide", ""], ["Cohen", "Sarel", ""], ["Friedrich", "Tobias", ""], ["Schirneck", "Martin", ""]]}, {"id": "2107.03570", "submitter": "Chunlin Sun", "authors": "Wenzhi Gao, Chunlin Sun, Yuyang Ye, Yinyu Ye", "title": "Boosting Method in Approximately Solving Linear Programming with Fast\n  Online Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a new algorithm combining the idea of ``boosting''\nwith the first-order algorithm to approximately solve a class of (Integer)\nLinear programs(LPs) arisen in general resource allocation problems. Not only\ncan this algorithm solve LPs directly, but also can be applied to accelerate\nthe Column Generation method. As a direct solver, our algorithm achieves a\nprovable $O(\\sqrt{n/K})$ optimality gap, where $n$ is the number of variables\nand $K$ is the number of data duplication bearing the same intuition as the\nboosting algorithm. We use numerical experiments to demonstrate the\neffectiveness of our algorithm and several variants.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 02:34:05 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Gao", "Wenzhi", ""], ["Sun", "Chunlin", ""], ["Ye", "Yuyang", ""], ["Ye", "Yinyu", ""]]}, {"id": "2107.03662", "submitter": "Vasilis Livanos", "authors": "Chandra Chekuri and Vasilis Livanos", "title": "On Submodular Prophet Inequalities and Correlation Gap", "comments": "38 pages, 1 figure, SAGT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prophet inequalities and secretary problems have been extensively studied in\nrecent years due to their elegance, connections to online algorithms,\nstochastic optimization, and mechanism design problems in game theoretic\nsettings. Rubinstein and Singla developed a notion of combinatorial prophet\ninequalities in order to generalize the standard prophet inequality setting to\ncombinatorial valuation functions such as submodular and subadditive functions.\nFor non-negative submodular functions they demonstrated a constant factor\nprophet inequality for matroid constraints. Along the way they showed a variant\nof the correlation gap for non-negative submodular functions.\n  In this paper we revisit their notion of correlation gap as well as the\nstandard notion of correlation gap and prove much tighter and cleaner bounds.\nVia these bounds and other insights we obtain substantially improved constant\nfactor combinatorial prophet inequalities for both monotone and non-monotone\nsubmodular functions over any constraint that admits an Online Contention\nResolution Scheme. In addition to improved bounds we describe efficient\npolynomial-time algorithms that achieve these bounds.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 07:48:04 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Chekuri", "Chandra", ""], ["Livanos", "Vasilis", ""]]}, {"id": "2107.03795", "submitter": "Marilena Leichter", "authors": "Marilena Leichter, Benjamin Moseley, Kirk Pruhs", "title": "An Efficient Reduction of a Gammoid to a Partition Matroid", "comments": "Full version of a paper accepted at ESA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our main contribution is a polynomial-time algorithm to reduce a\n$k$-colorable gammoid to a $(2k-2)$-colorable partition matroid. It is known\nthat there are gammoids that can not be reduced to any $(2k-3)$-colorable\npartition matroid, so this result is tight. We then discuss how such a\nreduction can be used to obtain polynomial-time algorithms with better\napproximation ratios for various natural problems related to coloring and list\ncoloring the intersection of matroids.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 12:12:37 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Leichter", "Marilena", ""], ["Moseley", "Benjamin", ""], ["Pruhs", "Kirk", ""]]}, {"id": "2107.03821", "submitter": "Jakub Tetek", "authors": "Jakub T\\v{e}tek, Mikkel Thorup", "title": "Sampling and Counting Edges via Vertex Accesses", "comments": "This paper subsumes the arXiv report (arXiv:2009.11178) which only\n  contains the result on sampling one edge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of sampling and counting edges from a graph on $n$\nvertices where our basic access is via uniformly sampled vertices. When we have\na vertex, we can see its degree, and access its neighbors. Eden and Rosenbaum\n[SOSA 2018] have shown it is possible to sample an edge $\\epsilon$-uniformly in\n$O(\\sqrt{1/\\epsilon}\\frac{n}{\\sqrt{m}})$ vertex accesses. Here, we get down to\nexpected $O(\\log(1/\\epsilon)\\frac{n}{\\sqrt{m}})$ vertex accesses. Next, we\nconsider the problem of sampling $s>1$ edges. For this we introduce a model\nthat we call hash-based neighbor access. We show that, w.h.p, we can sample $s$\nedges exactly uniformly at random, with or without replacement, in\n$\\tilde{O}(\\sqrt{s} \\frac{n}{\\sqrt{m}} + s)$ vertex accesses. We present a\nmatching lower bound of $\\Omega(\\sqrt{s} \\frac{n}{\\sqrt{m}} + s)$ which holds\nfor $\\epsilon$-uniform edge multi-sampling with some constant $\\epsilon>0$ even\nthough our positive result has $\\epsilon=0$.\n  We then give an algorithm for edge counting. W.h.p., we count the number of\nedges to within error $\\epsilon$ in time $\\tilde{O}(\\frac{n}{\\epsilon\\sqrt{m}}\n+ \\frac{1}{\\epsilon^2})$. When $\\epsilon$ is not too small (for $\\epsilon \\geq\n\\frac{\\sqrt m}{n}$), we present a near-matching lower-bound of\n$\\Omega(\\frac{n}{\\epsilon \\sqrt{m}})$. In the same range, the previous best\nupper and lower bounds were polynomially worse in $\\epsilon$.\n  Finally, we give an algorithm that instead of hash-based neighbor access uses\nthe more standard pair queries (``are vertices $u$ and $v$ adjacent''). W.h.p.\nit returns $1+\\epsilon$ approximation of the number of edges and runs in\nexpected time $\\tilde{O}(\\frac{n}{\\epsilon \\sqrt{m}} + \\frac{1}{\\epsilon^4})$.\nThis matches our lower bound when $\\epsilon$ is not too small, specifically for\n$\\epsilon \\geq \\frac{m^{1/6}}{n^{1/3}}$.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 12:59:08 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["T\u011btek", "Jakub", ""], ["Thorup", "Mikkel", ""]]}, {"id": "2107.03885", "submitter": "Boaz Menuhin", "authors": "Boaz Menuhin and Moni Naor", "title": "Keep That Card in Mind: Card Guessing with Limited Memory", "comments": "53 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A card guessing game is played between two players, Guesser and Dealer. At\nthe beginning of the game, the Dealer holds a deck of $n$ cards (labeled $1,\n..., n$). For $n$ turns, the Dealer draws a card from the deck, the Guesser\nguesses which card was drawn, and then the card is discarded from the deck. The\nGuesser receives a point for each correctly guessed card. With perfect memory,\na Guesser can keep track of all cards that were played so far and pick at\nrandom a card that has not appeared so far, yielding in expectation $\\ln n$\ncorrect guesses. With no memory, the best a Guesser can do will result in a\nsingle guess in expectation. We consider the case of a memory bounded Guesser\nthat has $m < n$ memory bits. We show that the performance of such a memory\nbounded Guesser depends much on the behavior of the Dealer. In more detail, we\nshow that there is a gap between the static case, where the Dealer draws cards\nfrom a properly shuffled deck or a prearranged one, and the adaptive case,\nwhere the Dealer draws cards thoughtfully, in an adversarial manner.\nSpecifically:\n  1. We show a Guesser with $O(\\log^2 n)$ memory bits that scores a near\noptimal result against any static Dealer.\n  2. We show that no Guesser with $m$ bits of memory can score better than\n$O(\\sqrt{m})$ correct guesses, thus, no Guesser can score better than $\\min\n\\{\\sqrt{m}, \\ln n\\}$, i.e., the above Guesser is optimal.\n  3. We show an efficient adaptive Dealer against which no Guesser with $m$\nmemory bits can make more than $\\ln m + 2 \\ln \\log n + O(1)$ correct guesses in\nexpectation.\n  These results are (almost) tight, and we prove them using compression\narguments that harness the guessing strategy for encoding.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 15:09:20 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Menuhin", "Boaz", ""], ["Naor", "Moni", ""]]}, {"id": "2107.03916", "submitter": "Dimitrios Los", "authors": "Dimitrios Los and Thomas Sauerwald", "title": "Balanced Allocations with Incomplete Information: The Power of Two\n  Queries", "comments": "43 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of allocating $m$ balls into $n$ bins with incomplete\ninformation. In the classical two-choice process, a ball first queries the load\nof $\\textit{two}$ randomly chosen bins and is then placed in the least loaded\nbin. In our setting, each ball also samples two random bins but can only\nestimate a bin's load by sending $\\textit{binary queries}$ of the form \"Is the\nload at least the median?\" or \"Is the load at least $100$?\".\n  For the lightly loaded case $m=O(n)$, one can achieve an $O(\\sqrt{\\log n/\\log\n\\log n})$ maximum load with one query per chosen bin using an oblivious\nstrategy, as shown by Feldheim and Gurel-Gurevich (2018). For the case\n$m=\\Omega(n)$, the authors conjectured that the same strategy achieves a\nmaximum load of $m/n+O(\\sqrt{\\log n/\\log \\log n})$. In this work, we disprove\nthis conjecture by showing a lower bound of $m/n+\\Omega( \\sqrt{\\log n})$ for a\nfixed $m=\\Theta(n \\sqrt{\\log n})$, and a lower bound of $m/n+\\Omega(\\log\nn/\\log\\log n)$ for some $m$ depending on the used strategy. Surprisingly, these\nlower bounds hold even for any $\\textit{adaptive strategy}$ with one query,\ni.e., queries may depend on the full history of the process.\n  We complement this negative result by proving a positive result for multiple\nqueries. In particular, we show that with only two binary queries per chosen\nbin, there is an oblivious strategy which ensures a maximum load of\n$m/n+O(\\sqrt{\\log n})$ whp for any $m \\geq 1$. For any $k=O(\\log \\log n)$\nbinary queries, the upper bound on the maximum load improves to $m/n+O(k(\\log\nn)^{1/k})$ whp for any $m \\geq 1$. Hence for $k=\\Theta(\\log\\log n)$, we recover\nthe two-choice result up to a constant multiplicative factor, including the\nheavily loaded case where $m=\\Omega(n)$. One novel aspect of our proof\ntechniques is the use of multiple super-exponential potential functions, which\nmight be of use in future work.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 15:49:19 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Los", "Dimitrios", ""], ["Sauerwald", "Thomas", ""]]}, {"id": "2107.03932", "submitter": "Kewen Wu", "authors": "Kun He, Xiaoming Sun, Kewen Wu", "title": "Perfect Sampling for (Atomic) Lov\\'asz Local Lemma", "comments": "56 pages, 1 table, 5 figures, 9 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a Markov chain based perfect sampler for uniform sampling solutions\nof constraint satisfaction problems (CSP). Under some mild Lov\\'asz local lemma\nconditions where each constraint of the CSP has a small number of forbidden\nlocal configurations, our algorithm is accurate and efficient: it outputs a\nperfect uniform random solution and its expected running time is quasilinear in\nthe number of variables. Prior to our work, perfect samplers are only shown to\nexist for CSPs under much more restrictive conditions (Guo, Jerrum, and Liu,\nJACM'19).\n  Our algorithm has two components:\n  1. A simple perfect sampling algorithm using bounding chains (Huber, STOC'98;\nHaggstrom and Nelander, Scandinavian Journal of Statistics'99). This sampler is\nefficient if each variable domain is small.\n  2. A simple but powerful state tensorization trick to reduce large domains to\nsmaller ones. This trick is a generalization of state compression (Feng, He,\nand Yin, STOC'21).\n  The crux of our analysis is a simple information percolation argument which\nallows us to achieve bounds even beyond current best approximate samplers\n(Jain, Pham, and Vuong, ArXiv'21).\n  Previous related works either use intricate algorithms or need sophisticated\nanalysis or even both. Thus we view the simplicity of both our algorithm and\nanalysis as a strength of our work.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 16:04:40 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["He", "Kun", ""], ["Sun", "Xiaoming", ""], ["Wu", "Kewen", ""]]}, {"id": "2107.04100", "submitter": "Adam Kurpisz", "authors": "Adam Kurpisz, Aaron Potechin, Elias Samuel Wirth", "title": "SoS certification for symmetric quadratic functions and its connection\n  to constrained Boolean hypercube optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the rank of the Sum of Squares (SoS) hierarchy over the Boolean\nhypercube for Symmetric Quadratic Functions (SQFs) in $n$ variables with roots\nplaced in points $k-1$ and $k$. Functions of this type have played a central\nrole in deepening the understanding of the performance of the SoS method for\nvarious unconstrained Boolean hypercube optimization problems, including the\nMax Cut problem. Recently, Lee, Prakash, de Wolf, and Yuen proved a lower bound\non the SoS rank for SQFs of $\\Omega(\\sqrt{k(n-k)})$ and conjectured the lower\nbound of $\\Omega(n)$ by similarity to a polynomial representation of the\n$n$-bit OR function.\n  Using Chebyshev polynomials, we refute the Lee -- Prakash -- de~Wolf -- Yuen\nconjecture and prove that the SoS rank for SQFs is at most\n$O(\\sqrt{nk}\\log(n))$.\n  We connect this result to two constrained Boolean hypercube optimization\nproblems. First, we provide a degree $O( \\sqrt{n})$ SoS certificate that\nmatches the known SoS rank lower bound for an instance of Min Knapsack, a\nproblem that was intensively studied in the literature. Second, we study an\ninstance of the Set Cover problem for which Bienstock and Zuckerberg\nconjectured an SoS rank lower bound of $n/4$. We refute the Bienstock --\nZuckerberg conjecture and provide a degree $O(\\sqrt{n}\\log(n))$ SoS certificate\nfor this problem.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 20:38:25 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Kurpisz", "Adam", ""], ["Potechin", "Aaron", ""], ["Wirth", "Elias Samuel", ""]]}, {"id": "2107.04252", "submitter": "Bala Krishnamoorthy", "authors": "Matthew Broussard and Bala Krishnamoorthy", "title": "A Tight Max-Flow Min-Cut Duality Theorem for Non-Linear Multicommodity\n  Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Max-Flow Min-Cut theorem is the classical duality result for the Max-Flow\nproblem, which considers flow of a single commodity. We study a multiple\ncommodity generalization of Max-Flow in which flows are composed of real-valued\nk-vectors through networks with arc capacities formed by regions in \\R^k. Given\nthe absence of a clear notion of ordering in the multicommodity case, we define\nthe generalized max flow as the feasible region of all flow values.\n  We define a collection of concepts and operations on flows and cuts in the\nmulticommodity setting. We study the mutual capacity of a set of cuts, defined\nas the set of flows that can pass through all cuts in the set. We present a\nmethod to calculate the mutual capacity of pairs of cuts, and then generalize\nthe same to a method of calculation for arbitrary sets of cuts. We show that\nthe mutual capacity is exactly the set of feasible flows in the network, and\nhence is equal to the max flow. Furthermore, we present a simple class of the\nmulticommodity max flow problem where computations using this tight duality\nresult could run significantly faster than default brute force computations.\n  We also study more tractable special cases of the multicommodity max flow\nproblem where the objective is to transport a maximum real or integer multiple\nof a given vector through the network. We devise an augmenting cycle search\nalgorithm that reduces the optimization problem to one with m constraints in at\nmost \\R^{(m-n+1)k} space from one that requires mn constraints in \\R^{mk} space\nfor a network with n nodes and m edges. We present efficient algorithms that\ncompute eps-approximations to both the ratio and the integer ratio maximum flow\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 06:52:44 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Broussard", "Matthew", ""], ["Krishnamoorthy", "Bala", ""]]}, {"id": "2107.04368", "submitter": "Michael McKay", "authors": "Michael McKay and David Manlove", "title": "The Three-Dimensional Stable Roommates Problem with Additively Separable\n  Preferences", "comments": "A full version of a paper accepted to the 14th International\n  Symposium on Algorithmic Game Theory (SAGT 2021), with 38 pages (including 2\n  pages of references) and 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Stable Roommates problem involves matching a set of agents into pairs\nbased on the agents' strict ordinal preference lists. The matching must be\nstable, meaning that no two agents strictly prefer each other to their assigned\npartners. A number of three-dimensional variants exist, in which agents are\ninstead matched into triples. Both the original problem and these variants can\nalso be viewed as hedonic games. We formalise a three-dimensional variant using\ngeneral additively separable preferences, in which each agent provides an\ninteger valuation of every other agent. In this variant, we show that a stable\nmatching may not exist and that the related decision problem is NP-complete,\neven when the valuations are binary. In contrast, we show that if the\nvaluations are binary and symmetric then a stable matching must exist and can\nbe found in polynomial time. We also consider the related problem of finding a\nstable matching with maximum utilitarian welfare when valuations are binary and\nsymmetric. We show that this optimisation problem is NP-hard and present a\nnovel 2-approximation algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 11:20:03 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["McKay", "Michael", ""], ["Manlove", "David", ""]]}, {"id": "2107.04423", "submitter": "Emily Diana", "authors": "Emily Diana, Wesley Gill, Michael Kearns, Krishnaram Kenthapadi, Aaron\n  Roth, and Saeed Sharifi-Malvajerdi", "title": "Multiaccurate Proxies for Downstream Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of training a model that must obey demographic fairness\nconditions when the sensitive features are not available at training time -- in\nother words, how can we train a model to be fair by race when we don't have\ndata about race? We adopt a fairness pipeline perspective, in which an\n\"upstream\" learner that does have access to the sensitive features will learn a\nproxy model for these features from the other attributes. The goal of the proxy\nis to allow a general \"downstream\" learner -- with minimal assumptions on their\nprediction task -- to be able to use the proxy to train a model that is fair\nwith respect to the true sensitive features. We show that obeying multiaccuracy\nconstraints with respect to the downstream model class suffices for this\npurpose, and provide sample- and oracle efficient-algorithms and generalization\nbounds for learning such proxies. In general, multiaccuracy can be much easier\nto satisfy than classification accuracy, and can be satisfied even when the\nsensitive features are hard to predict.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 13:16:44 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Diana", "Emily", ""], ["Gill", "Wesley", ""], ["Kearns", "Michael", ""], ["Kenthapadi", "Krishnaram", ""], ["Roth", "Aaron", ""], ["Sharifi-Malvajerdi", "Saeed", ""]]}, {"id": "2107.04482", "submitter": "Niels Gr\\\"uttemeier", "authors": "Niels Gr\\\"uttemeier, Christian Komusiewicz, Nils Morawietz, Frank\n  Sommer", "title": "Preventing Small $\\mathbf{(s,t)}$-Cuts by Protecting Edges", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce and study Weighted Min $(s,t)$-Cut Prevention, where we are\ngiven a graph $G=(V,E)$ with vertices $s$ and $t$ and an edge cost function and\nthe aim is to choose an edge set $D$ of total cost at most $d$ such that $G$\nhas no $(s,t)$-edge cut of capacity at most $a$ that is disjoint from $D$. We\nshow that Weighted Min $(s,t)$-Cut Prevention is NP-hard even on subcubcic\ngraphs when all edges have capacity and cost one and provide a comprehensive\nstudy of the parameterized complexity of the problem. We show, for example\nW[1]-hardness with respect to $d$ and an FPT algorithm for $a$.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 15:15:38 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Gr\u00fcttemeier", "Niels", ""], ["Komusiewicz", "Christian", ""], ["Morawietz", "Nils", ""], ["Sommer", "Frank", ""]]}, {"id": "2107.04660", "submitter": "Ryan Rossi", "authors": "Tung Mai, Anup Rao, Ryan A. Rossi, Saeed Seddighin", "title": "Optimal Space and Time for Streaming Pattern Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study longest common substring, pattern matching, and\nwildcard pattern matching in the asymmetric streaming model. In this streaming\nmodel, we have random access to one string and streaming access to the other\none. We present streaming algorithms with provable guarantees for these three\nfundamental problems. In particular, our algorithms for pattern matching\nimprove the upper bound and beat the unconditional lower bounds on the memory\nof randomized and deterministic streaming algorithms. In addition to this, we\npresent algorithms for wildcard pattern matching in the asymmetric streaming\nmodel that have optimal space and time.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 20:25:40 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Mai", "Tung", ""], ["Rao", "Anup", ""], ["Rossi", "Ryan A.", ""], ["Seddighin", "Saeed", ""]]}, {"id": "2107.04674", "submitter": "Victor Verdugo", "authors": "Sebastian Perez-Salazar, Alfredo Torrico, Victor Verdugo", "title": "Preserving Diversity when Partitioning: A Geometric Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diversity plays a crucial role in multiple contexts such as team formation,\nrepresentation of minority groups and generally when allocating resources\nfairly. Given a community composed by individuals of different types, we study\nthe problem of partitioning this community such that the global diversity is\npreserved as much as possible in each subgroup. We consider the diversity\nmetric introduced by Simpson in his influential work that, roughly speaking,\ncorresponds to the inverse probability that two individuals are from the same\ntype when taken uniformly at random, with replacement, from the community of\ninterest. We provide a novel perspective by reinterpreting this quantity in\ngeometric terms. We characterize the instances in which the optimal partition\nexactly preserves the global diversity in each subgroup. When this is not\npossible, we provide an efficient polynomial-time algorithm that outputs an\noptimal partition for the problem with two types. Finally, we discuss further\nchallenges and open questions for the problem that considers more than two\ntypes.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 20:48:45 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Perez-Salazar", "Sebastian", ""], ["Torrico", "Alfredo", ""], ["Verdugo", "Victor", ""]]}, {"id": "2107.04699", "submitter": "Guohui Lin", "authors": "Yong Chen, Zhi-Zhong Chen, Curtis Kennedy, Guohui Lin, Yao Xu and An\n  Zhang", "title": "Approximation algorithms for the directed path partition problems", "comments": "Extended abstract to appear in FAW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a directed graph $G = (V, E)$, the $k$-path partition problem is to\nfind a minimum collection of vertex-disjoint directed paths each of order at\nmost $k$ to cover all the vertices of $V$. The problem has various applications\nin facility location, network monitoring, transportation and others. Its\nspecial case on undirected graphs has received much attention recently, but the\ngeneral directed version is seemingly untouched in the literature. We present\nthe first $k/2$-approximation algorithm, for any $k \\ge 3$, based on a novel\nconcept of augmenting path to minimize the number of singletons in the\npartition. When $k \\ge 7$, we present an improved $(k+2)/3$-approximation\nalgorithm based on the maximum path-cycle cover followed by a careful $2$-cycle\nelimination process. When $k = 3$, we define the second novel kind of\naugmenting paths and propose an improved $13/9$-approximation algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 22:21:02 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chen", "Yong", ""], ["Chen", "Zhi-Zhong", ""], ["Kennedy", "Curtis", ""], ["Lin", "Guohui", ""], ["Xu", "Yao", ""], ["Zhang", "An", ""]]}, {"id": "2107.04754", "submitter": "Aleck Johnsen", "authors": "Jason Hartline and Aleck Johnsen", "title": "Lower Bounds for Prior Independent Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prior independent framework for algorithm design considers how well an\nalgorithm that does not know the distribution of its inputs approximates the\nexpected performance of the optimal algorithm for this distribution. This paper\ngives a method that is agnostic to problem setting for proving lower bounds on\nthe prior independent approximation factor of any algorithm. The method\nconstructs a correlated distribution over inputs that can be generated both as\na distribution over i.i.d. good-for-algorithms distributions and as a\ndistribution over i.i.d. bad-for-algorithms distributions. Prior independent\nalgorithms are upper-bounded by the optimal algorithm for the latter\ndistribution even when the true distribution is the former. Thus, the ratio of\nthe expected performances of the Bayesian optimal algorithms for these two\ndecompositions is a lower bound on the prior independent approximation ratio.\nThe techniques of the paper connect prior independent algorithm design, Yao's\nMinimax Principle, and information design. We apply this framework to give new\nlower bounds on several canonical prior independent mechanism design problems.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 04:03:08 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Hartline", "Jason", ""], ["Johnsen", "Aleck", ""]]}, {"id": "2107.04763", "submitter": "Hao Sun", "authors": "Alexander G\\\"oke, Jochen Koenemann, Matthias Mnich, Hao Sun", "title": "Hitting Weighted Even Cycles in Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical branch of graph algorithms is graph transversals, where one seeks\n<a minimum-weight subset of nodes in a node-weighted graph $G$ which intersects\nall copies of subgraphs $F$ from a fixed family $\\mathcal F$.\n  Many such graph transversal problems have been shown to admit polynomial-time\napproximation schemes (PTAS) for planar input graphs $G$, using a variety of\ntechniques like the shifting technique (Baker, J. ACM 1994), bidimensionality\n(Fomin et al., SODA 2011), or connectivity domination (Cohen-Addad et al., STOC\n2016).\n  These techniques do not seem to apply to graph transversals with parity\nconstraints, which have recently received significant attention, but for which\nno PTASs are known.\n  In the even-cycle transversal (ECT) problem, the goal is to find a\nminimum-weight hitting set for the set of even cycles in an undirected graph.\n  For ECT, Fiorini et al. (IPCO 2010) showed that the integrality gap of the\nstandard covering LP relaxation is $\\Theta(\\log n)$, and that adding sparsity\ninequalities reduces the integrality gap to~10.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 04:53:48 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["G\u00f6ke", "Alexander", ""], ["Koenemann", "Jochen", ""], ["Mnich", "Matthias", ""], ["Sun", "Hao", ""]]}, {"id": "2107.04919", "submitter": "Corwin Sinnamon", "authors": "Maria Hartmann, L\\'aszl\\'o Kozma, Corwin Sinnamon, Robert E. Tarjan", "title": "Analysis of Smooth Heaps and Slim Heaps", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The smooth heap is a recently introduced self-adjusting heap [Kozma,\nSaranurak, 2018] similar to the pairing heap [Fredman, Sedgewick, Sleator,\nTarjan, 1986]. The smooth heap was obtained as a heap-counterpart of Greedy\nBST, a binary search tree updating strategy conjectured to be\n\\emph{instance-optimal} [Lucas, 1988], [Munro, 2000]. Several adaptive\nproperties of smooth heaps follow from this connection; moreover, the smooth\nheap itself has been conjectured to be instance-optimal within a certain class\nof heaps. Nevertheless, no general analysis of smooth heaps has existed until\nnow, the only previous analysis showing that, when used in \\emph{sorting mode}\n($n$ insertions followed by $n$ delete-min operations), smooth heaps sort $n$\nnumbers in $O(n\\lg n)$ time.\n  In this paper we describe a simpler variant of the smooth heap we call the\n\\emph{slim heap}. We give a new, self-contained analysis of smooth heaps and\nslim heaps in unrestricted operation, obtaining amortized bounds that match the\nbest bounds known for self-adjusting heaps. Previous experimental work has\nfound the pairing heap to dominate other data structures in this class in\nvarious settings. Our tests show that smooth heaps and slim heaps are\ncompetitive with pairing heaps, outperforming them in some cases, while being\ncomparably easy to implement.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 22:08:43 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Hartmann", "Maria", ""], ["Kozma", "L\u00e1szl\u00f3", ""], ["Sinnamon", "Corwin", ""], ["Tarjan", "Robert E.", ""]]}, {"id": "2107.04993", "submitter": "Michael Bekos", "authors": "Jawaherul Md. Alam, Michael A. Bekos, Martin Gronemann, Michael\n  Kaufmann, Sergey Pupyrev", "title": "The Mixed Page Number of Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A linear layout of a graph typically consists of a total vertex order, and a\npartition of the edges into sets of either non-crossing edges, called stacks,\nor non-nested edges, called queues. The stack (queue) number of a graph is the\nminimum number of required stacks (queues) in a linear layout. Mixed linear\nlayouts combine these layouts by allowing each set of edges to form either a\nstack or a queue. In this work we initiate the study of the mixed page number\nof a graph which corresponds to the minimum number of such sets.\n  First, we study the edge density of graphs with bounded mixed page number.\nThen, we focus on complete and complete bipartite graphs, for which we derive\nlower and upper bounds on their mixed page number. Our findings indicate that\ncombining stacks and queues is more powerful in various ways compared to the\ntwo traditional layouts.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 08:49:26 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Alam", "Jawaherul Md.", ""], ["Bekos", "Michael A.", ""], ["Gronemann", "Martin", ""], ["Kaufmann", "Michael", ""], ["Pupyrev", "Sergey", ""]]}, {"id": "2107.04997", "submitter": "Benwei Wu", "authors": "Kai Han, Benwei Wu, Jing Tang, Shuang Cui, Cigdem Aslay, Laks V. S.\n  Lakshmanan", "title": "Efficient and Effective Algorithms for Revenue Maximization in Social\n  Advertising", "comments": "extended version of the paper in sigmod2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the revenue maximization problem in social advertising, where a\nsocial network platform owner needs to select seed users for a group of\nadvertisers, each with a payment budget, such that the total expected revenue\nthat the owner gains from the advertisers by propagating their ads in the\nnetwork is maximized. Previous studies on this problem show that it is\nintractable and present approximation algorithms. We revisit this problem from\na fresh perspective and develop novel efficient approximation algorithms, both\nunder the setting where an exact influence oracle is assumed and under one\nwhere this assumption is relaxed. Our approximation ratios significantly\nimprove upon the previous ones. Furthermore, we empirically show, using\nextensive experiments on four datasets, that our algorithms considerably\noutperform the existing methods on both the solution quality and computation\nefficiency.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 08:58:53 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 04:12:38 GMT"}, {"version": "v3", "created": "Mon, 26 Jul 2021 11:04:40 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Han", "Kai", ""], ["Wu", "Benwei", ""], ["Tang", "Jing", ""], ["Cui", "Shuang", ""], ["Aslay", "Cigdem", ""], ["Lakshmanan", "Laks V. S.", ""]]}, {"id": "2107.05018", "submitter": "Stanislav \\v{Z}ivn\\'y", "authors": "Lorenzo Ciardo and Stanislav \\v{Z}ivn\\'y", "title": "CLAP: A New Algorithm for Promise CSPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for Promise Constraint Satisfaction Problems\n(PCSPs). It is a combination of the Constraint Basic LP relaxation and the\nAffine IP relaxation (CLAP). We give a characterisation of the power of CLAP in\nterms of a minion homomorphism. Using this characterisation, we identify a\ncertain weak notion of symmetry which, if satisfied by infinitely many\npolymorphisms of PCSPs, guarantees tractability.\n  We demonstrate that there are PCSPs solved by CLAP that are not solved by any\nof the existing algorithms for PCSPs; in particular, not by the BLP+AIP\nalgorithm of Brakensiek and Guruswami [SODA'20] and not by a reduction to\ntractable finite-domain CSPs.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 10:51:40 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ciardo", "Lorenzo", ""], ["\u017divn\u00fd", "Stanislav", ""]]}, {"id": "2107.05082", "submitter": "Jorge Silva", "authors": "Jorge F. Silva and Pablo Piantanida", "title": "On Universal D-Semifaithful Coding for Memoryless Sources with Infinite\n  Alphabets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The problem of variable length and fixed-distortion universal source coding\n(or D-semifaithful source coding) for stationary and memoryless sources on\ncountably infinite alphabets ($\\infty$-alphabets) is addressed in this paper.\nThe main results of this work offer a set of sufficient conditions (from weaker\nto stronger) to obtain weak minimax universality, strong minimax universality,\nand corresponding achievable rates of convergences for the worse-case\nredundancy for the family of stationary memoryless sources whose densities are\ndominated by an envelope function (or the envelope family) on\n$\\infty$-alphabets. An important implication of these results is that universal\nD-semifaithful source coding is not feasible for the complete family of\nstationary and memoryless sources on $\\infty$-alphabets. To demonstrate this\ninfeasibility, a sufficient condition for the impossibility is presented for\nthe envelope family. Interestingly, it matches the well-known impossibility\ncondition in the context of lossless (variable-length) universal source coding.\nMore generally, this work offers a simple description of what is needed to\nachieve universal D-semifaithful coding for a family of distributions\n$\\Lambda$. This reduces to finding a collection of quantizations of the product\nspace at different block-lengths -- reflecting the fixed distortion restriction\n-- that satisfy two asymptotic requirements: the first is a universal\nquantization condition with respect to $\\Lambda$, and the second is a vanishing\ninformation radius (I-radius) condition for $\\Lambda$ reminiscent of the\ncondition known for lossless universal source coding.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 16:32:08 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Silva", "Jorge F.", ""], ["Piantanida", "Pablo", ""]]}, {"id": "2107.05198", "submitter": "Debajyoti Mondal", "authors": "J. Mark Keil, Debajyoti Mondal, Ehsan Moradi, Yakov Nekrich", "title": "Finding a Maximum Clique in a Grounded 1-Bend String Graph", "comments": "A preliminary version of the paper was presented at the 32nd Canadian\n  Conference on Computational Geometry (CCCG 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A grounded 1-bend string graph is an intersection graph of a set of polygonal\nlines, each with one bend, such that the lines lie above a common horizontal\nline $\\ell$ and have exactly one endpoint on $\\ell$. We show that the problem\nof finding a maximum clique in a grounded 1-bend string graph is APX-hard, even\nfor strictly $y$-monotone strings. For general 1-bend strings, the problem\nremains APX-hard even if we restrict the position of the bends and end-points\nto lie on at most three parallel horizontal lines. We give fast algorithms to\ncompute a maximum clique for different subclasses of grounded segment graphs,\nwhich are formed by restricting the strings to various forms of $L$-shapes.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 05:17:35 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Keil", "J. Mark", ""], ["Mondal", "Debajyoti", ""], ["Moradi", "Ehsan", ""], ["Nekrich", "Yakov", ""]]}, {"id": "2107.05434", "submitter": "Pawe{\\l} Rz\\k{a}\\.zewski", "authors": "Tara Abrishami and Maria Chudnovsky and Cemil Dibek and Pawe{\\l}\n  Rz\\k{a}\\.zewski", "title": "Polynomial-time algorithm for Maximum Independent Set in bounded-degree\n  graphs with no long induced claws", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For graphs $G$ and $H$, we say that $G$ is $H$-free if it does not contain\n$H$ as an induced subgraph. Already in the early 1980s Alekseev observed that\nif $H$ is connected, then the \\textsc{Max Weight Independent Set} problem\n(MWIS) remains \\textsc{NP}-hard in $H$-free graphs, unless $H$ is a path or a\nsubdivided claw, i.e., a graph obtained from the three-leaf star by subdividing\neach edge some number of times (possibly zero). Since then determining the\ncomplexity of MWIS in these remaining cases is one of the most important\nproblems in algorithmic graph theory.\n  A general belief is that the problem is polynomial-time solvable, which is\nwitnessed by algorithmic results for graphs excluding some small paths or\nsubdivided claws. A more conclusive evidence was given by the recent\nbreakthrough result by Gartland and Lokshtanov [FOCS 2020]: They proved that\nMWIS can be solved in quasipolynomial time in $H$-free graphs, where $H$ is any\nfixed path. If $H$ is an arbitrary subdivided claw, we know much less: The\nproblem admits a QPTAS and a subexponential-time algorithm [Chudnovsky et al.,\nSODA 2019].\n  In this paper we make an important step towards solving the problem by\nshowing that for any subdivided claw $H$, MWIS is polynomial-time solvable in\n$H$-free graphs of bounded degree.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 14:03:00 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Abrishami", "Tara", ""], ["Chudnovsky", "Maria", ""], ["Dibek", "Cemil", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""]]}, {"id": "2107.05486", "submitter": "Heng Guo", "authors": "Andreas Galanis, Heng Guo, Jiaheng Wang", "title": "Inapproximability of counting hypergraph colourings", "comments": "abstract shortened for arXiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in approximate counting have made startling progress in\ndeveloping fast algorithmic methods for approximating the number of solutions\nto constraint satisfaction problems (CSPs) with large arities, using\nconnections to the Lovasz Local Lemma. Nevertheless, the boundaries of these\nmethods for CSPs with non-Boolean domain are not well-understood. Our goal in\nthis paper is to fill in this gap and obtain strong inapproximability results\nby studying the prototypical problem in this class of CSPs, hypergraph\ncolourings.\n  More precisely, we focus on the problem of approximately counting\n$q$-colourings on $K$-uniform hypergraphs with bounded degree $\\Delta$. An\nefficient algorithm exists if $\\Delta\\lesssim \\frac{q^{K/3-1}}{4^KK^2}$ (Jain,\nPham, and Voung, 2021; He, Sun, and Wu, 2021). Somewhat surprisingly however, a\nhardness bound is not known even for the easier problem of finding colourings.\nFor the counting problem, the situation is even less clear and there is no\nevidence of the right constant controlling the growth of the exponent in terms\nof $K$.\n  To this end, we first establish that for general $q$ computational hardness\nfor finding a colouring on simple/linear hypergraphs occurs at $\\Delta\\gtrsim\nKq^K$, almost matching the algorithm from the Lovasz Local Lemma. Our second\nand main contribution is to obtain a far more refined bound for the counting\nproblem that goes well beyond the hardness of finding a colouring and which we\nconjecture that is asymptotically tight (up to constant factors). We show in\nparticular that for all even $q\\geq 4$ it is NP-hard to approximate the number\nof colourings when $\\Delta\\gtrsim q^{K/2}$.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 15:02:00 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Galanis", "Andreas", ""], ["Guo", "Heng", ""], ["Wang", "Jiaheng", ""]]}, {"id": "2107.05518", "submitter": "Maximilian Katzmann", "authors": "Thomas Bl\\\"asius, Tobias Friedrich, Maximilian Katzmann, Daniel\n  Stephan", "title": "Routing in Strongly Hyperbolic Unit Disk Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Greedy routing has been studied successfully on Euclidean unit disk graphs,\nwhich we interpret as a special case of hyperbolic unit disk graphs. While\nsparse Euclidean unit disk graphs exhibit grid-like structure, we introduce\nstrongly hyperbolic unit disk graphs as the natural counterpart containing\ngraphs that have hierarchical network structures.\n  We develop and analyze a routing scheme that utilizes these hierarchies. On\narbitrary graphs this scheme guarantees a worst case stretch of $\\max\\{3,\n1+2b/a\\}$ for $a > 0$ and $b > 1$. Moreover, it stores $\\mathcal{O}(k(\\log^2{n}\n+ \\log{k}))$ bits at each vertex and takes $\\mathcal{O}(k)$ time for a routing\ndecision, where $k = \\pi e (1 + a)/(2(b - 1)) (b^2 \\text{diam}(G) - 1) R +\n\\log_b(\\text{diam}(G)) + 1$, on strongly hyperbolic unit disk graphs with\nthreshold radius $R > 0$. In particular, for hyperbolic random graphs, which\nhave previously been used to model hierarchical networks like the internet, $k\n= \\mathcal{O}(\\log^2{n})$ holds asymptotically almost surely. Thus, we obtain a\nworst-case stretch of $3$, $\\mathcal{O}(\\log^4 n)$ bits of storage per vertex,\nand $\\mathcal{O}(\\log^2 n)$ time per routing decision on such networks. This\nbeats existing worst-case lower bounds. Our proof of concept implementation\nindicates that the obtained results translate well to real-world networks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 15:46:09 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Friedrich", "Tobias", ""], ["Katzmann", "Maximilian", ""], ["Stephan", "Daniel", ""]]}, {"id": "2107.05567", "submitter": "Dmitriy Kunisky", "authors": "Dmitriy Kunisky and Jonathan Niles-Weed", "title": "Strong recovery of geometric planted matchings", "comments": "47 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of efficiently recovering the matching between an\nunlabelled collection of $n$ points in $\\mathbb{R}^d$ and a small random\nperturbation of those points. We consider a model where the initial points are\ni.i.d. standard Gaussian vectors, perturbed by adding i.i.d. Gaussian vectors\nwith variance $\\sigma^2$. In this setting, the maximum likelihood estimator\n(MLE) can be found in polynomial time as the solution of a linear assignment\nproblem. We establish thresholds on $\\sigma^2$ for the MLE to perfectly recover\nthe planted matching (making no errors) and to strongly recover the planted\nmatching (making $o(n)$ errors) both for $d$ constant and $d = d(n)$ growing\narbitrarily. Between these two thresholds, we show that the MLE makes\n$n^{\\delta + o(1)}$ errors for an explicit $\\delta \\in (0, 1)$. These results\nextend to the geometric setting a recent line of work on recovering matchings\nplanted in random graphs with independently-weighted edges. Our proof\ntechniques rely on careful analysis of the combinatorial structure of partial\nmatchings in large, weakly dependent random graphs using the first and second\nmoment methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 16:44:48 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Kunisky", "Dmitriy", ""], ["Niles-Weed", "Jonathan", ""]]}, {"id": "2107.05582", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane and Christos Tzamos", "title": "Forster Decomposition and Learning Halfspaces with Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Forster transform is an operation that turns a distribution into one with\ngood anti-concentration properties. While a Forster transform does not always\nexist, we show that any distribution can be efficiently decomposed as a\ndisjoint mixture of few distributions for which a Forster transform exists and\ncan be computed efficiently. As the main application of this result, we obtain\nthe first polynomial-time algorithm for distribution-independent PAC learning\nof halfspaces in the Massart noise model with strongly polynomial sample\ncomplexity, i.e., independent of the bit complexity of the examples. Previous\nalgorithms for this learning problem incurred sample complexity scaling\npolynomially with the bit complexity, even though such a dependence is not\ninformation-theoretically necessary.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 17:00:59 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Tzamos", "Christos", ""]]}, {"id": "2107.05672", "submitter": "Rajesh Jayaram", "authors": "Rajesh Jayaram, Alireza Samadian, David P. Woodruff, Peng Ye", "title": "In-Database Regression in Input Sparsity Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketching is a powerful dimensionality reduction technique for accelerating\nalgorithms for data analysis. A crucial step in sketching methods is to compute\na subspace embedding (SE) for a large matrix $\\mathbf{A} \\in \\mathbb{R}^{N\n\\times d}$. SE's are the primary tool for obtaining extremely efficient\nsolutions for many linear-algebraic tasks, such as least squares regression and\nlow rank approximation. Computing an SE often requires an explicit\nrepresentation of $\\mathbf{A}$ and running time proportional to the size of\n$\\mathbf{A}$. However, if $\\mathbf{A}= \\mathbf{T}_1 \\Join \\mathbf{T}_2 \\Join\n\\dots \\Join \\mathbf{T}_m$ is the result of a database join query on several\nsmaller tables $\\mathbf{T}_i \\in \\mathbb{R}^{n_i \\times d_i}$, then this\nrunning time can be prohibitive, as $\\mathbf{A}$ itself can have as many as\n$O(n_1 n_2 \\cdots n_m)$ rows.\n  In this work, we design subspace embeddings for database joins which can be\ncomputed significantly faster than computing the join. For the case of a two\ntable join $\\mathbf{A} = \\mathbf{T}_1 \\Join \\mathbf{T}_2$ we give\ninput-sparsity algorithms for computing subspace embeddings, with running time\nbounded by the number of non-zero entries in $\\mathbf{T}_1,\\mathbf{T}_2$. This\nresults in input-sparsity time algorithms for high accuracy regression,\nsignificantly improving upon the running time of prior FAQ-based methods for\nregression. We extend our results to arbitrary joins for the ridge regression\nproblem, also considerably improving the running time of prior methods.\nEmpirically, we apply our method to real datasets and show that it is\nsignificantly faster than existing algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 18:23:28 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Jayaram", "Rajesh", ""], ["Samadian", "Alireza", ""], ["Woodruff", "David P.", ""], ["Ye", "Peng", ""]]}, {"id": "2107.05690", "submitter": "Zihan Tan", "authors": "Zihan Tan, Yifeng Teng, Mingfei Zhao", "title": "Worst-Case Welfare of Item Pricing in the Tollbooth Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DM cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the worst-case welfare of item pricing in the tollbooth problem. The\nproblem was first introduced by Guruswami et al, and is a special case of the\ncombinatorial auction in which (i) each of the $m$ items in the auction is an\nedge of some underlying graph; and (ii) each of the $n$ buyers is single-minded\nand only interested in buying all edges of a single path. We consider the\ncompetitive ratio between the hindsight optimal welfare and the optimal\nworst-case welfare among all item-pricing mechanisms, when the order of the\narriving buyers is adversarial. On the one hand, we prove an $\\Omega(m^{1/8})$\nlower bound of the competitive ratio for general graphs. We show that an\n$m^{\\Omega(1)}$ competitive ratio is unavoidable even if the graph is a grid,\nor if the capacity of every edge is augmented by a constant $c$. On the other\nhand, we study the competitive ratio for special families of graphs. In\nparticular, we improve the ratio when the input graph $G$ is a tree, from 8\n(proved by Cheung and Swamy) to 3. We prove that the ratio is $2$ (tight) when\n$G$ is a cycle and $O(\\log^2 m)$ when $G$ is an outerplanar graph.\n  All positive results above require that the seller can choose a proper\ntie-breaking rule to maximize the welfare. In the paper we also consider the\nsetting where the tie-breaking power is on the buyers' side, i.e. the buyer can\nchoose whether or not to buy her demand path when the total price of edges in\nthe path equals her value. We show that the gap between the two settings is at\nleast a constant even when the underlying graph is a single path (this special\ncase is also known as the highway problem). Meanwhile, in this setting where\nbuyers have the tie-breaking power, we also prove an $O(1)$ upper bound of\ncompetitive ratio for special families of graphs.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 19:02:19 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Tan", "Zihan", ""], ["Teng", "Yifeng", ""], ["Zhao", "Mingfei", ""]]}, {"id": "2107.05717", "submitter": "Manuel C\\'aceres", "authors": "Manuel C\\'aceres, Massimo Cairo, Brendan Mumey, Romeo Rizzi and\n  Alexandru I. Tomescu", "title": "Sparsifying, Shrinking and Splicing for Minimum Path Cover in\n  Parameterized Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A minimum path cover (MPC) of a directed acyclic graph (DAG) $G = (V,E)$ is a\nminimum-size set of paths that together cover all the vertices of the DAG.\nComputing an MPC is a basic polynomial problem, dating back to Dilworth's and\nFulkerson's results in the 1950s. Since the size $k$ of an MPC (also known as\nthe width) can be small in practical applications, research has also studied\nalgorithms whose complexity is parameterized on $k$. We obtain two new MPC\nparameterized algorithms for DAGs running in time $O(k^2|V|\\log{|V|} + |E|)$\nand $O(k^3|V| + |E|)$. We also obtain a parallel algorithm running in $O(k^2|V|\n+ |E|)$ parallel steps and using $O(\\log{|V|})$ processors (in the PRAM model).\nOur latter two algorithms are the first solving the problem in parameterized\nlinear time. Finally, we present an algorithm running in time $O(k^2|V|)$ for\ntransforming any MPC to another MPC using less than $2|V|$ distinct edges,\nwhich we prove to be asymptotically tight. As such, we also obtain edge\nsparsification algorithms preserving the width of the DAG with the same running\ntime as our MPC algorithms. At the core of all our algorithms we interleave the\nusage of three techniques: transitive sparsification, shrinking of a path\ncover, and the splicing of a set of paths along a given path.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 20:13:30 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["C\u00e1ceres", "Manuel", ""], ["Cairo", "Massimo", ""], ["Mumey", "Brendan", ""], ["Rizzi", "Romeo", ""], ["Tomescu", "Alexandru I.", ""]]}, {"id": "2107.05746", "submitter": "Binghui Peng", "authors": "Thomas Chen, Xi Chen, Binghui Peng, Mihalis Yannakakis", "title": "Computational Hardness of the Hylland-Zeckhauser Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the complexity of the classic Hylland-Zeckhauser scheme [HZ'79] for\none-sided matching markets. We show that the problem of finding an\n$\\epsilon$-approximate equilibrium in the HZ scheme is PPAD-hard, and this\nholds even when $\\epsilon$ is polynomially small and when each agent has no\nmore than four distinct utility values. Our hardness result, when combined with\nthe PPAD membership result of [VY'21], resolves the approximation complexity of\nthe HZ scheme. We also show that the problem of approximating the optimal\nsocial welfare (the weight of the matching) achievable by HZ equilibria within\na certain constant factor is NP-hard.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 21:31:28 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Chen", "Thomas", ""], ["Chen", "Xi", ""], ["Peng", "Binghui", ""], ["Yannakakis", "Mihalis", ""]]}, {"id": "2107.05753", "submitter": "Aleksander {\\L}ukasiewicz", "authors": "Dariusz Dereniowski, Aleksander {\\L}ukasiewicz, Przemys{\\l}aw\n  Uzna\\'nski", "title": "Noisy searching: simple, fast and correct", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work revisits the multiplicative weights update technique (MWU) which\nhas a variety of applications, especially in learning and searching algorithms.\nIn particular, the Bayesian update method is a well known version of MWU that\nis particularly applicable for the problem of searching in a given domain. An\nideal scenario for that method is when the input distribution is known a priori\nand each single update maximizes the information gain. In this work we consider\ntwo search domains - linear orders (sorted arrays) and graphs, where the aim of\nthe search is to locate an unknown target by performing as few queries as\npossible. Searching such domains is well understood when each query provides a\ncorrect answer and the input target distribution is uniform. Hence, we consider\ntwo generalizations: the noisy search both with arbitrary and adversarial\n(i.e., unknown) target distributions. We obtain several results providing full\ncharacterization of the query complexities in the three settings: adversarial\nMonte Carlo, adversarial Las Vegas and distributional Las Vegas. Our algorithms\neither improve, simplify or patch earlier ambiguities in the literature - see\nthe works of Emamjomeh-Zadeh et al. [STOC 2016], Dereniowski et. al. [SOSA@SODA\n2019] and Ben-Or and Hassidim [FOCS 2008]. In particular, all algorithms give\nstrategies that provide the optimal number of queries up to lower-order terms.\nOur technical contribution lies in providing generic search techniques that are\nable to deal with the fact that, in general, queries guarantee only suboptimal\ninformation gain.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 21:50:50 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Dereniowski", "Dariusz", ""], ["\u0141ukasiewicz", "Aleksander", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "2107.05793", "submitter": "S M Ferdous", "authors": "S M Ferdous, Alex Pothen, Arif Khan, Ajay Panyala, Mahantesh\n  Halappanavar", "title": "A Parallel Approximation Algorithm for Maximizing Submodular\n  $b$-Matching", "comments": "10 pages, accepted for SIAM ACDA 21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We design new serial and parallel approximation algorithms for computing a\nmaximum weight $b$-matching in an edge-weighted graph with a submodular\nobjective function. This problem is NP-hard; the new algorithms have\napproximation ratio $1/3$, and are relaxations of the Greedy algorithm that\nrely only on local information in the graph, making them parallelizable. We\nhave designed and implemented Local Lazy Greedy algorithms for both serial and\nparallel computers. We have applied the approximate submodular $b$-matching\nalgorithm to assign tasks to processors in the computation of Fock matrices in\nquantum chemistry on parallel computers. The assignment seeks to reduce the run\ntime by balancing the computational load on the processors and bounding the\nnumber of messages that each processor sends. We show that the new assignment\nof tasks to processors provides a four fold speedup over the currently used\nassignment in the NWChemEx software on $8000$ processors on the Summit\nsupercomputer at Oak Ridge National Lab.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 00:35:20 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ferdous", "S M", ""], ["Pothen", "Alex", ""], ["Khan", "Arif", ""], ["Panyala", "Ajay", ""], ["Halappanavar", "Mahantesh", ""]]}, {"id": "2107.05822", "submitter": "Daogao Liu", "authors": "Jian Li, Daogao Liu", "title": "Markov Game with Switching Costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a general Markov game with metric switching costs: in each round,\nthe player adaptively chooses one of several Markov chains to advance with the\nobjective of minimizing the expected cost for at least $k$ chains to reach\ntheir target states. If the player decides to play a different chain, an\nadditional switching cost is incurred. The special case in which there is no\nswitching cost was solved optimally by Dumitriu, Tetali, and Winkler [DTW03] by\na variant of the celebrated Gittins Index for the classical multi-armed bandit\n(MAB) problem with Markovian rewards [Gittins 74, Gittins79]. However, for\nmulti-armed bandit (MAB) with nontrivial switching cost, even if the switching\ncost is a constant, the classic paper by Banks and Sundaram [BS94] showed that\nno index strategy can be optimal.\n  In this paper, we complement their result and show there is a simple index\nstrategy that achieves a constant approximation factor if the switching cost is\nconstant and $k=1$. To the best of our knowledge, this is the first index\nstrategy that achieves a constant approximation factor for a general MAB\nvariant with switching costs. For the general metric, we propose a more\ninvolved constant-factor approximation algorithm, via a nontrivial reduction to\nthe stochastic $k$-TSP problem, in which a Markov chain is approximated by a\nrandom variable. Our analysis makes extensive use of various interesting\nproperties of the Gittins index.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 03:00:21 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Li", "Jian", ""], ["Liu", "Daogao", ""]]}, {"id": "2107.05972", "submitter": "Arnaud Mary", "authors": "Caroline Brosse, Vincent Limouzy and Arnaud Mary", "title": "Polynomial delay algorithm for minimal chordal completions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by the problem of enumerating all tree decompositions of a graph,\nwe consider in this article the problem of listing all the minimal chordal\ncompletions of a graph. In \\cite{carmeli2020} (\\textsc{Pods 2017}) Carmeli\n\\emph{et al.} proved that all minimal chordal completions or equivalently all\nproper tree decompositions of a graph can be listed in incremental polynomial\ntime using exponential space. The total running time of their algorithm is\nquadratic in the number of solutions and the existence of an algorithm whose\ncomplexity depends only linearly on the number of solutions remained open. We\nclose this question by providing a polynomial delay algorithm to solve this\nproblem which, moreover, uses polynomial space.\n  Our algorithm relies on \\emph{Proximity Search}, a framework recently\nintroduced by Conte \\emph{et al.} \\cite{conte-uno2019} (\\textsc{Stoc 2019})\nwhich has been shown powerful to obtain polynomial delay algorithms, but\ngenerally requires exponential space. In order to obtain a polynomial space\nalgorithm for our problem, we introduce a new general method called\n\\emph{canonical path reconstruction} to design polynomial delay and polynomial\nspace algorithms based on proximity search.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 10:39:24 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Brosse", "Caroline", ""], ["Limouzy", "Vincent", ""], ["Mary", "Arnaud", ""]]}, {"id": "2107.06111", "submitter": "Falko Hegerfeld", "authors": "Falko Hegerfeld, Stefan Kratsch", "title": "Towards exact structural thresholds for parameterized complexity", "comments": "49 pages, 12 figures, shortened abstract due to character limit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameterized complexity seeks to use input structure to obtain faster\nalgorithms for NP-hard problems. This has been most successful for graphs of\nlow treewidth: Many problems admit fast algorithms relative to treewidth and\nmany of them are optimal under SETH. Fewer such results are known for more\ngeneral structure such as low clique-width and more restrictive structure such\nas low deletion distance to a sparse graph class.\n  Despite these successes, such results remain \"islands'' within the realm of\npossible structure. Rather than adding more islands, we seek to determine the\ntransitions between them, that is, we aim for structural thresholds where the\ncomplexity increases as input structure becomes more general. Going from\ndeletion distance to treewidth, is a single deletion set to a graph with simple\ncomponents enough to yield the same lower bound as for treewidth or does it\ntake many disjoint separators? Going from treewidth to clique-width, how much\nmore density entails the same complexity as clique-width? Conversely, what is\nthe most restrictive structure that yields the same lower bound?\n  For treewidth, we obtain both refined and new lower bounds that apply already\nto graphs with a single separator $X$ such that $G-X$ has treewidth $r=O(1)$,\nwhile $G$ has treewidth $|X|+O(1)$. We rule out algorithms running in time\n$O^*((r+1-\\epsilon)^{k})$ for Deletion to $r$-Colorable parameterized by\n$k=|X|$. For clique-width, we rule out time $O^*((2^r-\\epsilon)^k)$ for\nDeletion to $r$-Colorable, where $X$ is now allowed to consist of $k$\ntwinclasses. There are further results on Vertex Cover, Dominating Set and\nMaximum Cut. All lower bounds are matched by existing and newly designed\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 14:06:41 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Hegerfeld", "Falko", ""], ["Kratsch", "Stefan", ""]]}, {"id": "2107.06216", "submitter": "Sahil Singla", "authors": "Anupam Gupta, Amit Kumar, and Sahil Singla", "title": "Bag-of-Tasks Scheduling on Related Machines", "comments": "Preliminary version in APPROX 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online scheduling to minimize weighted completion time on related\nmachines, where each job consists of several tasks that can be concurrently\nexecuted. A job gets completed when all its component tasks finish. We obtain\nan $O(K^3 \\log^2 K)$-competitive algorithm in the non-clairvoyant setting,\nwhere $K$ denotes the number of distinct machine speeds. The analysis is based\non dual-fitting on a precedence-constrained LP relaxation that may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 16:16:50 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Gupta", "Anupam", ""], ["Kumar", "Amit", ""], ["Singla", "Sahil", ""]]}, {"id": "2107.06232", "submitter": "Marek Soko{\\l}owski", "authors": "Konrad Majewski, Micha{\\l} Pilipczuk, Marek Soko{\\l}owski", "title": "Maintaining $\\mathsf{CMSO}_2$ properties on dynamic structures with\n  bounded feedback vertex number", "comments": "80 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\varphi$ be a sentence of $\\mathsf{CMSO}_2$ (monadic second-order logic\nwith quantification over edge subsets and counting modular predicates) over the\nsignature of graphs. We present a dynamic data structure that for a given graph\n$G$ that is updated by edge insertions and edge deletions, maintains whether\n$\\varphi$ is satisfied in $G$. The data structure is required to correctly\nreport the outcome only when the feedback vertex number of $G$ does not exceed\na fixed constant $k$, otherwise it reports that the feedback vertex number is\ntoo large. With this assumption, we guarantee amortized update time ${\\cal\nO}_{\\varphi,k}(\\log n)$.\n  By combining this result with a classic theorem of Erd\\H{o}s and P\\'osa, we\ngive a fully dynamic data structure that maintains whether a graph contains a\npacking of $k$ vertex-disjoint cycles with amortized update time ${\\cal\nO}_{k}(\\log n)$. Our data structure also works in a larger generality of\nrelational structures over binary signatures.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 16:35:10 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Majewski", "Konrad", ""], ["Pilipczuk", "Micha\u0142", ""], ["Soko\u0142owski", "Marek", ""]]}, {"id": "2107.06259", "submitter": "Wenshuo Guo", "authors": "Wenshuo Guo, Michael I. Jordan, Manolis Zampetakis", "title": "Robust Learning of Optimal Auctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning revenue-optimal multi-bidder auctions from\nsamples when the samples of bidders' valuations can be adversarially corrupted\nor drawn from distributions that are adversarially perturbed. First, we prove\ntight upper bounds on the revenue we can obtain with a corrupted distribution\nunder a population model, for both regular valuation distributions and\ndistributions with monotone hazard rate (MHR). We then propose new algorithms\nthat, given only an ``approximate distribution'' for the bidder's valuation,\ncan learn a mechanism whose revenue is nearly optimal simultaneously for all\n``true distributions'' that are $\\alpha$-close to the original distribution in\nKolmogorov-Smirnov distance. The proposed algorithms operate beyond the setting\nof bounded distributions that have been studied in prior works, and are\nguaranteed to obtain a fraction $1-O(\\alpha)$ of the optimal revenue under the\ntrue distribution when the distributions are MHR. Moreover, they are guaranteed\nto yield at least a fraction $1-O(\\sqrt{\\alpha})$ of the optimal revenue when\nthe distributions are regular. We prove that these upper bounds cannot be\nfurther improved, by providing matching lower bounds. Lastly, we derive sample\ncomplexity upper bounds for learning a near-optimal auction for both MHR and\nregular distributions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 17:37:21 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Guo", "Wenshuo", ""], ["Jordan", "Michael I.", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "2107.06399", "submitter": "Van Bang Le", "authors": "Van Bang Le and Jan Arne Telle", "title": "The Perfect Matching Cut Problem Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a graph, a perfect matching cut is an edge cut that is a perfect matching.\nPerfect Matching Cut (PMC) is the problem of deciding whether a given graph has\na perfect matching cut, and is known to be NP-complete. We revisit the problem\nand show that PMC remains NP-complete when restricted to bipartite graphs of\nmaximum degree 3 and arbitrarily large girth. Complementing this hardness\nresult, we give two graph classes in which PMC is polynomial time solvable. The\nfirst one includes claw-free graphs and graphs without an induced path on five\nvertices, the second one properly contains all chordal graphs. Assuming the\nExponential Time Hypothesis, we show there is no $O^*(2^{o(n)})$-time algorithm\nfor PMC even when restricted to $n$-vertex bipartite graphs, and also show that\nPMC can be solved in $O^*(1.2721^n)$ time by means of an exact branching\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 21:16:24 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Le", "Van Bang", ""], ["Telle", "Jan Arne", ""]]}, {"id": "2107.06406", "submitter": "Mohsen Heidari", "authors": "Mohsen Heidari, Arun Padakandla and Wojciech Szpankowski", "title": "A Theoretical Framework for Learning from Quantum Data", "comments": null, "journal-ref": "IEEE International Symposium on Information Theory (ISIT), 2021", "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over decades traditional information theory of source and channel coding\nadvances toward learning and effective extraction of information from data. We\npropose to go one step further and offer a theoretical foundation for learning\nclassical patterns from quantum data. However, there are several roadblocks to\nlay the groundwork for such a generalization. First, classical data must be\nreplaced by a density operator over a Hilbert space. Hence, deviated from\nproblems such as state tomography, our samples are i.i.d density operators. The\nsecond challenge is even more profound since we must realize that our only\ninteraction with a quantum state is through a measurement which -- due to\nno-cloning quantum postulate -- loses information after measuring it. With this\nin mind, we present a quantum counterpart of the well-known PAC framework.\nBased on that, we propose a quantum analogous of the ERM algorithm for learning\nmeasurement hypothesis classes. Then, we establish upper bounds on the quantum\nsample complexity quantum concept classes.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 21:39:47 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Heidari", "Mohsen", ""], ["Padakandla", "Arun", ""], ["Szpankowski", "Wojciech", ""]]}, {"id": "2107.06571", "submitter": "Martina Gallato", "authors": "Friedrich Eisenbrand, Martina Gallato, Ola Svensson, Moritz Venzin", "title": "A QPTAS for stabbing rectangles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the following geometric optimization problem: Given $ n $\naxis-aligned rectangles in the plane, the goal is to find a set of horizontal\nsegments of minimum total length such that each rectangle is stabbed. A segment\nstabs a rectangle if it intersects both its left and right edge. As such, this\nstabbing problem falls into the category of weighted geometric set cover\nproblems for which techniques that improve upon the general ${\\Theta}(\\log\nn)$-approximation guarantee have received a lot of attention in the literature.\nChan at al. (2018) have shown that rectangle stabbing is NP-hard and that it\nadmits a constant-factor approximation algorithm based on Varadarajan's\nquasi-uniform sampling method. In this work we make progress on rectangle\nstabbing on two fronts. First, we present a quasi-polynomial time approximation\nscheme (QPTAS) for rectangle stabbing. Furthermore, we provide a simple\n$8$-approximation algorithm that avoids the framework of Varadarajan. This\nsettles two open problems raised by Chan et al. (2018).\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 09:22:38 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Eisenbrand", "Friedrich", ""], ["Gallato", "Martina", ""], ["Svensson", "Ola", ""], ["Venzin", "Moritz", ""]]}, {"id": "2107.06582", "submitter": "Talya Eden", "authors": "Amartya Shankha Biswas, Talya Eden, Ronitt Rubinfeld", "title": "Towards a Decomposition-Optimal Algorithm for Counting and Sampling\n  Arbitrary Motifs in Sublinear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sampling and approximately counting an arbitrary\ngiven motif $H$ in a graph $G$, where access to $G$ is given via queries:\ndegree, neighbor, and pair, as well as uniform edge sample queries. Previous\nalgorithms for these tasks were based on a decomposition of $H$ into a\ncollection of odd cycles and stars, denoted $\\mathcal{D}^*(H)=\\{O_{k_1},\n\\ldots, O_{k_q}, S_{p_1}, \\ldots, S_{p_\\ell}\\}$. These algorithms were shown to\nbe optimal for the case where $H$ is a clique or an odd-length cycle, but no\nother lower bounds were known.\n  We present a new algorithm for sampling and approximately counting arbitrary\nmotifs which, up to $\\textrm{poly}(\\log n)$ factors, is always at least as good\nas previous results, and for most graphs $G$ is strictly better. The main\ningredient leading to this improvement is an improved uniform algorithm for\nsampling stars, which might be of independent interest, as it allows to sample\nvertices according to the $p$-th moment of the degree distribution.\n  Finally, we prove that this algorithm is \\emph{decomposition-optimal} for\ndecompositions that contain at least one odd cycle. These are the first lower\nbounds for motifs $H$ with a nontrivial decomposition, i.e., motifs that have\nmore than a single component in their decomposition.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 10:05:26 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 15:17:30 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Biswas", "Amartya Shankha", ""], ["Eden", "Talya", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "2107.06615", "submitter": "Simon Omlor", "authors": "Alexander Munteanu, Simon Omlor, David Woodruff", "title": "Oblivious sketching for logistic regression", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What guarantees are possible for solving logistic regression in one pass over\na data stream? To answer this question, we present the first data oblivious\nsketch for logistic regression. Our sketch can be computed in input sparsity\ntime over a turnstile data stream and reduces the size of a $d$-dimensional\ndata set from $n$ to only $\\operatorname{poly}(\\mu d\\log n)$ weighted points,\nwhere $\\mu$ is a useful parameter which captures the complexity of compressing\nthe data. Solving (weighted) logistic regression on the sketch gives an $O(\\log\nn)$-approximation to the original problem on the full data set. We also show\nhow to obtain an $O(1)$-approximation with slight modifications. Our sketches\nare fast, simple, easy to implement, and our experiments demonstrate their\npracticality.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 11:29:26 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Munteanu", "Alexander", ""], ["Omlor", "Simon", ""], ["Woodruff", "David", ""]]}, {"id": "2107.06626", "submitter": "Ora Nova Fandina", "authors": "Yair Bartal and Ora Nova Fandina and Kasper Green Larsen", "title": "Optimality of the Johnson-Lindenstrauss Dimensionality Reduction for\n  Practical Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is well known that the Johnson-Lindenstrauss dimensionality reduction\nmethod is optimal for worst case distortion. While in practice many other\nmethods and heuristics are used, not much is known in terms of bounds on their\nperformance. The question of whether the JL method is optimal for practical\nmeasures of distortion was recently raised in \\cite{BFN19} (NeurIPS'19). They\nprovided upper bounds on its quality for a wide range of practical measures and\nshowed that indeed these are best possible in many cases. Yet, some of the most\nimportant cases, including the fundamental case of average distortion were left\nopen. In particular, they show that the JL transform has $1+\\epsilon$ average\ndistortion for embedding into $k$-dimensional Euclidean space, where\n$k=O(1/\\eps^2)$, and for more general $q$-norms of distortion, $k =\nO(\\max\\{1/\\eps^2,q/\\eps\\})$, whereas tight lower bounds were established only\nfor large values of $q$ via reduction to the worst case.\n  In this paper we prove that these bounds are best possible for any\ndimensionality reduction method, for any $1 \\leq q \\leq O(\\frac{\\log (2\\eps^2\nn)}{\\eps})$ and $\\epsilon \\geq \\frac{1}{\\sqrt{n}}$, where $n$ is the size of\nthe subset of Euclidean space.\n  Our results imply that the JL method is optimal for various distortion\nmeasures commonly used in practice, such as {\\it stress, energy} and {\\it\nrelative error}. We prove that if any of these measures is bounded by $\\eps$\nthen $k=\\Omega(1/\\eps^2)$, for any $\\epsilon \\geq \\frac{1}{\\sqrt{n}}$, matching\nthe upper bounds of \\cite{BFN19} and extending their tightness results for the\nfull range moment analysis.\n  Our results may indicate that the JL dimensionality reduction method should\nbe considered more often in practical applications, and the bounds we provide\nfor its quality should be served as a measure for comparison when evaluating\nthe performance of other methods and heuristics.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 12:00:46 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Bartal", "Yair", ""], ["Fandina", "Ora Nova", ""], ["Larsen", "Kasper Green", ""]]}, {"id": "2107.06649", "submitter": "Shant Boodaghians", "authors": "Shant Boodaghians, Bhaskar Ray Chaudhury, Ruta Mehta", "title": "Polynomial Time Algorithms to Find an Approximate Competitive\n  Equilibrium for Chores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Competitive equilibrium with equal income (CEEI) is considered one of the\nbest mechanisms to allocate a set of items among agents fairly and efficiently.\nIn this paper, we study the computation of CEEI when items are chores that are\ndisliked (negatively valued) by agents, under 1-homogeneous and concave utility\nfunctions which includes linear functions as a subcase. It is well-known that,\neven with linear utilities, the set of CEEI may be non-convex and disconnected,\nand the problem is PPAD-hard in the more general exchange model. In contrast to\nthese negative results, we design FPTAS: A polynomial-time algorithm to compute\n$\\epsilon$-approximate CEEI where the running-time depends polynomially on\n$1/\\epsilon$.\n  Our algorithm relies on the recent characterization due to Bogomolnaia et\nal.~(2017) of the CEEI set as exactly the KKT points of a non-convex\nminimization problem that have all coordinates non-zero. Due to this non-zero\nconstraint, naive gradient-based methods fail to find the desired local minima\nas they are attracted towards zero. We develop an exterior-point method that\nalternates between guessing non-zero KKT points and maximizing the objective\nalong supporting hyperplanes at these points. We show that this procedure must\nconverge quickly to an approximate KKT point which then can be mapped to an\napproximate CEEI; this exterior point method may be of independent interest.\nWhen utility functions are linear, we give explicit procedures for finding the\nexact iterates, and as a result show that a stronger form of approximate CEEI\ncan be found in polynomial time. Finally, we note that our algorithm extends to\nthe setting of un-equal incomes (CE), and to mixed manna with linear utilities\nwhere each agent may like (positively value) some items and dislike (negatively\nvalue) others.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 22:15:31 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 14:06:35 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Boodaghians", "Shant", ""], ["Chaudhury", "Bhaskar Ray", ""], ["Mehta", "Ruta", ""]]}, {"id": "2107.06715", "submitter": "Tanmay Inamdar", "authors": "Fedor V. Fomin, Petr A. Golovach, Tanmay Inamdar, Saket Saurabh", "title": "ETH Tight Algorithms for Geometric Intersection Graphs: Now in\n  Polynomial Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  De Berg et al. in [SICOMP 2020] gave an algorithmic framework for\nsubexponential algorithms on geometric graphs with tight (up to ETH) running\ntimes. This framework is based on dynamic programming on graphs of weighted\ntreewidth resulting in algorithms that use super-polynomial space. We introduce\nthe notion of weighted treedepth and use it to refine the framework of de Berg\net al. for obtaining polynomial space (with tight running times) on geometric\ngraphs. As a result, we prove that for any fixed dimension $d \\ge 2$ on\nintersection graphs of similarly-sized fat objects many well-known graph\nproblems including Independent Set, $r$-Dominating Set for constant $r$, Cycle\nCover, Hamiltonian Cycle, Hamiltonian Path, Steiner Tree, Connected Vertex\nCover, Feedback Vertex Set, and (Connected) Odd Cycle Transversal are solvable\nin time $2^{O(n^{1-1/d})}$ and within polynomial space.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 14:01:25 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Inamdar", "Tanmay", ""], ["Saurabh", "Saket", ""]]}, {"id": "2107.06817", "submitter": "Michael Leybovich", "authors": "Michael Leybovich and Oded Shmueli", "title": "Efficient Set of Vectors Search", "comments": "6 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider a similarity measure between two sets $A$ and $B$ of vectors,\nthat balances the average and maximum cosine distance between pairs of vectors,\none from set $A$ and one from set $B$. As a motivation for this measure, we\npresent lineage tracking in a database. To practically realize this measure, we\nneed an approximate search algorithm that given a set of vectors $A$ and sets\nof vectors $B_1,...,B_n$, the algorithm quickly locates the set $B_i$ that\nmaximizes the similarity measure. For the case where all sets are singleton\nsets, essentially each is a single vector, there are known efficient\napproximate search algorithms, e.g., approximated versions of tree search\nalgorithms, locality-sensitive hashing (LSH), vector quantization (VQ) and\nproximity graph algorithms. In this work, we present approximate search\nalgorithms for the general case. The underlying idea in these algorithms is\nencoding a set of vectors via a \"long\" single vector.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 16:22:20 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Leybovich", "Michael", ""], ["Shmueli", "Oded", ""]]}, {"id": "2107.06876", "submitter": "Johannes Klicpera", "authors": "Johannes Klicpera, Marten Lienen, Stephan G\\\"unnemann", "title": "Scalable Optimal Transport in High Dimensions for Graph Distances,\n  Embedding Alignment, and More", "comments": "Published as a conference paper at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DS cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The current best practice for computing optimal transport (OT) is via entropy\nregularization and Sinkhorn iterations. This algorithm runs in quadratic time\nas it requires the full pairwise cost matrix, which is prohibitively expensive\nfor large sets of objects. In this work we propose two effective log-linear\ntime approximations of the cost matrix: First, a sparse approximation based on\nlocality-sensitive hashing (LSH) and, second, a Nystr\\\"om approximation with\nLSH-based sparse corrections, which we call locally corrected Nystr\\\"om (LCN).\nThese approximations enable general log-linear time algorithms for\nentropy-regularized OT that perform well even for the complex, high-dimensional\nspaces common in deep learning. We analyse these approximations theoretically\nand evaluate them experimentally both directly and end-to-end as a component\nfor real-world applications. Using our approximations for unsupervised word\nembedding alignment enables us to speed up a state-of-the-art method by a\nfactor of 3 while also improving the accuracy by 3.1 percentage points without\nany additional model changes. For graph distance regression we propose the\ngraph transport network (GTN), which combines graph neural networks (GNNs) with\nenhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales\nlog-linearly in the number of nodes.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 17:40:08 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Klicpera", "Johannes", ""], ["Lienen", "Marten", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "2107.06889", "submitter": "Pawe{\\l} Rz\\k{a}\\.zewski", "authors": "Jacob Focke and D\\'aniel Marx and Pawe{\\l} Rz\\k{a}\\.zewski", "title": "Counting list homomorphisms from graphs of bounded treewidth: tight\n  complexity bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to give precise bounds on the counting complexity of\na family of generalized coloring problems (list homomorphisms) on\nbounded-treewidth graphs. Given graphs $G$, $H$, and lists $L(v)\\subseteq V(H)$\nfor every $v\\in V(G)$, a {\\em list homomorphism} is a function $f:V(G)\\to V(H)$\nthat preserves the edges (i.e., $uv\\in E(G)$ implies $f(u)f(v)\\in E(H)$) and\nrespects the lists (i.e., $f(v)\\in L(v))$. Standard techniques show that if $G$\nis given with a tree decomposition of width $t$, then the number of list\nhomomorphisms can be counted in time $|V(H)|^t\\cdot n^{\\mathcal{O}(1)}$. Our\nmain result is determining, for every fixed graph $H$, how much the base\n$|V(H)|$ in the running time can be improved. For a connected graph $H$ we\ndefine $\\operatorname{irr}(H)$ the following way: if $H$ has a loop or is\nnonbipartite, then $\\operatorname{irr}(H)$ is the maximum size of a set\n$S\\subseteq V(H)$ where any two vertices have different neighborhoods; if $H$\nis bipartite, then $\\operatorname{irr}(H)$ is the maximum size of such a set\nthat is fully in one of the bipartition classes. For disconnected $H$, we\ndefine $\\operatorname{irr}(H)$ as the maximum of $\\operatorname{irr}(C)$ over\nevery connected component $C$ of $H$. We show that, for every fixed graph $H$,\nthe number of list homomorphisms from $(G,L)$ to $H$\n  * can be counted in time $\\operatorname{irr}(H)^t\\cdot n^{\\mathcal{O}(1)}$ if\na tree decomposition of $G$ having width at most $t$ is given in the input, and\n  * cannot be counted in time $(\\operatorname{irr}(H)-\\epsilon)^t\\cdot\nn^{\\mathcal{O}(1)}$ for any $\\epsilon>0$, even if a tree decomposition of $G$\nhaving width at most $t$ is given in the input, unless the #SETH fails.\n  Thereby we give a precise and complete complexity classification featuring\nmatching upper and lower bounds for all target graphs with or without loops.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 12:19:57 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Focke", "Jacob", ""], ["Marx", "D\u00e1niel", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""]]}, {"id": "2107.06951", "submitter": "Manuel Lladser", "authors": "Perrin E. Ruth, Manuel E. Lladser", "title": "Levenshtein Graphs: Resolvability, Automorphisms & Determining Sets", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the notion of Levenshtein graphs, an analog to Hamming graphs\nbut using the edit distance instead of the Hamming distance; in particular,\nLevenshtein graphs allow for underlying strings (nodes) of different lengths.\nWe characterize various properties of these graphs, including a necessary and\nsufficient condition for their geodesic distance to be identical to the edit\ndistance, their automorphism group and determining number, and an upper bound\non their metric dimension. Regarding the latter, we construct a resolving set\ncomposed of two-run strings and an algorithm that computes the edit distance\nbetween a string of length $k$ and any single-run or two-run string in $O(k)$\noperations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 19:30:15 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Ruth", "Perrin E.", ""], ["Lladser", "Manuel E.", ""]]}, {"id": "2107.06980", "submitter": "Yifeng Teng", "authors": "Melika Abolhassani, Hossein Esfandiari, Yasamin Nazari,\n  Balasubramanian Sivan, Yifeng Teng, Creighton Thomas", "title": "Online Allocation and Display Ads Optimization with Surplus Supply", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we study a scenario where a publisher seeks to maximize its\ntotal revenue across two sales channels: guaranteed contracts that promise to\ndeliver a certain number of impressions to the advertisers, and spot demands\nthrough an Ad Exchange. On the one hand, if a guaranteed contract is not fully\ndelivered, it incurs a penalty for the publisher. On the other hand, the\npublisher might be able to sell an impression at a high price in the Ad\nExchange. How does a publisher maximize its total revenue as a sum of the\nrevenue from the Ad Exchange and the loss from the under-delivery penalty? We\nstudy this problem parameterized by \\emph{supply factor $f$}: a notion we\nintroduce that, intuitively, captures the number of times a publisher can\nsatisfy all its guaranteed contracts given its inventory supply. In this work\nwe present a fast simple deterministic algorithm with the optimal competitive\nratio. The algorithm and the optimal competitive ratio are a function of the\nsupply factor, penalty, and the distribution of the bids in the Ad Exchange.\n  Beyond the yield optimization problem, classic online allocation problems\nsuch as online bipartite matching of [Karp-Vazirani-Vazirani '90] and its\nvertex-weighted variant of [Aggarwal et al. '11] can be studied in the presence\nof the additional supply guaranteed by the supply factor. We show that a supply\nfactor of $f$ improves the approximation factors from $1-1/e$ to $f-fe^{-1/f}$.\nOur approximation factor is tight and approaches $1$ as $f \\to \\infty$.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 20:32:14 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Abolhassani", "Melika", ""], ["Esfandiari", "Hossein", ""], ["Nazari", "Yasamin", ""], ["Sivan", "Balasubramanian", ""], ["Teng", "Yifeng", ""], ["Thomas", "Creighton", ""]]}, {"id": "2107.07103", "submitter": "Huanjian Zhou", "authors": "Baoxiang Wang, Huanjian Zhou", "title": "Multilinear extension of $k$-submodular functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A $k$-submodular function is a function that given $k$ disjoint subsets\noutputs a value that is submodular in every orthant. In this paper, we provide\na new framework for $k$-submodular maximization problems, by relaxing the\noptimization to the continuous space with the multilinear extension of\n$k$-submodular functions and a variant of pipage rounding that recovers the\ndiscrete solution. The multilinear extension introduces new techniques to\nanalyze and optimize $k$-submodular functions.\n  When the function is monotone, we propose almost $\\frac{1}{2}$-approximation\nalgorithms for unconstrained maximization and maximization under total size and\nknapsack constraints. For unconstrained monotone and non-monotone maximization,\nwe propose an algorithm that is almost as good as any combinatorial algorithm\nbased on Iwata, Tanigawa, and Yoshida's meta-framework\n($\\frac{k}{2k-1}$-approximation for the monotone case and\n$\\frac{k^2+1}{2k^2+1}$-approximation for the non-monotone case).\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 03:48:05 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Wang", "Baoxiang", ""], ["Zhou", "Huanjian", ""]]}, {"id": "2107.07141", "submitter": "Anubhav Baweja", "authors": "Anubhav Baweja, Justin Jia, David P. Woodruff", "title": "An Efficient Semi-Streaming PTAS for Tournament Feedback ArcSet with Few\n  Passes", "comments": "29 pages, 4 figures, 1 table, 8 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the first semi-streaming PTAS for the minimum feedback arc set\nproblem on directed tournaments in a small number of passes. Namely, we obtain\na $(1 + \\varepsilon)$-approximation in polynomial time $O \\left( \\text{poly}(n)\n2^{\\text{poly}(1/\\varepsilon)} \\right)$, with $p$ passes in $n^{1+1/p} \\cdot\n\\text{poly}\\left(\\frac{\\log n}{\\varepsilon}\\right)$ space. The only previous\nalgorithm with this pass/space trade-off gave a $3$-approximation (SODA, 2020),\nand other polynomial-time algorithms which achieved a\n$(1+\\varepsilon)$-approximation did so with quadratic memory or with a linear\nnumber of passes. We also present a new time/space trade-off for $1$-pass\nalgorithms that solve the tournament feedback arc set problem. This problem has\nseveral applications in machine learning such as creating linear classifiers\nand doing Bayesian inference. We also provide several additional algorithms and\nlower bounds for related streaming problems on directed graphs, which is a\nmostly unexplored territory.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 05:59:17 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Baweja", "Anubhav", ""], ["Jia", "Justin", ""], ["Woodruff", "David P.", ""]]}, {"id": "2107.07183", "submitter": "Moran Feldman", "authors": "Moran Feldman, Ashkan Norouzi-Fard, Ola Svensson and Rico Zenklusen", "title": "Streaming Submodular Maximization with Matroid and Matching Constraints", "comments": "58 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in (semi-)streaming algorithms for monotone submodular\nfunction maximization has led to tight results for a simple cardinality\nconstraint. However, current techniques fail to give a similar understanding\nfor natural generalizations such as matroid and matching constraints. This\npaper aims at closing this gap. For a single matroid of rank $k$ (i.e., any\nsolution has cardinality at most $k$), our main results are:\n  $\\bullet$ A single-pass streaming algorithm that uses $\\widetilde{O}(k)$\nmemory and achieves an approximation guarantee of 0.3178.\n  $\\bullet$ A multi-pass streaming algorithm that uses $\\widetilde{O}(k)$\nmemory and achieves an approximation guarantee of $(1-1/e - \\varepsilon)$ by\ntaking constant number of passes over the stream.\n  This improves on the previously best approximation guarantees of 1/4 and 1/2\nfor single-pass and multi-pass streaming algorithms, respectively. In fact, our\nmulti-pass streaming algorithm is tight in that any algorithm with a better\nguarantee than 1/2 must make several passes through the stream and any\nalgorithm that beats our guarantee $1-1/e$ must make linearly many passes.\n  For the problem of maximizing a monotone submodular function subject to a\nbipartite matching constraint (which is a special case of matroid\nintersection), we show that it is not possible to obtain better than\n0.3715-approximation in a single pass, which improves over a recent\ninapproximability of 0.522 for this problem. Furthermore, given a plausible\nassumption, our inapproximability result improves to $1/3 \\approx 0.333$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 08:18:35 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Feldman", "Moran", ""], ["Norouzi-Fard", "Ashkan", ""], ["Svensson", "Ola", ""], ["Zenklusen", "Rico", ""]]}, {"id": "2107.07347", "submitter": "Amir Zandieh", "authors": "Karl Bringmann, Michael Kapralov, Mikhail Makarov, Vasileios Nakos,\n  Amir Yagudin, Amir Zandieh", "title": "Sparse Fourier Transform by traversing Cooley-Tukey FFT computation\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the dominant Fourier coefficients of a vector is a common task in\nmany fields, such as signal processing, learning theory, and computational\ncomplexity. In the Sparse Fast Fourier Transform (Sparse FFT) problem, one is\ngiven oracle access to a $d$-dimensional vector $x$ of size $N$, and is asked\nto compute the best $k$-term approximation of its Discrete Fourier Transform,\nquickly and using few samples of the input vector $x$. While the sample\ncomplexity of this problem is quite well understood, all previous approaches\neither suffer from an exponential dependence of runtime on the dimension $d$ or\ncan only tolerate a trivial amount of noise. This is in sharp contrast with the\nclassical FFT algorithm of Cooley and Tukey, which is stable and completely\ninsensitive to the dimension of the input vector: its runtime is $O(N\\log N)$\nin any dimension $d$.\n  In this work, we introduce a new high-dimensional Sparse FFT toolkit and use\nit to obtain new algorithms, both on the exact, as well as in the case of\nbounded $\\ell_2$ noise. This toolkit includes i) a new strategy for exploring a\npruned FFT computation tree that reduces the cost of filtering, ii) new\nstructural properties of adaptive aliasing filters recently introduced by\nKapralov, Velingker and Zandieh'SODA'19, and iii) a novel lazy estimation\nargument, suited to reducing the cost of estimation in FFT tree-traversal\napproaches. Our robust algorithm can be viewed as a highly optimized sparse,\nstable extension of the Cooley-Tukey FFT algorithm.\n  Finally, we explain the barriers we have faced by proving a conditional\nquadratic lower bound on the running time of the well-studied non-equispaced\nFourier transform problem. This resolves a natural and frequently asked\nquestion in computational Fourier transforms. Lastly, we provide a preliminary\nexperimental evaluation comparing the runtime of our algorithm to FFTW and SFFT\n2.0.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 14:01:15 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Bringmann", "Karl", ""], ["Kapralov", "Michael", ""], ["Makarov", "Mikhail", ""], ["Nakos", "Vasileios", ""], ["Yagudin", "Amir", ""], ["Zandieh", "Amir", ""]]}, {"id": "2107.07358", "submitter": "Fabrizio Grandoni", "authors": "Fabrizio Grandoni, Rafail Ostrovsky, Yuval Rabani, Leonard J.\n  Schulman, Rakesh Venkat", "title": "A Refined Approximation for Euclidean k-Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the Euclidean $k$-Means problem we are given a collection of $n$ points\n$D$ in an Euclidean space and a positive integer $k$. Our goal is to identify a\ncollection of $k$ points in the same space (centers) so as to minimize the sum\nof the squared Euclidean distances between each point in $D$ and the closest\ncenter. This problem is known to be APX-hard and the current best approximation\nratio is a primal-dual $6.357$ approximation based on a standard LP for the\nproblem [Ahmadian et al. FOCS'17, SICOMP'20].\n  In this note we show how a minor modification of Ahmadian et al.'s analysis\nleads to a slightly improved $6.12903$ approximation. As a related result, we\nalso show that the mentioned LP has integrality gap at least\n$\\frac{16+\\sqrt{5}}{15}>1.2157$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 14:35:04 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Grandoni", "Fabrizio", ""], ["Ostrovsky", "Rafail", ""], ["Rabani", "Yuval", ""], ["Schulman", "Leonard J.", ""], ["Venkat", "Rakesh", ""]]}, {"id": "2107.07383", "submitter": "Nidhi Purohit", "authors": "Sayan Bandyapadhyay, Fedor V. Fomin, Petr A. Golovach, Nidhi Purohit,\n  Kirill Simonov", "title": "Lossy Kernelization of Same-Size Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we study the $k$-median clustering problem with an additional\nequal-size constraint on the clusters, from the perspective of parameterized\npreprocessing. Our main result is the first lossy ($2$-approximate) polynomial\nkernel for this problem, parameterized by the cost of clustering. We complement\nthis result by establishing lower bounds for the problem that eliminate the\nexistences of an (exact) kernel of polynomial size and a PTAS.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 15:17:28 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Bandyapadhyay", "Sayan", ""], ["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Purohit", "Nidhi", ""], ["Simonov", "Kirill", ""]]}, {"id": "2107.07403", "submitter": "Rico Zenklusen", "authors": "Vera Traub and Rico Zenklusen", "title": "Local Search for Weighted Tree Augmentation and Steiner Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique that allows for improving on some relative greedy\nprocedures by well-chosen (non-oblivious) local search algorithms. Relative\ngreedy procedures are a particular type of greedy algorithm that start with a\nsimple, though weak, solution, and iteratively replace parts of this starting\nsolution by stronger components. Some well-known applications of relative\ngreedy algorithms include approximation algorithms for Steiner Tree and, more\nrecently, for connectivity augmentation problems.\n  The main application of our technique leads to a\n$(1.5+\\epsilon)$-approximation for Weighted Tree Augmentation, improving on a\nrecent relative greedy based method with approximation factor $1+\\ln 2 +\n\\epsilon\\approx 1.69$. Furthermore, we show how our local search technique can\nbe applied to Steiner Tree, leading to an alternative way to obtain the\ncurrently best known approximation factor of $\\ln 4 + \\epsilon$. Contrary to\nprior methods, our approach is purely combinatorial without the need to solve\nan LP. Nevertheless, the solution value can still be bounded in terms of the\nwell-known hypergraphic LP, leading to an alternative, and arguably simpler,\ntechnique to bound its integrality gap by $\\ln 4$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 15:43:20 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Traub", "Vera", ""], ["Zenklusen", "Rico", ""]]}, {"id": "2107.07480", "submitter": "Micha{\\l} Derezi\\'nski", "authors": "Micha{\\l} Derezi\\'nski, Jonathan Lacotte, Mert Pilanci and Michael W.\n  Mahoney", "title": "Newton-LESS: Sparsification without Trade-offs for the Sketched Newton\n  Update", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In second-order optimization, a potential bottleneck can be computing the\nHessian matrix of the optimized function at every iteration. Randomized\nsketching has emerged as a powerful technique for constructing estimates of the\nHessian which can be used to perform approximate Newton steps. This involves\nmultiplication by a random sketching matrix, which introduces a trade-off\nbetween the computational cost of sketching and the convergence rate of the\noptimization algorithm. A theoretically desirable but practically much too\nexpensive choice is to use a dense Gaussian sketching matrix, which produces\nunbiased estimates of the exact Newton step and which offers strong\nproblem-independent convergence guarantees. We show that the Gaussian sketching\nmatrix can be drastically sparsified, significantly reducing the computational\ncost of sketching, without substantially affecting its convergence properties.\nThis approach, called Newton-LESS, is based on a recently introduced sketching\ntechnique: LEverage Score Sparsified (LESS) embeddings. We prove that\nNewton-LESS enjoys nearly the same problem-independent local convergence rate\nas Gaussian embeddings, not just up to constant factors but even down to lower\norder terms, for a large class of optimization tasks. In particular, this leads\nto a new state-of-the-art convergence result for an iterative least squares\nsolver. Finally, we extend LESS embeddings to include uniformly sparsified\nrandom sign matrices which can be implemented efficiently and which perform\nwell in numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:33:05 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Derezi\u0144ski", "Micha\u0142", ""], ["Lacotte", "Jonathan", ""], ["Pilanci", "Mert", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "2107.07513", "submitter": "George Moustakides", "authors": "George V. Moustakides, Xujun Liu and Olgica Milenkovic", "title": "Optimal Stopping Methodology for the Secretary Problem with Random\n  Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Candidates arrive sequentially for an interview process which results in them\nbeing ranked relative to their predecessors. Based on the ranks available at\neach time, one must develop a decision mechanism that selects or dismisses the\ncurrent candidate in an effort to maximize the chance to select the best. This\nclassical version of the \"Secretary problem\" has been studied in depth using\nmostly combinatorial approaches, along with numerous other variants. In this\nwork we consider a particular new version where during reviewing one is allowed\nto query an external expert to improve the probability of making the correct\ndecision. Unlike existing formulations, we consider experts that are not\nnecessarily infallible and may provide suggestions that can be faulty. For the\nsolution of our problem we adopt a probabilistic methodology and view the\nquerying times as consecutive stopping times which we optimize with the help of\noptimal stopping theory. For each querying time we must also design a mechanism\nto decide whether we should terminate the search at the querying time or not.\nThis decision is straightforward under the usual assumption of infallible\nexperts but, when experts are faulty, it has a far more intricate structure.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 10:50:08 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Moustakides", "George V.", ""], ["Liu", "Xujun", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "2107.07623", "submitter": "Luca Ganassali", "authors": "Luca Ganassali, Laurent Massouli\\'e, Marc Lelarge", "title": "Correlation detection in trees for partial graph alignment", "comments": "22 pages, 1 figure. Preliminary version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider alignment of sparse graphs, which consists in finding a mapping\nbetween the nodes of two graphs which preserves most of the edges. Our approach\nis to compare local structures in the two graphs, matching two nodes if their\nneighborhoods are 'close enough': for correlated Erd\\H{o}s-R\\'enyi random\ngraphs, this problem can be locally rephrased in terms of testing whether a\npair of branching trees is drawn from either a product distribution, or a\ncorrelated distribution. We design an optimal test for this problem which gives\nrise to a message-passing algorithm for graph alignment, which provably returns\nin polynomial time a positive fraction of correctly matched vertices, and a\nvanishing fraction of mismatches. With an average degree $\\lambda = O(1)$ in\nthe graphs, and a correlation parameter $s \\in [0,1]$, this result holds with\n$\\lambda s$ large enough, and $1-s$ small enough, completing the recent\nstate-of-the-art diagram. Tighter conditions for determining whether partial\ngraph alignment (or correlation detection in trees) is feasible in polynomial\ntime are given in terms of Kullback-Leibler divergences.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 22:02:27 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Ganassali", "Luca", ""], ["Massouli\u00e9", "Laurent", ""], ["Lelarge", "Marc", ""]]}, {"id": "2107.07625", "submitter": "Nick Fischer", "authors": "Karl Bringmann, Nick Fischer, Vasileios Nakos", "title": "Deterministic and Las Vegas Algorithms for Sparse Nonnegative\n  Convolution", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computing the convolution $A\\star B$ of two length-$n$ integer vectors $A,B$\nis a core problem in several disciplines. It frequently comes up in algorithms\nfor Knapsack, $k$-SUM, All-Pairs Shortest Paths, and string pattern matching\nproblems. For these applications it typically suffices to compute convolutions\nof nonnegative vectors. This problem can be classically solved in time $O(n\\log\nn)$ using the Fast Fourier Transform.\n  However, often the involved vectors are sparse and hence one could hope for\noutput-sensitive algorithms to compute nonnegative convolutions. This question\nwas raised by Muthukrishnan and solved by Cole and Hariharan (STOC '02) by a\nrandomized algorithm running in near-linear time in the (unknown) output-size\n$t$. Chan and Lewenstein (STOC '15) presented a deterministic algorithm with a\n$2^{O(\\sqrt{\\log t\\cdot\\log\\log n})}$ overhead in running time and the\nadditional assumption that a small superset of the output is given; this\nassumption was later removed by Bringmann and Nakos (ICALP '21).\n  In this paper we present the first deterministic near-linear-time algorithm\nfor computing sparse nonnegative convolutions. This immediately gives improved\ndeterministic algorithms for the state-of-the-art of output-sensitive Subset\nSum, block-mass pattern matching, $N$-fold Boolean convolution, and others,\nmatching up to log-factors the fastest known randomized algorithms for these\nproblems. Our algorithm is a blend of algebraic and combinatorial ideas and\ntechniques.\n  Additionally, we provide two fast Las Vegas algorithms for computing sparse\nnonnegative convolutions. In particular, we present a simple $O(t\\log^2t)$ time\nalgorithm, which is an accessible alternative to Cole and Hariharan's\nalgorithm. We further refine this new algorithm to run in Las Vegas time\n$O(t\\log t\\cdot\\log\\log t)$, matching the running time of the dense case apart\nfrom the $\\log\\log t$ factor.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 22:07:58 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Bringmann", "Karl", ""], ["Fischer", "Nick", ""], ["Nakos", "Vasileios", ""]]}, {"id": "2107.07657", "submitter": "Shuli Jiang", "authors": "Shuli Jiang, Dongyu Li, Irene Mengze Li, Arvind V. Mahankali, David P.\n  Woodruff", "title": "Streaming and Distributed Algorithms for Robust Column Subset Selection", "comments": "Proceedings of the 38th International Conference on Machine Learning\n  (ICML 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first single-pass streaming algorithm for Column Subset Selection\nwith respect to the entrywise $\\ell_p$-norm with $1 \\leq p < 2$. We study the\n$\\ell_p$ norm loss since it is often considered more robust to noise than the\nstandard Frobenius norm. Given an input matrix $A \\in \\mathbb{R}^{d \\times n}$\n($n \\gg d$), our algorithm achieves a multiplicative $k^{\\frac{1}{p} -\n\\frac{1}{2}}\\text{poly}(\\log nd)$-approximation to the error with respect to\nthe best possible column subset of size $k$. Furthermore, the space complexity\nof the streaming algorithm is optimal up to a logarithmic factor. Our streaming\nalgorithm also extends naturally to a 1-round distributed protocol with nearly\noptimal communication cost. A key ingredient in our algorithms is a reduction\nto column subset selection in the $\\ell_{p,2}$-norm, which corresponds to the\n$p$-norm of the vector of Euclidean norms of each of the columns of $A$. This\nenables us to leverage strong coreset constructions for the Euclidean norm,\nwhich previously had not been applied in this context. We also give the first\nprovable guarantees for greedy column subset selection in the $\\ell_{1, 2}$\nnorm, which can be used as an alternative, practical subroutine in our\nalgorithms. Finally, we show that our algorithms give significant practical\nadvantages on real-world data analysis tasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 01:05:08 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Jiang", "Shuli", ""], ["Li", "Dongyu", ""], ["Li", "Irene Mengze", ""], ["Mahankali", "Arvind V.", ""], ["Woodruff", "David P.", ""]]}, {"id": "2107.07703", "submitter": "Otmar Ertl", "authors": "Otmar Ertl", "title": "Estimation from Partially Sampled Distributed Traces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling is often a necessary evil to reduce the processing and storage costs\nof distributed tracing. In this work, we describe a scalable and adaptive\nsampling approach that can preserve events of interest better than the widely\nused head-based sampling approach. Sampling rates can be chosen individually\nand independently for every span, allowing to take span attributes and local\nresource constraints into account. The resulting traces are often only\npartially and not completely sampled which complicates statistical analysis. To\nexploit the given information, an unbiased estimation algorithm is presented.\nEven though it does not need to know whether the traces are complete, it\nreduces the estimation error in many cases compared to considering only\ncomplete traces.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 04:41:24 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Ertl", "Otmar", ""]]}, {"id": "2107.07792", "submitter": "Andr\\'e Nusser", "authors": "Karl Bringmann, Anne Driemel, Andr\\'e Nusser, Ioannis Psarros", "title": "Tight Bounds for Approximate Near Neighbor Searching for Time Series\n  under the Fr\\'echet Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the $c$-approximate near neighbor problem under the continuous\nFr\\'echet distance: Given a set of $n$ polygonal curves with $m$ vertices, a\nradius $\\delta > 0$, and a parameter $k \\leq m$, we want to preprocess the\ncurves into a data structure that, given a query curve $q$ with $k$ vertices,\neither returns an input curve with Fr\\'echet distance at most $c\\cdot \\delta$\nto $q$, or returns that there exists no input curve with Fr\\'echet distance at\nmost $\\delta$ to $q$. We focus on the case where the input and the queries are\none-dimensional polygonal curves -- also called time series -- and we give a\ncomprehensive analysis for this case. We obtain new upper bounds that provide\ndifferent tradeoffs between approximation factor, preprocessing time, and query\ntime.\n  Our data structures improve upon the state of the art in several ways. We\nshow that for any $0 < \\varepsilon \\leq 1$ an approximation factor of\n$(1+\\varepsilon)$ can be achieved within the same asymptotic time bounds as the\npreviously best result for $(2+\\varepsilon)$. Moreover, we show that an\napproximation factor of $(2+\\varepsilon)$ can be obtained by using\npreprocessing time and space $O(nm)$, which is linear in the input size, and\nquery time in $O(\\frac{1}{\\varepsilon})^{k+2}$, where the previously best\nresult used preprocessing time in $n \\cdot O(\\frac{m}{\\varepsilon k})^k$ and\nquery time in $O(1)^k$. We complement our upper bounds with matching\nconditional lower bounds based on the Orthogonal Vectors Hypothesis.\nInterestingly, some of our lower bounds already hold for any super-constant\nvalue of $k$. This is achieved by proving hardness of a one-sided sparse\nversion of the Orthogonal Vectors problem as an intermediate problem, which we\nbelieve to be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 09:35:16 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Bringmann", "Karl", ""], ["Driemel", "Anne", ""], ["Nusser", "Andr\u00e9", ""], ["Psarros", "Ioannis", ""]]}, {"id": "2107.07800", "submitter": "Nikhil Kumar", "authors": "Antonios Antoniadis, Gunjan Kumar, Nikhil Kumar", "title": "Skeletons and Minimum Energy Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem where $n$ jobs, each with a release time, a deadline and\na required processing time are to be feasibly scheduled in a single- or\nmulti-processor setting so as to minimize the total energy consumption of the\nschedule. A processor has two available states: a \\emph{sleep state} where no\nenergy is consumed but also no processing can take place, and an \\emph{active\nstate} which consumes energy at a rate of one, and in which jobs can be\nprocessed. Transitioning from the active to the sleep does not incur any\nfurther energy cost, but transitioning from the sleep to the active state\nrequires $q$ energy units. Jobs may be preempted and (in the multi-processor\ncase) migrated.\n  The single-processor case of the problem is known to be solvable in\npolynomial time via an involved dynamic program, whereas the only known\napproximation algorithm for the multi-processor case attains an approximation\nfactor of $3$ and is based on rounding the solution to a linear programming\nrelaxation of the problem. In this work, we present efficient and combinatorial\napproximation algorithms for both the single- and the multi-processor setting.\nBefore, only an algorithm based on linear programming was known for the\nmulti-processor case. Our algorithms build upon the concept of a\n\\emph{skeleton}, a basic (and not necessarily feasible) schedule that captures\nthe fact that some processor(s) must be active at some time point during an\ninterval. Finally, we further demonstrate the power of skeletons by providing\nan $2$-approximation algorithm for the multiprocessor case, thus improving upon\nthe recent breakthrough $3$-approximation result. Our algorithm is based on a\nnovel rounding scheme of a linear-programming relaxation of the problem which\nincorporates skeletons.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 10:01:06 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Antoniadis", "Antonios", ""], ["Kumar", "Gunjan", ""], ["Kumar", "Nikhil", ""]]}, {"id": "2107.07815", "submitter": "Juli\\'an Mestre", "authors": "Juli\\'an Mestre, Sergey Pupyrev and Seeun William Umboh", "title": "On the Extended TSP Problem", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the theoretical study of Ext-TSP, a problem that originates in\nthe area of profile-guided binary optimization. Given a graph $G=(V, E)$ with\npositive edge weights $w: E \\rightarrow R^+$, and a non-increasing discount\nfunction $f(\\cdot)$ such that $f(1) = 1$ and $f(i) = 0$ for $i > k$, for some\nparameter $k$ that is part of the problem definition. The problem is to\nsequence the vertices $V$ so as to maximize $\\sum_{(u, v) \\in E} f(|d_u -\nd_v|)\\cdot w(u,v)$, where $d_v \\in \\{1, \\ldots, |V| \\}$ is the position of\nvertex~$v$ in the sequence.\n  We show that \\prob{Ext-TSP} is APX-hard to approximate in general and we give\na $(k+1)$-approximation algorithm for general graphs and a PTAS for some sparse\ngraph classes such as planar or treewidth-bounded graphs.\n  Interestingly, the problem remains challenging even on very simple graph\nclasses; indeed, there is no exact $n^{o(k)}$ time algorithm for trees unless\nthe ETH fails. We complement this negative result with an exact $n^{O(k)}$ time\nalgorithm for trees.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 10:52:11 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Mestre", "Juli\u00e1n", ""], ["Pupyrev", "Sergey", ""], ["Umboh", "Seeun William", ""]]}, {"id": "2107.07841", "submitter": "Kheeran K. Naidu", "authors": "Christian Konrad, Kheeran K. Naidu", "title": "On Two-Pass Streaming Algorithms for Maximum Bipartite Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study two-pass streaming algorithms for \\textsf{Maximum Bipartite\nMatching} (\\textsf{MBM}). All known two-pass streaming algorithms for\n\\textsf{MBM} operate in a similar fashion: They compute a maximal matching in\nthe first pass and find 3-augmenting paths in the second in order to augment\nthe matching found in the first pass. Our aim is to explore the limitations of\nthis approach and to determine whether current techniques can be used to\nfurther improve the state-of-the-art algorithms. We give the following results:\n  We show that every two-pass streaming algorithm that solely computes a\nmaximal matching in the first pass and outputs a $(2/3+\\epsilon)$-approximation\nrequires $n^{1+\\Omega(\\frac{1}{\\log \\log n})}$ space, for every $\\epsilon > 0$,\nwhere $n$ is the number of vertices of the input graph. This result is obtained\nby extending the Ruzsa-Szemer\\'{e}di graph construction of [GKK, SODA'12] so as\nto ensure that the resulting graph has a close to perfect matching, the key\nproperty needed in our construction. This result may be of independent\ninterest.\n  Furthermore, we combine the two main techniques, i.e., subsampling followed\nby the \\textsc{Greedy} matching algorithm [Konrad, MFCS'18] which gives a\n$2-\\sqrt{2} \\approx 0.5857$-approximation, and the computation of\n\\emph{degree-bounded semi-matchings} [EHM, ICDMW'16][KT, APPROX'17] which gives\na $\\frac{1}{2} + \\frac{1}{12} \\approx 0.5833$-approximation, and obtain a\nmeta-algorithm that yields Konrad's and Esfandiari et al.'s algorithms as\nspecial cases. This unifies two strands of research. By optimizing parameters,\nwe discover that Konrad's algorithm is optimal for the implied class of\nalgorithms and, perhaps surprisingly, that there is a second optimal algorithm.\nWe show that the analysis of our meta-algorithm is best possible. Our results\nimply that further improvements, if possible, require new techniques.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 12:02:24 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Konrad", "Christian", ""], ["Naidu", "Kheeran K.", ""]]}, {"id": "2107.07889", "submitter": "Yi Li", "authors": "Yifei Jiang, Yi Li, Yiming Sun, Jiaxin Wang, David P. Woodruff", "title": "Single Pass Entrywise-Transformed Low Rank Approximation", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In applications such as natural language processing or computer vision, one\nis given a large $n \\times d$ matrix $A = (a_{i,j})$ and would like to compute\na matrix decomposition, e.g., a low rank approximation, of a function $f(A) =\n(f(a_{i,j}))$ applied entrywise to $A$. A very important special case is the\nlikelihood function $f\\left( A \\right ) = \\log{\\left( \\left| a_{ij}\\right|\n+1\\right)}$. A natural way to do this would be to simply apply $f$ to each\nentry of $A$, and then compute the matrix decomposition, but this requires\nstoring all of $A$ as well as multiple passes over its entries. Recent work of\nLiang et al.\\ shows how to find a rank-$k$ factorization to $f(A)$ for an $n\n\\times n$ matrix $A$ using only $n \\cdot \\operatorname{poly}(\\epsilon^{-1}k\\log\nn)$ words of memory, with overall error $10\\|f(A)-[f(A)]_k\\|_F^2 +\n\\operatorname{poly}(\\epsilon/k) \\|f(A)\\|_{1,2}^2$, where $[f(A)]_k$ is the best\nrank-$k$ approximation to $f(A)$ and $\\|f(A)\\|_{1,2}^2$ is the square of the\nsum of Euclidean lengths of rows of $f(A)$. Their algorithm uses three passes\nover the entries of $A$. The authors pose the open question of obtaining an\nalgorithm with $n \\cdot \\operatorname{poly}(\\epsilon^{-1}k\\log n)$ words of\nmemory using only a single pass over the entries of $A$. In this paper we\nresolve this open question, obtaining the first single-pass algorithm for this\nproblem and for the same class of functions $f$ studied by Liang et al.\nMoreover, our error is $\\|f(A)-[f(A)]_k\\|_F^2 + \\operatorname{poly}(\\epsilon/k)\n\\|f(A)\\|_F^2$, where $\\|f(A)\\|_F^2$ is the sum of squares of Euclidean lengths\nof rows of $f(A)$. Thus our error is significantly smaller, as it removes the\nfactor of $10$ and also $\\|f(A)\\|_F^2 \\leq \\|f(A)\\|_{1,2}^2$. We also give an\nalgorithm for regression, pointing out an error in previous work, and\nempirically validate our results.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 13:22:29 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Jiang", "Yifei", ""], ["Li", "Yi", ""], ["Sun", "Yiming", ""], ["Wang", "Jiaxin", ""], ["Woodruff", "David P.", ""]]}, {"id": "2107.07930", "submitter": "Chaos Dong", "authors": "Chaos Dong and Fang Wang", "title": "DxHash: A Scalable Consistent Hash Based on the Pseudo-Random Sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Consistent hasing has played a fundamental role as a data router and a load\nbalancer in various fields, such as distributed database, cloud infrastructure,\nand peer-to-peer network. However, the existing consistent hashing schemes\ncan't meet the requirements simultaneously, including full consistency,\nscalability, small memory footprint, low update time and low query complexity.\nThus, We propose DxHash, a scalable consistent hashing algorithm based on the\npseudo-random sequence. For the scenario of distributed storage, there are two\noptimizations based on DXHash are proposed. First, the Weighted DxHash can\nadjust the workloads on arbitrary nodes. Second, the Asymmetric Replica\nStrategy (ARS) is combining the replica strategy in distributed storage with\nthe scaleup process to improve the availability of the system and reduce the\nremapping rate. The evaluation indicates that compared with the state-of-art\nworks, DxHash achieves significant improvements on the 5 requirements. Even\nwith 50% failure ratio, DxHash still can complete 16.5 million queries per\nsecond. What's more, the two optimizations both achieve their own results.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 14:45:57 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Dong", "Chaos", ""], ["Wang", "Fang", ""]]}, {"id": "2107.08090", "submitter": "Praneeth Kacham", "authors": "Nadiia Chepurko, Kenneth L. Clarkson, Praneeth Kacham and David P.\n  Woodruff", "title": "Near-Optimal Algorithms for Linear Algebra in the Current Matrix\n  Multiplication Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Currently, in the numerical linear algebra community, it is thought that to\nobtain nearly-optimal bounds for various problems such as rank computation,\nfinding a maximal linearly independent subset of columns, regression, low rank\napproximation, maximum matching on general graphs and linear matroid union, one\nwould need to resolve the main open question of Nelson and Nguyen (FOCS, 2013)\nregarding the logarithmic factors in the sketching dimension for existing\nconstant factor approximation oblivious subspace embeddings. We show how to\nbypass this question using a refined sketching technique, and obtain optimal or\nnearly optimal bounds for these problems. A key technique we use is an explicit\nmapping of Indyk based on uncertainty principles and extractors, which after\nfirst applying known oblivious subspace embeddings, allows us to quickly spread\nout the mass of the vector so that sampling is now effective, and we avoid a\nlogarithmic factor that is standard in the sketching dimension resulting from\nmatrix Chernoff bounds. For the fundamental problems of rank computation and\nfinding a linearly independent subset of columns, our algorithms improve\nCheung, Kwok, and Lau (JACM, 2013) and are optimal to within a constant factor\nand a $\\log\\log(n)$-factor, respectively. Further, for constant factor\nregression and low rank approximation we give the first optimal algorithms, for\nthe current matrix multiplication exponent.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 19:34:10 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Chepurko", "Nadiia", ""], ["Clarkson", "Kenneth L.", ""], ["Kacham", "Praneeth", ""], ["Woodruff", "David P.", ""]]}, {"id": "2107.08277", "submitter": "Evangelia Gergatsouli", "authors": "Dimitris Fotakis, Evangelia Gergatsouli, Themis Gouleakis, Nikolas\n  Patris", "title": "Learning Augmented Online Facility Location", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the research agenda initiated by Munoz & Vassilvitskii [1] and\nLykouris & Vassilvitskii [2] on learning-augmented online algorithms for\nclassical online optimization problems, in this work, we consider the Online\nFacility Location problem under this framework. In Online Facility Location\n(OFL), demands arrive one-by-one in a metric space and must be (irrevocably)\nassigned to an open facility upon arrival, without any knowledge about future\ndemands.\n  We present an online algorithm for OFL that exploits potentially imperfect\npredictions on the locations of the optimal facilities. We prove that the\ncompetitive ratio decreases smoothly from sublogarithmic in the number of\ndemands to constant, as the error, i.e., the total distance of the predicted\nlocations to the optimal facility locations, decreases towards zero. We\ncomplement our analysis with a matching lower bound establishing that the\ndependence of the algorithm's competitive ratio on the error is optimal, up to\nconstant factors. Finally, we evaluate our algorithm on real world data and\ncompare our learning augmented approach with the current best online algorithm\nfor the problem.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 16:44:27 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Fotakis", "Dimitris", ""], ["Gergatsouli", "Evangelia", ""], ["Gouleakis", "Themis", ""], ["Patris", "Nikolas", ""]]}, {"id": "2107.08292", "submitter": "Stefan Canzar", "authors": "Mislav Bla\\v{z}evi\\'c, Stefan Canzar, Khaled Elbassioni, Domagoj\n  Matijevi\\'c", "title": "Anti Tai Mapping for Unordered Labeled Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The well-studied Tai mapping between two rooted labeled trees $T_1(V_1, E_1)$\nand $T_2(V_2, E_2)$ defines a one-to-one mapping between nodes in $T_1$ and\n$T_2$ that preserves ancestor relationship. For unordered trees the problem of\nfinding a maximum-weight Tai mapping is known to be NP-complete. In this work,\nwe define an anti Tai mapping $M\\subseteq V_1\\times V_2$ as a binary relation\nbetween two unordered labeled trees such that any two $(x,y), (x', y')\\in M$\nviolate ancestor relationship and thus cannot be part of the same Tai mapping,\ni.e. $(x\\le x' \\iff y\\not \\le y') \\vee (x'\\le x \\iff y'\\not \\le y)$, given an\nancestor order $x<x'$ meaning that $x$ is an ancestor of $x'$. Finding a\nmaximum-weight anti Tai mapping arises in the cutting plane method for solving\nthe maximum-weight Tai mapping problem via integer programming. We give an\nefficient polynomial-time algorithm for finding a maximum-weight anti Tai\nmapping for the case when one of the two trees is a path and further show how\nto extend this result in order to provide a polynomially computable lower bound\non the optimal anti Tai mapping for two unordered labeled trees. The latter\nresult stems from the special class of anti Tai mapping defined by the more\nrestricted condition $x\\sim x' \\iff y\\not\\sim y'$, where $\\sim$ denotes that\ntwo nodes belong to the same root-to-leaf path. For this class, we give an\nefficient algorithm that solves the problem directly on two unordered trees in\n$O(|V_1|^2|V_2|^2)$.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 17:37:26 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Bla\u017eevi\u0107", "Mislav", ""], ["Canzar", "Stefan", ""], ["Elbassioni", "Khaled", ""], ["Matijevi\u0107", "Domagoj", ""]]}, {"id": "2107.08473", "submitter": "Eli Ben-Sasson", "authors": "Eli Ben-Sasson, Dan Carmon, Swastik Kopparty and David Levit", "title": "Elliptic Curve Fast Fourier Transform (ECFFT) Part I: Fast Polynomial\n  Algorithms over all Finite Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For smooth finite fields $F_q$ (i.e., when $q-1$ factors into small primes)\nthe Fast Fourier Transform (FFT) leads to the fastest known algebraic\nalgorithms for many basic polynomial operations, such as multiplication,\ndivision, interpolation and multi-point evaluation. However, the same\noperations over fields with no smooth order root of unity suffer from an\nasymptotic slowdown. The classical algorithm of Schonhage and Strassen incurred\na multiplicative slowdown factor of $\\log \\log n$ on top of the smooth case.\nRecent remarkable results of Harvey, van der Hoeven and Lecerf dramatically\nreduced this multiplicative overhead to $\\exp(\\log^* (n))$.\n  We introduce a new approach to fast algorithms for polynomial operations over\nall large finite fields. The key idea is to replace the group of roots of unity\nwith a set of points $L \\subset F$ suitably related to a well-chosen elliptic\ncurve group (the set $L$ itself is not a group). The key advantage of this\napproach is that elliptic curve groups can be of any size in the Hasse-Weil\ninterval $[q+1 \\pm 2\\sqrt{q}]$ and thus can have subgroups of large, smooth\norder, which an FFT-like divide and conquer algorithm can exploit. Compare this\nwith multiplicative subgroups over whose order must divide $q-1$.\n  For polynomials represented by their evaluation over subsets of $L$, we show\nthat multiplication, division, degree-computation, interpolation, evaluation\nand Reed-Solomon encoding (also known as low-degree extension) with fixed\nevaluation points can all be computed with arithmetic circuits of size similar\nto what is achievable with the classical FFTs when the field size is special.\nFor several problems, this yields the asymptotically smallest known arithmetic\ncircuits even in the standard monomial representation of polynomials.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 15:27:37 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ben-Sasson", "Eli", ""], ["Carmon", "Dan", ""], ["Kopparty", "Swastik", ""], ["Levit", "David", ""]]}, {"id": "2107.08480", "submitter": "Viet-Dung Nguyen", "authors": "Viet Dung Nguyen, Ba Thai Pham, Phan Thuan Do", "title": "Efficient algorithms for maximum induced matching problem in permutation\n  and trapezoid graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first design an $\\mathcal{O}(n^2)$ solution for finding a maximum induced\nmatching in permutation graphs given their permutation models, based on a\ndynamic programming algorithm with the aid of the sweep line technique. With\nthe support of the disjoint-set data structure, we improve the complexity to\n$\\mathcal{O}(m + n)$. Consequently, we extend this result to give an\n$\\mathcal{O}(m + n)$ algorithm for the same problem in trapezoid graphs. By\ncombining our algorithms with the current best graph identification algorithms,\nwe can solve the MIM problem in permutation and trapezoid graphs in linear and\n$\\mathcal{O}(n^2)$ time, respectively. Our results are far better than the best\nknown $\\mathcal{O}(mn)$ algorithm for the maximum induced matching problem in\nboth graph classes, which was proposed by Habib et al.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 15:53:32 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Nguyen", "Viet Dung", ""], ["Pham", "Ba Thai", ""], ["Do", "Phan Thuan", ""]]}, {"id": "2107.08542", "submitter": "Linh Anh Nguyen D.Sc.", "authors": "Linh Anh Nguyen", "title": "Computing the Fuzzy Partition Corresponding to the Greatest Fuzzy\n  Auto-Bisimulation of a Fuzzy Graph-Based Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy graph-based structures such as fuzzy automata, fuzzy labeled transition\nsystems, fuzzy Kripke models, fuzzy social networks and fuzzy interpretations\nin fuzzy description logics are useful in various applications. Given two\nstates, two actors or two individuals $x$ and $x'$ in such structures $G$ and\n$G'$, respectively, the similarity degree between them can be defined to be\n$Z(x,x')$, where $Z$ is the greatest fuzzy bisimulation between $G$ and $G'$\nw.r.t. some t-norm-based fuzzy logic. Such a similarity measure has the\nHennessy-Milner property of fuzzy bisimulations as a strong logical foundation.\nA fuzzy bisimulation between a fuzzy structure $G$ and itself is called a fuzzy\nauto-bisimulation of $G$. The greatest fuzzy auto-bisimulation of an\nimage-finite fuzzy graph-based structure is a fuzzy equivalence relation. It is\nuseful for classification and clustering.\n  In this paper, we design an efficient algorithm with the complexity\n$O((m\\log{l} + n)\\log{n})$ for computing the fuzzy partition corresponding to\nthe greatest fuzzy auto-bisimulation of a finite fuzzy labeled graph $G$ under\nthe G\\\"odel semantics, where $n$, $m$ and $l$ are the number of vertices, the\nnumber of non-zero edges and the number of different fuzzy degrees of edges of\n$G$, respectively. Our notion of fuzzy partition is novel, defined only for\nfinite sets with respect to the G\\\"odel t-norm, with the aim to facilitate the\ncomputation of the greatest fuzzy auto-bisimulation. By using that algorithm,\nwe also provide an algorithm with the complexity $O(m\\cdot\\log{l}\\cdot\\log{n} +\nn^2)$ for computing the greatest fuzzy bisimulation between two finite fuzzy\nlabeled graphs under the G\\\"odel semantics. This latter algorithm is better\n(has a lower complexity order) than the previously known algorithms for the\nconsidered problem. Our algorithms can be restated for the other mentioned\nfuzzy graph-based structures.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 21:39:10 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Nguyen", "Linh Anh", ""]]}, {"id": "2107.08615", "submitter": "Shunsuke Inenaga", "authors": "Tooru Akagi, Mitsuru Funakoshi, Shunsuke Inenaga", "title": "Sensitivity of string compressors and repetitiveness measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The sensitivity of a string compression algorithm $C$ asks how much the\noutput size $C(T)$ for an input string $T$ can increase when a single character\nedit operation is performed on $T$. This notion enables one to measure the\nrobustness of compression algorithms in terms of errors and/or dynamic changes\noccurring in the input string. In this paper, we analyze the worst-case\nmultiplicative sensitivity of string compression algorithms, defined by\n$\\max_{T \\in \\Sigma^n}\\{C(T')/C(T) : ed(T, T') = 1\\}$, where $ed(T, T')$\ndenotes the edit distance between $T$ and $T'$. For the most common versions of\nthe Lempel-Ziv 77 compressors, we prove that the worst-case multiplicative\nsensitivity is only a small constant (2 or 3, depending on the version of the\nLempel-Ziv 77 and the edit operation type). We strengthen our upper bound\nresults by presenting matching lower bounds on the worst-case sensitivity for\nall these major versions of the Lempel-Ziv 77 factorizations. This contrasts\nwith the previously known related results such that the size $z_{\\rm 78}$ of\nthe Lempel-Ziv 78 factorization can increase by a factor of $\\Omega(n^{3/4})$\n[Lagarde and Perifel, 2018], and the number $r$ of runs in the Burrows-Wheeler\ntransform can increase by a factor of $\\Omega(\\log n)$ [Giuliani et al., 2021]\nwhen a character is prepended to an input string of length $n$. We also study\nthe worst-case sensitivity of several grammar compression algorithms including\nBisection, AVL-grammar, GCIS, and CDAWG. Further, we extend the notion of the\nworst-case sensitivity to string repetitiveness measures such as the smallest\nstring attractor size $\\gamma$ and the substring complexity $\\delta$, and\npresent matching upper and lower bounds of the worst-case multiplicative\nsensitivity for $\\gamma$ and $\\delta$.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 05:23:30 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Akagi", "Tooru", ""], ["Funakoshi", "Mitsuru", ""], ["Inenaga", "Shunsuke", ""]]}, {"id": "2107.08725", "submitter": "Leah Epstein", "authors": "Leah Epstein", "title": "Several methods of analysis for cardinality constrained bin packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a known variant of bin packing called {\\it cardinality\nconstrained bin packing}, also called {\\it bin packing with cardinality\nconstraints} (BPCC). In this problem, there is a parameter k\\geq 2, and items\nof rational sizes in [0,1] are to be packed into bins, such that no bin has\nmore than k items or total size larger than 1. The goal is to minimize the\nnumber of bins.\n  A recently introduced concept, called the price of clustering, deals with\ninputs that are presented in a way that they are split into clusters. Thus, an\nitem has two attributes which are its size and its cluster. The goal is to\nmeasure the relation between an optimal solution that cannot combine items of\ndifferent clusters into bins, and an optimal solution that can combine items of\ndifferent clusters arbitrarily. Usually the number of clusters may be large,\nwhile clusters are relatively small, though not trivially small. Such problems\nare related to greedy bin packing algorithms, and to batched bin packing, which\nis similar to the price of clustering, but there is a constant number of large\nclusters. We analyze the price of clustering for BPCC, including the parametric\ncase with bounded item sizes. We discuss several greedy algorithms for this\nproblem that were not studied in the past, and comment on batched bin packing.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 09:51:59 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Epstein", "Leah", ""]]}, {"id": "2107.08848", "submitter": "Marcus Pappik", "authors": "Tobias Friedrich, Andreas G\\\"obel, Maximilian Katzmann, Martin S.\n  Krejca, Marcus Pappik", "title": "Algorithms for general hard-constraint point processes via\n  discretization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a general model for continuous spin systems with hard-core\ninteractions. Our model allows for a mixture of $q$ types of particles on a\n$d$-dimensional Euclidean region $\\mathbb{V}$ of volume $\\nu(\\mathbb{V})$. For\neach type, particle positions are distributed according to a Poisson point\nprocess. The Gibbs distribution over possible system states is given by the\nmixture of these point processes conditioned that no two particles are closer\nthan some distance parameterized by a $q \\times q$ matrix. This model\nencompasses classical continuous spin systems, such as the hard-sphere model or\nthe Widom-Rowlinson model. We present sufficient conditions for approximating\nthe partition function of this model, which is the normalizing factor of its\nGibbs measure. For the hard-sphere model, our method yields a randomized\napproximation algorithm with running time polynomial in $\\nu(\\mathbb{V})$ for\nthe known uniqueness regime of the Gibbs measure. In the same parameter regime,\nwe obtain a quasi-polynomial deterministic approximation, which, to our\nknowledge, is the first rigorous deterministic algorithm for a continuous spin\nsystem. We obtain similar approximation results for all continuous spin systems\ncaptured by our model and, in particular, the first explicit approximation\nbounds for the Widom-Rowlinson model. Additionally, we show how to obtain\nefficient approximate samplers for the Gibbs distributions of the respective\nspin systems within the same parameter regimes. Key to our method is reducing\nthe continuous model to a discrete instance of the hard-core model with size\npolynomial in $\\nu(\\mathbb{V})$. This generalizes existing discretization\nschemes for the hard-sphere model and, additionally, improves the required\nnumber of vertices of the generated graph from super-exponential to quadratic\nin $\\nu(\\mathbb{V})$, which we argue to be tight.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 13:05:22 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Friedrich", "Tobias", ""], ["G\u00f6bel", "Andreas", ""], ["Katzmann", "Maximilian", ""], ["Krejca", "Martin S.", ""], ["Pappik", "Marcus", ""]]}, {"id": "2107.09206", "submitter": "Juliusz Straszy\\'nski", "authors": "Jakub Radoszewski, Wojciech Rytter, Juliusz Straszy\\'nski, Tomasz\n  Wale\\'n and Wiktor Zuba", "title": "Hardness of Detecting Abelian and Additive Square Factors in Strings", "comments": "Accepted to ESA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove 3SUM-hardness (no strongly subquadratic-time algorithm, assuming the\n3SUM conjecture) of several problems related to finding Abelian square and\nadditive square factors in a string. In particular, we conclude conditional\noptimality of the state-of-the-art algorithms for finding such factors.\n  Overall, we show 3SUM-hardness of (a) detecting an Abelian square factor of\nan odd half-length, (b) computing centers of all Abelian square factors, (c)\ndetecting an additive square factor in a length-$n$ string of integers of\nmagnitude $n^{\\mathcal{O}(1)}$, and (d) a problem of computing a double 3-term\narithmetic progression (i.e., finding indices $i \\ne j$ such that\n$(x_i+x_j)/2=x_{(i+j)/2}$) in a sequence of integers $x_1,\\dots,x_n$ of\nmagnitude $n^{\\mathcal{O}(1)}$.\n  Problem (d) is essentially a convolution version of the AVERAGE problem that\nwas proposed in a manuscript of Erickson. We obtain a conditional lower bound\nfor it with the aid of techniques recently developed by Dudek et al. [STOC\n2020]. Problem (d) immediately reduces to problem (c) and is a step in\nreductions to problems (a) and (b). In conditional lower bounds for problems\n(a) and (b) we apply an encoding of Amir et al. [ICALP 2014] and extend it\nusing several string gadgets that include arbitrarily long Abelian-square-free\nstrings.\n  Our reductions also imply conditional lower bounds for detecting Abelian\nsquares in strings over a constant-sized alphabet. We also show a subquadratic\nupper bound in this case, applying a result of Chan and Lewenstein [STOC 2015].\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 00:24:32 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Straszy\u0144ski", "Juliusz", ""], ["Wale\u0144", "Tomasz", ""], ["Zuba", "Wiktor", ""]]}, {"id": "2107.09310", "submitter": "Matthew Bold", "authors": "Matthew Bold and Marc Goerigk", "title": "Recoverable Robust Single Machine Scheduling with Interval Uncertainty", "comments": "22 pages, 11 figures. In submission at Discrete Applied Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the recoverable robust single machine scheduling problem under\ninterval uncertainty. In this setting, jobs have first-stage processing times p\nand second-stage processing times q and we aim to find a first-stage and\nsecond-stage schedule with a minimum combined sum of completion times, such\nthat at least Delta jobs share the same position in both schedules.\n  We provide positive complexity results for some important special cases of\nthis problem, as well as derive a 2-approximation algorithm to the full\nproblem. Computational experiments examine the performance of an exact\nmixed-integer programming formulation and the approximation algorithm, and\ndemonstrate the strength of a proposed polynomial time greedy heuristic.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 07:58:40 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Bold", "Matthew", ""], ["Goerigk", "Marc", ""]]}, {"id": "2107.09461", "submitter": "Zhize Li", "authors": "Zhize Li, Peter Richt\\'arik", "title": "CANITA: Faster Rates for Distributed Convex Optimization with\n  Communication Compression", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the high communication cost in distributed and federated learning,\nmethods relying on compressed communication are becoming increasingly popular.\nBesides, the best theoretically and practically performing gradient-type\nmethods invariably rely on some form of acceleration/momentum to reduce the\nnumber of communications (faster convergence), e.g., Nesterov's accelerated\ngradient descent (Nesterov, 2004) and Adam (Kingma and Ba, 2014). In order to\ncombine the benefits of communication compression and convergence acceleration,\nwe propose a \\emph{compressed and accelerated} gradient method for distributed\noptimization, which we call CANITA. Our CANITA achieves the \\emph{first\naccelerated rate}\n$O\\bigg(\\sqrt{\\Big(1+\\sqrt{\\frac{\\omega^3}{n}}\\Big)\\frac{L}{\\epsilon}} +\n\\omega\\big(\\frac{1}{\\epsilon}\\big)^{\\frac{1}{3}}\\bigg)$, which improves upon\nthe state-of-the-art non-accelerated rate\n$O\\left((1+\\frac{\\omega}{n})\\frac{L}{\\epsilon} +\n\\frac{\\omega^2+n}{\\omega+n}\\frac{1}{\\epsilon}\\right)$ of DIANA (Khaled et al.,\n2020b) for distributed general convex problems, where $\\epsilon$ is the target\nerror, $L$ is the smooth parameter of the objective, $n$ is the number of\nmachines/devices, and $\\omega$ is the compression parameter (larger $\\omega$\nmeans more compression can be applied, and no compression implies $\\omega=0$).\nOur results show that as long as the number of devices $n$ is large (often true\nin distributed/federated learning), or the compression $\\omega$ is not very\nhigh, CANITA achieves the faster convergence rate\n$O\\Big(\\sqrt{\\frac{L}{\\epsilon}}\\Big)$, i.e., the number of communication\nrounds is $O\\Big(\\sqrt{\\frac{L}{\\epsilon}}\\Big)$ (vs.\n$O\\big(\\frac{L}{\\epsilon}\\big)$ achieved by previous works). As a result,\nCANITA enjoys the advantages of both compression (compressed communication in\neach round) and acceleration (much fewer communication rounds).\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 13:01:56 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Li", "Zhize", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "2107.09480", "submitter": "Giosu\\'e Lo Bosco", "authors": "Domenico Amato and Raffaele Giancarlo and Giosu\\`e Lo Bosco", "title": "Learned Sorted Table Search and Static Indexes in Small Space:\n  Methodological and Practical Insights via an Experimental Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sorted Table Search Procedures are the quintessential query-answering tool,\nstill very useful, e.g, Search Engines (Google Chrome). Speeding them up, in\nsmall additional space with respect to the table being searched into, is still\na quite significant achievement. Static Learned Indexes have been very\nsuccessful in achieving such a speed-up, but leave open a major question: To\nwhat extent one can enjoy the speed-up of Learned Indexes while using constant\nor nearly constant additional space. By generalizing the experimental\nmethodology of a recent benchmarking study on Learned Indexes, we shed light on\nthis question, by considering two scenarios. The first, quite elementary, i.e.,\ntextbook code, and the second using advanced Learned Indexing algorithms and\nthe supporting sophisticated software platforms. Although in both cases one\nwould expect a positive answer, its achievement is not as simple as it seems.\nIndeed, our extensive set of experiments reveal a complex relationship between\nquery time and model space. The findings regarding this relationship and the\ncorresponding quantitative estimates, across memory levels, can be of interest\nto algorithm designers and of use to practitioners as well. As an essential\npart of our research, we introduce two new models that are of interest in their\nown right. The first is a constant space model that can be seen as a\ngeneralization of $k$-ary search, while the second is a synoptic {\\bf RMI}, in\nwhich we can control model space usage.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 16:06:55 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 13:56:52 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 13:18:02 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Amato", "Domenico", ""], ["Giancarlo", "Raffaele", ""], ["Bosco", "Giosu\u00e8 Lo", ""]]}, {"id": "2107.09481", "submitter": "Sayan Bandyapadhyay", "authors": "Sayan Bandyapadhyay, Fedor V. Fomin, Petr A. Golovach, Nidhi Purohit,\n  Kirill Simonov", "title": "FPT Approximation for Fair Minimum-Load Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider the Minimum-Load $k$-Clustering/Facility Location\n(MLkC) problem where we are given a set $P$ of $n$ points in a metric space\nthat we have to cluster and an integer $k$ that denotes the number of clusters.\nAdditionally, we are given a set $F$ of cluster centers in the same metric\nspace. The goal is to select a set $C\\subseteq F$ of $k$ centers and assign\neach point in $P$ to a center in $C$, such that the maximum load over all\ncenters is minimized. Here the load of a center is the sum of the distances\nbetween it and the points assigned to it.\n  Although clustering/facility location problems have a rich literature, the\nminimum-load objective is not studied substantially, and hence MLkC has\nremained a poorly understood problem. More interestingly, the problem is\nnotoriously hard even in some special cases including the one in line metrics\nas shown by Ahmadian et al. [ACM Trans. Algo. 2018]. They also show\nAPX-hardness of the problem in the plane. On the other hand, the best-known\napproximation factor for MLkC is $O(k)$, even in the plane.\n  In this work, we study a fair version of MLkC inspired by the work of\nChierichetti et al. [NeurIPS, 2017], which generalizes MLkC. Here the input\npoints are colored by one of the $\\ell$ colors denoting the group they belong\nto. MLkC is the special case with $\\ell=1$. Considering this problem, we are\nable to obtain a $3$-approximation in $f(k,\\ell)\\cdot n^{O(1)}$ time. Also, our\nscheme leads to an improved $(1 + \\epsilon)$-approximation in case of Euclidean\nnorm, and in this case, the running time depends only polynomially on the\ndimension $d$. Our results imply the same approximations for MLkC with running\ntime $f(k)\\cdot n^{O(1)}$, achieving the first constant approximations for this\nproblem in general and Euclidean metric spaces.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 13:34:13 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Bandyapadhyay", "Sayan", ""], ["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Purohit", "Nidhi", ""], ["Simonov", "Kirill", ""]]}, {"id": "2107.09497", "submitter": "Diptarka Chakraborty", "authors": "Diptarka Chakraborty, Debarati Das, and Robert Krauthgamer", "title": "Approximate Trace Reconstruction via Median String (in Average-Case)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider an \\emph{approximate} version of the trace reconstruction\nproblem, where the goal is to recover an unknown string $s\\in\\{0,1\\}^n$ from\n$m$ traces (each trace is generated independently by passing $s$ through a\nprobabilistic insertion-deletion channel with rate $p$). We present a\ndeterministic near-linear time algorithm for the average-case model, where $s$\nis random, that uses only \\emph{three} traces. It runs in near-linear time\n$\\tilde O(n)$ and with high probability reports a string within edit distance\n$O(\\epsilon p n)$ from $s$ for $\\epsilon=\\tilde O(p)$, which significantly\nimproves over the straightforward bound of $O(pn)$.\n  Technically, our algorithm computes a $(1+\\epsilon)$-approximate median of\nthe three input traces. To prove its correctness, our probabilistic analysis\nshows that an approximate median is indeed close to the unknown $s$. To achieve\na near-linear time bound, we have to bypass the well-known dynamic programming\nalgorithm that computes an optimal median in time $O(n^3)$.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 13:44:54 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Chakraborty", "Diptarka", ""], ["Das", "Debarati", ""], ["Krauthgamer", "Robert", ""]]}, {"id": "2107.09743", "submitter": "S Thomas McCormick", "authors": "Maxwell Allman, Venus Lo, S. Thomas McCormick", "title": "Complexity of Source-Sink Monotone 2-Parameter Min Cut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  There are many applications of max flow with capacities that depend on one or\nmore parameters. Many of these applications fall into the \"Source-Sink\nMonotone\" framework, a special case of Topkis's monotonic optimization\nframework, which implies that the parametric min cuts are nested. When there is\na single parameter, this property implies that the number of distinct min cuts\nis linear in the number of nodes, which is quite useful for constructing\nalgorithms to identify all possible min cuts.\n  When there are multiple Source-Sink Monotone parameters and the vector of\nparameters are ordered in the usual vector sense, the resulting min cuts are\nstill nested. However, the number of distinct min cuts was an open question. We\nshow that even with only two parameters, the number of distinct min cuts can be\nexponential in the number of nodes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 19:41:19 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Allman", "Maxwell", ""], ["Lo", "Venus", ""], ["McCormick", "S. Thomas", ""]]}, {"id": "2107.09770", "submitter": "Michael Dinitz", "authors": "Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, Sergei\n  Vassilvitskii", "title": "Faster Matchings via Learned Duals", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A recent line of research investigates how algorithms can be augmented with\nmachine-learned predictions to overcome worst case lower bounds. This area has\nrevealed interesting algorithmic insights into problems, with particular\nsuccess in the design of competitive online algorithms. However, the question\nof improving algorithm running times with predictions has largely been\nunexplored.\n  We take a first step in this direction by combining the idea of\nmachine-learned predictions with the idea of \"warm-starting\" primal-dual\nalgorithms. We consider one of the most important primitives in combinatorial\noptimization: weighted bipartite matching and its generalization to\n$b$-matching. We identify three key challenges when using learned dual\nvariables in a primal-dual algorithm. First, predicted duals may be infeasible,\nso we give an algorithm that efficiently maps predicted infeasible duals to\nnearby feasible solutions. Second, once the duals are feasible, they may not be\noptimal, so we show that they can be used to quickly find an optimal solution.\nFinally, such predictions are useful only if they can be learned, so we show\nthat the problem of learning duals for matching has low sample complexity. We\nvalidate our theoretical findings through experiments on both real and\nsynthetic data. As a result we give a rigorous, practical, and empirically\neffective method to compute bipartite matchings.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 21:11:09 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Dinitz", "Michael", ""], ["Im", "Sungjin", ""], ["Lavastida", "Thomas", ""], ["Moseley", "Benjamin", ""], ["Vassilvitskii", "Sergei", ""]]}, {"id": "2107.09790", "submitter": "Farzam Ebrahimnejad", "authors": "Farzam Ebrahimnejad and James R. Lee", "title": "Non-existence of annular separators in geometric graphs", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benjamini and Papasoglou (2011) showed that planar graphs with uniform\npolynomial volume growth admit $1$-dimensional annular separators: The vertices\nat graph distance $R$ from any vertex can be separated from those at distance\n$2R$ by removing at most $O(R)$ vertices. They asked whether geometric\n$d$-dimensional graphs with uniform polynomial volume growth similarly admit\n$(d-1)$-dimensional annular separators when $d > 2$. We show that this fails in\na strong sense: For any $d \\geq 3$ and every $s \\geq 1$, there is a collection\nof interior-disjoint spheres in $\\mathbb{R}^d$ whose tangency graph $G$ has\nuniform polynomial growth, but such that all annular separators in $G$ have\ncardinality at least $R^s$.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 22:39:00 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Ebrahimnejad", "Farzam", ""], ["Lee", "James R.", ""]]}, {"id": "2107.09803", "submitter": "Michela Meister", "authors": "Michela Meister and Jon Kleinberg", "title": "Optimizing the order of actions in contact tracing", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contact tracing is a key tool for managing epidemic diseases like HIV,\ntuberculosis, and COVID-19. Manual investigations by human contact tracers\nremain a dominant way in which this is carried out. This process is limited by\nthe number of contact tracers available, who are often overburdened during an\noutbreak or epidemic. As a result, a crucial decision in any contact tracing\nstrategy is, given a set of contacts, which person should a tracer trace next?\nIn this work, we develop a formal model that articulates these questions and\nprovides a framework for comparing contact tracing strategies. Through\nanalyzing our model, we give provably optimal prioritization policies via a\nclean connection to a tool from operations research called a \"branching\nbandit\". Examining these policies gives qualitative insight into trade-offs in\ncontact tracing applications.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 23:19:22 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Meister", "Michela", ""], ["Kleinberg", "Jon", ""]]}, {"id": "2107.09885", "submitter": "Naoto Ohsaka", "authors": "Naoto Ohsaka", "title": "On Reconfigurability of Target Sets", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of deciding reconfigurability of target sets of a graph.\nGiven a graph $G$ with vertex thresholds $\\tau$, consider a dynamic process in\nwhich vertex $v$ becomes activated once at least $\\tau(v)$ of its neighbors are\nactivated. A vertex set $S$ is called a target set if all vertices of $G$ would\nbe activated when initially activating vertices of $S$. In the Target Set\nReconfiguration problem, given two target sets $X$ and $Y$ of the same size, we\nare required to determine whether $X$ can be transformed into $Y$ by repeatedly\nswapping one vertex in the current set with another vertex not in the current\nset preserving every intermediate set as a target set. In this paper, we\ninvestigate the complexity of Target Set Reconfiguration in restricted cases.\nOn the hardness side, we prove that Target Set Reconfiguration is\nPSPACE-complete on bipartite planar graphs of degree $3$ or $4$ and of\nthreshold $2$, bipartite $3$-regular graphs of threshold $1$ or $2$, and split\ngraphs, which is in contrast to the fact that a special case called Vertex\nCover Reconfiguration is in P for the last graph class. On the positive side,\nwe present a polynomial-time algorithm for Target Set Reconfiguration on graphs\nof maximum degree $2$ and trees. The latter result can be thought of as a\ngeneralization of that for Vertex Cover Reconfiguration.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 05:55:53 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Ohsaka", "Naoto", ""]]}, {"id": "2107.09930", "submitter": "Yi Lv", "authors": "Chao Wang, Gustavo Petri, Yi Lv, Teng Long, Zhiming Liu", "title": "Decidability of Liveness on the TSO Memory Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important property of concurrent objects is whether they support progress\n-a special case of liveness-guarantees, which ensure the termination of\nindividual method calls under system fairness assumptions. Liveness properties\nhave been proposed for concurrent objects. Typical liveness properties\nincludelock-freedom,wait-freedom,deadlock-freedom,starvation-freedom and\nobstruction-freedom. It is known that the five liveness properties above are\ndecidable on the Sequential Consistency (SC) memory model for a bounded number\nof processes. However, the problem of decidability of liveness for finite state\nconcurrent programs running on relaxed memory models remains open. In this\npaper we address this problem for the Total Store Order (TSO) memory model,as\nfound in the x86 architecture. We prove that lock-freedom,\nwait-freedom,deadlock-freedom and starvation-freedom are undecidable on TSO for\na bounded number of processes, while obstruction-freedom is decidable.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 08:08:51 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Wang", "Chao", ""], ["Petri", "Gustavo", ""], ["Lv", "Yi", ""], ["Long", "Teng", ""], ["Liu", "Zhiming", ""]]}, {"id": "2107.10078", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya, Cl\\'ement L. Canonne, Aditya Vikram Singh, and\n  Himanshu Tyagi", "title": "Optimal Rates for Nonparametric Density Estimation under Communication\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider density estimation for Besov spaces when each sample is quantized\nto only a limited number of bits. We provide a noninteractive adaptive\nestimator that exploits the sparsity of wavelet bases, along with a\nsimulate-and-infer technique from parametric estimation under communication\nconstraints. We show that our estimator is nearly rate-optimal by deriving\nminimax lower bounds that hold even when interactive protocols are allowed.\nInterestingly, while our wavelet-based estimator is almost rate-optimal for\nSobolev spaces as well, it is unclear whether the standard Fourier basis, which\narise naturally for those spaces, can be used to achieve the same performance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 13:43:44 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Singh", "Aditya Vikram", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "2107.10196", "submitter": "Raja Selvarajan Mr", "authors": "Raja Selvarajan, Vivek Dixit, Xingshan Cui, Travis S. Humble, and\n  Sabre Kais", "title": "Prime Factorization Using Quantum Variational Imaginary Time Evolution", "comments": "9 pages, 5 graphs, 1 circuit image, 1 supplementary page", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The road to computing on quantum devices has been accelerated by the promises\nthat come from using Shor's algorithm to reduce the complexity of prime\nfactorization. However, this promise hast not yet been realized due to noisy\nqubits and lack of robust error correction schemes. Here we explore a\npromising, alternative method for prime factorization that uses\nwell-established techniques from variational imaginary time evolution. We\ncreate a Hamiltonian whose ground state encodes the solution to the problem and\nuse variational techniques to evolve a state iteratively towards these prime\nfactors. We show that the number of circuits evaluated in each iteration scales\nas O(n^{5}d), where n is the bit-length of the number to be factorized and $d$\nis the depth of the circuit. We use a single layer of entangling gates to\nfactorize several numbers represented using 7, 8, and 9-qubit Hamiltonians. We\nalso verify the method's performance by implementing it on the IBMQ Lima\nhardware.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 19:22:46 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Selvarajan", "Raja", ""], ["Dixit", "Vivek", ""], ["Cui", "Xingshan", ""], ["Humble", "Travis S.", ""], ["Kais", "Sabre", ""]]}, {"id": "2107.10209", "submitter": "Alex Tang", "authors": "Pranjal Awasthi, Alex Tang, Aravindan Vijayaraghavan", "title": "Efficient Algorithms for Learning Depth-2 Neural Networks with General\n  ReLU Activations", "comments": "36 pages (including appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present polynomial time and sample efficient algorithms for learning an\nunknown depth-2 feedforward neural network with general ReLU activations, under\nmild non-degeneracy assumptions. In particular, we consider learning an unknown\nnetwork of the form $f(x) = {a}^{\\mathsf{T}}\\sigma({W}^\\mathsf{T}x+b)$, where\n$x$ is drawn from the Gaussian distribution, and $\\sigma(t) := \\max(t,0)$ is\nthe ReLU activation. Prior works for learning networks with ReLU activations\nassume that the bias $b$ is zero. In order to deal with the presence of the\nbias terms, our proposed algorithm consists of robustly decomposing multiple\nhigher order tensors arising from the Hermite expansion of the function $f(x)$.\nUsing these ideas we also establish identifiability of the network parameters\nunder minimal assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 17:06:03 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Tang", "Alex", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "2107.10450", "submitter": "Sutanu Gayen", "authors": "Arnab Bhattacharyya, Davin Choo, Rishikesh Gajjala, Sutanu Gayen,\n  Yuhao Wang", "title": "Learning Sparse Fixed-Structure Gaussian Bayesian Networks", "comments": "30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian Bayesian networks (a.k.a. linear Gaussian structural equation\nmodels) are widely used to model causal interactions among continuous\nvariables. In this work, we study the problem of learning a fixed-structure\nGaussian Bayesian network up to a bounded error in total variation distance. We\nanalyze the commonly used node-wise least squares regression (LeastSquares) and\nprove that it has a near-optimal sample complexity. We also study a couple of\nnew algorithms for the problem:\n  - BatchAvgLeastSquares takes the average of several batches of least squares\nsolutions at each node, so that one can interpolate between the batch size and\nthe number of batches. We show that BatchAvgLeastSquares also has near-optimal\nsample complexity.\n  - CauchyEst takes the median of solutions to several batches of linear\nsystems at each node. We show that the algorithm specialized to polytrees,\nCauchyEstTree, has near-optimal sample complexity.\n  Experimentally, we show that for uncontaminated, realizable data, the\nLeastSquares algorithm performs best, but in the presence of contamination or\nDAG misspecification, CauchyEst/CauchyEstTree and BatchAvgLeastSquares\nrespectively perform better.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 04:17:46 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Choo", "Davin", ""], ["Gajjala", "Rishikesh", ""], ["Gayen", "Sutanu", ""], ["Wang", "Yuhao", ""]]}, {"id": "2107.10454", "submitter": "Majid Farhadi", "authors": "Majid Farhadi, Alejandro Toriello, Prasad Tetali", "title": "The Traveling Firefighter Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the $L_p$ Traveling Salesman Problem ($L_p$-TSP), given by an\norigin, a set of destinations, and underlying distances. The objective is to\nschedule a destination visit sequence for a traveler of unit speed to minimize\nthe Minkowski $p$-norm of the resulting vector of visit/service times. For $p =\n\\infty$ the problem becomes a path variant of the TSP, and for $p = 1$ it\ndefines the Traveling Repairman Problem (TRP), both at the center of classical\ncombinatorial optimization. We provide an approximation preserving\npolynomial-time reduction of $L_p$-TSP to the segmented-TSP Problem [Sitters\n'14] and further study the case of $p = 2$, which we term the Traveling\nFirefighter Problem (TFP), when the cost due to a delay in service is quadratic\nin time.\n  We also study the all-norm-TSP problem [Golovin et al. '08], in which the\nobjective is to find a route that is (approximately) optimal with respect to\nthe minimization of any norm of the visit times, and improve corresponding\n(in)approximability bounds on metric spaces.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 04:55:45 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Farhadi", "Majid", ""], ["Toriello", "Alejandro", ""], ["Tetali", "Prasad", ""]]}, {"id": "2107.10516", "submitter": "David Wajc", "authors": "Kristen Kessel, Amin Saberi, Ali Shameli, David Wajc", "title": "The Stationary Prophet Inequality Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a continuous and infinite time horizon counterpart to the classic\nprophet inequality, which we term the stationary prophet inequality problem.\nHere, copies of a good arrive and perish according to Poisson point processes.\nBuyers arrive similarly and make take-it-or-leave-it offers for unsold items.\nThe objective is to maximize the (infinite) time average revenue of the seller.\n  Our main results are pricing-based policies which (i) achieve a\n$1/2$-approximation of the optimal offline policy, which is best possible, and\n(ii) achieve a better than $(1-1/e)$-approximation of the optimal online\npolicy. Result (i) improves upon bounds implied by recent work of Collina et\nal. (WINE'20), and is the first optimal prophet inequality for a stationary\nproblem. Result (ii) improves upon a $1-1/e$ bound implied by recent work of\nAouad and Sarita\\c{c} (EC'20), and shows that this prevalent bound in online\nalgorithms is not optimal for this problem.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 08:19:46 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Kessel", "Kristen", ""], ["Saberi", "Amin", ""], ["Shameli", "Ali", ""], ["Wajc", "David", ""]]}, {"id": "2107.10654", "submitter": "Matthew Fahrbach", "authors": "Matthew Fahrbach, Mehrdad Ghadiri, Thomas Fu", "title": "Fast Low-Rank Tensor Decomposition by Ridge Leverage Score Sampling", "comments": "29 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank tensor decomposition generalizes low-rank matrix approximation and\nis a powerful technique for discovering low-dimensional structure in\nhigh-dimensional data. In this paper, we study Tucker decompositions and use\ntools from randomized numerical linear algebra called ridge leverage scores to\naccelerate the core tensor update step in the widely-used alternating least\nsquares (ALS) algorithm. Updating the core tensor, a severe bottleneck in ALS,\nis a highly-structured ridge regression problem where the design matrix is a\nKronecker product of the factor matrices. We show how to use approximate ridge\nleverage scores to construct a sketched instance for any ridge regression\nproblem such that the solution vector for the sketched problem is a\n$(1+\\varepsilon)$-approximation to the original instance. Moreover, we show\nthat classical leverage scores suffice as an approximation, which then allows\nus to exploit the Kronecker structure and update the core tensor in time that\ndepends predominantly on the rank and the sketching parameters (i.e., sublinear\nin the size of the input tensor). We also give upper bounds for ridge leverage\nscores as rows are removed from the design matrix (e.g., if the tensor has\nmissing entries), and we demonstrate the effectiveness of our approximate ridge\nregressioni algorithm for large, low-rank Tucker decompositions on both\nsynthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 13:32:47 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Fahrbach", "Matthew", ""], ["Ghadiri", "Mehrdad", ""], ["Fu", "Thomas", ""]]}, {"id": "2107.10675", "submitter": "Henrik Leopold", "authors": "Jan Mendling, Beno\\^it Depaire, Henrik Leopold", "title": "Theory and Practice of Algorithm Engineering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an ongoing debate in computer science how algorithms should best be\nstudied. Some scholars have argued that experimental evaluations should be\nconducted, others emphasize the benefits of formal analysis. We believe that\nthis debate less of a question of either-or, because both views can be\nintegrated into an overarching framework. It is the ambition of this paper to\ndevelop such a framework of algorithm engineering with a theoretical foundation\nin the philosophy of science. We take the empirical nature of algorithm\nengineering as a starting point. Our theoretical framework builds on three\nareas discussed in the philosophy of science: ontology, epistemology and\nmethodology. In essence, ontology describes algorithm engineering as being\nconcerned with algorithmic problems, algorithmic tasks, algorithm designs and\nalgorithm implementations. Epistemology describes the body of knowledge of\nalgorithm engineering as a collection of prescriptive and descriptive\nknowledge, residing in World 3 of Popper's Three Worlds model. Methodology\nrefers to the steps how we can systematically enhance our knowledge of specific\nalgorithms. In this context, we identified seven validity concerns and discuss\nhow researchers can respond to falsification. Our framework has important\nimplications for researching algorithms in various areas of computer science.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 14:32:33 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Mendling", "Jan", ""], ["Depaire", "Beno\u00eet", ""], ["Leopold", "Henrik", ""]]}, {"id": "2107.10689", "submitter": "Peter Zeman", "authors": "Vikraman Arvind, Roman Nedela, Ilia Ponomarenko, Peter Zeman", "title": "Testing isomorphism of chordal graphs of bounded leafage is\n  fixed-parameter tractable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that testing isomorphism of chordal graphs is as hard as the\ngeneral graph isomorphism problem. Every chordal graph can be represented as\nthe intersection graph of some subtrees of a tree. The leafage of a chordal\ngraph, is defined to be the minimum number of leaves in the representing tree.\nWe construct a fixed-parameter tractable algorithm testing isomorphism of\nchordal graphs with bounded leafage. The key point is a fixed-parameter\ntractable algorithm finding the automorphism group of a colored order-3\nhypergraph with bounded sizes of color classes of vertices.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 13:52:58 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Arvind", "Vikraman", ""], ["Nedela", "Roman", ""], ["Ponomarenko", "Ilia", ""], ["Zeman", "Peter", ""]]}, {"id": "2107.10764", "submitter": "Naixu Guo", "authors": "Naixu Guo, Kosuke Mitarai, Keisuke Fujii", "title": "Nonlinear transformation of complex amplitudes via quantum singular\n  value transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the linearity of quantum operations, it is not straightforward to\nimplement nonlinear transformations on a quantum computer, making some\npractical tasks like a neural network hard to be achieved. In this work, we\ndefine a task called nonlinear transformation of complex amplitudes and provide\nan algorithm to achieve this task. Specifically, we construct a block-encoding\nof complex amplitudes from a state preparation oracle. This allows us to\ntransform the complex amplitudes by using quantum singular value\ntransformation. We evaluate the required overhead in terms of input dimension\nand precision, which reveals that the algorithm depends on the roughly square\nroot of input dimension and achieves an exponential speedup on precision\ncompared with previous work. We also discuss its possible applications to\nquantum machine learning, where complex amplitudes encoding classical or\nquantum data are processed by the proposed method. This paper provides a\npromising way to introduce highly complex nonlinearity of the quantum states,\nwhich is essentially missing in quantum mechanics.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 15:47:50 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Guo", "Naixu", ""], ["Mitarai", "Kosuke", ""], ["Fujii", "Keisuke", ""]]}, {"id": "2107.10777", "submitter": "Vijay Vazirani", "authors": "Vijay V. Vazirani", "title": "Randomized Online Algorithms for Adwords", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.GT math.CO math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The general adwords problem has remained largely unresolved. We define a\nsubcase called {\\em $k$-TYPICAL}, $k \\in \\Zplus$, as follows: the total budget\nof all the bidders is sufficient to buy $k$ bids for each bidder. This seems a\nreasonable assumption for a ``typical'' instance, at least for moderate values\nof $k$. We give a randomized online algorithm achieving a competitive ratio of\n$\\left(1 - {1 \\over e} - {1 \\over k} \\right) $ for this problem. We also give\nrandomized online algorithms for other special cases of adwords.\n  The key to these results is a simplification of the proof for RANKING, the\noptimal algorithm for online bipartite matching, given in \\cite{KVV}. Our\nalgorithms for adwords can be seen as natural extensions of RANKING.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 16:09:33 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Vazirani", "Vijay V.", ""]]}, {"id": "2107.10870", "submitter": "Satchit Sivakumar", "authors": "Mark Bun, Marco Gaboardi, Satchit Sivakumar", "title": "Multiclass versus Binary Differentially Private PAC Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show a generic reduction from multiclass differentially private PAC\nlearning to binary private PAC learning. We apply this transformation to a\nrecently proposed binary private PAC learner to obtain a private multiclass\nlearner with sample complexity that has a polynomial dependence on the\nmulticlass Littlestone dimension and a poly-logarithmic dependence on the\nnumber of classes. This yields an exponential improvement in the dependence on\nboth parameters over learners from previous work. Our proof extends the notion\nof $\\Psi$-dimension defined in work of Ben-David et al. [JCSS '95] to the\nonline setting and explores its general properties.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 18:06:39 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Bun", "Mark", ""], ["Gaboardi", "Marco", ""], ["Sivakumar", "Satchit", ""]]}, {"id": "2107.11053", "submitter": "Guanting Chen", "authors": "Guanting Chen, Johann Demetrio Gaebler, Matt Peng, Chunlin Sun, Yinyu\n  Ye", "title": "An Adaptive State Aggregation Algorithm for Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value iteration is a well-known method of solving Markov Decision Processes\n(MDPs) that is simple to implement and boasts strong theoretical convergence\nguarantees. However, the computational cost of value iteration quickly becomes\ninfeasible as the size of the state space increases. Various methods have been\nproposed to overcome this issue for value iteration in large state and action\nspace MDPs, often at the price, however, of generalizability and algorithmic\nsimplicity. In this paper, we propose an intuitive algorithm for solving MDPs\nthat reduces the cost of value iteration updates by dynamically grouping\ntogether states with similar cost-to-go values. We also prove that our\nalgorithm converges almost surely to within \\(2\\varepsilon / (1 - \\gamma)\\) of\nthe true optimal value in the \\(\\ell^\\infty\\) norm, where \\(\\gamma\\) is the\ndiscount factor and aggregated states differ by at most \\(\\varepsilon\\).\nNumerical experiments on a variety of simulated environments confirm the\nrobustness of our algorithm and its ability to solve MDPs with much cheaper\nupdates especially as the scale of the MDP problem increases.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 07:19:43 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Chen", "Guanting", ""], ["Gaebler", "Johann Demetrio", ""], ["Peng", "Matt", ""], ["Sun", "Chunlin", ""], ["Ye", "Yinyu", ""]]}, {"id": "2107.11333", "submitter": "Shaojie Tang", "authors": "Shaojie Tang", "title": "Robust Adaptive Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most of existing studies on adaptive submodular optimization focus on the\naverage-case, i.e., their objective is to find a policy that maximizes the\nexpected utility over a known distribution of realizations. However, a policy\nthat has a good average-case performance may have very poor performance under\nthe worst-case realization. In this study, we propose to study two variants of\nadaptive submodular optimization problems, namely, worst-case adaptive\nsubmodular maximization and robust submodular maximization. The first problem\naims to find a policy that maximizes the worst-case utility and the latter one\naims to find a policy, if any, that achieves both near optimal average-case\nutility and worst-case utility simultaneously. We introduce a new class of\nstochastic functions, called \\emph{worst-case submodular function}. For the\nworst-case adaptive submodular maximization problem subject to a $p$-system\nconstraint, we develop an adaptive worst-case greedy policy that achieves a\n$\\frac{1}{p+1}$ approximation ratio against the optimal worst-case utility if\nthe utility function is worst-case submodular. For the robust adaptive\nsubmodular maximization problem subject to a cardinality constraint, if the\nutility function is both worst-case submodular and adaptive submodular, we\ndevelop a hybrid adaptive policy that achieves an approximation close to\n$1-e^{-\\frac{1}{2}}$ under both worst case setting and average case setting\nsimultaneously. We also describe several applications of our theoretical\nresults, including pool-base active learning, stochastic submodular set cover\nand adaptive viral marketing.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 16:22:50 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 03:11:31 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Tang", "Shaojie", ""]]}, {"id": "2107.11440", "submitter": "Mikl\\'os Cs\\H{u}r\\\"os", "authors": "Miklos Csuros", "title": "Gain-loss-duplication models on a phylogeny: exact algorithms for\n  computing the likelihood and its gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gene gain-loss-duplication models are commonly based on continuous-time\nbirth-death processes. Employed in a phylogenetic context, such models have\nbeen increasingly popular in studies of gene content evolution across multiple\ngenomes. While the applications are becoming more varied and demanding,\nbioinformatics methods for probabilistic inference on copy numbers (or\ninteger-valued evolutionary characters, in general) are scarce. We describe a\nflexible probabilistic framework for phylogenetic gene-loss-duplication models.\nThe framework is based on a novel elementary representation by dependent random\nvariables with well-characterized conditional distributions: binomial, P\\'olya\n(negative binomial), and Poisson. The corresponding graphical model yields\nexact numerical procedures for computing the likelihood and the posterior\ndistribution of ancestral copy numbers. The resulting algorithms take quadratic\ntime in the total number of copies. In addition, we show how the likelihood\ngradient can be computed by a linear-time algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 19:48:49 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Csuros", "Miklos", ""]]}, {"id": "2107.11526", "submitter": "Uri Stemmer", "authors": "Menachem Sadigurschi, Uri Stemmer", "title": "On the Sample Complexity of Privately Learning Axis-Aligned Rectangles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the fundamental problem of learning Axis-Aligned-Rectangles over a\nfinite grid $X^d\\subseteq{\\mathbb{R}}^d$ with differential privacy. Existing\nresults show that the sample complexity of this problem is at most $\\min\\left\\{\nd{\\cdot}\\log|X| \\;,\\; d^{1.5}{\\cdot}\\left(\\log^*|X| \\right)^{1.5}\\right\\}$.\nThat is, existing constructions either require sample complexity that grows\nlinearly with $\\log|X|$, or else it grows super linearly with the dimension\n$d$. We present a novel algorithm that reduces the sample complexity to only\n$\\tilde{O}\\left\\{d{\\cdot}\\left(\\log^*|X|\\right)^{1.5}\\right\\}$, attaining a\ndimensionality optimal dependency without requiring the sample complexity to\ngrow with $\\log|X|$.The technique used in order to attain this improvement\ninvolves the deletion of \"exposed\" data-points on the go, in a fashion designed\nto avoid the cost of the adaptive composition theorems. The core of this\ntechnique may be of individual interest, introducing a new method for\nconstructing statistically-efficient private algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 04:06:11 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Sadigurschi", "Menachem", ""], ["Stemmer", "Uri", ""]]}, {"id": "2107.11530", "submitter": "Sandip Sinha", "authors": "Xi Chen, Anindya De, Chin Ho Lee, Rocco A. Servedio, Sandip Sinha", "title": "Near-Optimal Average-Case Approximate Trace Reconstruction from Few\n  Traces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the standard trace reconstruction problem, the goal is to \\emph{exactly}\nreconstruct an unknown source string $\\mathsf{x} \\in \\{0,1\\}^n$ from\nindependent \"traces\", which are copies of $\\mathsf{x}$ that have been corrupted\nby a $\\delta$-deletion channel which independently deletes each bit of\n$\\mathsf{x}$ with probability $\\delta$ and concatenates the surviving bits. We\nstudy the \\emph{approximate} trace reconstruction problem, in which the goal is\nonly to obtain a high-accuracy approximation of $\\mathsf{x}$ rather than an\nexact reconstruction.\n  We give an efficient algorithm, and a near-matching lower bound, for\napproximate reconstruction of a random source string $\\mathsf{x} \\in \\{0,1\\}^n$\nfrom few traces. Our main algorithmic result is a polynomial-time algorithm\nwith the following property: for any deletion rate $0 < \\delta < 1$ (which may\ndepend on $n$), for almost every source string $\\mathsf{x} \\in \\{0,1\\}^n$,\ngiven any number $M \\leq \\Theta(1/\\delta)$ of traces from\n$\\mathrm{Del}_\\delta(\\mathsf{x})$, the algorithm constructs a hypothesis string\n$\\widehat{\\mathsf{x}}$ that has edit distance at most $n \\cdot (\\delta\nM)^{\\Omega(M)}$ from $\\mathsf{x}$. We also prove a near-matching\ninformation-theoretic lower bound showing that given $M \\leq \\Theta(1/\\delta)$\ntraces from $\\mathrm{Del}_\\delta(\\mathsf{x})$ for a random $n$-bit string\n$\\mathsf{x}$, the smallest possible expected edit distance that any algorithm\ncan achieve, regardless of its running time, is $n \\cdot (\\delta M)^{O(M)}$.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 04:33:07 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Chen", "Xi", ""], ["De", "Anindya", ""], ["Lee", "Chin Ho", ""], ["Servedio", "Rocco A.", ""], ["Sinha", "Sandip", ""]]}, {"id": "2107.11712", "submitter": "Sutanu Gayen", "authors": "Arnab Bhattacharyya, Sutanu Gayen, Saravanan Kandasamy, Vedant Raval,\n  N. V. Vinodchandran", "title": "Efficient inference of interventional distributions", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of efficiently inferring interventional distributions\nin a causal Bayesian network from a finite number of observations. Let\n$\\mathcal{P}$ be a causal model on a set $\\mathbf{V}$ of observable variables\non a given causal graph $G$. For sets $\\mathbf{X},\\mathbf{Y}\\subseteq\n\\mathbf{V}$, and setting ${\\bf x}$ to $\\mathbf{X}$, let $P_{\\bf x}(\\mathbf{Y})$\ndenote the interventional distribution on $\\mathbf{Y}$ with respect to an\nintervention ${\\bf x}$ to variables ${\\bf x}$. Shpitser and Pearl (AAAI 2006),\nbuilding on the work of Tian and Pearl (AAAI 2001), gave an exact\ncharacterization of the class of causal graphs for which the interventional\ndistribution $P_{\\bf x}({\\mathbf{Y}})$ can be uniquely determined. We give the\nfirst efficient version of the Shpitser-Pearl algorithm. In particular, under\nnatural assumptions, we give a polynomial-time algorithm that on input a causal\ngraph $G$ on observable variables $\\mathbf{V}$, a setting ${\\bf x}$ of a set\n$\\mathbf{X} \\subseteq \\mathbf{V}$ of bounded size, outputs succinct\ndescriptions of both an evaluator and a generator for a distribution $\\hat{P}$\nthat is $\\varepsilon$-close (in total variation distance) to $P_{\\bf\nx}({\\mathbf{Y}})$ where $Y=\\mathbf{V}\\setminus \\mathbf{X}$, if $P_{\\bf\nx}(\\mathbf{Y})$ is identifiable. We also show that when $\\mathbf{Y}$ is an\narbitrary set, there is no efficient algorithm that outputs an evaluator of a\ndistribution that is $\\varepsilon$-close to $P_{\\bf x}({\\mathbf{Y}})$ unless\nall problems that have statistical zero-knowledge proofs, including the Graph\nIsomorphism problem, have efficient randomized algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 02:40:01 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 15:14:09 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Gayen", "Sutanu", ""], ["Kandasamy", "Saravanan", ""], ["Raval", "Vedant", ""], ["Vinodchandran", "N. V.", ""]]}, {"id": "2107.11784", "submitter": "Tapani Toivonen Dr.", "authors": "Tapani Toivonen", "title": "Power of human-algorithm collaboration in solving combinatorial\n  optimization problems", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many combinatorial optimization problems are often considered intractable to\nsolve exactly or by approximation. An example of such problem is maximum clique\nwhich -- under standard assumptions in complexity theory -- cannot be solved in\nsub-exponential time or be approximated within polynomial factor efficiently.\nWe show that if a polynomial time algorithm can query informative Gaussian\npriors from an expert $poly(n)$ times, then a class of combinatorial\noptimization problems can be solved efficiently in expectation up to a\nmultiplicative factor $\\epsilon$ where $\\epsilon$ is arbitrary constant. While\nour proposed methods are merely theoretical, they cast new light on how to\napproach solving these problems that have been usually considered intractable.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 11:21:59 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Toivonen", "Tapani", ""]]}, {"id": "2107.11839", "submitter": "Albert Cheu", "authors": "Albert Cheu", "title": "Differential Privacy in the Shuffle Model: A Survey of Separations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Differential privacy is often studied in one of two models. In the central\nmodel, a single analyzer has the responsibility of performing a\nprivacy-preserving computation on data. But in the local model, each data owner\nensures their own privacy. Although it removes the need to trust the analyzer,\nlocal privacy comes at a price: a locally private protocol is less accurate\nthan a centrally private counterpart when solving many learning and estimation\nproblems. Protocols in the shuffle model are designed to attain the best of\nboth worlds: recent work has shown high accuracy is possible with only a mild\ntrust assumption. This survey paper gives an overview of novel shuffle\nprotocols, along with lower bounds that establish the limits of the new model.\nWe also summarize work that show the promise of interactivity in the shuffle\nmodel.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 16:40:21 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Cheu", "Albert", ""]]}, {"id": "2107.11886", "submitter": "Jay Mardia", "authors": "Jay Mardia", "title": "Logspace Reducibility From Secret Leakage Planted Clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The planted clique problem is well-studied in the context of observing,\nexplaining, and predicting interesting computational phenomena associated with\nstatistical problems. When equating computational efficiency with the existence\nof polynomial time algorithms, the computational hardness of (some variant of)\nthe planted clique problem can be used to infer the computational hardness of a\nhost of other statistical problems.\n  Is this ability to transfer computational hardness from (some variant of) the\nplanted clique problem to other statistical problems robust to changing our\nnotion of computational efficiency to space efficiency?\n  We answer this question affirmatively for three different statistical\nproblems, namely Sparse PCA, submatrix detection, and testing almost k-wise\nindependence. The key challenge is that space efficient randomized reductions\nneed to repeatedly access the randomness they use. Known reductions to these\nproblems are all randomized and need polynomially many random bits to\nimplement. Since we can not store polynomially many random bits in memory, it\nis unclear how to implement these existing reductions space efficiently. There\nare two ideas involved in circumventing this issue and implementing known\nreductions to these problems space efficiently.\n  1. When solving statistical problems, we can use parts of the input itself as\nrandomness.\n  2. Secret leakage variants of the planted clique problem with appropriate\nsecret leakage can be more useful than the standard planted clique problem when\nwe want to use parts of the input as randomness.\n  (abstract shortened due to arxiv constraints)\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 20:33:15 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Mardia", "Jay", ""]]}, {"id": "2107.12245", "submitter": "Radovan \\v{C}erven\\'y", "authors": "Radovan \\v{C}erven\\'y, Pratibha Choudhary, Ond\\v{r}ej Such\\'y", "title": "On Kernels for d-Path Vertex Cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the kernelization of $d$-Path Vertex Cover ($d$-PVC)\nproblem. Given a graph $G$, the problem requires finding whether there exists a\nset of at most $k$ vertices whose removal from $G$ results in a graph that does\nnot contain a path (not necessarily induced) of length $d$. It is known that\n$d$-PVC is NP-complete for $d \\geq 2$. Since the problem generalizes to\n$d$-Hitting Set, it is known to admit a kernel of size $(2d-1)k^{d-1}+k$. We\nimprove on this by giving better kernels. Specifically, we give $O(k^2)$ size\n(vertices and edges) kernels for the cases when $d = 4$ and $d = 5$. Further,\nwe give an $O(k^3)$ size kernel for $d$-PVC.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 14:48:47 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["\u010cerven\u00fd", "Radovan", ""], ["Choudhary", "Pratibha", ""], ["Such\u00fd", "Ond\u0159ej", ""]]}, {"id": "2107.12267", "submitter": "Amer Mouawad", "authors": "Alexandre Cooper, Stephanie Maaz, Amer E.Mouawad, Naomi Nishimura", "title": "Parameterized complexity of reconfiguration of atoms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work is motivated by the challenges presented in preparing arrays of\natoms for use in quantum simulation. The recently-developed process of loading\natoms into traps results in approximately half of the traps being filled. To\nconsolidate the atoms so that they form a dense and regular arrangement, such\nas all locations in a grid, atoms are rearranged using moving optical tweezers.\nTime is of the essence, as the longer that the process takes and the more that\natoms are moved, the higher the chance that atoms will be lost in the process.\n  Viewed as a problem on graphs, we wish to solve the problem of reconfiguring\none arrangement of tokens (representing atoms) to another using as few moves as\npossible. Because the problem is NP-complete on general graphs as well as on\ngrids, we focus on the parameterized complexity for various parameters,\nconsidering both undirected and directed graphs, and tokens with and without\nlabels. For unlabelled tokens, the problem is in FPT when parameterizing by the\nnumber of tokens, the number of moves, or the number of moves plus the number\nof vertices without tokens in either the source or target configuration, but\nintractable when parameterizing by the difference between the number of moves\nand the number of differences in the placement of tokens in the source and\ntarget configurations. When labels are added to tokens, however, most of the\ntractability results are replaced by hardness results.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 15:16:50 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Cooper", "Alexandre", ""], ["Maaz", "Stephanie", ""], ["Mouawad", "Amer E.", ""], ["Nishimura", "Naomi", ""]]}, {"id": "2107.12332", "submitter": "Vitaly Aksenov", "authors": "Vitaly Aksenov", "title": "Overview of Bachelors Theses 2021", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we review Bachelors Theses done under the supervision of Vitaly\nAksenov at ITMO University. This overview contains the short description of six\ntheses: \"Development of a Streaming Algorithm for the Decomposition of Graph\nMetrics to Tree Metrics\" by Oleg Fafurin, \"Development of Memory-friendly\nConcurrent Data Structures\" by Roman Smirnov, \"Theoretical Analysis of the\nPerformance of Concurrent Data Structures\" by Daniil Bolotov, \"Parallel Batched\nInterpolation Search Tree\" by Alena Martsenyuk, \"Parallel Batched\nSelf-adjusting Data Structures\" by Vitalii Krasnov, and \"Parallel Batched\nPersistent Binary Search Trees\" by Ildar Zinatulin.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 17:15:25 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Aksenov", "Vitaly", ""]]}, {"id": "2107.12367", "submitter": "Shivam Nadimpalli", "authors": "Anindya De, Shivam Nadimpalli, Rocco A. Servedio", "title": "Approximating Sumset Size", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a subset $A$ of the $n$-dimensional Boolean hypercube $\\mathbb{F}_2^n$,\nthe sumset $A+A$ is the set $\\{a+a': a, a' \\in A\\}$ where addition is in\n$\\mathbb{F}_2^n$. Sumsets play an important role in additive combinatorics,\nwhere they feature in many central results of the field.\n  The main result of this paper is a sublinear-time algorithm for the problem\nof sumset size estimation. In more detail, our algorithm is given oracle access\nto (the indicator function of) an arbitrary $A \\subseteq \\mathbb{F}_2^n$ and an\naccuracy parameter $\\epsilon > 0$, and with high probability it outputs a value\n$0 \\leq v \\leq 1$ that is $\\pm \\epsilon$-close to $\\mathrm{Vol}(A' + A')$ for\nsome perturbation $A' \\subseteq A$ of $A$ satisfying $\\mathrm{Vol}(A \\setminus\nA') \\leq \\epsilon.$ It is easy to see that without the relaxation of dealing\nwith $A'$ rather than $A$, any algorithm for estimating $\\mathrm{Vol}(A+A)$ to\nany nontrivial accuracy must make $2^{\\Omega(n)}$ queries. In contrast, we give\nan algorithm whose query complexity depends only on $\\epsilon$ and is\ncompletely independent of the ambient dimension $n$.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 17:59:23 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["De", "Anindya", ""], ["Nadimpalli", "Shivam", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "2107.12373", "submitter": "Sonia Cromp", "authors": "Sonia Cromp, Alireza Samadian, Kirk Pruhs", "title": "Relational Boosted Regression Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks use data housed in relational databases to train boosted\nregression tree models. In this paper, we give a relational adaptation of the\ngreedy algorithm for training boosted regression trees. For the subproblem of\ncalculating the sum of squared residuals of the dataset, which dominates the\nruntime of the boosting algorithm, we provide a $(1 + \\epsilon)$-approximation\nusing the tensor sketch technique. Employing this approximation within the\nrelational boosted regression trees algorithm leads to learning similar model\nparameters, but with asymptotically better runtime.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 20:29:28 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Cromp", "Sonia", ""], ["Samadian", "Alireza", ""], ["Pruhs", "Kirk", ""]]}, {"id": "2107.12973", "submitter": "Kshitij Gajjar", "authors": "Henning Fernau and Kshitij Gajjar", "title": "The Space Complexity of Sum Labelling", "comments": "24 pages, to be presented at FCT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A graph is called a sum graph if its vertices can be labelled by distinct\npositive integers such that there is an edge between two vertices if and only\nif the sum of their labels is the label of another vertex of the graph. Most\npapers on sum graphs consider combinatorial questions like the minimum number\nof isolated vertices that need to be added to a given graph to make it a sum\ngraph. In this paper, we initiate the study of sum graphs from the viewpoint of\ncomputational complexity. Notice that every $n$-vertex sum graph can be\nrepresented by a sorted list of $n$ positive integers where edge queries can be\nanswered in $O(\\log n)$ time. Therefore, limiting the size of the vertex labels\nalso upper-bounds the space complexity of storing the graph in the database.\n  We show that every $n$-vertex, $m$-edge, $d$-degenerate graph can be made a\nsum graph by adding at most $m$ isolated vertices to it, such that the size of\neach vertex label is at most $O(n^2d)$. This enables us to store the graph\nusing $O(m\\log n)$ bits of memory. For sparse graphs (graphs with $O(n)$\nedges), this matches the trivial lower bound of $\\Omega(n\\log n)$. Since planar\ngraphs and forests have constant degeneracy, our result implies an upper bound\nof $O(n^2)$ on their label size. The previously best known upper bound on the\nlabel size of general graphs with the minimum number of isolated vertices was\n$O(4^n)$, due to Kratochv\\'il, Miller & Nguyen. Furthermore, their proof was\nexistential, whereas our labelling can be constructed in polynomial time.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 17:34:07 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Fernau", "Henning", ""], ["Gajjar", "Kshitij", ""]]}, {"id": "2107.13071", "submitter": "Fran\\c{c}ois Sellier", "authors": "Chien-Chung Huang, Fran\\c{c}ois Sellier", "title": "Semi-Streaming Algorithms for Submodular Function Maximization under\n  $b$-Matching Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of maximizing a non-negative submodular function\nunder the $b$-matching constraint, in the semi-streaming model. When the\nfunction is linear, monotone, and non-monotone, we obtain the approximation\nratios of $2+\\varepsilon$, $3 + 2 \\sqrt{2} \\approx 5.828$, and $4 + 2 \\sqrt{3}\n\\approx 7.464$, respectively. We also consider a generalized problem, where a\n$k$-uniform hypergraph is given, along with an extra matroid constraint imposed\non the edges, with the same goal of finding a $b$-matching that maximizes a\nsubmodular function. We extend our technique to this case to obtain an\nalgorithm with an approximation of $\\frac{8}{3}k+O(1)$.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 20:15:59 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Huang", "Chien-Chung", ""], ["Sellier", "Fran\u00e7ois", ""]]}, {"id": "2107.13206", "submitter": "Karl Bringmann", "authors": "Karl Bringmann and Vasileios Nakos", "title": "Top-k-Convolution and the Quest for Near-Linear Output-Sensitive Subset\n  Sum", "comments": "40 pages, full version of STOC'20 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical Subset Sum problem we are given a set $X$ and a target $t$,\nand the task is to decide whether there exists a subset of $X$ which sums to\n$t$. A recent line of research has resulted in $\\tilde{O}(t)$-time algorithms,\nwhich are (near-)optimal under popular complexity-theoretic assumptions. On the\nother hand, the standard dynamic programming algorithm runs in time $O(n \\cdot\n|\\mathcal{S}(X,t)|)$, where $\\mathcal{S}(X,t)$ is the set of all subset sums of\n$X$ that are smaller than $t$. Furthermore, all known pseudopolynomial\nalgorithms actually solve a stronger task, since they actually compute the\nwhole set $\\mathcal{S}(X,t)$.\n  As the aforementioned two running times are incomparable, in this paper we\nask whether one can achieve the best of both worlds: running time\n$\\tilde{O}(|\\mathcal{S}(X,t)|)$. In particular, we ask whether\n$\\mathcal{S}(X,t)$ can be computed in near-linear time in the output-size.\nUsing a diverse toolkit containing techniques such as color coding, sparse\nrecovery, and sumset estimates, we make considerable progress towards this\nquestion and design an algorithm running in time\n$\\tilde{O}(|\\mathcal{S}(X,t)|^{4/3})$.\n  Central to our approach is the study of top-$k$-convolution, a natural\nproblem of independent interest: given sparse polynomials with non-negative\ncoefficients, compute the lowest $k$ non-zero monomials of their product. We\ndesign an algorithm running in time $\\tilde{O}(k^{4/3})$, by a combination of\nsparse convolution and sumset estimates considered in Additive Combinatorics.\nMoreover, we provide evidence that going beyond some of the barriers we have\nfaced requires either an algorithmic breakthrough or possibly new techniques\nfrom Additive Combinatorics on how to pass from information on restricted\nsumsets to information on unrestricted sumsets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 07:35:29 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Bringmann", "Karl", ""], ["Nakos", "Vasileios", ""]]}, {"id": "2107.13298", "submitter": "Julian Schwarz", "authors": "Tobias Harks and Julian Schwarz", "title": "Generalized Nash Equilibrium Problems with Mixed-Integer Variables", "comments": "28 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DM cs.DS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider generalized Nash equilibrium problems (GNEPs) with non-convex\nstrategy spaces and non-convex cost functions. This general class of games\nincludes the important case of games with mixed-integer variables for which\nonly a few results are known in the literature. We present a new approach to\ncharacterize equilibria via a convexification technique using the Nikaido-Isoda\nfunction. To any given instance $I$ of the GNEP, we derive a convexified\ninstance $I^\\text{conv}$ and show that every feasible strategy profile for $I$\nis an equilibrium if and only if it is an equilibrium for $I^\\text{conv}$ and\nthe convexified cost functions coincide with the initial ones. Based on this\ngeneral result we identify important classes of GNEPs which allow us to\nreformulate the equilibrium problem via standard optimization problems.\n  $1.$ First, quasi-linear GNEPs are introduced where for fixed strategies of\nthe opponent players, the cost function of every player is linear and the\nconvex hull of the respective strategy space is polyhedral. For this game class\nwe reformulate the equilibrium problem for $I^\\text{conv}$ as a standard\n(non-linear) optimization problem.\n  $2.$ Secondly, we study GNEPs with joint constraint sets. We introduce the\nnew class of projective-closed GNEPs for which we show that $I^\\text{conv}$\nfalls into the class of jointly convex GNEPs. As an important application, we\nshow that general GNEPs with shared binary sets $\\{0,1\\}^k$ are\nprojective-closed.\n  $3.$ Thirdly, we discuss the class of quasi-separable GNEPs in which roughly\nspeaking the players' cost functions depend on their own strategy only. We show\nthat they admit a special structure leading to a characterization of equilibria\nvia solutions of a convex optimization problem.\n  $4.$ Finally, we present numerical results regarding the computation of\nequilibria for a class of quasi-linear and projective-closed GNEPs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 11:45:20 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Harks", "Tobias", ""], ["Schwarz", "Julian", ""]]}, {"id": "2107.13309", "submitter": "Chhaya Trehan", "authors": "Michael Elkin, Chhaya Trehan", "title": "$(1+\\epsilon)$-Approximate Shortest Paths in Dynamic Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing approximate shortest paths in the dynamic streaming setting is a\nfundamental challenge that has been intensively studied during the last decade.\nCurrently existing solutions for this problem either build a sparse\nmultiplicative spanner of the input graph and compute shortest paths in the\nspanner offline, or compute an exact single source BFS tree.\n  Solutions of the first type are doomed to incur a stretch-space tradeoff of\n$2\\kappa-1$ versus $n^{1+1/\\kappa}$, for an integer parameter $\\kappa$. (In\nfact, existing solutions also incur an extra factor of $1+\\epsilon$ in the\nstretch for weighted graphs, and an additional factor of $\\log^{O(1)}n$ in the\nspace.) The only existing solution of the second type uses $n^{1/2 -\nO(1/\\kappa)}$ passes over the stream (for space $O(n^{1+1/\\kappa})$), and\napplies only to unweighted graphs.\n  In this paper we show that $(1+\\epsilon)$-approximate single-source shortest\npaths can be computed in this setting with $\\tilde{O}(n^{1+1/\\kappa})$ space\nusing just \\emph{constantly} many passes in unweighted graphs, and\npolylogarithmically many passes in weighted graphs (assuming $\\epsilon$ and\n$\\kappa$ are constant). Moreover, in fact, the same result applies for\nmulti-source shortest paths, as long as the number of sources is\n$O(n^{1/\\kappa})$.\n  We achieve these results by devising efficient dynamic streaming\nconstructions of $(1 + \\epsilon, \\beta)$-spanners and hopsets. We believe that\nthese constructions are of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 12:05:01 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Elkin", "Michael", ""], ["Trehan", "Chhaya", ""]]}, {"id": "2107.13344", "submitter": "Vasileios Nakos", "authors": "Dimitris Fotakis and Panagiotis Kostopanagiotis and Vasileios Nakos\n  and Georgios Piliouras and Stratis Skoulakis", "title": "On the Approximability of Multistage Min-Sum Set Cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the polynomial-time approximability of the multistage version\nof Min-Sum Set Cover ($\\mathrm{DSSC}$), a natural and intriguing generalization\nof the classical List Update problem. In $\\mathrm{DSSC}$, we maintain a\nsequence of permutations $(\\pi^0, \\pi^1, \\ldots, \\pi^T)$ on $n$ elements, based\non a sequence of requests $(R^1, \\ldots, R^T)$. We aim to minimize the total\ncost of updating $\\pi^{t-1}$ to $\\pi^{t}$, quantified by the Kendall tau\ndistance $\\mathrm{D}_{\\mathrm{KT}}(\\pi^{t-1}, \\pi^t)$, plus the total cost of\ncovering each request $R^t$ with the current permutation $\\pi^t$, quantified by\nthe position of the first element of $R^t$ in $\\pi^t$.\n  Using a reduction from Set Cover, we show that $\\mathrm{DSSC}$ does not admit\nan $O(1)$-approximation, unless $\\mathrm{P} = \\mathrm{NP}$, and that any\n$o(\\log n)$ (resp. $o(r)$) approximation to $\\mathrm{DSSC}$ implies a\nsublogarithmic (resp. $o(r)$) approximation to Set Cover (resp. where each\nelement appears at most $r$ times). Our main technical contribution is to show\nthat $\\mathrm{DSSC}$ can be approximated in polynomial-time within a factor of\n$O(\\log^2 n)$ in general instances, by randomized rounding, and within a factor\nof $O(r^2)$, if all requests have cardinality at most $r$, by deterministic\nrounding.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 13:18:05 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Fotakis", "Dimitris", ""], ["Kostopanagiotis", "Panagiotis", ""], ["Nakos", "Vasileios", ""], ["Piliouras", "Georgios", ""], ["Skoulakis", "Stratis", ""]]}, {"id": "2107.13492", "submitter": "Fabien Tricoire", "authors": "Fabien Tricoire", "title": "BROUTE: a benchmark suite for the implementation of standard vehicle\n  routing algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce BROUTE, a benchmark suite for vehicle routing optimization\nalgorithms. We define a selection of algorithms traditionally used in vehicle\nrouting optimization. They capture essential features that are also relevant in\noptimization algorithms for different application domains, like local search\nmove evaluation, memory allocation, dynamic programming, or insertion and\ndeletion from a list. Each algorithm is deterministic. We implement these\nbenchmark algorithms using a selection of programming languages and different\ndata structures. BROUTE is free, open-source, and can be used to inform early\ndecisions in projects that involve programming, such as which language to use.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 16:59:21 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Tricoire", "Fabien", ""]]}, {"id": "2107.13638", "submitter": "Sebastian Berndt", "authors": "Sebastian Berndt and Max A. Deppert and Klaus Jansen and Lars\n  Rohwedder", "title": "Load Balancing: The Long Road from Theory to Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a long history of approximation schemes for the problem of\nscheduling jobs on identical machines to minimize the makespan. Such a scheme\ngrants a $(1+\\epsilon)$-approximation solution for every $\\epsilon > 0$, but\nthe running time grows exponentially in $1/\\epsilon$. For a long time, these\nschemes seemed like a purely theoretical concept. Even solving instances for\nmoderate values of $\\epsilon$ seemed completely illusional. In an effort to\nbridge theory and practice, we refine recent ILP techniques to develop the\nfastest known approximation scheme for this problem. An implementation of this\nalgorithm reaches values of $\\epsilon$ lower than $2/11\\approx 18.2\\%$ within a\nreasonable timespan. This is the approximation guarantee of MULTIFIT, which, to\nthe best of our knowledge, has the best proven guarantee of any non-scheme\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 20:51:06 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Berndt", "Sebastian", ""], ["Deppert", "Max A.", ""], ["Jansen", "Klaus", ""], ["Rohwedder", "Lars", ""]]}, {"id": "2107.13658", "submitter": "Sergey Pupyrev", "authors": "Martin N\\\"ollenburg and Sergey Pupyrev", "title": "On Families of Planar DAGs with Constant Stack Number", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $k$-stack layout (or $k$-page book embedding) of a graph consists of a\ntotal order of the vertices, and a partition of the edges into $k$ sets of\nnon-crossing edges with respect to the vertex order. The stack number of a\ngraph is the minimum $k$ such that it admits a $k$-stack layout.\n  In this paper we study a long-standing problem regarding the stack number of\nplanar directed acyclic graphs (DAGs), for which the vertex order has to\nrespect the orientation of the edges. We investigate upper and lower bounds on\nthe stack number of several families of planar graphs: We prove constant upper\nbounds on the stack number of single-source and monotone outerplanar DAGs and\nof outerpath DAGs, and improve the constant upper bound for upward planar\n3-trees. Further, we provide computer-aided lower bounds for upward (outer-)\nplanar DAGs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 22:33:26 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["N\u00f6llenburg", "Martin", ""], ["Pupyrev", "Sergey", ""]]}, {"id": "2107.13801", "submitter": "Pranav Venkatram", "authors": "Pranav Venkatram", "title": "A New Lossless Data Compression Algorithm Exploiting Positional\n  Redundancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A new run length encoding algorithm for lossless data compression that\nexploits positional redundancy by representing data in a two-dimensional model\nof concentric circles is presented. This visual transform enables detection of\nruns (each of a different character) in which runs need not be contiguous and\nhence, is a generalization of run length encoding. Its advantages and drawbacks\nare characterized by comparing its performance with TurboRLE.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 07:59:12 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Venkatram", "Pranav", ""]]}, {"id": "2107.14080", "submitter": "Mobolaji Williams", "authors": "Mobolaji Williams", "title": "Large N limit of the knapsack problem", "comments": "18 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.DS math-ph math.MP math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the simplest formulation of the knapsack problem, one seeks to maximize\nthe total value of a collection of objects such that the total weight remains\nbelow a certain limit. In this work, we move from computer science to physics\nand formulate the knapsack problem as a statistical physics system and compute\nthe corresponding partition function. We approximate the result in the large\nnumber limit and from this approximation develop a new algorithm for the\nproblem. We compare the performance of this algorithm to that of other\napproximation algorithms, finding that the new algorithm is faster than most of\nthese approaches while still retaining high accuracy. From its speed and\naccuracy relationship, we argue that the algorithm is a manifestation of a\ngreedy algorithm. We conclude by discussing ways to extend the formalism to\nmake its underlying heuristics more rigorous or to apply the approach to other\ncombinatorial optimization problems. In all, this work exists at the\nintersection between computer science and statistical physics and represents a\nnew analytical approach to solving the problems in the former using methods of\nthe latter.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 15:24:02 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Williams", "Mobolaji", ""]]}, {"id": "2107.14126", "submitter": "Michail Theofilatos", "authors": "George B. Mertzios, Othon Michail, George Skretas, Paul G. Spirakis,\n  Michail Theofilatos", "title": "The Complexity of Growing a Graph", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by biological processes, we introduce here the model of growing\ngraphs, a new model of highly dynamic networks. Such networks have as nodes\nentities that can self-replicate and thus can expand the size of the network.\nThis gives rise to the problem of creating a target network $G$ starting from a\nsingle entity (node). To properly model this, we assume that every node $u$ can\ngenerate at most one node $v$ at every round (or time slot), and every\ngenerated node $v$ can activate edges with other nodes only at the time of its\nbirth, provided that these nodes are up to a small distance $d$ away from $v$.\nWe show that the most interesting case is when the distance is $d=2$. Edge\ndeletions are allowed at any time slot. This creates a natural balance between\nhow fast (time) and how efficiently (number of deleted edges) a target network\ncan be generated. A central question here is, given a target network $G$ of $n$\nnodes, can $G$ be constructed in the model of growing graphs in at most $k$\ntime slots and with at most $\\ell$ excess edges (i.e., auxiliary edges $\\notin\nE(G)$ that are activated and later deleted)? We consider here both centralized\nand distributed algorithms for such questions (and also their computational\ncomplexity). Our results include lower bounds based on properties of the target\nnetwork and algorithms for general graph classes that try to balance speed and\nefficiency. We then show that the optimal number of time slots to construct an\ninput target graph with zero-waste (i.e., no edge deletions allowed), is hard\neven to approximate within $n^{1-\\varepsilon}$, for any $\\varepsilon>0$, unless\nP=NP. On the contrary, the question of the feasibility of constructing a given\ntarget graph in $\\log n$ time slots and zero-waste, can be answered in\npolynomial time. Finally, we initiate a discussion on possible extensions for\nthis model for a distributed setting.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 15:59:53 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Mertzios", "George B.", ""], ["Michail", "Othon", ""], ["Skretas", "George", ""], ["Spirakis", "Paul G.", ""], ["Theofilatos", "Michail", ""]]}, {"id": "2107.14221", "submitter": "Lazar Milenkovic", "authors": "Omri Kahalon, Hung Le, Lazar Milenkovic, Shay Solomon", "title": "Can't See The Forest for the Trees: Navigating Metric Spaces by Bounded\n  Hop-Diameter Spanners", "comments": "Abstract truncated to fit arXiv limits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spanners for metric spaces have been extensively studied, both in general\nmetrics and in restricted classes, perhaps most notably in low-dimensional\nEuclidean spaces -- due to their numerous applications. Euclidean spanners can\nbe viewed as means of compressing the $\\binom{n}{2}$ pairwise distances of a\n$d$-dimensional Euclidean space into $O(n) = O_{\\epsilon,d}(n)$ spanner edges,\nso that the spanner distances preserve the original distances to within a\nfactor of $1+\\epsilon$, for any $\\epsilon > 0$. Moreover, one can compute such\nspanners in optimal $O(n \\log n)$ time. Once the spanner has been computed, it\nserves as a \"proxy\" overlay network, on which the computation can proceed,\nwhich gives rise to huge savings in space and other important quality measures.\n  On the negative side, by working on the spanner rather than the original\nmetric, one loses the key property of being able to efficiently \"navigate\"\nbetween pairs of points. While in the original metric, one can go from any\npoint to any other via a direct edge, it is unclear how to efficiently navigate\nin the spanner: How can we translate the existence of a \"good\" path into an\nefficient algorithm finding it? Moreover, usually by \"good\" path we mean a path\nwhose weight approximates the original distance between its endpoints -- but a\npriori the number of edges (or \"hops\") in the path could be huge. To control\nthe hop-length of paths, one can try to upper bound the spanner's hop-diameter,\nbut naturally bounded hop-diameter spanners are more complex than spanners with\nunbounded hop-diameter, which might render the algorithmic task of efficiently\nfinding good paths more challenging.\n  The original metric enables us to navigate optimally -- a single hop (for any\ntwo points) with the exact distance, but the price is high -- $\\Theta(n^2)$\nedges. [...]\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:54:30 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Kahalon", "Omri", ""], ["Le", "Hung", ""], ["Milenkovic", "Lazar", ""], ["Solomon", "Shay", ""]]}]