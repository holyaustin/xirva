[{"id": "1612.00034", "submitter": "John Wright", "authors": "Ryan O'Donnell and John Wright", "title": "Efficient quantum tomography II", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following [OW16], we continue our analysis of: (1) \"Quantum tomography\",\ni.e., learning a quantum state, i.e., the quantum generalization of learning a\ndiscrete probability distribution; (2) The distribution of Young diagrams\noutput by the RSK algorithm on random words. Regarding (2), we introduce two\npowerful new tools: (i) A precise upper bound on the expected length of the\nlongest union of $k$ disjoint increasing subsequences in a random length-$n$\nword with letter distribution $\\alpha_1 \\geq \\alpha_2 \\geq \\cdots \\geq\n\\alpha_d$; (ii) A new majorization property of the RSK algorithm that allows\none to analyze the Young diagram formed by the lower rows $\\lambda_k,\n\\lambda_{k+1}, \\dots$ of its output.\n  These tools allow us to prove several new theorems concerning the\ndistribution of random Young diagrams in the nonasymptotic regime, giving\nconcrete error bounds that are optimal, or nearly so, in all parameters. As one\nexample, we give a fundamentally new proof of the fact that the expected length\nof the longest increasing sequence in a random length-$n$ permutation is\nbounded by $2\\sqrt{n}$. This is the $k = 1$, $\\alpha_i \\equiv \\frac1d$, $d \\to\n\\infty$ special case of a much more general result we prove: the expected\nlength of the $k$th Young diagram row produced by an $\\alpha$-random word is\n$\\alpha_k n \\pm 2\\sqrt{\\alpha_kd n}$.\n  From our new analyses of random Young diagrams we derive several new results\nin quantum tomography, including: (i) Learning the eigenvalues of an unknown\nstate to $\\epsilon$-accuracy in Hellinger-squared, chi-squared, or KL distance,\nusing $n = O(d^2/\\epsilon)$ copies; (ii) Learning the optimal rank-$k$\napproximation of an unknown state to $\\epsilon$-fidelity (Hellinger-squared\ndistance) using $n = \\widetilde{O}(kd/\\epsilon)$ copies.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 21:19:44 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["O'Donnell", "Ryan", ""], ["Wright", "John", ""]]}, {"id": "1612.00156", "submitter": "Chao Xu", "authors": "Krist\\'of B\\'erczi, Karthekeyan Chandrasekaran, Tam\\'as Kir\\'aly,\n  Euiwoong Lee, Chao Xu", "title": "Global and fixed-terminal cuts in digraphs", "comments": "37 pages, 5 figures, APPROX 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity of multicut-like problems may vary significantly\ndepending on whether the terminals are fixed or not. In this work we present a\ncomprehensive study of this phenomenon in two types of cut problems in directed\ngraphs: double cut and bicut.\n  1. The fixed-terminal edge-weighted double cut is known to be solvable\nefficiently. We show a tight approximability factor of $2$ for the\nfixed-terminal node-weighted double cut. We show that the global node-weighted\ndouble cut cannot be approximated to a factor smaller than $3/2$ under the\nUnique Games Conjecture (UGC).\n  2. The fixed-terminal edge-weighted bicut is known to have a tight\napproximability factor of $2$. We show that the global edge-weighted bicut is\napproximable to a factor strictly better than $2$, and that the global\nnode-weighted bicut cannot be approximated to a factor smaller than $3/2$ under\nUGC.\n  3. In relation to these investigations, we also prove two results on\nundirected graphs which are of independent interest. First, we show\nNP-completeness and a tight inapproximability bound of $4/3$ for the\nnode-weighted $3$-cut problem. Second, we show that for constant $k$, there\nexists an efficient algorithm to solve the minimum $\\{s,t\\}$-separating $k$-cut\nproblem.\n  Our techniques for the algorithms are combinatorial, based on LPs and based\non enumeration of approximate min-cuts. Our hardness results are based on\ncombinatorial reductions and integrality gap instances.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 06:09:30 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 12:10:07 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["B\u00e9rczi", "Krist\u00f3f", ""], ["Chandrasekaran", "Karthekeyan", ""], ["Kir\u00e1ly", "Tam\u00e1s", ""], ["Lee", "Euiwoong", ""], ["Xu", "Chao", ""]]}, {"id": "1612.00190", "submitter": "Veerle Timmermans", "authors": "Veerle Timmermans and Tobias Harks", "title": "Equilibrium Computation in Resource Allocation Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the equilibrium computation problem for two classical resource\nallocation games: atomic splittable congestion games and multimarket Cournot\noligopolies. For atomic splittable congestion games with singleton strategies\nand player-specific affine cost functions, we devise the first polynomial time\nalgorithm computing a pure Nash equilibrium. Our algorithm is combinatorial and\ncomputes the exact equilibrium assuming rational input. The idea is to compute\nan equilibrium for an associated integrally-splittable singleton congestion\ngame in which the players can only split their demands in integral multiples of\na common packet size. While integral games have been considered in the\nliterature before, no polynomial time algorithm computing an equilibrium was\nknown. Also for this class, we devise the first polynomial time algorithm and\nuse it as a building block for our main algorithm.\n  We then develop a polynomial time computable transformation mapping a\nmultimarket Cournot competition game with firm-specific affine price functions\nand quadratic costs to an associated atomic splittable congestion game as\ndescribed above. The transformation preserves equilibria in either games and,\nthus, leads -- via our first algorithm -- to a polynomial time algorithm\ncomputing Cournot equilibria. Finally, our analysis for integrally-splittable\ngames implies new bounds on the difference between real and integral Cournot\nequilibria. The bounds can be seen as a generalization of the recent bounds for\nsingle market oligopolies obtained by Todd [2016].\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 10:09:47 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 12:00:15 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 10:48:38 GMT"}, {"version": "v4", "created": "Thu, 15 Mar 2018 21:24:14 GMT"}, {"version": "v5", "created": "Wed, 8 Aug 2018 07:00:21 GMT"}, {"version": "v6", "created": "Tue, 5 Nov 2019 08:20:17 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Timmermans", "Veerle", ""], ["Harks", "Tobias", ""]]}, {"id": "1612.00277", "submitter": "Jacques-Henri Jourdan", "authors": "Jacques-Henri Jourdan (MPI Software systems, GALLIUM)", "title": "Sparsity Preserving Algorithms for Octagons", "comments": "in Isabella Mastroeni. Numerical and symbolic abstract domains, Sep\n  2016, Edinburgh, United Kingdom. Elsevier, Numerical and symbolic abstract\n  domains, pp.14, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Known algorithms for manipulating octagons do not preserve their sparsity,\nleading typically to quadratic or cubic time and space complexities even if no\nrelation among variables is known when they are all bounded. In this paper, we\npresent new algorithms, which use and return octagons represented as weakly\nclosed difference bound matrices, preserve the sparsity of their input and have\nbetter performance in the case their inputs are sparse. We prove that these\nalgorithms are as precise as the known ones.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 14:44:20 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Jourdan", "Jacques-Henri", "", "MPI Software systems, GALLIUM"]]}, {"id": "1612.00547", "submitter": "Yair Carmon", "authors": "Yair Carmon and John C. Duchi", "title": "Gradient Descent Efficiently Finds the Cubic-Regularized Non-Convex\n  Newton Step", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimization of non-convex quadratic forms regularized by a\ncubic term, which exhibit multiple saddle points and poor local minima.\nNonetheless, we prove that, under mild assumptions, gradient descent\napproximates the $\\textit{global minimum}$ to within $\\varepsilon$ accuracy in\n$O(\\varepsilon^{-1}\\log(1/\\varepsilon))$ steps for large $\\varepsilon$ and\n$O(\\log(1/\\varepsilon))$ steps for small $\\varepsilon$ (compared to a condition\nnumber we define), with at most logarithmic dependence on the problem\ndimension. When we use gradient descent to approximate the Nesterov-Polyak\ncubic-regularized Newton step, our result implies a rate of convergence to\nsecond-order stationary points of general smooth non-convex functions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 02:40:23 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 05:04:53 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Carmon", "Yair", ""], ["Duchi", "John C.", ""]]}, {"id": "1612.00807", "submitter": "Renato J Cintra", "authors": "R. J. Cintra, F. M. Bayer, V. A. Coutinho, S. Kulasekera, A.\n  Madanayake", "title": "Energy-efficient 8-point DCT Approximations: Theory and Hardware\n  Architectures", "comments": "21 pages, 7 figures, 5 tables", "journal-ref": "Circuits, Systems, and Signal Processing, November 2016, Volume\n  35, Issue 11, pp 4009-4029", "doi": "10.1007/s00034-015-0233-z", "report-no": null, "categories": "cs.MM cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its remarkable energy compaction properties, the discrete cosine\ntransform (DCT) is employed in a multitude of compression standards, such as\nJPEG and H.265/HEVC. Several low-complexity integer approximations for the DCT\nhave been proposed for both 1-D and 2-D signal analysis. The increasing demand\nfor low-complexity, energy efficient methods require algorithms with even lower\ncomputational costs. In this paper, new 8-point DCT approximations with very\nlow arithmetic complexity are presented. The new transforms are proposed based\non pruning state-of-the-art DCT approximations. The proposed algorithms were\nassessed in terms of arithmetic complexity, energy retention capability, and\nimage compression performance. In addition, a metric combining performance and\ncomputational complexity measures was proposed. Results showed good performance\nand extremely low computational complexity. Introduced algorithms were mapped\ninto systolic-array digital architectures and physically realized as digital\nprototype circuits using FPGA technology and mapped to 45nm CMOS technology.\nAll hardware-related metrics showed low resource consumption of the proposed\npruned approximate transforms. The best proposed transform according to the\nintroduced metric presents a reduction in power consumption of 21--25%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 19:47:28 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Coutinho", "V. A.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""]]}, {"id": "1612.00889", "submitter": "Harry Lang", "authors": "Vladimir Braverman, Dan Feldman, Harry Lang, Adiel Statman, Samson\n  Zhou", "title": "New Frameworks for Offline and Streaming Coreset Constructions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $P$ be a set (called points), $Q$ be a set (called queries) and a\nfunction $ f:P\\times Q\\to [0,\\infty)$ (called cost). For an error parameter\n$\\epsilon>0$, a set $S\\subseteq P$ with a \\emph{weight function} $w:P\n\\rightarrow [0,\\infty)$ is an $\\epsilon$-coreset if $\\sum_{s\\in S}w(s) f(s,q)$\napproximates $\\sum_{p\\in P} f(p,q)$ up to a multiplicative factor of\n$1\\pm\\epsilon$ for every given query $q\\in Q$.\n  We construct coresets for the $k$-means clustering of $n$ input points, both\nin an arbitrary metric space and $d$-dimensional Euclidean space. For Euclidean\nspace, we present the first coreset whose size is simultaneously independent of\nboth $d$ and $n$. In particular, this is the first coreset of size $o(n)$ for a\nstream of $n$ sparse points in a $d \\ge n$ dimensional space (e.g. adjacency\nmatrices of graphs). We also provide the first generalizations of such coresets\nfor handling outliers. For arbitrary metric spaces, we improve the dependence\non $k$ to $k \\log k$ and present a matching lower bound.\n  For $M$-estimator clustering (special cases include the well-known $k$-median\nand $k$-means clustering), we introduce a new technique for converting an\noffline coreset construction to the streaming setting. Our method yields\nstreaming coreset algorithms requiring the storage of $O(S + k \\log n)$ points,\nwhere $S$ is the size of the offline coreset. In comparison, the previous\nstate-of-the-art was the merge-and-reduce technique that required $O(S\n\\log^{2a+1} n)$ points, where $a$ is the exponent in the offline construction's\ndependence on $\\epsilon^{-1}$. For example, combining our offline and streaming\nresults, we produce a streaming metric $k$-means coreset algorithm using\n$O(\\epsilon^{-2} k \\log k \\log n)$ points of storage. The previous\nstate-of-the-art required $O(\\epsilon^{-4} k \\log k \\log^{6} n)$ points.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 23:04:16 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 15:53:34 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Braverman", "Vladimir", ""], ["Feldman", "Dan", ""], ["Lang", "Harry", ""], ["Statman", "Adiel", ""], ["Zhou", "Samson", ""]]}, {"id": "1612.00958", "submitter": "Anna Lubiw", "authors": "Anna Lubiw and Vinayak Pathak", "title": "Reconfiguring Ordered Bases of a Matroid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a matroid with an ordered (or \"labelled\") basis, a basis exchange step\nremoves one element with label $l$ and replaces it by a new element that\nresults in a new basis, and with the new element assigned label $l$. We prove\nthat one labelled basis can be reconfigured to another if and only if for every\nlabel, the initial and final elements with that label lie in the same connected\ncomponent of the matroid. Furthermore, we prove that when the reconfiguration\nis possible, the number of basis exchange steps required is $O(r^{1.5})$ for a\nrank $r$ matroid. For a graphic matroid we improve the bound to $O(r \\log r)$.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 11:34:53 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Lubiw", "Anna", ""], ["Pathak", "Vinayak", ""]]}, {"id": "1612.00960", "submitter": "Tasuku Soma", "authors": "Tasuku Soma, Yuichi Yoshida", "title": "Non-monotone DR-Submodular Function Maximization", "comments": "This paper is to appear in AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider non-monotone DR-submodular function maximization, where\nDR-submodularity (diminishing return submodularity) is an extension of\nsubmodularity for functions over the integer lattice based on the concept of\nthe diminishing return property. Maximizing non-monotone DR-submodular\nfunctions has many applications in machine learning that cannot be captured by\nsubmodular set functions. In this paper, we present a\n$\\frac{1}{2+\\epsilon}$-approximation algorithm with a running time of roughly\n$O(\\frac{n}{\\epsilon}\\log^2 B)$, where $n$ is the size of the ground set, $B$\nis the maximum value of a coordinate, and $\\epsilon > 0$ is a parameter. The\napproximation ratio is almost tight and the dependency of running time on $B$\nis exponentially smaller than the naive greedy algorithm. Experiments on\nsynthetic and real-world datasets demonstrate that our algorithm outputs almost\nthe best solution compared to other baseline algorithms, whereas its running\ntime is several orders of magnitude faster.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 11:37:28 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Soma", "Tasuku", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1612.00989", "submitter": "Amanj Khorramian", "authors": "Amanj Khorramian and Akira Matsubayashi", "title": "Online Page Migration on Ring Networks in Uniform Model", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the problem of page migration in ring networks. A ring\nnetwork is a connected graph, in which each node is connected with exactly two\nother nodes. In this problem, one of the nodes in a given network holds a page\nof size D. This node is called the server and the page is a non-duplicable data\nin the network. Requests are issued by nodes to access the page one after\nanother. Every time a new request is issued, the server must serve the request\nand may migrate to another node before the next request arrives. A service\ncosts the distance between the server and the requesting node, and the\nmigration costs the distance of the migration multiplied by D. The problem is\nto minimize the total costs of services and migrations. We study this problem\nin uniform model, for which the page has a unit size, i.e. D=1. A\n3.326-competitive algorithm improving the current best upper bound is designed.\nWe show that this ratio is tight for our algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 17:20:17 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Khorramian", "Amanj", ""], ["Matsubayashi", "Akira", ""]]}, {"id": "1612.01038", "submitter": "Bogdan Armaselu", "authors": "Bogdan Armaselu and Ovidiu Daescu", "title": "Approximation Algorithms for the Maximum Profit Pick-up Problem with\n  Time Windows and Capacity Constraint", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the Maximum Profit Pick-up Problem with Time Windows\nand Capacity Constraint (MP-PPTWC). Our main results are 3 polynomial time\nalgorithms, all having constant approximation factors. The first algorithm has\nan approximation ratio of $~46 (1 + (71/60 + \\frac{\\alpha}{\\sqrt{10+p}})\n\\epsilon) \\log T$, where: (i) $\\epsilon > 0$ and $T$ are constants; (ii) The\nmaximum quantity supplied is $q_{max} = O(n^p) q_{min}$, for some $p > 0$,\nwhere $q_{min}$ is the minimum quantity supplied; (iii) $\\alpha > 0$ is a\nconstant such that the optimal number of vehicles is always at least $\\sqrt{10\n+ p} / \\alpha$. The second algorithm has an approximation ratio of $\\simeq 46\n(1 + \\epsilon + \\frac{(2 + \\alpha) \\epsilon}{\\sqrt{10 + p}}) \\log T$. Finally,\nthe third algorithm has an approximation ratio of $\\simeq 11 (1 + 2 \\epsilon)\n\\log T$. While our algorithms may seem to have quite high approximation ratios,\nin practice they work well and, in the majority of cases, the profit obtained\nis at least 1/2 of the optimum.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 00:07:40 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Armaselu", "Bogdan", ""], ["Daescu", "Ovidiu", ""]]}, {"id": "1612.01053", "submitter": "EPTCS", "authors": "Alexander Heu{\\ss}ner (Otto-Friedrich Universit\\\"at Bamberg), Aleks\n  Kissinger (Radboud University Nijmegen), Anton Wijs (Eindhoven University of\n  Technology)", "title": "Proceedings Second Graphs as Models Workshop", "comments": null, "journal-ref": "EPTCS 231, 2016", "doi": "10.4204/EPTCS.231", "report-no": null, "categories": "cs.DS cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are used as models in all areas of computer science: examples are\nstate space graphs, control flow graphs, syntax graphs, UML-type models of all\nkinds, network layouts, social networks, dependency graphs, and so forth. Once\nsuch graphical models are constructed, they can be analysed and transformed to\nverify their correctness within a domain, discover new properties, or produce\nnew equivalent and/or optimised versions.\n  Graphs as Models' main focus is the exchange and collaboration of researchers\nfrom different backgrounds. The workshop serves as platform to boost inter- and\ntransdisciplinary research and wants to serve as leeway for new ideas. Thus,\nbesides classical research presentations, the workshop is highly geared toward\nnumerous interactive sessions.\n  The second edition of the Graphs as Models workshop was held on 2-3 June 2016\nin Eindhoven, The Netherlands, colocated with the 19th European Joint\nConferences on Theory and Practice of Software (ETAPS 2016).\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 03:15:16 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Heu\u00dfner", "Alexander", "", "Otto-Friedrich Universit\u00e4t Bamberg"], ["Kissinger", "Aleks", "", "Radboud University Nijmegen"], ["Wijs", "Anton", "", "Eindhoven University of\n  Technology"]]}, {"id": "1612.01171", "submitter": "D\\'aniel Marx", "authors": "D\\'aniel Marx, Anastasios Sidiropoulos", "title": "The limited blessing of low dimensionality: when $1-1/d$ is the best\n  possible exponent for $d$-dimensional geometric problems", "comments": "Full version of SoCG 2014 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are studying $d$-dimensional geometric problems that have algorithms with\n$1-1/d$ appearing in the exponent of the running time, for example, in the form\nof $2^{n^{1-1/d}}$ or $n^{k^{1-1/d}}$. This means that these algorithms perform\nsomewhat better in low dimensions, but the running time is almost the same r\nall large values $d$ of the dimension. Our main result is showing that for some\nof these problems the dependence on $1-1/d$ is best possible under a standard\ncomplexity assumption. We show that, assuming the Exponential Time Hypothesis,\n  --- $d$-dimensional Euclidean TSP on $n$ points cannot be solved in time\n$2^{O(n^{1-1/d-\\epsilon})}$ for any $\\epsilon>0$, and\n  --- the problem of finding a set of $k$ pairwise nonintersecting\n$d$-dimensional unit balls/axis parallel unit cubes cannot be solved in time\n$f(k)n^{o(k^{1-1/d})}$ for any computable function $f$.\n  These lower bounds essentially match the known algorithms for these problems.\nTo obtain these results, we first prove lower bounds on the complexity of\nConstraint Satisfaction Problems (CSPs) whose constraint graphs are\n$d$-dimensional grids. We state the complexity results on CSPs in a way to make\nthem convenient starting points for problem-specific reductions to particular\n$d$-dimensional geometric problems and to be reusable in the future for further\nresults of similar flavor.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 19:27:17 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Marx", "D\u00e1niel", ""], ["Sidiropoulos", "Anastasios", ""]]}, {"id": "1612.01178", "submitter": "Michael Sutton", "authors": "Michael Sutton, Tal Ben-Nun, Amnon Barak, Sreepathi Pai, Keshav\n  Pingali", "title": "Adaptive Work-Efficient Connected Components on the GPU", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents an adaptive work-efficient approach for implementing the\nConnected Components algorithm on GPUs. The results show a considerable\nincrease in performance (up to 6.8$\\times$) over current state-of-the-art\nsolutions.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 20:57:12 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Sutton", "Michael", ""], ["Ben-Nun", "Tal", ""], ["Barak", "Amnon", ""], ["Pai", "Sreepathi", ""], ["Pingali", "Keshav", ""]]}, {"id": "1612.01492", "submitter": "Jennifer Iglesias", "authors": "Jennifer Iglesias and Rajmohan Rajaraman and R Ravi and Ravi Sundaram", "title": "Plane Gossip: Approximating rumor spread in planar graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the design of schedules for multi-commodity multicast; we are given\nan undirected graph $G$ and a collection of source destination pairs, and the\ngoal is to schedule a minimum-length sequence of matchings that connects every\nsource with its respective destination. Multi-commodity multicast models a\nclassic information dissemination problem in networks where the primary\ncommunication constraint is the number of connections that a node can make, not\nlink bandwidth.\n  Multi-commodity multicast is closely related to the problem of finding a\nsubgraph, $H$, of optimal poise, where the poise is defined as the sum of the\nmaximum degree of $H$ and the maximum distance between any source-destination\npair in $H$. We first show that the minimum poise subgraph for single-commodity\nmulticast can be approximated to within a factor of $O(\\log k)$ with respect to\nthe value of a natural LP relaxation in an instance with $k$ terminals. This is\nthe first upper bound on the integrality gap of the natural LP. Using this\npoise result and shortest-path separators in planar graphs, we obtain a\n$O(\\log^3 k\\log n/(\\log\\log n))$-approximation for multi-commodity multicast\nfor planar graphs.\n  We also study the minimum-time radio gossip problem in planar graphs where a\nmessage from each node must be transmitted to all other nodes under a model\nwhere nodes can broadcast to all neighbors in a single step but only nodes with\na single broadcasting neighbor get a message. We give an $O(\\log^2\nn)$-approximation for radio gossip in planar graphs breaking previous barriers.\nThis is the first bound for radio gossip that does not rely on the maximum\ndegree of the graph.\n  Finally, we show that our techniques for planar graphs extend to graphs with\nexcluded minors. We establish polylogarithmic-approximation algorithms for both\nmulti-commodity multicast and radio gossip problems in minor-free graphs.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 19:41:00 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 20:07:45 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Iglesias", "Jennifer", ""], ["Rajaraman", "Rajmohan", ""], ["Ravi", "R", ""], ["Sundaram", "Ravi", ""]]}, {"id": "1612.01506", "submitter": "Jan Holub", "authors": "Jorma Tarhio, Jan Holub, Emanuele Giaquinta", "title": "Technology Beats Algorithms (in Exact String Matching)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More than 120 algorithms have been developed for exact string matching within\nthe last 40 years. We show by experiments that the \\naive{} algorithm\nexploiting SIMD instructions of modern CPUs (with symbols compared in a special\norder) is the fastest one for patterns of length up to about 50 symbols and\nextremely good for longer patterns and small alphabets. The algorithm compares\n16 or 32 characters in parallel by applying SSE2 or AVX2 instructions,\nrespectively. Moreover, it uses loop peeling to further speed up the searching\nphase. We tried several orders for comparisons of pattern symbols and the\nincreasing order of their probabilities in the text was the best.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 20:34:39 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Tarhio", "Jorma", ""], ["Holub", "Jan", ""], ["Giaquinta", "Emanuele", ""]]}, {"id": "1612.01507", "submitter": "Santosh Vempala", "authors": "Yin Tat Lee and Santosh S. Vempala", "title": "Eldan's Stochastic Localization and the KLS Conjecture: Isoperimetry,\n  Concentration and Mixing", "comments": "This version merges arXiv:1612.01507 and arXiv:1712.01791", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.CG cs.DS math.MG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Cheeger constant for $n$-dimensional isotropic logconcave\nmeasures is $O(n^{1/4})$, improving on the previous best bound of\n$O(n^{1/3}\\sqrt{\\log n}).$ As corollaries we obtain the same improved bound on\nthe thin-shell estimate, Poincar\\'{e} constant and Lipschitz concentration\nconstant and an alternative proof of this bound for the isotropic (slicing)\nconstant; it also follows that the ball walk for sampling from an isotropic\nlogconcave density in ${\\bf R}^{n}$ converges in $O^{*}(n^{2.5})$ steps from a\nwarm start. The proof is based on gradually transforming any logconcave density\nto one that has a significant Gaussian factor via a Martingale process.\n  Extending this proof technique, we prove that the log-Sobolev constant of any\nisotropic logconcave density in ${\\bf R}^{n}$ with support of diameter $D$ is\n$\\Omega(1/D)$, resolving a question posed by Frieze and Kannan in 1997. This is\nasymptotically the best possible estimate and improves on the previous bound of\n$\\Omega(1/D^{2})$ by Kannan-Lov\\'{a}sz-Montenegro. It follows that for any\nisotropic logconcave density, the ball walk with step size\n$\\delta=\\Theta(1/\\sqrt{n})$ mixes in $O\\left(n^{2}D\\right)$ proper steps from\n\\emph{any }starting point. This improves on the previous best bound of\n$O(n^{2}D^{2})$ and is also asymptotically tight.\n  The new bound leads to the following large deviation inequality for an\n$L$-Lipschitz function $g$ over an isotropic logconcave density $p$: for any\n$t>0$, \\[ Pr_{x\\sim p}\\left(\\left|g(x)-\\bar{g}\\right|\\geq L\\cdot\nt\\right)\\leq\\exp(-\\frac{c\\cdot t^{2}}{t+\\sqrt{n}}) \\] where $\\bar{g}$ is the\nmedian or mean of $g$ for $x\\sim p$; this generalizes and improves on previous\nbounds by Paouris and by Guedon-Milman. The technique also bounds the ``small\nball'' probability in terms of the Cheeger constant, and recovers the current\nbest bound.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 20:36:28 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 20:40:47 GMT"}, {"version": "v3", "created": "Sun, 27 Jan 2019 04:40:54 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Lee", "Yin Tat", ""], ["Vempala", "Santosh S.", ""]]}, {"id": "1612.01514", "submitter": "Siddhartha Jayanti", "authors": "Siddhartha V. Jayanti and Robert E. Tarjan", "title": "A Randomized Concurrent Algorithm for Disjoint Set Union", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The disjoint set union problem is a basic problem in data structures with a\nwide variety of applications. We extend a known efficient sequential algorithm\nfor this problem to obtain a simple and efficient concurrent wait-free\nalgorithm running on an asynchronous parallel random access machine (APRAM).\nCrucial to our result is the use of randomization. Under a certain independence\nassumption, for a problem instance in which there are n elements, m operations,\nand p processes, our algorithm does Theta(m (alpha(n, m/(np)) + log(np/m + 1)))\nexpected work, where the expectation is over the random choices made by the\nalgorithm and alpha is a functional inverse of Ackermann's function. In\naddition, each operation takes O(log n) steps with high probability. Our\nalgorithm is significantly simpler and more efficient than previous algorithms\nproposed by Anderson and Woll. Under our independence assumption, our algorithm\nachieves almost-linear speed-up for applications in which all or most of the\nprocesses can be kept busy.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 20:52:30 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Jayanti", "Siddhartha V.", ""], ["Tarjan", "Robert E.", ""]]}, {"id": "1612.01527", "submitter": "Joshua Grochow", "authors": "Joshua A. Grochow and Cristopher Moore", "title": "Matrix multiplication algorithms from group orbits", "comments": "Added transparent proof of Strassen's algorithm and its\n  generalization using lattices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.AG math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to construct highly symmetric algorithms for matrix\nmultiplication. In particular, we consider algorithms which decompose the\nmatrix multiplication tensor into a sum of rank-1 tensors, where the\ndecomposition itself consists of orbits under some finite group action. We show\nhow to use the representation theory of the corresponding group to derive\nsimple constraints on the decomposition, which we solve by hand for n=2,3,4,5,\nrecovering Strassen's algorithm (in a particularly symmetric form) and new\nalgorithms for larger n. While these new algorithms do not improve the known\nupper bounds on tensor rank or the matrix multiplication exponent, they are\nbeautiful in their own right, and we point out modifications of this idea that\ncould plausibly lead to further improvements. Our constructions also suggest\nfurther patterns that could be mined for new algorithms, including a\ntantalizing connection with lattices. In particular, using lattices we give the\nmost transparent proof to date of Strassen's algorithm; the same proof works\nfor all n, to yield a decomposition with $n^3 - n + 1$ terms.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 21:00:02 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 20:13:02 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Grochow", "Joshua A.", ""], ["Moore", "Cristopher", ""]]}, {"id": "1612.01610", "submitter": "David Harris", "authors": "David G. Harris", "title": "Tighter inapproximability for set cover", "comments": "We discovered that these results have already appeared in Dinur &\n  Steurer, \"Analytical approach to parallel repetition.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set Cover is a classic NP-hard problem; as shown by Slav\\'{i}k (1997) the\ngreedy algorithm gives an approximation ratio of $\\ln n - \\ln \\ln n +\n\\Theta(1)$. A series of works by Lund \\& Yannakakis (1994), Feige (1998),\nMoshkovitz (2015) have shown that, under the assumption $P \\neq NP$, it is\nimpossible to obtain a polynomial-time approximation ratio with approximation\nratio $(1 - \\alpha) \\ln n$, for any constant $\\alpha > 0$.\n  In this note, we show that under the Exponential Time Hypothesis (a stronger\ncomplexity-theoretic assumptions than $P \\neq NP$), there are no\npolynomial-time algorithms achieving approximation ratio $\\ln n - C \\ln \\ln n$,\nwhere $C$ is some universal constant. Thus, the greedy algorithm achieves an\nessentially optimal approximation ratio (up to the coefficient of $\\ln \\ln n$).\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 00:53:00 GMT"}, {"version": "v2", "created": "Sat, 31 Dec 2016 18:00:39 GMT"}, {"version": "v3", "created": "Wed, 15 Feb 2017 21:55:13 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Harris", "David G.", ""]]}, {"id": "1612.01693", "submitter": "Avni Verma", "authors": "Avni Verma and Kamalakar Karlapalem", "title": "Design and Evaluation of Alternate Enumeration Techniques for Subset Sum\n  Problem", "comments": "39 pages, 4 figures, 19 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subset sum problem, also referred as SSP, is a NP-Hard computational\nproblem. SSP has its applications in broad domains like cryptography, number\ntheory, operation research and complexity theory. The most famous algorithm for\nsolving SSP is Backtracking Algorithm which has exponential time complexity.\nTherefore, our goal is to design and develop better alternate enumeration\ntechniques for faster generation of SSP solutions. Given the set of first n\nnatural numbers which is denoted by Xn and a target sum S, we propose various\nalternate enumeration techniques which find all the subsets of Xn that add up\nto sum S.\n  In this paper, we present the mathematics behind this exponential problem. We\nanalyze the distribution of power set of Xn and present formulas which show\ndefinite patterns and relations among these subsets. We introduce three major\ndistributions for power set of Xn: Sum Distribution, Length-Sum Distribution\nand Element Distribution. These distributions are prepossessing procedures for\nvarious alternate enumeration techniques for solving SSP. We propose novel\nalgorithms: Subset Generation using Sum Distribution, Subset Generation using\nLength-Sum Distribution, Basic Bucket Algorithm, Maximum and Minimum Frequency\nDriven Bucket Algorithms and Local Search using Maximal and Minimal Subsets for\nenumerating SSP.\n  We compare the performance of these approaches against the traditional\nbacktracking algorithm. The efficiency and effectiveness of these algorithms\nare presented with the help of these experimental results. Furthermore, we\nstudied the over solution set of subsets generated by various algorithms to get\nthe complete solution for subset sum problem. Finally, we present a conjecture\nabout upper bound on the number of subsets that has to be enumerated to get all\nsolutions for Subset Sum Problem.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 08:02:57 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 06:20:50 GMT"}, {"version": "v3", "created": "Mon, 12 Dec 2016 13:59:34 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Verma", "Avni", ""], ["Karlapalem", "Kamalakar", ""]]}, {"id": "1612.01748", "submitter": "Frederik Rye Skjoldjensen", "authors": "Philip Bille, Inge Li G{\\o}rtz, Frederik Rye Skjoldjensen", "title": "Deterministic Indexing for Packed Strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a string $S$ of length $n$, the classic string indexing problem is to\npreprocess $S$ into a compact data structure that supports efficient subsequent\npattern queries. In the \\emph{deterministic} variant the goal is to solve the\nstring indexing problem without any randomization (at preprocessing time or\nquery time). In the \\emph{packed} variant the strings are stored with several\ncharacter in a single word, giving us the opportunity to read multiple\ncharacters simultaneously. Our main result is a new string index in the\ndeterministic \\emph{and} packed setting. Given a packed string $S$ of length\n$n$ over an alphabet $\\sigma$, we show how to preprocess $S$ in $O(n)$\n(deterministic) time and space $O(n)$ such that given a packed pattern string\nof length $m$ we can support queries in (deterministic) time $O\\left(m/\\alpha +\n\\log m + \\log \\log \\sigma\\right), $ where $\\alpha = w / \\log \\sigma$ is the\nnumber of characters packed in a word of size $w = \\Theta(\\log n)$. Our query\ntime is always at least as good as the previous best known bounds and whenever\nseveral characters are packed in a word, i.e., $\\log \\sigma \\ll w$, the query\ntimes are faster.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 10:55:34 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Bille", "Philip", ""], ["G\u00f8rtz", "Inge Li", ""], ["Skjoldjensen", "Frederik Rye", ""]]}, {"id": "1612.01817", "submitter": "Igor Carboni Oliveira", "authors": "Igor C. Oliveira, Rahul Santhanam", "title": "Pseudodeterministic Constructions in Subexponential Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.CO math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study pseudodeterministic constructions, i.e., randomized algorithms which\noutput the same solution on most computation paths. We establish\nunconditionally that there is an infinite sequence $\\{p_n\\}_{n \\in \\mathbb{N}}$\nof increasing primes and a randomized algorithm $A$ running in expected\nsub-exponential time such that for each $n$, on input $1^{|p_n|}$, $A$ outputs\n$p_n$ with probability $1$. In other words, our result provides a\npseudodeterministic construction of primes in sub-exponential time which works\ninfinitely often.\n  This result follows from a much more general theorem about\npseudodeterministic constructions. A property $Q \\subseteq \\{0,1\\}^{*}$ is\n$\\gamma$-dense if for large enough $n$, $|Q \\cap \\{0,1\\}^n| \\geq \\gamma 2^n$.\nWe show that for each $c > 0$ at least one of the following holds: (1) There is\na pseudodeterministic polynomial time construction of a family $\\{H_n\\}$ of\nsets, $H_n \\subseteq \\{0,1\\}^n$, such that for each $(1/n^c)$-dense property $Q\n\\in \\mathsf{DTIME}(n^c)$ and every large enough $n$, $H_n \\cap Q \\neq\n\\emptyset$; or (2) There is a deterministic sub-exponential time construction\nof a family $\\{H'_n\\}$ of sets, $H'_n \\subseteq \\{0,1\\}^n$, such that for each\n$(1/n^c)$-dense property $Q \\in \\mathsf{DTIME}(n^c)$ and for infinitely many\nvalues of $n$, $H'_n \\cap Q \\neq \\emptyset$.\n  We provide further algorithmic applications that might be of independent\ninterest. Perhaps intriguingly, while our main results are unconditional, they\nhave a non-constructive element, arising from a sequence of applications of the\nhardness versus randomness paradigm.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 14:20:41 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Oliveira", "Igor C.", ""], ["Santhanam", "Rahul", ""]]}, {"id": "1612.01829", "submitter": "Waldo G\\'alvez", "authors": "Waldo G\\'alvez, Jos\\'e A. Soto, Jos\\'e Verschae", "title": "Symmetry exploitation for Online Machine Covering with Bounded Migration", "comments": "26 pages, 3 figures; full version of ESA 2018 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online models that allow recourse are highly effective in situations where\nclassical models are too pessimistic. One such problem is the online machine\ncovering problem on identical machines. In this setting, jobs arrive one by one\nand must be assigned to machines with the objective of maximizing the minimum\nmachine load. When a job arrives, we are allowed to reassign some jobs as long\nas their total size is (at most) proportional to the processing time of the\narriving job. The proportionality constant is called the migration factor of\nthe algorithm. Using a rounding procedure with useful structural properties for\nonline packing and covering problems, we design first a simple $(1.7 +\n\\varepsilon)$-competitive algorithm using a migration factor of\n$O(1/\\varepsilon)$ which maintains at every arrival a locally optimal solution\nwith respect to the Jump neighborhood. After that, we present as our main\ncontribution a more involved $(4/3+\\varepsilon)$-competitive algorithm using a\nmigration factor of $\\tilde{O}(1/\\varepsilon^3)$. At every arrival, we run an\nadaptation of the Largest Processing Time first (LPT) algorithm. Since the new\njob can cause a complete change of the assignment of smaller jobs in both\ncases, a low migration factor is achieved by carefully exploiting the highly\nsymmetric structure obtained by the rounding procedure.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 14:43:44 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 11:33:48 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["G\u00e1lvez", "Waldo", ""], ["Soto", "Jos\u00e9 A.", ""], ["Verschae", "Jos\u00e9", ""]]}, {"id": "1612.01855", "submitter": "Tobias Wicky", "authors": "Tobias Wicky, Edgar Solomonik, Torsten Hoefler", "title": "Communication-Avoiding Parallel Algorithms for Solving Triangular\n  Systems of Linear Equations", "comments": "10 pages, 1 figure, accepted at IPDPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new parallel algorithm for solving triangular systems with\nmultiple right hand sides (TRSM). TRSM is used extensively in numerical linear\nalgebra computations, both to solve triangular linear systems of equations as\nwell as to compute factorizations with triangular matrices, such as Cholesky,\nLU, and QR. Our algorithm achieves better theoretical scalability than known\nalternatives, while maintaining numerical stability, via selective use of\ntriangular matrix inversion. We leverage the fact that triangular inversion and\nmatrix multiplication are more parallelizable than the standard TRSM algorithm.\nBy only inverting triangular blocks along the diagonal of the initial matrix,\nwe generalize the usual way of TRSM computation and the full matrix inversion\napproach. This flexibility leads to an efficient algorithm for any ratio of the\nnumber of right hand sides to the triangular matrix dimension. We provide a\ndetailed communication cost analysis for our algorithm as well as for the\nrecursive triangular matrix inversion. This cost analysis makes it possible to\ndetermine optimal block sizes and processor grids a priori. Relative to the\nbest known algorithms for TRSM, our approach can require asymptotically fewer\nmessages, while performing optimal amounts of computation and communication in\nterms of words sent.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 15:20:38 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 10:57:58 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Wicky", "Tobias", ""], ["Solomonik", "Edgar", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1612.01966", "submitter": "David Bergman", "authors": "David Bergman, Carlos H. Cardonha, Andre A. Cire, Arvind U.\n  Raghunathan", "title": "On the Minimum Chordal Completion Polytope", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is chordal if every cycle of length at least four contains a chord,\nthat is, an edge connecting two nonconsecutive vertices of the cycle. Several\nclassical applications in sparse linear systems, database management, computer\nvision, and semidefinite programming can be reduced to finding the minimum\nnumber of edges to add to a graph so that it becomes chordal, known as the\nminimum chordal completion problem (MCCP). In this article we propose a new\nformulation for the MCCP which does not rely on finding perfect elimination\norderings of the graph, as has been considered in previous work. We introduce\nseveral families of facet-defining inequalities for cycle subgraphs and\ninvestigate the underlying separation problems, showing that some key\ninequalities are NP-Hard to separate. We also show general properties of the\nproposed polyhedra, indicating certain conditions and methods through which\nfacets and inequalities associated with the polytope of a certain graph can be\nadapted in order to become valid and eventually facet-defining for some of its\nsubgraphs or supergraphs. Numerical studies combining heuristic separation\nmethods based on a threshold rounding and lazy-constraint generation indicate\nthat our approach substantially outperforms existing methods for the MCCP,\nsolving many benchmark graphs to optimality for the first time.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 19:55:03 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Bergman", "David", ""], ["Cardonha", "Carlos H.", ""], ["Cire", "Andre A.", ""], ["Raghunathan", "Arvind U.", ""]]}, {"id": "1612.02034", "submitter": "Inbal Talgam-Cohen", "authors": "Uriel Feige, Michal Feldman, Inbal Talgam-Cohen", "title": "Approximate Modularity Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set functions with convenient properties (such as submodularity) appear in\napplication areas of current interest, such as algorithmic game theory, and\nallow for improved optimization algorithms. It is natural to ask (e.g., in the\ncontext of data driven optimization) how robust such properties are, and\nwhether small deviations from them can be tolerated. We consider two such\nquestions in the important special case of linear set functions.\n  One question that we address is whether any set function that approximately\nsatisfies the modularity equation (linear functions satisfy the modularity\nequation exactly) is close to a linear function. The answer to this is positive\n(in a precise formal sense) as shown by Kalton and Roberts [1983] (and further\nimproved by Bondarenko, Prymak, and Radchenko [2013]). We revisit their proof\nidea that is based on expander graphs, and provide significantly stronger upper\nbounds by combining it with new techniques. Furthermore, we provide improved\nlower bounds for this problem.\n  Another question that we address is that of how to learn a linear function\n$h$ that is close to an approximately linear function $f$, while querying the\nvalue of $f$ on only a small number of sets. We present a deterministic\nalgorithm that makes only linearly many (in the number of items) nonadaptive\nqueries, by this improving over a previous algorithm of Chierichetti, Das,\nDasgupta and Kumar [2015] that is randomized and makes more than a quadratic\nnumber of queries. Our learning algorithm is based on a Hadamard transform.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 21:28:29 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 18:38:12 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Feige", "Uriel", ""], ["Feldman", "Michal", ""], ["Talgam-Cohen", "Inbal", ""]]}, {"id": "1612.02168", "submitter": "Yoann Dieudonn\\'e", "authors": "S\\'ebastien Bouchard, Marjorie Bournat, Yoann Dieudonn\\'e, Swan Dubois\n  and Franck Petit", "title": "Asynchronous approach in the plane: A deterministic polynomial algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the task of approach of two mobile agents having the\nsame limited range of vision and moving asynchronously in the plane. This task\nconsists in getting them in finite time within each other's range of vision.\nThe agents execute the same deterministic algorithm and are assumed to have a\ncompass showing the cardinal directions as well as a unit measure. On the other\nhand, they do not share any global coordinates system (like GPS), cannot\ncommunicate and have distinct labels. Each agent knows its label but does not\nknow the label of the other agent or the initial position of the other agent\nrelative to its own. The route of an agent is a sequence of segments that are\nsubsequently traversed in order to achieve approach. For each agent, the\ncomputation of its route depends only on its algorithm and its label. An\nadversary chooses the initial positions of both agents in the plane and\ncontrols the way each of them moves along every segment of the routes, in\nparticular by arbitrarily varying the speeds of the agents. A deterministic\napproach algorithm is a deterministic algorithm that always allows two agents\nwith any distinct labels to solve the task of approach regardless of the\nchoices and the behavior of the adversary. The cost of a complete execution of\nan approach algorithm is the length of both parts of route travelled by the\nagents until approach is completed. Let $\\Delta$ and $l$ be the initial\ndistance separating the agents and the length of the shortest label,\nrespectively. Assuming that $\\Delta$ and $l$ are unknown to both agents, does\nthere exist a deterministic approach algorithm always working at a cost that is\npolynomial in $\\Delta$ and $l$? In this paper, we provide a positive answer to\nthe above question by designing such an algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 09:50:02 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 15:46:52 GMT"}, {"version": "v3", "created": "Fri, 5 May 2017 09:32:03 GMT"}, {"version": "v4", "created": "Mon, 9 Jul 2018 12:48:22 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Bouchard", "S\u00e9bastien", ""], ["Bournat", "Marjorie", ""], ["Dieudonn\u00e9", "Yoann", ""], ["Dubois", "Swan", ""], ["Petit", "Franck", ""]]}, {"id": "1612.02284", "submitter": "Jason Qin", "authors": "Jason Qin, Denys Kim, Yumei Tung", "title": "LogLog-Beta and More: A New Algorithm for Cardinality Estimation Based\n  on LogLog Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The information presented in this paper defines LogLog-Beta. LogLog-Beta is a\nnew algorithm for estimating cardinalities based on LogLog counting. The new\nalgorithm uses only one formula and needs no additional bias corrections for\nthe entire range of cardinalities, therefore, it is more efficient and simpler\nto implement. Our simulations show that the accuracy provided by the new\nalgorithm is as good as or better than the accuracy provided by either of\nHyperLogLog or HyperLogLog++. In addition to LogLog-Beta we also provide\nanother one-formula estimator for cardinalities based on order statistics, a\nmodification of an algorithm developed by Lumbroso.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 15:21:19 GMT"}, {"version": "v2", "created": "Sun, 1 Jan 2017 02:42:48 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 18:09:02 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Qin", "Jason", ""], ["Kim", "Denys", ""], ["Tung", "Yumei", ""]]}, {"id": "1612.02327", "submitter": "Hossein Esfandiari", "authors": "MohammadHossein Bateni, Hossein Esfandiari, Vahab Mirrokni", "title": "Distributed Coverage Maximization via Sketching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coverage problems are central in optimization and have a wide range of\napplications in data mining and machine learning. While several distributed\nalgorithms have been developed for coverage problems, the existing methods\nsuffer from several limitations, e.g., they all achieve either suboptimal\napproximation guarantees or suboptimal space and memory complexities. In\naddition, previous algorithms developed for submodular maximization assume\noracle access, and ignore the computational complexity of communicating large\nsubsets or computing the size of the union of the subsets in this subfamily. In\nthis paper, we develop an improved distributed algorithm for the $k$-cover and\nthe set cover with $\\lambda$ outliers problems, with almost optimal\napproximation guarantees, almost optimal memory complexity, and linear\ncommunication complexity running in only four rounds of computation. Finally,\nwe perform an extensive empirical study of our algorithms on a number of\npublicly available real data sets, and show that using sketches of size $30$ to\n$600$ times smaller than the input, one can solve the coverage maximization\nproblem with quality very close to that of the state-of-the-art single-machine\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 16:50:32 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 05:15:49 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Bateni", "MohammadHossein", ""], ["Esfandiari", "Hossein", ""], ["Mirrokni", "Vahab", ""]]}, {"id": "1612.02384", "submitter": "Aur\\'elien Ooms", "authors": "Luis Barba, Jean Cardinal, John Iacono, Stefan Langerman, Aur\\'elien\n  Ooms, Noam Solomon", "title": "Subquadratic Algorithms for Algebraic Generalizations of 3SUM", "comments": "Submitted to SoCG'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3SUM problem asks if an input $n$-set of real numbers contains a triple\nwhose sum is zero. We consider the 3POL problem, a natural generalization of\n3SUM where we replace the sum function by a constant-degree polynomial in three\nvariables. The motivations are threefold. Raz, Sharir, and de Zeeuw gave a\n$O(n^{11/6})$ upper bound on the number of solutions of trivariate polynomial\nequations when the solutions are taken from the cartesian product of three\n$n$-sets of real numbers. We give algorithms for the corresponding problem of\ncounting such solutions. Gr\\o nlund and Pettie recently designed subquadratic\nalgorithms for 3SUM. We generalize their results to 3POL. Finally, we shed\nlight on the General Position Testing (GPT) problem: \"Given $n$ points in the\nplane, do three of them lie on a line?\", a key problem in computational\ngeometry.\n  We prove that there exist bounded-degree algebraic decision trees of depth\n$O(n^{\\frac{12}{7}+\\varepsilon})$ that solve 3POL, and that 3POL can be solved\nin $O(n^2 {(\\log \\log n)}^\\frac{3}{2} / {(\\log n)}^\\frac{1}{2})$ time in the\nreal-RAM model. Among the possible applications of those results, we show how\nto solve GPT in subquadratic time when the input points lie on $o({(\\log\nn)}^\\frac{1}{6}/{(\\log \\log n)}^\\frac{1}{2})$ constant-degree polynomial\ncurves. This constitutes a first step towards closing the major open question\nof whether GPT can be solved in subquadratic time.\n  To obtain these results, we generalize important tools --- such as batch\nrange searching and dominance reporting --- to a polynomial setting. We expect\nthese new tools to be useful in other applications.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 19:27:02 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Barba", "Luis", ""], ["Cardinal", "Jean", ""], ["Iacono", "John", ""], ["Langerman", "Stefan", ""], ["Ooms", "Aur\u00e9lien", ""], ["Solomon", "Noam", ""]]}, {"id": "1612.02503", "submitter": "Mahmoud Abo Khamis", "authors": "Mahmoud Abo Khamis, Hung Q. Ngo, Dan Suciu", "title": "What do Shannon-type Inequalities, Submodular Width, and Disjunctive\n  Datalog have to do with one another?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on bounding the output size of a conjunctive query with\nfunctional dependencies and degree constraints have shown a deep connection\nbetween fundamental questions in information theory and database theory. We\nprove analogous output bounds for disjunctive datalog rules, and answer several\nopen questions regarding the tightness and looseness of these bounds along the\nway. Our bounds are intimately related to Shannon-type information\ninequalities. We devise the notion of a \"proof sequence\" of a specific class of\nShannon-type information inequalities called \"Shannon flow inequalities\". We\nthen show how such a proof sequence can be interpreted as symbolic instructions\nguiding an algorithm called \"PANDA\", which answers disjunctive datalog rules\nwithin the time that the size bound predicted. We show that PANDA can be used\nas a black-box to devise algorithms matching precisely the fractional hypertree\nwidth and the submodular width runtimes for aggregate and conjunctive queries\nwith functional dependencies and degree constraints.\n  Our results improve upon known results in three ways. First, our bounds and\nalgorithms are for the much more general class of disjunctive datalog rules, of\nwhich conjunctive queries are a special case. Second, the runtime of PANDA\nmatches precisely the submodular width bound, while the previous algorithm by\nMarx has a runtime that is polynomial in this bound. Third, our bounds and\nalgorithms work for queries with input cardinality bounds, functional\ndependencies, and degree constraints.\n  Overall, our results show a deep connection between three seemingly unrelated\nlines of research; and, our results on proof sequences for Shannon flow\ninequalities might be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 01:06:40 GMT"}, {"version": "v2", "created": "Sun, 18 Dec 2016 08:10:36 GMT"}, {"version": "v3", "created": "Sat, 25 Mar 2017 23:18:03 GMT"}, {"version": "v4", "created": "Wed, 1 Nov 2017 07:13:09 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Ngo", "Hung Q.", ""], ["Suciu", "Dan", ""]]}, {"id": "1612.02531", "submitter": "Andrew McGregor", "authors": "Andrew McGregor and Sofya Vorotnikova", "title": "A Note on Logarithmic Space Stream Algorithms for Matchings in Low\n  Arboricity Graphs", "comments": "An update to the proof of Theorem 3. See paper for details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data stream algorithm for estimating the size of the maximum\nmatching of a low arboricity graph. Recall that a graph has arboricity $\\alpha$\nif its edges can be partitioned into at most $\\alpha$ forests and that a planar\ngraph has arboricity $\\alpha=3$. Estimating the size of the maximum matching in\nsuch graphs has been a focus of recent data stream research.\n  A surprising result on this problem was recently proved by Cormode et al.\nThey designed an ingenious algorithm that returned a\n$(22.5\\alpha+6)(1+\\epsilon)$ approximation using a single pass over the edges\nof the graph (ordered arbitrarily) and $O(\\epsilon^{-2}\\alpha \\cdot \\log n\n\\cdot \\log_{1+\\epsilon} n)$ space. In this note, we improve the approximation\nfactor to $(\\alpha+2)(1+\\epsilon)$ via a tighter analysis and show that, with a\nmodification of their algorithm, the space required can be reduced to\n$O(\\epsilon^{-2} \\log n)$.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 05:15:07 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 03:55:08 GMT"}, {"version": "v3", "created": "Mon, 14 Aug 2017 17:52:57 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["McGregor", "Andrew", ""], ["Vorotnikova", "Sofya", ""]]}, {"id": "1612.02557", "submitter": "Sebastian Deorowicz", "authors": "Marek Kokot, Sebastian Deorowicz, Agnieszka Debudaj-Grabysz", "title": "Sorting Data on Ultra-Large Scale with RADULS. New Incarnation of Radix\n  Sort", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces RADULS, a new parallel sorter based on radix sort\nalgorithm, intended to organize ultra-large data sets efficiently. For example\n4G 16-byte records can be sorted with 16 threads in less than 15 seconds on\nIntel Xeon-based workstation. The implementation of RADULS is not only highly\noptimized to gain such an excellent performance, but also parallelized in a\ncache friendly manner to make the most of modern multicore architectures.\nBesides, our parallel scheduler launches a few different procedures at runtime,\naccording to the current parameters of the execution, for proper workload\nmanagement. All experiments show RADULS to be superior to competing algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 08:20:48 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Kokot", "Marek", ""], ["Deorowicz", "Sebastian", ""], ["Debudaj-Grabysz", "Agnieszka", ""]]}, {"id": "1612.02663", "submitter": "David Harris", "authors": "David G. Harris, Aravind Srinivasan", "title": "A constructive algorithm for the LLL on permutations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there has been significant progress on algorithmic aspects of the\nLov\\'{a}sz Local Lemma (LLL) in recent years, a noteworthy exception is when\nthe LLL is used in the context of random permutations. The breakthrough\nalgorithm of Moser & Tardos only works in the setting of independent variables,\nand does not apply in this context. We resolve this by developing a randomized\npolynomial-time algorithm for such applications. A noteworthy application is\nfor Latin transversals: the best-known general result here (Bissacot et al.,\nimproving on Erd\\H{o}s and Spencer), states that any $n \\times n$ matrix in\nwhich each entry appears at most $(27/256)n$ times, has a Latin transversal. We\npresent the first polynomial-time algorithm to construct such a transversal. We\nalso develop RNC algorithms for Latin transversals, rainbow Hamiltonian cycles,\nstrong chromatic number, and hypergraph packing.\n  In addition to efficiently finding a configuration which avoids bad-events,\nthe algorithm of Moser & Tardos has many powerful extensions and properties.\nThese include a well-characterized distribution on the output distribution,\nparallel algorithms, and a partial resampling variant. We show that our\nalgorithm has nearly all of the same useful properties as the Moser-Tardos\nalgorithm, and present a comparison of this aspect with recent works on the LLL\nin general probability spaces.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 14:29:10 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Harris", "David G.", ""], ["Srinivasan", "Aravind", ""]]}, {"id": "1612.02712", "submitter": "Yingyu Liang", "authors": "Nan Du, Yingyu Liang, Maria-Florina Balcan, Manuel Gomez-Rodriguez,\n  Hongyuan Zha, Le Song", "title": "Scalable Influence Maximization for Multiple Products in Continuous-Time\n  Diffusion Networks", "comments": "45 pages, to appear in Journal of Machine Learning Research. arXiv\n  admin note: substantial text overlap with arXiv:1312.2164, arXiv:1311.3669", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical viral marketing model identifies influential users in a social\nnetwork to maximize a single product adoption assuming unlimited user\nattention, campaign budgets, and time. In reality, multiple products need\ncampaigns, users have limited attention, convincing users incurs costs, and\nadvertisers have limited budgets and expect the adoptions to be maximized soon.\nFacing these user, monetary, and timing constraints, we formulate the problem\nas a submodular maximization task in a continuous-time diffusion model under\nthe intersection of a matroid and multiple knapsack constraints. We propose a\nrandomized algorithm estimating the user influence in a network\n($|\\mathcal{V}|$ nodes, $|\\mathcal{E}|$ edges) to an accuracy of $\\epsilon$\nwith $n=\\mathcal{O}(1/\\epsilon^2)$ randomizations and\n$\\tilde{\\mathcal{O}}(n|\\mathcal{E}|+n|\\mathcal{V}|)$ computations. By\nexploiting the influence estimation algorithm as a subroutine, we develop an\nadaptive threshold greedy algorithm achieving an approximation factor $k_a/(2+2\nk)$ of the optimal when $k_a$ out of the $k$ knapsack constraints are active.\nExtensive experiments on networks of millions of nodes demonstrate that the\nproposed algorithms achieve the state-of-the-art in terms of effectiveness and\nscalability.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 16:15:57 GMT"}, {"version": "v2", "created": "Sun, 29 Jan 2017 07:29:49 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Du", "Nan", ""], ["Liang", "Yingyu", ""], ["Balcan", "Maria-Florina", ""], ["Gomez-Rodriguez", "Manuel", ""], ["Zha", "Hongyuan", ""], ["Song", "Le", ""]]}, {"id": "1612.02788", "submitter": "Nikhil Vyas", "authors": "Nikhil Bansal, Shashwat Garg, Jesper Nederlof, Nikhil Vyas", "title": "Faster Space-Efficient Algorithms for Subset Sum, k-Sum and Related\n  Problems", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present space efficient Monte Carlo algorithms that solve Subset Sum and\nKnapsack instances with $n$ items using $O^*(2^{0.86n})$ time and polynomial\nspace, where the $O^*(\\cdot)$ notation suppresses factors polynomial in the\ninput size. Both algorithms assume random read-only access to random bits.\nModulo this mild assumption, this resolves a long-standing open problem in\nexact algorithms for NP-hard problems. These results can be extended to solve\nBinary Linear Programming on $n$ variables with few constraints in a similar\nrunning time. We also show that for any constant $k\\geq 2$, random instances of\n$k$-Sum can be solved using $O(n^{k-0.5}polylog(n))$ time and $O(\\log n)$\nspace, without the assumption of random access to random bits.\n  Underlying these results is an algorithm that determines whether two given\nlists of length $n$ with integers bounded by a polynomial in $n$ share a common\nvalue. Assuming random read-only access to random bits, we show that this\nproblem can be solved using $O(\\log n)$ space significantly faster than the\ntrivial $O(n^2)$ time algorithm if no value occurs too often in the same list.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 20:09:55 GMT"}, {"version": "v2", "created": "Sat, 24 Jun 2017 18:35:30 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Bansal", "Nikhil", ""], ["Garg", "Shashwat", ""], ["Nederlof", "Jesper", ""], ["Vyas", "Nikhil", ""]]}, {"id": "1612.02912", "submitter": "Anilesh Kollagunta Krishnaswamy", "authors": "Ashish Goel, Anilesh Kollagunta Krishnaswamy, Kamesh Munagala", "title": "Metric Distortion of Social Choice Rules: Lower Bounds and Fairness\n  Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study social choice rules under the utilitarian distortion framework, with\nan additional metric assumption on the agents' costs over the alternatives. In\nthis approach, these costs are given by an underlying metric on the set of all\nagents plus alternatives. Social choice rules have access to only the ordinal\npreferences of agents but not the latent cardinal costs that induce them.\nDistortion is then defined as the ratio between the social cost (typically the\nsum of agent costs) of the alternative chosen by the mechanism at hand, and\nthat of the optimal alternative chosen by an omniscient algorithm. The\nworst-case distortion of a social choice rule is, therefore, a measure of how\nclose it always gets to the optimal alternative without any knowledge of the\nunderlying costs. Under this model, it has been conjectured that Ranked Pairs,\nthe well-known weighted-tournament rule, achieves a distortion of at most 3\n[Anshelevich et al. 2015]. We disprove this conjecture by constructing a\nsequence of instances which shows that the worst-case distortion of Ranked\nPairs is at least 5. Our lower bound on the worst case distortion of Ranked\nPairs matches a previously known upper bound for the Copeland rule, proving\nthat in the worst case, the simpler Copeland rule is at least as good as Ranked\nPairs. And as long as we are limited to (weighted or unweighted) tournament\nrules, we demonstrate that randomization cannot help achieve an expected\nworst-case distortion of less than 3. Using the concept of approximate\nmajorization within the distortion framework, we prove that Copeland and\nRandomized Dictatorship achieve low constant factor fairness-ratios (5 and 3\nrespectively), which is a considerable generalization of similar results for\nthe sum of costs and single largest cost objectives. In addition to all of the\nabove, we outline several interesting directions for further research in this\nspace.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 04:33:24 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 16:41:31 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Goel", "Ashish", ""], ["Krishnaswamy", "Anilesh Kollagunta", ""], ["Munagala", "Kamesh", ""]]}, {"id": "1612.02914", "submitter": "Aleksandar Nikolov", "authors": "Assimakis Kattis, Aleksandar Nikolov", "title": "Lower Bounds for Differential Privacy from Gaussian Width", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the optimal sample complexity of a given workload of linear queries\nunder the constraints of differential privacy. The sample complexity of a query\nanswering mechanism under error parameter $\\alpha$ is the smallest $n$ such\nthat the mechanism answers the workload with error at most $\\alpha$ on any\ndatabase of size $n$. Following a line of research started by Hardt and Talwar\n[STOC 2010], we analyze sample complexity using the tools of asymptotic convex\ngeometry. We study the sensitivity polytope, a natural convex body associated\nwith a query workload that quantifies how query answers can change between\nneighboring databases. This is the information that, roughly speaking, is\nprotected by a differentially private algorithm, and, for this reason, we\nexpect that a \"bigger\" sensitivity polytope implies larger sample complexity.\nOur results identify the mean Gaussian width as an appropriate measure of the\nsize of the polytope, and show sample complexity lower bounds in terms of this\nquantity. Our lower bounds completely characterize the workloads for which the\nGaussian noise mechanism is optimal up to constants as those having\nasymptotically maximal Gaussian width.\n  Our techniques also yield an alternative proof of Pisier's Volume Number\nTheorem which also suggests an approach to improving the parameters of the\ntheorem.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 04:44:52 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Kattis", "Assimakis", ""], ["Nikolov", "Aleksandar", ""]]}, {"id": "1612.02962", "submitter": "Ran Ben Bassat", "authors": "Ran Ben Basat, Gil Einziger, Roy Friedman, Yaron Kassner", "title": "Randomized Admission Policy for Efficient Top-k and Frequency Estimation", "comments": "Conference version accepted to IEEE INFOCOM2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network management protocols often require timely and meaningful insight\nabout per flow network traffic. This paper introduces Randomized Admission\nPolicy (RAP) - a novel algorithm for the frequency and top-k estimation\nproblems, which are fundamental in network monitoring. We demonstrate space\nreductions compared to the alternatives by a factor of up to 32 on real packet\ntraces and up to 128 on heavy-tailed workloads. For top-k identification, RAP\nexhibits memory savings by a factor of between 4 and 64 depending on the skew\nof the workload. These empirical results are backed by formal analysis,\nindicating the asymptotic space improvement of our probabilistic admission\napproach. Additionally, we present d-Way RAP, a hardware friendly variant of\nRAP that empirically maintains its space and accuracy benefits.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 10:06:40 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Basat", "Ran Ben", ""], ["Einziger", "Gil", ""], ["Friedman", "Roy", ""], ["Kassner", "Yaron", ""]]}, {"id": "1612.02990", "submitter": "Yuk Kuroki", "authors": "Yuko Kuroki and Tomomi Matsui", "title": "Approximation Algorithm for Cycle-Star Hub Network Design Problems and\n  Cycle-Metric Labeling Problems", "comments": "14 pages, 2 figures; to appear in WALCOM 2017", "journal-ref": "Journal of Graph Algorithms and Applications (JGAA), vol. 23\n  (2019), No. 1, pp. 93-110", "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a single allocation hub-and-spoke network design problem which\nallocates each non-hub node to exactly one of given hub nodes so as to minimize\nthe total transportation cost. This paper deals with a case in which the hubs\nare located in a cycle, which is called a cycle-star hub network design\nproblem. The problem is essentially equivalent to a cycle-metric labeling\nproblem. The problem is useful in the design of networks in telecommunications\nand airline transportation systems.We propose a $2(1-1/h)$-approximation\nalgorithm where $h$ denotes the number of hub nodes. Our algorithm solves a\nlinear relaxation problem and employs a dependent rounding procedure. We\nanalyze our algorithm by approximating a given cycle-metric matrix by a convex\ncombination of Monge matrices.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 12:04:20 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kuroki", "Yuko", ""], ["Matsui", "Tomomi", ""]]}, {"id": "1612.03017", "submitter": "Gramoz Goranci", "authors": "Gramoz Goranci, Harald Raecke", "title": "Vertex Sparsification in Trees", "comments": "An extended abstract will appear in Proceedings of WAOA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an unweighted tree $T=(V,E)$ with terminals $K \\subset V$, we show how\nto obtain a $2$-quality vertex flow and cut sparsifier $H$ with $V_H = K$. We\nprove that our result is essentially tight by providing a $2-o(1)$ lower-bound\non the quality of any cut sparsifier for stars. In addition we give improved\nresults for quasi-bipartite graphs. First, we show how to obtain a $2$-quality\nflow sparsifier with $V_H = K$ for such graphs. We then consider the other\nextreme and construct exact sparsifiers of size $O(2^{k})$, when the input\ngraph is unweighted.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 13:41:05 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Goranci", "Gramoz", ""], ["Raecke", "Harald", ""]]}, {"id": "1612.03097", "submitter": "Rahil Sharma", "authors": "Rahil Sharma", "title": "Hard Capacitated Set Cover and Uncapacitated Geometric Set Cover", "comments": "This report has original work mentioned in Section 2.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first part of this report describes the following result that,\nlogarithmic approximation factor for hard capacitated set cover can be achieved\nfrom Wolsey's work [9], using a simpler and more intuitive analysis. We further\nshow in our work, that O(log n) approximation factor can be achieved for the\nsame problem by applying analysis of general set cover to analyze Wolsey's\nalgorithm [5]. This work is based on the key observation that we make in Lemma\n3 of this report. The second part of the report describes the geometric hitting\nset problem, where X is a ground set of points in a plane and S is a set of\naxis parallel rectangles. It is shown that epsilon-nets of size O(1/epsilon log\nlog 1/epsilon) can be computed in polynomial time. Applying Bronnimann and\nGoodrich result [3] gives the hitting set of size O(log log OPT) for this\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 04:54:25 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Sharma", "Rahil", ""]]}, {"id": "1612.03147", "submitter": "Gautam Kamath", "authors": "Constantinos Daskalakis, Nishanth Dikkala, Gautam Kamath", "title": "Testing Ising Models", "comments": "Appeared SODA 2018. Final version to appear in IEEE Transactions on\n  Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given samples from an unknown multivariate distribution $p$, is it possible\nto distinguish whether $p$ is the product of its marginals versus $p$ being far\nfrom every product distribution? Similarly, is it possible to distinguish\nwhether $p$ equals a given distribution $q$ versus $p$ and $q$ being far from\neach other? These problems of testing independence and goodness-of-fit have\nreceived enormous attention in statistics, information theory, and theoretical\ncomputer science, with sample-optimal algorithms known in several interesting\nregimes of parameters. Unfortunately, it has also been understood that these\nproblems become intractable in large dimensions, necessitating exponential\nsample complexity.\n  Motivated by the exponential lower bounds for general distributions as well\nas the ubiquity of Markov Random Fields (MRFs) in the modeling of\nhigh-dimensional distributions, we initiate the study of distribution testing\non structured multivariate distributions, and in particular the prototypical\nexample of MRFs: the Ising Model. We demonstrate that, in this structured\nsetting, we can avoid the curse of dimensionality, obtaining sample and time\nefficient testers for independence and goodness-of-fit. One of the key\ntechnical challenges we face along the way is bounding the variance of\nfunctions of the Ising model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 20:04:56 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 05:34:14 GMT"}, {"version": "v3", "created": "Fri, 7 Apr 2017 15:06:28 GMT"}, {"version": "v4", "created": "Mon, 30 Oct 2017 20:34:46 GMT"}, {"version": "v5", "created": "Tue, 5 Feb 2019 16:33:20 GMT"}, {"version": "v6", "created": "Wed, 10 Jul 2019 22:02:20 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Dikkala", "Nishanth", ""], ["Kamath", "Gautam", ""]]}, {"id": "1612.03148", "submitter": "Anindya De", "authors": "Anindya De and Ryan O'Donnell and Rocco Servedio", "title": "Optimal mean-based algorithms for trace reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the (deletion-channel) trace reconstruction problem, there is an unknown\n$n$-bit source string $x$. An algorithm is given access to independent traces\nof $x$, where a trace is formed by deleting each bit of~$x$ independently with\nprobability~$\\delta$. The goal of the algorithm is to recover~$x$ exactly (with\nhigh probability), while minimizing samples (number of traces) and running\ntime.\n  Previously, the best known algorithm for the trace reconstruction problem was\ndue to Holenstein~et~al.; it uses $\\exp(\\tilde{O}(n^{1/2}))$ samples and\nrunning time for any fixed $0 < \\delta < 1$. It is also what we call a\n\"mean-based algorithm\", meaning that it only uses the empirical means of the\nindividual bits of the traces. Holenstein~et~al.~also gave a lower bound,\nshowing that any mean-based algorithm must use at least $n^{\\tilde{\\Omega}(\\log\nn)}$ samples.\n  In this paper we improve both of these results, obtaining matching upper and\nlower bounds for mean-based trace reconstruction. For any constant deletion\nrate $0 < \\delta < 1$, we give a mean-based algorithm that uses\n$\\exp(O(n^{1/3}))$ time and traces; we also prove that any mean-based algorithm\nmust use at least $\\exp(\\Omega(n^{1/3}))$ traces. In fact, we obtain matching\nupper and lower bounds even for $\\delta$ subconstant and $\\rho := 1-\\delta$\nsubconstant: when $(\\log^3 n)/n \\ll \\delta \\leq 1/2$ the bound is\n$\\exp(-\\Theta(\\delta n)^{1/3})$, and when $1/\\sqrt{n} \\ll \\rho \\leq 1/2$ the\nbound is $\\exp(-\\Theta(n/\\rho)^{1/3})$.\n  Our proofs involve estimates for the maxima of Littlewood polynomials on\ncomplex disks. We show that these techniques can also be used to perform trace\nreconstruction with random insertions and bit-flips in addition to deletions.\nWe also find a surprising result: for deletion probabilities $\\delta > 1/2$,\nthe presence of insertions can actually help with trace reconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 20:05:19 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["De", "Anindya", ""], ["O'Donnell", "Ryan", ""], ["Servedio", "Rocco", ""]]}, {"id": "1612.03156", "submitter": "Cl\\'ement Canonne", "authors": "Clement Canonne, Ilias Diakonikolas, Daniel Kane, Alistair Stewart", "title": "Testing Bayesian Networks", "comments": "To appear in IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work initiates a systematic investigation of testing high-dimensional\nstructured distributions by focusing on testing Bayesian networks -- the\nprototypical family of directed graphical models. A Bayesian network is defined\nby a directed acyclic graph, where we associate a random variable with each\nnode. The value at any particular node is conditionally independent of all the\nother non-descendant nodes once its parents are fixed. Specifically, we study\nthe properties of identity testing and closeness testing of Bayesian networks.\nOur main contribution is the first non-trivial efficient testing algorithms for\nthese problems and corresponding information-theoretic lower bounds. For a wide\nrange of parameter settings, our testing algorithms have sample complexity\nsublinear in the dimension and are sample-optimal, up to constant factors.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 20:34:40 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 02:49:37 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Canonne", "Clement", ""], ["Diakonikolas", "Ilias", ""], ["Kane", "Daniel", ""], ["Stewart", "Alistair", ""]]}, {"id": "1612.03161", "submitter": "Brendan Lucier", "authors": "Paul D\\\"utting, Michal Feldman, Thomas Kesselheim and Brendan Lucier", "title": "Prophet Inequalities Made Easy: Stochastic Optimization by Pricing\n  Non-Stochastic Inputs", "comments": "Extended abstract appeared in the Proceedings of the 58th IEEE\n  Symposium on Foundations of Computer Science, FOCS 2017, Berkeley, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for stochastic online maximization problems\nwith combinatorial feasibility constraints. The framework establishes prophet\ninequalities by constructing price-based online approximation algorithms, a\nnatural extension of threshold algorithms for settings beyond binary selection.\nOur analysis takes the form of an extension theorem: we derive sufficient\nconditions on prices when all weights are known in advance, then prove that the\nresulting approximation guarantees extend directly to stochastic settings. Our\nframework unifies and simplifies much of the existing literature on prophet\ninequalities and posted price mechanisms, and is used to derive new and\nimproved results for combinatorial markets (with and without complements),\nmulti-dimensional matroids, and sparse packing problems. Finally, we highlight\na surprising connection between the smoothness framework for bounding the price\nof anarchy of mechanisms and our framework, and show that many smooth\nmechanisms can be recast as posted price mechanisms with comparable performance\nguarantees.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 20:50:32 GMT"}, {"version": "v2", "created": "Sun, 9 Jul 2017 19:22:37 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["D\u00fctting", "Paul", ""], ["Feldman", "Michal", ""], ["Kesselheim", "Thomas", ""], ["Lucier", "Brendan", ""]]}, {"id": "1612.03308", "submitter": "Adri\\'an G\\'omez-Brand\\'on", "authors": "Nieves R. Brisaboa, Adri\\'an G\\'omez-Brand\\'on, Gonzalo Navarro,\n  Jos\\'e R. Param\\'a", "title": "GraCT: A Grammar based Compressed representation of Trajectories", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "String Processing and Information Retrieval: 23rd International\n  Symposium, SPIRE 2016, Beppu, Japan, October 18-20, 2016, Proceedings.\n  Springer International Publishing. pp 218-230. ISBN: 9783319460482", "doi": "10.1007/978-3-319-46049-9_21", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a compressed data structure to store free trajectories of moving\nobjects (ships over the sea, for example) allowing spatio-temporal queries. Our\nmethod, GraCT, uses a $k^2$-tree to store the absolute positions of all objects\nat regular time intervals (snapshots), whereas the positions between snapshots\nare represented as logs of relative movements compressed with Re-Pair. Our\nexperimental evaluation shows important savings in space and time with respect\nto a fair baseline.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 15:50:19 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["G\u00f3mez-Brand\u00f3n", "Adri\u00e1n", ""], ["Navarro", "Gonzalo", ""], ["Param\u00e1", "Jos\u00e9 R.", ""]]}, {"id": "1612.03339", "submitter": "Juli\\'an Mestre", "authors": "Maurice Cheung and Juli\\'an Mestre and David B. Shmoys and Jos\\'e\n  Verschae", "title": "A Primal-Dual Approximation Algorithm for Min-Sum Single-Machine\n  Scheduling Problems", "comments": "26 pages. A preliminary version appeared in APPROX 2011. arXiv admin\n  note: text overlap with arXiv:1403.0298", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following single-machine scheduling problem, which is often\ndenoted $1||\\sum f_{j}$: we are given $n$ jobs to be scheduled on a single\nmachine, where each job $j$ has an integral processing time $p_j$, and there is\na nondecreasing, nonnegative cost function $f_j(C_{j})$ that specifies the cost\nof finishing $j$ at time $C_{j}$; the objective is to minimize $\\sum_{j=1}^n\nf_j(C_j)$. Bansal \\& Pruhs recently gave the first constant approximation\nalgorithm with a performance guarantee of 16. We improve on this result by\ngiving a primal-dual pseudo-polynomial-time algorithm based on the recently\nintroduced knapsack-cover inequalities. The algorithm finds a schedule of cost\nat most four times the constructed dual solution. Although we show that this\nbound is tight for our algorithm, we leave open the question of whether the\nintegrality gap of the LP is less than 4. Finally, we show how the technique\ncan be adapted to yield, for any $\\epsilon >0$, a $(4+\\epsilon )$-approximation\nalgorithm for this problem.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 20:36:09 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Cheung", "Maurice", ""], ["Mestre", "Juli\u00e1n", ""], ["Shmoys", "David B.", ""], ["Verschae", "Jos\u00e9", ""]]}, {"id": "1612.03343", "submitter": "Johannes Schneider", "authors": "Johannes Schneider", "title": "Oblivious Sorting and Queues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic oblivious LIFO (Stack), FIFO, double-ended and\ndouble-ended priority queue as well as an oblivious mergesort and quicksort\nalgorithm. Our techniques and ideas include concatenating queues end-to-end,\nsize balancing of multiple arrays, several multi-level partitionings of an\narray. Our queues are the first to enable executions of pop and push operations\nwithout any change of the data structure (controlled by a parameter). This\nenables interesting applications in computing on encrypted data such as hiding\nconfidential expressions. Mergesort becomes practical using our LIFO queue, ie.\nit improves prior work (STOC '14) by a factor of (more than) 1000 in terms of\ncomparisons for all practically relevant queue sizes. We are the first to\npresent double-ended (priority) and LIFO queues as well as oblivious quicksort\nwhich is asymptotically optimal. Aside from theortical analysis, we also\nprovide an empirical evaluation of all queues.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 21:04:55 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Schneider", "Johannes", ""]]}, {"id": "1612.03456", "submitter": "Bryce Sandlund", "authors": "Eric Bach and Bryce Sandlund", "title": "Baby-Step Giant-Step Algorithms for the Symmetric Group", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DS math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study discrete logarithms in the setting of group actions. Suppose that\n$G$ is a group that acts on a set $S$. When $r,s \\in S$, a solution $g \\in G$\nto $r^g = s$ can be thought of as a kind of logarithm. In this paper, we study\nthe case where $G = S_n$, and develop analogs to the Shanks baby-step /\ngiant-step procedure for ordinary discrete logarithms. Specifically, we compute\ntwo sets $A, B \\subseteq S_n$ such that every permutation of $S_n$ can be\nwritten as a product $ab$ of elements $a \\in A$ and $b \\in B$. Our\ndeterministic procedure is optimal up to constant factors, in the sense that\n$A$ and $B$ can be computed in optimal asymptotic complexity, and $|A|$ and\n$|B|$ are a small constant from $\\sqrt{n!}$ in size. We also analyze randomized\n\"collision\" algorithms for the same problem.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 19:15:48 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Bach", "Eric", ""], ["Sandlund", "Bryce", ""]]}, {"id": "1612.03461", "submitter": "Renato J Cintra", "authors": "V. A. Coutinho, R. J. Cintra, F. M. Bayer, S. Kulasekera, A.\n  Madanayake", "title": "Low-complexity Pruned 8-point DCT Approximations for Image Encoding", "comments": "13 pages, 6 figures, 3 tables", "journal-ref": null, "doi": "10.1109/CONIELECOMP.2015.7086923", "report-no": null, "categories": "cs.MM cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two multiplierless pruned 8-point discrete cosine transform (DCT)\napproximation are presented. Both transforms present lower arithmetic\ncomplexity than state-of-the-art methods. The performance of such new methods\nwas assessed in the image compression context. A JPEG-like simulation was\nperformed, demonstrating the adequateness and competitiveness of the introduced\nmethods. Digital VLSI implementation in CMOS technology was also considered.\nBoth presented methods were realized in Berkeley Emulation Engine (BEE3).\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 19:41:44 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Coutinho", "V. A.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""]]}, {"id": "1612.03607", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, Felix Reidl and Magnus Wahlstr\\\"om", "title": "$k$-Distinct In- and Out-Branchings in Digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An out-branching and an in-branching of a digraph $D$ are called $k$-distinct\nif each of them has $k$ arcs absent in the other. Bang-Jensen, Saurabh and\nSimonsen (2016) proved that the problem of deciding whether a strongly\nconnected digraph $D$ has $k$-distinct out-branching and in-branching is\nfixed-parameter tractable (FPT) when parameterized by $k$. They asked whether\nthe problem remains FPT when extended to arbitrary digraphs. Bang-Jensen and\nYeo (2008) asked whether the same problem is FPT when the out-branching and\nin-branching have the same root. By linking the two problems with the problem\nof whether a digraph has an out-branching with at least $k$ leaves (a leaf is a\nvertex of out-degree zero), we first solve the problem of Bang-Jensen and Yeo\n(2008). We then develop a new digraph decomposition called the rooted cut\ndecomposition and using it we prove that the problem of Bang-Jensen et al.\n(2016) is FPT for all digraphs. We believe that the \\emph{rooted cut\ndecomposition} will be useful for obtaining other results on digraphs.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 10:45:01 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 15:25:38 GMT"}, {"version": "v3", "created": "Mon, 15 Jan 2018 09:15:18 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Gutin", "Gregory", ""], ["Reidl", "Felix", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "1612.03735", "submitter": "Israela Solomon", "authors": "Israela Solomon", "title": "A Note on Testing Intersection of Convex Sets in Sublinear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple sublinear time algorithm for testing the following\ngeometric property. Let $P_1, ..., P_n$ be $n$ convex sets in $\\mathbb{R}^d$\n($n \\gg d$), such as polytopes, balls, etc. We assume that the complexity of\neach set depends only on $d$ (and not on the number of sets $n$). We test the\nproperty that there exists a common point in all sets, i.e. that their\nintersection is nonempty. Our goal is to distinguish between the case where the\nintersection is nonempty, and the case where even after removing many of the\nsets the intersection is empty. In particular, our algorithm returns PASS if\nall of the $n$ sets intersect, and returns FAIL with probability at least\n$1-\\epsilon$ if no point belongs to $\\frac{\\alpha}{d+1} n$ sets, for any given\n$0 < \\alpha, \\epsilon < 1$.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 23:36:09 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Solomon", "Israela", ""]]}, {"id": "1612.03856", "submitter": "Sebastian Krinninger", "authors": "Monika Henzinger, Sebastian Krinninger, Danupon Nanongkai", "title": "Improved Algorithms for Decremental Single-Source Reachability on\n  Directed Graphs", "comments": "This paper was presented at the International Colloquium on Automata,\n  Languages and Programming (ICALP) 2015. A full version combining the findings\n  of this paper and its predecessor [Henzinger et al. STOC 2014] is available\n  at arXiv:1504.07959", "journal-ref": null, "doi": "10.1007/978-3-662-47672-7_59", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently we presented the first algorithm for maintaining the set of nodes\nreachable from a source node in a directed graph that is modified by edge\ndeletions with $o(mn)$ total update time, where $m$ is the number of edges and\n$n$ is the number of nodes in the graph [Henzinger et al. STOC 2014]. The\nalgorithm is a combination of several different algorithms, each for a\ndifferent $m$ vs. $n$ trade-off. For the case of $m = \\Theta(n^{1.5})$ the\nrunning time is $O(n^{2.47})$, just barely below $mn = \\Theta(n^{2.5})$. In\nthis paper we simplify the previous algorithm using new algorithmic ideas and\nachieve an improved running time of $\\tilde O(\\min(m^{7/6} n^{2/3}, m^{3/4}\nn^{5/4 + o(1)}, m^{2/3} n^{4/3+o(1)} + m^{3/7} n^{12/7+o(1)}))$. This gives,\ne.g., $O(n^{2.36})$ for the notorious case $m = \\Theta(n^{1.5})$. We obtain the\nsame upper bounds for the problem of maintaining the strongly connected\ncomponents of a directed graph undergoing edge deletions. Our algorithms are\ncorrect with high probabililty against an oblivious adversary.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 19:14:47 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Henzinger", "Monika", ""], ["Krinninger", "Sebastian", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1612.03880", "submitter": "Piyush Srivastava", "authors": "Quentin Berthet, Philippe Rigollet, Piyush Srivastava", "title": "Exact recovery in the Ising blockmodel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem associated to recovering the block structure of an\nIsing model given independent observations on the binary hypercube. This new\nmodel, called the Ising blockmodel, is a perturbation of the mean field\napproximation of the Ising model known as the Curie-Weiss model: the sites are\npartitioned into two blocks of equal size and the interaction between those of\nthe same block is stronger than across blocks, to account for more order within\neach block. We study probabilistic, statistical and computational aspects of\nthis model in the high-dimensional case when the number of sites may be much\nlarger than the sample size.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 20:15:54 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 18:28:36 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Berthet", "Quentin", ""], ["Rigollet", "Philippe", ""], ["Srivastava", "Piyush", ""]]}, {"id": "1612.04037", "submitter": "EPTCS", "authors": "Jan Bouda, Luk\\'a\\v{s} Hol\\'ik, Jan Kofro\\v{n}, Jan Strej\\v{c}ek, Adam\n  Rambousek", "title": "Proceedings 11th Doctoral Workshop on Mathematical and Engineering\n  Methods in Computer Science", "comments": null, "journal-ref": "EPTCS 233, 2016", "doi": "10.4204/EPTCS.233", "report-no": null, "categories": "cs.LO cs.DS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MEMICS provides a forum for doctoral students interested in applications of\nmathematical and engineering methods in computer science. Besides a rich\ntechnical programme (including invited talks, regular papers, and\npresentations), MEMICS also offers friendly social activities and exciting\nopportunities for meeting like-minded people. MEMICS submissions traditionally\ncover all areas of computer science (such as parallel and distributed\ncomputing, computer networks, modern hardware and its design, non-traditional\ncomputing architectures, information systems and databases, multimedia and\ngraphics, verification and testing, computer security, as well as all related\nareas of theoretical computer science).\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 05:47:19 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Bouda", "Jan", ""], ["Hol\u00edk", "Luk\u00e1\u0161", ""], ["Kofro\u0148", "Jan", ""], ["Strej\u010dek", "Jan", ""], ["Rambousek", "Adam", ""]]}, {"id": "1612.04094", "submitter": "Fernando Silva-Coira", "authors": "Nieves R. Brisaboa, Ana Cerdeira-Pena, Narciso L\\'opez-L\\'opez,\n  Gonzalo Navarro, Miguel R. Penabad, Fernando Silva-Coira", "title": "Efficient Representation of Multidimensional Data over Hierarchical\n  Domains", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "String Processing and Information Retrieval: 23rd International\n  Symposium, SPIRE 2016, Beppu, Japan, October 18-20, 2016, Proceedings.\n  Springer International Publishing. pp 191-203. ISBN: 9783319460482", "doi": "10.1007/978-3-319-46049-9_19", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of representing multidimensional data where the\ndomain of each dimension is organized hierarchically, and the queries require\nsummary information at a different node in the hierarchy of each dimension.\nThis is the typical case of OLAP databases. A basic approach is to represent\neach hierarchy as a one-dimensional line and recast the queries as\nmultidimensional range queries. This approach can be implemented compactly by\ngeneralizing to more dimensions the $k^2$-treap, a compact representation of\ntwo-dimensional points that allows for efficient summarization queries along\ngeneric ranges. Instead, we propose a more flexible generalization, which\ninstead of a generic quadtree-like partition of the space, follows the domain\nhierarchies across each dimension to organize the partitioning. The resulting\nstructure is much more efficient than a generic multidimensional structure,\nsince queries are resolved by aggregating much fewer nodes of the tree.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 10:52:03 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Cerdeira-Pena", "Ana", ""], ["L\u00f3pez-L\u00f3pez", "Narciso", ""], ["Navarro", "Gonzalo", ""], ["Penabad", "Miguel R.", ""], ["Silva-Coira", "Fernando", ""]]}, {"id": "1612.04208", "submitter": "Yijie Han", "authors": "Yijie Han", "title": "An \\~{O}$(n^2)$ Time Matrix Multiplication Algorithm", "comments": "Version 11 and Version 12 section 2 laid the foundation of this\n  algorithm but has a problem unresolved. This version is a tuned version of\n  Section 2 of Version 12. This version corrects the problem in Version 11 and\n  Section 2 of Version 12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show, for the input vectors $(a_0, a_1, ..., a_{n-1})$ and $(b_0, b_1,\n..., b_{n-1})$, where $a_i$'s and $b_j$'s are real numbers, after \\~{O}$(n)$\ntime preprocessing for each of them, the vector multiplication $(a_0, a_1, ...,\na_{n-1})(b_0, b_1, ..., b_{n-1})^T$ can be computed in \\~{O}$(1)$ time. This\nenables the matrix multiplication of two $n\\times n$ matrices to be computed in\n\\~{O}$(n^2)$ time.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 21:53:22 GMT"}, {"version": "v10", "created": "Tue, 17 Sep 2019 23:02:55 GMT"}, {"version": "v11", "created": "Sun, 12 Apr 2020 19:00:39 GMT"}, {"version": "v12", "created": "Sun, 3 May 2020 16:45:07 GMT"}, {"version": "v13", "created": "Mon, 1 Jun 2020 17:15:17 GMT"}, {"version": "v14", "created": "Wed, 1 Jul 2020 14:40:14 GMT"}, {"version": "v15", "created": "Mon, 7 Sep 2020 18:21:27 GMT"}, {"version": "v16", "created": "Sun, 11 Oct 2020 12:26:55 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 01:25:47 GMT"}, {"version": "v3", "created": "Wed, 1 Mar 2017 17:11:57 GMT"}, {"version": "v4", "created": "Thu, 9 Mar 2017 15:17:36 GMT"}, {"version": "v5", "created": "Sun, 12 Mar 2017 20:34:29 GMT"}, {"version": "v6", "created": "Sun, 26 Mar 2017 00:33:26 GMT"}, {"version": "v7", "created": "Mon, 15 Jul 2019 08:49:29 GMT"}, {"version": "v8", "created": "Thu, 18 Jul 2019 23:19:04 GMT"}, {"version": "v9", "created": "Sat, 14 Sep 2019 20:30:07 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Han", "Yijie", ""]]}, {"id": "1612.04209", "submitter": "Daniil Galaktionov", "authors": "Nieves R. Brisaboa, Antonio Fari\\~na, Daniil Galaktionov, M. Andrea\n  Rodr\\'iguez", "title": "Compact Trip Representation over Networks", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "23rd International Symposium, SPIRE 2016, Beppu, Japan, October\n  18-20, 2016, Proceedings pp 240-253", "doi": "10.1007/978-3-319-46049-9_23", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new Compact Trip Representation (CTR) that allows us to manage\nusers' trips (moving objects) over networks. These could be public\ntransportation networks (buses, subway, trains, and so on) where nodes are\nstations or stops, or road networks where nodes are intersections. CTR\nrepresents the sequences of nodes and time instants in users' trips. The\nspatial component is handled with a data structure based on the well-known\nCompressed Suffix Array (CSA), which provides both a compact representation and\ninteresting indexing capabilities. We also represent the temporal component of\nthe trips, that is, the time instants when users visit nodes in their trips. We\ncreate a sequence with these time instants, which are then self-indexed with a\nbalanced Wavelet Matrix (WM). This gives us the ability to solve range-interval\nqueries efficiently. We show how CTR can solve relevant spatial and\nspatio-temporal queries over large sets of trajectories. Finally, we also\nprovide experimental results to show the space requirements and query\nefficiency of CTR.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 14:45:29 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Fari\u00f1a", "Antonio", ""], ["Galaktionov", "Daniil", ""], ["Rodr\u00edguez", "M. Andrea", ""]]}, {"id": "1612.04304", "submitter": "Aleksandar Nikolov", "authors": "Daniel Dadush, Shashwat Garg, Shachar Lovett, Aleksandar Nikolov", "title": "Towards a Constructive Version of Banaszczyk's Vector Balancing Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.FA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important theorem of Banaszczyk (Random Structures & Algorithms `98)\nstates that for any sequence of vectors of $\\ell_2$ norm at most $1/5$ and any\nconvex body $K$ of Gaussian measure $1/2$ in $\\mathbb{R}^n$, there exists a\nsigned combination of these vectors which lands inside $K$. A major open\nproblem is to devise a constructive version of Banaszczyk's vector balancing\ntheorem, i.e. to find an efficient algorithm which constructs the signed\ncombination.\n  We make progress towards this goal along several fronts. As our first\ncontribution, we show an equivalence between Banaszczyk's theorem and the\nexistence of $O(1)$-subgaussian distributions over signed combinations. For the\ncase of symmetric convex bodies, our equivalence implies the existence of a\nuniversal signing algorithm (i.e. independent of the body), which simply\nsamples from the subgaussian sign distribution and checks to see if the\nassociated combination lands inside the body. For asymmetric convex bodies, we\nprovide a novel recentering procedure, which allows us to reduce to the case\nwhere the body is symmetric.\n  As our second main contribution, we show that the above framework can be\nefficiently implemented when the vectors have length $O(1/\\sqrt{\\log n})$,\nrecovering Banaszczyk's results under this stronger assumption. More precisely,\nwe use random walk techniques to produce the required $O(1)$-subgaussian\nsigning distributions when the vectors have length $O(1/\\sqrt{\\log n})$, and\nuse a stochastic gradient ascent method to implement the recentering procedure\nfor asymmetric bodies.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 18:12:44 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Dadush", "Daniel", ""], ["Garg", "Shashwat", ""], ["Lovett", "Shachar", ""], ["Nikolov", "Aleksandar", ""]]}, {"id": "1612.04319", "submitter": "Igor Shinkar", "authors": "Igor Shinkar", "title": "On Coloring Random Subgraphs of a Fixed Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an arbitrary graph $G$ we study the chromatic number of a random\nsubgraph $G_{1/2}$ obtained from $G$ by removing each edge independently with\nprobability $1/2$. Studying $\\chi(G_{1/2})$ has been suggested by\nBukh~\\cite{Bukh}, who asked whether $\\mathbb{E}[\\chi(G_{1/2})] \\geq \\Omega(\n\\chi(G)/\\log(\\chi(G)))$ holds for all graphs $G$. In this paper we show that\nfor any graph $G$ with chromatic number $k = \\chi(G)$ and for all $d \\leq\nk^{1/3}$ it holds that $\\Pr[\\chi(G_{1/2}) \\leq d] < \\exp \\left(-\n\\Omega\\left(\\frac{k(k-d^3)}{d^3}\\right)\\right)$. In particular, $\\Pr[G_{1/2}\n\\text{ is bipartite}] < \\exp \\left(- \\Omega \\left(k^2 \\right)\\right)$. The\nlater bound is tight up to a constant in $\\Omega(\\cdot)$, and is attained when\n$G$ is the complete graph on $k$ vertices.\n  As a technical lemma, that may be of independent interest, we prove that if\nin \\emph{any} $d^3$ coloring of the vertices of $G$ there are at least $t$\nmonochromatic edges, then $\\Pr[\\chi(G_{1/2}) \\leq d] < e^{-\n\\Omega\\left(t\\right)}$.\n  We also prove that for any graph $G$ with chromatic number $k = \\chi(G)$ and\nindependence number $\\alpha(G) \\leq O(n/k)$ it holds that\n$\\mathbb{E}[\\chi(G_{1/2})] \\geq \\Omega \\left( k/\\log(k) \\right)$. This gives a\npositive answer to the question of Bukh for a large family of graphs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 19:00:55 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 20:49:34 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Shinkar", "Igor", ""]]}, {"id": "1612.04484", "submitter": "Wusuo Liu", "authors": "Charlie Wusuo Liu", "title": "FLSSS: A Novel Algorithmic Framework for Combinatorial Optimization\n  Problems in the Subset Sum Family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article details the algorithmics in FLSSS, an R package for solving\nvarious subset sum problems. The fundamental algorithm engages the problem via\ncombinatorial space compression adaptive to constraints, relaxations and\nvariations that are often crucial for data analytics in practice. Such\nadaptation conversely enables the compression algorithm to drain every bit of\ninformation a sorted superset could bring for rapid convergence.\nMultidimensional extension follows a novel decomposition of the problem and is\nfriendly to multithreading. Data structures supporting the algorithms have\ntrivial space complexity. The framework offers exact algorithms for the\nmultidimensional knapsack problem and the generalized assignment problem.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 04:34:27 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 17:35:36 GMT"}, {"version": "v3", "created": "Fri, 23 Nov 2018 21:19:54 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Liu", "Charlie Wusuo", ""]]}, {"id": "1612.04571", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Liwei Wang", "title": "A Refined Analysis of LSH for Well-dispersed Data Points", "comments": "Paper accepted to SIAM Conference on Analytic Algorithmics and\n  Combinatorics (ANALCO) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near neighbor problems are fundamental in algorithms for high-dimensional\nEuclidean spaces. While classical approaches suffer from the curse of\ndimensionality, locality sensitive hashing (LSH) can effectively solve\na-approximate r-near neighbor problem, and has been proven to be optimal in the\nworst case. However, for real-world data sets, LSH can naturally benefit from\nwell-dispersed data and low doubling dimension, leading to significantly\nimproved performance. In this paper, we address this issue and propose a\nrefined analyses for running time of approximating near neighbors queries via\nLSH. We characterize dispersion of data using N_b, the number of b*r-near pairs\namong the data points. Combined with optimal data-oblivious LSH scheme, we get\na new query time bound depending on N_b and doubling dimension. For many\nnatural scenarios where points are well-dispersed or lying in a\nlow-doubling-dimension space, our result leads to sharper performance than\nexisting worst-case analysis. This paper not only present first rigorous proof\non how LSHs make use of the structure of data points, but also provide\nimportant insights into parameter setting in the practice of LSH beyond worst\ncase. Besides, the techniques in our analysis involve a generalized version of\nsphere packing problem, which might be of some independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 10:52:01 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Mou", "Wenlong", ""], ["Wang", "Liwei", ""]]}, {"id": "1612.04659", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Zhi Wang, Liwei Wang", "title": "Stable Memory Allocation in the Hippocampus: Fundamental Limits and\n  Neural Realization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is believed that hippocampus functions as a memory allocator in brain, the\nmechanism of which remains unrevealed. In Valiant's neuroidal model, the\nhippocampus was described as a randomly connected graph, the computation on\nwhich maps input to a set of activated neuroids with stable size. Valiant\nproposed three requirements for the hippocampal circuit to become a stable\nmemory allocator (SMA): stability, continuity and orthogonality. The\nfunctionality of SMA in hippocampus is essential in further computation within\ncortex, according to Valiant's model.\n  In this paper, we put these requirements for memorization functions into\nrigorous mathematical formulation and introduce the concept of capacity, based\non the probability of erroneous allocation. We prove fundamental limits for the\ncapacity and error probability of SMA, in both data-independent and\ndata-dependent settings. We also establish an example of stable memory\nallocator that can be implemented via neuroidal circuits. Both theoretical\nbounds and simulation results show that the neural SMA functions well.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 14:26:05 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Mou", "Wenlong", ""], ["Wang", "Zhi", ""], ["Wang", "Liwei", ""]]}, {"id": "1612.04666", "submitter": "Nick Duffield", "authors": "Nick Duffield and Balachander Krishnamurthy", "title": "Efficient Sampling for Better OSN Data Provisioning", "comments": "Accepted to appear in the Proceedings of the 54th Annual Allerton\n  Conference on Communication, Control, and Computing, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data concerning the users and usage of Online Social Networks (OSNs) has\nbecome available externally, from public resources (e.g., user profiles),\nparticipation in OSNs (e.g., establishing relationships and recording\ntransactions such as user updates) and APIs of the OSN provider (such as the\nTwitter API). APIs let OSN providers monetize the release of data while helping\ncontrol measurement load, e.g. by providing samples with different\ncost-granularity tradeoffs. To date, this approach has been more suited to\nreleasing transactional data, with graphical data still being obtained by\nresource intensive methods such a graph crawling. In this paper, we propose a\nmethod for OSNs to provide samples of the user graph of tunable size, in\nnon-intersecting increments, with sample selection that can be weighted to\nenhance accuracy when estimating different features of the graph.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 14:41:17 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Duffield", "Nick", ""], ["Krishnamurthy", "Balachander", ""]]}, {"id": "1612.04689", "submitter": "Ruben Becker", "authors": "Ruben Becker, Andreas Karrenbauer, Kurt Mehlhorn", "title": "An Integer Interior Point Method for Min-Cost Flow Using Arc\n  Contractions and Deletions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interior point method for the min-cost flow problem that uses\narc contractions and deletions to steer clear from the boundary of the polytope\nwhen path-following methods come too close. We obtain a randomized algorithm\nrunning in expected $\\tilde O( m^{3/2} )$ time that only visits integer lattice\npoints in the vicinity of the central path of the polytope. This enables us to\nuse integer arithmetic like classical combinatorial algorithms typically do. We\nprovide explicit bounds on the size of the numbers that appear during all\ncomputations. By presenting an integer arithmetic interior point algorithm we\navoid the tediousness of floating point error analysis and achieve a method\nthat is guaranteed to be free of any numerical issues. We thereby eliminate one\nof the drawbacks of numerical methods in contrast to combinatorial min-cost\nflow algorithms that still yield the most efficient implementations in\npractice, despite their inferior worst-case time complexity.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 15:25:50 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Becker", "Ruben", ""], ["Karrenbauer", "Andreas", ""], ["Mehlhorn", "Kurt", ""]]}, {"id": "1612.04782", "submitter": "Rebecca Hoberg", "authors": "Rebecca Hoberg and Thomas Rothvoss", "title": "An Improved Deterministic Rescaling for Linear Programming Algorithms", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The perceptron algorithm for linear programming, arising from machine\nlearning, has been around since the 1950s. While not a polynomial-time\nalgorithm, it is useful in practice due to its simplicity and robustness. In\n2004, Dunagan and Vempala showed that a randomized rescaling turns the\nperceptron method into a polynomial time algorithm, and later Pe\\~{n}a and\nSoheili gave a deterministic rescaling. In this paper, we give a deterministic\nrescaling for the perceptron algorithm that improves upon the previous\nrescaling methods by making it possible to rescale much earlier. This results\nin a faster running time for the rescaled perceptron algorithm. We will also\ndemonstrate that the same rescaling methods yield a polynomial time algorithm\nbased on the multiplicative weights update method. This draws a connection to\nan area that has received a lot of recent attention in theoretical computer\nscience.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 19:43:43 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Hoberg", "Rebecca", ""], ["Rothvoss", "Thomas", ""]]}, {"id": "1612.04790", "submitter": "Vishnu Narayan", "authors": "Vishnu V. Narayan", "title": "A 17/12-Approximation Algorithm for 2-Vertex-Connected Spanning\n  Subgraphs on Graphs with Minimum Degree At Least 3", "comments": "Revised Lemma 1 and Theorem 2, results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain a polynomial-time 17/12-approximation algorithm for the\nminimum-cost 2-vertex-connected spanning subgraph problem, restricted to graphs\nof minimum degree at least 3. Our algorithm uses the framework of\near-decompositions for approximating connectivity problems, which was\npreviously used in algorithms for finding the smallest 2-edge-connected\nspanning subgraph by Cheriyan, Seb\\H{o} and Szigeti (SIAM J.Discrete Math.\n2001) who gave a 17/12-approximation algorithm for this problem, and by\nSeb\\H{o} and Vygen (Combinatorica 2014), who improved the approximation ratio\nto 4/3.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 20:07:40 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 18:25:21 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Narayan", "Vishnu V.", ""]]}, {"id": "1612.04794", "submitter": "Yang Jiao", "authors": "Yang Jiao, R. Ravi, Wolfgang Gatterbauer", "title": "Algorithms for Automatic Ranking of Participants and Tasks in an\n  Anonymized Contest", "comments": "21 pages, 5 figures, preprint, full version of paper to appear in\n  WALCOM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new set of problems based on the Chain Editing problem. In our\nversion of Chain Editing, we are given a set of anonymous participants and a\nset of undisclosed tasks that every participant attempts. For each\nparticipant-task pair, we know whether the participant has succeeded at the\ntask or not. We assume that participants vary in their ability to solve tasks,\nand that tasks vary in their difficulty to be solved. In an ideal world,\nstronger participants should succeed at a superset of tasks that weaker\nparticipants succeed at. Similarly, easier tasks should be completed\nsuccessfully by a superset of participants who succeed at harder tasks. In\nreality, it can happen that a stronger participant fails at a task that a\nweaker participants succeeds at. Our goal is to find a perfect nesting of the\nparticipant-task relations by flipping a minimum number of participant-task\nrelations, implying such a \"nearest perfect ordering\" to be the one that is\nclosest to the truth of participant strengths and task difficulties. Many\nvariants of the problem are known to be NP-hard.\n  We propose six natural $k$-near versions of the Chain Editing problem and\nclassify their complexity. The input to a $k$-near Chain Editing problem\nincludes an initial ordering of the participants (or tasks) that we are\nrequired to respect by moving each participant (or task) at most $k$ positions\nfrom the initial ordering. We obtain surprising results on the complexity of\nthe six $k$-near problems: Five of the problems are polynomial-time solvable\nusing dynamic programming, but one of them is NP-hard.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 20:20:29 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2016 16:25:51 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Jiao", "Yang", ""], ["Ravi", "R.", ""], ["Gatterbauer", "Wolfgang", ""]]}, {"id": "1612.04985", "submitter": "EPTCS", "authors": "Pavel Dvo\\v{r}\\'ak (Computer Science Institute of Charles University\n  Charles University Prague, Czech Republic), Du\\v{s}an Knop (Department of\n  Applied Mathematics Charles University Prague, Czech Republic), Tom\\'a\\v{s}\n  Masa\\v{r}\\'ik (Department of Applied Mathematics Charles University Prague,\n  Czech Republic)", "title": "Anti-Path Cover on Sparse Graph Classes", "comments": "In Proceedings MEMICS 2016, arXiv:1612.04037", "journal-ref": "EPTCS 233, 2016, pp. 82-86", "doi": "10.4204/EPTCS.233.8", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that it is possible to use Bondy-Chvatal closure to design an FPT\nalgorithm that decides whether or not it is possible to cover vertices of an\ninput graph by at most k vertex disjoint paths in the complement of the input\ngraph. More precisely, we show that if a graph has tree-width at most w and its\ncomplement is closed under Bondy-Chvatal closure, then it is possible to bound\nneighborhood diversity of the complement by a function of w only. A simpler\nproof where tree-depth is used instead of tree-width is also presented.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 08:50:49 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Dvo\u0159\u00e1k", "Pavel", "", "Computer Science Institute of Charles University\n  Charles University Prague, Czech Republic"], ["Knop", "Du\u0161an", "", "Department of\n  Applied Mathematics Charles University Prague, Czech Republic"], ["Masa\u0159\u00edk", "Tom\u00e1\u0161", "", "Department of Applied Mathematics Charles University Prague,\n  Czech Republic"]]}, {"id": "1612.05191", "submitter": "Tung Mai", "authors": "Nima Anari, Tung Mai, Shayan Oveis Gharan, Vijay V. Vazirani", "title": "Nash Social Welfare for Indivisible Items under Separable,\n  Piecewise-Linear Concave Utilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Cole and Gkatzelis gave the first constant factor approximation\nalgorithm for the problem of allocating indivisible items to agents, under\nadditive valuations, so as to maximize the Nash Social Welfare. We give\nconstant factor algorithms for a substantial generalization of their problem --\nto the case of separable, piecewise-linear concave utility functions. We give\ntwo such algorithms, the first using market equilibria and the second using the\ntheory of stable polynomials.\n  In AGT, there is a paucity of methods for the design of mechanisms for the\nallocation of indivisible goods and the result of Cole and Gkatzelis seemed to\nbe taking a major step towards filling this gap. Our result can be seen as\nanother step in this direction.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 18:55:27 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 21:11:31 GMT"}, {"version": "v3", "created": "Thu, 6 Apr 2017 22:28:31 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Anari", "Nima", ""], ["Mai", "Tung", ""], ["Gharan", "Shayan Oveis", ""], ["Vazirani", "Vijay V.", ""]]}, {"id": "1612.05222", "submitter": "Richard Santiago", "authors": "Richard Santiago and F. Bruce Shepherd", "title": "Multivariate Submodular Optimization", "comments": "arXiv admin note: text overlap with arXiv:1803.03767", "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning (ICML), PMLR 97:5599-5609, 2019", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular functions have found a wealth of new applications in data science\nand machine learning models in recent years. This has been coupled with many\nalgorithmic advances in the area of submodular optimization: (SO)\n$\\min/\\max~f(S): S \\in \\mathcal{F}$, where $\\mathcal{F}$ is a given family of\nfeasible sets over a ground set $V$ and $f:2^V \\rightarrow \\mathbb{R}$ is\nsubmodular. In this work we focus on a more general class of \\emph{multivariate\nsubmodular optimization} (MVSO) problems: $\\min/\\max~f (S_1,S_2,\\ldots,S_k):\nS_1 \\uplus S_2 \\uplus \\cdots \\uplus S_k \\in \\mathcal{F}$. Here we use $\\uplus$\nto denote disjoint union and hence this model is attractive where resources are\nbeing allocated across $k$ agents, who share a `joint' multivariate nonnegative\nobjective $f(S_1,S_2,\\ldots,S_k)$ that captures some type of submodularity\n(i.e. diminishing returns) property. We provide some explicit examples and\npotential applications for this new framework.\n  For maximization, we show that practical algorithms such as accelerated\ngreedy variants and distributed algorithms achieve good approximation\nguarantees for very general families (such as matroids and $p$-systems). For\narbitrary families, we show that monotone (resp. nonmonotone) MVSO admits an\n$\\alpha (1-1/e)$ (resp. $\\alpha \\cdot 0.385$) approximation whenever monotone\n(resp. nonmonotone) SO admits an $\\alpha$-approximation over the multilinear\nformulation. This substantially expands the family of tractable models for\nsubmodular maximization. For minimization, we show that if SO admits a\n$\\beta$-approximation over \\emph{modular} functions, then MVSO admits a\n$\\frac{\\beta \\cdot n}{1+(n-1)(1-c)}$-approximation where $c\\in [0,1]$ denotes\nthe curvature of $f$, and this is essentially tight. Finally, we prove that\nMVSO has an $\\alpha k$-approximation whenever SO admits an\n$\\alpha$-approximation over the convex formulation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 20:20:11 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 21:37:47 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2018 00:30:06 GMT"}, {"version": "v4", "created": "Mon, 28 Jan 2019 21:25:15 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Santiago", "Richard", ""], ["Shepherd", "F. Bruce", ""]]}, {"id": "1612.05317", "submitter": "Seiichiro Tani", "authors": "Seiichiro Tani", "title": "A Fast Exact Quantum Algorithm for Solitude Verification", "comments": "26 pages, accepted for publication in Quantum Information and\n  Computation (QIC)", "journal-ref": "Quantum Information & Computation, vol.17, no.1&2, pp.15--40, 2017", "doi": null, "report-no": null, "categories": "quant-ph cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solitude verification is arguably one of the simplest fundamental problems in\ndistributed computing, where the goal is to verify that there is a unique\ncontender in a network. This paper devises a quantum algorithm that exactly\nsolves the problem on an anonymous network, which is known as a network model\nwith minimal assumptions [Angluin, STOC'80]. The algorithm runs in $O(N)$\nrounds if every party initially has the common knowledge of an upper bound $N$\non the number of parties. This implies that all solvable problems can be solved\nin $O(N)$ rounds on average without error (i.e., with zero-sided error) on the\nnetwork. As a generalization, a quantum algorithm that works in $O(N\\log_2\n(\\max\\{k,2\\}))$ rounds is obtained for the problem of exactly computing any\nsymmetric Boolean function, over $n$ distributed input bits, which is constant\nover all the $n$ bits whose sum is larger than $k$ for $k\\in \\{0,1,\\dots,\nN-1\\}$. All these algorithms work with the bit complexities bounded by a\npolynomial in $N$.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 00:08:48 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Tani", "Seiichiro", ""]]}, {"id": "1612.05419", "submitter": "Sumedh Tirodkar", "authors": "Sumedh Tirodkar and Sundar Vishwanathan", "title": "Maximum Matching on Trees in the Online Preemptive and the Incremental\n  Dynamic Graph Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Maximum Cardinality Matching (MCM) and the Maximum Weight\nMatching (MWM) problems, on trees and on some special classes of graphs, in the\nOnline Preemptive and the Incremental Dynamic Graph models. In the {\\em Online\nPreemptive} model, the edges of a graph are revealed one by one and the\nalgorithm is required to always maintain a valid matching. On seeing an edge,\nthe algorithm has to either accept or reject the edge. If accepted, then the\nadjacent edges are discarded, and all rejections are permanent. In this model,\nthe complexity of the problems is settled for deterministic algorithms. Epstein\net al. gave a $5.356$-competitive randomized algorithm for MWM, and also proved\na lower bound of $1.693$ for MCM. The same lower bound applies for MWM.\n  In this paper we show that some of the results can be improved in the case of\ntrees and some special classes of graphs. In the online preemptive model, we\npresent a $64/33$-competitive (in expectation) randomized algorithm for MCM on\ntrees.\n  Inspired by the above mentioned algorithm for MCM, we present the main result\nof the paper, a randomized algorithm for MCM with a \"worst case\" update time of\n$O(1)$, in the incremental dynamic graph model, which is $3/2$-approximate (in\nexpectation) on trees, and $1.8$-approximate (in expectation) on general graphs\nwith maximum degree $3$. Note that this algorithm works only against an\noblivious adversary. Hence, we derandomize this algorithm, and give a $(3/2 +\n\\epsilon)$-approximate deterministic algorithm for MCM on trees, with an\namortized update time of $O(1/\\epsilon)$.\n  We also present a minor result for MWM in the online preemptive model, a\n$3$-competitive (in expectation) randomized algorithm on growing trees (where\nthe input revealed upto any stage is always a tree, i.e. a new edge never\nconnects two disconnected trees).\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 10:38:56 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 07:54:00 GMT"}, {"version": "v3", "created": "Sat, 20 Jan 2018 02:22:26 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Tirodkar", "Sumedh", ""], ["Vishwanathan", "Sundar", ""]]}, {"id": "1612.05440", "submitter": "Konstantinos Semertzidis", "authors": "Konstantinos Semertzidis, Evaggelia Pitoura, Evimaria Terzi,\n  Panayiotis Tsaparas", "title": "Best Friends Forever (BFF): Finding Lasting Dense Subgraphs", "comments": "15 pages, 10 figures, 8 tables", "journal-ref": "Data Mining and Knowledge Discovery - Journal Track of ECML PKDD\n  2019", "doi": "10.1007/s10618-018-0602-x", "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs form a natural model for relationships and interactions between\nentities, for example, between people in social and cooperation networks,\nservers in computer networks, or tags and words in documents and tweets. But,\nwhich of these relationships or interactions are the most lasting ones? In this\npaper, we study the following problem: given a set of graph snapshots, which\nmay correspond to the state of an evolving graph at different time instances,\nidentify the set of nodes that are the most densely connected in all snapshots.\nWe call this problem the Best Friends For Ever (BFF) problem. We provide\ndefinitions for density over multiple graph snapshots, that capture different\nsemantics of connectedness over time, and we study the corresponding variants\nof the BFF problem. We then look at the On-Off BFF (O^2BFF) problem that\nrelaxes the requirement of nodes being connected in all snapshots, and asks for\nthe densest set of nodes in at least $k$ of a given set of graph snapshots. We\nshow that this problem is NP-complete for all definitions of density, and we\npropose a set of efficient algorithms. Finally, we present experiments with\nsynthetic and real datasets that show both the efficiency of our algorithms and\nthe usefulness of the BFF and the O^2BFF problems.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 12:09:17 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 16:09:22 GMT"}, {"version": "v3", "created": "Tue, 3 Oct 2017 01:45:24 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Semertzidis", "Konstantinos", ""], ["Pitoura", "Evaggelia", ""], ["Terzi", "Evimaria", ""], ["Tsaparas", "Panayiotis", ""]]}, {"id": "1612.05441", "submitter": "Paul Swoboda", "authors": "Paul Swoboda and Bjoern Andres", "title": "A Message Passing Algorithm for the Minimum Cost Multicut Problem", "comments": "Added acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dual decomposition and linear program relaxation of the NP -hard\nminimum cost multicut problem. Unlike other polyhedral relaxations of the\nmulticut polytope, it is amenable to efficient optimization by message passing.\nLike other polyhedral elaxations, it can be tightened efficiently by cutting\nplanes. We define an algorithm that alternates between message passing and\nefficient separation of cycle- and odd-wheel inequalities. This algorithm is\nmore efficient than state-of-the-art algorithms based on linear programming,\nincluding algorithms written in the framework of leading commercial software,\nas we show in experiments with large instances of the problem from applications\nin computer vision, biomedical image analysis and data mining.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 12:10:34 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 11:55:04 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Swoboda", "Paul", ""], ["Andres", "Bjoern", ""]]}, {"id": "1612.05460", "submitter": "Paul Swoboda", "authors": "Paul Swoboda and Jan Kuske and Bogdan Savchynskyy", "title": "A Dual Ascent Framework for Lagrangean Decomposition of Combinatorial\n  Problems", "comments": "Added acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general dual ascent framework for Lagrangean decomposition of\ncombinatorial problems. Although methods of this type have shown their\nefficiency for a number of problems, so far there was no general algorithm\napplicable to multiple problem types. In his work, we propose such a general\nalgorithm. It depends on several parameters, which can be used to optimize its\nperformance in each particular setting. We demonstrate efficacy of our method\non graph matching and multicut problems, where it outperforms state-of-the-art\nsolvers including those based on subgradient optimization and off-the-shelf\nlinear programming solvers.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 13:32:18 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 11:53:40 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Swoboda", "Paul", ""], ["Kuske", "Jan", ""], ["Savchynskyy", "Bogdan", ""]]}, {"id": "1612.05474", "submitter": "Michael Holzhauser", "authors": "Michael Holzhauser and Sven O. Krumke", "title": "A Generalized Approximation Framework for Fractional Network Flow and\n  Packing Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the fractional packing framework of Garg and Koenemann to the\ncase of linear fractional packing problems over polyhedral cones. More\nprecisely, we provide approximation algorithms for problems of the form\n$\\max\\{c^T x : Ax \\leq b, x \\in C \\}$, where the matrix $A$ contains no\nnegative entries and $C$ is a cone that is generated by a finite set $S$ of\nnon-negative vectors. While the cone is allowed to require an exponential-sized\nrepresentation, we assume that we can access it via one of three types of\noracles. For each of these oracles, we present positive results for the\napproximability of the packing problem. In contrast to other frameworks, the\npresented one allows the use of arbitrary linear objective functions and can be\napplied to a large class of packing problems without much effort. In\nparticular, our framework instantly allows to derive fast and simple fully\npolynomial-time approximation algorithms (FPTASs) for a large set of network\nflow problems, such as budget-constrained versions of traditional network\nflows, multicommodity flows, or generalized flows. Some of these FPTASs\nrepresent the first ones of their kind, while others match existing results but\noffer a much simpler proof.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 14:12:07 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Holzhauser", "Michael", ""], ["Krumke", "Sven O.", ""]]}, {"id": "1612.05531", "submitter": "Pierre-Louis Giscard", "authors": "Pierre-Louis Giscard, Nils Kriege, Richard C. Wilson", "title": "A general purpose algorithm for counting simple cycles and simple paths\n  of any length", "comments": null, "journal-ref": null, "doi": "10.1007/s00453-019-00552-1", "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a general purpose algorithm for counting simple cycles and simple\npaths of any length $\\ell$ on a (weighted di)graph on $N$ vertices and $M$\nedges, achieving a time complexity of\n$O\\left(N+M+\\big(\\ell^\\omega+\\ell\\Delta\\big) |S_\\ell|\\right)$. In this\nexpression, $|S_\\ell|$ is the number of (weakly) connected induced subgraphs of\n$G$ on at most $\\ell$ vertices, $\\Delta$ is the maximum degree of any vertex\nand $\\omega$ is the exponent of matrix multiplication. We compare the algorithm\ncomplexity both theoretically and experimentally with most of the existing\nalgorithms for the same task. These comparisons show that the algorithm\ndescribed here is the best general purpose algorithm for the class of graphs\nwhere $(\\ell^{\\omega-1}\\Delta^{-1}+1) |S_\\ell|\\leq |\\text{Cycle}_\\ell|$, with\n$|\\text{Cycle}_\\ell|$ the total number of simple cycles of length at most\n$\\ell$, including backtracks and self-loops. On Erd\\H{o}s-R\\'enyi random\ngraphs, we find empirically that this happens when the edge probability is\nlarger than circa $4/N$. In addition, we show that some real-world networks\nalso belong to this class. Finally, the algorithm permits the enumeration of\nsimple cycles and simple paths on networks where vertices are labeled from an\nalphabet on $n$ letters with a time complexity of\n$O\\left(N+M+\\big(n^\\ell\\ell^\\omega+\\ell\\Delta\\big) |S_\\ell|\\right)$. A Matlab\nimplementation of the algorithm proposed here is available for download.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 16:12:58 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Giscard", "Pierre-Louis", ""], ["Kriege", "Nils", ""], ["Wilson", "Richard C.", ""]]}, {"id": "1612.05623", "submitter": "Zeyu Zhang", "authors": "Michael Dinitz and Zeyu Zhang", "title": "Approximating Approximate Distance Oracles", "comments": "To appear in ITCS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a finite metric space $(V,d)$, an approximate distance oracle is a data\nstructure which, when queried on two points $u,v \\in V$, returns an\napproximation to the the actual distance between $u$ and $v$ which is within\nsome bounded stretch factor of the true distance. There has been significant\nwork on the tradeoff between the important parameters of approximate distance\noracles (and in particular between the size, stretch, and query time), but in\nthis paper we take a different point of view, that of per-instance\noptimization. If we are given an particular input metric space and stretch\nbound, can we find the smallest possible approximate distance oracle for that\nparticular input? Since this question is not even well-defined, we restrict our\nattention to well-known classes of approximate distance oracles, and study\nwhether we can optimize over those classes.\n  In particular, we give an $O(\\log n)$-approximation to the problem of finding\nthe smallest stretch $3$ Thorup-Zwick distance oracle, as well as the problem\nof finding the smallest P\\v{a}tra\\c{s}cu-Roditty distance oracle. We also prove\na matching $\\Omega(\\log n)$ lower bound for both problems, and an\n$\\Omega(n^{\\frac{1}{k}-\\frac{1}{2^{k-1}}})$ integrality gap for the more\ngeneral stretch $(2k-1)$ Thorup-Zwick distance oracle. We also consider the\nproblem of approximating the best TZ or PR approximate distance oracle\n\\emph{with outliers}, and show that more advanced techniques (SDP relaxations\nin particular) allow us to optimize even in the presence of outliers.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 20:33:28 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Dinitz", "Michael", ""], ["Zhang", "Zeyu", ""]]}, {"id": "1612.05665", "submitter": "Yihan Sun", "authors": "Yihan Sun and Daniel Ferizovic and Guy E. Blelloch", "title": "PAM: Parallel Augmented Maps", "comments": null, "journal-ref": "Yihan Sun, Daniel Ferizovic, and Guy E. Belloch. 2018. PAM:\n  parallel augmented maps. In Proceedings of the 23rd ACM SIGPLAN Symposium on\n  Principles and Practice of Parallel Programming (PPoPP '18). ACM, New York,\n  NY, USA, 290-304", "doi": "10.1145/3178487.3178509", "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordered (key-value) maps are an important and widely-used data type for\nlarge-scale data processing frameworks. Beyond simple search, insertion and\ndeletion, more advanced operations such as range extraction, filtering, and\nbulk updates form a critical part of these frameworks.\n  We describe an interface for ordered maps that is augmented to support fast\nrange queries and sums, and introduce a parallel and concurrent library called\nPAM (Parallel Augmented Maps) that implements the interface. The interface\nincludes a wide variety of functions on augmented maps ranging from basic\ninsertion and deletion to more interesting functions such as union,\nintersection, filtering, extracting ranges, splitting, and range-sums. We\ndescribe algorithms for these functions that are efficient both in theory and\npractice.\n  As examples of the use of the interface and the performance of PAM, we apply\nthe library to four applications: simple range sums, interval trees, 2D range\ntrees, and ranked word index searching. The interface greatly simplifies the\nimplementation of these data structures over direct implementations.\nSequentially the code achieves performance that matches or exceeds existing\nlibraries designed specially for a single application, and in parallel our\nimplementation gets speedups ranging from 40 to 90 on 72 cores with 2-way\nhyperthreading.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 22:02:49 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 22:36:15 GMT"}, {"version": "v3", "created": "Mon, 26 Mar 2018 18:31:41 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Sun", "Yihan", ""], ["Ferizovic", "Daniel", ""], ["Blelloch", "Guy E.", ""]]}, {"id": "1612.05733", "submitter": "Robert Ganian", "authors": "Robert Ganian, M. S. Ramanujan, Stefan Szeider", "title": "Backdoors to Tractable Valued CSP", "comments": "Accepted to CP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the notion of a strong backdoor from the CSP setting to the Valued\nCSP setting (VCSP, for short). This provides a means for augmenting a class of\ntractable VCSP instances to instances that are outside the class but of small\ndistance to the class, where the distance is measured in terms of the size of a\nsmallest backdoor. We establish that VCSP is fixed-parameter tractable when\nparameterized by the size of a smallest backdoor into every tractable class of\nVCSP instances characterized by a (possibly infinite) tractable valued\nconstraint language of finite arity and finite domain. We further extend this\nfixed-parameter tractability result to so-called \"scattered classes\" of VCSP\ninstances where each connected component may belong to a different tractable\nclass.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 11:33:37 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Ganian", "Robert", ""], ["Ramanujan", "M. S.", ""], ["Szeider", "Stefan", ""]]}, {"id": "1612.06016", "submitter": "Yuichi Yoshida", "authors": "Eric Blais and Yuichi Yoshida", "title": "A Characterization of Constant-Sample Testable Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the set of properties of Boolean-valued functions on a finite\ndomain $\\mathcal{X}$ that are testable with a constant number of samples.\nSpecifically, we show that a property $\\mathcal{P}$ is testable with a constant\nnumber of samples if and only if it is (essentially) a $k$-part symmetric\nproperty for some constant $k$, where a property is {\\em $k$-part symmetric} if\nthere is a partition $S_1,\\ldots,S_k$ of $\\mathcal{X}$ such that whether\n$f:\\mathcal{X} \\to \\{0,1\\}$ satisfies the property is determined solely by the\ndensities of $f$ on $S_1,\\ldots,S_k$. We use this characterization to obtain a\nnumber of corollaries, namely: (i) A graph property $\\mathcal{P}$ is testable\nwith a constant number of samples if and only if whether a graph $G$ satisfies\n$\\mathcal{P}$ is (essentially) determined by the edge density of $G$. (ii) An\naffine-invariant property $\\mathcal{P}$ of functions $f:\\mathbb{F}_p^n \\to\n\\{0,1\\}$ is testable with a constant number of samples if and only if whether\n$f$ satisfies $\\mathcal{P}$ is (essentially) determined by the density of $f$.\n(iii) For every constant $d \\geq 1$, monotonicity of functions $f : [n]^d \\to\n\\{0, 1\\}$ on the $d$-dimensional hypergrid is testable with a constant number\nof samples.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 00:52:17 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Blais", "Eric", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1612.06057", "submitter": "Rameshwar Pratap", "authors": "Raghav Kulkarni, Rameshwar Pratap", "title": "Similarity preserving compressions of high dimensional sparse data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of internet has resulted in an explosion of data consisting of\nmillions of articles, images, songs, and videos. Most of this data is high\ndimensional and sparse. The need to perform an efficient search for similar\nobjects in such high dimensional big datasets is becoming increasingly common.\nEven with the rapid growth in computing power, the brute-force search for such\na task is impractical and at times impossible. Therefore it is quite natural to\ninvestigate the techniques that compress the dimension of the data-set while\npreserving the similarity between data objects.\n  In this work, we propose an efficient compression scheme mapping binary\nvectors into binary vectors and simultaneously preserving Hamming distance and\nInner Product. The length of our compression depends only on the sparsity and\nis independent of the dimension of the data. Moreover our schemes provide\none-shot solution for Hamming distance and Inner Product, and work in the\nstreaming setting as well. In contrast with the \"local projection\" strategies\nused by most of the previous schemes, our scheme combines (using sparsity) the\nfollowing two strategies: $1.$ Partitioning the dimensions into several\nbuckets, $2.$ Then obtaining \"global linear summaries\" in each of these\nbuckets. We generalize our scheme for real-valued data and obtain compressions\nfor Euclidean distance, Inner Product, and $k$-way Inner Product.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 06:27:45 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Kulkarni", "Raghav", ""], ["Pratap", "Rameshwar", ""]]}, {"id": "1612.06191", "submitter": "Gregory Gutin", "authors": "Pierre Berg\\'e and Jason Crampton and Gregory Gutin and R\\'emi\n  Watrigant", "title": "The Authorization Policy Existence Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraints such as separation-of-duty are widely used to specify\nrequirements that supplement basic authorization policies. However, the\nexistence of constraints (and authorization policies) may mean that a user is\nunable to fulfill her/his organizational duties because access to resources has\nbeen denied. In short, there is a tension between the need to protect resources\n(using policies and constraints) and the availability of resources. Recent work\non workflow satisfiability and resiliency in access control asks whether this\ntension compromises the ability of an organization to achieve its objectives.\nIn this paper, we develop a new method of specifying constraints which subsumes\nmuch related work and allows a wider range of constraints to be specified. The\nuse of such constraints leads naturally to a range of questions related to\n\"policy existence\", where a positive answer means that an organization's\nobjectives can be realized. We analyze the complexity of these policy existence\nquestions and, for particular sub-classes of constraints defined by our\nlanguage, develop fixed-parameter tractable algorithms to solve them.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 14:18:13 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Berg\u00e9", "Pierre", ""], ["Crampton", "Jason", ""], ["Gutin", "Gregory", ""], ["Watrigant", "R\u00e9mi", ""]]}, {"id": "1612.06203", "submitter": "Dominic Moylett", "authors": "Dominic J. Moylett and Noah Linden and Ashley Montanaro", "title": "Quantum speedup of the Travelling Salesman Problem for bounded-degree\n  graphs", "comments": "12 pages, 3 figures, 1 table", "journal-ref": "Phys. Rev. A 95, 032323 (2017)", "doi": "10.1103/PhysRevA.95.032323", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Travelling Salesman Problem is one of the most famous problems in graph\ntheory. However, little is currently known about the extent to which quantum\ncomputers could speed up algorithms for the problem. In this paper, we prove a\nquadratic quantum speedup when the degree of each vertex is at most 3 by\napplying a quantum backtracking algorithm to a classical algorithm by Xiao and\nNagamochi. We then use similar techniques to accelerate a classical algorithm\nfor when the degree of each vertex is at most 4, before speeding up\nhigher-degree graphs via reductions to these instances.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 14:34:58 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Moylett", "Dominic J.", ""], ["Linden", "Noah", ""], ["Montanaro", "Ashley", ""]]}, {"id": "1612.06260", "submitter": "Zachary Charles", "authors": "Zachary Charles", "title": "Generating Random Factored Ideals in Number Fields", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized polynomial-time algorithm to generate a random\ninteger according to the distribution of norms of ideals at most N in any given\nnumber field, along with the factorization of the integer. Using this\nalgorithm, we can produce a random ideal in the ring of algebraic integers\nuniformly at random among ideals with norm up to N, in polynomial time. We also\npresent a variant of this algorithm for generating ideals in function fields.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 22:50:42 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 15:06:58 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Charles", "Zachary", ""]]}, {"id": "1612.06335", "submitter": "Ray Li", "authors": "Venkatesan Guruswami and Ray Li", "title": "Coding against deletions in oblivious and online models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider binary error correcting codes when errors are deletions. A basic\nchallenge concerning deletion codes is determining $p_0^{(adv)}$, the zero-rate\nthreshold of adversarial deletions, defined to be the supremum of all $p$ for\nwhich there exists a code family with rate bounded away from 0 capable of\ncorrecting a fraction $p$ of adversarial deletions. A recent construction of\ndeletion-correcting codes [Bukh et al 17] shows that $p_0^{(adv)} \\ge\n\\sqrt{2}-1$, and the trivial upper bound, $p_0^{(adv)}\\le\\frac{1}{2}$, is the\nbest known. Perhaps surprisingly, we do not know whether or not $p_0^{(adv)} =\n1/2$.\n  In this work, to gain further insight into deletion codes, we explore two\nrelated error models: oblivious deletions and online deletions, which are in\nbetween random and adversarial deletions in power. In the oblivious model, the\nchannel can inflict an arbitrary pattern of $pn$ deletions, picked without\nknowledge of the codeword. We prove the existence of binary codes of positive\nrate that can correct any fraction $p < 1$ of oblivious deletions, establishing\nthat the associated zero-rate threshold $p_0^{(obliv)}$ equals $1$.\n  For online deletions, where the channel decides whether to delete bit $x_i$\nbased only on knowledge of bits $x_1x_2\\dots x_i$, define the deterministic\nzero-rate threshold for online deletions $p_0^{(on,d)}$ to be the supremum of\n$p$ for which there exist deterministic codes against an online channel causing\n$pn$ deletions with low average probability of error. That is, the probability\nthat a randomly chosen codeword is decoded incorrectly is small. We prove\n$p_0^{(adv)}=\\frac{1}{2}$ if and only if $p_0^{(on,d)}=\\frac{1}{2}$.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 20:36:01 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 15:49:00 GMT"}, {"version": "v3", "created": "Fri, 5 May 2017 17:23:00 GMT"}, {"version": "v4", "created": "Wed, 26 Jul 2017 02:39:04 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Li", "Ray", ""]]}, {"id": "1612.06473", "submitter": "Igor Shinkar", "authors": "Indranil Banerjee, Dana Richards, Igor Shinkar", "title": "Sorting Networks On Restricted Topologies", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sorting number of a graph with $n$ vertices is the minimum depth of a\nsorting network with $n$ inputs and outputs that uses only the edges of the\ngraph to perform comparisons. Many known results on sorting networks can be\nstated in terms of sorting numbers of different classes of graphs. In this\npaper we show the following general results about the sorting number of graphs.\n  Any $n$-vertex graph that contains a simple path of length $d$ has a sorting\nnetwork of depth $O(n \\log(n/d))$.\n  Any $n$-vertex graph with maximal degree $\\Delta$ has a sorting network of\ndepth $O(\\Delta n)$.\n  We also provide several results that relate the sorting number of a graph\nwith its routing number, size of its maximal matching, and other well known\ngraph properties. Additionally, we give some new bounds on the sorting number\nfor some typical graphs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 01:17:26 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 00:02:52 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Banerjee", "Indranil", ""], ["Richards", "Dana", ""], ["Shinkar", "Igor", ""]]}, {"id": "1612.06661", "submitter": "Roman Vershynin", "authors": "Roman Vershynin", "title": "Four lectures on probabilistic methods for data science", "comments": "Lectures given at 2016 PCMI Graduate Summer School in Mathematics of\n  Data. Some typos, inaccuracies fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods of high-dimensional probability play a central role in applications\nfor statistics, signal processing theoretical computer science and related\nfields. These lectures present a sample of particularly useful tools of\nhigh-dimensional probability, focusing on the classical and matrix Bernstein's\ninequality and the uniform matrix deviation inequality. We illustrate these\ntools with applications for dimension reduction, network analysis, covariance\nestimation, matrix completion and sparse signal recovery. The lectures are\ngeared towards beginning graduate students who have taken a rigorous course in\nprobability but may not have any experience in data science applications.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 13:44:34 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 21:37:30 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Vershynin", "Roman", ""]]}, {"id": "1612.06675", "submitter": "Matteo Ceccarello", "authors": "Matteo Ceccarello, Carlo Fantozzi, Andrea Pietracaprina, Geppino\n  Pucci, Fabio Vandin", "title": "Clustering Uncertain Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An uncertain graph $\\mathcal{G} = (V, E, p : E \\rightarrow (0,1])$ can be\nviewed as a probability space whose outcomes (referred to as \\emph{possible\nworlds}) are subgraphs of $\\mathcal{G}$ where any edge $e\\in E$ occurs with\nprobability $p(e)$, independently of the other edges. These graphs naturally\narise in many application domains where data management systems are required to\ncope with uncertainty in interrelated data, such as computational biology,\nsocial network analysis, network reliability, and privacy enforcement, among\nthe others. For this reason, it is important to devise fundamental querying and\nmining primitives for uncertain graphs. This paper contributes to this endeavor\nwith the development of novel strategies for clustering uncertain graphs.\nSpecifically, given an uncertain graph $\\mathcal{G}$ and an integer $k$, we aim\nat partitioning its nodes into $k$ clusters, each featuring a distinguished\ncenter node, so to maximize the minimum/average connection probability of any\nnode to its cluster's center, in a random possible world. We assess the\nNP-hardness of maximizing the minimum connection probability, even in the\npresence of an oracle for the connection probabilities, and develop efficient\napproximation algorithms for both problems and some useful variants. Unlike\nprevious works in the literature, our algorithms feature provable approximation\nguarantees and are capable to keep the granularity of the returned clustering\nunder control. Our theoretical findings are complemented with several\nexperiments that compare our algorithms against some relevant competitors, with\nrespect to both running-time and quality of the returned clusterings.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 14:24:11 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 15:30:11 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Ceccarello", "Matteo", ""], ["Fantozzi", "Carlo", ""], ["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""], ["Vandin", "Fabio", ""]]}, {"id": "1612.06962", "submitter": "Zijun Wu", "authors": "Zijun Wu, Rolf Moehring, Jianhui Lai", "title": "Stochastic Runtime Analysis of a Cross Entropy Algorithm for Traveling\n  Salesman Problems", "comments": "40 pages, 7 figures", "journal-ref": "Theoretical Computer Science, 2017", "doi": "10.1016/j.tcs.2017.10.012", "report-no": null, "categories": "cs.DS cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article analyzes the stochastic runtime of a Cross-Entropy Algorithm on\ntwo classes of traveling salesman problems. The algorithm shares main features\nof the famous Max-Min Ant System with iteration-best reinforcement.\n  For simple instances that have a $\\{1,n\\}$-valued distance function and a\nunique optimal solution, we prove a stochastic runtime of $O(n^{6+\\epsilon})$\nwith the vertex-based random solution generation, and a stochastic runtime of\n$O(n^{3+\\epsilon}\\ln n)$ with the edge-based random solution generation for an\narbitrary $\\epsilon\\in (0,1)$. These runtimes are very close to the known\nexpected runtime for variants of Max-Min Ant System with best-so-far\nreinforcement. They are obtained for the stronger notion of stochastic runtime,\nwhich means that an optimal solution is obtained in that time with an\noverwhelming probability, i.e., a probability tending exponentially fast to one\nwith growing problem size.\n  We also inspect more complex instances with $n$ vertices positioned on an\n$m\\times m$ grid. When the $n$ vertices span a convex polygon, we obtain a\nstochastic runtime of $O(n^{3}m^{5+\\epsilon})$ with the vertex-based random\nsolution generation, and a stochastic runtime of $O(n^{2}m^{5+\\epsilon})$ for\nthe edge-based random solution generation. When there are $k = O(1)$ many\nvertices inside a convex polygon spanned by the other $n-k$ vertices, we obtain\na stochastic runtime of $O(n^{4}m^{5+\\epsilon}+n^{6k-1}m^{\\epsilon})$ with the\nvertex-based random solution generation, and a stochastic runtime of\n$O(n^{3}m^{5+\\epsilon}+n^{3k}m^{\\epsilon})$ with the edge-based random solution\ngeneration. These runtimes are better than the expected runtime for the\nso-called $(\\mu\\!+\\!\\lambda)$ EA reported in a recent article, and again\nobtained for the stronger notion of stochastic runtime.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 03:24:26 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 03:24:57 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Wu", "Zijun", ""], ["Moehring", "Rolf", ""], ["Lai", "Jianhui", ""]]}, {"id": "1612.07276", "submitter": "Martin Derka", "authors": "Martin Derka, Therese Biedl", "title": "Splitting $B_2$-VPG graphs into outer-string and co-comparability graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that any $B_2$-VPG graph (i.e., an intersection graph\nof orthogonal curves with at most 2 bends) can be decomposed into $O(\\log n)$\nouterstring graphs or $O(\\log^3 n)$ permutation graphs. This leads to better\napproximation algorithms for hereditary graph problems, such as independent\nset, clique and clique cover, on $B_2$-VPG graphs.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 18:58:17 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Derka", "Martin", ""], ["Biedl", "Therese", ""]]}, {"id": "1612.07404", "submitter": "Arijit Khan", "authors": "Arijit Khan", "title": "Vertex-Centric Graph Processing: The Good, the Bad, and the Ugly", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed graph algorithms that adopt an iterative vertex-centric\nframework for graph processing, popularized by the Google's Pregel system.\nSince then, there are several attempts to implement many graph algorithms in a\nvertex-centric framework, as well as efforts to design optimization techniques\nfor improving the efficiency. However, to the best of our knowledge, there has\nnot been any systematic study to compare these vertex-centric implementations\nwith their sequential counterparts. Our paper addresses this gap in two ways.\n(1) We analyze the computational complexity of such implementations with the\nnotion of time-processor product, and benchmark several vertex-centric graph\nalgorithms whether they perform more work with respect to their best-known\nsequential solutions. (2) Employing the concept of balanced practical Pregel\nalgorithms, we study if these implementations suffer from imbalanced workload\nand large number of iterations. Our findings illustrate that with the exception\nof Euler tour tree algorithm, all other algorithms either perform more work\nthan their best-known sequential approach, or suffer from imbalanced workload/\nlarge number of iterations, or even both. We also emphasize on graph algorithms\nthat are fundamentally difficult to be expressed in vertex-centric frameworks,\nand conclude by discussing the road ahead for distributed graph processing.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 00:53:41 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Khan", "Arijit", ""]]}, {"id": "1612.07475", "submitter": "Shunsuke Inenaga", "authors": "Shunsuke Inenaga and Heikki Hyyr\\\"o", "title": "A hardness result and new algorithm for the longest common palindromic\n  subsequence problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2-LCPS problem, first introduced by Chowdhury et al. [Fundam. Inform.,\n129(4):329-340, 2014], asks one to compute (the length of) a longest\npalindromic common subsequence between two given strings $A$ and $B$. We show\nthat the 2-LCPS problem is at least as hard as the well-studied longest common\nsubsequence problem for four strings (the 4-LCS problem). Then, we present a\nnew algorithm which solves the 2-LCPS problem in $O(\\sigma M^2 + n)$ time,\nwhere $n$ denotes the length of $A$ and $B$, $M$ denotes the number of matching\npositions between $A$ and $B$, and $\\sigma$ denotes the number of distinct\ncharacters occurring in both $A$ and $B$. Our new algorithm is faster than\nChowdhury et al.'s sparse algorithm when $\\sigma = o(\\log^2n \\log\\log n)$.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 08:04:21 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Inenaga", "Shunsuke", ""], ["Hyyr\u00f6", "Heikki", ""]]}, {"id": "1612.07493", "submitter": "Srinivasa Rao Satti", "authors": "Seungbum Jo and Srinivasa Rao Satti", "title": "Simultaneous encodings for range and next/previous larger/smaller value\n  queries", "comments": "15 pages, 4 figures (with added details in Figure 3 wrt the journal\n  version)", "journal-ref": "Theoretical Computer Science, 654: 80-91 (2016)", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an array of $n$ elements from a total order, we propose encodings that\nsupport various range queries (range minimum, range maximum and their\nvariants), and previous and next smaller/larger value queries. When query time\nis not of concern, we obtain a $4.088n + o(n)$-bit encoding that supports all\nthese queries. For the case when we need to support all these queries in\nconstant time, we give an encoding that takes $4.585n + o(n)$ bits, where $n$\nis the length of input array. This improves the $5.08n+o(n)$-bit encoding\nobtained by encoding the colored $2d$-Min and Max heaps proposed by\nFischer~[TCS, 2011]. We first extend the original DFUDS [Algorithmica, 2005]\nencoding of the colored 2d-Min (Max) heap that supports the queries in constant\ntime. Then, we combine the extended DFUDS of $2d$-Min heap and $2d$-Max heap\nusing the Min-Max encoding of Gawrychowski and Nicholson [ICALP, 2015] with\nsome modifications. We also obtain encodings that take lesser space and support\na subset of these queries.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 08:59:45 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Jo", "Seungbum", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "1612.07516", "submitter": "Sascha Brauer", "authors": "Johannes Bl\\\"omer, Sascha Brauer, Kathrin Bujna", "title": "On Coreset Constructions for the Fuzzy $K$-Means Problem", "comments": "Coreset Construction unchanged, improved applications section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fuzzy $K$-means problem is a popular generalization of the well-known\n$K$-means problem to soft clusterings. We present the first coresets for fuzzy\n$K$-means with size linear in the dimension, polynomial in the number of\nclusters, and poly-logarithmic in the number of points. We show that these\ncoresets can be employed in the computation of a $(1+\\epsilon)$-approximation\nfor fuzzy $K$-means, improving previously presented results. We further show\nthat our coresets can be maintained in an insertion-only streaming setting,\nwhere data points arrive one-by-one.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 10:10:11 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 10:02:03 GMT"}, {"version": "v3", "created": "Thu, 27 Sep 2018 06:50:30 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Bl\u00f6mer", "Johannes", ""], ["Brauer", "Sascha", ""], ["Bujna", "Kathrin", ""]]}, {"id": "1612.07710", "submitter": "Tobias Christiani", "authors": "Tobias Christiani and Rasmus Pagh", "title": "Set Similarity Search Beyond MinHash", "comments": "The first arXiv version of this paper introduced an upper bound for\n  Jaccard similarity search that was based on a miscalculation which led the\n  authors to believe that the \"hardest instances\" for Jaccard similarity search\n  using Chosen Path occurs when all sets have the same size. The question of\n  which existing technique is better depends on set sizes and similarity\n  thresholds (details in paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximate set similarity search under\nBraun-Blanquet similarity $B(\\mathbf{x}, \\mathbf{y}) = |\\mathbf{x} \\cap\n\\mathbf{y}| / \\max(|\\mathbf{x}|, |\\mathbf{y}|)$. The $(b_2, b_2)$-approximate\nBraun-Blanquet similarity search problem is to preprocess a collection of sets\n$P$ such that, given a query set $\\mathbf{q}$, if there exists $\\mathbf{x} \\in\nP$ with $B(\\mathbf{q}, \\mathbf{x}) \\geq b_1$, then we can efficiently return\n$\\mathbf{x}' \\in P$ with $B(\\mathbf{q}, \\mathbf{x}') > b_2$.\n  We present a simple data structure that solves this problem with space usage\n$O(n^{1+\\rho}\\log n + \\sum_{\\mathbf{x} \\in P}|\\mathbf{x}|)$ and query time\n$O(|\\mathbf{q}|n^{\\rho} \\log n)$ where $n = |P|$ and $\\rho =\n\\log(1/b_1)/\\log(1/b_2)$. Making use of existing lower bounds for\nlocality-sensitive hashing by O'Donnell et al. (TOCT 2014) we show that this\nvalue of $\\rho$ is tight across the parameter space, i.e., for every choice of\nconstants $0 < b_2 < b_1 < 1$.\n  In the case where all sets have the same size our solution strictly improves\nupon the value of $\\rho$ that can be obtained through the use of\nstate-of-the-art data-independent techniques in the Indyk-Motwani\nlocality-sensitive hashing framework (STOC 1998) such as Broder's MinHash (CCS\n1997) for Jaccard similarity and Andoni et al.'s cross-polytope LSH (NIPS 2015)\nfor cosine similarity. Surprisingly, even though our solution is\ndata-independent, for a large part of the parameter space we outperform the\ncurrently best data-dependent method by Andoni and Razenshteyn (STOC 2015).\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 17:22:23 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 17:18:21 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Christiani", "Tobias", ""], ["Pagh", "Rasmus", ""]]}, {"id": "1612.07866", "submitter": "Nike Sun", "authors": "Andrea Montanari and Nike Sun", "title": "Spectral algorithms for tensor completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the tensor completion problem, one seeks to estimate a low-rank tensor\nbased on a random sample of revealed entries. In terms of the required sample\nsize, earlier work revealed a large gap between estimation with unbounded\ncomputational resources (using, for instance, tensor nuclear norm minimization)\nand polynomial-time algorithms. Among the latter, the best statistical\nguarantees have been proved, for third-order tensors, using the sixth level of\nthe sum-of-squares (SOS) semidefinite programming hierarchy (Barak and Moitra,\n2014). However, the SOS approach does not scale well to large problem\ninstances. By contrast, spectral methods --- based on unfolding or matricizing\nthe tensor --- are attractive for their low complexity, but have been believed\nto require a much larger sample size.\n  This paper presents two main contributions. First, we propose a new\nunfolding-based method, which outperforms naive ones for symmetric $k$-th order\ntensors of rank $r$. For this result we make a study of singular space\nestimation for partially revealed matrices of large aspect ratio, which may be\nof independent interest. For third-order tensors, our algorithm matches the SOS\nmethod in terms of sample size (requiring about $rd^{3/2}$ revealed entries),\nsubject to a worse rank condition ($r\\ll d^{3/4}$ rather than $r\\ll d^{3/2}$).\nWe complement this result with a different spectral algorithm for third-order\ntensors in the overcomplete ($r\\ge d$) regime. Under a random model, this\nsecond approach succeeds in estimating tensors of rank $d\\le r \\ll d^{3/2}$\nfrom about $rd^{3/2}$ revealed entries.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 03:59:29 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Montanari", "Andrea", ""], ["Sun", "Nike", ""]]}, {"id": "1612.07925", "submitter": "Justin Ward", "authors": "Sara Ahmadian, Ashkan Norouzi-Fard, Ola Svensson, Justin Ward", "title": "Better Guarantees for k-Means and Euclidean k-Median by Primal-Dual\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a classic topic in optimization with $k$-means being one of the\nmost fundamental such problems. In the absence of any restrictions on the\ninput, the best known algorithm for $k$-means with a provable guarantee is a\nsimple local search heuristic yielding an approximation guarantee of\n$9+\\epsilon$, a ratio that is known to be tight with respect to such methods.\n  We overcome this barrier by presenting a new primal-dual approach that allows\nus to (1) exploit the geometric structure of $k$-means and (2) to satisfy the\nhard constraint that at most $k$ clusters are selected without deteriorating\nthe approximation guarantee. Our main result is a $6.357$-approximation\nalgorithm with respect to the standard LP relaxation. Our techniques are quite\ngeneral and we also show improved guarantees for the general version of\n$k$-means where the underlying metric is not required to be Euclidean and for\n$k$-median in Euclidean metrics.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 10:34:53 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 13:36:10 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Ahmadian", "Sara", ""], ["Norouzi-Fard", "Ashkan", ""], ["Svensson", "Ola", ""], ["Ward", "Justin", ""]]}, {"id": "1612.08075", "submitter": "Daniel Graf", "authors": "Loukas Georgiadis, Daniel Graf, Giuseppe F. Italiano, Nikos Parotsidis\n  and Przemys{\\l}aw Uzna\\'nski", "title": "All-Pairs 2-Reachability in $\\mathcal{O}(n^{\\omega}\\log n)$ Time", "comments": "27 pages. Full version of ICALP'17 abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $2$-reachability problem we are given a directed graph $G$ and we wish\nto determine if there are two (edge or vertex) disjoint paths from $u$ to $v$,\nfor a given pair of vertices $u$ and $v$. In this paper, we present an\nalgorithm that computes $2$-reachability information for all pairs of vertices\nin $\\mathcal{O}(n^{\\omega}\\log n)$ time, where $n$ is the number of vertices\nand $\\omega$ is the matrix multiplication exponent. Hence, we show that the\nrunning time of all-pairs $2$-reachability is only within a $\\log$ factor of\ntransitive closure.\n  Moreover, our algorithm produces a witness (i.e., a separating edge or a\nseparating vertex) for all pair of vertices where $2$-reachability does not\nhold. By processing these witnesses, we can compute all the edge- and\nvertex-dominator trees of $G$ in $\\mathcal{O}(n^2)$ additional time, which in\nturn enables us to answer various connectivity queries in $\\mathcal{O}(1)$\ntime. For instance, we can test in constant time if there is a path from $u$ to\n$v$ avoiding an edge $e$, for any pair of query vertices $u$ and $v$, and any\nquery edge $e$, or if there is a path from $u$ to $v$ avoiding a vertex $w$,\nfor any query vertices $u$, $v$, and $w$.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 20:05:32 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 13:22:30 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Georgiadis", "Loukas", ""], ["Graf", "Daniel", ""], ["Italiano", "Giuseppe F.", ""], ["Parotsidis", "Nikos", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1612.08097", "submitter": "Rahul Saladi", "authors": "Saladi Rahul", "title": "New Results for Adaptive and Approximate Counting of Inversions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting inversions is a classic and important problem in databases. The\nnumber of inversions, $K^*$, in a list $L=(L(1),L(2),\\ldots,L(n))$ is defined\nas the number of pairs $i < j$ with $L(i) > L(j)$. In this paper, new results\nfor this problem are presented:\n  (1) In the I/O-model, an adaptive algorithm is presented for calculating\n$K^{*}$. The algorithm performs $O(\\frac{N}{B}+\n\\frac{N}{B}\\log_{M/B}(\\frac{K^*}{NB}))$ I/Os. When $K^{*}=O(NM)$, then the\nalgorithm takes only $O(\\frac{N}{B})$ I/Os. This algorithm can be modified to\nmatch the state of the art for the comparison based model and the RAM model.\n  (2) In the RAM model, a linear-time algorithm is presented to obtain a tight\nestimate of $K^*$; specifically, a value which lies with high probability in\nthe range $[(1-\\frac{\\log N}{N^{1/4}})K^*,(1+\\frac{\\log N}{N^{1/4}})K^*]$. The\nstate of the art linear-time algorithm works for the special case where $L$ is\na permutation, i.e., each $L(i)$ is a distinct integer in the range $[1,N]$. In\nthis paper, we handle a general case where each $L(i)$ is a real number.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 21:13:49 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Rahul", "Saladi", ""]]}, {"id": "1612.08234", "submitter": "Mohammad Reza Hooshmandasl", "authors": "M. Rajaati and M. R. Hooshmandasl and M. J. Dinneen and A. Shakiba", "title": "On fixed-parameter tractability of the mixed domination problem for\n  graphs with bounded tree-width", "comments": "Accepted for the publication in the Journal of Discrete Mathematics &\n  Theoretical Computer Science (DMTCS). 25 pages, 4 figures, 17 tables, 4\n  algorithms", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 20 no.\n  2, Graph Theory (July 31, 2018) dmtcs:4686", "doi": "10.23638/DMTCS-20-2-2", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixed dominating set for a graph $G = (V,E)$ is a set $S\\subseteq V \\cup E$\nsuch that every element $x \\in (V \\cup E) \\backslash S$ is either adjacent or\nincident to an element of $S$. The mixed domination number of a graph $G$,\ndenoted by $\\gamma_m(G)$, is the minimum cardinality of mixed dominating sets\nof $G$. Any mixed dominating set with the cardinality of $\\gamma_m(G)$ is\ncalled a minimum mixed dominating set. The mixed domination set (MDS) problem\nis to find a minimum mixed dominating set for a graph $G$ and is known to be an\nNP-complete problem. In this paper, we present a novel approach to find all of\nthe mixed dominating sets, called the AMDS problem, of a graph with bounded\ntree-width $tw$. Our new technique of assigning power values to edges and\nvertices, and combining with dynamic programming, leads to a fixed-parameter\nalgorithm of time $O(3^{tw^{2}}\\times tw^2 \\times |V|)$. This shows that MDS is\nfixed-parameter tractable with respect to tree-width. In addition, we\ntheoretically improve the proposed algorithm to solve the MDS problem in\n$O(6^{tw} \\times |V|)$ time.\n", "versions": [{"version": "v1", "created": "Sun, 25 Dec 2016 04:14:24 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 05:51:34 GMT"}, {"version": "v3", "created": "Sat, 14 Jul 2018 17:22:15 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Rajaati", "M.", ""], ["Hooshmandasl", "M. R.", ""], ["Dinneen", "M. J.", ""], ["Shakiba", "A.", ""]]}, {"id": "1612.08555", "submitter": "Samuel L. Smith", "authors": "Samuel L Smith", "title": "Monte Carlo Sort for unreliable human comparisons", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms which sort lists of real numbers into ascending order have been\nstudied for decades. They are typically based on a series of pairwise\ncomparisons and run entirely on chip. However people routinely sort lists which\ndepend on subjective or complex judgements that cannot be automated. Examples\ninclude marketing research; where surveys are used to learn about customer\npreferences for products, the recruiting process; where interviewers attempt to\nrank potential employees, and sporting tournaments; where we infer team\nrankings from a series of one on one matches. We develop a novel sorting\nalgorithm, where each pairwise comparison reflects a subjective human judgement\nabout which element is bigger or better. We introduce a finite and large error\nrate to each judgement, and we take the cost of each comparison to\nsignificantly exceed the cost of other computational steps. The algorithm must\nrequest the most informative sequence of comparisons from the user; in order to\nidentify the correct sorted list with minimum human input. Our Discrete\nAdiabatic Monte Carlo approach exploits the gradual acquisition of information\nby tracking a set of plausible hypotheses which are updated after each\nadditional comparison.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 09:54:07 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Smith", "Samuel L", ""]]}, {"id": "1612.08654", "submitter": "Ching-Lueh Chang", "authors": "Ching-Lueh Chang", "title": "Metric 1-median selection with fewer queries", "comments": "A preliminary version of this paper appears in {\\em Proceedings of\n  the 2017 IEEE International Conference on Applied System Innovation},\n  Sapporo, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $h\\colon\\mathbb{Z}^+\\to\\mathbb{Z}^+\\setminus\\{1\\}$ be any function such\nthat $h(n)$ and $\\lceil n^{1/h(n)}\\rceil$ are computable from $n$ in\n$O(h(n)\\cdot n^{1+1/h(n)})$ time. We show that given any $n$-point metric space\n$(M,d)$, the problem of finding $\\mathop{\\mathrm{argmin}}_{i\\in M}\\,\\sum_{j\\in\nM}\\,d(i,j)$ (breaking ties arbitrarily) has a deterministic, $O(h(n)\\cdot\nn^{1+1/h(n)})$-time, $O(n^{1+1/h(n)})$-query, $(2\\,h(n))$-approximation and\nnonadaptive algorithm. Our proofs modify those of Chang~\\cite{Cha15, Cha15CMCT}\nwith the following improvements: (1) We improve Chang's~\\cite{Cha15} query\ncomplexity of $O(h(n)\\cdot n^{1+1/h(n)})$ to $O(n^{1+1/h(n)})$, everything else\nbeing equal. (2) Chang's~\\cite{Cha15CMCT} unpublished work establishes our\nresult only when $n$ is a perfect $(h(n))$th power.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 15:19:59 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 05:22:02 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Chang", "Ching-Lueh", ""]]}, {"id": "1612.08795", "submitter": "Tengyu Ma", "authors": "Sanjeev Arora, Rong Ge, Tengyu Ma, Andrej Risteski", "title": "Provable learning of Noisy-or Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning applications use latent variable models to explain\nstructure in data, whereby visible variables (= coordinates of the given\ndatapoint) are explained as a probabilistic function of some hidden variables.\nFinding parameters with the maximum likelihood is NP-hard even in very simple\nsettings. In recent years, provably efficient algorithms were nevertheless\ndeveloped for models with linear structures: topic models, mixture models,\nhidden markov models, etc. These algorithms use matrix or tensor decomposition,\nand make some reasonable assumptions about the parameters of the underlying\nmodel.\n  But matrix or tensor decomposition seems of little use when the latent\nvariable model has nonlinearities. The current paper shows how to make\nprogress: tensor decomposition is applied for learning the single-layer {\\em\nnoisy or} network, which is a textbook example of a Bayes net, and used for\nexample in the classic QMR-DT software for diagnosing which disease(s) a\npatient may have by observing the symptoms he/she exhibits.\n  The technical novelty here, which should be useful in other settings in\nfuture, is analysis of tensor decomposition in presence of systematic error\n(i.e., where the noise/error is correlated with the signal, and doesn't\ndecrease as number of samples goes to infinity). This requires rethinking all\nsteps of tensor decomposition methods from the ground up.\n  For simplicity our analysis is stated assuming that the network parameters\nwere chosen from a probability distribution but the method seems more generally\napplicable.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 03:35:59 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Arora", "Sanjeev", ""], ["Ge", "Rong", ""], ["Ma", "Tengyu", ""], ["Risteski", "Andrej", ""]]}, {"id": "1612.08821", "submitter": "Ophir Friedler", "authors": "Alon Eden, Michal Feldman, Ophir Friedler, Inbal Talgam-Cohen and S.\n  Matthew Weinberg", "title": "The Competition Complexity of Auctions: A Bulow-Klemperer Result for\n  Multi-Dimensional Bidders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A seminal result of Bulow and Klemperer [1989] demonstrates the power of\ncompetition for extracting revenue: when selling a single item to $n$ bidders\nwhose values are drawn i.i.d. from a regular distribution, the simple\nwelfare-maximizing VCG mechanism (in this case, a second price-auction) with\none additional bidder extracts at least as much revenue in expectation as the\noptimal mechanism. The beauty of this theorem stems from the fact that VCG is a\n{\\em prior-independent} mechanism, where the seller possesses no information\nabout the distribution, and yet, by recruiting one additional bidder it\nperforms better than any prior-dependent mechanism tailored exactly to the\ndistribution at hand (without the additional bidder).\n  In this work, we establish the first {\\em full Bulow-Klemperer} results in\n{\\em multi-dimensional} environments, proving that by recruiting additional\nbidders, the revenue of the VCG mechanism surpasses that of the optimal\n(possibly randomized, Bayesian incentive compatible) mechanism. For a given\nenvironment with i.i.d. bidders, we term the number of additional bidders\nneeded to achieve this guarantee the environment's {\\em competition\ncomplexity}.\n  Using the recent duality-based framework of Cai et al. [2016] for reasoning\nabout optimal revenue, we show that the competition complexity of $n$ bidders\nwith additive valuations over $m$ independent, regular items is at most\n$n+2m-2$ and at least $\\log(m)$. We extend our results to bidders with additive\nvaluations subject to downward-closed constraints, showing that these\nsignificantly more general valuations increase the competition complexity by at\nmost an additive $m-1$ factor. We further improve this bound for the special\ncase of matroid constraints, and provide additional extensions as well.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 08:09:29 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Eden", "Alon", ""], ["Feldman", "Michal", ""], ["Friedler", "Ophir", ""], ["Talgam-Cohen", "Inbal", ""], ["Weinberg", "S. Matthew", ""]]}, {"id": "1612.08958", "submitter": "Peter Hoyer", "authors": "Peter Hoyer and Mojtaba Komeili", "title": "Efficient quantum walk on the grid with multiple marked elements", "comments": "18 pages, to appear in STACS 2017, the 34th International Symposium\n  on Theoretical Aspects of Computer Science", "journal-ref": "34th Symposium on Theoretical Aspects of Computer Science (STACS),\n  vol 66 of LIPIcs, pp. 42:1-42:14, 2017", "doi": "10.4230/LIPIcs.STACS.2017.42", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a quantum algorithm for finding a marked element on the grid when\nthere are multiple marked elements. Our algorithm uses quadratically fewer\nsteps than a random walk on the grid, ignoring logarithmic factors. This is the\nfirst known quantum walk that finds a marked element in a number of steps less\nthan the square-root of the extended hitting time. We also give a new tighter\nupper bound on the extended hitting time of a marked subset, expressed in terms\nof the hitting times of its members.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 19:31:35 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Hoyer", "Peter", ""], ["Komeili", "Mojtaba", ""]]}, {"id": "1612.09083", "submitter": "Anubhav Baweja", "authors": "Anubhav Baweja", "title": "A Constant Optimization of the Binary Indexed Tree Query Operation", "comments": "7 pages, 2 figures, 2 graphs, 4 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several data structures which can calculate the prefix sums of an\narray efficiently, while handling point updates on the array, such as Segment\nTrees and Binary Indexed Trees (BIT). Both these data structures can handle the\nthese two operations (query and update) in $O(\\log{n})$ time. In this paper, we\npresent a data structure similar to the BIT, but with an even smaller constant.\nTo do this, we use Zeckendorf's Theorem, a property of the Fibonacci sequence\nof numbers. The new data structure achieves the same complexity of\n$O(\\log{n})$, but requires about $\\log_{\\phi^{2}} n$ computations for the Query\nOperation as opposed to the $\\log_{2} n$ computations required for a BIT Query\nOperation in the worst case.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 09:48:11 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Baweja", "Anubhav", ""]]}, {"id": "1612.09145", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Adrian Kosowski and Przemys{\\l}aw Uzna\\'nski", "title": "Ergodic Effects in Token Circulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a dynamical process in a network which distributes all particles\n(tokens) located at a node among its neighbors, in a round-robin manner.\n  We show that in the recurrent state of this dynamics (i.e., disregarding a\npolynomially long initialization phase of the system), the number of particles\nlocated on a given edge, averaged over an interval of time, is tightly\nconcentrated around the average particle density in the system. Formally, for a\nsystem of $k$ particles in a graph of $m$ edges, during any interval of length\n$T$, this time-averaged value is $k/m \\pm \\widetilde{O}(1/T)$, whenever\n$\\gcd(m,k) = \\widetilde{O}(1)$ (and so, e.g., whenever $m$ is a prime number).\nTo achieve these bounds, we link the behavior of the studied dynamics to\nergodic properties of traversals based on Eulerian circuits on a symmetric\ndirected graph. These results are proved through sum set methods and are likely\nto be of independent interest.\n  As a corollary, we also obtain bounds on the \\emph{idleness} of the studied\ndynamics, i.e., on the longest possible time between two consecutive\nappearances of a token on an edge, taken over all edges. Designing trajectories\nfor $k$ tokens in a way which minimizes idleness is fundamental to the study of\nthe patrolling problem in networks. Our results immediately imply a bound of\n$\\widetilde{O}(m/k)$ on the idleness of the studied process, showing that it is\na distributed $\\widetilde{O}(1)$-competitive solution to the patrolling task,\nfor all of the covered cases. Our work also provides some further insights that\nmay be interesting in load-balancing applications.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 13:54:35 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 12:30:20 GMT"}, {"version": "v3", "created": "Fri, 3 Nov 2017 20:29:52 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Kosowski", "Adrian", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1612.09168", "submitter": "Parham Ghayour", "authors": "Parham Ghayour", "title": "A New Algorithm to Compare the Magnitude of Two RNS Numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparison of two numbers in RNS systems is a challenging task. In this\npaper, a new algorithm to compare the magnitude of two RNS numbers, using a\nclustering method has been proposed. In the clustering process, each inputted\nnumber is assigned to a cluster. To compare the magnitude of two numbers, first\nthe clusters of these numbers and their differences are obtained. Then by\ncomparing these clusters, the relative magnitude of two numbers is determined.\nAll of these processes are performed in RNS system without converting numbers\nto the binary system.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 15:17:19 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Ghayour", "Parham", ""]]}, {"id": "1612.09171", "submitter": "Yun Kuen Cheung", "authors": "Yun Kuen Cheung and Richard Cole", "title": "A Unified Approach to Analyzing Asynchronous Coordinate Descent and\n  Tatonnement", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.GT math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns asynchrony in iterative processes, focusing on gradient\ndescent and tatonnement, a fundamental price dynamic.\n  Gradient descent is an important class of iterative algorithms for minimizing\nconvex functions. Classically, gradient descent has been a sequential and\nsynchronous process, although distributed and asynchronous variants have been\nstudied since the 1980s. Coordinate descent is a commonly studied version of\ngradient descent. In this paper, we focus on asynchronous coordinate descent on\nconvex functions $F:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ of the form $F(x) = f(x)\n+ \\sum_{k=1}^n \\Psi_k(x_k)$, where $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ is a\nsmooth convex function, and each $\\Psi_k:\\mathbb{R}\\rightarrow\\mathbb{R}$ is a\nunivariate and possibly non-smooth convex function. Such functions occur in\nmany data analysis and machine learning problems.\n  We give new analyses of cyclic coordinate descent, a parallel asynchronous\nstochastic coordinate descent, and a rather general worst-case parallel\nasynchronous coordinate descent. For all of these, we either obtain sharply\nimproved bounds, or provide the first analyses. Our analyses all use a common\namortized framework. The application of this framework to the asynchronous\nstochastic version requires some new ideas, for it is not obvious how to ensure\na uniform distribution where it is needed in the face of asynchronous actions\nthat may undo uniformity. We believe that our approach may well be applicable\nto the analysis of other iterative asynchronous stochastic processes.\n  We extend the framework to show that an asynchronous version of tatonnement,\na fundamental price dynamic widely studied in general equilibrium theory,\nconverges toward a market equilibrium for Fisher markets with CES utilities or\nLeontief utilities, for which tatonnement is equivalent to coordinate descent.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 15:27:08 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Cheung", "Yun Kuen", ""], ["Cole", "Richard", ""]]}, {"id": "1612.09299", "submitter": "Christian Kudahl", "authors": "Jhoirene Clemente, Juraj Hromkovic, Dennis Komm, Christian Kudahl", "title": "Advice Complexity of the Online Search Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The online search problem is a fundamental problem in finance. The numerous\ndirect applications include searching for optimal prices for commodity trading\nand trading foreign currencies. In this paper, we analyze the advice complexity\nof this problem. In particular, we are interested in identifying the minimum\namount of information needed in order to achieve a certain competitive ratio.\nWe design an algorithm that reads b bits of advice and achieves a competitive\nratio of (M/m)^(1/(2^b+1)) where M and m are the maximum and minimum price in\nthe input. We also give a matching lower bound. Furthermore, we compare the\npower of advice and randomization for this problem.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 20:59:09 GMT"}, {"version": "v2", "created": "Sun, 8 Jan 2017 19:51:04 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Clemente", "Jhoirene", ""], ["Hromkovic", "Juraj", ""], ["Komm", "Dennis", ""], ["Kudahl", "Christian", ""]]}, {"id": "1612.09368", "submitter": "Dongxiao Yu", "authors": "Na Wang, Dongxiao Yu, Hai Jin, Chen Qian, Xia Xie, Qiang-Sheng Hua", "title": "Parallel Algorithms for Core Maintenance in Dynamic Graphs", "comments": "11 pages,9 figures,1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper initiates the studies of parallel algorithms for core maintenance\nin dynamic graphs. The core number is a fundamental index reflecting the\ncohesiveness of a graph, which are widely used in large-scale graph analytics.\nThe core maintenance problem requires to update the core numbers of vertices\nafter a set of edges and vertices are inserted into or deleted from the graph.\nWe investigate the parallelism in the core update process when multiple edges\nand vertices are inserted or deleted. Specifically, we discover a structure\ncalled superior edge set, the insertion or deletion of edges in which can be\nprocessed in parallel. Based on the structure of superior edge set, efficient\nparallel algorithms are then devised for incremental and decremental core\nmaintenance respectively. To the best of our knowledge, the proposed algorithms\nare the first parallel ones for the fundamental core maintenance problem. The\nalgorithms show a significant speedup in the processing time compared with\nprevious results that sequentially handle edge and vertex insertions/deletions.\nFinally, extensive experiments are conducted on different types of real-world\nand synthetic datasets, and the results illustrate the efficiency, stability\nand scalability of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 02:01:33 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Wang", "Na", ""], ["Yu", "Dongxiao", ""], ["Jin", "Hai", ""], ["Qian", "Chen", ""], ["Xie", "Xia", ""], ["Hua", "Qiang-Sheng", ""]]}, {"id": "1612.09388", "submitter": "Mohit Garg", "authors": "Mohit Garg and Jaikumar Radhakrishnan", "title": "Set membership with non-adaptive bit probes", "comments": "31 pages, full version of 'Set membership with non-adaptive bit\n  probes. STACS 2017 (to appear)'. arXiv admin note: text overlap with\n  arXiv:1504.02035", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the non-adaptive bit-probe complexity of the set membership\nproblem, where a set S of size at most n from a universe of size m is to be\nrepresented as a short bit vector in order to answer membership queries of the\nform \"Is x in S?\" by non-adaptively probing the bit vector at t places. Let\ns_N(m,n,t) be the minimum number of bits of storage needed for such a scheme.\nIn this work, we show existence of non-adaptive and adaptive schemes for a\nrange of t that improves an upper bound of Buhrman, Miltersen, Radhakrishnan\nand Srinivasan (2002) on s_N(m,n,t). For three non-adaptive probes, we improve\nthe previous best lower bound on s_N(m,n,3) by Alon and Feige (2009).\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 04:50:26 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Garg", "Mohit", ""], ["Radhakrishnan", "Jaikumar", ""]]}]