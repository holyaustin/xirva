[{"id": "1710.00223", "submitter": "Vinod Reddy", "authors": "I. Vinod Reddy", "title": "Parameterized Algorithms for Conflict-free Colorings of Graphs", "comments": "appears in SOFSEM Student Research Forum 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the conflict-free coloring of graphs induced by\nneighborhoods. A coloring of a graph is conflict-free if every vertex has a\nuniquely colored vertex in its neighborhood. The conflict-free coloring problem\nis to color the vertices of a graph using the minimum number of colors such\nthat the coloring is conflict-free. We consider both closed neighborhoods,\nwhere the neighborhood of a vertex includes itself, and open neighborhoods,\nwhere a vertex does not included in its neighborhood. We study the\nparameterized complexity of conflict-free closed neighborhood coloring and\nconflict-free open neighborhood coloring problems. We show that both problems\nare fixed-parameter tractable (FPT) when parameterized by the cluster vertex\ndeletion number of the input graph. This generalizes the result of Gargano et\nal.(2015) that conflict-free coloring is fixed-parameter tractable\nparameterized by the vertex cover number. Also, we show that both problems\nadmit an additive constant approximation algorithm when parameterized by the\ndistance to threshold graphs.\n  We also study the complexity of the problem on special graph classes. We show\nthat both problems can be solved in polynomial time on cographs. For split\ngraphs, we give a polynomial time algorithm for closed neighborhood\nconflict-free coloring problem, whereas we show that open neighborhood\nconflict-free coloring is NP-complete. We show that interval graphs can be\nconflict-free colored using at most four colors.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 16:59:10 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Reddy", "I. Vinod", ""]]}, {"id": "1710.00264", "submitter": "Samuel Hopkins", "authors": "Samuel B. Hopkins and David Steurer", "title": "Bayesian estimation from few samples: community detection and related\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient meta-algorithm for Bayesian estimation problems that\nis based on low-degree polynomials, semidefinite programming, and tensor\ndecomposition. The algorithm is inspired by recent lower bound constructions\nfor sum-of-squares and related to the method of moments. Our focus is on sample\ncomplexity bounds that are as tight as possible (up to additive lower-order\nterms) and often achieve statistical thresholds or conjectured computational\nthresholds.\n  Our algorithm recovers the best known bounds for community detection in the\nsparse stochastic block model, a widely-studied class of estimation problems\nfor community detection in graphs. We obtain the first recovery guarantees for\nthe mixed-membership stochastic block model (Airoldi et el.) in constant\naverage degree graphs---up to what we conjecture to be the computational\nthreshold for this model. We show that our algorithm exhibits a sharp\ncomputational threshold for the stochastic block model with multiple\ncommunities beyond the Kesten--Stigum bound---giving evidence that this task\nmay require exponential time.\n  The basic strategy of our algorithm is strikingly simple: we compute the\nbest-possible low-degree approximation for the moments of the posterior\ndistribution of the parameters and use a robust tensor decomposition algorithm\nto recover the parameters from these approximate posterior moments.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 21:58:34 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Hopkins", "Samuel B.", ""], ["Steurer", "David", ""]]}, {"id": "1710.00287", "submitter": "Khoa Trinh", "authors": "David G. Harris, Thomas Pensyl, Aravind Srinivasan, Khoa Trinh", "title": "A Lottery Model for Center-type Problems With Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give tight approximation algorithms for the $k$-center and\nmatroid center problems with outliers. Unfairness arises naturally in this\nsetting: certain clients could always be considered as outliers. To address\nthis issue, we introduce a lottery model in which each client $j$ is allowed to\nsubmit a parameter $p_j \\in [0,1]$ and we look for a random solution that\ncovers every client $j$ with probability at least $p_j$. Our techniques include\na randomized rounding procedure to round a point inside a matroid intersection\npolytope to a basis plus at most one extra item such that all marginal\nprobabilities are preserved and such that a certain linear function of the\nvariables does not decrease in the process with probability one.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 03:52:38 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Harris", "David G.", ""], ["Pensyl", "Thomas", ""], ["Srinivasan", "Aravind", ""], ["Trinh", "Khoa", ""]]}, {"id": "1710.00466", "submitter": "Huda Chuangpishit", "authors": "Huda Chuangpishit, Jurek Czyzowicz, Leszek Gasieniec, Konstantinos\n  Georgiou, Tomasz Jurdzinski, Evangelos Kranakis", "title": "Patrolling a Path Connecting a Set of Points with Unbalanced Frequencies\n  of Visits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patrolling consists of scheduling perpetual movements of a collection of\nmobile robots, so that each point of the environment is regularly revisited by\nany robot in the collection. In previous research, it was assumed that all\npoints of the environment needed to be revisited with the same minimal\nfrequency. In this paper we study efficient patrolling protocols for points\nlocated on a path, where each point may have a different constraint on\nfrequency of visits. The problem of visiting such divergent points was recently\nposed by Gasieniec et al. in [13], where the authors study protocols using a\nsingle robot patrolling a set of $n$ points located in nodes of a complete\ngraph and in Euclidean spaces. The focus in this paper is on patrolling with\ntwo robots. We adopt a scenario in which all points to be patrolled are located\non a line. We provide several approximation algorithms concluding with the best\ncurrently known $\\sqrt 3$-approximation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 03:04:21 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Chuangpishit", "Huda", ""], ["Czyzowicz", "Jurek", ""], ["Gasieniec", "Leszek", ""], ["Georgiou", "Konstantinos", ""], ["Jurdzinski", "Tomasz", ""], ["Kranakis", "Evangelos", ""]]}, {"id": "1710.00586", "submitter": "Isaac Goldstein", "authors": "Isaac Goldstein, Moshe Lewenstein and Ely Porat", "title": "Orthogonal Vectors Indexing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, intensive research work has been dedicated to prove\nconditional lower bounds in order to reveal the inner structure of the class P.\nThese conditional lower bounds are based on many popular conjectures on\nwell-studied problems. One of the most heavily used conjectures is the\ncelebrated Strong Exponential Time Hypothesis (SETH). It turns out that\nconditional hardness proved based on SETH goes, in many cases, through an\nintermediate problem - the Orthogonal Vectors (OV) problem.\n  Almost all research work regarding conditional lower bound was concentrated\non time complexity. Very little attention was directed toward space complexity.\nIn a recent work, Goldstein et al.[WADS 2017] set the stage for proving\nconditional lower bounds regarding space and its interplay with time. In this\nspirit, it is tempting to investigate the space complexity of a data structure\nvariant of OV which is called \\emph{OV indexing}. In this problem $n$ boolean\nvectors of size $c\\log{n}$ are given for preprocessing. As a query, a vector\n$v$ is given and we are required to verify if there is an input vector that is\northogonal to it or not.\n  This OV indexing problem is interesting in its own, but it also likely to\nhave strong implications on problems known to be conditionally hard, in terms\nof time complexity, based on OV. Having this in mind, we study OV indexing in\nthis paper from many aspects. We give some space-efficient algorithms for the\nproblem, show a tradeoff between space and query time, describe how to solve\nits reporting variant, shed light on an interesting connection between this\nproblem and the well-studied SetDisjointness problem and demonstrate how it can\nbe solved more efficiently on random input.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 11:21:43 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 05:13:06 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Goldstein", "Isaac", ""], ["Lewenstein", "Moshe", ""], ["Porat", "Ely", ""]]}, {"id": "1710.00668", "submitter": "Tom\\'a\\v{s} Masa\\v{r}\\'ik", "authors": "Pavel Dvo\\v{r}\\'ak, Andreas Emil Feldmann, Du\\v{s}an Knop, Tom\\'a\\v{s}\n  Masa\\v{r}\\'ik, Tom\\'a\\v{s} Toufar, Pavel Vesel\\'y", "title": "Parameterized Approximation Schemes for Steiner Trees with Small Number\n  of Steiner Vertices", "comments": "23 pages, 6 figures An extended abstract appeared in proceedings of\n  STACS 2018", "journal-ref": "SIAM Journal on Discrete Mathematics 35(1), 546-574 (2021)", "doi": "10.1137/18M1209489", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Steiner Tree problem, in which a set of terminal vertices needs\nto be connected in the cheapest possible way in an edge-weighted graph. This\nproblem has been extensively studied from the viewpoint of approximation and\nalso parametrization. In particular, on one hand Steiner Tree is known to be\nAPX-hard, and W[2]-hard on the other, if parameterized by the number of\nnon-terminals (Steiner vertices) in the optimum solution. In contrast to this\nwe give an efficient parameterized approximation scheme (EPAS), which\ncircumvents both hardness results. Moreover, our methods imply the existence of\na polynomial size approximate kernelization scheme (PSAKS) for the considered\nparameter.\n  We further study the parameterized approximability of other variants of\nSteiner Tree, such as Directed Steiner Tree and Steiner Forest. For neither of\nthese an EPAS is likely to exist for the studied parameter: for Steiner Forest\nan easy observation shows that the problem is APX-hard, even if the input graph\ncontains no Steiner vertices. For Directed Steiner Tree we prove that\napproximating within any function of the studied parameter is W[1]-hard.\nNevertheless, we show that an EPAS exists for Unweighted Directed Steiner Tree,\nbut a PSAKS does not. We also prove that there is an EPAS and a PSAKS for\nSteiner Forest if in addition to the number of Steiner vertices, the number of\nconnected components of an optimal solution is considered to be a parameter.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 14:10:38 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 20:17:30 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 07:10:32 GMT"}, {"version": "v4", "created": "Tue, 14 Jul 2020 06:53:15 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Dvo\u0159\u00e1k", "Pavel", ""], ["Feldmann", "Andreas Emil", ""], ["Knop", "Du\u0161an", ""], ["Masa\u0159\u00edk", "Tom\u00e1\u0161", ""], ["Toufar", "Tom\u00e1\u0161", ""], ["Vesel\u00fd", "Pavel", ""]]}, {"id": "1710.00775", "submitter": "Euripides Markou", "authors": "Jurek Czyzowicz and Maxime Godon and Evangelos Kranakis and Arnaud\n  Labourel and Euripides Markou", "title": "Exploring Graphs with Time Constraints by Unreliable Collections of\n  Mobile Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph environment must be explored by a collection of mobile robots. Some\nof the robots, a priori unknown, may turn out to be unreliable. The graph is\nweighted and each node is assigned a deadline. The exploration is successful if\neach node of the graph is visited before its deadline by a reliable robot. The\nedge weight corresponds to the time needed by a robot to traverse the edge.\nGiven the number of robots which may crash, is it possible to design an\nalgorithm, which will always guarantee the exploration, independently of the\nchoice of the subset of unreliable robots by the adversary? We find the optimal\ntime, during which the graph may be explored. Our approach permits to find the\nmaximal number of robots, which may turn out to be unreliable, and the graph is\nstill guaranteed to be explored.\n  We concentrate on line graphs and rings, for which we give positive results.\nWe start with the case of the collections involving only reliable robots. We\ngive algorithms finding optimal times needed for exploration when the robots\nare assigned to fixed initial positions as well as when such starting positions\nmay be determined by the algorithm. We extend our consideration to the case\nwhen some number of robots may be unreliable. Our most surprising result is\nthat solving the line exploration problem with robots at given positions, which\nmay involve crash-faulty ones, is NP-hard. The same problem has polynomial\nsolutions for a ring and for the case when the initial robots' positions on the\nline are arbitrary.\n  The exploration problem is shown to be NP-hard for star graphs, even when the\nteam consists of only two reliable robots.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 16:49:24 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Czyzowicz", "Jurek", ""], ["Godon", "Maxime", ""], ["Kranakis", "Evangelos", ""], ["Labourel", "Arnaud", ""], ["Markou", "Euripides", ""]]}, {"id": "1710.00852", "submitter": "Rui A.  da Costa", "authors": "N. A. M. Ara\\'ujo, R. A. da Costa, S. N. Dorogovtsev, and J. F. F.\n  Mendes", "title": "Finding the optimal nets for self-folding Kirigami", "comments": "6 pages, 5 figures, Supplemental Material, Source Code", "journal-ref": "Phys. Rev. Lett. 120, 188001 (2018)", "doi": "10.1103/PhysRevLett.120.188001", "report-no": null, "categories": "cs.DS cond-mat.soft cond-mat.stat-mech physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional shells can be synthesized from the spontaneous self-folding\nof two-dimensional templates of interconnected panels, called nets. However,\nsome nets are more likely to self-fold into the desired shell under random\nmovements. The optimal nets are the ones that maximize the number of vertex\nconnections, i.e., vertices that have only two of its faces cut away from each\nother in the net. Previous methods for finding such nets are based on random\nsearch and thus do not guarantee the optimal solution. Here, we propose a\ndeterministic procedure. We map the connectivity of the shell into a shell\ngraph, where the nodes and links of the graph represent the vertices and edges\nof the shell, respectively. Identifying the nets that maximize the number of\nvertex connections corresponds to finding the set of maximum leaf spanning\ntrees of the shell graph. This method allows not only to design the\nself-assembly of much larger shell structures but also to apply additional\ndesign criteria, as a complete catalog of the maximum leaf spanning trees is\nobtained.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 18:11:45 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 21:13:32 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Ara\u00fajo", "N. A. M.", ""], ["da Costa", "R. A.", ""], ["Dorogovtsev", "S. N.", ""], ["Mendes", "J. F. F.", ""]]}, {"id": "1710.00904", "submitter": "Xuchao Zhang", "authors": "Xuchao Zhang, Liang Zhao, Arnold P. Boedihardjo, Chang-Tien Lu", "title": "Online and Distributed Robust Regressions under Adversarial Data\n  Corruption", "comments": "Accepted by ICDM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's era of big data, robust least-squares regression becomes a more\nchallenging problem when considering the adversarial corruption along with\nexplosive growth of datasets. Traditional robust methods can handle the noise\nbut suffer from several challenges when applied in huge dataset including 1)\ncomputational infeasibility of handling an entire dataset at once, 2) existence\nof heterogeneously distributed corruption, and 3) difficulty in corruption\nestimation when data cannot be entirely loaded. This paper proposes online and\ndistributed robust regression approaches, both of which can concurrently\naddress all the above challenges. Specifically, the distributed algorithm\noptimizes the regression coefficients of each data block via heuristic hard\nthresholding and combines all the estimates in a distributed robust\nconsolidation. Furthermore, an online version of the distributed algorithm is\nproposed to incrementally update the existing estimates with new incoming data.\nWe also prove that our algorithms benefit from strong robustness guarantees in\nterms of regression coefficient recovery with a constant upper bound on the\nerror of state-of-the-art batch methods. Extensive experiments on synthetic and\nreal datasets demonstrate that our approaches are superior to those of existing\nmethods in effectiveness, with competitive efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 20:55:39 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Zhang", "Xuchao", ""], ["Zhao", "Liang", ""], ["Boedihardjo", "Arnold P.", ""], ["Lu", "Chang-Tien", ""]]}, {"id": "1710.00944", "submitter": "Mikhail Gudim", "authors": "Mikhail Gudim", "title": "Ordered Dags: HypercubeSort", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalise the insertion into a binary heap to any directed acyclic graph\n(DAG) with one source vertex. This lets us formulate a general method for\nconverting any such DAG into a data structure with priority queue interface. We\napply our method to a hypercube DAG to obtain a sorting algorithm of complexity\n$\\mathcal{O}(n\\log^2 (n))$. As another curious application, we derive a\nrelationship between length of longest path and maximum degree of a vertex in a\nDAG.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 00:16:54 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Gudim", "Mikhail", ""]]}, {"id": "1710.00950", "submitter": "Hanna Sumita", "authors": "Yasushi Kawase, Kei Kimura, Kazuhisa Makino, Hanna Sumita", "title": "Optimal Matroid Partitioning Problems", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies optimal matroid partitioning problems for various\nobjective functions. In the problem, we are given a finite set $E$ and $k$\nweighted matroids $(E, \\mathcal{I}_i, w_i)$, $i = 1, \\dots, k$, and our task is\nto find a minimum partition $(I_1,\\dots,I_k)$ of $E$ such that $I_i \\in\n\\mathcal{I}_i$ for all $i$. For each objective function, we give a\npolynomial-time algorithm or prove NP-hardness. In particular, for the case\nwhen the given weighted matroids are identical and the objective function is\nthe sum of the maximum weight in each set (i.e., $\\sum_{i=1}^k\\max_{e\\in\nI_i}w_i(e)$), we show that the problem is strongly NP-hard but admits a PTAS.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 01:00:19 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Kawase", "Yasushi", ""], ["Kimura", "Kei", ""], ["Makino", "Kazuhisa", ""], ["Sumita", "Hanna", ""]]}, {"id": "1710.00953", "submitter": "Michal Mankowski", "authors": "Sommer Gentry, Michal Mankowski, T. S. Michael, Dorry Segev", "title": "Maximum Matchings in Graphs for Allocating Kidney Paired Donation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relatives and friends of an end-stage renal disease patient who offer to\ndonate a kidney are often found to be incompatible with their intended\nrecipients. Kidney paired donation matches one patient and his incompatible\ndonor with another patient and donor in the same situation for an organ\nexchange. Let patient- donor pairs be the vertices of an undirected graph G,\nwith an edge connecting any two reciprocally compatible vertices. A matching in\nG is a feasible set of paired donations. We describe various optimization\nproblems on kidney paired donation graphs G and the merits of each in clinical\ntransplantation. Because some matches are geographically undesirable, and the\nexpected lifespan of a transplanted kidney depends on the immunologic\nconcordance of donor and recipient, we weight the edges of G and seek a maximum\nedge-weight matching. Unfortunately, such matchings might not have the maximum\ncardinality; there is a risk of an unpredictable trade-off between the quality\nand quantity of paired donations. We propose an edge-weighting of G which\nguarantees that every matching with maximum weight also has maximum\ncardinality, and also maximizes the number of transplants for an exceptional\nsubset of recipients, while reducing travel and favoring immunologic\nconcordance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 01:27:50 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 09:07:58 GMT"}, {"version": "v3", "created": "Sun, 22 Oct 2017 09:14:58 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Gentry", "Sommer", ""], ["Mankowski", "Michal", ""], ["Michael", "T. S.", ""], ["Segev", "Dorry", ""]]}, {"id": "1710.01143", "submitter": "Elisabetta Bergamini", "authors": "Patrick Bisenius, Elisabetta Bergamini, Eugenio Angriman, Henning\n  Meyerhenke", "title": "Computing Top-k Closeness Centrality in Fully-dynamic Graphs", "comments": "Accepted for publication at the 20th SIAM Workshop on Algorithm\n  Engineering and Experiments (ALENEX 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Closeness is a widely-studied centrality measure. Since it requires all\npairwise distances, computing closeness for all nodes is infeasible for large\nreal-world networks. However, for many applications, it is only necessary to\nfind the k most central nodes and not all closeness values. Prior work has\nshown that computing the top-k nodes with highest closeness can be done much\nfaster than computing closeness for all nodes in real-world networks. However,\nfor networks that evolve over time, no dynamic top-k closeness algorithm exists\nthat improves on static recomputation. In this paper, we present several\ntechniques that allow us to efficiently compute the k nodes with highest\n(harmonic) closeness after an edge insertion or an edge deletion. Our\nalgorithms use information obtained during earlier computations to omit\nunnecessary work. However, they do not require asymptotically more memory than\nthe static algorithms (i. e., linear in the number of nodes). We propose\nseparate algorithms for complex networks (which exhibit the small-world\nproperty) and networks with large diameter such as street networks, and we\ncompare them against static recomputation on a variety of real-world networks.\nOn many instances, our dynamic algorithms are two orders of magnitude faster\nthan recomputation; on some large graphs, we even reach average speedups\nbetween $10^3$ and $10^4$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 13:32:51 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Bisenius", "Patrick", ""], ["Bergamini", "Elisabetta", ""], ["Angriman", "Eugenio", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1710.01144", "submitter": "Henning Meyerhenke", "authors": "Elisabetta Bergamini, Tanya Gonser, Henning Meyerhenke", "title": "Scaling up Group Closeness Maximization", "comments": "A previous version of this paper appeared in the Proc. of 20th SIAM\n  Workshop on Algorithm Engineering and Experiments (ALENEX 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Closeness is a widely-used centrality measure in social network analysis. For\na node it indicates the reciprocal of the average shortest-path distance to the\nother nodes of the network. While the identification of the k nodes with\nhighest closeness received significant attention, many applications are\nactually interested in finding a group of nodes that is central as a whole. For\nthis problem, only recently a greedy algorithm has been proposed [Chen et al.,\nADC 2016]. The approximation factor of (1 - 1/e) proposed by Chen et al. for\nthis algorithm does not hold, though, as we show in this version of our paper.\nSince their implementation of the greedy algorithm was still too slow for large\nnetworks, Chen et al. also proposed a heuristic without approximation\nguarantee.\n  In the present paper we develop new techniques to speed up the greedy\nalgorithm. Compared to the previous implementation, our approach is orders of\nmagnitude faster and, compared to the heuristic proposed by Chen et al., we\nalways find a solution with better quality in a comparable running time in our\nexperiments. Our method Greedy++ allows us to estimate the group with maximum\ncloseness on networks with up to hundreds of millions of edges in minutes or at\nmost a few hours. The greedy approach by [Chen et al., ADC 2016] would take\nseveral days already on networks with hundreds of thousands of edges. Our\nexperiments show that the solution found by Greedy++ is actually very close to\nthe optimum (...)\n  Note: This paper version fixes the issue of relying on the presumed (but\nincorrect) submodularity of group closeness. While this has implications on the\ntheoretical assessment of the greedy algorithm, our algorithm variant and its\nimplementation remain unaffected. The reason is that Greedy++ relies (among\nothers) on the supermodularity of farness, which does hold.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 13:35:28 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 15:36:22 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Bergamini", "Elisabetta", ""], ["Gonser", "Tanya", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1710.01431", "submitter": "Adithya Vadapalli", "authors": "Grigory Yaroslavtsev, Adithya Vadapalli", "title": "Massively Parallel Algorithms and Hardness for Single-Linkage Clustering\n  Under $\\ell_p$-Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present massively parallel (MPC) algorithms and hardness of approximation\nresults for computing Single-Linkage Clustering of $n$ input $d$-dimensional\nvectors under Hamming, $\\ell_1, \\ell_2$ and $\\ell_\\infty$ distances. All our\nalgorithms run in $O(\\log n)$ rounds of MPC for any fixed $d$ and achieve\n$(1+\\epsilon)$-approximation for all distances (except Hamming for which we\nshow an exact algorithm). We also show constant-factor inapproximability\nresults for $o(\\log n)$-round algorithms under standard MPC hardness\nassumptions (for sufficiently large dimension depending on the distance used).\nEfficiency of implementation of our algorithms in Apache Spark is demonstrated\nthrough experiments on a variety of datasets exhibiting speedups of several\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 00:48:54 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 04:08:40 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Yaroslavtsev", "Grigory", ""], ["Vadapalli", "Adithya", ""]]}, {"id": "1710.01516", "submitter": "Stefano Leucci", "authors": "Davide Bil\\`o, Feliciano Colella, Luciano Gual\\`a, Stefano Leucci,\n  Guido Proietti", "title": "An Improved Algorithm for Computing All the Best Swap Edges of a Tree\n  Spanner", "comments": "17 pages, 4 figures, ISAAC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tree $\\sigma$-spanner of a positively real-weighted $n$-vertex and $m$-edge\nundirected graph $G$ is a spanning tree $T$ of $G$ which approximately\npreserves (i.e., up to a multiplicative stretch factor $\\sigma$) distances in\n$G$. Tree spanners with provably good stretch factors find applications in\ncommunication networks, distributed systems, and network design. However,\nfinding an optimal or even a good tree spanner is a very hard computational\ntask. Thus, if one has to face a transient edge failure in $T$, the overall\neffort that has to be afforded to rebuild a new tree spanner (i.e.,\ncomputational costs, set-up of new links, updating of the routing tables, etc.)\ncan be rather prohibitive. To circumvent this drawback, an effective\nalternative is that of associating with each tree edge a best possible (in\nterms of resulting stretch) swap edge -- a well-established approach in the\nliterature for several other tree topologies. Correspondingly, the problem of\ncomputing all the best swap edges of a tree spanner is a challenging\nalgorithmic problem, since solving it efficiently means to exploit the\nstructure of shortest paths not only in $G$, but also in all the scenarios in\nwhich an edge of $T$ has failed. For this problem we provide a very efficient\nsolution, running in $O(n^2 \\log^4 n)$ time, which drastically improves (almost\nby a quadratic factor in $n$ in dense graphs!) on the previous known best\nresult.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 09:30:06 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Bil\u00f2", "Davide", ""], ["Colella", "Feliciano", ""], ["Gual\u00e0", "Luciano", ""], ["Leucci", "Stefano", ""], ["Proietti", "Guido", ""]]}, {"id": "1710.01620", "submitter": "Wouter Kuijper", "authors": "Wouter Kuijper, Victor Ermolaev, Olivier Devillers", "title": "Celestial Walk: A Terminating Oblivious Walk for Convex Subdivisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new oblivious walking strategy for convex subdivisions. Our walk\nis faster than the straight walk and more generally applicable than the\nvisibility walk. To prove termination of our walk we use a novel monotonically\ndecreasing distance measure.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 14:32:02 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Kuijper", "Wouter", ""], ["Ermolaev", "Victor", ""], ["Devillers", "Olivier", ""]]}, {"id": "1710.01800", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang, Varsha Dani, Thomas P. Hayes, Qizheng He, Wenzheng Li,\n  Seth Pettie", "title": "The Energy Complexity of Broadcast", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy is often the most constrained resource in networks of battery-powered\ndevices, and as devices become smaller, they spend a larger fraction of their\nenergy on communication (transceiver usage) not computation. As an imperfect\nproxy for true energy usage, we define energy complexity to be the number of\ntime slots a device transmits/listens; idle time and computation are free.\n  In this paper we investigate the energy complexity of fundamental\ncommunication primitives such as broadcast in multi-hop radio networks. We\nconsider models with collision detection (CD) and without (No-CD), as well as\nboth randomized and deterministic algorithms. Some take-away messages from this\nwork include:\n  1. The energy complexity of broadcast in a multi-hop network is intimately\nconnected to the time complexity of leader election in a single-hop (clique)\nnetwork. Many existing lower bounds on time complexity immediately transfer to\nenergy complexity. For example, in the CD and No-CD models, we need\n$\\Omega(\\log n)$ and $\\Omega(\\log^2 n)$ energy, respectively.\n  2. The energy lower bounds above can almost be achieved, given sufficient\n($\\Omega(n)$) time. In the CD and No-CD models we can solve broadcast using\n$O(\\frac{\\log n\\log\\log n}{\\log\\log\\log n})$ energy and $O(\\log^3 n)$ energy,\nrespectively.\n  3. The complexity measures of Energy and Time are in conflict, and it is an\nopen problem whether both can be minimized simultaneously. We give a tradeoff\nshowing it is possible to be nearly optimal in both measures simultaneously.\nFor any constant $\\epsilon>0$, broadcast can be solved in\n$O(D^{1+\\epsilon}\\log^{O(1/\\epsilon)} n)$ time with $O(\\log^{O(1/\\epsilon)} n)$\nenergy, where $D$ is the diameter of the network.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 20:54:42 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Dani", "Varsha", ""], ["Hayes", "Thomas P.", ""], ["He", "Qizheng", ""], ["Li", "Wenzheng", ""], ["Pettie", "Seth", ""]]}, {"id": "1710.01896", "submitter": "Florian Kurpicz", "authors": "Johannes Fischer, Florian Kurpicz", "title": "Dismantling DivSufSort", "comments": "Presented at the Prague Stringology Conference 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first concise description of the fastest known suffix sorting\nalgorithm in main memory, the DivSufSort by Yuta Mori. We then present an\nextension that also computes the LCP-array, which is competitive with the\nfastest known LCP-array construction algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 06:48:13 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Fischer", "Johannes", ""], ["Kurpicz", "Florian", ""]]}, {"id": "1710.01952", "submitter": "Adri\\'an G\\'omez-Brand\\'on", "authors": "Nieves R. Brisaboa, Travis Gagie, Adri\\'an G\\'omez-Brand\\'on, Gonzalo\n  Navarro, Jos\\'e R. Param\\'a", "title": "Efficient Compression and Indexing of Trajectories", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "String Processing and Information Retrieval: 24th International\n  Symposium, SPIRE 2017, Palermo, Italy, September 26-29, 2017, Proceedings.\n  Springer International Publishing. pp 103-115. ISBN: 9783319674278", "doi": "10.1007/978-3-319-67428-5_10", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new compressed representation of free trajectories of moving\nobjects. It combines a partial-sums-based structure that retrieves in constant\ntime the position of the object at any instant, with a hierarchical\nminimum-bounding-boxes representation that allows determining if the object is\nseen in a certain rectangular area during a time period. Combined with spatial\nsnapshots at regular intervals, the representation is shown to outperform\nclassical ones by orders of magnitude in space, and also to outperform previous\ncompressed representations in time performance, when using the same amount of\nspace.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 10:30:27 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Gagie", "Travis", ""], ["G\u00f3mez-Brand\u00f3n", "Adri\u00e1n", ""], ["Navarro", "Gonzalo", ""], ["Param\u00e1", "Jos\u00e9 R.", ""]]}, {"id": "1710.01965", "submitter": "Paolo Giulio Franciosa", "authors": "Giorgio Ausiello, Paolo Giulio Franciosa, Isabella Lari and Andrea\n  Ribichini", "title": "Max flow vitality in general and $st$-planar graphs", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\emph{vitality} of an arc/node of a graph with respect to the maximum\nflow between two fixed nodes $s$ and $t$ is defined as the reduction of the\nmaximum flow caused by the removal of that arc/node. In this paper we address\nthe issue of determining the vitality of arcs and/or nodes for the maximum flow\nproblem. We show how to compute the vitality of all arcs in a general\nundirected graph by solving only $2(n-1)$ max flow instances and, In\n$st$-planar graphs (directed or undirected) we show how to compute the vitality\nof all arcs and all nodes in $O(n)$ worst-case time. Moreover, after\ndetermining the vitality of arcs and/or nodes, and given a planar embedding of\nthe graph, we can determine the vitality of a `contiguous' set of arcs/nodes in\ntime proportional to the size of the set.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 11:12:33 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 21:08:13 GMT"}, {"version": "v3", "created": "Tue, 17 Jul 2018 16:30:53 GMT"}, {"version": "v4", "created": "Thu, 20 Dec 2018 15:20:17 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Ausiello", "Giorgio", ""], ["Franciosa", "Paolo Giulio", ""], ["Lari", "Isabella", ""], ["Ribichini", "Andrea", ""]]}, {"id": "1710.01968", "submitter": "Sebastian Schlag", "authors": "Robin Andre, Sebastian Schlag and Christian Schulz", "title": "Memetic Multilevel Hypergraph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraph partitioning has a wide range of important applications such as\nVLSI design or scientific computing. With focus on solution quality, we develop\nthe first multilevel memetic algorithm to tackle the problem. Key components of\nour contribution are new effective multilevel recombination and mutation\noperations that provide a large amount of diversity. We perform a wide range of\nexperiments on a benchmark set containing instances from application areas such\nVLSI, SAT solving, social networks, and scientific computing. Compared to the\nstate-of-the-art hypergraph partitioning tools hMetis, PaToH, and KaHyPar, our\nnew algorithm computes the best result on almost all instances.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 11:20:45 GMT"}, {"version": "v2", "created": "Sat, 3 Feb 2018 12:37:55 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Andre", "Robin", ""], ["Schlag", "Sebastian", ""], ["Schulz", "Christian", ""]]}, {"id": "1710.01985", "submitter": "Jacques Dark", "authors": "Graham Cormode and Jacques Dark", "title": "Fast Sketch-based Recovery of Correlation Outliers", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data sources can be interpreted as time-series, and a key problem is to\nidentify which pairs out of a large collection of signals are highly\ncorrelated. We expect that there will be few, large, interesting correlations,\nwhile most signal pairs do not have any strong correlation. We abstract this as\nthe problem of identifying the highly correlated pairs in a collection of n\nmostly pairwise uncorrelated random variables, where observations of the\nvariables arrives as a stream. Dimensionality reduction can remove dependence\non the number of observations, but further techniques are required to tame the\nquadratic (in n) cost of a search through all possible pairs.\n  We develop a new algorithm for rapidly finding large correlations based on\nsketch techniques with an added twist: we quickly generate sketches of random\ncombinations of signals, and use these in concert with ideas from coding theory\nto decode the identity of correlated pairs. We prove correctness and compare\nperformance and effectiveness with the best LSH (locality sensitive hashing)\nbased approach.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 12:38:47 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Cormode", "Graham", ""], ["Dark", "Jacques", ""]]}, {"id": "1710.02058", "submitter": "Victor Verdugo", "authors": "Beno\\^it Groz, Frederik Mallmann-Trenn, Claire Mathieu, and Victor\n  Verdugo", "title": "Skyline Computation with Noisy Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of $n$ points in a $d$-dimensional space, we seek to compute the\nskyline, i.e., those points that are not strictly dominated by any other point,\nusing few comparisons between elements. We adopt the noisy comparison model\n[FRPU94] where comparisons fail with constant probability and confidence can be\nincreased through independent repetitions of a comparison. In this model\nmotivated by Crowdsourcing applications, Groz & Milo [GM15] show three bounds\non the query complexity for the skyline problem. We improve significantly on\nthat state of the art and provide two output-sensitive algorithms computing the\nskyline with respective query complexity $O(nd\\log (dk/\\delta))$ and $O(ndk\\log\n(k/\\delta))$ where $k$ is the size of the skyline and $\\delta$ the expected\nprobability that our algorithm fails to return the correct answer. These\nresults are tight for low dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 14:59:14 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 17:20:11 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Groz", "Beno\u00eet", ""], ["Mallmann-Trenn", "Frederik", ""], ["Mathieu", "Claire", ""], ["Verdugo", "Victor", ""]]}, {"id": "1710.02108", "submitter": "Lorenzo De Stefani", "authors": "Lorenzo De Stefani, Erisa Terolli and Eli Upfal", "title": "Tiered Sampling: An Efficient Method for Approximate Counting Sparse\n  Motifs in Massive Graph Streams", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Tiered Sampling, a novel technique for approximate counting\nsparse motifs in massive graphs whose edges are observed in a stream. Our\ntechnique requires only a single pass on the data and uses a memory of fixed\nsize $M$, which can be magnitudes smaller than the number of edges.\n  Our methods addresses the challenging task of counting sparse motifs -\nsub-graph patterns that have low probability to appear in a sample of $M$ edges\nin the graph, which is the maximum amount of data available to the algorithms\nin each step. To obtain an unbiased and low variance estimate of the count we\npartition the available memory to tiers (layers) of reservoir samples. While\nthe base layer is a standard reservoir sample of edges, other layers are\nreservoir samples of sub-structures of the desired motif. By storing more\nfrequent sub-structures of the motif, we increase the probability of detecting\nan occurrence of the sparse motif we are counting, thus decreasing the variance\nand error of the estimate.\n  We demonstrate the advantage of our method in the specific applications of\ncounting sparse 4 and 5-cliques in massive graphs. We present a complete\nanalytical analysis and extensive experimental results using both synthetic and\nreal-world data. Our results demonstrate the advantage of our method in\nobtaining high-quality approximations for the number of 4 and 5-cliques for\nlarge graphs using a very limited amount of memory, significantly outperforming\nthe single edge sample approach for counting sparse motifs in large scale\ngraphs.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 16:46:59 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["De Stefani", "Lorenzo", ""], ["Terolli", "Erisa", ""], ["Upfal", "Eli", ""]]}, {"id": "1710.02141", "submitter": "Qilian Yu", "authors": "Qilian Yu, Hang Li, Yun Liao, Shuguang Cui", "title": "Fast Budgeted Influence Maximization over Multi-Action Event Logs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a social network, influence maximization is the problem of identifying a\nset of users that own the maximum {\\it influence ability} across the network.\nIn this paper, a novel credit distribution (CD) based model, termed as the\nmulti-action CD (mCD) model, is introduced to quantify the influence ability of\neach user, which works with practical datasets where one type of action could\nbe recorded for multiple times. Based on this model, influence maximization is\nformulated as a submodular maximization problem under a general knapsack\nconstraint, which is NP-hard. An efficient streaming algorithm with one-round\nscan over the user set is developed to find a suboptimal solution.\nSpecifically, we first solve a special case of knapsack constraints, i.e., a\ncardinality constraint, and show that the developed streaming algorithm can\nachieve ($\\frac{1}{2}-\\epsilon$)-approximation of the optimality. Furthermore,\nfor the general knapsack case, we show that the modified streaming algorithm\ncan achieve ($\\frac{1}{3}-\\epsilon$)-approximation of the optimality. Finally,\nexperiments are conducted over real Twitter dataset and demonstrate that the\nmCD model enjoys high accuracy compared to the conventional CD model in\nestimating the total number of people who get influenced in a social network.\nMoreover, through the comparison to the conventional CD, non-CD models, and the\nmCD model with the greedy algorithm on the performance of the influence\nmaximization problem, we show the effectiveness and efficiency of the proposed\nmCD model with the streaming algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 05:59:48 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 21:56:35 GMT"}, {"version": "v3", "created": "Fri, 26 Jan 2018 06:58:18 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Yu", "Qilian", ""], ["Li", "Hang", ""], ["Liao", "Yun", ""], ["Cui", "Shuguang", ""]]}, {"id": "1710.02226", "submitter": "Patrick Brosi", "authors": "Hannah Bast (1), Patrick Brosi (1), Sabine Storandt (2) ((1)\n  University of Freiburg, (2) JMU W\\\"urzburg)", "title": "Efficient Generation of Geographically Accurate Transit Maps", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LOOM (Line-Ordering Optimized Maps), a fully automatic generator\nof geographically accurate transit maps. The input to LOOM is data about the\nlines of a given transit network, namely for each line, the sequence of\nstations it serves and the geographical course the vehicles of this line take.\nWe parse this data from GTFS, the prevailing standard for public transit data.\nLOOM proceeds in three stages: (1) construct a so-called line graph, where\nedges correspond to segments of the network with the same set of lines\nfollowing the same course; (2) construct an ILP that yields a line ordering for\neach edge which minimizes the total number of line crossings and line\nseparations; (3) based on the line graph and the ILP solution, draw the map. As\na naive ILP formulation is too demanding, we derive a new custom-tailored\nformulation which requires significantly fewer constraints. Furthermore, we\npresent engineering techniques which use structural properties of the line\ngraph to further reduce the ILP size. For the subway network of New York, we\ncan reduce the number of constraints from 229,000 in the naive ILP formulation\nto about 4,500 with our techniques, enabling solution times of less than a\nsecond. Since our maps respect the geography of the transit network, they can\nbe used for tiles and overlays in typical map services. Previous research work\neither did not take the geographical course of the lines into account, or was\nconcerned with schematic maps without optimizing line crossings or line\nseparations.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 21:43:57 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Bast", "Hannah", ""], ["Brosi", "Patrick", ""], ["Storandt", "Sabine", ""]]}, {"id": "1710.02300", "submitter": "Petr Golovach", "authors": "Fedor V. Fomin, Petr A. Golovach, Daniel Lokshtanov, Saket Saurabh", "title": "Covering vectors by spaces: Regular matroids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seymour's decomposition theorem for regular matroids is a fundamental result\nwith a number of combinatorial and algorithmic applications. In this work we\ndemonstrate how this theorem can be used in the design of parameterized\nalgorithms on regular matroids. We consider the problem of covering a set of\nvectors of a given finite dimensional linear space (vector space) by a subspace\ngenerated by a set of vectors of minimum size. Specifically, in the Space Cover\nproblem, we are given a matrix M and a subset of its columns T; the task is to\nfind a minimum set F of columns of M disjoint with T such that that the linear\nspan of F contains all vectors of T. For graphic matroids this problem is\nessentially Stainer Forest and for cographic matroids this is a generalization\nof Multiway Cut. Our main result is the algorithm with running time\n2^{O(k)}||M|| ^{O(1)} solving Space Cover in the case when M is a totally\nunimodular matrix over rationals, where k is the size of F. In other words, we\nshow that on regular matroids the problem is fixed-parameter tractable\nparameterized by the rank of the covering subspace.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 08:02:35 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Lokshtanov", "Daniel", ""], ["Saurabh", "Saket", ""]]}, {"id": "1710.02581", "submitter": "Tongyang Li", "authors": "Fernando G. S. L. Brand\\~ao, Amir Kalev, Tongyang Li, Cedric Yen-Yu\n  Lin, Krysta M. Svore, Xiaodi Wu", "title": "Quantum SDP Solvers: Large Speed-ups, Optimality, and Applications to\n  Quantum Learning", "comments": "40 pages. To appear at the 46th International Colloquium on Automata,\n  Languages and Programming (ICALP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give two quantum algorithms for solving semidefinite programs (SDPs)\nproviding quantum speed-ups. We consider SDP instances with $m$ constraint\nmatrices, each of dimension $n$, rank at most $r$, and sparsity $s$. The first\nalgorithm assumes access to an oracle to the matrices at unit cost. We show\nthat it has run time\n$\\tilde{O}(s^2(\\sqrt{m}\\epsilon^{-10}+\\sqrt{n}\\epsilon^{-12}))$, with\n$\\epsilon$ the error of the solution. This gives an optimal dependence in terms\nof $m, n$ and quadratic improvement over previous quantum algorithms when\n$m\\approx n$. The second algorithm assumes a fully quantum input model in which\nthe matrices are given as quantum states. We show that its run time is\n$\\tilde{O}(\\sqrt{m}+\\text{poly}(r))\\cdot\\text{poly}(\\log m,\\log\nn,B,\\epsilon^{-1})$, with $B$ an upper bound on the trace-norm of all input\nmatrices. In particular the complexity depends only poly-logarithmically in $n$\nand polynomially in $r$.\n  We apply the second SDP solver to learn a good description of a quantum state\nwith respect to a set of measurements: Given $m$ measurements and a supply of\ncopies of an unknown state $\\rho$ with rank at most $r$, we show we can find in\ntime $\\sqrt{m}\\cdot\\text{poly}(\\log m,\\log n,r,\\epsilon^{-1})$ a description of\nthe state as a quantum circuit preparing a density matrix which has the same\nexpectation values as $\\rho$ on the $m$ measurements, up to error $\\epsilon$.\nThe density matrix obtained is an approximation to the maximum entropy state\nconsistent with the measurement data considered in Jaynes' principle from\nstatistical mechanics.\n  As in previous work, we obtain our algorithm by \"quantizing\" classical SDP\nsolvers based on the matrix multiplicative weight method. One of our main\ntechnical contributions is a quantum Gibbs state sampler for low-rank\nHamiltonians with a poly-logarithmic dependence on its dimension, which could\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 20:43:14 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 00:25:13 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 21:40:35 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Brand\u00e3o", "Fernando G. S. L.", ""], ["Kalev", "Amir", ""], ["Li", "Tongyang", ""], ["Lin", "Cedric Yen-Yu", ""], ["Svore", "Krysta M.", ""], ["Wu", "Xiaodi", ""]]}, {"id": "1710.02587", "submitter": "Tsz Chiu Kwok", "authors": "Tsz Chiu Kwok, Lap Chi Lau, Yin Tat Lee and Akshay Ramachandran", "title": "The Paulsen Problem, Continuous Operator Scaling, and Smoothed Analysis", "comments": "Added Subsection 1.4; Incorporated comments and fixed typos; Minor\n  changes in various places", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.FA math.OA math.OC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Paulsen problem is a basic open problem in operator theory: Given vectors\n$u_1, \\ldots, u_n \\in \\mathbb R^d$ that are $\\epsilon$-nearly satisfying the\nParseval's condition and the equal norm condition, is it close to a set of\nvectors $v_1, \\ldots, v_n \\in \\mathbb R^d$ that exactly satisfy the Parseval's\ncondition and the equal norm condition? Given $u_1, \\ldots, u_n$, the squared\ndistance (to the set of exact solutions) is defined as $\\inf_{v} \\sum_{i=1}^n\n\\| u_i - v_i \\|_2^2$ where the infimum is over the set of exact solutions.\nPrevious results show that the squared distance of any $\\epsilon$-nearly\nsolution is at most $O({\\rm{poly}}(d,n,\\epsilon))$ and there are\n$\\epsilon$-nearly solutions with squared distance at least $\\Omega(d\\epsilon)$.\nThe fundamental open question is whether the squared distance can be\nindependent of the number of vectors $n$.\n  We answer this question affirmatively by proving that the squared distance of\nany $\\epsilon$-nearly solution is $O(d^{13/2} \\epsilon)$. Our approach is based\non a continuous version of the operator scaling algorithm and consists of two\nparts. First, we define a dynamical system based on operator scaling and use it\nto prove that the squared distance of any $\\epsilon$-nearly solution is $O(d^2\nn \\epsilon)$. Then, we show that by randomly perturbing the input vectors, the\ndynamical system will converge faster and the squared distance of an\n$\\epsilon$-nearly solution is $O(d^{5/2} \\epsilon)$ when $n$ is large enough\nand $\\epsilon$ is small enough. To analyze the convergence of the dynamical\nsystem, we develop some new techniques in lower bounding the operator capacity,\na concept introduced by Gurvits to analyze the operator scaling algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 21:09:34 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 19:16:15 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Kwok", "Tsz Chiu", ""], ["Lau", "Lap Chi", ""], ["Lee", "Yin Tat", ""], ["Ramachandran", "Akshay", ""]]}, {"id": "1710.02608", "submitter": "Jamie Haddock", "authors": "Jesus De Loera, Jamie Haddock, Luis Rademacher", "title": "The Minimum Euclidean-Norm Point on a Convex Polytope: Wolfe's\n  Combinatorial Algorithm is Exponential", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of Philip Wolfe's method for the minimum Euclidean-norm point\nproblem over a convex polytope has remained unknown since he proposed the\nmethod in 1974. The method is important because it is used as a subroutine for\none of the most practical algorithms for submodular function minimization. We\npresent the first example that Wolfe's method takes exponential time.\nAdditionally, we improve previous results to show that linear programming\nreduces in strongly-polynomial time to the minimum norm point problem over a\nsimplex.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 23:44:43 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 20:12:39 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["De Loera", "Jesus", ""], ["Haddock", "Jamie", ""], ["Rademacher", "Luis", ""]]}, {"id": "1710.02637", "submitter": "Yan Gu", "authors": "Naama Ben-David, Guy E. Blelloch, Jeremy T. Fineman, Phillip B.\n  Gibbons, Yan Gu, Charles McGuffey, Julian Shun", "title": "Implicit Decomposition for Write-Efficient Connectivity Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The future of main memory appears to lie in the direction of new technologies\nthat provide strong capacity-to-performance ratios, but have write operations\nthat are much more expensive than reads in terms of latency, bandwidth, and\nenergy. Motivated by this trend, we propose sequential and parallel algorithms\nto solve graph connectivity problems using significantly fewer writes than\nconventional algorithms. Our primary algorithmic tool is the construction of an\n$o(n)$-sized \"implicit decomposition\" of a bounded-degree graph $G$ on $n$\nnodes, which combined with read-only access to $G$ enables fast answers to\nconnectivity and biconnectivity queries on $G$. The construction breaks the\nlinear-write \"barrier\", resulting in costs that are asymptotically lower than\nconventional algorithms while adding only a modest cost to querying time. For\ngeneral non-sparse graphs on $m$ edges, we also provide the first $o(m)$ writes\nand $O(m)$ operations parallel algorithms for connectivity and biconnectivity.\nThese algorithms provide insight into how applications can efficiently process\ncomputations on large graphs in systems with read-write asymmetry.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 05:33:19 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Ben-David", "Naama", ""], ["Blelloch", "Guy E.", ""], ["Fineman", "Jeremy T.", ""], ["Gibbons", "Phillip B.", ""], ["Gu", "Yan", ""], ["McGuffey", "Charles", ""], ["Shun", "Julian", ""]]}, {"id": "1710.02690", "submitter": "Rebecca Steorts", "authors": "Beidi Chen, Anshumali Shrivastava, Rebecca C. Steorts", "title": "Unique Entity Estimation with Application to the Syrian Conflict", "comments": "35 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution identifies and removes duplicate entities in large, noisy\ndatabases and has grown in both usage and new developments as a result of\nincreased data availability. Nevertheless, entity resolution has tradeoffs\nregarding assumptions of the data generation process, error rates, and\ncomputational scalability that make it a difficult task for real applications.\nIn this paper, we focus on a related problem of unique entity estimation, which\nis the task of estimating the unique number of entities and associated standard\nerrors in a data set with duplicate entities. Unique entity estimation shares\nmany fundamental challenges of entity resolution, namely, that the\ncomputational cost of all-to-all entity comparisons is intractable for large\ndatabases. To circumvent this computational barrier, we propose an efficient\n(near-linear time) estimation algorithm based on locality sensitive hashing.\nOur estimator, under realistic assumptions, is unbiased and has provably low\nvariance compared to existing random sampling based approaches. In addition, we\nempirically show its superiority over the state-of-the-art estimators on three\nreal applications. The motivation for our work is to derive an accurate\nestimate of the documented, identifiable deaths in the ongoing Syrian conflict.\nOur methodology, when applied to the Syrian data set, provides an estimate of\n$191,874 \\pm 1772$ documented, identifiable deaths, which is very close to the\nHuman Rights Data Analysis Group (HRDAG) estimate of 191,369. Our work provides\nan example of challenges and efforts involved in solving a real, noisy\nchallenging problem where modeling assumptions may not hold.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 14:30:20 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Chen", "Beidi", ""], ["Shrivastava", "Anshumali", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1710.02736", "submitter": "Holden Lee", "authors": "Rong Ge, Holden Lee, Andrej Risteski", "title": "Beyond Log-concavity: Provable Guarantees for Sampling Multi-modal\n  Distributions using Simulated Tempering Langevin Monte Carlo", "comments": "53 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key task in Bayesian statistics is sampling from distributions that are\nonly specified up to a partition function (i.e., constant of proportionality).\nHowever, without any assumptions, sampling (even approximately) can be #P-hard,\nand few works have provided \"beyond worst-case\" guarantees for such settings.\n  For log-concave distributions, classical results going back to Bakry and\n\\'Emery (1985) show that natural continuous-time Markov chains called Langevin\ndiffusions mix in polynomial time. The most salient feature of log-concavity\nviolated in practice is uni-modality: commonly, the distributions we wish to\nsample from are multi-modal. In the presence of multiple deep and\nwell-separated modes, Langevin diffusion suffers from torpid mixing.\n  We address this problem by combining Langevin diffusion with simulated\ntempering. The result is a Markov chain that mixes more rapidly by\ntransitioning between different temperatures of the distribution. We analyze\nthis Markov chain for the canonical multi-modal distribution: a mixture of\ngaussians (of equal variance). The algorithm based on our Markov chain provably\nsamples from distributions that are close to mixtures of gaussians, given\naccess to the gradient of the log-pdf. For the analysis, we use a spectral\ndecomposition theorem for graphs (Gharan and Trevisan, 2014) and a Markov chain\ndecomposition technique (Madras and Randall, 2002).\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 19:55:51 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 01:49:53 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Ge", "Rong", ""], ["Lee", "Holden", ""], ["Risteski", "Andrej", ""]]}, {"id": "1710.02827", "submitter": "Biaoshuai Tao", "authors": "Grant Schoenebeck and Biaoshuai Tao", "title": "Beyond Worst-Case (In)approximability of Nonsubmodular Influence\n  Maximization", "comments": "53 pages, 20 figures; Conference short version - WINE 2017: The 13th\n  Conference on Web and Internet Economics; Journal full version - ACM:\n  Transactions on Computation Theory, 2019", "journal-ref": "ACM Transactions on Computation Theory (April 2019)", "doi": "10.1145/3313904", "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing the spread of influence in a social\nnetwork by choosing a fixed number of initial seeds, formally referred to as\nthe influence maximization problem. It admits a $(1-1/e)$-factor approximation\nalgorithm if the influence function is submodular. Otherwise, in the worst\ncase, the problem is NP-hard to approximate to within a factor of\n$N^{1-\\varepsilon}$. This paper studies whether this worst-case hardness result\ncan be circumvented by making assumptions about either the underlying network\ntopology or the cascade model. All of our assumptions are motivated by many\nreal life social network cascades.\n  First, we present strong inapproximability results for a very restricted\nclass of networks called the (stochastic) hierarchical blockmodel, a special\ncase of the well-studied (stochastic) blockmodel in which relationships between\nblocks admit a tree structure. We also provide a dynamic-program based\npolynomial time algorithm which optimally computes a directed variant of the\ninfluence maximization problem on hierarchical blockmodel networks. Our\nalgorithm indicates that the inapproximability result is due to the\nbidirectionality of influence between agent-blocks.\n  Second, we present strong inapproximability results for a class of influence\nfunctions that are \"almost\" submodular, called 2-quasi-submodular. Our\ninapproximability results hold even for any 2-quasi-submodular $f$ fixed in\nadvance. This result also indicates that the \"threshold\" between submodularity\nand nonsubmodularity is sharp, regarding the approximability of influence\nmaximization.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 12:21:29 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 17:50:39 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Schoenebeck", "Grant", ""], ["Tao", "Biaoshuai", ""]]}, {"id": "1710.02901", "submitter": "Amir Ali Ahmadi", "authors": "Amir Ali Ahmadi, Anirudha Majumdar", "title": "Response to \"Counterexample to global convergence of DSOS and SDSOS\n  hierarchies\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent note [8], the author provides a counterexample to the global\nconvergence of what his work refers to as \"the DSOS and SDSOS hierarchies\" for\npolynomial optimization problems (POPs) and purports that this refutes claims\nin our extended abstract [4] and slides in [3]. The goal of this paper is to\nclarify that neither [4], nor [3], and certainly not our full paper [5], ever\ndefined DSOS or SDSOS hierarchies as it is done in [8]. It goes without saying\nthat no claims about convergence properties of the hierarchies in [8] were ever\nmade as a consequence. What was stated in [4,3] was completely different: we\nstated that there exist hierarchies based on DSOS and SDSOS optimization that\nconverge. This is indeed true as we discuss in this response. We also emphasize\nthat we were well aware that some (S)DSOS hierarchies do not converge even if\ntheir natural SOS counterparts do. This is readily implied by an example in our\nprior work [5], which makes the counterexample in [8] superfluous. Finally, we\nprovide concrete counterarguments to claims made in [8] that aim to challenge\nthe scalability improvements obtained by DSOS and SDSOS optimization as\ncompared to sum of squares (SOS) optimization.\n  [3] A. A. Ahmadi and A. Majumdar. DSOS and SDSOS: More tractable alternatives\nto SOS. Slides at the meeting on Geometry and Algebra of Linear Matrix\nInequalities, CIRM, Marseille, 2013. [4] A. A. Ahmadi and A. Majumdar. DSOS and\nSDSOS optimization: LP and SOCP-based alternatives to sum of squares\noptimization. In proceedings of the 48th annual IEEE Conference on Information\nSciences and Systems, 2014. [5] A. A. Ahmadi and A. Majumdar. DSOS and SDSOS\noptimization: more tractable alternatives to sum of squares and semidefinite\noptimization. arXiv:1706.02586, 2017. [8] C. Josz. Counterexample to global\nconvergence of DSOS and SDSOS hierarchies. arXiv:1707.02964, 2017.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 00:22:00 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Ahmadi", "Amir Ali", ""], ["Majumdar", "Anirudha", ""]]}, {"id": "1710.03091", "submitter": "Young-San Lin", "authors": "Young-San Lin and Thanh Nguyen", "title": "On Variants of Network Flow Stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a general stable flow problem in a directed and capacitated\nnetwork, where each vertex has a strict preference list over the incoming and\noutgoing edges. A flow is stable if no group of vertices forming a path can\nmutually benefit by rerouting the flow. Motivated by applications in supply\nchain networks, we generalize the traditional Kirchhoff's law, requiring the\noutflow is equal to the inflow at every nonterminal node, to a monotone\npiecewise linear relationship between the inflows and the outflows. We show the\nexistence of a stable flow using Scarf's Lemma, and provide a polynomial time\nalgorithm to find such a stable flow. We further show that finding a minimum\ncost generalized stable network is NP-hard, while the problem is polynomial\ntime solvable for the traditional stable flow satisfying Kirchhoff's law.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 13:47:46 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 14:08:39 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Lin", "Young-San", ""], ["Nguyen", "Thanh", ""]]}, {"id": "1710.03148", "submitter": "Stanislav Zivny", "authors": "Clement Carbonnel, Miguel Romero, Stanislav Zivny", "title": "The complexity of general-valued CSPs seen from the other side", "comments": "v2: Full version of a FOCS'18 paper; improved presentation and small\n  corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constraint satisfaction problem (CSP) is concerned with homomorphisms\nbetween two structures. For CSPs with restricted left-hand side structures, the\nresults of Dalmau, Kolaitis, and Vardi [CP'02], Grohe [FOCS'03/JACM'07], and\nAtserias, Bulatov, and Dalmau [ICALP'07] establish the precise borderline of\npolynomial-time solvability (subject to complexity-theoretic assumptions) and\nof solvability by bounded-consistency algorithms (unconditionally) as bounded\ntreewidth modulo homomorphic equivalence.\n  The general-valued constraint satisfaction problem (VCSP) is a generalisation\nof the CSP concerned with homomorphisms between two valued structures. For\nVCSPs with restricted left-hand side valued structures, we establish the\nprecise borderline of polynomial-time solvability (subject to\ncomplexity-theoretic assumptions) and of solvability by the $k$-th level of the\nSherali-Adams LP hierarchy (unconditionally). We also obtain results on related\nproblems concerned with finding a solution and recognising the tractable cases;\nthe latter has an application in database theory.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 15:32:14 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 16:31:28 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Carbonnel", "Clement", ""], ["Romero", "Miguel", ""], ["Zivny", "Stanislav", ""]]}, {"id": "1710.03155", "submitter": "Ran Ben Basat", "authors": "Ran Ben Basat and Gil Einziger and Roy Friedman", "title": "Fast Flow Volume Estimation", "comments": "To appear in ACM ICDCN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of jumbo frames means growing variance in the size\nof packets transmitted in modern networks. Consequently, network monitoring\ntools must maintain explicit traffic volume statistics rather than settle for\npacket counting as before. We present constant time algorithms for volume\nestimations in streams and sliding windows, which are faster than previous\nwork. Our solutions are formally analyzed and are extensively evaluated over\nmultiple real-world packet traces as well as synthetic ones. For streams, we\ndemonstrate a run-time improvement of up to 2.4X compared to the state of the\nart. On sliding windows, we exhibit a memory reduction of over 100X on all\ntraces and an asymptotic runtime improvement to a constant. Finally, we apply\nour approach to hierarchical heavy hitters and achieve an empirical 2.4-7X\nspeedup.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 15:42:58 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 08:39:42 GMT"}, {"version": "v3", "created": "Sun, 15 Oct 2017 05:12:59 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Basat", "Ran Ben", ""], ["Einziger", "Gil", ""], ["Friedman", "Roy", ""]]}, {"id": "1710.03164", "submitter": "Greg Bodwin", "authors": "Greg Bodwin, Michael Dinitz, Merav Parter, Virginia Vassilevska\n  Williams", "title": "Optimal Vertex Fault Tolerant Spanners (for fixed stretch)", "comments": "To appear in SODA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $k$-spanner of a graph $G$ is a sparse subgraph $H$ whose shortest path\ndistances match those of $G$ up to a multiplicative error $k$. In this paper we\nstudy spanners that are resistant to faults. A subgraph $H \\subseteq G$ is an\n$f$ vertex fault tolerant (VFT) $k$-spanner if $H \\setminus F$ is a $k$-spanner\nof $G \\setminus F$ for any small set $F$ of $f$ vertices that might \"fail.\" One\nof the main questions in the area is: what is the minimum size of an $f$ fault\ntolerant $k$-spanner that holds for all $n$ node graphs (as a function of $f$,\n$k$ and $n$)? This question was first studied in the context of geometric\ngraphs [Levcopoulos et al. STOC '98, Czumaj and Zhao SoCG '03] and has more\nrecently been considered in general undirected graphs [Chechik et al. STOC '09,\nDinitz and Krauthgamer PODC '11].\n  In this paper, we settle the question of the optimal size of a VFT spanner,\nin the setting where the stretch factor $k$ is fixed. Specifically, we prove\nthat every (undirected, possibly weighted) $n$-node graph $G$ has a\n$(2k-1)$-spanner resilient to $f$ vertex faults with $O_k(f^{1 - 1/k} n^{1 +\n1/k})$ edges, and this is fully optimal (unless the famous Erdos Girth\nConjecture is false). Our lower bound even generalizes to imply that no data\nstructure capable of approximating $dist_{G \\setminus F}(s, t)$ similarly can\nbeat the space usage of our spanner in the worst case. We also consider the\nedge fault tolerant (EFT) model, defined analogously with edge failures rather\nthan vertex failures. We show that the same spanner upper bound applies in this\nsetting. Our data structure lower bound extends to the case $k=2$ (and hence we\nclose the EFT problem for $3$-approximations), but it falls to $\\Omega(f^{1/2 -\n1/(2k)} \\cdot n^{1 + 1/k})$ for $k \\ge 3$. We leave it as an open problem to\nclose this gap.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 16:03:49 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Bodwin", "Greg", ""], ["Dinitz", "Michael", ""], ["Parter", "Merav", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "1710.03358", "submitter": "Philip Klein", "authors": "Vincent Cohen-Addad and Philip N. Klein and Neal E. Young", "title": "Balanced power diagrams for redistricting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for redistricting, decomposing a geographical area into\nsubareas, called districts, so that the populations of the districts are as\nclose as possible and the districts are compact and contiguous. Each district\nis the intersection of a polygon with the geographical area. The polygons are\nconvex and the average number of sides per polygon is less than six. The\npolygons tend to be quite compact. With each polygon is associated a center.\nThe center is the centroid of the locations of the residents associated with\nthe polygon. The algorithm can be viewed as a heuristic for finding centers and\na balanced assignment of residents to centers so as to minimize the sum of\nsquared distances of residents to centers; hence the solution can be said to\nhave low dispersion.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 00:08:08 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 14:53:42 GMT"}, {"version": "v3", "created": "Sun, 7 Jan 2018 16:11:00 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Klein", "Philip N.", ""], ["Young", "Neal E.", ""]]}, {"id": "1710.03395", "submitter": "Diptarama Hendrian", "authors": "Diptarama Hendrian and Shunsuke Inenaga and Ryo Yoshinaka and Ayumi\n  Shinohara", "title": "Efficient Dynamic Dictionary Matching with DAWGs and AC-automata", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.tcs.2018.04.016", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dictionary matching is a task to find all occurrences of patterns in a\nset $D$ (called a dictionary) on a text $T$. The Aho-Corasick-automaton\n(AC-automaton) is a data structure which enables us to solve the dictionary\nmatching problem in $O(d\\log\\sigma)$ preprocessing time and\n$O(n\\log\\sigma+occ)$ matching time, where $d$ is the total length of the\npatterns in $D$, $n$ is the length of the text, $\\sigma$ is the alphabet size,\nand $occ$ is the total number of occurrences of all the patterns in the text.\nThe dynamic dictionary matching is a variant where patterns may dynamically be\ninserted into and deleted from $D$. This problem is called semi-dynamic\ndictionary matching if only insertions are allowed. In this paper, we propose\ntwo efficient algorithms. For a pattern of length $m$, our first algorithm\nsupports insertions in $O(m\\log\\sigma+\\log d/\\log\\log d)$ time and pattern\nmatching in $O(n\\log\\sigma+occ)$ time for the semi-dynamic setting and supports\nboth insertions and deletions in $O(\\sigma m+\\log d/\\log\\log d)$ time and\npattern matching in $O(n(\\log d/\\log\\log d+\\log\\sigma)+occ(\\log d/\\log\\log d))$\ntime for the dynamic setting by some modifications. This algorithm is based on\nthe directed acyclic word graph. Our second algorithm, which is based on the\nAC-automaton, supports insertions in $O(m\\log \\sigma+u_f+u_o)$ time for the\nsemi-dynamic setting and supports both insertions and deletions in $O(\\sigma\nm+u_f+u_o)$ time for the dynamic setting, where $u_f$ and $u_o$ respectively\ndenote the numbers of states in which the failure function and the output\nfunction need to be updated. This algorithm performs pattern matching in\n$O(n\\log\\sigma+occ)$ time for both settings. Our algorithm achieves optimal\nupdate time for AC-automaton based methods over constant-size alphabets, since\nany algorithm which explicitly maintains the AC-automaton requires\n$\\Omega(m+u_f+u_o)$ update time.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 03:56:33 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 02:14:28 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 00:58:47 GMT"}, {"version": "v4", "created": "Thu, 21 Feb 2019 03:29:53 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Hendrian", "Diptarama", ""], ["Inenaga", "Shunsuke", ""], ["Yoshinaka", "Ryo", ""], ["Shinohara", "Ayumi", ""]]}, {"id": "1710.03435", "submitter": "Martin Skrodzki", "authors": "Martin Skrodzki, Ulrich Reitebuch, Alex McDonough", "title": "Combinatorial and Asymptotical Results on the Neighborhood Grid", "comments": "33 pages, 18 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2009, Joselli et al introduced the Neighborhood Grid data structure for\nfast computation of neighborhood estimates in point clouds. Even though the\ndata structure has been used in several applications and shown to be\npractically relevant, it is theoretically not yet well understood. The purpose\nof this paper is to present a polynomial-time algorithm to build the data\nstructure. Furthermore, it is investigated whether the presented algorithm is\noptimal. This investigations leads to several combinatorial questions for which\npartial results are given. Finally, we present several limits and experiments\nregarding the quality of the obtained neighborhood relation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 07:58:14 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 19:35:09 GMT"}, {"version": "v3", "created": "Wed, 23 Oct 2019 18:22:04 GMT"}, {"version": "v4", "created": "Thu, 1 Oct 2020 03:10:30 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Skrodzki", "Martin", ""], ["Reitebuch", "Ulrich", ""], ["McDonough", "Alex", ""]]}, {"id": "1710.03852", "submitter": "Hongwei Liang", "authors": "Hongwei Liang, Ke Wang", "title": "Top-k Route Search through Submodularity Modeling of Recurrent POI\n  Features", "comments": "11 pages, 7 figures, 2 tables", "journal-ref": "Hongwei Liang and Ke Wang. 2018. Top-k Route Search through\n  Submodularity Modeling of Recurrent POI Features. In The 41st International\n  ACM SIGIR Conference on Research & Development in Information Retrieval\n  (SIGIR '18). ACM, 545-554", "doi": "10.1145/3209978.3210038", "report-no": null, "categories": "cs.SI cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a practical top-k route search problem: given a collection of\npoints of interest (POIs) with rated features and traveling costs between POIs,\na user wants to find k routes from a source to a destination and limited in a\ncost budget, that maximally match her needs on feature preferences. One\nchallenge is dealing with the personalized diversity requirement where users\nhave various trade-off between quantity (the number of POIs with a specified\nfeature) and variety (the coverage of specified features). Another challenge is\nthe large scale of the POI map and the great many alternative routes to search.\nWe model the personalized diversity requirement by the whole class of\nsubmodular functions, and present an optimal solution to the top-k route search\nproblem through indices for retrieving relevant POIs in both feature and route\nspaces and various strategies for pruning the search space using user\npreferences and constraints. We also present promising heuristic solutions and\nevaluate all the solutions on real life data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 23:00:38 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 22:22:18 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Liang", "Hongwei", ""], ["Wang", "Ke", ""]]}, {"id": "1710.04040", "submitter": "Irina Utkina", "authors": "Irina Utkina", "title": "Using modular decomposition technique to solve the maximum clique\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we use the modular decomposition technique for exact solving\nthe weighted maximum clique problem. Our algorithm takes the modular\ndecomposition tree from the paper of Tedder et. al. and finds solution\nrecursively. Also, we propose algorithms to construct graphs with modules. We\nshow some interesting results, comparing our solution with Ostergard's\nalgorithm on DIMACS benchmarks and on generated graphs\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 12:51:06 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Utkina", "Irina", ""]]}, {"id": "1710.04073", "submitter": "Matthieu Latapy", "authors": "Matthieu Latapy, Tiphaine Viard, Cl\\'emence Magnien", "title": "Stream Graphs and Link Streams for the Modeling of Interactions over\n  Time", "comments": "Keywords: stream graphs, link streams, temporal networks,\n  time-varying graphs, dynamic graphs, dynamic networks, longitudinal networks,\n  interactions, time, graphs, networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM cs.DS physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph theory provides a language for studying the structure of relations, and\nit is often used to study interactions over time too. However, it poorly\ncaptures the both temporal and structural nature of interactions, that calls\nfor a dedicated formalism. In this paper, we generalize graph concepts in order\nto cope with both aspects in a consistent way. We start with elementary\nconcepts like density, clusters, or paths, and derive from them more advanced\nconcepts like cliques, degrees, clustering coefficients, or connected\ncomponents. We obtain a language to directly deal with interactions over time,\nsimilar to the language provided by graphs to deal with relations. This\nformalism is self-consistent: usual relations between different concepts are\npreserved. It is also consistent with graph theory: graph concepts are special\ncases of the ones we introduce. This makes it easy to generalize higher-level\nobjects such as quotient graphs, line graphs, k-cores, and centralities. This\npaper also considers discrete versus continuous time assumptions, instantaneous\nlinks, and extensions to more complex cases.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 14:03:40 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Latapy", "Matthieu", ""], ["Viard", "Tiphaine", ""], ["Magnien", "Cl\u00e9mence", ""]]}, {"id": "1710.04091", "submitter": "Jan Schiemann", "authors": "Christian Glazik, Jan Schiemann and Anand Srivastav", "title": "Finding Euler Tours in One Pass in the W-Streaming Model with O(n\n  log(n)) RAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding an Euler tour in an undirected graph G in the\nW-Streaming model with O(n polylog(n)) RAM, where n resp. m is the number of\nnodes resp. edges of G. Our main result is the first one pass W-Streaming\nalgorithm computing an Euler tour of G in the form of an edge successor\nfunction with only O(n log(n)) RAM which is optimal for this setting (e.g., Sun\nand Woodruff (2015)). The previously best-known result in this model is\nimplicitly given by Demetrescu et al. (2010) with the parallel algorithm of\nAtallah and Vishkin (1984) using O(m/n) passes under the same RAM limitation.\nFor graphs with \\omega(n) edges this is non-constant. Our overall approach is\nto partition the edges into edge-disjoint cycles and to merge the cycles until\na single Euler tour is achieved. Note that in the W-Streaming model such a\nmerging is far from being obvious as the limited RAM allows the processing of\nonly a constant number of cycles at once. This enforces us to merge cycles that\npartially are no longer present in RAM. Furthermore, the successor of an edge\ncannot be changed after the edge has left RAM. So, we steadily have to output\nedges and their designated successors, not knowing the appearance of edges and\ncycles yet to come. We solve this problem with a special edge swapping\ntechnique, for which two certain edges per node are sufficient to merge tours\nwithout having all of their edges in RAM. Mathematically, this is controlled by\nstructural results on the space of certain equivalence classes corresponding to\ncycles and the characterization of associated successor functions. For example,\nwe give conditions under which the swapping of edge successors leads to a\nmerging of equivalence classes.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 14:35:53 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Glazik", "Christian", ""], ["Schiemann", "Jan", ""], ["Srivastav", "Anand", ""]]}, {"id": "1710.04101", "submitter": "Oren Salzman", "authors": "Nika Haghtalab, Simon Mackenzie, Ariel D. Procaccia, Oren Salzman and\n  Siddhartha S. Srinivasa", "title": "The Provable Virtue of Laziness in Motion Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lazy Shortest Path (LazySP) class consists of motion-planning algorithms\nthat only evaluate edges along shortest paths between the source and target.\nThese algorithms were designed to minimize the number of edge evaluations in\nsettings where edge evaluation dominates the running time of the algorithm; but\nhow close to optimal are LazySP algorithms in terms of this objective? Our main\nresult is an analytical upper bound, in a probabilistic model, on the number of\nedge evaluations required by LazySP algorithms; a matching lower bound shows\nthat these algorithms are asymptotically optimal in the worst case.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 15:00:47 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Haghtalab", "Nika", ""], ["Mackenzie", "Simon", ""], ["Procaccia", "Ariel D.", ""], ["Salzman", "Oren", ""], ["Srinivasa", "Siddhartha S.", ""]]}, {"id": "1710.04234", "submitter": "Alexandre Drouin", "authors": "Alexandre Drouin, Toby Dylan Hocking, Fran\\c{c}ois Laviolette", "title": "Maximum Margin Interval Trees", "comments": "Accepted for presentation at the 31st Conference on Neural\n  Information Processing Systems (NIPS 2017), Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a regression function using censored or interval-valued output data\nis an important problem in fields such as genomics and medicine. The goal is to\nlearn a real-valued prediction function, and the training output labels\nindicate an interval of possible values. Whereas most existing algorithms for\nthis task are linear models, in this paper we investigate learning nonlinear\ntree models. We propose to learn a tree by minimizing a margin-based\ndiscriminative objective function, and we provide a dynamic programming\nalgorithm for computing the optimal solution in log-linear time. We show\nempirically that this algorithm achieves state-of-the-art speed and prediction\naccuracy in a benchmark of several data sets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 18:02:38 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 16:48:57 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Drouin", "Alexandre", ""], ["Hocking", "Toby Dylan", ""], ["Laviolette", "Fran\u00e7ois", ""]]}, {"id": "1710.04376", "submitter": "Tomoaki Ogasawara", "authors": "Yoichi Iwata, Tomoaki Ogasawara, Naoto Ohsaka", "title": "On the Power of Tree-Depth for Fully Polynomial FPT Algorithms", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many classical problems in P whose time complexities have not been\nimproved over the past decades. Recent studies of \"Hardness in P\" have revealed\nthat, for several of such problems, the current fastest algorithm is the best\npossible under some complexity assumptions. To bypass this difficulty, Fomin et\nal. (SODA 2017) introduced the concept of fully polynomial FPT algorithms. For\na problem with the current best time complexity $O(n^c)$, the goal is to design\nan algorithm running in $k^{O(1)}n^{c'}$ time for a parameter $k$ and a\nconstant $c'<c$. In this paper, we investigate the complexity of graph problems\nin P parameterized by tree-depth, a graph parameter related to tree-width. We\nshow that a simple divide-and-conquer method can solve many graph problems,\nincluding Weighted Matching, Negative Cycle Detection, Minimum Weight Cycle,\nReplacement Paths, and 2-hop Cover, in $O(\\mathrm{td}\\cdot m)$ time or\n$O(\\mathrm{td}\\cdot (m+n\\log n))$ time, where $\\mathrm{td}$ is the tree-depth\nof the input graph. Because any graph of tree-width $\\mathrm{tw}$ has\ntree-depth at most $(\\mathrm{tw}+1)\\log_2 n$, our algorithms also run in\n$O(\\mathrm{tw}\\cdot m\\log n)$ time or $O(\\mathrm{tw}\\cdot (m+n\\log n)\\log n)$\ntime. These results match or improve the previous best algorithms parameterized\nby tree-width. Especially, we solve an open problem of fully polynomial FPT\nalgorithm for Weighted Matching parameterized by tree-width posed by Fomin et\nal.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 05:46:58 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 16:42:58 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Iwata", "Yoichi", ""], ["Ogasawara", "Tomoaki", ""], ["Ohsaka", "Naoto", ""]]}, {"id": "1710.04469", "submitter": "Ali Shoker", "authors": "Carlos Baquero, Paulo Sergio Almeida and Ali Shoker", "title": "Pure Operation-Based Replicated Data Types", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed systems designed to serve clients across the world often make use\nof geo-replication to attain low latency and high availability. Conflict-free\nReplicated Data Types (CRDTs) allow the design of predictable multi-master\nreplication and support eventual consistency of replicas that are allowed to\ntransiently diverge. CRDTs come in two flavors: state-based, where a state is\nchanged locally and shipped and merged into other replicas; operation-based,\nwhere operations are issued locally and reliably causal broadcast to all other\nreplicas. However, the standard definition of op-based CRDTs is very\nencompassing, allowing even sending the full-state, and thus imposing storage\nand dissemination overheads as well as blurring the distinction from\nstate-based CRDTs. We introduce pure op-based CRDTs, that can only send\noperations to other replicas, drawing a clear distinction from state-based\nones. Data types with commutative operations can be trivially implemented as\npure op-based CRDTs using standard reliable causal delivery; whereas data types\nhaving non-commutative operations are implemented using a PO-Log, a partially\nordered log of operations, and making use of an extended API, i.e., a Tagged\nCausal Stable Broadcast (TCSB), that provides extra causality information upon\ndelivery and later informs when delivered messages become causally stable,\nallowing further PO-Log compaction. The framework is illustrated by a catalog\nof pure op-based specifications for classic CRDTs, including counters,\nmulti-value registers, add-wins and remove-wins sets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 12:18:30 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Baquero", "Carlos", ""], ["Almeida", "Paulo Sergio", ""], ["Shoker", "Ali", ""]]}, {"id": "1710.04551", "submitter": "Joost Engelfriet", "authors": "Joost Engelfriet", "title": "The Trees of Hanoi", "comments": "11 pages, slightly revised version of a note from 1981", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The game of the Towers of Hanoi is generalized to binary trees. First, a\nstraightforward solution of the game is discussed. Second, a shorter solution\nis presented, which is then shown to be optimal.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 14:56:56 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Engelfriet", "Joost", ""]]}, {"id": "1710.04576", "submitter": "Martin Wilhelm", "authors": "Martin Wilhelm", "title": "Balancing expression dags for more efficient lazy adaptive evaluation", "comments": "MACIS 2017 Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arithmetic expression dags are widely applied in robust geometric computing.\nIn this paper we restructure expression dags by balancing consecutive additions\nor multiplications. We predict an asymptotic improvement in running time and\nexperimentally confirm the theoretical results. Finally, we discuss some\npitfalls of the approach resulting from changes in evaluation order.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 15:48:45 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Wilhelm", "Martin", ""]]}, {"id": "1710.04640", "submitter": "Marcos Villagra", "authors": "Javier T. Akagi and Carlos F. Gaona and Fabricio Mendoza and Manjil P.\n  Saikia and Marcos Villagra", "title": "Hard and Easy Instances of L-Tromino Tilings", "comments": "Full extended version of LNCS 11355:82-95 (WALCOM 2019)", "journal-ref": "Theoretical Computer Science 2020", "doi": "10.1016/j.tcs.2020.02.025", "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study tilings of regions in the square lattice with L-shaped trominoes.\nDeciding the existence of a tiling with L-trominoes for an arbitrary region in\ngeneral is NP-complete, nonetheless, we identify restrictions to the problem\nwhere it either remains NP-complete or has a polynomial time algorithm. First,\nwe characterize the possibility of when an Aztec rectangle and an Aztec diamond\nhas an L-tromino tiling. Then, we study tilings of arbitrary regions where only\n$180^\\circ$ rotations of L-trominoes are available. For this particular case we\nshow that deciding the existence of a tiling remains NP-complete; yet, if a\nregion does not contains certain so-called \"forbidden polyominoes\" as\nsub-regions, then there exists a polynomial time algorithm for deciding a\ntiling.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 17:52:22 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 14:42:21 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2018 18:09:24 GMT"}, {"version": "v4", "created": "Mon, 24 Feb 2020 17:17:50 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Akagi", "Javier T.", ""], ["Gaona", "Carlos F.", ""], ["Mendoza", "Fabricio", ""], ["Saikia", "Manjil P.", ""], ["Villagra", "Marcos", ""]]}, {"id": "1710.04740", "submitter": "Alfredo Torrico", "authors": "Alfredo Torrico, Mohit Singh, Sebastian Pokutta, Nika Haghtalab,\n  Joseph (Seffi) Naor and Nima Anari", "title": "Structured Robust Submodular Maximization: Offline and Online Algorithms", "comments": "Updated version, with a general class of offline algorithms and\n  experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained submodular function maximization has been used in subset\nselection problems such as selection of most informative sensor locations.\nWhile these models have been quite popular, the solutions Constrained\nsubmodular function maximization has been used in subset selection problems\nsuch as selection of most informative sensor locations. While these models have\nbeen quite popular, the solutions obtained via this approach are unstable to\nperturbations in data defining the submodular functions. Robust submodular\nmaximization has been proposed as a richer model that aims to overcome this\ndiscrepancy as well as increase the modeling scope of submodular optimization.\n  In this work, we consider robust submodular maximization with structured\ncombinatorial constraints and give efficient algorithms with provable\nguarantees. Our approach is applicable to constraints defined by single or\nmultiple matroids, knapsack as well as distributionally robust criteria. We\nconsider both the offline setting where the data defining the problem is known\nin advance as well as the online setting where the input data is revealed over\ntime. For the offline setting, we give a general (nearly) optimal bi-criteria\napproximation algorithm that relies on new extensions of classical algorithms\nfor submodular maximization. For the online version of the problem, we give an\nalgorithm that returns a bi-criteria solution with sub-linear regret.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 22:41:00 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 00:41:26 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 23:38:30 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Torrico", "Alfredo", "", "Seffi"], ["Singh", "Mohit", "", "Seffi"], ["Pokutta", "Sebastian", "", "Seffi"], ["Haghtalab", "Nika", "", "Seffi"], ["Joseph", "", "", "Seffi"], ["Naor", "", ""], ["Anari", "Nima", ""]]}, {"id": "1710.04886", "submitter": "Kevin Mcilhany", "authors": "Kevin McIlhany, Stephen Wiggins", "title": "High Dimensional Cluster Analysis Using Path Lengths", "comments": "52 pages, 94 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hierarchical scheme for clustering data is presented which applies to\nspaces with a high number of dimension ($N_{_{D}}>3$). The data set is first\nreduced to a smaller set of partitions (multi-dimensional bins). Multiple\nclustering techniques are used, including spectral clustering, however, new\ntechniques are also introduced based on the path length between partitions that\nare connected to one another. A Line-Of-Sight algorithm is also developed for\nclustering. A test bank of 12 data sets with varying properties is used to\nexpose the strengths and weaknesses of each technique. Finally, a robust\nclustering technique is discussed based on reaching a consensus among the\nmultiple approaches, overcoming the weaknesses found individually.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 12:08:37 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["McIlhany", "Kevin", ""], ["Wiggins", "Stephen", ""]]}, {"id": "1710.05017", "submitter": "Samuel Hopkins", "authors": "Samuel B. Hopkins and Pravesh K. Kothari and Aaron Potechin and Prasad\n  Raghavendra and Tselil Schramm and David Steurer", "title": "The power of sum-of-squares for detecting hidden structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study planted problems---finding hidden structures in random noisy\ninputs---through the lens of the sum-of-squares semidefinite programming\nhierarchy (SoS). This family of powerful semidefinite programs has recently\nyielded many new algorithms for planted problems, often achieving the best\nknown polynomial-time guarantees in terms of accuracy of recovered solutions\nand robustness to noise. One theme in recent work is the design of spectral\nalgorithms which match the guarantees of SoS algorithms for planted problems.\nClassical spectral algorithms are often unable to accomplish this: the twist in\nthese new spectral algorithms is the use of spectral structure of matrices\nwhose entries are low-degree polynomials of the input variables. We prove that\nfor a wide class of planted problems, including refuting random constraint\nsatisfaction problems, tensor and sparse PCA, densest-k-subgraph, community\ndetection in stochastic block models, planted clique, and others, eigenvalues\nof degree-d matrix polynomials are as powerful as SoS semidefinite programs of\nroughly degree d. For such problems it is therefore always possible to match\nthe guarantees of SoS without solving a large semidefinite program. Using\nrelated ideas on SoS algorithms and low-degree matrix polynomials (and inspired\nby recent work on SoS and the planted clique problem by Barak et al.), we prove\nnew nearly-tight SoS lower bounds for the tensor and sparse principal component\nanalysis problems. Our lower bounds for sparse principal component analysis are\nthe first to suggest that going beyond existing algorithms for this problem may\nrequire sub-exponential time.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 17:37:03 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Hopkins", "Samuel B.", ""], ["Kothari", "Pravesh K.", ""], ["Potechin", "Aaron", ""], ["Raghavendra", "Prasad", ""], ["Schramm", "Tselil", ""], ["Steurer", "David", ""]]}, {"id": "1710.05257", "submitter": "Tong Wang", "authors": "Tong Wang", "title": "Multi-Value Rule Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present the Multi-vAlue Rule Set (MARS) model for interpretable\nclassification with feature efficient presentations. MARS introduces a more\ngeneralized form of association rules that allows multiple values in a\ncondition. Rules of this form are more concise than traditional single-valued\nrules in capturing and describing patterns in data. MARS mitigates the problem\nof dealing with continuous features and high-cardinality categorical features\nfaced by rule-based models. Our formulation also pursues a higher efficiency of\nfeature utilization, which reduces the cognitive load to understand the\ndecision process. We propose an efficient inference method for learning a\nmaximum a posteriori model, incorporating theoretically grounded bounds to\niteratively reduce the search space to improve search efficiency. Experiments\nwith synthetic and real-world data demonstrate that MARS models have\nsignificantly smaller complexity and fewer features, providing better\ninterpretability while being competitive in predictive accuracy. We conducted a\nusability study with human subjects and results show that MARS is the easiest\nto use compared with other competing rule-based models, in terms of the correct\nrate and response time. Overall, MARS introduces a new approach to rule-based\nmodels that balance accuracy and interpretability with feature-efficient\nrepresentations.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 01:22:56 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Wang", "Tong", ""]]}, {"id": "1710.05422", "submitter": "Ehsan Emamjomeh-Zadeh", "authors": "Ehsan Emamjomeh-Zadeh, David Kempe", "title": "A General Framework for Robust Interactive Learning", "comments": "In NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for interactively learning models, such as\n(binary or non-binary) classifiers, orderings/rankings of items, or clusterings\nof data points. Our framework is based on a generalization of Angluin's\nequivalence query model and Littlestone's online learning model: in each\niteration, the algorithm proposes a model, and the user either accepts it or\nreveals a specific mistake in the proposal. The feedback is correct only with\nprobability $p > 1/2$ (and adversarially incorrect with probability $1 - p$),\ni.e., the algorithm must be able to learn in the presence of arbitrary noise.\nThe algorithm's goal is to learn the ground truth model using few iterations.\n  Our general framework is based on a graph representation of the models and\nuser feedback. To be able to learn efficiently, it is sufficient that there be\na graph $G$ whose nodes are the models and (weighted) edges capture the user\nfeedback, with the property that if $s, s^*$ are the proposed and target\nmodels, respectively, then any (correct) user feedback $s'$ must lie on a\nshortest $s$-$s^*$ path in $G$. Under this one assumption, there is a natural\nalgorithm reminiscent of the Multiplicative Weights Update algorithm, which\nwill efficiently learn $s^*$ even in the presence of noise in the user's\nfeedback.\n  From this general result, we rederive with barely any extra effort classic\nresults on learning of classifiers and a recent result on interactive\nclustering; in addition, we easily obtain new interactive learning algorithms\nfor ordering/ranking.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 00:13:20 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Emamjomeh-Zadeh", "Ehsan", ""], ["Kempe", "David", ""]]}, {"id": "1710.05491", "submitter": "Roohani Sharma", "authors": "Daniel Lokshtanov, Saket Saurabh, Roohani Sharma, Meirav Zehavi", "title": "Balanced Judicious Partition is Fixed-Parameter Tractable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The family of judicious partitioning problems, introduced by Bollob\\'as and\nScott to the field of extremal combinatorics, has been extensively studied from\na structural point of view for over two decades. This rich realm of problems\naims to counterbalance the objectives of classical partitioning problems such\nas Min Cut, Min Bisection and Max Cut. While these classical problems focus\nsolely on the minimization/maximization of the number of edges crossing the\ncut, judicious (bi)partitioning problems ask the natural question of the\nminimization/maximization of the number of edges lying in the (two) sides of\nthe cut. In particular, Judicious Bipartition (JB) seeks a bipartition that is\n\"judicious\" in the sense that neither side is burdened by too many edges, and\nBalanced JB also requires that the sizes of the sides themselves are \"balanced\"\nin the sense that neither of them is too large. Both of these problems were\ndefined in the work by Bollob\\'as and Scott, and have received notable\nscientific attention since then. In this paper, we shed light on the study of\njudicious partitioning problems from the viewpoint of algorithm design.\nSpecifically, we prove that BJB is FPT (which also proves that JB is FPT).\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 03:58:22 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Saurabh", "Saket", ""], ["Sharma", "Roohani", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1710.06025", "submitter": "Tongyang Li", "authors": "Tongyang Li, Xiaodi Wu", "title": "Quantum query complexity of entropy estimation", "comments": "43 pages, 1 figure", "journal-ref": "IEEE Transactions on Information Theory 65 (2019), no. 5,\n  2899-2921", "doi": "10.1109/TIT.2018.2883306", "report-no": null, "categories": "quant-ph cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of Shannon and R\\'enyi entropies of unknown discrete distributions\nis a fundamental problem in statistical property testing and an active research\ntopic in both theoretical computer science and information theory. Tight bounds\non the number of samples to estimate these entropies have been established in\nthe classical setting, while little is known about their quantum counterparts.\nIn this paper, we give the first quantum algorithms for estimating\n$\\alpha$-R\\'enyi entropies (Shannon entropy being 1-Renyi entropy). In\nparticular, we demonstrate a quadratic quantum speedup for Shannon entropy\nestimation and a generic quantum speedup for $\\alpha$-R\\'enyi entropy\nestimation for all $\\alpha\\geq 0$, including a tight bound for the\ncollision-entropy (2-R\\'enyi entropy). We also provide quantum upper bounds for\nextreme cases such as the Hartley entropy (i.e., the logarithm of the support\nsize of a distribution, corresponding to $\\alpha=0$) and the min-entropy case\n(i.e., $\\alpha=+\\infty$), as well as the Kullback-Leibler divergence between\ntwo distributions. Moreover, we complement our results with quantum lower\nbounds on $\\alpha$-R\\'enyi entropy estimation for all $\\alpha\\geq 0$.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 23:08:16 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Li", "Tongyang", ""], ["Wu", "Xiaodi", ""]]}, {"id": "1710.06261", "submitter": "Santosh Vempala", "authors": "Yin Tat Lee and Santosh S. Vempala", "title": "Convergence Rate of Riemannian Hamiltonian Monte Carlo and Faster\n  Polytope Volume Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first rigorous proof of the convergence of Riemannian Hamiltonian\nMonte Carlo, a general (and practical) method for sampling Gibbs distributions.\nOur analysis shows that the rate of convergence is bounded in terms of natural\nsmoothness parameters of an associated Riemannian manifold. We then apply the\nmethod with the manifold defined by the log barrier function to the problems of\n(1) uniformly sampling a polytope and (2) computing its volume, the latter by\nextending Gaussian cooling to the manifold setting. In both cases, the total\nnumber of steps needed is O^{*}(mn^{\\frac{2}{3}}), improving the state of the\nart. A key ingredient of our analysis is a proof of an analog of the KLS\nconjecture for Gibbs distributions over manifolds.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 13:30:27 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Lee", "Yin Tat", ""], ["Vempala", "Santosh S.", ""]]}, {"id": "1710.06298", "submitter": "Preux Philippe", "authors": "Georgios Papoudakis (SEQUEL), Philippe Preux (CRIStAL, SEQUEL), Martin\n  Monperrus (KTH)", "title": "A generative model for sparse, evolving digraphs", "comments": null, "journal-ref": "6th International Conference on Complex Networks and their\n  applications, Nov 2017, Lyon, France", "doi": "10.1007/978-3-319-72150-7_43", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating graphs that are similar to real ones is an open problem, while the\nsimilarity notion is quite elusive and hard to formalize. In this paper, we\nfocus on sparse digraphs and propose SDG, an algorithm that aims at generating\ngraphs similar to real ones. Since real graphs are evolving and this evolution\nis important to study in order to understand the underlying dynamical system,\nwe tackle the problem of generating series of graphs. We propose SEDGE, an\nalgorithm meant to generate series of graphs similar to a real series. SEDGE is\nan extension of SDG. We consider graphs that are representations of software\nprograms and show experimentally that our approach outperforms other existing\napproaches. Experiments show the performance of both algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 14:09:26 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Papoudakis", "Georgios", "", "SEQUEL"], ["Preux", "Philippe", "", "CRIStAL, SEQUEL"], ["Monperrus", "Martin", "", "KTH"]]}, {"id": "1710.06339", "submitter": "Guru Guruganesh", "authors": "Guru Guruganesh and Euiwoong Lee", "title": "Understanding the Correlation Gap for Matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of vertices $V$ with $|V| = n$, a weight vector $w \\in\n(\\mathbb{R}^+ \\cup \\{ 0 \\})^{\\binom{V}{2}}$, and a probability vector $x \\in\n[0, 1]^{\\binom{V}{2}}$ in the matching polytope, we study the quantity\n$\\frac{E_{G}[ \\nu_w(G)]}{\\sum_{(u, v) \\in \\binom{V}{2}} w_{u, v} x_{u, v}}$\nwhere $G$ is a random graph where each edge $e$ with weight $w_e$ appears with\nprobability $x_e$ independently, and let $\\nu_w(G)$ denotes the weight of the\nmaximum matching of $G$. This quantity is closely related to correlation gap\nand contention resolution schemes, which are important tools in the design of\napproximation algorithms, algorithmic game theory, and stochastic optimization.\n  We provide lower bounds for the above quantity for general and bipartite\ngraphs, and for weighted and unweighted settings. he best known upper bound is\n$0.54$ by Karp and Sipser, and the best lower bound is $0.4$. We show that it\nis at least $0.47$ for unweighted bipartite graphs, at least $0.45$ for\nweighted bipartite graphs, and at lea st $0.43$ for weighted general graphs. To\nachieve our results, we construct local distribution schemes on the dual which\nmay be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 15:14:44 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Guruganesh", "Guru", ""], ["Lee", "Euiwoong", ""]]}, {"id": "1710.06384", "submitter": "David Holzm\\\"uller", "authors": "David Holzm\\\"uller", "title": "Efficient Neighbor-Finding on Space-Filling Curves", "comments": "This is a slightly modified version of my bachelor thesis in\n  mathematics. The corresponding code can be found at\n  https://github.com/dholzmueller/sfcpp . Changes in v2: Added e-mail address\n  and links to github project, fixed footnote hyperlinks. Changes in v3: Minor\n  changes, corrected statement of Corollary 4.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space-filling curves (SFC, also known as FASS-curves) are a useful tool in\nscientific computing and other areas of computer science to sequentialize\nmultidimensional grids in a cache-efficient and parallelization-friendly way\nfor storage in an array. Many algorithms, for example grid-based numerical PDE\nsolvers, have to access all neighbor cells of each grid cell during a grid\ntraversal. While the array indices of neighbors can be stored in a cell, they\nstill have to be computed for initialization or when the grid is adaptively\nrefined. A fast neighbor-finding algorithm can thus significantly improve the\nruntime of computations on multidimensional grids.\n  In this thesis, we show how neighbors on many regular grids ordered by\nspace-filling curves can be found in an average-case time complexity of $O(1)$.\nIn general, this assumes that the local orientation (i.e. a variable of a\ndescribing grammar) of the SFC inside the grid cell is known in advance, which\ncan be efficiently realized during traversals. Supported SFCs include Hilbert,\nPeano and Sierpinski curves in arbitrary dimensions. We assume that integer\narithmetic operations can be performed in $O(1)$, i.e. independent of the size\nof the integer. We do not deal with the case of adaptively refined grids here.\nHowever, it appears that a generalization of the algorithm to suitable adaptive\ngrids is possible. To formulate the neighbor-finding algorithm and prove its\ncorrectness and runtime properties, a modeling framework is introduced. This\nframework extends the idea of vertex-labeling to a description using grammars\nand matrices. With the sfcpp library, we provide a C++ implementation to render\nSFCs generated by such models and automatically compute all lookup tables\nneeded for the neighbor-finding algorithm. Furthermore, optimized\nneighbor-finding implementations for various SFCs are included for which we\nprovide runtime measurements.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 16:54:28 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 09:02:14 GMT"}, {"version": "v3", "created": "Sat, 2 Nov 2019 11:20:47 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Holzm\u00fcller", "David", ""]]}, {"id": "1710.06500", "submitter": "Randal E. Bryant", "authors": "Randal E. Bryant", "title": "Chain Reduction for Binary and Zero-Suppressed Decision Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chain reduction enables reduced ordered binary decision diagrams (BDDs) and\nzero-suppressed binary decision diagrams (ZDDs) to each take advantage of the\nothers' ability to symbolically represent Boolean functions in compact form.\nFor any Boolean function, its chain-reduced ZDD (CZDD) representation will be\nno larger than its ZDD representation, and at most twice the size of its BDD\nrepresentation. The chain-reduced BDD (CBDD) of a function will be no larger\nthan its BDD representation, and at most three times the size of its CZDD\nrepresentation. Extensions to the standard algorithms for operating on BDDs and\nZDDs enable them to operate on the chain-reduced versions. Experimental\nevaluations on representative benchmarks for encoding word lists, solving\ncombinatorial problems, and operating on digital circuits indicate that chain\nreduction can provide significant benefits in terms of both memory and\nexecution time.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 21:01:37 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Bryant", "Randal E.", ""]]}, {"id": "1710.06559", "submitter": "Asahi Takaoka", "authors": "Asahi Takaoka", "title": "A recognition algorithm for simple-triangle graphs", "comments": "revised, results unchanged, reference changed. 12 pages 12pt, 1\n  figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple-triangle graph is the intersection graph of triangles that are\ndefined by a point on a horizontal line and an interval on another horizontal\nline. The time complexity of the recognition problem for simple-triangle graphs\nwas a longstanding open problem, which was recently settled. This paper\nprovides a new recognition algorithm for simple-triangle graphs to improve the\ntime bound from $O(n^2 \\overline{m})$ to $O(nm)$, where $n$, $m$, and\n$\\overline{m}$ are the number of vertices, edges, and non-edges of the graph,\nrespectively. The algorithm uses the vertex ordering characterization that a\ngraph is a simple-triangle graph if and only if there is a linear ordering of\nthe vertices containing both an alternating orientation of the graph and a\ntransitive orientation of the complement of the graph. We also show, as a\nbyproduct, that an alternating orientation can be obtained in $O(nm)$ time for\ncocomparability graphs, and it is NP-complete to decide whether a graph has an\norientation that is alternating and acyclic.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 02:30:04 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 16:04:14 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Takaoka", "Asahi", ""]]}, {"id": "1710.07107", "submitter": "Tiphaine Viard", "authors": "Tiphaine Viard, Rapha\\\"el Fournier-S'niehotta, Cl\\'emence Magnien,\n  Matthieu Latapy", "title": "Discovering Patterns of Interest in IP Traffic Using Cliques in\n  Bipartite Link Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying IP traffic is crucial for many applications. We focus here on the\ndetection of (structurally and temporally) dense sequences of interactions,\nthat may indicate botnets or coordinated network scans. More precisely, we\nmodel a MAWI capture of IP traffic as a link streams, i.e. a sequence of\ninteractions $(t_1 , t_2 , u, v)$ meaning that devices $u$ and $v$ exchanged\npackets from time $t_1$ to time $t_2$ . This traffic is captured on a single\nrouter and so has a bipartite structure: links occur only between nodes in two\ndisjoint sets. We design a method for finding interesting bipartite cliques in\nsuch link streams, i.e. two sets of nodes and a time interval such that all\nnodes in the first set are linked to all nodes in the second set throughout the\ntime interval. We then explore the bipartite cliques present in the considered\ntrace. Comparison with the MAWILab classification of anomalous IP addresses\nshows that the found cliques succeed in detecting anomalous network activity.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 12:06:57 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 06:08:34 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Viard", "Tiphaine", ""], ["Fournier-S'niehotta", "Rapha\u00ebl", ""], ["Magnien", "Cl\u00e9mence", ""], ["Latapy", "Matthieu", ""]]}, {"id": "1710.07132", "submitter": "Micha{\\l}  Karpi\\'nski", "authors": "Micha{\\l} Karpi\\'nski and Krzysztof Piecuch", "title": "On vertex coloring without monochromatic triangles", "comments": "Extended abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a certain relaxation of the classic vertex coloring problem, namely,\na coloring of vertices of undirected, simple graphs, such that there are no\nmonochromatic triangles. We give the first classification of the problem in\nterms of classic and parametrized algorithms. Several computational complexity\nresults are also presented, which improve on the previous results found in the\nliterature. We propose the new structural parameter for undirected, simple\ngraphs -- the triangle-free chromatic number $\\chi_3$. We bound $\\chi_3$ by\nother known structural parameters. We also present two classes of graphs with\ninteresting coloring properties, that play pivotal role in proving useful\nobservation about our problem. We give/ask several conjectures/questions\nthroughout this paper to encourage new research in the area of graph coloring.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 13:24:58 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Karpi\u0144ski", "Micha\u0142", ""], ["Piecuch", "Krzysztof", ""]]}, {"id": "1710.07145", "submitter": "Andrzej Pelc", "authors": "Andrzej Pelc", "title": "Reaching a Target in the Plane with no Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mobile agent has to reach a target in the Euclidean plane. Both the agent\nand the target are modeled as points. In the beginning, the agent is at\ndistance at most $D>0$ from the target. Reaching the target means that the\nagent gets at a {\\em sensing distance} at most $r>0$ from it. The agent has a\nmeasure of length and a compass. We consider two scenarios: in the {\\em static}\nscenario the target is inert, and in the {\\em dynamic} scenario it may move\narbitrarily at any (possibly varying) speed bounded by $v$. The agent has no\ninformation about the parameters of the problem, in particular it does not know\n$D$, $r$ or $v$. The goal is to reach the target at lowest possible cost,\nmeasured by the total length of the trajectory of the agent.\n  Our main result is establishing the minimum cost (up to multiplicative\nconstants) of reaching the target under both scenarios, and providing the\noptimal algorithm for the agent. For the static scenario the minimum cost is\n$\\Theta((\\log D + \\log \\frac{1}{r}) D^2/r)$, and for the dynamic scenario it is\n$\\Theta((\\log M + \\log \\frac{1}{r}) M^2/r)$, where $M=\\max(D,v)$. Under the\nlatter scenario, the speed of the agent in our algorithm grows exponentially\nwith time, and we prove that for an agent whose speed grows only polynomially\nwith time, this cost is impossible to achieve.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 13:53:51 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Pelc", "Andrzej", ""]]}, {"id": "1710.07148", "submitter": "Lars Jaffke", "authors": "Lars Jaffke, O-joung Kwon, Jan Arne Telle", "title": "A unified polynomial-time algorithm for Feedback Vertex Set on graphs of\n  bounded mim-width", "comments": "26 pages, 3 figures; accepted at STACS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a first polynomial-time algorithm for (Weighted) Feedback Vertex Set\non graphs of bounded maximum induced matching width (mim-width). Explicitly,\ngiven a branch decomposition of mim-width $w$, we give an\n$n^{\\mathcal{O}(w)}$-time algorithm that solves Feedback Vertex Set. This\nprovides a unified algorithm for many well-known classes, such as Interval\ngraphs and Permutation graphs, and furthermore, it gives the first\npolynomial-time algorithms for other classes of bounded mim-width, such as\nCircular Permutation and Circular $k$-Trapezoid graphs for fixed $k$. In all\nthese classes the decomposition is computable in polynomial time, as shown by\nBelmonte and Vatshelle [Theor. Comput. Sci. 2013]. We show that powers of\ngraphs of tree-width $w - 1$ or path-width $w$ and powers of graphs of\nclique-width $w$ have mim-width at most $w$. These results extensively provide\nnew classes of bounded mim-width. We prove a slight strengthening of the first\nstatement which implies that, surprisingly, Leaf Power graphs which are of\nimportance in the field of phylogenetic studies have mim-width at most $1$.\nGiven a tree decomposition of width $w-1$, a path decomposition of width $w$,\nor a clique-width $w$-expression of a graph, one can for any value of $k$ find\na mim-width decomposition of its $k$-power in polynomial time, and apply our\nalgorithm to solve Feedback Vertex Set on the $k$-power in time\n$n^{\\mathcal{O}(w)}$. In contrast to Feedback Vertex Set, we show that\nHamiltonian Cycle is NP-complete even on graphs of linear mim-width $1$, which\nfurther hints at the expressive power of the mim-width parameter.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 14:01:09 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 10:59:10 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Jaffke", "Lars", ""], ["Kwon", "O-joung", ""], ["Telle", "Jan Arne", ""]]}, {"id": "1710.07356", "submitter": "Kuan Cheng", "authors": "Kuan Cheng, Xin Li, Ke Wu", "title": "Synchronization Strings: Efficient and Fast Deterministic Constructions\n  over Small Alphabets", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronization strings are recently introduced by Haeupler and Shahrasbi\n[HS17a] in the study of codes for correcting insertion and deletion errors\n(insdel codes). A synchronization string is an encoding of the indices of the\nsymbols in a string, and together with an appropriate decoding algorithm it can\ntransform insertion and deletion errors into standard symbol erasures and\ncorruptions. This reduces the problem of constructing insdel codes to the\nproblem of constructing standard error correcting codes, which is much better\nunderstood. Besides this, synchronization strings are also useful in other\napplications such as synchronization sequences and interactive coding schemes.\n  Amazingly, [HS17a] showed that for any error parameter $\\varepsilon>0$,\nsynchronization strings of arbitrary length exist over an alphabet whose size\ndepends only on $\\varepsilon$. Specifically, they obtained an alphabet size of\n$O(\\varepsilon^{-4})$, as well as a randomized construction that runs in\nexpected time $O(n^5)$. However, it remains an interesting question to find\ndeterministic and more efficient constructions.\n  In this paper, we improve the construction in [HS17a] in three aspects: we\nachieve a smaller alphabet size, a deterministic construction, and a faster\nalgorithm. Along the way we introduce a new combinatorial object, and establish\na new connection between synchronization strings and insdel codes --- such\ncodes can be used in a simple way to construct synchronization strings. This\nnew connection complements the connection found in [HS17a], and may be of\nindependent interest. In an independent work [HS17c], Haeupler and Shahrasbi\nalso give deterministic constructions of synchronization strings over arbitrary\nlength (or even infinite length). Their constructions can achieve linear\nconstruction time, but have alphabet size $\\varepsilon^{-O(1)}$, which may be\nlarger than ours.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 21:13:57 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 02:07:04 GMT"}, {"version": "v3", "created": "Sun, 29 Oct 2017 00:45:38 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Cheng", "Kuan", ""], ["Li", "Xin", ""], ["Wu", "Ke", ""]]}, {"id": "1710.07505", "submitter": "Ralph Neininger", "authors": "Ralph Neininger and Jasmin Straub", "title": "Probabilistic Analysis of the Dual-Pivot Quicksort \"Count\"", "comments": "To appear in the proceedings of Analytic Algorithmics and\n  Combinatorics (ANALCO18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Aum\\\"uller and Dietzfelbinger proposed a version of a dual-pivot\nquicksort, called \"Count\", which is optimal among dual-pivot versions with\nrespect to the average number of key comparisons required. In this note we\nprovide further probabilistic analysis of \"Count\". We derive an exact formula\nfor the average number of swaps needed by \"Count\" as well as an asymptotic\nformula for the variance of the number of swaps and a limit law. Also for the\nnumber of key comparisons the asymptotic variance and a limit law are\nidentified. We also consider both complexity measures jointly and find their\nasymptotic correlation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 12:32:37 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Neininger", "Ralph", ""], ["Straub", "Jasmin", ""]]}, {"id": "1710.07565", "submitter": "Sebastian Lamm", "authors": "Daniel Funke, Sebastian Lamm, Ulrich Meyer, Peter Sanders, Manuel\n  Penschuck, Christian Schulz, Darren Strash, Moritz von Looz", "title": "Communication-free Massively Distributed Graph Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing massive complex networks yields promising insights about our\neveryday lives. Building scalable algorithms to do so is a challenging task\nthat requires a careful analysis and an extensive evaluation. However,\nengineering such algorithms is often hindered by the scarcity of\npublicly~available~datasets.\n  Network generators serve as a tool to alleviate this problem by providing\nsynthetic instances with controllable parameters. However, many network\ngenerators fail to provide instances on a massive scale due to their sequential\nnature or resource constraints. Additionally, truly scalable network generators\nare few and often limited in their realism.\n  In this work, we present novel generators for a variety of network models\nthat are frequently used as benchmarks. By making use of pseudorandomization\nand divide-and-conquer schemes, our generators follow a communication-free\nparadigm. The resulting generators are thus embarrassingly parallel and have a\nnear optimal scaling behavior. This allows us to generate instances of up to\n$2^{43}$ vertices and $2^{47}$ edges in less than 22 minutes on 32768 cores.\nTherefore, our generators allow new graph families to be used on an\nunprecedented scale.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 15:10:59 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 09:46:00 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2019 10:12:06 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Funke", "Daniel", ""], ["Lamm", "Sebastian", ""], ["Meyer", "Ulrich", ""], ["Sanders", "Peter", ""], ["Penschuck", "Manuel", ""], ["Schulz", "Christian", ""], ["Strash", "Darren", ""], ["von Looz", "Moritz", ""]]}, {"id": "1710.07584", "submitter": "Julien Fradin", "authors": "Guillaume Fertin, Julien Fradin, Christian Komusiewicz", "title": "The Maximum Colorful Arborescence problem parameterized by the structure\n  of its color hierarchy graph", "comments": "Submitted a 12 pages version (+ rest in Appendix for referees) to CPM\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let G=(V,A) be a vertex-colored arc-weighted directed acyclic graph (DAG)\nrooted in some vertex r, and let H be its color hierarchy graph, defined as\nfollows: V(H) is the color set C of G, and an arc from color c to color c'\nexists in H if there is an arc in G from a vertex of color c to a vertex of\ncolor c'. In this paper, we study the MAXIMUM COLORFUL ARBORESCENCE problem (or\nMCA), which takes as input a DAG G with the additional constraint that H is\nalso a DAG, and aims at finding in G an arborescence rooted in r, of maximum\nweight, and in which no color appears more than once. The MCA problem is\nmotivated by the inference of unknown metabolites from mass spectrometry\nexperiments. However, whereas the problem has been studied for roughly ten\nyears, the crucial property that H is necessarily a DAG has only been pointed\nout and exploited very recently. In this paper, we further investigate MCA\nunder this new light, by providing algorithmic results for the problem, with a\nspecific focus on fixed-parameterized tractability (FPT) issues, and relatively\nto different structural parameters of H. In particular, we provide an\nO*(3^{nhs}) time algorithm for solving MCA, where nhs is the number of vertices\nof indegree at least two in H, thereby improving the O*(3^{|C|}) algorithm from\n[B\\\"ocker et al. 2008]. We also prove that MCA is W[2]-hard relatively to the\ntreewidth Ht of H, and further show that it is FPT relatively to Ht+lc, where\nlc = |V| - |C|.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 15:55:58 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 16:59:08 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Fertin", "Guillaume", ""], ["Fradin", "Julien", ""], ["Komusiewicz", "Christian", ""]]}, {"id": "1710.07600", "submitter": "Yury Maximov", "authors": "Andrii Riazanov, Yury Maximov, Michael Chertkov", "title": "Belief Propagation Min-Sum Algorithm for Generalized Min-Cost Network\n  Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief Propagation algorithms are instruments used broadly to solve graphical\nmodel optimization and statistical inference problems. In the general case of a\nloopy Graphical Model, Belief Propagation is a heuristic which is quite\nsuccessful in practice, even though its empirical success, typically, lacks\ntheoretical guarantees. This paper extends the short list of special cases\nwhere correctness and/or convergence of a Belief Propagation algorithm is\nproven. We generalize formulation of Min-Sum Network Flow problem by relaxing\nthe flow conservation (balance) constraints and then proving that the Belief\nPropagation algorithm converges to the exact result.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 16:39:35 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 09:12:09 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Riazanov", "Andrii", ""], ["Maximov", "Yury", ""], ["Chertkov", "Michael", ""]]}, {"id": "1710.07601", "submitter": "Till Fluschnik", "authors": "Till Fluschnik, George B. Mertzios, and Andr\\'e Nichterlein", "title": "Kernelization Lower Bounds for Finding Constant-Size Subgraphs", "comments": "An extended abstract appeared in Proceedings of the 14th Conference\n  on Computability in Europe (CiE 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernelization is an important tool in parameterized algorithmics. Given an\ninput instance accompanied by a parameter, the goal is to compute in polynomial\ntime an equivalent instance of the same problem such that the size of the\nreduced instance only depends on the parameter and not on the size of the\noriginal instance. In this paper, we provide a first conceptual study on limits\nof kernelization for several polynomial-time solvable problems. For instance,\nwe consider the problem of finding a triangle with negative sum of edge weights\nparameterized by the maximum degree of the input graph. We prove that a\nlinear-time computable strict kernel of truly subcubic size for this problem\nviolates the popular APSP-conjecture.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 16:48:35 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 09:23:04 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Fluschnik", "Till", ""], ["Mertzios", "George B.", ""], ["Nichterlein", "Andr\u00e9", ""]]}, {"id": "1710.07719", "submitter": "Alessandro Arlotto", "authors": "Alessandro Arlotto and Itai Gurvich", "title": "Uniformly bounded regret in the multi-secretary problem", "comments": "42 pages, 7 figures", "journal-ref": "Stochastic Systems, 2019, 9, 231-260", "doi": "10.1287/stsy.2018.0028", "report-no": null, "categories": "math.PR cs.DM cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the secretary problem of Cayley (1875) and Moser (1956), $n$ non-negative,\nindependent, random variables with common distribution are sequentially\npresented to a decision maker who decides when to stop and collect the most\nrecent realization. The goal is to maximize the expected value of the collected\nelement. In the $k$-choice variant, the decision maker is allowed to make $k\n\\leq n$ selections to maximize the expected total value of the selected\nelements. Assuming that the values are drawn from a known distribution with\nfinite support, we prove that the best regret---the expected gap between the\noptimal online policy and its offline counterpart in which all $n$ values are\nmade visible at time $0$---is uniformly bounded in the the number of candidates\n$n$ and the budget $k$. Our proof is constructive: we develop an adaptive\nBudget-Ratio policy that achieves this performance. The policy selects or skips\nvalues depending on where the ratio of the residual budget to the remaining\ntime stands relative to multiple thresholds that correspond to middle points of\nthe distribution. We also prove that being adaptive is crucial: in general, the\nminimal regret among non-adaptive policies grows like the square root of $n$.\nThe difference is the value of adaptiveness.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 22:18:24 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 20:39:24 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Arlotto", "Alessandro", ""], ["Gurvich", "Itai", ""]]}, {"id": "1710.07774", "submitter": "Shaofeng Jiang", "authors": "T-H. Hubert Chan, Haotian Jiang, Shaofeng H.-C. Jiang", "title": "A Unified PTAS for Prize Collecting TSP and Steiner Tree Problem in\n  Doubling Metrics", "comments": "Appeared in ESA 2018. This is the full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified polynomial-time approximation scheme (PTAS) for the\nprize collecting traveling salesman problem (PCTSP) and the prize collecting\nSteiner tree problem (PCSTP) in doubling metrics. Given a metric space and a\npenalty function on a subset of points known as terminals, a solution is a\nsubgraph on points in the metric space, whose cost is the weight of its edges\nplus the penalty due to terminals not covered by the subgraph. Under our\nunified framework, the solution subgraph needs to be Eulerian for PCTSP, while\nit needs to be connected for PCSTP. Before our work, even a QPTAS for the\nproblems in doubling metrics is not known.\n  Our unified PTAS is based on the previous dynamic programming frameworks\nproposed in [Talwar STOC 2004] and [Bartal, Gottlieb, Krauthgamer STOC 2012].\nHowever, since it is unknown which part of the optimal cost is due to edge\nlengths and which part is due to penalties of uncovered terminals, we need to\ndevelop new techniques to apply previous divide-and-conquer strategies and\nsparse instance decompositions.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 08:33:37 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 18:29:18 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Chan", "T-H. Hubert", ""], ["Jiang", "Haotian", ""], ["Jiang", "Shaofeng H. -C.", ""]]}, {"id": "1710.07836", "submitter": "Mike Steel Prof.", "authors": "Joan Carles Pons, Charles Semple, Mike Steel", "title": "Tree-based networks: characterisations, metrics, and support trees", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic networks generalise phylogenetic trees and allow for the\naccurate representation of the evolutionary history of a set of present-day\nspecies whose past includes reticulate events such as hybridisation and lateral\ngene transfer. One way to obtain such a network is by starting with a (rooted)\nphylogenetic tree $T$, called a base tree, and adding arcs between arcs of $T$.\nThe class of phylogenetic networks that can be obtained in this way is called\ntree-based networks and includes the prominent classes of tree-child and\nreticulation-visible networks. Initially defined for binary phylogenetic\nnetworks, tree-based networks naturally extend to arbitrary phylogenetic\nnetworks. In this paper, we generalise recent tree-based characterisations and\nassociated proximity measures for binary phylogenetic networks to arbitrary\nphylogenetic networks. These characterisations are in terms of matchings in\nbipartite graphs, path partitions, and antichains. Some of the generalisations\nare straightforward to establish using the original approach, while others\nrequire a very different approach. Furthermore, for an arbitrary tree-based\nnetwork $N$, we characterise the support trees of $N$, that is, the tree-based\nembeddings of $N$. We use this characterisation to give an explicit formula for\nthe number of support trees of $N$ when $N$ is binary. This formula is written\nin terms of the components of a bipartite graph.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 18:18:10 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 10:46:55 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Pons", "Joan Carles", ""], ["Semple", "Charles", ""], ["Steel", "Mike", ""]]}, {"id": "1710.07887", "submitter": "Jinshuo Dong", "authors": "Jinshuo Dong, Aaron Roth, Zachary Schutzman, Bo Waggoner, Zhiwei\n  Steven Wu", "title": "Strategic Classification from Revealed Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an online linear classification problem, in which the data is\ngenerated by strategic agents who manipulate their features in an effort to\nchange the classification outcome. In rounds, the learner deploys a classifier,\nand an adversarially chosen agent arrives, possibly manipulating her features\nto optimally respond to the learner. The learner has no knowledge of the\nagents' utility functions or \"real\" features, which may vary widely across\nagents. Instead, the learner is only able to observe their \"revealed\npreferences\" --- i.e. the actual manipulated feature vectors they provide. For\na broad family of agent cost functions, we give a computationally efficient\nlearning algorithm that is able to obtain diminishing \"Stackelberg regret\" ---\na form of policy regret that guarantees that the learner is obtaining loss\nnearly as small as that of the best classifier in hindsight, even allowing for\nthe fact that agents will best-respond differently to the optimal classifier.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 04:29:32 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Dong", "Jinshuo", ""], ["Roth", "Aaron", ""], ["Schutzman", "Zachary", ""], ["Waggoner", "Bo", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1710.07992", "submitter": "Veeresh Devireddy", "authors": "Veeresh D, Thimmaraju S. N, Ravish G. K", "title": "Twin Sort Technique", "comments": "core computer algorithm, 3 pages, conference", "journal-ref": "International Journal of Combined Research & Development October\n  2014", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective behind the Twin Sort technique is to sort the list of unordered\ndata elements efficiently and to allow efficient and simple arrangement of data\nelements within the data structure with optimization of comparisons and\niterations in the sorting method. This sorting technique effectively terminates\nthe iterations when there is no need of comparison if the elements are all\nsorted in between the iterations. Unlike Quick sort, Merge sorting technique,\nthis new sorting technique is based on the iterative method of sorting elements\nwithin the data structure. So it will be advantageous for optimization of\niterations when there is no need for sorting elements. Finally, the Twin Sort\ntechnique is more efficient and simple method of arranging elements within a\ndata structure and it is easy to implement when comparing to the other sorting\ntechnique. By the introduction of optimization of comparison and iterations, it\nwill never allow the arranging task on the ordered elements.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 18:16:41 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["D", "Veeresh", ""], ["N", "Thimmaraju S.", ""], ["K", "Ravish G.", ""]]}, {"id": "1710.08029", "submitter": "Fran\\c{c}ois Serre", "authors": "Fran\\c{c}ois Serre and Markus P\\\"uschel", "title": "Characterizing and Enumerating Walsh-Hadamard Transform Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a way of characterizing the algorithms computing a Walsh-Hadamard\ntransform that consist of a sequence of arrays of butterflies\n($I_{2^{n-1}}\\otimes \\text{DFT}_2$) interleaved by linear permutations. Linear\npermutations are those that map linearly the binary representation of its\nelement indices. We also propose a method to enumerate these algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 22:13:27 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Serre", "Fran\u00e7ois", ""], ["P\u00fcschel", "Markus", ""]]}, {"id": "1710.08231", "submitter": "Yaroslav Akhremtsev", "authors": "Yaroslav Akhremtsev, Peter Sanders, Christian Schulz", "title": "High-Quality Shared-Memory Graph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partitioning graphs into blocks of roughly equal size such that few edges run\nbetween blocks is a frequently needed operation in processing graphs. Recently,\nsize, variety, and structural complexity of these networks has grown\ndramatically. Unfortunately, previous approaches to parallel graph partitioning\nhave problems in this context since they often show a negative trade-off\nbetween speed and quality. We present an approach to multi-level shared-memory\nparallel graph partitioning that guarantees balanced solutions, shows high\nspeed-ups for a variety of large graphs and yields very good quality\nindependently of the number of cores used. For example, on 31 cores, our\nalgorithm partitions our largest test instance into 16 blocks cutting less than\nhalf the number of edges than our main competitor when both algorithms are\ngiven the same amount of time. Important ingredients include parallel label\npropagation for both coarsening and improvement, parallel initial partitioning,\na simple yet effective approach to parallel localized local search, and fast\nlocality preserving hash tables.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 12:20:18 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 10:37:00 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 15:41:18 GMT"}, {"version": "v4", "created": "Mon, 15 Oct 2018 09:30:36 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Akhremtsev", "Yaroslav", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1710.08255", "submitter": "Lorenz H\\\"ubschle-Schneider", "authors": "Lorenz H\\\"ubschle-Schneider and Peter Sanders", "title": "Communication Efficient Checking of Big Data Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose fast probabilistic algorithms with low (i.e., sublinear in the\ninput size) communication volume to check the correctness of operations in Big\nData processing frameworks and distributed databases. Our checkers cover many\nof the commonly used operations, including sum, average, median, and minimum\naggregation, as well as sorting, union, merge, and zip. An experimental\nevaluation of our implementation in Thrill (Bingmann et al., 2016) confirms the\nlow overhead and high failure detection rate predicted by theoretical analysis.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 13:15:53 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 10:34:38 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["H\u00fcbschle-Schneider", "Lorenz", ""], ["Sanders", "Peter", ""]]}, {"id": "1710.08267", "submitter": "Anna Hermann", "authors": "Ulrich Brenner and Anna Hermann", "title": "Faster Carry Bit Computation for Adder Circuits with Prescribed Arrival\n  Times", "comments": null, "journal-ref": "ACM Trans. Algorithms 15, 4, Article 45 (July 2019)", "doi": "10.1145/3340321", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem of constructing fast circuits for the\ncarry bit computation in binary addition. Up to a small additive constant, the\ncarry bit computation reduces to computing an \\aop, i.e., a formula of type\n$t_0 \\land (t_1 \\lor (t_2 \\land ( \\dots t_{m-1}) \\dots )$ or $t_0 \\lor (t_1\n\\land (t_2 \\lor ( \\dots t_{m-1}) \\dots )$. We present an algorithm that\ncomputes the fastest known Boolean circuit for an \\aop~ with given arrival\ntimes $a(t_0), \\dotsc, a(t_{m-1})$ for the input signals. Our objective\nfunction is delay, a natural generalization of depth with respect to arrival\ntimes. The maximum delay of the circuit we compute is $\\log_2 W + \\log_2 \\log_2\nm + \\log_2 \\log_2 \\log_2 m + 4.3$, where $W := \\sum_{i = 0}^{m-1} 2^{a(t_i)}$.\nNote that $\\lceil \\log_2 W \\rceil$ is a lower bound on the delay of any circuit\ndepending on inputs $t_0, \\dotsc, t_{m-1}$ with prescribed arrival times. Our\nmethod yields the fastest circuits for \\aop s, carry bit computation and adders\nin terms of delay known so far.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 14:14:39 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 12:16:04 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 11:11:13 GMT"}, {"version": "v4", "created": "Fri, 25 Oct 2019 07:14:42 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Brenner", "Ulrich", ""], ["Hermann", "Anna", ""]]}, {"id": "1710.08291", "submitter": "Yoann Dieudonn\\'e", "authors": "S\\'ebastien Bouchard, Yoann Dieudonn\\'e, Andrzej Pelc, Franck Petit", "title": "Deterministic Rendezvous at a Node of Agents with Arbitrary Velocities", "comments": "arXiv admin note: text overlap with arXiv:1704.08880", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of rendezvous in networks modeled as undirected graphs.\nTwo mobile agents with different labels, starting at different nodes of an\nanonymous graph, have to meet. This task has been considered in the literature\nunder two alternative scenarios: weak and strong. Under the weak scenario,\nagents may meet either at a node or inside an edge. Under the strong scenario,\nthey have to meet at a node, and they do not even notice meetings inside an\nedge. Rendezvous algorithms under the strong scenario are known for synchronous\nagents. For asynchronous agents, rendezvous under the strong scenario is\nimpossible even in the two-node graph, and hence only algorithms under the weak\nscenario were constructed. In this paper we show that rendezvous under the\nstrong scenario is possible for agents with restricted asynchrony: agents have\nthe same measure of time but the adversary can arbitrarily impose the speed of\ntraversing each edge by each of the agents. We construct a deterministic\nrendezvous algorithm for such agents, working in time polynomial in the size of\nthe graph, in the length of the smaller label, and in the largest edge\ntraversal time.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 13:46:08 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 13:45:25 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Bouchard", "S\u00e9bastien", ""], ["Dieudonn\u00e9", "Yoann", ""], ["Pelc", "Andrzej", ""], ["Petit", "Franck", ""]]}, {"id": "1710.08381", "submitter": "Shreyas Pai", "authors": "Sayan Bandyapadhyay and Tanmay Inamdar and Shreyas Pai and Sriram V.\n  Pemmaraju", "title": "Near-Optimal Clustering in the $k$-machine model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering problem, in its many variants, has numerous applications in\noperations research and computer science (e.g., in applications in\nbioinformatics, image processing, social network analysis, etc.). As sizes of\ndata sets have grown rapidly, researchers have focused on designing algorithms\nfor clustering problems in models of computation suited for large-scale\ncomputation such as MapReduce, Pregel, and streaming models. The $k$-machine\nmodel (Klauck et al., SODA 2015) is a simple, message-passing model for\nlarge-scale distributed graph processing. This paper considers three of the\nmost prominent examples of clustering problems: the uncapacitated facility\nlocation problem, the $p$-median problem, and the $p$-center problem and\npresents $O(1)$-factor approximation algorithms for these problems running in\n$\\tilde{O}(n/k)$ rounds in the $k$-machine model. These algorithms are optimal\nup to polylogarithmic factors because this paper also shows\n$\\tilde{\\Omega}(n/k)$ lower bounds for obtaining polynomial-factor\napproximation algorithms for these problems. These are the first results for\nclustering problems in the $k$-machine model.\n  We assume that the metric provided as input for these clustering problems in\nonly implicitly provided, as an edge-weighted graph and in a nutshell, our main\ntechnical contribution is to show that constant-factor approximation algorithms\nfor all three clustering problems can be obtained by learning only a small\nportion of the input metric.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 16:57:50 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Bandyapadhyay", "Sayan", ""], ["Inamdar", "Tanmay", ""], ["Pai", "Shreyas", ""], ["Pemmaraju", "Sriram V.", ""]]}, {"id": "1710.08436", "submitter": "Yun William Yu", "authors": "Yun William Yu and Griffin M. Weber", "title": "HyperMinHash: MinHash in LogLog space", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this extended abstract, we describe and analyze a lossy compression of\nMinHash from buckets of size $O(\\log n)$ to buckets of size $O(\\log\\log n)$ by\nencoding using floating-point notation. This new compressed sketch, which we\ncall HyperMinHash, as we build off a HyperLogLog scaffold, can be used as a\ndrop-in replacement of MinHash. Unlike comparable Jaccard index fingerprinting\nalgorithms in sub-logarithmic space (such as b-bit MinHash), HyperMinHash\nretains MinHash's features of streaming updates, unions, and cardinality\nestimation. For a multiplicative approximation error $1+ \\epsilon$ on a Jaccard\nindex $ t $, given a random oracle, HyperMinHash needs $O\\left(\\epsilon^{-2}\n\\left( \\log\\log n + \\log \\frac{1}{ t \\epsilon} \\right)\\right)$ space.\nHyperMinHash allows estimating Jaccard indices of 0.01 for set cardinalities on\nthe order of $10^{19}$ with relative error of around 10\\% using 64KiB of\nmemory; MinHash can only estimate Jaccard indices for cardinalities of\n$10^{10}$ with the same memory consumption.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 18:02:16 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 18:28:24 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 02:23:43 GMT"}, {"version": "v4", "created": "Fri, 6 Jul 2018 20:36:33 GMT"}, {"version": "v5", "created": "Sat, 13 Jul 2019 15:29:47 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Yu", "Yun William", ""], ["Weber", "Griffin M.", ""]]}, {"id": "1710.08455", "submitter": "Samuel Gutekunst", "authors": "Samuel C. Gutekunst and David P. Williamson", "title": "The Unbounded Integrality Gap of a Semidefinite Relaxation of the\n  Traveling Salesman Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a semidefinite programming relaxation of the traveling salesman\nproblem introduced by de Klerk, Pasechnik, and Sotirov [8] and show that their\nrelaxation has an unbounded integrality gap. In particular, we give a family of\ninstances such that the gap increases linearly with $n$. To obtain this result,\nwe search for feasible solutions within a highly structured class of matrices;\nthe problem of finding such solutions reduces to finding feasible solutions for\na related linear program, which we do analytically. The solutions we find imply\nthe unbounded integrality gap. Further, they imply several corollaries that\nhelp us better understand the semidefinite program and its relationship to\nother TSP relaxations. Using the same technique, we show that a more general\nsemidefinite program introduced by de Klerk, de Oliveira Filho, and Pasechnik\n[7] for the $k$-cycle cover problem also has an unbounded integrality gap.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 18:49:21 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Gutekunst", "Samuel C.", ""], ["Williamson", "David P.", ""]]}, {"id": "1710.08488", "submitter": "Euiwoong Lee", "authors": "Anupam Gupta, Euiwoong Lee, Jason Li", "title": "An FPT Algorithm Beating 2-Approximation for $k$-Cut", "comments": "26 pages, 4 figures, to appear in SODA '18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $k$-Cut problem, we are given an edge-weighted graph $G$ and an\ninteger $k$, and have to remove a set of edges with minimum total weight so\nthat $G$ has at least $k$ connected components. Prior work on this problem\ngives, for all $h \\in [2,k]$, a $(2-h/k)$-approximation algorithm for $k$-cut\nthat runs in time $n^{O(h)}$. Hence to get a $(2 - \\varepsilon)$-approximation\nalgorithm for some absolute constant $\\varepsilon$, the best runtime using\nprior techniques is $n^{O(k\\varepsilon)}$. Moreover, it was recently shown that\ngetting a $(2 - \\varepsilon)$-approximation for general $k$ is NP-hard,\nassuming the Small Set Expansion Hypothesis.\n  If we use the size of the cut as the parameter, an FPT algorithm to find the\nexact $k$-Cut is known, but solving the $k$-Cut problem exactly is $W[1]$-hard\nif we parameterize only by the natural parameter of $k$. An immediate question\nis: \\emph{can we approximate $k$-Cut better in FPT-time, using $k$ as the\nparameter?}\n  We answer this question positively. We show that for some absolute constant\n$\\varepsilon > 0$, there exists a $(2 - \\varepsilon)$-approximation algorithm\nthat runs in time $2^{O(k^6)} \\cdot \\widetilde{O} (n^4)$. This is the first FPT\nalgorithm that is parameterized only by $k$ and strictly improves the\n$2$-approximation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 20:10:40 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Gupta", "Anupam", ""], ["Lee", "Euiwoong", ""], ["Li", "Jason", ""]]}, {"id": "1710.08607", "submitter": "Shweta Jain", "authors": "Talya Eden and Shweta Jain and Ali Pinar and Dana Ron and C. Seshadhri", "title": "Provable and practical approximations for the degree distribution using\n  sublinear graph samples", "comments": "Longer version of the WWW 2018 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The degree distribution is one of the most fundamental properties used in the\nanalysis of massive graphs. There is a large literature on graph sampling,\nwhere the goal is to estimate properties (especially the degree distribution)\nof a large graph through a small, random sample. The degree distribution\nestimation poses a significant challenge, due to its heavy-tailed nature and\nthe large variance in degrees.\n  We design a new algorithm, SADDLES, for this problem, using recent\nmathematical techniques from the field of sublinear algorithms. The SADDLES\nalgorithm gives provably accurate outputs for all values of the degree\ndistribution. For the analysis, we define two fatness measures of the degree\ndistribution, called the $h$-index and the $z$-index. We prove that SADDLES is\nsublinear in the graph size when these indices are large. A corollary of this\nresult is a provably sublinear algorithm for any degree distribution bounded\nbelow by a power law.\n  We deploy our new algorithm on a variety of real datasets and demonstrate its\nexcellent empirical behavior. In all instances, we get extremely accurate\napproximations for all values in the degree distribution by observing at most\n$1\\%$ of the vertices. This is a major improvement over the state-of-the-art\nsampling algorithms, which typically sample more than $10\\%$ of the vertices to\ngive comparable results. We also observe that the $h$ and $z$-indices of real\ngraphs are large, validating our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 05:52:43 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 05:32:23 GMT"}, {"version": "v3", "created": "Tue, 28 Aug 2018 05:22:07 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Eden", "Talya", ""], ["Jain", "Shweta", ""], ["Pinar", "Ali", ""], ["Ron", "Dana", ""], ["Seshadhri", "C.", ""]]}, {"id": "1710.08815", "submitter": "Shahrzad Haddadan", "authors": "Flavio Chierichetti and Shahrzad Haddadan", "title": "On the Complexity of Sampling Nodes Uniformly from a Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a number of graph exploration problems in the following natural\nscenario: an algorithm starts exploring an undirected graph from some seed\nnode; the algorithm, for an arbitrary node $v$ that it is aware of, can ask an\noracle to return the set of the neighbors of $v$. (In social network analysis,\na call to this oracle corresponds to downloading the profile page of user $v$\nin a social network.) The goal of the algorithm is to either learn something\n(e.g., average degree) about the graph, or to return some random function of\nthe graph (e.g., a uniform-at-random node), while accessing/downloading as few\nnodes of the graph as possible. Motivated by practical applications, we study\nthe complexities of a variety of problems in terms of the graph's mixing time\nand average degree -- two measures that are believed to be quite small in\nreal-world social networks, and that have often been used in the applied\nliterature to bound the performance of online exploration algorithms. Our main\nresult is that the algorithm has to access $\\Omega\\left(t_{\\rm mix} d_{\\rm avg}\n\\epsilon^{-2} \\ln \\delta^{-1}\\right)$ nodes to obtain, with probability at\nleast $1-\\delta$, an $\\epsilon$-additive approximation of the average of a\nbounded function on the nodes of a graph -- this lower bound matches the\nperformance of an algorithm that was proposed in the literature. We also give\ntight bounds for the problem of returning a close-to-uniform-at-random node\nfrom the graph. Finally, we give lower bounds for the problems of estimating\nthe average degree of the graph, and the number of nodes of the graph.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 14:50:08 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Chierichetti", "Flavio", ""], ["Haddadan", "Shahrzad", ""]]}, {"id": "1710.08937", "submitter": "Vincent Froese", "authors": "Markus Brill, Till Fluschnik, Vincent Froese, Brijnesh Jain, Rolf\n  Niedermeier, David Schultz", "title": "Exact Mean Computation in Dynamic Time Warping Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic time warping constitutes a major tool for analyzing time series. In\nparticular, computing a mean series of a given sample of series in dynamic time\nwarping spaces (by minimizing the Fr\\'echet function) is a challenging\ncomputational problem, so far solved by several heuristic and inexact\nstrategies. We spot some inaccuracies in the literature on exact mean\ncomputation in dynamic time warping spaces. Our contributions comprise an exact\ndynamic program computing a mean (useful for benchmarking and evaluating known\nheuristics). Based on this dynamic program, we empirically study properties\nlike uniqueness and length of a mean. Moreover, experimental evaluations reveal\nsubstantial deficits of state-of-the-art heuristics in terms of their output\nquality. We also give an exact polynomial-time algorithm for the special case\nof binary time series.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 18:12:35 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 19:13:55 GMT"}, {"version": "v3", "created": "Thu, 31 May 2018 12:16:00 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Brill", "Markus", ""], ["Fluschnik", "Till", ""], ["Froese", "Vincent", ""], ["Jain", "Brijnesh", ""], ["Niedermeier", "Rolf", ""], ["Schultz", "David", ""]]}, {"id": "1710.08953", "submitter": "Frank Gurski", "authors": "Frank Gurski and Carolin Rehs", "title": "Counting and Enumerating Independent Sets with Applications to Knapsack\n  Problems", "comments": "26 pages; 9 Figures; 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce methods to count and enumerate all maximal independent, all\nmaximum independent sets, and all independent sets in threshold graphs and\nk-threshold graphs. Within threshold graphs and k-threshold graphs independent\nsets correspond to feasible solutions in related knapsack instances. We give\nseveral characterizations for knapsack instances and multidimensional knapsack\ninstances which allow an equivalent graph. This allows us to solve special\nknapsack instances as well as special multidimensional knapsack instances for\nfixed number of dimensions in polynomial time. We also conclude lower bounds on\nthe number of necessary bins within several bin packing problems.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 19:01:12 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Gurski", "Frank", ""], ["Rehs", "Carolin", ""]]}, {"id": "1710.09002", "submitter": "Jelena Diakonikolas", "authors": "Jelena Diakonikolas and Lorenzo Orecchia", "title": "Solving Packing and Covering LPs in $\\tilde{O}(\\frac{1}{\\epsilon^2})$\n  Distributed Iterations with a Single Algorithm and Simpler Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Packing and covering linear programs belong to the narrow class of linear\nprograms that are efficiently solvable in parallel and distributed models of\ncomputation, yet are a powerful modeling tool for a wide range of fundamental\nproblems in theoretical computer science, operations research, and many other\nareas. Following recent progress in obtaining faster distributed and parallel\nalgorithms for packing and covering linear programs, we present a simple\nalgorithm whose iteration count matches the best known\n$\\tilde{O}(\\frac{1}{\\epsilon^2})$ for this class of problems. The algorithm is\nsimilar to the algorithm of [Allen-Zhu and Orecchia, 2015], it can be\ninterpreted as Nesterov's dual averaging, and it constructs approximate\nsolutions to both primal (packing) and dual (covering) problems. However, the\nanalysis relies on the construction of an approximate optimality gap and a\nprimal-dual view, leading to a more intuitive interpretation. Moreover, our\nanalysis suggests that all existing algorithms for solving packing and covering\nlinear programs in parallel/distributed models of computation are, in fact,\nunaccelerated, and raises the question of designing accelerated algorithms for\nthis class of problems.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 21:54:31 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Diakonikolas", "Jelena", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1710.09422", "submitter": "Jessie Jamieson", "authors": "Robert A. Bridges, Jessie D. Jamieson, and Joel W. Reed", "title": "Setting the threshold for high throughput detectors: A mathematical\n  approach for ensembles of dynamic, heterogeneous, probabilistic anomaly\n  detectors", "comments": "11 pages, 5 figures. Proceedings of IEEE Big Data Conference, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection (AD) has garnered ample attention in security research, as\nsuch algorithms complement existing signature-based methods but promise\ndetection of never-before-seen attacks. Cyber operations manage a high volume\nof heterogeneous log data; hence, AD in such operations involves multiple\n(e.g., per IP, per data type) ensembles of detectors modeling heterogeneous\ncharacteristics (e.g., rate, size, type) often with adaptive online models\nproducing alerts in near real time. Because of high data volume, setting the\nthreshold for each detector in such a system is an essential yet underdeveloped\nconfiguration issue that, if slightly mistuned, can leave the system useless,\neither producing a myriad of alerts and flooding downstream systems, or giving\nnone. In this work, we build on the foundations of Ferragut et al. to provide a\nset of rigorous results for understanding the relationship between threshold\nvalues and alert quantities, and we propose an algorithm for setting the\nthreshold in practice. Specifically, we give an algorithm for setting the\nthreshold of multiple, heterogeneous, possibly dynamic detectors completely a\npriori, in principle. Indeed, if the underlying distribution of the incoming\ndata is known (closely estimated), the algorithm provides provably manageable\nthresholds. If the distribution is unknown (e.g., has changed over time) our\nanalysis reveals how the model distribution differs from the actual\ndistribution, indicating a period of model refitting is necessary. We provide\nempirical experiments showing the efficacy of the capability by regulating the\nalert rate of a system with $\\approx$2,500 adaptive detectors scoring over 1.5M\nevents in 5 hours. Further, we demonstrate on the real network data and\ndetection framework of Harshaw et al. the alternative case, showing how the\ninability to regulate alerts indicates the detection model is a bad fit to the\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 18:53:48 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Bridges", "Robert A.", ""], ["Jamieson", "Jessie D.", ""], ["Reed", "Joel W.", ""]]}, {"id": "1710.09595", "submitter": "Kamil Khadiev", "authors": "Kamil Khadiev, Aliya Khadieva, Dmitry Kravchenko, Alexander Rivosh,\n  Ramis Yamilov and Ilnaz Mannapov", "title": "Quantum versus Classical Online Streaming Algorithms with Logarithmic\n  Size of Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.FL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online algorithms with respect to the competitive ratio. Here, we\ninvestigate quantum and classical one-way automata with non-constant size of\nmemory (streaming algorithms) as a model for online algorithms. We construct\nproblems that can be solved by quantum online streaming algorithms better than\nby classical ones in a case of logarithmic or sublogarithmic size of memory.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 08:53:20 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 12:46:06 GMT"}, {"version": "v3", "created": "Thu, 20 Jun 2019 10:44:11 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Khadiev", "Kamil", ""], ["Khadieva", "Aliya", ""], ["Kravchenko", "Dmitry", ""], ["Rivosh", "Alexander", ""], ["Yamilov", "Ramis", ""], ["Mannapov", "Ilnaz", ""]]}, {"id": "1710.09605", "submitter": "Tim Zeitz", "authors": "Michael Hamann, Ben Strasser, Dorothea Wagner and Tim Zeitz", "title": "Distributed Graph Clustering using Modularity and Map Equation", "comments": "14 pages, 3 figures; v3: Camera ready for Euro-Par 2018, more\n  details, more results; v2: extended experiments to include comparison with\n  competing algorithms, shortened for submission to Euro-Par 2018", "journal-ref": null, "doi": "10.1007/978-3-319-96983-1_49", "report-no": null, "categories": "cs.DS cs.DC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study large-scale, distributed graph clustering. Given an undirected\ngraph, our objective is to partition the nodes into disjoint sets called\nclusters. A cluster should contain many internal edges while being sparsely\nconnected to other clusters. In the context of a social network, a cluster\ncould be a group of friends. Modularity and map equation are established\nformalizations of this internally-dense-externally-sparse principle. We present\ntwo versions of a simple distributed algorithm to optimize both measures. They\nare based on Thrill, a distributed big data processing framework that\nimplements an extended MapReduce model. The algorithms for the two measures,\nDSLM-Mod and DSLM-Map, differ only slightly. Adapting them for similar quality\nmeasures is straight-forward. We conduct an extensive experimental study on\nreal-world graphs and on synthetic benchmark graphs with up to 68 billion\nedges. Our algorithms are fast while detecting clusterings similar to those\ndetected by other sequential, parallel and distributed clustering algorithms.\nCompared to the distributed GossipMap algorithm, DSLM-Map needs less memory, is\nup to an order of magnitude faster and achieves better quality.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 09:24:40 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 15:55:27 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 13:09:41 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Hamann", "Michael", ""], ["Strasser", "Ben", ""], ["Wagner", "Dorothea", ""], ["Zeitz", "Tim", ""]]}, {"id": "1710.09795", "submitter": "Amirbehshad Shahrasbi", "authors": "Bernhard Haeupler, Amirbehshad Shahrasbi", "title": "Synchronization Strings: Explicit Constructions, Local Decoding, and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives new results for synchronization strings, a powerful\ncombinatorial object that allows to efficiently deal with insertions and\ndeletions in various communication settings:\n  $\\bullet$ We give a deterministic, linear time synchronization string\nconstruction, improving over an $O(n^5)$ time randomized construction.\nIndependently of this work, a deterministic $O(n\\log^2\\log n)$ time\nconstruction was just put on arXiv by Cheng, Li, and Wu. We also give a\ndeterministic linear time construction of an infinite synchronization string,\nwhich was not known to be computable before. Both constructions are highly\nexplicit, i.e., the $i^{th}$ symbol can be computed in $O(\\log i)$ time.\n  $\\bullet$ This paper also introduces a generalized notion we call\nlong-distance synchronization strings that allow for local and very fast\ndecoding. In particular, only $O(\\log^3 n)$ time and access to logarithmically\nmany symbols is required to decode any index.\n  We give several applications for these results:\n  $\\bullet$ For any $\\delta<1$ and $\\epsilon>0$ we provide an insdel correcting\ncode with rate $1-\\delta-\\epsilon$ which can correct any $O(\\delta)$ fraction\nof insdel errors in $O(n\\log^3n)$ time. This near linear computational\nefficiency is surprising given that we do not even know how to compute the\n(edit) distance between the decoding input and output in sub-quadratic time. We\nshow that such codes can not only efficiently recover from $\\delta$ fraction of\ninsdel errors but, similar to [Schulman, Zuckerman; TransInf'99], also from any\n$O(\\delta/\\log n)$ fraction of block transpositions and replications.\n  $\\bullet$ We show that highly explicitness and local decoding allow for\ninfinite channel simulations with exponentially smaller memory and decoding\ntime requirements. These simulations can be used to give the first near linear\ntime interactive coding scheme for insdel errors.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 16:31:56 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 03:08:14 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Shahrasbi", "Amirbehshad", ""]]}, {"id": "1710.09859", "submitter": "Guilherme Fran\\c{c}a", "authors": "Guilherme Fran\\c{c}a, Maria L. Rizzo, Joshua T. Vogelstein", "title": "Kernel k-Groups via Hartigan's Method", "comments": "several improvements; connections with community detection and\n  stochastic block model. Matches published version", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2020", "doi": "10.1109/TPAMI.2020.2998120", "report-no": null, "categories": "stat.ML cs.CV cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy statistics was proposed by Sz\\' ekely in the 80's inspired by Newton's\ngravitational potential in classical mechanics and it provides a model-free\nhypothesis test for equality of distributions. In its original form, energy\nstatistics was formulated in Euclidean spaces. More recently, it was\ngeneralized to metric spaces of negative type. In this paper, we consider a\nformulation for the clustering problem using a weighted version of energy\nstatistics in spaces of negative type. We show that this approach leads to a\nquadratically constrained quadratic program in the associated kernel space,\nestablishing connections with graph partitioning problems and kernel methods in\nmachine learning. To find local solutions of such an optimization problem, we\npropose kernel k-groups, which is an extension of Hartigan's method to kernel\nspaces. Kernel k-groups is cheaper than spectral clustering and has the same\ncomputational cost as kernel k-means (which is based on Lloyd's heuristic) but\nour numerical results show an improved performance, especially in higher\ndimensions. Moreover, we verify the efficiency of kernel k-groups in community\ndetection in sparse stochastic block models which has fascinating applications\nin several areas of science.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 18:38:28 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 14:02:55 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 15:29:58 GMT"}, {"version": "v4", "created": "Thu, 11 Jun 2020 19:57:09 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Fran\u00e7a", "Guilherme", ""], ["Rizzo", "Maria L.", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1710.09951", "submitter": "Justin Hsu", "authors": "Justin Hsu", "title": "Probabilistic Couplings for Probabilistic Reasoning", "comments": "PhD thesis, University of Pennsylvania, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis explores proofs by coupling from the perspective of formal\nverification. Long employed in probability theory and theoretical computer\nscience, these proofs construct couplings between the output distributions of\ntwo probabilistic processes. Couplings can imply various guarantees comparing\ntwo runs of a probabilistic computation. We first show that proofs in the\nprogram logic pRHL describe couplings. We formalize couplings that establish\nvarious probabilistic properties, including distribution equivalence,\nconvergence, and stochastic domination. Then we give a proofs-as-programs\ninterpretation: a coupling proof encodes a probabilistic product program, whose\nproperties imply relational properties of the original programs. We design the\nlogic xpRHL to construct the product, with extensions to model shift coupling\nand path coupling. We then propose an approximate version of probabilistic\ncoupling and a corresponding proof technique---proof by approximate\ncoupling---inspired by the logic apRHL, a version of pRHL for building\napproximate liftings. Drawing on ideas from existing privacy proofs, we extend\napRHL with novel proof rules for constructing new approximate couplings. We\ngive an approximate coupling proof of privacy for the Sparse Vector mechanism,\na well-known algorithm from the privacy literature whose privacy proof is\nnotoriously subtle, and produce the first formalized proof of privacy for\nSparse Vector in apRHL. Finally, we propose several more sophisticated\nconstructions for approximate couplings: a principle for showing\naccuracy-dependent privacy, a generalization of the advanced composition\ntheorem, and an optimal approximate coupling relating two subsets. We also show\nequivalences between our approximate couplings and other existing definitions.\nThese ingredients support the first formalized proof of privacy for the Between\nThresholds mechanism.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 00:13:45 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 00:44:30 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Hsu", "Justin", ""]]}, {"id": "1710.09961", "submitter": "Ata Turk", "authors": "Duru T\\\"urko\\u{g}lu, Ata Turk", "title": "Edge-Based Wedge Sampling to Estimate Triangle Counts in Very Large\n  Graphs", "comments": "ICDM 2017, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of triangles in a graph is useful to deduce a plethora of\nimportant features of the network that the graph is modeling. However, finding\nthe exact value of this number is computationally expensive. Hence, a number of\napproximation algorithms based on random sampling of edges, or wedges (adjacent\nedge pairs) have been proposed for estimating this value. We argue that for\nlarge sparse graphs with power-law degree distribution, random edge sampling\nrequires sampling large number of edges before providing enough information for\naccurate estimation, and existing wedge sampling methods lead to biased\nsamplings, which in turn lead to less accurate estimations. In this paper, we\npropose a hybrid algorithm between edge and wedge sampling that addresses the\ndeficiencies of both approaches. We start with uniform edge sampling and then\nextend each selected edge to form a wedge that is more informative for\nestimating the overall triangle count. The core estimate we make is the number\nof triangles each sampled edge in the first phase participates in. This\napproach provides accurate approximations with very small sampling ratios,\noutperforming the state-of-the-art up to 8 times in sample size while providing\nestimations with 95% confidence.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 01:29:02 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["T\u00fcrko\u011flu", "Duru", ""], ["Turk", "Ata", ""]]}, {"id": "1710.09988", "submitter": "Xian Wu", "authors": "Aaron Sidford, Mengdi Wang, Xian Wu, Yinyu Ye", "title": "Variance Reduced Value Iteration and Faster Algorithms for Solving\n  Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide faster algorithms for approximately solving\ndiscounted Markov Decision Processes in multiple parameter regimes. Given a\ndiscounted Markov Decision Process (DMDP) with $|S|$ states, $|A|$ actions,\ndiscount factor $\\gamma\\in(0,1)$, and rewards in the range $[-M, M]$, we show\nhow to compute an $\\epsilon$-optimal policy, with probability $1 - \\delta$ in\ntime \\[ \\tilde{O}\\left( \\left(|S|^2 |A| + \\frac{|S| |A|}{(1 - \\gamma)^3}\n\\right)\n  \\log\\left( \\frac{M}{\\epsilon} \\right) \\log\\left( \\frac{1}{\\delta} \\right)\n\\right) ~ . \\] This contribution reflects the first nearly linear time, nearly\nlinearly convergent algorithm for solving DMDPs for intermediate values of\n$\\gamma$.\n  We also show how to obtain improved sublinear time algorithms provided we can\nsample from the transition function in $O(1)$ time. Under this assumption we\nprovide an algorithm which computes an $\\epsilon$-optimal policy with\nprobability $1 - \\delta$ in time \\[ \\tilde{O} \\left(\\frac{|S| |A| M^2}{(1 -\n\\gamma)^4 \\epsilon^2} \\log \\left(\\frac{1}{\\delta}\\right) \\right) ~. \\]\n  Lastly, we extend both these algorithms to solve finite horizon MDPs. Our\nalgorithms improve upon the previous best for approximately computing optimal\npolicies for fixed-horizon MDPs in multiple parameter regimes.\n  Interestingly, we obtain our results by a careful modification of approximate\nvalue iteration. We show how to combine classic approximate value iteration\nanalysis with new techniques in variance reduction. Our fastest algorithms\nleverage further insights to ensure that our algorithms make monotonic progress\ntowards the optimal value. This paper is one of few instances in using sampling\nto obtain a linearly convergent linear programming algorithm and we hope that\nthe analysis may be useful more broadly.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 04:44:24 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 13:59:02 GMT"}, {"version": "v3", "created": "Wed, 23 Dec 2020 05:24:40 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Sidford", "Aaron", ""], ["Wang", "Mengdi", ""], ["Wu", "Xian", ""], ["Ye", "Yinyu", ""]]}, {"id": "1710.10026", "submitter": "Somenath Biswas", "authors": "Debojyoti Dey, Pranjal Dutta, and Somenath Biswas", "title": "A note on faithful coupling of Markov chains", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One often needs to turn a coupling $(X_i, Y_i)_{i\\geq 0}$ of a Markov chain\ninto a sticky coupling where once $X_T = Y_T$ at some $T$, then from then on,\nat each subsequent time step $T'\\geq T$, we shall have $X_{T'} = Y_{T'}$.\nHowever, not all of what are considered couplings in literature, even Markovian\ncouplings, can be turned into sticky couplings, as proved by Rosenthal through\na counter example. Rosenthal then proposed a strengthening of the Markovian\ncoupling notion, termed as faithful coupling, from which a sticky coupling can\nindeed be obtained. We identify the reason why a sticky coupling could not be\nobtained in the counter example of Rosenthal, which motivates us to define a\ntype of coupling which can obviously be turned into a sticky coupling. We show\nthen that the new type of coupling that we define, and the faithful coupling as\ndefined by Rosenthal, are actually identical. Our note may be seen as a\ndemonstration of the naturalness of the notion of faithful coupling.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 08:37:26 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Dey", "Debojyoti", ""], ["Dutta", "Pranjal", ""], ["Biswas", "Somenath", ""]]}, {"id": "1710.10057", "submitter": "Huang Lingxiao", "authors": "L. Elisa Celis, Lingxiao Huang, Nisheeth K. Vishnoi", "title": "Multiwinner Voting with Fairness Constraints", "comments": "The conference version of this paper appears in IJCAI-ECAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiwinner voting rules are used to select a small representative subset of\ncandidates or items from a larger set given the preferences of voters. However,\nif candidates have sensitive attributes such as gender or ethnicity (when\nselecting a committee), or specified types such as political leaning (when\nselecting a subset of news items), an algorithm that chooses a subset by\noptimizing a multiwinner voting rule may be unbalanced in its selection -- it\nmay under or over represent a particular gender or political orientation in the\nexamples above. We introduce an algorithmic framework for multiwinner voting\nproblems when there is an additional requirement that the selected subset\nshould be \"fair\" with respect to a given set of attributes. Our framework\nprovides the flexibility to (1) specify fairness with respect to multiple,\nnon-disjoint attributes (e.g., ethnicity and gender) and (2) specify a score\nfunction. We study the computational complexity of this constrained multiwinner\nvoting problem for monotone and submodular score functions and present several\napproximation algorithms and matching hardness of approximation results for\nvarious attribute group structure and types of score functions. We also present\nsimulations that suggest that adding fairness constraints may not affect the\nscores significantly when compared to the unconstrained case.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 10:13:31 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 19:19:15 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Celis", "L. Elisa", ""], ["Huang", "Lingxiao", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1710.10091", "submitter": "Mathias Rav", "authors": "Lars Arge, Mathias Rav, Svend C. Svendsen, Jakob Truelsen", "title": "External Memory Pipelining Made Easy With TPIE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When handling large datasets that exceed the capacity of the main memory,\nmovement of data between main memory and external memory (disk), rather than\nactual (CPU) computation time, is often the bottleneck in the computation.\nSince data is moved between disk and main memory in large contiguous blocks,\nthis has led to the development of a large number of I/O-efficient algorithms\nthat minimize the number of such block movements.\n  TPIE is one of two major libraries that have been developed to support\nI/O-efficient algorithm implementations. TPIE provides an interface where list\nstream processing and sorting can be implemented in a simple and modular way\nwithout having to worry about memory management or block movement. However, if\ncare is not taken, such streaming-based implementations can lead to practically\ninefficient algorithms since lists of data items are typically written to (and\nread from) disk between components.\n  In this paper we present a major extension of the TPIE library that includes\na pipelining framework that allows for practically efficient streaming-based\nimplementations while minimizing I/O-overhead between streaming components. The\nframework pipelines streaming components to avoid I/Os between components, that\nis, it processes several components simultaneously while passing output from\none component directly to the input of the next component in main memory. TPIE\nautomatically determines which components to pipeline and performs the required\nmain memory management, and the extension also includes support for\nparallelization of internal memory computation and progress tracking across an\nentire application. The extended library has already been used to evaluate\nI/O-efficient algorithms in the research literature and is heavily used in\nI/O-efficient commercial terrain processing applications by the Danish startup\nSCALGO.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 11:52:14 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Arge", "Lars", ""], ["Rav", "Mathias", ""], ["Svendsen", "Svend C.", ""], ["Truelsen", "Jakob", ""]]}, {"id": "1710.10105", "submitter": "Felipe A. Louza", "authors": "Felipe A. Louza, W. F. Smyth, Giovanni Manzini, Guilherme P. Telles", "title": "Lyndon Array Construction during Burrows-Wheeler Inversion", "comments": null, "journal-ref": "Journal of Discrete Algorithms, 50 (2018), 2-9", "doi": "10.1016/j.jda.2018.08.001", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an algorithm to compute the Lyndon array of a string\n$T$ of length $n$ as a byproduct of the inversion of the Burrows-Wheeler\ntransform of $T$. Our algorithm runs in linear time using only a stack in\naddition to the data structures used for Burrows-Wheeler inversion. We compare\nour algorithm with two other linear-time algorithms for Lyndon array\nconstruction and show that computing the Burrows-Wheeler transform and then\nconstructing the Lyndon array is competitive compared to the known approaches.\nWe also propose a new balanced parenthesis representation for the Lyndon array\nthat uses $2n+o(n)$ bits of space and supports constant time access. This\nrepresentation can be built in linear time using $O(n)$ words of space, or in\n$O(n\\log n/\\log\\log n)$ time using asymptotically the same space as $T$.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 12:37:14 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Louza", "Felipe A.", ""], ["Smyth", "W. F.", ""], ["Manzini", "Giovanni", ""], ["Telles", "Guilherme P.", ""]]}, {"id": "1710.10339", "submitter": "Patrick Girardet", "authors": "Kevin K. H. Cheung, Patrick D. Girardet", "title": "Improved approximation of layout problems on random graphs", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Inspired by previous work of Diaz, Petit, Serna, and Trevisan (Approximating\nlayout problems on random graphs Discrete Mathematics, 235, 2001, 245--253), we\nshow that several well-known graph layout problems are approximable to within a\nfactor arbitrarily close to 1 of the optimal with high probability for random\ngraphs drawn from an Erd\\\"os-Renyi distribution with appropriate sparsity\nconditions. Moreover, we show that the same results hold for the analogous\nproblems on directed acyclic graphs.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 21:36:13 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Cheung", "Kevin K. H.", ""], ["Girardet", "Patrick D.", ""]]}, {"id": "1710.10457", "submitter": "Wenzheng Li", "authors": "Shichuan Deng, Wenzheng Li, Xuan Wu", "title": "Wasserstein Identity Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniformity testing and the more general identity testing are well studied\nproblems in distributional property testing. Most previous work focuses on\ntesting under $L_1$-distance. However, when the support is very large or even\ncontinuous, testing under $L_1$-distance may require a huge (even infinite)\nnumber of samples. Motivated by such issues, we consider the identity testing\nin Wasserstein distance (a.k.a. transportation distance and earthmover\ndistance) on a metric space (discrete or continuous).\n  In this paper, we propose the Wasserstein identity testing problem (Identity\nTesting in Wasserstein distance). We obtain nearly optimal worst-case sample\ncomplexity for the problem. Moreover, for a large class of probability\ndistributions satisfying the so-called \"Doubling Condition\", we provide nearly\ninstance-optimal sample complexity.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 12:39:40 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Deng", "Shichuan", ""], ["Li", "Wenzheng", ""], ["Wu", "Xuan", ""]]}, {"id": "1710.10545", "submitter": "Hadley Black", "authors": "Hadley Black, Deeparnab Chakrabarty, C. Seshadhri", "title": "A $o(d) \\cdot \\text{polylog}~n$ Monotonicity Tester for Boolean\n  Functions over the Hypergrid $[n]^d$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study monotonicity testing of Boolean functions over the hypergrid $[n]^d$\nand design a non-adaptive tester with $1$-sided error whose query complexity is\n$\\tilde{O}(d^{5/6})\\cdot \\text{poly}(\\log n,1/\\epsilon)$. Previous to our work,\nthe best known testers had query complexity linear in $d$ but independent of\n$n$. We improve upon these testers as long as $n = 2^{d^{o(1)}}$.\n  To obtain our results, we work with what we call the augmented hypergrid,\nwhich adds extra edges to the hypergrid. Our main technical contribution is a\nMargulis-style isoperimetric result for the augmented hypergrid, and our\ntester, like previous testers for the hypercube domain, performs directed\nrandom walks on this structure.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 01:00:12 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Black", "Hadley", ""], ["Chakrabarty", "Deeparnab", ""], ["Seshadhri", "C.", ""]]}, {"id": "1710.10592", "submitter": "Soheil Behnezhad", "authors": "Soheil Behnezhad, Nima Reyhani", "title": "Almost Optimal Stochastic Weighted Matching With Few Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the {\\em stochastic matching} problem. An edge-weighted general\n(i.e., not necessarily bipartite) graph $G(V, E)$ is given in the input, where\neach edge in $E$ is {\\em realized} independently with probability $p$; the\nrealization is initially unknown, however, we are able to {\\em query} the edges\nto determine whether they are realized. The goal is to query only a small\nnumber of edges to find a {\\em realized matching} that is sufficiently close to\nthe maximum matching among all realized edges. This problem has received a\nconsiderable attention during the past decade due to its numerous real-world\napplications in kidney-exchange, matchmaking services, online labor markets,\nand advertisements.\n  Our main result is an {\\em adaptive} algorithm that for any arbitrarily small\n$\\epsilon > 0$, finds a $(1-\\epsilon)$-approximation in expectation, by\nquerying only $O(1)$ edges per vertex. We further show that our approach leads\nto a $(1/2-\\epsilon)$-approximate {\\em non-adaptive} algorithm that also\nqueries only $O(1)$ edges per vertex. Prior to our work, no nontrivial\napproximation was known for weighted graphs using a constant per-vertex budget.\nThe state-of-the-art adaptive (resp. non-adaptive) algorithm of Maehara and\nYamaguchi [SODA 2018] achieves a $(1-\\epsilon)$-approximation (resp.\n$(1/2-\\epsilon)$-approximation) by querying up to $O(w\\log{n})$ edges per\nvertex where $w$ denotes the maximum integer edge-weight. Our result is a\nsubstantial improvement over this bound and has an appealing message: No matter\nwhat the structure of the input graph is, one can get arbitrarily close to the\noptimum solution by querying only a constant number of edges per vertex.\n  To obtain our results, we introduce novel properties of a generalization of\n{\\em augmenting paths} to weighted matchings that may be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 10:21:39 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 07:40:18 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 07:52:00 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Behnezhad", "Soheil", ""], ["Reyhani", "Nima", ""]]}, {"id": "1710.10655", "submitter": "Lalit Jain", "authors": "Anna C. Gilbert, Lalit Jain", "title": "If it ain't broke, don't fix it: Sparse metric repair", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern data-intensive computational problems either require, or benefit\nfrom distance or similarity data that adhere to a metric. The algorithms run\nfaster or have better performance guarantees. Unfortunately, in real\napplications, the data are messy and values are noisy. The distances between\nthe data points are far from satisfying a metric. Indeed, there are a number of\ndifferent algorithms for finding the closest set of distances to the given ones\nthat also satisfy a metric (sometimes with the extra condition of being\nEuclidean). These algorithms can have unintended consequences, they can change\na large number of the original data points, and alter many other features of\nthe data.\n  The goal of sparse metric repair is to make as few changes as possible to the\noriginal data set or underlying distances so as to ensure the resulting\ndistances satisfy the properties of a metric. In other words, we seek to\nminimize the sparsity (or the $\\ell_0$ \"norm\") of the changes we make to the\ndistances subject to the new distances satisfying a metric. We give three\ndifferent combinatorial algorithms to repair a metric sparsely. In one setting\nthe algorithm is guaranteed to return the sparsest solution and in the other\nsettings, the algorithms repair the metric. Without prior information, the\nalgorithms run in time proportional to the cube of the number of input data\npoints and, with prior information we can reduce the running time considerably.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 17:48:01 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Gilbert", "Anna C.", ""], ["Jain", "Lalit", ""]]}, {"id": "1710.10660", "submitter": "Cl\\'ement Canonne", "authors": "Omri Ben-Eliezer, Cl\\'ement L. Canonne", "title": "Improved Bounds for Testing Forbidden Order Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sequence $f\\colon\\{1,\\dots,n\\}\\to\\mathbb{R}$ contains a permutation $\\pi$\nof length $k$ if there exist $i_1<\\dots<i_k$ such that, for all $x,y$,\n$f(i_x)<f(i_y)$ if and only if $\\pi(x)<\\pi(y)$; otherwise, $f$ is said to be\n$\\pi$-free. In this work, we consider the problem of testing for $\\pi$-freeness\nwith one-sided error, continuing the investigation of [Newman et al., SODA'17].\n  We demonstrate a surprising behavior for non-adaptive tests with one-sided\nerror: While a trivial sampling-based approach yields an $\\varepsilon$-test for\n$\\pi$-freeness making $\\Theta(\\varepsilon^{-1/k} n^{1-1/k})$ queries, our lower\nbounds imply that this is almost optimal for most permutations! Specifically,\nfor most permutations $\\pi$ of length $k$, any non-adaptive one-sided\n$\\varepsilon$-test requires\n$\\varepsilon^{-1/(k-\\Theta(1))}n^{1-1/(k-\\Theta(1))}$ queries; furthermore, the\npermutations that are hardest to test require\n$\\Theta(\\varepsilon^{-1/(k-1)}n^{1-1/(k-1)})$ queries, which is tight in $n$\nand $\\varepsilon$.\n  Additionally, we show two hierarchical behaviors here. First, for any $k$ and\n$l\\leq k-1$, there exists some $\\pi$ of length $k$ that requires\n$\\tilde{\\Theta}_{\\varepsilon}(n^{1-1/l})$ non-adaptive queries. Second, we show\nan adaptivity hierarchy for $\\pi=(1,3,2)$ by proving upper and lower bounds for\n(one- and two-sided) testing of $\\pi$-freeness with $r$ rounds of adaptivity.\nThe results answer open questions of Newman et al. and [Canonne and Gur,\nCCC'17].\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 18:06:27 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Ben-Eliezer", "Omri", ""], ["Canonne", "Cl\u00e9ment L.", ""]]}, {"id": "1710.10836", "submitter": "Jithin Mathews", "authors": "Jithin Mathews, Priya Mehta, S.V. Kasi Visweswara Rao, Ch. Sobhan Babu", "title": "An algorithmic approach to handle circular trading in commercial taxing\n  system", "comments": "10 pages, 7 figures, 1 table, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tax manipulation comes in a variety of forms with different motivations and\nof varying complexities. In this paper, we deal with a specific technique used\nby tax-evaders known as circular trading. In particular, we define algorithms\nfor the detection and analysis of circular trade. To achieve this, we have\nmodelled the whole system as a directed graph with the actors being vertices\nand the transactions among them as directed edges. We illustrate the results\nobtained after running the proposed algorithm on the commercial tax dataset of\nthe government of Telangana, India, which contains the transaction details of a\nset of participants involved in a known circular trade.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 10:00:26 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 09:29:37 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Mathews", "Jithin", ""], ["Mehta", "Priya", ""], ["Rao", "S. V. Kasi Visweswara", ""], ["Babu", "Ch. Sobhan", ""]]}, {"id": "1710.10904", "submitter": "Kai Wang", "authors": "G. Calinescu (1), F. Jaehn (2), M. Li (3) and K. Wang (3) ((1)\n  Department of Computer Science, Illinois Institute of Technology, Chicago, IL\n  60616, USA, (2) Management Science and Operations Research, Helmut Schmidt\n  University -- University of the Federal Armed Forces Hamburg, Holstenhofweg\n  85, D-22043 Hamburg, Germany, (3) Department of Computer Science, City\n  University of Hong Kong, 83 Tat Chee Avenue, Kowloon, Hong Kong SAR, China)", "title": "An FPTAS of Minimizing Total Weighted Completion Time on Single Machine\n  with Position Constraint", "comments": "13 pages, The 28th International Symposium on Algorithms and\n  Computation (ISAAC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the classical scheduling problem of minimizing the\ntotal weighted completion time on a single machine with the constraint that one\nspecific job must be scheduled at a specified position. We give dynamic\nprograms with pseudo-polynomial running time, and a fully polynomial-time\napproximation scheme (FPTAS).\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 12:43:47 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Calinescu", "G.", ""], ["Jaehn", "F.", ""], ["Li", "M.", ""], ["Wang", "K.", ""]]}, {"id": "1710.10964", "submitter": "Dominik Kempa", "authors": "Dominik Kempa and Nicola Prezza", "title": "At the Roots of Dictionary Compression: String Attractors", "comments": "In Proceedings of 50th Annual ACM SIGACT Symposium on the Theory of\n  Computing (STOC'18)", "journal-ref": null, "doi": "10.1145/3188745.3188814", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known fact in the field of lossless text compression is that\nhigh-order entropy is a weak model when the input contains long repetitions.\nMotivated by this, decades of research have generated myriads of so-called\ndictionary compressors: algorithms able to reduce the text's size by exploiting\nits repetitiveness. Lempel-Ziv 77 is one of the most successful and well-known\ntools of this kind, followed by straight-line programs, run-length\nBurrows-Wheeler transform, macro schemes, collage systems, and the compact\ndirected acyclic word graph. In this paper, we show that these techniques are\ndifferent solutions to the same, elegant, combinatorial problem: to find a\nsmall set of positions capturing all text's substrings. We call such a set a\nstring attractor. We first show reductions between dictionary compressors and\nstring attractors. This gives the approximation ratios of dictionary\ncompressors with respect to the smallest string attractor and uncovers new\nrelations between the output sizes of different compressors. We show that the\n$k$-attractor problem: deciding whether a text has a size-$t$ set of positions\ncapturing substrings of length at most $k$, is NP-complete for $k\\geq 3$. We\nprovide several approximation techniques for the smallest $k$-attractor, show\nthat the problem is APX-complete for constant $k$, and give strong\ninapproximability results. To conclude, we provide matching lower and upper\nbounds for the random access problem on string attractors. The upper bound is\nproved by showing a data structure supporting queries in optimal time. Our data\nstructure is universal: by our reductions to string attractors, it supports\nrandom access on any dictionary-compression scheme. In particular, it matches\nthe lower bound also on LZ77, straight-line programs, collage systems, and\nmacro schemes, and therefore closes (at once) the random access problem for all\nthese compressors.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 14:24:35 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 14:19:42 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 10:28:19 GMT"}, {"version": "v4", "created": "Tue, 28 May 2019 15:16:16 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Kempa", "Dominik", ""], ["Prezza", "Nicola", ""]]}, {"id": "1710.10979", "submitter": "Petr Golovach", "authors": "Petr A. Golovach, Pinar Heggernes, Paloma Lima, and Pedro Montealegre", "title": "Finding Connected Secluded Subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems related to finding induced subgraphs satisfying given properties\nform one of the most studied areas within graph algorithms. Such problems have\ngiven rise to breakthrough results and led to development of new techniques\nboth within the traditional P vs NP dichotomy and within parameterized\ncomplexity. The \\Pi-Subgraph problem asks whether an input graph contains an\ninduced subgraph on at least k vertices satisfying graph property \\Pi. For many\napplications, it is desirable that the found subgraph has as few connections to\nthe rest of the graph as possible, which gives rise to the Secluded\n\\Pi-Subgraph problem. Here, input k is the size of the desired subgraph, and\ninput t is a limit on the number of neighbors this subgraph has in the rest of\nthe graph. This problem has been studied from a parameterized perspective, and\nunfortunately it turns out to be W[1]-hard for many graph properties \\Pi, even\nwhen parameterized by k+t. We show that the situation changes when we are\nlooking for a connected induced subgraph satisfying \\Pi. In particular, we show\nthat the Connected \\Pi-Secluded Subgraph problem is FPT when parameterized by\njust t for many important graph properties \\Pi.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 14:34:45 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Golovach", "Petr A.", ""], ["Heggernes", "Pinar", ""], ["Lima", "Paloma", ""], ["Montealegre", "Pedro", ""]]}, {"id": "1710.11200", "submitter": "Renato J Cintra", "authors": "N. Rajapaksha, A. Madanayake, R. J. Cintra, J. Adikari, V. S. Dimitrov", "title": "VLSI Computational Architectures for the Arithmetic Cosine Transform", "comments": "8 pages, 2 figures, 6 tables", "journal-ref": "IEEE Transactions on Computers, vol. 64, no. 9, Sep 2015", "doi": "10.1109/TC.2014.2366732", "report-no": null, "categories": "cs.AR cs.DS cs.MM math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrete cosine transform (DCT) is a widely-used and important signal\nprocessing tool employed in a plethora of applications. Typical fast algorithms\nfor nearly-exact computation of DCT require floating point arithmetic, are\nmultiplier intensive, and accumulate round-off errors. Recently proposed fast\nalgorithm arithmetic cosine transform (ACT) calculates the DCT exactly using\nonly additions and integer constant multiplications, with very low area\ncomplexity, for null mean input sequences. The ACT can also be computed\nnon-exactly for any input sequence, with low area complexity and low power\nconsumption, utilizing the novel architecture described. However, as a\ntrade-off, the ACT algorithm requires 10 non-uniformly sampled data points to\ncalculate the 8-point DCT. This requirement can easily be satisfied for\napplications dealing with spatial signals such as image sensors and biomedical\nsensor arrays, by placing sensor elements in a non-uniform grid. In this work,\na hardware architecture for the computation of the null mean ACT is proposed,\nfollowed by a novel architectures that extend the ACT for non-null mean\nsignals. All circuits are physically implemented and tested using the Xilinx\nXC6VLX240T FPGA device and synthesized for 45 nm TSMC standard-cell library for\nperformance assessment.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 19:06:19 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Rajapaksha", "N.", ""], ["Madanayake", "A.", ""], ["Cintra", "R. J.", ""], ["Adikari", "J.", ""], ["Dimitrov", "V. S.", ""]]}, {"id": "1710.11213", "submitter": "Sahil Singla", "authors": "Soheil Ehsani, MohammadTaghi Hajiaghayi, Thomas Kesselheim, and Sahil\n  Singla", "title": "Prophet Secretary for Combinatorial Auctions and Matroids", "comments": "Preliminary version appeared in SODA 2018. This version improves the\n  writeup on Fixed-Threshold algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The secretary and the prophet inequality problems are central to the field of\nStopping Theory. Recently, there has been a lot of work in generalizing these\nmodels to multiple items because of their applications in mechanism design. The\nmost important of these generalizations are to matroids and to combinatorial\nauctions (extends bipartite matching). Kleinberg-Weinberg \\cite{KW-STOC12} and\nFeldman et al. \\cite{feldman2015combinatorial} show that for adversarial\narrival order of random variables the optimal prophet inequalities give a\n$1/2$-approximation. For many settings, however, it's conceivable that the\narrival order is chosen uniformly at random, akin to the secretary problem. For\nsuch a random arrival model, we improve upon the $1/2$-approximation and obtain\n$(1-1/e)$-approximation prophet inequalities for both matroids and\ncombinatorial auctions. This also gives improvements to the results of Yan\n\\cite{yan2011mechanism} and Esfandiari et al. \\cite{esfandiari2015prophet} who\nworked in the special cases where we can fully control the arrival order or\nwhen there is only a single item.\n  Our techniques are threshold based. We convert our discrete problem into a\ncontinuous setting and then give a generic template on how to dynamically\nadjust these thresholds to lower bound the expected total welfare.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 19:41:38 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2018 17:13:41 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Ehsani", "Soheil", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Kesselheim", "Thomas", ""], ["Singla", "Sahil", ""]]}, {"id": "1710.11250", "submitter": "Greg Bodwin", "authors": "Amir Abboud, Greg Bodwin", "title": "Reachability Preservers: New Extremal Bounds and Approximation\n  Algorithms", "comments": "To appear in SODA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we prove new results about the extremal structure of paths in\ndirected graphs. Say we are given a directed graph $G = (V, E)$ on $n$ nodes, a\nset of sources $S \\subseteq V$ of size $|S|=n^{1/3}$, and a subset $P \\subseteq\nS \\times V$ of pairs $(s,t)$ where $s \\in S$, of size $O(n^{2/3})$, such that\nfor all pairs $(s,t) \\in P$, there is a path from $s$ to $t$. Our goal is to\nremove as many edges from $G$ as possible while maintaining the reachability of\nall pairs in $P$. How many edges will we have to keep? Can you always go down\nto $n^{1+o(1)}$ edges? Or maybe for some nasty graphs $G$ you cannot even go\nbelow the simple bound of $O(n^{4/3})$ edges?\n  In this paper, we make polynomial progress in both the upper and lower bounds\nfor these Reachability Preservers over bounds that were implicit in the\nliterature. We show that in the above scenario, $O(n)$ edges will always be\nsufficient, and in general one is even guaranteed a subgraph on $O(n+\\sqrt{n\n\\cdot |P|\\cdot |S|})$ edges that preserves the reachability of all pairs in\n$P$. We complement this with a lower bound graph construction, establishing\nthat the above result fully characterizes the settings in which we are\nguaranteed a preserver of size $O(n)$. Moreover, we design an efficient\nalgorithm that can always compute a preserver of existentially optimal size.\n  The second contribution of this paper is a new connection between extremal\ngraph sparsification results and classical Steiner Network Design problems.\nSurprisingly, prior to this work, the osmosis of techniques between these two\nfields had been superficial. This allows us to improve the state of the art\napproximation algorithms for the most basic Steiner-type problem in directed\ngraphs from the $O(n^{0.6+\\varepsilon})$ of Chlamatac, Dinitz, Kortsarz, and\nLaekhanukit (SODA'17) to $O(n^{0.577+\\varepsilon})$.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 21:32:21 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Abboud", "Amir", ""], ["Bodwin", "Greg", ""]]}, {"id": "1710.11253", "submitter": "Pavel Kolev", "authors": "Karl Bringmann, Pavel Kolev, David P. Woodruff", "title": "Approximation Algorithms for $\\ell_0$-Low Rank Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the $\\ell_0$-Low Rank Approximation Problem, where the goal is,\ngiven an $m \\times n$ matrix $A$, to output a rank-$k$ matrix $A'$ for which\n$\\|A'-A\\|_0$ is minimized. Here, for a matrix $B$, $\\|B\\|_0$ denotes the number\nof its non-zero entries. This NP-hard variant of low rank approximation is\nnatural for problems with no underlying metric, and its goal is to minimize the\nnumber of disagreeing data positions. We provide approximation algorithms which\nsignificantly improve the running time and approximation factor of previous\nwork. For $k > 1$, we show how to find, in poly$(mn)$ time for every $k$, a\nrank $O(k \\log(n/k))$ matrix $A'$ for which $\\|A'-A\\|_0 \\leq O(k^2 \\log(n/k))\n\\mathrm{OPT}$. To the best of our knowledge, this is the first algorithm with\nprovable guarantees for the $\\ell_0$-Low Rank Approximation Problem for $k >\n1$, even for bicriteria algorithms. For the well-studied case when $k = 1$, we\ngive a $(2+\\epsilon)$-approximation in {\\it sublinear time}, which is\nimpossible for other variants of low rank approximation such as for the\nFrobenius norm. We strengthen this for the well-studied case of binary matrices\nto obtain a $(1+O(\\psi))$-approximation in sublinear time, where $\\psi =\n\\mathrm{OPT}/\\lVert A\\rVert_0$. For small $\\psi$, our approximation factor is\n$1+o(1)$.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 21:49:48 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 15:06:01 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Bringmann", "Karl", ""], ["Kolev", "Pavel", ""], ["Woodruff", "David P.", ""]]}, {"id": "1710.11306", "submitter": "Panos P. Markopoulos", "authors": "Panos P. Markopoulos, Dimitris G. Chachlakis, and Evangelos E.\n  Papalexakis", "title": "The Exact Solution to Rank-1 L1-norm TUCKER2 Decomposition", "comments": "This is a preprint; An edited/finalized version of this manuscript\n  has been submitted for publication to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2018.2790901", "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study rank-1 {L1-norm-based TUCKER2} (L1-TUCKER2) decomposition of 3-way\ntensors, treated as a collection of $N$ $D \\times M$ matrices that are to be\njointly decomposed. Our contributions are as follows. i) We prove that the\nproblem is equivalent to combinatorial optimization over $N$ antipodal-binary\nvariables. ii) We derive the first two algorithms in the literature for its\nexact solution. The first algorithm has cost exponential in $N$; the second one\nhas cost polynomial in $N$ (under a mild assumption). Our algorithms are\naccompanied by formal complexity analysis. iii) We conduct numerical studies to\ncompare the performance of exact L1-TUCKER2 (proposed) with standard HOSVD,\nHOOI, GLRAM, PCA, L1-PCA, and TPCA-L1. Our studies show that L1-TUCKER2\noutperforms (in tensor approximation) all the above counterparts when the\nprocessed data are outlier corrupted.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 03:01:35 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Markopoulos", "Panos P.", ""], ["Chachlakis", "Dimitris G.", ""], ["Papalexakis", "Evangelos E.", ""]]}, {"id": "1710.11414", "submitter": "Koji M. Kobayashi", "authors": "Koji M. Kobayashi", "title": "Improved Bounds for Online Dominating Sets of Trees", "comments": "An extended abstract of this paper appears in Proc. of ISAAC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The online dominating set problem is an online variant of the minimum\ndominating set problem, which is one of the most important NP-hard problems on\ngraphs. This problem is defined as follows: Given an undirected graph $G = (V,\nE)$, in which $V$ is a set of vertices and $E$ is a set of edges. We say that a\nset $D \\subseteq V$ of vertices is a {\\em dominating set} of $G$ if for each $v\n\\in V \\setminus D$, there exists a vertex $u \\in D$ such that $\\{ u, v \\} \\in\nE$. The vertices are revealed to an online algorithm one by one over time. When\na vertex is revealed, edges between the vertex and vertices revealed in the\npast are also revealed. A revelaed subtree is connected at any time.\nImmediately after the revelation of each vertex, an online algorithm can choose\nvertices which were already revealed irrevocably and must maintain a dominating\nset of a graph revealed so far. The cost of an algorithm on a given tree is the\nnumber of vertices chosen by it, and its objective is to minimize the cost.\nEidenbenz (Technical report, Institute of Theoretical Computer Science, ETH\nZ\\\"{u}rich, 2002) and Boyar et al.\\ (SWAT 2016) studied the case in which given\ngraphs are trees. They designed a deterministic online algorithm whose\ncompetitive ratio is at most three, and proved that a lower bound on the\ncompetitive ratio of any deterministic algorithm is two. In this paper, we also\nfocus on trees. We establish a matching lower bound for any deterministic\nalgorithm. Moreover, we design a randomized online algorithm whose competitive\nratio is at most $5/2 = 2.5$, and show that the competitive ratio of any\nrandomized algorithm is at least $4/3 \\approx 1.333$.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 11:42:06 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Kobayashi", "Koji M.", ""]]}, {"id": "1710.11462", "submitter": "Pratik Ghosal", "authors": "Pratik Ghosal and Katarzyna Paluch", "title": "Manipulation Strategies for the Rank Maximal Matching Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider manipulation strategies for the rank-maximal matching problem. In\nthe rank-maximal matching problem we are given a bipartite graph $G = (A \\cup\nP, E)$ such that $A$ denotes a set of applicants and $P$ a set of posts. Each\napplicant $a \\in A$ has a preference list over the set of his neighbours in\n$G$, possibly involving ties. Preference lists are represented by ranks on the\nedges - an edge $(a,p)$ has rank $i$, denoted as $rank(a,p)=i$, if post $p$\nbelongs to one of $a$'s $i$-th choices. A rank-maximal matching is one in which\nthe maximum number of applicants is matched to their rank one posts and subject\nto this condition, the maximum number of applicants is matched to their rank\ntwo posts, and so on. A rank-maximal matching can be computed in $O(\\min(c\n\\sqrt{n},n) m)$ time, where $n$ denotes the number of applicants, $m$ the\nnumber of edges and $c$ the maximum rank of an edge in an optimal solution.\n  A central authority matches applicants to posts. It does so using one of the\nrank-maximal matchings. Since there may be more than one rank- maximal matching\nof $G$, we assume that the central authority chooses any one of them randomly.\nLet $a_1$ be a manipulative applicant, who knows the preference lists of all\nthe other applicants and wants to falsify his preference list so that he has a\nchance of getting better posts than if he were truthful. In the first problem\naddressed in this paper the manipulative applicant $a_1$ wants to ensure that\nhe is never matched to any post worse than the most preferred among those of\nrank greater than one and obtainable when he is truthful. In the second problem\nthe manipulator wants to construct such a preference list that the worst post\nhe can become matched to by the central authority is best possible or in other\nwords, $a_1$ wants to minimize the maximal rank of a post he can become matched\nto.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 13:35:56 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 14:32:34 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Ghosal", "Pratik", ""], ["Paluch", "Katarzyna", ""]]}, {"id": "1710.11513", "submitter": "Florian Sikora", "authors": "\\'Edouard Bonnet, Pawe{\\l} Rz\\k{a}\\.zewski, Florian Sikora", "title": "Designing RNA Secondary Structures is Hard", "comments": "Version accepted in RECOMB 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO q-bio.BM q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An RNA sequence is a word over an alphabet on four elements $\\{A,C,G,U\\}$\ncalled bases. RNA sequences fold into secondary structures where some bases\nmatch one another while others remain unpaired. Pseudoknot-free secondary\nstructures can be represented as well-parenthesized expressions with additional\ndots, where pairs of matching parentheses symbolize paired bases and dots,\nunpaired bases. The two fundamental problems in RNA algorithmic are to predict\nhow sequences fold within some model of energy and to design sequences of bases\nwhich will fold into targeted secondary structures. Predicting how a given RNA\nsequence folds into a pseudoknot-free secondary structure is known to be\nsolvable in cubic time since the eighties and in truly subcubic time by a\nrecent result of Bringmann et al. (FOCS 2016). As a stark contrast, it is\nunknown whether or not designing a given RNA secondary structure is a tractable\ntask; this has been raised as a challenging open question by Anne Condon (ICALP\n2003). Because of its crucial importance in a number of fields such as\npharmaceutical research and biochemistry, there are dozens of heuristics and\nsoftware libraries dedicated to RNA secondary structure design. It is therefore\nrather surprising that the computational complexity of this central problem in\nbioinformatics has been unsettled for decades.\n  In this paper we show that, in the simplest model of energy which is the\nWatson-Crick model the design of secondary structures is NP-complete if one\nadds natural constraints of the form: index $i$ of the sequence has to be\nlabeled by base $b$. This negative result suggests that the same lower bound\nholds for more realistic models of energy. It is noteworthy that the additional\nconstraints are by no means artificial: they are provided by all the RNA design\npieces of software and they do correspond to the actual practice.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 14:37:05 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 13:30:13 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""], ["Sikora", "Florian", ""]]}, {"id": "1710.11592", "submitter": "Aravindan Vijayaraghavan", "authors": "Oded Regev and Aravindan Vijayaraghavan", "title": "On Learning Mixtures of Well-Separated Gaussians", "comments": "Appeared in FOCS 2017. 55 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of efficiently learning mixtures of a large number of\nspherical Gaussians, when the components of the mixture are well separated. In\nthe most basic form of this problem, we are given samples from a uniform\nmixture of $k$ standard spherical Gaussians, and the goal is to estimate the\nmeans up to accuracy $\\delta$ using $poly(k,d, 1/\\delta)$ samples.\n  In this work, we study the following question: what is the minimum separation\nneeded between the means for solving this task? The best known algorithm due to\nVempala and Wang [JCSS 2004] requires a separation of roughly\n$\\min\\{k,d\\}^{1/4}$. On the other hand, Moitra and Valiant [FOCS 2010] showed\nthat with separation $o(1)$, exponentially many samples are required. We\naddress the significant gap between these two bounds, by showing the following\nresults.\n  1. We show that with separation $o(\\sqrt{\\log k})$, super-polynomially many\nsamples are required. In fact, this holds even when the $k$ means of the\nGaussians are picked at random in $d=O(\\log k)$ dimensions.\n  2. We show that with separation $\\Omega(\\sqrt{\\log k})$, $poly(k,d,1/\\delta)$\nsamples suffice. Note that the bound on the separation is independent of\n$\\delta$. This result is based on a new and efficient \"accuracy boosting\"\nalgorithm that takes as input coarse estimates of the true means and in time\n$poly(k,d, 1/\\delta)$ outputs estimates of the means up to arbitrary accuracy\n$\\delta$ assuming the separation between the means is $\\Omega(\\min\\{\\sqrt{\\log\nk},\\sqrt{d}\\})$ (independently of $\\delta$).\n  We also present a computationally efficient algorithm in $d=O(1)$ dimensions\nwith only $\\Omega(\\sqrt{d})$ separation. These results together essentially\ncharacterize the optimal order of separation between components that is needed\nto learn a mixture of $k$ spherical Gaussians with polynomial samples.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 17:10:21 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Regev", "Oded", ""], ["Vijayaraghavan", "Aravindan", ""]]}]