[{"id": "1605.00058", "submitter": "Tselil Schramm", "authors": "Prasad Raghavendra, Satish Rao, Tselil Schramm", "title": "Strongly Refuting Random CSPs Below the Spectral Threshold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random constraint satisfaction problems (CSPs) are known to exhibit threshold\nphenomena: given a uniformly random instance of a CSP with $n$ variables and\n$m$ clauses, there is a value of $m = \\Omega(n)$ beyond which the CSP will be\nunsatisfiable with high probability. Strong refutation is the problem of\ncertifying that no variable assignment satisfies more than a constant fraction\nof clauses; this is the natural algorithmic problem in the unsatisfiable regime\n(when $m/n = \\omega(1)$).\n  Intuitively, strong refutation should become easier as the clause density\n$m/n$ grows, because the contradictions introduced by the random clauses become\nmore locally apparent. For CSPs such as $k$-SAT and $k$-XOR, there is a\nlong-standing gap between the clause density at which efficient strong\nrefutation algorithms are known, $m/n \\ge \\widetilde O(n^{k/2-1})$, and the\nclause density at which instances become unsatisfiable with high probability,\n$m/n = \\omega (1)$.\n  In this paper, we give spectral and sum-of-squares algorithms for strongly\nrefuting random $k$-XOR instances with clause density $m/n \\ge \\widetilde\nO(n^{(k/2-1)(1-\\delta)})$ in time $\\exp(\\widetilde O(n^{\\delta}))$ or in\n$\\widetilde O(n^{\\delta})$ rounds of the sum-of-squares hierarchy, for any\n$\\delta \\in [0,1)$ and any integer $k \\ge 3$. Our algorithms provide a smooth\ntransition between the clause density at which polynomial-time algorithms are\nknown at $\\delta = 0$, and brute-force refutation at the satisfiability\nthreshold when $\\delta = 1$. We also leverage our $k$-XOR results to obtain\nstrong refutation algorithms for SAT (or any other Boolean CSP) at similar\nclause densities. Our algorithms match the known sum-of-squares lower bounds\ndue to Grigoriev and Schonebeck, up to logarithmic factors.\n  Additionally, we extend our techniques to give new results for certifying\nupper bounds on the injective tensor norm of random tensors.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 02:53:31 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 17:00:53 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Raghavendra", "Prasad", ""], ["Rao", "Satish", ""], ["Schramm", "Tselil", ""]]}, {"id": "1605.00065", "submitter": "Arindam Pal", "authors": "Gopinath Mishra and Subhra Mazumdar and Arindam Pal", "title": "Improved Algorithms for the Evacuation Route Planning Problem", "comments": "Published in the proceedings of International Conference on\n  Combinatorial Optimization and Applications (COCOA) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergency evacuation is the process of movement of people away from the\nthreat or actual occurrence of hazards such as natural disasters, terrorist\nattacks, fires and bombs. In this paper, we focus on evacuation from a\nbuilding, but the ideas can be applied to city and region evacuation. We define\nthe problem and show how it can be modeled using graphs. The resulting\noptimization problem can be formulated as an integer linear program. Though\nthis can be solved exactly, this approach does not scale well for graphs with\nthousands of nodes and several hundred thousands of edges. This is impractical\nfor large graphs.\n  We study a special case of this problem, where there is only a single source\nand a single sink. For this case, we give an improved algorithm \\emph{Single\nSource Single Sink Evacuation Route Planner (SSEP)}, whose evacuation time is\nalways at most that of a famous algorithm \\emph{Capacity Constrained Route\nPlanner (CCRP)}, and whose running time is strictly less than that of CCRP. We\nprove this mathematically and give supporting results by extensive experiments.\nWe also study randomized behavior model of people and give some interesting\nresults.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 05:22:21 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Mishra", "Gopinath", ""], ["Mazumdar", "Subhra", ""], ["Pal", "Arindam", ""]]}, {"id": "1605.00089", "submitter": "Pan Peng", "authors": "Zengfeng Huang, Pan Peng", "title": "Dynamic Graph Stream Algorithms in $o(n)$ Space", "comments": "ICALP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study graph problems in dynamic streaming model, where the\ninput is defined by a sequence of edge insertions and deletions. As many\nnatural problems require $\\Omega(n)$ space, where $n$ is the number of\nvertices, existing works mainly focused on designing $\\tilde{O}(n)$ space\nalgorithms. Although sublinear in the number of edges for dense graphs, it\ncould still be too large for many applications (e.g. $n$ is huge or the graph\nis sparse). In this work, we give single-pass algorithms beating this space\nbarrier for two classes of problems.\n  We present $o(n)$ space algorithms for estimating the number of connected\ncomponents with additive error $\\varepsilon n$ and\n$(1+\\varepsilon)$-approximating the weight of minimum spanning tree, for any\nsmall constant $\\varepsilon>0$. The latter improves previous $\\tilde{O}(n)$\nspace algorithm given by Ahn et al. (SODA 2012) for connected graphs with\nbounded edge weights.\n  We initiate the study of approximate graph property testing in the dynamic\nstreaming model, where we want to distinguish graphs satisfying the property\nfrom graphs that are $\\varepsilon$-far from having the property. We consider\nthe problem of testing $k$-edge connectivity, $k$-vertex connectivity,\ncycle-freeness and bipartiteness (of planar graphs), for which, we provide\nalgorithms using roughly $\\tilde{O}(n^{1-\\varepsilon})$ space, which is $o(n)$\nfor any constant $\\varepsilon$.\n  To complement our algorithms, we present $\\Omega(n^{1-O(\\varepsilon)})$ space\nlower bounds for these problems, which show that such a dependence on\n$\\varepsilon$ is necessary.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 10:17:30 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Huang", "Zengfeng", ""], ["Peng", "Pan", ""]]}, {"id": "1605.00119", "submitter": "Jian-Jia Chen", "authors": "Jian-Jia Chen, Wen-Hung Huang, Cong Liu", "title": "Automatic Parameter Derivations in k2U Framework", "comments": "arXiv admin note: text overlap with arXiv:1505.02155", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have recently developed a general schedulability test framework, called\nk2U, which can be applied to deal with a large variety of task models that have\nbeen widely studied in real-time embedded systems. The k2U framework provides\nseveral means for the users to convert arbitrary schedulability tests\n(regardless of platforms and task models) into polynomial-time tests with\nclosed mathematical expressions. However, the applicability (as well as the\nperformance) of the k2U framework relies on the users to index the tasks\nproperly and define certain constant parameters.\n  This report describes how to automatically index the tasks properly and\nderive those parameters. We will cover several typical schedulability tests in\nreal-time systems to explain how to systematically and automatically derive\nthose parameters required by the k2U framework. This automation significantly\nempowers the k2U framework to handle a wide range of classes of real-time\nexecution platforms and task models, including uniprocessor scheduling,\nmultiprocessor scheduling, self-suspending task systems, real-time tasks with\narrival jitter, services and virtualizations with bounded delays, etc.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 14:42:10 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Chen", "Jian-Jia", ""], ["Huang", "Wen-Hung", ""], ["Liu", "Cong", ""]]}, {"id": "1605.00124", "submitter": "Jian-Jia Chen", "authors": "Jian-Jia Chen", "title": "A Note on the Exact Schedulability Analysis for Segmented\n  Self-Suspending Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report considers a sporadic real-time task system with $n$ sporadic\ntasks on a uniprocessor platform, in which the lowest-priority task is a\nsegmented self-suspension task and the other higher-priority tasks are ordinary\nsporadic real-time tasks. This report proves that the schedulability analysis\nfor fixed-priority preemptive scheduling even with only one segmented\nself-suspending task as the lowest-priority task is $co{\\cal NP}$-hard in the\nstrong sense. Under fixed-priority preemptive scheduling, Nelissen et al. in\nECRTS 2015 provided a mixed-integer linear programming (MILP) formulation to\ncalculate an upper bound on the worst-case response time of the lowest-priority\nself-suspending task. This report provides a comprehensive support to explain\nseveral hidden properties that were not provided in their paper. We also\nprovide an input task set to explain why the resulting solution of their MILP\nformulation can be quite far from the exact worst-case response time.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 15:37:00 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 16:01:36 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Chen", "Jian-Jia", ""]]}, {"id": "1605.00137", "submitter": "Jacek Cicho\\'n", "authors": "Jacek Cichon, Rafal Kapelko, Dominik Markiewicz", "title": "On Leader Green Election", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the number of survivors in the Leader Green Election (LGE)\nalgorithm introduced by P. Jacquet, D. Milioris and P. Muhlethaler in 2013. Our\nmethod is based on the Rice method and gives quite precise formulas. We derive\nupper bounds on the number of survivors in this algorithm and we propose a\nproper use of LGE.\n  Finally, we discuss one property of a general urns and balls problem and show\na lower bound for a required number of rounds for a large class of distributed\nleader election protocols.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 17:07:47 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 20:23:09 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Cichon", "Jacek", ""], ["Kapelko", "Rafal", ""], ["Markiewicz", "Dominik", ""]]}, {"id": "1605.00139", "submitter": "Heng Guo", "authors": "Heng Guo and Mark Jerrum", "title": "Random cluster dynamics for the Ising model is rapidly mixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the mixing time of Glauber (single edge update) dynamics for the\nrandom cluster model at $q=2$ is bounded by a polynomial in the size of the\nunderlying graph. As a consequence, the Swendsen-Wang algorithm for the\nferromagnetic Ising model at any temperature has the same polynomial mixing\ntime bound.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 17:09:02 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Guo", "Heng", ""], ["Jerrum", "Mark", ""]]}, {"id": "1605.00372", "submitter": "Ching-Chi Lin", "authors": "Ching-Chi Lin and Cheng-Yu Hsieh", "title": "A Linear-Time Algorithm for the Weighted Paired-Domination Problem on\n  Block Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a graph $G = (V,E)$, a vertex subset $S\\subseteq V(G)$ is said to be a\ndominating set of $G$ if every vertex not in $S$ is adjacent to a vertex in\n$S$. A dominating set $S$ of $G$ is called a paired-dominating set of $G$ if\nthe induced subgraph $G[S]$ contains a perfect matching. In this paper, we\npropose an $O(n+m)$-time algorithm for the weighted paired-domination problem\non block graphs using dynamic programming, which strengthens the results in\n[Theoret. Comput. Sci., 410(47--49):5063--5071, 2009] and [J. Comb. Optim.,\n19(4):457--470, 2010]. Moreover, the algorithm can be completed in $O(n)$ time\nif the block-cut-vertex structure of $G$ is given.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 07:19:17 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Lin", "Ching-Chi", ""], ["Hsieh", "Cheng-Yu", ""]]}, {"id": "1605.00422", "submitter": "Sebastian Muskalla", "authors": "Roland Meyer and Sebastian Muskalla", "title": "Munchausen Iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for solving polynomial equations over idempotent\nomega-continuous semirings. The idea is to iterate over the semiring of\nfunctions rather than the semiring of interest, and only evaluate when needed.\nThe key operation is substitution. In the initial step, we compute a linear\ncompletion of the system of equations that exhaustively inserts the equations\ninto one another. With functions as approximants, the following steps insert\nthe current approximant into itself. Since the iteration improves its precision\nby substitution rather than computation we named it Munchausen, after the\nfictional baron that pulled himself out of a swamp by his own hair. The first\nresult shows that an evaluation of the n-th Munchausen approximant coincides\nwith the 2^n-th Newton approximant. Second, we show how to compute linear\ncompletions with standard techniques from automata theory. In particular, we\nare not bound to (but can use) the notion of differentials prominent in Newton\niteration.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 10:32:39 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Meyer", "Roland", ""], ["Muskalla", "Sebastian", ""]]}, {"id": "1605.00532", "submitter": "Ignasi Sau", "authors": "Didem G\\\"oz\\\"upek, Sibel \\\"Ozkan, Christophe Paul, Ignasi Sau,\n  Mordechai Shalom", "title": "Parameterized complexity of the MINCCA problem on graphs of bounded\n  decomposability", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an edge-colored graph, the cost incurred at a vertex on a path when two\nincident edges with different colors are traversed is called reload or\nchangeover cost. The \"Minimum Changeover Cost Arborescence\" (MINCCA) problem\nconsists in finding an arborescence with a given root vertex such that the\ntotal changeover cost of the internal vertices is minimized. It has been\nrecently proved by G\\\"oz\\\"upek et al. [TCS 2016] that the problem is FPT when\nparameterized by the treewidth and the maximum degree of the input graph. In\nthis article we present the following results for the MINCCA problem:\n  - the problem is W[1]-hard parameterized by the treedepth of the input graph,\neven on graphs of average degree at most 8. In particular, it is W[1]-hard\nparameterized by the treewidth of the input graph, which answers the main open\nproblem of G\\\"oz\\\"upek et al. [TCS 2016];\n  - it is W[1]-hard on multigraphs parameterized by the tree-cutwidth of the\ninput multigraph;\n  - it is FPT parameterized by the star tree-cutwidth of the input graph, which\nis a slightly restricted version of tree-cutwidth. This result strictly\ngeneralizes the FPT result given in G\\\"oz\\\"upek et al. [TCS 2016];\n  - it remains NP-hard on planar graphs even when restricted to instances with\nat most 6 colors and 0/1 symmetric costs, or when restricted to instances with\nat most 8 colors, maximum degree bounded by 4, and 0/1 symmetric costs.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 15:39:50 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["G\u00f6z\u00fcpek", "Didem", ""], ["\u00d6zkan", "Sibel", ""], ["Paul", "Christophe", ""], ["Sau", "Ignasi", ""], ["Shalom", "Mordechai", ""]]}, {"id": "1605.00558", "submitter": "Aristide Grange", "authors": "Aristide Grange, Imed Kacem, S\\'ebastien Martin", "title": "Algorithms for the Pagination Problem, a Bin Packing with Overlapping\n  Items", "comments": "15 pages, 8 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the strongly NP-complete pagination problem, an extension of BIN\nPACKING where packing together two items may make them occupy less volume than\nthe sum of their individual sizes. To achieve this property, an item is defined\nas a finite set of symbols from a given alphabet: while, in BIN PACKING, any\ntwo such sets would be disjoint, in PAGINATION, they can share zero, one or\nmore symbols. After formulating the problem as an integer linear program, we\ntry to approximate its solutions with several families of algorithms: from\nstraightforward adaptations of classical BIN PACKING heuristics, to dedicated\nalgorithms (greedy and non-greedy), to standard and grouping genetic\nalgorithms. All of them are studied first theoretically, then experimentally on\nan extensive random test set. Based upon these data, we propose a predictive\nmeasure of the statistical difficulty of a given instance, and finally\nrecommend which algorithm should be used in which case, depending on either\ntime constraints or quality requirements.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 16:45:02 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 13:40:17 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 10:01:50 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Grange", "Aristide", ""], ["Kacem", "Imed", ""], ["Martin", "S\u00e9bastien", ""]]}, {"id": "1605.00914", "submitter": "Martin Romauch", "authors": "Martin Romauch, Richard F. Hartl", "title": "Capacity Planning for Cluster Tools in the Semiconductor Industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new model for Cluster-tools with two load locks.\nCluster-tools are widely used to automate single wafer processing in\nsemiconductor industry. The load locks are the entry points into the vacuum of\nthe Cluster-tool's mainframe. Usually there are two of them available. Each lot\nbeing processed, is dedicated to a single load-lock. Therefore at most two\ndifferent lots (with possibly different processing times and qualification) can\nbe processed simultaneously. This restriction is one of the major potential\nbottlenecks.\n  Capacity planning is one of the possible applications for the proposed model\nand the paper demonstrates the integration into a more general framework that\nconsiders different tool types and different operational modes.\n  The paper also generalizes an earlier model that is limited to three\nprocessing chambers. The proposed modeling approach is based on makespan\nreductions by parallel processing. It turns out that the performance of the new\napproach is similar, when compared to the generalized model for three chambers,\nbut the new approach outperforms the generalized model for four and more\nchambers.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 14:07:32 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Romauch", "Martin", ""], ["Hartl", "Richard F.", ""]]}, {"id": "1605.01079", "submitter": "Tiago Tresoldi", "authors": "Tiago Tresoldi", "title": "Newer method of string comparison: the Modified Moving Contracting\n  Window Pattern Algorithm", "comments": "9 pages, 1 table, 1 source-code listing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new algorithm, the Modified Moving Contracting Window\nPattern Algorithm (CMCWPM), for the calculation of field similarity. It\nstrongly relies on previous work by Yang et al. (2001), correcting previous\nwork in which characters marked as inaccessible for further pattern matching\nwere not treated as boundaries between subfields, occasionally leading to\nhigher than expected scores of field similarity. A reference Python\nimplementation is provided.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 20:28:39 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Tresoldi", "Tiago", ""]]}, {"id": "1605.01106", "submitter": "Greg Bodwin", "authors": "Greg Bodwin", "title": "New Results on Linear Size Distance Preservers", "comments": "Appeared in SODA '16 under the title \"Linear Size Distance\n  Preservers\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $p$ node pairs in an $n$-node graph, a distance preserver is a sparse\nsubgraph that agrees with the original graph on all of the given pairwise\ndistances. We prove the following bounds on the number of edges needed for a\ndistance preserver:\n  - Any $p$ node pairs in a directed weighted graph have a distance preserver\non $O(n + n^{2/3} p)$ edges.\n  - Any $p = \\Omega\\left(\\frac{n^2}{rs(n)}\\right)$ node pairs in an undirected\nunweighted graph have a distance preserver on $O(p)$ edges, where $rs(n)$ is\nthe Ruzsa-Szemer\\'edi function from combinatorial graph theory.\n  - As a lower bound, there are examples where one needs $\\omega(\\sigma^2)$\nedges to preserve all pairwise distances within a subset of $\\sigma =\no(n^{2/3})$ nodes in an undirected weighted graph. If we additionally require\nthat the graph is unweighted, then the range of this lower bound falls slightly\nto $\\sigma \\le n^{2/3 - o(1)}$.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 22:35:29 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 10:27:13 GMT"}, {"version": "v3", "created": "Wed, 2 Jan 2019 22:10:07 GMT"}, {"version": "v4", "created": "Wed, 30 Dec 2020 15:38:26 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Bodwin", "Greg", ""]]}, {"id": "1605.01488", "submitter": "Takaaki Nishimoto", "authors": "Takaaki Nishimoto and Tomohiro I and Shunsuke Inenaga and Hideo Bannai\n  and Masayuki Takeda", "title": "Fully dynamic data structure for LCE queries in compressed space", "comments": "arXiv admin note: text overlap with arXiv:1504.06954", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Longest Common Extension (LCE) query on a text $T$ of length $N$ asks for\nthe length of the longest common prefix of suffixes starting at given two\npositions. We show that the signature encoding $\\mathcal{G}$ of size $w =\nO(\\min(z \\log N \\log^* M, N))$ [Mehlhorn et al., Algorithmica 17(2):183-198,\n1997] of $T$, which can be seen as a compressed representation of $T$, has a\ncapability to support LCE queries in $O(\\log N + \\log \\ell \\log^* M)$ time,\nwhere $\\ell$ is the answer to the query, $z$ is the size of the Lempel-Ziv77\n(LZ77) factorization of $T$, and $M \\geq 4N$ is an integer that can be handled\nin constant time under word RAM model. In compressed space, this is the fastest\ndeterministic LCE data structure in many cases. Moreover, $\\mathcal{G}$ can be\nenhanced to support efficient update operations: After processing $\\mathcal{G}$\nin $O(w f_{\\mathcal{A}})$ time, we can insert/delete any (sub)string of length\n$y$ into/from an arbitrary position of $T$ in $O((y+ \\log N\\log^* M)\nf_{\\mathcal{A}})$ time, where $f_{\\mathcal{A}} = O(\\min \\{ \\frac{\\log\\log M\n\\log\\log w}{\\log\\log\\log M}, \\sqrt{\\frac{\\log w}{\\log\\log w}} \\})$. This yields\nthe first fully dynamic LCE data structure. We also present efficient\nconstruction algorithms from various types of inputs: We can construct\n$\\mathcal{G}$ in $O(N f_{\\mathcal{A}})$ time from uncompressed string $T$; in\n$O(n \\log\\log n \\log N \\log^* M)$ time from grammar-compressed string $T$\nrepresented by a straight-line program of size $n$; and in $O(z f_{\\mathcal{A}}\n\\log N \\log^* M)$ time from LZ77-compressed string $T$ with $z$ factors. On top\nof the above contributions, we show several applications of our data structures\nwhich improve previous best known results on grammar-compressed string\nprocessing.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 04:29:05 GMT"}, {"version": "v2", "created": "Sun, 26 Jun 2016 12:46:11 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Nishimoto", "Takaaki", ""], ["I", "Tomohiro", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1605.01539", "submitter": "Szymon Grabowski", "authors": "Szymon Grabowski, Marcin Raniszewski", "title": "Rank and select: Another lesson learned", "comments": "Compared to v1: slightly optimized rank implementations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank and select queries on bitmaps are essential building bricks of many\ncompressed data structures, including text indexes, membership and range\nsupporting spatial data structures, compressed graphs, and more. Theoretically\nconsidered yet in 1980s, these primitives have also been a subject of vivid\nresearch concerning their practical incarnations in the last decade. We present\na few novel rank/select variants, focusing mostly on speed, obtaining\ncompetitive space-time results in the compressed setting. Our findings can be\nsummarized as follows: $(i)$ no single rank/select solution works best on any\nkind of data (ours are optimized for concatenated bit arrays obtained from\nwavelet trees for real text datasets), $(ii)$ it pays to efficiently handle\nblocks consisting of all 0 or all 1 bits, $(iii)$ compressed select does not\nhave to be significantly slower than compressed rank at a comparable memory\nuse.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 09:39:59 GMT"}, {"version": "v2", "created": "Thu, 12 May 2016 16:01:30 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Grabowski", "Szymon", ""], ["Raniszewski", "Marcin", ""]]}, {"id": "1605.01695", "submitter": "Kasper Green Larsen", "authors": "Kasper Green Larsen and Ryan Williams", "title": "Faster Online Matrix-Vector Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Online Boolean Matrix-Vector Multiplication (OMV) problem\nstudied by Henzinger et al. [STOC'15]: given an $n \\times n$ Boolean matrix\n$M$, we receive $n$ Boolean vectors $v_1,\\ldots,v_n$ one at a time, and are\nrequired to output $M v_i$ (over the Boolean semiring) before seeing the vector\n$v_{i+1}$, for all $i$. Previous known algorithms for this problem are\ncombinatorial, running in $O(n^3/\\log^2 n)$ time. Henzinger et al. conjecture\nthere is no $O(n^{3-\\varepsilon})$ time algorithm for OMV, for all $\\varepsilon\n> 0$; their OMV conjecture is shown to imply strong hardness results for many\nbasic dynamic problems.\n  We give a substantially faster method for computing OMV, running in\n$n^3/2^{\\Omega(\\sqrt{\\log n})}$ randomized time. In fact, after seeing\n$2^{\\omega(\\sqrt{\\log n})}$ vectors, we already achieve\n$n^2/2^{\\Omega(\\sqrt{\\log n})}$ amortized time for matrix-vector\nmultiplication. Our approach gives a way to reduce matrix-vector multiplication\nto solving a version of the Orthogonal Vectors problem, which in turn reduces\nto \"small\" algebraic matrix-matrix multiplication. Applications include faster\nindependent set detection, partial match retrieval, and 2-CNF evaluation.\n  We also show how a modification of our method gives a cell probe data\nstructure for OMV with worst case $O(n^{7/4}/\\sqrt{w})$ time per query vector,\nwhere $w$ is the word size. This result rules out an unconditional proof of the\nOMV conjecture using purely information-theoretic arguments.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 19:16:27 GMT"}, {"version": "v2", "created": "Fri, 6 May 2016 17:56:17 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Larsen", "Kasper Green", ""], ["Williams", "Ryan", ""]]}, {"id": "1605.01717", "submitter": "Aleksander M\\k{a}dry", "authors": "Michael B. Cohen, Aleksander Madry, Piotr Sankowski, Adrian Vladu", "title": "Negative-Weight Shortest Paths and Unit Capacity Minimum Cost Flow in\n  $\\tilde{O}(m^{10/7} \\log W)$ Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a set of combinatorial optimization problems on\nweighted graphs: the shortest path problem with negative weights, the weighted\nperfect bipartite matching problem, the unit-capacity minimum-cost maximum flow\nproblem and the weighted perfect bipartite $b$-matching problem under the\nassumption that $\\Vert b\\Vert_1=O(m)$. We show that each one of these four\nproblems can be solved in $\\tilde{O}(m^{10/7}\\log W)$ time, where $W$ is the\nabsolute maximum weight of an edge in the graph, which gives the first in over\n25 years polynomial improvement in their sparse-graph time complexity.\n  At a high level, our algorithms build on the interior-point method-based\nframework developed by Madry (FOCS 2013) for solving unit-capacity maximum flow\nproblem. We develop a refined way to analyze this framework, as well as provide\nnew variants of the underlying preconditioning and perturbation techniques.\nConsequently, we are able to extend the whole interior-point method-based\napproach to make it applicable in the weighted graph regime.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 19:57:30 GMT"}, {"version": "v2", "created": "Sun, 8 May 2016 19:30:58 GMT"}, {"version": "v3", "created": "Thu, 14 Jul 2016 01:22:31 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Cohen", "Michael B.", ""], ["Madry", "Aleksander", ""], ["Sankowski", "Piotr", ""], ["Vladu", "Adrian", ""]]}, {"id": "1605.01752", "submitter": "Stefan Hoffmann", "authors": "Stefan Hoffmann and Egon Wanke", "title": "Minimum Power Range Assignment for Symmetric Connectivity in Sensor\n  Networks with two Power Levels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the problem of assigning a transmission power to every\nnode of a wireless sensor network. The goal is to minimize the total power\nconsumption while ensuring that the resulting communication graph is connected.\nWe focus on a restricted version of this Range Assignment (RA) problem in which\nthere are two different power levels. We only consider symmetrical transmission\nlinks to allow easy integration with low level wireless protocols that\ntypically require bidirectional communication between two neighboring nodes. We\nintroduce a parameterized polynomial time approximation algorithm with a\nperformance ratio arbitrarily close to $\\pi^2/6$. Additionally, we give an\nalmost linear time approximation algorithm with a tight quality bound of $7/4$.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 20:48:14 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Hoffmann", "Stefan", ""], ["Wanke", "Egon", ""]]}, {"id": "1605.01814", "submitter": "Abram Magner", "authors": "Michael Drmota, Abram Magner, Wojciech Szpankowski", "title": "Asymmetric R\\'enyi Problem and PATRICIA Tries", "comments": "18 pages, 5 figures, to appear in Proceedings of the 27th\n  International Conference on Probabilistic, Combinatorial and Asymptotic\n  Methods for the Analysis of Algorithms, Krakow, Poland, 4-8 July 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1960, R\\'enyi asked for the number of random queries necessary to recover\na hidden bijective labeling of n distinct objects. In each query one selects a\nrandom subset of labels and asks, what is the set of objects that have these\nlabels? We consider here an asymmetric version of the problem in which in every\nquery an object is chosen with probability p > 1/2 and we ignore \"inconclusive\"\nqueries. We study the number of queries needed to recover the labeling in its\nentirety (the height), to recover at least one single element (the fillup\nlevel), and to recover a randomly chosen element (the typical depth). This\nproblem exhibits several remarkable behaviors: the depth D_n converges in\nprobability but not almost surely and while it satisfies the central limit\ntheorem its local limit theorem doesn't hold; the height H_n and the fillup\nlevel F_n exhibit phase transitions with respect to p in the second term. To\nobtain these results, we take a unified approach via the analysis of the\nexternal profile, defined at level k as the number of elements recovered by the\nkth query. We first establish new precise asymptotic results for the average\nand variance, and a central limit law, for the external profile in the regime\nwhere it grows polynomially with n. We then extend the external profile results\nto the boundaries of the central region, leading to the solution of our problem\nfor the height and fillup level. As a bonus, our analysis implies novel results\nfor analogous parameters of random PATRICIA tries.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 04:10:34 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Drmota", "Michael", ""], ["Magner", "Abram", ""], ["Szpankowski", "Wojciech", ""]]}, {"id": "1605.01866", "submitter": "Saeed Akhoondian Amiri", "authors": "Saeed Akhoondian Amiri, Stephan Kreutzer, D\\'aniel Marx, Roman\n  Rabinovich", "title": "Routing with Congestion in Acyclic Digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the version of the $k$-disjoint paths problem where $k$ demand pairs\n$(s_1,t_1)$, $\\dots$, $(s_k,t_k)$ are specified in the input and the paths in\nthe solution are allowed to intersect, but such that no vertex is on more than\n$c$ paths. We show that on directed acyclic graphs the problem is solvable in\ntime $n^{O(d)}$ if we allow congestion $k-d$ for $k$ paths. Furthermore, we\nshow that, under a suitable complexity theoretic assumption, the problem cannot\nbe solved in time $f(k)n^{o(d/\\log d)}$ for any computable function $f$.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 09:21:02 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Amiri", "Saeed Akhoondian", ""], ["Kreutzer", "Stephan", ""], ["Marx", "D\u00e1niel", ""], ["Rabinovich", "Roman", ""]]}, {"id": "1605.02022", "submitter": "Janne H. Korhonen", "authors": "Janne H. Korhonen", "title": "Deterministic MST Sparsification in the Congested Clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple deterministic constant-round algorithm in the congested\nclique model for reducing the number of edges in a graph to $n^{1+\\varepsilon}$\nwhile preserving the minimum spanning forest, where $\\varepsilon > 0$ is any\nconstant. This implies that in the congested clique model, it is sufficient to\nimprove MST and other connectivity algorithms on graphs with slightly\nsuperlinear number of edges to obtain a general improvement. As a byproduct, we\nalso obtain a simple alternative proof showing that MST can be computed\ndeterministically in $O(\\log \\log n)$ rounds.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 18:22:34 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Korhonen", "Janne H.", ""]]}, {"id": "1605.02038", "submitter": "Daniel Karapetyan Dr", "authors": "Daniel Karapetyan and Abraham P. Punnen and Andrew J. Parkes", "title": "Markov Chain methods for the bipartite Boolean quadratic programming\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Bipartite Boolean Quadratic Programming Problem (BBQP) which is\nan extension of the well known Boolean Quadratic Programming Problem (BQP).\nApplications of the BBQP include mining discrete patterns from binary data,\napproximating matrices by rank-one binary matrices, computing the cut-norm of a\nmatrix, and solving optimisation problems such as maximum weight biclique,\nbipartite maximum weight cut, maximum weight induced sub-graph of a bipartite\ngraph, etc. For the BBQP, we first present several algorithmic components,\nspecifically, hill climbers and mutations, and then show how to combine them in\na high-performance metaheuristic. Instead of hand-tuning a standard\nmetaheuristic to test the efficiency of the hybrid of the components, we chose\nto use an automated generation of a multi-component metaheuristic to save human\ntime, and also improve objectivity in the analysis and comparisons of\ncomponents. For this we designed a new metaheuristic schema which we call\nConditional Markov Chain Search (CMCS). We show that CMCS is flexible enough to\nmodel several standard metaheuristics; this flexibility is controlled by\nmultiple numeric parameters, and so is convenient for automated generation. We\nstudy the configurations revealed by our approach and show that the best of\nthem outperforms the previous state-of-the-art BBQP algorithm by several orders\nof magnitude. In our experiments we use benchmark instances introduced in the\npreliminary version of this paper and described here, which have already become\nthe de facto standard in the BBQP literature.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 19:06:56 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 18:44:11 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Karapetyan", "Daniel", ""], ["Punnen", "Abraham P.", ""], ["Parkes", "Andrew J.", ""]]}, {"id": "1605.02045", "submitter": "David Fern\\'andez-Baca", "authors": "Yun Deng and David Fern\\'andez-Baca", "title": "Fast Compatibility Testing for Phylogenies with Nested Taxa", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-labeled trees are phylogenies whose internal nodes may be labeled by\nhigher-order taxa. Thus, a leaf labeled Mus musculus could nest within a\nsubtree whose root node is labeled Rodentia, which itself could nest within a\nsubtree whose root is labeled Mammalia. Suppose we are given collection\n$\\mathcal P$ of semi-labeled trees over various subsets of a set of taxa. The\nancestral compatibility problem asks whether there is a semi-labeled tree\n$\\mathcal T$ that respects the clusterings and the ancestor/descendant\nrelationships implied by the trees in $\\mathcal P$. We give a\n$\\tilde{O}(M_{\\mathcal{P}})$ algorithm for the ancestral compatibility problem,\nwhere $M_{\\mathcal{P}}$ is the total number of nodes and edges in the trees in\n$\\mathcal P$. Unlike the best previous algorithm, the running time of our\nmethod does not depend on the degrees of the nodes in the input trees.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 19:14:49 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Deng", "Yun", ""], ["Fern\u00e1ndez-Baca", "David", ""]]}, {"id": "1605.02065", "submitter": "Thomas Steinke", "authors": "Mark Bun, Thomas Steinke", "title": "Concentrated Differential Privacy: Simplifications, Extensions, and\n  Lower Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Concentrated differential privacy\" was recently introduced by Dwork and\nRothblum as a relaxation of differential privacy, which permits sharper\nanalyses of many privacy-preserving computations. We present an alternative\nformulation of the concept of concentrated differential privacy in terms of the\nRenyi divergence between the distributions obtained by running an algorithm on\nneighboring inputs. With this reformulation in hand, we prove sharper\nquantitative results, establish lower bounds, and raise a few new questions. We\nalso unify this approach with approximate differential privacy by giving an\nappropriate definition of \"approximate concentrated differential privacy.\"\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 19:57:35 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Bun", "Mark", ""], ["Steinke", "Thomas", ""]]}, {"id": "1605.02123", "submitter": "Philippe Jacquet", "authors": "Philippe Jacquet and Wojciech Szpankowski", "title": "Average Size of a Suffix Tree for Markov Sources", "comments": "AofA 2016 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a suffix tree built from a sequence generated by a Markovian source.\nSuch sources are more realistic probabilistic models for text generation, data\ncompression, molecular applications, and so forth. We prove that the average\nsize of such a suffix tree is asymptotically equivalent to the average size of\na trie built over $n$ independent sequences from the same Markovian source.\nThis equivalence is only known for memoryless sources. We then derive a formula\nfor the size of a trie under Markovian model to complete the analysis for\nsuffix trees. We accomplish our goal by applying some novel techniques of\nanalytic combinatorics on words also known as analytic pattern matching.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 00:53:54 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Jacquet", "Philippe", ""], ["Szpankowski", "Wojciech", ""]]}, {"id": "1605.02156", "submitter": "Massimo Cairo", "authors": "Massimo Cairo and Romeo Rizzi", "title": "The Complexity of Simulation and Matrix Multiplication", "comments": "Submitted. Changed in v2: This is a major rewrite of the paper. The\n  introduction has been expanded considerably, some notation has been\n  simplified, proofs of general results on reachability games have been moved\n  to the appendix, more intuitive arguments for proofs have been provided,\n  moving the formal arguments to the appendix, more references have been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the simulation preorder of a given Kripke structure (i.e., a\ndirected graph with $n$ labeled vertices) has crucial applications in model\nchecking of temporal logic. It amounts to solving a specific two-players\nreachability game, called simulation game. We offer the first conditional lower\nbounds for this problem, and we relate its complexity (for computation,\nverification, and certification) to some variants of $n\\times n$ matrix\nmultiplication.\n  We show that any $O(n^{\\alpha})$-time algorithm for simulation games, even\nrestricting to acyclic games/structures, can be used to compute $n\\times n$\nboolean matrix multiplication (BMM) in $O(n^{\\alpha})$ time. This is the first\nevidence that improving the existing $O(n^{3})$-time solutions may be\ndifficult, without resorting to fast matrix multiplication. In the acyclic\ncase, we match this lower bound presenting the first subcubic algorithm, based\non fast BMM, and running in $n^{\\omega+o(1)}$ time (where $\\omega<2.376$ is the\nexponent of matrix multiplication).\n  For both acyclic and cyclic structures, we point out the existence of natural\nand canonical $O(n^{2})$-size certificates, that can be verified in truly\nsubcubic time. In the acyclic case, $O(n^{2})$ time is sufficient, employing\nstandard matrix product verification. In the cyclic case, a $\\max$-semi-boolean\nmatrix multiplication (MSBMM) is used, i.e., a matrix multiplication on the\nsemi-ring $(\\max,\\times)$ where one matrix contains only $0$'s and $1$'s. This\nMSBMM is computable (hence verifiable) in truly subcubic\n$n^{(3+\\omega)/2+o(1)}$ time by reduction to $(\\max,\\min)$-multiplication.\n  Finally, we show a reduction from MSBMM to cyclic simulation games which\nimplies a separation between the cyclic and the acyclic cases, unless MSBMM can\nbe verified in $n^{\\omega+o(1)}$ time.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 08:18:31 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 15:51:14 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Cairo", "Massimo", ""], ["Rizzi", "Romeo", ""]]}, {"id": "1605.02168", "submitter": "Alexander Loboda", "authors": "Alexander A. Loboda, Maxim N. Artyomov, Alexey A. Sergushichev", "title": "Solving generalized maximum-weight connected subgraph problem for\n  network enrichment analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network enrichment analysis methods allow to identify active modules without\nbeing biased towards a priori defined pathways. One of mathematical\nformulations of such analysis is a reduction to a maximum-weight connected\nsubgraph problem. In particular, in analysis of metabolic networks a\ngeneralized maximum-weight connected subgraph (GMWCS) problem, where both nodes\nand edges are scored, naturally arises. Here we present the first to our\nknowledge practical exact GMWCS solver. We have tested it on real-world\ninstances and compared to similar solvers. First, the results show that on\nnode-weighted instances GMWCS solver has a similar performance to the best\nsolver for that problem. Second, GMWCS solver is faster compared to the closest\nanalogue when run on GMWCS instances with edge weights.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 10:32:57 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Loboda", "Alexander A.", ""], ["Artyomov", "Maxim N.", ""], ["Sergushichev", "Alexey A.", ""]]}, {"id": "1605.02224", "submitter": "Lorenzo De Stefani", "authors": "Gianfranco Bilardi and Lorenzo De Stefani", "title": "The I/O complexity of Strassen's matrix multiplication with\n  recomputation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tight $\\Omega((n/\\sqrt{M})^{\\log_2 7}M)$ lower bound is derived on the \\io\ncomplexity of Strassen's algorithm to multiply two $n \\times n$ matrices, in a\ntwo-level storage hierarchy with $M$ words of fast memory. A proof technique is\nintroduced, which exploits the Grigoriev's flow of the matrix multiplication\nfunction as well as some combinatorial properties of the Strassen computational\ndirected acyclic graph (CDAG). Applications to parallel computation are also\ndeveloped. The result generalizes a similar bound previously obtained under the\nconstraint of no-recomputation, that is, that intermediate results cannot be\ncomputed more than once. For this restricted case, another lower bound\ntechnique is presented, which leads to a simpler analysis of the \\io complexity\nof Strassen's algorithm and can be readily extended to other \"Strassen-like\"\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 17:53:46 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Bilardi", "Gianfranco", ""], ["De Stefani", "Lorenzo", ""]]}, {"id": "1605.02337", "submitter": "Jiajun Liu", "authors": "Jiajun Liu, Kun Zhao, Philipp Sommer, Shuo Shang, Brano Kusy, Jae-Gil\n  Lee, Raja Jurdak", "title": "A Novel Framework for Online Amnesic Trajectory Compression in\n  Resource-constrained Environments", "comments": "arXiv admin note: substantial text overlap with arXiv:1412.0321", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art trajectory compression methods usually involve high\nspace-time complexity or yield unsatisfactory compression rates, leading to\nrapid exhaustion of memory, computation, storage and energy resources. Their\nability is commonly limited when operating in a resource-constrained\nenvironment especially when the data volume (even when compressed) far exceeds\nthe storage limit. Hence we propose a novel online framework for error-bounded\ntrajectory compression and ageing called the Amnesic Bounded Quadrant System\n(ABQS), whose core is the Bounded Quadrant System (BQS) algorithm family that\nincludes a normal version (BQS), Fast version (FBQS), and a Progressive version\n(PBQS). ABQS intelligently manages a given storage and compresses the\ntrajectories with different error tolerances subject to their ages. In the\nexperiments, we conduct comprehensive evaluations for the BQS algorithm family\nand the ABQS framework. Using empirical GPS traces from flying foxes and cars,\nand synthetic data from simulation, we demonstrate the effectiveness of the\nstandalone BQS algorithms in significantly reducing the time and space\ncomplexity of trajectory compression, while greatly improving the compression\nrates of the state-of-the-art algorithms (up to 45%). We also show that the\noperational time of the target resource-constrained hardware platform can be\nprolonged by up to 41%. We then verify that with ABQS, given data volumes that\nare far greater than storage space, ABQS is able to achieve 15 to 400 times\nsmaller errors than the baselines. We also show that the algorithm is robust to\nextreme trajectory shapes.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 17:14:19 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Liu", "Jiajun", ""], ["Zhao", "Kun", ""], ["Sommer", "Philipp", ""], ["Shang", "Shuo", ""], ["Kusy", "Brano", ""], ["Lee", "Jae-Gil", ""], ["Jurdak", "Raja", ""]]}, {"id": "1605.02352", "submitter": "Henning Sulzbach", "authors": "Kevin Leckey, Ralph Neininger and Henning Sulzbach", "title": "Process convergence for the complexity of Radix Selection on Markov\n  sources", "comments": "main results significantly improved, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental algorithm for selecting ranks from a finite subset of an\nordered set is Radix Selection. This algorithm requires the data to be given as\nstrings of symbols over an ordered alphabet, e.g., binary expansions of real\nnumbers. Its complexity is measured by the number of symbols that have to be\nread. In this paper the model of independent data identically generated from a\nMarkov chain is considered. The complexity is studied as a stochastic process\nindexed by the set of infinite strings over the given alphabet. The orders of\nmean and variance of the complexity and, after normalization, a limit theorem\nwith a centered Gaussian process as limit are derived. This implies an analysis\nfor two standard models for the ranks: uniformly chosen ranks, also called\ngrand averages, and the worst case rank complexities which are of interest in\ncomputer science. For uniform data and the asymmetric Bernoulli model (i.e.\nmemoryless sources), we also find weak convergence for the normalized process\nof complexities when indexed by the ranks while for more general Markov sources\nthese processes are not tight under the standard normalizations.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 19:27:10 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 19:42:17 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Leckey", "Kevin", ""], ["Neininger", "Ralph", ""], ["Sulzbach", "Henning", ""]]}, {"id": "1605.02353", "submitter": "Rasmus J Kyng", "authors": "Rasmus Kyng, Sushant Sachdeva", "title": "Approximate Gaussian Elimination for Laplacians: Fast, Sparse, and\n  Simple", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to perform sparse approximate Gaussian elimination for Laplacian\nmatrices. We present a simple, nearly linear time algorithm that approximates a\nLaplacian by a matrix with a sparse Cholesky factorization, the version of\nGaussian elimination for symmetric matrices. This is the first nearly linear\ntime solver for Laplacian systems that is based purely on random sampling, and\ndoes not use any graph theoretic constructions such as low-stretch trees,\nsparsifiers, or expanders. The crux of our analysis is a novel concentration\nbound for matrix martingales where the differences are sums of conditionally\nindependent variables.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 19:34:05 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Kyng", "Rasmus", ""], ["Sachdeva", "Sushant", ""]]}, {"id": "1605.02530", "submitter": "Andreas Emil Feldmann", "authors": "Andreas Emil Feldmann", "title": "Fixed Parameter Approximations for k-Center Problems in Low Highway\n  Dimension Graphs", "comments": "A preliminary version appeared at the 42nd International Colloquium\n  on Automata, Languages, and Programming (ICALP 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the $k$-Center problem and some generalizations. For $k$-Center a\nset of $k$ center vertices needs to be found in a graph $G$ with edge lengths,\nsuch that the distance from any vertex of $G$ to its nearest center is\nminimized. This problem naturally occurs in transportation networks, and\ntherefore we model the inputs as graphs with bounded highway dimension, as\nproposed by Abraham et al. [SODA 2010].\n  We show both approximation and fixed-parameter hardness results, and how to\novercome them using fixed-parameter approximations, where the two paradigms are\ncombined. In particular, we prove that for any $\\varepsilon>0$ computing a\n$(2-\\varepsilon)$-approximation is W[2]-hard for parameter $k$ and NP-hard for\ngraphs with highway dimension $O(\\log^2 n)$. The latter does not rule out\nfixed-parameter $(2-\\varepsilon)$-approximations for the highway dimension\nparameter $h$, but implies that such an algorithm must have at least doubly\nexponential running time in $h$ if it exists, unless the ETH fails. On the\npositive side, we show how to get below the approximation factor of $2$ by\ncombining the parameters $k$ and $h$: we develop a fixed-parameter\n$3/2$-approximation with running time $2^{O(kh\\log h)}\\cdot n^{O(1)}$.\nAdditionally we prove that, unless P=NP, our techniques cannot be used to\ncompute fixed-parameter $(2-\\varepsilon)$-approximations for only the parameter\n$h$.\n  We also provide similar fixed-parameter approximations for the weighted\n$k$-Center and $(k,\\mathcal{F})$-Partition problems, which generalize\n$k$-Center.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 11:24:00 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 08:09:09 GMT"}, {"version": "v3", "created": "Fri, 26 Apr 2019 11:07:24 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Feldmann", "Andreas Emil", ""]]}, {"id": "1605.02569", "submitter": "Bastien Pasdeloup", "authors": "Bastien Pasdeloup, Vincent Gripon, Gr\\'egoire Mercier, Dominique\n  Pastor and Michael G. Rabbat", "title": "Characterization and Inference of Graph Diffusion Processes from\n  Observations of Stationary Signals", "comments": "Submitted to IEEE Transactions on Signal and Information Processing\n  over Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tools from the field of graph signal processing exploit knowledge of the\nunderlying graph's structure (e.g., as encoded in the Laplacian matrix) to\nprocess signals on the graph. Therefore, in the case when no graph is\navailable, graph signal processing tools cannot be used anymore. Researchers\nhave proposed approaches to infer a graph topology from observations of signals\non its nodes. Since the problem is ill-posed, these approaches make\nassumptions, such as smoothness of the signals on the graph, or sparsity\npriors. In this paper, we propose a characterization of the space of valid\ngraphs, in the sense that they can explain stationary signals. To simplify the\nexposition in this paper, we focus here on the case where signals were i.i.d.\nat some point back in time and were observed after diffusion on a graph. We\nshow that the set of graphs verifying this assumption has a strong connection\nwith the eigenvectors of the covariance matrix, and forms a convex set. Along\nwith a theoretical study in which these eigenvectors are assumed to be known,\nwe consider the practical case when the observations are noisy, and\nexperimentally observe how fast the set of valid graphs converges to the set\nobtained when the exact eigenvectors are known, as the number of observations\ngrows. To illustrate how this characterization can be used for graph recovery,\nwe present two methods for selecting a particular point in this set under\nchosen criteria, namely graph simplicity and sparsity. Additionally, we\nintroduce a measure to evaluate how much a graph is adapted to signals under a\nstationarity assumption. Finally, we evaluate how state-of-the-art methods\nrelate to this framework through experiments on a dataset of temperatures.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 13:01:22 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 22:57:58 GMT"}, {"version": "v3", "created": "Wed, 1 Mar 2017 13:46:46 GMT"}, {"version": "v4", "created": "Tue, 6 Jun 2017 16:26:36 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Pasdeloup", "Bastien", ""], ["Gripon", "Vincent", ""], ["Mercier", "Gr\u00e9goire", ""], ["Pastor", "Dominique", ""], ["Rabbat", "Michael G.", ""]]}, {"id": "1605.02673", "submitter": "Thomas Dybdahl Ahle", "authors": "Thomas D. Ahle, Martin Aum\\\"uller, and Rasmus Pagh", "title": "Parameter-free Locality Sensitive Hashing for Spherical Range Reporting", "comments": "21 pages, 5 figures, due to the limitation \"The abstract field cannot\n  be longer than 1,920 characters\", the abstract appearing here is slightly\n  shorter than that in the PDF file", "journal-ref": null, "doi": "10.1137/1.9781611974782.16", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a data structure for *spherical range reporting* on a point set\n$S$, i.e., reporting all points in $S$ that lie within radius $r$ of a given\nquery point $q$. Our solution builds upon the Locality-Sensitive Hashing (LSH)\nframework of Indyk and Motwani, which represents the asymptotically best\nsolutions to near neighbor problems in high dimensions. While traditional LSH\ndata structures have several parameters whose optimal values depend on the\ndistance distribution from $q$ to the points of $S$, our data structure is\nparameter-free, except for the space usage, which is configurable by the user.\nNevertheless, its expected query time basically matches that of an LSH data\nstructure whose parameters have been *optimally chosen for the data and query*\nin question under the given space constraints. In particular, our data\nstructure provides a smooth trade-off between hard queries (typically addressed\nby standard LSH) and easy queries such as those where the number of points to\nreport is a constant fraction of $S$, or where almost all points in $S$ are far\naway from the query point. In contrast, known data structures fix LSH\nparameters based on certain parameters of the input alone.\n  The algorithm has expected query time bounded by $O(t (n/t)^\\rho)$, where $t$\nis the number of points to report and $\\rho\\in (0,1)$ depends on the data\ndistribution and the strength of the LSH family used. We further present a\nparameter-free way of using multi-probing, for LSH families that support it,\nand show that for many such families this approach allows us to get expected\nquery time close to $O(n^\\rho+t)$, which is the best we can hope to achieve\nusing LSH. The previously best running time in high dimensions was $\\Omega(t\nn^\\rho)$. For many data distributions where the intrinsic dimensionality of the\npoint set close to $q$ is low, we can give improved upper bounds on the\nexpected query time.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 17:48:34 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 12:18:09 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Ahle", "Thomas D.", ""], ["Aum\u00fcller", "Martin", ""], ["Pagh", "Rasmus", ""]]}, {"id": "1605.02687", "submitter": "Tobias Christiani", "authors": "Tobias Christiani", "title": "A Framework for Similarity Search with Space-Time Tradeoffs using\n  Locality-Sensitive Filtering", "comments": "Accepted to SODA'17. See the paper for the complete abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for similarity search based on Locality-Sensitive\nFiltering (LSF), generalizing the Indyk-Motwani (STOC 1998) Locality-Sensitive\nHashing (LSH) framework to support space-time tradeoffs. Given a family of\nfilters, defined as a distribution over pairs of subsets of space with certain\nlocality-sensitivity properties, we can solve the approximate near neighbor\nproblem in $d$-dimensional space for an $n$-point data set with query time\n$dn^{\\rho_q+o(1)}$, update time $dn^{\\rho_u+o(1)}$, and space usage $dn + n^{1\n+ \\rho_u + o(1)}$. The space-time tradeoff is tied to the tradeoff between\nquery time and update time, controlled by the exponents $\\rho_q, \\rho_u$ that\nare determined by the filter family. Locality-sensitive filtering was\nintroduced by Becker et al. (SODA 2016) together with a framework yielding a\nsingle, balanced, tradeoff between query time and space, further relying on the\nassumption of an efficient oracle for the filter evaluation algorithm. We\nextend the LSF framework to support space-time tradeoffs and through a\ncombination of existing techniques we remove the oracle assumption.\n  Building on a filter family for the unit sphere by Laarhoven (arXiv 2015) we\nuse a kernel embedding technique by Rahimi & Recht (NIPS 2007) to show a\nsolution to the $(r,cr)$-near neighbor problem in $\\ell_s^d$-space for $0 < s\n\\leq 2$ with query and update exponents\n$\\rho_q=\\frac{c^s(1+\\lambda)^2}{(c^s+\\lambda)^2}$ and\n$\\rho_u=\\frac{c^s(1-\\lambda)^2}{(c^s+\\lambda)^2}$ where $\\lambda\\in[-1,1]$ is a\ntradeoff parameter. This result improves upon the space-time tradeoff of\nKapralov (PODS 2015) and is shown to be optimal in the case of a balanced\ntradeoff. Finally, we show a lower bound for the space-time tradeoff on the\nunit sphere that matches Laarhoven's and our own upper bound in the case of\nrandom data.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 18:29:47 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 15:15:16 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 09:48:14 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Christiani", "Tobias", ""]]}, {"id": "1605.02701", "submitter": "Ilya Razenshteyn", "authors": "Alexandr Andoni, Thijs Laarhoven, Ilya Razenshteyn, Erik Waingarten", "title": "Lower Bounds on Time-Space Trade-Offs for Approximate Near Neighbors", "comments": "47 pages, 2 figures; v2: substantially revised introduction, lots of\n  small corrections; subsumed by arXiv:1608.03580 [cs.DS] (along with\n  arXiv:1511.07527 [cs.DS])", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show tight lower bounds for the entire trade-off between space and query\ntime for the Approximate Near Neighbor search problem. Our lower bounds hold in\na restricted model of computation, which captures all hashing-based approaches.\nIn articular, our lower bound matches the upper bound recently shown in\n[Laarhoven 2015] for the random instance on a Euclidean sphere (which we show\nin fact extends to the entire space $\\mathbb{R}^d$ using the techniques from\n[Andoni, Razenshteyn 2015]).\n  We also show tight, unconditional cell-probe lower bounds for one and two\nprobes, improving upon the best known bounds from [Panigrahy, Talwar, Wieder\n2010]. In particular, this is the first space lower bound (for any static data\nstructure) for two probes which is not polynomially smaller than for one probe.\nTo show the result for two probes, we establish and exploit a connection to\nlocally-decodable codes.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 19:13:06 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 22:23:46 GMT"}, {"version": "v3", "created": "Thu, 18 Aug 2016 22:03:33 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Andoni", "Alexandr", ""], ["Laarhoven", "Thijs", ""], ["Razenshteyn", "Ilya", ""], ["Waingarten", "Erik", ""]]}, {"id": "1605.02772", "submitter": "Sofia Kleisarchaki", "authors": "Sofia Kleisarchaki, Sihem Amer-Yahia, Ahlame Douzal-Chouakria,\n  Vassilis Christophides", "title": "Querying Temporal Drifts at Multiple Granularities (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a large body of work on online drift detection with the goal of\ndynamically finding and maintaining changes in data streams. In this paper, we\nadopt a query-based approach to drift detection. Our approach relies on {\\em a\ndrift index}, a structure that captures drift at different time granularities\nand enables flexible {\\em drift queries}. We formalize different drift queries\nthat represent real-world scenarios and develop query evaluation algorithms\nthat use different materializations of the drift index as well as strategies\nfor online index maintenance. We describe a thorough study of the performance\nof our algorithms on real-world and synthetic datasets with varying change\nrates.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 20:38:52 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 08:22:47 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Kleisarchaki", "Sofia", ""], ["Amer-Yahia", "Sihem", ""], ["Douzal-Chouakria", "Ahlame", ""], ["Christophides", "Vassilis", ""]]}, {"id": "1605.02882", "submitter": "Shashwat Garg", "authors": "Nikhil Bansal, Daniel Dadush, Shashwat Garg", "title": "An Algorithm for Koml\\'os Conjecture Matching Banaszczyk's bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding a low discrepancy coloring for sparse set\nsystems where each element lies in at most t sets. We give an efficient\nalgorithm that finds a coloring with discrepancy O((t log n)^{1/2}), matching\nthe best known non-constructive bound for the problem due to Banaszczyk. The\nprevious algorithms only achieved an O(t^{1/2} log n) bound. The result also\nextends to the more general Koml\\'{o}s setting and gives an algorithmic\nO(log^{1/2} n) bound.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 08:05:58 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 02:23:05 GMT"}, {"version": "v3", "created": "Sat, 10 Sep 2016 19:24:49 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Bansal", "Nikhil", ""], ["Dadush", "Daniel", ""], ["Garg", "Shashwat", ""]]}, {"id": "1605.03001", "submitter": "Marcin Pilipczuk", "authors": "Bart M. P. Jansen and Marcin Pilipczuk", "title": "Approximation and Kernelization for Chordal Vertex Deletion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Chordal Vertex Deletion (ChVD) problem asks to delete a minimum number of\nvertices from an input graph to obtain a chordal graph. In this paper we\ndevelop a polynomial kernel for ChVD under the parameterization by the solution\nsize, as well as poly(opt) approximation algorithm. The first result answers an\nopen problem of Marx from 2006 [WG 2006, LNCS 4271, 37-48].\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 13:29:55 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["Pilipczuk", "Marcin", ""]]}, {"id": "1605.03019", "submitter": "Samuli Lepp\\\"anen", "authors": "Adam Kurpisz, Samuli Lepp\\\"anen, Monaldo Mastrolilli", "title": "Tight Sum-of-Squares lower bounds for binary polynomial optimization\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give two results concerning the power of the Sum-of-Squares(SoS)/Lasserre\nhierarchy. For binary polynomial optimization problems of degree $2d$ and an\nodd number of variables $n$, we prove that $\\frac{n+2d-1}{2}$ levels of the\nSoS/Lasserre hierarchy are necessary to provide the exact optimal value. This\nmatches the recent upper bound result by Sakaue, Takeda, Kim and Ito.\n  Additionally, we study a conjecture by Laurent, who considered the linear\nrepresentation of a set with no integral points. She showed that the\nSherali-Adams hierarchy requires $n$ levels to detect the empty integer hull,\nand conjectured that the SoS/Lasserre rank for the same problem is $n-1$. We\ndisprove this conjecture and derive lower and upper bounds for the rank.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 14:03:44 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Kurpisz", "Adam", ""], ["Lepp\u00e4nen", "Samuli", ""], ["Mastrolilli", "Monaldo", ""]]}, {"id": "1605.03045", "submitter": "Micha{\\l} Pilipczuk", "authors": "Miko{\\l}aj Boja\\'nczyk and Micha{\\l} Pilipczuk", "title": "Definability equals recognizability for graphs of bounded treewidth", "comments": "21 pages, an extended abstract will appear in the proceedings of LICS\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.DM cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a conjecture of Courcelle, which states that a graph property is\ndefinable in MSO with modular counting predicates on graphs of constant\ntreewidth if, and only if it is recognizable in the following sense:\nconstant-width tree decompositions of graphs satisfying the property can be\nrecognized by tree automata. While the forward implication is a classic fact\nknown as Courcelle's theorem, the converse direction remained open\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 15:03:07 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Boja\u0144czyk", "Miko\u0142aj", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1605.03059", "submitter": "Yann Vaxes", "authors": "Victor Chepoi, Feodor F. Dragan, and Yann Vax\\`es", "title": "Core congestion is inherent in hyperbolic networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the impact the negative curvature has on the traffic\ncongestion in large-scale networks. We prove that every Gromov hyperbolic\nnetwork $G$ admits a core, thus answering in the positive a conjecture by\nJonckheere, Lou, Bonahon, and Baryshnikov, Internet Mathematics, 7 (2011) which\nis based on the experimental observation by Narayan and Saniee, Physical Review\nE, 84 (2011) that real-world networks with small hyperbolicity have a core\ncongestion. Namely, we prove that for every subset $X$ of vertices of a\n$\\delta$-hyperbolic graph $G$ there exists a vertex $m$ of $G$ such that the\ndisk $D(m,4 \\delta)$ of radius $4 \\delta$ centered at $m$ intercepts at least\none half of the total flow between all pairs of vertices of $X$, where the flow\nbetween two vertices $x,y\\in X$ is carried by geodesic (or quasi-geodesic)\n$(x,y)$-paths. A set $S$ intercepts the flow between two nodes $x$ and $y$ if\n$S$ intersect every shortest path between $x$ and $y$. Differently from what\nwas conjectured by Jonckheere et al., we show that $m$ is not (and cannot be)\nthe center of mass of $X$ but is a node close to the median of $X$ in the\nso-called injective hull of $X$. In case of non-uniform traffic between nodes\nof $X$ (in this case, the unit flow exists only between certain pairs of nodes\nof $X$ defined by a commodity graph $R$), we prove a primal-dual result showing\nthat for any $\\rho>5\\delta$ the size of a $\\rho$-multi-core (i.e., the number\nof disks of radius $\\rho$) intercepting all pairs of $R$ is upper bounded by\nthe maximum number of pairwise $(\\rho-3\\delta)$-apart pairs of $R$.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 15:32:02 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 21:04:36 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Chepoi", "Victor", ""], ["Dragan", "Feodor F.", ""], ["Vax\u00e8s", "Yann", ""]]}, {"id": "1605.03071", "submitter": "Florian Sikora", "authors": "Riccardo Dondi, Florian Sikora", "title": "Parameterized Complexity and Approximation Issues for the Colorful\n  Components Problems", "comments": null, "journal-ref": "Theoretical Computer Science Volume 739, 29 August 2018, Pages\n  1-12", "doi": "10.1016/j.tcs.2018.04.044", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quest for colorful components (connected components where each color is\nassociated with at most one vertex) inside a vertex-colored graph has been\nwidely considered in the last ten years. Here we consider two variants, Minimum\nColorful Components (MCC) and Maximum Edges in transitive Closure (MEC),\nintroduced in 2011 in the context of orthology gene identification in\nbioinformatics. The input of both MCC and MEC is a vertex-colored graph. MCC\nasks for the removal of a subset of edges, so that the resulting graph is\npartitioned in the minimum number of colorful connected components; MEC asks\nfor the removal of a subset of edges, so that the resulting graph is\npartitioned in colorful connected components and the number of edges in the\ntransitive closure of such a graph is maximized. We study the parameterized and\napproximation complexity of MCC and MEC, for general and restricted instances.\n  For MCC on trees we show that the problem is basically equivalent to Minimum\nCut on Trees, thus MCC is not approximable within factor $1.36 - \\varepsilon$,\nit is fixed-parameter tractable and it admits a poly-kernel (when the parameter\nis the number of colorful components). Moreover, we show that MCC, while it is\npolynomial time solvable on paths, it is NP-hard even for graphs with constant\ndistance to disjoint paths number. Then we consider the parameterized\ncomplexity of MEC when parameterized by the number $k$ of edges in the\ntransitive closure of a solution (the graph obtained by removing edges so that\nit is partitioned in colorful connected components). We give a fixed-parameter\nalgorithm for MEC paramterized by $k$ and, when the input graph is a tree, we\ngive a poly-kernel.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 15:55:58 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 15:14:38 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Dondi", "Riccardo", ""], ["Sikora", "Florian", ""]]}, {"id": "1605.03086", "submitter": "Elchanan Mossel", "authors": "Charles Bordenave and Uriel Feige and Elchanan Mossel", "title": "Shotgun Assembly of Random Jigsaw Puzzles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent work, Mossel and Ross considered the shotgun assembly problem for\na random jigsaw puzzle. Their model consists of a puzzle - an $n\\times n$ grid,\nwhere each vertex is viewed as a center of a piece. They assume that each of\nthe four edges adjacent to a vertex, is assigned one of $q$ colors\n(corresponding to \"jigs\", or cut shapes) uniformly at random. Mossel and Ross\nasked: how large should $q = q(n)$ be so that with high probability the puzzle\ncan be assembled uniquely given the collection of individual tiles? They showed\nthat if $q = \\omega(n^2)$, then the puzzle can be assembled uniquely with high\nprobability, while if $q = o(n^{2/3})$, then with high probability the puzzle\ncannot be uniquely assembled. Here we improve the upper bound and show that for\nany $\\eps > 0$, the puzzle can be assembled uniquely with high probability if\n$q \\geq n^{1+\\eps}$. The proof uses an algorithm of $n^{\\Theta(1/\\eps)}$\nrunning time.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 16:31:26 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Bordenave", "Charles", ""], ["Feige", "Uriel", ""], ["Mossel", "Elchanan", ""]]}, {"id": "1605.03152", "submitter": "Marcos Villagra", "authors": "Benjamin Baran and Marcos Villagra", "title": "Multiobjective Optimization in a Quantum Adiabatic Computer", "comments": "11 pages, 3 figures. In v3, more typos were corrected. A shorter and\n  preliminary version appeared in Proceedings of the 42nd Latin American\n  Conference on Informatics (CLEI), Valparaiso, Chile", "journal-ref": "Axioms 2019", "doi": "10.3390/axioms8010032", "report-no": null, "categories": "cs.DS math.OC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a quantum algorithm for multiobjective combinatorial\noptimization. We show how to map a convex combination of objective functions\nonto a Hamiltonian and then use that Hamiltonian to prove that the quantum\nadiabatic algorithm of Farhi \\emph{et al.} [arXiv:quant-ph/0001106] can find\nPareto-optimal solutions in finite time provided certain convex combinations of\nobjectives are used and the underlying multiobjective problem meets certain\nrestrictions.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 19:05:12 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 18:54:44 GMT"}, {"version": "v3", "created": "Fri, 30 Jun 2017 18:42:23 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Baran", "Benjamin", ""], ["Villagra", "Marcos", ""]]}, {"id": "1605.03203", "submitter": "Chaitanya Swamy", "authors": "Andre Linhares and Chaitanya Swamy", "title": "Approximating Min-Cost Chain-Constrained Spanning Trees: A Reduction\n  from Weighted to Unweighted Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the {\\em min-cost chain-constrained spanning-tree} (abbreviated\n\\mcst) problem: find a min-cost spanning tree in a graph subject to degree\nconstraints on a nested family of node sets. We devise the {\\em first} polytime\nalgorithm that finds a spanning tree that (i) violates the degree constraints\nby at most a constant factor {\\em and} (ii) whose cost is within a constant\nfactor of the optimum. Previously, only an algorithm for {\\em unweighted} \\cst\nwas known \\cite{olver}, which satisfied (i) but did not yield any cost bounds.\nThis also yields the first result that obtains an $O(1)$-factor for {\\em both}\nthe cost approximation and violation of degree constraints for any\nspanning-tree problem with general degree bounds on node sets, where an edge\nparticipates in a super-constant number of degree constraints.\n  A notable feature of our algorithm is that we {\\em reduce} \\mcst to\nunweighted \\cst (and then utilize \\cite{olver}) via a novel application of {\\em\nLagrangian duality} to simplify the {\\em cost structure} of the underlying\nproblem and obtain a decomposition into certain uniform-cost subproblems.\n  We show that this Lagrangian-relaxation based idea is in fact applicable more\ngenerally and, for any cost-minimization problem with packing side-constraints,\nyields a reduction from the weighted to the unweighted problem. We believe that\nthis reduction is of independent interest. As another application of our\ntechnique, we consider the {\\em $k$-budgeted matroid basis} problem, where we\nbuild upon a recent rounding algorithm of \\cite{BansalN16} to obtain an\nimproved $n^{O(k^{1.5}/\\epsilon)}$-time algorithm that returns a solution that\nsatisfies (any) one of the budget constraints exactly and incurs a\n$(1+\\epsilon)$-violation of the other budget constraints.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 20:30:50 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Linhares", "Andre", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "1605.03243", "submitter": "Moustapha Diaby", "authors": "Moustapha Diaby, Mark H. Karwan, and Lei Sun", "title": "On \"Exponential Lower Bounds for Polytopes in Combinatorial\n  Optimization\" by Fiorini et al. (2015): A Refutation For Models With Disjoint\n  Sets of Descriptive Variables", "comments": "7 pages; Minor typos fixed; Presentation of counter-example\n  simplified", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a numerical refutation of the developments of Fiorini et al.\n(2015)* for models with disjoint sets of descriptive variables. We also provide\nan insight into the meaning of the existence of a one-to-one linear map between\nsolutions of such models.\n  *: Fiorini, S., S. Massar, S. Pokutta, H.R. Tiwary, and R. de Wolf (2015).\nExponential Lower Bounds for Polytopes in Combinatorial Optimization. Journal\nof the ACM 62:2, Article No. 17.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 23:23:20 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 12:11:29 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Diaby", "Moustapha", ""], ["Karwan", "Mark H.", ""], ["Sun", "Lei", ""]]}, {"id": "1605.03266", "submitter": "Dave Bacon", "authors": "Dave Bacon", "title": "A Quantum Approach to the Unique Sink Orientation Problem", "comments": "5 pages, 2 figures", "journal-ref": "Phys. Rev. A 96, 012323 (2017)", "doi": "10.1103/PhysRevA.96.012323", "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider quantum algorithms for the unique sink orientation problem on\ncubes. This problem is widely considered to be of intermediate computational\ncomplexity. This is because there no known polynomial algorithm (classical or\nquantum) from the problem and yet it arrises as part of a series of problems\nfor which it being intractable would imply complexity theoretic collapses. We\ngive a reduction which proves that if one can efficiently evaluate the kth\npower of the unique sink orientation outmap, then there exists a polynomial\ntime quantum algorithm for the unique sink orientation problem on cubes.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 03:07:35 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 17:14:04 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Bacon", "Dave", ""]]}, {"id": "1605.03390", "submitter": "Mark Daniel Ward", "authors": "Jeffrey Gaither and Mark Daniel Ward", "title": "Variance of the Internal Profile in Suffix Trees", "comments": "19 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The precise analysis of the variance of the profile of a suffix tree has been\na longstanding open problem. We analyze three regimes of the asymptotic growth\nof the variance of the profile of a suffix tree built from a randomly generated\nbinary string, in the nonuniform case. We utilize combinatorics on words,\nsingularity analysis, and the Mellin transform.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 11:47:59 GMT"}, {"version": "v2", "created": "Thu, 12 May 2016 22:54:52 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Gaither", "Jeffrey", ""], ["Ward", "Mark Daniel", ""]]}, {"id": "1605.03613", "submitter": "Huck Bennett", "authors": "Huck Bennett, Daniel Dadush, Noah Stephens-Davidowitz", "title": "On the Lattice Distortion Problem", "comments": "This is the full version of a paper that appeared in ESA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study the \\emph{Lattice Distortion Problem} (LDP). LDP asks\nhow \"similar\" two lattices are. I.e., what is the minimal distortion of a\nlinear bijection between the two lattices? LDP generalizes the Lattice\nIsomorphism Problem (the lattice analogue of Graph Isomorphism), which simply\nasks whether the minimal distortion is one.\n  As our first contribution, we show that the distortion between any two\nlattices is approximated up to a $n^{O(\\log n)}$ factor by a simple function of\ntheir successive minima. Our methods are constructive, allowing us to compute\nlow-distortion mappings that are within a $2^{O(n \\log \\log n/\\log n)}$ factor\nof optimal in polynomial time and within a $n^{O(\\log n)}$ factor of optimal in\nsingly exponential time. Our algorithms rely on a notion of basis reduction\nintroduced by Seysen (Combinatorica 1993), which we show is intimately related\nto lattice distortion. Lastly, we show that LDP is NP-hard to approximate to\nwithin any constant factor (under randomized reductions), by a reduction from\nthe Shortest Vector Problem.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 20:31:01 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 19:54:46 GMT"}, {"version": "v3", "created": "Sun, 30 Oct 2016 10:54:32 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Bennett", "Huck", ""], ["Dadush", "Daniel", ""], ["Stephens-Davidowitz", "Noah", ""]]}, {"id": "1605.03643", "submitter": "William Devanny", "authors": "William E. Devanny, Michael T. Goodrich, Kristopher Jetviroj", "title": "Parallel Equivalence Class Sorting: Algorithms, Lower Bounds, and\n  Distribution-Based Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study parallel comparison-based algorithms for finding all equivalence\nclasses of a set of $n$ elements, where sorting according to some total order\nis not possible. Such scenarios arise, for example, in applications, such as in\ndistributed computer security, where each of $n$ agents are working to identify\nthe private group to which they belong, with the only operation available to\nthem being a zero-knowledge pairwise-comparison (which is sometimes called a\n\"secret handshake\") that reveals only whether two agents are in the same group\nor in different groups. We provide new parallel algorithms for this problem, as\nwell as new lower bounds and distribution-based analysis.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 00:01:22 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Devanny", "William E.", ""], ["Goodrich", "Michael T.", ""], ["Jetviroj", "Kristopher", ""]]}, {"id": "1605.03692", "submitter": "Prachi Goyal", "authors": "Deeparnab Chakrabarty, Prachi Goyal and Ravishankar Krishnaswamy", "title": "The Non-Uniform k-Center Problem", "comments": "Adjusted the figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce and study the Non-Uniform k-Center problem\n(NUkC). Given a finite metric space $(X,d)$ and a collection of balls of radii\n$\\{r_1\\geq \\cdots \\ge r_k\\}$, the NUkC problem is to find a placement of their\ncenters on the metric space and find the minimum dilation $\\alpha$, such that\nthe union of balls of radius $\\alpha\\cdot r_i$ around the $i$th center covers\nall the points in $X$. This problem naturally arises as a min-max vehicle\nrouting problem with fleets of different speeds.\n  The NUkC problem generalizes the classic $k$-center problem when all the $k$\nradii are the same (which can be assumed to be $1$ after scaling). It also\ngeneralizes the $k$-center with outliers (kCwO) problem when there are $k$\nballs of radius $1$ and $\\ell$ balls of radius $0$. There are $2$-approximation\nand $3$-approximation algorithms known for these problems respectively; the\nformer is best possible unless P=NP and the latter remains unimproved for 15\nyears.\n  We first observe that no $O(1)$-approximation is to the optimal dilation is\npossible unless P=NP, implying that the NUkC problem is more non-trivial than\nthe above two problems. Our main algorithmic result is an\n$(O(1),O(1))$-bi-criteria approximation result: we give an $O(1)$-approximation\nto the optimal dilation, however, we may open $\\Theta(1)$ centers of each\nradii. Our techniques also allow us to prove a simple (uni-criteria), optimal\n$2$-approximation to the kCwO problem improving upon the long-standing\n$3$-factor. Our main technical contribution is a connection between the NUkC\nproblem and the so-called firefighter problems on trees which have been studied\nrecently in the TCS community.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 06:18:32 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 11:03:07 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Goyal", "Prachi", ""], ["Krishnaswamy", "Ravishankar", ""]]}, {"id": "1605.03719", "submitter": "Pierre Fraigniaud", "authors": "Pierre Fraigniaud and Ivan Rapaport and Ville Salo and Ioan Todinca", "title": "Distributed Testing of Excluded Subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study property testing in the context of distributed computing, under the\nclassical CONGEST model. It is known that testing whether a graph is\ntriangle-free can be done in a constant number of rounds, where the constant\ndepends on how far the input graph is from being triangle-free. We show that,\nfor every connected 4-node graph H, testing whether a graph is H-free can be\ndone in a constant number of rounds too. The constant also depends on how far\nthe input graph is from being H-free, and the dependence is identical to the\none in the case of testing triangles. Hence, in particular, testing whether a\ngraph is K_4-free, and testing whether a graph is C_4-free can be done in a\nconstant number of rounds (where K_k denotes the k-node clique, and C_k denotes\nthe k-node cycle). On the other hand, we show that testing K_k-freeness and\nC_k-freeness for k>4 appear to be much harder. Specifically, we investigate two\nnatural types of generic algorithms for testing H-freeness, called DFS tester\nand BFS tester. The latter captures the previously known algorithm to test the\npresence of triangles, while the former captures our generic algorithm to test\nthe presence of a 4-node graph pattern H. We prove that both DFS and BFS\ntesters fail to test K_k-freeness and C_k-freeness in a constant number of\nrounds for k>4.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 08:15:54 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Fraigniaud", "Pierre", ""], ["Rapaport", "Ivan", ""], ["Salo", "Ville", ""], ["Todinca", "Ioan", ""]]}, {"id": "1605.03797", "submitter": "S{\\o}ren Dahlgaard", "authors": "Amir Abboud and S{\\o}ren Dahlgaard", "title": "Popular Conjectures as a Barrier for Dynamic Planar Graph Algorithms", "comments": "20 pages, 4 figures. Abstract has been truncated to fit arXiv limits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic shortest paths problem on planar graphs asks us to preprocess a\nplanar graph $G$ such that we may support insertions and deletions of edges in\n$G$ as well as distance queries between any two nodes $u,v$ subject to the\nconstraint that the graph remains planar at all times. This problem has been\nextensively studied in both the theory and experimental communities over the\npast decades and gets solved millions of times every day by companies like\nGoogle, Microsoft, and Uber. The best known algorithm performs queries and\nupdates in $\\tilde{O}(n^{2/3})$ time, based on ideas of a seminal paper by\nFakcharoenphol and Rao [FOCS'01]. A $(1+\\varepsilon)$-approximation algorithm\nof Abraham et al. [STOC'12] performs updates and queries in\n$\\tilde{O}(\\sqrt{n})$ time. An algorithm with $O(polylog(n))$ runtime would be\na major breakthrough. However, such runtimes are only known for a\n$(1+\\varepsilon)$-approximation in a model where only restricted weight updates\nare allowed due to Abraham et al. [SODA'16], or for easier problems like\nconnectivity.\n  In this paper, we follow a recent and very active line of work on showing\nlower bounds for polynomial time problems based on popular conjectures,\nobtaining the first such results for natural problems in planar graphs. Such\nresults were previously out of reach due to the highly non-planar nature of\nknown reductions and the impossibility of \"planarizing gadgets\". We introduce a\nnew framework which is inspired by techniques from the literatures on distance\nlabelling schemes and on parameterized complexity.\n  Using our framework, we show that no algorithm for dynamic shortest paths or\nmaximum weight bipartite matching in planar graphs can support both updates and\nqueries in amortized $O(n^{\\frac{1}{2}-\\varepsilon})$ time, for\n$\\varepsilon>0$, unless the classical APSP problem can be solved in truly\nsubcubic time, [...]\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 13:12:58 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Abboud", "Amir", ""], ["Dahlgaard", "S\u00f8ren", ""]]}, {"id": "1605.03871", "submitter": "Hendrik Molter", "authors": "Anne-Sophie Himmel and Hendrik Molter and Rolf Niedermeier and Manuel\n  Sorge", "title": "Adapting the Bron-Kerbosch Algorithm for Enumerating Maximal Cliques in\n  Temporal Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamics of interactions play an increasingly important role in the analysis\nof complex networks. A modeling framework to capture this are temporal graphs\nwhich consist of a set of vertices (entities in the network) and a set of\ntime-stamped binary interactions between the vertices. We focus on enumerating\ndelta-cliques, an extension of the concept of cliques to temporal graphs: for a\ngiven time period delta, a delta-clique in a temporal graph is a set of\nvertices and a time interval such that all vertices interact with each other at\nleast after every delta time steps within the time interval. Viard, Latapy, and\nMagnien [ASONAM 2015, TCS 2016] proposed a greedy algorithm for enumerating all\nmaximal delta-cliques in temporal graphs. In contrast to this approach, we\nadapt the Bron-Kerbosch algorithm - an efficient, recursive backtracking\nalgorithm which enumerates all maximal cliques in static graphs - to the\ntemporal setting. We obtain encouraging results both in theory (concerning\nworst-case running time analysis based on the parameter \"delta-slice\ndegeneracy\" of the underlying graph) as well as in practice with experiments on\nreal-world data. The latter culminates in an improvement for most interesting\ndelte-values concerning running time in comparison with the algorithm of Viard,\nLatapy, and Magnien.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 15:59:48 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 16:02:55 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Himmel", "Anne-Sophie", ""], ["Molter", "Hendrik", ""], ["Niedermeier", "Rolf", ""], ["Sorge", "Manuel", ""]]}, {"id": "1605.03933", "submitter": "Jieming Mao", "authors": "Xi Chen, Sivakanth Gopi, Jieming Mao, Jon Schneider", "title": "Competitive analysis of the top-K ranking problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in recommender systems, web search, social choice\nand crowdsourcing, we consider the problem of identifying the set of top $K$\nitems from noisy pairwise comparisons. In our setting, we are non-actively\ngiven $r$ pairwise comparisons between each pair of $n$ items, where each\ncomparison has noise constrained by a very general noise model called the\nstrong stochastic transitivity (SST) model. We analyze the competitive ratio of\nalgorithms for the top-$K$ problem. In particular, we present a linear time\nalgorithm for the top-$K$ problem which has a competitive ratio of\n$\\tilde{O}(\\sqrt{n})$; i.e. to solve any instance of top-$K$, our algorithm\nneeds at most $\\tilde{O}(\\sqrt{n})$ times as many samples needed as the best\npossible algorithm for that instance (in contrast, all previous known\nalgorithms for the top-$K$ problem have competitive ratios of\n$\\tilde{\\Omega}(n)$ or worse). We further show that this is tight: any\nalgorithm for the top-$K$ problem has competitive ratio at least\n$\\tilde{\\Omega}(\\sqrt{n})$.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 19:07:31 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Chen", "Xi", ""], ["Gopi", "Sivakanth", ""], ["Mao", "Jieming", ""], ["Schneider", "Jon", ""]]}, {"id": "1605.03949", "submitter": "Chris Schwiegelshohn", "authors": "Marc Bury, Chris Schwiegelshohn, Mara Sorella", "title": "Efficient Similarity Search in Dynamic Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Jaccard index is an important similarity measure for item sets and\nBoolean data. On large datasets, an exact similarity computation is often\ninfeasible for all item pairs both due to time and space constraints, giving\nrise to faster approximate methods. The algorithm of choice used to quickly\ncompute the Jaccard index $\\frac{\\vert A \\cap B \\vert}{\\vert A\\cup B\\vert}$ of\ntwo item sets $A$ and $B$ is usually a form of min-hashing. Most min-hashing\nschemes are maintainable in data streams processing only additions, but none\nare known to work when facing item-wise deletions. In this paper, we\ninvestigate scalable approximation algorithms for rational set similarities, a\nbroad class of similarity measures including Jaccard. Motivated by a result of\nChierichetti and Kumar [J. ACM 2015] who showed any rational set similarity $S$\nadmits a locality sensitive hashing (LSH) scheme if and only if the\ncorresponding distance $1-S$ is a metric, we can show that there exists a space\nefficient summary maintaining a $(1\\pm \\varepsilon)$ multiplicative\napproximation to $1-S$ in dynamic data streams. This in turn also yields a\n$\\varepsilon$ additive approximation of the similarity. The existence of these\napproximations hints at, but does not directly imply a LSH scheme in dynamic\ndata streams. Our second and main contribution now lies in the design of such a\nLSH scheme maintainable in dynamic data streams. The scheme is space efficient,\neasy to implement and to the best of our knowledge the first of its kind able\nto process deletions.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 19:30:17 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 19:52:44 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 16:50:02 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Bury", "Marc", ""], ["Schwiegelshohn", "Chris", ""], ["Sorella", "Mara", ""]]}, {"id": "1605.04031", "submitter": "Patricio Poblete", "authors": "Patricio V. Poblete and Alfredo Viola", "title": "Robin Hood Hashing really has constant average search cost and variance\n  in full tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thirty years ago, the Robin Hood collision resolution strategy was introduced\nfor open addressing hash tables, and a recurrence equation was found for the\ndistribution of its search cost. Although this recurrence could not be solved\nanalytically, it allowed for numerical computations that, remarkably, suggested\nthat the variance of the search cost approached a value of $1.883$ when the\ntable was full. Furthermore, by using a non-standard mean-centered search\nalgorithm, this would imply that searches could be performed in expected\nconstant time even in a full table.\n  In spite of the time elapsed since these observations were made, no progress\nhas been made in proving them. In this paper we introduce a technique to work\naround the intractability of the recurrence equation by solving instead an\nassociated differential equation. While this does not provide an exact\nsolution, it is sufficiently powerful to prove a bound for the variance, and\nthus obtain a proof that the variance of Robin Hood is bounded by a small\nconstant for load factors arbitrarily close to 1. As a corollary, this proves\nthat the mean-centered search algorithm runs in expected constant time.\n  We also use this technique to study the performance of Robin Hood hash tables\nunder a long sequence of insertions and deletions, where deletions are\nimplemented by marking elements as deleted. We prove that, in this case, the\nvariance is bounded by $1/(1-\\alpha)+O(1)$, where $\\alpha$ is the load factor.\n  To model the behavior of these hash tables, we use a unified approach that\ncan be applied also to study the First-Come-First-Served and\nLast-Come-First-Served collision resolution disciplines, both with and without\ndeletions.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 02:24:31 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Poblete", "Patricio V.", ""], ["Viola", "Alfredo", ""]]}, {"id": "1605.04098", "submitter": "Giovanna Rosone", "authors": "Anthony J. Cox, Fabio Garofalo, Giovanna Rosone, Marinella Sciortino", "title": "Lightweight LCP Construction for Very Large Collections of Strings", "comments": "This manuscript version is made available under the CC-BY-NC-ND 4.0\n  license http://creativecommons.org/licenses/by-nc-nd/4.0/ The final version\n  of this manuscript is in press in Journal of Discrete Algorithms", "journal-ref": null, "doi": "10.1016/j.jda.2016.03.003", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The longest common prefix array is a very advantageous data structure that,\ncombined with the suffix array and the Burrows-Wheeler transform, allows to\nefficiently compute some combinatorial properties of a string useful in several\napplications, especially in biological contexts. Nowadays, the input data for\nmany problems are big collections of strings, for instance the data coming from\n\"next-generation\" DNA sequencing (NGS) technologies. In this paper we present\nthe first lightweight algorithm (called extLCP) for the simultaneous\ncomputation of the longest common prefix array and the Burrows-Wheeler\ntransform of a very large collection of strings having any length. The\ncomputation is realized by performing disk data accesses only via sequential\nscans, and the total disk space usage never needs more than twice the output\nsize, excluding the disk space required for the input. Moreover, extLCP allows\nto compute also the suffix array of the strings of the collection, without any\nother further data structure is needed. Finally, we test our algorithm on real\ndata and compare our results with another tool capable to work in external\nmemory on large collections of strings.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 09:42:10 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Cox", "Anthony J.", ""], ["Garofalo", "Fabio", ""], ["Rosone", "Giovanna", ""], ["Sciortino", "Marinella", ""]]}, {"id": "1605.04160", "submitter": "Mohammad Obiedat", "authors": "Mohammad Obiedat", "title": "Searching Lattice Data Structures of Varying Degrees of Sortedness", "comments": "21 pages", "journal-ref": "International Journal of Data Structures, 1(1) 64-76 (2015)", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lattice data structures are space efficient and cache-suitable data\nstructures. The basic searching, insertion, and deletion operations are of time\ncomplexity $O(\\sqrt{N})$. We give a jump searching algorithm of time complexity\n$O(J(L)\\log(N))$, where $J(L)$ is the jump factor of the lattice. $J(L)$\napproaches $4$ when the degree of sortedness of the lattice approaches\n$\\sqrt{N}$. A sorting procedure of time complexity $O(\\sqrt{N})$ that can be\nused, during the system idle time, to increase the degree of sortedness of the\nlattice is given.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 12:51:02 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Obiedat", "Mohammad", ""]]}, {"id": "1605.04284", "submitter": "Michael Dinitz", "authors": "Eden Chlamt\\'a\\v{c}, Michael Dinitz, Christian Konrad, Guy Kortsarz,\n  and George Rabanca", "title": "The Densest k-Subhypergraph Problem", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Densest $k$-Subgraph (D$k$S) problem, and its corresponding minimization\nproblem Smallest $p$-Edge Subgraph (S$p$ES), have come to play a central role\nin approximation algorithms. This is due both to their practical importance,\nand their usefulness as a tool for solving and establishing approximation\nbounds for other problems. These two problems are not well understood, and it\nis widely believed that they do not an admit a subpolynomial approximation\nratio (although the best known hardness results do not rule this out).\n  In this paper we generalize both D$k$S and S$p$ES from graphs to hypergraphs.\nWe consider the Densest $k$-Subhypergraph problem (given a hypergraph $(V, E)$,\nfind a subset $W\\subseteq V$ of $k$ vertices so as to maximize the number of\nhyperedges contained in $W$) and define the Minimum $p$-Union problem (given a\nhypergraph, choose $p$ of the hyperedges so as to minimize the number of\nvertices in their union). We focus in particular on the case where all\nhyperedges have size 3, as this is the simplest non-graph setting. For this\ncase we provide an $O(n^{4(4-\\sqrt{3})/13 + \\epsilon}) \\leq\nO(n^{0.697831+\\epsilon})$-approximation (for arbitrary constant $\\epsilon > 0$)\nfor Densest $k$-Subhypergraph and an $\\tilde O(n^{2/5})$-approximation for\nMinimum $p$-Union. We also give an $O(\\sqrt{m})$-approximation for Minimum\n$p$-Union in general hypergraphs. Finally, we examine the interesting special\ncase of interval hypergraphs (instances where the vertices are a subset of the\nnatural numbers and the hyperedges are intervals of the line) and prove that\nboth problems admit an exact polynomial time solution on these instances.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 18:57:39 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Chlamt\u00e1\u010d", "Eden", ""], ["Dinitz", "Michael", ""], ["Konrad", "Christian", ""], ["Kortsarz", "Guy", ""], ["Rabanca", "George", ""]]}, {"id": "1605.04421", "submitter": "Travis Gagie", "authors": "Anthony J. Cox, Andrea Farruggia, Travis Gagie, Simon J. Puglisi and\n  Jouni Sir\\'en", "title": "RLZAP: Relative Lempel-Ziv with Adaptive Pointers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relative Lempel-Ziv (RLZ) is a popular algorithm for compressing databases of\ngenomes from individuals of the same species when fast random access is\ndesired. With Kuruppu et al.'s (SPIRE 2010) original implementation, a\nreference genome is selected and then the other genomes are greedily parsed\ninto phrases exactly matching substrings of the reference. Deorowicz and\nGrabowski (Bioinformatics, 2011) pointed out that letting each phrase end with\na mismatch character usually gives better compression because many of the\ndifferences between individuals' genomes are single-nucleotide substitutions.\nFerrada et al. (SPIRE 2014) then pointed out that also using relative pointers\nand run-length compressing them usually gives even better compression. In this\npaper we generalize Ferrada et al.'s idea to handle well also short insertions,\ndeletions and multi-character substitutions. We show experimentally that our\ngeneralization achieves better compression than Ferrada et al.'s implementation\nwith comparable random-access times.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2016 13:49:27 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Cox", "Anthony J.", ""], ["Farruggia", "Andrea", ""], ["Gagie", "Travis", ""], ["Puglisi", "Simon J.", ""], ["Sir\u00e9n", "Jouni", ""]]}, {"id": "1605.04491", "submitter": "Tianyi Zhang", "authors": "Ran Duan and Tianyi Zhang", "title": "Improved distance sensitivity oracles via tree partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an improved structure of distance sensitivity oracle (DSO). The\ntask is to pre-process a non-negatively weighted graph so that a data structure\ncan quickly answer replacement path length for every triple of source, terminal\nand failed vertex. The previous best algorithm constructs in time\n$\\tilde{O}(mn)$ a distance sensitivity oracle of size $O(n^2\\log n)$ that\nprocesses queries in $O(1)$ time. As an improvement, our oracle takes up\n$O(n^2)$ space, while preserving $O(1)$ query efficiency and $\\tilde{O}(mn)$\npreprocessing time. One should notice that space complexity and query time of\nour novel data structure are asymptotically optimal.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 02:16:05 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Duan", "Ran", ""], ["Zhang", "Tianyi", ""]]}, {"id": "1605.04538", "submitter": "Ofer Neiman", "authors": "Michael Elkin and Ofer Neiman", "title": "Hopsets with Constant Hopbound, and Applications to Approximate Shortest\n  Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $(\\beta,\\epsilon)$-hopset for a weighted undirected $n$-vertex graph\n$G=(V,E)$ is a set of edges, whose addition to the graph guarantees that every\npair of vertices has a path between them that contains at most $\\beta$ edges,\nwhose length is within $1+\\epsilon$ of the shortest path. In her seminal paper,\nCohen \\cite[JACM 2000]{C00} introduced the notion of hopsets in the context of\nparallel computation of approximate shortest paths, and since then it has found\nnumerous applications in various other settings, such as dynamic graph\nalgorithms, distributed computing, and the streaming model.\n  Cohen \\cite{C00} devised efficient algorithms for constructing hopsets with\n{\\em polylogarithmic} in $n$ number of hops. Her constructions remain the\nstate-of-the--art since the publication of her paper in STOC'94, i.e., for more\nthan two decades.\n  In this paper we exhibit the first construction of sparse hopsets with a {\\em\nconstant number of hops}. We also find efficient algorithms for hopsets in\nvarious computational settings, improving the best known constructions.\nGenerally, our hopsets strictly outperform the hopsets of \\cite{C00}, both in\nterms of their parameters, and in terms of the resources required to construct\nthem.\n  We demonstrate the applicability of our results for the fundamental problem\nof computing approximate shortest paths from $s$ sources. Our results improve\nthe running time for this problem in the parallel, distributed and streaming\nmodels, for a vast range of $s$.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 12:56:42 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Elkin", "Michael", ""], ["Neiman", "Ofer", ""]]}, {"id": "1605.04611", "submitter": "Ray Li", "authors": "Venkatesan Guruswami, Ray Li", "title": "Efficiently decodable insertion/deletion codes for high-noise and\n  high-rate regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work constructs codes that are efficiently decodable from a constant\nfraction of \\emph{worst-case} insertion and deletion errors in three parameter\nsettings: (i) Binary codes with rate approaching 1; (ii) Codes with constant\nrate for error fraction approaching 1 over fixed alphabet size; and (iii)\nConstant rate codes over an alphabet of size $k$ for error fraction approaching\n$(k-1)/(k+1)$. When errors are constrained to deletions alone, efficiently\ndecodable codes in each of these regimes were constructed recently. We complete\nthe picture by constructing similar codes that are efficiently decodable in the\ninsertion/deletion regime.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 22:30:28 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Li", "Ray", ""]]}, {"id": "1605.04651", "submitter": "Yihan Sun", "authors": "Guy E. Blelloch, Yan Gu, Yihan Sun", "title": "Efficient Construction of Probabilistic Tree Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe an algorithm that embeds a graph metric $(V,d_G)$\non an undirected weighted graph $G=(V,E)$ into a distribution of tree metrics\n$(T,D_T)$ such that for every pair $u,v\\in V$, $d_G(u,v)\\leq d_T(u,v)$ and\n${\\bf{E}}_{T}[d_T(u,v)]\\leq O(\\log n)\\cdot d_G(u,v)$. Such embeddings have\nproved highly useful in designing fast approximation algorithms, as many hard\nproblems on graphs are easy to solve on tree instances. For a graph with $n$\nvertices and $m$ edges, our algorithm runs in $O(m\\log n)$ time with high\nprobability, which improves the previous upper bound of $O(m\\log^3 n)$ shown by\nMendel et al.\\,in 2009.\n  The key component of our algorithm is a new approximate single-source\nshortest-path algorithm, which implements the priority queue with a new data\nstructure, the \"bucket-tree structure\". The algorithm has three properties: it\nonly requires linear time in the number of edges in the input graph; the\ncomputed distances have a distance preserving property; and when computing the\nshortest-paths to the $k$-nearest vertices from the source, it only requires to\nvisit these vertices and their edge lists. These properties are essential to\nguarantee the correctness and the stated time bound.\n  Using this shortest-path algorithm, we show how to generate an intermediate\nstructure, the approximate dominance sequences of the input graph, in $O(m \\log\nn)$ time, and further propose a simple yet efficient algorithm to converted\nthis sequence to a tree embedding in $O(n\\log n)$ time, both with high\nprobability. Combining the three subroutines gives the stated time bound of the\nalgorithm.\n  Then we show that this efficient construction can facilitate some\napplications. We proved that FRT trees (the generated tree embedding) are\nRamsey partitions with asymptotically tight bound, so the construction of a\nseries of distance oracles can be accelerated.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 05:28:30 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 03:19:08 GMT"}, {"version": "v3", "created": "Sat, 18 Feb 2017 07:06:00 GMT"}, {"version": "v4", "created": "Thu, 25 May 2017 23:44:49 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Blelloch", "Guy E.", ""], ["Gu", "Yan", ""], ["Sun", "Yihan", ""]]}, {"id": "1605.04679", "submitter": "Satoshi Takabe", "authors": "Satoshi Takabe, Koji Hukushima", "title": "Typical Performance of Approximation Algorithms for NP-hard Problems", "comments": "21 pages, 5 figures; typos are fixed", "journal-ref": "J. Stat. Mech. (2016) 113401", "doi": "10.1088/1742-5468/2016/11/113401", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical performance of approximation algorithms is studied for randomized\nminimum vertex cover problems. A wide class of random graph ensembles\ncharacterized by an arbitrary degree distribution is discussed with some\ntheoretical frameworks. Here three approximation algorithms are examined; the\nlinear-programming relaxation, the loopy-belief propagation, and the\nleaf-removal algorithm. The former two algorithms are analyzed using the\nstatistical-mechanical technique while the average-case analysis of the last\none is studied by the generating function method. These algorithms have a\nthreshold in the typical performance with increasing the average degree of the\nrandom graph, below which they find true optimal solutions with high\nprobability. Our study reveals that there exist only three cases determined by\nthe order of the typical-performance thresholds. We provide some conditions for\nclassifying the graph ensembles and demonstrate explicitly examples for the\ndifference in the threshold.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 08:42:42 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 04:39:39 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Takabe", "Satoshi", ""], ["Hukushima", "Koji", ""]]}, {"id": "1605.04982", "submitter": "Hadas Shachnai", "authors": "Dmitriy Katz, Baruch Schieber, and Hadas Shachnai", "title": "Flexible Resource Allocation for Clouds and All-Optical Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the cloud computing paradigm, and by key optimization problems\nin all-optical networks, we study two variants of the classic job interval\nscheduling problem, where a reusable resource is allocated to competing job\nintervals in a flexible manner. Each job, $J_i$, requires the use of up to\n$r_{max}(i)$ units of the resource, with a profit of $p_i \\geq 1$ accrued for\neach allocated unit. The goal is to feasibly schedule a subset of the jobs so\nas to maximize the total profit. The resource can be allocated either in\ncontiguous or non-contiguous blocks. These problems can be viewed as flexible\nvariants of the well known storage allocation and bandwidth allocation\nproblems.\n  We show that the contiguous version is strongly NP-hard, already for\ninstances where all jobs have the same profit and the same maximum resource\nrequirement. For such instances, we derive the best possible positive result,\nnamely, a polynomial time approximation scheme. We further show that the\ncontiguous variant admits a $(\\frac{5}{4} + \\varepsilon)$-approximation\nalgorithm, for any fixed $\\varepsilon > 0$, on instances whose job intervals\nform a proper interval graph. At the heart of the algorithm lies a non-standard\nparameterization of the approximation ratio itself, which is of independent\ninterest.\n  For the non-contiguous case, we uncover an interesting relation to the paging\nproblem that leads to a simple $O(n \\log n)$ algorithm for uniform profit\ninstances of n jobs. The algorithm is easy to implement and is thus practical.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 23:17:06 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Katz", "Dmitriy", ""], ["Schieber", "Baruch", ""], ["Shachnai", "Hadas", ""]]}, {"id": "1605.05067", "submitter": "Simone Faro", "authors": "Simone Faro", "title": "Exact Online String Matching Bibliography", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note we present a comprehensive bibliography for the online\nexact string matching problem. The problem consists in finding all occurrences\nof a given pattern in a text. It is an extensively studied problem in computer\nscience, mainly due to its direct applications to such diverse areas as text,\nimage and signal processing, speech analysis and recognition, data compression,\ninformation retrieval, computational biology and chemistry. Since 1970 more\nthan 120 string matching algorithms have been proposed. In this note we present\na comprehensive list of (almost) all string matching algorithms. The list is\nupdated to May 2016.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 09:14:50 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Faro", "Simone", ""]]}, {"id": "1605.05102", "submitter": "Kunihiro Wasa", "authors": "Kunihiro Wasa", "title": "Enumeration of Enumeration Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we enumerate enumeration problems and algorithms. This survey\nis under construction. If you know some results not in this survey or there is\nanything wrong, please let me know.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 10:43:44 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 09:27:57 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Wasa", "Kunihiro", ""]]}, {"id": "1605.05109", "submitter": "Seri Khoury", "authors": "Amir Abboud, Keren Censor-Hillel and Seri Khoury", "title": "Near-Linear Lower Bounds for Distributed Distance Computations, Even in\n  Sparse Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new technique for constructing sparse graphs that allow us to\nprove near-linear lower bounds on the round complexity of computing distances\nin the CONGEST model. Specifically, we show an $\\widetilde{\\Omega}(n)$ lower\nbound for computing the diameter in sparse networks, which was previously known\nonly for dense networks [Frishknecht et al., SODA 2012]. In fact, we can even\nmodify our construction to obtain graphs with constant degree, using a simple\nbut powerful degree-reduction technique which we define.\n  Moreover, our technique allows us to show $\\widetilde{\\Omega}(n)$ lower\nbounds for computing $(\\frac{3}{2}-\\varepsilon)$-approximations of the diameter\nor the radius, and for computing a $(\\frac{5}{3}-\\varepsilon)$-approximation of\nall eccentricities. For radius, we are unaware of any previous lower bounds.\nFor diameter, these greatly improve upon previous lower bounds and are tight up\nto polylogarithmic factors [Frishknecht et al., SODA 2012], and for\neccentricities the improvement is both in the lower bound and in the\napproximation factor [Holzer and Wattenhofer, PODC 2012].\n  Interestingly, our technique also allows showing an almost-linear lower bound\nfor the verification of $(\\alpha,\\beta)$-spanners, for $\\alpha < \\beta+1$.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 11:02:16 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Abboud", "Amir", ""], ["Censor-Hillel", "Keren", ""], ["Khoury", "Seri", ""]]}, {"id": "1605.05231", "submitter": "Junqi Tang", "authors": "Junqi Tang", "title": "The Non-uniform Fast Fourier Transform in Computed Tomography", "comments": "50 pages. A Masters thesis achieved in the Institute of Digital\n  Communications, the University of Edinburgh. Computer Science/Computation\n  Complexity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project is aimed at designing the fast forward projection algorithm and\nalso the backprojection algorithm for cone beam CT imaging systems with\ncircular X-ray source trajectory. The principle of the designs is based on\nutilizing the potential computational efficiency which the Fourier Slice\nTheorem and the Non-uniform Fast Fourier Transform (NUFFT) will bring forth. In\nthis Masters report, the detailed design of the NUFFT based forward projector\nincluding a novel 3D (derivative of) Radon space resampling method will be\ngiven. Meanwhile the complexity of the NUFFT based forward projector is\nanalysed and compared with the non-Fourier based CT projector, and the\nadvantage of the NUFFT based forward projection in terms of the computational\nefficiency is demonstrated in this report. Base on the design of the forward\nalgorithm, the NUFFT based 3D direct reconstruction algorithm will be derived.\nThe experiments will be taken to test the performance of the forward algorithm\nand the backprojection algorithm to show the practicability and accuracy of\nthese designs by comparing them jointly with the well-acknowledged cone beam CT\noperators: the CT linear interpolation forward projector and the FDK algorithm.\nThis Master report will demonstrate a novel and efficient way of implementing\nthe cone beam CT operator, a detailed summary of the project, and the future\nresearch prospects of the NUFFT based cone beam CT algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 16:39:54 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Tang", "Junqi", ""]]}, {"id": "1605.05236", "submitter": "William Kuszmaul", "authors": "William Kuszmaul", "title": "Fast Concurrent Cuckoo Kick-Out Eviction Schemes for High-Density Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cuckoo hashing guarantees constant-time lookups regardless of table density,\nmaking it a viable candidate for high-density tables. Cuckoo hashing insertions\nperform poorly at high table densities, however. In this paper, we mitigate\nthis problem through the introduction of novel kick-out eviction algorithms.\nExperimentally, our algorithms reduce the number of bins viewed per insertion\nfor high-density tables by as much as a factor of ten.\n  We also introduce an optimistic concurrency scheme for transactional\nmulti-writer cuckoo hash tables (not using hardware transactional memory). For\ndelete-light workloads, one of our kick-out algorithms avoids all competition\nbetween insertions with high probability, and significantly reduces\ntransaction-abort frequency. This result is extended to arbitrary workloads\nusing a new synchronization mechanism called a claim flag.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 16:55:43 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Kuszmaul", "William", ""]]}, {"id": "1605.05404", "submitter": "Matthias Petri", "authors": "Simon Gog and Alistair Moffat and Matthias Petri", "title": "CSA++: Fast Pattern Search for Large Alphabets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexed pattern search in text has been studied for many decades. For small\nalphabets, the FM-Index provides unmatched performance, in terms of both space\nrequired and search speed. For large alphabets -- for example, when the tokens\nare words -- the situation is more complex, and FM-Index representations are\ncompact, but potentially slow. In this paper we apply recent innovations from\nthe field of inverted indexing and document retrieval to compressed pattern\nsearch, including for alphabets into the millions. Commencing with the\npractical compressed suffix array structure developed by Sadakane, we show that\nthe Elias-Fano code-based approach to document indexing can be adapted to\nprovide new tradeoff options in indexed pattern search, and offers\nsignificantly faster pattern processing compared to previous implementations,\nas well as reduced space requirements. We report a detailed experimental\nevaluation that demonstrates the relative advantages of the new approach, using\nthe standard Pizza&Chili methodology and files, as well as applied use-cases\nderived from large-scale data compression, and from natural language\nprocessing. For large alphabets, the new structure gives rise to space\nrequirements that are close to those of the most highly-compressed FM-Index\nvariants, in conjunction with unparalleled search throughput rates.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 00:20:00 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Gog", "Simon", ""], ["Moffat", "Alistair", ""], ["Petri", "Matthias", ""]]}, {"id": "1605.05436", "submitter": "Michael Goodrich", "authors": "Michael T. Goodrich, Ahmed Eldawy", "title": "Parallel Algorithms for Summing Floating-Point Numbers", "comments": "Conference version appears in SPAA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of exactly summing n floating-point numbers is a fundamental\nproblem that has many applications in large-scale simulations and computational\ngeometry. Unfortunately, due to the round-off error in standard floating-point\noperations, this problem becomes very challenging. Moreover, all existing\nsolutions rely on sequential algorithms which cannot scale to the huge datasets\nthat need to be processed.\n  In this paper, we provide several efficient parallel algorithms for summing n\nfloating point numbers, so as to produce a faithfully rounded floating-point\nrepresentation of the sum. We present algorithms in PRAM, external-memory, and\nMapReduce models, and we also provide an experimental analysis of our MapReduce\nalgorithms, due to their simplicity and practical efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 04:20:41 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Goodrich", "Michael T.", ""], ["Eldawy", "Ahmed", ""]]}, {"id": "1605.05773", "submitter": "Richard Barnes", "authors": "Richard Barnes, Clarence Lehman, David Mulla", "title": "Distributed Parallel D8 Up-Slope Area Calculation in Digital Elevation\n  Models", "comments": "6 pages, 2 figures, 8 algorithms", "journal-ref": "Proceedings of the 2011 International Conference on Parallel and\n  Distributed Processing Techniques and Applications. Las Vegas, NV. p. 833-838", "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a parallel algorithm for calculating the\neight-directional (D8) up-slope contributing area in digital elevation models\n(DEMs). In contrast with previous algorithms, which have potentially unbounded\ninter-node communications, the algorithm presented here realizes strict bounds\non the number of inter-node communications. Those bounds in turn allow D8\nattributes to be processed for arbitrarily large DEMs on hardware ranging from\naverage desktops to supercomputers. The algorithm can use the OpenMP and MPI\nparallel computing models, either in combination or separately. It partitions\nthe DEM between slave nodes, calculates an internal up-slope area by replacing\ninformation from other slaves with variables representing unknown quantities,\npasses the results on to a master node which combines all the slaves' data, and\npasses information back to each slave, which then computes its final result. In\nthis way each slave's DEM partition is treated as a simple unit in the DEM as a\nwhole and only two communications take place per node.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 22:13:03 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Barnes", "Richard", ""], ["Lehman", "Clarence", ""], ["Mulla", "David", ""]]}, {"id": "1605.05944", "submitter": "Kimmo Fredriksson", "authors": "Kimmo Fredriksson", "title": "Geometric Near-neighbor Access Tree (GNAT) revisited", "comments": "Minor changes, submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric Near-neighbor Access Tree (GNAT) is a metric space indexing method\nbased on hierarchical hyperplane partitioning of the space. While GNAT is very\nefficient in proximity searching, it has a bad reputation of being a memory\nhog. We show that this is partially based on too coarse analysis, and that the\nmemory requirements can be lowered while at the same time improving the search\nefficiency. We also show how to make GNAT memory adaptive in a smooth way, and\nthat the hyperplane partitioning can be replaced with ball partitioning, which\ncan further improve the search performance. We conclude with experimental\nresults showing the new methods can give significant performance boost.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 13:31:36 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 07:35:48 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Fredriksson", "Kimmo", ""]]}, {"id": "1605.06072", "submitter": "Alan Frieze", "authors": "Alan Frieze, Wesley Pegden", "title": "Online purchasing under uncertainty", "comments": "Minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose there is a collection $x_1,x_2,\\dots,x_N$ of independent uniform\n$[0,1]$ random variables, and a hypergraph $\\cF$ of \\emph{target structures} on\nthe vertex set $\\{1,\\dots,N\\}$. We would like to buy a target structure at\nsmall cost, but we do not know all the costs $x_i$ ahead of time. Instead, we\ninspect the random variables $x_i$ one at a time, and after each inspection,\nchoose to either keep the vertex $i$ at cost $x_i$, or reject vertex $i$\nforever.\n  In the present paper, we consider the case where $\\{1,\\dots,N\\}$ is the\nedge-set of some graph, and the target structures are the spanning trees of a\ngraph, spanning arborescences of a digraph, the paths between a fixed pair of\nvertices, perfect matchings, Hamilton cycles or the cliques of some fixed size.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 18:08:10 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 13:46:37 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Frieze", "Alan", ""], ["Pegden", "Wesley", ""]]}, {"id": "1605.06183", "submitter": "David Tian", "authors": "Wenhong Tian", "title": "On Polynomial Time Approximation Bounded Solution for TSP and NP\n  Complete Problems", "comments": "9. arXiv admin note: text overlap with arXiv:1502.00447", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The question of whether all problems in NP class are also in P class is\ngenerally considered one of the most important open questions in mathematics\nand theoretical computer science as it has far-reaching consequences to other\nproblems in mathematics, computer science, biology, philosophy and\ncryptography. There are intensive research on proving `NP not equal to P' and\n`NP equals to P'. However, none of the `proved' results is commonly accepted by\nthe research community up to date. In this paper, motived by approximability of\ntraveling salesman problem (TSP) in polynomial time, we aim to provide a new\nperspective: showing that NP=P from polynomial time approximation-bounded\nsolutions of TSP in Euclidean space.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 00:34:38 GMT"}, {"version": "v2", "created": "Sun, 18 Dec 2016 09:56:53 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Tian", "Wenhong", ""]]}, {"id": "1605.06327", "submitter": "Kyle Burke", "authors": "Mara Bovee, Kyle Burke, and Craig Tennenhouse", "title": "Games from Basic Data Structures", "comments": "11 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider combinatorial game rulesets based on data\nstructures normally covered in an undergraduate Computer Science Data\nStructures course: arrays, stacks, queues, priority queues, sets, linked lists,\nand binary trees. We describe many rulesets as well as computational and\nmathematical properties about them. Two of the rulesets, Tower Nim and Myopic\nCol, are new. We show polynomial-time solutions to Tower Nim and to Myopic Col\non paths.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 12:41:13 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Bovee", "Mara", ""], ["Burke", "Kyle", ""], ["Tennenhouse", "Craig", ""]]}, {"id": "1605.06423", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, Trevor Campbell, Tamara Broderick", "title": "Coresets for Scalable Bayesian Logistic Regression", "comments": "In Proceedings of Advances in Neural Information Processing Systems\n  (NIPS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Bayesian methods in large-scale data settings is attractive\nbecause of the rich hierarchical models, uncertainty quantification, and prior\nspecification they provide. Standard Bayesian inference algorithms are\ncomputationally expensive, however, making their direct application to large\ndatasets difficult or infeasible. Recent work on scaling Bayesian inference has\nfocused on modifying the underlying algorithms to, for example, use only a\nrandom data subsample at each iteration. We leverage the insight that data is\noften redundant to instead obtain a weighted subset of the data (called a\ncoreset) that is much smaller than the original dataset. We can then use this\nsmall coreset in any number of existing posterior inference algorithms without\nmodification. In this paper, we develop an efficient coreset construction\nalgorithm for Bayesian logistic regression models. We provide theoretical\nguarantees on the size and approximation quality of the coreset -- both for\nfixed, known datasets, and in expectation for a wide class of data generative\nmodels. Crucially, the proposed approach also permits efficient construction of\nthe coreset in both streaming and parallel settings, with minimal additional\neffort. We demonstrate the efficacy of our approach on a number of synthetic\nand real-world datasets, and find that, in practice, the size of the coreset is\nindependent of the original dataset size. Furthermore, constructing the coreset\ntakes a negligible amount of time compared to that required to run MCMC on it.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 16:26:45 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 14:12:19 GMT"}, {"version": "v3", "created": "Mon, 6 Feb 2017 15:11:30 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Campbell", "Trevor", ""], ["Broderick", "Tamara", ""]]}, {"id": "1605.06615", "submitter": "Travis Gagie", "authors": "Antonio Fari\\~na, Travis Gagie, Szymon Grabowski, Giovanni Manzini,\n  Gonzalo Navarro and Alberto Ord\\'o\\~nez", "title": "Efficient and Compact Representations of Some Non-Canonical Prefix-Free\n  Codes", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. A preliminary version was\n  presented at the 23rd International Symposium on String Processing and\n  Information Retrieval (SPIRE '16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many kinds of prefix-free codes there are efficient and compact\nalternatives to the traditional tree-based representation. Since these put the\ncodes into canonical form, however, they can only be used when we can choose\nthe order in which codewords are assigned to symbols. In this paper we first\nshow how, given a probability distribution over an alphabet of $\\sigma$\nsymbols, we can store an optimal alphabetic prefix-free code in $\\Oh{\\sigma\n\\log L}$ bits such that we can encode and decode any codeword of length $\\ell$\nin $\\Oh{\\min (\\ell, \\log L)}$ time, where $L$ is the maximum codeword length.\nWith $\\Oh{2^{L^\\epsilon}}$ further bits, for any constant $\\epsilon>0$, we can\nencode and decode $\\Oh{\\log \\ell}$ time. We then show how to store a nearly\noptimal alphabetic prefix-free code in \\(o (\\sigma)\\) bits such that we can\nencode and decode in constant time. We also consider a kind of optimal\nprefix-free code introduced recently where the codewords' lengths are\nnon-decreasing if arranged in lexicographic order of their reverses. We reduce\ntheir storage space to $\\Oh{\\sigma \\log L}$ while maintaining encoding and\ndecoding times in $\\Oh{\\ell}$. We also show how, with $\\Oh{2^{\\epsilon L}}$\nfurther bits, we can encode and decode in constant time. All of our results\nhold in the word-RAM model.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 10:02:19 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 23:38:21 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 13:41:06 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Fari\u00f1a", "Antonio", ""], ["Gagie", "Travis", ""], ["Grabowski", "Szymon", ""], ["Manzini", "Giovanni", ""], ["Navarro", "Gonzalo", ""], ["Ord\u00f3\u00f1ez", "Alberto", ""]]}, {"id": "1605.06702", "submitter": "Henry Cohn", "authors": "Jonah Blasiak, Thomas Church, Henry Cohn, Joshua A. Grochow, Eric\n  Naslund, William F. Sawin, Chris Umans", "title": "On cap sets and the group-theoretic approach to matrix multiplication", "comments": "27 pages", "journal-ref": "Discrete Analysis, 2017:3, 27 pp", "doi": "10.19086/da.1245", "report-no": null, "categories": "math.CO cs.DS math.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In 2003, Cohn and Umans described a framework for proving upper bounds on the\nexponent $\\omega$ of matrix multiplication by reducing matrix multiplication to\ngroup algebra multiplication, and in 2005 Cohn, Kleinberg, Szegedy, and Umans\nproposed specific conjectures for how to obtain $\\omega=2$. In this paper we\nrule out obtaining $\\omega=2$ in this framework from abelian groups of bounded\nexponent. To do this we bound the size of tricolored sum-free sets in such\ngroups, extending the breakthrough results of Croot, Lev, Pach, Ellenberg, and\nGijswijt on cap sets. As a byproduct of our proof, we show that a variant of\ntensor rank due to Tao gives a quantitative understanding of the notion of\nunstable tensor from geometric invariant theory.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 21:28:27 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 18:56:52 GMT"}, {"version": "v3", "created": "Wed, 10 Aug 2016 09:56:10 GMT"}, {"version": "v4", "created": "Sat, 14 Jan 2017 23:54:19 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Blasiak", "Jonah", ""], ["Church", "Thomas", ""], ["Cohn", "Henry", ""], ["Grochow", "Joshua A.", ""], ["Naslund", "Eric", ""], ["Sawin", "William F.", ""], ["Umans", "Chris", ""]]}, {"id": "1605.06852", "submitter": "Arnold Filtser", "authors": "Arnold Filtser and Shay Solomon", "title": "The Greedy Spanner is Existentially Optimal", "comments": "Preliminary version appeared in PODC 2016. To appear in SICOMP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The greedy spanner is arguably the simplest and most well-studied spanner\nconstruction. Experimental results demonstrate that it is at least as good as\nany other spanner construction, in terms of both the size and weight\nparameters. However, a rigorous proof for this statement has remained elusive.\n  In this work we fill in the theoretical gap via a surprisingly simple\nobservation: The greedy spanner is \\emph{existentially optimal} (or\nexistentially near-optimal) for several important graph families, in terms of\nboth the size and weight. Roughly speaking, the greedy spanner is said to be\nexistentially optimal (or near-optimal) for a graph family $\\mathcal G$ if the\nworst performance of the greedy spanner over all graphs in $\\mathcal G$ is just\nas good (or nearly as good) as the worst performance of an optimal spanner over\nall graphs in $\\mathcal G$.\n  Focusing on the weight parameter, the state-of-the-art spanner constructions\nfor both general graphs (due to Chechik and Wulff-Nilsen [SODA'16]) and\ndoubling metrics (due to Gottlieb [FOCS'15]) are complex. Plugging our\nobservation on these results, we conclude that the greedy spanner achieves\nnear-optimal weight guarantees for both general graphs and doubling metrics,\nthus resolving two longstanding conjectures in the area.\n  Further, we observe that approximate-greedy spanners are existentially\nnear-optimal as well. Consequently, we provide an $O(n \\log n)$-time\nconstruction of $(1+\\epsilon)$-spanners for doubling metrics with constant\nlightness and degree. Our construction improves Gottlieb's construction, whose\nruntime is $O(n \\log^2 n)$ and whose number of edges and degree are unbounded,\nand remarkably, it matches the state-of-the-art Euclidean result (due to\nGudmundsson et al.\\ [SICOMP'02]) in all the involved parameters (up to\ndependencies on $\\epsilon$ and the dimension).\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 20:45:32 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 22:59:45 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Filtser", "Arnold", ""], ["Solomon", "Shay", ""]]}, {"id": "1605.06950", "submitter": "James Newling", "authors": "James Newling, Fran\\c{c}ois Fleuret", "title": "A Sub-Quadratic Exact Medoid Algorithm", "comments": "Version 2: Added acknowledgements, Version 3: Post-acceptance at\n  AISTATS 2017, Version 4: N-1 -> N denominator correction", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm, trimed, for obtaining the medoid of a set, that\nis the element of the set which minimises the mean distance to all other\nelements. The algorithm is shown to have, under certain assumptions, expected\nrun time O(N^(3/2)) in R^d where N is the set size, making it the first\nsub-quadratic exact medoid algorithm for d>1. Experiments show that it performs\nvery well on spatial network data, frequently requiring two orders of magnitude\nfewer distance calculations than state-of-the-art approximate algorithms. As an\napplication, we show how trimed can be used as a component in an accelerated\nK-medoids algorithm, and then how it can be relaxed to obtain further\ncomputational gains with only a minor loss in cluster quality.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 09:24:59 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 07:44:29 GMT"}, {"version": "v3", "created": "Tue, 11 Apr 2017 07:55:33 GMT"}, {"version": "v4", "created": "Wed, 12 Apr 2017 18:25:34 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Newling", "James", ""], ["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "1605.06992", "submitter": "Sourav Dutta", "authors": "Sourav Dutta", "title": "SONIK: Efficient In-situ All Item Rank Generation using Bit Operations", "comments": "12 pages, 2 figures, WCSE 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting, a classical combinatorial process, forms the bedrock of numerous\nalgorithms with varied applications. A related problem involves efficiently\nfinding the corresponding ranks of all the elements - catering to rank queries,\ndata partitioning and allocation, etc. Although, the element ranks can be\nsubsequently obtained by initially sorting the elements, such procedures\ninvolve O(n log n) computations and might not be suitable with large input\nsizes for hard real-time systems or for applications with data re-ordering\nconstraints. This paper proposes SONIK, a non-comparison linear time and space\nalgorithm using bit operations inspired by radix sort for computing the ranks\nof all input integer elements, thereby providing implicit sorting. The element\nranks are generated in-situ, i.e., directly at the corresponding element\nposition without re-ordering or recourse to any other sorting mechanism.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 12:24:16 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 08:18:13 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Dutta", "Sourav", ""]]}, {"id": "1605.07030", "submitter": "Victor Sadikov", "authors": "Victor Sadikov and Oliver Rutishauser", "title": "Isotropic Dynamic Hierarchical Clustering", "comments": "6 pages with 3 examples", "journal-ref": "GJCST-C Volume 16 Issue 3 pp 24-32 y 2016", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We face a need of discovering a pattern in locations of a great number of\npoints in a high-dimensional space. Goal is to group the close points together.\nWe are interested in a hierarchical structure, like a B-tree. B-Trees are\nhierarchical, balanced, and they can be constructed dynamically. B-Tree\napproach allows to determine the structure without any supervised learning or a\npriori knowlwdge. The space is Euclidean and isotropic. Unfortunately, there\nare no B-Tree implementations processing indices in a symmetrical and\nisotropical way. Some implementations are based on constructing compound\nasymmetrical indices from point coordinates; and the others split the nodes\nalong the coordinate hyper-planes. We need to process tens of millions of\npoints in a thousand-dimensional space. The application has to be scalable.\nIdeally, a cluster should be an ellipsoid, but it would require to store O(n2)\nellipse axes. So, we are using multi-dimensional balls defined by the centers\nand radii. Calculation of statistical values like the mean and the average\ndeviation, can be done in an incremental way. While adding a point to a tree,\nthe statistical values for nodes recalculated in O(1) time. We support both,\nbrute force O(2n) and greedy O(n2) split algorithms. Statistical and aggregated\nnode information also allows to manipulate (to search, to delete) aggregated\nsets of closely located points. Hierarchical information retrieval. When\nsearching, the user is provided with the highest appropriate nodes in the tree\nhierarchy, with the most important clusters emerging in the hierarchy\nautomatically. Then, if interested, the user may navigate down the tree to more\nspecific points. The system is implemented as a library of Java classes\nrepresenting Points, Sets of points with aggregated statistical information,\nB-tree, and Nodes with a support of serialization and storage in a MySQL\ndatabase.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 14:32:33 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Sadikov", "Victor", ""], ["Rutishauser", "Oliver", ""]]}, {"id": "1605.07162", "submitter": "Lijie Chen", "authors": "Lijie Chen, Anupam Gupta, Jian Li", "title": "Pure Exploration of Multi-armed Bandit Under Matroid Constraints", "comments": "Accepted for presentation at Conference on Learning Theory (COLT)\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the pure exploration problem subject to a matroid constraint\n(Best-Basis) in a stochastic multi-armed bandit game. In a Best-Basis instance,\nwe are given $n$ stochastic arms with unknown reward distributions, as well as\na matroid $\\mathcal{M}$ over the arms. Let the weight of an arm be the mean of\nits reward distribution. Our goal is to identify a basis of $\\mathcal{M}$ with\nthe maximum total weight, using as few samples as possible.\n  The problem is a significant generalization of the best arm identification\nproblem and the top-$k$ arm identification problem, which have attracted\nsignificant attentions in recent years. We study both the exact and PAC\nversions of Best-Basis, and provide algorithms with nearly-optimal sample\ncomplexities for these versions. Our results generalize and/or improve on\nseveral previous results for the top-$k$ arm identification problem and the\ncombinatorial pure exploration problem when the combinatorial constraint is a\nmatroid.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 19:51:42 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 19:20:41 GMT"}, {"version": "v3", "created": "Wed, 25 May 2016 16:03:23 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Chen", "Lijie", ""], ["Gupta", "Anupam", ""], ["Li", "Jian", ""]]}, {"id": "1605.07172", "submitter": "Shreyas Sekar", "authors": "Shreyas Sekar, Milan Vojnovic, Se-Young Yun", "title": "Submodular Maximization using Test Scores", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the canonical problem of maximizing a stochastic submodular function\nsubject to a cardinality constraint, where the goal is to select a subset from\na ground set of items with uncertain individual performances to maximize their\nexpected group value. Although near-optimal algorithms have been proposed for\nthis problem, practical concerns regarding scalability, compatibility with\ndistributed implementation, and expensive oracle queries persist in large-scale\napplications. Motivated by online platforms that rely on individual item scores\nfor content recommendation and team selection, we propose a special class of\nalgorithms that select items based solely on individual performance measures\nknown as test scores. The central contribution of this work is a novel and\nsystematic framework for designing test score based algorithms for a broad\nclass of naturally occurring utility functions. We introduce a new scoring\nmechanism that we refer to as replication test scores and prove that as long as\nthe objective function satisfies a diminishing returns property, one can\nleverage these scores to compute solutions that are within a constant factor of\nthe optimum. We then extend our results to the more general stochastic\nsubmodular welfare maximization problem, where the goal is to select items and\nassign them to multiple groups to maximize the sum of the expected group\nvalues. For this more difficult problem, we show that replication test scores\ncan be used to develop an algorithm that approximates the optimum solution up\nto a logarithmic factor. The techniques presented in this work bridge the gap\nbetween the rigorous theoretical work on submodular optimization and simple,\nscalable heuristics that are useful in certain domains.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 15:51:59 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 06:21:50 GMT"}, {"version": "v3", "created": "Sun, 14 Oct 2018 02:20:10 GMT"}, {"version": "v4", "created": "Thu, 9 May 2019 15:46:27 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Sekar", "Shreyas", ""], ["Vojnovic", "Milan", ""], ["Yun", "Se-Young", ""]]}, {"id": "1605.07220", "submitter": "Cewei Cui", "authors": "Cewei Cui and Zhe Dang", "title": "Canonical Number and NutCracker: Heuristic Algorithms for the Graph\n  Isomorphism Problem using Free Energy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops two heuristic algorithms to solve graph isomorphism,\nusing free energy encoding. The first algorithm uses four types of encoding\nrefinement techniques such that every graph can be distinguished by a canonical\nnumber computed by the algorithm. The second algorithm injects energy into the\ngraph to conduct individualization such that the correspondence relation\nbetween a pair of isomorphic graphs can be found. The core principle behind the\ntwo algorithms is encoding discrete structures as real numbers. A large set of\nexperiments demonstrated the effectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 22:04:40 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 09:14:03 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Cui", "Cewei", ""], ["Dang", "Zhe", ""]]}, {"id": "1605.07272", "submitter": "Tengyu Ma", "authors": "Rong Ge, Jason D. Lee, Tengyu Ma", "title": "Matrix Completion has No Spurious Local Minimum", "comments": "NIPS'16 best student paper. fixed Theorem 2.3 in preliminary section\n  in the previous version. The results are not affected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion is a basic machine learning problem that has wide\napplications, especially in collaborative filtering and recommender systems.\nSimple non-convex optimization algorithms are popular and effective in\npractice. Despite recent progress in proving various non-convex algorithms\nconverge from a good initial point, it remains unclear why random or arbitrary\ninitialization suffices in practice. We prove that the commonly used non-convex\nobjective function for \\textit{positive semidefinite} matrix completion has no\nspurious local minima --- all local minima must also be global. Therefore, many\npopular optimization algorithms such as (stochastic) gradient descent can\nprovably solve positive semidefinite matrix completion with \\textit{arbitrary}\ninitialization in polynomial time. The result can be generalized to the setting\nwhen the observed entries contain noise. We believe that our main proof\nstrategy can be useful for understanding geometric properties of other\nstatistical problems involving partial or noisy observations.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 02:53:27 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 19:58:48 GMT"}, {"version": "v3", "created": "Sun, 29 Jan 2017 18:45:48 GMT"}, {"version": "v4", "created": "Sun, 22 Jul 2018 05:20:12 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Ge", "Rong", ""], ["Lee", "Jason D.", ""], ["Ma", "Tengyu", ""]]}, {"id": "1605.07285", "submitter": "Andrea Lincoln", "authors": "Andrea Lincoln, Virginia Vassilevska Williams, Joshua R. Wang, R. Ryan\n  Williams", "title": "Deterministic Time-Space Tradeoffs for k-SUM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of numbers, the $k$-SUM problem asks for a subset of $k$ numbers\nthat sums to zero. When the numbers are integers, the time and space complexity\nof $k$-SUM is generally studied in the word-RAM model; when the numbers are\nreals, the complexity is studied in the real-RAM model, and space is measured\nby the number of reals held in memory at any point.\n  We present a time and space efficient deterministic self-reduction for the\n$k$-SUM problem which holds for both models, and has many interesting\nconsequences. To illustrate:\n  * $3$-SUM is in deterministic time $O(n^2 \\lg\\lg(n)/\\lg(n))$ and space\n$O\\left(\\sqrt{\\frac{n \\lg(n)}{\\lg\\lg(n)}}\\right)$. In general, any\npolylogarithmic-time improvement over quadratic time for $3$-SUM can be\nconverted into an algorithm with an identical time improvement but low space\ncomplexity as well. * $3$-SUM is in deterministic time $O(n^2)$ and space\n$O(\\sqrt n)$, derandomizing an algorithm of Wang.\n  * A popular conjecture states that 3-SUM requires $n^{2-o(1)}$ time on the\nword-RAM. We show that the 3-SUM Conjecture is in fact equivalent to the\n(seemingly weaker) conjecture that every $O(n^{.51})$-space algorithm for\n$3$-SUM requires at least $n^{2-o(1)}$ time on the word-RAM.\n  * For $k \\ge 4$, $k$-SUM is in deterministic $O(n^{k - 2 + 2/k})$ time and\n$O(\\sqrt{n})$ space.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 04:43:14 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Lincoln", "Andrea", ""], ["Williams", "Virginia Vassilevska", ""], ["Wang", "Joshua R.", ""], ["Williams", "R. Ryan", ""]]}, {"id": "1605.07503", "submitter": "Carlos Barron-Romero Prof.", "authors": "Carlos Barr\\'on-Romero", "title": "A novel algorithm for solving the Decision Boolean Satisfiability\n  Problem without algebra", "comments": "arXiv admin note: text overlap with arXiv:1602.06867 Published in\n  COMTEL 2016 (http://www.comtel.pe/memoriacomtel/COMTEL2016.pdf) See in\n  http://academicos.azc.uam.mx/cbr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper depicts an algorithm for solving the Decision Boolean\nSatisfiability Problem using the binary numerical properties of a Special\nDecision Satisfiability Problem, parallel execution, object oriented, and short\ntermination. The two operations: expansion and simplification are used to\nexplains why using algebra grows the resolution steps. It is proved that its\ncomplexity has an upper bound of $2^{n-1}$ where $n$ is the number of logical\nvariables of the given problem.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 02:06:46 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 20:46:29 GMT"}, {"version": "v3", "created": "Sat, 28 May 2016 09:03:36 GMT"}, {"version": "v4", "created": "Tue, 31 May 2016 00:18:18 GMT"}, {"version": "v5", "created": "Mon, 16 Jan 2017 02:13:50 GMT"}, {"version": "v6", "created": "Sun, 26 Nov 2017 21:36:45 GMT"}, {"version": "v7", "created": "Sat, 14 Apr 2018 08:58:50 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Barr\u00f3n-Romero", "Carlos", ""]]}, {"id": "1605.07583", "submitter": "Cameron Musco", "authors": "Cameron Musco and Christopher Musco", "title": "Recursive Sampling for the Nystr\\\"om Method", "comments": "To appear, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first algorithm for kernel Nystr\\\"om approximation that runs in\n*linear time in the number of training points* and is provably accurate for all\nkernel matrices, without dependence on regularity or incoherence conditions.\nThe algorithm projects the kernel onto a set of $s$ landmark points sampled by\ntheir *ridge leverage scores*, requiring just $O(ns)$ kernel evaluations and\n$O(ns^2)$ additional runtime. While leverage score sampling has long been known\nto give strong theoretical guarantees for Nystr\\\"om approximation, by employing\na fast recursive sampling scheme, our algorithm is the first to make the\napproach scalable. Empirically we show that it finds more accurate, lower rank\nkernel approximations in less time than popular techniques such as uniformly\nsampled Nystr\\\"om approximation and the random Fourier features method.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 18:56:57 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 19:48:44 GMT"}, {"version": "v3", "created": "Tue, 28 Feb 2017 16:37:17 GMT"}, {"version": "v4", "created": "Thu, 16 Mar 2017 17:58:14 GMT"}, {"version": "v5", "created": "Fri, 3 Nov 2017 14:40:15 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Musco", "Cameron", ""], ["Musco", "Christopher", ""]]}, {"id": "1605.07959", "submitter": "Tom\\'a\\v{s} Toufar", "authors": "Tom\\'a\\v{s} Masa\\v{r}\\'ik, Tom\\'a\\v{s} Toufar", "title": "Parameterized complexity of fair deletion problems", "comments": "17 pages. The hardness results from v1 were extended to FO logic", "journal-ref": "Discrete Applied Mathematics 278 (2020) 51-61", "doi": "10.1016/j.dam.2019.06.001", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deletion problems are those where given a graph $G$ and a graph property\n$\\pi$, the goal is to find a subset of edges such that after its removal the\ngraph $G$ will satisfy the property $\\pi$. Typically, we want to minimize the\nnumber of elements removed. In fair deletion problems we change the objective:\nwe minimize the maximum number of deletions in a neighborhood of a single\nvertex.\n  We study the parameterized complexity of fair deletion problems with respect\nto the structural parameters of the tree-width, the path-width, the size of a\nminimum feedback vertex set, the neighborhood diversity, and the size of\nminimum vertex cover of graph $G$. We prove the W[1]-hardness of the fair FO\nvertex-deletion problem with respect to the first three parameters combined.\nMoreover, we show that there is no algorithm for fair FO vertex-deletion\nproblem running in time $n^{o(k^{1/3})}$, where $n$ is the size of the graph\nand $k$ is the sum of the first three mentioned parameters, provided that the\nExponential Time Hypothesis holds.\n  On the other hand, we provide an FPT algorithm for the fair MSO edge-deletion\nproblem parameterized by the size of minimum vertex cover and an FPT algorithm\nfor the fair MSO vertex-deletion problem parameterized by the neighborhood\ndiversity\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 16:34:29 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 15:43:56 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Masa\u0159\u00edk", "Tom\u00e1\u0161", ""], ["Toufar", "Tom\u00e1\u0161", ""]]}, {"id": "1605.07990", "submitter": "Thang N. Dinh", "authors": "Hung T. Nguyen, My T. Thai, and Thang N. Dinh", "title": "Stop-and-Stare: Optimal Sampling Algorithms for Viral Marketing in\n  Billion-scale Networks", "comments": "Correct the errors in the proofs for SSA/D-SSA. Update D-SSA to\n  estimate \\epsilon(s) instead of \\delta(s)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Influence Maximization (IM), that seeks a small set of key users who spread\nthe influence widely into the network, is a core problem in multiple domains.\nIt finds applications in viral marketing, epidemic control, and assessing\ncascading failures within complex systems. Despite the huge amount of effort,\nIM in billion-scale networks such as Facebook, Twitter, and World Wide Web has\nnot been satisfactorily solved. Even the state-of-the-art methods such as TIM+\nand IMM may take days on those networks.\n  In this paper, we propose SSA and D-SSA, two novel sampling frameworks for\nIM-based viral marketing problems. SSA and D-SSA are up to 1200 times faster\nthan the SIGMOD'15 best method, IMM, while providing the same\n$(1-1/e-\\epsilon)$ approximation guarantee. Underlying our frameworks is an\ninnovative Stop-and-Stare strategy in which they stop at exponential check\npoints to verify (stare) if there is adequate statistical evidence on the\nsolution quality. Theoretically, we prove that SSA and D-SSA are the first\napproximation algorithms that use (asymptotically) minimum numbers of samples,\nmeeting strict theoretical thresholds characterized for IM. The absolute\nsuperiority of SSA and D-SSA are confirmed through extensive experiments on\nreal network data for IM and another topic-aware viral marketing problem, named\nTVM. The source code is available at https://github.com/hungnt55/Stop-and-Stare\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 18:15:01 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 14:40:39 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 05:15:27 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Nguyen", "Hung T.", ""], ["Thai", "My T.", ""], ["Dinh", "Thang N.", ""]]}, {"id": "1605.08023", "submitter": "Shiqiang Wang", "authors": "Shiqiang Wang, Murtaza Zafer, Kin K. Leung", "title": "Online Placement of Multi-Component Applications in Edge Computing\n  Environments", "comments": "This is the author's version of the paper accepted for publication in\n  IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2017.2665971", "report-no": null, "categories": "cs.DC cs.DS cs.NI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile edge computing is a new cloud computing paradigm which makes use of\nsmall-sized edge-clouds to provide real-time services to users. These mobile\nedge-clouds (MECs) are located in close proximity to users, thus enabling users\nto seamlessly access applications running on MECs. Due to the co-existence of\nthe core (centralized) cloud, users, and one or multiple layers of MECs, an\nimportant problem is to decide where (on which computational entity) to place\ndifferent components of an application. This problem, known as the application\nor workload placement problem, is notoriously hard, and therefore, heuristic\nalgorithms without performance guarantees are generally employed in common\npractice, which may unknowingly suffer from poor performance as compared to the\noptimal solution. In this paper, we address the application placement problem\nand focus on developing algorithms with provable performance bounds. We model\nthe user application as an application graph and the physical computing system\nas a physical graph, with resource demands/availabilities annotated on these\ngraphs. We first consider the placement of a linear application graph and\npropose an algorithm for finding its optimal solution. Using this result, we\nthen generalize the formulation and obtain online approximation algorithms with\npolynomial-logarithmic (poly-log) competitive ratio for tree application graph\nplacement. We jointly consider node and link assignment, and incorporate\nmultiple types of computational resources at nodes.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 19:45:06 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 14:32:51 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Wang", "Shiqiang", ""], ["Zafer", "Murtaza", ""], ["Leung", "Kin K.", ""]]}, {"id": "1605.08107", "submitter": "Omer Gold", "authors": "Omer Gold and Micha Sharir", "title": "Dominance Product and High-Dimensional Closest Pair under $L_\\infty$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $S$ of $n$ points in $\\mathbb{R}^d$, the Closest Pair problem is\nto find a pair of distinct points in $S$ at minimum distance. When $d$ is\nconstant, there are efficient algorithms that solve this problem, and fast\napproximate solutions for general $d$. However, obtaining an exact solution in\nvery high dimensions seems to be much less understood. We consider the\nhigh-dimensional $L_\\infty$ Closest Pair problem, where $d=n^r$ for some $r >\n0$, and the underlying metric is $L_\\infty$.\n  We improve and simplify previous results for $L_\\infty$ Closest Pair, showing\nthat it can be solved by a deterministic strongly-polynomial algorithm that\nruns in $O(DP(n,d)\\log n)$ time, and by a randomized algorithm that runs in\n$O(DP(n,d))$ expected time, where $DP(n,d)$ is the time bound for computing the\n{\\em dominance product} for $n$ points in $\\mathbb{R}^d$. That is a matrix $D$,\nsuch that $D[i,j] = \\bigl| \\{k \\mid p_i[k] \\leq p_j[k]\\} \\bigr|$; this is the\nnumber of coordinates at which $p_j$ dominates $p_i$. For integer coordinates\nfrom some interval $[-M, M]$, we obtain an algorithm that runs in\n$\\tilde{O}\\left(\\min\\{Mn^{\\omega(1,r,1)},\\, DP(n,d)\\}\\right)$ time, where\n$\\omega(1,r,1)$ is the exponent of multiplying an $n \\times n^r$ matrix by an\n$n^r \\times n$ matrix.\n  We also give slightly better bounds for $DP(n,d)$, by using more recent\nrectangular matrix multiplication bounds. Computing the dominance product\nitself is an important task, since it is applied in many algorithms as a major\nblack-box ingredient, such as algorithms for APBP (all pairs bottleneck paths),\nand variants of APSP (all pairs shortest paths).\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 00:23:34 GMT"}, {"version": "v2", "created": "Sat, 24 Jun 2017 05:13:36 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Gold", "Omer", ""], ["Sharir", "Micha", ""]]}, {"id": "1605.08143", "submitter": "David Lee", "authors": "Ashish Goel and David T. Lee", "title": "Towards large-scale deliberative decision-making: small groups and the\n  importance of triads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CY cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though deliberation is a critical component of democratic decision-making,\nexisting deliberative processes do not scale to large groups of people.\nMotivated by this, we propose a model in which large-scale decision-making\ntakes place through a sequence of small group interactions. Our model considers\na group of participants, each having an opinion which together form a graph. We\nshow that for median graphs, a class of graphs including grids and trees, it is\npossible to use a small number of three-person interactions to tightly\napproximate the wisdom of the crowd, defined here to be the generalized median\nof participant opinions, even when agents are strategic. Interestingly, we also\nshow that this sharply contrasts with small groups of size two, for which we\nprove an impossibility result. Specifically, we show that it is impossible to\nuse sequences of two-person interactions satisfying natural axioms to find a\ntight approximation of the generalized median, even when agents are\nnon-strategic. Our results demonstrate the potential of small group\ninteractions for reaching global decision-making properties.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 05:04:21 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 01:47:15 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Goel", "Ashish", ""], ["Lee", "David T.", ""]]}, {"id": "1605.08194", "submitter": "Jakub Pachocki", "authors": "Jakub Pachocki", "title": "Analysis of Resparsification", "comments": "preliminary draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that schemes for sparsifying matrices based on iteratively resampling\nrows yield guarantees matching classic 'offline' sparsifiers (see e.g. Spielman\nand Srivastava [STOC 2008]).\n  In particular, this gives a formal analysis of a scheme very similar to the\none proposed by Kelner and Levin [TCS 2013].\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 08:53:14 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Pachocki", "Jakub", ""]]}, {"id": "1605.08319", "submitter": "Pierre Peterlongo", "authors": "Camille Marchet and Antoine Limasset and Lucie Bittner and Pierre\n  Peterlongo", "title": "A resource-frugal probabilistic dictionary and applications in\n  (meta)genomics", "comments": "Submitted to PSC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genomic and metagenomic fields, generating huge sets of short genomic\nsequences, brought their own share of high performance problems. To extract\nrelevant pieces of information from the huge data sets generated by current\nsequencing techniques, one must rely on extremely scalable methods and\nsolutions. Indexing billions of objects is a task considered too expensive\nwhile being a fundamental need in this field. In this paper we propose a\nstraightforward indexing structure that scales to billions of element and we\npropose two direct applications in genomics and metagenomics. We show that our\nproposal solves problem instances for which no other known solution scales-up.\nWe believe that many tools and applications could benefit from either the\nfundamental data structure we provide or from the applications developed from\nthis structure.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 15:07:47 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Marchet", "Camille", ""], ["Limasset", "Antoine", ""], ["Bittner", "Lucie", ""], ["Peterlongo", "Pierre", ""]]}, {"id": "1605.08448", "submitter": "Jayson Lynch", "authors": "Erik D. Demaine, Jayson Lynch, Geronimo J. Mirano, Nirvan Tyagi", "title": "Energy-Efficient Algorithms", "comments": "40 pages, 8 pdf figures, full version of work published in ITCS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the systematic study of the energy complexity of algorithms (in\naddition to time and space complexity) based on Landauer's Principle in\nphysics, which gives a lower bound on the amount of energy a system must\ndissipate if it destroys information. We propose energy-aware variations of\nthree standard models of computation: circuit RAM, word RAM, and\ntransdichotomous RAM. On top of these models, we build familiar high-level\nprimitives such as control logic, memory allocation, and garbage collection\nwith zero energy complexity and only constant-factor overheads in space and\ntime complexity, enabling simple expression of energy-efficient algorithms. We\nanalyze several classic algorithms in our models and develop low-energy\nvariations: comparison sort, insertion sort, counting sort, breadth-first\nsearch, Bellman-Ford, Floyd-Warshall, matrix all-pairs shortest paths, AVL\ntrees, binary heaps, and dynamic arrays. We explore the time/space/energy\ntrade-off and develop several general techniques for analyzing algorithms and\nreducing their energy complexity. These results lay a theoretical foundation\nfor a new field of semi-reversible computing and provide a new framework for\nthe investigation of algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 20:32:50 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Demaine", "Erik D.", ""], ["Lynch", "Jayson", ""], ["Mirano", "Geronimo J.", ""], ["Tyagi", "Nirvan", ""]]}, {"id": "1605.08490", "submitter": "Nate Veldt", "authors": "Nate Veldt, David F. Gleich, Michael W. Mahoney", "title": "A Simple and Strongly-Local Flow-Based Method for Cut Improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many graph-based learning problems can be cast as finding a good set of\nvertices nearby a seed set, and a powerful methodology for these problems is\nbased on maximum flows. We introduce and analyze a new method for\nlocally-biased graph-based learning called SimpleLocal, which finds good\nconductance cuts near a set of seed vertices. An important feature of our\nalgorithm is that it is strongly-local, meaning it does not need to explore the\nentire graph to find cuts that are locally optimal. This method solves the same\nobjective as existing strongly-local flow-based methods, but it enables a\nsimple implementation. We also show how it achieves localization through an\nimplicit L1-norm penalty term. As a flow-based method, our algorithm exhibits\nseveral ad- vantages in terms of cut optimality and accurate identification of\ntarget regions in a graph. We demonstrate the power of SimpleLocal by solving\nproblems on a 467 million edge graph based on an MRI scan.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 01:55:31 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Veldt", "Nate", ""], ["Gleich", "David F.", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1605.08540", "submitter": "Yota Otachi", "authors": "R\\'emy Belmonte, Yota Otachi, Pascal Schweitzer", "title": "Induced Minor Free Graphs: Isomorphism and Clique-width", "comments": "16 pages, 5 figures. An extended abstract of this paper previously\n  appeared in the proceedings of the 41st International Workshop on\n  Graph-Theoretic Concepts in Computer Science (WG 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two graphs $G$ and $H$, we say that $G$ contains $H$ as an induced\nminor if a graph isomorphic to $H$ can be obtained from $G$ by a sequence of\nvertex deletions and edge contractions. We study the complexity of Graph\nIsomorphism on graphs that exclude a fixed graph as an induced minor. More\nprecisely, we determine for every graph $H$ that Graph Isomorphism is\npolynomial-time solvable on $H$-induced-minor-free graphs or that it is\nGI-complete. Additionally, we classify those graphs $H$ for which\n$H$-induced-minor-free graphs have bounded clique-width. These two results\ncomplement similar dichotomies for graphs that exclude a fixed graph as an\ninduced subgraph, minor, or subgraph.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 08:40:02 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Belmonte", "R\u00e9my", ""], ["Otachi", "Yota", ""], ["Schweitzer", "Pascal", ""]]}, {"id": "1605.08616", "submitter": "Jo\\~ao Pedro Pedroso", "authors": "Jo\\~ao Pedro Pedroso and Shiro Ikeda", "title": "Maximum-expectation matching under recourse", "comments": "The Institute of Statistical Mathematics, Tokyo, Japan", "journal-ref": null, "doi": null, "report-no": "ISM Research Memorandum, 1197, 24 May 2016", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of maximizing the expected size of a\nmatching in the case of unreliable vertices and/or edges. The assumption is\nthat upon failure, remaining vertices that have not been matched may be subject\nto a new assignment. This process may be repeated a given number of times, and\nthe objective is to end with the overall maximum number of matched vertices.\n  The origin of this problem is in kidney exchange programs, going on in\nseveral countries, where a vertex is an incompatible patient-donor pair; the\nobjective is to match these pairs so as to maximize the number of served\npatients. A new scheme is proposed for matching rearrangement in case of\nfailure, along with a prototype algorithm for computing the optimal expectation\nfor the number of matched vertices.\n  Computational experiments reveal the relevance and limitations of the\nalgorithm, in general terms and for the kidney exchange application.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 12:57:01 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Pedroso", "Jo\u00e3o Pedro", ""], ["Ikeda", "Shiro", ""]]}, {"id": "1605.08738", "submitter": "R\\'emi Watrigant", "authors": "Jason Crampton, Gregory Gutin, Martin Kouteck\\'y and R\\'emi Watrigant", "title": "Parameterized Resiliency Problems via Integer Linear Programming", "comments": "This paper is based on two papers published in conference proceedings\n  of AAIM 2016 and CIAC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an extension of decision problems called resiliency problems. In\nresiliency problems, the goal is to decide whether an instance remains positive\nafter any (appropriately defined) perturbation has been applied to it. To\ntackle these kinds of problems, some of which might be of practical interest,\nwe introduce a notion of resiliency for Integer Linear Programs (ILP) and show\nhow to use a result of Eisenbrand and Shmonin (Math. Oper. Res., 2008) on\nParametric Linear Programming to prove that ILP Resiliency is fixed-parameter\ntractable (FPT) under a certain parameterization. To demonstrate the utility of\nour result, we consider natural resiliency versions of several concrete\nproblems, and prove that they are FPT under natural parameterizations. Our\nfirst results concern a four-variate problem which generalizes the Disjoint Set\nCover problem and which is of interest in access control. We obtain a complete\nparameterized complexity classification for every possible combination of the\nparameters. Then, we introduce and study a resiliency version of the Closest\nString problem, for which we extend an FPT result of Gramm et al.\n(Algorithmica, 2003). We also consider problems in the fields of scheduling and\nsocial choice. We believe that many other problems can be tackled by our\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 18:15:50 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 15:53:49 GMT"}, {"version": "v3", "created": "Mon, 14 Nov 2016 08:36:42 GMT"}, {"version": "v4", "created": "Tue, 8 Aug 2017 11:37:17 GMT"}, {"version": "v5", "created": "Thu, 3 May 2018 12:13:43 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Crampton", "Jason", ""], ["Gutin", "Gregory", ""], ["Kouteck\u00fd", "Martin", ""], ["Watrigant", "R\u00e9mi", ""]]}, {"id": "1605.08754", "submitter": "Cameron Musco", "authors": "Dan Garber, Elad Hazan, Chi Jin, Sham M. Kakade, Cameron Musco,\n  Praneeth Netrapalli, Aaron Sidford", "title": "Faster Eigenvector Computation via Shift-and-Invert Preconditioning", "comments": "Appearing in ICML 2016. Combination of work in arXiv:1509.05647 and\n  arXiv:1510.08896", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give faster algorithms and improved sample complexities for estimating the\ntop eigenvector of a matrix $\\Sigma$ -- i.e. computing a unit vector $x$ such\nthat $x^T \\Sigma x \\ge (1-\\epsilon)\\lambda_1(\\Sigma)$:\n  Offline Eigenvector Estimation: Given an explicit $A \\in \\mathbb{R}^{n \\times\nd}$ with $\\Sigma = A^TA$, we show how to compute an $\\epsilon$ approximate top\neigenvector in time $\\tilde O([nnz(A) + \\frac{d*sr(A)}{gap^2} ]* \\log\n1/\\epsilon )$ and $\\tilde O([\\frac{nnz(A)^{3/4} (d*sr(A))^{1/4}}{\\sqrt{gap}} ]\n* \\log 1/\\epsilon )$. Here $nnz(A)$ is the number of nonzeros in $A$, $sr(A)$\nis the stable rank, $gap$ is the relative eigengap. By separating the $gap$\ndependence from the $nnz(A)$ term, our first runtime improves upon the\nclassical power and Lanczos methods. It also improves prior work using fast\nsubspace embeddings [AC09, CW13] and stochastic optimization [Sha15c], giving\nsignificantly better dependencies on $sr(A)$ and $\\epsilon$. Our second running\ntime improves these further when $nnz(A) \\le \\frac{d*sr(A)}{gap^2}$.\n  Online Eigenvector Estimation: Given a distribution $D$ with covariance\nmatrix $\\Sigma$ and a vector $x_0$ which is an $O(gap)$ approximate top\neigenvector for $\\Sigma$, we show how to refine to an $\\epsilon$ approximation\nusing $ O(\\frac{var(D)}{gap*\\epsilon})$ samples from $D$. Here $var(D)$ is a\nnatural notion of variance. Combining our algorithm with previous work to\ninitialize $x_0$, we obtain improved sample complexity and runtime results\nunder a variety of assumptions on $D$.\n  We achieve our results using a general framework that we believe is of\nindependent interest. We give a robust analysis of the classic method of\nshift-and-invert preconditioning to reduce eigenvector computation to\napproximately solving a sequence of linear systems. We then apply fast\nstochastic variance reduced gradient (SVRG) based system solvers to achieve our\nclaims.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 03:53:00 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Garber", "Dan", ""], ["Hazan", "Elad", ""], ["Jin", "Chi", ""], ["Kakade", "Sham M.", ""], ["Musco", "Cameron", ""], ["Netrapalli", "Praneeth", ""], ["Sidford", "Aaron", ""]]}, {"id": "1605.08792", "submitter": "Ameya Velingker", "authors": "Bernhard Haeupler, Ameya Velingker", "title": "Bridging the Capacity Gap Between Interactive and One-Way Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the communication rate of coding schemes for interactive\ncommunication that transform any two-party interactive protocol into a protocol\nthat is robust to noise.\n  Recently, Haeupler (FOCS '14) showed that if an $\\epsilon > 0$ fraction of\ntransmissions are corrupted, adversarially or randomly, then it is possible to\nachieve a communication rate of $1 - \\widetilde{O}(\\sqrt{\\epsilon})$.\nFurthermore, Haeupler conjectured that this rate is optimal for general input\nprotocols. This stands in contrast to the classical setting of one-way\ncommunication in which error-correcting codes are known to achieve an optimal\ncommunication rate of $1 - \\Theta(H(\\epsilon)) = 1 -\n\\widetilde{\\Theta}(\\epsilon)$.\n  In this work, we show that the quadratically smaller rate loss of the one-way\nsetting can also be achieved in interactive coding schemes for a very natural\nclass of input protocols. We introduce the notion of average message length, or\nthe average number of bits a party sends before receiving a reply, as a natural\nparameter for measuring the level of interactivity in a protocol. Moreover, we\nshow that any protocol with average message length $\\ell =\n\\Omega(\\mathrm{poly}(1/\\epsilon))$ can be simulated by a protocol with optimal\ncommunication rate $1 - \\Theta(H(\\epsilon))$ over an oblivious adversarial\nchannel with error fraction $\\epsilon$. Furthermore, under the additional\nassumption of access to public shared randomness, the optimal communication\nrate is achieved ratelessly, i.e., the communication rate adapts automatically\nto the actual error rate $\\epsilon$ without having to specify it in advance.\n  This shows that the capacity gap between one-way and interactive\ncommunication can be bridged even for very small (constant in $\\epsilon$)\naverage message lengths, which are likely to be found in many applications.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 20:19:28 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Velingker", "Ameya", ""]]}, {"id": "1605.08795", "submitter": "Jason Altschuler", "authors": "Jason Altschuler, Aditya Bhaskara, Gang Fu, Vahab Mirrokni, Afshin\n  Rostamizadeh, Morteza Zadimoghaddam", "title": "Greedy Column Subset Selection: New Bounds and Distributed Algorithms", "comments": "to appear in International Conference on Machine Learning (ICML) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of column subset selection has recently attracted a large body of\nresearch, with feature selection serving as one obvious and important\napplication. Among the techniques that have been applied to solve this problem,\nthe greedy algorithm has been shown to be quite effective in practice. However,\ntheoretical guarantees on its performance have not been explored thoroughly,\nespecially in a distributed setting. In this paper, we study the greedy\nalgorithm for the column subset selection problem from a theoretical and\nempirical perspective and show its effectiveness in a distributed setting. In\nparticular, we provide an improved approximation guarantee for the greedy\nalgorithm which we show is tight up to a constant factor, and present the first\ndistributed implementation with provable approximation factors. We use the idea\nof randomized composable core-sets, developed recently in the context of\nsubmodular maximization. Finally, we validate the effectiveness of this\ndistributed algorithm via an empirical study.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 20:33:42 GMT"}, {"version": "v2", "created": "Sat, 11 Jun 2016 16:07:57 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Altschuler", "Jason", ""], ["Bhaskara", "Aditya", ""], ["Fu", "Gang", ""], ["Mirrokni", "Vahab", ""], ["Rostamizadeh", "Afshin", ""], ["Zadimoghaddam", "Morteza", ""]]}, {"id": "1605.08935", "submitter": "Bill Smyth", "authors": "Frantisek Franek and A. S. M. Sohidull Islam and M. Sohel Rahman and\n  W. F. Smyth", "title": "Algorithms to Compute the Lyndon Array", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first describe three algorithms for computing the Lyndon array that have\nbeen suggested in the literature, but for which no structured exposition has\nbeen given. Two of these algorithms execute in quadratic time in the worst\ncase, the third achieves linear time, but at the expense of prior computation\nof both the suffix array and the inverse suffix array of x. We then go on to\ndescribe two variants of a new algorithm that avoids prior computation of\nglobal data structures and executes in worst-case n log n time. Experimental\nevidence suggests that all but one of these five algorithms require only linear\nexecution time in practice, with the two new algorithms faster by a small\nfactor. We conjecture that there exists a fast and worst-case linear-time\nalgorithm to compute the Lyndon array that is also elementary (making no use of\nglobal data structures such as the suffix array).\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 21:07:22 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Franek", "Frantisek", ""], ["Islam", "A. S. M. Sohidull", ""], ["Rahman", "M. Sohel", ""], ["Smyth", "W. F.", ""]]}, {"id": "1605.08961", "submitter": "Anastasios Kyrillidis", "authors": "Megasthenis Asteris, Anastasios Kyrillidis, Oluwasanmi Koyejo, Russell\n  Poldrack", "title": "A simple and provable algorithm for sparse diagonal CCA", "comments": "To appear at ICML 2016, 14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT math.IT math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two sets of variables, derived from a common set of samples, sparse\nCanonical Correlation Analysis (CCA) seeks linear combinations of a small\nnumber of variables in each set, such that the induced canonical variables are\nmaximally correlated. Sparse CCA is NP-hard.\n  We propose a novel combinatorial algorithm for sparse diagonal CCA, i.e.,\nsparse CCA under the additional assumption that variables within each set are\nstandardized and uncorrelated. Our algorithm operates on a low rank\napproximation of the input data and its computational complexity scales\nlinearly with the number of input variables. It is simple to implement, and\nparallelizable. In contrast to most existing approaches, our algorithm\nadministers precise control on the sparsity of the extracted canonical vectors,\nand comes with theoretical data-dependent global approximation guarantees, that\nhinge on the spectrum of the input data. Finally, it can be straightforwardly\nadapted to other constrained variants of CCA enforcing structure beyond\nsparsity.\n  We empirically evaluate the proposed scheme and apply it on a real\nneuroimaging dataset to investigate associations between brain activity and\nbehavior measurements.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 03:56:23 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Asteris", "Megasthenis", ""], ["Kyrillidis", "Anastasios", ""], ["Koyejo", "Oluwasanmi", ""], ["Poldrack", "Russell", ""]]}, {"id": "1605.09042", "submitter": "Sungsoo Ahn", "authors": "Sungsoo Ahn, Michael Chertkov, Jinwoo Shin", "title": "MCMC assisted by Belief Propagation", "comments": "Fixed minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most\npopular algorithms for computational inference in Graphical Models (GM). In\nprinciple, MCMC is an exact probabilistic method which, however, often suffers\nfrom exponentially slow mixing. In contrast, BP is a deterministic method,\nwhich is typically fast, empirically very successful, however in general\nlacking control of accuracy over loopy graphs. In this paper, we introduce MCMC\nalgorithms correcting the approximation error of BP, i.e., we provide a way to\ncompensate for BP errors via a consecutive BP-aware MCMC. Our framework is\nbased on the Loop Calculus (LC) approach which allows expressing the BP error\nas a sum of weighted generalized loops. Although the full series is\ncomputationally intractable, it is known that a truncated series, summing up\nall 2-regular loops, is computable in polynomial-time for planar pair-wise\nbinary GMs and it also provides a highly accurate approximation empirically.\nMotivated by this, we first propose a polynomial-time approximation MCMC scheme\nfor the truncated series of general (non-planar) pair-wise binary models. Our\nmain idea here is to use the Worm algorithm, known to provide fast mixing in\nother (related) problems, and then design an appropriate rejection scheme to\nsample 2-regular loops. Furthermore, we also design an efficient rejection-free\nMCMC scheme for approximating the full series. The main novelty underlying our\ndesign is in utilizing the concept of cycle basis, which provides an efficient\ndecomposition of the generalized loops. In essence, the proposed MCMC schemes\nrun on transformed GM built upon the non-trivial BP solution, and our\nexperiments show that this synthesis of BP and MCMC outperforms both direct\nMCMC and bare BP schemes.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 18:24:45 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 05:48:45 GMT"}, {"version": "v3", "created": "Mon, 24 Oct 2016 02:58:38 GMT"}, {"version": "v4", "created": "Wed, 9 Nov 2016 14:55:22 GMT"}, {"version": "v5", "created": "Mon, 21 Nov 2016 01:32:03 GMT"}, {"version": "v6", "created": "Mon, 11 May 2020 03:20:05 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Ahn", "Sungsoo", ""], ["Chertkov", "Michael", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1605.09190", "submitter": "Fahad Mortuza", "authors": "Fahad Bin Mortuza", "title": "A Polynomial Time Graph Isomorphism Algorithm For Graphs That Are Not\n  Locally Triangle-Free", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show the existence of a polynomial time graph isomorphism\nalgorithm for all graphs excluding graphs that are locally trianglefree. This\nparticular class of graphs allows to divide the graph into neighbourhood\nsub-graph where each of induced sub-graph (neighbourhood) has at least 2\nvertices. We construct all possible permutations for each induced sub-graph\nusing a search tree. We construct automorphisms of subgraphs based on these\npermutations. Finally, we decide isomorphism through automorphisms .\n  The author expects that the solution, present in this paper, may lead to a\nfaster algorithm for the general case of graph isomorphism (using \" barycentric\nsubdivision\" ). The paper might affect group isomorphism also as we may\nconstruct graphs (corresponds to a particular group) in way so we can avoid it\nto be a triangle free graph. Since,for a given group G , each choice of a\ngenerating set will give a different Cayley graph.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 22:58:29 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 16:31:59 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Mortuza", "Fahad Bin", ""]]}, {"id": "1605.09227", "submitter": "Colin White", "authors": "Maria-Florina Balcan, Ellen Vitercik, Colin White", "title": "Learning Combinatorial Functions from Pairwise Comparisons", "comments": "1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large body of work in machine learning has focused on the problem of\nlearning a close approximation to an underlying combinatorial function, given a\nsmall set of labeled examples. However, for real-valued functions, cardinal\nlabels might not be accessible, or it may be difficult for an expert to\nconsistently assign real-valued labels over the entire set of examples. For\ninstance, it is notoriously hard for consumers to reliably assign values to\nbundles of merchandise. Instead, it might be much easier for a consumer to\nreport which of two bundles she likes better. With this motivation in mind, we\nconsider an alternative learning model, wherein the algorithm must learn the\nunderlying function up to pairwise comparisons, from pairwise comparisons. In\nthis model, we present a series of novel algorithms that learn over a wide\nvariety of combinatorial function classes. These range from graph functions to\nbroad classes of valuation functions that are fundamentally important in\nmicroeconomic theory, the analysis of social networks, and machine learning,\nsuch as coverage, submodular, XOS, and subadditive functions, as well as\nfunctions with sparse Fourier support.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 13:38:47 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Vitercik", "Ellen", ""], ["White", "Colin", ""]]}, {"id": "1605.09399", "submitter": "Helmut Katzgraber", "authors": "Zheng Zhu, Chao Fang, and Helmut G. Katzgraber", "title": "borealis - A generalized global update algorithm for Boolean\n  optimization problems", "comments": "19 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cond-mat.dis-nn cond-mat.stat-mech cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization problems with Boolean variables that fall into the\nnondeterministic polynomial (NP) class are of fundamental importance in\ncomputer science, mathematics, physics and industrial applications. Most\nnotably, solving constraint-satisfaction problems, which are related to\nspin-glass-like Hamiltonians in physics, remains a difficult numerical task. As\nsuch, there has been great interest in designing efficient heuristics to solve\nthese computationally difficult problems. Inspired by parallel tempering Monte\nCarlo in conjunction with the rejection-free isoenergetic cluster algorithm\ndeveloped for Ising spin glasses, we present a generalized global update\noptimization heuristic that can be applied to different NP-complete problems\nwith Boolean variables. The global cluster updates allow for a wide-spread\nsampling of phase space, thus considerably speeding up optimization. By\ncarefully tuning the pseudo-temperature (needed to randomize the\nconfigurations) of the problem, we show that the method can efficiently tackle\noptimization problems with over-constraints or on topologies with a large\nsite-percolation threshold. We illustrate the efficiency of the heuristic on\nparadigmatic optimization problems, such as the maximum satisfiability problem\nand the vertex cover problem.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 20:07:18 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Zhu", "Zheng", ""], ["Fang", "Chao", ""], ["Katzgraber", "Helmut G.", ""]]}, {"id": "1605.09425", "submitter": "Jenny Lam", "authors": "David Eppstein, Michael T. Goodrich, Jenny Lam, Nil Mamano, Michael\n  Mitzenmacher, Manuel Torres", "title": "Models and Algorithms for Graph Watermarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce models and algorithmic foundations for graph watermarking. Our\nframeworks include security definitions and proofs, as well as\ncharacterizations when graph watermarking is algorithmically feasible, in spite\nof the fact that the general problem is NP-complete by simple reductions from\nthe subgraph isomorphism or graph edit distance problems. In the digital\nwatermarking of many types of files, an implicit step in the recovery of a\nwatermark is the mapping of individual pieces of data, such as image pixels or\nmovie frames, from one object to another. In graphs, this step corresponds to\napproximately matching vertices of one graph to another based on graph\ninvariants such as vertex degree. Our approach is based on characterizing the\nfeasibility of graph watermarking in terms of keygen, marking, and\nidentification functions defined over graph families with known distributions.\nWe demonstrate the strength of this approach with exemplary watermarking\nschemes for two random graph models, the classic Erd\\H{o}s-R\\'{e}nyi model and\na random power-law graph model, both of which are used to model real-world\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 21:46:31 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Eppstein", "David", ""], ["Goodrich", "Michael T.", ""], ["Lam", "Jenny", ""], ["Mamano", "Nil", ""], ["Mitzenmacher", "Michael", ""], ["Torres", "Manuel", ""]]}, {"id": "1605.09520", "submitter": "Petr Hlin\\v{e}n\\'y", "authors": "Petr Hlin\\v{e}n\\'y", "title": "A Simpler Self-reduction Algorithm for Matroid Path-width", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path-width of matroids naturally generalizes the better known parameter of\npath-width for graphs, and is NP-hard by a reduction from the graph case. While\nthe term matroid path-width was formally introduced by Geelen-Gerards-Whittle\n[JCTB 2006] in pure matroid theory, it was soon recognized by Kashyap [SIDMA\n2008] that it is the same concept as long-studied so called trellis complexity\nin coding theory, later named trellis-width, and hence it is an interesting\nnotion also from the algorithmic perspective. It follows from a result of\nHlineny [JCTB 2006] that the decision problem, whether a given matroid over a\nfinite field has path-width at most t, is fixed-parameter tractable (FPT) in t,\nbut this result does not give any clue about constructing a path-decomposition.\nThe first constructive and rather complicated FPT algorithm for path-width of\nmatroids over a finite field was given by Jeong-Kim-Oum [SODA 2016]. Here we\npropose a simpler \"self-reduction\" FPT algorithm for a path-decomposition.\nPrecisely, we design an efficient routine that constructs an optimal\npath-decomposition of a matroid by calling any subroutine for testing whether\nthe path-width of a matroid is at most t (such as the aforementioned decision\nalgorithm for matroid path-width).\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 08:02:24 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 07:15:31 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Hlin\u011bn\u00fd", "Petr", ""]]}, {"id": "1605.09558", "submitter": "Takaaki Nishimoto", "authors": "Takaaki Nishimoto and Tomohiro I and Shunsuke Inenaga and Hideo Bannai\n  and Masayuki Takeda", "title": "Dynamic index and LZ factorization in compressed space", "comments": "arXiv admin note: substantial text overlap with arXiv:1605.01488;\n  text overlap with arXiv:1504.06954", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new \\emph{dynamic compressed index} of $O(w)$\nspace for a dynamic text $T$, where $w = O(\\min(z \\log N \\log^*M, N))$ is the\nsize of the signature encoding of $T$, $z$ is the size of the Lempel-Ziv77\n(LZ77) factorization of $T$, $N$ is the length of $T$, and $M \\geq 3N$ is an\ninteger that can be handled in constant time under word RAM model. Our index\nsupports searching for a pattern $P$ in $T$ in $O(|P| f_{\\mathcal{A}} + \\log w\n\\log |P| \\log^* M (\\log N + \\log |P| \\log^* M) + \\mathit{occ} \\log N)$ time and\ninsertion/deletion of a substring of length $y$ in $O((y+ \\log N\\log^* M)\\log w\n\\log N \\log^* M)$ time, where $f_{\\mathcal{A}} = O(\\min \\{ \\frac{\\log\\log M\n\\log\\log w}{\\log\\log\\log M}, \\sqrt{\\frac{\\log w}{\\log\\log w}} \\})$. Also, we\npropose a new space-efficient LZ77 factorization algorithm for a given text of\nlength $N$, which runs in $O(N f_{\\mathcal{A}} + z \\log w \\log^3 N (\\log^*\nN)^2)$ time with $O(w)$ working space.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 10:21:59 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 08:17:54 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Nishimoto", "Takaaki", ""], ["I", "Tomohiro", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1605.09721", "submitter": "Dimitris Papailiopoulos", "authors": "Xinghao Pan, Maximilian Lam, Stephen Tu, Dimitris Papailiopoulos, Ce\n  Zhang, Michael I. Jordan, Kannan Ramchandran, Chris Re, Benjamin Recht", "title": "CYCLADES: Conflict-free Asynchronous Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CYCLADES, a general framework for parallelizing stochastic\noptimization algorithms in a shared memory setting. CYCLADES is asynchronous\nduring shared model updates, and requires no memory locking mechanisms, similar\nto HOGWILD!-type algorithms. Unlike HOGWILD!, CYCLADES introduces no conflicts\nduring the parallel execution, and offers a black-box analysis for provable\nspeedups across a large family of algorithms. Due to its inherent conflict-free\nnature and cache locality, our multi-core implementation of CYCLADES\nconsistently outperforms HOGWILD!-type algorithms on sufficiently sparse\ndatasets, leading to up to 40% speedup gains compared to the HOGWILD!\nimplementation of SGD, and up to 5x gains over asynchronous implementations of\nvariance reduction algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 17:15:01 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Pan", "Xinghao", ""], ["Lam", "Maximilian", ""], ["Tu", "Stephen", ""], ["Papailiopoulos", "Dimitris", ""], ["Zhang", "Ce", ""], ["Jordan", "Michael I.", ""], ["Ramchandran", "Kannan", ""], ["Re", "Chris", ""], ["Recht", "Benjamin", ""]]}, {"id": "1605.09784", "submitter": "Ryan Curtin", "authors": "Ryan R. Curtin, Andrew B. Gardner", "title": "Fast approximate furthest neighbors with data-dependent hashing", "comments": "Submitted to SISAP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hashing strategy for approximate furthest neighbor search\nthat selects projection bases using the data distribution. This strategy leads\nto an algorithm, which we call DrusillaHash, that is able to outperform\nexisting approximate furthest neighbor strategies. Our strategy is motivated by\nan empirical study of the behavior of the furthest neighbor search problem,\nwhich lends intuition for where our algorithm is most useful. We also present a\nvariant of the algorithm that gives an absolute approximation guarantee; to our\nknowledge, this is the first such approximate furthest neighbor hashing\napproach to give such a guarantee. Performance studies indicate that\nDrusillaHash can achieve comparable levels of approximation to other algorithms\nwhile giving up to an order of magnitude speedup. An implementation is\navailable in the mlpack machine learning library (found at\nhttp://www.mlpack.org).\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 19:38:06 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Curtin", "Ryan R.", ""], ["Gardner", "Andrew B.", ""]]}]