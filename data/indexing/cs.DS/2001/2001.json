[{"id": "2001.00004", "submitter": "Rakesh Mohanty", "authors": "Rakesh Mohanty, Debasis Dwibedy, Shreeya Swagatika Sahoo", "title": "New Competitive Analysis Results of Online List Scheduling Algorithm", "comments": "9 pages, In Proceeding of the 14th Annual ADMA Conference 2018, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online algorithm has been an emerging area of interest for researchers in\nvarious domains of computer science. The online $m$-machine list scheduling\nproblem introduced by Graham has gained theoretical as well as practical\nsignificance in the development of competitive analysis as a performance\nmeasure for online algorithms. In this paper, we study and explore the\nperformance of Graham's online \\textit{list scheduling algorithm(LSA)} for\nindependent jobs. In the literature, \\textit{LSA} has already been proved to be\n$2-\\frac{1}{m}$ competitive, where $m$ is the number of machines. We present\ntwo new upper bound results on competitive analysis of \\textit{LSA}. We obtain\nupper bounds on the competitive ratio of $2-\\frac{2}{m}$ and\n$2-\\frac{m^2-m+1}{m^2}$ respectively for practically significant two special\nclasses of input job sequences. Our analytical results can motivate the\npractitioners to design improved competitive online algorithms for the\n$m$-machine list scheduling problem by characterization of real life input\nsequences.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 07:20:53 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Mohanty", "Rakesh", ""], ["Dwibedy", "Debasis", ""], ["Sahoo", "Shreeya Swagatika", ""]]}, {"id": "2001.00066", "submitter": "Julian Enoch", "authors": "Julian Enoch", "title": "Nested Column Generation decomposition for solving the Routing and\n  Spectrum Allocation problem in Elastic Optical Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the continued growth of Internet traffic, and the scarcity of the\noptical spectrum, there is a continuous need to optimize the usage of this\nresource. In the process of provisioning elastic optical networks using the\nflexible frequency grid paradigm, telecommunication operators must deal with a\ncombinatorial optimization problem that is NP-complete namely the Routing and\nSpectrum Allocation(RSA) problem. Following on our previous study, where we\nused Integer Linear Programming, and proposed a Column Generation algorithm\nbased on a Lightpath decomposition, which proved to be the most efficient so\nfar, we now consider the traditional Configuration decomposition that has been\nstudied in other works in the past. In the process, we created an new\nmathematical model using two variable sets instead of a single variable set.\nEqually important,we independently rediscovered the Nested Column Generation\ntechnique, and we used it to propose an algorithm that led to a considerable\nimprovement on the previous algorithms that use the same Configuration\ndecomposition. When compared to the latest such existing study, our algorithm\nachieved an accuracy gap of 1% as opposed to 14.3% for the previous study, and\na running time two orders of magnitude faster on average.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 20:14:55 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 16:10:24 GMT"}], "update_date": "2020-01-21", "authors_parsed": [["Enoch", "Julian", ""]]}, {"id": "2001.00072", "submitter": "Ellis Hershkowitz", "authors": "Bernhard Haeupler, D Ellis Hershkowitz, David Wajc", "title": "Near-Optimal Schedules for Simultaneous Multicasts", "comments": "In ICALP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the store-and-forward packet routing problem for simultaneous\nmulticasts, in which multiple packets have to be forwarded along given trees as\nfast as possible.\n  This is a natural generalization of the seminal work of Leighton, Maggs and\nRao, which solved this problem for unicasts, i.e. the case where all trees are\npaths. They showed the existence of asymptotically optimal $O(C + D)$-length\nschedules, where the congestion $C$ is the maximum number of packets sent over\nan edge and the dilation $D$ is the maximum depth of a tree. This improves over\nthe trivial $O(CD)$ length schedules.\n  We prove a lower bound for multicasts, which shows that there do not always\nexist schedules of non-trivial length, $o(CD)$. On the positive side, we\nconstruct $O(C+D+\\log^2 n)$-length schedules in any $n$-node network. These\nschedules are near-optimal, since our lower bound shows that this length cannot\nbe improved to $O(C+D) + o(\\log n)$.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 20:48:01 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 15:34:25 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Hershkowitz", "D Ellis", ""], ["Wajc", "David", ""]]}, {"id": "2001.00076", "submitter": "Ari Kobren", "authors": "Nicholas Monath, Ari Kobren, Akshay Krishnamurthy, Michael Glass,\n  Andrew McCallum", "title": "Scalable Hierarchical Clustering with Tree Grafting", "comments": "23 pages (appendix included), published at KDD 2019", "journal-ref": null, "doi": "10.1145/3292500.3330929", "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Grinch, a new algorithm for large-scale, non-greedy hierarchical\nclustering with general linkage functions that compute arbitrary similarity\nbetween two point sets. The key components of Grinch are its rotate and graft\nsubroutines that efficiently reconfigure the hierarchy as new points arrive,\nsupporting discovery of clusters with complex structure. Grinch is motivated by\na new notion of separability for clustering with linkage functions: we prove\nthat when the model is consistent with a ground-truth clustering, Grinch is\nguaranteed to produce a cluster tree containing the ground-truth, independent\nof data arrival order. Our empirical results on benchmark and author\ncoreference datasets (with standard and learned linkage functions) show that\nGrinch is more accurate than other scalable methods, and orders of magnitude\nfaster than hierarchical agglomerative clustering.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 20:56:15 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Monath", "Nicholas", ""], ["Kobren", "Ari", ""], ["Krishnamurthy", "Akshay", ""], ["Glass", "Michael", ""], ["McCallum", "Andrew", ""]]}, {"id": "2001.00204", "submitter": "Leonie Selbach", "authors": "Maike Buchin and Leonie Selbach", "title": "Partitioning algorithms for weighted cactus graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of partitioning a weighted graph into connected\ncomponents such that each component fulfills upper and lower weight\nconstraints. Partitioning into a minimum, maximum or a fixed number of clusters\nis NP-hard in general but polynomial-time solvable on trees. In this paper, we\npresent a polynomial-time algorithm for cactus graphs. For other optimization\ngoals or additional constraints, the partition problem becomes NP-hard even on\ntrees and for a lower weight bound equal to zero. We show that our method can\nbe used as an algorithmic framework to solve other partition problems for\ncactus graphs in pseudo-polynomial time.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 13:31:08 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 09:47:11 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Buchin", "Maike", ""], ["Selbach", "Leonie", ""]]}, {"id": "2001.00211", "submitter": "Tomasz Kociumaka", "authors": "Timothy M. Chan, Shay Golan, Tomasz Kociumaka, Tsvi Kopelowitz, Ely\n  Porat", "title": "Approximating Text-to-Pattern Hamming Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit a fundamental problem in string matching: given a pattern of\nlength m and a text of length n, both over an alphabet of size $\\sigma$,\ncompute the Hamming distance between the pattern and the text at every\nlocation. Several $(1+\\epsilon)$-approximation algorithms have been proposed in\nthe literature, with running time of the form $O(\\epsilon^{-O(1)}n\\log n\\log\nm)$, all using fast Fourier transform (FFT). We describe a simple\n$(1+\\epsilon)$-approximation algorithm that is faster and does not need FFT.\nCombining our approach with additional ideas leads to numerous new results:\n  - We obtain the first linear-time approximation algorithm; the running time\nis $O(\\epsilon^{-2}n)$.\n  - We obtain a faster exact algorithm computing all Hamming distances up to a\ngiven threshold k; its running time improves previous results by logarithmic\nfactors and is linear if $k\\le\\sqrt m$.\n  - We obtain approximation algorithms with better $\\epsilon$-dependence using\nrectangular matrix multiplication. The time-bound is $\\~O(n)$ when the pattern\nis sufficiently long: $m\\ge \\epsilon^{-28}$. Previous algorithms require\n$\\~O(\\epsilon^{-1}n)$ time.\n  - When k is not too small, we obtain a truly sublinear-time algorithm to find\nall locations with Hamming distance approximately (up to a constant factor)\nless than k, in $O((n/k^{\\Omega(1)}+occ)n^{o(1)})$ time, where occ is the\noutput size. The algorithm leads to a property tester, returning true if an\nexact match exists and false if the Hamming distance is more than $\\delta m$ at\nevery location, running in $\\~O(\\delta^{-1/3}n^{2/3}+\\delta^{-1}n/m)$ time.\n  - We obtain a streaming algorithm to report all locations with Hamming\ndistance approximately less than k, using $\\~O(\\epsilon^{-2}\\sqrt k)$ space.\nPreviously, streaming algorithms were known for the exact problem with \\~O(k)\nspace or for the approximate problem with $\\~O(\\epsilon^{-O(1)}\\sqrt m)$ space.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 14:03:31 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Chan", "Timothy M.", ""], ["Golan", "Shay", ""], ["Kociumaka", "Tomasz", ""], ["Kopelowitz", "Tsvi", ""], ["Porat", "Ely", ""]]}, {"id": "2001.00218", "submitter": "Thiago Serra", "authors": "Thiago Serra, Abhinav Kumar, Srikumar Ramalingam", "title": "Lossless Compression of Deep Neural Networks", "comments": "CPAIOR 2020 (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been successful in many predictive modeling tasks,\nsuch as image and language recognition, where large neural networks are often\nused to obtain good accuracy. Consequently, it is challenging to deploy these\nnetworks under limited computational resources, such as in mobile devices. In\nthis work, we introduce an algorithm that removes units and layers of a neural\nnetwork while not changing the output that is produced, which thus implies a\nlossless compression. This algorithm, which we denote as LEO (Lossless\nExpressiveness Optimization), relies on Mixed-Integer Linear Programming (MILP)\nto identify Rectified Linear Units (ReLUs) with linear behavior over the input\ndomain. By using L1 regularization to induce such behavior, we can benefit from\ntraining over a larger architecture than we would later use in the environment\nwhere the trained neural network is deployed.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 15:04:43 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 11:19:36 GMT"}, {"version": "v3", "created": "Sat, 22 Feb 2020 16:09:43 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Serra", "Thiago", ""], ["Kumar", "Abhinav", ""], ["Ramalingam", "Srikumar", ""]]}, {"id": "2001.00257", "submitter": "Pattara Sukprasert", "authors": "Parinya Chalermsook, Samir Khuller, Pattara Sukprasert, Sumedha Uniyal", "title": "Multi-transversals for Triangles and the Tuza's Conjecture", "comments": "Accepted at SODA'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a primal and dual relationship about triangles: For\nany graph $G$, let $\\nu(G)$ be the maximum number of edge-disjoint triangles in\n$G$, and $\\tau(G)$ be the minimum subset $F$ of edges such that $G \\setminus F$\nis triangle-free. It is easy to see that $\\nu(G) \\leq \\tau(G) \\leq 3 \\nu(G)$,\nand in fact, this rather obvious inequality holds for a much more general\nprimal-dual relation between $k$-hyper matching and covering in hypergraphs.\nTuza conjectured in $1981$ that $\\tau(G) \\leq 2 \\nu(G)$, and this question has\nreceived attention from various groups of researchers in discrete mathematics,\nsettling various special cases such as planar graphs and generalized to bounded\nmaximum average degree graphs, some cases of minor-free graphs, and very dense\ngraphs. Despite these efforts, the conjecture in general graphs has remained\nwide open for almost four decades.\n  In this paper, we provide a proof of a non-trivial consequence of the\nconjecture; that is, for every $k \\geq 2$, there exist a (multi)-set $F\n\\subseteq E(G): |F| \\leq 2k \\nu(G)$ such that each triangle in $G$ overlaps at\nleast $k$ elements in $F$. Our result can be seen as a strengthened statement\nof Krivelevich's result on the fractional version of Tuza's conjecture (and we\ngive some examples illustrating this.) The main technical ingredient of our\nresult is a charging argument, that locally identifies edges in $F$ based on a\nlocal view of the packing solution. This idea might be useful in further\nstudying the primal-dual relations in general and the Tuza's conjecture in\nparticular.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 18:01:53 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 06:44:53 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Chalermsook", "Parinya", ""], ["Khuller", "Samir", ""], ["Sukprasert", "Pattara", ""], ["Uniyal", "Sumedha", ""]]}, {"id": "2001.00303", "submitter": "Kuikui Liu", "authors": "Nima Anari, Kuikui Liu, Shayan Oveis Gharan", "title": "Spectral Independence in High-Dimensional Expanders and Applications to\n  the Hardcore Model", "comments": "Fixed a bug in the decoupling lemma of section 4, and in the proof of\n  Theorem 3.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We say a probability distribution $\\mu$ is spectrally independent if an\nassociated correlation matrix has a bounded largest eigenvalue for the\ndistribution and all of its conditional distributions. We prove that if $\\mu$\nis spectrally independent, then the corresponding high dimensional simplicial\ncomplex is a local spectral expander. Using a line of recent works on mixing\ntime of high dimensional walks on simplicial complexes\n\\cite{KM17,DK17,KO18,AL19}, this implies that the corresponding Glauber\ndynamics mixes rapidly and generates (approximate) samples from $\\mu$.\n  As an application, we show that natural Glauber dynamics mixes rapidly (in\npolynomial time) to generate a random independent set from the hardcore model\nup to the uniqueness threshold. This improves the quasi-polynomial running time\nof Weitz's deterministic correlation decay algorithm \\cite{Wei06} for\nestimating the hardcore partition function, also answering a long-standing open\nproblem of mixing time of Glauber dynamics \\cite{LV97,LV99,DG00,Vig01,EHSVY16}.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 02:46:11 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 08:37:49 GMT"}, {"version": "v3", "created": "Thu, 17 Sep 2020 18:18:36 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Anari", "Nima", ""], ["Liu", "Kuikui", ""], ["Gharan", "Shayan Oveis", ""]]}, {"id": "2001.00336", "submitter": "Thatchaphol Saranurak", "authors": "Sayan Bhattacharya, Danupon Nanongkai, Thatchaphol Saranurak", "title": "Coarse-Grained Complexity for Dynamic Algorithms", "comments": "To be published at SODA 2020. The abstract is truncated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, the only way to argue polynomial lower bounds for dynamic algorithms\nis via fine-grained complexity arguments. These arguments rely on strong\nassumptions about specific problems such as the Strong Exponential Time\nHypothesis (SETH) and the Online Matrix-Vector Multiplication Conjecture (OMv).\nWhile they have led to many exciting discoveries, dynamic algorithms still miss\nout some benefits and lessons from the traditional ``coarse-grained'' approach\nthat relates together classes of problems such as P and NP. In this paper we\ninitiate the study of coarse-grained complexity theory for dynamic algorithms.\nBelow are among questions that this theory can answer.\n  What if dynamic Orthogonal Vector (OV) is easy in the cell-probe model? A\nresearch program for proving polynomial unconditional lower bounds for dynamic\nOV in the cell-probe model is motivated by the fact that many conditional lower\nbounds can be shown via reductions from the dynamic OV problem. Since the\ncell-probe model is more powerful than word RAM and has historically allowed\nsmaller upper bounds, it might turn out that dynamic OV is easy in the\ncell-probe model, making this research direction infeasible. Our theory implies\nthat if this is the case, there will be very interesting algorithmic\nconsequences: If dynamic OV can be maintained in polylogarithmic worst-case\nupdate time in the cell-probe model, then so are several important dynamic\nproblems such as $k$-edge connectivity, $(1+\\epsilon)$-approximate mincut,\n$(1+\\epsilon)$-approximate matching, planar nearest neighbors, Chan's subset\nunion and 3-vs-4 diameter. The same conclusion can be made when we replace\ndynamic OV by, e.g., subgraph connectivity, single source reachability, Chan's\nsubset union, and 3-vs-4 diameter.\n  Lower bounds for $k$-edge connectivity via dynamic OV? (see the full abstract\nin the pdf file).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 06:14:54 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Bhattacharya", "Sayan", ""], ["Nanongkai", "Danupon", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "2001.00413", "submitter": "Aleksandar Prokopec", "authors": "Aleksandar Prokopec, Trevor Brown, Dan Alistarh", "title": "Analysis and Evaluation of Non-Blocking Interpolation Search Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We start by summarizing the recently proposed implementation of the first\nnon-blocking concurrent interpolation search tree (C-IST) data structure. We\nthen analyze the individual operations of the C-IST, and show that they are\ncorrect and linearizable. We furthermore show that lookup (and several other\nnon-destructive operations) are wait-free, and that the insert and delete\noperations are lock-free. We continue by showing that the C-IST has the\nfollowing properties. For arbitrary key distributions, this data structure\nensures worst-case $O(\\log n + p)$ amortized time for search, insertion and\ndeletion traversals. When the input key distributions are smooth, lookups run\nin expected $O(\\log \\log n + p)$ time, and insertion and deletion run in\nexpected amortized $O(\\log \\log n + p)$ time, where $p$ is a bound on the\nnumber of threads. Finally, we present an extended experimental evaluation of\nthe non-blocking IST performance.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 12:36:41 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Prokopec", "Aleksandar", ""], ["Brown", "Trevor", ""], ["Alistarh", "Dan", ""]]}, {"id": "2001.00562", "submitter": "Varad Pande", "authors": "Varad R. Pande", "title": "Optimal Entropy Compression and Purification in Quantum Bits", "comments": "26 pages, 12 + 1 (external) figures; v3: revised manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global unitary transformations that optimally increase the bias of any mixed\ncomputation qubit in a quantum system, represented by a diagonal density\nmatrix, towards a particular state of the computational basis which, in effect,\nincreases its purity are presented. Quantum circuits that achieve this by\nimplementing the above data compression technique, a generalization of the\n3B-Comp [Fernandez, Lloyd, Mor, Roychowdhury (2004); arXiv: quant-ph/0401135]\nused before, are described. These circuits enable purity increment in the\ncomputation qubit by maximally transferring part of its von Neumann or Shannon\nentropy to any number of surrounding qubits and are valid for the complete\nrange of initial biases. Using the optswaps, a practicable new method that\nalgorithmically achieves hierarchy-dependent cooling of qubits to their\nrespective limits in an engineered quantum register opened to the heat-bath is\ndelineated. In addition to multi-qubit purification and satisfying two of\nDiVincenzo's criteria for quantum computation in some architectures, the\nimplications of this work for quantum data compression and quantum\nthermodynamics are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 18:55:14 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 08:05:03 GMT"}, {"version": "v3", "created": "Sun, 20 Jun 2021 03:41:54 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Pande", "Varad R.", ""]]}, {"id": "2001.00858", "submitter": "Lucas Assun\\c{c}\\~ao", "authors": "Lucas Assun\\c{c}\\~ao and Geraldo Robson Mateus", "title": "A cutting-plane algorithm for the Steiner team orienteering problem", "comments": null, "journal-ref": "Computers & Industrial Engineering, Volume 135, September 2019,\n  Pages 922-939", "doi": "10.1016/j.cie.2019.06.051", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Team Orienteering Problem (TOP) is an NP-hard routing problem in which a\nfleet of identical vehicles aims at collecting rewards (prizes) available at\ngiven locations, while satisfying restrictions on the travel times. In TOP,\neach location can be visited by at most one vehicle, and the goal is to\nmaximize the total sum of rewards collected by the vehicles within a given time\nlimit. In this paper, we propose a generalization of TOP, namely the Steiner\nTeam Orienteering Problem (STOP). In STOP, we provide, additionally, a subset\nof mandatory locations. In this sense, STOP also aims at maximizing the total\nsum of rewards collected within the time limit, but, now, every mandatory\nlocation must be visited. In this work, we propose a new commodity-based\nformulation for STOP and use it within a cutting-plane scheme. The algorithm\nbenefits from the compactness and strength of the proposed formulation and\nworks by separating three families of valid inequalities, which consist of some\ngeneral connectivity constraints, classical lifted cover inequalities based on\ndual bounds and a class of conflict cuts. To our knowledge, the last class of\ninequalities is also introduced in this work. A state-of-the-art branch-and-cut\nalgorithm from the literature of TOP is adapted to STOP and used as baseline to\nevaluate the performance of the cutting-plane. Extensive computational\nexperiments show the competitiveness of the new algorithm while solving several\nSTOP and TOP instances. In particular, it is able to solve, in total, 14 more\nTOP instances than any other previous exact algorithm and finds eight new\noptimality certificates. With respect to the new STOP instances introduced in\nthis work, our algorithm solves 30 more instances than the baseline.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 15:04:35 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Assun\u00e7\u00e3o", "Lucas", ""], ["Mateus", "Geraldo Robson", ""]]}, {"id": "2001.00880", "submitter": "Aldo Procacci", "authors": "Paula M. S. Fialho, Bernardo N. B. de Lima, Aldo Procacci", "title": "Moser-Tardos resampling algorithm, entropy compression method and the\n  subset gas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a connection between the entropy compression method and the\nMoser-Tardos algorithmic version of the Lov\\'asz local lemma through the\ncluster expansion of the subset gas. We also show that the Moser-Tardos\nresampling algorithm and the entropy compression bactracking algorithm produce\nidentical bounds.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 16:30:43 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Fialho", "Paula M. S.", ""], ["de Lima", "Bernardo N. B.", ""], ["Procacci", "Aldo", ""]]}, {"id": "2001.00894", "submitter": "Mohammad Shadravan", "authors": "Mohammad Shadravan", "title": "Submodular Matroid Secretary Problem with Shortlists", "comments": "arXiv admin note: text overlap with arXiv:1809.05082", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the matroid secretary problem, the elements of a matroid $\\mathcal{M}$\narrive in random order. Once we observe an item we need to irrevocably decide\nwhether or not to accept it. The set of selected elements should form an\nindependent set of the matroid. The goal is to maximize the total sum of the\nvalues assigned to these elements.\n  We introduce a version of this problem motivated by the shortlist model in\n[Agrawal et al.]. In this setting, the algorithm is allowed to choose a subset\nof items as part of a shortlist, possibly more than $k=rk(\\mathcal{M})$ items.\nThen, after seeing the entire input, the algorithm can choose an independent\nsubset from the shortlist. Furthermore we generalize the objective function to\nany monotone submodular function. Is there an online algorithm achieve a\nconstant competitive ratio using a shortlist of size $O(k)$? We design an\nalgorithm that achieves a $\\frac{1}{2}(1-1/e^2-\\epsilon-O(1/k))$ competitive\nratio for any constant $\\epsilon>0$, using a shortlist of size $O(k)$. This is\nespecially surprising considering that the best known competitive ratio for the\nmatroid secretary problem is $O(\\log \\log k)$.\n  An important application of our algorithm is for the random order streaming\nof submodular functions. We show that our algorithm can be implemented in the\nstreaming setting using $O(k)$ memory. It achieves a\n$\\frac{1}{2}(1-1/e^2-\\epsilon-O(1/k))$ approximation. The previously best known\napproximation ratio for streaming submodular maximization under matroid\nconstraint is 0.25 (adversarial order) due to [Feldman et al.], [Chekuri et\nal.], and [Chakrabarti et al.]. Moreover, we generalize our results to the case\nof p-matchoid constraints and give a\n$\\frac{1}{p+1}(1-1/e^{p+1}-\\epsilon-O(1/k))$ approximation using $O(k)$ memory,\nwhich asymptotically approaches the best known offline guarantee\n$\\frac{1}{p+1}$ [Nemhauser et al.]\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 17:13:52 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Shadravan", "Mohammad", ""]]}, {"id": "2001.00943", "submitter": "Lucas Assun\\c{c}\\~ao", "authors": "Lucas Assun\\c{c}\\~ao, Andr\\'ea Cynthia Santos, Thiago F. Noronha and\n  Rafael Andrade", "title": "On the Finite Optimal Convergence of Logic-Based Benders' Decomposition\n  in Solving 0-1 Min-max Regret Optimization Problems with Interval Costs", "comments": "arXiv admin note: substantial text overlap with arXiv:1609.09179", "journal-ref": "Proceedings of the International Symposium on Combinatorial\n  Optimization (ISCO) 2016: Lecture Notes in Computer Science, vol 9849.\n  Combinatorial Optimization pp 1-12. Springer, Cham", "doi": "10.1007/978-3-319-45587-7_1", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a class of problems under interval data uncertainty\ncomposed of min-max regret versions of classical 0-1 optimization problems with\ninterval costs. We refer to them as interval 0-1 min-max regret problems. The\nstate-of-the-art exact algorithms for this class of problems work by solving a\ncorresponding mixed integer linear programming formulation in a Benders'\ndecomposition fashion. Each of the possibly exponentially many Benders' cuts is\nseparated on the fly through the resolution of an instance of the classical 0-1\noptimization problem counterpart. Since these separation subproblems may be\nNP-hard, not all of them can be modeled by means of linear programming, unless\nP = NP. In these cases, the convergence of the aforementioned algorithms are\nnot guaranteed in a straightforward manner. In fact, to the best of our\nknowledge, their finite convergence has not been explicitly proved for any\ninterval 0-1 min-max regret problem. In this work, we formally describe these\nalgorithms through the definition of a logic-based Benders' decomposition\nframework and prove their convergence to an optimal solution in a finite number\nof iterations. As this framework is applicable to any interval 0-1 min-max\nregret problem, its finite optimal convergence also holds in the cases where\nthe separation subproblems are NP-hard.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 15:41:48 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Assun\u00e7\u00e3o", "Lucas", ""], ["Santos", "Andr\u00e9a Cynthia", ""], ["Noronha", "Thiago F.", ""], ["Andrade", "Rafael", ""]]}, {"id": "2001.01035", "submitter": "Sang-Sub Kim", "authors": "Sang-Sub Kim", "title": "Computing Euclidean k-Center over Sliding Windows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Euclidean $k$-center problem in sliding window model, input points are\ngiven in a data stream and the goal is to find the $k$ smallest congruent balls\nwhose union covers the $N$ most recent points of the stream. In this model,\ninput points are allowed to be examined only once and the amount of space that\ncan be used to store relative information is limited. Cohen-Addad et\nal.~\\cite{cohen-2016} gave a $(6+\\epsilon)$-approximation for the metric\n$k$-center problem using O($k/\\epsilon \\log \\alpha$) points, where $\\alpha$ is\nthe ratio of the largest and smallest distance and is assumed to be known in\nadvance. In this paper, we present a $(3+\\epsilon)$-approximation algorithm for\nthe Euclidean $1$-center problem using O($1/\\epsilon \\log \\alpha$) points. We\npresent an algorithm for the Euclidean $k$-center problem that maintains a\ncoreset of size $O(k)$. Our algorithm gives a $(c+2\\sqrt{3} +\n\\epsilon)$-approximation for the Euclidean $k$-center problem using\nO($k/\\epsilon \\log \\alpha$) points by using any given $c$-approximation for the\ncoreset where $c$ is a positive real number. For example, by using the\n$2$-approximation~\\cite{feder-greene-1988} of the coreset, our algorithm gives\na $(2+2\\sqrt{3} + \\epsilon)$-approximation ($\\approx 5.465$) using $O(k\\log k)$\ntime. This is an improvement over the approximation factor of $(6+\\epsilon)$ by\nCohen-Addad et al.~\\cite{cohen-2016} with the same space complexity and smaller\nupdate time per point. Moreover we remove the assumption that $\\alpha$ is known\nin advance. Our idea can be adapted to the metric diameter problem and the\nmetric $k$-center problem to remove the assumption. For low dimensional\nEuclidean space, we give an approximation algorithm that guarantees an even\nbetter approximation.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 04:36:24 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Kim", "Sang-Sub", ""]]}, {"id": "2001.01125", "submitter": "Bertrand Simon", "authors": "Martin B\\\"ohm and Bertrand Simon", "title": "Discovering and Certifying Lower Bounds for the Online Bin Stretching\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several problems in the theory of online computation where tight\nlower bounds on the competitive ratio are unknown and expected to be difficult\nto describe in a short form. A good example is the Online Bin Stretching\nproblem, in which the task is to pack the incoming items online into bins while\nminimizing the load of the largest bin. Additionally, the optimal load of the\nentire instance is known in advance.\n  The contribution of this paper is twofold. First, we provide the first\nnon-trivial lower bounds for Online Bin Stretching with 6, 7 and 8 bins, and\nincrease the best known lower bound for 3 bins. We describe in detail the\nalgorithmic improvements which were necessary for the discovery of the new\nlower bounds, which are several orders of magnitude more complex. The lower\nbounds are presented in the form of directed acyclic graphs.\n  Second, we use the Coq proof assistant to formalize the Online Bin Stretching\nproblem and certify these large lower bound graphs. The script we propose\ncertified as well all the previously claimed lower bounds, which until now were\nnever formally proven. To the best of our knowledge, this is the first use of a\nformal verification toolkit to certify a lower bound for an online problem.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 20:51:09 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["B\u00f6hm", "Martin", ""], ["Simon", "Bertrand", ""]]}, {"id": "2001.01146", "submitter": "Li-Yang Tan", "authors": "Moses Charikar, Weiyun Ma, Li-Yang Tan", "title": "New lower bounds for Massively Parallel Computation from query\n  complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Roughgarden, Vassilvitskii, and Wang (JACM 18) recently introduced a novel\nframework for proving lower bounds for Massively Parallel Computation using\ntechniques from boolean function complexity. We extend their framework in two\ndifferent ways, to capture two common features of Massively Parallel\nComputation:\n  $\\circ$ Adaptivity, where machines can write to and adaptively read from\nshared memory throughout the execution of the computation. Recent work of\nBehnezhad et al. (SPAA 19) showed that adaptivity enables significantly\nimproved round complexities for a number of central graph problems.\n  $\\circ$ Promise problems, where the algorithm only has to succeed on certain\ninputs. These inputs may have special structure that is of particular interest,\nor they may be representative of hard instances of the overall problem.\n  Using this extended framework, we give the first unconditional lower bounds\non the complexity of distinguishing whether an input graph is a cycle of length\n$n$ or two cycles of length $n/2$. This promise problem, 1v2-Cycle, has emerged\nas a central problem in the study of Massively Parallel Computation. We prove\nthat any adaptive algorithm for the 1v2-Cycle problem with I/O capacity\n$O(n^{\\varepsilon})$ per machine requires $\\Omega(1/\\varepsilon)$ rounds,\nmatching a recent upper bound of Behnezhad et al.\n  In addition to strengthening the connections between Massively Parallel\nComputation and boolean function complexity, we also develop new machinery to\nreason about the latter. At the heart of our proofs are optimal lower bounds on\nthe query complexity and approximate certificate complexity of the 1v2-Cycle\nproblem.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 00:43:50 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Charikar", "Moses", ""], ["Ma", "Weiyun", ""], ["Tan", "Li-Yang", ""]]}, {"id": "2001.01194", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen and Yun Yang", "title": "Cutoff for exact recovery of Gaussian mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.IT math.IT math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We determine the information-theoretic cutoff value on separation of cluster\ncenters for exact recovery of cluster labels in a $K$-component Gaussian\nmixture model with equal cluster sizes. Moreover, we show that a semidefinite\nprogramming (SDP) relaxation of the $K$-means clustering method achieves such\nsharp threshold for exact recovery without assuming the symmetry of cluster\ncenters.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 08:57:04 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 19:44:07 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 11:14:47 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Chen", "Xiaohui", ""], ["Yang", "Yun", ""]]}, {"id": "2001.01230", "submitter": "Juho Lauri", "authors": "Juho Lauri, Sourav Dutta, Marco Grassia, Deepak Ajwani", "title": "Learning fine-grained search space pruning and heuristics for\n  combinatorial optimization", "comments": "Integrates three works which appeared at AAAI'19 [arXiv:1902.08455],\n  the DSO workshop at IJCAI'19 [arXiv:1910.00517] and CIKM'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial optimization problems arise in a wide range of applications\nfrom diverse domains. Many of these problems are NP-hard and designing\nefficient heuristics for them requires considerable time and experimentation.\nOn the other hand, the number of optimization problems in the industry\ncontinues to grow. In recent years, machine learning techniques have been\nexplored to address this gap. We propose a framework for leveraging machine\nlearning techniques to scale-up exact combinatorial optimization algorithms. In\ncontrast to the existing approaches based on deep-learning, reinforcement\nlearning and restricted Boltzmann machines that attempt to directly learn the\noutput of the optimization problem from its input (with limited success), our\nframework learns the relatively simpler task of pruning the elements in order\nto reduce the size of the problem instances. In addition, our framework uses\nonly interpretable learning models based on intuitive features and thus the\nlearning process provides deeper insights into the optimization problem and the\ninstance class, that can be used for designing better heuristics. For the\nclassical maximum clique enumeration problem, we show that our framework can\nprune a large fraction of the input graph (around 99 % of nodes in case of\nsparse graphs) and still detect almost all of the maximum cliques. This results\nin several fold speedups of state-of-the-art algorithms. Furthermore, the model\nused in our framework highlights that the chi-squared value of neighborhood\ndegree has a statistically significant correlation with the presence of a node\nin a maximum clique, particularly in dense graphs which constitute a\nsignificant challenge for modern solvers. We leverage this insight to design a\nnovel heuristic for this problem outperforming the state-of-the-art. Our\nheuristic is also of independent interest for maximum clique detection and\nenumeration.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 13:10:39 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Lauri", "Juho", ""], ["Dutta", "Sourav", ""], ["Grassia", "Marco", ""], ["Ajwani", "Deepak", ""]]}, {"id": "2001.01289", "submitter": "Pawe{\\l} Gawrychowski", "authors": "Bart{\\l}omiej Dudek, Pawe{\\l} Gawrychowski, Tatiana Starikovskaya", "title": "All non-trivial variants of 3-LDT are equivalent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popular 3-SUM conjecture states that there is no strongly subquadratic\ntime algorithm for checking if a given set of integers contains three distinct\nelements that sum up to zero. A closely related problem is to check if a given\nset of integers contains distinct $x_1, x_2, x_3$ such that $x_1+x_2=2x_3$.\nThis can be reduced to 3-SUM in almost-linear time, but surprisingly a reverse\nreduction establishing 3-SUM hardness was not known.\n  We provide such a reduction, thus resolving an open question of Erickson. In\nfact, we consider a more general problem called 3-LDT parameterized by integer\nparameters $\\alpha_1, \\alpha_2, \\alpha_3$ and $t$. In this problem, we need to\ncheck if a given set of integers contains distinct elements $x_1, x_2, x_3$\nsuch that $\\alpha_1 x_1+\\alpha_2 x_2 +\\alpha_3 x_3 = t$. For some combinations\nof the parameters, every instance of this problem is a NO-instance or there\nexists a simple almost-linear time algorithm. We call such variants trivial. We\nprove that all non-trivial variants of 3-LDT are equivalent under subquadratic\nreductions. Our main technical contribution is an efficient deterministic\nprocedure based on the famous Behrend's construction that partitions a given\nset of integers into few subsets that avoid a chosen linear equation.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 18:43:04 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Dudek", "Bart\u0142omiej", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Starikovskaya", "Tatiana", ""]]}, {"id": "2001.01440", "submitter": "Michael Hecht", "authors": "Michael Hecht, Krzysztof Gonciarz and Szabolcs Horv\\'at", "title": "Tight Localizations of Feedback Sets", "comments": "manuscript submitted to ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical NP-hard feedback arc set problem (FASP) and feedback vertex set\nproblem (FVSP) ask for a minimum set of arcs $\\varepsilon \\subseteq E$ or\nvertices $\\nu \\subseteq V$ whose removal $G\\setminus \\varepsilon$, $G\\setminus\n\\nu$ makes a given multi-digraph $G=(V,E)$ acyclic, respectively. Though both\nproblems are known to be APX-hard, approximation algorithms or proofs of\ninapproximability are unknown. We propose a new\n$\\mathcal{O}(|V||E|^4)$-heuristic for the directed FASP. While a ratio of $r\n\\approx 1.3606$ is known to be a lower bound for the APX-hardness, at least by\nempirical validation we achieve an approximation of $r \\leq 2$. The most\nrelevant applications, such as circuit testing, ask for solving the FASP on\nlarge sparse graphs, which can be done efficiently within tight error bounds\ndue to our approach.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 09:01:27 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 15:56:49 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Hecht", "Michael", ""], ["Gonciarz", "Krzysztof", ""], ["Horv\u00e1t", "Szabolcs", ""]]}, {"id": "2001.01452", "submitter": "Nimrod Fiat", "authors": "Nimrod Fiat, Dana Ron", "title": "On Efficient Distance Approximation for Graph Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distance-approximation algorithm for a graph property $\\mathcal{P}$ in the\nadjacency-matrix model is given an approximation parameter $\\epsilon \\in (0,1)$\nand query access to the adjacency matrix of a graph $G=(V,E)$. It is required\nto output an estimate of the \\emph{distance} between $G$ and the closest graph\n$G'=(V,E')$ that satisfies $\\mathcal{P}$, where the distance between graphs is\nthe size of the symmetric difference between their edge sets, normalized by\n$|V|^2$. In this work we introduce property covers, as a framework for using\ndistance-approximation algorithms for \"simple\" properties to design\ndistance-approximation. Applying this framework we present\ndistance-approximation algorithms with $poly(1/\\epsilon)$ query complexity for\ninduced $P_3$-freeness, induced $P_4$-freeness, and Chordality. For induced\n$C_4$-freeness our algorithm has query complexity $exp(poly(1/\\epsilon))$.\nThese complexities essentially match the corresponding known results for\ntesting these properties and provide an exponential improvement on previously\nknown results.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 09:38:51 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Fiat", "Nimrod", ""], ["Ron", "Dana", ""]]}, {"id": "2001.01531", "submitter": "Srikrishnan Divakaran", "authors": "Srikrishnan Divakaran", "title": "An Optimal Algorithm for 1-D Cutting Stock Problem", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an $n\\Delta^{O(k^2)}$ time algorithm to obtain an optimal solution\nfor $1$-dimensional cutting stock problem: the bin packing problem of packing\n$n$ items onto unit capacity bins under the restriction that the number of item\nsizes $k$ is fixed, where $\\Delta$ is the reciprocal of the size of the\nsmallest item. We employ elementary ideas in both the design and analysis our\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 12:50:26 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Divakaran", "Srikrishnan", ""]]}, {"id": "2001.01661", "submitter": "Konstantinos Semertzidis", "authors": "Konstantinos Semertzidis, Evaggelia Pitoura", "title": "A Hybrid Approach to Temporal Pattern Matching", "comments": "4 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary objective of graph pattern matching is to find all appearances of\nan input graph pattern query in a large data graph. Such appearances are called\nmatches. In this paper, we are interested in finding matches of interaction\npatterns in temporal graphs. To this end, we propose a hybrid approach that\nachieves effective filtering of potential matches based both on structure and\ntime. Our approach exploits a graph representation where edges are ordered by\ntime. We present experiments with real datasets that illustrate the efficiency\nof our approach.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 16:52:44 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 11:22:10 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Semertzidis", "Konstantinos", ""], ["Pitoura", "Evaggelia", ""]]}, {"id": "2001.01715", "submitter": "Kevin Schewior", "authors": "Chien-Chung Huang, Mathieu Mari, Claire Mathieu, Kevin Schewior, Jens\n  Vygen", "title": "An Approximation Algorithm for Fully Planar Edge-Disjoint Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise a constant-factor approximation algorithm for the maximization\nversion of the edge-disjoint paths problem if the supply graph together with\nthe demand edges form a planar graph. By planar duality this is equivalent to\npacking cuts in a planar graph such that each cut contains exactly one demand\nedge. We also show that the natural linear programming relaxations have\nconstant integrality gap, yielding an approximate max-multiflow min-multicut\ntheorem.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 18:59:00 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Huang", "Chien-Chung", ""], ["Mari", "Mathieu", ""], ["Mathieu", "Claire", ""], ["Schewior", "Kevin", ""], ["Vygen", "Jens", ""]]}, {"id": "2001.01803", "submitter": "Zhuoran Dang", "authors": "Zhuoran Dang", "title": "A new approach on estimating the fluid temperature in a multiphase flow\n  system using particle filter method", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fluid temperature is important for the analysis of the heat transfers in\nthermal hydraulics. An accurate measurement or estimation of the fluid\ntemperature in multiphase flows is challenging. This is due to that the\nthermocouple signal that mixes with temperature signals for each phase and\nnon-negligible noises. This study provides a new approach to estimate the local\nfluid temperature in multiphase flows using experimental time-series\ntemperature signal. The thermocouple signal is considered to be a sequence with\nMarkov property and the particle filter method is utilized in the new method to\nextract the fluid temperature. A complete description of the new method is\npresented in this article.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 22:41:16 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Dang", "Zhuoran", ""]]}, {"id": "2001.01835", "submitter": "Patrick Rodler", "authors": "Patrick Rodler", "title": "Understanding the QuickXPlain Algorithm: Simple Explanation and Formal\n  Proof", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In his seminal paper of 2004, Ulrich Junker proposed the QuickXPlain\nalgorithm, which provides a divide-and-conquer computation strategy to find\nwithin a given set an irreducible subset with a particular (monotone) property.\nBeside its original application in the domain of constraint satisfaction\nproblems, the algorithm has since then found widespread adoption in areas as\ndifferent as model-based diagnosis, recommender systems, verification, or the\nSemantic Web. This popularity is due to the frequent occurrence of the problem\nof finding irreducible subsets on the one hand, and to QuickXPlain's general\napplicability and favorable computational complexity on the other hand.\n  However, although (we regularly experience) people are having a hard time\nunderstanding QuickXPlain and seeing why it works correctly, a proof of\ncorrectness of the algorithm has never been published. This is what we account\nfor in this work, by explaining QuickXPlain in a novel tried and tested way and\nby presenting an intelligible formal proof of it. Apart from showing the\ncorrectness of the algorithm and excluding the later detection of errors (proof\nand trust effect), the added value of the availability of a formal proof is,\ne.g., (i) that the workings of the algorithm often become completely clear only\nafter studying, verifying and comprehending the proof (didactic effect), (ii)\nthe shown proof methodology can be used as a guidance for proving other\nrecursive algorithms (transfer effect), and (iii) the possibility of providing\n\"gapless\" correctness proofs of systems that rely on (results computed by)\nQuickXPlain, such as numerous model-based debuggers (completeness effect).\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 01:37:41 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 07:59:09 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Rodler", "Patrick", ""]]}, {"id": "2001.01914", "submitter": "Kamil Khadiev", "authors": "Kamil Khadiev and Artem Ilikaev", "title": "Quantum Algorithms for the Most Frequently String Search, Intersection\n  of Two String Sequences and Sorting of Strings Problems", "comments": "THe paper was presented on TPNC 2019", "journal-ref": "TPNC 2019. Lecture Notes in Computer Science, vol 11934. Springer,\n  Cham", "doi": "10.1007/978-3-030-34500-6_17", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study algorithms for solving three problems on strings. The first one is\nthe Most Frequently String Search Problem. The problem is the following. Assume\nthat we have a sequence of $n$ strings of length $k$. The problem is finding\nthe string that occurs in the sequence most often. We propose a quantum\nalgorithm that has a query complexity $\\tilde{O}(n \\sqrt{k})$. This algorithm\nshows speed-up comparing with the deterministic algorithm that requires\n$\\Omega(nk)$ queries. The second one is searching intersection of two sequences\nof strings. All strings have the same length $k$. The size of the first set is\n$n$ and the size of the second set is $m$. We propose a quantum algorithm that\nhas a query complexity $\\tilde{O}((n+m) \\sqrt{k})$. This algorithm shows\nspeed-up comparing with the deterministic algorithm that requires\n$\\Omega((n+m)k)$ queries. The third problem is sorting of $n$ strings of length\n$k$. On the one hand, it is known that quantum algorithms cannot sort objects\nasymptotically faster than classical ones. On the other hand, we focus on\nsorting strings that are not arbitrary objects. We propose a quantum algorithm\nthat has a query complexity $O(n (\\log n)^2 \\sqrt{k})$. This algorithm shows\nspeed-up comparing with the deterministic algorithm (radix sort) that requires\n$\\Omega((n+d)k)$ queries, where $d$ is a size of the alphabet.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 07:22:02 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Khadiev", "Kamil", ""], ["Ilikaev", "Artem", ""]]}, {"id": "2001.01961", "submitter": "Riccardo Dondi", "authors": "Riccardo Dondi, Giancarlo Mauri, Italo Zoppis", "title": "Complexity Issues of String to Graph Approximate Matching", "comments": "Extended version of a paper accepted to LATA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of matching a query string to a directed graph, whose vertices\nare labeled by strings, has application in different fields, from data mining\nto computational biology. Several variants of the problem have been considered,\ndepending on the fact that the match is exact or approximate and, in this\nlatter case, which edit operations are considered and where are allowed. In\nthis paper we present results on the complexity of the approximate matching\nproblem, where edit operations are symbol substitutions and are allowed only on\nthe graph labels or both on the graph labels and the query string. We introduce\na variant of the problem that asks whether there exists a path in a graph that\nrepresents a query string with any number of edit operations and we show that\nis is NP-complete, even when labels have length one and in the case the\nalphabet is binary. Moreover, when it is parameterized by the length of the\ninput string and graph labels have length one, we show that the problem is\nfixed-parameter tractable and it is unlikely to admit a polynomial kernel. The\nNP-completeness of this problem leads to the inapproximability (within any\nfactor) of the approximate matching when edit operations are allowed only on\nthe graph labels. Moreover, we show that the variants of approximate string\nmatching to graph we consider are not fixed-parameter tractable, when the\nparameter is the number of edit operations, even for graphs that have distance\none from a DAG. The reduction for this latter result allows us to prove the\ninapproximability of the variant where edit operations can be applied both on\nthe query string and on graph labels.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 10:40:52 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Dondi", "Riccardo", ""], ["Mauri", "Giancarlo", ""], ["Zoppis", "Italo", ""]]}, {"id": "2001.02139", "submitter": "Jens Stoye", "authors": "Leonard Bohnenk\\\"amper, Mar\\'ilia D. V. Braga, Daniel Doerr, Jens\n  Stoye", "title": "Computing the rearrangement distance of natural genomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of genomic distances has been a very active field of\ncomputational comparative genomics over the last 25 years. Substantial results\ninclude the polynomial-time computability of the inversion distance by\nHannenhalli and Pevzner in 1995 and the introduction of the double-cut and join\n(DCJ) distance by Yancopoulos et al. in 2005. Both results, however, rely on\nthe assumption that the genomes under comparison contain the same set of unique\nmarkers (syntenic genomic regions, sometimes also referred to as genes). In\n2015, Shao, Lin and Moret relax this condition by allowing for duplicate\nmarkers in the analysis. This generalized version of the genomic distance\nproblem is NP-hard, and they give an ILP solution that is efficient enough to\nbe applied to real-world datasets. A restriction of their approach is that it\ncan be applied only to balanced genomes, that have equal numbers of duplicates\nof any marker. Therefore it still needs a delicate preprocessing of the input\ndata in which excessive copies of unbalanced markers have to be removed.\n  In this paper we present an algorithm solving the genomic distance problem\nfor natural genomes, in which any marker may occur an arbitrary number of\ntimes. Our method is based on a new graph data structure, the multi-relational\ndiagram, that allows an elegant extension of the ILP by Shao, Lin and Moret to\ncount runs of markers that are under- or over-represented in one genome with\nrespect to the other and need to be inserted or deleted, respectively. With\nthis extension, previous restrictions on the genome configurations are lifted,\nfor the first time enabling an uncompromising rearrangement analysis. Any\nmarker sequence can directly be used for the distance calculation.\n  The evaluation of our approach shows that it can be used to analyze genomes\nwith up to a few ten thousand markers, which we demonstrate on simulated and\nreal data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 15:55:52 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 13:49:47 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 15:28:36 GMT"}, {"version": "v4", "created": "Fri, 2 Oct 2020 18:40:45 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Bohnenk\u00e4mper", "Leonard", ""], ["Braga", "Mar\u00edlia D. V.", ""], ["Doerr", "Daniel", ""], ["Stoye", "Jens", ""]]}, {"id": "2001.02172", "submitter": "Philipp G\\\"otze", "authors": "Philipp G\\\"otze, Arun Kumar Tharanatha, Kai-Uwe Sattler", "title": "Data Structure Primitives on Persistent Memory: An Evaluation", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent Memory (PMem), as already available, e.g., with Intel Optane DC\nPersistent Memory, represents a very promising, next-generation memory solution\nwith a significant impact on database architectures. Several data structures\nfor this new technology and its properties have already been proposed. However,\nprimarily only complete structures are presented and evaluated. Thus, the\nimplications of the individual ideas and PMem features are concealed.\nTherefore, in this paper, we disassemble the structures presented so far,\nidentify their underlying design primitives, and assign them to appropriate\ndesign goals regarding PMem. As a result of our comprehensive experiments on\nreal PM hardware, we can reveal the trade-offs of the primitives for various\naccess patterns. This allowed us to pinpoint their best use cases as well as\nvulnerabilities. Besides our general insights regarding PMem-based data\nstructure design, we also discovered new combinations not examined in the\nliterature so far.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 16:56:23 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 13:51:59 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["G\u00f6tze", "Philipp", ""], ["Tharanatha", "Arun Kumar", ""], ["Sattler", "Kai-Uwe", ""]]}, {"id": "2001.02191", "submitter": "Michele Scquizzato", "authors": "Danupon Nanongkai and Michele Scquizzato", "title": "Equivalence Classes and Conditional Hardness in Massively Parallel\n  Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Massively Parallel Computation (MPC) model serves as a common abstraction\nof many modern large-scale data processing frameworks, and has been receiving\nincreasingly more attention over the past few years, especially in the context\nof classical graph problems. So far, the only way to argue lower bounds for\nthis model is to condition on conjectures about the hardness of some specific\nproblems, such as graph connectivity on promise graphs that are either one\ncycle or two cycles, usually called the one cycle vs. two cycles problem. This\nis unlike the traditional arguments based on conjectures about complexity\nclasses (e.g., $\\textsf{P} \\neq \\textsf{NP}$), which are often more robust in\nthe sense that refuting them would lead to groundbreaking algorithms for a\nwhole bunch of problems.\n  In this paper we present connections between problems and classes of problems\nthat allow the latter type of arguments. These connections concern the class of\nproblems solvable in a sublogarithmic amount of rounds in the MPC model,\ndenoted by $\\textsf{MPC}(o(\\log N))$, and some standard classes concerning\nspace complexity, namely $\\textsf{L}$ and $\\textsf{NL}$, and suggest\nconjectures that are robust in the sense that refuting them would lead to many\nsurprisingly fast new algorithms in the MPC model. We also obtain new\nconditional lower bounds, and prove new reductions and equivalences between\nproblems in the MPC model.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 17:40:00 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Nanongkai", "Danupon", ""], ["Scquizzato", "Michele", ""]]}, {"id": "2001.02411", "submitter": "Jakub Pek\\'arek", "authors": "Zden\\v{e}k Dvo\\v{r}\\'ak, Jakub Pek\\'arek", "title": "Induced odd cycle packing number, independent sets, and chromatic number", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\emph{induced odd cycle packing number} $\\text{iocp}(G)$ of a graph $G$\nis the maximum integer $k$ such that $G$ contains an induced subgraph\nconsisting of $k$ pairwise vertex-disjoint odd cycles. Motivated by\napplications to geometric graphs, Bonamy et al. proved that graphs of bounded\ninduced odd cycle packing number, bounded VC dimension, and linear independence\nnumber admit a randomized EPTAS for the independence number. We show that the\nassumption of bounded VC dimension is not necessary, exhibiting a randomized\nalgorithm that for any integers $k\\ge 0$ and $t\\ge 1$ and any $n$-vertex graph\n$G$ of induced odd cycle packing number returns in time $O_{k,t}(n^{k+4})$ an\nindependent set of $G$ whose size is at least $\\alpha(G)-n/t$ with high\nprobability. In addition, we present $\\chi$-boundedness results for graphs with\nbounded odd cycle packing number, and use them to design a QPTAS for the\nindependence number only assuming bounded induced odd cycle packing number.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 08:12:41 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Dvo\u0159\u00e1k", "Zden\u011bk", ""], ["Pek\u00e1rek", "Jakub", ""]]}, {"id": "2001.02727", "submitter": "David Williamson", "authors": "Alice Paul and David P. Williamson", "title": "Easy Capacitated Facility Location Problems, with Connections to\n  Lot-Sizing", "comments": null, "journal-ref": null, "doi": "10.1016/j.orl.2019.12.006", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we consider the capacitated facility location problem when the\ntransportation costs of the instance satisfy the Monge property. We show that a\nstraightforward dynamic program finds the optimal solution when the demands are\npolynomially bounded. When demands are not polynomially bounded, we give a\nfully polynomial-time approximation scheme by adapting an algorithm and\nanalysis of Van Hoesel and Wagelmans.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 20:28:42 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Paul", "Alice", ""], ["Williamson", "David P.", ""]]}, {"id": "2001.02817", "submitter": "Nate Veldt", "authors": "Nate Veldt and Austin R. Benson and Jon Kleinberg", "title": "Hypergraph Cuts with General Splitting Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum $s$-$t$ cut problem in graphs is one of the most fundamental\nproblems in combinatorial optimization, and graph cuts underlie algorithms\nthroughout discrete mathematics, theoretical computer science, operations\nresearch, and data science. While graphs are a standard model for pairwise\nrelationships, hypergraphs provide the flexibility to model multi-way\nrelationships, and are now a standard model for complex data and systems.\nHowever, when generalizing from graphs to hypergraphs, the notion of a \"cut\nhyperedge\" is less clear, as a hyperedge's nodes can be split in several ways.\nHere, we develop a framework for hypergraph cuts by considering the problem of\nseparating two terminal nodes in a hypergraph in a way that minimizes a sum of\npenalties at split hyperedges. In our setup, different ways of splitting the\nsame hyperedge have different penalties, and the penalty is encoded by what we\ncall a splitting function.\n  Our framework opens a rich space on the foundations of hypergraph cuts. We\nfirst identify a natural class of cardinality-based hyperedge splitting\nfunctions that depend only on the number of nodes on each side of the split. In\nthis case, we show that the general hypergraph $s$-$t$ cut problem can be\nreduced to a tractable graph $s$-$t$ cut problem if and only if the splitting\nfunctions are submodular. We also identify a wide regime of non-submodular\nsplitting functions for which the problem is NP-hard. We also analyze\nextensions to multiway cuts with at least three terminal nodes and identify a\nnatural class of splitting functions for which the problem can be reduced in an\napproximation-preserving way to the node-weighted multiway cut problem in\ngraphs, again subject to a submodularity property. Finally, we outline several\nopen questions on general hypergraph cut problems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 02:46:10 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Veldt", "Nate", ""], ["Benson", "Austin R.", ""], ["Kleinberg", "Jon", ""]]}, {"id": "2001.02818", "submitter": "Ala Shayeghi", "authors": "Debbie Leung, Ashwin Nayak, Ala Shayeghi, Dave Touchette, Penghui Yao,\n  Nengkun Yu", "title": "Capacity Approaching Coding for Low Noise Interactive Quantum\n  Communication, Part I: Large Alphabets", "comments": "94 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of implementing two-party interactive quantum\ncommunication over noisy channels, a necessary endeavor if we wish to fully\nreap quantum advantages for communication. For an arbitrary protocol with $n$\nmessages, designed for a noiseless qudit channel over a $\\mathrm{poly}(n)$ size\nalphabet, our main result is a simulation method that fails with probability\nless than $2^{-\\Theta(n\\epsilon)}$ and uses a qudit channel over the same\nalphabet $n\\left(1+\\Theta \\left(\\sqrt{\\epsilon}\\right)\\right)$ times, of which\nan $\\epsilon$ fraction can be corrupted adversarially. The simulation is thus\ncapacity achieving to leading order, and we conjecture that it is optimal up to\na constant factor in the $\\sqrt{\\epsilon}$ term. Furthermore, the simulation is\nin a model that does not require pre-shared resources such as randomness or\nentanglement between the communicating parties. Our work improves over the best\npreviously known quantum result where the overhead is a non-explicit large\nconstant [Brassard et al., FOCS'14] for low $\\epsilon$.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 02:48:43 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Leung", "Debbie", ""], ["Nayak", "Ashwin", ""], ["Shayeghi", "Ala", ""], ["Touchette", "Dave", ""], ["Yao", "Penghui", ""], ["Yu", "Nengkun", ""]]}, {"id": "2001.02827", "submitter": "Vedat Levi Alev", "authors": "Vedat Levi Alev, Lap Chi Lau", "title": "Improved Analysis of Higher Order Random Walks and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation of this work is to extend the techniques of higher order\nrandom walks on simplicial complexes to analyze mixing times of Markov chains\nfor combinatorial problems. Our main result is a sharp upper bound on the\nsecond eigenvalue of the down-up walk on a pure simplicial complex, in terms of\nthe second eigenvalues of its links. We show some applications of this result\nin analyzing mixing times of Markov chains, including sampling independent sets\nof a graph and sampling common independent sets of two partition matroids.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 04:00:35 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 15:20:33 GMT"}, {"version": "v3", "created": "Thu, 6 Feb 2020 05:37:44 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Alev", "Vedat Levi", ""], ["Lau", "Lap Chi", ""]]}, {"id": "2001.03015", "submitter": "Mustafa Ozdayi", "authors": "Mustafa Safa Ozdayi", "title": "Achieving Competitiveness in Online Problems", "comments": "Semester Project at EPFL. Supervised by Ola Svensson and Sangxia\n  Huang", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the setting of online algorithms, the input is initially not present but\nrather arrive one-by-one over time and after each input, the algorithm has to\nmake a decision. Depending on the formulation of the problem, the algorithm\nmight be allowed to change its previous decisions or not at a later time. We\nanalyze two problems to show that it is possible for an online algorithm to\nbecome more competitive by changing its former decisions. We first consider the\nonline edge orientation in which the edges arrive one-by-one to an empty graph\nand the aim is to orient them in a way such that the maximum in-degree is\nminimized. We then consider the online bipartite b-matching. In this problem,\nwe are given a bipartite graph where one side of the graph is initially present\nand where the other side arrive online. The goal is to maintain a matching set\nsuch that the maximum degree in the set is minimized. For both of the problems,\nthe best achievable competitive ratio is $\\Theta(\\log n)$ over n input arrivals\nwhen decisions are irreversible. We study three algorithms for these problems,\ntwo for the former and one for the latter, that achieve O(1) competitive ratio\nby changing O(n) of their decisions over n arrivals. In addition to that, we\nanalyze one of the algorithms, the shortest path algorithm, against an\nadversary. Through that, we prove some new results about algorithms\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 16:24:56 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Ozdayi", "Mustafa Safa", ""]]}, {"id": "2001.03050", "submitter": "Guangyi Zhang", "authors": "Guangyi Zhang and Aristides Gionis", "title": "Maximizing diversity over clustered data", "comments": "SDM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum diversity aims at selecting a diverse set of high-quality objects\nfrom a collection, which is a fundamental problem and has a wide range of\napplications, e.g., in Web search. Diversity under a uniform or partition\nmatroid constraint naturally describes useful cardinality or budget\nrequirements, and admits simple approximation algorithms. When applied to\nclustered data, however, popular algorithms such as picking objects iteratively\nand performing local search lose their approximation guarantees towards maximum\nintra-cluster diversity because they fail to optimize the objective in a global\nmanner. We propose an algorithm that greedily adds a pair of objects instead of\na singleton, and which attains a constant approximation factor over clustered\ndata. We further extend the algorithm to the case of monotone and submodular\nquality function, and under a partition matroid constraint. We also devise a\ntechnique to make our algorithm scalable, and on the way we obtain a\nmodification that gives better solutions in practice while maintaining the\napproximation guarantee in theory. Our algorithm achieves excellent\nperformance, compared to strong baselines in a mix of synthetic and real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 15:11:37 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 12:02:23 GMT"}, {"version": "v3", "created": "Sat, 25 Jan 2020 21:42:55 GMT"}, {"version": "v4", "created": "Sat, 10 Apr 2021 12:14:10 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhang", "Guangyi", ""], ["Gionis", "Aristides", ""]]}, {"id": "2001.03098", "submitter": "Leon Kellerhals", "authors": "Leon Kellerhals and Tomohiro Koana", "title": "Parameterized Complexity of Geodetic Set", "comments": "Accepted at IPEC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vertex set $S$ of a graph $G$ is geodetic if every vertex of $G$ lies on a\nshortest path between two vertices in $S$. Given a graph $G$ and $k \\in \\mathbb\nN$, the NP-hard Geodetic Set problem asks whether there is a geodetic set of\nsize at most $k$. Complementing various works on Geodetic Set restricted to\nspecial graph classes, we initiate a parameterized complexity study of Geodetic\nSet and show, on the negative side, that Geodetic Set is W[1]-hard when\nparameterized by feedback vertex number, path-width, and solution size,\ncombined. On the positive side, we develop fixed-parameter algorithms with\nrespect to the feedback edge number, the tree-depth, and the modular-width of\nthe input graph.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 16:54:03 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 08:31:07 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 16:33:16 GMT"}, {"version": "v4", "created": "Wed, 30 Sep 2020 16:39:55 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Kellerhals", "Leon", ""], ["Koana", "Tomohiro", ""]]}, {"id": "2001.03107", "submitter": "Lukas N\\\"olke", "authors": "Nicole Megow (1) and Lukas N\\\"olke (1) ((1) University of Bremen,\n  Germany)", "title": "Online Minimum Cost Matching on the Line with Recourse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online minimum cost matching on the line, $n$ requests appear one by one\nand have to be matched immediately and irrevocably to a given set of servers,\nall on the real line. The goal is to minimize the sum of distances from the\nrequests to their respective servers. Despite all research efforts, it remains\nan intriguing open question whether there exists an $O(1)$-competitive\nalgorithm. The best known online algorithm by Raghvendra [SoCG18] achieves a\ncompetitive factor of $\\Theta(\\log n)$. This result matches a lower bound of\n$\\Omega(\\log n)$ [Latin18] that holds for a quite large class of online\nalgorithms, including all deterministic algorithms in the literature.\n  In this work we approach the problem in a recourse model where we allow to\nrevoke online decisions to some extent. We show an $O(1)$-competitive algorithm\nfor online matching on the line that uses at most $O(n\\log n)$ reassignments.\nThis is the first non-trivial result for min-cost bipartite matching with\nrecourse. For so-called alternating instances, with no more than one request\nbetween two servers, we obtain a near-optimal result. We give a\n$(1+\\varepsilon)$-competitive algorithm that reassigns any request at most\n$O(\\varepsilon^{-1.001})$ times. This special case is interesting as the\naforementioned quite general lower bound $\\Omega(\\log n)$ holds for such\ninstances.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 17:11:05 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Megow", "Nicole", ""], ["N\u00f6lke", "Lukas", ""]]}, {"id": "2001.03128", "submitter": "Konstantinos Semertzidis", "authors": "Ioannis Kouvatis, Konstantinos Semertzidis, Maria Zerva, Evaggelia\n  Pitoura, Panayiotis Tsaparas", "title": "Forming Compatible Teams in Signed Networks", "comments": "In Proceedings of the 23rd International Conference on Extending\n  Database Technology (EDBT), 2020", "journal-ref": null, "doi": "10.5441/002/edbt.2020.33", "report-no": null, "categories": "cs.DS cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of team formation in a social network asks for a set of\nindividuals who not only have the required skills to perform a task but who can\nalso communicate effectively with each other. Existing work assumes that all\nlinks in a social network are positive, that is, they indicate friendship or\ncollaboration between individuals. However, it is often the case that the\nnetwork is signed, that is, it contains both positive and negative links,\ncorresponding to friend and foe relationships. Building on the concept of\nstructural balance, we provide definitions of compatibility between pairs of\nusers in a signed network, and algorithms for computing it. We then define the\nteam formation problem in signed networks, where we ask for a compatible team\nof individuals that can perform a task with small communication cost. We show\nthat the problem is NP-hard even when there are no communication cost\nconstraints, and we provide heuristic algorithms for solving it. We present\nexperimental results with real data to investigate the properties of the\ndifferent compatibility definitions, and the effectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 17:43:41 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 16:00:46 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 16:56:10 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Kouvatis", "Ioannis", ""], ["Semertzidis", "Konstantinos", ""], ["Zerva", "Maria", ""], ["Pitoura", "Evaggelia", ""], ["Tsaparas", "Panayiotis", ""]]}, {"id": "2001.03147", "submitter": "Carlos Baquero", "authors": "Ariel Shtul and Carlos Baquero and Paulo S\\'ergio Almeida", "title": "Age-Partitioned Bloom Filters", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bloom filters (BF) are widely used for approximate membership queries over a\nset of elements. BF variants allow removals, sets of unbounded size or querying\na sliding window over an unbounded stream. However, for this last case the best\ncurrent approaches are dictionary based (e.g., based on Cuckoo Filters or\nTinyTable), and it may seem that BF-based approaches will never be competitive\nto dictionary-based ones. In this paper we present Age-Partitioned Bloom\nFilters, a BF-based approach for duplicate detection in sliding windows that\nnot only is competitive in time-complexity, but has better space usage than\ncurrent dictionary-based approaches (e.g., SWAMP), at the cost of some moderate\nslack. APBFs retain the BF simplicity, unlike dictionary-based approaches,\nimportant for hardware-based implementations, and can integrate known\nimprovements such as double hashing or blocking. We present an Age-Partitioned\nBlocked Bloom Filter variant which can operate with 2-3 cache-line accesses per\ninsertion and around 2-4 per query, even for high accuracy filters.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 18:23:11 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Shtul", "Ariel", ""], ["Baquero", "Carlos", ""], ["Almeida", "Paulo S\u00e9rgio", ""]]}, {"id": "2001.03161", "submitter": "Pratibha Choudhary", "authors": "Pratibha Choudhary and Venkatesh Raman", "title": "Improved Kernels for Tracking Path Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking of moving objects is crucial to security systems and networks. Given\na graph $G$, terminal vertices $s$ and $t$, and an integer $k$, the\n\\textsc{Tracking Paths} problem asks whether there exists at most $k$ vertices,\nwhich if marked as trackers, would ensure that the sequence of trackers\nencountered in each s-t path is unique. It is known that the problem is NP-hard\nand admits a kernel (reducible to an equivalent instance) with\n$\\mathcal{O}(k^6)$ vertices and $\\mathcal{O}(k^7)$ edges, when parameterized by\nthe size of the output (tracking set) $k$ [5]. An interesting question that\nremains open is whether the existing kernel can be improved. In this paper we\nanswer this affirmatively: (i) For general graphs, we show the existence of a\nkernel of size $\\mathcal{O}(k^2)$, (ii) For planar graphs, we improve this\nfurther by giving a kernel of size $\\mathcal{O}(k)$. In addition, we also show\nthat finding a tracking set of size at most $n-k$ for a graph on $n$ vertices\nis hard for the parameterized complexity class W[1], when parameterized by $k$.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 18:49:22 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 12:14:44 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Choudhary", "Pratibha", ""], ["Raman", "Venkatesh", ""]]}, {"id": "2001.03196", "submitter": "Baichuan Mo", "authors": "Baichuan Mo, Zhenliang Ma, Haris N. Koutsopoulos, Jinhua Zhao", "title": "Assignment-based Path Choice Estimation for Metro Systems Using Smart\n  Card Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban rail services are the principal means of public transportation in many\ncities. To understand the crowding patterns and develop efficient operation\nstrategies in the system, obtaining path choices is important. This paper\nproposed an assignment-based path choice estimation framework using automated\nfare collection (AFC) data. The framework captures the inherent correlation of\ncrowding among stations, as well as the interaction between path choice and\nleft behind. The path choice estimation is formulated as an optimization\nproblem. The original problem is intractable because of a non-analytical\nconstraint and a non-linear equation constraint. A solution procedure is\nproposed to decompose the original problem into three tractable sub-problems,\nwhich can be solved efficiently. The model is validated using both synthetic\ndata and real-world AFC data in Hong Kong Mass Transit Railway (MTR) system.\nThe synthetic data test validates the model's effectiveness in estimating path\nchoice parameters, which can outperform the purely simulation-based\noptimization methods in both accuracy and efficiency. The test results using\nactual data show that the estimated path shares are more reasonable than\nsurvey-derived path shares and uniform path shares. Model robustness in terms\nof different initial values and different case study dates are also verified.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 19:32:59 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 17:54:54 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Mo", "Baichuan", ""], ["Ma", "Zhenliang", ""], ["Koutsopoulos", "Haris N.", ""], ["Zhao", "Jinhua", ""]]}, {"id": "2001.03741", "submitter": "Jean Claude Bajard", "authors": "Jean Claude Bajard and J\\'er\\'emy Marrez and Thomas Plantard and\n  Pascal V\\'eron", "title": "On Polynomial Modular Number Systems over $\\mathbb{Z}/p\\mathbb{Z}$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since their introduction in 2004, Polynomial Modular Number Systems (PMNS)\nhave become a very interesting tool for implementing cryptosystems relying on\nmodular arithmetic in a secure and efficient way. However, while their\nimplementation is simple, their parameterization is not trivial and relies on a\nsuitable choice of the polynomial on which the PMNS operates. The initial\nproposals were based on particular binomials and trinomials. But these\npolynomials do not always provide systems with interesting characteristics such\nas small digits, fast reduction, etc.\n  In this work, we study a larger family of polynomials that can be exploited\nto design a safe and efficient PMNS. To do so, we first state a complete\nexistence theorem for PMNS which provides bounds on the size of the digits for\na generic polynomial, significantly improving previous bounds. Then, we present\nclasses of suitable polynomials which provide numerous PMNS for safe and\nefficient arithmetic.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 11:45:12 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 07:48:34 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Bajard", "Jean Claude", ""], ["Marrez", "J\u00e9r\u00e9my", ""], ["Plantard", "Thomas", ""], ["V\u00e9ron", "Pascal", ""]]}, {"id": "2001.03743", "submitter": "Viet Vo", "authors": "Viet Vo, Shangqi Lai, Xingliang Yuan, Shi-Feng Sun, Surya Nepal, and\n  Joseph K. Liu", "title": "Accelerating Forward and Backward Private Searchable Encryption Using\n  Trusted Execution", "comments": "SGX-based dynamic SSE protocol with Forward and Backward Privacy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searchable encryption (SE) is one of the key enablers for building encrypted\ndatabases. It allows a cloud server to search over encrypted data without\ndecryption. Dynamic SE additionally includes data addition and deletion\noperations to enrich the functions of encrypted databases. Recent attacks\nexploiting the leakage in dynamic operations drive rapid development of new SE\nschemes revealing less information while performing updates; they are also\nknown as forward and backward private SE. Newly added data is no longer\nlinkable to queries issued before, and deleted data is no longer searchable in\nqueries issued later. However, those advanced SE schemes reduce the efficiency\nof SE, especially in the communication cost between the client and server. In\nthis paper, we resort to the hardware-assisted solution, aka Intel SGX, to ease\nthe above bottleneck. Our key idea is to leverage SGX to take over the most\ntasks of the client, i.e., tracking keyword states along with data addition and\ncaching deleted data. However, handling large datasets is non-trivial due to\nthe I/O and memory constraints of the SGX enclave. We further develop batch\ndata processing and state compression technique to reduce the communication\noverhead between the SGX and untrusted server, and minimise the memory\nfootprint in the enclave. We conduct a comprehensive set of evaluations on both\nsynthetic and real-world datasets, which confirm that our designs outperform\nthe prior art.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 12:16:53 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 00:06:46 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Vo", "Viet", ""], ["Lai", "Shangqi", ""], ["Yuan", "Xingliang", ""], ["Sun", "Shi-Feng", ""], ["Nepal", "Surya", ""], ["Liu", "Joseph K.", ""]]}, {"id": "2001.03788", "submitter": "R Jaberi", "authors": "Raed Jaberi", "title": "Minimum $2$-vertex-twinless connected spanning subgraph problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a $2$-vertex-twinless connected directed graph $G=(V,E)$, the minimum\n$2$-vertex-twinless connected spanning subgraph problem is to find a minimum\ncardinality edge subset $E^{t} \\subseteq E$ such that the subgraph $(V,E^{t})$\nis $2$-vertex-twinless connected. Let $G^{1}$ be a minimal $2$-vertex-connected\nsubgraph of $G$. In this paper we present a $(2+a_{t}/2)$-approximation\nalgorithm for the minimum $2$-vertex-twinless connected spanning subgraph\nproblem, where $a_{t}$ is the number of twinless articulation points in\n$G^{1}$.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 19:35:08 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Jaberi", "Raed", ""]]}, {"id": "2001.03794", "submitter": "\\'Edouard Bonnet", "authors": "Pierre Aboulker, \\'Edouard Bonnet, Eun Jung Kim, and Florian Sikora", "title": "Grundy Coloring & friends, Half-Graphs, Bicliques", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first-fit coloring is a heuristic that assigns to each vertex, arriving\nin a specified order $\\sigma$, the smallest available color. The problem Grundy\nColoring asks how many colors are needed for the most adversarial vertex\nordering $\\sigma$, i.e., the maximum number of colors that the first-fit\ncoloring requires over all possible vertex orderings. Since its inception by\nGrundy in 1939, Grundy Coloring has been examined for its structural and\nalgorithmic aspects. A brute-force $f(k)n^{2^{k-1}}$-time algorithm for Grundy\nColoring on general graphs is not difficult to obtain, where $k$ is the number\nof colors required by the most adversarial vertex ordering. It was asked\nseveral times whether the dependency on $k$ in the exponent of $n$ can be\navoided or reduced, and its answer seemed elusive until now. We prove that\nGrundy Coloring is W[1]-hard and the brute-force algorithm is essentially\noptimal under the Exponential Time Hypothesis, thus settling this question by\nthe negative.\n  The key ingredient in our W[1]-hardness proof is to use so-called half-graphs\nas a building block to transmit a color from one vertex to another. Leveraging\nthe half-graphs, we also prove that b-Chromatic Core is W[1]-hard, whose\nparameterized complexity was posed as an open question by Panolan et al. [JCSS\n'17]. A natural follow-up question is, how the parameterized complexity changes\nin the absence of (large) half-graphs. We establish fixed-parameter\ntractability on $K_{t,t}$-free graphs for b-Chromatic Core and Partial Grundy\nColoring, making a step toward answering this question. The key combinatorial\nlemma underlying the tractability result might be of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 20:17:11 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Aboulker", "Pierre", ""], ["Bonnet", "\u00c9douard", ""], ["Kim", "Eun Jung", ""], ["Sikora", "Florian", ""]]}, {"id": "2001.04020", "submitter": "Liang Huang", "authors": "Liang Huang, He Zhang, Dezhong Deng, Kai Zhao, Kaibo Liu, David A.\n  Hendrix, David H. Mathews", "title": "LinearFold: linear-time approximate RNA folding by 5'-to-3' dynamic\n  programming and beam search", "comments": "10 pages main text (8 figures); 5 pages supplementary information (7\n  figures). In Proceedings of ISMB 2019", "journal-ref": "Bioinformatics, Volume 35, Issue 14, July 2019, Pages i295--i304", "doi": "10.1093/bioinformatics/btz375", "report-no": null, "categories": "q-bio.BM cs.DS math.CO physics.bio-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Predicting the secondary structure of an RNA sequence is useful\nin many applications. Existing algorithms (based on dynamic programming) suffer\nfrom a major limitation: their runtimes scale cubically with the RNA length,\nand this slowness limits their use in genome-wide applications.\n  Results: We present a novel alternative $O(n^3)$-time dynamic programming\nalgorithm for RNA folding that is amenable to heuristics that make it run in\n$O(n)$ time and $O(n)$ space, while producing a high-quality approximation to\nthe optimal solution. Inspired by incremental parsing for context-free grammars\nin computational linguistics, our alternative dynamic programming algorithm\nscans the sequence in a left-to-right (5'-to-3') direction rather than in a\nbottom-up fashion, which allows us to employ the effective beam pruning\nheuristic. Our work, though inexact, is the first RNA folding algorithm to\nachieve linear runtime (and linear space) without imposing constraints on the\noutput structure. Surprisingly, our approximate search results in even higher\noverall accuracy on a diverse database of sequences with known structures. More\ninterestingly, it leads to significantly more accurate predictions on the\nlongest sequence families in that database (16S and 23S Ribosomal RNAs), as\nwell as improved accuracies for long-range base pairs (500+ nucleotides apart),\nboth of which are well known to be challenging for the current models.\n  Availability: Our source code is available at\nhttps://github.com/LinearFold/LinearFold, and our webserver is at\nhttp://linearfold.org (sequence limit: 100,000nt).\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 00:03:23 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Huang", "Liang", ""], ["Zhang", "He", ""], ["Deng", "Dezhong", ""], ["Zhao", "Kai", ""], ["Liu", "Kaibo", ""], ["Hendrix", "David A.", ""], ["Mathews", "David H.", ""]]}, {"id": "2001.04191", "submitter": "Johannes Klaus Fichte", "authors": "Johannes K. Fichte, Markus Hecher, Patrick Thier, Stefan Woltran", "title": "Exploiting Database Management Systems and Treewidth for Counting", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": "10.1017/S147106842100003X", "report-no": null, "categories": "cs.AI cs.DS math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bounded treewidth is one of the most cited combinatorial invariants, which\nwas applied in the literature for solving several counting problems\nefficiently. A canonical counting problem is #SAT, which asks to count the\nsatisfying assignments of a Boolean formula. Recent work shows that\nbenchmarking instances for #SAT often have reasonably small treewidth. This\npaper deals with counting problems for instances of small treewidth. We\nintroduce a general framework to solve counting questions based on\nstate-of-the-art database management systems (DBMS). Our framework takes\nexplicitly advantage of small treewidth by solving instances using dynamic\nprogramming (DP) on tree decompositions (TD). Therefore, we implement the\nconcept of DP into a DBMS (PostgreSQL), since DP algorithms are already often\ngiven in terms of table manipulations in theory. This allows for elegant\nspecifications of DP algorithms and the use of SQL to manipulate records and\ntables, which gives us a natural approach to bring DP algorithms into practice.\nTo the best of our knowledge, we present the first approach to employ a DBMS\nfor algorithms on TDs. A key advantage of our approach is that DBMS naturally\nallow to deal with huge tables with a limited amount of main memory (RAM),\nparallelization, as well as suspending computation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 12:45:22 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 16:54:41 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Fichte", "Johannes K.", ""], ["Hecher", "Markus", ""], ["Thier", "Patrick", ""], ["Woltran", "Stefan", ""]]}, {"id": "2001.04219", "submitter": "Markus Hecher", "authors": "Markus Hecher, Michael Morak, Stefan Woltran", "title": "Structural Decompositions of Epistemic Logic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.CL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epistemic logic programs (ELPs) are a popular generalization of standard\nAnswer Set Programming (ASP) providing means for reasoning over answer sets\nwithin the language. This richer formalism comes at the price of higher\ncomputational complexity reaching up to the fourth level of the polynomial\nhierarchy. However, in contrast to standard ASP, dedicated investigations\ntowards tractability have not been undertaken yet. In this paper, we give first\nresults in this direction and show that central ELP problems can be solved in\nlinear time for ELPs exhibiting structural properties in terms of bounded\ntreewidth. We also provide a full dynamic programming algorithm that adheres to\nthese bounds. Finally, we show that applying treewidth to a novel dependency\nstructure---given in terms of epistemic literals---allows to bound the number\nof ASP solver calls in typical ELP solving procedures.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 13:16:13 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Hecher", "Markus", ""], ["Morak", "Michael", ""], ["Woltran", "Stefan", ""]]}, {"id": "2001.04333", "submitter": "Marcin Jurdzi\\'nski", "authors": "Marcin Jurdzi\\'nski, R\\'emi Morvan", "title": "A Universal Attractor Decomposition Algorithm for Parity Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An attractor decomposition meta-algorithm for solving parity games is given\nthat generalizes the classic McNaughton-Zielonka algorithm and its recent\nquasi-polynomial variants due to Parys (2019), and to Lehtinen, Schewe, and\nWojtczak (2019). The central concepts studied and exploited are attractor\ndecompositions of dominia in parity games and the ordered trees that describe\nthe inductive structure of attractor decompositions.\n  The main technical results include the embeddable decomposition theorem and\nthe dominion separation theorem that together help establish a precise\nstructural condition for the correctness of the universal algorithm: it\nsuffices that the two ordered trees given to the algorithm as inputs embed the\ntrees of some attractor decompositions of the largest dominia for each of the\ntwo players, respectively.\n  The universal algorithm yields McNaughton-Zielonka, Parys's, and\nLehtinen-Schewe-Wojtczak algorithms as special cases when suitable universal\ntrees are given to it as inputs. The main technical results provide a unified\nproof of correctness and deep structural insights into those algorithms.\n  A symbolic implementation of the universal algorithm is also given that\nimproves the symbolic space complexity of solving parity games in\nquasi-polynomial time from $O(d \\lg n)$---achieved by Chatterjee,\nDvo\\v{r}\\'{a}k, Henzinger, and Svozil (2018)---down to $O(\\lg d)$, where $n$ is\nthe number of vertices and $d$ is the number of distinct priorities in a parity\ngame. This not only exponentially improves the dependence on $d$, but it also\nentirely removes the dependence on $n$.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 15:19:05 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Jurdzi\u0144ski", "Marcin", ""], ["Morvan", "R\u00e9mi", ""]]}, {"id": "2001.04413", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Yuanzhi Li", "title": "Backward Feature Correction: How Deep Learning Performs Deep Learning", "comments": "V2 adds more experiments, V3 polishes writing and improves\n  experiments, V4 makes minor fixes to the figures, V5 polishes writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does a 110-layer ResNet learn a high-complexity classifier using\nrelatively few training examples and short training time? We present a theory\ntowards explaining this in terms of Hierarchical Learning. We refer\nhierarchical learning as the learner learns to represent a complicated target\nfunction by decomposing it into a sequence of simpler functions to reduce\nsample and time complexity. We formally analyze how multi-layer neural networks\ncan perform such hierarchical learning efficiently and automatically by\napplying SGD.\n  On the conceptual side, we present, to the best of our knowledge, the FIRST\ntheory result indicating how deep neural networks can still be sample and time\nefficient using SGD on certain hierarchical learning tasks, when NO KNOWN\nexisting algorithm is efficient. We establish a new principle called \"backward\nfeature correction\", where training higher-level layers in the network can\nimprove the features of lower-level ones. We believe this is the key to\nunderstand the deep learning process in multi-layer neural networks.\n  On the technical side, we show for regression and even binary classification,\nfor every input dimension $d>0$, there is a concept class of degree $\\omega(1)$\npolynomials so that, using $\\omega(1)$-layer neural networks as learners, SGD\ncan learn any function from this class in $\\mathsf{poly}(d)$ time and sample\ncomplexity to any $\\frac{1}{\\mathsf{poly}(d)}$ error, through learning to\nrepresent it as a composition of $\\omega(1)$ layers of quadratic functions. In\ncontrast, we do not know any other simple algorithm (including layer-wise\ntraining or applying kernel method sequentially) that can learn this concept\nclass in $\\mathsf{poly}(d)$ time even to any $d^{-0.01}$ error. As a side\nresult, we prove $d^{\\omega(1)}$ lower bounds for several non-hierarchical\nlearners, including any kernel methods, neural tangent or neural compositional\nkernels.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 17:28:29 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 17:47:15 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 03:28:52 GMT"}, {"version": "v4", "created": "Thu, 10 Sep 2020 17:48:37 GMT"}, {"version": "v5", "created": "Sat, 13 Mar 2021 12:05:09 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "2001.04447", "submitter": "Arnold Filtser", "authors": "Arnold Filtser", "title": "Scattering and Sparse Partitions, and their Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A partition $\\mathcal{P}$ of a weighted graph $G$ is\n$(\\sigma,\\tau,\\Delta)$-sparse if every cluster has diameter at most $\\Delta$,\nand every ball of radius $\\Delta/\\sigma$ intersects at most $\\tau$ clusters.\nSimilarly, $\\mathcal{P}$ is $(\\sigma,\\tau,\\Delta)$-scattering if instead for\nballs we require that every shortest path of length at most $\\Delta/\\sigma$\nintersects at most $\\tau$ clusters. Given a graph $G$ that admits a\n$(\\sigma,\\tau,\\Delta)$-sparse partition for all $\\Delta>0$, Jia et al. [STOC05]\nconstructed a solution for the Universal Steiner Tree problem (and also\nUniversal TSP) with stretch $O(\\tau\\sigma^2\\log_\\tau n)$. Given a graph $G$\nthat admits a $(\\sigma,\\tau,\\Delta)$-scattering partition for all $\\Delta>0$,\nwe construct a solution for the Steiner Point Removal problem with stretch\n$O(\\tau^3\\sigma^3)$. We then construct sparse and scattering partitions for\nvarious different graph families, receiving many new results for the Universal\nSteiner Tree and Steiner Point Removal problems.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 18:34:44 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 21:59:07 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Filtser", "Arnold", ""]]}, {"id": "2001.04505", "submitter": "W B Langdon", "authors": "William B. Langdon", "title": "Fast Generation of Big Random Binary Trees", "comments": "C++ code:\n  http://www.cs.ucl.ac.uk/staff/W.Langdon/ftp/gp-code/rand_tree.cc_r1.43", "journal-ref": null, "doi": null, "report-no": "RN/20/01 (Revision 1.1)", "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  random_tree() is a linear time and space C++ implementation able to create\ntrees of up to a billion nodes for genetic programming and genetic improvement\nexperiments. A 3.60GHz CPU can generate more than 18 million random nodes for\nGP program trees per second.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 19:20:14 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Langdon", "William B.", ""]]}, {"id": "2001.04525", "submitter": "Anisur Molla Rahaman", "authors": "Subhrangsu Mandal, Anisur Rahaman Molla, and William K. Moses Jr", "title": "Live Exploration with Mobile Robots in a Dynamic Ring, Revisited", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph exploration problem requires a group of mobile robots, initially\nplaced arbitrarily on the nodes of a graph, to work collaboratively to explore\nthe graph such that each node is eventually visited by at least one robot. One\nimportant requirement of exploration is the {\\em termination} condition, i.e.,\nthe robots must know that exploration is completed. The problem of live\nexploration of a dynamic ring using mobile robots was recently introduced in\n[Di Luna et al., ICDCS 2016]. In it, they proposed multiple algorithms to solve\nexploration in fully synchronous and semi-synchronous settings with various\nguarantees when $2$ robots were involved. They also provided guarantees that\nwith certain assumptions, exploration of the ring using two robots was\nimpossible. An important question left open was how the presence of $3$ robots\nwould affect the results. In this paper, we try to settle this question in a\nfully synchronous setting and also show how to extend our results to a\nsemi-synchronous setting.\n  In particular, we present algorithms for exploration with explicit\ntermination using $3$ robots in conjunction with either (i) unique IDs of the\nrobots and edge crossing detection capability (i.e., two robots moving in\nopposite directions through an edge in the same round can detect each other),\nor (ii) access to randomness. The time complexity of our deterministic\nalgorithm is asymptotically optimal. We also provide complementary\nimpossibility results showing that there does not exist any explicit\ntermination algorithm for $2$ robots. The theoretical analysis and\ncomprehensive simulations of our algorithm show the effectiveness and\nefficiency of the algorithm in dynamic rings. We also present an algorithm to\nachieve exploration with partial termination using $3$ robots in the\nsemi-synchronous setting.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 20:29:20 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Mandal", "Subhrangsu", ""], ["Molla", "Anisur Rahaman", ""], ["Moses", "William K.", "Jr"]]}, {"id": "2001.04555", "submitter": "Feras Saad", "authors": "Feras A. Saad, Cameron E. Freer, Martin C. Rinard, Vikash K.\n  Mansinghka", "title": "Optimal Approximate Sampling from Discrete Probability Distributions", "comments": null, "journal-ref": "Proc. ACM Program. Lang. 4, POPL, Article 36 (January 2020)", "doi": "10.1145/3371104", "report-no": null, "categories": "cs.DS cs.DM cs.IT math.IT math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a fundamental problem in random variate generation:\ngiven access to a random source that emits a stream of independent fair bits,\nwhat is the most accurate and entropy-efficient algorithm for sampling from a\ndiscrete probability distribution $(p_1, \\dots, p_n)$, where the probabilities\nof the output distribution $(\\hat{p}_1, \\dots, \\hat{p}_n)$ of the sampling\nalgorithm must be specified using at most $k$ bits of precision? We present a\ntheoretical framework for formulating this problem and provide new techniques\nfor finding sampling algorithms that are optimal both statistically (in the\nsense of sampling accuracy) and information-theoretically (in the sense of\nentropy consumption). We leverage these results to build a system that, for a\nbroad family of measures of statistical accuracy, delivers a sampling algorithm\nwhose expected entropy usage is minimal among those that induce the same\ndistribution (i.e., is \"entropy-optimal\") and whose output distribution\n$(\\hat{p}_1, \\dots, \\hat{p}_n)$ is a closest approximation to the target\ndistribution $(p_1, \\dots, p_n)$ among all entropy-optimal sampling algorithms\nthat operate within the specified $k$-bit precision. This optimal approximate\nsampler is also a closer approximation than any (possibly entropy-suboptimal)\nsampler that consumes a bounded amount of entropy with the specified precision,\na class which includes floating-point implementations of inversion sampling and\nrelated methods found in many software libraries. We evaluate the accuracy,\nentropy consumption, precision requirements, and wall-clock runtime of our\noptimal approximate sampling algorithms on a broad set of distributions,\ndemonstrating the ways that they are superior to existing approximate samplers\nand establishing that they often consume significantly fewer resources than are\nneeded by exact samplers.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 22:48:07 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Saad", "Feras A.", ""], ["Freer", "Cameron E.", ""], ["Rinard", "Martin C.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "2001.04698", "submitter": "Rakesh Mohanty", "authors": "Debasis Dwibedy, Rakesh Mohanty", "title": "Online Scheduling with Makespan Minimization: State of the Art Results,\n  Research Challenges and Open Problems", "comments": "37 pages, 13 Tables, Submitted to Computer Science Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online scheduling has been a well studied and challenging research problem\nover the last five decades since the pioneering work of Graham with immense\npractical significance in various applications such as interactive parallel\nprocessing, routing in communication networks, distributed data management,\nclient-server communications, traffic management in transportation, industrial\nmanufacturing and production. In this problem, a sequence of jobs is received\none by one in order by the scheduler for scheduling over a number of machines.\nOn arrival of a job, the scheduler assigns the job irrevocably to a machine\nbefore the availability of the next job with an objective to minimize the\ncompletion time of the scheduled jobs. This paper highlights the state of the\nart contributions for online scheduling of a sequence of independent jobs on\nidentical and uniform related machines with a special focus on preemptive and\nnon-preemptive processing formats by considering makespan minimization as the\noptimality criterion. We present the fundamental aspects of online scheduling\nfrom a beginner's perspective along with a background of general scheduling\nframework. Important competitive analysis results obtained by well-known\ndeterministic and randomized online scheduling algorithms in the literature are\npresented along with research challenges and open problems. Two of the emerging\nrecent trends such as resource augmentation and semi-online scheduling are\ndiscussed as a motivation for future research work.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 10:19:15 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Dwibedy", "Debasis", ""], ["Mohanty", "Rakesh", ""]]}, {"id": "2001.04760", "submitter": "Rita Hartel", "authors": "Stefan B\\\"ottcher, Rita Hartel and Sven Peeters", "title": "Simulation computation in grammar-compressed graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like [1], we present an algorithm to compute the simulation of a query\npattern in a graph of labeled nodes and unlabeled edges. However, our algorithm\nworks on a compressed graph grammar, instead of on the original graph. The\nspeed-up of our algorithm compared to the algorithm in [1] grows with the size\nof the graph and with the compression strength.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 13:19:41 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["B\u00f6ttcher", "Stefan", ""], ["Hartel", "Rita", ""], ["Peeters", "Sven", ""]]}, {"id": "2001.04908", "submitter": "Florian Nelles", "authors": "Stefan Kratsch and Florian Nelles", "title": "Efficient parameterized algorithms for computing all-pairs shortest\n  paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing all-pairs shortest paths is a fundamental and much-studied problem\nwith many applications. Unfortunately, despite intense study, there are still\nno significantly faster algorithms for it than the $\\mathcal{O}(n^3)$ time\nalgorithm due to Floyd and Warshall (1962). Somewhat faster algorithms exist\nfor the vertex-weighted version if fast matrix multiplication may be used.\nYuster (SODA 2009) gave an algorithm running in time $\\mathcal{O}(n^{2.842})$,\nbut no combinatorial, truly subcubic algorithm is known.\n  Motivated by the recent framework of efficient parameterized algorithms (or\n\"FPT in P\"), we investigate the influence of the graph parameters clique-width\n($cw$) and modular-width ($mw$) on the running times of algorithms for solving\nAll-Pairs Shortest Paths. We obtain efficient (and combinatorial) parameterized\nalgorithms on non-negative vertex-weighted graphs of times\n$\\mathcal{O}(cw^2n^2)$, resp. $\\mathcal{O}(mw^2n + n^2)$. If fast matrix\nmultiplication is allowed then the latter can be improved to\n$\\mathcal{O}(mw^{1.842}n + n^2)$ using the algorithm of Yuster as a black box.\nThe algorithm relative to modular-width is adaptive, meaning that the running\ntime matches the best unparameterized algorithm for parameter value $mw$ equal\nto $n$, and they outperform them already for $mw \\in \\mathcal{O}(n^{1 -\n\\varepsilon})$ for any $\\varepsilon > 0$.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 17:05:12 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Kratsch", "Stefan", ""], ["Nelles", "Florian", ""]]}, {"id": "2001.05015", "submitter": "Sungjin Im", "authors": "Sungjin Im and Maryam Shadloo", "title": "Weighted Completion Time Minimization for Unrelated Machines via\n  Iterative Fair Contention Resolution", "comments": "Appeared in SODA 2020. This is a full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a 1.488-approximation for the classic scheduling problem of\nminimizing total weighted completion time on unrelated machines. This is a\nconsiderable improvement on the recent breakthrough of $(1.5 -\n10^{-7})$-approximation (STOC 2016, Bansal-Srinivasan-Svensson) and the\nfollow-up result of $(1.5 - 1/6000)$-approximation (FOCS 2017, Li). Bansal et\nal. introduced a novel rounding scheme yielding strong negative correlations\nfor the first time and applied it to the scheduling problem to obtain their\nbreakthrough, which resolved the open problem if one can beat out the\nlong-standing $1.5$-approximation barrier based on independent rounding. Our\nkey technical contribution is in achieving significantly stronger negative\ncorrelations via iterative fair contention resolution, which is of independent\ninterest. Previously, Bansal et al. obtained strong negative correlations via a\nvariant of pipage type rounding and Li used it as a black box.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 19:35:01 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Im", "Sungjin", ""], ["Shadloo", "Maryam", ""]]}, {"id": "2001.05053", "submitter": "Kevin Yeo", "authors": "Giuseppe Persiano and Kevin Yeo", "title": "Tight Static Lower Bounds for Non-Adaptive Data Structures", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we study the static cell probe complexity of non-adaptive data\nstructures that maintain a subset of $n$ points from a universe consisting of\n$m=n^{1+\\Omega(1)}$ points. A data structure is defined to be non-adaptive when\nthe memory locations that are chosen to be accessed during a query depend only\non the query inputs and not on the contents of memory. We prove an $\\Omega(\\log\nm / \\log (sw/n\\log m))$ static cell probe complexity lower bound for\nnon-adaptive data structures that solve the fundamental dictionary problem\nwhere $s$ denotes the space of the data structure in the number of cells and\n$w$ is the cell size in bits. Our lower bounds hold for all word sizes\nincluding the bit probe model ($w = 1$) and are matched by the upper bounds of\nBoninger et al. [FSTTCS'17].\n  Our results imply a sharp dichotomy between dictionary data structures with\none round of adaptive and at least two rounds of adaptivity. We show that\n$O(1)$, or $O(\\log^{1-\\epsilon}(m))$, overhead dictionary constructions are\nonly achievable with at least two rounds of adaptivity. In particular, we show\nthat many $O(1)$ dictionary constructions with two rounds of adaptivity such as\ncuckoo hashing are optimal in terms of adaptivity. On the other hand,\nnon-adaptive dictionaries must use significantly more overhead.\n  Finally, our results also imply static lower bounds for the non-adaptive\npredecessor problem. Our static lower bounds peak higher than the previous,\nbest known lower bounds of $\\Omega(\\log m / \\log w)$ for the dynamic\npredecessor problem by Boninger et al. [FSTTCS'17] and Ramamoorthy and Rao\n[CCC'18] in the natural setting of linear space $s = \\Theta(n)$ where each\npoint can fit in a single cell $w = \\Theta(\\log m)$. Furthermore, our results\nare stronger as they apply to the static setting unlike the previous lower\nbounds that only applied in the dynamic setting.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 21:26:17 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 21:12:55 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Persiano", "Giuseppe", ""], ["Yeo", "Kevin", ""]]}, {"id": "2001.05157", "submitter": "Runyu Zhang", "authors": "Jiaqi Dong, Runyu Zhang, Chaoshu Yang, Yujuan Tan, and Duo Liu", "title": "An Efficient and Wear-Leveling-Aware Frequent-Pattern Mining on\n  Non-Volatile Memory", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/1570/1/012087", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent-pattern mining is a common approach to reveal the valuable hidden\ntrends behind data. However, existing frequent-pattern mining algorithms are\ndesigned for DRAM, instead of persistent memories (PMs), which can lead to\nsevere performance and energy overhead due to the utterly different\ncharacteristics between DRAM and PMs when they are running on PMs. In this\npaper, we propose an efficient and Wear-leveling-aware Frequent-Pattern Mining\nscheme, WFPM, to solve this problem. The proposed WFPM is evaluated by a series\nof experiments based on realistic datasets from diversified application\nscenarios, where WFPM achieves 32.0% performance improvement and prolongs the\nNVM lifetime of header table by 7.4x over the EvFP-Tree.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 07:21:11 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Dong", "Jiaqi", ""], ["Zhang", "Runyu", ""], ["Yang", "Chaoshu", ""], ["Tan", "Yujuan", ""], ["Liu", "Duo", ""]]}, {"id": "2001.05236", "submitter": "Felix Reidl", "authors": "Felix Reidl, Blair D. Sullivan", "title": "A color-avoiding approach to subgraph counting in bounded expansion\n  classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an algorithm to count the number of occurrences of a pattern graph\n$H$ as an induced subgraph in a host graph $G$. If $G$ belongs to a bounded\nexpansion class, the algorithm runs in linear time. Our design choices are\nmotivated by the need for an approach that can be engineered into a practical\nimplementation for sparse host graphs.\n  Specifically, we introduce a decomposition of the pattern $H$ called a\ncounting dag $\\vec C(H)$ which encodes an order-aware, inclusion-exclusion\ncounting method for $H$. Given such a counting dag and a suitable linear\nordering $\\mathbb G$ of $G$ as input, our algorithm can count the number of\ntimes $H$ appears as an induced subgraph in $G$ in time $O(\\|\\vec C\\| \\cdot h\n\\text{wcol}_{h}(\\mathbb G)^{h-1} |G|)$, where $\\text{wcol}_h(\\mathbb G)$\ndenotes the maximum size of the weakly $h$-reachable sets in $\\mathbb G$. This\nimplies, combined with previous results, an algorithm with running time\n$O(4^{h^2}h (\\text{wcol}_h(G)+1)^{h^3} |G|)$ which only takes $H$ and $G$ as\ninput.\n  We note that with a small modification, our algorithm can instead use\nstrongly $h$-reachable sets with running time $O(\\|\\vec C\\| \\cdot h\n\\text{col}_{h}(\\mathbb G)^{h-1} |G|)$, resulting in an overall complexity of\n$O(4^{h^2}h \\text{col}_h(G)^{h^2} |G|)$ when only given $H$ and $G$.\n  Because orderings with small weakly/strongly reachable sets can be computed\nrelatively efficiently in practice [11], our algorithm provides a promising\nalternative to algorithms using the traditional $p$-treedepth colouring\nframework [13]. We describe preliminary experimental results from an initial\nopen source implementation which highlight its potential.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 11:10:09 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Reidl", "Felix", ""], ["Sullivan", "Blair D.", ""]]}, {"id": "2001.05239", "submitter": "Dmitry Kosolobov", "authors": "Dmitry Kosolobov and Oleg Merkurev", "title": "Optimal Skeleton Huffman Trees Revisited", "comments": "12 pages, 3 figures, accepted to CSR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A skeleton Huffman tree is a Huffman tree in which all disjoint maximal\nperfect subtrees are shrunk into leaves. Skeleton Huffman trees, besides saving\nstorage space, are also used for faster decoding and for speeding up\nHuffman-shaped wavelet trees. In 2017 Klein et al. introduced an optimal\nskeleton tree: for given symbol frequencies, it has the least number of nodes\namong all optimal prefix-free code trees (not necessarily Huffman's) with\nshrunk perfect subtrees. Klein et al. described a simple algorithm that, for\nfixed codeword lengths, finds a skeleton tree with the least number of nodes;\nwith this algorithm one can process each set of optimal codeword lengths to\nfind an optimal skeleton tree. However, there are exponentially many such sets\nin the worst case. We describe an $O(n^2\\log n)$-time algorithm that, given $n$\nsymbol frequencies, constructs an optimal skeleton tree and its corresponding\noptimal code.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 11:13:17 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 15:28:11 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Kosolobov", "Dmitry", ""], ["Merkurev", "Oleg", ""]]}, {"id": "2001.05304", "submitter": "Sebastiano Vigna", "authors": "Guy Steele and Sebastiano Vigna", "title": "Computationally easy, spectrally good multipliers for congruential\n  pseudorandom number generators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Congruential pseudorandom number generators rely on good multipliers, that\nis, integers that have good performance with respect to the spectral test. We\nprovide lists of multipliers with a good lattice structure up to dimension\neight and up to lag eight for generators with typical power-of-two moduli,\nanalyzing in detail multipliers close to the square root of the modulus, whose\nproduct can be computed quickly.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 13:23:36 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 10:47:59 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Steele", "Guy", ""], ["Vigna", "Sebastiano", ""]]}, {"id": "2001.05323", "submitter": "Will Perkins", "authors": "Tyler Helmuth, Will Perkins, Samantha Petti", "title": "Correlation decay for hard spheres via Markov chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve upon all known lower bounds on the critical fugacity and critical\ndensity of the hard sphere model in dimensions two and higher. As the dimension\ntends to infinity our improvements are by factors of $2$ and $1.7$,\nrespectively. We make these improvements by utilizing techniques from\ntheoretical computer science to show that a certain Markov chain for sampling\nfrom the hard sphere model mixes rapidly at low enough fugacities. We then\nprove an equivalence between optimal spatial and temporal mixing for hard\nspheres to deduce our results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 13:42:00 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 17:58:03 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Helmuth", "Tyler", ""], ["Perkins", "Will", ""], ["Petti", "Samantha", ""]]}, {"id": "2001.05364", "submitter": "Falko Hegerfeld", "authors": "Falko Hegerfeld and Stefan Kratsch", "title": "Solving connectivity problems parameterized by treedepth in\n  single-exponential time and polynomial space", "comments": "27 pages; accepted at STACS 2020", "journal-ref": null, "doi": "10.4230/LIPIcs.STACS.2020.29", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A breakthrough result of Cygan et al. (FOCS 2011) showed that connectivity\nproblems parameterized by treewidth can be solved much faster than the\npreviously best known time $\\mathcal{O}^*(2^{\\mathcal{O}(tw \\log(tw))})$. Using\ntheir inspired Cut\\&Count technique, they obtained $\\mathcal{O}^*(\\alpha^{tw})$\ntime algorithms for many such problems. Moreover, they proved these running\ntimes to be optimal assuming the Strong Exponential-Time Hypothesis.\nUnfortunately, like other dynamic programming algorithms on tree\ndecompositions, these algorithms also require exponential space, and this is\nwidely believed to be unavoidable. In contrast, for the slightly larger\nparameter called treedepth, there are already several examples of algorithms\nmatching the time bounds obtained for treewidth, but using only polynomial\nspace. Nevertheless, this has remained open for connectivity problems.\n  In the present work, we close this knowledge gap by applying the Cut\\&Count\ntechnique to graphs of small treedepth. While the general idea is unchanged, we\nhave to design novel procedures for counting consistently cut solution\ncandidates using only polynomial space. Concretely, we obtain time\n$\\mathcal{O}^*(3^d)$ and polynomial space for Connected Vertex Cover, Feedback\nVertex Set, and Steiner Tree on graphs of treedepth $d$. Similarly, we obtain\ntime $\\mathcal{O}^*(4^d)$ and polynomial space for Connected Dominating Set and\nConnected Odd Cycle Transversal.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 15:07:24 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 12:11:36 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Hegerfeld", "Falko", ""], ["Kratsch", "Stefan", ""]]}, {"id": "2001.05451", "submitter": "Yujie Wang", "authors": "Yujie Wang", "title": "Improvement of an Approximated Self-Improving Sorter and Error Analysis\n  of its Estimated Entropy", "comments": "I found there is a critical error in this submission, therefore, I\n  withdraw this draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The self-improving sorter proposed by Ailon et al. consists of two phases: a\nrelatively long training phase and rapid operation phase. In this study, we\nhave developed an efficient way to further improve this sorter by approximating\nits training phase to be faster but not sacrificing much performance in the\noperation phase. It is very necessary to ensure the accuracy of the estimated\nentropy when we test the performance of this approximated sorter. Thus we\nfurther developed a useful formula to calculate an upper bound for the 'error'\nof the estimated entropy derived from the input data with unknown\ndistributions. Our work will contribute to the better use of this\nself-improving sorter for huge data in a quicker way.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 17:49:28 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 13:18:01 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Wang", "Yujie", ""]]}, {"id": "2001.05671", "submitter": "Yuto Nakashima", "authors": "Kohei Yamada, Yuto Nakashima, Shunsuke Inenaga, Hideo Bannai, and\n  Masayuki Takeda", "title": "Faster STR-EC-LCS Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The longest common subsequence (LCS) problem is a central problem in\nstringology that finds the longest common subsequence of given two strings $A$\nand $B$. More recently, a set of four constrained LCS problems (called\ngeneralized constrained LCS problem) were proposed by Chen and Chao [J. Comb.\nOptim, 2011]. In this paper, we consider the substring-excluding constrained\nLCS (STR-EC-LCS) problem. A string $Z$ is said to be an STR-EC-LCS of two given\nstrings $A$ and $B$ excluding $P$ if, $Z$ is one of the longest common\nsubsequences of $A$ and $B$ that does not contain $P$ as a substring. Wang et\nal. proposed a dynamic programming solution which computes an STR-EC-LCS in\n$O(mnr)$ time and space where $m = |A|, n = |B|, r = |P|$ [Inf. Process. Lett.,\n2013]. In this paper, we show a new solution for the STR-EC-LCS problem. Our\nalgorithm computes an STR-EC-LCS in $O(n|\\Sigma| + (L+1)(m-L+1)r)$ time where\n$|\\Sigma| \\leq \\min\\{m, n\\}$ denotes the set of distinct characters occurring\nin both $A$ and $B$, and $L$ is the length of the STR-EC-LCS. This algorithm is\nfaster than the $O(mnr)$-time algorithm for short/long STR-EC-LCS (namely, $L\n\\in O(1)$ or $m-L \\in O(1)$), and is at least as efficient as the $O(mnr)$-time\nalgorithm for all cases.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 06:30:29 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Yamada", "Kohei", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "2001.05921", "submitter": "Marc Hellmuth", "authors": "Marc Hellmuth, Carsten R. Seemann and Peter F. Stadler", "title": "Generalized Fitch Graphs III: Symmetrized Fitch maps and Sets of\n  Symmetric Binary Relations that are explained by Unrooted Edge-labeled Trees", "comments": null, "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 23 no.\n  1, Graph Theory (June 3, 2021) dmtcs:7493", "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS math.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Binary relations derived from labeled rooted trees play an import role in\nmathematical biology as formal models of evolutionary relationships. The\n(symmetrized) Fitch relation formalizes xenology as the pairs of genes\nseparated by at least one horizontal transfer event. As a natural\ngeneralization, we consider symmetrized Fitch maps, that is, symmetric maps\n$\\varepsilon$ that assign a subset of colors to each pair of vertices in $X$\nand that can be explained by a tree $T$ with edges that are labeled with\nsubsets of colors in the sense that the color $m$ appears in $\\varepsilon(x,y)$\nif and only if $m$ appears in a label along the unique path between $x$ and $y$\nin $T$. We first give an alternative characterization of the monochromatic case\nand then give a characterization of symmetrized Fitch maps in terms of\ncompatibility of a certain set of quartets. We show that recognition of\nsymmetrized Fitch maps is NP-complete. In the restricted case where\n$|\\varepsilon(x,y)|\\leq 1$ the problem becomes polynomial, since such maps\ncoincide with class of monochromatic Fitch maps whose graph-representations\nform precisely the class of complete multi-partite graphs.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 16:14:36 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 08:51:04 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 11:13:11 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Hellmuth", "Marc", ""], ["Seemann", "Carsten R.", ""], ["Stadler", "Peter F.", ""]]}, {"id": "2001.05976", "submitter": "Bartlomiej Dudek", "authors": "Bart{\\l}omiej Dudek, Pawe{\\l} Gawrychowski, Tatiana Starikovskaya", "title": "Generalised Pattern Matching Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the problem of $\\texttt{Generalised Pattern Matching}\\ (\\texttt{GPM})$\n[STOC'94, Muthukrishnan and Palem], we are given a text $T$ of length $n$ over\nan alphabet $\\Sigma_T$, a pattern $P$ of length $m$ over an alphabet\n$\\Sigma_P$, and a matching relationship $\\subseteq \\Sigma_T \\times \\Sigma_P$,\nand must return all substrings of $T$ that match $P$ (reporting) or the number\nof mismatches between each substring of $T$ of length $m$ and $P$ (counting).\nIn this work, we improve over all previously known algorithms for this problem\nfor various parameters describing the input instance:\n  * $\\mathcal{D}\\,$ being the maximum number of characters that match a fixed\ncharacter,\n  * $\\mathcal{S}\\,$ being the number of pairs of matching characters,\n  * $\\mathcal{I}\\,$ being the total number of disjoint intervals of characters\nthat match the $m$ characters of the pattern $P$.\n  At the heart of our new deterministic upper bounds for $\\mathcal{D}\\,$ and\n$\\mathcal{S}\\,$ lies a faster construction of superimposed codes, which solves\nan open problem posed in [FOCS'97, Indyk] and can be of independent interest.\nTo conclude, we demonstrate first lower bounds for $\\texttt{GPM}$. We start by\nshowing that any deterministic or Monte Carlo algorithm for $\\texttt{GPM}$ must\nuse $\\Omega(\\mathcal{S})$ time, and then proceed to show higher lower bounds\nfor combinatorial algorithms. These bounds show that our algorithms are almost\noptimal, unless a radically new approach is developed.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 18:20:04 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Dudek", "Bart\u0142omiej", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Starikovskaya", "Tatiana", ""]]}, {"id": "2001.06005", "submitter": "David Shmoys", "authors": "Jan Karel Lenstra and David B. Shmoys", "title": "Elements of Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the winter of 1976, Alexander Rinnooy Kan and Jan Karel Lenstra defended\ntheir PhD theses at the University of Amsterdam. Gene Lawler was on their\ncommittees. It was a natural idea to turn the theses into a textbook on\nscheduling. They set out to compile a survey with Ron Graham (1979), but\nprogress on the book was hampered by the many research opportunities offered by\nthe field. After David Shmoys joined the team in the mid 1980's, several\nchapters were drafted, and the survey was rewritten (1993). Gene passed away in\n1994. Colleagues were asked to contribute chapters or to complete existing\ndrafts. However, by the turn of the century the project was losing its\nmomentum, and finite convergence to completion fell beyond our reach.\n  Over the years, several chapters have been used in the classroom. We continue\nto receive requests from colleagues who look for a text on the elements of\nscheduling at an advanced undergraduate or early graduate level. This document\nis a compilation of what currently exists. We have made a marginal effort in\npatching it up at some places but is essentially what was written long ago. We\ndid make an attempt to include most of the citations in the bibliography.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 17:09:15 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Lenstra", "Jan Karel", ""], ["Shmoys", "David B.", ""]]}, {"id": "2001.06159", "submitter": "Debasis Dwibedy", "authors": "Debasis Dwibedy, Rakesh Mohanty", "title": "A New Fairness Model based on User's Objective for Multi-user\n  Multi-processor Online Scheduling", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resources of a multi-user system in multi-processor online scheduling are\nshared by competing users in which fairness is a major performance criterion\nfor resource allocation. Fairness ensures equality in resource sharing among\nthe users. According to our knowledge, fairness based on the user's objective\nhas neither been comprehensively studied nor a formal fairness model has been\nwell defined in the literature. This motivates us to explore and define a new\nmodel to ensure algorithmic fairness with quantitative performance measures\nbased on optimization of the user's objective. In this paper, we propose a new\nmodel for fairness in Multi-user Multi-processor Online Scheduling\nProblem(MUMPOSP). We introduce and formally define quantitative fairness\nmeasures based on user's objective by optimizing makespan for individual user\nin our proposed fairness model. We also define the unfairness of deprived users\nand absolute fairness of an algorithm. We obtain lower bound results for the\nabsolute fairness for m identical machines with equal length jobs. We show that\nour proposed fairness model can serve as a framework for measuring algorithmic\nfairness by considering various optimality criteria such as flow time and sum\nof completion times.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 05:11:55 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Dwibedy", "Debasis", ""], ["Mohanty", "Rakesh", ""]]}, {"id": "2001.06305", "submitter": "Shantanav Chakraborty", "authors": "Shantanav Chakraborty, Kyle Luh, J\\'er\\'emie Roland", "title": "How fast do quantum walks mix?", "comments": "4 pages + Supplementary Material, 1 Figure. Accepted for publication\n  in Physical Review Letters. Uses the results of Sec. VIII of\n  arXiv:1904.11895v1 and focuses on the mixing of quantum walks on random\n  graphs. For a comparison between different notions of quantum mixing refer to\n  arXiv:1904.11895v2", "journal-ref": null, "doi": "10.1103/PhysRevLett.124.050501", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental problem of sampling from the limiting distribution of quantum\nwalks on networks, known as \\emph{mixing}, finds widespread applications in\nseveral areas of quantum information and computation. Of particular interest in\nmost of these applications, is the minimum time beyond which the instantaneous\nprobability distribution of the quantum walk remains close to this limiting\ndistribution, known as the \\emph{quantum mixing time}. However this quantity is\nonly known for a handful of specific networks. In this letter, we prove an\nupper bound on the quantum mixing time for \\emph{almost all networks}, i.e.\\\nthe fraction of networks for which our bound holds, goes to one in the\nasymptotic limit. To this end, using several results in random matrix theory,\nwe find the quantum mixing time of Erd\\\"os-Renyi random networks: networks of\n$n$ nodes where each edge exists with probability $p$ independently. For\nexample for dense random networks, where $p$ is a constant, we show that the\nquantum mixing time is $\\mathcal{O}\\left(n^{3/2 + o(1)}\\right)$. Besides\nopening avenues for the analytical study of quantum dynamics on random\nnetworks, our work could find applications beyond quantum information\nprocessing. Owing to the universality of Wigner random matrices, our results on\nthe spectral properties of random graphs hold for general classes of random\nmatrices that are ubiquitous in several areas of physics. In particular, our\nresults could lead to novel insights into the equilibration times of isolated\nquantum systems defined by random Hamiltonians, a foundational problem in\nquantum statistical mechanics.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 10:45:41 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Chakraborty", "Shantanav", ""], ["Luh", "Kyle", ""], ["Roland", "J\u00e9r\u00e9mie", ""]]}, {"id": "2001.06316", "submitter": "Maximilien Gadouleau", "authors": "Samuel Hunt and Maximilien Gadouleau", "title": "Grover's Algorithm and Many-Valued Quantum Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the engineering endeavour to realise quantum computers progresses, we\nconsider that such machines need not rely on binary as their de facto unit of\ninformation. We investigate Grover's algorithm under a generalised quantum\ncircuit model, in which the information and transformations can be expressed in\nany arity, and analyse the structural and behavioural properties while\npreserving the semantics; namely, searching for the unique preimage to an\noutput a function. We conclude by demonstrating that the generalised procedure\nretains $O(\\sqrt{N})$ time complexity.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 14:02:50 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Hunt", "Samuel", ""], ["Gadouleau", "Maximilien", ""]]}, {"id": "2001.06407", "submitter": "Sean Cleary", "authors": "Sean Cleary and Roland Maio", "title": "Counting difficult tree pairs with respect to the rotation distance\n  problem", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotation distance between rooted binary trees is the minimum number of simple\nrotations needed to transform one tree into the other. Computing the rotation\ndistance between a pair of rooted trees can be quickly reduced in cases where\nthere is a common edge between the trees, or where a single rotation introduces\na common edge. Tree pairs which do not have such a reduction are difficult tree\npairs, where there is no generally known first step. Here, we describe efforts\nto count and estimate the number of such difficult tree pairs, and find that\nthe fraction decreases exponentially fast toward zero. We also describe how\nknowing the number of distinct instances of the rotation distance problem is a\nhelpful factor in making the computations more feasible.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 16:20:31 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 04:12:49 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Cleary", "Sean", ""], ["Maio", "Roland", ""]]}, {"id": "2001.06422", "submitter": "Sean Cleary", "authors": "Sean Cleary and Roland Maio", "title": "An efficient sampling algorithm for difficult tree pairs", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is an open question whether there exists a polynomial-time algorithm for\ncomputing the rotation distances between pairs of extended ordered binary\ntrees. The problem of computing the rotation distance between an arbitrary pair\nof trees, (S, T), can be efficiently reduced to the problem of computing the\nrotation distance between a difficult pair of trees (S', T'), where there is no\nknown first step which is guaranteed to be the beginning of a minimal length\npath. Of interest, therefore, is how to sample such difficult pairs of trees of\na fixed size. We show that it is possible to do so efficiently, and present\nsuch an algorithm that runs in time $O(n^4)$.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 16:44:24 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Cleary", "Sean", ""], ["Maio", "Roland", ""]]}, {"id": "2001.06550", "submitter": "Hongyu Zheng", "authors": "Hongyu Zheng, Carl Kingsford, Guillaume Mar\\c{c}ais", "title": "Lower density selection schemes via small universal hitting sets with\n  short remaining path length", "comments": "16+7 pages. Accepted to RECOMB 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Universal hitting sets are sets of words that are unavoidable: every long\nenough sequence is hit by the set (i.e., it contains a word from the set).\nThere is a tight relationship between universal hitting sets and minimizers\nschemes, where minimizers schemes with low density (i.e., efficient schemes)\ncorrespond to universal hitting sets of small size. Local schemes are a\ngeneralization of minimizers schemes which can be used as replacement for\nminimizers scheme with the possibility of being much more efficient. We\nestablish the link between efficient local schemes and the minimum length of a\nstring that must be hit by a universal hitting set. We give bounds for the\nremaining path length of the Mykkeltveit universal hitting set. Additionally,\nwe create a local scheme with the lowest known density that is only a log\nfactor away from the theoretical lower bound.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 18:25:31 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Zheng", "Hongyu", ""], ["Kingsford", "Carl", ""], ["Mar\u00e7ais", "Guillaume", ""]]}, {"id": "2001.06561", "submitter": "Heinrich Hartmann", "authors": "Heinrich Hartmann and Theo Schlossnagle", "title": "Circllhist -- A Log-Linear Histogram Data Structure for IT\n  Infrastructure Monitoring", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The circllhist histogram is a fast and memory efficient data structure for\nsummarizing large numbers of latency measurements. It is particularly suited\nfor applications in IT infrastructure monitoring, and provides nano-second data\ninsertion, full mergeability, accurate approximation of quantiles with a-priori\nbounds on the relative error.\n  Open-source implementations are available for\nC/lua/python/Go/Java/JavaScript.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 23:56:34 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Hartmann", "Heinrich", ""], ["Schlossnagle", "Theo", ""]]}, {"id": "2001.06630", "submitter": "Shiqi Zhang", "authors": "Xinxun Zeng, Shiqi Zhang, Bo Tang", "title": "RCELF: A Residual-based Approach for Influence Maximization Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influence Maximization Problem (IMP) is selecting a seed set of nodes in the\nsocial network to spread the influence as widely as possible. It has many\napplications in multiple domains, e.g., viral marketing is frequently used for\nnew products or activities advertisements. While it is a classic and\nwell-studied problem in computer science, unfortunately, all those proposed\ntechniques are compromising among time efficiency, memory consumption, and\nresult quality. In this paper, we conduct comprehensive experimental studies on\nthe state-of-the-art IMP approximate approaches to reveal the underlying\ntrade-off strategies. Interestingly, we find that even the state-of-the-art\napproaches are impractical when the propagation probability of the network have\nbeen taken into consideration. With the findings of existing approaches, we\npropose a novel residual-based approach (i.e., RCELF) for IMP, which i)\novercomes the deficiencies of existing approximate approaches, and ii) provides\ntheoretical guaranteed results with high efficiency in both time- and space-\nperspectives. We demonstrate the superiority of our proposal by extensive\nexperimental evaluation on real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 09:09:34 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 06:56:44 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Zeng", "Xinxun", ""], ["Zhang", "Shiqi", ""], ["Tang", "Bo", ""]]}, {"id": "2001.06741", "submitter": "Jakub Truszkowski", "authors": "Jakub Truszkowski, Celine Scornavacca, Fabio Pardi", "title": "Computing the probability of gene trees concordant with the species tree\n  in the multispecies coalescent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multispecies coalescent process models the genealogical relationships of\ngenes sampled from several species, enabling useful predictions about phenomena\nsuch as the discordance between the gene tree and the species phylogeny due to\nincomplete lineage sorting. Conversely, knowledge of large collections of gene\ntrees can inform us about several aspects of the species phylogeny, such as its\ntopology and ancestral population sizes. A fundamental open problem in this\ncontext is how to efficiently compute the probability of a gene tree topology,\ngiven the species phylogeny. Although a number of algorithms for this task have\nbeen proposed, they either produce approximate results, or, when they are\nexact, they do not scale to large data sets. In this paper, we present some\nprogress towards exact and efficient computation of the probability of a gene\ntree topology. We provide a new algorithm that, given a species tree and the\nnumber of genes sampled for each species, calculates the probability that the\ngene tree topology will be concordant with the species tree. Moreover, we\nprovide an algorithm that computes the probability of any specific gene tree\ntopology concordant with the species tree. Both algorithms run in polynomial\ntime and have been implemented in Python. Experiments show that they are able\nto analyse data sets where thousands of genes are sampled, in a matter of\nminutes to hours.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 23:50:52 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 21:40:48 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Truszkowski", "Jakub", ""], ["Scornavacca", "Celine", ""], ["Pardi", "Fabio", ""]]}, {"id": "2001.06784", "submitter": "Shweta Jain", "authors": "Shweta Jain, C. Seshadhri", "title": "The Power of Pivoting for Exact Clique Counting", "comments": "10 pages, WSDM 2020", "journal-ref": null, "doi": "10.1145/3336191.3371839", "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clique counting is a fundamental task in network analysis, and even the\nsimplest setting of $3$-cliques (triangles) has been the center of much recent\nresearch. Getting the count of $k$-cliques for larger $k$ is algorithmically\nchallenging, due to the exponential blowup in the search space of large\ncliques. But a number of recent applications (especially for community\ndetection or clustering) use larger clique counts. Moreover, one often desires\n\\textit{local} counts, the number of $k$-cliques per vertex/edge.\n  Our main result is Pivoter, an algorithm that exactly counts the number of\n$k$-cliques, \\textit{for all values of $k$}. It is surprisingly effective in\npractice, and is able to get clique counts of graphs that were beyond the reach\nof previous work. For example, Pivoter gets all clique counts in a social\nnetwork with a 100M edges within two hours on a commodity machine. Previous\nparallel algorithms do not terminate in days. Pivoter can also feasibly get\nlocal per-vertex and per-edge $k$-clique counts (for all $k$) for many public\ndata sets with tens of millions of edges. To the best of our knowledge, this is\nthe first algorithm that achieves such results.\n  The main insight is the construction of a Succinct Clique Tree (SCT) that\nstores a compressed unique representation of all cliques in an input graph. It\nis built using a technique called \\textit{pivoting}, a classic approach by\nBron-Kerbosch to reduce the recursion tree of backtracking algorithms for\nmaximal cliques. Remarkably, the SCT can be built without actually enumerating\nall cliques, and provides a succinct data structure from which exact clique\nstatistics ($k$-clique counts, local counts) can be read off efficiently.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 06:50:15 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Jain", "Shweta", ""], ["Seshadhri", "C.", ""]]}, {"id": "2001.06841", "submitter": "Sungjin Im", "authors": "Sungjin Im, Benjamin Moseley, Kamesh Munagala, Kirk Pruhs", "title": "Dynamic Weighted Fairness with Minimal Disruptions", "comments": "To appear in Proceedings of the ACM on Measurement and Analysis of\n  Computing Systems (POMACS) 2020 (SIGMETRICS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the following dynamic fair allocation problem:\nGiven a sequence of job arrivals and departures, the goal is to maintain an\napproximately fair allocation of the resource against a target fair allocation\npolicy, while minimizing the total number of disruptions, which is the number\nof times the allocation of any job is changed. We consider a rich class of fair\nallocation policies that significantly generalize those considered in previous\nwork.\n  We first consider the models where jobs only arrive, or jobs only depart. We\npresent tight upper and lower bounds for the number of disruptions required to\nmaintain a constant approximate fair allocation every time step. In particular,\nfor the canonical case where jobs have weights and the resource allocation is\nproportional to the job's weight, we show that maintaining a constant\napproximate fair allocation requires $\\Theta(\\log^* n)$ disruptions per job,\nalmost matching the bounds in prior work for the unit weight case. For the more\ngeneral setting where the allocation policy only decreases the allocation to a\njob when new jobs arrive, we show that maintaining a constant approximate fair\nallocation requires $\\Theta(\\log n)$ disruptions per job. We then consider the\nmodel where jobs can both arrive and depart. We first show strong lower bounds\non the number of disruptions required to maintain constant approximate fairness\nfor arbitrary instances. In contrast we then show that there there is an\nalgorithm that can maintain constant approximate fairness with $O(1)$ expected\ndisruptions per job if the weights of the jobs are independent of the jobs\narrival and departure order. We finally show how our results can be extended to\nthe setting with multiple resources.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 14:55:46 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Im", "Sungjin", ""], ["Moseley", "Benjamin", ""], ["Munagala", "Kamesh", ""], ["Pruhs", "Kirk", ""]]}, {"id": "2001.06864", "submitter": "Veli M\\\"akinen", "authors": "Veli M\\\"akinen and Kristoffer Sahlin", "title": "Chaining with overlaps revisited", "comments": "Final version to appear in CPM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chaining algorithms aim to form a semi-global alignment of two sequences\nbased on a set of anchoring local alignments as input. Depending on the\noptimization criteria and the exact definition of a chain, there are several\n$O(n \\log n)$ time algorithms to solve this problem optimally, where $n$ is the\nnumber of input anchors.\n  In this paper, we focus on a formulation allowing the anchors to overlap in a\nchain. This formulation was studied by Shibuya and Kurochin (WABI 2003), but\ntheir algorithm comes with no proof of correctness. We revisit and modify their\nalgorithm to consider a strict definition of precedence relation on anchors,\nadding the required derivation to convince on the correctness of the resulting\nalgorithm that runs in $O(n \\log^2 n)$ time on anchors formed by exact matches.\nWith the more relaxed definition of precedence relation considered by Shibuya\nand Kurochin or when anchors are non-nested such as matches of uniform length\n($k$-mers), the algorithm takes $O(n \\log n)$ time.\n  We also establish a connection between chaining with overlaps to the widely\nstudied longest common subsequence (LCS) problem.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 16:58:58 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 10:26:16 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["M\u00e4kinen", "Veli", ""], ["Sahlin", "Kristoffer", ""]]}, {"id": "2001.06867", "submitter": "Fedor Fomin", "authors": "Christophe Crespelle, P{\\aa}l Gr{\\o}n{\\aa}s Drange, Fedor V. Fomin,\n  Petr A. Golovach", "title": "A survey of parameterized algorithms and the complexity of edge\n  modification", "comments": "Incorporated comments from Marcin Pilipczuk, William Lochet, and\n  Dekel Tsur", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The survey provides an overview of the developing area of parameterized\nalgorithms for graph modification problems. We concentrate on edge modification\nproblems, where the task is to change a small number of adjacencies in a graph\nin order to satisfy some required property.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 17:20:20 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 12:06:29 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Crespelle", "Christophe", ""], ["Drange", "P\u00e5l Gr\u00f8n\u00e5s", ""], ["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""]]}, {"id": "2001.06935", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Tim Davis, Chansup Byun, William Arcand, David Bestor,\n  William Bergeron, Vijay Gadepally, Matthew Hubbell, Michael Houle, Michael\n  Jones, Anna Klein, Peter Michaleas, Lauren Milechin, Julie Mullen, Andrew\n  Prout, Antonio Rosa, Siddharth Samsi, Charles Yee, Albert Reuther", "title": "75,000,000,000 Streaming Inserts/Second Using Hierarchical Hypersparse\n  GraphBLAS Matrices", "comments": "4 pages, 4 figures, 28 references, accepted to IPDPS GrAPL 2020.\n  arXiv admin note: substantial text overlap with arXiv:1907.04217", "journal-ref": null, "doi": "10.1109/IPDPSW50202.2020.00046", "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.PF cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SuiteSparse GraphBLAS C-library implements high performance hypersparse\nmatrices with bindings to a variety of languages (Python, Julia, and\nMatlab/Octave). GraphBLAS provides a lightweight in-memory database\nimplementation of hypersparse matrices that are ideal for analyzing many types\nof network data, while providing rigorous mathematical guarantees, such as\nlinearity. Streaming updates of hypersparse matrices put enormous pressure on\nthe memory hierarchy. This work benchmarks an implementation of hierarchical\nhypersparse matrices that reduces memory pressure and dramatically increases\nthe update rate into a hypersparse matrices. The parameters of hierarchical\nhypersparse matrices rely on controlling the number of entries in each level in\nthe hierarchy before an update is cascaded. The parameters are easily tunable\nto achieve optimal performance for a variety of applications. Hierarchical\nhypersparse matrices achieve over 1,000,000 updates per second in a single\ninstance. Scaling to 31,000 instances of hierarchical hypersparse matrices\narrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update\nrate of 75,000,000,000 updates per second. This capability allows the MIT\nSuperCloud to analyze extremely large streaming network data sets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 01:41:17 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 20:50:53 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Kepner", "Jeremy", ""], ["Davis", "Tim", ""], ["Byun", "Chansup", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "William", ""], ["Gadepally", "Vijay", ""], ["Hubbell", "Matthew", ""], ["Houle", "Michael", ""], ["Jones", "Michael", ""], ["Klein", "Anna", ""], ["Michaleas", "Peter", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Samsi", "Siddharth", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "2001.07011", "submitter": "Felix Happach", "authors": "Felix Happach and Andreas S. Schulz", "title": "Approximation Algorithms and LP Relaxations for Scheduling Problems\n  Related to Min-Sum Set Cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider single-machine scheduling problems that are natural\ngeneralizations or variations of the min-sum set cover problem and the min-sum\nvertex cover problem. For each of these problems, we give new approximation\nalgorithms. Some of these algorithms rely on time-indexed LP relaxations. We\nshow how a variant of alpha-point scheduling leads to the best-known\napproximation ratios, including a guarantee of 4 for an interesting special\ncase of the so-called generalized min-sum set cover problem. We also make\nexplicit the connection between the greedy algorithm for min-sum set cover and\nthe concept of Sidney decomposition for precedence-constrained single-machine\nscheduling, and show how this leads to a 4-approximation algorithm for\nsingle-machine scheduling with so-called bipartite OR-precedence constraints.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 08:21:00 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Happach", "Felix", ""], ["Schulz", "Andreas S.", ""]]}, {"id": "2001.07061", "submitter": "Debasis Dwibedy", "authors": "Debasis Dwibedy, Rakesh Mohanty", "title": "A 2-Competitive Largest Job on Least Loaded Machine Online Algorithm\n  based on Multi Lists Scheduling Model", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online scheduling in identical machines with makespan minimization has been a\nwell studied research problem in the literature. In online scheduling, the\nscheduler receives a list of jobs one by one and assigns each incoming job on\nthe fly irrevocably to one of the machines before the arrival of the next job.\nIn this paper, we study a variant of the online scheduling problem, where jobs\nare requested on the fly from k distinct sources. On simultaneous arrival of\njobs from multiple sources, it is a non-trivial research challenge to ensure\nfairness in scheduling with respect to distinct sources, while optimizing the\noverall makespan of the schedule. We develop a novel Multi Lists Scheduling\nModel(MLS) and propose a 2-competitive deterministic online algorithm namely\nLargest Job on Least Loaded Machine(LJLLM) based on the MLS model to address\nthe above mentioned research challenge.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 11:30:17 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Dwibedy", "Debasis", ""], ["Mohanty", "Rakesh", ""]]}, {"id": "2001.07134", "submitter": "Christian Schulz", "authors": "Marcelo Fonseca Faraj, Alexander van der Grinten, Henning Meyerhenke,\n  Jesper Larsson Tr\\\"aff, and Christian Schulz", "title": "High-Quality Hierarchical Process Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partitioning graphs into blocks of roughly equal size such that few edges run\nbetween blocks is a frequently needed operation when processing graphs on a\nparallel computer. When a topology of a distributed system is known an\nimportant task is then to map the blocks of the partition onto the processors\nsuch that the overall communication cost is reduced. We present novel\nmultilevel algorithms that integrate graph partitioning and process mapping.\nImportant ingredients of our algorithm include fast label propagation, more\nlocalized local search, initial partitioning, as well as a compressed data\nstructure to compute processor distances without storing a distance matrix.\nExperiments indicate that our algorithms speed up the overall mapping process\nand, due to the integrated multilevel approach, also find much better solutions\nin practice. For example, one configuration of our algorithm yields better\nsolutions than the previous state-of-the-art in terms of mapping quality while\nbeing a factor 62 faster. Compared to the currently fastest iterated multilevel\nmapping algorithm Scotch, we obtain 16% better solutions while investing\nslightly more running time.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 15:05:05 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 12:02:09 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Faraj", "Marcelo Fonseca", ""], ["van der Grinten", "Alexander", ""], ["Meyerhenke", "Henning", ""], ["Tr\u00e4ff", "Jesper Larsson", ""], ["Schulz", "Christian", ""]]}, {"id": "2001.07158", "submitter": "Suhas Thejaswi", "authors": "Suhas Thejaswi, Aristides Gionis, Juho Lauri", "title": "Finding path motifs in large temporal graphs using algebraic\n  fingerprints", "comments": "version prior to peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a family of pattern-detection problems in vertex-colored temporal\ngraphs. In particular, given a vertex-colored temporal graph and a multiset of\ncolors as a query, we search for temporal paths in the graph that contain the\ncolors specified in the query. These types of problems have several\napplications, for example in recommending tours for tourists or detecting\nabnormal behavior in a network of financial transactions. For the family of\npattern-detection problems we consider, we establish complexity results and\ndesign an algebraic-algorithmic framework based on constrained multilinear\nsieving. We demonstrate that our solution scales to massive graphs with up to a\nbillion edges for a multiset query with five colors and up to hundred million\nedges for a multiset query with ten colors, despite the problems being NP-hard.\nOur implementation, which is publicly available, exhibits practical edge-linear\nscalability and is highly optimized. For instance, in a real-world graph\ndataset with more than six million edges and a multiset query with ten colors,\nwe can extract an optimum solution in less than eight minutes on a Haswell\ndesktop with four cores.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 16:13:27 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 04:31:40 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 13:18:08 GMT"}, {"version": "v4", "created": "Mon, 27 Jul 2020 14:40:49 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Thejaswi", "Suhas", ""], ["Gionis", "Aristides", ""], ["Lauri", "Juho", ""]]}, {"id": "2001.07282", "submitter": "Joseph Chow", "authors": "Theodoros P. Pantelidis, Li Li, Tai-Yu Ma, Joseph Y. J. Chow, Saif\n  Eddin G. Jabari", "title": "A node-charge graph-based online carshare rebalancing policy with\n  capacitated electric charging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Viability of electric car-sharing operations depends on rebalancing\nalgorithms. Earlier methods in the literature suggest a trend toward non-myopic\nalgorithms using queueing principles. We propose a new rebalancing policy using\ncost function approximation. The cost function is modeled as a p-median\nrelocation problem with minimum cost flow conservation and path-based charging\nstation capacities on a static node-charge graph structure. The cost function\nis NP-complete, so a heuristic is proposed that ensures feasible solutions that\ncan be solved in an online system. The algorithm is validated in a case study\nof electric carshare in Brooklyn, New York, with demand data shared from BMW\nReachNow operations in September 2017 (262 vehicle fleet, 231 pickups per day,\n303 traffic analysis zones (TAZs)) and charging station location data (18\ncharging stations with 4 port capacities). The proposed non-myopic rebalancing\nheuristic reduces the cost increase compared to myopic rebalancing by 38%.\nOther managerial insights are further discussed.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 23:28:29 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 13:39:46 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 15:45:32 GMT"}, {"version": "v4", "created": "Mon, 15 Mar 2021 01:54:36 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Pantelidis", "Theodoros P.", ""], ["Li", "Li", ""], ["Ma", "Tai-Yu", ""], ["Chow", "Joseph Y. J.", ""], ["Jabari", "Saif Eddin G.", ""]]}, {"id": "2001.07477", "submitter": "Ofer Neiman", "authors": "Michael Elkin and Ofer Neiman", "title": "Near-Additive Spanners and Near-Exact Hopsets, A Unified View", "comments": "A survey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an {\\em unweighted} undirected graph $G = (V,E)$, and a pair of\nparameters $\\epsilon > 0$, $\\beta = 1,2,\\ldots$, a subgraph $G' =(V,H)$, $H\n\\subseteq E$, of $G$ is a {\\em $(1+\\epsilon,\\beta)$-spanner} (aka, a {\\em\nnear-additive spanner}) of $G$ if for every $u,v \\in V$, $$d_{G'}(u,v) \\le\n(1+\\epsilon)d_G(u,v) + \\beta~.$$ It was shown in \\cite{EP01} that for any\n$n$-vertex $G$ as above, and any $\\epsilon > 0$ and $\\kappa = 1,2,\\ldots$,\nthere exists a $(1+\\epsilon,\\beta)$-spanner $G'$ with\n$O_{\\epsilon,\\kappa}(n^{1+1/\\kappa})$ edges, with $$\\beta = \\beta_{EP} =\n\\left({{\\log \\kappa} \\over \\epsilon}\\right)^{\\log \\kappa - 2}~.$$ This bound\nremains state-of-the-art, and its dependence on $\\epsilon$ (for the case of\nsmall $\\kappa$) was shown to be tight in \\cite{ABP18}.\n  Given a {\\em weighted} undirected graph $G = (V,E,\\omega)$, and a pair of\nparameters $\\epsilon > 0$, $\\beta = 1,2,\\ldots$, a graph $G'= (V,H,\\omega')$ is\na {\\em $(1+\\epsilon,\\beta)$-hopset} (aka, a {\\em near-exact hopset}) of $G$ if\nfor every $u,v \\in V$, $$d_G(u,v) \\le d_{G\\cup G'}^{(\\beta)}(u,v) \\le\n(1+\\epsilon)d_G(u,v)~,$$ where $ d_{G\\cup G'}^{(\\beta)}(u,v)$ stands for a\n$\\beta$-(hop)-bounded distance between $u$ and $v$ in the union graph $G \\cup\nG'$. It was shown in \\cite{EN16} that for any $n$-vertex $G$ and $\\epsilon$ and\n$\\kappa$ as above, there exists a $(1+\\epsilon,\\beta)$-hopset with\n$\\tilde{O}(n^{1+1/\\kappa})$ edges, with $\\beta = \\beta_{EP}$.\n  Not only the two results of \\cite{EP01} and \\cite{EN16} are strikingly\nsimilar, but so are also their proof techniques. Moreover, Thorup-Zwick's later\nconstruction of near-additive spanners \\cite{TZ06} was also shown in\n\\cite{EN19,HP17} to provide hopsets with analogous (to that of \\cite{TZ06})\nproperties.\n  In this survey we explore this intriguing phenomenon, sketch the basic proof\ntechniques used for these results, and highlight open questions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 12:32:20 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Elkin", "Michael", ""], ["Neiman", "Ofer", ""]]}, {"id": "2001.07672", "submitter": "Meng-Tsung Tsai", "authors": "Yi-Jun Chang, Martin Farach-Colton, Tsan-Sheng Hsu, Meng-Tsung Tsai", "title": "Streaming Complexity of Spanning Tree Computation", "comments": "This is the full version of a conference paper to appear in the\n  Proceedings of 37th International Symposium on Theoretical Aspects of\n  Computer Science (STACS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semi-streaming model is a variant of the streaming model frequently used\nfor the computation of graph problems. It allows the edges of an $n$-node input\ngraph to be read sequentially in $p$ passes using $\\tilde{O}(n)$ space. In this\nmodel, some graph problems, such as spanning trees and $k$-connectivity, can be\nexactly solved in a single pass; while other graph problems, such as triangle\ndetection and unweighted all-pairs shortest paths, are known to require\n$\\tilde{\\Omega}(n)$ passes to compute. For many fundamental graph problems, the\ntractability in these models is open. In this paper, we study the tractability\nof computing some standard spanning trees. Our results are:\n  (1) Maximum-Leaf Spanning Trees. This problem is known to be APX-complete\nwith inapproximability constant $\\rho\\in[245/244,2)$. By constructing an\n$\\varepsilon$-MLST sparsifier, we show that for every constant $\\varepsilon >\n0$, MLST can be approximated in a single pass to within a factor of\n$1+\\varepsilon$ w.h.p. (albeit in super-polynomial time for $\\varepsilon \\le\n\\rho-1$ assuming $\\mathrm{P} \\ne \\mathrm{NP}$).\n  (2) BFS Trees. It is known that BFS trees require $\\omega(1)$ passes to\ncompute, but the na\\\"{i}ve approach needs $O(n)$ passes. We devise a new\nrandomized algorithm that reduces the pass complexity to $O(\\sqrt{n})$, and it\noffers a smooth tradeoff between pass complexity and space usage.\n  (3) DFS Trees. The current best algorithm by Khan and Mehta {[}STACS 2019{]}\ntakes $\\tilde{O}(h)$ passes, where $h$ is the height of computed DFS trees. Our\ncontribution is twofold. First, we provide a simple alternative proof of this\nresult, via a new connection to sparse certificates for $k$-node-connectivity.\nSecond, we present a randomized algorithm that reduces the pass complexity to\n$O(\\sqrt{n})$, and it also offers a smooth tradeoff between pass complexity and\nspace usage.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 17:49:16 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Farach-Colton", "Martin", ""], ["Hsu", "Tsan-Sheng", ""], ["Tsai", "Meng-Tsung", ""]]}, {"id": "2001.07741", "submitter": "Greg Bodwin", "authors": "Greg Bodwin", "title": "A Note on Distance-Preserving Graph Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider problems of the following type: given a graph $G$, how many edges\nare needed in the worst case for a sparse subgraph $H$ that approximately\npreserves distances between a given set of node pairs $P$? Examples include\npairwise spanners, distance preservers, reachability preservers, etc. There has\nbeen a trend in the area of simple constructions based on the hitting set\ntechnique, followed by somewhat more complicated constructions that improve\nover the bounds obtained from hitting sets by roughly a $\\log$ factor. In this\nnote, we point out that the simpler constructions based on hitting sets don't\nactually need an extra $\\log$ factor in the first place. This simplifies and\nunifies a few proofs in the area, and it improves the size of the $+4$ pairwise\nspanner from $\\widetilde{O}(np^{2/7})$ [Kavitha Th. Comp. Sys. '17] to\n$O(np^{2/7})$.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 19:11:11 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 23:22:03 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 16:41:21 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Bodwin", "Greg", ""]]}, {"id": "2001.07749", "submitter": "Wolfgang Garn", "authors": "Wolfgang Garn", "title": "Closed form distance formula for the balanced multiple travelling\n  salesmen", "comments": "10 pages, 11 figures, 3 tables, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a first contribution the mTSP is solved using an exact method and two\nheuristics, where the number of nodes per route is balanced. The first\nheuristic uses a nearest node approach and the second assigns the closest\nvehicle (salesman). A comparison of heuristics with test-instances being in the\nEuclidean plane showed similar solution quality and runtime. On average, the\nnearest node solutions are approximately one percent better. The closest\nvehicle heuristic is especially important when the nodes (customers) are not\nknown in advance, e.g. for online routing. Whilst the nearest node is\npreferable when one vehicle has to be used multiple times to service all\ncustomers. The second contribution is a closed form formula that describes the\nmTSP distance dependent on the number of vehicles and customers. Increasing the\nnumber of salesman results in an approximately linear distance growth for\nuniformly distributed nodes in a Euclidean grid plane. The distance growth is\nalmost proportional to the square root of number of customers (nodes). These\ntwo insights are combined in a single formula. The minimum distance of a node\nto $n$ uniformly distributed random (real and integer) points was derived and\nexpressed as functional relationship dependent on the number of vehicles. This\ngives theoretical underpinnings and is in agreement with the distances found\nvia the previous mTSP heuristics. Hence, this allows to compute all expected\nmTSP distances without the need of running the heuristics.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 19:26:12 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 08:54:11 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Garn", "Wolfgang", ""]]}, {"id": "2001.07765", "submitter": "Christophe Crespelle", "authors": "Christophe Crespelle, Daniel Lokshtanov, Thi Ha Duong Phan, Eric\n  Thierry", "title": "Faster and Enhanced Inclusion-Minimal Cograph Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design two incremental algorithms for computing an inclusion-minimal\ncompletion of an arbitrary graph into a cograph. The first one is able to do so\nwhile providing an additional property which is crucial in practice to obtain\ninclusion-minimal completions using as few edges as possible : it is able to\ncompute a minimum-cardinality completion of the neighbourhood of the new vertex\nintroduced at each incremental step. It runs in $O(n+m')$ time, where $m'$ is\nthe number of edges in the completed graph. This matches the complexity of the\nalgorithm in [Lokshtanov, Mancini and Papadopoulos 2010] and positively answers\none of their open questions. Our second algorithm improves the complexity of\ninclusion-minimal completion to $O(n+m\\log^2 n)$ when the additional property\nabove is not required. Moreover, we prove that many very sparse graphs, having\nonly $O(n)$ edges, require $\\Omega(n^2)$ edges in any of their cograph\ncompletions. For these graphs, which include many of those encountered in\napplications, the improvement we obtain on the complexity scales as $O(n/\\log^2\nn)$.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 20:29:53 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Crespelle", "Christophe", ""], ["Lokshtanov", "Daniel", ""], ["Phan", "Thi Ha Duong", ""], ["Thierry", "Eric", ""]]}, {"id": "2001.07784", "submitter": "Michael Dinitz", "authors": "Michael Dinitz and Benjamin Moseley", "title": "Scheduling for Weighted Flow and Completion Times in Reconfigurable\n  Networks", "comments": "10 pages. Appears in INFOCOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New optical technologies offer the ability to reconfigure network topologies\ndynamically, rather than setting them once and for all. This is true in both\noptical wide area networks (optical WANs) and in datacenters, despite the many\ndifferences between these two settings. Because of these new technologies,\nthere has been a surge of both practical and theoretical research on algorithms\nto take advantage of them. In particular, Jia et al. [INFOCOM '17] designed\nonline scheduling algorithms for dynamically reconfigurable topologies for both\nthe makespan and sum of completion times objectives. In this paper, we work in\nthe same setting but study an objective that is more meaningful in an online\nsetting: the sum of flow times. The flow time of a job is the total amount of\ntime that it spends in the system, which may be considerably smaller than its\ncompletion time if it is released late. We provide competitive algorithms for\nthe online setting with speed augmentation, and also give a lower bound proving\nthat speed augmentation is in fact necessary. As a side effect of our\ntechniques, we also improve and generalize the results of Jia et al. on\ncompletion times by giving an $O(1)$-competitive algorithm for arbitrary sizes\nand release times even when nodes have different degree bounds, and moreover\nallow for the weighted sum of completion times (or flow times).\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 21:31:54 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Dinitz", "Michael", ""], ["Moseley", "Benjamin", ""]]}, {"id": "2001.07819", "submitter": "Krishnakumar Balasubramanian", "authors": "Zhongruo Wang, Krishnakumar Balasubramanian, Shiqian Ma, Meisam\n  Razaviyayn", "title": "Zeroth-Order Algorithms for Nonconvex Minimax Problems with Improved\n  Complexities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study zeroth-order algorithms for minimax optimization\nproblems that are nonconvex in one variable and strongly-concave in the other\nvariable. Such minimax optimization problems have attracted significant\nattention lately due to their applications in modern machine learning tasks. We\nfirst design and analyze the Zeroth-Order Gradient Descent Ascent\n(\\texttt{ZO-GDA}) algorithm, and provide improved results compared to existing\nworks, in terms of oracle complexity. Next, we propose the Zeroth-Order\nGradient Descent Multi-Step Ascent (\\texttt{ZO-GDMSA}) algorithm that\nsignificantly improves the oracle complexity of \\texttt{ZO-GDA}. We also\nprovide stochastic version of \\texttt{ZO-GDA} and \\texttt{ZO-GDMSA} to handle\nstochastic nonconvex minimax problems, and provide oracle complexity results.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 00:05:14 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Wang", "Zhongruo", ""], ["Balasubramanian", "Krishnakumar", ""], ["Ma", "Shiqian", ""], ["Razaviyayn", "Meisam", ""]]}, {"id": "2001.07887", "submitter": "Elbert Du", "authors": "Elbert Du, Stan Zhang", "title": "A Pseudopolynomial Algorithm to Minimize Maximum Lateness on Multiple\n  Related Machines", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we will find a pseudopolynomial algorithm to solve $Qm \\mid\n\\mid L_{\\max}$ and then we will prove that it is impossible to get any\nconstant-factor approximation in polynomial time, and thus also impossible to\nhave a PTAS for this problem. We will also show that the the problem when we\ndon't assume a fixed number of machines, $P \\mid \\mid L_{\\max}$, is strongly\nNP-hard.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 05:56:35 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Du", "Elbert", ""], ["Zhang", "Stan", ""]]}, {"id": "2001.08409", "submitter": "Ji\\v{r}\\'i Fink", "authors": "Ji\\v{r}\\'i Fink, Johann L. Hurink", "title": "A Greedy algorithm for local heating", "comments": "11 pages, no picture", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a planning problem for supplying hot water in domestic\nenvironment. Hereby, boilers (e.g. gas or electric boilers, heat pumps or\nmicroCHPs) are used to heat water and store it for domestic demands. We\nconsider a simple boiler which is either turned on or turned off and is\nconnected to a buffer of limited capacity. The energy needed to run the boiler\nhas to be bought e.g. on a day-ahead market, so we are interested in a planning\nwhich minimizes the cost to supply the boiler with energy in order to fulfill\nthe given heat demand. We present a greedy algorithm for this heating problem\nwhose time complexity is O(T {\\alpha}(T )) where T is the number of time\nintervals and {\\alpha} is the inverse of Ackermann function.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 08:56:21 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Fink", "Ji\u0159\u00ed", ""], ["Hurink", "Johann L.", ""]]}, {"id": "2001.08416", "submitter": "Ji\\v{r}\\'i Fink", "authors": "Ji\\v{r}\\'i Fink, Martin Loebl", "title": "Arc-routing for winter road maintenance", "comments": "22 pages, no figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The arc-routing problems are known to be notoriously hard. We study here a\nnatural arc-routing problem on trees and more generally on bounded tree-width\ngraphs and surprisingly show that it can be solved in a polynomial time. This\nimplies a sub-exponential algorithm for the planar graphs and small number of\nmaintaining cars, which is of practical relevance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 09:10:17 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 12:00:35 GMT"}, {"version": "v3", "created": "Sat, 14 Nov 2020 07:16:46 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Fink", "Ji\u0159\u00ed", ""], ["Loebl", "Martin", ""]]}, {"id": "2001.08417", "submitter": "Irena Rusu Ph.D.", "authors": "Irena Rusu", "title": "Sorting Permutations with Fixed Pinnacle Set", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a positive answer to a question raised by Davis et al. ({\\em Discrete\nMathematics} 341, 2018), concerning permutations with the same pinnacle set.\nGiven $\\pi\\in S_n$, a {\\em pinnacle} of $\\pi$ is an element $\\pi_i$ ($i\\neq\n1,n$) such that $\\pi_{i-1}<\\pi_i>\\pi_{i+1}$. The question is: given\n$\\pi,\\pi'\\in S_n$ with the same pinnacle set $S$, is there a sequence of\noperations that transforms $\\pi$ into $\\pi'$ such that all the intermediate\npermutations have pinnacle set $S$? We introduce {\\em balanced reversals},\ndefined as reversals that do not modify the pinnacle set of the permutation to\nwhich they are applied. Then we show that $\\pi$ may be sorted by balanced\nreversals (i.e. transformed into a standard permutation $\\Id_S$), implying that\n$\\pi$ may be transformed into $\\pi'$ using at most $4n-2\\min\\{p,3\\}$ balanced\nreversals, where $p=|S|\\geq 1$. In case $p=0$, at most $2n-1$ balanced\nreversals are needed.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 09:13:34 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Rusu", "Irena", ""]]}, {"id": "2001.08468", "submitter": "Shuichi Miyazaki", "authors": "Koki Hamada, Shuichi Miyazaki, Kazuya Okamoto", "title": "Strongly Stable and Maximum Weakly Stable Noncrossing Matchings", "comments": "Conference version appeared in IWOCA 2020. This version adds new\n  results: Theorem 1 is strengthened so that NP-completeness holds even if each\n  person's preference list is of length at most two and ties appear in only\n  men's preference lists. Theorem 2 is added, which shows that the problem can\n  be solved in linear time if the length of preference lists of one side is\n  bounded by one", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In IWOCA 2019, Ruangwises and Itoh introduced stable noncrossing matchings,\nwhere participants of each side are aligned on each of two parallel lines, and\nno two matching edges are allowed to cross each other. They defined two\nstability notions, strongly stable noncrossing matching (SSNM) and weakly\nstable noncrossing matching (WSNM), depending on the strength of blocking\npairs. They proved that a WSNM always exists and presented an $O(n^{2})$-time\nalgorithm to find one for an instance with $n$ men and $n$ women. They also\nposed open questions of the complexities of determining existence of an SSNM\nand finding a largest WSNM. In this paper, we show that both problems are\nsolvable in polynomial time. Our algorithms are applicable to extensions where\npreference lists may include ties, except for one case which we show to be\nNP-complete. This NP-completeness holds even if each person's preference list\nis of length at most two and ties appear in only men's preference lists. To\ncomplement this intractability, we show that the problem is solvable in\npolynomial time if the length of preference lists of one side is bounded by one\n(but that of the other side is unbounded).\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 12:19:01 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 02:36:17 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Hamada", "Koki", ""], ["Miyazaki", "Shuichi", ""], ["Okamoto", "Kazuya", ""]]}, {"id": "2001.08510", "submitter": "Laurent Feuilloley", "authors": "Laurent Feuilloley", "title": "Bibliography of distributed approximation beyond bounded degree", "comments": "An annotated bibliography", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is an informal bibliography of the papers dealing with\ndistributed approximation algorithms. A classic setting for such algorithms is\nbounded degree graphs, but there is a whole set of techniques that have been\ndeveloped for other classes. These later classes are the focus of the current\nwork. These classes have a geometric nature (planar, bounded genus and\nunit-disk graphs) and/or have bounded parameters (arboricity, expansion,\ngrowth, independence) or forbidden structures (forbidden minors).\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 16:49:41 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 07:55:44 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Feuilloley", "Laurent", ""]]}, {"id": "2001.08516", "submitter": "Timo Bingmann", "authors": "Timo Bingmann, Peter Sanders, Matthias Schimek", "title": "Communication-Efficient String Sorting", "comments": "Full version to appear at IPDPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been surprisingly little work on algorithms for sorting strings on\ndistributed-memory parallel machines. We develop efficient algorithms for this\nproblem based on the multi-way merging principle. These algorithms inspect only\ncharacters that are needed to determine the sorting order. Moreover,\ncommunication volume is reduced by also communicating (roughly) only those\ncharacters and by communicating repetitions of the same prefixes only once.\nExperiments on up to 1280 cores reveal that these algorithm are often more than\nfive times faster than previous algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 13:57:47 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Bingmann", "Timo", ""], ["Sanders", "Peter", ""], ["Schimek", "Matthias", ""]]}, {"id": "2001.08632", "submitter": "Ji\\v{r}\\'i Fink", "authors": "Ji\\v{r}\\'i Fink", "title": "Approximation algorithms for scheduling a group of heat pumps", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies planning problems for a group of heating systems which\nsupply the hot water demand for domestic use in houses. These systems (e.g. gas\nor electric boilers, heat pumps or microCHPs) use an external energy source to\nheat up water and store this hot water for supplying the domestic demands. The\nlatter allows to some extent a decoupling of the heat production from the heat\ndemand. We focus on the situation where each heating system has its own demand\nand buffer and the supply of the heating systems is coming from a common\nsource. In practice, the common source may lead to a coupling of the planning\nfor the group of heating systems. The bottleneck to supply the energy may be\nthe capacity of the distribution system (e.g. the electricity networks or the\ngas network). As this has to be dimensioned for the maximal consumption, it is\nimportant to minimize the maximal peak. This planning problem is known to be\n\\NP-hard. We present polynomial-time approximation algorithms for four variants\nof peak minimization problems, and we determine the worst-case approximation\nerror.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 16:10:25 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Fink", "Ji\u0159\u00ed", ""]]}, {"id": "2001.08659", "submitter": "Sergey Dovgal", "authors": "\\'Elie de Panafieu and Sergey Dovgal", "title": "Counting directed acyclic and elementary digraphs", "comments": "10 pages; Accepted to FPSAC. Updated in accordance with the comments\n  of reviewers", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed acyclic graphs (DAGs) can be characterised as directed graphs whose\nstrongly connected components are isolated vertices. Using this restriction on\nthe strong components, we discover that when $m = cn$, where $m$ is the number\nof directed edges, $n$ is the number of vertices, and $c < 1$, the asymptotic\nprobability that a random digraph is acyclic is an explicit function $p(c)$,\nsuch that $p(0) = 1$ and $p(1) = 0$. When $m = n(1 + \\mu n^{-1/3})$, the\nasymptotic behaviour changes, and the probability that a digraph is acyclic\nbecomes $n^{-1/3} C(\\mu)$, where $C(\\mu)$ is an explicit function of $\\mu$.\n{\\L}uczak and Seierstad (2009, Random Structures & Algorithms, 35(3), 271--293)\nshowed that, as $\\mu \\to -\\infty$, the strongly connected components of a\nrandom digraph with $n$ vertices and $m = n(1 + \\mu n^{-1/3})$ directed edges\nare, with high probability, only isolated vertices and cycles. We call such\ndigraphs elementary digraphs. We express the probability that a random digraph\nis elementary as a function of $\\mu$. Those results are obtained using\ntechniques from analytic combinatorics, developed in particular to study random\ngraphs.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 16:54:50 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 18:08:39 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["de Panafieu", "\u00c9lie", ""], ["Dovgal", "Sergey", ""]]}, {"id": "2001.08679", "submitter": "Shashank Vatedka", "authors": "Shashank Vatedka and Venkat Chandar and Aslan Tchamkerten", "title": "$O(\\log \\log n)$ Worst-Case Local Decoding and Update Efficiency for\n  Data Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of data compression with local decoding and\nlocal update. A compression scheme has worst-case local decoding $d_{wc}$ if\nany bit of the raw file can be recovered by probing at most $d_{wc}$ bits of\nthe compressed sequence, and has update efficiency of $u_{wc}$ if a single bit\nof the raw file can be updated by modifying at most $u_{wc}$ bits of the\ncompressed sequence. This article provides an entropy-achieving compression\nscheme for memoryless sources that simultaneously achieves $ O(\\log\\log n) $\nlocal decoding and update efficiency. Key to this achievability result is a\nnovel succinct data structure for sparse sequences which allows efficient local\ndecoding and local update. Under general assumptions on the local decoder and\nupdate algorithms, a converse result shows that $d_{wc}$ and $u_{wc}$ must grow\nas $ \\Omega(\\log\\log n) $.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 17:20:36 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Vatedka", "Shashank", ""], ["Chandar", "Venkat", ""], ["Tchamkerten", "Aslan", ""]]}, {"id": "2001.08767", "submitter": "Anay Mehrotra", "authors": "L. Elisa Celis and Anay Mehrotra and Nisheeth K. Vishnoi", "title": "Interventions for Ranking in the Presence of Implicit Bias", "comments": "This paper will appear at the ACM FAT* 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit bias is the unconscious attribution of particular qualities (or lack\nthereof) to a member from a particular social group (e.g., defined by gender or\nrace). Studies on implicit bias have shown that these unconscious stereotypes\ncan have adverse outcomes in various social contexts, such as job screening,\nteaching, or policing. Recently, (Kleinberg and Raghavan, 2018) considered a\nmathematical model for implicit bias and showed the effectiveness of the Rooney\nRule as a constraint to improve the utility of the outcome for certain cases of\nthe subset selection problem. Here we study the problem of designing\ninterventions for the generalization of subset selection -- ranking -- that\nrequires to output an ordered set and is a central primitive in various social\nand computational contexts. We present a family of simple and interpretable\nconstraints and show that they can optimally mitigate implicit bias for a\ngeneralization of the model studied in (Kleinberg and Raghavan, 2018).\nSubsequently, we prove that under natural distributional assumptions on the\nutilities of items, simple, Rooney Rule-like, constraints can also surprisingly\nrecover almost all the utility lost due to implicit biases. Finally, we augment\nour theoretical results with empirical findings on real-world distributions\nfrom the IIT-JEE (2009) dataset and the Semantic Scholar Research corpus.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 19:11:31 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Celis", "L. Elisa", ""], ["Mehrotra", "Anay", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "2001.08977", "submitter": "Pratibha Choudhary", "authors": "Aritra Banik, Pratibha Choudhary, Venkatesh Raman, Saket Saurabh", "title": "Fixed-parameter tractable algorithms for Tracking Shortest Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the parameterized complexity of the problem of tracking shortest\ns-t paths in graphs, motivated by applications in security and wireless\nnetworks. Given an undirected and unweighted graph with a source s and a\ndestination t, Tracking Shortest Paths asks if there exists a k-sized subset of\nvertices (referred to as tracking set) that intersects each shortest s-t path\nin a distinct set of vertices. We first generalize this problem for set\nsystems, namely Tracking Set System, where given a family of subsets of a\nuniverse, we are required to find a subset of elements from the universe that\nhas a unique intersection with each set in the family. Tracking Set System is\nshown to be fixed-parameter tractable due to its relation with a known problem,\nTest Cover. By a reduction to the well-studied d-hitting set problem, we give a\npolynomial (with respect to k) kernel for the case when the set sizes are\nbounded by d. This also helps solving Tracking Shortest Paths when the input\ngraph diameter is bounded by d. While the results for Tracking Set System help\nto show that Tracking Shortest Paths is fixed-parameter tractable, we also give\nan independent algorithm by using some preprocessing rules, resulting in an\nimproved running time.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 13:09:01 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 10:57:28 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Banik", "Aritra", ""], ["Choudhary", "Pratibha", ""], ["Raman", "Venkatesh", ""], ["Saurabh", "Saket", ""]]}, {"id": "2001.09122", "submitter": "Lydia Zakynthinou", "authors": "Thomas Steinke and Lydia Zakynthinou", "title": "Reasoning About Generalization via Conditional Mutual Information", "comments": "58 pages. Changes from previous version: Added discussion on related\n  work and updated references. Simplified part of the proof of Theorem 4.10", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an information-theoretic framework for studying the generalization\nproperties of machine learning algorithms. Our framework ties together existing\napproaches, including uniform convergence bounds and recent methods for\nadaptive data analysis. Specifically, we use Conditional Mutual Information\n(CMI) to quantify how well the input (i.e., the training data) can be\nrecognized given the output (i.e., the trained model) of the learning\nalgorithm. We show that bounds on CMI can be obtained from VC dimension,\ncompression schemes, differential privacy, and other methods. We then show that\nbounded CMI implies various forms of generalization.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 18:13:04 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 05:48:00 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2020 00:42:03 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Steinke", "Thomas", ""], ["Zakynthinou", "Lydia", ""]]}, {"id": "2001.09453", "submitter": "Aristides Gionis", "authors": "Ryuta Matsuno and Aristides Gionis", "title": "Improved mixing time for k-subgraph sampling", "comments": "Detailed version of a paper to appear in SIAM Data Mining 2020\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the local structure of a graph provides valuable insights about\nthe underlying phenomena from which the graph has originated. Sampling and\nexamining k-subgraphs is a widely used approach to understand the local\nstructure of a graph. In this paper, we study the problem of sampling uniformly\nk-subgraphs from a given graph. We analyze a few different Markov chain Monte\nCarlo (MCMC) approaches, and obtain analytical results on their mixing times,\nwhich improve significantly the state of the art. In particular, we improve the\nbound on the mixing times of the standard MCMC approach, and the\nstate-of-the-art MCMC sampling method PSRW, using the canonical-paths argument.\nIn addition, we propose a novel sampling method, which we call recursive\nsubgraph sampling, RSS, and its optimized variant RSS+. The proposed methods,\nRSS and RSS+, are significantly faster than existing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 13:45:15 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Matsuno", "Ryuta", ""], ["Gionis", "Aristides", ""]]}, {"id": "2001.10037", "submitter": "Mohammad Salavatipour", "authors": "Dylan Hyatt-Denesik, Mirmahdi Rahgoshay, Mohammad R. Salavatipour", "title": "Approximations for Throughput Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the classical problem of throughput maximization. In\nthis problem we have a collection $J$ of $n$ jobs, each having a release time\n$r_j$, deadline $d_j$, and processing time $p_j$. They have to be scheduled\nnon-preemptively on $m$ identical parallel machines. The goal is to find a\nschedule which maximizes the number of jobs scheduled entirely in their\n$[r_j,d_j]$ window. This problem has been studied extensively (even for the\ncase of $m=1$). Several special cases of the problem remain open. Bar-Noy et\nal. [STOC1999] presented an algorithm with ratio $1-1/(1+1/m)^m$ for $m$\nmachines, which approaches $1-1/e$ as $m$ increases. For $m=1$,\nChuzhoy-Ostrovsky-Rabani [FOCS2001] presented an algorithm with approximation\nwith ratio $1-\\frac{1}{e}-\\varepsilon$ (for any $\\varepsilon>0$). Recently\nIm-Li-Moseley [IPCO2017] presented an algorithm with ratio\n$1-1/e-\\varepsilon_0$ for some absolute constant $\\varepsilon_0>0$ for any\nfixed $m$. They also presented an algorithm with ratio $1-O(\\sqrt{\\log\nm/m})-\\varepsilon$ for general $m$ which approaches 1 as $m$ grows. The\napproximability of the problem for $m=O(1)$ remains a major open question. Even\nfor the case of $m=1$ and $c=O(1)$ distinct processing times the problem is\nopen (Sgall [ESA2012]). In this paper we study the case of $m=O(1)$ and show\nthat if there are $c$ distinct processing times, i.e. $p_j$'s come from a set\nof size $c$, then there is a $(1-\\varepsilon)$-approximation that runs in time\n$O(n^{mc^7\\varepsilon^{-6}}\\log T)$, where $T$ is the largest deadline.\nTherefore, for constant $m$ and constant $c$ this yields a PTAS. Our algorithm\nis based on proving structural properties for a near optimum solution that\nallows one to use a dynamic programming with pruning.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 19:48:02 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 03:00:49 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Hyatt-Denesik", "Dylan", ""], ["Rahgoshay", "Mirmahdi", ""], ["Salavatipour", "Mohammad R.", ""]]}, {"id": "2001.10148", "submitter": "Silvano Colombo Tosatto", "authors": "Silvano Colombo Tosatto, Guido Governatori, Nick Van Beest", "title": "Business Process Full Compliance with Respect to a Set of Conditional\n  Obligation in Polynomial Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new methodology to evaluate whether a business\nprocess model is fully compliant with a regulatory framework composed of a set\nof conditional obligations. The methodology is based failure delta-constraints\nthat are evaluated on bottom-up aggregations of a tree-like representation of\nbusiness process models. While the generic problem of proving full compliance\nis in coNP-complete, we show that verifying full compliance can be done in\npolynomial time using our methodology, for an acyclic structured process model\ngiven a regulatory framework composed by a set of conditional obligations,\nwhose elements are restricted to be represented by propositional literals\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 02:45:57 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Tosatto", "Silvano Colombo", ""], ["Governatori", "Guido", ""], ["Van Beest", "Nick", ""]]}, {"id": "2001.10500", "submitter": "Stefan Walzer", "authors": "Stefan Walzer", "title": "Peeling Close to the Orientability Threshold: Spatial Coupling in\n  Hashing-Based Data Structures", "comments": "This revision makes substantial changes to the presentation of the\n  material. The introduction was completely rewritten. The variable $n$ was\n  rescaled so that $n$ rather than $n(z+1)$ is the number of vertices. Some\n  discussion was added in the experimental section. The technical content\n  (Sections 2-5) is essentially unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multiple-choice data structures each element $x$ in a set $S$ of $m$ keys\nis associated with a random set $e(x) \\subseteq [n]$ of buckets with capacity\n$\\ell \\geq 1$ by hash functions. This setting is captured by the hypergraph $H\n= ([n],\\{e(x) \\mid x \\in S\\})$. Accomodating each key in an associated bucket\namounts to finding an $\\ell$-orientation of $H$ assigning to each hyperedge an\nincident vertex such that each vertex is assigned at most $\\ell$ hyperedges. If\neach subhypergraph of $H$ has minimum degree at most $\\ell$, then an\n$\\ell$-orientation can be found greedily and $H$ is called $\\ell$-peelable.\nPeelability has a central role in invertible Bloom lookup tables and can speed\nup the construction of retrieval data structures, perfect hash functions and\ncuckoo hash tables.\n  Many hypergraphs exhibit sharp density thresholds with respect to\n$\\ell$-orientability and $\\ell$-peelability, i.e. as the density $c =\n\\frac{m}{n}$ grows past a critical value, the probability of these properties\ndrops from almost $1$ to almost $0$. In fully random $k$-uniform hypergraphs\nthe thresholds $c_{k,\\ell}^*$ for $\\ell$-orientability significantly exceed the\nthresholds for $\\ell$-peelability. In this paper, for every $k \\geq 2$ and\n$\\ell \\geq 1$ with $(k,\\ell) \\neq (2,1)$ and every $z > 0$, we construct a new\nfamily of random $k$-uniform hypergraphs with i.i.d. random hyperedges such\nthat both the $\\ell$-peelability and the $\\ell$-orientability thresholds\napproach $c_{k,\\ell}^*$ as $z \\rightarrow \\infty$.\n  We exploit the phenomenon of threshold saturation via spatial coupling\ndiscovered in the context of low-density parity-check codes. Once the\nconnection to data structures is in plain sight, a framework by Kudekar,\nRichardson and Urbanke (2015) does the heavy lifting in our proof.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 18:03:35 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 14:09:11 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Walzer", "Stefan", ""]]}, {"id": "2001.10567", "submitter": "Serikzhan Kazi", "authors": "Meng He and Serikzhan Kazi", "title": "Path Query Data Structures in Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We perform experimental studies on data structures that answer path median,\npath counting, and path reporting queries in weighted trees. These query\nproblems generalize the well-known range median query problem in arrays, as\nwell as the $2d$ orthogonal range counting and reporting problems in planar\npoint sets, to tree structured data. We propose practical realizations of the\nlatest theoretical results on path queries. Our data structures, which use tree\nextraction, heavy-path decomposition and wavelet trees, are implemented in both\nsuccinct and pointer-based form. Our succinct data structures are further\nspecialized to be plain or entropy-compressed. Through experiments on large\nsets, we show that succinct data structures for path queries may present a\nviable alternative to standard pointer-based realizations, in practical\nscenarios. Compared to na{\\\"i}ve approaches that compute the answer by explicit\ntraversal of the query path, our succinct data structures are several times\nfaster in path median queries and perform comparably in path counting and path\nreporting queries, while being several times more space-efficient. Plain\npointer-based realizations of our data structures, requiring a few times more\nspace than the na{\\\"i}ve ones, yield up to $100$-times speed-up over them.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 19:32:22 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 10:32:43 GMT"}, {"version": "v3", "created": "Sat, 18 Apr 2020 00:36:37 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["He", "Meng", ""], ["Kazi", "Serikzhan", ""]]}, {"id": "2001.10600", "submitter": "Sahil Singla", "authors": "Nicole Immorlica, Sahil Singla, and Bo Waggoner", "title": "Prophet Inequalities with Linear Correlations and Augmentations", "comments": "31 pages. Appears in EC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a classical online decision problem, a decision-maker who is trying to\nmaximize her value inspects a sequence of arriving items to learn their values\n(drawn from known distributions), and decides when to stop the process by\ntaking the current item. The goal is to prove a \"prophet inequality\": that she\ncan do approximately as well as a prophet with foreknowledge of all the values.\nIn this work, we investigate this problem when the values are allowed to be\ncorrelated. Since non-trivial guarantees are impossible for arbitrary\ncorrelations, we consider a natural \"linear\" correlation structure introduced\nby Bateni et al. [ESA 2015] as a generalization of the common-base value model\nof Chawla et al. [GEB 2015].\n  A key challenge is that threshold-based algorithms, which are commonly used\nfor prophet inequalities, no longer guarantee good performance for linear\ncorrelations. We relate this roadblock to another \"augmentations\" challenge\nthat might be of independent interest: many existing prophet inequality\nalgorithms are not robust to slight increase in the values of the arriving\nitems. We leverage this intuition to prove bounds (matching up to constant\nfactors) that decay gracefully with the amount of correlation of the arriving\nitems. We extend these results to the case of selecting multiple items by\ndesigning a new $(1+o(1))$ approximation ratio algorithm that is robust to\naugmentations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 21:44:07 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 17:35:58 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Immorlica", "Nicole", ""], ["Singla", "Sahil", ""], ["Waggoner", "Bo", ""]]}, {"id": "2001.10751", "submitter": "Maximilian Probst", "authors": "Maximilian Probst Gutenberg, Virginia Vassilevska Williams, Nicole\n  Wein", "title": "New Algorithms and Hardness for Incremental Single-Source Shortest Paths\n  in Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the dynamic Single-Source Shortest Paths (SSSP) problem, we are given a\ngraph $G=(V,E)$ subject to edge insertions and deletions and a source vertex\n$s\\in V$, and the goal is to maintain the distance $d(s,t)$ for all $t\\in V$.\n  Fine-grained complexity has provided strong lower bounds for exact partially\ndynamic SSSP and approximate fully dynamic SSSP [ESA'04, FOCS'14, STOC'15].\nThus much focus has been directed towards finding efficient partially dynamic\n$(1+\\epsilon)$-approximate SSSP algorithms [STOC'14, ICALP'15, SODA'14,\nFOCS'14, STOC'16, SODA'17, ICALP'17, ICALP'19, STOC'19, SODA'20, SODA'20].\nDespite this rich literature, for directed graphs there are no known\ndeterministic algorithms for $(1+\\epsilon)$-approximate dynamic SSSP that\nperform better than the classic ES-tree [JACM'81]. We present the first such\nalgorithm.\n  We present a \\emph{deterministic} data structure for incremental SSSP in\nweighted digraphs with total update time $\\tilde{O}(n^2 \\log W)$ which is\nnear-optimal for very dense graphs; here $W$ is the ratio of the largest weight\nin the graph to the smallest. Our algorithm also improves over the best known\npartially dynamic \\emph{randomized} algorithm for directed SSSP by Henzinger et\nal. [STOC'14, ICALP'15] if $m=\\omega(n^{1.1})$.\n  We also provide improved conditional lower bounds. Henzinger et al. [STOC'15]\nshowed that under the OMv Hypothesis, the partially dynamic exact $s$-$t$\nShortest Path problem in undirected graphs requires amortized update or query\ntime $m^{1/2-o(1)}$, given polynomial preprocessing time. Under a hypothesis\nabout finding Cliques, we improve the update and query lower bound for\nalgorithms with polynomial preprocessing time to $m^{0.626-o(1)}$. Further,\nunder the $k$-Cycle hypothesis, we show that any partially dynamic SSSP\nalgorithm with $O(m^{2-\\epsilon})$ preprocessing time requires amortized update\nor query time $m^{1-o(1)}$.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 10:36:30 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Gutenberg", "Maximilian Probst", ""], ["Williams", "Virginia Vassilevska", ""], ["Wein", "Nicole", ""]]}, {"id": "2001.10801", "submitter": "Maximilian Probst Gutenberg", "authors": "Maximilian Probst Gutenberg, Christian Wulff-Nilsen", "title": "Fully-Dynamic All-Pairs Shortest Paths: Improved Worst-Case Time and\n  Space Bounds", "comments": "Appeared in SODA'20", "journal-ref": null, "doi": "10.1137/1.9781611975994.156", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a directed weighted graph $G=(V,E)$ undergoing vertex insertions\n\\emph{and} deletions, the All-Pairs Shortest Paths (APSP) problem asks to\nmaintain a data structure that processes updates efficiently and returns after\neach update the distance matrix to the current version of $G$. In two\nbreakthrough results, Italiano and Demetrescu [STOC '03] presented an algorithm\nthat requires $\\tilde{O}(n^2)$ \\emph{amortized} update time, and Thorup showed\nin [STOC '05] that \\emph{worst-case} update time $\\tilde{O}(n^{2+3/4})$ can be\nachieved. In this article, we make substantial progress on the problem. We\npresent the following new results:\n  (1) We present the first deterministic data structure that breaks the\n$\\tilde{O}(n^{2+3/4})$ worst-case update time bound by Thorup which has been\nstanding for almost 15 years. We improve the worst-case update time to\n$\\tilde{O}(n^{2+5/7}) = \\tilde{O}(n^{2.71..})$ and to $\\tilde{O}(n^{2+3/5}) =\n\\tilde{O}(n^{2.6})$ for unweighted graphs.\n  (2) We present a simple deterministic algorithm with $\\tilde{O}(n^{2+3/4})$\nworst-case update time ($\\tilde{O}(n^{2+2/3})$ for unweighted graphs), and a\nsimple Las-Vegas algorithm with worst-case update time $\\tilde{O}(n^{2+2/3})$\n($\\tilde{O}(n^{2 + 1/2})$ for unweighted graphs) that works against a\nnon-oblivious adversary. Both data structures require space $\\tilde{O}(n^2)$.\nThese are the first exact dynamic algorithms with truly-subcubic update time\n\\emph{and} space usage. This makes significant progress on an open question\nposed in multiple articles [COCOON'01, STOC '03, ICALP'04, Encyclopedia of\nAlgorithms '08] and is critical to algorithms in practice [TALG '06] where\nlarge space usage is prohibitive. Moreover, they match the worst-case update\ntime of the best previous algorithms and the second algorithm improves upon a\nMonte-Carlo algorithm in a weaker adversary model with the same running time\n[SODA '17].\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 13:16:15 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 09:46:12 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Gutenberg", "Maximilian Probst", ""], ["Wulff-Nilsen", "Christian", ""]]}, {"id": "2001.10809", "submitter": "Maximilian Probst Gutenberg", "authors": "Maximilian Probst Gutenberg, Christian Wulff-Nilsen", "title": "Deterministic Algorithms for Decremental Approximate Shortest Paths:\n  Faster and Simpler", "comments": "Appeared in SODA'20", "journal-ref": null, "doi": "10.1137/1.9781611975994.154", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the decremental $(1+\\epsilon)$-approximate Single-Source Shortest Path\n(SSSP) problem, we are given a graph $G=(V,E)$ with $n = |V|, m = |E|$,\nundergoing edge deletions, and a distinguished source $s \\in V$, and we are\nasked to process edge deletions efficiently and answer queries for distance\nestimates $\\widetilde{\\mathbf{dist}}_G(s,v)$ for each $v \\in V$, at any stage,\nsuch that $\\mathbf{dist}_G(s,v) \\leq \\widetilde{\\mathbf{dist}}_G(s,v) \\leq (1+\n\\epsilon)\\mathbf{dist}_G(s,v)$. In the decremental $(1+\\epsilon)$-approximate\nAll-Pairs Shortest Path (APSP) problem, we are asked to answer queries for\ndistance estimates $\\widetilde{\\mathbf{dist}}_G(u,v)$ for every $u,v \\in V$. In\nthis article, we consider the problems for undirected, unweighted graphs.\n  We present a new \\emph{deterministic} algorithm for the decremental\n$(1+\\epsilon)$-approximate SSSP problem that takes total update time $O(mn^{0.5\n+ o(1)})$. Our algorithm improves on the currently best algorithm for dense\ngraphs by Chechik and Bernstein [STOC 2016] with total update time\n$\\tilde{O}(n^2)$ and the best existing algorithm for sparse graphs with running\ntime $\\tilde{O}(n^{1.25}\\sqrt{m})$ [SODA 2017] whenever $m = O(n^{1.5 -\no(1)})$.\n  In order to obtain this new algorithm, we develop several new techniques\nincluding improved decremental cover data structures for graphs, a more\nefficient notion of the heavy/light decomposition framework introduced by\nChechik and Bernstein and the first clustering technique to maintain a dynamic\n\\emph{sparse} emulator in the deterministic setting.\n  As a by-product, we also obtain a new simple deterministic algorithm for the\ndecremental $(1+\\epsilon)$-approximate APSP problem with near-optimal total\nrunning time $\\tilde{O}(mn /\\epsilon)$ matching the time complexity of the\nsophisticated but rather involved algorithm by Henzinger, Forster and Nanongkai\n[FOCS 2013].\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 13:22:22 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Gutenberg", "Maximilian Probst", ""], ["Wulff-Nilsen", "Christian", ""]]}, {"id": "2001.10821", "submitter": "Maximilian Probst Gutenberg", "authors": "Maximilian Probst Gutenberg, Christian Wulff-Nilsen", "title": "Decremental SSSP in Weighted Digraphs: Faster and Against an Adaptive\n  Adversary", "comments": "Appeared at SODA'20", "journal-ref": null, "doi": "10.1137/1.9781611975994.155", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a dynamic digraph $G = (V,E)$ undergoing edge deletions and given $s\\in\nV$ and $\\epsilon>0$, we consider the problem of maintaining\n$(1+\\epsilon)$-approximate shortest path distances from $s$ to all vertices in\n$G$ over the sequence of deletions. Even and Shiloach (J.~ACM'$81$) give a\ndeterministic data structure for the exact version of the problem with total\nupdate time $O(mn)$. Henzinger et al. (STOC'$14$, ICALP'$15$) give a Monte\nCarlo data structure for the approximate version with improved total update\ntime $ O(mn^{0.9 + o(1)}\\log W)$ where $W$ is the ratio between the largest and\nsmallest edge weight. A drawback of their data structure is that they only work\nagainst an oblivious adversary, meaning that the sequence of deletions needs to\nbe fixed in advance. This limits its application as a black box inside\nalgorithms. We present the following $(1+\\epsilon)$-approximate data\nstructures:\n  (1) the first data structure is Las Vegas and works against an adaptive\nadversary; it has total expected update time $\\tilde O(m^{2/3}n^{4/3})$ for\nunweighted graphs and $\\tilde O(m^{3/4}n^{5/4}\\log W)$ for weighted graphs,\n  (2) the second data structure is Las Vegas and assumes an oblivious\nadversary; it has total expected update time $\\tilde O(\\sqrt m n^{3/2})$ for\nunweighted graphs and $\\tilde O(m^{2/3}n^{4/3}\\log W)$ for weighted graphs,\n  (3) the third data structure is Monte Carlo and is correct w.h.p.~against an\noblivious adversary; it has total expected update time $\\tilde O((mn)^{7/8}\\log\nW) = \\tilde O(mn^{3/4}\\log W)$.\n  Each of our data structures can be queried at any stage of $G$ in constant\nworst-case time; if the adversary is oblivious, a query can be extended to also\nreport such a path in time proportional to its length. Our update times are\nfaster than those of Henzinger et al.~for all graph densities.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 13:31:06 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Gutenberg", "Maximilian Probst", ""], ["Wulff-Nilsen", "Christian", ""]]}, {"id": "2001.10875", "submitter": "Frances Cooper", "authors": "Frances Cooper and David Manlove", "title": "Algorithms for new types of fair stable matchings", "comments": "12 page paper, 1 page of references, 16 pages of appendices, 7\n  figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding \"fair\" stable matchings in the Stable\nMarriage problem with Incomplete lists (SMI). For an instance $I$ of SMI there\nmay be many stable matchings, providing significantly different outcomes for\nthe sets of men and women. We introduce two new notions of fairness in SMI.\nFirstly, a regret-equal stable matching minimises the difference in ranks of a\nworst-off man and a worst-off woman, among all stable matchings. Secondly, a\nmin-regret sum stable matching minimises the sum of ranks of a worst-off man\nand a worst-off woman, among all stable matchings. We present two new efficient\nalgorithms to find stable matchings of these types. Firstly, the Regret-Equal\nDegree Iteration Algorithm finds a regret-equal stable matching in $O(d_0 nm)$\ntime, where $d_0$ is the absolute difference in ranks between a worst-off man\nand a worst-off woman in the man-optimal stable matching, $n$ is the number of\nmen or women, and $m$ is the total length of all preference lists. Secondly,\nthe Min-Regret Sum Algorithm finds a min-regret sum stable matching in $O(d_s\nm)$ time, where $d_s$ is the difference in the ranks between a worst-off man in\neach of the woman-optimal and man-optimal stable matchings. Experiments to\ncompare several types of fair optimal stable matchings were conducted and show\nthat the Regret-Equal Degree Iteration Algorithm produces matchings that are\ncompetitive with respect to other fairness objectives. On the other hand,\nexisting types of \"fair\" stable matchings did not provide as close an\napproximation to regret-equal stable matchings.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 14:54:51 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 07:53:27 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2020 13:38:36 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2020 16:03:10 GMT"}, {"version": "v5", "created": "Thu, 16 Apr 2020 13:19:04 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Cooper", "Frances", ""], ["Manlove", "David", ""]]}, {"id": "2001.11070", "submitter": "Amir Kafshdar Goharshady", "authors": "Krishnendu Chatterjee, Amir Kafshdar Goharshady, Rasmus Ibsen-Jensen,\n  Andreas Pavlogiannis", "title": "Optimal and Perfectly Parallel Algorithms for On-demand Data-flow\n  Analysis", "comments": "A conference version appeared in ESOP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interprocedural data-flow analyses form an expressive and useful paradigm of\nnumerous static analysis applications, such as live variables analysis, alias\nanalysis and null pointers analysis. The most widely-used framework for\ninterprocedural data-flow analysis is IFDS, which encompasses distributive\ndata-flow functions over a finite domain. On-demand data-flow analyses restrict\nthe focus of the analysis on specific program locations and data facts. This\nsetting provides a natural split between (i) an offline (or preprocessing)\nphase, where the program is partially analyzed and analysis summaries are\ncreated, and (ii) an online (or query) phase, where analysis queries arrive on\ndemand and the summaries are used to speed up answering queries.\n  In this work, we consider on-demand IFDS analyses where the queries concern\nprogram locations of the same procedure (aka same-context queries). We exploit\nthe fact that flow graphs of programs have low treewidth to develop faster\nalgorithms that are space and time optimal for many common data-flow analyses,\nin both the preprocessing and the query phase. We also use treewidth to develop\nquery solutions that are embarrassingly parallelizable, i.e. the total work for\nanswering each query is split to a number of threads such that each thread\nperforms only a constant amount of work. Finally, we implement a static\nanalyzer based on our algorithms, and perform a series of on-demand analysis\nexperiments on standard benchmarks. Our experimental results show a drastic\nspeed-up of the queries after only a lightweight preprocessing phase, which\nsignificantly outperforms existing techniques.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 19:54:52 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 18:05:35 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Goharshady", "Amir Kafshdar", ""], ["Ibsen-Jensen", "Rasmus", ""], ["Pavlogiannis", "Andreas", ""]]}, {"id": "2001.11570", "submitter": "Luiz Silva", "authors": "L. A. G. Silva, L. A. B. Kowada, N. R. Rocco, M. E. M. T. Walter", "title": "An algebraic 1.375-approximation algorithm for the Transposition\n  Distance Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genome rearrangements, the mutational event transposition swaps two\nadjacent blocks of genes in one chromosome. The Transposition Distance Problem\n(TDP) aims to find the minimum number of transpositions required to transform\none chromosome into another, both represented as permutations.\n  TDP is $\\mathcal{NP}$-hard and the best approximation algorithm with a\n$1.375$ ratio was proposed in 2006 by Elias and Hartman. Their algorithm\nemploys simplification, a technique used to transform an input permutation\n$\\pi$ into a simple permutation $\\hat{\\pi}$, presumably easier to handle with.\n$\\hat{\\pi}$ is obtained by inserting new symbols into $\\pi$ in a way that the\nlower bound of the transposition distance of $\\pi$ is kept on $\\hat{\\pi}$. A\nsequence of transpositions sorting $\\hat{\\pi}$ can be mimicked to sort $\\pi$.\n  First, we show that the algorithm of Elias and Hartman may require one extra\ntransposition above the approximation ratio of $1.375$, depending on how the\ninput permutation is simplified. Next, using an algebraic formalism, a new\nupper bound for the transposition distance is proposed. From this result, a new\n$1.375$-approximation algorithm is proposed to solve TDP skipping\nsimplification and ensuring the approximation ratio of $1.375$ for all the\npermutations in the Symmetric Group $S_n$.\n  Implementations of our algorithm and of Elias and Hartman were tested against\nshort permutations of maximum length $12$. The results show that our algorithm,\nin addition to keeping the approximation below the $1.375$ ratio, outperforms\nthe algorithm of Elias and Hartman in relation to the rate of the correct\nanswers, i.e., the computed distances that are equal to the transposition\ndistance.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 21:21:06 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 18:43:08 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 12:43:58 GMT"}, {"version": "v4", "created": "Mon, 12 Oct 2020 10:01:18 GMT"}, {"version": "v5", "created": "Mon, 1 Feb 2021 14:35:25 GMT"}, {"version": "v6", "created": "Tue, 2 Feb 2021 21:03:25 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Silva", "L. A. G.", ""], ["Kowada", "L. A. B.", ""], ["Rocco", "N. R.", ""], ["Walter", "M. E. M. T.", ""]]}, {"id": "2001.11607", "submitter": "Oliver Serang", "authors": "Oliver Serang", "title": "Optimally selecting the top $k$ values from $X+Y$ with layer-ordered\n  heaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selection and sorting the Cartesian sum, $X+Y$, are classic and important\nproblems. Here, a new algorithm is presented, which generates the top $k$\nvalues of the form $X_i+Y_j$. The algorithm relies only on median-of-medians\nand is simple to implement. Furthermore, it uses data structures contiguous in\nmemory, and is fast in practice. The presented algorithm is demonstrated to be\ntheoretically optimal.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 23:38:04 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 19:57:04 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Serang", "Oliver", ""]]}, {"id": "2001.11932", "submitter": "Nina Mesing Stausholm", "authors": "Rasmus Pagh and Nina Mesing Stausholm", "title": "Efficient Differentially Private $F_0$ Linear Sketching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A powerful feature of linear sketches is that from sketches of two data\nvectors, one can compute the sketch of the difference between the vectors. This\nallows us to answer fine-grained questions about the difference between two\ndata sets. In this work, we consider how to construct sketches for weighted\n$F_0$, i.e., the summed weights of the elements in the data set, that are\nsmall, differentially private, and computationally efficient. Let a weight\nvector $w\\in(0,1]^u$ be given. For $x\\in\\{0,1\\}^u$ we are interested in\nestimating $\\Vert x\\circ w\\Vert_1$ where $\\circ$ is the Hadamard product\n(entrywise product). Building on a technique of Kushilevitz et al.~(STOC 1998),\nwe introduce a sketch (depending on $w$) that is linear over GF(2), mapping a\nvector $x\\in \\{0,1\\}^u$ to $Hx\\in\\{0,1\\}^\\tau$ for a matrix $H$ sampled from a\nsuitable distribution $\\mathcal{H}$. Differential privacy is achieved by using\nrandomized response, flipping each bit of $Hx$ with probability $p<1/2$. We\nshow that for every choice of $0<\\beta < 1$ and $\\varepsilon=O(1)$ there exists\n$p<1/2$ and a distribution $\\mathcal{H}$ of linear sketches of size $\\tau =\nO(\\log^2(u)\\varepsilon^{-2}\\beta^{-2})$ such that: 1) For random\n$H\\sim\\mathcal{H}$ and noise vector $\\varphi$, given $Hx + \\varphi$ we can\ncompute an estimate of $\\Vert x\\circ w\\Vert_1$ that is accurate within a factor\n$1\\pm\\beta$, plus additive error $O(\\log(u)\\varepsilon^{-2}\\beta^{-2})$, with\nprobability $1-1/u$, and 2) For every $H\\sim\\mathcal{H}$, $Hx + \\varphi$ is\n$\\varepsilon$-differentially private over the randomness in $\\varphi$. The\nspecial case $w=(1,\\dots,1)$ is unweighted $F_0$. Our results both improve the\nefficiency of existing methods for unweighted $F_0$ estimating and extend to a\nweighted generalization. We also give a distributed streaming implementation\nfor estimating the size of the union between two input streams.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 16:18:27 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 09:14:10 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 12:33:12 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Pagh", "Rasmus", ""], ["Stausholm", "Nina Mesing", ""]]}, {"id": "2001.11959", "submitter": "Bruno Grenet", "authors": "Pascal Giorgi, Bruno Grenet, Armelle Perret du Cray", "title": "Essentially Optimal Sparse Polynomial Multiplication", "comments": "12 pages", "journal-ref": "Proc. ISSAC'20, pp 202-209, ACM, 2020", "doi": "10.1145/3373207.3404026", "report-no": null, "categories": "cs.SC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic algorithm to compute the product of two univariate\nsparse polynomials over a field with a number of bit operations that is\nquasi-linear in the size of the input and the output. Our algorithm works for\nany field of characteristic zero or larger than the degree. We mainly rely on\nsparse interpolation and on a new algorithm for verifying a sparse product that\nhas also a quasi-linear time complexity. Using Kronecker substitution\ntechniques we extend our result to the multivariate case.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 17:23:45 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 09:15:22 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Giorgi", "Pascal", ""], ["Grenet", "Bruno", ""], ["Cray", "Armelle Perret du", ""]]}, {"id": "2001.11997", "submitter": "Mathieu Mari", "authors": "Piotr Krysta, Mathieu Mari, Nan Zhi", "title": "Ultimate greedy approximation of independent sets in subcubic graphs", "comments": "Preliminary version of this paper has been published in the\n  proceedings of ACM-SIAM SODA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the approximability of the maximum size independent set (MIS)\nproblem in bounded degree graphs. This is one of the most classic and widely\nstudied NP-hard optimization problems. We focus on the well known minimum\ndegree greedy algorithm for this problem. This algorithm iteratively chooses a\nminimum degree vertex in the graph, adds it to the solution and removes its\nneighbors, until the remaining graph is empty. The approximation ratios of this\nalgorithm have been very widely studied, where it is augmented with an advice\nthat tells the greedy which minimum degree vertex to choose if it is not\nunique. Our main contribution is a new mathematical theory for the design of\nsuch greedy algorithms with efficiently computable advice and for the analysis\nof their approximation ratios. With this new theory we obtain the ultimate\napproximation ratio of 5/4 for greedy on graphs with maximum degree 3, which\ncompletely solves the open problem from the paper by Halldorsson and Yoshihara\n(1995). Our algorithm is the fastest currently known algorithm with this\napproximation ratio on such graphs. We apply our new algorithm to the minimum\nvertex cover problem on graphs with maximum degree 3 to obtain a substantially\nfaster 6/5-approximation algorithm than the one currently known. We complement\nour positive, upper bound results with negative, lower bound results which\nprove that the problem of designing good advice for greedy is computationally\nhard and even hard to approximate on various classes of graphs. These results\nsignificantly improve on such previously known hardness results. Moreover,\nthese results suggest that obtaining the upper bound results on the design and\nanalysis of greedy advice is non-trivial.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 18:39:58 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Krysta", "Piotr", ""], ["Mari", "Mathieu", ""], ["Zhi", "Nan", ""]]}]