[{"id": "1607.00138", "submitter": "Noemi Passing", "authors": "Tobias Marschall, Noemi E. Passing", "title": "Representing Pattern Matching Algorithms by Polynomial-Size Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern matching algorithms to find exact occurrences of a pattern\n$S\\in\\Sigma^m$ in a text $T\\in\\Sigma^n$ have been analyzed extensively with\nrespect to asymptotic best, worst, and average case runtime. For more detailed\nanalyses, the number of text character accesses $X^{\\mathcal{A},S}_n$ performed\nby an algorithm $\\mathcal{A}$ when searching a random text of length $n$ for a\nfixed pattern $S$ has been considered. Constructing a state space and\ncorresponding transition rules (e.g. in a Markov chain) that reflect the\nbehavior of a pattern matching algorithm is a key step in existing analyses of\n$X^{\\mathcal{A},S}_n$ in both the asymptotic ($n\\to\\infty$) and the\nnon-asymptotic regime. The size of this state space is hence a crucial\nparameter for such analyses. In this paper, we introduce a general methodology\nto construct corresponding state spaces and demonstrate that it applies to a\nwide range of algorithms, including Boyer-Moore (BM), Boyer-Moore-Horspool\n(BMH), Backward Oracle Matching (BOM), and Backward (Non-Deterministic) DAWG\nMatching (B(N)DM). In all cases except BOM, our method leads to state spaces of\nsize $O(m^3)$ for pattern length $m$, a result that has previously only been\nobtained for BMH. In all other cases, only state spaces with size exponential\nin $m$ had been reported. Our results immediately imply an algorithm to compute\nthe distribution of $X^{\\mathcal{A},S}_n$ for fixed $S$, fixed $n$, and\n$\\mathcal{A}\\in\\{\\text{BM},\\text{BMH},\\text{B(N)DM}\\}$ in polynomial time for a\nvery general class of random text models.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 07:44:07 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Marschall", "Tobias", ""], ["Passing", "Noemi E.", ""]]}, {"id": "1607.00208", "submitter": "Thiagarajan Hema", "authors": "T. Hema and K.S. Easwarakumar", "title": "An Optimal Algorithm for Range Search on Multidimensional Points", "comments": null, "journal-ref": "Asian Journal of Information Technology, 15:11,1723-1730, 2016", "doi": "10.3923/ajit.2016.1723.1730", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper proposes an efficient and novel method to address range search on\nmultidimensional points in $\\theta(t)$ time, where $t$ is the number of points\nreported in $\\Re^k$ space. This is accomplished by introducing a new data\nstructure, called BITS $k$d-tree. This structure also supports fast updation\nthat takes $\\theta(1)$ time for insertion and $O(\\log n)$ time for deletion.\nThe earlier best known algorithm for this problem is $O(\\log^k n+t)$ time in\nthe pointer machine model.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 11:36:17 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Hema", "T.", ""], ["Easwarakumar", "K. S.", ""]]}, {"id": "1607.00507", "submitter": "Vladislav Shchukin", "authors": "A.G. D'yachkov, I.V. Vorobyev, N.A. Polyanskii, V.Yu. Shchukin", "title": "Adaptive Learning a Hidden Hypergraph", "comments": "ACCT 2016, 6 pages. arXiv admin note: text overlap with\n  arXiv:1601.06705", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a hidden hypergraph is a natural generalization of the classical\ngroup testing problem that consists in detecting unknown hypergraph\n$H_{un}=H(V,E)$ by carrying out edge-detecting tests. In the given paper we\nfocus our attention only on a specific family $\\mathcal{F}(t,s,\\ell)$ of\nlocalized hypergraphs for which the total number of vertices $|V| = t$, the\nnumber of edges $|E|\\le s$, $s\\ll t$, and the cardinality of any edge\n$|e|\\le\\ell$, $\\ell\\ll t$. Our goal is to identify all edges of $H_{un}\\in\n\\mathcal{F}(t,s,\\ell)$ by using the minimal number of tests. We provide an\nadaptive algorithm that matches the information theory bound, i.e., the total\nnumber of tests of the algorithm in the worst case is at most $s\\ell\\log_2\nt(1+o(1))$.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 13:26:08 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["D'yachkov", "A. G.", ""], ["Vorobyev", "I. V.", ""], ["Polyanskii", "N. A.", ""], ["Shchukin", "V. Yu.", ""]]}, {"id": "1607.00854", "submitter": "Thomas Rothvoss", "authors": "Thomas Rothvoss", "title": "Lecture Notes on the ARV Algorithm for Sparsest Cut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the landmarks in approximation algorithms is the $O(\\sqrt{\\log\nn})$-approximation algorithm for the Uniform Sparsest Cut problem by Arora, Rao\nand Vazirani from 2004. The algorithm is based on a semidefinite program that\nfinds an embedding of the nodes respecting the triangle inequality. Their core\nargument shows that a random hyperplane approach will find two large sets of\n$\\Theta(n)$ many nodes each that have a distance of $\\Theta(1/\\sqrt{\\log n})$\nto each other if measured in terms of $\\|\\cdot \\|_2^2$.\n  Here we give a detailed set of lecture notes describing the algorithm. For\nthe proof of the Structure Theorem we use a cleaner argument based on expected\nmaxima over $k$-neighborhoods that significantly simplifies the analysis.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 12:30:15 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Rothvoss", "Thomas", ""]]}, {"id": "1607.01162", "submitter": "Yixin Cao", "authors": "Yuping Ke and Yixin Cao and Xiating Ouyang and Jianxin Wang", "title": "Unit Interval Vertex Deletion: Fewer Vertices are Relevant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unit interval vertex deletion problem asks for a set of at most $k$\nvertices whose deletion from an $n$-vertex graph makes it a unit interval\ngraph. We develop an $O(k^4)$-vertex kernel for the problem, significantly\nimproving the $O(k^{53})$-vertex kernel of Fomin, Saurabh, and Villanger\n[ESA'12; SIAM J. Discrete Math 27(2013)]. We introduce a novel way of\norganizing cliques of a unit interval graph. Our constructive proof for the\ncorrectness of our algorithm, using interval models, greatly simplifies the\ndestructive proofs, based on forbidden induced subgraphs, for similar problems\nin literature.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 09:18:40 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Ke", "Yuping", ""], ["Cao", "Yixin", ""], ["Ouyang", "Xiating", ""], ["Wang", "Jianxin", ""]]}, {"id": "1607.01167", "submitter": "Guus Regts", "authors": "Viresh Patel, Guus Regts", "title": "Deterministic polynomial-time approximation algorithms for partition\n  functions and graph polynomials", "comments": "27 pages; some changes have been made based on referee comments. In\n  particular a tiny error in Proposition 4.4 has been fixed. The introduction\n  and concluding remarks have also been rewritten to incorporate the most\n  recent developments. Accepted for publication in SIAM Journal on Computation", "journal-ref": "SIAM J. Comput., 46(6), 1893-1919", "doi": "10.1137/16M1101003", "report-no": null, "categories": "math.CO cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show a new way of constructing deterministic polynomial-time\napproximation algorithms for computing complex-valued evaluations of a large\nclass of graph polynomials on bounded degree graphs. In particular, our\napproach works for the Tutte polynomial and independence polynomial, as well as\npartition functions of complex-valued spin and edge-coloring models.\n  More specifically, we define a large class of graph polynomials $\\mathcal C$\nand show that if $p\\in \\cal C$ and there is a disk $D$ centered at zero in the\ncomplex plane such that $p(G)$ does not vanish on $D$ for all bounded degree\ngraphs $G$, then for each $z$ in the interior of $D$ there exists a\ndeterministic polynomial-time approximation algorithm for evaluating $p(G)$ at\n$z$. This gives an explicit connection between absence of zeros of graph\npolynomials and the existence of efficient approximation algorithms, allowing\nus to show new relationships between well-known conjectures.\n  Our work builds on a recent line of work initiated by. Barvinok, which\nprovides a new algorithmic approach besides the existing Markov chain Monte\nCarlo method and the correlation decay method for these types of problems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 09:34:32 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 17:24:22 GMT"}, {"version": "v3", "created": "Tue, 18 Jul 2017 11:20:32 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Patel", "Viresh", ""], ["Regts", "Guus", ""]]}, {"id": "1607.01229", "submitter": "Sandy Heydrich", "authors": "David Blitz, Sandy Heydrich, Rob van Stee, Andr\\'e van Vliet, Gerhard\n  J. Woeginger", "title": "Improved Lower Bounds for Online Hypercube and Rectangle Packing", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Packing a given sequence of items into as few bins as possible in an online\nfashion is a widely studied problem. We improve lower bounds for packing boxes\ninto bins in two or more dimensions, both for general algorithms for squares\nand rectangles (in two dimensions) and for an important subclass, so-called\nHarmonic-type algorithms for hypercubes (in two or more dimensions). Lastly, we\nshow that two adaptions of ideas from a one-dimensional packing algorithm to\nsquare packing do not help to break the barrier of 2.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 12:51:03 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 09:09:15 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Blitz", "David", ""], ["Heydrich", "Sandy", ""], ["van Stee", "Rob", ""], ["van Vliet", "Andr\u00e9", ""], ["Woeginger", "Gerhard J.", ""]]}, {"id": "1607.01299", "submitter": "Sascha Witt", "authors": "Sascha Witt", "title": "Trip-Based Public Transit Routing Using Condensed Search Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of planning Pareto-optimal journeys in public transit\nnetworks. Most existing algorithms and speed-up techniques work by computing\nsubjourneys to intermediary stops until the destination is reached. In\ncontrast, the trip-based model focuses on trips and transfers between them,\nconstructing journeys as a sequence of trips. In this paper, we develop a\nspeed-up technique for this model inspired by principles behind existing\nstate-of-the-art speed-up techniques, Transfer Pattern and Hub Labelling. The\nresulting algorithm allows us to compute Pareto-optimal (with respect to\narrival time and number of transfers) 24-hour profiles on very large real-world\nnetworks in less than half a millisecond. Compared to the current state of the\nart for bicriteria queries on public transit networks, this is up to two orders\nof magnitude faster, while increasing preprocessing overhead by at most one\norder of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 15:32:06 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 13:39:34 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Witt", "Sascha", ""]]}, {"id": "1607.01551", "submitter": "Tarun Kathuria", "authors": "Tarun Kathuria, Amit Deshpande", "title": "On Sampling and Greedy MAP Inference of Constrained Determinantal Point\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subset selection problems ask for a small, diverse yet representative subset\nof the given data. When pairwise similarities are captured by a kernel, the\ndeterminants of submatrices provide a measure of diversity or independence of\nitems within a subset. Matroid theory gives another notion of independence,\nthus giving rise to optimization and sampling questions about Determinantal\nPoint Processes (DPPs) under matroid constraints. Partition constraints, as a\nspecial case, arise naturally when incorporating additional labeling or\nclustering information, besides the kernel, in DPPs. Finding the maximum\ndeterminant submatrix under matroid constraints on its row/column indices has\nbeen previously studied. However, the corresponding question of sampling from\nDPPs under matroid constraints has been unresolved, beyond the simple\ncardinality constrained k-DPPs. We give the first polynomial time algorithm to\nsample exactly from DPPs under partition constraints, for any constant number\nof partitions. We complement this by a complexity theoretic barrier that rules\nout such a result under general matroid constraints. Our experiments indicate\nthat partition-constrained DPPs offer more flexibility and more diversity than\nk-DPPs and their naive extensions, while being reasonably efficient in running\ntime. We also show that a simple greedy initialization followed by local search\ngives improved approximation guarantees for the problem of MAP inference from\nk- DPPs on well-conditioned kernels. Our experiments show that this improvement\nis significant for larger values of k, supporting our theoretical result.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 10:40:23 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Kathuria", "Tarun", ""], ["Deshpande", "Amit", ""]]}, {"id": "1607.01657", "submitter": "Barun Gorain", "authors": "Barun Gorain and Andrzej Pelc", "title": "Deterministic Graph Exploration with Advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of graph exploration. An $n$-node graph has unlabeled\nnodes, and all ports at any node of degree $d$ are arbitrarily numbered\n$0,\\dots, d-1$. A mobile agent has to visit all nodes and stop. The exploration\ntime is the number of edge traversals. We consider the problem of how much\nknowledge the agent has to have a priori, in order to explore the graph in a\ngiven time, using a deterministic algorithm. This a priori information (advice)\nis provided to the agent by an oracle, in the form of a binary string, whose\nlength is called the size of advice. We consider two types of oracles. The\ninstance oracle knows the entire instance of the exploration problem, i.e., the\nport-numbered map of the graph and the starting node of the agent in this map.\nThe map oracle knows the port-numbered map of the graph but does not know the\nstarting node of the agent.\n  We first consider exploration in polynomial time, and determine the exact\nminimum size of advice to achieve it. This size is $\\log\\log\\log n -\\Theta(1)$,\nfor both types of oracles.\n  When advice is large, there are two natural time thresholds: $\\Theta(n^2)$\nfor a map oracle, and $\\Theta(n)$ for an instance oracle, that can be achieved\nwith sufficiently large advice. We show that, with a map oracle, time\n$\\Theta(n^2)$ cannot be improved in general, regardless of the size of advice.\nWe also show that the smallest size of advice to achieve this time is larger\nthan $n^\\delta$, for any $\\delta <1/3$.\n  For an instance oracle, advice of size $O(n\\log n)$ is enough to achieve time\n$O(n)$. We show that, with any advice of size $o(n\\log n)$, the time of\nexploration must be at least $n^\\epsilon$, for any $\\epsilon <2$, and with any\nadvice of size $O(n)$, the time must be $\\Omega(n^2)$.\n  We also investigate minimum advice sufficient for fast exploration of\nhamiltonian graphs.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 15:06:13 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 02:00:33 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Gorain", "Barun", ""], ["Pelc", "Andrzej", ""]]}, {"id": "1607.01718", "submitter": "Justin Eldridge", "authors": "Justin Eldridge, Mikhail Belkin, Yusu Wang", "title": "Graphons, mergeons, and so on!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop a theory of hierarchical clustering for graphs. Our\nmodeling assumption is that graphs are sampled from a graphon, which is a\npowerful and general model for generating graphs and analyzing large networks.\nGraphons are a far richer class of graph models than stochastic blockmodels,\nthe primary setting for recent progress in the statistical theory of graph\nclustering. We define what it means for an algorithm to produce the \"correct\"\nclustering, give sufficient conditions in which a method is statistically\nconsistent, and provide an explicit algorithm satisfying these properties.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 17:35:55 GMT"}, {"version": "v2", "created": "Sat, 29 Oct 2016 01:05:41 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2016 01:03:26 GMT"}, {"version": "v4", "created": "Mon, 22 May 2017 20:07:27 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Eldridge", "Justin", ""], ["Belkin", "Mikhail", ""], ["Wang", "Yusu", ""]]}, {"id": "1607.01842", "submitter": "Barak Shani", "authors": "Steven D. Galbraith, Joel Laity and Barak Shani", "title": "Finding Significant Fourier Coefficients: Clarifications,\n  Simplifications, Applications and Limitations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ideas from Fourier analysis have been used in cryptography for the last three\ndecades. Akavia, Goldwasser and Safra unified some of these ideas to give a\ncomplete algorithm that finds significant Fourier coefficients of functions on\nany finite abelian group. Their algorithm stimulated a lot of interest in the\ncryptography community, especially in the context of `bit security'. This\nmanuscript attempts to be a friendly and comprehensive guide to the tools and\nresults in this field. The intended readership is cryptographers who have heard\nabout these tools and seek an understanding of their mechanics and their\nusefulness and limitations. A compact overview of the algorithm is presented\nwith emphasis on the ideas behind it. We show how these ideas can be extended\nto a `modulus-switching' variant of the algorithm. We survey some applications\nof this algorithm, and explain that several results should be taken in the\nright context. In particular, we point out that some of the most important bit\nsecurity problems are still open. Our original contributions include: a\ndiscussion of the limitations on the usefulness of these tools; an answer to an\nopen question about the modular inversion hidden number problem.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 23:54:40 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 02:15:41 GMT"}, {"version": "v3", "created": "Tue, 31 Jan 2017 13:21:49 GMT"}, {"version": "v4", "created": "Thu, 13 Dec 2018 15:26:07 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Galbraith", "Steven D.", ""], ["Laity", "Joel", ""], ["Shani", "Barak", ""]]}, {"id": "1607.01993", "submitter": "Nikos Gorogiannis", "authors": "James Brotherston, Nikos Gorogiannis and Max Kanovich", "title": "Biabduction (and Related Problems) in Array Separation Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate array separation logic (ASL), a variant of symbolic-heap\nseparation logic in which the data structures are either pointers or arrays,\ni.e., contiguous blocks of allocated memory. This logic provides a language for\ncompositional memory safety proofs of imperative array programs.\n  We focus on the biabduction problem for this logic, which has been\nestablished as the key to automatic specification inference at the industrial\nscale. We present an NP decision procedure for biabduction in ASL that produces\nsolutions of reasonable quality, and we also show that the problem of finding a\nconsistent solution is NP-hard.\n  Along the way, we study satisfiability and entailment in our logic, giving\ndecision procedures and complexity bounds for both problems. We show\nsatisfiability to be NP-complete, and entailment to be decidable with high\ncomplexity. The somewhat surprising fact that biabduction is much simpler than\nentailment is explained by the fact that, as we show, the element of choice\nover biabduction solutions enables us to dramatically reduce the search space.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 12:49:04 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 21:44:27 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2016 11:20:20 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Brotherston", "James", ""], ["Gorogiannis", "Nikos", ""], ["Kanovich", "Max", ""]]}, {"id": "1607.02096", "submitter": "Christos Giatsidis", "authors": "Christos Giatsidis, Fragkiskos D. Malliaros, Nikolaos Tziortziotis,\n  Charanpal Dhanjal, Emmanouil Kiagias, Dimitrios M. Thilikos, Michalis\n  Vazirgiannis", "title": "A k-core Decomposition Framework for Graph Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graph clustering or community detection constitutes an important task for\ninvestigating the internal structure of graphs, with a plethora of applications\nin several domains. Traditional techniques for graph clustering, such as\nspectral methods, typically suffer from high time and space complexity. In this\narticle, we present CoreCluster, an efficient graph clustering framework based\non the concept of graph degeneracy, that can be used along with any known graph\nclustering algorithm. Our approach capitalizes on processing the graph in an\nhierarchical manner provided by its core expansion sequence, an ordered\npartition of the graph into different levels according to the k-core\ndecomposition. Such a partition provides an efficient way to process the graph\nin an incremental manner that preserves its clustering structure, while making\nthe execution of the chosen clustering algorithm much faster due to the smaller\nsize of the graph's partitions onto which the algorithm operates. An\nexperimental analysis on a multitude of real and synthetic data demonstrates\nthat our approach can be applied to any clustering algorithm accelerating the\nclustering process, while the quality of the clustering structure is preserved\nor even improved.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 17:31:18 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Giatsidis", "Christos", ""], ["Malliaros", "Fragkiskos D.", ""], ["Tziortziotis", "Nikolaos", ""], ["Dhanjal", "Charanpal", ""], ["Kiagias", "Emmanouil", ""], ["Thilikos", "Dimitrios M.", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "1607.02184", "submitter": "David Eppstein", "authors": "David Eppstein", "title": "Maximizing the Sum of Radii of Disjoint Balls or Disks", "comments": "20 pages, 11 figures. A preliminary version of this paper appeared at\n  the 28th Canadian Conference on Computational Geometry, Vancouver, 2016", "journal-ref": "J. Computational Geometry 8 (1): 316-339, 2017", "doi": "10.20382/jocg.v8i1a12", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding nonoverlapping balls with given centers in any metric space,\nmaximizing the sum of radii of the balls, can be expressed as a linear program.\nIts dual linear program expresses the problem of finding a minimum-weight set\nof cycles (allowing 2-cycles) covering all vertices in a complete geometric\ngraph. For points in a Euclidean space of any finite dimension~$d$, with any\nconvex distance function on this space, this graph can be replaced by a sparse\nsubgraph obeying a separator theorem. This graph structure leads to an\nalgorithm for finding the optimum set of balls in time $O(n^{2-1/d})$,\nimproving the $O(n^3)$ time of a naive cycle cover algorithm. As a subroutine,\nwe provide an algorithm for weighted bipartite matching in graphs with\nseparators, which speeds up the best previous algorithm for this problem on\nplanar bipartite graphs from $O(n^{3/2}\\log n)$ to $O(n^{3/2})$ time. We also\nshow how to constrain the balls to all have radius at least a given threshold\nvalue, and how to apply our radius-sum optimization algorithms to the problem\nof embedding a finite metric space into a star metric minimizing the average\ndistance to the hub.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 22:28:34 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 00:47:37 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Eppstein", "David", ""]]}, {"id": "1607.02282", "submitter": "Michael Holzhauser", "authors": "Michael Holzhauser, Sven O. Krumke, Clemens Thielen", "title": "On the Complexity and Approximability of Budget-Constrained Minimum Cost\n  Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the complexity and approximability of the budget-constrained\nminimum cost flow problem, which is an extension of the traditional minimum\ncost flow problem by a second kind of costs associated with each edge, whose\ntotal value in a feasible flow is constrained by a given budget B. This problem\ncan, e.g., be seen as the application of the {\\epsilon}-constraint method to\nthe bicriteria minimum cost flow problem. We show that we can solve the problem\nexactly in weakly polynomial time $O(\\log M \\cdot MCF(m,n,C,U))$, where C, U,\nand M are upper bounds on the largest absolute cost, largest capacity, and\nlargest absolute value of any number occuring in the input, respectively, and\nMCF(m,n,C,U) denotes the complexity of finding a traditional minimum cost flow.\nMoreover, we present two fully polynomial-time approximation schemes for the\nproblem on general graphs and one with an improved running-time for the problem\non acyclic graphs.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 09:22:47 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Holzhauser", "Michael", ""], ["Krumke", "Sven O.", ""], ["Thielen", "Clemens", ""]]}, {"id": "1607.02284", "submitter": "Michael Holzhauser", "authors": "Michael Holzhauser, Sven O. Krumke, Clemens Thielen", "title": "A Network Simplex Method for the Budget-Constrained Minimum Cost Flow\n  Problem", "comments": null, "journal-ref": "European Journal of Operational Research 259:3, pp. 864-872 (2017)", "doi": "10.1016/j.ejor.2016.11.024", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a specialized network simplex algorithm for the budget-constrained\nminimum cost flow problem, which is an extension of the traditional minimum\ncost flow problem by a second kind of costs associated with each edge, whose\ntotal value in a feasible flow is constrained by a given budget B. We present a\nfully combinatorial description of the algorithm that is based on a novel\nincorporation of two kinds of integral node potentials and three kinds of\nreduced costs. We prove optimality criteria and combine two methods that are\ncommonly used to avoid cycling in traditional network simplex algorithms into\nnew techniques that are applicable to our problem. With these techniques and\nour definition of the reduced costs, we are able to prove a pseudo-polynomial\nrunning time of the overall procedure, which can be further improved by\nincorporating Dantzig's pivoting rule. Moreover, we present computational\nresults that compare our procedure with Gurobi.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 09:34:56 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 12:23:30 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Holzhauser", "Michael", ""], ["Krumke", "Sven O.", ""], ["Thielen", "Clemens", ""]]}, {"id": "1607.02347", "submitter": "Giordano Da Lozzo", "authors": "Giordano Da Lozzo and Ignaz Rutter", "title": "On the Complexity of Realizing Facial Cycles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following combinatorial problem. Given a planar graph $G=(V,E)$\nand a set of simple cycles $\\mathcal C$ in $G$, find a planar embedding\n$\\mathcal E$ of $G$ such that the number of cycles in $\\mathcal C$ that bound a\nface in $\\mathcal E$ is maximized. We establish a tight border of tractability\nfor this problem in biconnected planar graphs by giving conditions under which\nthe problem is NP-hard and showing that relaxing any of these conditions makes\nthe problem polynomial-time solvable. Moreover, we give a $2$-approximation\nalgorithm for series-parallel graphs and a $(4+\\varepsilon)$-approximation for\nbiconnected planar graphs.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 12:55:03 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Da Lozzo", "Giordano", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1607.02437", "submitter": "David Adjiashvili", "authors": "David Adjiashvili and Viktor Bindewald and Dennis Michaels", "title": "Robust Assignments via Ear Decompositions and Randomized Rounding", "comments": "Full version of ICALP 2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-life planning problems require making a priori decisions before all\nparameters of the problem have been revealed. An important special case of such\nproblem arises in scheduling problems, where a set of tasks needs to be\nassigned to the available set of machines or personnel (resources), in a way\nthat all tasks have assigned resources, and no two tasks share the same\nresource. In its nominal form, the resulting computational problem becomes the\n\\emph{assignment problem} on general bipartite graphs.\n  This paper deals with a robust variant of the assignment problem modeling\nsituations where certain edges in the corresponding graph are \\emph{vulnerable}\nand may become unavailable after a solution has been chosen. The goal is to\nchoose a minimum-cost collection of edges such that if any vulnerable edge\nbecomes unavailable, the remaining part of the solution contains an assignment\nof all tasks.\n  We present approximation results and hardness proofs for this type of\nproblems, and establish several connections to well-known concepts from\nmatching theory, robust optimization and LP-based techniques.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 16:17:42 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Adjiashvili", "David", ""], ["Bindewald", "Viktor", ""], ["Michaels", "Dennis", ""]]}, {"id": "1607.02725", "submitter": "Bart M. P. Jansen", "authors": "Mark de Berg, Kevin Buchin, Bart M. P. Jansen and Gerhard Woeginger", "title": "Fine-Grained Complexity Analysis of Two Classic TSP Variants", "comments": "Extended abstract appears in the Proceedings of the 43rd\n  International Colloquium on Automata, Languages, and Programming (ICALP 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze two classic variants of the Traveling Salesman Problem using the\ntoolkit of fine-grained complexity. Our first set of results is motivated by\nthe Bitonic TSP problem: given a set of $n$ points in the plane, compute a\nshortest tour consisting of two monotone chains. It is a classic\ndynamic-programming exercise to solve this problem in $O(n^2)$ time. While the\nnear-quadratic dependency of similar dynamic programs for Longest Common\nSubsequence and Discrete Frechet Distance has recently been proven to be\nessentially optimal under the Strong Exponential Time Hypothesis, we show that\nbitonic tours can be found in subquadratic time. More precisely, we present an\nalgorithm that solves bitonic TSP in $O(n \\log^2 n)$ time and its bottleneck\nversion in $O(n \\log^3 n)$ time. Our second set of results concerns the popular\n$k$-OPT heuristic for TSP in the graph setting. More precisely, we study the\n$k$-OPT decision problem, which asks whether a given tour can be improved by a\n$k$-OPT move that replaces $k$ edges in the tour by $k$ new edges. A simple\nalgorithm solves $k$-OPT in $O(n^k)$ time for fixed $k$. For 2-OPT, this is\neasily seen to be optimal. For $k=3$ we prove that an algorithm with a runtime\nof the form $\\tilde{O}(n^{3-\\epsilon})$ exists if and only if All-Pairs\nShortest Paths in weighted digraphs has such an algorithm. The results for\n$k=2,3$ may suggest that the actual time complexity of $k$-OPT is\n$\\Theta(n^k)$. We show that this is not the case, by presenting an algorithm\nthat finds the best $k$-move in $O(n^{\\lfloor 2k/3 \\rfloor + 1})$ time for\nfixed $k \\geq 3$. This implies that 4-OPT can be solved in $O(n^3)$ time,\nmatching the best-known algorithm for 3-OPT. Finally, we show how to beat the\nquadratic barrier for $k=2$ in two important settings, namely for points in the\nplane and when we want to solve 2-OPT repeatedly.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 09:55:53 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["de Berg", "Mark", ""], ["Buchin", "Kevin", ""], ["Jansen", "Bart M. P.", ""], ["Woeginger", "Gerhard", ""]]}, {"id": "1607.02911", "submitter": "Genevieve Simonet", "authors": "Anne Berry and Genevi\\`eve Simonet", "title": "Computing the atom graph of a graph and the union join graph of a\n  hypergraph", "comments": "Submitted in Algorithms on July 11, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The atom graph of a graph is the graph whose vertices are the atoms obtained\nby clique minimal separator decomposition of this graph, and whose edges are\nthe edges of all possible atom trees of this graph. We provide two efficient\nalgorithms for computing this atom graph, with a complexity in $O(min(n^\\alpha\n\\log n, nm, n(n+\\overline{m}))$ time, which is no more than the complexity of\ncomputing the atoms in the general case. %\\par We extend our results to\n$\\alpha$-acyclic hypergraphs. We introduce the notion of union join graph,\nwhich is the union of all possible join trees; we apply our algorithms for atom\ngraphs to efficiently compute union join graphs.\n  Keywords: clique separator decomposition, atom tree, atom graph, clique tree,\nclique graph, $\\alpha$-acyclic hypergraph.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 12:00:41 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Berry", "Anne", ""], ["Simonet", "Genevi\u00e8ve", ""]]}, {"id": "1607.02922", "submitter": "Shamik Ghosh Prof.", "authors": "Sanchita Paul, Shamik Ghosh, Sourav Chakraborty and Malay Sen", "title": "Characterization and recognition of proper tagged probe interval graphs", "comments": "40 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interval graphs were used in the study of genomics by the famous molecular\nbiologist Benzer. Later on probe interval graphs were introduced by Zhang as a\ngeneralization of interval graphs for the study of cosmid contig mapping of\nDNA.\n  A tagged probe interval graph (briefly, TPIG) is motivated by similar\napplications to genomics, where the set of vertices is partitioned into two\nsets, namely, probes and nonprobes and there is an interval on the real line\ncorresponding to each vertex. The graph has an edge between two probe vertices\nif their corresponding intervals intersect, has an edge between a probe vertex\nand a nonprobe vertex if the interval corresponding to a nonprobe vertex\ncontains at least one end point of the interval corresponding to a probe vertex\nand the set of non-probe vertices is an independent set. This class of graphs\nhave been defined nearly two decades ago, but till today there is no known\nrecognition algorithm for it.\n  In this paper, we consider a natural subclass of TPIG, namely, the class of\nproper tagged probe interval graphs (in short PTPIG). We present\ncharacterization and a linear time recognition algorithm for PTPIG. To obtain\nthis characterization theorem we introduce a new concept called canonical\nsequence for proper interval graphs, which, we belief, has an independent\ninterest in the study of proper interval graphs. Also to obtain the recognition\nalgorithm for PTPIG, we introduce and solve a variation of consecutive $1$'s\nproblem, namely, oriented consecutive $1$'s problem and some variations of\nPQ-tree algorithm. We also discuss the interrelations between the classes of\nPTPIG and TPIG with probe interval graphs and probe proper interval graphs.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 12:42:23 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 17:11:30 GMT"}, {"version": "v3", "created": "Fri, 27 Jul 2018 12:35:47 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Paul", "Sanchita", ""], ["Ghosh", "Shamik", ""], ["Chakraborty", "Sourav", ""], ["Sen", "Malay", ""]]}, {"id": "1607.02951", "submitter": "Arnaud Casteigts", "authors": "Arnaud Casteigts, Yves M\\'etivier, John Michael Robson, Akka Zemmari", "title": "Design Patterns in Beeping Algorithms: Examples, Emulation, and Analysis", "comments": "Final version (accepted for publication in Information and\n  Computation, Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider networks of processes which interact with beeps. In the basic\nmodel defined by Cornejo and Kuhn (2010), processes can choose in each round\neither to beep or to listen. Those who beep are unable to detect simultaneous\nbeeps. Those who listen can only distinguish between silence and the presence\nof at least one beep. We refer to this model as $BL$ (beep or listen). Stronger\nmodels exist where the nodes can detect collision while they are beeping\n($B_{cd}L$), listening ($BL_{cd}$), or both ($B_{cd}L_{cd}$). Beeping models\nare weak in essence and even simple tasks are difficult or unfeasible within.\n  We present a set of generic building blocks (design patterns) which seem to\noccur frequently in the design of beeping algorithms. They include multi-slot\nphases: the fact of dividing the main loop into a number of specialised slots;\nexclusive beeps: having a single node beep at a time in a neighbourhood (within\none or two hops); adaptive probability: increasing or decreasing the\nprobability of beeping to produce more exclusive beeps; internal (resp.\nperipheral) collision detection: for detecting collision while beeping (resp.\nlistening). Based on these patterns, we provide algorithms for a number of\nbasic problems, including colouring, 2-hop colouring, degree computation, 2-hop\nMIS, and collision detection (in $BL$). The patterns make it possible to\nformulate these algorithms in a rather concise and elegant way. Their analyses\nare more technical; one of them improves significantly upon that of the best\nknown MIS algorithm by Jeavons et al. (2016). Finally, inspired by a technique\nfrom Afek et al. (2013), our last contribution is to show that any Las Vegas\nalgorithm relying on collision detection can be transposed into a Monte Carlo\nalgorithm without collision detection at the cost of a logarithmic slowdown,\nwhich we prove is optimal.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 13:56:45 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 09:25:08 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2018 14:29:55 GMT"}, {"version": "v4", "created": "Thu, 30 Aug 2018 13:02:11 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Casteigts", "Arnaud", ""], ["M\u00e9tivier", "Yves", ""], ["Robson", "John Michael", ""], ["Zemmari", "Akka", ""]]}, {"id": "1607.02955", "submitter": "Henning Meyerhenke", "authors": "Elisabetta Bergamini, Michael Wegner, Dimitar Lukarski, Henning\n  Meyerhenke", "title": "Estimating Current-Flow Closeness Centrality with a Multigrid Laplacian\n  Solver", "comments": "Conference version published in Proceedings of SIAM CSC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrices associated with graphs, such as the Laplacian, lead to numerous\ninteresting graph problems expressed as linear systems. One field where\nLaplacian linear systems play a role is network analysis, e. g. for certain\ncentrality measures that indicate if a node (or an edge) is important in the\nnetwork. One such centrality measure is current-flow closeness. To allow\nnetwork analysis workflows to profit from a fast Laplacian solver, we provide\nan implementation of the LAMG multigrid solver in the NetworKit package,\nfacilitating the computation of current-flow closeness values or related\nquantities. Our main contribution consists of two algorithms that accelerate\nthe current-flow computation for one node or a reasonably small node subset\nsignificantly. One sampling-based algorithm provides an unbiased estimation of\nthe related electrical farness, the other one is based on the\nJohnson-Lindenstrauss transform. Our inexact algorithms lead to very accurate\nresults in practice. Thanks to them one is now able to compute an estimation of\ncurrent-flow closeness of one node on networks with tens of millions of nodes\nand edges within seconds or a few minutes. From a network analytical point of\nview, our experiments indicate that current-flow closeness can discriminate\namong different nodes significantly better than traditional shortest-path\ncloseness and is also considerably more resistant to noise -- we thus show that\ntwo known drawbacks of shortest-path closeness are alleviated by the\ncurrent-flow variant.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 14:03:06 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 17:29:23 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Bergamini", "Elisabetta", ""], ["Wegner", "Michael", ""], ["Lukarski", "Dimitar", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1607.03084", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck and Ronen Eldan and Yin Tat Lee", "title": "Kernel-based methods for bandit convex optimization", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the adversarial convex bandit problem and we build the first\n$\\mathrm{poly}(T)$-time algorithm with $\\mathrm{poly}(n) \\sqrt{T}$-regret for\nthis problem. To do so we introduce three new ideas in the derivative-free\noptimization literature: (i) kernel methods, (ii) a generalization of Bernoulli\nconvolutions, and (iii) a new annealing schedule for exponential weights (with\nincreasing learning rate). The basic version of our algorithm achieves\n$\\tilde{O}(n^{9.5} \\sqrt{T})$-regret, and we show that a simple variant of this\nalgorithm can be run in $\\mathrm{poly}(n \\log(T))$-time per step at the cost of\nan additional $\\mathrm{poly}(n) T^{o(1)}$ factor in the regret. These results\nimprove upon the $\\tilde{O}(n^{11} \\sqrt{T})$-regret and\n$\\exp(\\mathrm{poly}(T))$-time result of the first two authors, and the\n$\\log(T)^{\\mathrm{poly}(n)} \\sqrt{T}$-regret and\n$\\log(T)^{\\mathrm{poly}(n)}$-time result of Hazan and Li. Furthermore we\nconjecture that another variant of the algorithm could achieve\n$\\tilde{O}(n^{1.5} \\sqrt{T})$-regret, and moreover that this regret is\nunimprovable (the current best lower bound being $\\Omega(n \\sqrt{T})$ and it is\nachieved with linear functions). For the simpler situation of zeroth order\nstochastic convex optimization this corresponds to the conjecture that the\noptimal query complexity is of order $n^3 / \\epsilon^2$.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 19:25:07 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Eldan", "Ronen", ""], ["Lee", "Yin Tat", ""]]}, {"id": "1607.03183", "submitter": "Andrej Risteski", "authors": "Andrej Risteski", "title": "How to calculate partition functions using convex programming\n  hierarchies: provable bounds for variational methods", "comments": "This paper was accepted for presentation at Conference on Learning\n  Theory (COLT) 2016", "journal-ref": "29th Annual Conference on Learning Theory (pp. 1402-1416), 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating partition functions for Ising\nmodels. We make use of recent tools in combinatorial optimization: the\nSherali-Adams and Lasserre convex programming hierarchies, in combination with\nvariational methods to get algorithms for calculating partition functions in\nthese families. These techniques give new, non-trivial approximation guarantees\nfor the partition function beyond the regime of correlation decay. They also\ngeneralize some classical results from statistical physics about the\nCurie-Weiss ferromagnetic Ising model, as well as provide a partition function\ncounterpart of classical results about max-cut on dense graphs\n\\cite{arora1995polynomial}. With this, we connect techniques from two\napparently disparate research areas -- optimization and counting/partition\nfunction approximations. (i.e. \\#-P type of problems).\n  Furthermore, we design to the best of our knowledge the first provable,\nconvex variational methods. Though in the literature there are a host of convex\nversions of variational methods \\cite{wainwright2003tree, wainwright2005new,\nheskes2006convexity, meshi2009convexifying}, they come with no guarantees\n(apart from some extremely special cases, like e.g. the graph has a single\ncycle \\cite{weiss2000correctness}). We consider dense and low threshold rank\ngraphs, and interestingly, the reason our approach works on these types of\ngraphs is because local correlations propagate to global correlations --\ncompletely the opposite of algorithms based on correlation decay. In the\nprocess we design novel entropy approximations based on the low-order moments\nof a distribution.\n  Our proof techniques are very simple and generic, and likely to be applicable\nto many other settings other than Ising models.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 22:10:04 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Risteski", "Andrej", ""]]}, {"id": "1607.03224", "submitter": "Yu Feng", "authors": "Yu Feng, Chirag Modi", "title": "A fast algorithm for identifying Friends-of-Friends halos", "comments": "11 pages, 6 figures. Published in Astronomy and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a simple and fast algorithm for identifying friends-of-friends\nfeatures and prove its correctness. The algorithm avoids unnecessary expensive\nneighbor queries, uses minimal memory overhead, and rejects slowdown in high\nover-density regions. We define our algorithm formally based on pair\nenumeration, a problem that has been heavily studied in fast 2-point\ncorrelation codes and our reference implementation employs a dual KD-tree\ncorrelation function code. We construct features in a hierarchical tree\nstructure, and use a splay operation to reduce the average cost of identifying\nthe root of a feature from $O[\\log L]$ to $O[1]$ ($L$ is the size of a feature)\nwithout additional memory costs. This reduces the overall time complexity of\nmerging trees from $O[L\\log L]$ to $O[L]$, reducing the number of operations\nper splay by orders of magnitude. We next introduce a pruning operation that\nskips merge operations between two fully self-connected KD-tree nodes. This\nimproves the robustness of the algorithm, reducing the number of merge\noperations in high density peaks from $O[\\delta^2]$ to $O[\\delta]$. We show\nthat for cosmological data set the algorithm eliminates more than half of merge\noperations for typically used linking lengths $b \\sim 0.2$ (relative to mean\nseparation). Furthermore, our algorithm is extremely simple and easy to\nimplement on top of an existing pair enumeration code, reusing the optimization\neffort that has been invested in fast correlation function codes.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 03:24:53 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 19:11:44 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Feng", "Yu", ""], ["Modi", "Chirag", ""]]}, {"id": "1607.03260", "submitter": "Nizar Ouni", "authors": "Nizar Ouni, Ridha Bouallegue", "title": "Modified LLL algorithm with shifted start column", "comments": null, "journal-ref": "International Journal of Wireless & Mobile Networks (IJWMN) Vol.\n  8, No. 3, June 2016", "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-input multiple-output (MIMO) systems are playing an important role\nin the recent wireless communication. The complexity of the different systems\nmodels challenge different researches to get a good complexity to performance\nbalance. Lattices Reduction Techniques and Lenstra-Lenstra-Lovasz (LLL)\nalgorithm bring more resources to investigate and can contribute to the\ncomplexity reduction purposes. In this paper, we are looking to modify the LLL\nalgorithm to reduce the computation operations by exploiting the structure of\nthe upper triangular matrix without big performance degradation. Basically, the\nfirst columns of the upper triangular matrix contain many zeroes, so the\nalgorithm will perform several operations with very limited income. We are\npresenting a performance and complexity study and our proposal show that we can\ngain in term of complexity while the performance results remains almost the\nsame.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 08:36:07 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Ouni", "Nizar", ""], ["Bouallegue", "Ridha", ""]]}, {"id": "1607.03292", "submitter": "Keren Zhou", "authors": "Keren Zhou, Guangming Tan, Wei Zhou", "title": "Quadboost: A Scalable Concurrent Quadtree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building concurrent spatial trees is more complicated than binary search\ntrees since a space hierarchy should be preserved during modifications. We\npresent a non-blocking quadtree-quadboost-that supports concurrent insert,\nremove, move, and contain operations. To increase its concurrency, we propose a\ndecoupling approach that separates physical adjustment from logical removal\nwithin the remove operation. In addition, we design a continuous find mechanism\nto reduce its search cost. The move operation combines the searches for\ndifferent keys together and modifies different positions with atomicity. The\nexperimental results show that quadboost scales well on a multi-core system\nwith 32 hardware threads. More than that, it outperforms existing concurrent\ntrees in retrieving two-dimensional keys with up to 109% improvement when the\nnumber of threads is large. The move operation proved to perform better than\nthe best-known algorithm, with up to 47%.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 10:05:56 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Zhou", "Keren", ""], ["Tan", "Guangming", ""], ["Zhou", "Wei", ""]]}, {"id": "1607.03360", "submitter": "Andrej Risteski", "authors": "Yuanzhi Li, Andrej Risteski", "title": "Approximate maximum entropy principles via Goemans-Williamson with\n  applications to provable variational methods", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well known maximum-entropy principle due to Jaynes, which states that\ngiven mean parameters, the maximum entropy distribution matching them is in an\nexponential family, has been very popular in machine learning due to its\n\"Occam's razor\" interpretation. Unfortunately, calculating the potentials in\nthe maximum-entropy distribution is intractable \\cite{bresler2014hardness}. We\nprovide computationally efficient versions of this principle when the mean\nparameters are pairwise moments: we design distributions that approximately\nmatch given pairwise moments, while having entropy which is comparable to the\nmaximum entropy distribution matching those moments.\n  We additionally provide surprising applications of the approximate maximum\nentropy principle to designing provable variational methods for partition\nfunction calculations for Ising models without any assumptions on the\npotentials of the model. More precisely, we show that in every temperature, we\ncan get approximation guarantees for the log-partition function comparable to\nthose in the low-temperature limit, which is the setting of optimization of\nquadratic forms over the hypercube. \\cite{alon2006approximating}\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 14:09:03 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Li", "Yuanzhi", ""], ["Risteski", "Andrej", ""]]}, {"id": "1607.03432", "submitter": "Marcin Wrochna", "authors": "Marthe Bonamy, {\\L}ukasz Kowalik, Micha{\\l} Pilipczuk, Arkadiusz\n  Soca{\\l}a, Marcin Wrochna", "title": "Tight lower bounds for the complexity of multicoloring", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the multicoloring problem, also known as ($a$:$b$)-coloring or $b$-fold\ncoloring, we are given a graph G and a set of $a$ colors, and the task is to\nassign a subset of $b$ colors to each vertex of G so that adjacent vertices\nreceive disjoint color subsets. This natural generalization of the classic\ncoloring problem (the $b=1$ case) is equivalent to finding a homomorphism to\nthe Kneser graph $KG_{a,b}$, and gives relaxations approaching the fractional\nchromatic number.\n  We study the complexity of determining whether a graph has an\n($a$:$b$)-coloring. Our main result is that this problem does not admit an\nalgorithm with running time $f(b)\\cdot 2^{o(\\log b)\\cdot n}$, for any\ncomputable $f(b)$, unless the Exponential Time Hypothesis (ETH) fails. A\n$(b+1)^n\\cdot \\text{poly}(n)$-time algorithm due to Nederlof [2008] shows that\nthis is tight. A direct corollary of our result is that the graph homomorphism\nproblem does not admit a $2^{O(n+h)}$ algorithm unless ETH fails, even if the\ntarget graph is required to be a Kneser graph. This refines the understanding\ngiven by the recent lower bound of Cygan et al. [SODA 2016].\n  The crucial ingredient in our hardness reduction is the usage of detecting\nmatrices of Lindstr\\\"om [Canad. Math. Bull., 1965], which is a combinatorial\ntool that, to the best of our knowledge, has not yet been used for proving\ncomplexity lower bounds. As a side result, we prove that the running time of\nthe algorithms of Abasi et al. [MFCS 2014] and of Gabizon et al. [ESA 2015] for\nthe r-monomial detection problem are optimal under ETH.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 16:38:13 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 14:03:54 GMT"}, {"version": "v3", "created": "Thu, 16 Feb 2017 19:34:13 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Bonamy", "Marthe", ""], ["Kowalik", "\u0141ukasz", ""], ["Pilipczuk", "Micha\u0142", ""], ["Soca\u0142a", "Arkadiusz", ""], ["Wrochna", "Marcin", ""]]}, {"id": "1607.03463", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li", "title": "LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain", "comments": "first circulated on May 20, 2016; this newer version improves writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study $k$-SVD that is to obtain the first $k$ singular vectors of a matrix\n$A$. Recently, a few breakthroughs have been discovered on $k$-SVD: Musco and\nMusco [1] proved the first gap-free convergence result using the block Krylov\nmethod, Shamir [2] discovered the first variance-reduction stochastic method,\nand Bhojanapalli et al. [3] provided the fastest $O(\\mathsf{nnz}(A) +\n\\mathsf{poly}(1/\\varepsilon))$-time algorithm using alternating minimization.\n  In this paper, we put forward a new and simple LazySVD framework to improve\nthe above breakthroughs. This framework leads to a faster gap-free method\noutperforming [1], and the first accelerated and stochastic method\noutperforming [2]. In the $O(\\mathsf{nnz}(A) + \\mathsf{poly}(1/\\varepsilon))$\nrunning-time regime, LazySVD outperforms [3] in certain parameter regimes\nwithout even using alternating minimization.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 18:41:52 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 18:55:31 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1607.03467", "submitter": "Fred Glover", "authors": "Fred Glover", "title": "Pseudo-Centroid Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-Centroid Clustering replaces the traditional concept of a centroid\nexpressed as a center of gravity with the notion of a pseudo-centroid (or a\ncoordinate free centroid) which has the advantage of applying to clustering\nproblems where points do not have numerical coordinates (or categorical\ncoordinates that are translated into numerical form). Such problems, for which\nclassical centroids do not exist, are particularly important in social\nsciences, marketing, psychology and economics, where distances are not computed\nfrom vector coordinates but rather are expressed in terms of characteristics\nsuch as affinity relationships, psychological preferences, advertising\nresponses, polling data, market interactions and so forth, where distances,\nbroadly conceived, measure the similarity (or dissimilarity) of\ncharacteristics, functions or structures.\n  We formulate a K-PC algorithm analogous to a K-Means algorithm, and identify\ntwo key types of pseudo-centroids, MinMax centroids and (weighted) MinSum\ncentroids, and describe how they respectively give rise to a K-MinMax algorithm\nand a K-MinSum algorithm which are analogous to a K-Means algorithm. The K-PC\nalgorithms are able to take advantage of problem structure to identify special\ndiversity-based and intensity-based starting methods to generate initial\npseudo-centroids and associated clusters, accompanied by theorems for the\nintensity-based methods that establish their ability to obtain best clusters of\na selected size from the points available at each stage of construction. We\nalso introduce a Regret-Threshold PC algorithm that modifies the K-PC algorithm\ntogether with an associated diversification method and a new criterion for\nevaluating the quality of a collection of clusters.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 19:08:24 GMT"}, {"version": "v2", "created": "Sat, 16 Jul 2016 13:57:33 GMT"}, {"version": "v3", "created": "Thu, 3 Nov 2016 16:04:37 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Glover", "Fred", ""]]}, {"id": "1607.03559", "submitter": "Stefanie Jegelka", "authors": "Chengtao Li, Stefanie Jegelka, Suvrit Sra", "title": "Fast Sampling for Strongly Rayleigh Measures with Application to\n  Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we consider sampling from (non-homogeneous) strongly Rayleigh\nprobability measures. As an important corollary, we obtain a fast mixing Markov\nChain sampler for Determinantal Point Processes.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 01:22:04 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Li", "Chengtao", ""], ["Jegelka", "Stefanie", ""], ["Sra", "Suvrit", ""]]}, {"id": "1607.03718", "submitter": "Elazar Goldenberg", "authors": "Diptarka Chakraborty, Elazar Goldenberg and Michal Kouck\\'y", "title": "Streaming Algorithms For Computing Edit Distance Without Exploiting\n  Suffix Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edit distance is a way of quantifying how similar two strings are to one\nanother by counting the minimum number of character insertions, deletions, and\nsubstitutions required to transform one string into the other.\n  In this paper we study the computational problem of computing the edit\ndistance between a pair of strings where their distance is bounded by a\nparameter $k\\ll n$. We present two streaming algorithms for computing edit\ndistance: One runs in time $O(n+k^2)$ and the other $n+O(k^3)$. By writing\n$n+O(k^3)$ we want to emphasize that the number of operations per an input\nsymbol is a small constant. In particular, the running time does not depend on\nthe alphabet size, and the algorithm should be easy to implement.\n  Previously a streaming algorithm with running time $O(n+k^4)$ was given in\nthe paper by the current authors (STOC'16). The best off-line algorithm runs in\ntime $O(n+k^2)$ (Landau et al., 1998) which is known to be optimal under the\nStrong Exponential Time Hypothesis.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 13:16:17 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Chakraborty", "Diptarka", ""], ["Goldenberg", "Elazar", ""], ["Kouck\u00fd", "Michal", ""]]}, {"id": "1607.03791", "submitter": "David Adjiashvili", "authors": "David Adjiashvili", "title": "Improved Approximation for Weighted Tree Augmentation with Bounded Costs", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Weighted Tree Augmentation Problem (WTAP) is a fundamental well-studied\nproblem in the field of network design. Given an undirected tree $G=(V,E)$, an\nadditional set of edges $L \\subseteq V\\times V$ disjoint from $E$ called\n\\textit{links}, and a cost vector $c\\in \\mathbb{R}_{\\geq 0}^L$, WTAP asks to\nfind a minimum-cost set $F\\subseteq L$ with the property that $(V,E\\cup F)$ is\n$2$-edge connected. The special case where $c_\\ell = 1$ for all $\\ell\\in L$ is\ncalled the Tree Augmentation Problem (TAP). Both problems are known to be\nNP-hard.\n  For the class of bounded cost vectors, we present a first improved\napproximation algorithm for WTAP since more than three decades. Concretely, for\nany $M\\in \\mathbb{Z}_{\\geq 1}$ and $\\epsilon > 0,$ we present an LP based\n$(\\delta+\\epsilon)$-approximation for WTAP restricted to cost vectors $c$ in\n$[1,M]^L$ for $\\delta \\approx 1.96417$. For the special case of TAP we improve\nthis factor to $\\frac{5}{3}+\\epsilon$.\n  Our results rely on a new LP, that significantly differs from existing LPs\nachieving improved bounds for TAP. We round a fractional solution in two\nphases. The first phase uses the fractional solution to decompose the tree and\nits fractional solution into so-called $\\beta$-simple pairs losing only an\n$\\epsilon$-factor in the objective function. We then show how to use the\nadditional constraints in our LP combined with the $\\beta$-simple structure to\nround a fractional solution in each part of the decomposition.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 15:32:18 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 09:05:28 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Adjiashvili", "David", ""]]}, {"id": "1607.03866", "submitter": "Anna Paola Muntoni", "authors": "Alfredo Braunstein, Anna Muntoni", "title": "Practical optimization of Steiner Trees via the cavity method", "comments": null, "journal-ref": "J. Stat. Mech. (2016) 073302", "doi": "10.1088/1742-5468/2016/07/073302", "report-no": null, "categories": "cs.DS cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization version of the cavity method for single instances, called\nMax-Sum, has been applied in the past to the Minimum Steiner Tree Problem on\nGraphs and variants. Max-Sum has been shown experimentally to give\nasymptotically optimal results on certain types of weighted random graphs, and\nto give good solutions in short computation times for some types of real\nnetworks. However, the hypotheses behind the formulation and the cavity method\nitself limit substantially the class of instances on which the approach gives\ngood results (or even converges). Moreover, in the standard model formulation,\nthe diameter of the tree solution is limited by a predefined bound, that\naffects both computation time and convergence properties. In this work we\ndescribe two main enhancements to the Max-Sum equations to be able to cope with\noptimization of real-world instances. First, we develop an alternative 'flat'\nmodel formulation, that allows to reduce substantially the relevant\nconfiguration space, making the approach feasible on instances with large\nsolution diameter, in particular when the number of terminal nodes is small.\nSecond, we propose an integration between Max-Sum and three greedy heuristics.\nThis integration allows to transform Max-Sum into a highly competitive\nself-contained algorithm, in which a feasible solution is given at each step of\nthe iterative procedure. Part of this development participated on the 2014\nDIMACS challenge on Steiner Problems, and we report the results here.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 19:00:23 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Braunstein", "Alfredo", ""], ["Muntoni", "Anna", ""]]}, {"id": "1607.03938", "submitter": "Cl\\'ement Canonne", "authors": "Eric Blais, Cl\\'ement L. Canonne, Talya Eden, Amit Levi, Dana Ron", "title": "Tolerant Junta Testing and the Connection to Submodular Optimization and\n  Function Isomorphism", "comments": "Polished the writing, corrected typos, and fixed an issue in the\n  proof of Theorem 1.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A function $f\\colon \\{-1,1\\}^n \\to \\{-1,1\\}$ is a $k$-junta if it depends on\nat most $k$ of its variables. We consider the problem of tolerant testing of\n$k$-juntas, where the testing algorithm must accept any function that is\n$\\epsilon$-close to some $k$-junta and reject any function that is\n$\\epsilon'$-far from every $k'$-junta for some $\\epsilon'= O(\\epsilon)$ and $k'\n= O(k)$.\n  Our first result is an algorithm that solves this problem with query\ncomplexity polynomial in $k$ and $1/\\epsilon$. This result is obtained via a\nnew polynomial-time approximation algorithm for submodular function\nminimization (SFM) under large cardinality constraints, which holds even when\nonly given an approximate oracle access to the function.\n  Our second result considers the case where $k'=k$. We show how to obtain a\nsmooth tradeoff between the amount of tolerance and the query complexity in\nthis setting. Specifically, we design an algorithm that given $\\rho\\in(0,1/2)$\naccepts any function that is $\\frac{\\epsilon\\rho}{16}$-close to some $k$-junta\nand rejects any function that is $\\epsilon$-far from every $k$-junta. The query\ncomplexity of the algorithm is $O\\big( \\frac{k\\log k}{\\epsilon\\rho(1-\\rho)^k}\n\\big)$.\n  Finally, we show how to apply the second result to the problem of tolerant\nisomorphism testing between two unknown Boolean functions $f$ and $g$. We give\nan algorithm for this problem whose query complexity only depends on the\n(unknown) smallest $k$ such that either $f$ or $g$ is close to being a\n$k$-junta.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 21:31:56 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 17:16:20 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Blais", "Eric", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Eden", "Talya", ""], ["Levi", "Amit", ""], ["Ron", "Dana", ""]]}, {"id": "1607.03961", "submitter": "Simon Korman", "authors": "Omri Ben-Eliezer and Simon Korman and Daniel Reichman", "title": "Deleting and Testing Forbidden Patterns in Multi-Dimensional Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the local behaviour of structured multi-dimensional data is a\nfundamental problem in various areas of computer science. As the amount of data\nis often huge, it is desirable to obtain sublinear time algorithms, and\nspecifically property testers, to understand local properties of the data.\n  We focus on the natural local problem of testing pattern freeness: given a\nlarge $d$-dimensional array $A$ and a fixed $d$-dimensional pattern $P$ over a\nfinite alphabet, we say that $A$ is $P$-free if it does not contain a copy of\nthe forbidden pattern $P$ as a consecutive subarray. The distance of $A$ to\n$P$-freeness is the fraction of entries of $A$ that need to be modified to make\nit $P$-free. For any $\\epsilon \\in [0,1]$ and any large enough pattern $P$ over\nany alphabet, other than a very small set of exceptional patterns, we design a\ntolerant tester that distinguishes between the case that the distance is at\nleast $\\epsilon$ and the case that it is at most $a_d \\epsilon$, with query\ncomplexity and running time $c_d \\epsilon^{-1}$, where $a_d < 1$ and $c_d$\ndepend only on $d$.\n  To analyze the testers we establish several combinatorial results, including\nthe following $d$-dimensional modification lemma, which might be of independent\ninterest: for any large enough pattern $P$ over any alphabet (excluding a small\nset of exceptional patterns for the binary case), and any array $A$ containing\na copy of $P$, one can delete this copy by modifying one of its locations\nwithout creating new $P$-copies in $A$.\n  Our results address an open question of Fischer and Newman, who asked whether\nthere exist efficient testers for properties related to tight substructures in\nmulti-dimensional structured data. They serve as a first step towards a general\nunderstanding of local properties of multi-dimensional arrays, as any such\nproperty can be characterized by a fixed family of forbidden patterns.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 23:55:56 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 20:01:50 GMT"}, {"version": "v3", "created": "Sun, 26 Mar 2017 12:43:59 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Ben-Eliezer", "Omri", ""], ["Korman", "Simon", ""], ["Reichman", "Daniel", ""]]}, {"id": "1607.03967", "submitter": "Johann Bengua", "authors": "Johann A. Bengua, Hoang D. Tuan, Ho N. Phien, Minh N. Do", "title": "Concatenated image completion via tensor augmentation and completion", "comments": "7 pages, 6 figures, submitted to ICSPCS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework called concatenated image completion\nvia tensor augmentation and completion (ICTAC), which recovers missing entries\nof color images with high accuracy. Typical images are second- or third-order\ntensors (2D/3D) depending if they are grayscale or color, hence tensor\ncompletion algorithms are ideal for their recovery. The proposed framework\nperforms image completion by concatenating copies of a single image that has\nmissing entries into a third-order tensor, applying a dimensionality\naugmentation technique to the tensor, utilizing a tensor completion algorithm\nfor recovering its missing entries, and finally extracting the recovered image\nfrom the tensor. The solution relies on two key components that have been\nrecently proposed to take advantage of the tensor train (TT) rank: A tensor\naugmentation tool called ket augmentation (KA) that represents a low-order\ntensor by a higher-order tensor, and the algorithm tensor completion by\nparallel matrix factorization via tensor train (TMac-TT), which has been\ndemonstrated to outperform state-of-the-art tensor completion algorithms.\nSimulation results for color image recovery show the clear advantage of our\nframework against current state-of-the-art tensor completion algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 00:24:33 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Bengua", "Johann A.", ""], ["Tuan", "Hoang D.", ""], ["Phien", "Ho N.", ""], ["Do", "Minh N.", ""]]}, {"id": "1607.03990", "submitter": "Jerry Li", "authors": "Jayadev Acharya, Ilias Diakonikolas, Jerry Li, Ludwig Schmidt", "title": "Fast Algorithms for Segmented Regression", "comments": "27 pages, appeared in ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fixed design segmented regression problem: Given noisy samples\nfrom a piecewise linear function $f$, we want to recover $f$ up to a desired\naccuracy in mean-squared error.\n  Previous rigorous approaches for this problem rely on dynamic programming\n(DP) and, while sample efficient, have running time quadratic in the sample\nsize. As our main contribution, we provide new sample near-linear time\nalgorithms for the problem that -- while not being minimax optimal -- achieve a\nsignificantly better sample-time tradeoff on large datasets compared to the DP\napproach. Our experimental evaluation shows that, compared with the DP\napproach, our algorithms provide a convergence rate that is only off by a\nfactor of $2$ to $4$, while achieving speedups of three orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 04:52:53 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Acharya", "Jayadev", ""], ["Diakonikolas", "Ilias", ""], ["Li", "Jerry", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1607.04002", "submitter": "Andreas Bjorklund", "authors": "Andreas Bj\\\"orklund, Petteri Kaski, Ioannis Koutis", "title": "Directed Hamiltonicity and Out-Branchings via Generalized Laplacians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are motivated by a tantalizing open question in exact algorithms: can we\ndetect whether an $n$-vertex directed graph $G$ has a Hamiltonian cycle in time\nsignificantly less than $2^n$? We present new randomized algorithms that\nimprove upon several previous works:\n  1. We show that for any constant $0<\\lambda<1$ and prime $p$ we can count the\nHamiltonian cycles modulo $p^{\\lfloor (1-\\lambda)\\frac{n}{3p}\\rfloor}$ in\nexpected time less than $c^n$ for a constant $c<2$ that depends only on $p$ and\n$\\lambda$. Such an algorithm was previously known only for the case of counting\nmodulo two [Bj\\\"orklund and Husfeldt, FOCS 2013].\n  2. We show that we can detect a Hamiltonian cycle in $O^*(3^{n-\\alpha(G)})$\ntime and polynomial space, where $\\alpha(G)$ is the size of the maximum\nindependent set in $G$. In particular, this yields an $O^*(3^{n/2})$ time\nalgorithm for bipartite directed graphs, which is faster than the\nexponential-space algorithm in [Cygan et al., STOC 2013].\n  Our algorithms are based on the algebraic combinatorics of \"incidence\nassignments\" that we can capture through evaluation of determinants of\nLaplacian-like matrices, inspired by the Matrix--Tree Theorem for directed\ngraphs. In addition to the novel algorithms for directed Hamiltonicity, we use\nthe Matrix--Tree Theorem to derive simple algebraic algorithms for detecting\nout-branchings. Specifically, we give an $O^*(2^k)$-time randomized algorithm\nfor detecting out-branchings with at least $k$ internal vertices, improving\nupon the algorithms of [Zehavi, ESA 2015] and [Bj\\\"orklund et al., ICALP 2015].\nWe also present an algebraic algorithm for the directed $k$-Leaf problem, based\non a non-standard monomial detection problem.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 06:06:39 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 15:40:57 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""], ["Kaski", "Petteri", ""], ["Koutis", "Ioannis", ""]]}, {"id": "1607.04149", "submitter": "Paul D\\\"utting", "authors": "Paul D\\\"utting and Thomas Kesselheim", "title": "Best-Response Dynamics in Combinatorial Auctions with Item Bidding", "comments": "Extended abstract in Proceedings of the 28th ACM-SIAM Symposium on\n  Discrete Algorithms, SODA 2017, Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a combinatorial auction with item bidding, agents participate in multiple\nsingle-item second-price auctions at once. As some items might be substitutes,\nagents need to strategize in order to maximize their utilities. A number of\nresults indicate that high welfare can be achieved this way, giving bounds on\nthe welfare at equilibrium. Recently, however, criticism has been raised that\nequilibria are hard to compute and therefore unlikely to be attained.\n  In this paper, we take a different perspective. We study simple best-response\ndynamics. That is, agents are activated one after the other and each activated\nagent updates his strategy myopically to a best response against the other\nagents' current strategies. Often these dynamics may take exponentially long\nbefore they converge or they may not converge at all. However, as we show,\nconvergence is not even necessary for good welfare guarantees. Given that\nagents' bid updates are aggressive enough but not too aggressive, the game will\nremain in states of good welfare after each agent has updated his bid at least\nonce.\n  In more detail, we show that if agents have fractionally subadditive\nvaluations, natural dynamics reach and remain in a state that provides a $1/3$\napproximation to the optimal welfare after each agent has updated his bid at\nleast once. For subadditive valuations, we can guarantee an $\\Omega(1/\\log m)$\napproximation in case of $m$ items that applies after each agent has updated\nhis bid at least once and at any point after that. The latter bound is\ncomplemented by a negative result, showing that no kind of best-response\ndynamics can guarantee more than an $o(\\log \\log m/\\log m)$ fraction of the\noptimal social welfare.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 14:30:02 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 11:33:34 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["D\u00fctting", "Paul", ""], ["Kesselheim", "Thomas", ""]]}, {"id": "1607.04200", "submitter": "Djamal Belazzougui", "authors": "Djamal Belazzougui and Qin Zhang", "title": "Edit Distance: Sketching, Streaming and Document Exchange", "comments": "Full version of an article to be presented at the 57th Annual IEEE\n  Symposium on Foundations of Computer Science (FOCS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that in the document exchange problem, where Alice holds $x \\in\n\\{0,1\\}^n$ and Bob holds $y \\in \\{0,1\\}^n$, Alice can send Bob a message of\nsize $O(K(\\log^2 K+\\log n))$ bits such that Bob can recover $x$ using the\nmessage and his input $y$ if the edit distance between $x$ and $y$ is no more\nthan $K$, and output \"error\" otherwise. Both the encoding and decoding can be\ndone in time $\\tilde{O}(n+\\mathsf{poly}(K))$. This result significantly\nimproves the previous communication bounds under polynomial encoding/decoding\ntime. We also show that in the referee model, where Alice and Bob hold $x$ and\n$y$ respectively, they can compute sketches of $x$ and $y$ of sizes\n$\\mathsf{poly}(K \\log n)$ bits (the encoding), and send to the referee, who can\nthen compute the edit distance between $x$ and $y$ together with all the edit\noperations if the edit distance is no more than $K$, and output \"error\"\notherwise (the decoding). To the best of our knowledge, this is the first\nresult for sketching edit distance using $\\mathsf{poly}(K \\log n)$ bits.\nMoreover, the encoding phase of our sketching algorithm can be performed by\nscanning the input string in one pass. Thus our sketching algorithm also\nimplies the first streaming algorithm for computing edit distance and all the\nedits exactly using $\\mathsf{poly}(K \\log n)$ bits of space.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 16:38:17 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Belazzougui", "Djamal", ""], ["Zhang", "Qin", ""]]}, {"id": "1607.04229", "submitter": "Arturs Backurs", "authors": "Arturs Backurs, Christos Tzamos", "title": "Improving Viterbi is Hard: Better Runtimes Imply Faster Clique\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic algorithm of Viterbi computes the most likely path in a Hidden\nMarkov Model (HMM) that results in a given sequence of observations. It runs in\ntime $O(Tn^2)$ given a sequence of $T$ observations from a HMM with $n$ states.\nDespite significant interest in the problem and prolonged effort by different\ncommunities, no known algorithm achieves more than a polylogarithmic speedup.\n  In this paper, we explain this difficulty by providing matching conditional\nlower bounds. We show that the Viterbi algorithm runtime is optimal up to\nsubpolynomial factors even when the number of distinct observations is small.\nOur lower bounds are based on assumptions that the best known algorithms for\nthe All-Pairs Shortest Paths problem (APSP) and for the Max-Weight $k$-Clique\nproblem in edge-weighted graphs are essentially tight.\n  Finally, using a recent algorithm by Green Larsen and Williams for online\nBoolean matrix-vector multiplication, we get a $2^{\\Omega(\\sqrt {\\log n})}$\nspeedup for the Viterbi algorithm when there are few distinct transition\nprobabilities in the HMM.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 17:58:09 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 18:19:54 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Backurs", "Arturs", ""], ["Tzamos", "Christos", ""]]}, {"id": "1607.04346", "submitter": "Yakov Nekrich", "authors": "J. Ian Munro, Gonzalo Navarro, Yakov Nekrich", "title": "Space-Efficient Construction of Compressed Indexes in Deterministic\n  Linear Time", "comments": "Extended version of a paper to appear at SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the compressed suffix array and the compressed suffix tree of a\nstring $T$ can be built in $O(n)$ deterministic time using $O(n\\log\\sigma)$\nbits of space, where $n$ is the string length and $\\sigma$ is the alphabet\nsize. Previously described deterministic algorithms either run in time that\ndepends on the alphabet size or need $\\omega(n\\log \\sigma)$ bits of working\nspace. Our result has immediate applications to other problems, such as\nyielding the first linear-time LZ77 and LZ78 parsing algorithms that use $O(n\n\\log\\sigma)$ bits.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 00:16:58 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 02:20:36 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Munro", "J. Ian", ""], ["Navarro", "Gonzalo", ""], ["Nekrich", "Yakov", ""]]}, {"id": "1607.04403", "submitter": "Jo\\~ao Pedro Pedroso", "authors": "Jo\\~ao Pedro Pedroso", "title": "Heuristics for Packing Semifluids", "comments": null, "journal-ref": null, "doi": null, "report-no": "DCC-2016-01", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical properties of materials are seldom studied in the context of packing\nproblems. In this work we study the behavior of semifluids: materials with\nparticular characteristics, that share properties both with solids and with\nfluids. We describe the importance of some specific semifluids in an industrial\ncontext, and propose methods for tackling the problem of packing them, taking\ninto account several practical requirements and physical constraints. Although\nthe focus of this paper is on the computation of practical solutions, it also\nuncovers interesting mathematical properties of this problem, which\ndifferentiate it from other packing problems.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 07:31:49 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Pedroso", "Jo\u00e3o Pedro", ""]]}, {"id": "1607.04431", "submitter": "Jens M. Schmidt", "authors": "Lena Schlipf and Jens M. Schmidt", "title": "Edge-Orders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical orderings and their relatives such as st-numberings have been used\nas a key tool in algorithmic graph theory for the last decades. Recently, a\nunifying concept behind all these orders has been shown: they can be described\nby a graph decomposition into parts that have a prescribed vertex-connectivity.\n  Despite extensive interest in canonical orderings, no analogue of this\nunifying concept is known for edge-connectivity. In this paper, we establish\nsuch a concept named edge-orders and show how to compute (1,1)-edge-orders of\n2-edge-connected graphs as well as (2,1)-edge-orders of 3-edge-connected graphs\nin linear time, respectively. While the former can be seen as the edge-variants\nof st-numberings, the latter are the edge-variants of Mondshein sequences and\nnon-separating ear decompositions. The methods that we use for obtaining such\nedge-orders differ considerably in almost all details from the ones used for\ntheir vertex-counterparts, as different graph-theoretic constructions are used\nin the inductive proof and standard reductions from edge- to\nvertex-connectivity are bound to fail.\n  As a first application, we consider the famous Edge-Independent Spanning Tree\nConjecture, which asserts that every k-edge-connected graph contains k rooted\nspanning trees that are pairwise edge-independent. We illustrate the impact of\nthe above edge-orders by deducing algorithms that construct 2- and 3-edge\nindependent spanning trees of 2- and 3-edge-connected graphs, the latter of\nwhich improves the best known running time from O(n^2) to linear time.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 09:33:27 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Schlipf", "Lena", ""], ["Schmidt", "Jens M.", ""]]}, {"id": "1607.04446", "submitter": "Hiroshi Sakamoto", "authors": "Shouhei Fukunaga, Yoshimasa Takabatake, I Tomohiro and Hiroshi\n  Sakamoto", "title": "Online Grammar Compression for Frequent Pattern Discovery", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various grammar compression algorithms have been proposed in the last decade.\nA grammar compression is a restricted CFG deriving the string\ndeterministically. An efficient grammar compression develops a smaller CFG by\nfinding duplicated patterns and removing them. This process is just a frequent\npattern discovery by grammatical inference. While we can get any frequent\npattern in linear time using a preprocessed string, a huge working space is\nrequired for longer patterns, and the whole string must be loaded into the\nmemory preliminarily. We propose an online algorithm approximating this problem\nwithin a compressed space. The main contribution is an improvement of the\npreviously best known approximation ratio $\\Omega(\\frac{1}{\\lg^2m})$ to\n$\\Omega(\\frac{1}{\\lg^*N\\lg m})$ where $m$ is the length of an optimal pattern\nin a string of length $N$ and $\\lg^*$ is the iteration of the logarithm base\n$2$. For a sufficiently large $N$, $\\lg^*N$ is practically constant. The\nexperimental results show that our algorithm extracts nearly optimal patterns\nand achieves a significant improvement in memory consumption compared to the\noffline algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 10:42:20 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 06:25:36 GMT"}, {"version": "v3", "created": "Wed, 31 Aug 2016 02:01:47 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Fukunaga", "Shouhei", ""], ["Takabatake", "Yoshimasa", ""], ["Tomohiro", "I", ""], ["Sakamoto", "Hiroshi", ""]]}, {"id": "1607.04500", "submitter": "Boas Kluiving", "authors": "Boas Kluiving and Wijnand van Woerkom", "title": "Number representations and term rewriting", "comments": "17 pages, 7 tables. For the automatic proofs, see\n  https://staff.fnwi.uva.nl/a.ponse/term_rewriting_proofs/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine a number of term rewriting system for integer number\nrepresentations, building further upon the datatype defining systems described\nin [2]. In particular, we look at automated methods for proving confluence and\ntermination in binary and decimal term rewriting systems for both append and\ntree constructor functions. We find that some of these term rewriting systems\nare not strongly terminating, which we resolve with minor changes to these\nsystems. Moreover, most of the term rewriting systems discussed do not exhibit\nthe confluence property, which seems more difficult to resolve.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 13:18:55 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Kluiving", "Boas", ""], ["van Woerkom", "Wijnand", ""]]}, {"id": "1607.04527", "submitter": "Yuichi Yoshida", "authors": "Yuichi Yoshida", "title": "Maximizing a Monotone Submodular Function with a Bounded Curvature under\n  a Knapsack Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing a monotone submodular function under a\nknapsack constraint. We show that, for any fixed $\\epsilon > 0$, there exists a\npolynomial-time algorithm with an approximation ratio $1-c/e-\\epsilon$, where\n$c \\in [0,1]$ is the (total) curvature of the input function. This\napproximation ratio is tight up to $\\epsilon$ for any $c \\in [0,1]$. To the\nbest of our knowledge, this is the first result for a knapsack constraint that\nincorporates the curvature to obtain an approximation ratio better than\n$1-1/e$, which is tight for general submodular functions. As an application of\nour result, we present a polynomial-time algorithm for the budget allocation\nproblem with an improved approximation ratio.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 14:33:40 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Yoshida", "Yuichi", ""]]}, {"id": "1607.04545", "submitter": "Pedro Montealegre", "authors": "Pedro Montealegre, Ioan Todinca", "title": "On Distance-$d$ Independent Set and other problems in graphs with few\n  minimal separators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fomin and Villanger (STACS 2010) proved that Maximum Independent Set,\nFeedback Vertex Set, and more generally the problem of finding a maximum\ninduced subgraph of treewith at most a constant $t$, can be solved in\npolynomial time on graph classes with polynomially many minimal separators. We\nextend these results in two directions. Let $\\Gpoly$ be the class of graphs\nwith at most $\\poly(n)$ minimal separators, for some polynomial $\\poly$.\n  We show that the odd powers of a graph $G$ have at most as many minimal\nseparators as $G$. Consequently, \\textsc{Distance-$d$ Independent Set}, which\nconsists in finding maximum set of vertices at pairwise distance at least $d$,\nis polynomial on $\\Gpoly$, for any even $d$. The problem is NP-hard on chordal\ngraphs for any odd $d \\geq 3$.\n  We also provide polynomial algorithms for Connected Vertex Cover and\nConnected Feedback Vertex Set on subclasses of $\\Gpoly$ including chordal and\ncircular-arc graphs, and we discuss variants of independent domination\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 15:09:12 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Montealegre", "Pedro", ""], ["Todinca", "Ioan", ""]]}, {"id": "1607.04557", "submitter": "Alfonso Cevallos", "authors": "Alfonso Cevallos, Friedrich Eisenbrand, Rico Zenklusen", "title": "Local Search for Max-Sum Diversification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide simple and fast polynomial time approximation schemes (PTASs) for\nseveral variants of the max-sum diversification problem which, in its most\nbasic form, is as follows: Given n points p_1,...,p_n in R^d and an integer k,\nselect k points such that the average Euclidean distance between these points\nis maximized. This problem commonly appears in information retrieval and\nweb-search in order to select a diverse set of points from the input. In this\ncontext, it has recently received a lot of attention.\n  We present new techniques to analyze natural local search algorithms. This\nleads to a (1-O(1/k))-approximation for distances of negative type, even\nsubject to any matroid constraint of rank k, in time O(n k^2 log k), when\nassuming that distance evaluations and calls to the independence oracle are\nconstant time. Negative type distances include as special cases Euclidean\ndistances and many further natural distances. Our result easily transforms into\na PTAS and improves on the only previously known PTAS for this setting, which\nrelies on convex optimization techniques in an n-dimensional space and is\nimpractical for large data sets. In contrast, our procedure has an (optimal)\nlinear dependence on n.\n  Using generalized exchange properties of matroid intersection, we show that a\nPTAS can be obtained for matroid intersection constraints as well. Moreover,\nour techniques, being based on local search, are conceptually simple and allow\nfor various extensions. In particular, we get asymptotically optimal\nO(1)-approximations when combining the classic dispersion function with a\nmonotone submodular objective, which is a very common class of functions to\nmeasure diversity and relevance. This result leverages recent advances on local\nsearch techniques based on proxy functions to obtain optimal approximations for\nmonotone submodular function maximization subject to a matroid constraint.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 15:38:02 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Cevallos", "Alfonso", ""], ["Eisenbrand", "Friedrich", ""], ["Zenklusen", "Rico", ""]]}, {"id": "1607.04604", "submitter": "Marek Suchenek", "authors": "Marek A. Suchenek", "title": "Best-case Analysis of MergeSort with an Application to the Sum of Digits\n  Problem, A manuscript (MS) v2", "comments": "This is a longer (12 pages added) version v2 of the article deposited\n  at ArXive on July 15, 2016, under the same title. These new Sections 7, 8,\n  and 9 contain proofs of Theorems 2.2, 3.1, and 4.1. Other than that, the\n  current version v2 is identical with the original version v1. 37 pages, 14\n  figures, a draft of a manuscript intended for future journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An exact formula \\[ B(n) = \\frac{n}{2}(\\lfloor \\lg n \\rfloor + 1) - \\sum\n_{k=0} ^{\\lfloor \\lg n \\rfloor} 2^k Zigzag(\\frac{n}{2^{k+1}}), \\] where \\[\nZigzag (x) = \\min (x - \\lfloor x \\rfloor, \\lceil x \\rceil - x), \\] for the\nminimal number $ B(n) $ of comparisons of keys performed by $ {\\tt MergeSort} $\non an $ n $-element array is derived and analyzed. The said formula is less\ncomplex than any other known formula for the same and can be evaluated in $\nO(\\log ^{c}) $ time, where $ c $ is a constant. It is shown that there is no\nclosed-form formula for the above.\n  Since the recurrence relation for the minimal number of comparisons of keys\nfor $ {\\tt MergeSort} $ is identical with a recurrence relation for the number\nof 1s in binary expansions of all integers between $ 0 $ and $ n $\n(exclusively), the above results extend to the sum of binary digits problem.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 18:21:48 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 19:52:33 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Suchenek", "Marek A.", ""]]}, {"id": "1607.04777", "submitter": "Florent Foucaud", "authors": "Florent Foucaud, Ararat Harutyunyan, Pavol Hell, Sylvain Legay, Yannis\n  Manoussakis, Reza Naserasr", "title": "The complexity of tropical graph homomorphisms", "comments": "27 pages, 13 figures, 1 table. Compared to the published version,\n  this version includes all proofs and some additional figures", "journal-ref": "Discrete Applied Mathematics 229:64-81, 2017", "doi": "10.1016/j.dam.2017.04.027", "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tropical graph $(H,c)$ consists of a graph $H$ and a (not necessarily\nproper) vertex-colouring $c$ of $H$. Given two tropical graphs $(G,c_1)$ and\n$(H,c)$, a homomorphism of $(G,c_1)$ to $(H,c)$ is a standard graph\nhomomorphism of $G$ to $H$ that also preserves the vertex-colours. We initiate\nthe study of the computational complexity of tropical graph homomorphism\nproblems. We consider two settings. First, when the tropical graph $(H,c)$ is\nfixed; this is a problem called $(H,c)$-COLOURING. Second, when the colouring\nof $H$ is part of the input; the associated decision problem is called\n$H$-TROPICAL-COLOURING. Each $(H,c)$-COLOURING problem is a constraint\nsatisfaction problem (CSP), and we show that a complexity dichotomy for the\nclass of $(H,c)$-COLOURING problems holds if and only if the Feder-Vardi\nDichotomy Conjecture for CSPs is true. This implies that $(H,c)$-COLOURING\nproblems form a rich class of decision problems. On the other hand, we were\nsuccessful in classifying the complexity of at least certain classes of\n$H$-TROPICAL-COLOURING problems.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 17:58:36 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 07:04:11 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Foucaud", "Florent", ""], ["Harutyunyan", "Ararat", ""], ["Hell", "Pavol", ""], ["Legay", "Sylvain", ""], ["Manoussakis", "Yannis", ""], ["Naserasr", "Reza", ""]]}, {"id": "1607.04787", "submitter": "Jakub Opr\\v{s}al", "authors": "V\\'ictor Dalmau, Marcin Kozik, Andrei Krokhin, Konstantin Makarychev,\n  Yury Makarychev, Jakub Opr\\v{s}al", "title": "Robust algorithms with polynomial loss for near-unanimity CSPs", "comments": "A preliminary version of this paper appeared in SODA 2017. Journal\n  referees' comments are incorporated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An instance of the Constraint Satisfaction Problem (CSP) is given by a family\nof constraints on overlapping sets of variables, and the goal is to assign\nvalues from a fixed domain to the variables so that all constraints are\nsatisfied. In the optimization version, the goal is to maximize the number of\nsatisfied constraints. An approximation algorithm for CSP is called robust if\nit outputs an assignment satisfying a $(1-g(\\varepsilon))$-fraction of\nconstraints on any $(1-\\varepsilon)$-satisfiable instance, where the loss\nfunction $g$ is such that $g(\\varepsilon)\\rightarrow 0$ as\n$\\varepsilon\\rightarrow 0$.\n  We study how the robust approximability of CSPs depends on the set of\nconstraint relations allowed in instances, the so-called constraint language.\nAll constraint languages admitting a robust polynomial-time algorithm (with\nsome $g$) have been characterised by Barto and Kozik, with the general bound on\nthe loss $g$ being doubly exponential, specifically\n$g(\\varepsilon)=O((\\log\\log(1/\\varepsilon))/\\log(1/\\varepsilon))$. It is\nnatural to ask when a better loss can be achieved: in particular, polynomial\nloss $g(\\varepsilon)=O(\\varepsilon^{1/k})$ for some constant $k$. In this\npaper, we consider CSPs with a constraint language having a near-unanimity\npolymorphism. We give two randomized robust algorithms with polynomial loss for\nsuch CSPs: one works for any near-unanimity polymorphism and the parameter $k$\nin the loss depends on the size of the domain and the arity of the relations in\n$\\Gamma$, while the other works for a special ternary near-unanimity operation\ncalled dual discriminator with $k=2$ for any domain size. In the latter case,\nthe CSP is a common generalisation of Unique Games with a fixed domain and\n2-SAT. In the former case, we use the algebraic approach to the CSP. Both cases\nuse the standard semidefinite programming relaxation for CSP.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 18:52:51 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 09:52:26 GMT"}, {"version": "v3", "created": "Mon, 8 Jan 2018 12:44:41 GMT"}, {"version": "v4", "created": "Tue, 4 Dec 2018 10:43:40 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Dalmau", "V\u00edctor", ""], ["Kozik", "Marcin", ""], ["Krokhin", "Andrei", ""], ["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""], ["Opr\u0161al", "Jakub", ""]]}, {"id": "1607.04829", "submitter": "Michael Codish", "authors": "Michael Frank and Michael Codish", "title": "Logic Programming with Graph Automorphism: Integrating naut with Prolog\n  (Tool Description)", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the plnauty~library, a Prolog interface to the nauty\ngraph-automorphism tool. Adding the capabilities of nauty to Prolog combines\nthe strength of the \"generate and prune\" approach that is commonly used in\nlogic programming and constraint solving, with the ability to reduce symmetries\nwhile reasoning over graph objects. Moreover, it enables the integration of\nnauty in existing tool-chains, such as SAT-solvers or finite domain constraints\ncompilers which exist for Prolog. The implementation consists of two\ncomponents: plnauty, an interface connecting \\nauty's C library with Prolog,\nand plgtools, a Prolog framework integrating the software component of nauty,\ncalled gtools, with Prolog. The complete tool is available as a SWI-Prolog\nmodule. We provide a series of usage examples including two that apply to\ngenerate Ramsey graphs. This paper is under consideration for publication in\nTPLP.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 04:55:50 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 05:58:40 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Frank", "Michael", ""], ["Codish", "Michael", ""]]}, {"id": "1607.04909", "submitter": "Djamal Belazzougui", "authors": "Djamal Belazzougui and Travis Gagie and Veli M\\\"akinen and Marco\n  Previtali", "title": "Fully Dynamic de Bruijn Graphs", "comments": "Presented at the 23rd edition of the International Symposium on\n  String Processing and Information Retrieval (SPIRE 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a space- and time-efficient fully dynamic implementation de Bruijn\ngraphs, which can also support fixed-length jumbled pattern matching.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 19:34:43 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 21:21:33 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Belazzougui", "Djamal", ""], ["Gagie", "Travis", ""], ["M\u00e4kinen", "Veli", ""], ["Previtali", "Marco", ""]]}, {"id": "1607.04911", "submitter": "Morten St\\\"ockel", "authors": "Mikkel Abrahamsen, Stephen Alstrup, Jacob Holm, Mathias B{\\ae}k Tejs\n  Knudsen, Morten St\\\"ockel", "title": "Near-Optimal Induced Universal Graphs for Bounded Degree Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph $U$ is an induced universal graph for a family $F$ of graphs if every\ngraph in $F$ is a vertex-induced subgraph of $U$. For the family of all\nundirected graphs on $n$ vertices Alstrup, Kaplan, Thorup, and Zwick [STOC\n2015] give an induced universal graph with $O\\!\\left(2^{n/2}\\right)$ vertices,\nmatching a lower bound by Moon [Proc. Glasgow Math. Assoc. 1965].\n  Let $k= \\lceil D/2 \\rceil$. Improving asymptotically on previous results by\nButler [Graphs and Combinatorics 2009] and Esperet, Arnaud and Ochem [IPL\n2008], we give an induced universal graph with $O\\!\\left(\\frac{k2^k}{k!}n^k\n\\right)$ vertices for the family of graphs with $n$ vertices of maximum degree\n$D$. For constant $D$, Butler gives a lower bound of\n$\\Omega\\!\\left(n^{D/2}\\right)$. For an odd constant $D\\geq 3$, Esperet et al.\nand Alon and Capalbo [SODA 2008] give a graph with\n$O\\!\\left(n^{k-\\frac{1}{D}}\\right)$ vertices. Using their techniques for any\n(including constant) even values of $D$ gives asymptotically worse bounds than\nwe present.\n  For large $D$, i.e. when $D = \\Omega\\left(\\log^3 n\\right)$, the previous best\nupper bound was ${n\\choose\\lceil D/2\\rceil} n^{O(1)}$ due to Adjiashvili and\nRotbart [ICALP 2014]. We give upper and lower bounds showing that the size is\n${\\lfloor n/2\\rfloor\\choose\\lfloor D/2\n\\rfloor}2^{\\pm\\tilde{O}\\left(\\sqrt{D}\\right)}$. Hence the optimal size is\n$2^{\\tilde{O}(D)}$ and our construction is within a factor of\n$2^{\\tilde{O}\\left(\\sqrt{D}\\right)}$ from this. The previous results were\nlarger by at least a factor of $2^{\\Omega(D)}$.\n  As a part of the above, proving a conjecture by Esperet et al., we construct\nan induced universal graph with $2n-1$ vertices for the family of graphs with\nmax degree $2$. In addition, we give results for acyclic graphs with max degree\n$2$ and cycle graphs. Our results imply the first labeling schemes that for any\n$D$ are at most $o(n)$ bits from optimal.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 19:45:01 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 20:40:49 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Abrahamsen", "Mikkel", ""], ["Alstrup", "Stephen", ""], ["Holm", "Jacob", ""], ["Knudsen", "Mathias B\u00e6k Tejs", ""], ["St\u00f6ckel", "Morten", ""]]}, {"id": "1607.04913", "submitter": "Ruosong Wang", "authors": "Lijie Chen, Ran Duan, Ruosong Wang, Hanrui Zhang, Tianyi Zhang", "title": "An Improved Algorithm for Incremental DFS Tree in Undirected Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth first search (DFS) tree is one of the most well-known data structures\nfor designing efficient graph algorithms. Given an undirected graph $G=(V,E)$\nwith $n$ vertices and $m$ edges, the textbook algorithm takes $O(n+m)$ time to\nconstruct a DFS tree. In this paper, we study the problem of maintaining a DFS\ntree when the graph is undergoing incremental updates. Formally, we show: Given\nan arbitrary online sequence of edge or vertex insertions, there is an\nalgorithm that reports a DFS tree in $O(n)$ worst case time per operation, and\nrequires $O\\left(\\min\\{m \\log n, n^2\\}\\right)$ preprocessing time.\n  Our result improves the previous $O(n \\log^3 n)$ worst case update time\nalgorithm by Baswana et al. and the $O(n \\log n)$ time by Nakamura and\nSadakane, and matches the trivial $\\Omega(n)$ lower bound when it is required\nto explicitly output a DFS tree.\n  Our result builds on the framework introduced in the breakthrough work by\nBaswana et al., together with a novel use of a tree-partition lemma by Duan and\nZhan, and the celebrated fractional cascading technique by Chazelle and Guibas.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 20:15:31 GMT"}, {"version": "v2", "created": "Sat, 27 Aug 2016 13:23:11 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 11:40:00 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Chen", "Lijie", ""], ["Duan", "Ran", ""], ["Wang", "Ruosong", ""], ["Zhang", "Hanrui", ""], ["Zhang", "Tianyi", ""]]}, {"id": "1607.04940", "submitter": "Kimon Fountoulakis", "authors": "Kimon Fountoulakis, David Gleich and Michael Mahoney", "title": "An optimization approach to locally-biased graph algorithms", "comments": "19 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally-biased graph algorithms are algorithms that attempt to find local or\nsmall-scale structure in a large data graph. In some cases, this can be\naccomplished by adding some sort of locality constraint and calling a\ntraditional graph algorithm; but more interesting are locally-biased graph\nalgorithms that compute answers by running a procedure that does not even look\nat most of the input graph. This corresponds more closely to what practitioners\nfrom various data science domains do, but it does not correspond well with the\nway that algorithmic and statistical theory is typically formulated. Recent\nwork from several research communities has focused on developing locally-biased\ngraph algorithms that come with strong complementary algorithmic and\nstatistical theory and that are useful in practice in downstream data science\napplications. We provide a review and overview of this work, highlighting\ncommonalities between seemingly-different approaches, and highlighting\npromising directions for future work.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 02:51:02 GMT"}, {"version": "v2", "created": "Sat, 22 Oct 2016 06:29:26 GMT"}, {"version": "v3", "created": "Mon, 5 Dec 2016 00:43:03 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Fountoulakis", "Kimon", ""], ["Gleich", "David", ""], ["Mahoney", "Michael", ""]]}, {"id": "1607.04984", "submitter": "He Sun", "authors": "He Sun, Luca Zanetti", "title": "Distributed Graph Clustering by Load Balancing", "comments": "There is a gap in the proof of the paper, which makes the paper's\n  main statement invalid. We presented an improved algorithm with a completely\n  different proof in the paper with arXiv:1711.01262", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph clustering is a fundamental computational problem with a number of\napplications in algorithm design, machine learning, data mining, and analysis\nof social networks. Over the past decades, researchers have proposed a number\nof algorithmic design methods for graph clustering. However, most of these\nmethods are based on complicated spectral techniques or convex optimisation,\nand cannot be applied directly for clustering many networks that occur in\npractice, whose information is often collected on different sites. Designing a\nsimple and distributed clustering algorithm is of great interest, and has wide\napplications for processing big datasets. In this paper we present a simple and\ndistributed algorithm for graph clustering: for a wide class of graphs that are\ncharacterised by a strong cluster-structure, our algorithm finishes in a\npoly-logarithmic number of rounds, and recovers a partition of the graph close\nto an optimal partition. The main component of our algorithm is an application\nof the random matching model of load balancing, which is a fundamental protocol\nin distributed computing and has been extensively studied in the past 20 years.\nHence, our result highlights an intrinsic and interesting connection between\ngraph clustering and load balancing. At a technical level, we present a purely\nalgebraic result characterising the early behaviours of load balancing\nprocesses for graphs exhibiting a cluster-structure. We believe that this\nresult can be further applied to analyse other gossip processes, such as rumour\nspreading and averaging processes.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 09:30:49 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 20:36:55 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2019 13:47:40 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Sun", "He", ""], ["Zanetti", "Luca", ""]]}, {"id": "1607.05008", "submitter": "Daniel Krenn", "authors": "Daniel Krenn", "title": "An Extended Note on the Comparison-optimal Dual Pivot Quickselect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note the precise minimum number of key comparisons any dual-pivot\nquickselect algorithm (without sampling) needs on average is determined. The\nresult is in the form of exact as well as asymptotic formul\\ae{} of this number\nof a comparison-optimal algorithm. It turns out that the main terms of these\nasymptotic expansions coincide with the main terms of the corresponding\nanalysis of the classical quickselect, but still---as this was shown for\nYaroslavskiy quickselect---more comparisons are needed in the dual-pivot\nvariant. The results are obtained by solving a second order differential\nequation for the generating function obtained from a recursive approach.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 10:48:06 GMT"}, {"version": "v2", "created": "Sun, 16 Oct 2016 12:13:09 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Krenn", "Daniel", ""]]}, {"id": "1607.05112", "submitter": "Kyle Fox", "authors": "Glencora Borradaile and Erin Wolf Chambers and Kyle Fox and Amir\n  Nayyeri", "title": "Minimum cycle and homology bases of surface embedded graphs", "comments": "A preliminary version of this work was presented at the 32nd Annual\n  International Symposium on Computational Geometry", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problems of finding a minimum cycle basis (a minimum weight set\nof cycles that form a basis for the cycle space) and a minimum homology basis\n(a minimum weight set of cycles that generates the $1$-dimensional\n($\\mathbb{Z}_2$)-homology classes) of an undirected graph embedded on a\nsurface. The problems are closely related, because the minimum cycle basis of a\ngraph contains its minimum homology basis, and the minimum homology basis of\nthe $1$-skeleton of any graph is exactly its minimum cycle basis.\n  For the minimum cycle basis problem, we give a deterministic\n$O(n^\\omega+2^{2g}n^2+m)$-time algorithm for graphs embedded on an orientable\nsurface of genus $g$. The best known existing algorithms for surface embedded\ngraphs are those for general graphs: an $O(m^\\omega)$ time Monte Carlo\nalgorithm and a deterministic $O(nm^2/\\log n + n^2 m)$ time algorithm. For the\nminimum homology basis problem, we give a deterministic $O((g+b)^3 n \\log n +\nm)$-time algorithm for graphs embedded on an orientable or non-orientable\nsurface of genus $g$ with $b$ boundary components, assuming shortest paths are\nunique, improving on existing algorithms for many values of $g$ and $n$. The\nassumption of unique shortest paths can be avoided with high probability using\nrandomization or deterministically by increasing the running time of the\nhomology basis algorithm by a factor of $O(\\log n)$.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 14:58:06 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Borradaile", "Glencora", ""], ["Chambers", "Erin Wolf", ""], ["Fox", "Kyle", ""], ["Nayyeri", "Amir", ""]]}, {"id": "1607.05122", "submitter": "Euiwoong Lee", "authors": "Euiwoong Lee", "title": "Partitioning a Graph into Small Pieces with Applications to Path\n  Transversal", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G = (V, E)$ and an integer $k$, we study $k$-Vertex Seperator\n(resp. $k$-Edge Separator), where the goal is to remove the minimum number of\nvertices (resp. edges) such that each connected component in the resulting\ngraph has at most $k$ vertices. Our primary focus is on the case where $k$ is\neither a constant or a slowly growing function of $n$ (e.g. $O(\\log n)$ or\n$n^{o(1)}$). Our problems can be interpreted as a special case of three general\nclasses of problems that have been studied separately (balanced graph\npartitioning, Hypergraph Vertex Cover (HVC), and fixed parameter tractability\n(FPT)).\n  Our main result is an $O(\\log k)$-approximation algorithm for $k$-Vertex\nSeperator that runs in time $2^{O(k)} n^{O(1)}$, and an $O(\\log\nk)$-approximation algorithm for $k$-Edge Separator that runs in time\n$n^{O(1)}$. Our result on $k$-Edge Seperator improves the best previous graph\npartitioning algorithm for small $k$. Our result on $k$-Vertex Seperator\nimproves the simple $(k+1)$-approximation from HVC. When $OPT > k$, the running\ntime $2^{O(k)} n^{O(1)}$ is faster than the lower bound $k^{\\Omega(OPT)}\nn^{\\Omega(1)}$ for exact algorithms assuming the Exponential Time Hypothesis.\nWhile the running time of $2^{O(k)} n^{O(1)}$ for $k$-Vertex Separator seems\nunsatisfactory, we show that the superpolynomial dependence on $k$ may be\nneeded to achieve a polylogarithmic approximation ratio, based on hardness of\nDensest $k$-Subgraph.\n  We also study $k$-Path Transversal, where the goal is to remove the minimum\nnumber of vertices such that there is no simple path of length $k$. With\nadditional ideas from FPT algorithms and graph theory, we present an $O(\\log\nk)$-approximation algorithm for $k$-Path Transversal that runs in time\n$2^{O(k^3 \\log k)} n^{O(1)}$. Previously, the existence of even $(1 -\n\\delta)k$-approximation algorithm for fixed $\\delta > 0$ was open.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 15:15:25 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Lee", "Euiwoong", ""]]}, {"id": "1607.05127", "submitter": "Sebastian Forster", "authors": "Ruben Becker, Sebastian Forster, Andreas Karrenbauer, Christoph Lenzen", "title": "Near-Optimal Approximate Shortest Paths and Transshipment in Distributed\n  and Streaming Models", "comments": "Accepted to SIAM Journal on Computing. Preliminary version in DISC\n  2017. Abstract shortened to fit arXiv's limitation to 1920 characters", "journal-ref": null, "doi": "10.4230/LIPIcs.DISC.2017.7", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for solving the transshipment problem - also known as\nuncapacitated minimum cost flow - up to a multiplicative error of $1 +\n\\varepsilon$ in undirected graphs with non-negative edge weights using a\ntailored gradient descent algorithm. Using $\\tilde{O}(\\cdot)$ to hide\npolylogarithmic factors in $n$ (the number of nodes in the graph), our gradient\ndescent algorithm takes $\\tilde O(\\varepsilon^{-2})$ iterations, and in each\niteration it solves an instance of the transshipment problem up to a\nmultiplicative error of $\\operatorname{polylog} n$. In particular, this allows\nus to perform a single iteration by computing a solution on a sparse spanner of\nlogarithmic stretch. Using a randomized rounding scheme, we can further extend\nthe method to finding approximate solutions for the single-source shortest\npaths (SSSP) problem. As a consequence, we improve upon prior work by obtaining\nthe following results: (1) Broadcast CONGEST model: $(1 +\n\\varepsilon)$-approximate SSSP using $\\tilde{O}((\\sqrt{n} +\nD)\\varepsilon^{-3})$ rounds, where $ D $ is the (hop) diameter of the network.\n(2) Broadcast congested clique model: $(1 + \\varepsilon)$-approximate\ntransshipment and SSSP using $\\tilde{O}(\\varepsilon^{-2})$ rounds. (3)\nMultipass streaming model: $(1 + \\varepsilon)$-approximate transshipment and\nSSSP using $\\tilde{O}(n)$ space and $\\tilde{O}(\\varepsilon^{-2})$ passes. The\npreviously fastest SSSP algorithms for these models leverage sparse hop sets.\nWe bypass the hop set construction; computing a spanner is sufficient with our\nmethod. The above bounds assume non-negative edge weights that are polynomially\nbounded in $n$; for general non-negative weights, running times scale with the\nlogarithm of the maximum ratio between non-zero weights.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 15:27:01 GMT"}, {"version": "v2", "created": "Sun, 6 Nov 2016 22:19:10 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 14:10:41 GMT"}, {"version": "v4", "created": "Tue, 2 Feb 2021 22:21:41 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Becker", "Ruben", ""], ["Forster", "Sebastian", ""], ["Karrenbauer", "Andreas", ""], ["Lenzen", "Christoph", ""]]}, {"id": "1607.05132", "submitter": "Sebastian Krinninger", "authors": "Ittai Abraham, Shiri Chechik, Sebastian Krinninger", "title": "Fully dynamic all-pairs shortest paths with worst-case update-time\n  revisited", "comments": "To be presented at the Symposium on Discrete Algorithms (SODA) 2017", "journal-ref": null, "doi": "10.1137/1.9781611974782.28", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the classic problem of dynamically maintaining shortest paths\nbetween all pairs of nodes of a directed weighted graph. The allowed updates\nare insertions and deletions of nodes and their incident edges. We give\nworst-case guarantees on the time needed to process a single update (in\ncontrast to related results, the update time is not amortized over a sequence\nof updates).\n  Our main result is a simple randomized algorithm that for any parameter $c>1$\nhas a worst-case update time of $O(cn^{2+2/3} \\log^{4/3}{n})$ and answers\ndistance queries correctly with probability $1-1/n^c$, against an adaptive\nonline adversary if the graph contains no negative cycle. The best\ndeterministic algorithm is by Thorup [STOC 2005] with a worst-case update time\nof $\\tilde O(n^{2+3/4})$ and assumes non-negative weights. This is the first\nimprovement for this problem for more than a decade. Conceptually, our\nalgorithm shows that randomization along with a more direct approach can\nprovide better bounds.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 15:35:39 GMT"}, {"version": "v2", "created": "Sun, 6 Nov 2016 22:29:06 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Abraham", "Ittai", ""], ["Chechik", "Shiri", ""], ["Krinninger", "Sebastian", ""]]}, {"id": "1607.05133", "submitter": "Euiwoong Lee", "authors": "Euiwoong Lee", "title": "Improved Hardness for Cut, Interdiction, and Firefighter Problems", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study variants of the classic $s$-$t$ cut problem and prove the following\nimproved hardness results assuming the Unique Games Conjecture (UGC).\n  - For any constant $k \\geq 2$ and $\\epsilon > 0$, we show that Directed\nMulticut with $k$ source-sink pairs is hard to approximate within a factor $k -\n\\epsilon$. This matches the trivial $k$-approximation algorithm. By a simple\nreduction, our result for $k = 2$ implies that Directed Multiway Cut with two\nterminals (also known as $s$-$t$ Bicut) is hard to approximate within a factor\n$2 - \\epsilon$, matching the trivial $2$-approximation algorithm. Previously,\nthe best hardness factor for these problems (for constant $k$) was $1.5 -\n\\epsilon$ under the UGC.\n  - For Length-Bounded Cut and Shortest Path Interdiction, we show that both\nproblems are hard to approximate within any constant factor, even if we allow\nbicriteria approximation. If we want to cut vertices or the graph is directed,\nour hardness factor for Length-Bounded Cut matches the best approximation ratio\nup to a constant. Previously, the best hardness factor was $1.1377$ for\nLength-Bounded Cut and $2$ for Shortest Path Interdiction.\n  - Assuming a variant of the UGC (implied by another variant of Bansal and\nKhot), we prove that it is hard to approximate Resource Minimization Fire\nContainment within any constant factor. Previously, the best hardness factor\nwas $2$.\n  Our results are based on a general method of converting an integrality gap\ninstance to a length-control dictatorship test for variants of the $s$-$t$ cut\nproblem, which may be useful for other problems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 15:39:21 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Lee", "Euiwoong", ""]]}, {"id": "1607.05157", "submitter": "Matthias Gall\\'e", "authors": "Matthias Galle", "title": "Multi-view pattern matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the \\textit{multi-view pattern matching} problem, where a text\ncan have multiple views. Each view is a string of the same size and drawn from\ndisjoint alphabets. The pattern is drawn from the union of all alphabets.\n  The algorithm we present is an extension of the Horspool algorithm, and in\nour experiments on synthetic data it shows an $3 \\times$ improvement over the\nnaive baseline.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 16:14:30 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Galle", "Matthias", ""]]}, {"id": "1607.05342", "submitter": "Fahad Panolan", "authors": "Fedor V. Fomin, Fahad Panolan, M. S. Ramanujan, Saket Saurabh", "title": "On the Optimality of Pseudo-polynomial Algorithms for Integer\n  Programming", "comments": "29 pages, To appear in ESA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classic Integer Programming (IP) problem, the objective is to decide\nwhether, for a given $m \\times n$ matrix $A$ and an $m$-vector $b=(b_1,\\dots,\nb_m)$, there is a non-negative integer $n$-vector $x$ such that $Ax=b$. Solving\n(IP) is an important step in numerous algorithms and it is important to obtain\nan understanding of the precise complexity of this problem as a function of\nnatural parameters of the input.\n  The classic pseudo-polynomial time algorithm of Papadimitriou [J. ACM 1981]\nfor instances of (IP) with a constant number of constraints was only recently\nimproved upon by Eisenbrand and Weismantel [SODA 2018] and Jansen and Rohwedder\n[ArXiv 2018]. We continue this line of work and show that under the Exponential\nTime Hypothesis (ETH), the algorithm of Jansen and Rohwedder is nearly optimal.\nWe also show that when the matrix $A$ is assumed to be non-negative, a\ncomponent of Papadimitriou's original algorithm is already nearly optimal under\nETH.\n  This motivates us to pick up the line of research initiated by Cunningham and\nGeelen [IPCO 2007] who studied the complexity of solving (IP) with non-negative\nmatrices in which the number of constraints may be unbounded, but the\nbranch-width of the column-matroid corresponding to the constraint matrix is a\nconstant. We prove a lower bound on the complexity of solving (IP) for such\ninstances and obtain optimal results with respect to a closely related\nparameter, path-width. Specifically, we prove matching upper and lower bounds\nfor (IP) when the path-width of the corresponding column-matroid is a constant.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 22:31:26 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 12:47:45 GMT"}, {"version": "v3", "created": "Tue, 17 Jul 2018 09:02:19 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Panolan", "Fahad", ""], ["Ramanujan", "M. S.", ""], ["Saurabh", "Saket", ""]]}, {"id": "1607.05397", "submitter": "Zhiwei Steven Wu", "authors": "Aaron Roth, Aleksandrs Slivkins, Jonathan Ullman, Zhiwei Steven Wu", "title": "Multidimensional Dynamic Pricing for Welfare Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of a seller dynamically pricing $d$ distinct types of\nindivisible goods, when faced with the online arrival of unit-demand buyers\ndrawn independently from an unknown distribution. The goods are not in limited\nsupply, but can only be produced at a limited rate and are costly to produce.\nThe seller observes only the bundle of goods purchased at each day, but nothing\nelse about the buyer's valuation function. Our main result is a dynamic pricing\nalgorithm for optimizing welfare (including the seller's cost of production)\nthat runs in time and a number of rounds that are polynomial in $d$ and the\napproximation parameter. We are able to do this despite the fact that (i) the\nprice-response function is not continuous, and even its fractional relaxation\nis a non-concave function of the prices, and (ii) the welfare is not observable\nto the seller.\n  We derive this result as an application of a general technique for optimizing\nwelfare over \\emph{divisible} goods, which is of independent interest. When\nbuyers have strongly concave, H\\\"older continuous valuation functions over $d$\ndivisible goods, we give a general polynomial time dynamic pricing technique.\nWe are able to apply this technique to the setting of unit demand buyers\ndespite the fact that in that setting the goods are not divisible, and the\nnatural fractional relaxation of a unit demand valuation is not strongly\nconcave. In order to apply our general technique, we introduce a novel price\nrandomization procedure which has the effect of implicitly inducing buyers to\n\"regularize\" their valuations with a strongly concave function. Finally, we\nalso extend our results to a limited-supply setting in which the number of\ncopies of each good cannot be replenished.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 04:22:00 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 01:06:58 GMT"}, {"version": "v3", "created": "Sat, 10 Jun 2017 20:10:09 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Roth", "Aaron", ""], ["Slivkins", "Aleksandrs", ""], ["Ullman", "Jonathan", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1607.05451", "submitter": "Johan von Tangen Sivertsen M.Sc", "authors": "Mayank Goswami, Rasmus Pagh, Francesco Silvestri and Johan Sivertsen", "title": "Distance Sensitive Bloom Filters Without False Negatives", "comments": "Published in SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bloom filter is a widely used data-structure for representing a set $S$ and\nanswering queries of the form \"Is $x$ in $S$?\". By allowing some false positive\nanswers (saying \"yes\" when the answer is in fact `no') Bloom filters use space\nsignificantly below what is required for storing $S$. In the distance sensitive\nsetting we work with a set $S$ of (Hamming) vectors and seek a data structure\nthat offers a similar trade-off, but answers queries of the form \"Is $x$ close\nto an element of $S$?\" (in Hamming distance). Previous work on distance\nsensitive Bloom filters have accepted false positive and false negative\nanswers. Absence of false negatives is of critical importance in many\napplications of Bloom filters, so it is natural to ask if this can be also\nachieved in the distance sensitive setting. Our main contributions are upper\nand lower bounds (that are tight in several cases) for space usage in the\ndistance sensitive setting where false negatives are not allowed.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 08:20:26 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 11:41:32 GMT"}, {"version": "v3", "created": "Wed, 2 Nov 2016 14:47:57 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Goswami", "Mayank", ""], ["Pagh", "Rasmus", ""], ["Silvestri", "Francesco", ""], ["Sivertsen", "Johan", ""]]}, {"id": "1607.05516", "submitter": "Petr Golovach", "authors": "Fedor V. Fomin, Petr A. Golovach, Daniel Lokshtanov, and Saket Saurabh", "title": "Spanning Circuits in Regular Matroids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental Matroid Theory problem of finding a circuit in a\nmatroid spanning a set T of given terminal elements. For graphic matroids this\ncorresponds to the problem of finding a simple cycle passing through a set of\ngiven terminal edges in a graph. The algorithmic study of the problem on\nregular matroids, a superclass of graphic matroids, was initiated by\nGaven\\v{c}iak, Kr\\'al', and Oum [ICALP'12], who proved that the case of the\nproblem with |T|=2 is fixed-parameter tractable (FPT) when parameterized by the\nlength of the circuit. We extend the result of Gaven\\v{c}iak, Kr\\'al', and Oum\nby showing that for regular matroids\n  - the Minimum Spanning Circuit problem, deciding whether there is a circuit\nwith at most \\ell elements containing T, is FPT parameterized by k=\\ell-|T|;\n  - the Spanning Circuit problem, deciding whether there is a circuit\ncontaining T, is FPT parameterized by |T|. We note that extending our\nalgorithmic findings to binary matroids, a superclass of regular matroids, is\nhighly unlikely: Minimum Spanning Circuit parameterized by \\ell is W[1]-hard on\nbinary matroids even when |T|=1. We also show a limit to how far our results\ncan be strengthened by considering a smaller parameter. More precisely, we\nprove that Minimum Spanning Circuit parameterized by |T| is W[1]-hard even on\ncographic matroids, a proper subclass of regular matroids.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 11:01:35 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Lokshtanov", "Daniel", ""], ["Saurabh", "Saket", ""]]}, {"id": "1607.05527", "submitter": "Tillmann Miltzow", "authors": "\\'Edouard Bonnet, Tillmann Miltzow", "title": "An Approximation Algorithm for the Art Gallery Problem", "comments": "25 pages, 4 pages proof ideas, many figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a simple polygon $\\mathcal{P}$ on $n$ vertices, two points $x,y$ in\n$\\mathcal{P}$ are said to be visible to each other if the line segment between\n$x$ and $y$ is contained in $\\mathcal{P}$. The Point Guard Art Gallery problem\nasks for a minimum set $S$ such that every point in $\\mathcal{P}$ is visible\nfrom a point in $S$. The set $S$ is referred to as guards. Assuming integer\ncoordinates and a specific general position assumption, we present the first\n$O(\\log \\text{OPT})$-approximation algorithm for the point guard problem for\nsimple polygons. This algorithm combines ideas of a paper of Efrat and\nHar-Peled [Inf. Process. Lett. 2006] and Deshpande et. al. [WADS 2007]. We also\npoint out a mistake in the latter.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 11:32:54 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Miltzow", "Tillmann", ""]]}, {"id": "1607.05597", "submitter": "Ami Paz", "authors": "Keren Censor-Hillel, Telikepalli Kavitha, Ami Paz, Amir Yehudayoff", "title": "Distributed Construction of Purely Additive Spanners", "comments": "An extended abstract of this work will be presented in DISC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the complexity of distributed construction of purely\nadditive spanners in the CONGEST model. We describe algorithms for building\nsuch spanners in several cases. Because of the need to simultaneously make\ndecisions at far apart locations, the algorithms use additional mechanisms\ncompared to their sequential counterparts.\n  We complement our algorithms with a lower bound on the number of rounds\nrequired for computing pairwise spanners. The standard reductions from\nset-disjointness and equality seem unsuitable for this task because no specific\nedge needs to be removed from the graph. Instead, to obtain our lower bound, we\ndefine a new communication complexity problem that reduces to computing a\nsparse spanner, and prove a lower bound on its communication complexity using\ninformation theory. This technique significantly extends the current toolbox\nused for obtaining lower bounds for the CONGEST model, and we believe it may\nfind additional applications.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 14:24:25 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Kavitha", "Telikepalli", ""], ["Paz", "Ami", ""], ["Yehudayoff", "Amir", ""]]}, {"id": "1607.05626", "submitter": "Jakub Radoszewski", "authors": "Jakub Radoszewski and Tatiana Starikovskaya", "title": "Streaming k-mismatch with error correcting and applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new streaming algorithm for the $k$-Mismatch problem, one of the\nmost basic problems in pattern matching. Given a pattern and a text, the task\nis to find all substrings of the text that are at the Hamming distance at most\n$k$ from the pattern. Our algorithm is enhanced with an important new feature\ncalled Error Correcting, and its complexities for $k=1$ and for a general $k$\nare comparable to those of the solutions for the $k$-Mismatch problem by Porat\nand Porat (FOCS 2009) and Clifford et al. (SODA 2016). In parallel to our\nresearch, a yet more efficient algorithm for the $k$-Mismatch problem with the\nError Correcting feature was developed by Clifford et al. (SODA 2019). Using\nthe new feature and recent work on streaming Multiple Pattern Matching we\ndevelop a series of streaming algorithms for pattern matching on weighted\nstrings, which are a commonly used representation of uncertain sequences in\nmolecular biology. We also show that these algorithms are space-optimal up to\npolylog factors.\n  A preliminary version of this work was published at DCC 2017 conference.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 15:18:59 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 19:07:33 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2019 10:49:14 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Radoszewski", "Jakub", ""], ["Starikovskaya", "Tatiana", ""]]}, {"id": "1607.05697", "submitter": "Calvin Newport", "authors": "Mohsen Ghaffari and Calvin Newport", "title": "How to Discreetly Spread a Rumor in a Crowd", "comments": "Extended abstract to appear at the 2016 International Symposium on\n  Distributed Computing (DISC 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study PUSH-PULL style rumor spreading algorithms in the\nmobile telephone model, a variant of the classical telephone model in which\neach node can participate in at most one connection per round; i.e., you can no\nlonger have multiple nodes pull information from the same source in a single\nround. Our model also includes two new parameterized generalizations: (1) the\nnetwork topology can undergo a bounded rate of change (for a parameterized rate\nthat spans from no changes to changes in every round); and (2) in each round,\neach node can advertise a bounded amount of information to all of its neighbors\nbefore connection decisions are made (for a parameterized number of bits that\nspans from no advertisement to large advertisements). We prove that in the\nmobile telephone model with no advertisements and no topology changes,\nPUSH-PULL style algorithms perform poorly with respect to a graph's vertex\nexpansion and graph conductance as compared to the known tight results in the\nclassical telephone model. We then prove, however, that if nodes are allowed to\nadvertise a single bit in each round, a natural variation of PUSH-PULL\nterminates in time that matches (within logarithmic factors) this strategy's\nperformance in the classical telephone model---even in the presence of frequent\ntopology changes. We also analyze how the performance of this algorithm\ndegrades as the rate of change increases toward the maximum possible amount. We\nargue that our model matches well the properties of emerging peer-to-peer\ncommunication standards for mobile devices, and that our efficient PUSH-PULL\nvariation that leverages small advertisements and adapts well to topology\nchanges is a good choice for rumor spreading in this increasingly important\nsetting.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 18:53:11 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Newport", "Calvin", ""]]}, {"id": "1607.05786", "submitter": "Nithin Varma", "authors": "Kashyap Dixit, Sofya Raskhodnikova, Abhradeep Thakurta and Nithin\n  Varma", "title": "Erasure-Resilient Property Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Property testers form an important class of sublinear algorithms. In the\nstandard property testing model, an algorithm accesses the input function via\nan oracle that returns function values at all queried domain points. In many\nrealistic situations, the oracle may be unable to reveal the function values at\nsome domain points due to privacy concerns, or when some of the values get\nerased by mistake or by an adversary.\n  We initiate a study of property testers that are resilient to the presence of\nadversarially erased function values. An alpha-erasure-resilient epsilon-tester\nfor a property P is given parameters alpha, epsilon in (0,1), along with oracle\naccess to a function f such that at most an alpha fraction of the function\nvalues have been erased. The tester does not know whether a point is erased\nunless it queries that point. The tester has to accept with high probability if\nthere is a way to assign values to the erased points such that the resulting\nfunction satisfies P. It has to reject with high probability if, for all\nassignments of values to the erased points, the resulting function has to be\nchanged in at least an epsilon-fraction of the nonerased domain points to\nsatisfy P.\n  We design erasure-resilient property testers for a large class of properties.\nFor some properties, it is possible to obtain erasure-resilient testers by\nusing standard testers as a black box. But there are more challenging\nproperties for which all known testers rely on querying a specific point. If\nthis point is erased, all these testers break. We give efficient\nerasure-resilient testers for several classes of such properties including\nmonotonicity, the Lipschitz property, and convexity. Finally, we describe a\nproperty that can be epsilon-tested with O(1/epsilon) queries in the standard\nmodel, whereas testing it in the erasure-resilient model requires number of\nqueries polynomial in the input size.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 00:32:23 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Dixit", "Kashyap", ""], ["Raskhodnikova", "Sofya", ""], ["Thakurta", "Abhradeep", ""], ["Varma", "Nithin", ""]]}, {"id": "1607.05791", "submitter": "Ioana O. Bercea", "authors": "Ioana O. Bercea, Volkan Isler, Samir Khuller", "title": "Minimizing Uncertainty through Sensor Placement with Angle Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of sensor placement in environments in which\nlocalization is a necessity, such as ad-hoc wireless sensor networks that allow\nthe placement of a few anchors that know their location or sensor arrays that\nare tracking a target. In most of these situations, the quality of localization\ndepends on the relative angle between the target and the pair of sensors\nobserving it. In this paper, we consider placing a small number of sensors\nwhich ensure good angular $\\alpha$-coverage: given $\\alpha$ in $[0,\\pi/2]$, for\neach target location $t$, there must be at least two sensors $s_1$ and $s_2$\nsuch that the $\\angle(s_1 t s_2)$ is in the interval $[\\alpha, \\pi-\\alpha]$.\nOne of the main difficulties encountered in such problems is that since the\nconstraints depend on at least two sensors, building a solution must account\nfor the inherent dependency between selected sensors, a feature that generic\nSet Cover techniques do not account for. We introduce a general framework that\nguarantees an angular coverage that is arbitrarily close to $\\alpha$ for any\n$\\alpha <= \\pi/3$ and apply it to a variety of problems to get bi-criteria\napproximations. When the angular coverage is required to be at least a constant\nfraction of $\\alpha$, we obtain results that are strictly better than what\nstandard geometric Set Cover methods give. When the angular coverage is\nrequired to be at least $(1-1/\\delta)\\cdot\\alpha$, we obtain a\n$\\mathcal{O}(\\log \\delta)$- approximation for sensor placement with\n$\\alpha$-coverage on the plane. In the presence of additional distance or\nvisibility constraints, the framework gives a $\\mathcal{O}(\\log\\delta\\cdot\\log\nk_{OPT})$-approximation, where $k_{OPT}$ is the size of the optimal solution.\nWe also use our framework to give a $\\mathcal{O}(\\log \\delta)$-approximation\nthat ensures $(1-1/\\delta)\\cdot \\alpha$-coverage and covers every target within\ndistance $3R$.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 00:52:59 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Bercea", "Ioana O.", ""], ["Isler", "Volkan", ""], ["Khuller", "Samir", ""]]}, {"id": "1607.05824", "submitter": "Haitao Wang", "authors": "Haitao Wang", "title": "On the Geodesic Centers of Polygonal Domains", "comments": "44 pages, 14 figures, a preliminary version to appear in ESA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of computing Euclidean geodesic centers\nof a polygonal domain $\\mathcal{P}$ with a total of $n$ vertices. We discover\nmany interesting observations. We give a necessary condition for a point being\na geodesic center. We show that there is at most one geodesic center among all\npoints of $\\mathcal{P}$ that have topologically-equivalent shortest path maps.\nThis implies that the total number of geodesic centers is bounded by the\ncombinatorial size of the shortest path map equivalence decomposition of\n$\\mathcal{P}$, which is known to be $O(n^{10})$. One key observation is a\n$\\pi$-range property on shortest path lengths when points are moving. With\nthese observations, we propose an algorithm that computes all geodesic centers\nin $O(n^{11}\\log n)$ time. Previously, an algorithm of $O(n^{12+\\epsilon})$\ntime was known for this problem, for any $\\epsilon>0$.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 04:54:07 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Wang", "Haitao", ""]]}, {"id": "1607.05850", "submitter": "Wolfgang Dvo\\v{r}\\'ak", "authors": "Krishnendu Chatterjee, Wolfgang Dvo\\v{r}\\'ak, Monika Henzinger,\n  Veronika Loitzenbauer", "title": "Conditionally Optimal Algorithms for Generalized B\\\"uchi Games", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.MFCS.2016.25", "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Games on graphs provide the appropriate framework to study several central\nproblems in computer science, such as the verification and synthesis of\nreactive systems. One of the most basic objectives for games on graphs is the\nliveness (or B\\\"uchi) objective that given a target set of vertices requires\nthat some vertex in the target set is visited infinitely often. We study\ngeneralized B\\\"uchi objectives (i.e., conjunction of liveness objectives), and\nimplications between two generalized B\\\"uchi objectives (known as GR(1)\nobjectives), that arise in numerous applications in computer-aided\nverification. We present improved algorithms and conditional super-linear lower\nbounds based on widely believed assumptions about the complexity of (A1)\ncombinatorial Boolean matrix multiplication and (A2) CNF-SAT. We consider graph\ngames with $n$ vertices, $m$ edges, and generalized B\\\"uchi objectives with $k$\nconjunctions. First, we present an algorithm with running time $O(k \\cdot\nn^2)$, improving the previously known $O(k \\cdot n \\cdot m)$ and $O(k^2 \\cdot\nn^2)$ worst-case bounds. Our algorithm is optimal for dense graphs under (A1).\nSecond, we show that the basic algorithm for the problem is optimal for sparse\ngraphs when the target sets have constant size under (A2). Finally, we consider\nGR(1) objectives, with $k_1$ conjunctions in the antecedent and $k_2$\nconjunctions in the consequent, and present an $O(k_1 \\cdot k_2 \\cdot\nn^{2.5})$-time algorithm, improving the previously known $O(k_1 \\cdot k_2 \\cdot\nn \\cdot m)$-time algorithm for $m > n^{1.5}$.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 07:57:57 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Dvo\u0159\u00e1k", "Wolfgang", ""], ["Henzinger", "Monika", ""], ["Loitzenbauer", "Veronika", ""]]}, {"id": "1607.05994", "submitter": "Omer Gold", "authors": "Omer Gold and Micha Sharir", "title": "Dynamic Time Warping and Geometric Edit Distance: Breaking the Quadratic\n  Barrier", "comments": "Removing the $\\log\\log\\log n$ factor from the runtime bound that\n  appeared in previous versions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Time Warping (DTW) and Geometric Edit Distance (GED) are basic\nsimilarity measures between curves or general temporal sequences (e.g., time\nseries) that are represented as sequences of points in some metric space $(X,\n\\mathrm{dist})$. The DTW and GED measures are massively used in various fields\nof computer science, computational biology, and engineering. Consequently, the\ntasks of computing these measures are among the core problems in P. Despite\nextensive efforts to find more efficient algorithms, the best-known algorithms\nfor computing the DTW or GED between two sequences of points in $X =\n\\mathbb{R}^d$ are long-standing dynamic programming algorithms that require\nquadratic runtime, even for the one-dimensional case $d = 1$, which is perhaps\none of the most used in practice.\n  In this paper, we break the nearly 50 years old quadratic time bound for\ncomputing DTW or GED between two sequences of $n$ points in $\\mathbb{R}$, by\npresenting deterministic algorithms that run in $O\\left( n^2 / \\log\\log n\n\\right)$ time. Our algorithms can be extended to work also for higher\ndimensional spaces $\\mathbb{R}^d$, for any constant $d$, when the underlying\ndistance-metric $\\mathrm{dist}$ is polyhedral (e.g., $L_1, L_\\infty$).\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 15:15:44 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 13:53:52 GMT"}, {"version": "v3", "created": "Sat, 5 Nov 2016 11:07:42 GMT"}, {"version": "v4", "created": "Tue, 28 Jan 2020 10:45:55 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Gold", "Omer", ""], ["Sharir", "Micha", ""]]}, {"id": "1607.06017", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li", "title": "Doubly Accelerated Methods for Faster CCA and Generalized\n  Eigendecomposition", "comments": "We have now stated more clearly why this paper has outperformed\n  relevant previous results, and included discussions for doubly-stochastic\n  methods. arXiv admin note: text overlap with arXiv:1607.03463", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study $k$-GenEV, the problem of finding the top $k$ generalized\neigenvectors, and $k$-CCA, the problem of finding the top $k$ vectors in\ncanonical-correlation analysis. We propose algorithms $\\mathtt{LazyEV}$ and\n$\\mathtt{LazyCCA}$ to solve the two problems with running times linearly\ndependent on the input size and on $k$.\n  Furthermore, our algorithms are DOUBLY-ACCELERATED: our running times depend\nonly on the square root of the matrix condition number, and on the square root\nof the eigengap. This is the first such result for both $k$-GenEV or $k$-CCA.\nWe also provide the first gap-free results, which provide running times that\ndepend on $1/\\sqrt{\\varepsilon}$ rather than the eigengap.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 16:43:18 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 03:18:24 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1607.06068", "submitter": "Michael Dinitz", "authors": "Eden Chlamt\\'a\\v{c}, Michael Dinitz, Guy Kortsarz, Bundit Laekhanukit", "title": "Approximating Spanners and Directed Steiner Forest: Upper and Lower\n  Bounds", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently found that there are very close connections between the\nexistence of additive spanners (subgraphs where all distances are preserved up\nto an additive stretch), distance preservers (subgraphs in which demand pairs\nhave their distance preserved exactly), and pairwise spanners (subgraphs in\nwhich demand pairs have their distance preserved up to a multiplicative or\nadditive stretch) [Abboud-Godwin SODA '16, Godwin-Williams SODA '16]. We study\nthese problems from an optimization point of view, where rather than studying\nthe existence of extremal instances we are given an instance and are asked to\nfind the sparsest possible spanner/preserver. We give an $O(n^{3/5 +\n\\epsilon})$-approximation for distance preservers and pairwise spanners (for\narbitrary constant $\\epsilon > 0$). This is the first nontrivial upper bound\nfor either problem, both of which are known to be as hard to approximate as\nLabel Cover. We also prove Label Cover hardness for approximating additive\nspanners, even for the cases of additive 1 stretch (where one might expect a\npolylogarithmic approximation, since the related multiplicative 2-spanner\nproblem admits an $O(\\log n)$-approximation) and additive polylogarithmic\nstretch (where the related multiplicative spanner problem has an\n$O(1)$-approximation).\n  Interestingly, the techniques we use in our approximation algorithm extend\nbeyond distance-based problem to pure connectivity network design problems. In\nparticular, our techniques allow us to give an $O(n^{3/5 +\n\\epsilon})$-approximation for the Directed Steiner Forest problem (for\narbitrary constant $\\epsilon > 0$) when all edges have uniform costs, improving\nthe previous best $O(n^{2/3 + \\epsilon})$-approximation due to Berman et\nal.~[ICALP '11] (which holds for general edge costs).\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 19:27:38 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Chlamt\u00e1\u010d", "Eden", ""], ["Dinitz", "Michael", ""], ["Kortsarz", "Guy", ""], ["Laekhanukit", "Bundit", ""]]}, {"id": "1607.06132", "submitter": "Marc Renault", "authors": "Spyros Angelopoulos, Marc P. Renault, and Pascal Schweitzer", "title": "Stochastic dominance and the bijective ratio of online algorithms", "comments": "Abridged abstract; full abstract in manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic dominance is a technique for evaluating the performance of online\nalgorithms that provides an intuitive, yet powerful stochastic order between\nthe compared algorithms. Accordingly this holds for bijective analysis, which\ncan be interpreted as stochastic dominance assuming the uniform distribution\nover requests. These techniques have been applied to some online problems, and\nhave provided a clear separation between algorithms whose performance varies\nsignificantly in practice. However, there are situations in which they are not\nreadily applicable due to the fact that they stipulate a stringent relation\nbetween the compared algorithms.\n  In this paper, we propose remedies for these shortcomings. First, we\nestablish sufficient conditions that allow us to prove the bijective optimality\nof a certain class of algorithms for a wide range of problems; we demonstrate\nthis approach in the context of some well-studied online problems. Second, to\naccount for situations in which two algorithms are incomparable or there is no\nclear optimum, we introduce the bijective ratio as a natural extension of\n(exact) bijective analysis. Our definition readily generalizes to stochastic\ndominance. This renders the concept of bijective analysis (and that of\nstochastic dominance) applicable to all online problems, and allows for the\nincorporation of other useful techniques such as amortized analysis. We\ndemonstrate the applicability of the bijective ratio to one of the fundamental\nonline problems, namely the continuous $k$-server problem on metrics such as\nthe line, the circle, and the star. Among other results, we show that the\ngreedy algorithm attains bijective ratios of $O(k)$ consistently across these\nmetrics. These results confirm extensive previous studies that gave evidence of\nthe efficiency of this algorithm on said metrics in practice, which, however,\nis not reflected in competitive analysis.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 21:36:54 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Angelopoulos", "Spyros", ""], ["Renault", "Marc P.", ""], ["Schweitzer", "Pascal", ""]]}, {"id": "1607.06141", "submitter": "Jonathan Ullman", "authors": "Lucas Kowalczyk, Tal Malkin, Jonathan Ullman, Mark Zhandry", "title": "Strong Hardness of Privacy from Weak Traitor Tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite much study, the computational complexity of differential privacy\nremains poorly understood. In this paper we consider the computational\ncomplexity of accurately answering a family $Q$ of statistical queries over a\ndata universe $X$ under differential privacy. A statistical query on a dataset\n$D \\in X^n$ asks \"what fraction of the elements of $D$ satisfy a given\npredicate $p$ on $X$?\" Dwork et al. (STOC'09) and Boneh and Zhandry (CRYPTO'14)\nshowed that if both $Q$ and $X$ are of polynomial size, then there is an\nefficient differentially private algorithm that accurately answers all the\nqueries, and if both $Q$ and $X$ are exponential size, then under a plausible\nassumption, no efficient algorithm exists.\n  We show that, under the same assumption, if either the number of queries or\nthe data universe is of exponential size, and the other has size at least\n$\\tilde{O}(n^7)$, then there is no differentially private algorithm that\nanswers all the queries. In both cases, the result is nearly quantitatively\ntight, since there is an efficient differentially private algorithm that\nanswers $\\tilde{\\Omega}(n^2)$ queries on an exponential size data universe, and\none that answers exponentially many queries on a data universe of size\n$\\tilde{\\Omega}(n^2)$.\n  Our proofs build on the connection between hardness results in differential\nprivacy and traitor-tracing schemes (Dwork et al., STOC'09; Ullman, STOC'13).\nWe prove our hardness result for a polynomial size query set (resp., data\nuniverse) by showing that they follow from the existence of a special type of\ntraitor-tracing scheme with very short ciphertexts (resp., secret keys), but\nvery weak security guarantees, and then constructing such a scheme.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 22:31:10 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Kowalczyk", "Lucas", ""], ["Malkin", "Tal", ""], ["Ullman", "Jonathan", ""], ["Zhandry", "Mark", ""]]}, {"id": "1607.06201", "submitter": "Edward Lee", "authors": "Serge Gaspers, Edward Lee", "title": "Faster Graph Coloring in Polynomial Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a polynomial-space algorithm that computes the number independent\nsets of any input graph in time $O(1.1387^n)$ for graphs with maximum degree 3\nand in time $O(1.2355^n)$ for general graphs, where n is the number of\nvertices. Together with the inclusion-exclusion approach of Bj\\\"orklund,\nHusfeldt, and Koivisto [SIAM J. Comput. 2009], this leads to a faster\npolynomial-space algorithm for the graph coloring problem with running time\n$O(2.2355^n)$. As a byproduct, we also obtain an exponential-space\n$O(1.2330^n)$ time algorithm for counting independent sets. Our main algorithm\ncounts independent sets in graphs with maximum degree 3 and no vertex with\nthree neighbors of degree 3. This polynomial-space algorithm is analyzed using\nthe recently introduced Separate, Measure and Conquer approach [Gaspers &\nSorkin, ICALP 2015]. Using Wahlstr\\\"om's compound measure approach, this\nimprovement in running time for small degree graphs is then bootstrapped to\nlarger degrees, giving the improvement for general graphs. Combining both\napproaches leads to some inflexibility in choosing vertices to branch on for\nthe small-degree cases, which we counter by structural graph properties.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 05:49:14 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 04:09:12 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Gaspers", "Serge", ""], ["Lee", "Edward", ""]]}, {"id": "1607.06203", "submitter": "Matus Telgarsky", "authors": "Daniel Hsu and Matus Telgarsky", "title": "Greedy bi-criteria approximations for $k$-medians and $k$-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the following natural greedy procedure for clustering\nin the bi-criterion setting: iteratively grow a set of centers, in each round\nadding the center from a candidate set that maximally decreases clustering\ncost. In the case of $k$-medians and $k$-means, the key results are as follows.\n  $\\bullet$ When the method considers all data points as candidate centers,\nthen selecting $\\mathcal{O}(k\\log(1/\\varepsilon))$ centers achieves cost at\nmost $2+\\varepsilon$ times the optimal cost with $k$ centers.\n  $\\bullet$ Alternatively, the same guarantees hold if each round samples\n$\\mathcal{O}(k/\\varepsilon^5)$ candidate centers proportionally to their\ncluster cost (as with $\\texttt{kmeans++}$, but holding centers fixed).\n  $\\bullet$ In the case of $k$-means, considering an augmented set of\n$n^{\\lceil1/\\varepsilon\\rceil}$ candidate centers gives $1+\\varepsilon$\napproximation with $\\mathcal{O}(k\\log(1/\\varepsilon))$ centers, the entire\nalgorithm taking\n$\\mathcal{O}(dk\\log(1/\\varepsilon)n^{1+\\lceil1/\\varepsilon\\rceil})$ time, where\n$n$ is the number of data points in $\\mathbb{R}^d$.\n  $\\bullet$ In the case of Euclidean $k$-medians, generating a candidate set\nvia $n^{\\mathcal{O}(1/\\varepsilon^2)}$ executions of stochastic gradient\ndescent with adaptively determined constraint sets will once again give\napproximation $1+\\varepsilon$ with $\\mathcal{O}(k\\log(1/\\varepsilon))$ centers\nin $dk\\log(1/\\varepsilon)n^{\\mathcal{O}(1/\\varepsilon^2)}$ time.\n  Ancillary results include: guarantees for cluster costs based on powers of\nmetrics; a brief, favorable empirical evaluation against $\\texttt{kmeans++}$;\ndata-dependent bounds allowing $1+\\varepsilon$ in the first two bullets above,\nfor example with $k$-medians over finite metric spaces.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 06:04:36 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Hsu", "Daniel", ""], ["Telgarsky", "Matus", ""]]}, {"id": "1607.06285", "submitter": "Celine Scornavacca", "authors": "Philippe Gambette, Leo van Iersel, Steven Kelk, Fabio Pardi and Celine\n  Scornavacca", "title": "Do branch lengths help to locate a tree in a phylogenetic network?", "comments": null, "journal-ref": null, "doi": "10.1007/s11538-016-0199-4", "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic networks are increasingly used in evolutionary biology to\nrepresent the history of species that have undergone reticulate events such as\nhorizontal gene transfer, hybrid speciation and recombination. One of the most\nfundamental questions that arise in this context is whether the evolution of a\ngene with one copy in all species can be explained by a given network. In\nmathematical terms, this is often translated in the following way: is a given\nphylogenetic tree contained in a given phylogenetic network? Recently this tree\ncontainment problem has been widely investigated from a computational\nperspective, but most studies have only focused on the topology of the phylo-\ngenies, ignoring a piece of information that, in the case of phylogenetic\ntrees, is routinely inferred by evolutionary analyses: branch lengths. These\nmeasure the amount of change (e.g., nucleotide substitutions) that has occurred\nalong each branch of the phylogeny. Here, we study a number of versions of the\ntree containment problem that explicitly account for branch lengths. We show\nthat, although length information has the potential to locate more precisely a\ntree within a network, the problem is computationally hard in its most general\nform. On a positive note, for a number of special cases of biological\nrelevance, we provide algorithms that solve this problem efficiently. This\nincludes the case of networks of limited complexity, for which it is possible\nto recover, among the trees contained by the network with the same topology as\nthe input tree, the closest one in terms of branch lengths.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 11:57:28 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Gambette", "Philippe", ""], ["van Iersel", "Leo", ""], ["Kelk", "Steven", ""], ["Pardi", "Fabio", ""], ["Scornavacca", "Celine", ""]]}, {"id": "1607.06442", "submitter": "Konstantin Makarychev", "authors": "Konstantin Makarychev and Yury Makarychev", "title": "Metric Perturbation Resilience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the notion of perturbation resilience introduced by Bilu and Linial\n(2010) and Awasthi, Blum, and Sheffet (2012). A clustering problem is\n$\\alpha$-perturbation resilient if the optimal clustering does not change when\nwe perturb all distances by a factor of at most $\\alpha$. We consider a class\nof clustering problems with center-based objectives, which includes such\nproblems as k-means, k-median, and k-center, and give an exact algorithm for\nclustering 2-perturbation resilient instances. Our result improves upon the\nresult of Balcan and Liang (2016), who gave an algorithm for clustering\n$1+\\sqrt{2}\\approx 2.41$ perturbation resilient instances. Our result is tight\nin the sense that no polynomial-time algorithm can solve\n$(2-\\varepsilon)$-perturbation resilient instances unless NP = RP, as was shown\nby Balcan, Haghtalab, and White (2016). We show that the algorithm works on\ninstances satisfying a slightly weaker and more natural condition than\nperturbation resilience, which we call metric perturbation resilience.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 19:37:04 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""]]}, {"id": "1607.06509", "submitter": "Saleh Soltan", "authors": "Saleh Soltan, Mihalis Yannakakis, and Gil Zussman", "title": "Doubly Balanced Connected Graph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study the Doubly Balanced Connected graph Partitioning\n(DBCP) problem: Let $G=(V,E)$ be a connected graph with a weight\n(supply/demand) function $p:V\\rightarrow \\{-1,+1\\}$ satisfying $p(V)=\\sum_{j\\in\nV} p(j)=0$. The objective is to partition $G$ into $(V_1,V_2)$ such that\n$G[V_1]$ and $G[V_2]$ are connected, $|p(V_1)|,|p(V_2)|\\leq c_p$, and\n$\\max\\{\\frac{|V_1|}{|V_2|},\\frac{|V_2|}{|V_1|}\\}\\leq c_s$, for some constants\n$c_p$ and $c_s$. When $G$ is 2-connected, we show that a solution with $c_p=1$\nand $c_s=3$ always exists and can be found in polynomial time. Moreover, when\n$G$ is 3-connected, we show that there is always a `perfect' solution (a\npartition with $p(V_1)=p(V_2)=0$ and $|V_1|=|V_2|$, if $|V|\\equiv 0\n(\\mathrm{mod}~4)$), and it can be found in polynomial time. Our techniques can\nbe extended, with similar results, to the case in which the weights are\narbitrary (not necessarily $\\pm 1$), and to the case that $p(V)\\neq 0$ and the\nexcess supply/demand should be split evenly. They also apply to the problem of\npartitioning a graph with two types of nodes into two large connected subgraphs\nthat preserve approximately the proportion of the two types.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 21:24:00 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Soltan", "Saleh", ""], ["Yannakakis", "Mihalis", ""], ["Zussman", "Gil", ""]]}, {"id": "1607.06517", "submitter": "Edith Cohen", "authors": "Edith Cohen", "title": "HyperLogLog Hyper Extended: Sketches for Concave Sublinear Frequency\n  Statistics", "comments": "15pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most common statistics computed over data elements is the number\nof distinct keys. A thread of research pioneered by Flajolet and Martin three\ndecades ago culminated in the design of optimal approximate counting sketches,\nwhich have size that is double logarithmic in the number of distinct keys and\nprovide estimates with a small relative error. Moreover, the sketches are\ncomposable, and thus suitable for streamed, parallel, or distributed\ncomputation.\n  We consider here all statistics of the frequency distribution of keys, where\na contribution of a key to the aggregate is concave and grows (sub)linearly\nwith its frequency. These fundamental aggregations are very common in text,\ngraphs, and logs analysis and include logarithms, low frequency moments, and\ncapping statistics.\n  We design composable sketches of double-logarithmic size for all concave\nsublinear statistics. Our design combines theoretical optimality and practical\nsimplicity. In a nutshell, we specify tailored mapping functions of data\nelements to output elements so that our target statistics on the data elements\nis approximated by the (max-) distinct statistics of the output elements, which\ncan be approximated using off-the-shelf sketches. Our key insight is relating\nthese target statistics to the {\\em complement Laplace} transform of the input\nfrequencies.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 21:58:37 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 19:08:13 GMT"}, {"version": "v3", "created": "Fri, 14 Oct 2016 18:51:18 GMT"}, {"version": "v4", "created": "Wed, 2 Nov 2016 17:27:15 GMT"}, {"version": "v5", "created": "Fri, 24 Feb 2017 03:06:17 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Cohen", "Edith", ""]]}, {"id": "1607.06618", "submitter": "Steffen Rechner", "authors": "Marius Erbert, Steffen Rechner, Matthias M\\\"uller-Hannemann", "title": "Gerbil: A Fast and Memory-Efficient $k$-mer Counter with GPU-Support", "comments": "A short version of this paper will appear in the proceedings of WABI\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic task in bioinformatics is the counting of $k$-mers in genome strings.\nThe $k$-mer counting problem is to build a histogram of all substrings of\nlength $k$ in a given genome sequence. We present the open source $k$-mer\ncounting software Gerbil that has been designed for the efficient counting of\n$k$-mers for $k\\geq32$. Given the technology trend towards long reads of\nnext-generation sequencers, support for large $k$ becomes increasingly\nimportant. While existing $k$-mer counting tools suffer from excessive memory\nresource consumption or degrading performance for large $k$, Gerbil is able to\nefficiently support large $k$ without much loss of performance. Our software\nimplements a two-disk approach. In the first step, DNA reads are loaded from\ndisk and distributed to temporary files that are stored at a working disk. In a\nsecond step, the temporary files are read again, split into $k$-mers and\ncounted via a hash table approach. In addition, Gerbil can optionally use GPUs\nto accelerate the counting step. For large $k$, we outperform state-of-the-art\nopen source $k$-mer counting tools for large genome data sets.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 09:48:53 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Erbert", "Marius", ""], ["Rechner", "Steffen", ""], ["M\u00fcller-Hannemann", "Matthias", ""]]}, {"id": "1607.06660", "submitter": "Nicola Prezza", "authors": "Alberto Policriti, Nicola Prezza", "title": "Fast Longest Common Extensions in Small Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the longest common extension (LCE) problem: to\ncompute the length $\\ell$ of the longest common prefix between any two suffixes\nof $T\\in \\Sigma^n$ with $ \\Sigma = \\{0, \\ldots \\sigma-1\\} $. We present two\nfast and space-efficient solutions based on (Karp-Rabin)\n\\textit{fingerprinting} and \\textit{sampling}. Our first data structure\nexploits properties of Mersenne prime numbers when used as moduli of the\nKarp-Rabin hash function and takes $n\\lceil \\log_2\\sigma\\rceil$ bits of space.\nOur second structure works with any prime modulus and takes $n\\lceil\n\\log_2\\sigma\\rceil + n/w + w\\log_2 n$ bits of space ($ w $ memory-word size).\nBoth structures support $\\mathcal O\\left(m\\log\\sigma/w \\right)$-time extraction\nof any length-$m$ text substring, $\\mathcal O(\\log\\ell)$-time LCE queries with\nhigh probability, and can be built in optimal $\\mathcal O(n)$ time. In the\nfirst case, ours is the first result showing that it is possible to answer LCE\nqueries in $o(n)$ time while using only $\\mathcal O(1)$ words on top of the\nspace required to store the text. Our results improve the state of the art in\nspace usage, query times, and preprocessing times and are extremely practical:\nwe present a C++ implementation that is very fast and space-efficient in\npractice.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 12:54:30 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Policriti", "Alberto", ""], ["Prezza", "Nicola", ""]]}, {"id": "1607.06665", "submitter": "Steven Chaplick", "authors": "Steven Chaplick, Minati De, Alexander Ravsky, Joachim Spoerhase", "title": "Approximation Schemes for Geometric Coverage Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their seminal work, Mustafa and Ray (2009) showed that a wide class of\ngeometric set cover (SC) problems admit a PTAS via local search -- this is one\nof the most general approaches known for such problems. Their result applies if\na naturally defined \"exchange graph\" for two feasible solutions is planar and\nis based on subdividing this graph via a planar separator theorem due to\nFrederickson (1987). Obtaining similar results for the related maximum\nk-coverage problem (MC) seems non-trivial due to the hard cardinality\nconstraint. In fact, while Badanidiyuru, Kleinberg, and Lee (2012) have shown\n(via a different analysis) that local search yields a PTAS for two-dimensional\nreal halfspaces, they only conjectured that the same holds true for dimension\nthree. Interestingly, at this point it was already known that local search\nprovides a PTAS for the corresponding set cover case and this followed directly\nfrom the approach of Mustafa and Ray.\n  In this work we provide a way to address the above-mentioned issue. First, we\npropose a color-balanced version of the planar separator theorem. The resulting\nsubdivision approximates locally in each part the global distribution of the\ncolors. Second, we show how this roughly balanced subdivision can be employed\nin a more careful analysis to strictly obey the hard cardinality constraint.\nMore specifically, we obtain a PTAS for any \"planarizable\" instance of MC and\nthus essentially for all cases where the corresponding SC instance can be\ntackled via the approach of Mustafa and Ray. As a corollary, we confirm the\nconjecture of Badanidiyuru, Kleinberg, and Lee regarding real half spaces in\ndimension three. We feel that our ideas could also be helpful in other\ngeometric settings involving a cardinality constraint.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 13:02:16 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 10:00:49 GMT"}, {"version": "v3", "created": "Thu, 23 Feb 2017 14:34:06 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Chaplick", "Steven", ""], ["De", "Minati", ""], ["Ravsky", "Alexander", ""], ["Spoerhase", "Joachim", ""]]}, {"id": "1607.06711", "submitter": "Ankit Garg", "authors": "Ankit Garg and Leonid Gurvits and Rafael Oliveira and Avi Wigderson", "title": "Algorithmic and optimization aspects of Brascamp-Lieb inequalities, via\n  Operator Scaling", "comments": "Fixed a bug in the proof of Lemma 8.1. We would like to thank\n  Nisheeth Vishnoi and Damian Straszak for pointing out this bug and also\n  allowing us to use their fix for it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The celebrated Brascamp-Lieb (BL) inequalities (and their extensions) are an\nimportant mathematical tool, unifying and generalizing numerous inequalities in\nanalysis, convex geometry and information theory. While their structural theory\nis very well understood, far less is known about computing their main\nparameters.\n  We give polynomial time algorithms to compute feasibility of BL-datum, the\noptimal BL-constant and a weak separation oracle for the BL-polytope. The same\nresult holds for the so-called Reverse BL inequalities of Barthe. The best\nknown algorithms for any of these tasks required at least exponential time.\n  The algorithms are obtained by a simple efficient reduction of a given\nBL-datum to an instance of the Operator Scaling problem defined by Gurvits, for\nwhich the present authors have provided a polynomial time algorithm. This\nreduction implies algorithmic versions of many of the known structural results,\nand in some cases provide proofs that are different or simpler than existing\nones.\n  Of particular interest is the fact that the operator scaling algorithm is\ncontinuous in its input. Thus as a simple corollary of our reduction we obtain\nexplicit bounds on the magnitude and continuity of the BL-constant in terms of\nthe BL-data. To the best of our knowledge no such bounds were known, as past\narguments relied on compactness. The continuity of BL-constants is important\nfor developing non-linear BL inequalities that have recently found so many\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 15:27:42 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 21:28:05 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 19:05:35 GMT"}, {"version": "v4", "created": "Fri, 13 Apr 2018 00:00:12 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Garg", "Ankit", ""], ["Gurvits", "Leonid", ""], ["Oliveira", "Rafael", ""], ["Wigderson", "Avi", ""]]}, {"id": "1607.06751", "submitter": "Mordechai Shalom", "authors": "Didem G\\\"oz\\\"upek and Mordechai Shalom", "title": "Edge Coloring with Minimum Reload/Changeover Costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an edge-colored graph, a traversal cost occurs at a vertex along a path\nwhen consecutive edges with different colors are traversed. The value of the\ntraversal cost depends only on the colors of the traversed edges. This concept\nleads to two global cost measures, namely the \\emph{reload cost} and the\n\\emph{changeover cost}, that have been studied in the literature and have\nvarious applications in telecommunications, transportation networks, and energy\ndistribution networks. Previous work focused on problems with an edge-colored\ngraph being part of the input. In this paper, we formulate and focus on two\npairs of problems that aim to find an edge coloring of a graph so as to\nminimize the reload and changeover costs. The first pair of problems aims to\nfind a proper edge coloring so that the reload/changeover cost of a set of\npaths is minimized. The second pair of problems aim to find a proper edge\ncoloring and a spanning tree so that the reload/changeover cost is minimized.\nWe present several hardness results as well as polynomial-time solvable special\ncases.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 17:17:59 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["G\u00f6z\u00fcpek", "Didem", ""], ["Shalom", "Mordechai", ""]]}, {"id": "1607.06757", "submitter": "Konrad Dabrowski", "authors": "Alexandre Blanch\\'e and Konrad K. Dabrowski and Matthew Johnson and\n  Dani\\\"el Paulusma", "title": "Hereditary Graph Classes: When the Complexities of Colouring and Clique\n  Cover Coincide", "comments": "19 Pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is $(H_1,H_2)$-free for a pair of graphs $H_1,H_2$ if it contains no\ninduced subgraph isomorphic to $H_1$ or $H_2$. In 2001, Kr\\'al',\nKratochv\\'{\\i}l, Tuza, and Woeginger initiated a study into the complexity of\nColouring for $(H_1,H_2)$-free graphs. Since then, others have tried to\ncomplete their study, but many cases remain open. We focus on those\n$(H_1,H_2)$-free graphs where $H_2$ is $\\overline{H_1}$, the complement of\n$H_1$. As these classes are closed under complementation, the computational\ncomplexities of Colouring and Clique Cover coincide. By combining new and known\nresults, we are able to classify the complexity of Colouring and Clique Cover\nfor $(H,\\overline{H})$-free graphs for all cases except when $H=sP_1+ P_3$ for\n$s\\geq 3$ or $H=sP_1+P_4$ for $s\\geq 2$. We also classify the complexity of\nColouring on graph classes characterized by forbidding a finite number of\nself-complementary induced subgraphs, and we initiate a study of $k$-Colouring\nfor $(P_r,\\overline{P_r})$-free graphs.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 17:32:39 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 20:29:41 GMT"}, {"version": "v3", "created": "Wed, 7 Jun 2017 11:10:25 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Blanch\u00e9", "Alexandre", ""], ["Dabrowski", "Konrad K.", ""], ["Johnson", "Matthew", ""], ["Paulusma", "Dani\u00ebl", ""]]}, {"id": "1607.06865", "submitter": "Seth Pettie", "authors": "Ran Duan and Seth Pettie", "title": "Connectivity Oracles for Graphs Subject to Vertex Failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new data structures for answering connectivity queries in graphs\nsubject to batched vertex failures. A deterministic structure processes a batch\nof $d\\leq d_{\\star}$ failed vertices in $\\tilde{O}(d^3)$ time and thereafter\nanswers connectivity queries in $O(d)$ time. It occupies space $O(d_{\\star}\nm\\log n)$. We develop a randomized Monte Carlo version of our data structure\nwith update time $\\tilde{O}(d^2)$, query time $O(d)$, and space $\\tilde{O}(m)$\nfor any failure bound $d\\le n$. This is the first connectivity oracle for\ngeneral graphs that can efficiently deal with an unbounded number of vertex\nfailures.\n  We also develop a more efficient Monte Carlo edge-failure connectivity\noracle. Using space $O(n\\log^2 n)$, $d$ edge failures are processed in $O(d\\log\nd\\log\\log n)$ time and thereafter, connectivity queries are answered in\n$O(\\log\\log n)$ time, which are correct w.h.p.\n  Our data structures are based on a new decomposition theorem for an\nundirected graph $G=(V,E)$, which is of independent interest. It states that\nfor any terminal set $U\\subseteq V$ we can remove a set $B$ of $|U|/(s-2)$\nvertices such that the remaining graph contains a Steiner forest for $U-B$ with\nmaximum degree $s$.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 00:19:08 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 01:01:51 GMT"}, {"version": "v3", "created": "Wed, 6 Sep 2017 21:27:26 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Duan", "Ran", ""], ["Pettie", "Seth", ""]]}, {"id": "1607.06883", "submitter": "Peter Robinson", "authors": "Gopal Pandurangan, Peter Robinson, Michele Scquizzato", "title": "A Time- and Message-Optimal Distributed Algorithm for Minimum Spanning\n  Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a randomized Las Vegas distributed algorithm that\nconstructs a minimum spanning tree (MST) in weighted networks with optimal (up\nto polylogarithmic factors) time and message complexity. This algorithm runs in\n$\\tilde{O}(D + \\sqrt{n})$ time and exchanges $\\tilde{O}(m)$ messages (both with\nhigh probability), where $n$ is the number of nodes of the network, $D$ is the\ndiameter, and $m$ is the number of edges. This is the first distributed MST\nalgorithm that matches \\emph{simultaneously} the time lower bound of\n$\\tilde{\\Omega}(D + \\sqrt{n})$ [Elkin, SIAM J. Comput. 2006] and the message\nlower bound of $\\Omega(m)$ [Kutten et al., J.ACM 2015] (which both apply to\nrandomized algorithms).\n  The prior time and message lower bounds are derived using two completely\ndifferent graph constructions; the existing lower bound construction that shows\none lower bound {\\em does not} work for the other. To complement our algorithm,\nwe present a new lower bound graph construction for which any distributed MST\nalgorithm requires \\emph{both} $\\tilde{\\Omega}(D + \\sqrt{n})$ rounds and\n$\\Omega(m)$ messages.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 03:22:38 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 17:49:23 GMT"}, {"version": "v3", "created": "Wed, 24 Jan 2018 04:08:38 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Pandurangan", "Gopal", ""], ["Robinson", "Peter", ""], ["Scquizzato", "Michele", ""]]}, {"id": "1607.07073", "submitter": "Loukas Georgiadis", "authors": "Loukas Georgiadis and Giuseppe F. Italiano and Nikos Parotsidis", "title": "Incremental $2$-Edge-Connectivity in Directed Graphs", "comments": "Full version of paper presented at ICALP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we initiate the study of the dynamic maintenance of\n$2$-edge-connectivity relationships in directed graphs. We present an algorithm\nthat can update the $2$-edge-connected blocks of a directed graph with $n$\nvertices through a sequence of $m$ edge insertions in a total of $O(mn)$ time.\nAfter each insertion, we can answer the following queries in asymptotically\noptimal time: (i) Test in constant time if two query vertices $v$ and $w$ are\n$2$-edge-connected. Moreover, if $v$ and $w$ are not $2$-edge-connected, we can\nproduce in constant time a \"witness\" of this property, by exhibiting an edge\nthat is contained in all paths from $v$ to $w$ or in all paths from $w$ to $v$.\n(ii) Report in $O(n)$ time all the $2$-edge-connected blocks of $G$. To the\nbest of our knowledge, this is the first dynamic algorithm for $2$-connectivity\nproblems on directed graphs, and it matches the best known bounds for simpler\nproblems, such as incremental transitive closure.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 17:47:11 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Georgiadis", "Loukas", ""], ["Italiano", "Giuseppe F.", ""], ["Parotsidis", "Nikos", ""]]}, {"id": "1607.07200", "submitter": "Vivek Madan", "authors": "Chandra Chekuri, Vivek Madan", "title": "Approximating Multicut and the Demand Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the minimum Multicut problem, the input is an edge-weighted supply graph\n$G=(V,E)$ and a simple demand graph $H=(V,F)$. Either $G$ and $H$ are directed\n(DMulC) or both are undirected (UMulC). The goal is to remove a minimum weight\nset of edges in $G$ such that there is no path from $s$ to $t$ in the remaining\ngraph for any $(s,t) \\in F$. UMulC admits an $O(\\log k)$-approximation where\n$k$ is the vertex cover size of $H$ while the best known approximation for\nDMulC is $\\min\\{k, \\tilde{O}(n^{11/23})\\}$. These approximations are obtained\nby proving corresponding results on the multicommodity flow-cut gap. In\ncontrast to these results some special cases of Multicut, such as the\nwell-studied Multiway Cut problem, admit a constant factor approximation in\nboth undirected and directed graphs. Motivated by both concrete instances from\napplications and abstract considerations, we consider the role that the\nstructure of the demand graph $H$ plays in determining the approximability of\nMulticut.\n  In undirected graphs our main result is a $2$-approximation in $n^{O(t)}$\ntime when the demand graph $H$ excludes an induced matching of size $t$. This\ngives a constant factor approximation for a specific demand graph that\nmotivated this work.\n  In contrast to undirected graphs, we prove that in directed graphs such\napproximation algorithms can not exist. Assuming the Unique Games Conjecture\n(UGC), for a large class of fixed demand graphs DMulC cannot be approximated to\na factor better than worst-case flow-cut gap. As a consequence we prove that\nfor any fixed $k$, assuming UGC, DMulC with $k$ demand pairs is hard to\napproximate to within a factor better than $k$. On the positive side, we prove\nan approximation of $k$ when the demand graph excludes certain graphs as an\ninduced subgraph. This generalizes the Multiway Cut result to a much larger\nclass of demand graphs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 10:54:18 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Chekuri", "Chandra", ""], ["Madan", "Vivek", ""]]}, {"id": "1607.07306", "submitter": "Ragavendran Gopalakrishnan", "authors": "Ragavendran Gopalakrishnan and Koyel Mukherjee and Theja Tulabandhula", "title": "The Costs and Benefits of Sharing: Sequential Individual Rationality and\n  Sequential Fairness", "comments": "Presented as a poster at EC 2016. Presented as an invited talk\n  (sponsored session) at INFORMS Annual Meeting 2016. Presented at MSOM Service\n  Operations SIG 2017. Currently under review at Management Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In designing dynamic shared service systems that incentivize customers to opt\nfor shared rather than exclusive service, the traditional notion of individual\nrationality may be insufficient, as a customer's estimated utility could\nfluctuate arbitrarily during their time in the shared system, as long as their\nrealized utility at service completion is not worse than that for exclusive\nservice. In this work, within a model that explicitly considers the\n\"inconvenience costs\" incurred by customers due to sharing, we introduce the\nnotion of sequential individual rationality (SIR) that requires that the\ndisutility of existing customers is nonincreasing as the system state changes\ndue to new customer arrivals. Next, under SIR, we observe that cost sharing can\nalso be viewed as benefit sharing, which inspires a natural definition of\nsequential fairness (SF) - the total incremental benefit due to a new customer\nis shared among existing customers in proportion to the incremental\ninconvenience suffered.\n  We demonstrate the effectiveness of these notions by applying them to a\nridesharing system, where unexpected detours to pick up subsequent passengers\ninconvenience the existing passengers. Imposing SIR and SF reveals interesting\nand surprising results, including: (a) natural limits on the incremental\ndetours permissible, (b) exact characterization of \"SIR-feasible\" routes, which\nboast sublinear upper and lower bounds on the fractional detours, (c) exact\ncharacterization of sequentially fair cost sharing schemes, which includes a\nstrong requirement that passengers must compensate each other for the detour\ninconveniences that they cause, and (d) new algorithmic problems related to and\nmotivated by SIR.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 15:08:45 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 22:08:08 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Gopalakrishnan", "Ragavendran", ""], ["Mukherjee", "Koyel", ""], ["Tulabandhula", "Theja", ""]]}, {"id": "1607.07431", "submitter": "Ainesh Bakshi", "authors": "Ainesh Bakshi and Nadiia Chepurko", "title": "Polynomial Time Algorithm for $2$-Stable Clustering Instances", "comments": "Bug in Lemma 3.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering with most objective functions is NP-Hard, even to approximate well\nin the worst case. Recently, there has been work on exploring different notions\nof stability which lend structure to the problem. The notion of stability,\n$\\alpha$-perturbation resilience, that we study in this paper was originally\nintroduced by Bilu et al.~\\cite{Bilu10}. The works of Awasthi et\nal~\\cite{Awasthi12} and Balcan et al.~\\cite{Balcan12} provide a polynomial time\nalgorithm for $3$-stable and $(1+\\sqrt{2})$-stable instances respectively. This\npaper provides a polynomial time algorithm for $2$-stable instances, improving\non and answering an open question in ~\\cite{Balcan12}.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 19:53:40 GMT"}, {"version": "v2", "created": "Sat, 11 Feb 2017 17:56:34 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Bakshi", "Ainesh", ""], ["Chepurko", "Nadiia", ""]]}, {"id": "1607.07497", "submitter": "Greg Bodwin", "authors": "Amir Abboud, Greg Bodwin, and Seth Pettie", "title": "A Hierarchy of Lower Bounds for Sublinear Additive Spanners", "comments": "Accepted to SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spanners, emulators, and approximate distance oracles can be viewed as lossy\ncompression schemes that represent an unweighted graph metric in small space,\nsay $\\tilde{O}(n^{1+\\delta})$ bits. There is an inherent tradeoff between the\nsparsity parameter $\\delta$ and the stretch function $f$ of the compression\nscheme, but the qualitative nature of this tradeoff has remained a persistent\nopen problem.\n  In this paper we show that the recent additive spanner lower bound of Abboud\nand Bodwin is just the first step in a hierarchy of lower bounds that fully\ncharacterize the asymptotic behavior of the optimal stretch function $f$ as a\nfunction of $\\delta \\in (0,1/3)$. Specifically, for any integer $k\\ge 2$, any\ncompression scheme with size $O(n^{1+\\frac{1}{2^k-1} - \\epsilon})$ has a\nsublinear additive stretch function $f$: $$f(d) = d +\n\\Omega(d^{1-\\frac{1}{k}}).$$ This lower bound matches Thorup and Zwick's (2006)\nconstruction of sublinear additive emulators. It also shows that Elkin and\nPeleg's $(1+\\epsilon,\\beta)$-spanners have an essentially optimal tradeoff\nbetween $\\delta,\\epsilon,$ and $\\beta$, and that the sublinear additive\nspanners of Pettie (2009) and Chechik (2013) are not too far from optimal.\n  To complement these lower bounds we present a new construction of\n$(1+\\epsilon, O(k/\\epsilon)^{k-1})$-spanners with size $O((k/\\epsilon)^{h_k}\nkn^{1+\\frac{1}{2^{k+1}-1}})$, where $h_k < 3/4$. This size bound improves on\nthe spanners of Elkin and Peleg (2004), Thorup and Zwick (2006), and Pettie\n(2009). According to our lower bounds neither the size nor stretch function can\nbe substantially improved.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 22:05:27 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 00:47:37 GMT"}, {"version": "v3", "created": "Tue, 10 Jan 2017 14:11:00 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Abboud", "Amir", ""], ["Bodwin", "Greg", ""], ["Pettie", "Seth", ""]]}, {"id": "1607.07553", "submitter": "Goran \\v{Z}u\\v{z}i\\'c", "authors": "Bernhard Haeupler, Taisuke Izumi, Goran Zuzic", "title": "Low-Congestion Shortcuts without Embedding", "comments": "Proceedings of the 2016 ACM Symposium on Principles of Distributed\n  Computing. ACM (2016) Distributed Computing, 2020 (DIST)", "journal-ref": null, "doi": "10.1145/2933057.2933112 10.1007/s00446-020-00383-2", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed optimization algorithms are frequently faced with solving\nsub-problems on disjoint connected parts of a network. Unfortunately, the\ndiameter of these parts can be significantly larger than the diameter of the\nunderlying network, leading to slow running times. Recent work by [Ghaffari and\nHauepler; SODA'16] showed that this phenomenon can be seen as the broad\nunderlying reason for the pervasive $\\Omega(\\sqrt{n} + D)$ lower bounds that\napply to most optimization problems in the CONGEST model. On the positive side,\nthis work also introduced low-congestion shortcuts as an elegant solution to\ncircumvent this problem in certain topologies of interest. Particularly, they\nshowed that there exist good shortcuts for any planar network and more\ngenerally any bounded genus network. This directly leads to fast $O(D\n\\log^{O(1)} n)$ distributed algorithms for MST and Min-Cut approximation, given\nthat one can efficiently construct these shortcuts in a distributed manner.\n  Unfortunately, the shortcut construction of [Ghaffari and Hauepler; SODA'16]\nrelies heavily on having access to a genus embedding of the network. Computing\nsuch an embedding distributedly, however, is a hard problem - even for planar\nnetworks. No distributed embedding algorithm for bounded genus graphs is in\nsight.\n  In this work, we side-step this problem by defining a restricted and more\nstructured form of shortcuts and giving a novel construction algorithm which\nefficiently finds a shortcut which is, up to a logarithmic factor, as good as\nthe best shortcut that exists for a given network. This new construction\nalgorithm directly leads to an $O(D \\log^{O(1)} n)$-round algorithm for solving\noptimization problems like MST for any topology for which good restricted\nshortcuts exist - without the need to compute any embedding. This includes the\nfirst efficient algorithm for bounded genus graphs.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 06:24:18 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 05:56:01 GMT"}, {"version": "v3", "created": "Sun, 23 Aug 2020 00:20:33 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Izumi", "Taisuke", ""], ["Zuzic", "Goran", ""]]}, {"id": "1607.07631", "submitter": "Jakub Tarnawski", "authors": "Christos Kalaitzis, Ola Svensson, Jakub Tarnawski", "title": "Unrelated Machine Scheduling of Jobs with Uniform Smith Ratios", "comments": "Accepted to ACM-SIAM Symposium on Discrete Algorithms (SODA) 2017", "journal-ref": "Proc. of 28th Annual ACM-SIAM Symposium on Discrete Algorithms\n  (SODA), 2017, pages 2654-2669", "doi": "10.1137/1.9781611974782.175", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classic problem of scheduling jobs on unrelated machines so\nas to minimize the weighted sum of completion times. Recently, for a small\nconstant $\\varepsilon >0 $, Bansal et al. gave a\n$(3/2-\\varepsilon)$-approximation algorithm improving upon the natural barrier\nof $3/2$ which follows from independent randomized rounding. In simplified\nterms, their result is obtained by an enhancement of independent randomized\nrounding via strong negative correlation properties.\n  In this work, we take a different approach and propose to use the same\nelegant rounding scheme for the weighted completion time objective as devised\nby Shmoys and Tardos for optimizing a linear function subject to makespan\nconstraints. Our main result is a $1.21$-approximation algorithm for the\nnatural special case where the weight of a job is proportional to its\nprocessing time (specifically, all jobs have the same Smith ratio), which\nexpresses the notion that each unit of work has the same weight. In addition,\nas a direct consequence of the rounding, our algorithm also achieves a\nbi-criteria $2$-approximation for the makespan objective. Our technical\ncontribution is a tight analysis of the expected cost of the solution compared\nto the one given by the Configuration-LP relaxation - we reduce this task to\nthat of understanding certain worst-case instances which are simple to analyze.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 10:37:46 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 13:45:59 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Kalaitzis", "Christos", ""], ["Svensson", "Ola", ""], ["Tarnawski", "Jakub", ""]]}, {"id": "1607.07647", "submitter": "Florian Meyer", "authors": "Florian Meyer, Paolo Braca, Peter Willett, Franz Hlawatsch", "title": "A Scalable Algorithm for Tracking an Unknown Number of Targets Using\n  Multiple Sensors", "comments": "13 pages, 8 figure", "journal-ref": null, "doi": "10.1109/TSP.2017.2688966", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for tracking an unknown number of targets based on\nmeasurements provided by multiple sensors. Our method achieves low\ncomputational complexity and excellent scalability by running belief\npropagation on a suitably devised factor graph. A redundant formulation of data\nassociation uncertainty and the use of \"augmented target states\" including\nbinary target indicators make it possible to exploit statistical independencies\nfor a drastic reduction of complexity. An increase in the number of targets,\nsensors, or measurements leads to additional variable nodes in the factor graph\nbut not to higher dimensions of the messages. As a consequence, the complexity\nof our method scales only quadratically in the number of targets, linearly in\nthe number of sensors, and linearly in the number of measurements per sensors.\nThe performance of the method compares well with that of previously proposed\nmethods, including methods with a less favorable scaling behavior. In\nparticular, our method can outperform multisensor versions of the probability\nhypothesis density (PHD) filter, the cardinalized PHD filter, and the\nmulti-Bernoulli filter.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 11:28:14 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Meyer", "Florian", ""], ["Braca", "Paolo", ""], ["Willett", "Peter", ""], ["Hlawatsch", "Franz", ""]]}, {"id": "1607.07673", "submitter": "Adrian Dumitrescu", "authors": "Adrian Dumitrescu", "title": "A Selectable Sloppy Heap", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the selection problem, namely that of computing the $i$th order\nstatistic of $n$ given elements. Here we offer a data structure called\n\\emph{selectable sloppy heap} handling a dynamic version in which upon request:\n(i)~a new element is inserted or (ii)~an element of a prescribed quantile group\nis deleted from the data structure. Each operation is executed in (ideal!)\nconstant time---and is thus independent of $n$ (the number of elements stored\nin the data structure)---provided that the number of quantile groups is fixed.\nThis is the first result of this kind accommodating both insertion and deletion\nin constant time. As such, our data structure outperforms the soft heap data\nstructure of Chazelle (which only offers constant amortized complexity for a\nfixed error rate $0<\\varepsilon \\leq 1/2$) in applications such as dynamic\npercentile maintenance. The design demonstrates how slowing down a certain\ncomputation can speed up the data structure.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 13:01:15 GMT"}, {"version": "v2", "created": "Thu, 10 Aug 2017 02:50:31 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Dumitrescu", "Adrian", ""]]}, {"id": "1607.07676", "submitter": "Pawe{\\l} Rz\\k{a}\\.zewski", "authors": "\\'Edouard Bonnet, Tillmann Miltzow, Pawe{\\l} Rz\\k{a}\\.zewski", "title": "Complexity of Token Swapping and its Variants", "comments": "23 pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Token Swapping problem we are given a graph with a token placed on\neach vertex. Each token has exactly one destination vertex, and we try to move\nall the tokens to their destinations, using the minimum number of swaps, i.e.,\noperations of exchanging the tokens on two adjacent vertices. As the main\nresult of this paper, we show that Token Swapping is $W[1]$-hard parameterized\nby the length $k$ of a shortest sequence of swaps. In fact, we prove that, for\nany computable function $f$, it cannot be solved in time $f(k)n^{o(k / \\log\nk)}$ where $n$ is the number of vertices of the input graph, unless the ETH\nfails. This lower bound almost matches the trivial $n^{O(k)}$-time algorithm.\n  We also consider two generalizations of the Token Swapping, namely Colored\nToken Swapping (where the tokens have different colors and tokens of the same\ncolor are indistinguishable), and Subset Token Swapping (where each token has a\nset of possible destinations). To complement the hardness result, we prove that\neven the most general variant, Subset Token Swapping, is FPT in nowhere-dense\ngraph classes.\n  Finally, we consider the complexities of all three problems in very\nrestricted classes of graphs: graphs of bounded treewidth and diameter, stars,\ncliques, and paths, trying to identify the borderlines between polynomial and\nNP-hard cases.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 13:09:50 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 12:25:23 GMT"}, {"version": "v3", "created": "Fri, 5 Jan 2018 11:05:30 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Miltzow", "Tillmann", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""]]}, {"id": "1607.07737", "submitter": "Holger Dell", "authors": "Ivona Bez\\'akov\\'a, Radu Curticapean, Holger Dell and Fedor V. Fomin", "title": "Finding Detours is Fixed-parameter Tractable", "comments": "Extended abstract appears at ICALP 2017", "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2017.54", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the following natural \"above guarantee\" parameterization of the\nclassical Longest Path problem: For given vertices s and t of a graph G, and an\ninteger k, the problem Longest Detour asks for an (s,t)-path in G that is at\nleast k longer than a shortest (s,t)-path. Using insights into structural graph\ntheory, we prove that Longest Detour is fixed-parameter tractable (FPT) on\nundirected graphs and actually even admits a single-exponential algorithm, that\nis, one of running time exp(O(k)) poly(n). This matches (up to the base of the\nexponential) the best algorithms for finding a path of length at least k.\n  Furthermore, we study the related problem Exact Detour that asks whether a\ngraph G contains an (s,t)-path that is exactly k longer than a shortest\n(s,t)-path. For this problem, we obtain a randomized algorithm with running\ntime about 2.746^k, and a deterministic algorithm with running time about\n6.745^k, showing that this problem is FPT as well. Our algorithms for Exact\nDetour apply to both undirected and directed graphs.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 15:00:18 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 16:22:53 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Bez\u00e1kov\u00e1", "Ivona", ""], ["Curticapean", "Radu", ""], ["Dell", "Holger", ""], ["Fomin", "Fedor V.", ""]]}, {"id": "1607.07818", "submitter": "Sariel Har-Peled", "authors": "Sariel Har-Peled", "title": "Computing the k Nearest-Neighbors for all Vertices via Dijkstra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are given a directed graph $G = (V,E)$ with $n$ vertices and $m$ edges,\nwith positive weights on the edges, and a parameter $k >0$. We show how to\ncompute, for every vertex $v \\in V$, its $k$ nearest-neighbors. The algorithm\nruns in $O( k ( n \\log n + m ) )$ time, and follows by a somewhat careful\nmodification of Dijkstra's shortest path algorithm.\n  This result is probably folklore, but we were unable to find a reference to\nit -- thus, this note.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 17:49:46 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Har-Peled", "Sariel", ""]]}, {"id": "1607.07837", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li", "title": "First Efficient Convergence for Streaming k-PCA: a Global, Gap-Free, and\n  Near-Optimal Rate", "comments": "REMARK: v4 adds discussions and polishes writing; v3 contains a\n  stronger Theorem 2, a new lower bound Theorem 6, as well as new Oja++ results\n  Theorem 4 and Theorem 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study streaming principal component analysis (PCA), that is to find, in\n$O(dk)$ space, the top $k$ eigenvectors of a $d\\times d$ hidden matrix $\\bf\n\\Sigma$ with online vectors drawn from covariance matrix $\\bf \\Sigma$.\n  We provide $\\textit{global}$ convergence for Oja's algorithm which is\npopularly used in practice but lacks theoretical understanding for $k>1$. We\nalso provide a modified variant $\\mathsf{Oja}^{++}$ that runs $\\textit{even\nfaster}$ than Oja's. Our results match the information theoretic lower bound in\nterms of dependency on error, on eigengap, on rank $k$, and on dimension $d$,\nup to poly-log factors. In addition, our convergence rate can be made gap-free,\nthat is proportional to the approximation error and independent of the\neigengap.\n  In contrast, for general rank $k$, before our work (1) it was open to design\nany algorithm with efficient global convergence rate; and (2) it was open to\ndesign any algorithm with (even local) gap-free convergence rate in $O(dk)$\nspace.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 18:46:21 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 02:00:20 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 17:09:52 GMT"}, {"version": "v4", "created": "Mon, 17 Apr 2017 02:40:11 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1607.07906", "submitter": "Krzysztof Sornat", "authors": "Marek Cygan, {\\L}ukasz Kowalik, Arkadiusz Soca{\\l}a, Krzysztof Sornat", "title": "Approximation and Parameterized Complexity of Minimax Approval Voting", "comments": "14 pages, 3 figures, 2 pseudocodes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present three results on the complexity of Minimax Approval Voting. First,\nwe study Minimax Approval Voting parameterized by the Hamming distance $d$ from\nthe solution to the votes. We show Minimax Approval Voting admits no algorithm\nrunning in time $\\mathcal{O}^\\star(2^{o(d\\log d)})$, unless the Exponential\nTime Hypothesis (ETH) fails. This means that the $\\mathcal{O}^\\star(d^{2d})$\nalgorithm of Misra et al. [AAMAS 2015] is essentially optimal. Motivated by\nthis, we then show a parameterized approximation scheme, running in time\n$\\mathcal{O}^\\star(\\left({3}/{\\epsilon}\\right)^{2d})$, which is essentially\ntight assuming ETH. Finally, we get a new polynomial-time randomized\napproximation scheme for Minimax Approval Voting, which runs in time\n$n^{\\mathcal{O}(1/\\epsilon^2 \\cdot \\log(1/\\epsilon))} \\cdot \\mathrm{poly}(m)$,\nalmost matching the running time of the fastest known PTAS for Closest String\ndue to Ma and Sun [SIAM J. Comp. 2009].\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 22:06:51 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Cygan", "Marek", ""], ["Kowalik", "\u0141ukasz", ""], ["Soca\u0142a", "Arkadiusz", ""], ["Sornat", "Krzysztof", ""]]}, {"id": "1607.07950", "submitter": "Pierre Le Bodic", "authors": "C\\'edric Bentz and Pierre Le Bodic", "title": "A note on \"Approximation schemes for a subclass of subset selection\n  problems\", and a faster FPTAS for the Minimum Knapsack Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruhs and Woeginger prove the existence of FPTAS's for a general class of\nminimization and maximization subset selection problems. Without losing\ngenerality from the original framework, we prove how better asymptotic\nworst-case running times can be achieved if a $\\rho$-approximation algorithm is\navailable, and in particular we obtain matching running times between\nmaximization and minimization subset selection problems. We directly apply this\nresult to the Minimum Knapsack Problem, for which the original framework yields\nan FPTAS with running time $O(n^5/\\epsilon)$, where $\\epsilon$ is the required\naccuracy and $n$ is the number of items, and obtain an FPTAS with running time\n$O(n^3/\\epsilon)$, thus improving the running time by a quadratic factor in the\nworst case.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 04:13:46 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Bentz", "C\u00e9dric", ""], ["Bodic", "Pierre Le", ""]]}, {"id": "1607.07957", "submitter": "Shinsaku Sakaue", "authors": "Shinsaku Sakaue", "title": "On maximizing a monotone k-submodular function subject to a matroid\n  constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $k$-submodular function is an extension of a submodular function in that\nits input is given by $k$ disjoint subsets instead of a single subset. For\nunconstrained nonnegative $k$-submodular maximization, Ward and \\v{Z}ivn\\'y\nproposed a constant-factor approximation algorithm, which was improved by the\nrecent work of Iwata, Tanigawa and Yoshida presenting a $1/2$-approximation\nalgorithm. Iwata et al. also provided a $k/(2k-1)$-approximation algorithm for\nmonotone $k$-submodular maximization and proved that its approximation ratio is\nasymptotically tight. More recently, Ohsaka and Yoshida proposed\nconstant-factor algorithms for monotone $k$-submodular maximization with\nseveral size constraints. However, while submodular maximization with various\nconstraints has been extensively studied, no approximation algorithm has been\ndeveloped for constrained $k$-submodular maximization, except for the case of\nsize constraints. In this paper, we prove that a greedy algorithm outputs a\n$1/2$-approximate solution for monotone $k$-submodular maximization with a\nmatroid constraint. The algorithm runs in $O(M|E|(\\text{MO} + k\\text{EO}))$\ntime, where $M$ is the size of a maximal optimal solution, $|E|$ is the size of\nthe ground set, and $\\text{MO}, \\text{EO}$ represent the time for the\nmembership oracle of the matroid and the evaluation oracle of the\n$k$-submodular function, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 04:55:34 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 11:34:24 GMT"}, {"version": "v3", "created": "Mon, 22 Aug 2016 08:59:05 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Sakaue", "Shinsaku", ""]]}, {"id": "1607.08041", "submitter": "Di Chen", "authors": "Di Chen, Mordecai Golin", "title": "Minmax Tree Facility Location and Sink Evacuation with Dynamic Confluent\n  Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be a graph modelling a building or road network in which edges\nhave-both travel times (lengths) and capacities associated with them. An edge's\ncapacity is the number of people that can enter that edge in a unit of time.\n  In emergencies, people evacuate towards the exits. If too many people try to\nevacuate through the same edge, congestion builds up and slows down the\nevacuation.\n  Graphs with both lengths and capacities are known as Dynamic Flow networks.\nAn evacuation plan for $G$ consists of a choice of exit locations and a\npartition of the people at the vertices into groups, with each group evacuating\nto the same exit. The evacuation time of a plan is the time it takes until the\nlast person evacuates. The $k$-sink evacuation problem is to provide an\nevacuation plan with $k$ exit locations that minimizes the evacuation time. It\nis known that this problem is NP-Hard for general graphs but no polynomial time\nalgorithm was previously known even for the case of $G$ a tree. This paper\npresents an $O(n k^2 \\log^5 n)$ algorithm for the $k$-sink evacuation problem\non trees. Our algorithms also apply to a more general class of problems, which\nwe call minmax tree facility location.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 11:24:30 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Chen", "Di", ""], ["Golin", "Mordecai", ""]]}, {"id": "1607.08176", "submitter": "Szymon Grabowski", "authors": "Tomasz Kowalski, Szymon Grabowski, Kimmo Fredriksson, Marcin\n  Raniszewski", "title": "Suffix arrays with a twist", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The suffix array is a classic full-text index, combining effectiveness with\nsimplicity. We discuss three approaches aiming to improve its efficiency even\nmore: changes to the navigation, data layout and adding extra data. In short,\nwe show that $(i)$ how we search for the right interval boundary impacts\nsignificantly the overall search speed, $(ii)$ a B-tree data layout easily wins\nover the standard one, $(iii)$ the well-known idea of a lookup table for the\nprefixes of the suffixes can be refined with using compression, $(iv)$ caching\nprefixes of the suffixes in a helper array can pose a(nother) practical\nspace-time tradeoff.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 16:50:05 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Kowalski", "Tomasz", ""], ["Grabowski", "Szymon", ""], ["Fredriksson", "Kimmo", ""], ["Raniszewski", "Marcin", ""]]}, {"id": "1607.08192", "submitter": "Radu Curticapean", "authors": "Radu Curticapean", "title": "Counting matchings with k unmatched vertices in planar graphs", "comments": "16 pages, to appear in ESA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of counting matchings in planar graphs. While perfect\nmatchings in planar graphs can be counted by a classical polynomial-time\nalgorithm, the problem of counting all matchings (possibly containing unmatched\nvertices, also known as defects) is known to be #P-complete on planar graphs.\nTo interpolate between the hard case of counting matchings and the easy case of\ncounting perfect matchings, we study the parameterized problem of counting\nmatchings with exactly k unmatched vertices in a planar graph G, on input G and\nk. This setting has a natural interpretation in statistical physics, and it is\na special case of counting perfect matchings in k-apex graphs (graphs that can\nbe turned planar by removing at most k vertices).\n  Starting from a recent #W[1]-hardness proof for counting perfect matchings on\nk-apex graphs, we obtain that counting matchings with k unmatched vertices in\nplanar graphs is #W[1]-hard. In contrast, given a plane graph G with s\ndistinguished faces, there is an $O(2^s \\cdot n^3)$ time algorithm for counting\nthose matchings with k unmatched vertices such that all unmatched vertices lie\non the distinguished faces. This implies an $f(k,s)\\cdot n^{O(1)}$ time\nalgorithm for counting perfect matchings in k-apex graphs whose apex\nneighborhood is covered by s faces.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 17:42:52 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Curticapean", "Radu", ""]]}, {"id": "1607.08337", "submitter": "Ofer Neiman", "authors": "Michael Elkin and Ofer Neiman", "title": "Efficient Algorithms for Constructing Very Sparse Spanners and Emulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Miller et al. \\cite{MPVX15} devised a distributed\\footnote{They actually\nshowed a PRAM algorithm. The distributed algorithm with these properties is\nimplicit in \\cite{MPVX15}.} algorithm in the CONGEST model, that given a\nparameter $k = 1,2,\\ldots$, constructs an $O(k)$-spanner of an input unweighted\n$n$-vertex graph with $O(n^{1+1/k})$ expected edges in $O(k)$ rounds of\ncommunication. In this paper we improve the result of \\cite{MPVX15}, by showing\na $k$-round distributed algorithm in the same model, that constructs a\n$(2k-1)$-spanner with $O(n^{1+1/k}/\\epsilon)$ edges, with probability $1-\n\\epsilon$, for any $\\epsilon>0$. Moreover, when $k = \\omega(\\log n)$, our\nalgorithm produces (still in $k$ rounds) {\\em ultra-sparse} spanners, i.e.,\nspanners of size $n(1+ o(1))$, with probability $1- o(1)$. To our knowledge,\nthis is the first distributed algorithm in the CONGEST or in the PRAM models\nthat constructs spanners or skeletons (i.e., connected spanning subgraphs) that\nsparse. Our algorithm can also be implemented in linear time in the standard\ncentralized model, and for large $k$, it provides spanners that are sparser\nthan any other spanner given by a known (near-)linear time algorithm.\n  We also devise improved bounds (and algorithms realizing these bounds) for\n$(1+\\epsilon,\\beta)$-spanners and emulators. In particular, we show that for\nany unweighted $n$-vertex graph and any $\\epsilon > 0$, there exists a $(1+\n\\epsilon, ({{\\log\\log n} \\over \\epsilon})^{\\log\\log n})$-emulator with $O(n)$\nedges. All previous constructions of $(1+\\epsilon,\\beta)$-spanners and\nemulators employ a superlinear number of edges, for all choices of parameters.\n  Finally, we provide some applications of our results to approximate shortest\npaths' computation in unweighted graphs.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 07:47:52 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 13:00:15 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Elkin", "Michael", ""], ["Neiman", "Ofer", ""]]}, {"id": "1607.08338", "submitter": "Marc Goerigk", "authors": "Marc Goerigk and Yogish Sabharwal and Anita Sch\\\"obel and Sandeep Sen", "title": "Improvable Knapsack Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variant of the knapsack problem, where items are available with\ndifferent possible weights. Using a separate budget for these item\nimprovements, the question is: Which items should be improved to which degree\nsuch that the resulting classic knapsack problem yields maximum profit?\n  We present a detailed analysis for several cases of improvable knapsack\nproblems, presenting constant factor approximation algorithms and two PTAS.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 07:52:01 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Goerigk", "Marc", ""], ["Sabharwal", "Yogish", ""], ["Sch\u00f6bel", "Anita", ""], ["Sen", "Sandeep", ""]]}, {"id": "1607.08342", "submitter": "Gianluca Della Vedova", "authors": "Paola Bonizzoni and Gianluca Della Vedova and Serena Nicosia and Marco\n  Previtali and Raffaella Rizzi", "title": "A New Lightweight Algorithm to compute the BWT and the LCP array of a\n  Set of Strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing of very large collections of strings such as those produced by the\nwidespread sequencing technologies, heavily relies on multi-string\ngeneralizations of the Burrows-Wheeler Transform (BWT), and for this problem\nvarious in-memory algorithms have been proposed. The rapid growing of data that\nare processed routinely, such as in bioinformatics, requires a large amount of\nmain memory, and this fact has motivated the development of algorithms, to\ncompute the BWT, that work almost entirely in external memory. On the other\nhand, the related problem of computing the Longest Common Prefix (LCP) array is\noften instrumental in several algorithms on collection of strings, such as\nthose that compute the suffix-prefix overlap among strings, which is an\nessential step for many genome assembly algorithms. The best current\nlightweight approach to compute BWT and LCP array on a set of $m$ strings, each\none $k$ characters long, has I/O complexity that is $O(mk^2 \\log |\\Sigma|)$\n(where $|\\Sigma|$ is the size of the alphabet), thus it is not optimal. In this\npaper we propose a novel approach to build BWT and LCP array (simultaneously)\nwith $O(kmL(\\log k +\\log \\sigma))$ I/O complexity, where $L$ is the length of\nlongest substring that appears at least twice in the input strings.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 08:02:23 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Bonizzoni", "Paola", ""], ["Della Vedova", "Gianluca", ""], ["Nicosia", "Serena", ""], ["Previtali", "Marco", ""], ["Rizzi", "Raffaella", ""]]}, {"id": "1607.08449", "submitter": "Karthik C. S.", "authors": "Jean-Daniel Boissonnat and Karthik C. S.", "title": "An Efficient Representation for Filtrations of Simplicial Complexes", "comments": "A preliminary version appeared in SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A filtration over a simplicial complex $K$ is an ordering of the simplices of\n$K$ such that all prefixes in the ordering are subcomplexes of $K$. Filtrations\nare at the core of Persistent Homology, a major tool in Topological Data\nAnalysis. In order to represent the filtration of a simplicial complex, the\nentire filtration can be appended to any data structure that explicitly stores\nall the simplices of the complex such as the Hasse diagram or the recently\nintroduced Simplex Tree [Algorithmica '14]. However, with the popularity of\nvarious computational methods that need to handle simplicial complexes, and\nwith the rapidly increasing size of the complexes, the task of finding a\ncompact data structure that can still support efficient queries is of great\ninterest.\n  In this paper, we propose a new data structure called the Critical Simplex\nDiagram (CSD) which is a variant of the Simplex Array List (SAL) [Algorithmica\n'17]. Our data structure allows one to store in a compact way the filtration of\na simplicial complex, and allows for the efficient implementation of a large\nrange of basic operations. Moreover, we prove that our data structure is\nessentially optimal with respect to the requisite storage space. Finally, we\nshow that the CSD representation admits fast construction algorithms for Flag\ncomplexes and relaxed Delaunay complexes.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 13:30:20 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 13:26:00 GMT"}, {"version": "v3", "created": "Sun, 4 Feb 2018 15:55:30 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Boissonnat", "Jean-Daniel", ""], ["S.", "Karthik C.", ""]]}, {"id": "1607.08456", "submitter": "Matth\\\"aus Kleindessner", "authors": "Matth\\\"aus Kleindessner and Ulrike von Luxburg", "title": "Kernel functions based on triplet comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given only information in the form of similarity triplets \"Object A is more\nsimilar to object B than to object C\" about a data set, we propose two ways of\ndefining a kernel function on the data set. While previous approaches construct\na low-dimensional Euclidean embedding of the data set that reflects the given\nsimilarity triplets, we aim at defining kernel functions that correspond to\nhigh-dimensional embeddings. These kernel functions can subsequently be used to\napply any kernel method to the data set.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 13:46:06 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 21:33:41 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Kleindessner", "Matth\u00e4us", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1607.08682", "submitter": "Chao Xu", "authors": "Chandra Chekuri, Chao Xu", "title": "Computing minimum cuts in hypergraphs", "comments": "23 pages, 2 figures. Fixed the proof of Theorem 3.5. Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study algorithmic and structural aspects of connectivity in hypergraphs.\nGiven a hypergraph $H=(V,E)$ with $n = |V|$, $m = |E|$ and $p = \\sum_{e \\in E}\n|e|$ the best known algorithm to compute a global minimum cut in $H$ runs in\ntime $O(np)$ for the uncapacitated case and in $O(np + n^2 \\log n)$ time for\nthe capacitated case. We show the following new results.\n  1. Given an uncapacitated hypergraph $H$ and an integer $k$ we describe an\nalgorithm that runs in $O(p)$ time to find a subhypergraph $H'$ with sum of\ndegrees $O(kn)$ that preserves all edge-connectivities up to $k$ (a\n$k$-sparsifier). This generalizes the corresponding result of Nagamochi and\nIbaraki from graphs to hypergraphs. Using this sparsification we obtain an $O(p\n+ \\lambda n^2)$ time algorithm for computing a global minimum cut of $H$ where\n$\\lambda$ is the minimum cut value.\n  2. We generalize Matula's argument for graphs to hypergraphs and obtain a\n$(2+\\epsilon)$-approximation to the global minimum cut in a capacitated\nhypergraph in $O(\\frac{1}{\\epsilon} (p \\log n + n \\log^2 n))$ time.\n  3. We show that a hypercactus representation of all the global minimum cuts\nof a capacitated hypergraph can be computed in $O(np + n^2 \\log n)$ time and\n$O(p)$ space.\n  We utilize vertex ordering based ideas to obtain our results. Unlike graphs\nwe observe that there are several different orderings for hypergraphs which\nyield different insights.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 03:22:56 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 00:40:42 GMT"}, {"version": "v3", "created": "Sun, 14 May 2017 09:21:42 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Chekuri", "Chandra", ""], ["Xu", "Chao", ""]]}, {"id": "1607.08754", "submitter": "Martin Tieves", "authors": "Arie Koster, Robert Schweidweiler and Martin Tieves", "title": "A flow based pruning scheme for enumerative equitable coloring\n  algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An equitable graph coloring is a proper vertex coloring of a graph G where\nthe sizes of the color classes differ by at most one. The equitable chromatic\nnumber is the smallest number k such that G admits such equitable k-coloring.\nWe focus on enumerative algorithms for the computation of the equitable\ncoloring number and propose a general scheme to derive pruning rules for them:\nWe show how the extendability of a partial coloring into an equitable coloring\ncan be modeled via network flows. Thus, we obtain pruning rules which can be\nchecked via flow algorithms. Computational experiments show that the search\ntree of enumerative algorithms can be significantly reduced in size by these\nrules and, in most instances, such naive approach even yields a faster\nalgorithm. Moreover, the stability, i.e., the number of solved instances within\na given time limit, is greatly improved.\n  Since the execution of flow algorithms at each node of a search tree is time\nconsuming, we derive arithmetic pruning rules (generalized Hall-conditions)\nfrom the network model. Adding these rules to an enumerative algorithm yields\nan even larger runtime improvement.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 10:02:26 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 08:16:25 GMT"}, {"version": "v3", "created": "Fri, 28 Oct 2016 13:46:20 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Koster", "Arie", ""], ["Schweidweiler", "Robert", ""], ["Tieves", "Martin", ""]]}, {"id": "1607.08805", "submitter": "Andreas T\\\"onnis", "authors": "Thomas Kesselheim and Andreas T\\\"onnis", "title": "Submodular Secretary Problems: Cardinality, Matching, and Linear\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study various generalizations of the secretary problem with submodular\nobjective functions. Generally, a set of requests is revealed step-by-step to\nan algorithm in random order. For each request, one option has to be selected\nso as to maximize a monotone submodular function while ensuring feasibility.\nFor our results, we assume that we are given an offline algorithm computing an\n$\\alpha$-approximation for the respective problem. This way, we separate\ncomputational limitations from the ones due to the online nature. When only\nfocusing on the online aspect, we can assume $\\alpha = 1$.\n  In the submodular secretary problem, feasibility constraints are cardinality\nconstraints. That is, out of a randomly ordered stream of entities, one has to\nselect a subset size $k$. For this problem, we present a\n$0.31\\alpha$-competitive algorithm for all $k$, which asymptotically reaches\ncompetitive ratio $\\frac{\\alpha}{e}$ for large $k$. In submodular secretary\nmatching, one side of a bipartite graph is revealed online. Upon arrival, each\nnode has to be matched permanently to an offline node or discarded irrevocably.\nWe give an $\\frac{\\alpha}{4}$-competitive algorithm. In both cases, we improve\nover previously best known competitive ratios, using a generalization of the\nalgorithm for the classic secretary problem.\n  Furthermore, we give an $O(\\alpha d^{-\\frac{2}{B-1}})$-competitive algorithm\nfor submodular function maximization subject to linear packing constraints.\nHere, $d$ is the column sparsity, that is the maximal number of none-zero\nentries in a column of the constraint matrix, and $B$ is the minimal capacity\nof the constraints. Notably, this bound is independent of the total number of\nconstraints. We improve the algorithm to be $O(\\alpha\nd^{-\\frac{1}{B-1}})$-competitive if both $d$ and $B$ are known to the algorithm\nbeforehand.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 13:38:34 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Kesselheim", "Thomas", ""], ["T\u00f6nnis", "Andreas", ""]]}, {"id": "1607.08806", "submitter": "Torsten M\\\"utze", "authors": "Petr Gregor and Torsten M\\\"utze", "title": "Trimming and gluing Gray codes", "comments": null, "journal-ref": "Theoretical Computer Science 714:74-95, 2018", "doi": "10.1016/j.tcs.2017.12.003", "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the algorithmic problem of generating each subset of\n$[n]:=\\{1,2,\\ldots,n\\}$ whose size is in some interval $[k,l]$, $0\\leq k\\leq\nl\\leq n$, exactly once (cyclically) by repeatedly adding or removing a single\nelement, or by exchanging a single element. For $k=0$ and $l=n$ this is the\nclassical problem of generating all $2^n$ subsets of $[n]$ by element\nadditions/removals, and for $k=l$ this is the classical problem of generating\nall $\\binom{n}{k}$ subsets of $[n]$ by element exchanges. We prove the\nexistence of such cyclic minimum-change enumerations for a large range of\nvalues $n$, $k$, and $l$, improving upon and generalizing several previous\nresults. For all these existential results we provide optimal algorithms to\ncompute the corresponding Gray codes in constant $\\mathcal{O}(1)$ time per\ngenerated set and $\\mathcal{O}(n)$ space. Rephrased in terms of graph theory,\nour results establish the existence of (almost) Hamilton cycles in the subgraph\nof the $n$-dimensional cube $Q_n$ induced by all levels $[k,l]$. We reduce all\nremaining open cases to a generalized version of the middle levels conjecture,\nwhich asserts that the subgraph of $Q_{2k+1}$ induced by all levels\n$[k-c,k+1+c]$, $c\\in\\{0,1,\\ldots,k\\}$, has a Hamilton cycle. We also prove an\napproximate version of this generalized conjecture, showing that this graph has\na cycle that visits a $(1-o(1))$-fraction of all vertices.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 13:42:37 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 15:29:27 GMT"}, {"version": "v3", "created": "Fri, 6 Jan 2017 16:10:09 GMT"}, {"version": "v4", "created": "Fri, 6 Oct 2017 13:16:58 GMT"}, {"version": "v5", "created": "Mon, 23 Oct 2017 12:57:32 GMT"}, {"version": "v6", "created": "Thu, 15 Feb 2018 15:25:18 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Gregor", "Petr", ""], ["M\u00fctze", "Torsten", ""]]}, {"id": "1607.08905", "submitter": "Alexander Shekhovtsov", "authors": "Mengtian Li, Alexander Shekhovtsov, Daniel Huber", "title": "Complexity of Discrete Energy Minimization Problems", "comments": "ECCV'16 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete energy minimization is widely-used in computer vision and machine\nlearning for problems such as MAP inference in graphical models. The problem,\nin general, is notoriously intractable, and finding the global optimal solution\nis known to be NP-hard. However, is it possible to approximate this problem\nwith a reasonable ratio bound on the solution quality in polynomial time? We\nshow in this paper that the answer is no. Specifically, we show that general\nenergy minimization, even in the 2-label pairwise case, and planar energy\nminimization with three or more labels are exp-APX-complete. This finding rules\nout the existence of any approximation algorithm with a sub-exponential\napproximation ratio in the input size for these two problems, including\nconstant factor approximations. Moreover, we collect and review the\ncomputational complexity of several subclass problems and arrange them on a\ncomplexity scale consisting of three major complexity classes -- PO, APX, and\nexp-APX, corresponding to problems that are solvable, approximable, and\ninapproximable in polynomial time. Problems in the first two complexity classes\ncan serve as alternative tractable formulations to the inapproximable ones.\nThis paper can help vision researchers to select an appropriate model for an\napplication or guide them in designing new algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 19:34:34 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Li", "Mengtian", ""], ["Shekhovtsov", "Alexander", ""], ["Huber", "Daniel", ""]]}]