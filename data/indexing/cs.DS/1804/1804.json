[{"id": "1804.00069", "submitter": "Edward Raff", "authors": "Edward Raff, Jared Sylvester, Charles Nicholas", "title": "Engineering a Simplified 0-Bit Consistent Weighted Sampling", "comments": null, "journal-ref": "In Proceedings of the 27th ACM International Conference on\n  Information and Knowledge Management. (2018) 1203-1212", "doi": "10.1145/3269206.3271690", "report-no": null, "categories": "stat.ML cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Min-Hashing approach to sketching has become an important tool in data\nanalysis, information retrial, and classification. To apply it to real-valued\ndatasets, the ICWS algorithm has become a seminal approach that is widely used,\nand provides state-of-the-art performance for this problem space. However, ICWS\nsuffers a computational burden as the sketch size K increases. We develop a new\nSimplified approach to the ICWS algorithm, that enables us to obtain over 20x\nspeedups compared to the standard algorithm. The veracity of our approach is\ndemonstrated empirically on multiple datasets and scenarios, showing that our\nnew Simplified CWS obtains the same quality of results while being an order of\nmagnitude faster.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 22:12:44 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 07:45:44 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Raff", "Edward", ""], ["Sylvester", "Jared", ""], ["Nicholas", "Charles", ""]]}, {"id": "1804.00137", "submitter": "Shiri Chechik", "authors": "Shiri Chechik and Doron Mukhtar", "title": "Optimal Distributed Coloring Algorithms for Planar Graphs in the LOCAL\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider distributed coloring for planar graphs with a\nsmall number of colors. We present an optimal (up to a constant factor)\n$O(\\log{n})$ time algorithm for 6-coloring planar graphs. Our algorithm is\nbased on a novel technique that in a nutshell detects small structures that can\nbe easily colored given a proper coloring of the rest of the vertices and\nremoves them from the graph until the graph contains a small enough number of\nedges. We believe this technique might be of independent interest.\n  In addition, we present a lower bound for 4-coloring planar graphs that\nessentially shows that any algorithm (deterministic or randomized) for\n$4$-coloring planar graphs requires $\\Omega(n)$ rounds. We therefore completely\nresolve the problems of 4-coloring and 6-coloring for planar graphs in the\nLOCAL model.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 08:45:41 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Chechik", "Shiri", ""], ["Mukhtar", "Doron", ""]]}, {"id": "1804.00141", "submitter": "Telikepalli Kavitha", "authors": "Telikepalli Kavitha", "title": "The Popular Roommates problem", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the popular matching problem in a roommates instance with strict\npreference lists. While popular matchings always exist in a bipartite instance,\nthey need not exist in a roommates instance. The complexity of the popular\nmatching problem in a roommates instance has been an open problem for several\nyears and here we show it is NP-hard. A sub-class of max-size popular matchings\ncalled dominant matchings has been well-studied in bipartite graphs. We show\nthat the dominant matching problem in a roommates instance is also NP-hard and\nthis is the case even when the instance admits a stable matching.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 09:20:47 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 09:35:30 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Kavitha", "Telikepalli", ""]]}, {"id": "1804.00206", "submitter": "Viktor Toman", "authors": "Krishnendu Chatterjee, Monika Henzinger, Veronika Loitzenbauer, Simin\n  Oraee, Viktor Toman", "title": "Symbolic Algorithms for Graphs and Markov Decision Processes with\n  Fairness Objectives", "comments": "Full version of the paper. To appear in CAV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a model and a specification, the fundamental model-checking problem\nasks for algorithmic verification of whether the model satisfies the\nspecification. We consider graphs and Markov decision processes (MDPs), which\nare fundamental models for reactive systems. One of the very basic\nspecifications that arise in verification of reactive systems is the strong\nfairness (aka Streett) objective. Given different types of requests and\ncorresponding grants, the objective requires that for each type, if the request\nevent happens infinitely often, then the corresponding grant event must also\nhappen infinitely often. All $\\omega$-regular objectives can be expressed as\nStreett objectives and hence they are canonical in verification. To handle the\nstate-space explosion, symbolic algorithms are required that operate on a\nsuccinct implicit representation of the system rather than explicitly accessing\nthe system. While explicit algorithms for graphs and MDPs with Streett\nobjectives have been widely studied, there has been no improvement of the basic\nsymbolic algorithms. The worst-case numbers of symbolic steps required for the\nbasic symbolic algorithms are as follows: quadratic for graphs and cubic for\nMDPs. In this work we present the first sub-quadratic symbolic algorithm for\ngraphs with Streett objectives, and our algorithm is sub-quadratic even for\nMDPs. Based on our algorithmic insights we present an implementation of the new\nsymbolic approach and show that it improves the existing approach on several\nacademic benchmark examples.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 19:53:40 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 15:38:54 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Henzinger", "Monika", ""], ["Loitzenbauer", "Veronika", ""], ["Oraee", "Simin", ""], ["Toman", "Viktor", ""]]}, {"id": "1804.00553", "submitter": "Tung Mai", "authors": "Tung Mai and Vijay V. Vazirani", "title": "Finding Stable Matchings that are Robust to Errors in the Input", "comments": "arXiv admin note: text overlap with arXiv:1802.06621", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding solutions to the stable matching problem that\nare robust to errors in the input and we obtain a polynomial time algorithm for\na special class of errors. In the process, we also initiate work on a new\nstructural question concerning the stable matching problem, namely finding\nrelationships between the lattices of solutions of two \"nearby\" instances.\n  Our main algorithmic result is the following: We identify a polynomially\nlarge class of errors, $D$, that can be introduced in a stable matching\ninstance. Given an instance $A$ of stable matching, let $B$ be the random\nvariable that represents the instance that results after introducing {\\em one}\nerror from $D$, chosen via a given discrete probability distribution. The\nproblem is to find a stable matching for $A$ that maximizes the probability of\nbeing stable for $B$ as well. Via new structural properties of the type\ndescribed in the question stated above, we give a combinatorial polynomial time\nalgorithm for this problem.\n  We also show that the set of robust stable matchings for instance $A$, under\nprobability distribution $p$, forms a sublattice of the lattice of stable\nmatchings for $A$. We give an efficient algorithm for finding a succinct\nrepresentation for this set; this representation has the property that any\nmember of the set can be efficiently retrieved from it.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 11:15:22 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 02:34:04 GMT"}, {"version": "v3", "created": "Mon, 25 Jun 2018 06:15:17 GMT"}, {"version": "v4", "created": "Fri, 14 Dec 2018 07:15:30 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Mai", "Tung", ""], ["Vazirani", "Vijay V.", ""]]}, {"id": "1804.01045", "submitter": "Kyle Fox", "authors": "Jeff Erickson and Kyle Fox and Luvsandondov Lkhamsuren", "title": "Holiest Minimum-Cost Paths and Flows in Surface Graphs", "comments": "29 pages, 2 figures, to appear at STOC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be an edge-weighted directed graph with $n$ vertices embedded on an\norientable surface of genus $g$. We describe a simple deterministic\nlexicographic perturbation scheme that guarantees uniqueness of minimum-cost\nflows and shortest paths in $G$. The perturbations take $O(gn)$ time to\ncompute. We use our perturbation scheme in a black box manner to derive a\ndeterministic $O(n \\log \\log n)$ time algorithm for minimum cut in\n\\emph{directed} edge-weighted planar graphs and a deterministic $O(g^2 n \\log\nn)$ time proprocessing scheme for the multiple-source shortest paths problem of\ncomputing a shortest path oracle for all vertices lying on a common face of a\nsurface embedded graph. The latter result yields faster deterministic\nnear-linear time algorithms for a variety of problems in constant genus surface\nembedded graphs.\n  Finally, we open the black box in order to generalize a recent linear-time\nalgorithm for multiple-source shortest paths in unweighted undirected planar\ngraphs to work in arbitrary orientable surfaces. Our algorithm runs in $O(g^2 n\n\\log g)$ time in this setting, and it can be used to give improved linear time\nalgorithms for several problems in unweighted undirected surface embedded\ngraphs of constant genus including the computation of minimum cuts, shortest\ntopologically non-trivial cycles, and minimum homology bases.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 15:50:09 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Erickson", "Jeff", ""], ["Fox", "Kyle", ""], ["Lkhamsuren", "Luvsandondov", ""]]}, {"id": "1804.01076", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Ankit Garg, Yuanzhi Li, Rafael Oliveira, Avi\n  Wigderson", "title": "Operator Scaling via Geodesically Convex Optimization, Invariant Theory\n  and Polynomial Identity Testing", "comments": "abstract to appear in STOC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.AG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new second-order method for geodesically convex optimization on\nthe natural hyperbolic metric over positive definite matrices. We apply it to\nsolve the operator scaling problem in time polynomial in the input size and\nlogarithmic in the error. This is an exponential improvement over previous\nalgorithms which were analyzed in the usual Euclidean, \"commutative\" metric\n(for which the above problem is not convex). Our method is general and\napplicable to other settings.\n  As a consequence, we solve the equivalence problem for the left-right group\naction underlying the operator scaling problem. This yields a deterministic\npolynomial-time algorithm for a new class of Polynomial Identity Testing (PIT)\nproblems, which was the original motivation for studying operator scaling.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 17:41:36 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Garg", "Ankit", ""], ["Li", "Yuanzhi", ""], ["Oliveira", "Rafael", ""], ["Wigderson", "Avi", ""]]}, {"id": "1804.01181", "submitter": "Arash Nouri", "authors": "Arash Nouri and Jorg-Rudiger Sack", "title": "Query Shortest Paths Amidst Growing Discs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The determination of collision-free shortest paths among growing discs has\npreviously been studied for discs with fixed growing rates. Here, we study a\nmore general case of this problem, where: (1) the speeds at which the discs are\ngrowing are polynomial functions of degree $\\dd$, and (2) the source and\ndestination points are given as query points. We show how to preprocess the $n$\ngrowing discs so that, for two given query points $s$ and $d$, a shortest path\nfrom $s$ to $d$ can be found in $O(n^2 \\log (\\dd n))$ time. The preprocessing\ntime of our algorithm is $O(n^2 \\log n + k \\log k)$ where $k$ is the number of\nintersections between the growing discs and the tangent paths (straight line\npaths which touch the boundaries of two growing discs). We also prove that $k\n\\in O(n^3\\dd)$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 22:18:29 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Nouri", "Arash", ""], ["Sack", "Jorg-Rudiger", ""]]}, {"id": "1804.01207", "submitter": "Roberto Baldacci", "authors": "Luca Ghezzi and Roberto Baldacci", "title": "A Euclidean Algorithm for Binary Cycles with Minimal Variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem is considered of arranging symbols around a cycle, in such a way\nthat distances between different instances of a same symbol be as uniformly\ndistributed as possible. A sequence of moments is defined for cycles, similarly\nto the well-known praxis in statistics and including mean and variance. Mean is\nseen to be invariant under permutations of the cycle. In the case of a binary\nalphabet of symbols, a fast, constructive, sequencing algorithm is introduced,\nstrongly resembling the celebrated Euclidean method for greatest common divisor\ncomputation, and the cycle returned is characterized in terms of symbol\ndistances. A minimal variance condition is proved, and the proposed Euclidean\nalgorithm is proved to satisfy it, thus being optimal. Applications to\nproductive systems and information processing are briefly discussed.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 01:41:59 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Ghezzi", "Luca", ""], ["Baldacci", "Roberto", ""]]}, {"id": "1804.01221", "submitter": "Max Simchowitz", "authors": "Max Simchowitz and Ahmed El Alaoui and Benjamin Recht", "title": "Tight Query Complexity Lower Bounds for PCA via Finite Sample Deformed\n  Wigner Law", "comments": "To appear in STOC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a \\emph{query complexity} lower bound for approximating the top $r$\ndimensional eigenspace of a matrix. We consider an oracle model where, given a\nsymmetric matrix $\\mathbf{M} \\in \\mathbb{R}^{d \\times d}$, an algorithm\n$\\mathsf{Alg}$ is allowed to make $\\mathsf{T}$ exact queries of the form\n$\\mathsf{w}^{(i)} = \\mathbf{M} \\mathsf{v}^{(i)}$ for $i$ in\n$\\{1,...,\\mathsf{T}\\}$, where $\\mathsf{v}^{(i)}$ is drawn from a distribution\nwhich depends arbitrarily on the past queries and measurements\n$\\{\\mathsf{v}^{(j)},\\mathsf{w}^{(i)}\\}_{1 \\le j \\le i-1}$. We show that for\nevery $\\mathtt{gap} \\in (0,1/2]$, there exists a distribution over matrices\n$\\mathbf{M}$ for which 1) $\\mathrm{gap}_r(\\mathbf{M}) = \\Omega(\\mathtt{gap})$\n(where $\\mathrm{gap}_r(\\mathbf{M})$ is the normalized gap between the $r$ and\n$r+1$-st largest-magnitude eigenvector of $\\mathbf{M}$), and 2) any algorithm\n$\\mathsf{Alg}$ which takes fewer than $\\mathrm{const} \\times \\frac{r \\log\nd}{\\sqrt{\\mathtt{gap}}}$ queries fails (with overwhelming probability) to\nidentity a matrix $\\widehat{\\mathsf{V}} \\in \\mathbb{R}^{d \\times r}$ with\northonormal columns for which $\\langle \\widehat{\\mathsf{V}}, \\mathbf{M}\n\\widehat{\\mathsf{V}}\\rangle \\ge (1 - \\mathrm{const} \\times\n\\mathtt{gap})\\sum_{i=1}^r \\lambda_i(\\mathbf{M})$. Our bound requires only that\n$d$ is a small polynomial in $1/\\mathtt{gap}$ and $r$, and matches the upper\nbounds of Musco and Musco '15. Moreover, it establishes a strict separation\nbetween convex optimization and \\emph{randomized}, \"strict-saddle\" non-convex\noptimization of which PCA is a canonical example: in the former, first-order\nmethods can have dimension-free iteration complexity, whereas in PCA, the\niteration complexity of gradient-based methods must necessarily grow with the\ndimension.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 03:00:06 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 20:55:06 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Simchowitz", "Max", ""], ["Alaoui", "Ahmed El", ""], ["Recht", "Benjamin", ""]]}, {"id": "1804.01256", "submitter": "Thomas Guyet", "authors": "Thomas Guyet (LACODAM), Ren\\'e Quiniou (LACODAM)", "title": "NegPSpan: efficient extraction of negative sequential patterns with\n  embedding constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining frequent sequential patterns consists in extracting recurrent\nbehaviors, modeled as patterns, in a big sequence dataset. Such patterns inform\nabout which events are frequently observed in sequences, i.e. what does really\nhappen. Sometimes, knowing that some specific event does not happen is more\ninformative than extracting a lot of observed events. Negative sequential\npatterns (NSP) formulate recurrent behaviors by patterns containing both\nobserved events and absent events. Few approaches have been proposed to mine\nsuch NSPs. In addition, the syntax and semantics of NSPs differ in the\ndifferent methods which makes it difficult to compare them. This article\nprovides a unified framework for the formulation of the syntax and the\nsemantics of NSPs. Then, we introduce a new algorithm, NegPSpan, that extracts\nNSPs using a PrefixSpan depth-first scheme and enabling maxgap constraints that\nother approaches do not take into account. The formal framework allows for\nhighlighting the differences between the proposed approach wrt to the methods\nfrom the literature, especially wrt the state of the art approach eNSP.\nIntensive experiments on synthetic and real datasets show that NegPSpan can\nextract meaningful NSPs and that it can process bigger datasets than eNSP\nthanks to significantly lower memory requirements and better computation times.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 06:47:32 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 13:42:47 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Guyet", "Thomas", "", "LACODAM"], ["Quiniou", "Ren\u00e9", "", "LACODAM"]]}, {"id": "1804.01308", "submitter": "Ran Ben Basat", "authors": "Ran Ben-Basat, Guy Even, Ken-ichi Kawarabayashi, Gregory Schwartzman", "title": "A Deterministic Distributed $2$-Approximation for Weighted Vertex Cover\n  in $O(\\log n\\log\\Delta / \\log^2\\log\\Delta)$ Rounds", "comments": "To appear in SIROCCO 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic distributed $2$-approximation algorithm for the\nMinimum Weight Vertex Cover problem in the CONGEST model whose round complexity\nis $O(\\log n \\log \\Delta / \\log^2 \\log \\Delta)$. This improves over the\ncurrently best known deterministic 2-approximation implied by [KVY94]. Our\nsolution generalizes the $(2+\\epsilon)$-approximation algorithm of [BCS17],\nimproving the dependency on $\\epsilon^{-1}$ from linear to logarithmic. In\naddition, for every $\\epsilon=(\\log \\Delta)^{-c}$, where $c\\geq 1$ is a\nconstant, our algorithm computes a $(2+\\epsilon)$-approximation in $O(\\log\n\\Delta / \\log \\log \\Delta)$~rounds (which is asymptotically optimal).\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 09:01:44 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 10:37:43 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 10:08:17 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Ben-Basat", "Ran", ""], ["Even", "Guy", ""], ["Kawarabayashi", "Ken-ichi", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "1804.01366", "submitter": "Euiwoong Lee", "authors": "Anupam Gupta, Euiwoong Lee, Jason Li, Pasin Manurangsi, Micha{\\l}\n  W{\\l}odarczyk", "title": "Losing Treewidth by Separating Subsets", "comments": "30 pages, 1 figure, to appear in SODA 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of deleting the smallest set $S$ of vertices (resp.\nedges) from a given graph $G$ such that the induced subgraph (resp. subgraph)\n$G \\setminus S$ belongs to some class $\\mathcal{H}$. We consider the case where\ngraphs in $\\mathcal{H}$ have treewidth bounded by $t$, and give a general\nframework to obtain approximation algorithms for both vertex and edge-deletion\nsettings from approximation algorithms for certain natural graph partitioning\nproblems called $k$-Subset Vertex Separator and $k$-Subset Edge Separator,\nrespectively.\n  For the vertex deletion setting, our framework combined with the current best\nresult for $k$-Subset Vertex Separator, yields a significant improvement in the\napproximation ratios for basic problems such as $k$-Treewidth Vertex Deletion\nand Planar-$F$ Vertex Deletion. Our algorithms are simpler than previous works\nand give the first uniform approximation algorithms under the natural\nparameterization.\n  For the edge deletion setting, we give improved approximation algorithms for\n$k$-Subset Edge Separator combining ideas from LP relaxations and important\nseparators. We present their applications in bounded-degree graphs, and also\ngive an APX-hardness result for the edge deletion problems.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 12:27:48 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 03:44:50 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Gupta", "Anupam", ""], ["Lee", "Euiwoong", ""], ["Li", "Jason", ""], ["Manurangsi", "Pasin", ""], ["W\u0142odarczyk", "Micha\u0142", ""]]}, {"id": "1804.01567", "submitter": "Bart{\\l}omiej Bosek", "authors": "Bart{\\l}omiej Bosek", "title": "On-line Chain Partitioning Approach to Scheduling", "comments": "PhD thesis defended at the Jagiellonian University (2008). 80 pages,\n  48 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An on-line chain partitioning algorithm receives the points of the poset from\nsome externally determined list. Being presented with a new point the algorithm\nlearns the comparability status of this new point to all previously presented\nones. As each point is received, the algorithm assigns this new point to a\nchain in an irrevocable manner and this assignment is made without knowledge of\nfuture points. Kierstead presented an algorithm using $(5^w-1)/4$ chains to\ncover each poset of width $w$. Felsner proved that width $2$ posets can be\npartitioned on-line into $5$ chains. We present an algorithm using $16$ chains\non posets of width $3$. This result significantly narrows down the previous\nbound of $31$. Moreover, we address the on-line chain partitioning problem for\ninterval orders. Kierstead and Trotter presented an algorithm using $3w-2$\nchains. We deal with an up-growing version of an on-line chain partition of\ninterval orders, i.e. we restrict possible inputs by the rule that each new\npoint is maximal at the moment of its arrival. We present an algorithm using\n$2w-1$ chains and show that there is no better one. These problems come from a\nneed for better algorithms that can be applied to scheduling. Each on-line\nchain partitioning algorithm schedules tasks in a multiprocessor environment,\nand therefore can be applied in order to minimize number of processors.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 14:52:26 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Bosek", "Bart\u0142omiej", ""]]}, {"id": "1804.01588", "submitter": "Hung Le", "authors": "Hung Le", "title": "A PTAS for subset TSP in minor-free graphs", "comments": "43 pages, 7 figures, complete revision of the old version, with new\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first PTAS for the subset Traveling Salesperson Problem (TSP) in\n$H$-minor-free graphs. This resolves a long standing open problem in a long\nline of work on designing PTASes for TSP in minor-closed families initiated by\nGrigni, Koutsoupias and Papadimitriou in FOCS'95. The main technical ingredient\nin our PTAS is a construction of a nearly light subset $(1+\\epsilon)$-spanner\nfor any given edge-weighted $H$-minor-free graph. This construction is based on\na necessary and sufficient condition given by \\emph{sparse spanner oracles}:\nlight subset spanners exist if and only if sparse spanner oracles exist. This\nrelationship allows us to obtain two new results: _ An $(1+\\epsilon)$-spanner\nwith lightness $O(\\epsilon^{-d+2})$ for any doubling metric of constant\ndimension $d$. This improves the earlier lightness bound $\\epsilon^{-O(d)}$\nobtained by Borradaile, Le and Wulff-Nilsen. _ An $(1+\\epsilon)$-spanner with\nsublinear lightness for any metric of constant correlation dimension.\nPreviously, no spanner with non-trivial lightness was known.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 19:54:15 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 01:36:02 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 23:18:02 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Le", "Hung", ""]]}, {"id": "1804.01614", "submitter": "Jianbin Qin", "authors": "Jianbin Qin and Chuan Xiao", "title": "Pigeonring: A Principle for Faster Thresholded Similarity Search", "comments": "17 pages, 38 figures. Accepted and published in VLDB 2019. Please\n  cite the VLDB paper: @article{DBLP:journals/pvldb/QinX18, author = {Jianbin\n  Qin and Chuan Xiao}, title = {Pigeonring: {A} Principle for Faster\n  Thresholded Similarity Search}, journal = {{PVLDB}}, year = {2018}, }", "journal-ref": "Proceedings of the VLDB Endowment, vol12, 2018, 1, 28-42", "doi": "10.14778/3275536.3275539", "report-no": null, "categories": "cs.DB cs.DS cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pigeonhole principle states that if $n$ items are contained in $m$ boxes,\nthen at least one box has no more than $n / m$ items. It is utilized to solve\nmany data management problems, especially for thresholded similarity searches.\nDespite many pigeonhole principle-based solutions proposed in the last few\ndecades, the condition stated by the principle is weak. It only constrains the\nnumber of items in a single box. By organizing the boxes in a ring, we propose\na new principle, called the pigeonring principle, which constrains the number\nof items in multiple boxes and yields stronger conditions. To utilize the new\nprinciple, we focus on problems defined in the form of identifying data objects\nwhose similarities or distances to the query is constrained by a threshold.\nMany solutions to these problems utilize the pigeonhole principle to find\ncandidates that satisfy a filtering condition. By the new principle, stronger\nfiltering conditions can be established. We show that the pigeonhole principle\nis a special case of the new principle. This suggests that all the pigeonhole\nprinciple-based solutions are possible to be accelerated by the new principle.\nA universal filtering framework is introduced to encompass the solutions to\nthese problems based on the new principle. Besides, we discuss how to quickly\nfind candidates specified by the new principle. The implementation requires\nonly minor modifications on top of existing pigeonhole principle-based\nalgorithms. Experimental results on real datasets demonstrate the applicability\nof the new principle as well as the superior performance of the algorithms\nbased on the new principle.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 22:01:43 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 21:42:29 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 03:10:24 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Qin", "Jianbin", ""], ["Xiao", "Chuan", ""]]}, {"id": "1804.01642", "submitter": "Jaros{\\l}aw B{\\l}asiok", "authors": "Jaros{\\l}aw B{\\l}asiok", "title": "Optimal streaming and tracking distinct elements with high probability", "comments": "Preliminary version of this paper appeard in SODA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distinct elements problem is one of the fundamental problems in streaming\nalgorithms --- given a stream of integers in the range $\\{1,\\ldots,n\\}$, we\nwish to provide a $(1+\\varepsilon)$ approximation to the number of distinct\nelements in the input. After a long line of research an optimal solution for\nthis problem with constant probability of success, using\n$\\mathcal{O}(\\frac{1}{\\varepsilon^2}+\\log n)$ bits of space, was given by Kane,\nNelson and Woodruff in 2010.\n  The standard approach used in order to achieve low failure probability\n$\\delta$ is to take the median of $\\log \\delta^{-1}$ parallel repetitions of\nthe original algorithm. We show that such a multiplicative space blow-up is\nunnecessary: we provide an optimal algorithm using $\\mathcal{O}(\\frac{\\log\n\\delta^{-1}}{\\varepsilon^2} + \\log n)$ bits of space --- matching known lower\nbounds for this problem. That is, the $\\log\\delta^{-1}$ factor does not\nmultiply the $\\log n$ term. This settles completely the space complexity of the\ndistinct elements problem with respect to all standard parameters.\n  We consider also the \\emph{strong tracking} (or \\emph{continuous monitoring})\nvariant of the distinct elements problem, where we want an algorithm which\nprovides an approximation of the number of distinct elements seen so far, at\nall times of the stream. We show that this variant can be solved using\n$\\mathcal{O}(\\frac{\\log \\log n + \\log \\delta^{-1}}{\\varepsilon^2} + \\log n)$\nbits of space, which we show to be optimal.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 01:13:27 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2019 15:44:25 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["B\u0142asiok", "Jaros\u0142aw", ""]]}, {"id": "1804.01736", "submitter": "Tatsuya Yokota", "authors": "Tatsuya Yokota, Burak Erem, Seyhmus Guler, Simon K. Warfield, Hidekata\n  Hontani", "title": "Missing Slice Recovery for Tensors Using a Low-rank Model in Embedded\n  Space", "comments": "accepted for CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let us consider a case where all of the elements in some continuous slices\nare missing in tensor data.\n  In this case, the nuclear-norm and total variation regularization methods\nusually fail to recover the missing elements.\n  The key problem is capturing some delay/shift-invariant structure.\n  In this study, we consider a low-rank model in an embedded space of a tensor.\n  For this purpose, we extend a delay embedding for a time series to a\n\"multi-way delay-embedding transform\" for a tensor, which takes a given\nincomplete tensor as the input and outputs a higher-order incomplete Hankel\ntensor.\n  The higher-order tensor is then recovered by Tucker-based low-rank tensor\nfactorization.\n  Finally, an estimated tensor can be obtained by using the inverse multi-way\ndelay embedding transform of the recovered higher-order tensor.\n  Our experiments showed that the proposed method successfully recovered\nmissing slices for some color images and functional magnetic resonance images.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 08:44:01 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Yokota", "Tatsuya", ""], ["Erem", "Burak", ""], ["Guler", "Seyhmus", ""], ["Warfield", "Simon K.", ""], ["Hontani", "Hidekata", ""]]}, {"id": "1804.01823", "submitter": "Shahbaz Khan", "authors": "Manoj Gupta, Shahbaz Khan", "title": "Simple dynamic algorithms for Maximal Independent Set and other problems", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most graphs in real life keep changing with time. These changes can be in the\nform of insertion or deletion of edges or vertices. Such rapidly changing\ngraphs motivate us to study dynamic graph algorithms. However, three important\ngraph problems that are perhaps not sufficiently addressed in the literature\ninclude independent sets, maximum matching (exact) and maximum flows.\n  Maximal Independent Set (MIS) is one of the most prominently studied problems\nin the distributed setting. Recently, the first dynamic MIS algorithm for\ndistributed networks was given by Censor-Hillel et al. [PODC16], requiring\nexpected $O(1)$ amortized rounds with $O(\\Delta)$ messages per update, where\n$\\Delta$ is the maximum degree of a vertex in the graph. They suggested an open\nproblem to maintain MIS in fully dynamic centralized setting more efficiently.\nAssadi et al. [STOC18] presented a deterministic centralized fully dynamic MIS\nalgorithm requiring $O(\\min\\{\\Delta,m^{3/4}\\})$ amortized time per update. This\nresult is quite complex involving an exhaustive case analysis. We report a\nsurprisingly simple deterministic centralized algorithm which improves the\namortized update time to $O(\\min\\{\\Delta,m^{2/3}\\})$.\n  Additionally, we present some other minor results related to dynamic MIS,\nMaximum Flow, and Maximum Matching. A common trait of all our results is that\ndespite improving state of the art upper bounds or matching state of the art\nlower bounds, they are surprisingly simple and are analysed using simple\namortization arguments. Further, they use no complicated data structures or\nblack box algorithms for their implementation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 13:08:16 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 09:42:51 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Gupta", "Manoj", ""], ["Khan", "Shahbaz", ""]]}, {"id": "1804.01937", "submitter": "Uwe Baier", "authors": "Uwe Baier", "title": "On Undetected Redundancy in the Burrows-Wheeler Transform", "comments": "20 pages, accepted for Combinatorial Pattern Matching 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Burrows-Wheeler-Transform (BWT) is an invertible permutation of a text\nknown to be highly compressible but also useful for sequence analysis, what\nmakes the BWT highly attractive for lossless data compression. In this paper,\nwe present a new technique to reduce the size of a BWT using its combinatorial\nproperties, while keeping it invertible. The technique can be applied to any\nBWT-based compressor, and, as experiments show, is able to reduce the encoding\nsize by 8-16 % on average and up to 33-57 % in the best cases (depending on the\nBWT-compressor used), making BWT-based compressors competitive or even superior\nto today's best lossless compressors.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 16:21:33 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Baier", "Uwe", ""]]}, {"id": "1804.01973", "submitter": "Andr\\'as Gily\\'en", "authors": "Shantanav Chakraborty and Andr\\'as Gily\\'en and Stacey Jeffery", "title": "The power of block-encoded matrix powers: improved regression techniques\n  via faster Hamiltonian simulation", "comments": "58 pages", "journal-ref": "In Proceedings of the 46th International Colloquium on Automata,\n  Languages, and Programming (ICALP 2019), pp. 33:1-33:14", "doi": "10.4230/LIPIcs.ICALP.2019.33", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the framework of block-encodings, introduced by Low and Chuang\n(under the name standard-form), to the study of quantum machine learning\nalgorithms and derive general results that are applicable to a variety of input\nmodels, including sparse matrix oracles and matrices stored in a data\nstructure. We develop several tools within the block-encoding framework, such\nas singular value estimation of a block-encoded matrix, and quantum linear\nsystem solvers using block-encodings. The presented results give new techniques\nfor Hamiltonian simulation of non-sparse matrices, which could be relevant for\ncertain quantum chemistry applications, and which in turn imply an exponential\nimprovement in the dependence on precision in quantum linear systems solvers\nfor non-sparse matrices.\n  In addition, we develop a technique of variable-time amplitude estimation,\nbased on Ambainis' variable-time amplitude amplification technique, which we\nare also able to apply within the framework.\n  As applications, we design the following algorithms: (1) a quantum algorithm\nfor the quantum weighted least squares problem, exhibiting a 6-th power\nimprovement in the dependence on the condition number and an exponential\nimprovement in the dependence on the precision over the previous best algorithm\nof Kerenidis and Prakash; (2) the first quantum algorithm for the quantum\ngeneralized least squares problem; and (3) quantum algorithms for estimating\nelectrical-network quantities, including effective resistance and dissipated\npower, improving upon previous work.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 17:51:01 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 18:54:53 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Chakraborty", "Shantanav", ""], ["Gily\u00e9n", "Andr\u00e1s", ""], ["Jeffery", "Stacey", ""]]}, {"id": "1804.02075", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Dariusz Dereniowski, Stefan Tiegel, Przemys{\\l}aw Uzna\\'nski, Daniel\n  Wolleb-Graf", "title": "A Framework for Searching in Graphs in the Presence of Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of searching for an unknown target vertex $t$ in a\n(possibly edge-weighted) graph. Each \\emph{vertex-query} points to a vertex $v$\nand the response either admits $v$ is the target or provides any neighbor\n$s\\not=v$ that lies on a shortest path from $v$ to $t$. This model has been\nintroduced for trees by Onak and Parys [FOCS 2006] and for general graphs by\nEmamjomeh-Zadeh et al. [STOC 2016]. In the latter, the authors provide\nalgorithms for the error-less case and for the independent noise model (where\neach query independently receives an erroneous answer with known probability\n$p<1/2$ and a correct one with probability $1-p$).\n  We study this problem in both adversarial errors and independent noise\nmodels. First, we show an algorithm that needs $\\frac{\\log_2 n}{1 - H(r)}$\nqueries against \\emph{adversarial} errors, where adversary is bounded with its\nrate of errors by a known constant $r<1/2$. Our algorithm is in fact a\nsimplification of previous work, and our refinement lies in invoking\namortization argument. We then show that our algorithm coupled with Chernoff\nbound argument leads to an algorithm for independent noise that is simpler and\nwith a query complexity that is both simpler and asymptotically better to one\nof Emamjomeh-Zadeh et al. [STOC 2016].\n  Our approach has a wide range of applications. First, it improves and\nsimplifies Robust Interactive Learning framework proposed by Emamjomeh-Zadeh et\nal. [NIPS 2017]. Secondly, performing analogous analysis for\n\\emph{edge-queries} (where query to edge $e$ returns its endpoint that is\ncloser to target) we actually recover (as a special case) noisy binary search\nalgorithm that is asymptotically optimal, matching the complexity of Feige et\nal. [SIAM J. Comput. 1994]. Thirdly, we improve and simplify upon existing\nalgorithm for searching of \\emph{unbounded} domains due to Aslam and Dhagat\n[STOC 1991].\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 22:49:15 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 17:31:28 GMT"}, {"version": "v3", "created": "Sun, 6 Jan 2019 20:17:09 GMT"}, {"version": "v4", "created": "Thu, 5 Mar 2020 21:27:19 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Dereniowski", "Dariusz", ""], ["Tiegel", "Stefan", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""], ["Wolleb-Graf", "Daniel", ""]]}, {"id": "1804.02112", "submitter": "Amr Elmasry", "authors": "Amr Elmasry, Mostafa Kahla, Fady Ahdy, Mahmoud Hashem", "title": "Red-Black Trees with Constant Update Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how a few modifications to the red-black trees allow for $O(1)$\nworst-case update time (once the position of the inserted or deleted element is\nknown). The resulting structure is based on relaxing some of the properties of\nthe red-black trees while guaranteeing that the height remains logarithmic with\nrespect to the number of nodes. Compared to the other search trees with\nconstant update time, our tree is the first to provide a tailored deletion\nprocedure without using the global rebuilding technique. In addition, our\nstructure is very simple to implement and allows for a simpler proof of\ncorrectness than those alternative trees.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 02:10:25 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Elmasry", "Amr", ""], ["Kahla", "Mostafa", ""], ["Ahdy", "Fady", ""], ["Hashem", "Mahmoud", ""]]}, {"id": "1804.02160", "submitter": "Yu Nakahata", "authors": "Yu Nakahata, Jun Kawahara, Shoji Kasahara", "title": "Enumerating Graph Partitions Without Too Small Connected Components\n  Using Zero-suppressed Binary and Ternary Decision Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Partitioning a graph into balanced components is important for several\napplications. For multi-objective problems, it is useful not only to find one\nsolution but also to enumerate all the solutions with good values of\nobjectives. However, there are a vast number of graph partitions in a graph,\nand thus it is difficult to enumerate desired graph partitions efficiently. In\nthis paper, an algorithm to enumerate all the graph partitions such that all\nthe weights of the connected components are at least a specified value is\nproposed. To deal with a large search space, we use zero-suppressed binary\ndecision diagrams (ZDDs) to represent sets of graph partitions and we design a\nnew algorithm based on frontier-based search, which is a framework to directly\nconstruct a ZDD. Our algorithm utilizes not only ZDDs but also ternary decision\ndiagrams (TDDs) and realizes an operation which seems difficult to be designed\nonly by ZDDs. Experimental results show that the proposed algorithm runs up to\ntens of times faster than an existing state-of-the-art algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 07:59:24 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Nakahata", "Yu", ""], ["Kawahara", "Jun", ""], ["Kasahara", "Shoji", ""]]}, {"id": "1804.02242", "submitter": "Christos Kalaitzis", "authors": "Fabrizio Grandoni and Christos Kalaitzis and Rico Zenklusen", "title": "Improved Approximation for Tree Augmentation: Saving by Rewiring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tree Augmentation Problem (TAP) is a fundamental network design problem\nin which we are given a tree and a set of additional edges, also called\n\\emph{links}. The task is to find a set of links, of minimum size, whose\naddition to the tree leads to a $2$-edge-connected graph. A long line of\nresults on TAP culminated in the previously best known approximation guarantee\nof $1.5$ achieved by a combinatorial approach due to Kortsarz and Nutov [ACM\nTransactions on Algorithms 2016], and also by an SDP-based approach by Cheriyan\nand Gao [Algorithmica 2017]. Moreover, an elegant LP-based\n$(1.5+\\epsilon)$-approximation has also been found very recently by Fiorini,\nGro\\ss, K\\\"onemann, and Sanit\\'a [SODA 2018]. In this paper, we show that an\napproximation factor below $1.5$ can be achieved, by presenting a\n$1.458$-approximation that is based on several new techniques.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 12:58:31 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Grandoni", "Fabrizio", ""], ["Kalaitzis", "Christos", ""], ["Zenklusen", "Rico", ""]]}, {"id": "1804.02269", "submitter": "Karol W\\k{e}grzycki", "authors": "Marcin Mucha, Karol W\\k{e}grzycki, Micha{\\l} W{\\l}odarczyk", "title": "A Subquadratic Approximation Scheme for Partition", "comments": "Extended abstract published in the proceedings of SODA 2019", "journal-ref": null, "doi": "10.1137/1.9781611975482.5", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subject of this paper is the time complexity of approximating Knapsack,\nSubset Sum, Partition, and some other related problems. The main result is an\n$\\widetilde{O}(n+1/\\varepsilon^{5/3})$ time randomized FPTAS for Partition,\nwhich is derived from a certain relaxed form of a randomized FPTAS for Subset\nSum. To the best of our knowledge, this is the first NP-hard problem that has\nbeen shown to admit a subquadratic time approximation scheme, i.e., one with\ntime complexity of $O((n+1/\\varepsilon)^{2-\\delta})$ for some $\\delta>0$. To\nput these developments in context, note that a quadratic FPTAS for \\partition\nhas been known for 40 years.\n  Our main contribution lies in designing a mechanism that reduces an instance\nof Subset Sum to several simpler instances, each with some special structure,\nand keeps track of interactions between them. This allows us to combine\ntechniques from approximation algorithms, pseudo-polynomial algorithms, and\nadditive combinatorics.\n  We also prove several related results. Notably, we improve approximation\nschemes for 3SUM, (min,+)-convolution, and Tree Sparsity. Finally, we argue why\nbreaking the quadratic barrier for approximate Knapsack is unlikely by giving\nan $\\Omega((n+1/\\varepsilon)^{2-o(1)})$ conditional lower bound.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 13:49:33 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 14:15:28 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Mucha", "Marcin", ""], ["W\u0119grzycki", "Karol", ""], ["W\u0142odarczyk", "Micha\u0142", ""]]}, {"id": "1804.02273", "submitter": "Vyacheslav Moklev", "authors": "Vyacheslav Moklev and Vladimir Ulyantsev", "title": "BFS Enumeration for Breaking Symmetries in Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are numerous NP-hard combinatorial problems which involve searching for\nan undirected graph satisfying a certain property. One way to solve such\nproblems is to translate a problem into an instance of the boolean\nsatisfiability (SAT) or constraint satisfaction (CSP) problem. Such reduction\nusually can give rise to numerous isomorphic representations of the same graph.\nOne way to reduce the search space and speed up the search under these\nconditions is to introduce symmetrybreaking predicates. In this paper we\nintroduce three novel and practically effective symmetry-breaking predicates\nfor an undirected connected graph search based on breadth-first search (BFS)\nenumeration and compare with existing symmetry-breaking methods on several\ngraph problems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 13:53:31 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Moklev", "Vyacheslav", ""], ["Ulyantsev", "Vladimir", ""]]}, {"id": "1804.02394", "submitter": "Eduard Gorbunov", "authors": "Pavel Dvurechensky and Eduard Gorbunov and Alexander Gasnikov", "title": "An Accelerated Directional Derivative Method for Smooth Stochastic\n  Convex Optimization", "comments": "arXiv admin note: text overlap with arXiv:1802.09022", "journal-ref": null, "doi": "10.1016/j.ejor.2020.08.027", "report-no": null, "categories": "math.OC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider smooth stochastic convex optimization problems in the context of\nalgorithms which are based on directional derivatives of the objective\nfunction. This context can be considered as an intermediate one between\nderivative-free optimization and gradient-based optimization. We assume that at\nany given point and for any given direction, a stochastic approximation for the\ndirectional derivative of the objective function at this point and in this\ndirection is available with some additive noise. The noise is assumed to be of\nan unknown nature, but bounded in the absolute value. We underline that we\nconsider directional derivatives in any direction, as opposed to coordinate\ndescent methods which use only derivatives in coordinate directions. For this\nsetting, we propose a non-accelerated and an accelerated directional derivative\nmethod and provide their complexity bounds. Our non-accelerated algorithm has a\ncomplexity bound which is similar to the gradient-based algorithm, that is,\nwithout any dimension-dependent factor. Our accelerated algorithm has a\ncomplexity bound which coincides with the complexity bound of the accelerated\ngradient-based algorithm up to a factor of square root of the problem\ndimension. We extend these results to strongly convex problems.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 11:36:35 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 09:02:49 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Dvurechensky", "Pavel", ""], ["Gorbunov", "Eduard", ""], ["Gasnikov", "Alexander", ""]]}, {"id": "1804.02465", "submitter": "Shuai Huang", "authors": "Shuai Huang, Ivan Dokmani\\'c", "title": "Reconstructing Point Sets from Distance Distributions", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, Vol. 69, 1181-1127, Mar.\n  2021", "doi": "10.1109/TSP.2021.3063458", "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of reconstructing a set of points on a line or a loop\nfrom their unassigned noisy pairwise distances. When the points lie on a line,\nthe problem is known as the turnpike; when they are on a loop, it is known as\nthe beltway. We approximate the problem by discretizing the domain and\nrepresenting the $N$ points via an $N$-hot encoding, which is a density\nsupported on the discretized domain. We show how the distance distribution is\nthen simply a collection of quadratic functionals of this density and propose\nto recover the point locations so that the estimated distance distribution\nmatches the measured distance distribution. This can be cast as a constrained\nnonconvex optimization problem which we solve using projected gradient descent\nwith a suitable spectral initializer. We derive conditions under which the\nproposed distance distribution matching approach locally converges to a global\noptimizer at a linear rate. Compared to the conventional backtracking approach,\nour method jointly reconstructs all the point locations and is robust to noise\nin the measurements. We substantiate these claims with state-of-the-art\nperformance across a number of numerical experiments. Our method is the first\npractical approach to solve the large-scale noisy beltway problem where the\npoints lie on a loop.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 21:44:54 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 17:07:11 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 04:11:32 GMT"}, {"version": "v4", "created": "Mon, 26 Apr 2021 02:13:44 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Shuai", ""], ["Dokmani\u0107", "Ivan", ""]]}, {"id": "1804.02484", "submitter": "Andrea Rocchetto", "authors": "Alessandro Rudi, Leonard Wossnig, Carlo Ciliberto, Andrea Rocchetto,\n  Massimiliano Pontil, Simone Severini", "title": "Approximating Hamiltonian dynamics with the Nystr\\\"om method", "comments": "v2: 22 pages, fixed typos in Eq.27 and 28 + other minor changes to\n  the presentation of the results; v3 final version accepted to Quantum; v4\n  DOIs added in order to comply with Quantum requirements", "journal-ref": "Quantum 4, 234 (2020)", "doi": "10.22331/q-2020-02-20-234", "report-no": null, "categories": "quant-ph cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simulating the time-evolution of quantum mechanical systems is BQP-hard and\nexpected to be one of the foremost applications of quantum computers. We\nconsider classical algorithms for the approximation of Hamiltonian dynamics\nusing subsampling methods from randomized numerical linear algebra. We derive a\nsimulation technique whose runtime scales polynomially in the number of qubits\nand the Frobenius norm of the Hamiltonian. As an immediate application, we show\nthat sample based quantum simulation, a type of evolution where the Hamiltonian\nis a density matrix, can be efficiently classically simulated under specific\nstructural conditions. Our main technical contribution is a randomized\nalgorithm for approximating Hermitian matrix exponentials. The proof leverages\na low-rank, symmetric approximation via the Nystr\\\"om method. Our results\nsuggest that under strong sampling assumptions there exist classical\npoly-logarithmic time simulations of quantum computations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 23:58:30 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 10:32:34 GMT"}, {"version": "v3", "created": "Wed, 5 Feb 2020 16:52:23 GMT"}, {"version": "v4", "created": "Mon, 17 Feb 2020 20:30:19 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Rudi", "Alessandro", ""], ["Wossnig", "Leonard", ""], ["Ciliberto", "Carlo", ""], ["Rocchetto", "Andrea", ""], ["Pontil", "Massimiliano", ""], ["Severini", "Simone", ""]]}, {"id": "1804.02513", "submitter": "Hasan Heydari Gharehbolagh", "authors": "Hasan Heydari, S. Mahmoud Taheri, Kaveh Kavousi", "title": "Distributed Maximal Independent Set on Scale-Free Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of distributed maximal independent set (MIS) is investigated on\ninhomogeneous random graphs with power-law weights by which the scale-free\nnetworks can be produced. Such a particular problem has been solved on graphs\nwith $n$ vertices by state-of-the-art algorithms with the time complexity of\n$O(\\log{n})$. We prove that for a scale-free network with power-law exponent\n$\\beta > 3$, the induced subgraph is constructed by vertices with degrees\nlarger than $\\log{n}\\log^{*}{n}$ is a scale-free network with $\\beta' = 2$,\nalmost surely (a.s.). Then, we propose a new algorithm that computes an MIS on\nscale-free networks with the time complexity of\n$O(\\frac{\\log{n}}{\\log{\\log{n}}})$ a.s., which is better than $O(\\log{n})$.\nFurthermore, we prove that on scale-free networks with $\\beta \\geq 3$, the\narboricity and degeneracy are less than $2^{log^{1/3}n}$ with high probability\n(w.h.p.). Finally, we prove that the time complexity of finding an MIS on\nscale-free networks with $\\beta\\geq 3$ is $O(log^{2/3}n)$ w.h.p.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 06:06:42 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 19:51:01 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Heydari", "Hasan", ""], ["Taheri", "S. Mahmoud", ""], ["Kavousi", "Kaveh", ""]]}, {"id": "1804.02530", "submitter": "Shaofeng Jiang", "authors": "Lingxiao Huang, Shaofeng H.-C. Jiang, Jian Li, Xuan Wu", "title": "$\\varepsilon$-Coresets for Clustering (with Outliers) in Doubling\n  Metrics", "comments": "Appeared in FOCS 2018, this is the full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of constructing $\\varepsilon$-coresets for the $(k,\nz)$-clustering problem in a doubling metric $M(X, d)$. An $\\varepsilon$-coreset\nis a weighted subset $S\\subseteq X$ with weight function $w : S \\rightarrow\n\\mathbb{R}_{\\geq 0}$, such that for any $k$-subset $C \\in [X]^k$, it holds that\n$\\sum_{x \\in S}{w(x) \\cdot d^z(x, C)} \\in (1 \\pm \\varepsilon) \\cdot \\sum_{x \\in\nX}{d^z(x, C)}$.\n  We present an efficient algorithm that constructs an $\\varepsilon$-coreset\nfor the $(k, z)$-clustering problem in $M(X, d)$, where the size of the coreset\nonly depends on the parameters $k, z, \\varepsilon$ and the doubling dimension\n$\\mathsf{ddim}(M)$. To the best of our knowledge, this is the first efficient\n$\\varepsilon$-coreset construction of size independent of $|X|$ for general\nclustering problems in doubling metrics.\n  To this end, we establish the first relation between the doubling dimension\nof $M(X, d)$ and the shattering dimension (or VC-dimension) of the range space\ninduced by the distance $d$. Such a relation was not known before, since one\ncan easily construct instances in which neither one can be bounded by (some\nfunction of) the other. Surprisingly, we show that if we allow a small\n$(1\\pm\\epsilon)$-distortion of the distance function $d$, and consider the\nnotion of $\\tau$-error probabilistic shattering dimension, we can prove an\nupper bound of $O( \\mathsf{ddim}(M)\\cdot \\log(1/\\varepsilon)\n+\\log\\log{\\frac{1}{\\tau}} )$ for the probabilistic shattering dimension for\neven weighted doubling metrics. We believe this new relation is of independent\ninterest and may find other applications.\n  We also study the robust coresets and centroid sets in doubling metrics. Our\nrobust coreset construction leads to new results in clustering and property\ntesting, and the centroid sets can be used to accelerate the local search\nalgorithms for clustering problems.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 09:07:31 GMT"}, {"version": "v2", "created": "Sat, 18 Aug 2018 07:23:58 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Huang", "Lingxiao", ""], ["Jiang", "Shaofeng H. -C.", ""], ["Li", "Jian", ""], ["Wu", "Xuan", ""]]}, {"id": "1804.02537", "submitter": "Arkadiusz Socala", "authors": "{\\L}ukasz Kowalik and Arkadiusz Soca{\\l}a", "title": "Tight Lower Bounds for List Edge Coloring", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fastest algorithms for edge coloring run in time $2^m n^{O(1)}$, where\n$m$ and $n$ are the number of edges and vertices of the input graph,\nrespectively. For dense graphs, this bound becomes $2^{\\Theta(n^2)}$. This is a\nsomewhat unique situation, since most of the studied graph problems admit\nalgorithms running in time $2^{O(n\\log n)}$. It is a notorious open problem to\neither show an algorithm for edge coloring running in time $2^{o(n^2)}$ or to\nrefute it, assuming Exponential Time Hypothesis (ETH) or other well established\nassumption.\n  We notice that the same question can be asked for list edge coloring, a\nwell-studied generalization of edge coloring where every edge comes with a set\n(often called a list) of allowed colors. Our main result states that list edge\ncoloring for simple graphs does not admit an algorithm running in time\n$2^{o(n^2)}$, unless ETH fails. Interestingly, the algorithm for edge coloring\nrunning in time $2^m n^{O(1)}$ generalizes to the list version without any\nasymptotic slow-down. Thus, our lower bound is essentially tight. This also\nmeans that in order to design an algorithm running in time $2^{o(n^2)}$ for\nedge coloring, one has to exploit its special features compared to the list\nversion.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 09:50:55 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Kowalik", "\u0141ukasz", ""], ["Soca\u0142a", "Arkadiusz", ""]]}, {"id": "1804.02584", "submitter": "Marek Adamczyk", "authors": "Marek Adamczyk, Micha{\\l} W{\\l}odarczyk", "title": "Random Order Contention Resolution Schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contention resolution schemes have proven to be an incredibly powerful\nconcept which allows to tackle a broad class of problems. The framework has\nbeen initially designed to handle submodular optimization under various types\nof constraints, that is, intersections of exchange systems (including\nmatroids), knapsacks, and unsplittable flows on trees. Later on, it turned out\nthat this framework perfectly extends to optimization under uncertainty, like\nstochastic probing and online selection problems, which further can be applied\nto mechanism design.\n  We add to this line of work by showing how to create contention resolution\nschemes for intersection of matroids and knapsacks when we work in the random\norder setting. More precisely, we do know the whole universe of elements in\nadvance, but they appear in an order given by a random permutation. Upon\narrival we need to irrevocably decide whether to take an element or not. We\nbring a novel technique for analyzing procedures in the random order setting\nthat is based on the martingale theory. This unified approach makes it easier\nto combine constraints, and we do not need to rely on the monotonicity of\ncontention resolution schemes.\n  Our paper fills the gaps, extends, and creates connections between many\nprevious results and techniques. The main application of our framework is a\n$k+4+\\varepsilon$ approximation ratio for the Bayesian multi-parameter\nunit-demand mechanism design under the constraint of $k$ matroids intersection,\nwhich improves upon the previous bounds of $4k-2$ and $e(k+1)$. Other results\ninclude improved approximation ratios for stochastic $k$-set packing and\nsubmodular stochastic probing over arbitrary non-negative submodular objective\nfunction, whereas previous results required the objective to be monotone.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 18:47:31 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 17:38:48 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 13:42:21 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Adamczyk", "Marek", ""], ["W\u0142odarczyk", "Micha\u0142", ""]]}, {"id": "1804.02627", "submitter": "Abu Reyan Ahmed", "authors": "Reyan Ahmed, Patrizio Angelini, Faryad Darabi Sahneh, Alon Efrat,\n  David Glickenstein, Martin Gronemann, Niklas Heinsohn, Stephen G. Kobourov,\n  Richard Spence, Joseph Watkins, and Alexander Wolff", "title": "Multi-Level Steiner Trees", "comments": "This paper has been accepted in 17th International Symposium on\n  Experimental Algorithms (SEA 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical Steiner tree problem, given an undirected, connected graph\n$G=(V,E)$ with non-negative edge costs and a set of \\emph{terminals}\n$T\\subseteq V$, the objective is to find a minimum-cost tree $E' \\subseteq E$\nthat spans the terminals. The problem is APX-hard; the best known approximation\nalgorithm has a ratio of $\\rho = \\ln(4)+\\varepsilon < 1.39$. In this paper, we\nstudy a natural generalization, the \\emph{multi-level Steiner tree} (MLST)\nproblem: given a nested sequence of terminals $T_{\\ell} \\subset \\dots \\subset\nT_1 \\subseteq V$, compute nested trees $E_{\\ell}\\subseteq \\dots \\subseteq\nE_1\\subseteq E$ that span the corresponding terminal sets with minimum total\ncost. The MLST problem and variants thereof have been studied under various\nnames including Multi-level Network Design, Quality-of-Service Multicast tree,\nGrade-of-Service Steiner tree, and Multi-Tier tree. Several approximation\nresults are known. We first present two simple $O(\\ell)$-approximation\nheuristics. Based on these, we introduce a rudimentary composite algorithm that\ngeneralizes the above heuristics, and determine its approximation ratio by\nsolving a linear program. We then present a method that guarantees the same\napproximation ratio using at most $2\\ell$ Steiner tree computations. We compare\nthese heuristics experimentally on various instances of up to 500 vertices\nusing three different network generation models. We also present various\ninteger linear programming (ILP) formulations for the MLST problem, and compare\ntheir running times on these instances. To our knowledge, the composite\nalgorithm achieves the best approximation ratio for up to $\\ell=100$ levels,\nwhich is sufficient for most applications such as network visualization or\ndesigning multi-level infrastructure.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 04:01:28 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 16:44:08 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Ahmed", "Reyan", ""], ["Angelini", "Patrizio", ""], ["Sahneh", "Faryad Darabi", ""], ["Efrat", "Alon", ""], ["Glickenstein", "David", ""], ["Gronemann", "Martin", ""], ["Heinsohn", "Niklas", ""], ["Kobourov", "Stephen G.", ""], ["Spence", "Richard", ""], ["Watkins", "Joseph", ""], ["Wolff", "Alexander", ""]]}, {"id": "1804.02731", "submitter": "Frances Cooper", "authors": "Frances Cooper and David Manlove", "title": "A 3/2-approximation algorithm for the Student-Project Allocation problem", "comments": "12 page paper, 60 pages (including appendix), 14 figures, 11 tables,\n  SEA 2018 Symposium on Experimental Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Student-Project Allocation problem with lecturer preferences over\nStudents (SPA-S) comprises three sets of agents, namely students, projects and\nlecturers, where students have preferences over projects and lecturers have\npreferences over students. In this scenario we seek a stable matching, that is,\nan assignment of students to projects such that there is no student and\nlecturer who have an incentive to deviate from their assignee/s. We study\nSPA-ST, the extension of SPA-S in which the preference lists of students and\nlecturers need not be strictly ordered, and may contain ties. In this scenario,\nstable matchings may be of different sizes, and it is known that MAX SPA-ST,\nthe problem of finding a maximum stable matching in SPA-ST, is NP-hard. We\npresent a linear-time 3/2-approximation algorithm for MAX SPA-ST and an Integer\nProgramming (IP) model to solve MAX SPA-ST optimally. We compare the\napproximation algorithm with the IP model experimentally using\nrandomly-generated data. We find that the performance of the approximation\nalgorithm easily surpassed the 3/2 bound, constructing a stable matching within\n92% of optimal in all cases, with the percentage being far higher for many\ninstances.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 17:55:18 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 08:50:28 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Cooper", "Frances", ""], ["Manlove", "David", ""]]}, {"id": "1804.02785", "submitter": "Huan Li", "authors": "Huan Li, Stacy Patterson, Yuhao Yi, Zhongzhi Zhang", "title": "Maximizing the Number of Spanning Trees in a Connected Graph", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of maximizing the number of spanning trees in a\nconnected graph by adding at most $k$ edges from a given candidate edge set. We\ngive both algorithmic and hardness results for this problem:\n  - We give a greedy algorithm that, using submodularity, obtains an\napproximation ratio of $(1 - 1/e - \\epsilon)$ in the exponent of the number of\nspanning trees for any $\\epsilon > 0$ in time $\\tilde{O}(m \\epsilon^{-1} + (n +\nq) \\epsilon^{-3})$, where $m$ and $q$ is the number of edges in the original\ngraph and the candidate edge set, respectively. Our running time is optimal\nwith respect to the input size up to logarithmic factors, and substantially\nimproves upon the $O(n^3)$ running time of the previous proposed greedy\nalgorithm with approximation ratio $(1 - 1/e)$ in the exponent. Notably, the\nindependence of our running time of $k$ is novel, comparing to conventional\ntop-$k$ selections on graphs that usually run in $\\Omega(mk)$ time. A key\ningredient of our greedy algorithm is a routine for maintaining effective\nresistances under edge additions in an online-offline hybrid setting.\n  - We show the exponential inapproximability of this problem by proving that\nthere exists a constant $c > 0$ such that it is NP-hard to approximate the\noptimum number of spanning trees in the exponent within $(1 - c)$. This\ninapproximability result follows from a reduction from the minimum path cover\nin undirected graphs, whose hardness again follows from the constant\ninapproximability of the Traveling Salesman Problem (TSP) with distances 1 and\n2. Thus, the approximation ratio of our algorithm is also optimal up to a\nconstant factor in the exponent. To our knowledge, this is the first hardness\nof approximation result for maximizing the number of spanning trees in a graph,\nor equivalently, by Kirchhoff's matrix-tree theorem, maximizing the determinant\nof an SDDM matrix.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 01:20:38 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2018 03:33:03 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Li", "Huan", ""], ["Patterson", "Stacy", ""], ["Yi", "Yuhao", ""], ["Zhang", "Zhongzhi", ""]]}, {"id": "1804.02801", "submitter": "Junjie Ye", "authors": "Wenjun Li, Junjie Ye, Yixin Cao", "title": "A $5k$-vertex Kernel for $P_2$-packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $P_2$-packing problem asks for whether a graph contains $k$\nvertex-disjoint paths each of length two. We continue the study of its\nkernelization algorithms, and develop a $5k$-vertex kernel.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 03:10:18 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Li", "Wenjun", ""], ["Ye", "Junjie", ""], ["Cao", "Yixin", ""]]}, {"id": "1804.02854", "submitter": "Vincent Froese", "authors": "Laurent Bulteau, Vincent Froese, Rolf Niedermeier", "title": "Tight Hardness Results for Consensus Problems on Circular Strings and\n  Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus problems for strings and sequences appear in numerous application\ncontexts, ranging from bioinformatics over data mining to machine learning.\nClosing some gaps in the literature, we show that several fundamental problems\nin this context are NP- and W[1]-hard, and that the known (partially\nbrute-force) algorithms are close to optimality assuming the Exponential Time\nHypothesis. Among our main contributions is to settle the complexity status of\ncomputing a mean in dynamic time warping spaces which, as pointed out by Brill\net al. [DMKD 2019], suffered from many unproven or false assumptions in the\nliterature. We prove this problem to be NP-hard and additionally show that a\nrecent dynamic programming algorithm is essentially optimal. In this context,\nwe study a broad family of circular string alignment problems. This family also\nserves as a key for our hardness reductions, and it is of independent\n(practical) interest in molecular biology. In particular, we show tight\nhardness and running time lower bounds for Circular Consensus String; notably,\nthe corresponding non-circular version is easily linear-time solvable.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 07:44:36 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 06:47:52 GMT"}, {"version": "v3", "created": "Thu, 1 Nov 2018 08:44:07 GMT"}, {"version": "v4", "created": "Thu, 11 Apr 2019 09:36:40 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Bulteau", "Laurent", ""], ["Froese", "Vincent", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "1804.02887", "submitter": "Mingyu Xiao", "authors": "Mingyu Xiao and Hiroshi Nagamochi", "title": "Some Reduction Operations to Pairwise Compatibility Graphs", "comments": "9 pages and 3 figures", "journal-ref": "Inf. Process. Lett. 153 (2020)", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph $G=(V,E)$ with a vertex set $V$ and an edge set $E$ is called a\npairwise compatibility graph (PCG, for short) if there are a tree $T$ whose\nleaf set is $V$, a non-negative edge weight $w$ in $T$, and two non-negative\nreals $d_{\\min}\\leq d_{\\max}$ such that $G$ has an edge $uv\\in E$ if and only\nif the distance between $u$ and $v$ in the weighted tree $(T,w)$ is in the\ninterval $[d_{\\min}, d_{\\max}]$. PCG is a new graph class motivated from\nbioinformatics. In this paper, we give some necessary and sufficient conditions\nfor PCG based on cut-vertices and twins, which provide reductions among PCGs.\nOur results imply that complete $k$-partite graph, cactus, and some other graph\nclasses are subsets of PCG.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 09:54:41 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Xiao", "Mingyu", ""], ["Nagamochi", "Hiroshi", ""]]}, {"id": "1804.02895", "submitter": "Mingyu Xiao", "authors": "Mingyu Xiao and Hiroshi Nagamochi", "title": "Characterizing Star-PCGs", "comments": "24 pages and 5 figures", "journal-ref": "Algorithmica 2020", "doi": "10.1007/s00453-020-00712-8", "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph $G$ is called a pairwise compatibility graph (PCG, for short) if it\nadmits a tuple $(T,w, d_{\\min},d_{\\max})$ of a tree $T$ whose leaf set is equal\nto the vertex set of $G$, a non-negative edge weight $w$, and two non-negative\nreals $d_{\\min}\\leq d_{\\max}$ such that $G$ has an edge between two vertices\n$u,v\\in V$ if and only if the distance between the two leaves $u$ and $v$ in\nthe weighted tree $(T,w)$ is in the interval $[d_{\\min}, d_{\\max}]$. The tree\n$T$ is also called a witness tree of the PCG $G$. The problem of testing if a\ngiven graph is a PCG is not known to be NP-hard yet. To obtain a complete\ncharacterization of PCGs is a wide open problem in computational biology and\ngraph theory. In literature, most witness trees admitted by known PCGs are\nstars and caterpillars. In this paper, we give a complete characterization for\na graph to be a star-PCG (a PCG that admits a star as its witness tree), which\nprovides us the first polynomial-time algorithm for recognizing star-PCGs.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 10:20:52 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Xiao", "Mingyu", ""], ["Nagamochi", "Hiroshi", ""]]}, {"id": "1804.02906", "submitter": "Philip Bille", "authors": "Philip Bille and Inge Li G{\\o}rtz", "title": "From Regular Expression Matching to Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a regular expression $R$ and a string $Q$, the regular expression\nparsing problem is to determine if $Q$ matches $R$ and if so, determine how it\nmatches, e.g., by a mapping of the characters of $Q$ to the characters in $R$.\nRegular expression parsing makes finding matches of a regular expression even\nmore useful by allowing us to directly extract subpatterns of the match, e.g.,\nfor extracting IP-addresses from internet traffic analysis or extracting\nsubparts of genomes from genetic data bases. We present a new general\ntechniques for efficiently converting a large class of algorithms that\ndetermine if a string $Q$ matches regular expression $R$ into algorithms that\ncan construct a corresponding mapping. As a consequence, we obtain the first\nefficient linear space solutions for regular expression parsing.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 10:46:48 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 11:54:20 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Bille", "Philip", ""], ["G\u00f8rtz", "Inge Li", ""]]}, {"id": "1804.02917", "submitter": "Francois Le Gall", "authors": "Fran\\c{c}ois Le Gall and Fr\\'ed\\'eric Magniez", "title": "Sublinear-Time Quantum Computation of the Diameter in CONGEST Networks", "comments": "21 pages; to appear in PODC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of the diameter is one of the most central problems in\ndistributed computation. In the standard CONGEST model, in which two adjacent\nnodes can exchange $O(\\log n)$ bits per round (here $n$ denotes the number of\nnodes of the network), it is known that exact computation of the diameter\nrequires $\\tilde \\Omega(n)$ rounds, even in networks with constant diameter. In\nthis paper we investigate quantum distributed algorithms for this problem in\nthe quantum CONGEST model, where two adjacent nodes can exchange $O(\\log n)$\nquantum bits per round. Our main result is a $\\tilde O(\\sqrt{nD})$-round\nquantum distributed algorithm for exact diameter computation, where $D$ denotes\nthe diameter. This shows a separation between the computational power of\nquantum and classical algorithms in the CONGEST model. We also show an\nunconditional lower bound $\\tilde \\Omega(\\sqrt{n})$ on the round complexity of\nany quantum algorithm computing the diameter, and furthermore show a tight\nlower bound $\\tilde \\Omega(\\sqrt{nD})$ for any distributed quantum algorithm in\nwhich each node can use only $\\textrm{poly}(\\log n)$ quantum bits of memory.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 11:24:24 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 00:12:56 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Gall", "Fran\u00e7ois Le", ""], ["Magniez", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1804.02949", "submitter": "Daniel Vial", "authors": "Daniel Vial, Vijay Subramanian", "title": "Personalized PageRank dimensionality and algorithmic implications", "comments": null, "journal-ref": "Proceedings of the ACM on Measurement and Analysis of Computing\n  Systems, June 2019", "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many systems, including the Internet, social networks, and the power grid,\ncan be represented as graphs. When analyzing graphs, it is often useful to\ncompute scores describing the relative importance or distance between nodes.\nOne example is Personalized PageRank (PPR), which assigns to each node $v$ a\nvector whose $i$-th entry describes the importance of the $i$-th node from the\nperspective of $v$. PPR has proven useful in many applications, such as\nrecommending who users should follow on social networks (if this $i$-th entry\nis large, $v$ may be interested in following the $i$-th user). Unfortunately,\ncomputing $n$ such PPR vectors (where $n$ is the number of nodes) is infeasible\nfor many graphs of interest.\n  In this work, we argue that the situation is not so dire. Our main result\nshows that the dimensionality of the set of PPR vectors scales sublinearly in\n$n$ with high probability, for a certain class of random graphs and for a\nnotion of dimensionality similar to rank. Put differently, we argue that the\neffective dimension of this set is much less than $n$, despite the fact that\nthe matrix containing these vectors has rank $n$. Furthermore, we show this\ndimensionality measure relates closely to the complexity of a PPR estimation\nscheme that was proposed (but not analyzed) by Jeh and Widom. This allows us to\nargue that accurately estimating all $n$ PPR vectors amounts to computing a\nvanishing fraction of the $n^2$ vector elements (when the technical assumptions\nof our main result are satisfied). Finally, we demonstrate empirically that\nsimilar conclusions hold when considering real-world networks, despite the\nassumptions of our theory not holding.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 12:56:17 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Vial", "Daniel", ""], ["Subramanian", "Vijay", ""]]}, {"id": "1804.03054", "submitter": "Samuel McCauley", "authors": "Samuel McCauley, Jesper W. Mikkelsen, Rasmus Pagh", "title": "Set Similarity Search for Skewed Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set similarity join, as well as the corresponding indexing problem set\nsimilarity search, are fundamental primitives for managing noisy or uncertain\ndata. For example, these primitives can be used in data cleaning to identify\ndifferent representations of the same object. In many cases one can represent\nan object as a sparse 0-1 vector, or equivalently as the set of nonzero entries\nin such a vector. A set similarity join can then be used to identify those\npairs that have an exceptionally large dot product (or intersection, when\nviewed as sets). We choose to focus on identifying vectors with large Pearson\ncorrelation, but results extend to other similarity measures. In particular, we\nconsider the indexing problem of identifying correlated vectors in a set S of\nvectors sampled from {0,1}^d. Given a query vector y and a parameter alpha in\n(0,1), we need to search for an alpha-correlated vector x in a data structure\nrepresenting the vectors of S. This kind of similarity search has been\nintensely studied in worst-case (non-random data) settings.\n  Existing theoretically well-founded methods for set similarity search are\noften inferior to heuristics that take advantage of skew in the data\ndistribution, i.e., widely differing frequencies of 1s across the d dimensions.\nThe main contribution of this paper is to analyze the set similarity problem\nunder a random data model that reflects the kind of skewed data distributions\nseen in practice, allowing theoretical results much stronger than what is\npossible in worst-case settings. Our indexing data structure is a recursive,\ndata-dependent partitioning of vectors inspired by recent advances in set\nsimilarity search. Previous data-dependent methods do not seem to allow us to\nexploit skew in item frequencies, so we believe that our work sheds further\nlight on the power of data dependence.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 15:21:11 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["McCauley", "Samuel", ""], ["Mikkelsen", "Jesper W.", ""], ["Pagh", "Rasmus", ""]]}, {"id": "1804.03112", "submitter": "Vera Traub", "authors": "Vera Traub and Jens Vygen", "title": "Beating the integrality ratio for s-t-tours in graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among various variants of the traveling salesman problem, the s-t-path graph\nTSP has the special feature that we know the exact integrality ratio, 3/2, and\nan approximation algorithm matching this ratio. In this paper, we go below this\nthreshold: we devise a polynomial-time algorithm for the s-t-path graph TSP\nwith approximation ratio 1.497. Our algorithm can be viewed as a refinement of\nthe 3/2-approximation algorithm by Seb\\H{o} and Vygen [2014], but we introduce\nseveral completely new techniques. These include a new type of\near-decomposition, an enhanced ear induction that reveals a novel connection to\nmatroid union, a stronger lower bound, and a reduction of general instances to\ninstances in which s and t have small distance (which works for general\nmetrics).\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 17:18:39 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 15:09:39 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Traub", "Vera", ""], ["Vygen", "Jens", ""]]}, {"id": "1804.03156", "submitter": "Sitan Chen", "authors": "Sitan Chen, Ankur Moitra", "title": "Linear Programming Bounds for Randomly Sampling Colorings", "comments": "30 pages, 3 figures; fixed some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math-ph math.MP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we study the problem of sampling random proper colorings of a bounded\ndegree graph. Let $k$ be the number of colors and let $d$ be the maximum\ndegree. In 1999, Vigoda showed that the Glauber dynamics is rapidly mixing for\nany $k > \\frac{11}{6} d$. It turns out that there is a natural barrier at\n$\\frac{11}{6}$, below which there is no one-step coupling that is contractive,\neven for the flip dynamics.\n  We use linear programming and duality arguments to guide our construction of\na better coupling. We fully characterize the obstructions to going beyond\n$\\frac{11}{6}$. These examples turn out to be quite brittle, and even starting\nfrom one, they are likely to break apart before the flip dynamics changes the\ndistance between two neighboring colorings. We use this intuition to design a\nvariable length coupling that shows that the Glauber dynamics is rapidly mixing\nfor any $k\\ge \\left(\\frac{11}{6} - \\epsilon_0\\right)d$ where $\\epsilon_0 \\geq\n9.4 \\cdot 10^{-5}$. This is the first improvement to Vigoda's analysis that\nholds for general graphs.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 18:00:14 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 05:54:39 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Chen", "Sitan", ""], ["Moitra", "Ankur", ""]]}, {"id": "1804.03195", "submitter": "Jonathan Schneider", "authors": "Renato Paes Leme and Jon Schneider", "title": "Contextual Search via Intrinsic Volumes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of contextual search, a multidimensional generalization\nof binary search that captures many problems in contextual decision-making. In\ncontextual search, a learner is trying to learn the value of a hidden vector $v\n\\in [0,1]^d$. Every round the learner is provided an adversarially-chosen\ncontext $u_t \\in \\mathbb{R}^d$, submits a guess $p_t$ for the value of $\\langle\nu_t, v\\rangle$, learns whether $p_t < \\langle u_t, v\\rangle$, and incurs loss\n$\\ell(\\langle u_t, v\\rangle, p_t)$ (for some loss function $\\ell$). The\nlearner's goal is to minimize their total loss over the course of $T$ rounds.\n  We present an algorithm for the contextual search problem for the symmetric\nloss function $\\ell(\\theta, p) = |\\theta - p|$ that achieves $O_{d}(1)$ total\nloss. We present a new algorithm for the dynamic pricing problem (which can be\nrealized as a special case of the contextual search problem) that achieves\n$O_{d}(\\log \\log T)$ total loss, improving on the previous best known upper\nbounds of $O_{d}(\\log T)$ and matching the known lower bounds (up to a\npolynomial dependence on $d$). Both algorithms make significant use of ideas\nfrom the field of integral geometry, most notably the notion of intrinsic\nvolumes of a convex set. To the best of our knowledge this is the first\napplication of intrinsic volumes to algorithm design.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 19:30:29 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 07:12:21 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Leme", "Renato Paes", ""], ["Schneider", "Jon", ""]]}, {"id": "1804.03197", "submitter": "Barna Saha", "authors": "Amir Abboud, Raghavendra Addanki, Fabrizio Grandoni, Debmalya\n  Panigrahi and Barna Saha", "title": "Dynamic Set Cover: Improved Algorithms & Lower Bounds", "comments": "The STOC final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new upper and lower bounds for the {\\em dynamic} set cover problem.\nFirst, we give a $(1+\\epsilon) f$-approximation for fully dynamic set cover in\n$O(f^2\\log n /\\epsilon^5)$ (amortized) update time, for any $\\epsilon > 0$,\nwhere $f$ is the maximum number of sets that an element belongs to. In the\ndecremental setting, the update time can be improved to $O(f^2/\\epsilon^5)$,\nwhile still obtaining an $(1+\\epsilon) f$-approximation. These are the first\nalgorithms that obtain an approximation factor linear in $f$ for dynamic set\ncover, thereby almost matching the best bounds known in the offline setting and\nimproving upon the previous best approximation of $O(f^2)$ in the dynamic\nsetting.\n  To complement our upper bounds, we also show that a linear dependence of the\nupdate time on $f$ is necessary unless we can tolerate much worse approximation\nfactors. Using the recent distributed PCP-framework, we show that any dynamic\nset cover algorithm that has an amortized update time of $O(f^{1-\\epsilon})$\nmust have an approximation factor that is $\\Omega(n^\\delta)$ for some constant\n$\\delta>0$ under the Strong Exponential Time Hypothesis.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 19:35:48 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2018 19:27:23 GMT"}, {"version": "v3", "created": "Tue, 14 May 2019 20:49:09 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Abboud", "Amir", ""], ["Addanki", "Raghavendra", ""], ["Grandoni", "Fabrizio", ""], ["Panigrahi", "Debmalya", ""], ["Saha", "Barna", ""]]}, {"id": "1804.03244", "submitter": "Alon Eden", "authors": "Alon Eden, Michal Feldman, Amos Fiat, Tzahi Taub", "title": "Prompt Scheduling for Selfish Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a prompt online mechanism for minimizing the sum of [weighted]\ncompletion times. This is the first prompt online algorithm for the problem.\nWhen such jobs are strategic agents, delaying scheduling decisions makes little\nsense. Moreover, the mechanism has a particularly simple form of an anonymous\nmenu of options.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 21:26:22 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Eden", "Alon", ""], ["Feldman", "Michal", ""], ["Fiat", "Amos", ""], ["Taub", "Tzahi", ""]]}, {"id": "1804.03256", "submitter": "Martin Wilhelm", "authors": "Martin Wilhelm", "title": "Restructuring expression dags for efficient parallelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DC cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the field of robust geometric computation it is often necessary to make\nexact decisions based on inexact floating-point arithmetic. One common approach\nis to store the computation history in an arithmetic expression dag and to\nre-evaluate the expression with increasing precision until an exact decision\ncan be made. We show that exact-decisions number types based on expression dags\ncan be evaluated faster in practice through parallelization on multiple cores.\nWe compare the impact of several restructuring methods for the expression dag\non its running time in a parallel environment.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 22:08:13 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Wilhelm", "Martin", ""]]}, {"id": "1804.03423", "submitter": "Robert Ganian", "authors": "Robert Ganian, Iyad Kanj, Sebastian Ordyniak, Stefan Szeider", "title": "Parameterized Algorithms for the Matrix Completion Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two matrix completion problems, in which we are given a matrix\nwith missing entries and the task is to complete the matrix in a way that (1)\nminimizes the rank, or (2) minimizes the number of distinct rows. We study the\nparameterized complexity of the two aforementioned problems with respect to\nseveral parameters of interest, including the minimum number of matrix rows,\ncolumns, and rows plus columns needed to cover all missing entries. We obtain\nnew algorithmic results showing that, for the bounded domain case, both\nproblems are fixed-parameter tractable with respect to all aforementioned\nparameters. We complement these results with a lower-bound result for the\nunbounded domain case that rules out fixed-parameter tractability w.r.t. some\nof the parameters under consideration.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 09:52:54 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 10:49:47 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Ganian", "Robert", ""], ["Kanj", "Iyad", ""], ["Ordyniak", "Sebastian", ""], ["Szeider", "Stefan", ""]]}, {"id": "1804.03436", "submitter": "Mauro Ianni", "authors": "Romolo Marotta, Mauro Ianni, Alessandro Pellegrini, Andrea Scarselli,\n  Francesco Quaglia", "title": "A Non-blocking Buddy System for Scalable Memory Allocation on Multi-core\n  Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common implementations of core memory allocation components, like the Linux\nbuddy system, handle concurrent allocation/release requests by synchronizing\nthreads via spin-locks. This approach is clearly not prone to scale with large\nthread counts, a problem that has been addressed in the literature by\nintroducing layered allocation services or replicating the core allocators-the\nbottom most ones within the layered architecture. Both these solutions tend to\nreduce the pressure of actual concurrent accesses to each individual core\nallocator. In this article we explore an alternative approach to scalability of\nmemory allocation/release, which can be still combined with those literature\nproposals. Conflict detection relies on conventional atomic machine\ninstructions in the Read-Modify-Write (RMW) class. Furthermore, beyond\nimproving scalability and performance, it can also avoid wasting clock cycles\nfor spin-lock operations by threads that could in principle carry out their\nmemory allocation/release in full concurrency. Thus, it is resilient to\nperformance degradation---in face of concurrent accesses---independently of the\ncurrent level of fragmentation of the handled memory blocks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 10:25:16 GMT"}, {"version": "v2", "created": "Sat, 19 May 2018 12:12:08 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Marotta", "Romolo", ""], ["Ianni", "Mauro", ""], ["Pellegrini", "Alessandro", ""], ["Scarselli", "Andrea", ""], ["Quaglia", "Francesco", ""]]}, {"id": "1804.03485", "submitter": "Sumedha Uniyal", "authors": "Parinya Chalermsook, Andreas Schmid and Sumedha Uniyal", "title": "A Tight Extremal Bound on the Lov\\'{a}sz Cactus Number in Planar Graphs", "comments": "This result appeared in STACS19", "journal-ref": null, "doi": "10.4230/LIPIcs.STACS.2019.19", "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cactus graph is a graph in which any two cycles are edge-disjoint. We\npresent a constructive proof of the fact that any plane graph $G$ contains a\ncactus subgraph $C$ where $C$ contains at least a $\\frac{1}{6}$ fraction of the\ntriangular faces of $G$. We also show that this ratio cannot be improved by\nshowing a tight lower bound. Together with an algorithm for linear matroid\nparity, our bound implies two approximation algorithms for computing \"dense\nplanar structures\" inside any graph: (i) A $\\frac{1}{6}$ approximation\nalgorithm for, given any graph $G$, finding a planar subgraph with a maximum\nnumber of triangular faces; this improves upon the previous\n$\\frac{1}{11}$-approximation; (ii) An alternate (and arguably more\nillustrative) proof of the $\\frac{4}{9}$ approximation algorithm for finding a\nplanar subgraph with a maximum number of edges.\n  Our bound is obtained by analyzing a natural local search strategy and\nheavily exploiting the exchange arguments. Therefore, this suggests the power\nof local search in handling problems of this kind.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 12:50:19 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 10:56:43 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 15:57:01 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Chalermsook", "Parinya", ""], ["Schmid", "Andreas", ""], ["Uniyal", "Sumedha", ""]]}, {"id": "1804.03594", "submitter": "Marc Goerigk", "authors": "Andr\\'e Chassein, Marc Goerigk, Adam Kasperski, Pawe{\\l} Zieli\\'nski", "title": "Approximating multiobjective combinatorial optimization problems with\n  the OWA criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with a multiobjective combinatorial optimization problem with\n$K$ linear cost functions. The popular Ordered Weighted Averaging (OWA)\ncriterion is used to aggregate the cost functions and compute a solution. It is\nwell known that minimizing OWA for most basic combinatorial problems is weakly\nNP-hard even if the number of objectives $K$ equals two, and strongly NP-hard\nwhen $K$ is a part of the input. In this paper, the problem with nonincreasing\nweights in the OWA criterion and a large $K$ is considered. A method of\nreducing the number of objectives by appropriately aggregating the objective\ncosts before solving the problem is proposed. It is shown that an optimal\nsolution to the reduced problem has a guaranteed worst-case approximation\nratio. Some new approximation results for the Hurwicz criterion, which is a\nspecial case of OWA, are also presented.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 15:41:29 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Chassein", "Andr\u00e9", ""], ["Goerigk", "Marc", ""], ["Kasperski", "Adam", ""], ["Zieli\u0144ski", "Pawe\u0142", ""]]}, {"id": "1804.03604", "submitter": "Bernhard Haeupler", "authors": "Bernhard Haeupler", "title": "Optimal Document Exchange and New Codes for Insertions and Deletions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first communication-optimal document exchange protocol. For any\n$n$ and $k < n$ our randomized scheme takes any $n$-bit file $F$ and computes a\n$\\Theta(k \\log \\frac{n}{k})$-bit summary from which one can reconstruct $F$,\nwith high probability, given a related file $F'$ with edit distance $ED(F,F')\n\\leq k$.\n  The size of our summary is information-theoretically order optimal for all\nvalues of $k$, giving a randomized solution to a longstanding open question of\n[Orlitsky; FOCS'91]. It also is the first non-trivial solution for the\ninteresting setting where a small constant fraction of symbols have been\nedited, producing an optimal summary of size $O(H(\\delta)n)$ for $k=\\delta n$.\nThis concludes a long series of better-and-better protocols which produce\nlarger summaries for sub-linear values of $k$ and sub-polynomial failure\nprobabilities. In particular, the recent break-through of [Belazzougui, Zhang;\nFOCS'16] assumes that $k < n^\\epsilon$, produces a summary of size $O(k\\log^2 k\n+ k\\log n)$, and succeeds with probability $1-(k \\log n)^{-O(1)}$.\n  We also give an efficient derandomized document exchange protocol with\nsummary size $O(k \\log^2 \\frac{n}{k})$. This improves, for any $k$, over a\ndeterministic document exchange protocol by Belazzougui with summary size\n$O(k^2 + k \\log^2 n)$. Our deterministic document exchange directly provides\nnew efficient systematic error correcting codes for insertions and deletions.\nThese (binary) codes correct any $\\delta$ fraction of adversarial\ninsertions/deletions while having a rate of $1 - O(\\delta \\log^2\n\\frac{1}{\\delta})$ and improve over the codes of Guruswami and Li and Haeupler,\nShahrasbi and Vitercik which have rate $1 - \\Theta\\left(\\sqrt{\\delta}\n\\log^{O(1)} \\frac{1}{\\epsilon}\\right)$.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 15:54:34 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 23:50:43 GMT"}, {"version": "v3", "created": "Wed, 25 Sep 2019 23:19:16 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Haeupler", "Bernhard", ""]]}, {"id": "1804.03636", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane and John Peebles", "title": "Testing Identity of Multidimensional Histograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of identity testing for multidimensional histogram\ndistributions. A distribution $p: D \\rightarrow \\mathbb{R}_+$, where $D\n\\subseteq \\mathbb{R}^d$, is called a $k$-histogram if there exists a partition\nof the domain into $k$ axis-aligned rectangles such that $p$ is constant within\neach such rectangle. Histograms are one of the most fundamental nonparametric\nfamilies of distributions and have been extensively studied in computer science\nand statistics. We give the first identity tester for this problem with {\\em\nsub-learning} sample complexity in any fixed dimension and a nearly-matching\nsample complexity lower bound.\n  In more detail, let $q$ be an unknown $d$-dimensional $k$-histogram\ndistribution in fixed dimension $d$, and $p$ be an explicitly given\n$d$-dimensional $k$-histogram. We want to correctly distinguish, with\nprobability at least $2/3$, between the case that $p = q$ versus $\\|p-q\\|_1\n\\geq \\epsilon$. We design an algorithm for this hypothesis testing problem with\nsample complexity $O((\\sqrt{k}/\\epsilon^2) 2^{d/2} \\log^{2.5 d}(k/\\epsilon))$\nthat runs in sample-polynomial time. Our algorithm is robust to model\nmisspecification, i.e., succeeds even if $q$ is only promised to be {\\em close}\nto a $k$-histogram. Moreover, for $k = 2^{\\Omega(d)}$, we show a sample\ncomplexity lower bound of $(\\sqrt{k}/\\epsilon^2) \\cdot \\Omega(\\log(k)/d)^{d-1}$\nwhen $d\\geq 2$. That is, for any fixed dimension $d$, our upper and lower\nbounds are nearly matching. Prior to our work, the sample complexity of the\n$d=1$ case was well-understood, but no algorithm with sub-learning sample\ncomplexity was known, even for $d=2$. Our new upper and lower bounds have\ninteresting conceptual implications regarding the relation between learning and\ntesting in this setting.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 17:28:47 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 02:42:51 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Peebles", "John", ""]]}, {"id": "1804.03644", "submitter": "Vijay Bhattiprolu", "authors": "Vijay Bhattiprolu, Mrinalkanti Ghosh, Venkatesan Guruswami, Euiwoong\n  Lee, Madhur Tulsiani", "title": "Approximating Operator Norms via Generalized Krivine Rounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the $(\\ell_p,\\ell_r)$-Grothendieck problem, which seeks to\nmaximize the bilinear form $y^T A x$ for an input matrix $A$ over vectors $x,y$\nwith $\\|x\\|_p=\\|y\\|_r=1$. The problem is equivalent to computing the $p \\to\nr^*$ operator norm of $A$. The case $p=r=\\infty$ corresponds to the classical\nGrothendieck problem. Our main result is an algorithm for arbitrary $p,r \\ge 2$\nwith approximation ratio $(1+\\epsilon_0)/(\\sinh^{-1}(1)\\cdot \\gamma_{p^*}\n\\,\\gamma_{r^*})$ for some fixed $\\epsilon_0 \\le 0.00863$. Comparing this with\nKrivine's approximation ratio of $(\\pi/2)/\\sinh^{-1}(1)$ for the original\nGrothendieck problem, our guarantee is off from the best known hardness factor\nof $(\\gamma_{p^*} \\gamma_{r^*})^{-1}$ for the problem by a factor similar to\nKrivine's defect.\n  Our approximation follows by bounding the value of the natural vector\nrelaxation for the problem which is convex when $p,r \\ge 2$. We give a\ngeneralization of random hyperplane rounding and relate the performance of this\nrounding to certain hypergeometric functions, which prescribe necessary\ntransformations to the vector solution before the rounding is applied. Unlike\nKrivine's Rounding where the relevant hypergeometric function was $\\arcsin$, we\nhave to study a family of hypergeometric functions. The bulk of our technical\nwork then involves methods from complex analysis to gain detailed information\nabout the Taylor series coefficients of the inverses of these hypergeometric\nfunctions, which then dictate our approximation factor.\n  Our result also implies improved bounds for \"factorization through\n$\\ell_{2}^{\\,n}$\" of operators from $\\ell_{p}^{\\,n}$ to $\\ell_{q}^{\\,m}$ (when\n$p\\geq 2 \\geq q$)--- such bounds are of significant interest in functional\nanalysis and our work provides modest supplementary evidence for an intriguing\nparallel between factorizability, and constant-factor approximability.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 17:46:12 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 04:43:00 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Bhattiprolu", "Vijay", ""], ["Ghosh", "Mrinalkanti", ""], ["Guruswami", "Venkatesan", ""], ["Lee", "Euiwoong", ""], ["Tulsiani", "Madhur", ""]]}, {"id": "1804.03822", "submitter": "Ryo Yoshinaka", "authors": "Jun Kawahara, Toshiki Saitoh, Hirofumi Suzuki, Ryo Yoshinaka", "title": "Enumerating All Subgraphs without Forbidden Induced Subgraphs via\n  Multivalued Decision Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general method performed over multivalued decision diagrams that\nenumerates all subgraphs of an input graph that are characterized by input\nforbidden induced subgraphs. Our method combines elaborations of classical set\noperations and the developing construction technique, called the frontier based\nsearch, for multivalued decision diagrams. Using the algorithm, we enumerated\nall the chordal graphs of size at most 10 on multivalued decision diagrams.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 05:58:56 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Kawahara", "Jun", ""], ["Saitoh", "Toshiki", ""], ["Suzuki", "Hirofumi", ""], ["Yoshinaka", "Ryo", ""]]}, {"id": "1804.03842", "submitter": "Remy Cazabet", "authors": "Sou\\^aad Boudebza, R\\'emy Cazabet (DM2L, LIRIS, UCBL), Fai\\c{c}al\n  Azouaou (ESI), Omar Nouali (LPL)", "title": "OLCPM: An Online Framework for Detecting Overlapping Communities in\n  Dynamic Social Networks", "comments": "Journal of Computer Communications, In press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community structure is one of the most prominent features of complex\nnetworks. Community structure detection is of great importance to provide\ninsights into the network structure and functionalities. Most proposals focus\non static networks. However, finding communities in a dynamic network is even\nmore challenging, especially when communities overlap with each other. In this\narticle , we present an online algorithm, called OLCPM, based on clique\npercolation and label propagation methods. OLCPM can detect overlapping\ncommunities and works on temporal networks with a fine granularity. By locally\nupdating the community structure, OLCPM delivers significant improvement in\nrunning time compared with previous clique percolation techniques. The\nexperimental results on both synthetic and real-world networks illustrate the\neffectiveness of the method.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 07:29:58 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Boudebza", "Sou\u00e2ad", "", "DM2L, LIRIS, UCBL"], ["Cazabet", "R\u00e9my", "", "DM2L, LIRIS, UCBL"], ["Azouaou", "Fai\u00e7al", "", "ESI"], ["Nouali", "Omar", "", "LPL"]]}, {"id": "1804.03884", "submitter": "Ignasi Sau", "authors": "J\\'ulio Ara\\'ujo, Cl\\'audia Linhares Sales, Ignasi Sau, Ana Silva", "title": "Weighted proper orientations of trees and graphs of bounded treewidth", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a simple graph $G$, a weight function $w:E(G)\\rightarrow \\mathbb{N}\n\\setminus \\{0\\}$, and an orientation $D$ of $G$, we define $\\mu^-(D) = \\max_{v\n\\in V(G)} w_D^-(v)$, where $w^-_D(v) = \\sum_{u\\in N_D^{-}(v)}w(uv)$. We say\nthat $D$ is a weighted proper orientation of $G$ if $w^-_D(u) \\neq w^-_D(v)$\nwhenever $u$ and $v$ are adjacent. We introduce the parameter weighted proper\norientation number of $G$, denoted by $\\overrightarrow{\\chi}(G,w)$, which is\nthe minimum, over all weighted proper orientations $D$ of $G$, of $\\mu^-(D)$.\nWhen all the weights are equal to 1, this parameter is equal to the proper\norientation number of $G$, which has been object of recent studies and whose\ndetermination is NP-hard in general, but polynomial-time solvable on trees.\nHere, we prove that the equivalent decision problem of the weighted proper\norientation number (i.e., $\\overrightarrow{\\chi}(G,w) \\leq k$?) is (weakly)\nNP-complete on trees but can be solved by a pseudo-polynomial time algorithm\nwhose running time depends on $k$. Furthermore, we present a dynamic\nprogramming algorithm to determine whether a general graph $G$ on $n$ vertices\nand treewidth at most ${\\sf tw}$ satisfies $\\overrightarrow{\\chi}(G,w) \\leq k$,\nrunning in time $O(2^{{\\sf tw}^2}\\cdot k^{3{\\sf tw}}\\cdot {\\sf tw} \\cdot n)$,\nand we complement this result by showing that the problem is W[1]-hard on\ngeneral graphs parameterized by the treewidth of $G$, even if the weights are\npolynomial in $n$.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 09:16:44 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Ara\u00fajo", "J\u00falio", ""], ["Sales", "Cl\u00e1udia Linhares", ""], ["Sau", "Ignasi", ""], ["Silva", "Ana", ""]]}, {"id": "1804.03929", "submitter": "Bernardo Lopo Tavares", "authors": "Bernardo Lopo Tavares", "title": "A synopsis of comparative metrics for classifications", "comments": "37 pages, 13 figures. Part of author's MSc thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Phylogeny is the study of the relations between biological entities. From it,\nthe need to compare tree-like graphs has risen and several metrics were\nestablished and researched, but since there is no definitive way to compare\nthem, its discussion is still open nowadays. All of them emphasize different\nfeatures of the structures and, of course, the efficiency of these computations\nalso varies. The work in this article is mainly expositive (a lifting from a\ncollection of papers and articles) with special care in its presentation\n(trying to mathematically formalize what was not presented that way previously)\nand filling (with original work) where information was not available (or at\nleast, to our knowledge) given the frame we set to fit these metrics, which was\nto state their discriminative power and time complexity. The Robinson Foulds,\nRobinson Foulds Length, Quartet, Triplet, Triplet Length, Geodesic metrics are\napproached with greater detail (stating also some of its problems in\nformulation and discussing its intricacies) but the reader can also expect that\nless used (but not necessarily less important or less promising) metrics will\nbe covered, which are Maximum Aggreement Subtree, Align, Cophenetic Correlation\nCoeficcient, Node, Similarity Based on Probability, Hybridization Number and\nSubtree Prune and Regraft. Finally, some challenges that sprouted from making\nthis synopsys are presented as a possible subject of study and research.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 11:23:30 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Tavares", "Bernardo Lopo", ""]]}, {"id": "1804.03953", "submitter": "Antonios Antoniadis", "authors": "Antonios Antoniadis, Krzysztof Fleszar, Ruben Hoeksma and Kevin\n  Schewior", "title": "A PTAS for Euclidean TSP with Hyperplane Neighborhoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Traveling Salesperson Problem with Neighborhoods (TSPN), we are given\na collection of geometric regions in some space. The goal is to output a tour\nof minimum length that visits at least one point in each region. Even in the\nEuclidean plane, TSPN is known to be APX-hard, which gives rise to studying\nmore tractable special cases of the problem. In this paper, we focus on the\nfundamental special case of regions that are hyperplanes in the $d$-dimensional\nEuclidean space. This case contrasts the much-better understood case of\nso-called fat regions.\n  While for $d=2$ an exact algorithm with running time $O(n^5)$ is known,\nsettling the exact approximability of the problem for $d=3$ has been repeatedly\nposed as an open question. To date, only an approximation algorithm with\nguarantee exponential in $d$ is known, and NP-hardness remains open.\n  For arbitrary fixed $d$, we develop a Polynomial Time Approximation Scheme\n(PTAS) that works for both the tour and path version of the problem. Our\nalgorithm is based on approximating the convex hull of the optimal tour by a\nconvex polytope of bounded complexity. Such polytopes are represented as\nsolutions of a sophisticated LP formulation, which we combine with the\nenumeration of crucial properties of the tour. As the approximation guarantee\napproaches $1$, our scheme adjusts the complexity of the considered polytopes\naccordingly.\n  In the analysis of our approximation scheme, we show that our search space\nincludes a sufficiently good approximation of the optimum. To do so, we develop\na novel and general sparsification technique to transform an arbitrary convex\npolytope into one with a constant number of vertices and, in turn, into one of\nbounded complexity in the above sense. Hereby, we maintain important properties\nof the polytope.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 12:13:57 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 14:08:10 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Antoniadis", "Antonios", ""], ["Fleszar", "Krzysztof", ""], ["Hoeksma", "Ruben", ""], ["Schewior", "Kevin", ""]]}, {"id": "1804.04016", "submitter": "Subrahmanyam Kalyanasundaram", "authors": "N. R. Aravind, Subrahmanyam Kalyanasundaram, Anjeneya Swami Kare", "title": "Bipartitioning Problems on Graphs with Bounded Tree-Width", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an undirected graph G, we consider the following problems: given a fixed\ngraph H, can we partition the vertices of G into two non-empty sets A and B\nsuch that neither the induced graph G[A] nor G[B] contain H (i) as a subgraph?\n(ii) as an induced subgraph? These problems are NP-complete and are expressible\nin monadic second order logic (MSOL). The MSOL formulation, together with\nCourcelle's theorem implies linear time solvability on graphs with bounded\ntree-width. This approach yields algorithms with running time f(|phi|, t) * n,\nwhere |phi| is the length of the MSOL formula, t is the tree-width of the graph\nand n is the number of vertices of the graph. The dependency of f(|phi|, t) on\n|phi| can be as bad as a tower of exponentials.\n  In this paper, we present explicit combinatorial algorithms for these\nproblems for graphs G whose tree-width is bounded. We obtain 2^{O(t^r)} * n\ntime algorithms when H is any fixed graph of order r. In the special case when\nH = K_r, a complete graph on r vertices, we get an 2^{O(t+r \\log t)} * n time\nalgorithm.\n  The techniques can be extended to provide FPT algorithms to determine the\nsmallest number q such that V can be partitioned into q parts such that none of\nthe parts have H as a subgraph (induced subgraph).\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 14:28:01 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Aravind", "N. R.", ""], ["Kalyanasundaram", "Subrahmanyam", ""], ["Kare", "Anjeneya Swami", ""]]}, {"id": "1804.04021", "submitter": "Henrik Barthels M.Sc.", "authors": "Henrik Barthels, Marcin Copik, Paolo Bientinesi", "title": "The Generalized Matrix Chain Algorithm", "comments": null, "journal-ref": "Proceedings of 2018 IEEE/ACM International Symposium on Code\n  Generation and Optimization, Vienna, Austria, February 24-28, 2018", "doi": "10.1145/3168804", "report-no": null, "categories": "cs.MS cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a generalized version of the matrix chain algorithm\nto generate efficient code for linear algebra problems, a task for which human\nexperts often invest days or even weeks of works. The standard matrix chain\nproblem consists in finding the parenthesization of a matrix product $M := A_1\nA_2 \\cdots A_n$ that minimizes the number of scalar operations. In practical\napplications, however, one frequently encounters more complicated expressions,\ninvolving transposition, inversion, and matrix properties. Indeed, the\ncomputation of such expressions relies on a set of computational kernels that\noffer functionality well beyond the simple matrix product. The challenge then\nshifts from finding an optimal parenthesization to finding an optimal mapping\nof the input expression to the available kernels. Furthermore, it is often the\ncase that a solution based on the minimization of scalar operations does not\nresult in the optimal solution in terms of execution time. In our experiments,\nthe generated code outperforms other libraries and languages on average by a\nfactor of about 9. The motivation for this work comes from the fact\nthat---despite great advances in the development of compilers---the task of\nmapping linear algebra problems to optimized kernels is still to be done\nmanually. In order to relieve the user from this complex task, new techniques\nfor the compilation of linear algebra expressions have to be developed.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 16:32:49 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Barthels", "Henrik", ""], ["Copik", "Marcin", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1804.04025", "submitter": "Guillem Perarnau", "authors": "Michelle Delcourt and Guillem Perarnau and Luke Postle", "title": "Rapid mixing of Glauber dynamics for colorings below Vigoda's $11/6$\n  threshold", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known conjecture in computer science and statistical physics is that\nGlauber dynamics on the set of $k$-colorings of a graph $G$ on $n$ vertices\nwith maximum degree $\\Delta$ is rapidly mixing for $k \\geq \\Delta +2$. In FOCS\n1999, Vigoda showed rapid mixing of flip dynamics with certain flip parameters\non the set of proper $k$-colorings for $k > \\frac{11}{6}\\Delta$, implying rapid\nmixing for Glauber dynamics. In this paper, we obtain the first improvement\nbeyond the $\\frac{11}{6}\\Delta$ barrier for general graphs by showing rapid\nmixing for $k > (\\frac{11}{6} - \\eta)\\Delta$ for some positive constant $\\eta$.\nThe key to our proof is combining path coupling with a new kind of metric that\nincorporates a count of the extremal configurations of the chain. Additionally,\nour results extend to list coloring, a widely studied generalization of\ncoloring. Combined, these results answer two open questions from Frieze and\nVigoda's 2007 survey paper on Glauber dynamics for colorings.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 14:38:23 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Delcourt", "Michelle", ""], ["Perarnau", "Guillem", ""], ["Postle", "Luke", ""]]}, {"id": "1804.04038", "submitter": "Yu Gao", "authors": "David Durfee, Yu Gao, Gramoz Goranci, Richard Peng", "title": "Fully Dynamic Effective Resistances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the \\emph{fully-dynamic} All-Pairs Effective\nResistance problem, where the goal is to maintain effective resistances on a\ngraph $G$ among any pair of query vertices under an intermixed sequence of edge\ninsertions and deletions in $G$. The effective resistance between a pair of\nvertices is a physics-motivated quantity that encapsulates both the congestion\nand the dilation of a flow. It is directly related to random walks, and it has\nbeen instrumental in the recent works for designing fast algorithms for\ncombinatorial optimization problems, graph sparsification, and network science.\n  We give a data-structure that maintains $(1+\\epsilon)$-approximations to\nall-pair effective resistances of a fully-dynamic unweighted, undirected\nmulti-graph $G$ with $\\tilde{O}(m^{4/5}\\epsilon^{-4})$ expected amortized\nupdate and query time, against an oblivious adversary. Key to our result is the\nmaintenance of a dynamic \\emph{Schur complement}~(also known as vertex\nresistance sparsifier) onto a set of terminal vertices of our choice.\n  This maintenance is obtained (1) by interpreting the Schur complement as a\nsum of random walks and (2) by randomly picking the vertex subset into which\nthe sparsifier is constructed. We can then show that each update in the graph\naffects a small number of such walks, which in turn leads to our sub-linear\nupdate time. We believe that this local representation of vertex sparsifiers\nmay be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 15:10:31 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Durfee", "David", ""], ["Gao", "Yu", ""], ["Goranci", "Gramoz", ""], ["Peng", "Richard", ""]]}, {"id": "1804.04051", "submitter": "Nisheeth Vishnoi", "authors": "Nisheeth K. Vishnoi and Ozan Yildiz", "title": "On Geodesically Convex Formulations for the Brascamp-Lieb Constant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CA math.MG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two non-convex formulations for computing the optimal constant in\nthe Brascamp-Lieb inequality corresponding to a given datum, and show that they\nare geodesically log-concave on the manifold of positive definite matrices\nendowed with the Riemannian metric corresponding to the Hessian of the\nlog-determinant function. The first formulation is present in the work of Lieb\nand the second is inspired by the work of Bennett et al. Recent works of Garg\net al.and Allen-Zhu et al. also imply a geodesically log-concave formulation of\nthe Brascamp-Lieb constant through a reduction to the operator scaling problem.\nHowever, the dimension of the arising optimization problem in their reduction\ndepends exponentially on the number of bits needed to describe the\nBrascamp-Lieb datum. The formulations presented here have dimensions that are\npolynomial in the bit complexity of the input datum.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 15:33:10 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Vishnoi", "Nisheeth K.", ""], ["Yildiz", "Ozan", ""]]}, {"id": "1804.04077", "submitter": "Marcin Pilipczuk", "authors": "G\\'abor Bacs\\'o and Daniel Lokshtanov and D\\'aniel Marx and Marcin\n  Pilipczuk and Zsolt Tuza and Erik Jan van Leeuwen", "title": "Subexponential-time Algorithms for Maximum Independent Set in $P_t$-free\n  and Broom-free Graphs", "comments": "A preliminary version of the paper, with weaker results and only a\n  subset of authors, appeared in the proceedings of IPEC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In algorithmic graph theory, a classic open question is to determine the\ncomplexity of the Maximum Independent Set problem on $P_t$-free graphs, that\nis, on graphs not containing any induced path on $t$ vertices. So far,\npolynomial-time algorithms are known only for $t\\le 5$ [Lokshtanov et al., SODA\n2014, 570--581, 2014], and an algorithm for $t=6$ announced recently [Grzesik\net al. Arxiv 1707.05491, 2017]. Here we study the existence of\nsubexponential-time algorithms for the problem: we show that for any $t\\ge 1$,\nthere is an algorithm for Maximum Independent Set on $P_t$-free graphs whose\nrunning time is subexponential in the number of vertices. Even for the weighted\nversion MWIS, the problem is solvable in $2^{O(\\sqrt {tn \\log n})}$ time on\n$P_t$-free graphs. For approximation of MIS in broom-free graphs, a similar\ntime bound is proved.\n  Scattered Set is the generalization of Maximum Independent Set where the\nvertices of the solution are required to be at distance at least $d$ from each\nother. We give a complete characterization of those graphs $H$ for which\n$d$-Scattered Set on $H$-free graphs can be solved in time subexponential in\nthe size of the input (that is, in the number of vertices plus the number of\nedges): If every component of $H$ is a path, then $d$-Scattered Set on $H$-free\ngraphs with $n$ vertices and $m$ edges can be solved in time\n$2^{O(|V(H)|\\sqrt{n+m}\\log (n+m))}$, even if $d$ is part of the input.\nOtherwise, assuming the Exponential-Time Hypothesis (ETH), there is no\n$2^{o(n+m)}$-time algorithm for $d$-Scattered Set for any fixed $d\\ge 3$ on\n$H$-free graphs with $n$-vertices and $m$-edges.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 16:32:17 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Bacs\u00f3", "G\u00e1bor", ""], ["Lokshtanov", "Daniel", ""], ["Marx", "D\u00e1niel", ""], ["Pilipczuk", "Marcin", ""], ["Tuza", "Zsolt", ""], ["van Leeuwen", "Erik Jan", ""]]}, {"id": "1804.04102", "submitter": "Victor Pan", "authors": "Victor Y. Pan", "title": "Fast Feasible and Unfeasible Matrix Multiplication", "comments": "35 pages, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast matrix-by-matrix multiplication (hereafter MM) is a highly recognized\nresearch subject. The record upper bound 3 of 1968 on the exponent of the\ncomplexity MM decreased below 2.38 by 1987, applies to celebrated problems in\nmany areas of computing, and is extensively cited in the Theory of Computing.\nFurther decrease of the exponent remains a celebrated challenge. Acceleration\nof MM in the Practice of Computing is a distinct challenge, because all known\nalgorithms supporting the exponents below 2.7733 improve straightforward MM\nonly for unfeasible MM of immense size, greatly exceeding the sizes of interest\nnowadays and in any foreseeable future. We first survey the mainstream study of\nthe acceleration of MM of unbounded sizes, cover the progress in decreasing the\nexponents of MM, comment on its impact on the theory and practice of computing,\nand recall various fundamental concepts and techniques supporting fast MM and\nnaturally introduced in that study by 1980. Then we demonstrate how the curse\nof recursion naturally entered the game of decreasing the record exponents.\nFinally we cover the State of the Art of efficient feasible MM, including some\nmost efficient known techniques and algorithms as well as various issues of\nnumerical and symbolic implementation. We hope that our review will help\nmotivate and properly focus further effort in this highly important area.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 17:18:16 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Pan", "Victor Y.", ""]]}, {"id": "1804.04178", "submitter": "Mahdi Boroujeni", "authors": "Mahdi Boroujeni and Soheil Ehsani and Mohammad Ghodsi and\n  MohammadTaghi HajiAghayi and Saeed Seddighin", "title": "Approximating Edit Distance in Truly Subquadratic Time: Quantum and\n  MapReduce", "comments": "A preliminary version of this paper was presented at SODA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edit distance between two strings is defined as the smallest number of\ninsertions, deletions, and substitutions that need to be made to transform one\nof the strings to another one. Approximating edit distance in subquadratic time\nis \"one of the biggest unsolved problems in the field of combinatorial pattern\nmatching\". Our main result is a quantum constant approximation algorithm for\ncomputing the edit distance in truly subquadratic time. More precisely, we give\nan $O(n^{1.858})$ quantum algorithm that approximates the edit distance within\na factor of $7$. We further extend this result to an $O(n^{1.781})$ quantum\nalgorithm that approximates the edit distance within a larger constant factor.\n  Our solutions are based on a framework for approximating edit distance in\nparallel settings. This framework requires as black box an algorithm that\ncomputes the distances of several smaller strings all at once. For a quantum\nalgorithm, we reduce the black box to \\textit{metric estimation} and provide\nefficient algorithms for approximating it. We further show that this framework\nenables us to approximate edit distance in distributed settings. To this end,\nwe provide a MapReduce algorithm to approximate edit distance within a factor\nof $3$, with sublinearly many machines and sublinear memory. Also, our\nalgorithm runs in a logarithmic number of rounds.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 19:20:04 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 20:43:21 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Boroujeni", "Mahdi", ""], ["Ehsani", "Soheil", ""], ["Ghodsi", "Mohammad", ""], ["HajiAghayi", "MohammadTaghi", ""], ["Seddighin", "Saeed", ""]]}, {"id": "1804.04239", "submitter": "Matthew Fahrbach", "authors": "Matthew Fahrbach, Gary L. Miller, Richard Peng, Saurabh Sawlani,\n  Junxing Wang, Shen Chen Xu", "title": "Graph Sketching Against Adaptive Adversaries Applied to the Minimum\n  Degree Algorithm", "comments": "58 pages, 3 figures. This is a substantially revised version of\n  arXiv:1711.08446 with an emphasis on the underlying theoretical problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the study of matrix elimination orderings in combinatorial\nscientific computing, we utilize graph sketching and local sampling to give a\ndata structure that provides access to approximate fill degrees of a matrix\nundergoing elimination in $O(\\text{polylog}(n))$ time per elimination and\nquery. We then study the problem of using this data structure in the minimum\ndegree algorithm, which is a widely-used heuristic for producing elimination\norderings for sparse matrices by repeatedly eliminating the vertex with\n(approximate) minimum fill degree. This leads to a nearly-linear time algorithm\nfor generating approximate greedy minimum degree orderings. Despite extensive\nstudies of algorithms for elimination orderings in combinatorial scientific\ncomputing, our result is the first rigorous incorporation of randomized tools\nin this setting, as well as the first nearly-linear time algorithm for\nproducing elimination orderings with provable approximation guarantees.\n  While our sketching data structure readily works in the oblivious adversary\nmodel, by repeatedly querying and greedily updating itself, it enters the\nadaptive adversarial model where the underlying sketches become prone to\nfailure due to dependency issues with their internal randomness. We show how to\nuse an additional sampling procedure to circumvent this problem and to create\nan independent access sequence. Our technique for decorrelating the interleaved\nqueries and updates to this randomized data structure may be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 21:51:23 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Fahrbach", "Matthew", ""], ["Miller", "Gary L.", ""], ["Peng", "Richard", ""], ["Sawlani", "Saurabh", ""], ["Wang", "Junxing", ""], ["Xu", "Shen Chen", ""]]}, {"id": "1804.04260", "submitter": "Houari Mahfoud", "authors": "Houari Mahfoud", "title": "Graph Pattern Matching Preserving Label-Repetition Constraints", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph pattern matching is a routine process for a wide variety of\napplications such as social network analysis. It is typically defined in terms\nof subgraph isomorphism which is NP-Complete. To lower its complexity, many\nextensions of graph simulation have been proposed which focus on some\ntopological constraints of pattern graphs that can be preserved in\npolynomial-time over data graphs. We discuss in this paper the satisfaction of\na new topological constraint, called Label-Repetition constraint. To the best\nof our knowledge, existing polynomial approaches fail to preserve this\nconstraint, and moreover, one can adopt only subgraph isomorphism for this end\nwhich is cost-prohibitive. We present first a necessary and sufficient\ncondition that a data subgraph must satisfy to preserve the Label-Repetition\nconstraints of the pattern graph. Furthermore, we define matching based on a\nnotion of triple simulation, an extension of graph simulation by considering\nthe new topological constraint. We show that with this extension, graph pattern\nmatching can be performed in polynomial-time, by providing such an algorithm.\nOur algorithm is sub-quadratic in the size of data graphs only, and quartic in\ngeneral. We show that our results can be combined with orthogonal approaches\nfor more expressive graph pattern matching.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 00:04:05 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Mahfoud", "Houari", ""]]}, {"id": "1804.04263", "submitter": "Rayan Chikhi", "authors": "Rayan Chikhi, Alexander Sch\\\"onhuth", "title": "Dualities in Tree Representations", "comments": "CPM 2018, extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A characterization of the tree $T^*$ such that\n$\\mathrm{BP}(T^*)=\\overleftrightarrow{\\mathrm{DFUDS}(T)}$, the reversal of\n$\\mathrm{DFUDS}(T)$ is given. An immediate consequence is a rigorous\ncharacterization of the tree $\\hat{T}$ such that\n$\\mathrm{BP}(\\hat{T})=\\mathrm{DFUDS}(T)$. In summary, $\\mathrm{BP}$ and\n$\\mathrm{DFUDS}$ are unified within an encompassing framework, which might have\nthe potential to imply future simplifications with regard to queries in\n$\\mathrm{BP}$ and/or $\\mathrm{DFUDS}$. Immediate benefits displayed here are to\nidentify so far unnoted commonalities in most recent work on the Range Minimum\nQuery problem, and to provide improvements for the Minimum Length Interval\nQuery problem.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 00:15:17 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 17:12:59 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Chikhi", "Rayan", ""], ["Sch\u00f6nhuth", "Alexander", ""]]}, {"id": "1804.04503", "submitter": "Daniel Alabi", "authors": "Daniel Alabi, Nicole Immorlica, Adam Tauman Kalai", "title": "Unleashing Linear Optimizers for Group-Fair Learning and Optimization", "comments": "Accepted for presentation at the Conference on Learning Theory (COLT)\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most systems and learning algorithms optimize average performance or average\nloss -- one reason being computational complexity. However, many objectives of\npractical interest are more complex than simply average loss. This arises, for\nexample, when balancing performance or loss with fairness across people. We\nprove that, from a computational perspective, optimizing arbitrary objectives\nthat take into account performance over a small number of groups is not\nsignificantly harder to optimize than average performance. Our main result is a\npolynomial-time reduction that uses a linear optimizer to optimize an arbitrary\n(Lipschitz continuous) function of performance over a (constant) number of\npossibly-overlapping groups. This includes fairness objectives over small\nnumbers of groups, and we further point out that other existing notions of\nfairness such as individual fairness can be cast as convex optimization and\nhence more standard convex techniques can be used. Beyond learning, our\napproach applies to multi-objective optimization, more generally.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 02:51:07 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 16:01:44 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Alabi", "Daniel", ""], ["Immorlica", "Nicole", ""], ["Kalai", "Adam Tauman", ""]]}, {"id": "1804.04720", "submitter": "Djamal Belazzougui", "authors": "Djamal Belazzougui and Paolo Boldi and Rasmus Pagh and Sebastiano\n  Vigna", "title": "Fast Prefix Search in Little Space, with Applications", "comments": "Presented at the 18th Annual European Symposium on Algorithms (ESA),\n  Liverpool (UK), September 6-8, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown in the indexing literature that there is an essential\ndifference between prefix/range searches on the one hand, and predecessor/rank\nsearches on the other hand, in that the former provably allows faster query\nresolution. Traditionally, prefix search is solved by data structures that are\nalso dictionaries---they actually contain the strings in $S$. For very large\ncollections stored in slow-access memory, we propose much more compact data\nstructures that support \\emph{weak} prefix searches---they return the ranks of\nmatching strings provided that \\emph{some} string in $S$ starts with the given\nprefix. In fact, we show that our most space-efficient data structure is\nasymptotically space-optimal. Previously, data structures such as String\nB-trees (and more complicated cache-oblivious string data structures) have\nimplicitly supported weak prefix queries, but they all have query time that\ngrows logarithmically with the size of the string collection. In contrast, our\ndata structures are simple, naturally cache-efficient, and have query time that\ndepends only on the length of the prefix, all the way down to constant query\ntime for strings that fit in one machine word. We give several applications of\nweak prefix searches, including exact prefix counting and approximate counting\nof tuples matching conjunctive prefix conditions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 20:35:06 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Belazzougui", "Djamal", ""], ["Boldi", "Paolo", ""], ["Pagh", "Rasmus", ""], ["Vigna", "Sebastiano", ""]]}, {"id": "1804.04739", "submitter": "Ankit Garg", "authors": "Peter B\\\"urgisser and Cole Franks and Ankit Garg and Rafael Oliveira\n  and Michael Walter and Avi Wigderson", "title": "Efficient algorithms for tensor scaling, quantum marginals and moment\n  polytopes", "comments": null, "journal-ref": "2018 IEEE 59th Annual Symposium on Foundations of Computer Science\n  (FOCS), 2018, pp. 883--897", "doi": "10.1109/FOCS.2018.00088", "report-no": null, "categories": "cs.DS cs.CC math-ph math.MP quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a polynomial time algorithm to approximately scale tensors of any\nformat to arbitrary prescribed marginals (whenever possible). This unifies and\ngeneralizes a sequence of past works on matrix, operator and tensor scaling.\nOur algorithm provides an efficient weak membership oracle for the associated\nmoment polytopes, an important family of implicitly-defined convex polytopes\nwith exponentially many facets and a wide range of applications. These include\nthe entanglement polytopes from quantum information theory (in particular, we\nobtain an efficient solution to the notorious one-body quantum marginal\nproblem) and the Kronecker polytopes from representation theory (which capture\nthe asymptotic support of Kronecker coefficients). Our algorithm can be applied\nto succinct descriptions of the input tensor whenever the marginals can be\nefficiently computed, as in the important case of matrix product states or\ntensor-train decompositions, widely used in computational physics and numerical\nmathematics.\n  We strengthen and generalize the alternating minimization approach of\nprevious papers by introducing the theory of highest weight vectors from\nrepresentation theory into the numerical optimization framework. We show that\nhighest weight vectors are natural potential functions for scaling algorithms\nand prove new bounds on their evaluations to obtain polynomial-time\nconvergence. Our techniques are general and we believe that they will be\ninstrumental to obtain efficient algorithms for moment polytopes beyond the\nones consider here, and more broadly, for other optimization problems\npossessing natural symmetries.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 22:21:10 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 02:16:11 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["B\u00fcrgisser", "Peter", ""], ["Franks", "Cole", ""], ["Garg", "Ankit", ""], ["Oliveira", "Rafael", ""], ["Walter", "Michael", ""], ["Wigderson", "Avi", ""]]}, {"id": "1804.04773", "submitter": "Warut Suksompong", "authors": "Warut Suksompong, Charles E. Leiserson, Tao B. Schardl", "title": "On the Efficiency of Localized Work Stealing", "comments": "13 pages, 1 figure", "journal-ref": "Information Processing Letters, 116(2):100-106 (2016)", "doi": "10.1016/j.ipl.2015.10.002", "report-no": null, "categories": "cs.DC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a variant of the work-stealing algorithm that we call\nthe localized work-stealing algorithm. The intuition behind this variant is\nthat because of locality, processors can benefit from working on their own\nwork. Consequently, when a processor is free, it makes a steal attempt to get\nback its own work. We call this type of steal a steal-back. We show that the\nexpected running time of the algorithm is $T_1/P+O(T_\\infty P)$, and that under\nthe \"even distribution of free agents assumption\", the expected running time of\nthe algorithm is $T_1/P+O(T_\\infty\\lg P)$. In addition, we obtain another\nrunning-time bound based on ratios between the sizes of serial tasks in the\ncomputation. If $M$ denotes the maximum ratio between the largest and the\nsmallest serial tasks of a processor after removing a total of $O(P)$ serial\ntasks across all processors from consideration, then the expected running time\nof the algorithm is $T_1/P+O(T_\\infty M)$.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 02:17:57 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Suksompong", "Warut", ""], ["Leiserson", "Charles E.", ""], ["Schardl", "Tao B.", ""]]}, {"id": "1804.04777", "submitter": "Lailong Luo", "authors": "Lailong Luo, Deke Guo, Richard T.B. Ma, Ori Rottenstreich, and Xueshan\n  Luo", "title": "Optimizing Bloom Filter: Challenges, Solutions, and Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bloom filter (BF) has been widely used to support membership query, i.e., to\njudge whether a given element x is a member of a given set S or not. Recent\nyears have seen a flourish design explosion of BF due to its characteristic of\nspace-efficiency and the functionality of constant-time membership query. The\nexisting reviews or surveys mainly focus on the applications of BF, but fall\nshort in covering the current trends, thereby lacking intrinsic understanding\nof their design philosophy. To this end, this survey provides an overview of BF\nand its variants, with an emphasis on the optimization techniques. Basically,\nwe survey the existing variants from two dimensions, i.e., performance and\ngeneralization. To improve the performance, dozens of variants devote\nthemselves to reducing the false positives and implementation costs. Besides,\ntens of variants generalize the BF framework in more scenarios by diversifying\nthe input sets and enriching the output functionalities. To summarize the\nexisting efforts, we conduct an in-depth study of the existing literature on BF\noptimization, covering more than 60 variants. We unearth the design philosophy\nof these variants and elaborate how the employed optimization techniques\nimprove BF. Furthermore, comprehensive analysis and qualitative comparison are\nconducted from the perspectives of BF components. Lastly, we highlight the\nfuture trends of designing BFs. This is, to the best of our knowledge, the\nfirst survey that accomplishes such goals.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 02:57:19 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 01:28:14 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Luo", "Lailong", ""], ["Guo", "Deke", ""], ["Ma", "Richard T. B.", ""], ["Rottenstreich", "Ori", ""], ["Luo", "Xueshan", ""]]}, {"id": "1804.04928", "submitter": "Sebastian Forster", "authors": "Sebastian Forster, Gramoz Goranci", "title": "Dynamic Low-Stretch Trees via Dynamic Low-Diameter Decompositions", "comments": "To be presented at the 51st Annual ACM Symposium on the Theory of\n  Computing (STOC 2019); abstract shortened to respect the arXiv limit of 1920\n  characters", "journal-ref": null, "doi": "10.1145/3313276.3316381", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spanning trees of low average stretch on the non-tree edges, as introduced by\nAlon et al. [SICOMP 1995], are a natural graph-theoretic object. In recent\nyears, they have found significant applications in solvers for symmetric\ndiagonally dominant (SDD) linear systems. In this work, we provide the first\ndynamic algorithm for maintaining such trees under edge insertions and\ndeletions to the input graph. Our algorithm has update time $ n^{1/2 + o(1)} $\nand the average stretch of the maintained tree is $ n^{o(1)} $, which matches\nthe stretch in the seminal result of Alon et al.\n  Similar to Alon et al., our dynamic low-stretch tree algorithm employs a\ndynamic hierarchy of low-diameter decompositions (LDDs). As a major building\nblock we use a dynamic LDD that we obtain by adapting the random-shift\nclustering of Miller et al. [SPAA 2013] to the dynamic setting. The major\ntechnical challenge in our approach is to control the propagation of updates\nwithin our hierarchy of LDDs: each update to one level of the hierarchy could\npotentially induce several insertions and deletions to the next level of the\nhierarchy. We achieve this goal by a sophisticated amortization approach.\n  We believe that the dynamic random-shift clustering might be useful for\nindependent applications. One of these applications is the dynamic spanner\nproblem. By combining the random-shift clustering with the recent spanner\nconstruction of Elkin and Neiman [SODA 2017]. We obtain a fully dynamic\nalgorithm for maintaining a spanner of stretch $ 2k - 1 $ and size $ O (n^{1 +\n1/k} \\log{n}) $ with amortized update time $ O (k \\log^2 n) $ for any integer $\n2 \\leq k \\leq \\log n $. Compared to the state-of-the art in this regime\n[Baswana et al. TALG '12], we improve upon the size of the spanner and the\nupdate time by a factor of $ k $.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 13:12:40 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 12:47:26 GMT"}, {"version": "v3", "created": "Mon, 24 Jun 2019 14:10:52 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Forster", "Sebastian", ""], ["Goranci", "Gramoz", ""]]}, {"id": "1804.05013", "submitter": "Sainyam Galhotra Mr", "authors": "Sainyam Galhotra, Arya Mazumdar, Soumyabrata Pal and Barna Saha", "title": "Connectivity in Random Annulus Graphs and the Geometric Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide new connectivity results for {\\em vertex-random graphs} or {\\em\nrandom annulus graphs} which are significant generalizations of random\ngeometric graphs. Random geometric graphs (RGG) are one of the most basic\nmodels of random graphs for spatial networks proposed by Gilbert in 1961,\nshortly after the introduction of the Erd\\H{o}s-R\\'{en}yi random graphs. They\nresemble social networks in many ways (e.g. by spontaneously creating cluster\nof nodes with high modularity). The connectivity properties of RGG have been\nstudied since its introduction, and analyzing them has been significantly\nharder than their Erd\\H{o}s-R\\'{en}yi counterparts due to correlated edge\nformation.\n  Our next contribution is in using the connectivity of random annulus graphs\nto provide necessary and sufficient conditions for efficient recovery of\ncommunities for {\\em the geometric block model} (GBM). The GBM is a\nprobabilistic model for community detection defined over an RGG in a similar\nspirit as the popular {\\em stochastic block model}, which is defined over an\nErd\\H{o}s-R\\'{en}yi random graph. The geometric block model inherits the\ntransitivity properties of RGGs and thus models communities better than a\nstochastic block model. However, analyzing them requires fresh perspectives as\nall prior tools fail due to correlation in edge formation. We provide a simple\nand efficient algorithm that can recover communities in GBM exactly with high\nprobability in the regime of connectivity.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 16:49:00 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 14:06:34 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 22:12:05 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Galhotra", "Sainyam", ""], ["Mazumdar", "Arya", ""], ["Pal", "Soumyabrata", ""], ["Saha", "Barna", ""]]}, {"id": "1804.05097", "submitter": "Martin Holm Cservenka M.Sc.", "authors": "Martin Holm Cservenka", "title": "Design and Implementation of Dynamic Memory Management in a Reversible\n  Object-Oriented Programming Language", "comments": "Master's Thesis, 231 pages, 63 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reversible object-oriented programming language (ROOPL) was presented in\nlate 2016 and proved that object-oriented programming paradigms works in the\nreversible setting. The language featured simple statically scoped objects\nwhich made non-trivial programs tedious, if not impossible to write using the\nlimited tools provided. We introduce an extension to ROOPL in form the new\nlanguage ROOPL++, featuring dynamic memory management and fixed-sized arrays\nfor increased language expressiveness. The language is a superset of ROOPL and\nhas formally been defined by its language semantics, type system and\ncomputational universality. Considerations for reversible memory manager\nlayouts are discussed and ultimately lead to the selection of the Buddy Memory\nlayout. Translations of the extensions added in ROOPL++ to the reversible\nassembly language PISA are presented to provide garbage-free computations. The\ndynamic memory management extension successfully increases the expressiveness\nof ROOPL and as a result, shows that non-trivial reversible data structures,\nsuch as binary trees and doubly-linked lists, are feasible and do not\ncontradict the reversible computing paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 00:23:21 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Cservenka", "Martin Holm", ""]]}, {"id": "1804.05208", "submitter": "Maxim Buzdalov", "authors": "Ilya Yakupov and Maxim Buzdalov", "title": "On Asynchronous Non-Dominated Sorting for Steady-State Multiobjective\n  Evolutionary Algorithms", "comments": "An extended abstract of this work will appear in proceedings of GECCO\n  2018", "journal-ref": null, "doi": "10.1145/3205651.3205802", "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In parallel and distributed environments, generational evolutionary\nalgorithms often do not exploit the full potential of the computation system\nsince they have to wait until the entire population is evaluated before\nstarting selection procedures. Steady-state algorithms are often seen as a\nsolution to this problem, since fitness evaluation can be done by multiple\nthreads in an asynchronous way. However, if the algorithm updates its state in\na complicated way, the threads will eventually have to wait until this update\nfinishes. State update procedures that are computationally expensive are common\nin multiobjective evolutionary algorithms.\n  We have implemented an asynchronous steady-state version of the NSGA-II\nalgorithm. Its most expensive part, non-dominated sorting, determines the time\nneeded to update the state. We turned the existing incremental non-dominated\nsorting algorithm into an asynchronous one using several concurrency\ntechniques: a single entry-level lock, finer-grained locks working with\nnon-domination levels, and a non-blocking approach using compare-and-set\noperations. Our experimental results reveal the trade-off between the\nwork-efficiency of the algorithm and the achieved amount of parallelism.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 12:03:25 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Yakupov", "Ilya", ""], ["Buzdalov", "Maxim", ""]]}, {"id": "1804.05230", "submitter": "Tselil Schramm", "authors": "Yash Deshpande, Andrea Montanari, Ryan O'Donnell, Tselil Schramm,\n  Subhabrata Sen", "title": "The threshold for SDP-refutation of random regular NAE-3SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike its cousin 3SAT, the NAE-3SAT (not-all-equal-3SAT) problem has the\nproperty that spectral/SDP algorithms can efficiently refute random instances\nwhen the constraint density is a large constant (with high probability). But do\nthese methods work immediately above the \"satisfiability threshold\", or is\nthere still a range of constraint densities for which random NAE-3SAT instances\nare unsatisfiable but hard to refute?\n  We show that the latter situation prevails, at least in the context of random\nregular instances and SDP-based refutation. More precisely, whereas a random\n$d$-regular instance of NAE-3SAT is easily shown to be unsatisfiable (whp) once\n$d \\geq 8$, we establish the following sharp threshold result regarding\nefficient refutation: If $d < 13.5$ then the basic SDP, even augmented with\ntriangle inequalities, fails to refute satisfiability (whp), if $d > 13.5$ then\neven the most basic spectral algorithm refutes satisfiability~(whp).\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 14:43:41 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Deshpande", "Yash", ""], ["Montanari", "Andrea", ""], ["O'Donnell", "Ryan", ""], ["Schramm", "Tselil", ""], ["Sen", "Subhabrata", ""]]}, {"id": "1804.05345", "submitter": "Cenk Baykal", "authors": "Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman,\n  Daniela Rus", "title": "Data-Dependent Coresets for Compressing Neural Networks with\n  Applications to Generalization Bounds", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient coresets-based neural network compression algorithm\nthat sparsifies the parameters of a trained fully-connected neural network in a\nmanner that provably approximates the network's output. Our approach is based\non an importance sampling scheme that judiciously defines a sampling\ndistribution over the neural network parameters, and as a result, retains\nparameters of high importance while discarding redundant ones. We leverage a\nnovel, empirical notion of sensitivity and extend traditional coreset\nconstructions to the application of compressing parameters. Our theoretical\nanalysis establishes guarantees on the size and accuracy of the resulting\ncompressed network and gives rise to generalization bounds that may provide new\ninsights into the generalization properties of neural networks. We demonstrate\nthe practical effectiveness of our algorithm on a variety of neural network\nconfigurations and real-world data sets.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 12:22:23 GMT"}, {"version": "v2", "created": "Thu, 10 May 2018 05:52:18 GMT"}, {"version": "v3", "created": "Fri, 18 May 2018 14:41:51 GMT"}, {"version": "v4", "created": "Wed, 5 Sep 2018 18:01:41 GMT"}, {"version": "v5", "created": "Wed, 20 Feb 2019 18:23:51 GMT"}, {"version": "v6", "created": "Sat, 18 May 2019 00:12:21 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Baykal", "Cenk", ""], ["Liebenwein", "Lucas", ""], ["Gilitschenski", "Igor", ""], ["Feldman", "Dan", ""], ["Rus", "Daniela", ""]]}, {"id": "1804.05379", "submitter": "Alina Ene", "authors": "Alina Ene, Huy L. Nguyen", "title": "Submodular Maximization with Nearly-optimal Approximation and Adaptivity\n  in Nearly-linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the tradeoff between the approximation guarantee and\nadaptivity for the problem of maximizing a monotone submodular function subject\nto a cardinality constraint. The adaptivity of an algorithm is the number of\nsequential rounds of queries it makes to the evaluation oracle of the function,\nwhere in every round the algorithm is allowed to make polynomially-many\nparallel queries. Adaptivity is an important consideration in settings where\nthe objective function is estimated using samples and in applications where\nadaptivity is the main running time bottleneck. Previous algorithms achieving a\nnearly-optimal $1 - 1/e - \\epsilon$ approximation require $\\Omega(n)$ rounds of\nadaptivity. In this work, we give the first algorithm that achieves a $1 - 1/e\n- \\epsilon$ approximation using $O(\\ln{n} / \\epsilon^2)$ rounds of adaptivity.\nThe number of function evaluations and additional running time of the algorithm\nare $O(n \\mathrm{poly}(\\log{n}, 1/\\epsilon))$.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 16:41:59 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 18:27:24 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "1804.05420", "submitter": "Ali Dasdan", "authors": "Ali Dasdan", "title": "A Weighted Generalization of the Graham-Diaconis Inequality for Ranked\n  List Similarity", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Graham-Diaconis inequality shows the equivalence between two well-known\nmethods of measuring the similarity of two given ranked lists of items:\nSpearman's footrule and Kendall's tau. The original inequality assumes\nunweighted items in input lists. In this paper, we first define versions of\nthese methods for weighted items. We then prove a generalization of the\ninequality for the weighted versions.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 20:15:15 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Dasdan", "Ali", ""]]}, {"id": "1804.05436", "submitter": "Yihong Wu", "authors": "Vivek Bagaria, Jian Ding, David Tse, Yihong Wu, Jiaming Xu", "title": "Hidden Hamiltonian Cycle Recovery via Linear Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of hidden Hamiltonian cycle recovery, where there is\nan unknown Hamiltonian cycle in an $n$-vertex complete graph that needs to be\ninferred from noisy edge measurements. The measurements are independent and\ndistributed according to $\\calP_n$ for edges in the cycle and $\\calQ_n$\notherwise. This formulation is motivated by a problem in genome assembly, where\nthe goal is to order a set of contigs (genome subsequences) according to their\npositions on the genome using long-range linking measurements between the\ncontigs. Computing the maximum likelihood estimate in this model reduces to a\nTraveling Salesman Problem (TSP). Despite the NP-hardness of TSP, we show that\na simple linear programming (LP) relaxation, namely the fractional $2$-factor\n(F2F) LP, recovers the hidden Hamiltonian cycle with high probability as $n \\to\n\\infty$ provided that $\\alpha_n - \\log n \\to \\infty$, where $\\alpha_n\n\\triangleq -2 \\log \\int \\sqrt{d P_n d Q_n}$ is the R\\'enyi divergence of order\n$\\frac{1}{2}$. This condition is information-theoretically optimal in the sense\nthat, under mild distributional assumptions, $\\alpha_n \\geq (1+o(1)) \\log n$ is\nnecessary for any algorithm to succeed regardless of the computational cost.\n  Departing from the usual proof techniques based on dual witness construction,\nthe analysis relies on the combinatorial characterization (in particular, the\nhalf-integrality) of the extreme points of the F2F polytope. Represented as\nbicolored multi-graphs, these extreme points are further decomposed into\nsimpler \"blossom-type\" structures for the large deviation analysis and counting\narguments. Evaluation of the algorithm on real data shows improvements over\nexisting approaches.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 21:58:02 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Bagaria", "Vivek", ""], ["Ding", "Jian", ""], ["Tse", "David", ""], ["Wu", "Yihong", ""], ["Xu", "Jiaming", ""]]}, {"id": "1804.05441", "submitter": "Udit Agarwal", "authors": "Udit Agarwal, Vijaya Ramachandran, Valerie King, Matteo Pontecorvi", "title": "A Deterministic Distributed Algorithm for Exact Weighted All-Pairs\n  Shortest Paths in $\\tilde{O}(n^{3/2})$ Rounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic distributed algorithm to compute all-pairs\nshortest paths(APSP) in an edge-weighted directed or undirected graph. Our\nalgorithm runs in $\\tilde{O}(n^{3/2})$ rounds in the Congest model, where $n$\nis the number of nodes in the graph. This is the first $o(n^2)$ rounds\ndeterministic distributed algorithm for the weighted APSP problem. Our\nalgorithm is fairly simple and incorporates a deterministic distributed\nalgorithm we develop for computing a `blocker set' \\cite{King99}, which has\nbeen used earlier in sequential dynamic computation of APSP.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 22:17:54 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Agarwal", "Udit", ""], ["Ramachandran", "Vijaya", ""], ["King", "Valerie", ""], ["Pontecorvi", "Matteo", ""]]}, {"id": "1804.05615", "submitter": "Samuel McCauley", "authors": "Samuel McCauley and Francesco Silvestri", "title": "Adaptive MapReduce Similarity Joins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity joins are a fundamental database operation. Given data sets S and\nR, the goal of a similarity join is to find all points x in S and y in R with\ndistance at most r. Recent research has investigated how locality-sensitive\nhashing (LSH) can be used for similarity join, and in particular two recent\nlines of work have made exciting progress on LSH-based join performance. Hu,\nTao, and Yi (PODS 17) investigated joins in a massively parallel setting,\nshowing strong results that adapt to the size of the output. Meanwhile, Ahle,\nAum\\\"uller, and Pagh (SODA 17) showed a sequential algorithm that adapts to the\nstructure of the data, matching classic bounds in the worst case but improving\nthem significantly on more structured data. We show that this adaptive strategy\ncan be adapted to the parallel setting, combining the advantages of these\napproaches. In particular, we show that a simple modification to Hu et al.'s\nalgorithm achieves bounds that depend on the density of points in the dataset\nas well as the total outsize of the output. Our algorithm uses no extra\nparameters over other LSH approaches (in particular, its execution does not\ndepend on the structure of the dataset), and is likely to be efficient in\npractice.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 11:35:32 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["McCauley", "Samuel", ""], ["Silvestri", "Francesco", ""]]}, {"id": "1804.05644", "submitter": "Andreas Paraskevopoulos", "authors": "Kalliopi Giannakopoulou, Andreas Paraskevopoulos and Christos\n  Zaroliagis", "title": "Multimodal Dynamic Journey Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present multimodal DTM, a new model for multimodal journey planning in\npublic (schedule-based) transport networks. Multimodal DTM constitutes an\nextension of the dynamic timetable model (DTM), developed originally for\nunimodal journey planning. Multimodal DTM exhibits a very fast query algorithm,\nmeeting the request for real-time response to best journey queries and an\nextremely fast update algorithm for updating the timetable information in case\nof delays. In particular, an experimental study on real-world metropolitan\nnetworks demonstrates that our methods compare favorably with other\nstate-of-the-art approaches when public transport along with unrestricted\nw.r.t. departing time traveling (walking and electric vehicles) is considered.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 12:58:24 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Giannakopoulou", "Kalliopi", ""], ["Paraskevopoulos", "Andreas", ""], ["Zaroliagis", "Christos", ""]]}, {"id": "1804.05776", "submitter": "Kuan Cheng", "authors": "Kuan Cheng, Zhengzhong Jin, Xin Li, Ke Wu", "title": "Deterministic Document Exchange Protocols, and Almost Optimal Binary\n  Codes for Edit Errors", "comments": "34 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two basic problems regarding edit error, i.e. document exchange and\nerror correcting codes for edit errors (insdel codes). For message length $n$\nand edit error upper bound $k$, it is known that in both problems the optimal\nsketch size or the optimal number of redundant bits is $\\Theta(k \\log\n\\frac{n}{k})$. However, known constructions are far from achieving these\nbounds.\n  We significantly improve previous results on both problems. For document\nexchange, we give an efficient deterministic protocol with sketch size\n$O(k\\log^2 \\frac{n}{k})$. This significantly improves the previous best known\ndeterministic protocol, which has sketch size $O(k^2 + k \\log^2 n)$\n(Belazzougui15). For binary insdel codes, we obtain the following results:\n  1. An explicit binary insdel code which encodes an $n$-bit message $x$\nagainst $k$ errors with redundancy $O(k \\log^2 \\frac{n}{k})$. In particular\nthis implies an explicit family of binary insdel codes that can correct\n$\\varepsilon$ fraction of insertions and deletions with rate $1-O(\\varepsilon\n\\log^2 (\\frac{1}{\\varepsilon}))=1-\\widetilde{O}(\\varepsilon)$.\n  2. An explicit binary insdel code which encodes an $n$-bit message $x$\nagainst $k$ errors with redundancy $O(k \\log n)$. This is the first explicit\nconstruction of binary insdel codes that has optimal redundancy for a wide\nrange of error parameters $k$, and this brings our understanding of binary\ninsdel codes much closer to that of standard binary error correcting codes.\n  In obtaining our results we introduce the notion of \\emph{$\\varepsilon$-self\nmatching hash functions} and \\emph{$\\varepsilon$-synchronization hash\nfunctions}. We believe our techniques can have further applications in the\nliterature.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 16:24:13 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 00:44:21 GMT"}, {"version": "v3", "created": "Fri, 20 Apr 2018 03:54:48 GMT"}, {"version": "v4", "created": "Tue, 17 Jul 2018 04:06:55 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Cheng", "Kuan", ""], ["Jin", "Zhengzhong", ""], ["Li", "Xin", ""], ["Wu", "Ke", ""]]}, {"id": "1804.05813", "submitter": "Walter Didimo", "authors": "Walter Didimo and Giuseppe Liotta and Maurizio Patrignani", "title": "Bend-minimum Orthogonal Drawings in Quadratic Time", "comments": "Appears in the Proceedings of the 26th International Symposium on\n  Graph Drawing and Network Visualization (GD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a planar $3$-graph (i.e., a planar graph with vertex degree at\nmost three) with $n$ vertices. We present the first $O(n^2)$-time algorithm\nthat computes a planar orthogonal drawing of $G$ with the minimum number of\nbends in the variable embedding setting. If either a distinguished edge or a\ndistinguished vertex of $G$ is constrained to be on the external face, a\nbend-minimum orthogonal drawing of $G$ that respects this constraint can be\ncomputed in $O(n)$ time. Different from previous approaches, our algorithm does\nnot use minimum cost flow models and computes drawings where every edge has at\nmost two bends.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 17:31:20 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 08:08:03 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 07:52:17 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Didimo", "Walter", ""], ["Liotta", "Giuseppe", ""], ["Patrignani", "Maurizio", ""]]}, {"id": "1804.05956", "submitter": "Hemant Malik", "authors": "Hemant Malik and Ovidiu Daescu", "title": "k-Maximum Subarrays for Small k: Divide-and-Conquer made simpler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given an array A of n real numbers, the maximum subarray problem is to find a\ncontiguous subarray which has the largest sum. The k-maximum subarrays problem\nis to find k such subarrays with the largest sums. For the 1-maximum subarray\nthe well known divide-and-conquer algorithm, presented in most textbooks,\nalthough suboptimal, is easy to implement and can be made optimal with a simple\nchange that speeds up the combine phase. On the other hand, the only known\ndivide-and-conquer algorithm for k > 1, that is efficient for small values of\nk, is difficult to implement, due to the intricacies of the combine phase. In\nthis paper we give a divide- and-conquer solution for the k-maximum subarray\nproblem that simplifies the combine phase considerably while preserving the\noverall running time.\n  In the process of designing the combine phase of the algorithm we provide a\nsimple, sublinear, O($k^{1/2} log^3 k$) time algorithm, for finding the k\nlargest sums of X + Y, where X and Y are sorted arrays of size n and $k <=\nn^2$. The k largest sums are implicitly represented, and can be enumerated with\nan additional O(k) time. To our knowledge, this is the first sublinear time\nalgorithm for this well studied problem.\n  Unlike previous solutions, that are fairly complicated and sometimes\ndifficult to implement, ours rely on simple operations such as merging sorted\narrays, binary search, and selecting the $k^{th}$ smallest number in an array.\nWe have implemented our algorithms and report excellent performance on test\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 21:54:54 GMT"}, {"version": "v2", "created": "Sun, 22 Jul 2018 15:13:39 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Malik", "Hemant", ""], ["Daescu", "Ovidiu", ""]]}, {"id": "1804.06231", "submitter": "James Willis", "authors": "James S. Willis, Matthieu Schaller, Pedro Gonnet, Richard G. Bower,\n  Peter W. Draper", "title": "An Efficient SIMD Implementation of Pseudo-Verlet Lists for Neighbour\n  Interactions in Particle-Based Codes", "comments": "10 pages, 3 figures. Proceedings of the ParCo 2017 conference,\n  Bologna, Italy, September 12-15th, 2017", "journal-ref": "Advances in Parallel Computing, Volume 32: Parallel Computing is\n  Everywhere (2018)", "doi": "10.3233/978-1-61499-843-3-507", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In particle-based simulations, neighbour finding (i.e finding pairs of\nparticles to interact within a given range) is the most time consuming part of\nthe computation. One of the best such algorithms, which can be used for both\nMolecular Dynamics (MD) and Smoothed Particle Hydrodynamics (SPH) simulations,\nis the pseudo-Verlet list algorithm. This algorithm, however, does not\nvectorise trivially, and hence makes it difficult to exploit SIMD-parallel\narchitectures. In this paper, we present several novel modifications as well as\na vectorisation strategy for the algorithm which lead to overall speed-ups over\nthe scalar version of the algorithm of 2.24x for the AVX instruction set (SIMD\nwidth of 8), 2.43x for AVX2, and 4.07x for AVX-512 (SIMD width of 16).\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 13:31:28 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Willis", "James S.", ""], ["Schaller", "Matthieu", ""], ["Gonnet", "Pedro", ""], ["Bower", "Richard G.", ""], ["Draper", "Peter W.", ""]]}, {"id": "1804.06355", "submitter": "Eric Balkanski", "authors": "Eric Balkanski, Aviad Rubinstein, Yaron Singer", "title": "An Exponential Speedup in Parallel Running Time for Submodular\n  Maximization without Loss in Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the adaptivity of submodular maximization. Adaptivity\nquantifies the number of sequential rounds that an algorithm makes when\nfunction evaluations can be executed in parallel. Adaptivity is a fundamental\nconcept that is heavily studied across a variety of areas in computer science,\nlargely due to the need for parallelizing computation. For the canonical\nproblem of maximizing a monotone submodular function under a cardinality\nconstraint, it is well known that a simple greedy algorithm achieves a $1-1/e$\napproximation and that this approximation is optimal for polynomial-time\nalgorithms. Somewhat surprisingly, despite extensive efforts on submodular\noptimization for large-scale datasets, until very recently there was no known\nalgorithm that achieves a constant factor approximation for this problem whose\nadaptivity is sublinear in the size of the ground set $n$.\n  Recent work by Balkanski and Singer describes an algorithm that obtains an\napproximation arbitrarily close to $1/3$ in $\\mathcal{O}(\\log n)$ adaptive\nrounds and shows that no algorithm can obtain a constant factor approximation\nin $\\tilde{o}(\\log n)$ adaptive rounds. This approach achieves an exponential\nspeedup in adaptivity (and parallel running time) at the expense of\napproximation quality.\n  In this paper we describe a novel approach that yields an algorithm whose\napproximation is arbitrarily close to the optimal $1-1/e$ guarantee in\n$\\mathcal{O}(\\log n)$ adaptive rounds. This algorithm therefore achieves an\nexponential speedup in parallel running time for submodular maximization at the\nexpense of an arbitrarily small loss in approximation quality. This guarantee\nis optimal in both approximation and adaptivity, up to lower order terms.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 16:28:26 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Balkanski", "Eric", ""], ["Rubinstein", "Aviad", ""], ["Singer", "Yaron", ""]]}, {"id": "1804.06361", "submitter": "L\\'aszl\\'o Kozma", "authors": "Andr\\'e Berger, L\\'aszl\\'o Kozma, Matthias Mnich, Roland Vincze", "title": "A time- and space-optimal algorithm for the many-visits TSP", "comments": "Small fixes, journal version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The many-visits traveling salesperson problem (MV-TSP) asks for an optimal\ntour of $n$ cities that visits each city $c$ a prescribed number $k_c$ of\ntimes. Travel costs may be asymmetric, and visiting a city twice in a row may\nincur a non-zero cost. The MV-TSP problem finds applications in scheduling,\ngeometric approximation, and Hamiltonicity of certain graph families.\n  The fastest known algorithm for MV-TSP is due to Cosmadakis and Papadimitriou\n(SICOMP, 1984). It runs in time $n^{O(n)} + O(n^3 \\log \\sum_c k_c )$ and\nrequires $n^{\\Theta(n)}$ space. An interesting feature of the\nCosmadakis-Papadimitriou algorithm is its \\emph{logarithmic} dependence on the\ntotal length $\\sum_c k_c$ of the tour, allowing the algorithm to handle\ninstances with very long tours. The \\emph{superexponential} dependence on the\nnumber of cities in both the time and space complexity, however, renders the\nalgorithm impractical for all but the narrowest range of this parameter.\n  In this paper we improve upon the Cosmadakis-Papadimitriou algorithm, giving\nan MV-TSP algorithm that runs in time $2^{O(n)}$, i.e.\\\n\\emph{single-exponential} in the number of cities, using \\emph{polynomial}\nspace. Our algorithm is deterministic, and arguably both simpler and easier to\nanalyse than the original approach of Cosmadakis and Papadimitriou. It involves\nan optimization over directed spanning trees and a recursive, centroid-based\ndecomposition of trees.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 16:35:38 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 13:08:11 GMT"}, {"version": "v3", "created": "Sat, 15 Dec 2018 09:33:32 GMT"}, {"version": "v4", "created": "Tue, 21 Apr 2020 12:47:28 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Berger", "Andr\u00e9", ""], ["Kozma", "L\u00e1szl\u00f3", ""], ["Mnich", "Matthias", ""], ["Vincze", "Roland", ""]]}, {"id": "1804.06428", "submitter": "Hung Le", "authors": "Hung Le and Baigong Zheng", "title": "Local Search is a PTAS for Feedback Vertex Set in Minor-free Graphs", "comments": "12 page 1 figure, major revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a simple local search gives a PTAS for the Feedback Vertex Set\n(FVS) problem in minor-free graphs. An efficient PTAS in minor-free graphs was\nknown for this problem by Fomin, Lokshtanov, Raman and Sauraubh. However, their\nalgorithm is a combination of many advanced algorithmic tools such as\ncontraction decomposition framework introduced by Demaine and Hajiaghayi,\nCourcelle's theorem and the Robertson and Seymour decomposition. In stark\ncontrast, our local search algorithm is very simple and easy to implement. It\nkeeps exchanging a constant number of vertices to improve the current solution\nuntil a local optimum is reached. Our main contribution is to show that the\nlocal optimum only differs the global optimum by $(1+\\epsilon)$ factor.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 18:39:33 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 06:28:35 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Le", "Hung", ""], ["Zheng", "Baigong", ""]]}, {"id": "1804.06515", "submitter": "David Eppstein", "authors": "David Eppstein", "title": "Faster Evaluation of Subtraction Games", "comments": "12 pages, 4 figures. To appear in the Proceedings of the 9th\n  International Conference on Fun with Algorithms (FUN 2018), Leibniz\n  International Proceedings in Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.NT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Subtraction games are played with one or more heaps of tokens, with players\ntaking turns removing from a single heap a number of tokens belonging to a\nspecified subtraction set; the last player to move wins. We describe how to\ncompute the set of winning heap sizes in single-heap subtraction games (for an\ninput consisting of the subtraction set and maximum heap size $n$), in time\n$\\tilde O(n)$, where the $\\tilde O$ elides logarithmic factors. For multi-heap\ngames, the optimal game play is determined by the nim-value of each heap; we\ndescribe how to compute the nim-values of all heaps of size up to~$n$ in time\n$\\tilde O(mn)$, where $m$ is the maximum nim-value occurring among these heap\nsizes. These time bounds improve naive dynamic programming algorithms with time\n$O(n|S|)$, because $m\\le|S|$ for all such games. We apply these results to the\ngame of subtract-a-square, whose set of winning positions is a maximal\nsquare-difference-free set of a type studied in number theory in connection\nwith the Furstenberg-S\\'ark\\\"ozy theorem. We provide experimental evidence\nthat, for this game, the set of winning positions has a density comparable to\nthat of the densest known square-difference-free sets, and has a modular\nstructure related to the known constructions for these dense sets.\nAdditionally, this game's nim-values are (experimentally) significantly smaller\nthan the size of its subtraction set, implying that our algorithm achieves a\npolynomial speedup over dynamic programming.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 00:39:09 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Eppstein", "David", ""]]}, {"id": "1804.06540", "submitter": "Liren Shan", "authors": "Liren Shan, Yuhao Yi, Zhongzhi Zhang", "title": "Improving information centrality of a node in complex networks by adding\n  edges", "comments": "7 pages, 2 figures, ijcai-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of increasing the centrality of a network node arises in many\npractical applications. In this paper, we study the optimization problem of\nmaximizing the information centrality $I_v$ of a given node $v$ in a network\nwith $n$ nodes and $m$ edges, by creating $k$ new edges incident to $v$. Since\n$I_v$ is the reciprocal of the sum of resistance distance $\\mathcal{R}_v$\nbetween $v$ and all nodes, we alternatively consider the problem of minimizing\n$\\mathcal{R}_v$ by adding $k$ new edges linked to $v$. We show that the\nobjective function is monotone and supermodular. We provide a simple greedy\nalgorithm with an approximation factor $\\left(1-\\frac{1}{e}\\right)$ and\n$O(n^3)$ running time. To speed up the computation, we also present an\nalgorithm to compute $\\left(1-\\frac{1}{e}-\\epsilon\\right)$-approximate\nresistance distance $\\mathcal{R}_v$ after iteratively adding $k$ edges, the\nrunning time of which is $\\widetilde{O} (mk\\epsilon^{-2})$ for any\n$\\epsilon>0$, where the $\\widetilde{O} (\\cdot)$ notation suppresses the ${\\rm\npoly} (\\log n)$ factors. We experimentally demonstrate the effectiveness and\nefficiency of our proposed algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 03:28:26 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Shan", "Liren", ""], ["Yi", "Yuhao", ""], ["Zhang", "Zhongzhi", ""]]}, {"id": "1804.06601", "submitter": "Toshio Suzuki", "authors": "Mika Shigemizu, Toshio Suzuki, Koki Usami", "title": "Independent Distributions on a Multi-Branching AND-OR Tree of Height 2", "comments": "12 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate an AND-OR tree T and a probability distribution d on the truth\nassignments to the leaves. Tarsi (1983) showed that if d is an independent and\nidentical distribution (IID) such that probability of a leaf having value 0 is\nneither 0 nor 1 then, under a certain assumptions, there exists an optimal\nalgorithm that is depth-first. We investigate the case where d is an\nindependent distribution (ID) and probability depends on each leaf. It is known\nthat in this general case, if height is greater than or equal to 3, Tarsi-type\nresult does not hold. It is also known that for a complete binary tree of\nheight 2, Tarsi-type result certainly holds. In this paper, we ask whether\nTarsi-type result holds for an AND-OR tree of height 2. Here, a child node of\nthe root is either an OR-gate or a leaf: The number of child nodes of an\ninternal node is arbitrary, and depends on an internal node. We give an\naffirmative answer. Our strategy of the proof is to reduce the problem to the\ncase of directional algorithms. We perform induction on the number of leaves,\nand modify Tarsi's method to suite height 2 trees. We discuss why our proof\ndoes not apply to height 3 trees.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 08:30:59 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Shigemizu", "Mika", ""], ["Suzuki", "Toshio", ""], ["Usami", "Koki", ""]]}, {"id": "1804.06675", "submitter": "Janosch Fuchs", "authors": "Hans-Joachim B\\\"ockenhauer and Janosch Fuchs and Walter Unger", "title": "The Graph Exploration Problem with Advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving an autonomous agent through an unknown environment is one of the\ncrucial problems for robotics and network analysis. Therefore, it received a\nlot of attention in the last decades and was analyzed in many different\nsettings. The graph exploration problem is a theoretical and abstract model,\nwhere an algorithm has to decide how the agent, also called explorer, moves\nthrough a network such that every point of interest is visited at least once.\nFor its decisions, the knowledge of the algorithm is limited by the perception\nof the explorer.\n  There are different models regarding the perception of the explorer. We look\nat the fixed graph scenario proposed by Kalyanasundaram and Pruhs (Proc. of\nICALP, 1993), where the explorer starts at a vertex of the network and sees all\nreachable vertices, their unique names and their distance from the current\nposition. Therefore, the algorithm recognizes already seen vertices and can\nadapt its strategy during exploring, because it does not forget anything.\n  Because the algorithm only learns the structure of the graph during\ncomputation, it cannot deterministically compute an optimal tour that visits\nevery vertex at least once without prior knowledge. Therefore, we are\ninterested in the amount of crucial a-priori information needed to solve the\nproblem optimally, which we measure in terms of the well-studied model of\nadvice complexity. [..]\n  We look at different variations of the graph exploration problem and\ndistinguish between directed or undirected edges, cyclic or non-cyclic\nsolutions, unit costs or individual costs for the edges and different amounts\nof a-priori structural knowledge of the explorer. [..] In this work, we present\nalgorithms with an advice complexity of $\\mathcal{O}(m+n)$, thus improving the\nclassical bound for sparse graphs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 12:20:55 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["B\u00f6ckenhauer", "Hans-Joachim", ""], ["Fuchs", "Janosch", ""], ["Unger", "Walter", ""]]}, {"id": "1804.06809", "submitter": "Szymon Grabowski", "authors": "Szymon Grabowski, Tomasz Kociumaka, Jakub Radoszewski", "title": "On Abelian Longest Common Factor with and without RLE", "comments": "Submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Abelian longest common factor problem in two scenarios: when\ninput strings are uncompressed and are of size $n$, and when the input strings\nare run-length encoded and their compressed representations have size at most\n$m$. The alphabet size is denoted by $\\sigma$. For the uncompressed problem, we\nshow an $o(n^2)$-time and $\\Oh(n)$-space algorithm in the case of\n$\\sigma=\\Oh(1)$, making a non-trivial use of tabulation. For the RLE-compressed\nproblem, we show two algorithms: one working in $\\Oh(m^2\\sigma^2 \\log^3 m)$\ntime and $\\Oh(m (\\sigma^2+\\log^2 m))$ space, which employs line sweep, and one\nthat works in $\\Oh(m^3)$ time and $\\Oh(m)$ space that applies in a careful way\na sliding-window-based approach. The latter improves upon the previously known\n$\\Oh(nm^2)$-time and $\\Oh(m^4)$-time algorithms that were recently developed by\nSugimoto et al.\\ (IWOCA 2017) and Grabowski (SPIRE 2017), respectively.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 16:45:46 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Grabowski", "Szymon", ""], ["Kociumaka", "Tomasz", ""], ["Radoszewski", "Jakub", ""]]}, {"id": "1804.06932", "submitter": "Yuancheng Yu", "authors": "Lijie Chen, Erik D. Demaine, Yuzhou Gu, Virginia Vassilevska Williams,\n  Yinzhan Xu, Yuancheng Yu", "title": "Nearly Optimal Separation Between Partially And Fully Retroactive Data\n  Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the introduction of retroactive data structures at SODA 2004, a major\nunsolved problem has been to bound the gap between the best partially\nretroactive data structure (where changes can be made to the past, but only the\npresent can be queried) and the best fully retroactive data structure (where\nthe past can also be queried) for any problem. It was proved in 2004 that any\npartially retroactive data structure with operation time $T(n,m)$ can be\ntransformed into a fully retroactive data structure with operation time\n$O(\\sqrt{m} \\cdot T(n,m))$, where $n$ is the size of the data structure and $m$\nis the number of operations in the timeline [Demaine 2004], but it has been\nopen for 14 years whether such a gap is necessary.\n  In this paper, we prove nearly matching upper and lower bounds on this gap\nfor all $n$ and $m$. We improve the upper bound for $n \\ll \\sqrt m$ by showing\na new transformation with multiplicative overhead $n \\log m$. We then prove a\nlower bound of $\\min\\{n \\log m, \\sqrt m\\}^{1-o(1)}$ assuming any of the\nfollowing conjectures:\n  - Conjecture I: Circuit SAT requires $2^{n - o(n)}$ time on $n$-input\ncircuits of size $2^{o(n)}$. (Far weaker than the well-believed SETH\nconjecture, which asserts that CNF SAT with $n$ variables and $O(n)$ clauses\nalready requires $2^{n-o(n)}$ time.)\n  - Conjecture II: Online $(\\min,+)$ product between an integer $n\\times n$\nmatrix and $n$ vectors requires $n^{3 - o(1)}$ time.\n  - Conjecture III (3-SUM Conjecture): Given three sets $A,B,C$ of integers,\neach of size $n$, deciding whether there exist $a \\in A, b \\in B, c \\in C$ such\nthat $a + b + c = 0$ requires $n^{2 - o(1)}$ time.\n  Our lower bound construction illustrates an interesting power of fully\nretroactive queries: they can be used to quickly solve batched pair evaluation.\nWe believe this technique can prove useful for other data structure lower\nbounds, especially dynamic ones.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 22:09:57 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 03:12:34 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Chen", "Lijie", ""], ["Demaine", "Erik D.", ""], ["Gu", "Yuzhou", ""], ["Williams", "Virginia Vassilevska", ""], ["Xu", "Yinzhan", ""], ["Yu", "Yuancheng", ""]]}, {"id": "1804.06952", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya and Cl\\'ement L. Canonne and Himanshu Tyagi", "title": "Distributed Simulation and Distributed Inference", "comments": "This work is superseded by the more recent \"Inference under\n  Information Constraints II: Communication Constraints and Shared Randomness\"\n  (arXiv:1905.08302), by the same authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent samples from an unknown probability distribution $\\bf p$ on a\ndomain of size $k$ are distributed across $n$ players, with each player holding\none sample. Each player can communicate $\\ell$ bits to a central referee in a\nsimultaneous message passing model of communication to help the referee infer a\nproperty of the unknown $\\bf p$. What is the least number of players for\ninference required in the communication-starved setting of $\\ell<\\log k$? We\nbegin by exploring a general \"simulate-and-infer\" strategy for such inference\nproblems where the center simulates the desired number of samples from the\nunknown distribution and applies standard inference algorithms for the\ncollocated setting. Our first result shows that for $\\ell<\\log k$ perfect\nsimulation of even a single sample is not possible. Nonetheless, we present a\nLas Vegas algorithm that simulates a single sample from the unknown\ndistribution using $O(k/2^\\ell)$ samples in expectation. As an immediate\ncorollary, we get that simulate-and-infer attains the optimal sample complexity\nof $\\Theta(k^2/2^\\ell\\epsilon^2)$ for learning the unknown distribution to\ntotal variation distance $\\epsilon$. For the prototypical testing problem of\nidentity testing, simulate-and-infer works with $O(k^{3/2}/2^\\ell\\epsilon^2)$\nsamples, a requirement that seems to be inherent for all communication\nprotocols not using any additional resources. Interestingly, we can break this\nbarrier using public coins. Specifically, we exhibit a public-coin\ncommunication protocol that performs identity testing using\n$O(k/\\sqrt{2^\\ell}\\epsilon^2)$ samples. Furthermore, we show that this is\noptimal up to constant factors. Our theoretically sample-optimal protocol is\neasy to implement in practice. Our proof of lower bound entails showing a\ncontraction in $\\chi^2$ distance of product distributions due to communication\nconstraints and may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 00:34:15 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2018 23:17:06 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 05:27:10 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "1804.07031", "submitter": "Alexander Svozil", "authors": "Krishnendu Chatterjee, Wolfgang Dvo\\v{r}\\'ak, Monika Henzinger and\n  Alexander Svozil", "title": "Algorithms and Conditional Lower Bounds for Planning Problems", "comments": "Accepted at ICAPS'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider planning problems for graphs, Markov decision processes (MDPs),\nand games on graphs. While graphs represent the most basic planning model, MDPs\nrepresent interaction with nature and games on graphs represent interaction\nwith an adversarial environment. We consider two planning problems where there\nare k different target sets, and the problems are as follows: (a) the coverage\nproblem asks whether there is a plan for each individual target set, and (b)\nthe sequential target reachability problem asks whether the targets can be\nreached in sequence. For the coverage problem, we present a linear-time\nalgorithm for graphs and quadratic conditional lower bound for MDPs and games\non graphs. For the sequential target problem, we present a linear-time\nalgorithm for graphs, a sub-quadratic algorithm for MDPs, and a quadratic\nconditional lower bound for games on graphs. Our results with conditional lower\nbounds establish (i) model-separation results showing that for the coverage\nproblem MDPs and games on graphs are harder than graphs and for the sequential\nreachability problem games on graphs are harder than MDPs and graphs; (ii)\nobjective-separation results showing that for MDPs the coverage problem is\nharder than the sequential target problem.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 08:12:50 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Dvo\u0159\u00e1k", "Wolfgang", ""], ["Henzinger", "Monika", ""], ["Svozil", "Alexander", ""]]}, {"id": "1804.07143", "submitter": "Tilo Wiedera", "authors": "Markus Chimani, Ivo Hedtke, Tilo Wiedera", "title": "Exact Algorithms for the Maximum Planar Subgraph Problem: New Models and\n  Experiments", "comments": "SEA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a graph $G$, the NP-hard Maximum Planar Subgraph problem asks for a\nplanar subgraph of $G$ with the maximum number of edges. The only known\nnon-trivial exact algorithm utilizes Kuratowski's famous planarity criterion\nand can be formulated as an integer linear program (ILP) or a pseudo-boolean\nsatisfiability problem (PBS). We examine three alternative characterizations of\nplanarity regarding their applicability to model maximum planar subgraphs. For\neach, we consider both ILP and PBS variants, investigate diverse formulation\naspects, and evaluate their practical performance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 13:19:11 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Chimani", "Markus", ""], ["Hedtke", "Ivo", ""], ["Wiedera", "Tilo", ""]]}, {"id": "1804.07431", "submitter": "Fan Wei", "authors": "Jacob Fox, Tim Roughgarden, C. Seshadhri, Fan Wei, Nicole Wein", "title": "Finding Cliques in Social Networks: A New Distribution-Free Model", "comments": "main text 13 pages; 2 figures; appendix 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new distribution-free model of social networks. Our definitions\nare motivated by one of the most universal signatures of social networks,\ntriadic closure---the property that pairs of vertices with common neighbors\ntend to be adjacent. Our most basic definition is that of a \"$c$-closed\" graph,\nwhere for every pair of vertices $u,v$ with at least $c$ common neighbors, $u$\nand $v$ are adjacent. We study the classic problem of enumerating all maximal\ncliques, an important task in social network analysis. We prove that this\nproblem is fixed-parameter tractable with respect to $c$ on $c$-closed graphs.\nOur results carry over to \"weakly $c$-closed graphs\", which only require a\nvertex deletion ordering that avoids pairs of non-adjacent vertices with $c$\ncommon neighbors. Numerical experiments show that well-studied social networks\ntend to be weakly $c$-closed for modest values of $c$.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 02:37:31 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Fox", "Jacob", ""], ["Roughgarden", "Tim", ""], ["Seshadhri", "C.", ""], ["Wei", "Fan", ""], ["Wein", "Nicole", ""]]}, {"id": "1804.07456", "submitter": "Arnold Filtser", "authors": "Arnold Filtser and Ofer Neiman", "title": "Light Spanners for High Dimensional Norms via Stochastic Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spanners for low dimensional spaces (e.g. Euclidean space of constant\ndimension, or doubling metrics) are well understood. This lies in contrast to\nthe situation in high dimensional spaces, where except for the work of\nHar-Peled, Indyk and Sidiropoulos (SODA 2013), who showed that any $n$-point\nEuclidean metric has an $O(t)$-spanner with $\\tilde{O}(n^{1+1/t^2})$ edges,\nlittle is known.\n  In this paper we study several aspects of spanners in high dimensional normed\nspaces. First, we build spanners for finite subsets of $\\ell_p$ with $1<p\\le\n2$. Second, our construction yields a spanner which is both sparse and also\n{\\em light}, i.e., its total weight is not much larger than that of the minimum\nspanning tree. In particular, we show that any $n$-point subset of $\\ell_p$ for\n$1<p\\le 2$ has an $O(t)$-spanner with $n^{1+\\tilde{O}(1/t^p)}$ edges and\nlightness $n^{\\tilde{O}(1/t^p)}$.\n  In fact, our results are more general, and they apply to any metric space\nadmitting a certain low diameter stochastic decomposition. It is known that\narbitrary metric spaces have an $O(t)$-spanner with lightness $O(n^{1/t})$. We\nexhibit the following tradeoff: metrics with decomposability parameter\n$\\nu=\\nu(t)$ admit an $O(t)$-spanner with lightness $\\tilde{O}(\\nu^{1/t})$. For\nexample, $n$-point Euclidean metrics have $\\nu\\le n^{1/t}$, metrics with\ndoubling constant $\\lambda$ have $\\nu\\le\\lambda$, and graphs of genus $g$ have\n$\\nu\\le g$. While these families do admit a ($1+\\epsilon$)-spanner, its\nlightness depend exponentially on the dimension (resp. $\\log g$). Our\nconstruction alleviates this exponential dependency, at the cost of incurring\nlarger stretch.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 06:01:57 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Filtser", "Arnold", ""], ["Neiman", "Ofer", ""]]}, {"id": "1804.07458", "submitter": "Xiaowei Wu", "authors": "Zhiyi Huang, Zhihao Gavin Tang, Xiaowei Wu and Yuhao Zhang", "title": "Online Vertex-Weighted Bipartite Matching: Beating 1-1/e with Random\n  Arrivals", "comments": "15 pages, 2 figures, appeared in ICALP 2018 and TALG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a weighted version of the ranking algorithm by Karp et al. (STOC\n1990), and prove a competitive ratio of 0.6534 for the vertex-weighted online\nbipartite matching problem when online vertices arrive in random order. Our\nresult shows that random arrivals help beating the 1-1/e barrier even in the\nvertex-weighted case. We build on the randomized primal-dual framework by\nDevanur et al. (SODA 2013) and design a two dimensional gain sharing function,\nwhich depends not only on the rank of the offline vertex, but also on the\narrival time of the online vertex. To our knowledge, this is the first\ncompetitive ratio strictly larger than 1-1/e for an online bipartite matching\nproblem achieved under the randomized primal-dual framework. Our algorithm has\na natural interpretation that offline vertices offer a larger portion of their\nweights to the online vertices as time goes by, and each online vertex matches\nthe neighbor with the highest offer at its arrival.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 06:17:17 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 19:56:49 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Huang", "Zhiyi", ""], ["Tang", "Zhihao Gavin", ""], ["Wu", "Xiaowei", ""], ["Zhang", "Yuhao", ""]]}, {"id": "1804.07496", "submitter": "Andre L\\\"offler", "authors": "Moritz Beck, Johannes Blum, Myroslav Kryven, Andre L\\\"offler, Johannes\n  Zink", "title": "Planar Steiner Orientation is NP-complete", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in graph theory are motivated by routing or flow problems.\nAmong these problems is Steiner Orientation: given a mixed graph G (having\ndirected and undirected edges) and a set T of k terminal pairs in G, is there\nan orientation of the undirected edges in G such that there is a directed path\nfor every terminal pair in T ? This problem was shown to be NP -complete by\nArkin and Hassin [1] and later W [1]-hard by Pilipczuk and Wahlstr\\\"om [7],\nparametrized by k. On the other hand, there is an XP algorithm by Cygan et al.\n[3] and a polynomial time algorithm for graphs without directed edges by Hassin\nand Megiddo [5]. Chitnis and Feldmann [2] showed W [1]-hardness of the problem\nfor graphs of genus 1. We consider a further restriction to planar graphs and\nshow NP -completeness.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 08:57:39 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Beck", "Moritz", ""], ["Blum", "Johannes", ""], ["Kryven", "Myroslav", ""], ["L\u00f6ffler", "Andre", ""], ["Zink", "Johannes", ""]]}, {"id": "1804.07575", "submitter": "Stefano Leucci", "authors": "Barbara Geissmann, Stefano Leucci, Chih-Hung Liu, Paolo Penna", "title": "Optimal Sorting with Persistent Comparison Errors", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sorting $n$ elements in the case of\n\\emph{persistent} comparison errors. In this model (Braverman and Mossel,\nSODA'08), each comparison between two elements can be wrong with some fixed\n(small) probability $p$, and \\emph{comparisons cannot be repeated}. Sorting\nperfectly in this model is impossible, and the objective is to minimize the\n\\emph{dislocation} of each element in the output sequence, that is, the\ndifference between its true rank and its position. Existing lower bounds for\nthis problem show that no algorithm can guarantee, with high probability,\n\\emph{maximum dislocation} and \\emph{total dislocation} better than\n$\\Omega(\\log n)$ and $\\Omega(n)$, respectively, regardless of its running time.\n  In this paper, we present the first \\emph{$O(n\\log n)$-time} sorting\nalgorithm that guarantees both \\emph{$O(\\log n)$ maximum dislocation} and\n\\emph{$O(n)$ total dislocation} with high probability. Besides improving over\nthe previous state-of-the art algorithms -- the best known algorithm had\nrunning time $\\tilde{O}(n^{3/2})$ -- our result indicates that comparison\nerrors do not make the problem computationally more difficult: a sequence with\nthe best possible dislocation can be obtained in $O(n\\log n)$ time and, even\nwithout comparison errors, $\\Omega(n\\log n)$ time is necessary to guarantee\nsuch dislocation bounds.\n  In order to achieve this optimal result, we solve two sub-problems, and the\nrespective methods have their own merits for further application. One is how to\nlocate a position in which to insert an element in an almost-sorted sequence\nhaving $O(\\log n)$ maximum dislocation in such a way that the dislocation of\nthe resulting sequence will still be $O(\\log n)$. The other is how to\nsimultaneously insert $m$ elements into an almost sorted sequence of $m$\ndifferent elements, such that the resulting sequence of $2m$ elements remains\nalmost sorted.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 12:33:42 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Geissmann", "Barbara", ""], ["Leucci", "Stefano", ""], ["Liu", "Chih-Hung", ""], ["Penna", "Paolo", ""]]}, {"id": "1804.07842", "submitter": "Pasin Manurangsi", "authors": "Eden Chlamt\\'a\\v{c}, Pasin Manurangsi", "title": "Sherali-Adams Integrality Gaps Matching the Log-Density Threshold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The log-density method is a powerful algorithmic framework which in recent\nyears has given rise to the best-known approximations for a variety of\nproblems, including Densest-$k$-Subgraph and Bipartite Small Set Vertex\nExpansion. These approximations have been conjectured to be optimal based on\nvarious instantiations of a general conjecture: that it is hard to distinguish\na fully random combinatorial structure from one which contains a similar\nplanted sub-structure with the same \"log-density\".\n  We bolster this conjecture by showing that in a random hypergraph with edge\nprobability $n^{-\\alpha}$, $\\tilde\\Omega(\\log n)$ rounds of Sherali-Adams with\ncannot rule out the existence of a $k$-subhypergraph with edge density\n$k^{-\\alpha-o(1)}$, for any $k$ and $\\alpha$. This holds even when the bound on\nthe objective function is lifted. This gives strong integrality gaps which\nexactly match the gap in the above distinguishing problems, as well as the\nbest-known approximations, for Densest $k$-Subgraph, Smallest $p$-Edge\nSubgraph, their hypergraph extensions, and Small Set Bipartite Vertex Expansion\n(or equivalently, Minimum $p$-Union). Previously, such integrality gaps were\nknown only for Densest $k$-Subgraph for one specific parameter setting.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 22:01:30 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Chlamt\u00e1\u010d", "Eden", ""], ["Manurangsi", "Pasin", ""]]}, {"id": "1804.07869", "submitter": "Baigong Zheng", "authors": "Glencora Borradaile, Hung Le and Baigong Zheng", "title": "Designing Practical PTASes for Minimum Feedback Vertex Set in Planar\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two algorithms for the minimum feedback vertex set problem in\nplanar graphs: an $O(n \\log n)$ PTAS using a linear kernel and balanced\nseparator, and a heuristic algorithm using kernelization and local search. We\nimplemented these algorithms and compared their performance with Becker and\nGeiger's 2-approximation algorithm. We observe that while our PTAS is\ncompetitive with the 2-approximation algorithm on large planar graphs, its\nrunning time is much longer. And our heuristic algorithm can outperform the\n2-approximation algorithm on most large planar graphs and provide a trade-off\nbetween running time and solution quality, i.e. a \"PTAS behavior\".\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 00:27:28 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Borradaile", "Glencora", ""], ["Le", "Hung", ""], ["Zheng", "Baigong", ""]]}, {"id": "1804.07901", "submitter": "Sixue Liu", "authors": "S. Cliff Liu", "title": "Chain, Generalization of Covering Code, and Deterministic Algorithm for\n  k-SAT", "comments": "In the 45th International Colloquium on Automata, Languages, and\n  Programming (ICALP 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the current fastest deterministic algorithm for $k$-SAT, improving\nthe upper bound $(2-2/k)^{n + o(n)}$ dues to Moser and Scheder [STOC'11]. The\nalgorithm combines a branching algorithm with the derandomized local search,\nwhose analysis relies on a special sequence of clauses called chain, and a\ngeneralization of covering code based on linear programming. We also provide a\nmore ingenious branching algorithm for $3$-SAT to establish the upper bound\n$1.32793^n$, improved from $1.3303^n$.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 06:31:00 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 03:24:14 GMT"}, {"version": "v3", "created": "Wed, 3 Oct 2018 04:56:13 GMT"}, {"version": "v4", "created": "Mon, 5 Nov 2018 05:12:49 GMT"}, {"version": "v5", "created": "Tue, 22 Jan 2019 02:54:26 GMT"}, {"version": "v6", "created": "Thu, 24 Jan 2019 15:08:25 GMT"}, {"version": "v7", "created": "Tue, 17 Mar 2020 20:03:12 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Liu", "S. Cliff", ""]]}, {"id": "1804.07975", "submitter": "Michael Lampis", "authors": "Michael Lampis", "title": "Finer Tight Bounds for Coloring on Clique-Width", "comments": "To appear in ICALP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the complexity of the classical $k$-Coloring problem parameterized\nby clique-width. This is a very well-studied problem that becomes highly\nintractable when the number of colors $k$ is large. However, much less is known\non its complexity for small, concrete values of $k$. In this paper, we\ncompletely determine the complexity of $k$-Coloring parameterized by\nclique-width for any fixed $k$, under the SETH. Specifically, we show that for\nall $k\\ge 3,\\epsilon>0$, $k$-Coloring cannot be solved in time\n$O^*((2^k-2-\\epsilon)^{cw})$, and give an algorithm running in time\n$O^*((2^k-2)^{cw})$. Thus, if the SETH is true, $2^k-2$ is the \"correct\" base\nof the exponent for every $k$.\n  Along the way, we also consider the complexity of $k$-Coloring parameterized\nby the related parameter modular treewidth ($mtw$). In this case we show that\nthe \"correct\" running time, under the SETH, is $O^*({k\\choose \\lfloor\nk/2\\rfloor}^{mtw})$. If we base our results on a weaker assumption (the ETH),\nthey imply that $k$-Coloring cannot be solved in time $n^{o(cw)}$, even on\ninstances with $O(\\log n)$ colors.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 14:41:14 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Lampis", "Michael", ""]]}, {"id": "1804.08001", "submitter": "Uri Stemmer", "authors": "Haim Kaplan, Uri Stemmer", "title": "Differentially Private k-Means with Constant Multiplicative Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design new differentially private algorithms for the Euclidean k-means\nproblem, both in the centralized model and in the local model of differential\nprivacy. In both models, our algorithms achieve significantly improved error\nguarantees than the previous state-of-the-art. In addition, in the local model,\nour algorithm significantly reduces the number of interaction rounds.\n  Although the problem has been widely studied in the context of differential\nprivacy, all of the existing constructions achieve only super constant\napproximation factors. We present, for the first time, efficient private\nalgorithms for the problem with constant multiplicative error. Furthermore, we\nshow how to modify our algorithms so they compute private corsets for k-means\nclustering in both models.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 17:41:04 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 16:08:54 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Kaplan", "Haim", ""], ["Stemmer", "Uri", ""]]}, {"id": "1804.08016", "submitter": "Alex Pothen", "authors": "Florin Dobrian, Mahantesh Halappanavar, Alex Pothen and Ahmed Al-Herz", "title": "A 2/3-Approximation Algorithm for Vertex-weighted Matching in Bipartite\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the maximum vertex-weighted matching problem (MVM), in which\nnon-negative weights are assigned to the vertices of a graph, the weight of a\nmatching is the sum of the weights of the matched vertices, and we are required\nto compute a matching of maximum weight. We describe an exact algorithm for MVM\nwith $O(|V|\\, |E|)$ time complexity, and then we design a $2/3$-approximation\nalgorithm for MVM on bipartite graphs by restricting the length of augmenting\npaths to at most three. The latter algorithm has time complexity $O(|E| + |V|\n\\log |V|)$.\n  The approximation algorithm solves two MVM problems on bipartite graphs, each\nwith weights only on one vertex part, and then finds a matching from these two\nmatchings using the Mendelsohn-Dulmage Theorem. The approximation ratio of the\nalgorithm is obtained by considering failed vertices, i.e., vertices that the\napproximation algorithm fails to match but the exact algorithm does. We show\nthat at every step of the algorithm there are two distinct heavier vertices\nthat we can charge each failed vertex to.\n  We have implemented the $2/3$-approximation algorithm for MVM and compare it\nwith four other algorithms: an exact MEM algorithm, the exact MVM algorithm, a\n$1/2$-approximation algorithm for MVM, and a scaling-based\n$(1-\\epsilon)$-approximation algorithm for MEM. We also show that MVM problems\nshould not be first transformed to MEM problems and solved using exact\nalgorithms for the latter, since this transformation can increase runtimes by\nseveral orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 19:40:55 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 15:02:28 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Dobrian", "Florin", ""], ["Halappanavar", "Mahantesh", ""], ["Pothen", "Alex", ""], ["Al-Herz", "Ahmed", ""]]}, {"id": "1804.08062", "submitter": "Karthik Abinav Sankararaman", "authors": "Brian Brubach and Karthik Abinav Sankararaman and Aravind Srinivasan\n  and Pan Xu", "title": "Attenuate Locally, Win Globally: An Attenuation-based Framework for\n  Online Stochastic Matching with Timeouts", "comments": "A short version appeared in AAMAS-2017. This version fixes some bugs\n  in the camera-ready version of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online matching problems have garnered significant attention in recent years\ndue to numerous applications in e-commerce, online advertisements,\nride-sharing, etc. Many of them capture the uncertainty in the real world by\nincluding stochasticity in both the arrival process and the matching process.\nThe Online Stochastic Matching with Timeouts problem introduced by Bansal, et\nal., (Algorithmica, 2012) models matching markets (e.g., E-Bay, Amazon). Buyers\narrive from an independent and identically distributed (i.i.d.) known\ndistribution on buyer profiles and can be shown a list of items one at a time.\nEach buyer has some probability of purchasing each item and a limit (timeout)\non the number of items they can be shown.\n  Bansal et al., (Algorithmica, 2012) gave a 0.12-competitive algorithm which\nwas improved by Adamczyk, et al., (ESA, 2015) to 0.24. We present an online\nattenuation framework that uses an algorithm for offline stochastic matching as\na black box. On the upper bound side, we show that this framework, combined\nwith a black-box adapted from Bansal et al., (Algorithmica, 2012), yields an\nonline algorithm which nearly doubles the ratio to 0.46. On the lower bound\nside, we show that no algorithm can achieve a ratio better than 0.632 using the\nstandard LP for this problem. This framework has a high potential for further\nimprovements since new algorithms for offline stochastic matching can directly\nimprove the ratio for the online problem.\n  Our online framework also has the potential for a variety of extensions. For\nexample, we introduce a natural generalization: Online Stochastic Matching with\nTwo-sided Timeouts in which both online and offline vertices have timeouts. Our\nframework provides the first algorithm for this problem achieving a ratio of\n0.30. We once again use the algorithm of Adamczyk et al., (ESA, 2015) as a\nblack-box and plug-it into our framework.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 03:20:56 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 16:16:21 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Brubach", "Brian", ""], ["Sankararaman", "Karthik Abinav", ""], ["Srinivasan", "Aravind", ""], ["Xu", "Pan", ""]]}, {"id": "1804.08097", "submitter": "Marcin Bienkowski", "authors": "Marcin Bienkowski and Artur Kraska and Hsiang-Hsuan Liu and Pawe{\\l}\n  Schmidt", "title": "A Primal-Dual Online Deterministic Algorithm for Matching with Delays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Min-cost Perfect Matching with Delays (MPMD) problem, 2 m requests\narrive over time at points of a metric space. An online algorithm has to\nconnect these requests in pairs, but a decision to match may be postponed till\na more suitable matching pair is found. The goal is to minimize the joint cost\nof connection and the total waiting time of all requests.\n  We present an O(m)-competitive deterministic algorithm for this problem,\nimproving on an existing bound of O(m^(log(5.5))) = O(m^2.46). Our algorithm\nalso solves (with the same competitive ratio) a bipartite variant of MPMD,\nwhere requests are either positive or negative and only requests with different\npolarities may be matched with each other. Unlike the existing randomized\nsolutions, our approach does not depend on the size of the metric space and\ndoes not have to know it in advance.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 11:11:31 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 15:15:04 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Bienkowski", "Marcin", ""], ["Kraska", "Artur", ""], ["Liu", "Hsiang-Hsuan", ""], ["Schmidt", "Pawe\u0142", ""]]}, {"id": "1804.08111", "submitter": "Andreas Galanis", "authors": "Antonio Blanca, Andreas Galanis, Leslie Ann Goldberg, Daniel\n  Stefankovic, Eric Vigoda, Kuan Yang", "title": "Sampling in Uniqueness from the Potts and Random-Cluster Models on\n  Random Regular Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sampling from the Potts model on random regular\ngraphs. It is conjectured that sampling is possible when the temperature of the\nmodel is in the uniqueness regime of the regular tree, but positive algorithmic\nresults have been for the most part elusive. In this paper, for all integers\n$q\\geq 3$ and $\\Delta\\geq 3$, we develop algorithms that produce samples within\nerror $o(1)$ from the $q$-state Potts model on random $\\Delta$-regular graphs,\nwhenever the temperature is in uniqueness, for both the ferromagnetic and\nantiferromagnetic cases.\n  The algorithm for the antiferromagnetic Potts model is based on iteratively\nadding the edges of the graph and resampling a bichromatic class that contains\nthe endpoints of the newly added edge. Key to the algorithm is how to perform\nthe resampling step efficiently since bichromatic classes may induce\nlinear-sized components. To this end, we exploit the tree uniqueness to show\nthat the average growth of bichromatic components is typically small, which\nallows us to use correlation decay algorithms for the resampling step. While\nthe precise uniqueness threshold on the tree is not known for general values of\n$q$ and $\\Delta$ in the antiferromagnetic case, our algorithm works throughout\nuniqueness regardless of its value.\n  In the case of the ferromagnetic Potts model, we simplify the algorithm\nsignificantly by utilising the random-cluster representation of the model. In\nparticular, we show that a percolation-type algorithm succeeds in sampling from\nthe random-cluster model with parameters $p,q$ on random $\\Delta$-regular\ngraphs for all values of $q\\geq 1$ and $p<p_c(q,\\Delta)$, where $p_c(q,\\Delta)$\ncorresponds to a uniqueness threshold for the model on the $\\Delta$-regular\ntree. When restricted to integer values of $q$, this yields a simplified\nalgorithm for the ferromagnetic Potts model on random $\\Delta$-regular graphs.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 13:11:24 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2019 12:21:58 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Blanca", "Antonio", ""], ["Galanis", "Andreas", ""], ["Goldberg", "Leslie Ann", ""], ["Stefankovic", "Daniel", ""], ["Vigoda", "Eric", ""], ["Yang", "Kuan", ""]]}, {"id": "1804.08172", "submitter": "Marco Molinaro", "authors": "Anupam Gupta, Ruta Mehta, Marco Molinaro", "title": "Maximizing Profit with Convex Costs in the Random-order Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose a set of requests arrives online: each request gives some value $v_i$\nif accepted, but requires using some amount of each of $d$ resources. Our cost\nis a convex function of the vector of total utilization of these $d$ resources.\nWhich requests should be accept to maximize our profit, i.e., the sum of values\nof the accepted demands, minus the convex cost?\n  We consider this problem in the random-order a.k.a. secretary model, and show\nan $O(d)$-competitive algorithm for the case where the convex cost function is\nalso supermodular. If the set of accepted demands must also be independent in a\ngiven matroid, we give an $O(d^3 \\alpha)$-competitive algorithm for the\nsupermodular case, and an improved $O(d^2\\alpha)$ if the convex cost function\nis also separable. Here $\\alpha$ is the competitive ratio of the best algorithm\nfor the submodular secretary problem. These extend and improve previous results\nknown for this problem. Our techniques are simple but use powerful ideas from\nconvex duality, which give clean interpretations of existing work, and allow us\nto give the extensions and improvements.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 21:07:54 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Gupta", "Anupam", ""], ["Mehta", "Ruta", ""], ["Molinaro", "Marco", ""]]}, {"id": "1804.08178", "submitter": "Wenxin Li", "authors": "Wenxin Li", "title": "Nearly Linear Time Deterministic Algorithms for Submodular Maximization\n  Under Knapsack Constraint and Beyond", "comments": "The cardinality constraint result and lower bound in v5 are included\n  in a separate paper arXiv:2006.09327", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the classic submodular maximization problem under\nknapsack constraints and beyond. We first present an\n$(7/16-\\varepsilon)$-approximate algorithm for single knapsack constraint,\nwhich requires $O(n\\cdot\\max\\{\\varepsilon^{-1},\\log\\log n\\})$ queries, and two\npasses in the streaming setting. This provides improvements in approximation\nratio, query complexity and number of passes on the stream. We next show that\nthere exists an $(1/2-\\varepsilon)$-approximate deterministic algorithm for\nconstant number of binary packing constraints, which achieves a query\ncomplexity of $O_{\\varepsilon}(n\\cdot\\log \\log n)$. One salient feature of our\ndeterministic algorithm is, both its approximation ratio and time complexity\nare independent of the number of constraints. Lastly we present nearly linear\ntime algorithms for the intersection of $p$-system and $d$ knapsack constraint,\nwe achieve approximation ratio of $(1/(p+\\frac{7}{4}d+1)-\\varepsilon)$ for\nmonotone objective and $(p/(p+1)(2p+\\frac{7}{4}d+1)-\\varepsilon)$ for\nnon-monotone objective.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 21:27:53 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 01:25:19 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 22:16:10 GMT"}, {"version": "v4", "created": "Mon, 15 Jun 2020 05:27:46 GMT"}, {"version": "v5", "created": "Wed, 15 Jul 2020 17:43:55 GMT"}, {"version": "v6", "created": "Mon, 21 Dec 2020 09:24:49 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Li", "Wenxin", ""]]}, {"id": "1804.08236", "submitter": "Yota Otachi", "authors": "R\\'emy Belmonte, Mehdi Khosravian Ghadikolaei, Masashi Kiyomi, Michael\n  Lampis, Yota Otachi", "title": "How Bad is the Freedom to Flood-It?", "comments": "19 pages, 6 figures, FUN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fixed-Flood-It and Free-Flood-It are combinatorial problems on graphs that\ngeneralize a very popular puzzle called Flood-It. Both problems consist of\nrecoloring moves whose goal is to produce a monochromatic (\"flooded\") graph as\nquickly as possible. Their difference is that in Free-Flood-It the player has\nthe additional freedom of choosing the vertex to play in each move. In this\npaper, we investigate how this freedom affects the complexity of the problem.\nIt turns out that the freedom is bad in some sense. We show that some cases\ntrivially solvable for Fixed-Flood-It become intractable for Free-Flood-It. We\nalso show that some tractable cases for Fixed-Flood-It are still tractable for\nFree-Flood-It but need considerably more involved arguments. We finally present\nsome combinatorial properties connecting or separating the two problems. In\nparticular, we show that the length of an optimal solution for Fixed-Flood-It\nis always at most twice that of Free-Flood-It, and this is tight.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 03:21:10 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Belmonte", "R\u00e9my", ""], ["Ghadikolaei", "Mehdi Khosravian", ""], ["Kiyomi", "Masashi", ""], ["Lampis", "Michael", ""], ["Otachi", "Yota", ""]]}, {"id": "1804.08285", "submitter": "Taku Onodera", "authors": "Taku Onodera and Tetsuo Shibuya", "title": "Succinct Oblivious RAM", "comments": "21 pages. A preliminary version of this paper appeared in STACS'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing the database space overhead is critical in big-data processing. In\nthis paper, we revisit oblivious RAM (ORAM) using big-data standard for the\ndatabase space overhead.\n  ORAM is a cryptographic primitive that enables users to perform arbitrary\ndatabase accesses without revealing the access pattern to the server. It is\nparticularly important today since cloud services become increasingly common\nmaking it necessary to protect users' private information from database access\npattern analyses. Previous ORAM studies focused mostly on reducing the access\noverhead. Consequently, the access overhead of the state-of-the-art ORAM\nconstructions is almost at practical levels in certain application scenarios\nsuch as secure processors. On the other hand, most existing ORAM constructions\nrequire $(1+\\Theta(1))n$ (say, $10n$) bits of server space where $n$ is the\ndatabase size. Though such space complexity is often considered to be\n\"optimal\", overhead such as $10 \\times$ is prohibitive for big-data\napplications in practice.\n  We propose ORAM constructions that take only $(1+o(1))n$ bits of server space\nwhile maintaining state-of-the-art performance in terms of the access overhead\nand the user space. We also give non-asymptotic analyses and simulation results\nwhich indicate that the proposed ORAM constructions are practically effective.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 08:44:22 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Onodera", "Taku", ""], ["Shibuya", "Tetsuo", ""]]}, {"id": "1804.08317", "submitter": "Abhinav Srivastav", "authors": "Giorgio Lucarelli, Benjamin Moseley, Nguyen Kim Thang, Abhinav\n  Srivastav and Denis Trystram", "title": "Online Non-Preemptive Scheduling to Minimize Weighted Flow-time on\n  Unrelated Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the online problem of scheduling independent jobs\n\\emph{non-preemptively} so as to minimize the weighted flow-time on a set of\nunrelated machines. There has been a considerable amount of work on this\nproblem in the preemptive setting where several competitive algorithms are\nknown in the classical competitive model. %Using the speed augmentation model,\nAnand et al. showed that the greedy algorithm is\n$O\\left(\\frac{1}{\\epsilon}\\right)$-competitive in the preemptive setting. In\nthe non-preemptive setting, Lucarelli et al. showed that there exists a strong\nlower bound for minimizing weighted flow-time even on a single machine.\nHowever, the problem in the non-preemptive setting admits a strong lower bound.\nRecently, Lucarelli et al. presented an algorithm that achieves a\n$O\\left(\\frac{1}{\\epsilon^2}\\right)$-competitive ratio when the algorithm is\nallowed to reject $\\epsilon$-fraction of total weight of jobs and\n$\\epsilon$-speed augmentation. They further showed that speed augmentation\nalone is insufficient to derive any competitive algorithm. An intriguing open\nquestion is whether there exists a scalable competitive algorithm that rejects\na small fraction of total weights.\n  In this paper, we affirmatively answer this question. Specifically, we show\nthat there exists a $O\\left(\\frac{1}{\\epsilon^3}\\right)$-competitive algorithm\nfor minimizing weighted flow-time on a set of unrelated machine that rejects at\nmost $O(\\epsilon)$-fraction of total weight of jobs. The design and analysis of\nthe algorithm is based on the primal-dual technique. Our result asserts that\nalternative models beyond speed augmentation should be explored when designing\nonline schedulers in the non-preemptive setting in an effort to find provably\ngood algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 10:04:05 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Lucarelli", "Giorgio", ""], ["Moseley", "Benjamin", ""], ["Thang", "Nguyen Kim", ""], ["Srivastav", "Abhinav", ""], ["Trystram", "Denis", ""]]}, {"id": "1804.08487", "submitter": "Manuel Penschuck", "authors": "Corrie Jacobien Carstens, Michael Hamann, Ulrich Meyer, Manuel\n  Penschuck, Hung Tran, Dorothea Wagner", "title": "Parallel and I/O-efficient Randomisation of Massive Networks using\n  Global Curveball Trades", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph randomisation is a crucial task in the analysis and synthesis of\nnetworks. It is typically implemented as an edge switching process (ESMC)\nrepeatedly swapping the nodes of random edge pairs while maintaining the\ndegrees involved. Curveball is a novel approach that instead considers the\nwhole neighbourhoods of randomly drawn node pairs. Its Markov chain converges\nto a uniform distribution, and experiments suggest that it requires less steps\nthan the established ESMC.\n  Since trades however are more expensive, we study Curveball's practical\nruntime by introducing the first efficient Curveball algorithms: the\nI/O-efficient EM-CB for simple undirected graphs and its internal memory\npendant IM-CB. Further, we investigate global trades processing every node in a\ngraph during a single super step, and show that undirected global trades\nconverge to a uniform distribution and perform superior in practice. We then\ndiscuss EM-GCB and EM-PGCB for global trades and give experimental evidence\nthat EM-PGCB achieves the quality of the state-of-the-art ESMC algorithm EM-ES\nnearly one order of magnitude faster.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 15:06:07 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 11:11:29 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Carstens", "Corrie Jacobien", ""], ["Hamann", "Michael", ""], ["Meyer", "Ulrich", ""], ["Penschuck", "Manuel", ""], ["Tran", "Hung", ""], ["Wagner", "Dorothea", ""]]}, {"id": "1804.08547", "submitter": "Micha{\\l} Ga\\'nczorz", "authors": "Micha{\\l} Ga\\'nczorz", "title": "Entropy bounds for grammar compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grammar compression represents a string as a context free grammar. Achieving\ncompression requires encoding such grammar as a binary string; there are a few\ncommonly used encodings. We bound the size of practically used encodings for\nseveral heuristical compression methods, including \\RePair and \\Greedy\nalgorithms: the standard encoding of \\RePair, which combines entropy coding and\nspecial encoding of a grammar, achieves $1.5|S|H_k(S)$, where $H_k(S)$ is\n$k$-th order entropy of $S$. We also show that by stopping after some iteration\nwe can achieve $|S|H_k(S)$. This is particularly interesting, as it explains a\nphenomenon observed in practice: introducing too many nonterminals causes the\nbit-size to grow. We generalize our approach to other compression methods like\n\\Greedy and a wide class of irreducible grammars as well as to other\npractically used bit encodings (including naive, which uses fixed-length\ncodes). Our approach not only proves the bounds but also partially explains why\n\\Greedy and \\RePair are much better in practice than other grammar based\nmethods. In some cases we argue that our estimations are optimal. The tools\nused in our analysis are of independent interest: we prove the new, optimal,\nbounds on the zeroth order entropy of parsing of a string.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 16:38:10 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 09:47:07 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Ga\u0144czorz", "Micha\u0142", ""]]}, {"id": "1804.08548", "submitter": "Christopher Musco", "authors": "Frederik Mallmann-Trenn, Cameron Musco, and Christopher Musco", "title": "Eigenvector Computation and Community Detection in Asynchronous Gossip\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple distributed algorithm for computing adjacency matrix\neigenvectors for the communication graph in an asynchronous gossip model. We\nshow how to use this algorithm to give state-of-the-art asynchronous community\ndetection algorithms when the communication graph is drawn from the\nwell-studied stochastic block model. Our methods also apply to a natural\nalternative model of randomized communication, where nodes within a community\ncommunicate more frequently than nodes in different communities. Our analysis\nsimplifies and generalizes prior work by forging a connection between\nasynchronous eigenvector computation and Oja's algorithm for streaming\nprincipal component analysis. We hope that our work serves as a starting point\nfor building further connections between the analysis of stochastic iterative\nmethods, like Oja's algorithm, and work on asynchronous and gossip-type\nalgorithms for distributed computation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 16:39:39 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 00:16:00 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Mallmann-Trenn", "Frederik", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""]]}, {"id": "1804.08603", "submitter": "Pranjal Awasthi", "authors": "Pranjal Awasthi and Aravindan Vijayaraghavan", "title": "Towards Learning Sparsely Used Dictionaries with Arbitrary Supports", "comments": "72 pages, fixed minor typos, and added a new reference in related\n  work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning is a popular approach for inferring a hidden basis or\ndictionary in which data has a sparse representation. Data generated from the\ndictionary A (an n by m matrix, with m > n in the over-complete setting) is\ngiven by Y = AX where X is a matrix whose columns have supports chosen from a\ndistribution over k-sparse vectors, and the non-zero values chosen from a\nsymmetric distribution. Given Y, the goal is to recover A and X in polynomial\ntime. Existing algorithms give polytime guarantees for recovering incoherent\ndictionaries, under strong distributional assumptions both on the supports of\nthe columns of X, and on the values of the non-zero entries. In this work, we\nstudy the following question: Can we design efficient algorithms for recovering\ndictionaries when the supports of the columns of X are arbitrary?\n  To address this question while circumventing the issue of\nnon-identifiability, we study a natural semirandom model for dictionary\nlearning where there are a large number of samples $y=Ax$ with arbitrary\nk-sparse supports for x, along with a few samples where the sparse supports are\nchosen uniformly at random. While the few samples with random supports ensures\nidentifiability, the support distribution can look almost arbitrary in\naggregate. Hence existing algorithmic techniques seem to break down as they\nmake strong assumptions on the supports.\n  Our main contribution is a new polynomial time algorithm for learning\nincoherent over-complete dictionaries that works under the semirandom model.\nAdditionally the same algorithm provides polynomial time guarantees in new\nparameter regimes when the supports are fully random. Finally using these\ntechniques, we also identify a minimal set of conditions on the supports under\nwhich the dictionary can be (information theoretically) recovered from\npolynomial samples for almost linear sparsity, i.e., $k=\\tilde{O}(n)$.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 17:57:33 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 15:27:40 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1804.08645", "submitter": "David Durfee Mr", "authors": "Rachel Cummings and David Durfee", "title": "Individual Sensitivity Preprocessing for Data Privacy", "comments": "Abbreviated abstract to accommodate character restrictions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sensitivity metric in differential privacy, which is informally defined\nas the largest marginal change in output between neighboring databases, is of\nsubstantial significance in determining the accuracy of private data analyses.\nTechniques for improving accuracy when the average sensitivity is much smaller\nthan the worst-case sensitivity have been developed within the differential\nprivacy literature, including tools such as smooth sensitivity,\nSample-and-Aggregate, Propose-Test-Release, and Lipschitz extensions.\n  In this work, we provide a new and general Sensitivity-Preprocessing\nframework for reducing sensitivity, where efficient application gives\nstate-of-the-art accuracy for privately outputting the important statistical\nmetrics median and mean when no underlying assumptions are made about the\ndatabase. In particular, our framework compares favorably to smooth sensitivity\nfor privately outputting median, in terms of both running time and accuracy.\nFurthermore, because our framework is a preprocessing step, it can also be\ncomplementary to smooth sensitivity and any other private mechanism, where\napplying both can achieve further gains in accuracy.\n  We additionally introduce a new notion of individual sensitivity and show\nthat it is an important metric in the variant definition of personalized\ndifferential privacy. We show that our algorithm can extend to this context and\nserve as a useful tool for this variant definition and its applications in\nmarkets for privacy.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 18:09:34 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 10:07:05 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Cummings", "Rachel", ""], ["Durfee", "David", ""]]}, {"id": "1804.08683", "submitter": "Yipu Wang", "authors": "Yipu Wang", "title": "Maximum Integer Flows in Directed Planar Graphs with Multiple Sources\n  and Sinks and Vertex Capacities", "comments": "22 pages, 6 figures. Old version. For current version see SODA 2019\n  proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding maximum flows in planar graphs with\ncapacities on both vertices and edges and with multiple sources and sinks. We\npresent three algorithms when the capacities are integers. The first algorithm\nruns in $O(n \\log^3 n + kn)$ time when all capacities are bounded, where $n$ is\nthe number of vertices in the graph and $k$ is the number of terminals. This\nalgorithm is the first to solve the vertex-disjoint paths problem in\nnear-linear time when $k$ is bounded but larger than 2. The second algorithm\nruns in $O(k^2(k^3 + \\Delta) n \\text{ polylog} (nU))$ time, where $U$ is the\nlargest finite capacity of a single vertex and $\\Delta$ is the maximum degree\nof a vertex. Finally, when $k=3$, we present an algorithm that runs in $O(n\n\\log n)$ time; this algorithm works even when the capacities are arbitrary\nreals. Our algorithms improve on the fastest previously known algorithms when\n$k$ and $\\Delta$ are small and $U$ is bounded by a polynomial in $n$. Prior to\nthis result, the fastest algorithms ran in $O(n^2 / \\log n)$ time for real\ncapacities and $O(n^{3/2} \\log n \\log U)$ for integer capacities.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 19:53:19 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 20:32:50 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 23:39:15 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wang", "Yipu", ""]]}, {"id": "1804.08731", "submitter": "Panagiotis Charalampopoulos", "authors": "Amihood Amir, Panagiotis Charalampopoulos, Solon P. Pissis, Jakub\n  Radoszewski", "title": "Longest Common Substring Made Fully Dynamic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the longest common substring (LCS) problem, we are given two strings $S$\nand $T$, each of length at most $n$, and we are asked to find a longest string\noccurring as a fragment of both $S$ and $T$. This is a classical and\nwell-studied problem in computer science with a known $\\mathcal{O}(n)$-time\nsolution. In the fully dynamic version of the problem, edit operations are\nallowed in either of the two strings, and we are asked to report an LCS after\neach such operation. We present the first solution to this problem that\nrequires sublinear time per edit operation. In particular, we show how to\nreturn an LCS in $\\tilde{\\mathcal{O}}(n^{2/3})$ time (or\n$\\tilde{\\mathcal{O}}(\\sqrt{n})$ time if edits are allowed in only one of the\ntwo strings) after each operation using $\\tilde{\\mathcal{O}}(n)$ space.\n  This line of research was recently initiated by the authors [SPIRE 2017] in a\nsomewhat restricted dynamic variant. An $\\tilde{\\mathcal{O}}(n)$-sized data\nstructure that returns an LCS of the two strings after a single edit operation\n(that is reverted afterwards) in $\\tilde{\\mathcal{O}}(1)$ time was presented.\nAt CPM 2018, three papers studied analogously restricted dynamic variants of\nproblems on strings. We show that our techniques can be used to obtain fully\ndynamic algorithms for several classical problems on strings, namely, computing\nthe longest repeat, the longest palindrome and the longest Lyndon substring of\na string. The only previously known sublinear-time dynamic algorithms for\nproblems on strings were obtained for maintaining a dynamic collection of\nstrings for comparison queries and for pattern matching with the most recent\nadvances made by Gawrychowski et al. [SODA 2018] and by Clifford et al. [STACS\n2018].\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 20:40:40 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 16:09:31 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Amir", "Amihood", ""], ["Charalampopoulos", "Panagiotis", ""], ["Pissis", "Solon P.", ""], ["Radoszewski", "Jakub", ""]]}, {"id": "1804.08791", "submitter": "Amariah Becker", "authors": "Amariah Becker", "title": "A Tight 4/3 Approximation for Capacitated Vehicle Routing in Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of clients with demands, the Capacitated Vehicle Routing problem\nis to find a set of tours that collectively cover all client demand, such that\nthe capacity of each vehicle is not exceeded and such that the sum of the tour\nlengths is minimized. In this paper, we provide a 4/3-approximation algorithm\nfor Capacitated Vehicle Routing on trees, improving over the previous\nbest-known approximation ratio of $(\\sqrt{41}-1)/4$ by Asano et al., while\nusing the same lower bound. Asano et al. show that there exist instances whose\noptimal cost is 4/3 times this lower bound. Notably, our 4/3 approximation\nratio is therefore tight for this lower bound, achieving the best-possible\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 00:22:16 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Becker", "Amariah", ""]]}, {"id": "1804.08819", "submitter": "Nguyen Pham", "authors": "Soumyottam Chatterjee, Reza Fathi, Gopal Pandurangan, Nguyen Dinh Pham", "title": "Fast and Efficient Distributed Computation of Hamiltonian Cycles in\n  Random Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present fast and efficient randomized distributed algorithms to find\nHamiltonian cycles in random graphs. In particular, we present a randomized\ndistributed algorithm for the $G(n,p)$ random graph model, with number of nodes\n$n$ and $p=\\frac{c\\ln n}{n^{\\delta}}$ (for any constant $0 < \\delta \\leq 1$ and\nfor a suitably large constant $c > 0$), that finds a Hamiltonian cycle with\nhigh probability in $\\tilde{O}(n^{\\delta})$ rounds (the notation $\\tilde{O}$\nhides a $\\text{polylog}(n)$ factor). Our algorithm works in the (synchronous)\nCONGEST model (i.e., only $O(\\log n)$-sized messages are communicated per edge\nper round) and its computational cost per node is sublinear (in $n$) per round\nand is fully-distributed (each node uses only $o(n)$ memory and all nodes'\ncomputations are essentially balanced).\n  Our algorithm improves over the previous best known result in terms of both\nthe running time as well as the edge sparsity of the graphs where it can\nsucceed; in particular, the denser the random graph, the smaller is the running\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 02:45:19 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Chatterjee", "Soumyottam", ""], ["Fathi", "Reza", ""], ["Pandurangan", "Gopal", ""], ["Pham", "Nguyen Dinh", ""]]}, {"id": "1804.08885", "submitter": "Astrid Pieterse", "authors": "Bart M. P. Jansen and Astrid Pieterse", "title": "Polynomial Kernels for Hitting Forbidden Minors under Structural\n  Parameterizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate polynomial-time preprocessing for the problem of hitting\nforbidden minors in a graph, using the framework of kernelization. For a fixed\nfinite set of connected graphs F, the F-Deletion problem is the following:\ngiven a graph G and integer k, is it possible to delete k vertices from G to\nensure the resulting graph does not contain any graph from F as a minor?\nEarlier work by Fomin, Lokshtanov, Misra, and Saurabh [FOCS'12] showed that\nwhen F contains a planar graph, an instance (G,k) can be reduced in polynomial\ntime to an equivalent one of size $k^{O(1)}$. In this work we focus on\nstructural measures of the complexity of an instance, with the aim of giving\nnontrivial preprocessing guarantees for instances whose solutions are large.\nMotivated by several impossibility results, we parameterize the F-Deletion\nproblem by the size of a vertex modulator whose removal results in a graph of\nconstant treedepth $\\eta$.\n  We prove that for each set F of connected graphs and constant $\\eta$, the\nF-Deletion problem parameterized by the size of a treedepth-$\\eta$ modulator\nhas a polynomial kernel. Our kernelization is fully explicit and does not\ndepend on protrusion reduction or well-quasi-ordering, which are sources of\nalgorithmic non-constructivity in earlier works on F-Deletion. Our main\ntechnical contribution is to analyze how models of a forbidden minor in a graph\nG with modulator X, interact with the various connected components of G-X. By\nbounding the number of different types of behavior that can occur by a\npolynomial in |X|, we obtain a polynomial kernel using a recursive\npreprocessing strategy. Our results extend earlier work for specific instances\nof F-Deletion such as Vertex Cover and Feedback Vertex Set. It also generalizes\nearlier preprocessing results for F-Deletion parameterized by a vertex cover,\nwhich is a treedepth-one modulator.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 08:08:10 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["Pieterse", "Astrid", ""]]}, {"id": "1804.08902", "submitter": "Ran Ben Basat", "authors": "Ran Ben Basat, Maayan Goldstein, Itai Segall", "title": "Learning Software Constraints via Installation Attempts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern software systems are expected to be secure and contain all the latest\nfeatures, even when new versions of software are released multiple times an\nhour. Each system may include many interacting packages. The problem of\ninstalling multiple dependent packages has been extensively studied in the\npast, yielding some promising solutions that work well in practice. However,\nthese assume that the developers declare all the dependencies and conflicts\nbetween the packages. Oftentimes, the entire repository structure may not be\nknown upfront, for example when packages are developed by different vendors. In\nthis paper, we present algorithms for learning dependencies, conflicts and\ndefective packages from installation attempts. Our algorithms use combinatorial\ndata structures to generate queries that test installations and discover the\nentire dependency structure. A query that the algorithms make corresponds to\ntrying to install a subset of packages and getting a Boolean feedback on\nwhether all constraints were satisfied in this subset. Our goal is to minimize\nthe query complexity of the algorithms. We prove lower and upper bounds on the\nnumber of queries that these algorithms require to make for different settings\nof the problem.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 08:49:00 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 16:13:19 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Basat", "Ran Ben", ""], ["Goldstein", "Maayan", ""], ["Segall", "Itai", ""]]}, {"id": "1804.08908", "submitter": "Hengjie Zhang", "authors": "Yuhao Du, Hengjie Zhang", "title": "Improved Algorithms for Fully Dynamic Maximal Independent Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining maximal independent set in dynamic graph is a fundamental open\nproblem in graph theory and the first sublinear time deterministic algorithm\nwas came up by Assadi, Onak, Schieber and Solomon(STOC'18), which achieves\n$O(m^{3/4})$ amortized update time. We have two main contributions in this\npaper. We present a new simple deterministic algorithm with\n$O(m^{2/3}\\sqrt{\\log m})$ amortized update time, which improves the previous\nbest result. And we also present the first randomized algorithm with expected\n$O(\\sqrt{m}\\log^{1.5}m)$ amortized time against an oblivious adversary.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 09:04:41 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Du", "Yuhao", ""], ["Zhang", "Hengjie", ""]]}, {"id": "1804.08948", "submitter": "Neelima Gupta", "authors": "Neelima Gupta and Aditya Pancholi", "title": "Improved Local Search Based Approximation Algorithm for Hard Uniform\n  Capacitated k-Median Problem", "comments": "22 pages including bibliography", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the hard uniform capacitated $k$- median problem\nusing local search heuristic. Obtaining a constant factor approximation for the\n\\ckm problem is open. All the existing solutions giving constant-factor\napproximation, violate at least one of the cardinality and the capacity\nconstraints. All except Koruplou et al are based on LP-relaxation.\n  We give $(3+\\epsilon)$ factor approximation algorithm for the problem\nviolating the cardinality by a factor of $8/3 \\approx 2.67$. There is a\ntrade-off between the approximation factor and the cardinality violation\nbetween our work and the existing work. Koruplou et al gave $(1 + \\alpha)$\napproximation factor with $(5 + 5/\\alpha)$ factor loss in cardinality using\nlocal search paradigm. Though the approximation factor can be made arbitrarily\nsmall, cardinality loss is at least $5$.\n  On the other hand, we improve upon the results in\n[capkmGijswijtL2013],[capkmshili2014], [Lisoda2016] in terms of factor-loss\nthough the cardinality loss is more in our case. Also, these results are\nobtained using LP-rounding, some of them being strengthened, whereas local\nsearch techniques are simple to apply and have been shown to perform well in\npractice via empirical studies.\n  We extend the result to hard uniform capacitated $k$-median with penalties.\nTo the best of our knowledge, ours is the first result for the problem.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 10:47:18 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Gupta", "Neelima", ""], ["Pancholi", "Aditya", ""]]}, {"id": "1804.08978", "submitter": "Karl Bringmann", "authors": "Amir Abboud and Karl Bringmann", "title": "Tighter Connections Between Formula-SAT and Shaving Logs", "comments": "Accepted at ICALP'18, 36 pages, v2: corrected some references", "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2018.458", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A noticeable fraction of Algorithms papers in the last few decades improve\nthe running time of well-known algorithms for fundamental problems by\nlogarithmic factors. For example, the $O(n^2)$ dynamic programming solution to\nthe Longest Common Subsequence problem (LCS) was improved to $O(n^2/\\log^2 n)$\nin several ways and using a variety of ingenious tricks. This line of research,\nalso known as \"the art of shaving log factors\", lacks a tool for proving\nnegative results. Specifically, how can we show that it is unlikely that LCS\ncan be solved in time $O(n^2/\\log^3 n)$?\n  Perhaps the only approach for such results was suggested in a recent paper of\nAbboud, Hansen, Vassilevska W. and Williams (STOC'16). The authors blame the\nhardness of shaving logs on the hardness of solving satisfiability on Boolean\nformulas (Formula-SAT) faster than exhaustive search. They show that an\n$O(n^2/\\log^{1000} n)$ algorithm for LCS would imply a major advance in circuit\nlower bounds. Whether this approach can lead to tighter barriers was unclear.\n  In this paper, we push this approach to its limit and, in particular, prove\nthat a well-known barrier from complexity theory stands in the way for shaving\nfive additional log factors for fundamental combinatorial problems. For LCS,\nregular expression pattern matching, as well as the Fr\\'echet distance problem\nfrom Computational Geometry, we show that an $O(n^2/\\log^{7+\\varepsilon} n)$\nruntime would imply new Formula-SAT algorithms.\n  Our main result is a reduction from SAT on formulas of size $s$ over $n$\nvariables to LCS on sequences of length $N=2^{n/2} \\cdot s^{1+o(1)}$. Our\nreduction is essentially as efficient as possible, and it greatly improves the\npreviously known reduction for LCS with $N=2^{n/2} \\cdot s^c$, for some $c \\geq\n100$.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 12:13:50 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 08:23:50 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Abboud", "Amir", ""], ["Bringmann", "Karl", ""]]}, {"id": "1804.09019", "submitter": "Asish Mukhopadhyay", "authors": "Md. Zamilur Rahman and Asish Mukhopadhyay and Yash P. Aneja", "title": "An Algorithm for Generating Strongly Chordal Graphs", "comments": "9 pages and 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strongly chordal graphs are a subclass of chordal graphs. The interest in\nthis subclass stems from the fact that many problems which are NP-complete for\nchordal graphs are solvable in polynomial time for this subclass. However, we\nare not aware of any algorithm that can generate instances of this class, often\nnecessary for testing purposes. In this paper, we address this issue. Our\nalgorithm first generates chordal graphs, using an available algorithm and then\nadds enough edges to make it strongly chordal, unless it is already so. The\nedge additions are based on a totally balanced matrix characterizations of\nstrongly chordal graphs.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 15:03:53 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Rahman", "Md. Zamilur", ""], ["Mukhopadhyay", "Asish", ""], ["Aneja", "Yash P.", ""]]}, {"id": "1804.09240", "submitter": "Vijay Subramanya", "authors": "Benjamin Moore, Naomi Nishimura, Vijay Subramanya", "title": "Reconfiguration of graph minors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the reconfiguration framework, we consider the various ways that a\ntarget graph $H$ is a {\\em minor} of a host graph $G$, where a subgraph of $G$\ncan be transformed into $H$ by means of {\\em edge contraction} (replacement of\nboth endpoints of an edge by a new vertex adjacent to any vertex adjacent to\neither endpoint). Equivalently, an {\\em $H$-model} of $G$ is a labeling of the\nvertices of $G$ with the vertices of $H$, where the contraction of all edges\nbetween identically-labeled vertices results in a graph containing\nrepresentations of all edges in $H$.\n  We explore the properties of $G$ and $H$ that result in a connected {\\em\nreconfiguration graph}, in which nodes represent $H$-models and two nodes are\nadjacent if their corresponding $H$-models differ by the label of a single\nvertex of $G$. Various operations on $G$ or $H$ are shown to preserve\nconnectivity. In addition, we demonstrate properties of graphs $G$ that result\nin connectivity for the target graphs $K_2$, $K_3$, and $K_4$, including a full\ncharacterization of graphs $G$ that result in connectivity for $K_2$-models, as\nwell as the relationship between connectivity of $G$ and other $H$-models.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 20:28:07 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Moore", "Benjamin", ""], ["Nishimura", "Naomi", ""], ["Subramanya", "Vijay", ""]]}, {"id": "1804.09393", "submitter": "Guillaume Ducoffe", "authors": "Guillaume Ducoffe, Alexandru Popa", "title": "A quasi linear-time b-Matching algorithm on distance-hereditary graphs\n  and bounded split-width graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a quasi linear-time algorithm for Maximum Matching on\ndistance-hereditary graphs and some of their generalizations. This improves on\n[Dragan, WG'97], who proposed such an algorithm for the subclass of\n(tent,hexahedron)-free distance-hereditary graphs. Furthermore, our result is\nderived from a more general one that is obtained for b-Matching. In the (unit\ncost) b-Matching problem, we are given a graph G = (V, E) together with a\nnonnegative integer capacity b v for every vertex v $\\in$ V. The objective is\nto assign nonnegative integer weights (x e) e$\\in$E so that: for every v $\\in$\nV the sum of the weights of its incident edges does not exceed b v , and\ne$\\in$E x e is maximized. We present the first algorithm for solving b-Matching\non cographs, distance-hereditary graphs and some of their generalizations in\nquasi linear time. For that, we use a decomposition algorithm that outputs for\nany graph G a collection of subgraphs of G with no edge-cutsets inducing a\ncomplete bipartite subgraph (a.k.a., splits). The latter collection is\nsometimes called a split decomposition of G. Furthermore, there exists a\ngeneric method in order to design graph algorithms based on split decomposition\n[Rao, DAM'08]. However, this technique only applies to \"localized\" problems:\nfor which a \"best\" partial solution for any given subgraph in a split\ndecomposition can be computed almost independently from the remaining of the\ngraph. Such framework does not apply to matching problems since an augmenting\npath may cross the subgraphs arbitrarily. We introduce a new technique that\nsomehow captures all the partial solutions for a given union of subgraphs in a\nsplit decomposition, in a compact and amenable way for algorithms - assuming\nsome piecewise linear assumption holds on the value of such solutions. The\nlatter assumption is shown to hold for b-Matching. Doing so, we prove that\nsolving b-Matching on any pair G, b can be reduced in quasi linear-time to\nsolving this problem on a collection of smaller graphs: that are obtained from\nthe subgraphs in any split decomposition of G by replacing every vertex with a\nconstant-size module. In particular, if G has a split decomposition where all\nsubgraphs have order at most a fixed k, then we can solve b-Matching for G, b\nin O((k log 2 k)$\\times$(m+n)$\\times$log ||b|| 1)-time. This answers an open\nquestion of [Coudert et al., SODA'18].\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 07:15:29 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Ducoffe", "Guillaume", ""], ["Popa", "Alexandru", ""]]}, {"id": "1804.09407", "submitter": "Guillaume Ducoffe", "authors": "Guillaume Ducoffe, Alexandru Popa", "title": "The use of a pruned modular decomposition for Maximum Matching\n  algorithms on some graph classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the following general question: given a graph class C on which we\ncan solve Maximum Matching in (quasi) linear time, does the same hold true for\nthe class of graphs that can be modularly decomposed into C ? A major\ndifficulty in this task is that the Maximum Matching problem is not preserved\nby quotient, thereby making difficult to exploit the structural properties of\nthe quotient subgraphs of the modular decomposition. So far, we are only aware\nof a recent framework in [Coudert et al., SODA'18] that only applies when the\nquotient subgraphs have bounded order and/or under additional assumptions on\nthe nontriv-ial modules in the graph. As a first attempt toward improving this\nframework we study the combined effect of modular decomposition with a pruning\nprocess over the quotient subgraphs. More precisely, we remove sequentially\nfrom all such subgraphs their so-called one-vertex extensions (i.e., pendant,\nanti-pendant, twin, universal and isolated vertices). Doing so, we obtain a\n\"pruned modular decomposition\", that can be computed in O(m log n)-time. Our\nmain result is that if all the pruned quotient subgraphs have bounded order\nthen a maximum matching can be computed in linear time. This result is mostly\nbased on two pruning rules on pendant and anti-pendant modules -- that are\nadjacent, respectively, to one or all but one other modules in the graph.\nFurthermore, these two latter rules are surprisingly intricate and we consider\nthem as our main technical contribution in the paper. We stress that the class\nof graphs that can be totally decomposed by the pruned modular decomposition\ncontains all the distance-hereditary graphs, and so, it is larger than\ncographs. In particular, as a byproduct of our approach we also obtain the\nfirst known linear-time algorithms for Maximum Matching on distance-hereditary\ngraphs and graphs with modular-treewidth at most one. Finally, we can use an\nextended version of our framework in order to compute a maximum matching, in\nlinear-time, for all graph classes that can be modularly decomposed into\ncycles. Our work is the first to explain why the existence of some nice\nordering over the modules of a graph, instead of just over its vertices, can\nhelp to speed up the computation of maximum matchings on some graph classes.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 07:51:46 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Ducoffe", "Guillaume", ""], ["Popa", "Alexandru", ""]]}, {"id": "1804.09411", "submitter": "Nil Mamano", "authors": "Gill Barequet, David Eppstein, Michael T. Goodrich, Nil Mamano", "title": "Stable-Matching Voronoi Diagrams: Combinatorial Complexity and\n  Algorithms", "comments": "34 pages, 21 figures, This is a full version of an extended abstract\n  presented in ICALP'18. To appear in JoCG. v2: upgraded version for JoCG", "journal-ref": "J. Computational Geometry 11 (1): 26-59, 2020", "doi": "10.20382/jocg.v11i1a2", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study algorithms and combinatorial complexity bounds for\n\\emph{stable-matching Voronoi diagrams}, where a set, $S$, of $n$ point sites\nin the plane determines a stable matching between the points in $\\mathbb{R}^2$\nand the sites in $S$ such that (i) the points prefer sites closer to them and\nsites prefer points closer to them, and (ii) each site has a quota or\n\"appetite\" indicating the area of the set of points that can be matched to it.\nThus, a stable-matching Voronoi diagram is a solution to the well-known post\noffice problem with the added (realistic) constraint that each post office has\na limit on the size of its jurisdiction. Previous work on the stable-matching\nVoronoi diagram provided existence and uniqueness proofs, but did not analyze\nits combinatorial or algorithmic complexity. In this paper, we show that a\nstable-matching Voronoi diagram of $n$ point sites has $O(n^{2+\\varepsilon})$\nfaces and edges, for any $\\varepsilon>0$, and show that this bound is almost\ntight by giving a family of diagrams with $\\Theta(n^2)$ faces and edges. We\nalso provide a discrete algorithm for constructing it in $O(n^3\\log n+n^2f(n))$\ntime in the real-RAM model of computation, where $f(n)$ is the runtime of a\ngeometric primitive (which we define) that can be approximated numerically, but\ncannot, in general, be performed exactly in an algebraic model of computation.\nWe show, however, how to compute the geometric primitive exactly for polygonal\nconvex distance functions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 08:08:16 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 15:24:41 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Barequet", "Gill", ""], ["Eppstein", "David", ""], ["Goodrich", "Michael T.", ""], ["Mamano", "Nil", ""]]}, {"id": "1804.09448", "submitter": "Thore Husfeldt", "authors": "Cornelius Brand, Holger Dell, and Thore Husfeldt", "title": "Extensor-Coding", "comments": "To appear at STOC 2018: Symposium on Theory of Computing, June 23-27,\n  2018, Los Angeles, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise an algorithm that approximately computes the number of paths of\nlength $k$ in a given directed graph with $n$ vertices up to a multiplicative\nerror of $1 \\pm \\varepsilon$. Our algorithm runs in time $\\varepsilon^{-2}\n4^k(n+m) \\operatorname{poly}(k)$. The algorithm is based on associating with\neach vertex an element in the exterior (or, Grassmann) algebra, called an\nextensor, and then performing computations in this algebra. This connection to\nexterior algebra generalizes a number of previous approaches for the longest\npath problem and is of independent conceptual interest. Using this approach, we\nalso obtain a deterministic $2^{k}\\cdot\\operatorname{poly}(n)$ time algorithm\nto find a $k$-path in a given directed graph that is promised to have few of\nthem. Our results and techniques generalize to the subgraph isomorphism problem\nwhen the subgraphs we are looking for have bounded pathwidth. Finally, we also\nobtain a randomized algorithm to detect $k$-multilinear terms in a multivariate\npolynomial given as a general algebraic circuit. To the best of our knowledge,\nthis was previously only known for algebraic circuits not involving negative\nconstants.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 09:29:09 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Brand", "Cornelius", ""], ["Dell", "Holger", ""], ["Husfeldt", "Thore", ""]]}, {"id": "1804.09673", "submitter": "Vasileios Nakos", "authors": "Vasileios Nakos, Xiaofei Shi, David P. Woodruff, Hongyang Zhang", "title": "Improved Algorithms for Adaptive Compressed Sensing", "comments": "To appear in ICALP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the problem of adaptive compressed sensing, one wants to estimate an\napproximately $k$-sparse vector $x\\in\\mathbb{R}^n$ from $m$ linear measurements\n$A_1 x, A_2 x,\\ldots, A_m x$, where $A_i$ can be chosen based on the outcomes\n$A_1 x,\\ldots, A_{i-1} x$ of previous measurements. The goal is to output a\nvector $\\hat{x}$ for which $$\\|x-\\hat{x}\\|_p \\le C \\cdot \\min_{k\\text{-sparse }\nx'} \\|x-x'\\|_q\\,$$ with probability at least $2/3$, where $C > 0$ is an\napproximation factor. Indyk, Price and Woodruff (FOCS'11) gave an algorithm for\n$p=q=2$ for $C = 1+\\epsilon$ with $\\Oh((k/\\epsilon) \\loglog (n/k))$\nmeasurements and $\\Oh(\\log^*(k) \\loglog (n))$ rounds of adaptivity. We first\nimprove their bounds, obtaining a scheme with $\\Oh(k \\cdot \\loglog (n/k)\n+(k/\\epsilon) \\cdot \\loglog(1/\\epsilon))$ measurements and $\\Oh(\\log^*(k)\n\\loglog (n))$ rounds, as well as a scheme with $\\Oh((k/\\epsilon) \\cdot \\loglog\n(n\\log (n/k)))$ measurements and an optimal $\\Oh(\\loglog (n))$ rounds. We then\nprovide novel adaptive compressed sensing schemes with improved bounds for\n$(p,p)$ for every $0 < p < 2$. We show that the improvement from $O(k\n\\log(n/k))$ measurements to $O(k \\log \\log (n/k))$ measurements in the adaptive\nsetting can persist with a better $\\epsilon$-dependence for other values of $p$\nand $q$. For example, when $(p,q) = (1,1)$, we obtain\n$O(\\frac{k}{\\sqrt{\\epsilon}} \\cdot \\log \\log n \\log^3 (\\frac{1}{\\epsilon}))$\nmeasurements.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 16:43:33 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Nakos", "Vasileios", ""], ["Shi", "Xiaofei", ""], ["Woodruff", "David P.", ""], ["Zhang", "Hongyang", ""]]}, {"id": "1804.09684", "submitter": "Anders Aamand", "authors": "Anders Aamand, Mathias B{\\ae}k Tejs Knudsen and Mikkel Thorup", "title": "Power of $d$ Choices with Simple Tabulation", "comments": "Accepted at ICALP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we are to place $m$ balls into $n$ bins sequentially using the\n$d$-choice paradigm: For each ball we are given a choice of $d$ bins, according\nto $d$ hash functions $h_1,\\dots,h_d$ and we place the ball in the least loaded\nof these bins breaking ties arbitrarily. Our interest is in the number of balls\nin the fullest bin after all $m$ balls have been placed.\n  Azar et al. [STOC'94] proved that when $m=O(n)$ and when the hash functions\nare fully random the maximum load is at most $\\frac{\\lg \\lg n }{\\lg d}+O(1)$\nwhp (i.e. with probability $1-O(n^{-\\gamma})$ for any choice of $\\gamma$).\n  In this paper we suppose that the $h_1,\\dots,h_d$ are simple tabulation hash\nfunctions. Generalising a result by Dahlgaard et al [SODA'16] we show that for\nan arbitrary constant $d\\geq 2$ the maximum load is $O(\\lg \\lg n)$ whp, and\nthat expected maximum load is at most $\\frac{\\lg \\lg n}{\\lg d}+O(1)$. We\nfurther show that by using a simple tie-breaking algorithm introduced by\nV\\\"ocking [J.ACM'03] the expected maximum load drops to $\\frac{\\lg \\lg n}{d\\lg\n\\varphi_d}+O(1)$ where $\\varphi_d$ is the rate of growth of the $d$-ary\nFibonacci numbers. Both of these expected bounds match those of the fully\nrandom setting.\n  The analysis by Dahlgaard et al. relies on a proof by P\\u{a}tra\\c{s}cu and\nThorup [J.ACM'11] concerning the use of simple tabulation for cuckoo hashing.\nWe need here a generalisation to $d>2$ hash functions, but the original proof\nis an 8-page tour de force of ad-hoc arguments that do not appear to\ngeneralise. Our main technical contribution is a shorter, simpler and more\naccessible proof of the result by P\\u{a}tra\\c{s}cu and Thorup, where the\nrelevant parts generalise nicely to the analysis of $d$ choices.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 17:23:48 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Aamand", "Anders", ""], ["Knudsen", "Mathias B\u00e6k Tejs", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1804.09741", "submitter": "Luis Meira", "authors": "Luis A. A. Meira, Vin\\'icius R. M\\'aximo, Alvaro L. Fazenda and\n  Arlindo F. da Concei\\c{c}\\~ao", "title": "An Faster Network Motif Detection Tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network motif provides a way to uncover the basic building blocks of most\ncomplex networks. This task usually demands high computer processing, specially\nfor motif with 5 or more vertices. This paper presents an extended methodology\nwith the following features: (i) search for motifs up to 6 vertices, (ii)\nmultithread processing, and a (iii) new enumeration algorithm with lower\ncomplexity. The algorithm to compute motifs solve isomorphism in $O(1)$ with\nthe use of hash table. Concurrent threads evaluates distinct graphs. The\nenumeration algorithm has smaller computational complexity. The experiments\nshows better performance with respect to other methods available in literature,\nallowing bioinformatic researchers to efficiently identify motifs of size 3, 4,\n5, and 6.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 18:23:42 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Meira", "Luis A. A.", ""], ["M\u00e1ximo", "Vin\u00edcius R.", ""], ["Fazenda", "Alvaro L.", ""], ["da Concei\u00e7\u00e3o", "Arlindo F.", ""]]}, {"id": "1804.09745", "submitter": "Greg Bodwin", "authors": "Greg Bodwin", "title": "On the Structure of Unique Shortest Paths in Graphs", "comments": "SODA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a structural theory of unique shortest paths in\nreal-weighted graphs. Our main goal is to characterize exactly which sets of\nnode sequences, which we call path systems, can be realized as unique shortest\npaths in a graph with arbitrary real edge weights. We say that such a path\nsystem is strongly metrizable.\n  An easy fact implicit in the literature is that a strongly metrizable path\nsystem must be consistent, meaning that no two of its paths may intersect,\nsplit apart, and then intersect again. Our main result characterizes strong\nmetrizability via some new forbidden intersection patterns along these lines.\nIn other words, we describe a family of forbidden patterns beyond consistency,\nand we prove that a path system is strongly metrizable if and only if it is\nconsistent and it avoids all of the patterns in this family. We offer separate\n(but closely related) characterizations in this way for the settings of\ndirected, undirected, and directed acyclic graphs.\n  Our characterizations are based on a new connection between shortest paths\nand topology; in particular, our new forbidden patterns are in natural\ncorrespondence with two-colored topological $2$-manifolds, which we visualize\nas polyhedra. We believe that this connection may be of independent interest,\nand we further show that it implies some additional structural corollaries that\nseem to suggest new and possibly deep-rooted connections between these areas.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 18:28:06 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 17:12:56 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Bodwin", "Greg", ""]]}, {"id": "1804.09758", "submitter": "Osman Dai", "authors": "Osman Emre Dai, Daniel Cullina, Negar Kiyavash, Matthias Grossglauser", "title": "Analysis of a Canonical Labeling Algorithm for the Alignment of\n  Correlated Erd\\H{o}s-R\\'enyi Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph alignment in two correlated random graphs refers to the task of\nidentifying the correspondence between vertex sets of the graphs. Recent\nresults have characterized the exact information-theoretic threshold for graph\nalignment in correlated Erd\\H{o}s-R\\'enyi graphs. However, very little is known\nabout the existence of efficient algorithms to achieve graph alignment without\nseeds.\n  In this work we identify a region in which a straightforward $O(n^{11/5} \\log\nn )$-time canonical labeling algorithm, initially introduced in the context of\ngraph isomorphism, succeeds in aligning correlated Erd\\H{o}s-R\\'enyi graphs.\nThe algorithm has two steps. In the first step, all vertices are labeled by\ntheir degrees and a trivial minimum distance alignment (i.e., sorting vertices\naccording to their degrees) matches a fixed number of highest degree vertices\nin the two graphs. Having identified this subset of vertices, the remaining\nvertices are matched using a alignment algorithm for bipartite graphs.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 19:09:19 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 21:21:41 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Dai", "Osman Emre", ""], ["Cullina", "Daniel", ""], ["Kiyavash", "Negar", ""], ["Grossglauser", "Matthias", ""]]}, {"id": "1804.09893", "submitter": "Haim Avron", "authors": "Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya\n  Velingker, Amir Zandieh", "title": "Random Fourier Features for Kernel Ridge Regression: Approximation\n  Bounds and Statistical Guarantees", "comments": "An extended abstract of this work appears in the Proceedings of the\n  34th International Conference on Machine Learning (ICML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Fourier features is one of the most popular techniques for scaling up\nkernel methods, such as kernel ridge regression. However, despite impressive\nempirical results, the statistical properties of random Fourier features are\nstill not well understood. In this paper we take steps toward filling this gap.\nSpecifically, we approach random Fourier features from a spectral matrix\napproximation point of view, give tight bounds on the number of Fourier\nfeatures required to achieve a spectral approximation, and show how spectral\nmatrix approximation bounds imply statistical guarantees for kernel ridge\nregression.\n  Qualitatively, our results are twofold: on the one hand, we show that random\nFourier feature approximation can provably speed up kernel ridge regression\nunder reasonable assumptions. At the same time, we show that the method is\nsuboptimal, and sampling from a modified distribution in Fourier space, given\nby the leverage function of the kernel, yields provably better performance. We\nstudy this optimal sampling distribution for the Gaussian kernel, achieving a\nnearly complete characterization for the case of low-dimensional bounded\ndatasets. Based on this characterization, we propose an efficient sampling\nscheme with guarantees superior to random Fourier features in this regime.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 05:34:25 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 09:17:40 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Avron", "Haim", ""], ["Kapralov", "Michael", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""], ["Velingker", "Ameya", ""], ["Zandieh", "Amir", ""]]}, {"id": "1804.09907", "submitter": "William Kuszmaul", "authors": "Moses Charikar, Ofir Geri, Michael P. Kim, William Kuszmaul", "title": "On Estimating Edit Distance: Alignment, Dimension Reduction, and\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edit distance is a fundamental measure of distance between strings and has\nbeen widely studied in computer science. While the problem of estimating edit\ndistance has been studied extensively, the equally important question of\nactually producing an alignment (i.e., the sequence of edits) has received far\nless attention. Somewhat surprisingly, we show that any algorithm to estimate\nedit distance can be used in a black-box fashion to produce an approximate\nalignment of strings, with modest loss in approximation factor and small loss\nin run time. Plugging in the result of Andoni, Krauthgamer, and Onak, we obtain\nan alignment that is a $(\\log n)^{O(1/\\varepsilon^2)}$ approximation in time\n$\\tilde{O}(n^{1 + \\varepsilon})$.\n  Closely related to the study of approximation algorithms is the study of\nmetric embeddings for edit distance. We show that min-hash techniques can be\nuseful in designing edit distance embeddings through three results: (1) An\nembedding from Ulam distance (edit distance over permutations) to Hamming space\nthat matches the best known distortion of $O(\\log n)$ and also implicitly\nencodes a sequence of edits between the strings; (2) In the case where the edit\ndistance between the input strings is known to have an upper bound $K$, we show\nthat embeddings of edit distance into Hamming space with distortion $f(n)$ can\nbe modified in a black-box fashion to give distortion\n$O(f(\\operatorname{poly}(K)))$ for a class of periodic-free strings; (3) A\nrandomized dimension-reduction map with contraction $c$ and asymptotically\noptimal expected distortion $O(c)$, improving on the previous $\\tilde{O}(c^{1 +\n2 / \\log \\log \\log n})$ distortion result of Batu, Ergun, and Sahinalp.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 06:34:20 GMT"}, {"version": "v2", "created": "Sat, 5 May 2018 01:44:58 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Charikar", "Moses", ""], ["Geri", "Ofir", ""], ["Kim", "Michael P.", ""], ["Kuszmaul", "William", ""]]}, {"id": "1804.09950", "submitter": "Kamil Khadiev", "authors": "Kamil Khadiev and Liliya Safina", "title": "Quantum Algorithm for Dynamic Programming Approach for DAGs.\n  Applications for Zhegalkin Polynomial Evaluation and Some Problems on DAGs", "comments": "UCNC2019 Conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a quantum algorithm for dynamic programming\napproach for problems on directed acyclic graphs (DAGs). The running time of\nthe algorithm is $O(\\sqrt{\\hat{n}m}\\log \\hat{n})$, and the running time of the\nbest known deterministic algorithm is $O(n+m)$, where $n$ is the number of\nvertices, $\\hat{n}$ is the number of vertices with at least one outgoing edge;\n$m$ is the number of edges. We show that we can solve problems that use OR,\nAND, NAND, MAX and MIN functions as the main transition steps. The approach is\nuseful for a couple of problems. One of them is computing a Boolean formula\nthat is represented by Zhegalkin polynomial, a Boolean circuit with shared\ninput and non-constant depth evaluating. Another two are the single source\nlongest paths search for weighted DAGs and the diameter search problem for\nunweighted DAGs.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 09:18:52 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 10:17:27 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Khadiev", "Kamil", ""], ["Safina", "Liliya", ""]]}, {"id": "1804.09996", "submitter": "Matthijs Douze", "authors": "Matthijs Douze and Alexandre Sablayrolles and Herv\\'e J\\'egou", "title": "Link and code: Fast indexing with graphs and compact regression codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search approaches based on graph walks have recently attained\noutstanding speed-accuracy trade-offs, taking aside the memory requirements. In\nthis paper, we revisit these approaches by considering, additionally, the\nmemory constraint required to index billions of images on a single server. This\nleads us to propose a method based both on graph traversal and compact\nrepresentations. We encode the indexed vectors using quantization and exploit\nthe graph structure to refine the similarity estimation.\n  In essence, our method takes the best of these two worlds: the search\nstrategy is based on nested graphs, thereby providing high precision with a\nrelatively small set of comparisons. At the same time it offers a significant\nmemory compression. As a result, our approach outperforms the state of the art\non operating points considering 64-128 bytes per vector, as demonstrated by our\nresults on two billion-scale public benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 11:24:42 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 10:01:51 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Douze", "Matthijs", ""], ["Sablayrolles", "Alexandre", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1804.10062", "submitter": "Armin Wei{\\ss}", "authors": "Stefan Edelkamp and Armin Wei{\\ss}", "title": "QuickMergesort: Practically Efficient Constant-Factor Optimal Sorting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem of internally sorting a sequence of $n$\nelements. In its best theoretical setting QuickMergesort, a combination\nQuicksort with Mergesort with a Median-of-$\\sqrt{n}$ pivot selection, requires\nat most $n \\log n - 1.3999n + o(n)$ element comparisons on the average. The\nquestions addressed in this paper is how to make this algorithm practical. As\nrefined pivot selection usually adds much overhead, we show that the\nMedian-of-3 pivot selection of QuickMergesort leads to at most $n \\log n -\n0{.}75n + o(n)$ element comparisons on average, while running fast on\nelementary data. The experiments show that QuickMergesort outperforms\nstate-of-the-art library implementations, including C++'s Introsort and Java's\nDual-Pivot Quicksort. Further trade-offs between a low running time and a low\nnumber of comparisons are studied. Moreover, we describe a practically\nefficient version with $n \\log n + O(n)$ comparisons in the worst case.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 13:50:38 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Edelkamp", "Stefan", ""], ["Wei\u00df", "Armin", ""]]}, {"id": "1804.10173", "submitter": "Florian Nelles", "authors": "Stefan Kratsch and Florian Nelles", "title": "Efficient and adaptive parameterized algorithms on modular\n  decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the influence of a graph parameter called modular-width on the time\ncomplexity for optimally solving well-known polynomial problems such as Maximum\nMatching, Triangle Counting, and Maximum $s$-$t$ Vertex-Capacitated Flow. The\nmodular-width of a graph depends on its (unique) modular decomposition tree,\nand can be computed in linear time $O(n+m)$ for graphs with $n$ vertices and\n$m$ edges. Modular decompositions are an important tool for graph algorithms,\ne.g., for linear-time recognition of certain graph classes. Throughout, we\nobtain efficient parameterized algorithms of running times $O(f(mw)n+m)$,\n$O(n+f(mw)m)$ , or $O(f(mw)+n+m)$ for graphs of modular-width $mw$. Our\nalgorithm for Maximum Matching, running in time $O(mw^2\\log mw \\cdot n+m)$, is\nboth faster and simpler than the recent $O(mw^4n+m)$ time algorithm of Coudert\net al. (SODA 2018). For several other problems, e.g., Triangle Counting and\nMaximum $b$-Matching, we give adaptive algorithms, meaning that their running\ntimes match the best unparameterized algorithms for worst-case modular-width of\n$mw=\\Theta(n)$ and they outperform them already for $mw=o(n)$, until reaching\nlinear time for $mw=O(1)$.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 17:01:21 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Kratsch", "Stefan", ""], ["Nelles", "Florian", ""]]}, {"id": "1804.10186", "submitter": "Pawe{\\l} Gawrychowski", "authors": "Bart{\\l}omiej Dudek and Pawe{\\l} Gawrychowski", "title": "Edit Distance between Unrooted Trees in Cubic Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edit distance between trees is a natural generalization of the classical edit\ndistance between strings, in which the allowed elementary operations are\ncontraction, uncontraction and relabeling of an edge. Demaine et al. [ACM\nTrans. on Algorithms, 6(1), 2009] showed how to compute the edit distance\nbetween rooted trees on $n$ nodes in $\\mathcal{O}(n^{3})$ time. However,\ngeneralizing their method to unrooted trees seems quite problematic, and the\nmost efficient known solution remains to be the previous $\\mathcal{O}(n^{3}\\log\nn)$ time algorithm by Klein [ESA 1998]. Given the lack of progress on improving\nthis complexity, it might appear that unrooted trees are simply more difficult\nthan rooted trees. We show that this is, in fact, not the case, and edit\ndistance between unrooted trees on $n$ nodes can be computed in\n$\\mathcal{O}(n^{3})$ time. A significantly faster solution is unlikely to\nexist, as Bringmann et al. [SODA 2018] proved that the complexity of computing\nthe edit distance between rooted trees cannot be decreased to\n$\\mathcal{O}(n^{3-\\epsilon})$ unless some popular conjecture fails, and the\nlower bound easily extends to unrooted trees. We also show that for two\nunrooted trees of size $m$ and $n$, where $m\\le n$, our algorithm can be\nmodified to run in $\\mathcal{O}(nm^2(1+\\log\\frac nm))$. This, again, matches\nthe complexity achieved by Demaine et al. for rooted trees, who also showed\nthat this is optimal if we restrict ourselves to the so-called decomposition\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 17:36:57 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Dudek", "Bart\u0142omiej", ""], ["Gawrychowski", "Pawe\u0142", ""]]}, {"id": "1804.10470", "submitter": "Pawe{\\l} Rz\\k{a}\\.zewski", "authors": "Karolina Okrasa and Pawe{\\l} Rz\\k{a}\\.zewski", "title": "Intersecting edge distinguishing colorings of hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An edge labeling of a graph distinguishes neighbors by sets (multisets,\nresp.), if for any two adjacent vertices $u$ and $v$ the sets (multisets,\nresp.) of labels appearing on edges incident to $u$ and $v$ are different. In\nan analogous way we define total labelings distinguishing neighbors by sets or\nmultisets: for each vertex, we consider labels on incident edges and the label\nof the vertex itself.\n  In this paper we show that these problems, and also other problems of similar\nflavor, admit an elegant and natural generalization as a hypergraph coloring\nproblem. An ieds-coloring (iedm-coloring, resp.) of a hypergraph is a vertex\ncoloring, in which the sets (multisets, resp.) of colors, that appear on every\npair of intersecting edges are different. We show upper bounds on the size of\nlists, which guarantee the existence of an ieds- or iedm-coloring, respecting\nthese lists. The proof is essentially a randomized algorithm, whose expected\ntime complexity is polynomial. As corollaries, we derive new results concerning\nthe list variants of graph labeling problems, distinguishing neighbors by sets\nor multisets. We also show that our method is robust and can be easily extended\nfor different, related problems.\n  We also investigate a close connection between edge labelings of bipartite\ngraphs, distinguishing neighbors by sets, and the so-called property \\textbf{B}\nof hypergraphs. We discuss computational aspects of the problem and present\nsome classes of bipartite graphs, which admit such a labeling using two labels.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 12:40:15 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Okrasa", "Karolina", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""]]}, {"id": "1804.10591", "submitter": "Stacey Jeffery", "authors": "Michael Jarret, Stacey Jeffery, Shelby Kimmel, Alvaro Piedrafita", "title": "Quantum Algorithms for Connectivity and Related Problems", "comments": "33 pages", "journal-ref": "European Symposium on Algorithms 2018: 49:1-49:13", "doi": "10.4230/LIPIcs.ESA.2018.49", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important family of span programs, st-connectivity span programs, have\nbeen used to design quantum algorithms in various contexts, including a number\nof graph problems and formula evaluation problems. The complexity of the\nresulting algorithms depends on the largest positive witness size of any\n1-input, and the largest negative witness size of any 0-input. Belovs and\nReichardt first showed that the positive witness size is exactly characterized\nby the effective resistance of the input graph, but only rough upper bounds\nwere known previously on the negative witness size. We show that the negative\nwitness size in an st-connectivity span program is exactly characterized by the\ncapacitance of the input graph. This gives a tight analysis for algorithms\nbased on st-connectivity span programs on any set of inputs.\n  We use this analysis to give a new quantum algorithm for estimating the\ncapacitance of a graph. We also describe a new quantum algorithm for deciding\nif a graph is connected, which improves the previous best quantum algorithm for\nthis problem if we're promised that either the graph has at least kappa > 1\ncomponents, or the graph is connected and has small average resistance, which\nis upper bounded by the diameter. We also give an alternative algorithm for\ndeciding if a graph is connected that can be better than our first algorithm\nwhen the maximum degree is small. Finally, using ideas from our second\nconnectivity algorithm, we give an algorithm for estimating the algebraic\nconnectivity of a graph, the second largest eigenvalue of the Laplacian.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 16:59:54 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Jarret", "Michael", ""], ["Jeffery", "Stacey", ""], ["Kimmel", "Shelby", ""], ["Piedrafita", "Alvaro", ""]]}, {"id": "1804.10670", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, M. S. Ramanujan, Felix Reidl, Magnus Wahlstr\\\"om", "title": "Alternative parameterizations of Metric Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of vertices $W$ in a graph $G$ is called resolving if for any two\ndistinct $x,y\\in V(G)$, there is $v\\in W$ such that ${\\rm dist}_G(v,x)\\neq{\\rm\ndist}_G(v,y)$, where ${\\rm dist}_G(u,v)$ denotes the length of a shortest path\nbetween $u$ and $v$ in the graph $G$. The metric dimension ${\\rm md}(G)$ of $G$\nis the minimum cardinality of a resolving set. The Metric Dimension problem,\ni.e. deciding whether ${\\rm md}(G)\\le k$, is NP-complete even for interval\ngraphs (Foucaud et al., 2017). We study Metric Dimension (for arbitrary graphs)\nfrom the lens of parameterized complexity. The problem parameterized by $k$ was\nproved to be $W[2]$-hard by Hartung and Nichterlein (2013) and we study the\ndual parameterization, i.e., the problem of whether ${\\rm md}(G)\\le n- k,$\nwhere $n$ is the order of $G$. We prove that the dual parameterization admits\n(a) a kernel with at most $3k^4$ vertices and (b) an algorithm of runtime\n$O^*(4^{k+o(k)}).$ Hartung and Nichterlein (2013) also observed that Metric\nDimension is fixed-parameter tractable when parameterized by the vertex cover\nnumber $vc(G)$ of the input graph. We complement this observation by showing\nthat it does not admit a polynomial kernel even when parameterized by $vc(G) +\nk$. Our reduction also gives evidence for non-existence of polynomial Turing\nkernels.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 20:14:11 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Gutin", "Gregory", ""], ["Ramanujan", "M. S.", ""], ["Reidl", "Felix", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "1804.10673", "submitter": "Prashant Pandey", "authors": "Mayank Goswami, Dzejla Medjedovic, Emina Mekic, and Prashant Pandey", "title": "Buffered Count-Min Sketch on SSD: Theory and Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequency estimation data structures such as the count-min sketch (CMS) have\nfound numerous applications in databases, networking, computational biology and\nother domains. Many applications that use the count-min sketch process massive\nand rapidly evolving datasets. For data-intensive applications that aim to keep\nthe overestimate error low, the count-min sketch may become too large to store\nin available RAM and may have to migrate to external storage (e.g., SSD.) Due\nto the random-read/write nature of hash operations of the count-min sketch,\nsimply placing it on SSD stifles the performance of time-critical applications,\nrequiring about 4-6 random reads/writes to SSD per estimate (lookup) and update\n(insert) operation.\n  In this paper, we expand on the preliminary idea of the Buffered Count-Min\nSketch (BCMS) [15], an SSD variant of the count-min sketch, that used hash\nlocalization to scale efficiently out of RAM while keeping the total error\nbounded. We describe the design and implementation of the buffered count-min\nsketch, and empirically show that our implementation achieves 3.7x-4.7x the\nspeedup on update (insert) and 4.3x speedup on estimate (lookup) operations.\n  Our design also offers an asymptotic improvement in the external-memory model\n[1] over the original data structure: r random I/Os are reduced to 1 I/O for\nthe estimate operation. For a data structure that uses k blocks on SSD, was the\nword/counter size, r as the number of rows, M as the number of bits in the main\nmemory, our data structure uses kwr/M amortized I/Os for updates, or, if kwr/M\n>1, 1 I/O in the worst case. In typical scenarios, kwr/M is much smaller than\n1. This is in contrast to O(r) I/Os incurred for each update in the original\ndata structure.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 20:19:39 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Goswami", "Mayank", ""], ["Medjedovic", "Dzejla", ""], ["Mekic", "Emina", ""], ["Pandey", "Prashant", ""]]}, {"id": "1804.10696", "submitter": "Aditya Bhaskara", "authors": "Aditya Bhaskara, Srivatsan Kumar", "title": "Low Rank Approximation in the Presence of Outliers", "comments": "The new version corrects a minor error in the analysis of the\n  algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of principal component analysis (PCA) in the presence\nof outliers. Given a matrix $A$ ($d \\times n$) and parameters $k, m$, the goal\nis to remove a set of at most $m$ columns of $A$ (known as outliers), so as to\nminimize the rank-$k$ approximation error of the remaining matrix. While much\nof the work on this problem has focused on recovery of the rank-$k$ subspace\nunder assumptions on the inliers and outliers, we focus on the approximation\nproblem above. Our main result shows that sampling-based methods developed in\nthe outlier-free case give non-trivial guarantees even in the presence of\noutliers. Using this insight, we develop a simple algorithm that has\nbi-criteria guarantees. Further, unlike similar formulations for clustering, we\nshow that bi-criteria guarantees are unavoidable for the problem, under\nappropriate complexity assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 21:36:03 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 17:46:48 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Bhaskara", "Aditya", ""], ["Kumar", "Srivatsan", ""]]}, {"id": "1804.10726", "submitter": "Zang Xinshi", "authors": "Xinshi Zang, Peiwen Hao, Xiaofeng Gao, Bin Yao, Guihai Chen", "title": "QDR-Tree: An Efcient Index Scheme for Complex Spatial Keyword Query", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the popularity of mobile devices and the development of geo-positioning\ntechnology, location-based services (LBS) attract much attention and top-k\nspatial keyword queries become increasingly complex. It is common to see that\nclients issue a query to find a restaurant serving pizza and steak, low in\nprice and noise level particularly. However, most of prior works focused only\non the spatial keyword while ignoring these independent numerical attributes.\nIn this paper we demonstrate, for the first time, the Attributes-Aware Spatial\nKeyword Query (ASKQ), and devise a two-layer hybrid index structure called\nQuad-cluster Dual-filtering R-Tree (QDR-Tree). In the keyword cluster layer, a\nQuad-Cluster Tree (QC-Tree) is built based on the hierarchical clustering\nalgorithm using kernel k-means to classify keywords. In the spatial layer, for\neach leaf node of the QC-Tree, we attach a Dual-Filtering R-Tree (DR-Tree) with\ntwo filtering algorithms, namely, keyword bitmap-based and attributes\nskyline-based filtering. Accordingly, efficient query processing algorithms are\nproposed. Through theoretical analysis, we have verified the optimization both\nin processing time and space consumption. Finally, massive experiments with\nreal-data demonstrate the efficiency and effectiveness of QDR-Tree.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 02:58:00 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Zang", "Xinshi", ""], ["Hao", "Peiwen", ""], ["Gao", "Xiaofeng", ""], ["Yao", "Bin", ""], ["Chen", "Guihai", ""]]}, {"id": "1804.10738", "submitter": "Alexander Rusciano", "authors": "Alexander Rusciano", "title": "A Riemannian Corollary of Helly's Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a notion of halfspace for Hadamard manifolds that is natural in\nthe context of convex optimization. For this notion of halfspace, we generalize\na classic result of Gr\\\"unbaum, which itself is a corollary of Helly's theorem.\nNamely, given a probability distribution on the manifold, there is a point for\nwhich all halfspaces based at this point have at least $\\frac{1}{n+1}$ of the\nmass. As an application, the gradient oracle complexity of convex optimization\nis polynomial in the parameters defining the problem.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 03:57:45 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 18:30:23 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Rusciano", "Alexander", ""]]}, {"id": "1804.10740", "submitter": "Ran Ben Basat", "authors": "Ran Ben Basat and Roy Friedman and Rana Shahout", "title": "Heavy Hitters over Interval Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heavy hitters and frequency measurements are fundamental in many networking\napplications such as load balancing, QoS, and network security. This paper\nconsiders a generalized sliding window model that supports frequency and heavy\nhitters queries over an interval given at \\emph{query time}. This enables\ndrill-down queries, in which the behavior of the network can be examined in\nfiner and finer granularities. For this model, we asymptotically improve the\nspace bounds of existing work, reduce the update and query time to a constant,\nand provide deterministic solutions. When evaluated over real Internet packet\ntraces, our fastest algorithm processes packets $90$--$250$ times faster,\nserves queries at least $730$ times quicker and consumes at least $40\\%$ less\nspace than the known method.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 04:58:49 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 21:20:16 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Basat", "Ran Ben", ""], ["Friedman", "Roy", ""], ["Shahout", "Rana", ""]]}, {"id": "1804.10791", "submitter": "Davide Bil\\`o", "authors": "Davide Bil\\`o", "title": "New algorithms for Steiner tree reoptimization", "comments": "21 pages; 1 figure; accepted for publications at the 45th\n  International Colloquium on Automata, Languages, and Programming (ICALP 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  {\\em Reoptimization} is a setting in which we are given an (near) optimal\nsolution of a problem instance and a local modification that slightly changes\nthe instance. The main goal is that of finding an (near) optimal solution of\nthe modified instance.\n  We investigate one of the most studied scenarios in reoptimization known as\n{\\em Steiner tree reoptimization}. Steiner tree reoptimization is a collection\nof strongly NP-hard optimization problems that are defined on top of the\nclassical Steiner tree problem and for which several constant-factor\napproximation algorithms have been designed in the last decade. In this paper\nwe improve upon all these results by developing a novel technique that allows\nus to design {\\em polynomial-time approximation schemes}. Remarkably, prior to\nthis paper, no approximation algorithm better than recomputing a solution from\nscratch was known for the elusive scenario in which the cost of a single edge\ndecreases. Our results are best possible since none of the problems addressed\nin this paper admits a fully polynomial-time approximation scheme, unless P=NP.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 12:00:14 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Bil\u00f2", "Davide", ""]]}, {"id": "1804.10827", "submitter": "Apoorv Vikram Singh", "authors": "Amit Deshpande, Anand Louis, Apoorv Vikram Singh", "title": "On Euclidean $k$-Means Clustering with $\\alpha$-Center Proximity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $k$-means clustering is NP-hard in the worst case but previous work has shown\nefficient algorithms assuming the optimal $k$-means clusters are \\emph{stable}\nunder additive or multiplicative perturbation of data. This has two caveats.\nFirst, we do not know how to efficiently verify this property of optimal\nsolutions that are NP-hard to compute in the first place. Second, the stability\nassumptions required for polynomial time $k$-means algorithms are often\nunreasonable when compared to the ground-truth clusters in real-world data. A\nconsequence of multiplicative perturbation resilience is \\emph{center\nproximity}, that is, every point is closer to the center of its own cluster\nthan the center of any other cluster, by some multiplicative factor $\\alpha >\n1$.\n  We study the problem of minimizing the Euclidean $k$-means objective only\nover clusterings that satisfy $\\alpha$-center proximity. We give a simple\nalgorithm to find the optimal $\\alpha$-center-proximal $k$-means clustering in\nrunning time exponential in $k$ and $1/(\\alpha - 1)$ but linear in the number\nof points and the dimension. We define an analogous $\\alpha$-center proximity\ncondition for outliers, and give similar algorithmic guarantees for $k$-means\nwith outliers and $\\alpha$-center proximity. On the hardness side we show that\nfor any $\\alpha' > 1$, there exists an $\\alpha \\leq \\alpha'$, $(\\alpha >1)$,\nand an $\\varepsilon_0 > 0$ such that minimizing the $k$-means objective over\nclusterings that satisfy $\\alpha$-center proximity is NP-hard to approximate\nwithin a multiplicative $(1+\\varepsilon_0)$ factor.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 16:17:15 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 13:09:19 GMT"}, {"version": "v3", "created": "Tue, 26 Feb 2019 12:49:55 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Deshpande", "Amit", ""], ["Louis", "Anand", ""], ["Singh", "Apoorv Vikram", ""]]}, {"id": "1804.10902", "submitter": "Yuchen Mao", "authors": "Siu-Wing Cheng and Yuchen Mao", "title": "Restricted Max-Min Fair Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The restricted max-min fair allocation problem seeks an allocation of\nresources to players that maximizes the minimum total value obtained by any\nplayer. It is NP-hard to approximate the problem to a ratio less than 2.\nComparing the current best algorithm for estimating the optimal value with the\ncurrent best for constructing an allocation, there is quite a gap between the\nratios that can be achieved in polynomial time: roughly 4 for estimation and\nroughly $6 + 2\\sqrt{10}$ for construction. We propose an algorithm that\nconstructs an allocation with value within a factor of $6 + \\delta$ from the\noptimum for any constant $\\delta > 0$. The running time is polynomial in the\ninput size for any constant $\\delta$ chosen.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 10:22:39 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 08:14:39 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Cheng", "Siu-Wing", ""], ["Mao", "Yuchen", ""]]}, {"id": "1804.10930", "submitter": "Tobias M\\\"omke", "authors": "Shilpa Garg and Tobias M\\\"omke", "title": "A QPTAS for Gapless MEC", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem Minimum Error Correction (MEC). A MEC instance is an\nn x m matrix M with entries from {0,1,-}. Feasible solutions are composed of\ntwo binary m-bit strings, together with an assignment of each row of M to one\nof the two strings. The objective is to minimize the number of mismatches\n(errors) where the row has a value that differs from the assigned solution\nstring. The symbol \"-\" is a wildcard that matches both 0 and 1. A MEC instance\nis gapless, if in each row of M all binary entries are consecutive.\n  Gapless-MEC is a relevant problem in computational biology, and it is closely\nrelated to segmentation problems that were introduced by\n[Kleinberg-Papadimitriou-Raghavan STOC'98] in the context of data mining.\n  Without restrictions, it is known to be UG-hard to compute an\nO(1)-approximate solution to MEC. For both MEC and Gapless-MEC, the best\npolynomial time approximation algorithm has a logarithmic performance\nguarantee. We partially settle the approximation status of Gapless-MEC by\nproviding a quasi-polynomial time approximation scheme (QPTAS). Additionally,\nfor the relevant case where the binary part of a row is not contained in the\nbinary part of another row, we provide a polynomial time approximation scheme\n(PTAS).\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 13:45:54 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Garg", "Shilpa", ""], ["M\u00f6mke", "Tobias", ""]]}, {"id": "1804.10947", "submitter": "Sumedha Uniyal", "authors": "Eyal Mizrachi, Roy Schwartz, Joachim Spoerhase, Sumedha Uniyal", "title": "A Tight Approximation for Submodular Maximization with Mixed Packing and\n  Covering Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in machine learning, such as subset selection and\ndata summarization, we consider the problem of maximizing a monotone submodular\nfunction subject to mixed packing and covering constraints. We present a tight\napproximation algorithm that for any constant $\\epsilon >0$ achieves a\nguarantee of $1-\\frac{1}{\\mathrm{e}}-\\epsilon$ while violating only the\ncovering constraints by a multiplicative factor of $1-\\epsilon$. Our algorithm\nis based on a novel enumeration method, which unlike previous known enumeration\ntechniques, can handle both packing and covering constraints. We extend the\nabove main result by additionally handling a matroid independence constraints\nas well as finding (approximate) pareto set optimal solutions when multiple\nsubmodular objectives are present. Finally, we propose a novel and purely\ncombinatorial dynamic programming approach that can be applied to several\nspecial cases of the problem yielding not only {\\em deterministic} but also\nconsiderably faster algorithms. For example, for the well studied special case\nof only packing constraints (Kulik {\\em et. al.} [Math. Oper. Res. `13] and\nChekuri {\\em et. al.} [FOCS `10]), we are able to present the first\ndeterministic non-trivial approximation algorithm. We believe our new\ncombinatorial approach might be of independent interest.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 14:51:11 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 18:06:46 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Mizrachi", "Eyal", ""], ["Schwartz", "Roy", ""], ["Spoerhase", "Joachim", ""], ["Uniyal", "Sumedha", ""]]}, {"id": "1804.10981", "submitter": "Suman Banerjee", "authors": "Suman Banerjee, Bithika Pal", "title": "On the Enumeration of Maximal $(\\Delta, \\gamma)$-Cliques of a Temporal\n  Network", "comments": "9 pages. Both the authors have done equal contributions in this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A temporal network is a mathematical way of precisely representing a time\nvarying relationship among a group of agents. In this paper, we introduce the\nnotion of $(\\Delta, \\gamma)$-Cliques of a temporal network, where every pair of\nvertices present in the clique communicates atleast $\\gamma$ times in each\n$\\Delta$ period within a given time duration. We present an algorithm for\nenumerating all such maximal cliques present in the network. We also implement\nthe proposed algorithm with three human contact network data sets. Based on the\nobtained results, we analyze the data set on multiple values of $\\Delta$ and\n$\\gamma$, which helps in finding out contact groups with different frequencies.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 19:39:57 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Banerjee", "Suman", ""], ["Pal", "Bithika", ""]]}, {"id": "1804.11086", "submitter": "Stefan Walzer", "authors": "Martin Dietzfelbinger, Philipp Schlag, Stefan Walzer", "title": "A Subquadratic Algorithm for 3XOR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $X$ of $n$ binary words of equal length $w$, the 3XOR problem\nasks for three elements $a, b, c \\in X$ such that $a \\oplus b=c$, where $\n\\oplus$ denotes the bitwise XOR operation. The problem can be easily solved on\na word RAM with word length $w$ in time $O(n^2 \\log{n})$. Using Han's fast\ninteger sorting algorithm (2002/2004) this can be reduced to $O(n^2\n\\log{\\log{n}})$. With randomization or a sophisticated deterministic dictionary\nconstruction, creating a hash table for $X$ with constant lookup time leads to\nan algorithm with (expected) running time $O(n^2)$. At present, seemingly no\nfaster algorithms are known. We present a surprisingly simple deterministic,\nquadratic time algorithm for 3XOR. Its core is a version of the Patricia trie\nfor $X$, which makes it possible to traverse the set $a \\oplus X$ in ascending\norder for arbitrary $a\\in \\{0, 1\\}^{w}$ in linear time.\n  Furthermore, we describe a randomized algorithm for 3XOR with expected\nrunning time $O(n^2\\cdot\\min\\{\\log^3{w}/w, (\\log\\log{n})^2/\\log^2 n\\})$. The\nalgorithm transfers techniques to our setting that were used by Baran, Demaine,\nand P\\u{a}tra\\c{s}cu (2005/2008) for solving the related int3SUM problem (the\nsame problem with integer addition in place of binary XOR) in expected time\n$o(n^2)$. As suggested by Jafargholi and Viola (2016), linear hash functions\nare employed. The latter authors also showed that assuming 3XOR needs expected\nrunning time $n^{2-o(1)}$ one can prove conditional lower bounds for triangle\nenumeration just as with 3SUM. We demonstrate that 3XOR can be reduced to other\nproblems as well, treating the examples offline SetDisjointness and offline\nSetIntersection, which were studied for 3SUM by Kopelowitz, Pettie, and Porat\n(2016).\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 09:04:20 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Dietzfelbinger", "Martin", ""], ["Schlag", "Philipp", ""], ["Walzer", "Stefan", ""]]}, {"id": "1804.11091", "submitter": "Tom\\'a\\v{s} Masa\\v{r}\\'ik", "authors": "Tereza Klimo\\v{s}ov\\'a, Josef Mal\\'ik, Tom\\'a\\v{s} Masa\\v{r}\\'ik, Jana\n  Novotn\\'a, Dani\\\"el Paulusma, Veronika Sl\\'ivov\\'a", "title": "Colouring $(P_r+P_s)$-Free Graphs", "comments": "20 pages, 6 figures. An extended abstract of this paper appeared in\n  the proceedings of ISAAC 2018", "journal-ref": "Algorithmica 82(7), 1833-1858 (2020)", "doi": "10.1007/s00453-020-00675-w", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The $k$-Colouring problem is to decide if the vertices of a graph can be\ncoloured with at most $k$ colours for a fixed integer $k$ such that no two\nadjacent vertices are coloured alike. If each vertex u must be assigned a\ncolour from a prescribed list $L(u) \\subseteq \\{1,\\cdots, k\\}$, then we obtain\nthe List $k$-Colouring problem. A graph $G$ is $H$-free if $G$ does not contain\n$H$ as an induced subgraph. We continue an extensive study into the complexity\nof these two problems for $H$-free graphs. The graph $P_r+P_s$ is the disjoint\nunion of the $r$-vertex path $P_r$ and the $s$-vertex path $P_s$. We prove that\nList $3$-Colouring is polynomial-time solvable for $(P_2+P_5)$-free graphs and\nfor $(P_3+P_4)$-free graphs. Combining our results with known results yields\ncomplete complexity classifications of $3$-Colouring and List $3$-Colouring on\n$H$-free graphs for all graphs $H$ up to seven vertices.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 09:14:49 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 15:20:16 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 01:26:35 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Klimo\u0161ov\u00e1", "Tereza", ""], ["Mal\u00edk", "Josef", ""], ["Masa\u0159\u00edk", "Tom\u00e1\u0161", ""], ["Novotn\u00e1", "Jana", ""], ["Paulusma", "Dani\u00ebl", ""], ["Sl\u00edvov\u00e1", "Veronika", ""]]}, {"id": "1804.11102", "submitter": "Van Bang Le", "authors": "Hoang-Oanh Le, Van Bang Le", "title": "A complexity dichotomy for Matching Cut in (bipartite) graphs of fixed\n  diameter", "comments": "To appear in Theoretical Computer Science", "journal-ref": null, "doi": "10.1016/j.tcs.2018.10.029", "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a graph, a matching cut is an edge cut that is a matching. Matching Cut is\nthe problem of deciding whether or not a given graph has a matching cut, which\nis known to be NP-complete even when restricted to bipartite graphs. It has\nbeen proved that Matching Cut is polynomially solvable for graphs of diameter\ntwo. In this paper, we show that, for any fixed integer $d\\ge 3$, Matching Cut\nis NP-complete in the class of graphs of diameter $d$. This resolves an open\nproblem posed by Borowiecki and Jesse-J\\'ozefczyk in [Matching cutsets in\ngraphs of diameter $2$, Theoretical Computer Science 407 (2008) 574-582].\n  We then show that, for any fixed integer $d\\ge 4$, Matching Cut is\nNP-complete even when restricted to the class of bipartite graphs of diameter\n$d$. Complementing the hardness results, we show that Matching Cut is\npolynomial-time solvable in the class of bipartite graphs of diameter at most\nthree, and point out a new and simple polynomial-time algorithm solving\nMatching Cut in graphs of diameter $2$.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 09:49:09 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 13:39:29 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Le", "Hoang-Oanh", ""], ["Le", "Van Bang", ""]]}, {"id": "1804.11181", "submitter": "Cristian Dumitrescu BSc.", "authors": "Cristian Dumitrescu", "title": "The clustered Sparrow algorithm", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study an extension of Schoning's algorithm [Schoning, 1991]\nfor 3SAT, the clustered Sparrow algorithm We also present strong arguments that\nthis algorithm is polynomial.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 13:35:28 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 13:26:35 GMT"}, {"version": "v3", "created": "Mon, 16 Jul 2018 13:59:38 GMT"}, {"version": "v4", "created": "Mon, 17 Sep 2018 14:40:55 GMT"}, {"version": "v5", "created": "Tue, 18 Sep 2018 13:34:15 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Dumitrescu", "Cristian", ""]]}]