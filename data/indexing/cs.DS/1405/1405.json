[{"id": "1405.0093", "submitter": "Rajesh Chitnis", "authors": "Rajesh Chitnis, Graham Cormode, MohammadTaghi Hajiaghayi and Morteza\n  Monemizadeh", "title": "Parameterized Streaming Algorithms for Vertex Cover", "comments": "Fixed some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As graphs continue to grow in size, we seek ways to effectively process such\ndata at scale. The model of streaming graph processing, in which a compact\nsummary is maintained as each edge insertion/deletion is observed, is an\nattractive one. However, few results are known for optimization problems over\nsuch dynamic graph streams.\n  In this paper, we introduce a new approach to handling graph streams, by\ninstead seeking solutions for the parameterized versions of these problems\nwhere we are given a parameter $k$ and the objective is to decide whether there\nis a solution bounded by $k$. By combining kernelization techniques with\nrandomized sketch structures, we obtain the first streaming algorithms for the\nparameterized versions of the Vertex Cover problem. We consider the following\nthree models for a graph stream on $n$ nodes:\n  1. The insertion-only model where the edges can only be added.\n  2. The dynamic model where edges can be both inserted and deleted.\n  3. The \\emph{promised} dynamic model where we are guaranteed that at each\ntimestamp there is a solution of size at most $k$.\n  In each of these three models we are able to design parameterized streaming\nalgorithms for the Vertex Cover problem. We are also able to show matching\nlower bound for the space complexity of our algorithms.\n  (Due to the arXiv limit of 1920 characters for abstract field, please see the\nabstract in the paper for detailed description of our results)\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 04:40:11 GMT"}, {"version": "v2", "created": "Wed, 23 Jul 2014 21:10:40 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Chitnis", "Rajesh", ""], ["Cormode", "Graham", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Monemizadeh", "Morteza", ""]]}, {"id": "1405.0170", "submitter": "Arnaud Casteigts", "authors": "Matthieu Barjon (LaBRI), Arnaud Casteigts (LaBRI), Serge Chaumette\n  (LaBRI), Colette Johnen (LaBRI), Yessin M. Neggaz (LaBRI)", "title": "Un algorithme de test pour la connexit\\'e temporelle des graphes\n  dynamiques de faible densit\\'e", "comments": null, "journal-ref": "ALGOTEL 2014 -- 16\\`emes Rencontres Francophones sur les Aspects\n  Algorithmiques des T\\'el\\'ecommunications, Le Bois-Plage-en-R\\'e : France\n  (2014)", "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of testing whether a dynamic graph is temporally\nconnected, i.e. a temporal path ({\\em journey}) exists between all pairs of\nvertices. We consider a discrete version of the problem, where the topology is\ngiven as an evolving graph $\\G=\\{G_1,G_2,...,G_{k}\\}$ in which only the set of\n(directed) edges varies. Two cases are studied, depending on whether a single\nedge or an unlimited number of edges can be crossed in a same $G_i$ (strict\njourneys {\\it vs} non-strict journeys). For strict journeys, two existing\nalgorithms designed for other problems can be adapted. However, we show that a\ndedicated approach achieves a better time complexity than one of these two\nalgorithms in all cases, and than the other one for those graphs whose density\nis low at any time (though arbitrary over time). The time complexity of our\nalgorithm is $O(k\\mu n)$, where $k=|\\G|$ is the number of time steps and\n$\\mu=max(|E_i|)$ is the maximum {\\em instant} density, to be contrasted with\n$m=|\\cup E_i|$, the {\\em cumulated} density. Indeed, it is not uncommon for a\nmobility scenario to satisfy, for instance, both $\\mu=o(n)$ and\n$m=\\Theta(n^2)$. We characterize the key values of $k, \\mu$ and $m$ for which\nour algorithm should be used. For non-strict journeys, for which no algorithm\nis known, we show that a similar strategy can be used to answer the question,\nstill in $O(k\\mu n)$ time.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 14:31:31 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Barjon", "Matthieu", "", "LaBRI"], ["Casteigts", "Arnaud", "", "LaBRI"], ["Chaumette", "Serge", "", "LaBRI"], ["Johnen", "Colette", "", "LaBRI"], ["Neggaz", "Yessin M.", "", "LaBRI"]]}, {"id": "1405.0189", "submitter": "Moshe Lewenstein", "authors": "Amihood Amir and Timothy Chan and Moshe Lewenstein and Noa Lewenstein", "title": "On Hardness of Jumbled Indexing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jumbled indexing is the problem of indexing a text $T$ for queries that ask\nwhether there is a substring of $T$ matching a pattern represented as a Parikh\nvector, i.e., the vector of frequency counts for each character. Jumbled\nindexing has garnered a lot of interest in the last four years. There is a\nnaive algorithm that preprocesses all answers in $O(n^2|\\Sigma|)$ time allowing\nquick queries afterwards, and there is another naive algorithm that requires no\npreprocessing but has $O(n\\log|\\Sigma|)$ query time. Despite a tremendous\namount of effort there has been little improvement over these running times.\n  In this paper we provide good reason for this. We show that, under a\n3SUM-hardness assumption, jumbled indexing for alphabets of size $\\omega(1)$\nrequires $\\Omega(n^{2-\\epsilon})$ preprocessing time or $\\Omega(n^{1-\\delta})$\nquery time for any $\\epsilon,\\delta>0$. In fact, under a stronger 3SUM-hardness\nassumption, for any constant alphabet size $r\\ge 3$ there exist describable\nfixed constant $\\epsilon_r$ and $\\delta_r$ such that jumbled indexing requires\n$\\Omega(n^{2-\\epsilon_r})$ preprocessing time or $\\Omega(n^{1-\\delta_r})$ query\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 15:30:37 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Amir", "Amihood", ""], ["Chan", "Timothy", ""], ["Lewenstein", "Moshe", ""], ["Lewenstein", "Noa", ""]]}, {"id": "1405.0329", "submitter": "Yixin Cao", "authors": "Yixin Cao, Luciano N. Grippo, Mart\\'in D. Safe", "title": "Forbidden Induced Subgraphs of Normal Helly Circular-Arc Graphs:\n  Characterization and Detection", "comments": "Preliminary results of this paper appeared in the proceedings of SBPO\n  2012 and FAW 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A normal Helly circular-arc graph is the intersection graph of arcs on a\ncircle of which no three or less arcs cover the whole circle. Lin, Soulignac,\nand Szwarcfiter [Discrete Appl. Math. 2013] characterized circular-arc graphs\nthat are not normal Helly circular-arc graphs, and used it to develop the first\nrecognition algorithm for this graph class. As open problems, they ask for the\nforbidden induced subgraph characterization and a direct recognition algorithm\nfor normal Helly circular-arc graphs, both of which are resolved by the current\npaper. Moreover, when the input is not a normal Helly circular-arc graph, our\nrecognition algorithm finds in linear time a minimal forbidden induced subgraph\nas certificate.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 01:03:56 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Cao", "Yixin", ""], ["Grippo", "Luciano N.", ""], ["Safe", "Mart\u00edn D.", ""]]}, {"id": "1405.0456", "submitter": "Tomasz Kociumaka", "authors": "Fabrizio Grandoni, Tomasz Kociumaka, Micha{\\l} W{\\l}odarczyk", "title": "An LP-Rounding $2\\sqrt{2}$ Approximation for Restricted Maximum Acyclic\n  Subgraph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical Maximum Acyclic Subgraph problem (MAS), given a\ndirected-edge weighted graph, we are required to find an ordering of the nodes\nthat maximizes the total weight of forward-directed edges. MAS admits a 2\napproximation, and this approximation is optimal under the Unique Game\nConjecture.\n  In this paper we consider a generalization of MAS, the Restricted Maximum\nAcyclic Subgraph problem (RMAS), where each node is associated with a list of\ninteger labels, and we have to find a labeling of the nodes so as to maximize\nthe weight of edges whose head label is larger than the tail label. The best\nknown (almost trivial) approximation for RMAS is 4.\n  The interest of RMAS is mostly due to its connections with the Vertex Pricing\nproblem (VP). In VP we are given an undirected graph with positive edge\nbudgets. A feasible solution consists of an assignment of non-negative prices\nto the nodes. The profit for each edge $e$ is the sum of its endpoints prices\nif that sum is at most the budget of $e$, and zero otherwise. Our goal is to\nmaximize the total profit. The best known approximation for VP, which works\nanalogously to the mentioned approximation algorithm for RMAS, is 4. Improving\non that is a challenging open problem. On the other hand, the best known 2\ninapproximability result is due to a reduction from a special case of RMAS.\n  In this paper we present an improved LP-rounding $2\\sqrt{2}$ approximation\nfor RMAS. Our result shows that, in order to prove a 4 hardness of\napproximation result for VP (if possible), one should consider reductions from\nharder problems. Alternatively, our approach might suggest a different way to\ndesign approximation algorithms for VP.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 17:18:53 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Grandoni", "Fabrizio", ""], ["Kociumaka", "Tomasz", ""], ["W\u0142odarczyk", "Micha\u0142", ""]]}, {"id": "1405.0527", "submitter": "Damien Woods", "authors": "Moya Chen, Doris Xin, Damien Woods", "title": "Parallel computation using active self-assembly", "comments": "Journal version to appear in Natural Computing. Earlier conference\n  version appeared at DNA19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.CC cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of the recently proposed nubot model of\nmolecular-scale self-assembly. The model generalises asynchronous cellular\nautomata to have non-local movement where large assemblies of molecules can be\npushed and pulled around, analogous to millions of molecular motors in animal\nmuscle effecting the rapid movement of macroscale arms and legs. We show that\nthe nubot model is capable of simulating Boolean circuits of polylogarithmic\ndepth and polynomial size, in only polylogarithmic expected time. In\ncomputational complexity terms, we show that any problem from the complexity\nclass NC is solvable in polylogarithmic expected time and polynomial workspace\nusing nubots.\n  Along the way, we give fast parallel nubot algorithms for a number of\nproblems including line growth, sorting, Boolean matrix multiplication and\nspace-bounded Turing machine simulation, all using a constant number of nubot\nstates (monomer types). Circuit depth is a well-studied notion of parallel\ntime, and our result implies that the nubot model is a highly parallel model of\ncomputation in a formal sense. Asynchronous cellular automata are not capable\nof this parallelism, and our result shows that adding a rigid-body movement\nprimitive to such a model, to get the nubot model, drastically increases\nparallel processing abilities.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 22:19:53 GMT"}, {"version": "v2", "created": "Fri, 5 Sep 2014 16:27:14 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["Chen", "Moya", ""], ["Xin", "Doris", ""], ["Woods", "Damien", ""]]}, {"id": "1405.0712", "submitter": "Ling Cheng", "authors": "Bo Cheng (Guangdong University of Foreign Studies) and Ling Cheng\n  (University of the Witwatersrand)", "title": "Single machine slack due-window assignment and scheduling of linear\n  time-dependent deteriorating jobs and a deteriorating maintenance activity", "comments": "Submitted - Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the slack due-window assignment model and study a\nsingle machine scheduling problem of linear time-dependent deteriorating jobs\nand a deteriorating maintenance activity. The cost for each job consists of\nfour components: earliness, tardiness, window location and window size. The\nobjective is to schedule the jobs and to assign the maintenance activity and\ndue-windows such that the total cost among all the jobs is minimized. A\npolynomial-time algorithm with the running time not exceeding $O(n^2logn)$ to\ngive a solution to this problem is introduced, where $n$ is the number of jobs.\n", "versions": [{"version": "v1", "created": "Sun, 4 May 2014 16:24:38 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Cheng", "Bo", "", "Guangdong University of Foreign Studies"], ["Cheng", "Ling", "", "University of the Witwatersrand"]]}, {"id": "1405.0740", "submitter": "Euiwoong Lee", "authors": "Euiwoong Lee", "title": "Hardness of Graph Pricing through Generalized Max-Dicut", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Graph Pricing problem is among the fundamental problems whose\napproximability is not well-understood. While there is a simple combinatorial\n1/4-approximation algorithm, the best hardness result remains at 1/2 assuming\nthe Unique Games Conjecture (UGC). We show that it is NP-hard to approximate\nwithin a factor better than 1/4 under the UGC, so that the simple combinatorial\nalgorithm might be the best possible. We also prove that for any $\\epsilon >\n0$, there exists $\\delta > 0$ such that the integrality gap of\n$n^{\\delta}$-rounds of the Sherali-Adams hierarchy of linear programming for\nGraph Pricing is at most 1/2 + $\\epsilon$.\n  This work is based on the effort to view the Graph Pricing problem as a\nConstraint Satisfaction Problem (CSP) simpler than the standard and complicated\nformulation. We propose the problem called Generalized Max-Dicut($T$), which\nhas a domain size $T + 1$ for every $T \\geq 1$. Generalized Max-Dicut(1) is\nwell-known Max-Dicut. There is an approximation-preserving reduction from\nGeneralized Max-Dicut on directed acyclic graphs (DAGs) to Graph Pricing, and\nboth our results are achieved through this reduction. Besides its connection to\nGraph Pricing, the hardness of Generalized Max-Dicut is interesting in its own\nright since in most arity two CSPs studied in the literature, SDP-based\nalgorithms perform better than LP-based or combinatorial algorithms --- for\nthis arity two CSP, a simple combinatorial algorithm does the best.\n", "versions": [{"version": "v1", "created": "Sun, 4 May 2014 20:39:43 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 02:29:47 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Lee", "Euiwoong", ""]]}, {"id": "1405.0762", "submitter": "Paul Accisano", "authors": "Paul Accisano and Alper \\\"Ung\\\"or", "title": "Finding a Curve in a Point Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $P$ be a polygonal curve in $\\mathbb{R}^D$ of length $n$, and $S$ be a\npoint set of size $k$. The Curve/Point Set Matching problem consists of finding\na polygonal curve $Q$ on $S$ such that its Fr\\'echet distance from $P$ is less\nthan a given $\\varepsilon$. In this paper, we consider this problem with the\nadded freedom to transform the input curve $P$ by translating it, rotating it,\nor applying an arbitrary affine transform. We present exact and approximation\nalgorithms for several variations of this problem.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 01:17:13 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Accisano", "Paul", ""], ["\u00dcng\u00f6r", "Alper", ""]]}, {"id": "1405.0789", "submitter": "Martin Skutella", "authors": "Martin Skutella", "title": "A note on the ring loading problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ring Loading Problem is an optimal routing problem arising in the\nplanning of optical communication networks which use bidirectional SONET rings.\nIn mathematical terms, it is an unsplittable multicommodity flow problem on\nundirected ring networks. We prove that any split routing solution to the Ring\nLoading Problem can be turned into an unsplittable solution while increasing\nthe load on any edge of the ring by no more than +(19/14)D, where D is the\nmaximum demand value. This improves upon a classical result of Schrijver,\nSeymour, and Winkler (1998) who obtained a slightly larger bound of +(3/2)D. We\nalso present an improved lower bound of +1.1 D (previously +1.01 D) on the best\npossible bound and disprove a famous long-standing conjecture of Schrijver et\nal. in this context.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 06:34:50 GMT"}, {"version": "v2", "created": "Tue, 3 Jun 2014 21:56:11 GMT"}, {"version": "v3", "created": "Tue, 19 Aug 2014 15:07:57 GMT"}], "update_date": "2014-08-20", "authors_parsed": [["Skutella", "Martin", ""]]}, {"id": "1405.0945", "submitter": "Konstantinos Georgiou", "authors": "Joseph Cheriyan, Zhihan Gao, Konstantinos Georgiou, Sahil Singla", "title": "On Integrality Ratios for Asymmetric TSP in the Sherali-Adams Hierarchy", "comments": "26 pages, 7 figures. An extended abstract of this work appeared in\n  the proceedings of the 40th International Colloquium on Automata, Languages,\n  and Programming ({ICALP} 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the ATSP (Asymmetric Traveling Salesman Problem), and our focus is\non negative results in the framework of the Sherali-Adams (SA) Lift and Project\nmethod.\n  Our main result pertains to the standard LP (linear programming) relaxation\nof ATSP, due to Dantzig, Fulkerson, and Johnson. For any fixed integer $t\\geq\n0$ and small $\\epsilon$, $0<\\epsilon\\ll{1}$, there exists a digraph $G$ on\n$\\nu=\\nu(t,\\epsilon)=O(t/\\epsilon)$ vertices such that the integrality ratio\nfor level~$t$ of the SA system starting with the standard LP on $G$ is $\\ge\n1+\\frac{1-\\epsilon}{2t+3} \\approx \\frac43, \\frac65, \\frac87, \\dots$. Thus, in\nterms of the input size, the result holds for any $t = 0,1,\\dots,\\Theta(\\nu)$\nlevels. Our key contribution is to identify a structural property of digraphs\nthat allows us to construct fractional feasible solutions for any level~$t$ of\nthe SA system starting from the standard~LP. Our hard instances are simple and\nsatisfy the structural property.\n  There is a further relaxation of the standard LP called the balanced LP, and\nour methods simplify considerably when the starting LP for the SA system is the\nbalanced~LP; in particular, the relevant structural property (of digraphs)\nsimplifies such that it is satisfied by the digraphs given by the well-known\nconstruction of Charikar, Goemans and Karloff (CGK). Consequently, the CGK\ndigraphs serve as hard instances, and we obtain an integrality ratio of $1\n+\\frac{1-\\epsilon}{t+1}$ for any level~$t$ of the SA system, where\n$0<\\epsilon\\ll{1}$ and the number of vertices is\n$\\nu(t,\\epsilon)=O((t/\\epsilon)^{(t/\\epsilon)})$.\n  Also, our results for the standard~LP extend to the Path-ATSP (find a min\ncost Hamiltonian dipath from a given source vertex to a given sink vertex).\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 16:18:01 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Cheriyan", "Joseph", ""], ["Gao", "Zhihan", ""], ["Georgiou", "Konstantinos", ""], ["Singla", "Sahil", ""]]}, {"id": "1405.1001", "submitter": "Theresa Migler-VonDollen", "authors": "Glencora Borradaile, Theresa Migler, Gordon Wilfong", "title": "Density decompositions of networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new topological descriptor of a network called the density\ndecomposition which is a partition of the nodes of a network into regions of\nuniform density. The decomposition we define is unique in the sense that a\ngiven network has exactly one density decomposition. The number of nodes in\neach partition defines a density distribution which we find is measurably\nsimilar to the degree distribution of given real networks (social, internet,\netc.) and measurably dissimilar in synthetic networks (preferential attachment,\nsmall world, etc.).\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 19:17:22 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 19:34:46 GMT"}, {"version": "v3", "created": "Fri, 15 Dec 2017 18:40:04 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Borradaile", "Glencora", ""], ["Migler", "Theresa", ""], ["Wilfong", "Gordon", ""]]}, {"id": "1405.1133", "submitter": "Ioana O. Bercea", "authors": "Ioana O. Bercea, Navin Goyal, David G. Harris, Aravind Srinivasan", "title": "On Computing Maximal Independent Sets of Hypergraphs in Parallel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether or not the problem of finding maximal independent sets (MIS) in\nhypergraphs is in (R)NC is one of the fundamental problems in the theory of\nparallel computing. Unlike the well-understood case of MIS in graphs, for the\nhypergraph problem, our knowledge is quite limited despite considerable work.\nIt is known that the problem is in \\emph{RNC} when the edges of the hypergraph\nhave constant size. For general hypergraphs with $n$ vertices and $m$ edges,\nthe fastest previously known algorithm works in time $O(\\sqrt{n})$ with\n$\\text{poly}(m,n)$ processors. In this paper we give an EREW PRAM algorithm\nthat works in time $n^{o(1)}$ with $\\text{poly}(m,n)$ processors on general\nhypergraphs satisfying $m \\leq n^{\\frac{\\log^{(2)}n}{8(\\log^{(3)}n)^2}}$, where\n$\\log^{(2)}n = \\log\\log n$ and $\\log^{(3)}n = \\log\\log\\log n$. Our algorithm is\nbased on a sampling idea that reduces the dimension of the hypergraph and\nemploys the algorithm for constant dimension hypergraphs as a subroutine.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 02:55:50 GMT"}, {"version": "v2", "created": "Tue, 12 Aug 2014 23:24:26 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Bercea", "Ioana O.", ""], ["Goyal", "Navin", ""], ["Harris", "David G.", ""], ["Srinivasan", "Aravind", ""]]}, {"id": "1405.1189", "submitter": "Shmuel Onn", "authors": "Shmuel Onn", "title": "The Huge Multiway Table Problem", "comments": null, "journal-ref": "Discrete Optimization, 14:72-77, 2014", "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deciding the existence of an $l\\times m\\times n$ integer threeway table with\ngiven line-sums is NP-complete already for fixed $l=3$, but is in P with both\n$l,m$ fixed. Here we consider {\\em huge} tables, where the variable dimension\n$n$ is encoded in {\\em binary}. Combining recent results on integer cones and\nGraver bases, we show that if the number of {\\em layer types} is fixed, then\nthe problem is in P, whereas if it is variable, then the problem is in NP\nintersect coNP. Our treatment goes through the more general class of $n$-fold\ninteger programming problems.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 08:34:44 GMT"}, {"version": "v2", "created": "Mon, 12 May 2014 15:36:06 GMT"}, {"version": "v3", "created": "Fri, 16 May 2014 08:10:07 GMT"}, {"version": "v4", "created": "Thu, 21 Aug 2014 11:17:24 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Onn", "Shmuel", ""]]}, {"id": "1405.1220", "submitter": "Alberto Ord\\'o\\~nez Pereira", "authors": "Francisco Claude, Gonzalo Navarro and Alberto Ord\\'o\\~nez", "title": "Efficient Compressed Wavelet Trees over Large Alphabets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The {\\em wavelet tree} is a flexible data structure that permits representing\nsequences $S[1,n]$ of symbols over an alphabet of size $\\sigma$, within\ncompressed space and supporting a wide range of operations on $S$. When\n$\\sigma$ is significant compared to $n$, current wavelet tree representations\nincur in noticeable space or time overheads. In this article we introduce the\n{\\em wavelet matrix}, an alternative representation for large alphabets that\nretains all the properties of wavelet trees but is significantly faster. We\nalso show how the wavelet matrix can be compressed up to the zero-order entropy\nof the sequence without sacrificing, and actually improving, its time\nperformance. Our experimental results show that the wavelet matrix outperforms\nall the wavelet tree variants along the space/time tradeoff map.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 10:36:07 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Claude", "Francisco", ""], ["Navarro", "Gonzalo", ""], ["Ord\u00f3\u00f1ez", "Alberto", ""]]}, {"id": "1405.1234", "submitter": "Abhishek Awasthi M.Sc.", "authors": "Abhishek Awasthi, J\\\"org L\\\"assig and Oliver Kramer", "title": "A Novel Approach to the Common Due-Date Problem on Single and Parallel\n  Machines", "comments": "Book Chapter 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel idea for the general case of the Common Due-Date\n(CDD) scheduling problem. The problem is about scheduling a certain number of\njobs on a single or parallel machines where all the jobs possess different\nprocessing times but a common due-date. The objective of the problem is to\nminimize the total penalty incurred due to earliness or tardiness of the job\ncompletions. This work presents exact polynomial algorithms for optimizing a\ngiven job sequence for single and identical parallel machines with the run-time\ncomplexities of $O(n \\log n)$ for both cases, where $n$ is the number of jobs.\nBesides, we show that our approach for the parallel machine case is also\nsuitable for non-identical parallel machines. We prove the optimality for the\nsingle machine case and the runtime complexities of both. Henceforth, we extend\nour approach to one particular dynamic case of the CDD and conclude the chapter\nwith our results for the benchmark instances provided in the OR-library.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 11:40:02 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Awasthi", "Abhishek", ""], ["L\u00e4ssig", "J\u00f6rg", ""], ["Kramer", "Oliver", ""]]}, {"id": "1405.1254", "submitter": "Sigal Oren", "authors": "Jon Kleinberg and Sigal Oren", "title": "Time-Inconsistent Planning: A Computational Problem in Behavioral\n  Economics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many settings, people exhibit behavior that is inconsistent across time\n--- we allocate a block of time to get work done and then procrastinate, or put\neffort into a project and then later fail to complete it. An active line of\nresearch in behavioral economics and related fields has developed and analyzed\nmodels for this type of time-inconsistent behavior.\n  Here we propose a graph-theoretic model of tasks and goals, in which\ndependencies among actions are represented by a directed graph, and a\ntime-inconsistent agent constructs a path through this graph. We first show how\ninstances of this path-finding problem on different input graphs can\nreconstruct a wide range of qualitative phenomena observed in the literature on\ntime-inconsistency, including procrastination, abandonment of long-range tasks,\nand the benefits of reduced sets of choices. We then explore a set of analyses\nthat quantify over the set of all graphs; among other results, we find that in\nany graph, there can be only polynomially many distinct forms of\ntime-inconsistent behavior; and any graph in which a time-inconsistent agent\nincurs significantly more cost than an optimal agent must contain a large\n\"procrastination\" structure as a minor. Finally, we use this graph-theoretic\nmodel to explore ways in which tasks can be designed to help motivate agents to\nreach designated goals.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 13:09:51 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Kleinberg", "Jon", ""], ["Oren", "Sigal", ""]]}, {"id": "1405.1298", "submitter": "Michael Zhou", "authors": "Michael X. Zhou", "title": "Classic Lagrangian may not be applicable to the traveling salesman\n  problem", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note, the dual problem for the traveling salesman problem is\nconstructed through the classic Lagrangian. The existence of optimality\nconditions is expressed as a corresponding inverse problem. A general 4-cities\ninstance is given, and the numerical experiment shows that the classic\nLagrangian may not be applicable to the traveling salesman problem.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 07:21:22 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Zhou", "Michael X.", ""]]}, {"id": "1405.1303", "submitter": "Alexander Barvinok", "authors": "Alexander Barvinok", "title": "Computing the permanent of (some) complex matrices", "comments": "12 pages, results extended to hafnians and multidimensional\n  permanents, minor improvements", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math-ph math.MP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic algorithm, which, for any given 0< epsilon < 1 and\nan nxn real or complex matrix A=(a_{ij}) such that | a_{ij}-1| < 0.19 for all\ni, j computes the permanent of A within relative error epsilon in n^{O(ln n -ln\nepsilon)} time. The method can be extended to computing hafnians and\nmultidimensional permanents.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 15:14:29 GMT"}, {"version": "v2", "created": "Tue, 24 Jun 2014 12:49:06 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Barvinok", "Alexander", ""]]}, {"id": "1405.1332", "submitter": "Rachel Ward", "authors": "Felix Krahmer and Rachel Ward", "title": "A unified framework for linear dimensionality reduction in L1", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA math.MG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a family of interpolation norms $\\| \\cdot \\|_{1,2,s}$ on $\\mathbb{R}^n$,\nwe provide a distribution over random matrices $\\Phi_s \\in \\mathbb{R}^{m \\times\nn}$ parametrized by sparsity level $s$ such that for a fixed set $X$ of $K$\npoints in $\\mathbb{R}^n$, if $m \\geq C s \\log(K)$ then with high probability,\n$\\frac{1}{2} \\| x \\|_{1,2,s} \\leq \\| \\Phi_s (x) \\|_1 \\leq 2 \\| x\\|_{1,2,s}$ for\nall $x\\in X$. Several existing results in the literature reduce to special\ncases of this result at different values of $s$: for $s=n$, $\\| x\\|_{1,2,n}\n\\equiv \\| x \\|_{1}$ and we recover that dimension reducing linear maps can\npreserve the $\\ell_1$-norm up to a distortion proportional to the dimension\nreduction factor, which is known to be the best possible such result. For\n$s=1$, $\\|x \\|_{1,2,1} \\equiv \\| x \\|_{2}$, and we recover an $\\ell_2 / \\ell_1$\nvariant of the Johnson-Lindenstrauss Lemma for Gaussian random matrices.\nFinally, if $x$ is $s$-sparse, then $\\| x \\|_{1,2,s} = \\| x \\|_1$ and we\nrecover that $s$-sparse vectors in $\\ell_1^n$ embed into $\\ell_1^{\\mathcal{O}(s\n\\log(n))}$ via sparse random matrix constructions.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 16:01:50 GMT"}, {"version": "v2", "created": "Wed, 11 Jun 2014 19:46:58 GMT"}, {"version": "v3", "created": "Fri, 15 Aug 2014 21:17:19 GMT"}, {"version": "v4", "created": "Tue, 20 Jan 2015 18:58:20 GMT"}, {"version": "v5", "created": "Mon, 1 Jun 2015 22:19:58 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Krahmer", "Felix", ""], ["Ward", "Rachel", ""]]}, {"id": "1405.1356", "submitter": "Stefan Fafianie", "authors": "Stefan Fafianie and Stefan Kratsch", "title": "Streaming Kernelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernelization is a formalization of preprocessing for combinatorially hard\nproblems. We modify the standard definition for kernelization, which allows any\npolynomial-time algorithm for the preprocessing, by requiring instead that the\npreprocessing runs in a streaming setting and uses\n$\\mathcal{O}(poly(k)\\log|x|)$ bits of memory on instances $(x,k)$. We obtain\nseveral results in this new setting, depending on the number of passes over the\ninput that such a streaming kernelization is allowed to make. Edge Dominating\nSet turns out as an interesting example because it has no single-pass\nkernelization but two passes over the input suffice to match the bounds of the\nbest standard kernelization.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 16:38:21 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Fafianie", "Stefan", ""], ["Kratsch", "Stefan", ""]]}, {"id": "1405.1374", "submitter": "Naman Agarwal", "authors": "Naman Agarwal and Guy Kindler and Alexandra Kolla and Luca Trevisan", "title": "Unique Games on the Hypercube", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the validity of the Unique Games Conjecture\nwhen the constraint graph is the boolean hypercube. We construct an almost\noptimal integrality gap instance on the Hypercube for the Goemans-Williamson\nsemidefinite program (SDP) for Max-2-LIN$(\\mathbb{Z}_2)$. We conjecture that\nadding triangle inequalities to the SDP provides a polynomial time algorithm to\nsolve Unique Games on the hypercube.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 22:18:05 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Agarwal", "Naman", ""], ["Kindler", "Guy", ""], ["Kolla", "Alexandra", ""], ["Trevisan", "Luca", ""]]}, {"id": "1405.1477", "submitter": "Charalampos Tsourakakis", "authors": "Charalampos E. Tsourakakis", "title": "A Novel Approach to Finding Near-Cliques: The Triangle-Densest Subgraph\n  Problem", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many graph mining applications rely on detecting subgraphs which are\nnear-cliques. There exists a dichotomy between the results in the existing work\nrelated to this problem: on the one hand the densest subgraph problem (DSP)\nwhich maximizes the average degree over all subgraphs is solvable in polynomial\ntime but for many networks fails to find subgraphs which are near-cliques. On\nthe other hand, formulations that are geared towards finding near-cliques are\nNP-hard and frequently inapproximable due to connections with the Maximum\nClique problem.\n  In this work, we propose a formulation which combines the best of both\nworlds: it is solvable in polynomial time and finds near-cliques when the DSP\nfails. Surprisingly, our formulation is a simple variation of the DSP.\nSpecifically, we define the triangle densest subgraph problem (TDSP): given\n$G(V,E)$, find a subset of vertices $S^*$ such that $\\tau(S^*)=\\max_{S\n\\subseteq V} \\frac{t(S)}{|S|}$, where $t(S)$ is the number of triangles induced\nby the set $S$. We provide various exact and approximation algorithms which the\nsolve the TDSP efficiently. Furthermore, we show how our algorithms adapt to\nthe more general problem of maximizing the $k$-clique average density. Finally,\nwe provide empirical evidence that the TDSP should be used whenever the output\nof the DSP fails to output a near-clique.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 00:28:23 GMT"}, {"version": "v2", "created": "Wed, 14 May 2014 19:28:36 GMT"}, {"version": "v3", "created": "Tue, 20 May 2014 21:50:07 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Tsourakakis", "Charalampos E.", ""]]}, {"id": "1405.1649", "submitter": "Danupon Nanongkai", "authors": "Shay Kutten, Danupon Nanongkai, Gopal Pandurangan, Peter Robinson", "title": "Distributed Symmetry Breaking in Hypergraphs", "comments": "Changes from the previous version: More references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fundamental local symmetry breaking problems such as Maximal Independent Set\n(MIS) and coloring have been recognized as important by the community, and\nstudied extensively in (standard) graphs. In particular, fast (i.e.,\nlogarithmic run time) randomized algorithms are well-established for MIS and\n$\\Delta +1$-coloring in both the LOCAL and CONGEST distributed computing\nmodels. On the other hand, comparatively much less is known on the complexity\nof distributed symmetry breaking in {\\em hypergraphs}. In particular, a key\nquestion is whether a fast (randomized) algorithm for MIS exists for\nhypergraphs.\n  In this paper, we study the distributed complexity of symmetry breaking in\nhypergraphs by presenting distributed randomized algorithms for a variety of\nfundamental problems under a natural distributed computing model for\nhypergraphs. We first show that MIS in hypergraphs (of arbitrary dimension) can\nbe solved in $O(\\log^2 n)$ rounds ($n$ is the number of nodes of the\nhypergraph) in the LOCAL model. We then present a key result of this paper ---\nan $O(\\Delta^{\\epsilon}\\text{polylog}(n))$-round hypergraph MIS algorithm in\nthe CONGEST model where $\\Delta$ is the maximum node degree of the hypergraph\nand $\\epsilon > 0$ is any arbitrarily small constant.\n  To demonstrate the usefulness of hypergraph MIS, we present applications of\nour hypergraph algorithm to solving problems in (standard) graphs. In\nparticular, the hypergraph MIS yields fast distributed algorithms for the {\\em\nbalanced minimal dominating set} problem (left open in Harris et al. [ICALP\n2013]) and the {\\em minimal connected dominating set problem}. We also present\ndistributed algorithms for coloring, maximal matching, and maximal clique in\nhypergraphs.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 15:48:36 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 14:22:05 GMT"}, {"version": "v3", "created": "Sun, 24 Aug 2014 16:03:50 GMT"}, {"version": "v4", "created": "Tue, 30 Sep 2014 08:30:10 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Kutten", "Shay", ""], ["Nanongkai", "Danupon", ""], ["Pandurangan", "Gopal", ""], ["Robinson", "Peter", ""]]}, {"id": "1405.1781", "submitter": "Arka Bhattacharya", "authors": "Arka Bhattacharya", "title": "Approximation Algorithms for the Asymmetric Traveling Salesman Problem :\n  Describing two recent methods", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper provides a description of the two recent approximation algorithms\nfor the Asymmetric Traveling Salesman Problem, giving the intuitive description\nof the works of Feige-Singh[1] and Asadpour et.al\\ [2].\\newline [1] improves\nthe previous $O(\\log n)$ approximation algorithm, by improving the constant\nfrom 0.84 to 0.66 and modifying the work of Kaplan et. al\\ [3] and also shows\nan efficient reduction from ATSPP to ATSP. Combining both the results, they\nfinally establish an approximation ratio of $\\left(\\frac{4}{3}+\\epsilon\n\\right)\\log n$ for ATSPP,\\ considering a small $\\epsilon>0$,\\ improving the\nwork of Chekuri and Pal.[4]\\newline Asadpour et.al, in their seminal work\\ [2],\ngives an $O\\left(\\frac{\\log n}{\\log \\log n}\\right)$ randomized algorithm for\nthe ATSP, by symmetrizing and modifying the solution of the Held-Karp\nrelaxation problem and then proving an exponential family distribution for\nprobabilistically constructing a maximum entropy spanning tree from a spanning\ntree polytope and then finally defining the thin-ness property and transforming\na thin spanning tree into an Eulerian walk.\\ The optimization methods used in\\\n[2] are quite elegant and the approximation ratio could further be improved, by\nmanipulating the thin-ness of the cuts.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 00:10:56 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Bhattacharya", "Arka", ""]]}, {"id": "1405.2300", "submitter": "Thomas Bl\\\"asius", "authors": "Thomas Bl\\\"asius and Guido Br\\\"uckner and Ignaz Rutter", "title": "Complexity of Higher-Degree Orthogonal Graph Embedding in the Kandinsky\n  Model", "comments": "39 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that finding orthogonal grid-embeddings of plane graphs (planar with\nfixed combinatorial embedding) with the minimum number of bends in the\nso-called Kandinsky model (which allows vertices of degree $> 4$) is\nNP-complete, thus solving a long-standing open problem. On the positive side,\nwe give an efficient algorithm for several restricted variants, such as graphs\nof bounded branch width and a subexponential exact algorithm for general plane\ngraphs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 12:39:11 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Br\u00fcckner", "Guido", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1405.2424", "submitter": "Florent Foucaud", "authors": "Florent Foucaud, George B. Mertzios, Reza Naserasr, Aline Parreau,\n  Petru Valicov", "title": "Identification, location-domination and metric dimension on interval and\n  permutation graphs. II. Algorithms and complexity", "comments": "22 pages, 9 figures. Some theorems have been restated and errors have\n  been corrected", "journal-ref": "Algorithmica 78(3):914-944 (2017)", "doi": "10.1007/s00453-016-0184-1", "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of finding optimal identifying codes, (open)\nlocating-dominating sets and resolving sets (denoted IDENTIFYING CODE, (OPEN)\nLOCATING-DOMINATING SET and METRIC DIMENSION) of an interval or a permutation\ngraph. In these problems, one asks to distinguish all vertices of a graph by a\nsubset of the vertices, using either the neighbourhood within the solution set\nor the distances to the solution vertices. Using a general reduction for this\nclass of problems, we prove that the decision problems associated to these four\nnotions are NP-complete, even for interval graphs of diameter $2$ and\npermutation graphs of diameter $2$. While IDENTIFYING CODE and (OPEN)\nLOCATING-DOMINATING SET are trivially fixed-parameter-tractable when\nparameterized by solution size, it is known that in the same setting METRIC\nDIMENSION is $W[2]$-hard. We show that for interval graphs, this\nparameterization of METRIC DIMENSION is fixed-parameter-tractable.\n", "versions": [{"version": "v1", "created": "Sat, 10 May 2014 11:50:05 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 16:34:42 GMT"}, {"version": "v3", "created": "Mon, 11 Jul 2016 14:17:20 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Foucaud", "Florent", ""], ["Mertzios", "George B.", ""], ["Naserasr", "Reza", ""], ["Parreau", "Aline", ""], ["Valicov", "Petru", ""]]}, {"id": "1405.2447", "submitter": "Marcin Wrochna", "authors": "Amer E. Mouawad and Naomi Nishimura and Venkatesh Raman and Marcin\n  Wrochna", "title": "Reconfiguration over tree decompositions", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vertex-subset graph problem $Q$ defines which subsets of the vertices of an\ninput graph are feasible solutions. The reconfiguration version of a\nvertex-subset problem $Q$ asks whether it is possible to transform one feasible\nsolution for $Q$ into another in at most $\\ell$ steps, where each step is a\nvertex addition or deletion, and each intermediate set is also a feasible\nsolution for $Q$ of size bounded by $k$. Motivated by recent results\nestablishing W[1]-hardness of the reconfiguration versions of most\nvertex-subset problems parameterized by $\\ell$, we investigate the complexity\nof such problems restricted to graphs of bounded treewidth. We show that the\nreconfiguration versions of most vertex-subset problems remain PSPACE-complete\non graphs of treewidth at most $t$ but are fixed-parameter tractable\nparameterized by $\\ell + t$ for all vertex-subset problems definable in monadic\nsecond-order logic (MSOL). To prove the latter result, we introduce a technique\nwhich allows us to circumvent cardinality constraints and define\nreconfiguration problems in MSOL.\n", "versions": [{"version": "v1", "created": "Sat, 10 May 2014 16:27:32 GMT"}, {"version": "v2", "created": "Mon, 29 Sep 2014 15:58:26 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Mouawad", "Amer E.", ""], ["Nishimura", "Naomi", ""], ["Raman", "Venkatesh", ""], ["Wrochna", "Marcin", ""]]}, {"id": "1405.2530", "submitter": "Yael Mordechai", "authors": "Dor Arad, Yael Mordechai and Hadas Shachnai", "title": "Tighter Bounds for Makespan Minimization on Unrelated Machines", "comments": "12 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1011.1168 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of scheduling $n$ jobs to minimize the makespan on\n$m$ unrelated machines, where job $j$ requires time $p_{ij}$ if processed on\nmachine $i$. A classic algorithm of Lenstra et al. yields the best known\napproximation ratio of $2$ for the problem. Improving this bound has been a\nprominent open problem for over two decades. In this paper we obtain a tighter\nbound for a wide subclass of instances which can be identified efficiently.\nSpecifically, we define the feasibility factor of a given instance as the\nminimum fraction of machines on which each job can be processed. We show that\nthere is a polynomial-time algorithm that, given values $L$ and $T$, and an\ninstance having a sufficiently large feasibility factor $h \\in (0,1]$, either\nproves that no schedule of mean machine completion time $L$ and makespan $T$\nexists, or else finds a schedule of makespan at most $T + L/h < 2T$. For the\nrestricted version of the problem, where for each job $j$ and machine $i$,\n$p_{ij} \\in \\{p_j, \\infty\\}$, we show that a simpler algorithm yields a better\nbound, thus improving for highly feasible instances the best known ratio of\n$33/17 + \\epsilon$, for any fixed $\\epsilon >0$, due to Svensson.\n", "versions": [{"version": "v1", "created": "Sun, 11 May 2014 13:03:20 GMT"}, {"version": "v2", "created": "Mon, 23 Jun 2014 12:33:48 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Arad", "Dor", ""], ["Mordechai", "Yael", ""], ["Shachnai", "Hadas", ""]]}, {"id": "1405.2571", "submitter": "Kazuya Haraguchi", "authors": "Kazuya Haraguchi", "title": "An Efficient Local Search for Partial Latin Square Extension Problem", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A partial Latin square (PLS) is a partial assignment of n symbols to an nxn\ngrid such that, in each row and in each column, each symbol appears at most\nonce. The partial Latin square extension problem is an NP-hard problem that\nasks for a largest extension of a given PLS. In this paper we propose an\nefficient local search for this problem. We focus on the local search such that\nthe neighborhood is defined by (p,q)-swap, i.e., removing exactly p symbols and\nthen assigning symbols to at most q empty cells. For p in {1,2,3}, our\nneighborhood search algorithm finds an improved solution or concludes that no\nsuch solution exists in O(n^{p+1}) time. We also propose a novel swap\noperation, Trellis-swap, which is a generalization of (1,q)-swap and\n(2,q)-swap. Our Trellis-neighborhood search algorithm takes O(n^{3.5}) time to\ndo the same thing. Using these neighborhood search algorithms, we design a\nprototype iterated local search algorithm and show its effectiveness in\ncomparison with state-of-the-art optimization solvers such as IBM ILOG CPLEX\nand LocalSolver.\n", "versions": [{"version": "v1", "created": "Sun, 11 May 2014 19:07:55 GMT"}, {"version": "v2", "created": "Fri, 7 Nov 2014 03:03:22 GMT"}, {"version": "v3", "created": "Tue, 27 Jan 2015 03:13:15 GMT"}, {"version": "v4", "created": "Thu, 5 Mar 2015 02:58:55 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Haraguchi", "Kazuya", ""]]}, {"id": "1405.2846", "submitter": "Ernst Berg", "authors": "Ernst D. Berg", "title": "Introduction to Dynamic Unary Encoding", "comments": "Seven pages of text, two pages of flow charts and two pages of data.\n  Introduces an encoding scheme and a mathematical object", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic unary encoding takes unary encoding to the next level. Every n-bit\nbinary string is an encoding of dynamic unary and every n-bit binary string is\nencodable by dynamic unary. By utilizing both forms of unary code and a single\nbit of parity information dynamic unary encoding partitions 2^n non-negative\nintegers into n sets of disjoint cycles of n-bit elements. These cycles have\nbeen employed as virtual data sets, binary transforms and as a mathematical\nobject. Characterization of both the cycles and of the cycle spectrum is given.\nExamples of encoding and decoding algorithms are given. Examples of other\nconstructs utilizing the principles of dynamic unary encoding are presented.\nThe cycle as a mathematical object is demonstrated.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 17:32:27 GMT"}, {"version": "v2", "created": "Sat, 17 May 2014 06:28:27 GMT"}, {"version": "v3", "created": "Wed, 21 May 2014 00:35:32 GMT"}, {"version": "v4", "created": "Sun, 8 Jun 2014 20:22:45 GMT"}, {"version": "v5", "created": "Mon, 16 Jun 2014 19:31:22 GMT"}, {"version": "v6", "created": "Sun, 7 Dec 2014 00:23:20 GMT"}, {"version": "v7", "created": "Mon, 15 Dec 2014 15:43:44 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Berg", "Ernst D.", ""]]}, {"id": "1405.2875", "submitter": "Chien-Ju Ho", "authors": "Chien-Ju Ho, Aleksandrs Slivkins, Jennifer Wortman Vaughan", "title": "Adaptive Contract Design for Crowdsourcing Markets: Bandit Algorithms\n  for Repeated Principal-Agent Problems", "comments": "This is the full version of a paper in the ACM Conference on\n  Economics and Computation (ACM-EC), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing markets have emerged as a popular platform for matching\navailable workers with tasks to complete. The payment for a particular task is\ntypically set by the task's requester, and may be adjusted based on the quality\nof the completed work, for example, through the use of \"bonus\" payments. In\nthis paper, we study the requester's problem of dynamically adjusting\nquality-contingent payments for tasks. We consider a multi-round version of the\nwell-known principal-agent model, whereby in each round a worker makes a\nstrategic choice of the effort level which is not directly observable by the\nrequester. In particular, our formulation significantly generalizes the\nbudget-free online task pricing problems studied in prior work.\n  We treat this problem as a multi-armed bandit problem, with each \"arm\"\nrepresenting a potential contract. To cope with the large (and in fact,\ninfinite) number of arms, we propose a new algorithm, AgnosticZooming, which\ndiscretizes the contract space into a finite number of regions, effectively\ntreating each region as a single arm. This discretization is adaptively\nrefined, so that more promising regions of the contract space are eventually\ndiscretized more finely. We analyze this algorithm, showing that it achieves\nregret sublinear in the time horizon and substantially improves over\nnon-adaptive discretization (which is the only competing approach in the\nliterature).\n  Our results advance the state of art on several different topics: the theory\nof crowdsourcing markets, principal-agent problems, multi-armed bandits, and\ndynamic pricing.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 18:52:28 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 04:21:07 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Ho", "Chien-Ju", ""], ["Slivkins", "Aleksandrs", ""], ["Vaughan", "Jennifer Wortman", ""]]}, {"id": "1405.2891", "submitter": "Robert Ganian", "authors": "Simone Bova, Robert Ganian, Stefan Szeider", "title": "Model Checking Existential Logic on Partially Ordered Sets", "comments": "accepted at CSL-LICS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of checking whether an existential sentence (that is, a\nfirst-order sentence in prefix form built using existential quantifiers and all\nBoolean connectives) is true in a finite partially ordered set (in short, a\nposet). A poset is a reflexive, antisymmetric, and transitive digraph. The\nproblem encompasses the fundamental embedding problem of finding an isomorphic\ncopy of a poset as an induced substructure of another poset.\n  Model checking existential logic is already NP-hard on a fixed poset; thus we\ninvestigate structural properties of posets yielding conditions for\nfixed-parameter tractability when the problem is parameterized by the sentence.\nWe identify width as a central structural property (the width of a poset is the\nmaximum size of a subset of pairwise incomparable elements); our main\nalgorithmic result is that model checking existential logic on classes of\nfinite posets of bounded width is fixed-parameter tractable. We observe a\nsimilar phenomenon in classical complexity, where we prove that the isomorphism\nproblem is polynomial-time tractable on classes of posets of bounded width;\nthis settles an open problem in order theory.\n  We surround our main algorithmic result with complexity results on less\nrestricted, natural neighboring classes of finite posets, establishing its\ntightness in this sense. We also relate our work with (and demonstrate its\nindependence of) fundamental fixed-parameter tractability results for model\nchecking on digraphs of bounded degree and bounded clique-width.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 12:19:26 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Bova", "Simone", ""], ["Ganian", "Robert", ""], ["Szeider", "Stefan", ""]]}, {"id": "1405.3739", "submitter": "Jonathan Schneider", "authors": "Erik Demaine, Nathan Pinsker, and Jon Schneider", "title": "Fast Dynamic Pointer Following via Link-Cut Trees", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of fast dynamic pointer following: given\na directed graph $G$ where each vertex has outdegree $1$, efficiently support\nthe operations of i) changing the outgoing edge of any vertex, and ii) find the\nvertex $k$ vertices `after' a given vertex. We exhibit a solution to this\nproblem based on link-cut trees that requires $O(\\lg n)$ time per operation,\nand prove that this is optimal in the cell-probe complexity model.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 04:37:03 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Demaine", "Erik", ""], ["Pinsker", "Nathan", ""], ["Schneider", "Jon", ""]]}, {"id": "1405.3817", "submitter": "Jesper W. Mikkelsen", "authors": "Lene M. Favrholdt and Jesper W. Mikkelsen", "title": "Online Edge Coloring of Paths and Trees with a Fixed Number of Colors", "comments": "Full paper to appear in Acta Informatica. A preliminary version\n  appeared in WAOA 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a version of online edge coloring, where the goal is to color as\nmany edges as possible using only a given number, $k$, of available colors. All\nof our results are with regard to competitive analysis. Previous attempts to\nidentify optimal algorithms for this problem have failed, even for bipartite\ngraphs. Thus, in this paper, we analyze even more restricted graph classes,\npaths and trees. For paths, we consider $k=2$, and for trees, we consider any\n$k \\geq 2$.\n  We prove that a natural greedy algorithm called First-Fit is optimal among\ndeterministic algorithms, on paths as well as trees. For paths, we give a\nrandomized algorithm, which is optimal and better than the best possible\ndeterministic algorithm. For trees, we prove that to obtain a better\ncompetitive ratio than First-Fit, the algorithm would have to be both\nrandomized and unfair (i.e., reject edges that could have been colored), and\neven such algorithms cannot be much better than First-Fit.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 12:09:34 GMT"}, {"version": "v2", "created": "Thu, 28 Aug 2014 19:15:23 GMT"}, {"version": "v3", "created": "Tue, 25 Oct 2016 11:43:42 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Favrholdt", "Lene M.", ""], ["Mikkelsen", "Jesper W.", ""]]}, {"id": "1405.4356", "submitter": "Sriram Pemmaraju", "authors": "James W. Hegeman, Sriram V. Pemmaraju", "title": "Lessons from the Congested Clique Applied to MapReduce", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main results of this paper are (I) a simulation algorithm which, under\nquite general constraints, transforms algorithms running on the Congested\nClique into algorithms running in the MapReduce model, and (II) a distributed\n$O(\\Delta)$-coloring algorithm running on the Congested Clique which has an\nexpected running time of (i) $O(1)$ rounds, if $\\Delta \\geq \\Theta(\\log^4 n)$;\nand (ii) $O(\\log \\log n)$ rounds otherwise. Applying the simulation theorem to\nthe Congested-Clique $O(\\Delta)$-coloring algorithm yields an $O(1)$-round\n$O(\\Delta)$-coloring algorithm in the MapReduce model.\n  Our simulation algorithm illustrates a natural correspondence between\nper-node bandwidth in the Congested Clique model and memory per machine in the\nMapReduce model. In the Congested Clique (and more generally, any network in\nthe $\\mathcal{CONGEST}$ model), the major impediment to constructing fast\nalgorithms is the $O(\\log n)$ restriction on message sizes. Similarly, in the\nMapReduce model, the combined restrictions on memory per machine and total\nsystem memory have a dominant effect on algorithm design. In showing a fairly\ngeneral simulation algorithm, we highlight the similarities and differences\nbetween these models.\n", "versions": [{"version": "v1", "created": "Sat, 17 May 2014 06:31:32 GMT"}, {"version": "v2", "created": "Thu, 19 Jun 2014 21:28:47 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Hegeman", "James W.", ""], ["Pemmaraju", "Sriram V.", ""]]}, {"id": "1405.4472", "submitter": "Holger Dell", "authors": "Holger Dell", "title": "AND-compression of NP-complete problems: Streamlined proof and minor\n  observations", "comments": "extended abstract appears in the Proceedings of the 9th International\n  Symposium on Parameterized and Exact Computation (IPEC 2014)", "journal-ref": "Algorithmica 75(2): 403-423 (2016)", "doi": "10.1007/s00453-015-0110-y 10.1007/978-3-319-13524-3_16", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drucker (2012) proved the following result: Unless the unlikely\ncomplexity-theoretic collapse coNP is in NP/poly occurs, there is no\nAND-compression for SAT. The result has implications for the compressibility\nand kernelizability of a whole range of NP-complete parameterized problems. We\npresent a streamlined proof of Drucker's theorem.\n  An AND-compression is a deterministic polynomial-time algorithm that maps a\nset of SAT-instances $x_1,\\dots,x_t$ to a single SAT-instance $y$ of size\npoly(max $|x_i|$) such that $y$ is satisfiable if and only if all $x_i$ are\nsatisfiable. The \"AND\" in the name stems from the fact that the predicate \"$y$\nis satisfiable\" can be written as the AND of all predicates \"$x_i$ is\nsatisfiable\". Drucker's result complements the result by Bodlaender et al.\n(2009) and Fortnow and Santhanam (2010), who proved the analogous statement for\nOR-compressions, and Drucker's proof not only subsumes that result but also\nextends it to randomized compression algorithms that are allowed to have a\ncertain probability of failure.\n  Drucker (2012) presented two proofs: The first uses information theory and\nthe minimax theorem from game theory, and the second is an elementary,\niterative proof that is not as general. In our proof, we realize the iterative\nstructure as a generalization of the arguments of Ko (1983) for P-selective\nsets, which use the fact that tournaments have dominating sets of logarithmic\nsize. We generalize this fact to hypergraph tournaments. Our proof achieves the\nfull generality of Drucker's theorem, avoids the minimax theorem, and restricts\nthe use of information theory to a single, intuitive lemma about the average\nnoise sensitivity of compressive maps. To prove this lemma, we use the same\ninformation-theoretic inequalities as Drucker.\n", "versions": [{"version": "v1", "created": "Sun, 18 May 2014 08:48:39 GMT"}, {"version": "v2", "created": "Tue, 23 Sep 2014 15:13:24 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Dell", "Holger", ""]]}, {"id": "1405.4534", "submitter": "Thenkarai Nagarajan Janakiraman", "authors": "Lakshmi Prabha S and T.N.Janakiraman", "title": "Polynomial-time Approximation Algorithm for finding Highly Comfortable\n  Team in any given Social Network", "comments": "The manuscript contains 38 pages and 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many indexes (measures or metrics) in Social Network Analysis\n(SNA), like density, cohesion, etc. In this paper, we define a new SNA index\ncalled \"comfortability\". One among the lack of many factors, which affect the\neffectiveness of a group, is \"comfortability\". So, comfortability is one of the\nimportant attributes (characteristics) for a successful team work. It is\nimportant to find a comfortable and successful team in any given social\nnetwork. In this paper, comfortable team, better comfortable team and highly\ncomfortable team of a social network are defined based on \\textbf{graph\ntheoretic concepts} and some of their structural properties are analyzed.\n  It is proved that forming better comfortable team or highly comfortable team\nin any connected network are NP-Complete using the concepts of domination in\ngraph theory. Next, we give a polynomial-time approximation algorithm for\nfinding such a highly comfortable team in any given network with performance\nratio O(\\ln \\Delta), where \\Delta is the maximum degree of a given network\n(graph). The time complexity of the algorithm is proved to be O(n^{3}), where n\nis the number of persons (vertices) in the network (graph). It is also proved\nthat our algorithm has reasonably reduced the dispersion rate.\n", "versions": [{"version": "v1", "created": "Sun, 18 May 2014 18:29:24 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["S", "Lakshmi Prabha", ""], ["Janakiraman", "T. N.", ""]]}, {"id": "1405.4892", "submitter": "Emanuele Giaquinta", "authors": "Sukhpal Singh Ghuman, Emanuele Giaquinta, Jorma Tarhio", "title": "Alternative Algorithms for Lyndon Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two variations of Duval's algorithm for computing the Lyndon\nfactorization of a word. The first algorithm is designed for the case of small\nalphabets and is able to skip a significant portion of the characters of the\nstring, for strings containing runs of the smallest character in the alphabet.\nExperimental results show that it is faster than Duval's original algorithm,\nmore than ten times in the case of long DNA strings. The second algorithm\ncomputes, given a run-length encoded string $R$ of length $\\rho$, the Lyndon\nfactorization of $R$ in $O(\\rho)$ time and constant space.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 20:54:56 GMT"}, {"version": "v2", "created": "Fri, 11 Jul 2014 13:07:58 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Ghuman", "Sukhpal Singh", ""], ["Giaquinta", "Emanuele", ""], ["Tarhio", "Jorma", ""]]}, {"id": "1405.5209", "submitter": "John Haslegrave", "authors": "John Haslegrave", "title": "Bounds on Herman's algorithm", "comments": "9 pages", "journal-ref": "Theoretical Computer Science 550 (2014)", "doi": "10.1016/j.tcs.2014.07.023", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herman's self-stabilisation algorithm allows a ring of $N$ processors having\nany odd number of tokens to reach a stable state where exactly one token\nremains. McIver and Morgan conjecture that the expected time taken for\nstabilisation is maximised when there are three equally-spaced tokens. We prove\nexact results on a related cost function, and obtain a bound on expected time\nwhich is very close to the conjectured bound.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 11:41:11 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Haslegrave", "John", ""]]}, {"id": "1405.5210", "submitter": "Bettina Klinz", "authors": "Ante \\'Custi\\'c and Bettina Klinz and Gerhard J. Woeginger", "title": "Planar 3-dimensional assignment problems with Monge-like cost arrays", "comments": "16 pages, appendix will follow in v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an $n\\times n\\times p$ cost array $C$ we consider the problem $p$-P3AP\nwhich consists in finding $p$ pairwise disjoint permutations\n$\\varphi_1,\\varphi_2,\\ldots,\\varphi_p$ of $\\{1,\\ldots,n\\}$ such that\n$\\sum_{k=1}^{p}\\sum_{i=1}^nc_{i\\varphi_k(i)k}$ is minimized. For the case $p=n$\nthe planar 3-dimensional assignment problem P3AP results.\n  Our main result concerns the $p$-P3AP on cost arrays $C$ that are layered\nMonge arrays. In a layered Monge array all $n\\times n$ matrices that result\nfrom fixing the third index $k$ are Monge matrices. We prove that the $p$-P3AP\nand the P3AP remain NP-hard for layered Monge arrays. Furthermore, we show that\nin the layered Monge case there always exists an optimal solution of the\n$p$-3PAP which can be represented as matrix with bandwidth $\\le 4p-3$. This\nstructural result allows us to provide a dynamic programming algorithm that\nsolves the $p$-P3AP in polynomial time on layered Monge arrays when $p$ is\nfixed.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 19:59:41 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["\u0106usti\u0107", "Ante", ""], ["Klinz", "Bettina", ""], ["Woeginger", "Gerhard J.", ""]]}, {"id": "1405.5371", "submitter": "Adam Kasperski", "authors": "Adam Kasperski, Pawel Zielinski", "title": "Single machine scheduling problems with uncertain parameters and the OWA\n  criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a class of single machine scheduling problems is discussed. It\nis assumed that job parameters, such as processing times, due dates, or weights\nare uncertain and their values are specified in the form of a discrete scenario\nset. The Ordered Weighted Averaging (OWA) aggregation operator is used to\nchoose an optimal schedule. The OWA operator generalizes traditional criteria\nin decision making under uncertainty, such as the maximum, average, median or\nHurwicz criterion. It also allows us to extend the robust approach to\nscheduling by taking into account various attitudes of decision makers towards\nthe risk. In this paper a general framework for solving single machine\nscheduling problems with the OWA criterion is proposed and some positive and\nnegative computational results for two basic single machine scheduling problems\nare provided.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 11:17:41 GMT"}, {"version": "v2", "created": "Thu, 27 Nov 2014 12:02:08 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Kasperski", "Adam", ""], ["Zielinski", "Pawel", ""]]}, {"id": "1405.5376", "submitter": "Adam Kasperski", "authors": "Adam Kasperski, Pawel Zielinski", "title": "Complexity of the robust weighted independent set problems on interval\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the max-min and min-max regret versions of the maximum\nweighted independent set problem on interval graphswith uncertain vertex\nweights. Both problems have been recently investigated by Nobibon and Leus\n(2014), who showed that they are NP-hard for two scenarios and strongly NP-hard\nif the number of scenarios is a part of the input. In this paper, new\ncomplexity and approximation results on the problems under consideration are\nprovided, which extend the ones previously obtained. Namely, for the discrete\nscenario uncertainty representation it is proven that if the number of\nscenarios $K$ is a part of the input, then the max-min version of the problem\nis not at all approximable. On the other hand, its min-max regret version is\napproximable within $K$ and not approximable within $O(\\log^{1-\\epsilon}K)$ for\nany $\\epsilon>0$ unless the problems in NP have quasi polynomial algorithms.\nFurthermore, for the interval uncertainty representation it is shown that the\nmin-max regret version is NP-hard and approximable within 2.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 11:31:08 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Kasperski", "Adam", ""], ["Zielinski", "Pawel", ""]]}, {"id": "1405.5381", "submitter": "Adam Kasperski", "authors": "Adam Kasperski, Adam Kurpisz, Pawel Zielinski", "title": "Approximability of the robust representatives selection problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper new complexity and approximation results on the robust versions\nof the representatives selection problem, under the scenario uncertainty\nrepresentation, are provided, which extend the results obtained in the recent\npapers by Dolgui and Kovalev (2012), and Deineko and Woeginger (2013). Namely,\nit is shown that if the number of scenarios is a part of input, then the\nmin-max (regret) representatives selection problem is not approximable within a\nratio of $O(\\log^{1-\\epsilon}K)$ for any $\\epsilon>0$, where $K$ is the number\nof scenarios, unless the problems in NP have quasi-polynomial time algorithms.\nAn approximation algorithm with an approximation ratio of $O(\\log K/ \\log \\log\nK)$ for the min-max version of the problem is also provided.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 11:36:42 GMT"}, {"version": "v2", "created": "Thu, 13 Nov 2014 05:15:11 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Kasperski", "Adam", ""], ["Kurpisz", "Adam", ""], ["Zielinski", "Pawel", ""]]}, {"id": "1405.5461", "submitter": "Justin Kopinsky", "authors": "Dan Alistarh, Justin Kopinsky, Alexander Matveev, Nir Shavit", "title": "The LevelArray: A Fast, Practical Long-Lived Renaming Algorithm", "comments": "ICDCS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The long-lived renaming problem appears in shared-memory systems where a set\nof threads need to register and deregister frequently from the computation,\nwhile concurrent operations scan the set of currently registered threads.\nInstances of this problem show up in concurrent implementations of\ntransactional memory, flat combining, thread barriers, and memory reclamation\nschemes for lock-free data structures. In this paper, we analyze a randomized\nsolution for long-lived renaming. The algorithmic technique we consider, called\nthe LevelArray, has previously been used for hashing and one-shot (single-use)\nrenaming. Our main contribu- tion is to prove that, in long-lived executions,\nwhere processes may register and deregister polynomially many times, the\ntechnique guarantees constant steps on average and O(log log n) steps with high\nprobability for registering, unit cost for deregistering, and O(n) steps for\ncollect queries, where n is an upper bound on the number of processes that may\nbe active at any point in time. We also show that the algorithm has the\nsurprising property that it is self-healing: under reasonable assumptions on\nthe schedule, operations running while the data structure is in a degraded\nstate implicitly help the data structure re-balance itself. This subtle\nmechanism obviates the need for expensive periodic rebuilding procedures. Our\nbenchmarks validate this approach, showing that, for typical use parameters,\nthe average number of steps a process takes to register is less than two and\nthe worst-case number of steps is bounded by six, even in executions with\nbillions of operations. We contrast this with other randomized implementations,\nwhose worst-case behavior we show to be unreliable, and with deterministic\nimplementations, whose cost is linear in n.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 15:57:58 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Alistarh", "Dan", ""], ["Kopinsky", "Justin", ""], ["Matveev", "Alexander", ""], ["Shavit", "Nir", ""]]}, {"id": "1405.5483", "submitter": "Szymon Grabowski", "authors": "Robert Susik, Szymon Grabowski, Kimmo Fredriksson", "title": "Multiple pattern matching revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical exact multiple string matching problem. Our\nsolution is based on $q$-grams combined with pattern superimposition,\nbit-parallelism and alphabet size reduction. We discuss the pros and cons of\nthe various alternatives of how to achieve best combination. Our method is\nclosely related to previous work by (Salmela et al., 2006). The experimental\nresults show that our method performs well on different alphabet sizes and that\nthey scale to large pattern sets.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 17:20:34 GMT"}, {"version": "v2", "created": "Thu, 22 May 2014 08:11:02 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Susik", "Robert", ""], ["Grabowski", "Szymon", ""], ["Fredriksson", "Kimmo", ""]]}, {"id": "1405.5572", "submitter": "Jianhang Gao", "authors": "Jianhang Gao, Qing Zhao, Anathram Swami", "title": "Minimum Information Dominating Set for Opinion Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inferring the opinions of a social network through\nstrategically sampling a minimum subset of nodes by exploiting correlations in\nnode opinions. We first introduce the concept of information dominating set\n(IDS). A subset of nodes in a given network is an IDS if knowing the opinions\nof nodes in this subset is sufficient to infer the opinion of the entire\nnetwork. We focus on two fundamental algorithmic problems: (i) given a subset\nof the network, how to determine whether it is an IDS; (ii) how to construct a\nminimum IDS. Assuming binary opinions and the local majority rule for opinion\ncorrelation, we show that the first problem is co-NP-complete and the second\nproblem is NP-hard in general networks. We then focus on networks with special\nstructures, in particular, acyclic networks. We show that in acyclic networks,\nboth problems admit linear-complexity solutions by establishing a connection\nbetween the IDS problems and the vertex cover problem. Our technique for\nestablishing the hardness of the IDS problems is based on a novel graph\ntransformation that transforms the IDS problems in a general network to that in\nan odd-degree network. This graph transformation technique not only gives an\napproximation algorithm to the IDS problems, but also provides a useful tool\nfor general studies related to the local majority rule. Besides opinion\nsampling for applications such as political polling and market survey, the\nconcept of IDS and the results obtained in this paper also find applications in\ndata compression and identifying critical nodes in information networks.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 23:29:02 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Gao", "Jianhang", ""], ["Zhao", "Qing", ""], ["Swami", "Anathram", ""]]}, {"id": "1405.5610", "submitter": "EPTCS", "authors": "Andreas Maletti (Universit\\\"at Leipzig), Daniel Quernheim\n  (Universit\\\"at Stuttgart)", "title": "Hyper-Minimization for Deterministic Weighted Tree Automata", "comments": "In Proceedings AFL 2014, arXiv:1405.5272", "journal-ref": "EPTCS 151, 2014, pp. 314-326", "doi": "10.4204/EPTCS.151.22", "report-no": null, "categories": "cs.FL cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyper-minimization is a state reduction technique that allows a finite change\nin the semantics. The theory for hyper-minimization of deterministic weighted\ntree automata is provided. The presence of weights slightly complicates the\nsituation in comparison to the unweighted case. In addition, the first\nhyper-minimization algorithm for deterministic weighted tree automata, weighted\nover commutative semifields, is provided together with some implementation\nremarks that enable an efficient implementation. In fact, the same run-time O(m\nlog n) as in the unweighted case is obtained, where m is the size of the\ndeterministic weighted tree automaton and n is its number of states.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 02:14:51 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Maletti", "Andreas", "", "Universit\u00e4t Leipzig"], ["Quernheim", "Daniel", "", "Universit\u00e4t Stuttgart"]]}, {"id": "1405.5613", "submitter": "Yuya Higashikawa", "authors": "Yuya Higashikawa, Mordecai J. Golin, and Naoki Katoh", "title": "Improved Algorithms for Multiple Sink Location Problems in Dynamic Path\n  Networks", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the k-sink location problem in dynamic path networks. In\nour model, a dynamic path network consists of an undirected path with positive\nedge lengths, uniform edge capacity, and positive vertex supplies. Here, each\nvertex supply corresponds to a set of evacuees. Then, the problem requires to\nfind the optimal location of $k$ sinks in a given path so that each evacuee is\nsent to one of k sinks. Let x denote a k-sink location. Under the optimal\nevacuation for a given x, there exists a (k-1)-dimensional vector d, called\n(k-1)-divider, such that each component represents the boundary dividing all\nevacuees between adjacent two sinks into two groups, i.e., all supplies in one\ngroup evacuate to the left sink and all supplies in the other group evacuate to\nthe right sink. Therefore, the goal is to find x and d which minimize the\nmaximum cost or the total cost, which are denoted by the minimax problem and\nthe minisum problem, respectively. We study the k-sink location problem in\ndynamic path networks with continuous model, and prove that the minimax problem\ncan be solved in O(kn) time and the minisum problem can be solved in O(n^2\nmin{k, 2^{sqrt{log k log log n}}}) time, where n is the number of vertices in\nthe given network. Note that these improve the previous results by [6].\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 02:31:04 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Higashikawa", "Yuya", ""], ["Golin", "Mordecai J.", ""], ["Katoh", "Naoki", ""]]}, {"id": "1405.5646", "submitter": "Christian Blum", "authors": "Christian Blum and Jos\\'e A. Lozano and Pedro Pinacho Davidson", "title": "Mathematical Programming Strategies for Solving the Minimum Common\n  String Partition Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum common string partition problem is an NP-hard combinatorial\noptimization problem with applications in computational biology. In this work\nwe propose the first integer linear programming model for solving this problem.\nMoreover, on the basis of the integer linear programming model we develop a\ndeterministic 2-phase heuristic which is applicable to larger problem\ninstances. The results show that provenly optimal solutions can be obtained for\nproblem instances of small and medium size from the literature by solving the\nproposed integer linear programming model with CPLEX. Furthermore, new\nbest-known solutions are obtained for all considered problem instances from the\nliterature. Concerning the heuristic, we were able to show that it outperforms\nheuristic competitors from the related literature.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 07:37:56 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Blum", "Christian", ""], ["Lozano", "Jos\u00e9 A.", ""], ["Davidson", "Pedro Pinacho", ""]]}, {"id": "1405.5754", "submitter": "Peter Schneider-Kamp", "authors": "Michael Codish and Lu\\'is Cruz-Filipe and Michael Frank and Peter\n  Schneider-Kamp", "title": "Twenty-Five Comparators is Optimal when Sorting Nine Inputs (and\n  Twenty-Nine for Ten)", "comments": "18 pages", "journal-ref": null, "doi": "10.1109/ICTAI.2014.36", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a computer-assisted non-existence proof of nine-input\nsorting networks consisting of 24 comparators, hence showing that the\n25-comparator sorting network found by Floyd in 1964 is optimal. As a\ncorollary, we obtain that the 29-comparator network found by Waksman in 1969 is\noptimal when sorting ten inputs.\n  This closes the two smallest open instances of the optimal size sorting\nnetwork problem, which have been open since the results of Floyd and Knuth from\n1966 proving optimality for sorting networks of up to eight inputs.\n  The proof involves a combination of two methodologies: one based on\nexploiting the abundance of symmetries in sorting networks, and the other,\nbased on an encoding of the problem to that of satisfiability of propositional\nlogic. We illustrate that, while each of these can single handed solve smaller\ninstances of the problem, it is their combination which leads to an efficient\nsolution for nine inputs.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 13:42:19 GMT"}, {"version": "v2", "created": "Thu, 29 May 2014 18:26:41 GMT"}, {"version": "v3", "created": "Tue, 24 Jun 2014 10:39:12 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Codish", "Michael", ""], ["Cruz-Filipe", "Lu\u00eds", ""], ["Frank", "Michael", ""], ["Schneider-Kamp", "Peter", ""]]}, {"id": "1405.5869", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search\n  (MIPS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first provably sublinear time algorithm for approximate\n\\emph{Maximum Inner Product Search} (MIPS). Our proposal is also the first\nhashing algorithm for searching with (un-normalized) inner product as the\nunderlying similarity measure. Finding hashing schemes for MIPS was considered\nhard. We formally show that the existing Locality Sensitive Hashing (LSH)\nframework is insufficient for solving MIPS, and then we extend the existing LSH\nframework to allow asymmetric hashing schemes. Our proposal is based on an\ninteresting mathematical phenomenon in which inner products, after independent\nasymmetric transformations, can be converted into the problem of approximate\nnear neighbor search. This key observation makes efficient sublinear hashing\nscheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, we\nprovide an explicit construction of provably fast hashing scheme for MIPS. The\nproposed construction and the extended LSH framework could be of independent\ntheoretical interest. Our proposed algorithm is simple and easy to implement.\nWe evaluate the method, for retrieving inner products, in the collaborative\nfiltering task of item recommendations on Netflix and Movielens datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 19:42:57 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1405.5873", "submitter": "Anastasios Kyrillidis", "authors": "Michail Vlachos and Nikolaos Freris and Anastasios Kyrillidis", "title": "Compressive Mining: Fast and Optimal Data Mining in the Compressed\n  Domain", "comments": "25 pages, 20 figures, accepted in VLDB", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world data typically contain repeated and periodic patterns. This\nsuggests that they can be effectively represented and compressed using only a\nfew coefficients of an appropriate basis (e.g., Fourier, Wavelets, etc.).\nHowever, distance estimation when the data are represented using different sets\nof coefficients is still a largely unexplored area. This work studies the\noptimization problems related to obtaining the \\emph{tightest} lower/upper\nbound on Euclidean distances when each data object is potentially compressed\nusing a different set of orthonormal coefficients. Our technique leads to\ntighter distance estimates, which translates into more accurate search,\nlearning and mining operations \\textit{directly} in the compressed domain.\n  We formulate the problem of estimating lower/upper distance bounds as an\noptimization problem. We establish the properties of optimal solutions, and\nleverage the theoretical analysis to develop a fast algorithm to obtain an\n\\emph{exact} solution to the problem. The suggested solution provides the\ntightest estimation of the $L_2$-norm or the correlation. We show that typical\ndata-analysis operations, such as k-NN search or k-Means clustering, can\noperate more accurately using the proposed compression and distance\nreconstruction technique. We compare it with many other prevalent compression\nand reconstruction techniques, including random projections and PCA-based\ntechniques. We highlight a surprising result, namely that when the data are\nhighly sparse in some basis, our technique may even outperform PCA-based\ncompression.\n  The contributions of this work are generic as our methodology is applicable\nto any sequential or high-dimensional data as well as to any orthogonal data\ntransformation used for the underlying data compression scheme.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 15:01:07 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Vlachos", "Michail", ""], ["Freris", "Nikolaos", ""], ["Kyrillidis", "Anastasios", ""]]}, {"id": "1405.5919", "submitter": "Szymon Grabowski", "authors": "Szymon Grabowski, Marcin Raniszewski", "title": "Two simple full-text indexes based on the suffix array", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two suffix array inspired full-text indexes. One, called SA-hash,\naugments the suffix array with a hash table to speed up pattern searches due to\nsignificantly narrowed search interval before the binary search phase. The\nother, called FBCSA, is a compact data structure, similar to M{\\\"a}kinen's\ncompact suffix array, but working on fixed sized blocks. Experiments on the\nPizza~\\&~Chili 200\\,MB datasets show that SA-hash is about 2--3 times faster in\npattern searches (counts) than the standard suffix array, for the price of\nrequiring $0.2n-1.1n$ bytes of extra space, where $n$ is the text length, and\nsetting a minimum pattern length. FBCSA is relatively fast in single cell\naccesses (a few times faster than related indexes at about the same or better\ncompression), but not competitive if many consecutive cells are to be\nextracted. Still, for the task of extracting, e.g., 10 successive cells its\ntime-space relation remains attractive.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 21:55:00 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 17:04:14 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Grabowski", "Szymon", ""], ["Raniszewski", "Marcin", ""]]}, {"id": "1405.5975", "submitter": "Jia Zhang", "authors": "Xiaoming Sun, Jia Zhang and Jialin Zhang", "title": "Solving Multi-choice Secretary Problem in Parallel: An Optimal\n  Observation-Selection Protocol", "comments": "This work is accepted by ISAAC 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical secretary problem investigates the question of how to hire the\nbest secretary from $n$ candidates who come in a uniformly random order. In\nthis work we investigate a parallel generalizations of this problem introduced\nby Feldman and Tennenholtz [14]. We call it shared $Q$-queue $J$-choice\n$K$-best secretary problem. In this problem, $n$ candidates are evenly\ndistributed into $Q$ queues, and instead of hiring the best one, the employer\nwants to hire $J$ candidates among the best $K$ persons. The $J$ quotas are\nshared by all queues. This problem is a generalized version of $J$-choice\n$K$-best problem which has been extensively studied and it has more practical\nvalue as it characterizes the parallel situation.\n  Although a few of works have been done about this generalization, to the best\nof our knowledge, no optimal deterministic protocol was known with general $Q$\nqueues. In this paper, we provide an optimal deterministic protocol for this\nproblem. The protocol is in the same style of the $1\\over e$-solution for the\nclassical secretary problem, but with multiple phases and adaptive criteria.\nOur protocol is very simple and efficient, and we show that several\ngeneralizations, such as the fractional $J$-choice $K$-best secretary problem\nand exclusive $Q$-queue $J$-choice $K$-best secretary problem, can be solved\noptimally by this protocol with slight modification and the latter one solves\nan open problem of Feldman and Tennenholtz [14].\n  In addition, we provide theoretical analysis for two typical cases, including\nthe 1-queue 1-choice $K$-best problem and the shared 2-queue 2-choice 2-best\nproblem. For the former, we prove a lower bound $1-O(\\frac{\\ln^2K}{K^2})$ of\nthe competitive ratio. For the latter, we show the optimal competitive ratio is\n$\\approx0.372$ while previously the best known result is 0.356 [14].\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 07:16:19 GMT"}, {"version": "v2", "created": "Wed, 17 Sep 2014 11:02:42 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Sun", "Xiaoming", ""], ["Zhang", "Jia", ""], ["Zhang", "Jialin", ""]]}, {"id": "1405.6347", "submitter": "Pawe{\\l} Kaftan", "authors": "Pawe{\\l} Kaftan", "title": "A more efficient way of finding Hamiltonian cycle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to find Hamiltonian cycle, algorithm should find edges that creates\na Hamiltonian cycle. Higher number of edges creates more possibilities to check\nto solve the problem. Algorithm rests on analysis of original graph and\nopposite graph to it. Algorithm can remove unnecessary edges from graph and\ntest when Hamiltonian cycle can't exist in graph. Algorithm prefers \"to think\nover\" which paths should be checked than check many wrong paths.\n", "versions": [{"version": "v1", "created": "Sat, 24 May 2014 21:41:28 GMT"}, {"version": "v2", "created": "Tue, 17 Jun 2014 08:19:12 GMT"}, {"version": "v3", "created": "Wed, 24 Sep 2014 19:53:45 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2015 19:59:07 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Kaftan", "Pawe\u0142", ""]]}, {"id": "1405.6503", "submitter": "J\\\"org Arndt", "authors": "J\\\"org Arndt", "title": "Subset-lex: did we miss an order?", "comments": "Two obvious errors corrected (indicated by \"Correction:\" in the LaTeX\n  source)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize a well-known algorithm for the generation of all subsets of a\nset in lexicographic order with respect to the sets as lists of elements\n(subset-lex order). We obtain algorithms for various combinatorial objects such\nas the subsets of a multiset, compositions and partitions represented as lists\nof parts, and for certain restricted growth strings. The algorithms are often\nloopless and require at most one extra variable for the computation of the next\nobject. The performance of the algorithms is very competitive even when not\nloopless. A Gray code corresponding to the subset-lex order and a Gray code for\ncompositions that was found during this work are described.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 08:48:00 GMT"}, {"version": "v2", "created": "Fri, 26 Dec 2014 16:38:38 GMT"}, {"version": "v3", "created": "Fri, 9 Jan 2015 14:35:15 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Arndt", "J\u00f6rg", ""]]}, {"id": "1405.6785", "submitter": "Panos P. Markopoulos", "authors": "Panos P. Markopoulos, George N. Karystinos, and Dimitris A. Pados", "title": "Optimal Algorithms for $L_1$-subspace Signal Processing", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2338077", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe ways to define and calculate $L_1$-norm signal subspaces which\nare less sensitive to outlying data than $L_2$-calculated subspaces. We start\nwith the computation of the $L_1$ maximum-projection principal component of a\ndata matrix containing $N$ signal samples of dimension $D$. We show that while\nthe general problem is formally NP-hard in asymptotically large $N$, $D$, the\ncase of engineering interest of fixed dimension $D$ and asymptotically large\nsample size $N$ is not. In particular, for the case where the sample size is\nless than the fixed dimension ($N<D$), we present in explicit form an optimal\nalgorithm of computational cost $2^N$. For the case $N \\geq D$, we present an\noptimal algorithm of complexity $\\mathcal O(N^D)$. We generalize to multiple\n$L_1$-max-projection components and present an explicit optimal $L_1$ subspace\ncalculation algorithm of complexity $\\mathcal O(N^{DK-K+1})$ where $K$ is the\ndesired number of $L_1$ principal components (subspace rank). We conclude with\nillustrations of $L_1$-subspace signal processing in the fields of data\ndimensionality reduction, direction-of-arrival estimation, and image\nconditioning/restoration.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 04:15:49 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Markopoulos", "Panos P.", ""], ["Karystinos", "George N.", ""], ["Pados", "Dimitris A.", ""]]}, {"id": "1405.6791", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman and Pravesh Kothari", "title": "Agnostic Learning of Disjunctions on Symmetric Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating and learning disjunctions (or\nequivalently, conjunctions) on symmetric distributions over $\\{0,1\\}^n$.\nSymmetric distributions are distributions whose PDF is invariant under any\npermutation of the variables. We give a simple proof that for every symmetric\ndistribution $\\mathcal{D}$, there exists a set of $n^{O(\\log{(1/\\epsilon)})}$\nfunctions $\\mathcal{S}$, such that for every disjunction $c$, there is function\n$p$, expressible as a linear combination of functions in $\\mathcal{S}$, such\nthat $p$ $\\epsilon$-approximates $c$ in $\\ell_1$ distance on $\\mathcal{D}$ or\n$\\mathbf{E}_{x \\sim \\mathcal{D}}[ |c(x)-p(x)|] \\leq \\epsilon$. This directly\ngives an agnostic learning algorithm for disjunctions on symmetric\ndistributions that runs in time $n^{O( \\log{(1/\\epsilon)})}$. The best known\nprevious bound is $n^{O(1/\\epsilon^4)}$ and follows from approximation of the\nmore general class of halfspaces (Wimmer, 2010). We also show that there exists\na symmetric distribution $\\mathcal{D}$, such that the minimum degree of a\npolynomial that $1/3$-approximates the disjunction of all $n$ variables is\n$\\ell_1$ distance on $\\mathcal{D}$ is $\\Omega( \\sqrt{n})$. Therefore the\nlearning result above cannot be achieved via $\\ell_1$-regression with a\npolynomial basis used in most other agnostic learning algorithms.\n  Our technique also gives a simple proof that for any product distribution\n$\\mathcal{D}$ and every disjunction $c$, there exists a polynomial $p$ of\ndegree $O(\\log{(1/\\epsilon)})$ such that $p$ $\\epsilon$-approximates $c$ in\n$\\ell_1$ distance on $\\mathcal{D}$. This was first proved by Blais et al.\n(2008) via a more involved argument.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 05:33:19 GMT"}, {"version": "v2", "created": "Mon, 25 May 2015 21:58:56 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Feldman", "Vitaly", ""], ["Kothari", "Pravesh", ""]]}, {"id": "1405.6802", "submitter": "Tony Guttmann", "authors": "Andrew R Conway and Anthony J Guttmann", "title": "On the growth rate of 1324-avoiding permutations", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an improved algorithm for counting the number of $1324$-avoiding\npermutations, resulting in 5 further terms of the generating function. We\nanalyse the known coefficients and find compelling evidence that unlike other\nclassical length-4 pattern-avoiding permutations, the generating function in\nthis case does not have an algebraic singularity. Rather, the number of\n1324-avoiding permutations of length $n$ behaves as $$B\\cdot \\mu^n \\cdot\n\\mu_1^{n^{\\sigma}} \\cdot n^g.$$ We estimate $\\mu=11.60 \\pm 0.01,$ $\\sigma=1/2,$\n$\\mu_1 = 0.0398 \\pm 0.0010,$ $g = -1.1 \\pm 0.2$ and $B =9.5 \\pm 1.0.$\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 06:15:16 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Conway", "Andrew R", ""], ["Guttmann", "Anthony J", ""]]}, {"id": "1405.6851", "submitter": "Kenya Ueno", "authors": "Kenya Ueno", "title": "Exact Algorithms for 0-1 Integer Programs with Linear Equality\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show $O(1.415^n)$-time and $O(1.190^n)$-space exact\nalgorithms for 0-1 integer programs where constraints are linear equalities and\ncoefficients are arbitrary real numbers. Our algorithms are quadratically\nfaster than exhaustive search and almost quadratically faster than an algorithm\nfor an inequality version of the problem by Impagliazzo, Lovett, Paturi and\nSchneider (arXiv:1401.5512), which motivated our work. Rather than improving\nthe time and space complexity, we advance to a simple direction as inclusion of\nmany NP-hard problems in terms of exact exponential algorithms. Specifically,\nwe extend our algorithms to linear optimization problems.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 09:55:45 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 11:56:33 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Ueno", "Kenya", ""]]}, {"id": "1405.6874", "submitter": "Sebastian Deorowicz", "authors": "Szymon Grabowski, Sebastian Deorowicz, {\\L}ukasz Roguski", "title": "Disk-based genome sequencing data compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: High-coverage sequencing data have significant, yet hard to\nexploit, redundancy. Most FASTQ compressors cannot efficiently compress the DNA\nstream of large datasets, since the redundancy between overlapping reads cannot\nbe easily captured in the (relatively small) main memory. More interesting\nsolutions for this problem are disk-based~(Yanovsky, 2011; Cox et al., 2012),\nwhere the better of these two, from Cox~{\\it et al.}~(2012), is based on the\nBurrows--Wheeler transform (BWT) and achieves 0.518 bits per base for a 134.0\nGb human genome sequencing collection with almost 45-fold coverage.\n  Results: We propose ORCOM (Overlapping Reads COmpression with Minimizers), a\ncompression algorithm dedicated to sequencing reads (DNA only). Our method\nmakes use of a conceptually simple and easily parallelizable idea of\nminimizers, to obtain 0.317 bits per base as the compression ratio, allowing to\nfit the 134.0 Gb dataset into only 5.31 GB of space.\n  Availability: http://sun.aei.polsl.pl/orcom under a free license.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 11:34:35 GMT"}, {"version": "v2", "created": "Thu, 18 Sep 2014 17:41:36 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Grabowski", "Szymon", ""], ["Deorowicz", "Sebastian", ""], ["Roguski", "\u0141ukasz", ""]]}, {"id": "1405.6920", "submitter": "Bogdan Dumitrescu", "authors": "Bogdan Dumitrescu", "title": "On the Relation Between the Randomized Extended Kaczmarz Algorithm and\n  Coordinate Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we compare the randomized extended Kaczmarz (EK) algorithm and\nrandomized coordinate descent (CD) for solving the full-rank overdetermined\nlinear least-squares problem and prove that CD needs less operations for\nsatisfying the same residual-related termination criteria. For the general\nleast-squares problems, we show that running first CD to compute the residual\nand then standard Kaczmarz on the resulting consistent system is more efficient\nthan EK.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 14:07:56 GMT"}, {"version": "v2", "created": "Sat, 30 Aug 2014 12:17:46 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Dumitrescu", "Bogdan", ""]]}, {"id": "1405.6987", "submitter": "Alessandro Checco", "authors": "Alessandro Checco, Douglas J. Leith", "title": "Fast, Responsive Decentralised Graph Colouring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve, in a fully decentralised way (\\ie with no message passing), the\nclassic problem of colouring a graph. We propose a novel algorithm that is\nautomatically responsive to topology changes, and we prove that it converges\nquickly to a proper colouring in $O(N\\log{N})$ time with high probability for\ngeneric graphs (and in $O(\\log{N})$ time if $\\Delta=O(1)$) when the number of\navailable colours is greater than $\\Delta$, the maximum degree of the graph.\n  We believe the proof techniques used in this work are of independent interest\nand provide new insight into the properties required to ensure fast convergence\nof decentralised algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 17:46:16 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2015 22:24:34 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 02:59:17 GMT"}, {"version": "v4", "created": "Sat, 2 Sep 2017 09:40:11 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Checco", "Alessandro", ""], ["Leith", "Douglas J.", ""]]}, {"id": "1405.7112", "submitter": "Peng Zhang", "authors": "Karl Wimmer, Yi Wu, Peng Zhang", "title": "Optimal query complexity for estimating the trace of a matrix", "comments": "full version of the paper in ICALP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an implicit $n\\times n$ matrix $A$ with oracle access $x^TA x$ for any\n$x\\in \\mathbb{R}^n$, we study the query complexity of randomized algorithms for\nestimating the trace of the matrix. This problem has many applications in\nquantum physics, machine learning, and pattern matching. Two metrics are\ncommonly used for evaluating the estimators: i) variance; ii) a high\nprobability multiplicative-approximation guarantee. Almost all the known\nestimators are of the form $\\frac{1}{k}\\sum_{i=1}^k x_i^T A x_i$ for $x_i\\in\n\\mathbb{R}^n$ being i.i.d. for some special distribution.\n  Our main results are summarized as follows. We give an exact characterization\nof the minimum variance unbiased estimator in the broad class of linear\nnonadaptive estimators (which subsumes all the existing known estimators). We\nalso consider the query complexity lower bounds for any (possibly nonlinear and\nadaptive) estimators: (1) We show that any estimator requires\n$\\Omega(1/\\epsilon)$ queries to have a guarantee of variance at most\n$\\epsilon$. (2) We show that any estimator requires\n$\\Omega(\\frac{1}{\\epsilon^2}\\log \\frac{1}{\\delta})$ queries to achieve a\n$(1\\pm\\epsilon)$-multiplicative approximation guarantee with probability at\nleast $1 - \\delta$. Both above lower bounds are asymptotically tight.\n  As a corollary, we also resolve a conjecture in the seminal work of Avron and\nToledo (Journal of the ACM 2011) regarding the sample complexity of the\nGaussian Estimator.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 03:29:16 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Wimmer", "Karl", ""], ["Wu", "Yi", ""], ["Zhang", "Peng", ""]]}, {"id": "1405.7192", "submitter": "Toby Walsh", "authors": "Toby Walsh", "title": "The PeerRank Method for Peer Assessment", "comments": "To appear in Proc. of ECAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the PeerRank method for peer assessment. This constructs a grade\nfor an agent based on the grades proposed by the agents evaluating the agent.\nSince the grade of an agent is a measure of their ability to grade correctly,\nthe PeerRank method weights grades by the grades of the grading agent. The\nPeerRank method also provides an incentive for agents to grade correctly. As\nthe grades of an agent depend on the grades of the grading agents, and as these\ngrades themselves depend on the grades of other agents, we define the PeerRank\nmethod by a fixed point equation similar to the PageRank method for ranking\nweb-pages. We identify some formal properties of the PeerRank method (for\nexample, it satisfies axioms of unanimity, no dummy, no discrimination and\nsymmetry), discuss some examples, compare with related work and evaluate the\nperformance on some synthetic data. Our results show considerable promise,\nreducing the error in grade predictions by a factor of 2 or more in many cases\nover the natural baseline of averaging peer grades.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 10:51:57 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Walsh", "Toby", ""]]}, {"id": "1405.7300", "submitter": "Calvin Newport", "authors": "Calvin Newport", "title": "Radio Network Lower Bounds Made Easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theoreticians have studied distributed algorithms in the radio network model\nfor close to three decades. A significant fraction of this work focuses on\nlower bounds for basic communication problems such as wake-up (symmetry\nbreaking among an unknown set of nodes) and broadcast (message dissemination\nthrough an unknown network topology). In this paper, we introduce a new\ntechnique for proving this type of bound, based on reduction from a\nprobabilistic hitting game, that simplifies and strengthens much of this\nexisting work. In more detail, in this single paper we prove new expected time\nand high probability lower bounds for wake-up and global broadcast in single\nand multichannel versions of the radio network model both with and without\ncollision detection. In doing so, we are able to reproduce results that\npreviously spanned a half-dozen papers published over a period of twenty-five\nyears. In addition to simplifying these existing results, our technique, in\nmany places, also improves the state of the art: of the eight bounds we prove,\nfour strictly strengthen the best known previous result (in terms of time\ncomplexity and/or generality of the algorithm class for which it holds), and\nthree provide the first known non-trivial bound for the case in question. The\nfact that the same technique can easily generate this diverse collection of\nlower bounds indicates a surprising unity underlying communication tasks in the\nradio network model---revealing that deep down, below the specifics of the\nproblem definition and model assumptions, communication in this setting reduces\nto finding efficient strategies for a simple game.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 16:33:13 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Newport", "Calvin", ""]]}, {"id": "1405.7497", "submitter": "Gianluca Della Vedova", "authors": "Paola Bonizzoni, Anna Paola Carrieri, Gianluca Della Vedova, Gabriella\n  Trucco", "title": "Algorithms for the Constrained Perfect Phylogeny with Persistent\n  Characters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The perfect phylogeny is one of the most used models in different areas of\ncomputational biology. In this paper we consider the problem of the Persistent\nPerfect Phylogeny (referred as P-PP) recently introduced to extend the perfect\nphylogeny model allowing persistent characters, that is characters can be\ngained and lost at most once. We define a natural generalization of the P-PP\nproblem obtained by requiring that for some pairs (character, species), neither\nthe species nor any of its ancestors can have the character. In other words,\nsome characters cannot be persistent for some species. This new problem is\ncalled Constrained P-PP (CP-PP).\n  Based on a graph formulation of the CP-PP problem, we are able to provide a\npolynomial time solution for the CP-PP problem for matrices having an empty\nconflict-graph. In particular we show that all such matrices admit a persistent\nperfect phylogeny in the unconstrained case. Using this result, we develop a\nparameterized algorithm for solving the CP-PP problem where the parameter is\nthe number of characters. A preliminary experimental analysis of the algorithm\nshows that it performs efficiently and it may analyze real haplotype data not\nconforming to the classical perfect phylogeny model.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 08:45:27 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Bonizzoni", "Paola", ""], ["Carrieri", "Anna Paola", ""], ["Della Vedova", "Gianluca", ""], ["Trucco", "Gabriella", ""]]}, {"id": "1405.7520", "submitter": "Gianluca Della Vedova", "authors": "Paola Bonizzoni, Gianluca Della Vedova, Yuri Pirola, Marco Previtali,\n  Raffaella Rizzi", "title": "An External-Memory Algorithm for String Graph Construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some recent results have introduced external-memory algorithms to compute\nself-indexes of a set of strings, mainly via computing the Burrows-Wheeler\nTransform (BWT) of the input strings. The motivations for those results stem\nfrom Bioinformatics, where a large number of short strings (called reads) are\nroutinely produced and analyzed. In that field, a fundamental problem is to\nassemble a genome from a large set of much shorter samples extracted from the\nunknown genome. The approaches that are currently used to tackle this problem\nare memory-intensive. This fact does not bode well with the ongoing increase in\nthe availability of genomic data. A data structure that is used in genome\nassembly is the string graph, where vertices correspond to samples and arcs\nrepresent two overlapping samples. In this paper we address an open problem: to\ndesign an external-memory algorithm to compute the string graph.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 11:09:55 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2015 15:08:26 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Bonizzoni", "Paola", ""], ["Della Vedova", "Gianluca", ""], ["Pirola", "Yuri", ""], ["Previtali", "Marco", ""], ["Rizzi", "Raffaella", ""]]}, {"id": "1405.7619", "submitter": "David B. Wilson", "authors": "David B. Wilson and Uri Zwick", "title": "A forward-backward single-source shortest paths algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new forward-backward variant of Dijkstra's and Spira's\nSingle-Source Shortest Paths (SSSP) algorithms. While essentially all SSSP\nalgorithm only scan edges forward, the new algorithm scans some edges backward.\nThe new algorithm assumes that edges in the outgoing and incoming adjacency\nlists of the vertices appear in non-decreasing order of weight. (Spira's\nalgorithm makes the same assumption about the outgoing adjacency lists, but\ndoes not use incoming adjacency lists.) The running time of the algorithm on a\ncomplete directed graph on $n$ vertices with independent exponential edge\nweights is $O(n)$, with very high probability. This improves on the previously\nbest result of $O(n\\log n)$, which is best possible if only forward scans are\nallowed, exhibiting an interesting separation between forward-only and\nforward-backward SSSP algorithms. As a consequence, we also get a new all-pairs\nshortest paths algorithm. The expected running time of the algorithm on\ncomplete graphs with independent exponential edge weights is $O(n^2)$, matching\na recent algorithm of Demetrescu and Italiano as analyzed by Peres et al.\nFurthermore, the probability that the new algorithm requires more than $O(n^2)$\ntime is exponentially small, improving on the $O(n^{-1/26})$ probability bound\nobtained by Peres et al.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 17:13:24 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Wilson", "David B.", ""], ["Zwick", "Uri", ""]]}, {"id": "1405.7859", "submitter": "Yixin Cao", "authors": "Yixin Cao and D\\'aniel Marx", "title": "Chordal Editing is Fixed-Parameter Tractable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph modification problems are typically asked as follows: is there a small\nset of operations that transforms a given graph to have a certain property. The\nmost commonly considered operations include vertex deletion, edge deletion, and\nedge addition; for the same property, one can define significantly different\nversions by allowing different operations. We study a very general graph\nmodification problem which allows all three types of operations: given a graph\n$G$ and integers $k_1$, $k_2$, and $k_3$, the \\textsc{chordal editing} problem\nasks whether $G$ can be transformed into a chordal graph by at most $k_1$\nvertex deletions, $k_2$ edge deletions, and $k_3$ edge additions. Clearly, this\nproblem generalizes both \\textsc{chordal vertex/edge deletion} and\n\\textsc{chordal completion} (also known as \\textsc{minimum fill-in}). Our main\nresult is an algorithm for \\textsc{chordal editing} in time $2^{O(k\\log\nk)}\\cdot n^{O(1)}$, where $k:=k_1+k_2+k_3$ and $n$ is the number of vertices of\n$G$. Therefore, the problem is fixed-parameter tractable parameterized by the\ntotal number of allowed operations. Our algorithm is both more efficient and\nconceptually simpler than the previously known algorithm for the special case\n\\textsc{chordal deletion}.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 13:32:18 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Cao", "Yixin", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1405.7910", "submitter": "Christos Boutsidis", "authors": "Christos Boutsidis and David P. Woodruff", "title": "Optimal CUR Matrix Decompositions", "comments": "small revision in lemma 4.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CUR decomposition of an $m \\times n$ matrix $A$ finds an $m \\times c$\nmatrix $C$ with a subset of $c < n$ columns of $A,$ together with an $r \\times\nn$ matrix $R$ with a subset of $r < m$ rows of $A,$ as well as a $c \\times r$\nlow-rank matrix $U$ such that the matrix $C U R$ approximates the matrix $A,$\nthat is, $ || A - CUR ||_F^2 \\le (1+\\epsilon) || A - A_k||_F^2$, where\n$||.||_F$ denotes the Frobenius norm and $A_k$ is the best $m \\times n$ matrix\nof rank $k$ constructed via the SVD. We present input-sparsity-time and\ndeterministic algorithms for constructing such a CUR decomposition where\n$c=O(k/\\epsilon)$ and $r=O(k/\\epsilon)$ and rank$(U) = k$. Up to constant\nfactors, our algorithms are simultaneously optimal in $c, r,$ and rank$(U)$.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 16:44:06 GMT"}, {"version": "v2", "created": "Wed, 16 Jul 2014 14:53:44 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Boutsidis", "Christos", ""], ["Woodruff", "David P.", ""]]}]