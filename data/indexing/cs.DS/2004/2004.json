[{"id": "2004.00010", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne, Gautam Kamath, Thomas Steinke", "title": "The Discrete Gaussian for Differential Privacy", "comments": "Improved time analysis, and generalisation to the multivariate case", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key tool for building differentially private systems is adding Gaussian\nnoise to the output of a function evaluated on a sensitive dataset.\nUnfortunately, using a continuous distribution presents several practical\nchallenges. First and foremost, finite computers cannot exactly represent\nsamples from continuous distributions, and previous work has demonstrated that\nseemingly innocuous numerical errors can entirely destroy privacy. Moreover,\nwhen the underlying data is itself discrete (e.g., population counts), adding\ncontinuous noise makes the result less interpretable.\n  With these shortcomings in mind, we introduce and analyze the discrete\nGaussian in the context of differential privacy. Specifically, we theoretically\nand experimentally show that adding discrete Gaussian noise provides\nessentially the same privacy and accuracy guarantees as the addition of\ncontinuous Gaussian noise. We also present an simple and efficient algorithm\nfor exact sampling from this distribution. This demonstrates its applicability\nfor privately answering counting queries, or more generally, low-sensitivity\ninteger-valued queries.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 18:00:00 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 16:47:40 GMT"}, {"version": "v3", "created": "Sat, 30 May 2020 19:29:06 GMT"}, {"version": "v4", "created": "Fri, 11 Sep 2020 12:39:15 GMT"}, {"version": "v5", "created": "Mon, 18 Jan 2021 23:30:49 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["Kamath", "Gautam", ""], ["Steinke", "Thomas", ""]]}, {"id": "2004.00518", "submitter": "Mehrnoosh Shafiee", "authors": "Mehrnoosh Shafiee and Javad Ghaderi", "title": "Scheduling Parallel-Task Jobs Subject to Packing and Placement\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by modern parallel computing applications, we consider the problem\nof scheduling parallel-task jobs with heterogeneous resource requirements in a\ncluster of machines. Each job consists of a set of tasks that can be processed\nin parallel, however, the job is considered completed only when all its tasks\nfinish their processing, which we refer to as \"synchronization\" constraint.\nFurther, assignment of tasks to machines is subject to \"placement\" constraints,\ni.e., each task can be processed only on a subset of machines, and processing\ntimes can also be machine dependent. Once a task is scheduled on a machine, it\nrequires a certain amount of resource from that machine for the duration of its\nprocessing. A machine can process (\"pack\") multiple tasks at the same time,\nhowever the cumulative resource requirement of the tasks should not exceed the\nmachine's capacity.\n  Our objective is to minimize the weighted average of the jobs' completion\ntimes. The problem, subject to synchronization, packing and placement\nconstraints, is NP-hard, and prior theoretical results only concern much\nsimpler models. For the case that migration of tasks among the\nplacement-feasible machines is allowed, we propose a preemptive algorithm with\nan approximation ratio of $(6+\\epsilon)$. In the special case that only one\nmachine can process each task, we design an algorithm with improved\napproximation ratio of $4$. Finally, in the case that migrations (and\npreemptions) are not allowed, we design an algorithm with an approximation\nratio of $24$. Our algorithms use a combination of linear program relaxation\nand greedy packing techniques. We present extensive simulation results, using a\nreal traffic trace, that demonstrate that our algorithms yield significant\ngains over the prior approaches.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 15:42:30 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 01:34:46 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Shafiee", "Mehrnoosh", ""], ["Ghaderi", "Javad", ""]]}, {"id": "2004.00547", "submitter": "Dimitrios Thilikos", "authors": "Guillaume Mescoff, Christophe Paul and Dimitrios Thilikos", "title": "A polynomial time algorithm to compute the connected tree-width of a\n  series-parallel graph", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the treewidth of a graph $G$ corresponds to the node\nsearch number where a team of cops is pursuing a robber that is lazy, visible\nand has the ability to move at infinite speed via unguarded path. In recent\npapers, connected node search strategies have been considered. A search\nstratregy is connected if at each step the set of vertices that is or has been\noccupied by the team of cops, induced a connected subgraph of $G$. It has been\nshown that the connected search number of a graph $G$ can be expressed as the\nconnected treewidth, denoted $\\mathbf{ctw}(G),$ that is defined as the minimum\nwidth of a rooted tree-decomposition $({{\\cal X},T,r})$ such that the union of\nthe bags corresponding to the nodes of a path of $T$ containing the root $r$ is\nconnected. Clearly we have that $\\mathbf{tw}(G)\\leqslant \\mathbf{ctw}(G)$. It\nis paper, we initiate the algorithmic study of connected treewidth. We design a\n$O(n^2\\cdot\\log n)$-time dynamic programming algorithm to compute the connected\ntreewidth of a biconnected series-parallel graphs. At the price of an extra $n$\nfactor in the running time, our algorithm genralizes to graphs of treewidth at\nmost $2$.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 16:17:55 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 09:02:28 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 11:17:42 GMT"}, {"version": "v4", "created": "Thu, 9 Jul 2020 09:50:09 GMT"}, {"version": "v5", "created": "Wed, 27 Jan 2021 14:44:49 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Mescoff", "Guillaume", ""], ["Paul", "Christophe", ""], ["Thilikos", "Dimitrios", ""]]}, {"id": "2004.00655", "submitter": "Barak Steindl", "authors": "Barak Steindl and Meirav Zehavi", "title": "Parameterized Analysis of Assignment Under Multiple Preferences", "comments": "43 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Assignment problem is a fundamental and well-studied problem in the\nintersection of Social Choice, Computational Economics and Discrete Allocation.\nIn the Assignment problem, a group of agents expresses preferences over a set\nof items, and the task is to find a pareto optimal allocation of items to\nagents. We introduce a generalized version of this problem, where each agent is\nequipped with multiple incomplete preference lists: each list (called a layer)\nis a ranking of items in a possibly different way according to a different\ncriterion. We introduce the concept of global optimality, which extends the\nnotion of pareto optimality to the multi-layered setting, and we focus on the\nproblem of deciding whether a globally optimal assignment exists. We study this\nproblem from the perspective of Parameterized Complexity: we consider several\nnatural parameters such as the number of layers, the number of agents, the\nnumber of items, and the maximum length of a preference list. We present a\ncomprehensive picture of the parameterized complexity of the problem with\nrespect to these parameters.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 18:17:30 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 16:01:48 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Steindl", "Barak", ""], ["Zehavi", "Meirav", ""]]}, {"id": "2004.01032", "submitter": "Alejandro Pacheco", "authors": "Francisco Claude, Gonzalo Navarro, and Alejandro Pacheco", "title": "Grammar-Compressed Indexes with Logarithmic Search Time", "comments": "arXiv admin note: substantial text overlap with arXiv:1110.4493", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let a text $T[1..n]$ be the only string generated by a context-free grammar\nwith $g$ (terminal and nonterminal) symbols, and of size $G$ (measured as the\nsum of the lengths of the right-hand sides of the rules). Such a grammar,\ncalled a grammar-compressed representation of $T$, can be encoded using\nessentially $G\\lg g$ bits. We introduce the first grammar-compressed index that\nuses $O(G\\lg n)$ bits and can find the $occ$ occurrences of patterns $P[1..m]$\nin time $O((m^2+occ)\\lg G)$. We implement the index and demonstrate its\npracticality in comparison with the state of the art, on highly repetitive text\ncollections.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 14:00:54 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Claude", "Francisco", ""], ["Navarro", "Gonzalo", ""], ["Pacheco", "Alejandro", ""]]}, {"id": "2004.01120", "submitter": "Nicola Prezza", "authors": "Nicola Prezza", "title": "On Locating Paths in Compressed Tries", "comments": "Improved toehold lemma running time; added more detailed proofs that\n  take care of all border cases in the locate strategy; postprint version to\n  appear in SODA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of compressing a trie while supporting\nthe powerful \\emph{locate} queries: to return the pre-order identifiers of all\nnodes reached by a path labeled with a given query pattern. Our result builds\non top of the XBWT tree transform of Ferragina et al. [FOCS 2005] and\ngeneralizes the \\emph{r-index} locate machinery of Gagie et al. [SODA 2018,\nJACM 2020] based on the run-length encoded Burrows-Wheeler transform (BWT). Our\nfirst contribution is to propose a suitable generalization of the run-length\nBWT to tries. We show that this natural generalization enjoys several of the\nuseful properties of its counterpart on strings: in particular, the transform\nnatively supports counting occurrences of a query pattern on the trie's paths\nand its size $r$ captures the trie's repetitiveness and lower-bounds a natural\nnotion of trie entropy. Our main contribution is a much deeper insight into the\ncombinatorial structure of this object. In detail, we show that a data\nstructure of $O(r\\log n) + 2n + o(n)$ bits, where $n$ is the number of nodes,\nallows locating the $occ$ occurrences of a pattern of length $m$ in\nnearly-optimal $O(m\\log\\sigma + occ)$ time, where $\\sigma$ is the alphabet's\nsize. Our solution consists in sampling $O(r)$ nodes that can be used as\n\"anchor points\" during the locate process. Once obtained the pre-order\nidentifier of the first pattern occurrence (in co-lexicographic order), we show\nthat a constant number of constant-time jumps between those anchor points lead\nto the identifier of the next pattern occurrence, thus enabling locating in\noptimal $O(1)$ time per occurrence.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 16:43:21 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 10:47:11 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2020 08:45:01 GMT"}, {"version": "v4", "created": "Wed, 16 Dec 2020 23:33:41 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Prezza", "Nicola", ""]]}, {"id": "2004.01156", "submitter": "Evangelos Kipouridis", "authors": "Anders Aamand, Debarati Das, Evangelos Kipouridis, Jakob B. T.\n  Knudsen, Peter M. R. Rasmussen, Mikkel Thorup", "title": "No Repetition: Fast Streaming with Highly Concentrated Hashing", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To get estimators that work within a certain error bound with high\nprobability, a common strategy is to design one that works with constant\nprobability, and then boost the probability using independent repetitions.\nImportant examples of this approach are small space algorithms for estimating\nthe number of distinct elements in a stream, or estimating the set similarity\nbetween large sets. Using standard strongly universal hashing to process each\nelement, we get a sketch based estimator where the probability of a too large\nerror is, say, 1/4. By performing $r$ independent repetitions and taking the\nmedian of the estimators, the error probability falls exponentially in $r$.\nHowever, running $r$ independent experiments increases the processing time by a\nfactor $r$.\n  Here we make the point that if we have a hash function with strong\nconcentration bounds, then we get the same high probability bounds without any\nneed for repetitions. Instead of $r$ independent sketches, we have a single\nsketch that is $r$ times bigger, so the total space is the same. However, we\nonly apply a single hash function, so we save a factor $r$ in time, and the\noverall algorithms just get simpler.\n  Fast practical hash functions with strong concentration bounds were recently\nproposed by Aamand em et al. (to appear in STOC 2020). Using their hashing\nschemes, the algorithms thus become very fast and practical, suitable for\nonline processing of high volume data streams.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 17:26:50 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Aamand", "Anders", ""], ["Das", "Debarati", ""], ["Kipouridis", "Evangelos", ""], ["Knudsen", "Jakob B. T.", ""], ["Rasmussen", "Peter M. R.", ""], ["Thorup", "Mikkel", ""]]}, {"id": "2004.01231", "submitter": "Shi Li", "authors": "Shi Li", "title": "Towards PTAS for Precedence Constrained Scheduling via Combinatorial\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classic problem of scheduling $n$ precedence constrained\nunit-size jobs on $m = O(1)$ machines so as to minimize the makespan. In a\nrecent breakthrough, Levey and Rothvoss \\cite{LR16} developed a\n$(1+\\epsilon)$-approximation for the problem with running time\n$\\exp\\Big(\\exp\\Big(O\\big(\\frac{m^2}{\\epsilon^2}\\log^2\\log n\\big)\\Big)\\Big)$,\nvia the Sherali-Adams lift of the basic linear programming relaxation for the\nproblem by $\\exp\\Big(O\\big(\\frac{m^2}{\\epsilon^2}\\log^2\\log n\\big)\\Big)$\nlevels. Garg \\cite{Garg18} recently improved the number of levels to $\\log\n^{O(m^2/\\epsilon^2)}n$, and thus the running time to $\\exp\\big(\\log\n^{O(m^2/\\epsilon^2)}n\\big)$, which is quasi-polynomial for constant $m$ and\n$\\epsilon$.\n  In this paper we present an algorithm that achieves\n$(1+\\epsilon)$-approximation for the problem with running time\n$n^{O\\left(\\frac{m^4}{\\epsilon^3}\\log^3\\log n\\right)}$, which is very close to\na polynomial for constant $m$ and $\\epsilon$. Unlike the algorithms of\nLevey-Rothvoss and Garg, which are based on linear-programming hierarchy, our\nalgorithm is purely combinatorial. For this problem, we show that the\nconditioning operations on the lifted LP solution can be replaced by making\nguesses about the optimum schedule.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 19:16:22 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 15:51:05 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Li", "Shi", ""]]}, {"id": "2004.01235", "submitter": "Max Van Mulken", "authors": "Kevin Buchin, Mart Hagedoorn, Irina Kostitsyna, Max van Mulken, Jolan\n  Rensen, Leo van Schooten", "title": "Dots & Polygons", "comments": "9 pages, 9 figures, a shorter version of this paper will appear at\n  the 29th International Computational Geometry Media Exposition at CG Week in\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new game, Dots & Polygons, played on a planar point set. Players\ntake turns connecting two points, and when a player closes a (simple) polygon,\nthe player scores its area. We show that deciding whether the game can be won\nfrom a given state, is NP-hard. We do so by a reduction from vertex-disjoint\ncycle packing in cubic planar graphs, including a self-contained reduction from\nplanar 3-Satisfiability to this cycle-packing problem. This also provides a\nsimple proof of the NP-hardness of the related game Dots & Boxes. For points in\nconvex position, we discuss a greedy strategy for Dots & Polygons.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 19:28:23 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 10:21:13 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Buchin", "Kevin", ""], ["Hagedoorn", "Mart", ""], ["Kostitsyna", "Irina", ""], ["van Mulken", "Max", ""], ["Rensen", "Jolan", ""], ["van Schooten", "Leo", ""]]}, {"id": "2004.01250", "submitter": "Thorsten Wi{\\ss}mann", "authors": "Thorsten Wi{\\ss}mann, Hans-Peter Deifel, Stefan Milius, Lutz\n  Schr\\\"oder", "title": "From Generic Partition Refinement to Weighted Tree Automata Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partition refinement is a method for minimizing automata and transition\nsystems of various types. Recently, we have developed a partition refinement\nalgorithm that is generic in the transition type of the given system and\nmatches the run time of the best known algorithms for many concrete types of\nsystems, e.g. deterministic automata as well as ordinary, weighted, and\nprobabilistic (labelled) transition systems. Genericity is achieved by\nmodelling transition types as functors on sets, and systems as coalgebras. In\nthe present work, we refine the run time analysis of our algorithm to cover\nadditional instances, notably weighted automata and, more generally, weighted\ntree automata. For weights in a cancellative monoid we match, and for\nnon-cancellative monoids such as (the additive monoid of) the tropical semiring\neven substantially improve, the asymptotic run time of the best known\nalgorithms. We have implemented our algorithm in a generic tool that is easily\ninstantiated to concrete system types by implementing a simple refinement\ninterface. Moreover, the algorithm and the tool are modular, and partition\nrefiners for new types of systems are obtained easily by composing\npre-implemented basic functors. Experiments show that even for complex system\ntypes, the tool is able to handle systems with millions of transitions.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 20:31:28 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 10:38:07 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 18:39:56 GMT"}, {"version": "v4", "created": "Wed, 25 Nov 2020 18:22:05 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Wi\u00dfmann", "Thorsten", ""], ["Deifel", "Hans-Peter", ""], ["Milius", "Stefan", ""], ["Schr\u00f6der", "Lutz", ""]]}, {"id": "2004.01274", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr", "title": "Does Comma Selection Help To Cope With Local Optima", "comments": "36 pages. Full version of a paper that appeared at GECCO 2020", "journal-ref": null, "doi": "10.1145/3377930.3389823", "report-no": null, "categories": "cs.NE cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One hope when using non-elitism in evolutionary computation is that the\nability to abandon the current-best solution aids leaving local optima. To\nimprove our understanding of this mechanism, we perform a rigorous runtime\nanalysis of a basic non-elitist evolutionary algorithm (EA), the\n$(\\mu,\\lambda)$ EA, on the most basic benchmark function with a local optimum,\nthe jump function. We prove that for all reasonable values of the parameters\nand the problem, the expected runtime of the $(\\mu,\\lambda)$~EA is, apart from\nlower order terms, at least as large as the expected runtime of its elitist\ncounterpart, the $(\\mu+\\lambda)$~EA (for which we conduct the first runtime\nanalysis on jump functions to allow this comparison). Consequently, the ability\nof the $(\\mu,\\lambda)$~EA to leave local optima to inferior solutions does not\nlead to a runtime advantage.\n  We complement this lower bound with an upper bound that, for broad ranges of\nthe parameters, is identical to our lower bound apart from lower order terms.\nThis is the first runtime result for a non-elitist algorithm on a multi-modal\nproblem that is tight apart from lower order terms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 21:39:33 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 11:47:10 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Doerr", "Benjamin", ""]]}, {"id": "2004.01493", "submitter": "Takaaki Nishimoto", "authors": "Takaaki Nishimoto and Yasuo Tabei", "title": "R-enum: Enumeration of Characteristic Substrings in BWT-runs Bounded\n  Space", "comments": "The content of the paper is significantly different from the previous\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enumerating characteristic substrings (e.g., maximal repeats, minimal unique\nsubstrings, and minimal absent words) in a given string has been an important\nresearch topic because there are a wide variety of applications in various\nareas such as string processing and computational biology. Although several\nenumeration algorithms for characteristic substrings have been proposed, they\nare not space-efficient in that their space-usage is proportional to the length\nof an input string. Recently, the run-length encoded Burrows-Wheeler transform\n(RLBWT) has attracted increased attention in string processing, and various\nalgorithms for the RLBWT have been developed. Developing enumeration algorithms\nfor characteristic substrings with the RLBWT, however, remains a challenge. In\nthis paper, we present r-enum (RLBWT-based enumeration), the first enumeration\nalgorithm for characteristic substrings based on RLBWT. R-enum runs in $O(n\n\\log \\log (n/r))$ time and with $O(r \\log n)$ bits of working space for string\nlength $n$ and number $r$ of runs in RLBWT, where $r$ is expected to be\nsignificantly smaller than $n$ for highly repetitive strings (i.e., strings\nwith many repetitions). Experiments using a benchmark dataset of highly\nrepetitive strings show that the results of r-enum are more space-efficient\nthan the previous results. In addition, we demonstrate the applicability of\nr-enum to a huge string by performing experiments on a 300-gigabyte string of\n100 human genomes.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 12:12:01 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 07:22:16 GMT"}, {"version": "v3", "created": "Fri, 15 May 2020 08:32:48 GMT"}, {"version": "v4", "created": "Tue, 2 Mar 2021 06:31:55 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Nishimoto", "Takaaki", ""], ["Tabei", "Yasuo", ""]]}, {"id": "2004.01668", "submitter": "Pavel Vesel\\'y", "authors": "Graham Cormode and Zohar Karnin and Edo Liberty and Justin Thaler and\n  Pavel Vesel\\'y", "title": "Relative Error Streaming Quantiles", "comments": "Full version of the paper to appear in PODS 2021. 46 pages, 2 figures", "journal-ref": null, "doi": "10.1145/3452021.3458323", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximating ranks, quantiles, and distributions over streaming data is a\ncentral task in data analysis and monitoring. Given a stream of $n$ items from\na data universe $\\mathcal{U}$ equipped with a total order, the task is to\ncompute a sketch (data structure) of size poly$(\\log(n), 1/\\varepsilon)$. Given\nthe sketch and a query item $y \\in \\mathcal{U}$, one should be able to\napproximate its rank in the stream, i.e., the number of stream elements smaller\nthan or equal to $y$.\n  Most works to date focused on additive $\\varepsilon n$ error approximation,\nculminating in the KLL sketch that achieved optimal asymptotic behavior. This\npaper investigates multiplicative $(1\\pm\\varepsilon)$-error approximations to\nthe rank. Practical motivation for multiplicative error stems from demands to\nunderstand the tails of distributions, and hence for sketches to be more\naccurate near extreme values.\n  The most space-efficient algorithms due to prior work store either\n$O(\\log(\\varepsilon^2 n)/\\varepsilon^2)$ or $O(\\log^3(\\varepsilon\nn)/\\varepsilon)$ universe items. This paper presents a randomized algorithm\nstoring $O(\\log^{1.5}(\\varepsilon n)/\\varepsilon)$ items, which is within an\n$O(\\sqrt{\\log(\\varepsilon n)})$ factor of optimal. The algorithm does not\nrequire prior knowledge of the stream length and is fully mergeable, rendering\nit suitable for parallel and distributed computing environments.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 16:44:34 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 15:52:25 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 16:38:57 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Cormode", "Graham", ""], ["Karnin", "Zohar", ""], ["Liberty", "Edo", ""], ["Thaler", "Justin", ""], ["Vesel\u00fd", "Pavel", ""]]}, {"id": "2004.01939", "submitter": "Quanquan C. Liu", "authors": "Thaddeus Dryja, Quanquan C. Liu, Neha Narula", "title": "A Lower Bound for Byzantine Agreement and Consensus for Adaptive\n  Adversaries using VDFs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale cryptocurrencies require the participation of millions of\nparticipants and support economic activity of billions of dollars, which has\nled to new lines of work in binary Byzantine Agreement (BBA) and consensus. The\nnew work aims to achieve communication-efficiency---given such a large $n$, not\neveryone can speak during the protocol. Several protocols have achieved\nconsensus with communication-efficiency, even under an adaptive adversary, but\nthey require additional strong assumptions---proof-of-work, memory-erasure,\netc. All of these protocols use multicast: every honest replica multicasts\nmessages to all other replicas. Under this model, we provide a new\ncommunication-efficient consensus protocol using Verifiable Delay Functions\n(VDFs) that is secure against adaptive adversaries and does not require the\nsame strong assumptions present in other protocols.\n  A natural question is whether we can extend the synchronous protocols to the\npartially synchronous setting---in this work, we show that using multicast, we\ncannot. Furthermore, we cannot achieve always safe communication-efficient\nprotocols (that maintain safety with probability 1) even in the synchronous\nsetting against a static adversary when honest replicas only choose to\nmulticast its messages. Considering these impossibility results, we describe a\nnew communication-efficient BBA protocol in a modified partially synchronous\nnetwork model which is secure against adaptive adversaries with high\nprobability.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 14:14:21 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Dryja", "Thaddeus", ""], ["Liu", "Quanquan C.", ""], ["Narula", "Neha", ""]]}, {"id": "2004.02035", "submitter": "John Ellis", "authors": "John Ellis and Ulrike Stege", "title": "Correction to: A Practical, Provably Linear Time, In-place and Stable\n  Merge Algorithm via the Perfect Shuffle", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We correct a paper previously submitted to CoRR. That paper claimed that the\nalgorithm there described was provably of linear time complexity in the average\ncase. The alleged proof of that statement contained an error, being based on an\ninvalid assumption, and is invalid. In this paper we present both experimental\nand analytical evidence that the time complexity is of order $N^2$ in the\naverage case, where $N$ is the total length of the merged sequences.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 22:19:05 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Ellis", "John", ""], ["Stege", "Ulrike", ""]]}, {"id": "2004.02066", "submitter": "Fotis Iliopoulos", "authors": "Fotis Iliopoulos", "title": "Improved bounds for coloring locally sparse hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that, for every $k \\ge 2$, every $k$-uniform hypergaph of degree\n$\\Delta$ and girth at least $5$ is efficiently $(1+o(1) )(k-1) (\\Delta / \\ln\n\\Delta )^{ 1/(k-1) } $-list colorable. As an application we obtain the\ncurrently best deterministic algorithm for list-coloring random hypergraphs of\nbounded average degree.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 01:33:09 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 20:22:50 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 16:08:30 GMT"}, {"version": "v4", "created": "Thu, 13 Aug 2020 19:32:50 GMT"}, {"version": "v5", "created": "Wed, 23 Jun 2021 21:35:19 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Iliopoulos", "Fotis", ""]]}, {"id": "2004.02089", "submitter": "Conrad M Albrecht", "authors": "Conrad M Albrecht, Marcus Freitag, Theodore G van Kessel, Siyuan Lu,\n  Hendrik F Hamann", "title": "Event Clustering & Event Series Characterization on Expected Frequency", "comments": null, "journal-ref": "2017 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData.2017.8258495", "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient clustering algorithm applicable to one-dimensional\ndata such as e.g. a series of timestamps. Given an expected frequency $\\Delta\nT^{-1}$, we introduce an $\\mathcal{O}(N)$-efficient method of characterizing\n$N$ events represented by an ordered series of timestamps $t_1,t_2,\\dots,t_N$.\nIn practice, the method proves useful to e.g. identify time intervals of\n\"missing\" data or to locate \"isolated events\". Moreover, we define measures to\nquantify a series of events by varying $\\Delta T$ to e.g. determine the quality\nof an Internet of Things service.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 04:06:59 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Albrecht", "Conrad M", ""], ["Freitag", "Marcus", ""], ["van Kessel", "Theodore G", ""], ["Lu", "Siyuan", ""], ["Hamann", "Hendrik F", ""]]}, {"id": "2004.02290", "submitter": "Jude Tchaye-Kondi", "authors": "Jude Tchaye-Kondi, Yanlong Zhai, Liehuang Zhu", "title": "A new hashing based nearest neighbors selection technique for big\n  datasets", "comments": "8 pages,6 figures", "journal-ref": null, "doi": "10.5121/csit.2021.110708", "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  KNN has the reputation to be the word simplest but efficient supervised\nlearning algorithm used for either classification or regression. KNN prediction\nefficiency highly depends on the size of its training data but when this\ntraining data grows KNN suffers from slowness in making decisions since it\nneeds to search nearest neighbors within the entire dataset at each decision\nmaking. This paper proposes a new technique that enables the selection of\nnearest neighbors directly in the neighborhood of a given observation. The\nproposed approach consists of dividing the data space into subcells of a\nvirtual grid built on top of data space. The mapping between the data points\nand subcells is performed using hashing. When it comes to select the nearest\nneighbors of a given observation, we firstly identify the cell the observation\nbelongs by using hashing, and then we look for nearest neighbors from that\ncentral cell and cells around it layer by layer. From our experiment\nperformance analysis on publicly available datasets, our algorithm outperforms\nthe original KNN in time efficiency with a prediction quality as good as that\nof KNN it also offers competitive performance with solutions like KDtree\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 19:36:00 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 03:26:41 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Tchaye-Kondi", "Jude", ""], ["Zhai", "Yanlong", ""], ["Zhu", "Liehuang", ""]]}, {"id": "2004.02298", "submitter": "Mark Jones Dr", "authors": "Mark Jones, Steven Kelk, Leen Stougie", "title": "Maximum parsimony distance on phylogenetictrees: a linear kernel and\n  constant factor approximation algorithm", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum parsimony distance is a measure used to quantify the dissimilarity of\ntwo unrooted phylogenetic trees. It is NP-hard to compute, and very few\npositive algorithmic results are known due to its complex combinatorial\nstructure. Here we address this shortcoming by showing that the problem is\nfixed parameter tractable. We do this by establishing a linear kernel i.e.,\nthat after applying certain reduction rules the resulting instance has size\nthat is bounded by a linear function of the distance. As powerful corollaries\nto this result we prove that the problem permits a polynomial-time\nconstant-factor approximation algorithm; that the treewidth of a natural\nauxiliary graph structure encountered in phylogenetics is bounded by a function\nof the distance; and that the distance is within a constant factor of the size\nof a maximum agreement forest of the two trees, a well studied object in\nphylogenetics.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 20:01:21 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Jones", "Mark", ""], ["Kelk", "Steven", ""], ["Stougie", "Leen", ""]]}, {"id": "2004.02335", "submitter": "David Arnas", "authors": "David Arnas and Carl Leake and Daniele Mortari", "title": "The n-dimensional k-vector and its application to orthogonal range\n  searching", "comments": "31 pages, 10 figures", "journal-ref": "Applied Mathematics and Computation, Vol. 372, 2020", "doi": "10.1016/j.amc.2019.125010", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on the definition and study of the n-dimensional k-vector,\nan algorithm devised to perform orthogonal range searching in static databases\nwith multiple dimensions. The methodology first finds the order in which to\nsearch the dimensions, and then, performs the search using a modified\nprojection method. In order to determine the dimension order, the algorithm\nuses the k-vector, a range searching technique for one dimension that\nidentifies the number of elements contained in the searching range. Then, using\nthis information, the algorithm predicts and selects the best approach to deal\nwith each dimension. The algorithm has a worst case complexity of\n$\\mathcal{O}(nd(k/n)^{2/d})$, where $k$ is the number of elements retrieved,\n$n$ is the number of elements in the database, and $d$ is the number of\ndimensions of the database. This work includes a detailed description of the\nmethodology as well as a study of the algorithm performance.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 22:26:05 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Arnas", "David", ""], ["Leake", "Carl", ""], ["Mortari", "Daniele", ""]]}, {"id": "2004.02338", "submitter": "Ferdinando Cicalese", "authors": "Ferdinando Cicalese and Nicol\\`o Pilati", "title": "The Tandem Duplication Distance Problem is hard over bounded alphabets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tandem duplication denotes the process of inserting a copy of a segment of\nDNA adjacent to its original position. More formally, a tandem duplication can\nbe thought of as an operation that converts a string $S = AXB$ into a string $T\n= AXXB.$ As they appear to be involved in genetic disorders, tandem\nduplications are widely studied in computational biology. Also, tandem\nduplication mechanisms have been recently studied in different contexts, from\nformal languages, to information theory, to error-correcting codes for DNA\nstorage systems.\n  The problem of determining the complexity of computing the tandem duplication\ndistance between two given strings was proposed by [Leupold et al., 2004] and,\nvery recently, it was shown to be NP-hard for the case of unbounded alphabets\n[Lafond et al., STACS2020]. In this paper, we significantly improve this result\nand show that the tandem duplication distance problem is NP-hard already for\nthe case of strings over an alphabet of size $\\leq 5.$ We also study some\nspecial classes of strings were it is possible to give linear time solutions to\nthe existence problem: given strings $S$ and $T$ over the same alphabet, decide\nwhether there exists a sequence of duplications converting $S$ into $T$. A\npolynomial time algorithm that solves the existence problem was only known for\nthe case of the binary alphabet.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 22:30:50 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 10:00:30 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Cicalese", "Ferdinando", ""], ["Pilati", "Nicol\u00f2", ""]]}, {"id": "2004.02339", "submitter": "David Arnas", "authors": "David Arnas and Carl Leake and Daniele Mortari", "title": "Random Sampling using k-vector", "comments": "23 pages, 4 figures", "journal-ref": "Computing in Science & Engineering, Vol. 21, No. 1, pp. 94-107,\n  2019", "doi": "10.1109/MCSE.2018.2882727", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces two new techniques for random number generation with any\nprescribed nonlinear distribution based on the k-vector methodology. The first\napproach is based on inverse transform sampling using the optimal k-vector to\ngenerate the samples by inverting the cumulative distribution. The second\napproach generates samples by performing random searches in a pre-generated\nlarge database previously built by massive inversion of the prescribed\nnonlinear distribution using the k-vector. Both methods are shown suitable for\nmassive generation of random samples. Examples are provided to clarify these\nmethodologies.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 22:32:33 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Arnas", "David", ""], ["Leake", "Carl", ""], ["Mortari", "Daniele", ""]]}, {"id": "2004.02342", "submitter": "David Arnas", "authors": "David Arnas and Daniele Mortari", "title": "Nonlinear Function Inversion using k-vector", "comments": "25 pages, 9 figures", "journal-ref": "Applied Mathematics and Computation Vol. 320, pp. 754-768, 2018", "doi": "10.1016/j.amc.2017.10.009", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a general numerical technique to invert one dimensional\nanalytic or tabulated nonlinear functions in assigned ranges of interest. The\nproposed approach is based on an optimal version of the k-vector range\nsearching, an ad-hoc modification devised for function inversion. The\noptimality consists of retrieving always the same number of data ($1,2,\\dots$)\nfor a specified searching range to initiate the root solver. This provides\nflexibility to adapt the technique to a variety of root solvers (e.g.,\nbisection, Newton, etc.), using a specified number of starting points. The\nproposed method allows to build an inverse function toolbox for a set of\nspecified nonlinear functions. In particular, the method is suitable when\nintensive inversions of the same function are required. The inversion is\nextremely fast (almost instantaneous), but it requires a one-time preprocessing\neffort.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 22:40:21 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Arnas", "David", ""], ["Mortari", "Daniele", ""]]}, {"id": "2004.02425", "submitter": "Kirankumar Shiragur", "authors": "Nima Anari, Moses Charikar, Kirankumar Shiragur, Aaron Sidford", "title": "The Bethe and Sinkhorn Permanents of Low Rank Matrices and Implications\n  for Profile Maximum Likelihood", "comments": "59 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of computing the likelihood of the\nprofile of a discrete distribution, i.e., the probability of observing the\nmultiset of element frequencies, and computing a profile maximum likelihood\n(PML) distribution, i.e., a distribution with the maximum profile likelihood.\nFor each problem we provide polynomial time algorithms that given $n$ i.i.d.\\\nsamples from a discrete distribution, achieve an approximation factor of\n$\\exp\\left(-O(\\sqrt{n} \\log n) \\right)$, improving upon the previous best-known\nbound achievable in polynomial time of $\\exp(-O(n^{2/3} \\log n))$ (Charikar,\nShiragur and Sidford, 2019). Through the work of Acharya, Das, Orlitsky and\nSuresh (2016), this implies a polynomial time universal estimator for symmetric\nproperties of discrete distributions in a broader range of error parameter.\n  We achieve these results by providing new bounds on the quality of\napproximation of the Bethe and Sinkhorn permanents (Vontobel, 2012 and 2014).\nWe show that each of these are $\\exp(O(k \\log(N/k)))$ approximations to the\npermanent of $N \\times N$ matrices with non-negative rank at most $k$,\nimproving upon the previous known bounds of $\\exp(O(N))$. To obtain our results\non PML, we exploit the fact that the PML objective is proportional to the\npermanent of a certain Vandermonde matrix with $\\sqrt{n}$ distinct columns,\ni.e. with non-negative rank at most $\\sqrt{n}$. As a by-product of our work we\nestablish a surprising connection between the convex relaxation in prior work\n(CSS19) and the well-studied Bethe and Sinkhorn approximations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 06:40:03 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Anari", "Nima", ""], ["Charikar", "Moses", ""], ["Shiragur", "Kirankumar", ""], ["Sidford", "Aaron", ""]]}, {"id": "2004.02437", "submitter": "Ren\\'e van Bevern", "authors": "Ren\\'e van Bevern and Viktoriia A. Slugina", "title": "A historical note on the 3/2-approximation algorithm for the metric\n  traveling salesman problem", "comments": "Version accepted to Historia Mathematica", "journal-ref": "Historia Mathematica, 53:118-127, 2020", "doi": "10.1016/j.hm.2020.04.003", "report-no": null, "categories": "cs.DS cs.DM math.HO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most fundamental results in combinatorial optimization is the\npolynomial-time 3/2-approximation algorithm for the metric traveling salesman\nproblem. It was presented by Christofides in 1976 and is well known as \"the\nChristofides algorithm\". Recently, some authors started calling it\n\"Christofides-Serdyukov algorithm\", pointing out that it was published\nindependently in the USSR in 1978. We provide some historic background on\nSerdyukov's findings and a translation of his article from Russian into\nEnglish.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 07:14:48 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 10:36:08 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["van Bevern", "Ren\u00e9", ""], ["Slugina", "Viktoriia A.", ""]]}, {"id": "2004.02502", "submitter": "Kengo Nakamura", "authors": "Kengo Nakamura, Shuhei Denzumi, Masaaki Nishino", "title": "Variable Shift SDD: A More Succinct Sentential Decision Diagram", "comments": "21 pages; Accepted for SEA2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Sentential Decision Diagram (SDD) is a tractable representation of\nBoolean functions that subsumes the famous Ordered Binary Decision Diagram\n(OBDD) as a strict subset. SDDs are attracting much attention because they are\nmore succinct than OBDDs, as well as having canonical forms and supporting many\nuseful queries and transformations such as model counting and Apply operation.\nIn this paper, we propose a more succinct variant of SDD named Variable Shift\nSDD (VS-SDD). The key idea is to create a unique representation for Boolean\nfunctions that are equivalent under a specific variable substitution. We show\nthat VS-SDDs are never larger than SDDs and there are cases in which the size\nof a VS-SDD is exponentially smaller than that of an SDD. Moreover, despite\nsuch succinctness, we show that numerous basic operations that are supported in\npolytime with SDD are also supported in polytime with VS-SDD. Experiments\nconfirm that VS-SDDs are significantly more succinct than SDDs when applied to\nclassical planning instances, where inherent symmetry exists.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 09:18:19 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Nakamura", "Kengo", ""], ["Denzumi", "Shuhei", ""], ["Nishino", "Masaaki", ""]]}, {"id": "2004.02530", "submitter": "Pat Morin", "authors": "Pat Morin", "title": "A Fast Algorithm for the Product Structure of Planar Graphs", "comments": "This version corrects some figures and adds a section on extracting a\n  tree-decomposition of $H$", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dujmovi\\'c et al (FOCS2019) recently proved that every planar graph $G$ is a\nsubgraph of $H\\boxtimes P$, where $\\boxtimes$ denotes the strong graph product,\n$H$ is a graph of treewidth 8 and $P$ is a path. This result has found numerous\napplications to linear graph layouts, graph colouring, and graph labelling. The\nproof given by Dujmovi\\'c et al is based on a similar decomposition of\nPilipczuk and Siebertz (SODA2019) which is constructive and leads to an\n$O(n^2)$ time algorithm for finding $H$ and the mapping from $V(G)$ onto\n$V(H\\boxtimes P)$. In this note, we show that this algorithm can be made to run\nin $O(n\\log n)$ time.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 10:04:14 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 12:46:09 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 17:59:35 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Morin", "Pat", ""]]}, {"id": "2004.02554", "submitter": "Haris Aziz", "authors": "Haris Aziz", "title": "Simultaneously Achieving Ex-ante and Ex-post Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a polynomial-time algorithm that computes an ex-ante envy-free\nlottery over envy-free up to one item (EF1) deterministic allocations. It has\nthe following advantages over a recently proposed algorithm: it does not rely\non the linear programming machinery including separation oracles; it is\nSD-efficient (both ex-ante and ex-post); and the ex-ante outcome is equivalent\nto the outcome returned by the well-known probabilistic serial rule. As a\nresult, we answer a question raised by Freeman, Shah, and Vaish (2020) whether\nthe outcome of the probabilistic serial rule can be implemented by ex-post EF1\nallocations. In the light of a couple of impossibility results that we prove,\nour algorithm can be viewed as satisfying a maximal set of properties. Under\nbinary utilities, our algorithm is also ex-ante group-strategyproof and ex-ante\nPareto optimal. Finally, we also show that checking whether a given random\nallocation can be implemented by a lottery over EF1 and Pareto optimal\nallocations is NP-hard.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 11:05:32 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 02:18:25 GMT"}, {"version": "v3", "created": "Sun, 28 Jun 2020 23:34:53 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Aziz", "Haris", ""]]}, {"id": "2004.02570", "submitter": "Hui Luo", "authors": "Hui Luo, Zhifeng Bao, Farhana M. Choudhury, and J. Shane Culpepper", "title": "Dynamic Ridesharing in Peak Travel Periods", "comments": null, "journal-ref": null, "doi": "10.1109/TKDE.2019.2961341", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a variant of the dynamic ridesharing problem with a\nspecific focus on peak hours: Given a set of drivers and rider requests, we aim\nto match drivers to each rider request by achieving two objectives: maximizing\nthe served rate and minimizing the total additional distance, subject to a\nseries of spatio-temporal constraints. Our problem can be distinguished from\nexisting work in three aspects: (1) Previous work did not fully explore the\nimpact of peak travel periods where the number of rider requests is much\ngreater than the number of available drivers. (2) Existing solutions usually\nrely on single objective optimization techniques, such as minimizing the total\ntravel cost. (3) When evaluating the overall system performance, the runtime\nspent on updating drivers' trip schedules as per incoming rider requests should\nbe incorporated, while it is excluded by most existing solutions. We propose an\nindex structure together with a set of pruning rules and an efficient algorithm\nto include new riders into drivers' existing trip schedule. To answer new rider\nrequests effectively, we propose two algorithms that match drivers with rider\nrequests. Finally, we perform extensive experiments on a large-scale test\ncollection to validate the proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 11:34:26 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Luo", "Hui", ""], ["Bao", "Zhifeng", ""], ["Choudhury", "Farhana M.", ""], ["Culpepper", "J. Shane", ""]]}, {"id": "2004.02580", "submitter": "Ziqiang Yu", "authors": "Ziqiang Yu, Xiaohui Yu, Nick Koudas, Yang Liu, Yifan Li, Yueting Chen,\n  Dingyu Yang", "title": "Distributed Processing of k Shortest Path Queries over Dynamic Road\n  Networks", "comments": "A shorter version of this technical report has been accepted for\n  publication as a full paper in ACM SIGMOD 2020: International Conference on\n  Management of Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of identifying the k-shortest paths (KSPs for short) in a dynamic\nroad network is essential to many location-based services. Road networks are\ndynamic in the sense that the weights of the edges in the corresponding graph\nconstantly change over time, representing evolving traffic conditions. Very\noften such services have to process numerous KSP queries over large road\nnetworks at the same time, thus there is a pressing need to identify\ndistributed solutions for this problem. However, most existing approaches are\ndesigned to identify KSPs on a static graph in a sequential manner (i.e., the\n(i+1)-th shortest path is generated based on the i-th shortest path),\nrestricting their scalability and applicability in a distributed setting. We\ntherefore propose KSP-DG, a distributed algorithm for identifying k-shortest\npaths in a dynamic graph. It is based on partitioning the entire graph into\nsmaller subgraphs, and reduces the problem of determining KSPs into the\ncomputation of partial KSPs in relevant subgraphs, which can execute in\nparallel on a cluster of servers. A distributed two-level index called DTLP is\ndeveloped to facilitate the efficient identification of relevant subgraphs. A\nsalient feature of DTLP is that it indexes a set of virtual paths that are\ninsensitive to varying traffic conditions, leading to very low maintenance cost\nin dynamic road networks. This is the first treatment of the problem of\nprocessing KSP queries over dynamic road networks. Extensive experiments\nconducted on real road networks confirm the superiority of our proposal over\nbaseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 11:52:00 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 00:28:10 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Yu", "Ziqiang", ""], ["Yu", "Xiaohui", ""], ["Koudas", "Nick", ""], ["Liu", "Yang", ""], ["Li", "Yifan", ""], ["Chen", "Yueting", ""], ["Yang", "Dingyu", ""]]}, {"id": "2004.02781", "submitter": "Gonzalo Navarro", "authors": "Gonzalo Navarro", "title": "Indexing Highly Repetitive String Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two decades ago, a breakthrough in indexing string collections made it\npossible to represent them within their compressed space while at the same time\noffering indexed search functionalities. As this new technology permeated\nthrough applications like bioinformatics, the string collections experienced a\ngrowth that outperforms Moore's Law and challenges our ability of handling them\neven in compressed form. It turns out, fortunately, that many of these rapidly\ngrowing string collections are highly repetitive, so that their information\ncontent is orders of magnitude lower than their plain size. The statistical\ncompression methods used for classical collections, however, are blind to this\nrepetitiveness, and therefore a new set of techniques has been developed in\norder to properly exploit it. The resulting indexes form a new generation of\ndata structures able to handle the huge repetitive string collections that we\nare facing.\n  In this survey we cover the algorithmic developments that have led to these\ndata structures. We describe the distinct compression paradigms that have been\nused to exploit repetitiveness, the fundamental algorithmic ideas that form the\nbase of all the existing indexes, and the various structures that have been\nproposed, comparing them both in theoretical and practical aspects. We conclude\nwith the current challenges in this fascinating field.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 16:16:26 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 23:51:22 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 23:08:04 GMT"}, {"version": "v4", "created": "Wed, 4 Nov 2020 18:46:46 GMT"}, {"version": "v5", "created": "Sat, 6 Feb 2021 18:39:02 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Navarro", "Gonzalo", ""]]}, {"id": "2004.02837", "submitter": "Jason Altschuler", "authors": "Jason M. Altschuler and Pablo A. Parrilo", "title": "Near-linear convergence of the Random Osborne algorithm for Matrix\n  Balancing", "comments": "v2: Fixed minor typos. Modified title for clarity. Corrected\n  statement of Thm 6.1; this does not affect our main results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit Matrix Balancing, a pre-conditioning task used ubiquitously for\ncomputing eigenvalues and matrix exponentials. Since 1960, Osborne's algorithm\nhas been the practitioners' algorithm of choice and is now implemented in most\nnumerical software packages. However, its theoretical properties are not well\nunderstood. Here, we show that a simple random variant of Osborne's algorithm\nconverges in near-linear time in the input sparsity. Specifically, it balances\n$K\\in\\mathbb{R}_{\\geq 0}^{n\\times n}$ after $O(m\\epsilon^{-2}\\log\\kappa)$\narithmetic operations, where $m$ is the number of nonzeros in $K$, $\\epsilon$\nis the $\\ell_1$ accuracy, and $\\kappa=\\sum_{ij}K_{ij}/(\\min_{ij:K_{ij}\\neq\n0}K_{ij})$ measures the conditioning of $K$. Previous work had established\nnear-linear runtimes either only for $\\ell_2$ accuracy (a weaker criterion\nwhich is less relevant for applications), or through an entirely different\nalgorithm based on (currently) impractical Laplacian solvers.\n  We further show that if the graph with adjacency matrix $K$ is moderately\nconnected--e.g., if $K$ has at least one positive row/column pair--then\nOsborne's algorithm initially converges exponentially fast, yielding an\nimproved runtime $O(m\\epsilon^{-1}\\log\\kappa)$. We also address numerical\nprecision by showing that these runtime bounds still hold when using\n$O(\\log(n\\kappa/\\epsilon))$-bit numbers.\n  Our results are established through an intuitive potential argument that\nleverages a convex optimization perspective of Osborne's algorithm, and relates\nthe per-iteration progress to the current imbalance as measured in Hellinger\ndistance. Unlike previous analyses, we critically exploit log-convexity of the\npotential. Our analysis extends to other variants of Osborne's algorithm: along\nthe way, we establish significantly improved runtime bounds for cyclic, greedy,\nand parallelized variants.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 17:31:58 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 22:49:52 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Altschuler", "Jason M.", ""], ["Parrilo", "Pablo A.", ""]]}, {"id": "2004.03033", "submitter": "Aleksander Cis{\\l}ak", "authors": "Aleksander Cis{\\l}ak and Szymon Grabowski", "title": "SOPanG 2: online searching over a pan-genome without false positives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: The pan-genome can be stored as elastic-degenerate (ED) string, a\nrecently introduced compact representation of multiple overlapping sequences.\nHowever, a search over the ED string does not indicate which individuals (if\nany) match the entire query.\n  Results: We augment the ED string with sources (individuals' indexes) and\npropose an extension of the SOPanG (Shift-Or for Pan-Genome) tool to report\nonly true positive matches, omitting those not occurring in any of the\nhaplotypes. The additional stage for checking the matches yields a penalty of\nless than 3.5% relative speed in practice, which means that SOPanG 2 is able to\nreport pattern matches in a pan-genome, mapping them onto individuals, at the\nsingle-thread throughput of above 430 MB/s on real data.\n  Availability and implementation: SOPanG 2 can be downloaded here:\ngithub.com/MrAlexSee/sopang\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 22:54:58 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 17:26:17 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Cis\u0142ak", "Aleksander", ""], ["Grabowski", "Szymon", ""]]}, {"id": "2004.03050", "submitter": "David Grimsman", "authors": "David Grimsman, Matthew R. Kirchner, Jo\\~ao P. Hespanha, Jason R.\n  Marden", "title": "The Impact of Message Passing in Agent-Based Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DS cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular maximization problems are a relevant model set for many real-world\napplications. Since these problems are generally NP-Hard, many methods have\nbeen developed to approximate the optimal solution in polynomial time. One such\napproach uses an agent-based greedy algorithm, where the goal is for each agent\nto choose an action from its action set such that the union of all actions\nchosen is as high-valued as possible. Recent work has shown how the performance\nof the greedy algorithm degrades as the amount of information shared among the\nagents decreases, whereas this work addresses the scenario where agents are\ncapable of sharing more information than allowed in the greedy algorithm.\nSpecifically, we show how performance guarantees increase as agents are capable\nof passing messages, which can augment the allowable decision set for each\nagent. Under these circumstances, we show a near-optimal method for message\npassing, and how much such an algorithm could increase performance for any\ngiven problem instance.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 00:24:55 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 23:21:06 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Grimsman", "David", ""], ["Kirchner", "Matthew R.", ""], ["Hespanha", "Jo\u00e3o P.", ""], ["Marden", "Jason R.", ""]]}, {"id": "2004.03114", "submitter": "Jason Altschuler", "authors": "Jason M. Altschuler and Pablo A. Parrilo", "title": "Approximating Min-Mean-Cycle for low-diameter graphs in near-optimal\n  time and memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit Min-Mean-Cycle, the classical problem of finding a cycle in a\nweighted directed graph with minimum mean weight. Despite an extensive\nalgorithmic literature, previous work falls short of a near-linear runtime in\nthe number of edges $m$--in fact, there is a natural barrier which precludes\nsuch a runtime for solving Min-Mean-Cycle exactly. Here, we give a much faster\napproximation algorithm that, for graphs with polylogarithmic diameter, has\nnear-linear runtime. In particular, this is the first algorithm whose runtime\nfor the complete graph scales in the number of vertices $n$ as\n$\\tilde{O}(n^2)$. Moreover--unconditionally on the diameter--the algorithm uses\nonly $O(n)$ memory beyond reading the input, making it \"memory-optimal\". The\nalgorithm is also simple to implement and has remarkable practical performance.\n  Our approach is based on solving a linear programming (LP) relaxation using\nentropic regularization, which effectively reduces the LP to a Matrix Balancing\nproblem--a la the popular reduction of Optimal Transport to Matrix Scaling. We\nthen round the fractional LP solution using a variant of the classical\nCycle-Cancelling algorithm that is sped up to near-linear runtime at the\nexpense of being approximate, and implemented in a memory-optimal manner.\n  We also provide an alternative algorithm with slightly faster theoretical\nruntime, albeit worse memory usage and practicality. This algorithm uses the\nsame rounding procedure, but solves the LP relaxation by leveraging recent\ndevelopments in area-convexity regularization. Its runtime scales inversely in\nthe approximation accuracy, which we show is optimal--barring a major\nbreakthrough in algorithmic graph theory, namely faster Shortest Paths\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 04:08:47 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Altschuler", "Jason M.", ""], ["Parrilo", "Pablo A.", ""]]}, {"id": "2004.03166", "submitter": "Yanjun Han", "authors": "Yanjun Han and Kirankumar Shiragur", "title": "On the Competitive Analysis and High Accuracy Optimality of Profile\n  Maximum Likelihood", "comments": "To appear at SODA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.IT math.IT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A striking result of [Acharya et al. 2017] showed that to estimate symmetric\nproperties of discrete distributions, plugging in the distribution that\nmaximizes the likelihood of observed multiset of frequencies, also known as the\nprofile maximum likelihood (PML) distribution, is competitive compared with any\nestimators regardless of the symmetric property. Specifically, given $n$\nobservations from the discrete distribution, if some estimator incurs an error\n$\\varepsilon$ with probability at most $\\delta$, then plugging in the PML\ndistribution incurs an error $2\\varepsilon$ with probability at most\n$\\delta\\cdot \\exp(3\\sqrt{n})$. In this paper, we strengthen the above result\nand show that using a careful chaining argument, the error probability can be\nreduced to $\\delta^{1-c}\\cdot \\exp(c'n^{1/3+c})$ for arbitrarily small\nconstants $c>0$ and some constant $c'>0$. In particular, we show that the PML\ndistribution is an optimal estimator of the sorted distribution: it is\n$\\varepsilon$-close in sorted $\\ell_1$ distance to the true distribution with\nsupport size $k$ for any $n=\\Omega(k/(\\varepsilon^2 \\log k))$ and $\\varepsilon\n\\gg n^{-1/3}$, which are the information-theoretically optimal sample\ncomplexity and the largest error regime where the classical empirical\ndistribution is sub-optimal, respectively.\n  In order to strengthen the analysis of the PML, a key ingredient is to employ\nnovel \"continuity\" properties of the PML distributions and construct a chain of\nsuitable quantized PMLs, or \"coverings\". We also construct a novel\napproximation-based estimator for the sorted distribution with a near-optimal\nconcentration property without any sample splitting, where as a byproduct we\nobtain better trade-offs between the polynomial approximation error and the\nmaximum magnitude of coefficients in the Poisson approximation of $1$-Lipschitz\nfunctions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 07:28:11 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 04:16:11 GMT"}, {"version": "v3", "created": "Sun, 1 Nov 2020 10:01:01 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Han", "Yanjun", ""], ["Shiragur", "Kirankumar", ""]]}, {"id": "2004.03206", "submitter": "Lukas Barth", "authors": "Lukas Barth, Dorothea Wagner", "title": "Zipping Segment Trees", "comments": "Accepted for publication at SEA 2020", "journal-ref": null, "doi": "10.4230/LIPIcs.SEA.2020.25", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stabbing queries in sets of intervals are usually answered using segment\ntrees. A dynamic variant of segment trees has been presented by van Kreveld and\nOvermars, which uses red-black trees to do rebalancing operations. This paper\npresents zipping segment trees - dynamic segment trees based on zip trees,\nwhich were recently introduced by Tarjan et al. To facilitate zipping segment\ntrees, we show how to uphold certain segment tree properties during the\noperations of a zip tree. We present an in-depth experimental evaluation and\ncomparison of dynamic segment trees based on red-black trees, weight-balanced\ntrees and several variants of the novel zipping segment trees. Our results\nindicate that zipping segment trees perform better than rotation-based\nalternatives.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 08:48:41 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Barth", "Lukas", ""], ["Wagner", "Dorothea", ""]]}, {"id": "2004.03477", "submitter": "Ciro Medeiros", "authors": "Ciro M. Medeiros, Martin A. Musicante, Umberto S. Costa", "title": "An Algorithm for Context-Free Path Queries over Graph Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RDF (Resource Description Framework) is a standard language to represent\ngraph databases. Query languages for RDF databases usually include primitives\nto support path queries, linking pairs of vertices of the graph that are\nconnected by a path of labels belonging to a given language. Languages such as\nSPARQL include support for paths defined by regular languages (by means of\nRegular Expressions). A context-free path query is a path query whose language\ncan be defined by a context-free grammar. Context-free path queries can be used\nto implement queries such as the \"same generation queries\", that are not\nexpressible by Regular Expressions. In this paper, we present a novel algorithm\nfor context-free path query processing. We prove the correctness of our\napproach and show its run-time and memory complexity. We show the viability of\nour approach by means of a prototype implemented in Go. We run our prototype\nusing the same cases of study as proposed in recent works, comparing our\nresults with another, recently published algorithm. The experiments include\nboth synthetic and real RDF databases. Our algorithm can be seen as a step\nforward, towards the implementation of more expressive query languages.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 15:26:50 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Medeiros", "Ciro M.", ""], ["Musicante", "Martin A.", ""], ["Costa", "Umberto S.", ""]]}, {"id": "2004.03493", "submitter": "Hanzhi Wang", "authors": "Hanzhi Wang, Zhewei Wei, Ye Yuan, Xiaoyong Du, Ji-Rong Wen", "title": "Exact Single-Source SimRank Computation on Large Graphs", "comments": "ACM SIGMOD 2020", "journal-ref": null, "doi": "10.1145/3318464.3389781", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SimRank is a popular measurement for evaluating the node-to-node similarities\nbased on the graph topology. In recent years, single-source and top-$k$ SimRank\nqueries have received increasing attention due to their applications in web\nmining, social network analysis, and spam detection. However, a fundamental\nobstacle in studying SimRank has been the lack of ground truths. The only exact\nalgorithm, Power Method, is computationally infeasible on graphs with more than\n$10^6$ nodes. Consequently, no existing work has evaluated the actual\ntrade-offs between query time and accuracy on large real-world graphs. In this\npaper, we present ExactSim, the first algorithm that computes the exact\nsingle-source and top-$k$ SimRank results on large graphs. With high\nprobability, this algorithm produces ground truths with a rigorous theoretical\nguarantee. We conduct extensive experiments on real-world datasets to\ndemonstrate the efficiency of ExactSim. The results show that ExactSim provides\nthe ground truth for any single-source SimRank query with a precision up to 7\ndecimal places within a reasonable query time.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 15:45:59 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 10:13:47 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 13:20:27 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Wang", "Hanzhi", ""], ["Wei", "Zhewei", ""], ["Yuan", "Ye", ""], ["Du", "Xiaoyong", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2004.03816", "submitter": "Liren Yu", "authors": "Liren Yu, Jiaming Xu, and Xiaojun Lin", "title": "Graph Matching with Partially-Correct Seeds", "comments": "43 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph matching aims to find the latent vertex correspondence between two\nedge-correlated graphs and has found numerous applications across different\nfields. In this paper, we study a seeded graph matching problem, which assumes\nthat a set of seeds, i.e., pre-mapped vertex-pairs, is given in advance. While\nmost previous work requires all seeds to be correct, we focus on the setting\nwhere the seeds are partially correct. Specifically, consider two correlated\ngraphs whose edges are sampled independently from a parent \\ER graph\n$\\mathcal{G}(n,p)$. A mapping between the vertices of the two graphs is\nprovided as seeds, of which an unknown $\\beta$ fraction is correct. We first\nanalyze a simple algorithm that matches vertices based on the number of common\nseeds in the $1$-hop neighborhoods, and then further propose a new algorithm\nthat uses seeds in the $2$-hop neighborhoods. We establish non-asymptotic\nperformance guarantees of perfect matching for both $1$-hop and $2$-hop\nalgorithms, showing that our new $2$-hop algorithm requires substantially fewer\ncorrect seeds than the $1$-hop algorithm when graphs are sparse. Moreover, by\ncombining our new performance guarantees for the $1$-hop and $2$-hop\nalgorithms, we attain the best-known results (in terms of the required fraction\nof correct seeds) across the entire range of graph sparsity and significantly\nimprove the previous results in\n\\cite{10.14778/2794367.2794371,lubars2018correcting} when $p\\ge n^{-5/6}$. For\ninstance, when $p$ is a constant or $p=n^{-3/4}$, we show that only\n$\\Omega(\\sqrt{n\\log n})$ correct seeds suffice for perfect matching, while the\npreviously best-known results demand $\\Omega(n)$ and $\\Omega(n^{3/4}\\log n)$\ncorrect seeds, respectively. Numerical experiments corroborate our theoretical\nfindings, demonstrating the superiority of our $2$-hop algorithm on a variety\nof synthetic and real graphs.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 05:25:52 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 17:32:14 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Yu", "Liren", ""], ["Xu", "Jiaming", ""], ["Lin", "Xiaojun", ""]]}, {"id": "2004.04003", "submitter": "Suman Banerjee", "authors": "Suman Banerjee, Mamata Jenamani, Dilip Kumar Pratihar", "title": "Earned Benefit Maximization in Social Networks Under Budget Constraint", "comments": "12 Pages, 16 Figures, Submitted to a Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a social network with nonuniform selection cost of the users, the\nproblem of \\textit{Budgeted Influence Maximization} (BIM in short) asks for\nselecting a subset of the nodes within an allocated budget for initial\nactivation, such that due to the cascading effect, influence in the network is\nmaximized. In this paper, we study this problem with a variation, where a set\nof nodes are designated as target nodes, each of them is assigned with a\nbenefit value, that can be earned by influencing them, and our goal is to\nmaximize the earned benefit by initially activating a set of nodes within the\nbudget. We call this problem as the \\textsc{Earned Benefit Maximization\nProblem}. First, we show that this problem is NP\\mbox{-}Hard and the benefit\nfunction is \\textit{monotone}, \\textit{sub\\mbox{-}modular} under the\n\\textit{Independent Cascade Model} of diffusion. We propose an incremental\ngreedy strategy for this problem and show, with minor modification it gives\n$(1-\\frac{1}{\\sqrt{e}})$\\mbox{-}factor approximation guarantee on the earned\nbenefit. Next, by exploiting the sub\\mbox{-}modularity property of the benefit\nfunction, we improve the efficiency of the proposed greedy algorithm. Then, we\npropose a hop\\mbox{-}based heuristic method, which works based on the\ncomputation of the `expected earned benefit' of the effective neighbors\ncorresponding to the target nodes. Finally, we perform a series of extensive\nexperiments with four real\\mbox{-}life, publicly available social network\ndatasets. From the experiments, we observe that the seed sets selected by the\nproposed algorithms can achieve more benefit compared to many existing methods.\nParticularly, the hop\\mbox{-}based approach is found to be more efficient than\nthe other ones for solving this problem.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 14:19:37 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Banerjee", "Suman", ""], ["Jenamani", "Mamata", ""], ["Pratihar", "Dilip Kumar", ""]]}, {"id": "2004.04250", "submitter": "Zhao Song", "authors": "Haotian Jiang, Yin Tat Lee, Zhao Song, Sam Chiu-wai Wong", "title": "An Improved Cutting Plane Method for Convex Optimization, Convex-Concave\n  Games and its Applications", "comments": "STOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Given a separation oracle for a convex set $K \\subset \\mathbb{R}^n$ that is\ncontained in a box of radius $R$, the goal is to either compute a point in $K$\nor prove that $K$ does not contain a ball of radius $\\epsilon$. We propose a\nnew cutting plane algorithm that uses an optimal $O(n \\log (\\kappa))$\nevaluations of the oracle and an additional $O(n^2)$ time per evaluation, where\n$\\kappa = nR/\\epsilon$.\n  $\\bullet$ This improves upon Vaidya's $O( \\text{SO} \\cdot n \\log (\\kappa) +\nn^{\\omega+1} \\log (\\kappa))$ time algorithm [Vaidya, FOCS 1989a] in terms of\npolynomial dependence on $n$, where $\\omega < 2.373$ is the exponent of matrix\nmultiplication and $\\text{SO}$ is the time for oracle evaluation.\n  $\\bullet$ This improves upon Lee-Sidford-Wong's $O( \\text{SO} \\cdot n \\log\n(\\kappa) + n^3 \\log^{O(1)} (\\kappa))$ time algorithm [Lee, Sidford and Wong,\nFOCS 2015] in terms of dependence on $\\kappa$.\n  For many important applications in economics, $\\kappa = \\Omega(\\exp(n))$ and\nthis leads to a significant difference between $\\log(\\kappa)$ and\n$\\mathrm{poly}(\\log (\\kappa))$. We also provide evidence that the $n^2$ time\nper evaluation cannot be improved and thus our running time is optimal.\n  A bottleneck of previous cutting plane methods is to compute leverage scores,\na measure of the relative importance of past constraints. Our result is\nachieved by a novel multi-layered data structure for leverage score\nmaintenance, which is a sophisticated combination of diverse techniques such as\nrandom projection, batched low-rank update, inverse maintenance, polynomial\ninterpolation, and fast rectangular matrix multiplication. Interestingly, our\nmethod requires a combination of different fast rectangular matrix\nmultiplication algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 20:56:40 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Jiang", "Haotian", ""], ["Lee", "Yin Tat", ""], ["Song", "Zhao", ""], ["Wong", "Sam Chiu-wai", ""]]}, {"id": "2004.04344", "submitter": "Sayed Kamaledin Ghiasi-Shirazi", "authors": "Kamaledin Ghiasi-Shirazi, Taraneh Ghandi, Ali Taghizadeh, Ali\n  Rahimi-Baigi", "title": "A Pedagogically Sound yet Efficient Deletion algorithm for Red-Black\n  Trees: The Parity-Seeking Delete Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Red-black (RB) trees are one of the most efficient variants of balanced\nbinary search trees. However, they have always been blamed for being too\ncomplicated, hard to explain, and not suitable for pedagogical purposes.\nSedgewick (2008) proposed left-leaning red-black (LLRB) trees in which red\nlinks are restricted to left children, and proposed recursive concise insert\nand delete algorithms. However, the top-down deletion algorithm of LLRB is\nstill very complicated and highly inefficient. In this paper, we first consider\n2-3 red-black trees in which both children cannot be red. We propose a\nparity-seeking delete algorithm with the basic idea of making the deficient\nsubtree on a par with its sibling: either by fixing the deficient subtree or by\nmaking the sibling deficient, as well, ascending deficiency to the parent node.\nThis is the first pedagogically sound algorithm for the delete operation in\nred-black trees. Then, we amend our algorithm and propose a parity-seeking\ndelete algorithm for classical RB trees. Our experiments show that, despite\nhaving more rotations, 2-3 RB trees are almost as efficient as RB trees and\ntwice faster than LLRB trees. Besides, RB trees with the proposed\nparity-seeking delete algorithm have the same number of rotations and almost\nidentical running time as the classic delete algorithm. While being extremely\nefficient, the proposed parity-seeking delete algorithm is easily\nunderstandable and suitable for pedagogical purposes.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 03:12:03 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Ghiasi-Shirazi", "Kamaledin", ""], ["Ghandi", "Taraneh", ""], ["Taghizadeh", "Ali", ""], ["Rahimi-Baigi", "Ali", ""]]}, {"id": "2004.04496", "submitter": "Maximilian Probst Gutenberg", "authors": "Aaron Bernstein, Maximilian Probst Gutenberg and Christian\n  Wulff-Nilsen", "title": "Near-Optimal Decremental SSSP in Dense Weighted Digraphs", "comments": "Accepted to FOCS'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the decremental Single-Source Shortest Path problem (SSSP), we are given a\nweighted directed graph $G=(V,E,w)$ undergoing edge deletions and a source\nvertex $r \\in V$; let $n = |V|, m = |E|$ and $W$ be the aspect ratio of the\ngraph. The goal is to obtain a data structure that maintains shortest paths\nfrom $r$ to all vertices in $V$ and can answer distance queries in $O(1)$ time,\nas well as return the corresponding path $P$ in $O(|P|)$ time.\n  This problem was first considered by Even and Shiloach [JACM'81], who\nprovided an algorithm with total update time $O(mn)$ for unweighted undirected\ngraphs; this was later extended to directed weighted graphs [FOCS'95, STOC'99].\nThere are conditional lower bounds showing that $O(mn)$ is in fact near-optimal\n[ESA'04, FOCS'14, STOC'15, STOC'20]. In a breakthrough result, Forster et al.\nshowed that it is possible to achieve total update time $mn^{0.9+o(1)}\\log W$\nif the algorithm is allowed to return $(1+{\\epsilon})$-approximate paths,\ninstead of exact ones [STOC'14, ICALP'15]. No further progress was made until\nProbst Gutenberg and Wulff-Nilsen [SODA'20] provided a new approach for the\nproblem, which yields total time $\\tilde{O}(\\min{m^{2/3}n^{4/3}\\log W,\n(mn)^{7/8} \\log W})$.\n  Our result builds on this recent approach, but overcomes its limitations by\nintroducing a significantly more powerful abstraction, as well as a different\ncore subroutine. Our new framework yields a decremental\n$(1+{\\epsilon})$-approximate SSSP data structure with total update time\n$\\tilde{O}(n^2 \\log^4 W)$. Our algorithm is thus near-optimal for dense graphs\nwith polynomial edge-weights. Our framework can also be applied to sparse\ngraphs to obtain total update time $\\tilde{O}(mn^{2/3} \\log^3 W)$.\n  Our main technique allows us to convert SSSP algorithms for DAGs to ones for\ngeneral graphs, which we believe has significant potential to influence future\nwork.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 11:50:41 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 18:38:04 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Bernstein", "Aaron", ""], ["Gutenberg", "Maximilian Probst", ""], ["Wulff-Nilsen", "Christian", ""]]}, {"id": "2004.04586", "submitter": "Shuhei Denzumi", "authors": "Kotaro Matsuda, Shuhei Denzumi and Kunihiko Sadakane", "title": "Storing Set Families More Compactly with Top ZDDs", "comments": "19 pages; Accepted for SEA2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-suppressed Binary Decision Diagrams (ZDDs) are data structures for\nrepresenting set families in a compressed form. With ZDDs, many valuable\noperations on set families can be done in time polynomial in ZDD size. In some\ncases, however, the size of ZDDs for representing large set families becomes\ntoo huge to store them in the main memory. This paper proposes top ZDD, a novel\nrepresentation of ZDDs which uses less space than existing ones. The top ZDD is\nan extension of top tree, which compresses trees, to compress directed acyclic\ngraphs by sharing identical subgraphs. We prove that navigational operations on\nZDDs can be done in time poly-logarithmicin ZDD size, and show that there exist\nset families for which the size of the top ZDD is exponentially smaller than\nthat of the ZDD. We also show experimentally that our top ZDDs have smaller\nsize than ZDDs for real data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 15:14:15 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Matsuda", "Kotaro", ""], ["Denzumi", "Shuhei", ""], ["Sadakane", "Kunihiko", ""]]}, {"id": "2004.04666", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Chen Wang", "title": "Exploration with Limited Memory: Streaming Algorithms for Coin Tossing,\n  Noisy Comparisons, and Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following abstract coin tossing problem: Given a set of $n$\ncoins with unknown biases, find the most biased coin using a minimal number of\ncoin tosses. This is a common abstraction of various exploration problems in\ntheoretical computer science and machine learning and has been studied\nextensively over the years. In particular, algorithms with optimal sample\ncomplexity (number of coin tosses) have been known for this problem for quite\nsome time.\n  Motivated by applications to processing massive datasets, we study the space\ncomplexity of solving this problem with optimal number of coin tosses in the\nstreaming model. In this model, the coins are arriving one by one and the\nalgorithm is only allowed to store a limited number of coins at any point --\nany coin not present in the memory is lost and can no longer be tossed or\ncompared to arriving coins. Prior algorithms for the coin tossing problem with\noptimal sample complexity are based on iterative elimination of coins which\ninherently require storing all the coins, leading to memory-inefficient\nstreaming algorithms.\n  We remedy this state-of-affairs by presenting a series of improved streaming\nalgorithms for this problem: we start with a simple algorithm which require\nstoring only $O(\\log{n})$ coins and then iteratively refine it further and\nfurther, leading to algorithms with $O(\\log\\log{(n)})$ memory, $O(\\log^*{(n)})$\nmemory, and finally a one that only stores a single extra coin in memory -- the\nsame exact space needed to just store the best coin throughout the stream.\n  Furthermore, we extend our algorithms to the problem of finding the $k$ most\nbiased coins as well as other exploration problems such as finding top-$k$\nelements using noisy comparisons or finding an $\\epsilon$-best arm in\nstochastic multi-armed bandits, and obtain efficient streaming algorithms for\nthese problems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 16:54:43 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Assadi", "Sepehr", ""], ["Wang", "Chen", ""]]}, {"id": "2004.04772", "submitter": "Ofir Geri", "authors": "Edith Cohen, Ofir Geri, Rasmus Pagh", "title": "Composable Sketches for Functions of Frequencies: Beyond the Worst Case", "comments": "Full version of a paper from ICML 2020. Python implementation\n  available as part of the supplemental material accompanying the ICML\n  publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been increased interest in using machine learning\ntechniques to improve classical algorithms. In this paper we study when it is\npossible to construct compact, composable sketches for weighted sampling and\nstatistics estimation according to functions of data frequencies. Such\nstructures are now central components of large-scale data analytics and machine\nlearning pipelines. However, many common functions, such as thresholds and\n$p$th frequency moments with $p>2$, are known to require polynomial size\nsketches in the worst case. We explore performance beyond the worst case under\ntwo different types of assumptions. The first is having access to noisy advice\non item frequencies. This continues the line of work of Hsu et al. (ICLR 2019),\nwho assume predictions are provided by a machine learning model. The second is\nproviding guaranteed performance on a restricted class of input frequency\ndistributions that are better aligned with what is observed in practice. This\nextends the work on heavy hitters under Zipfian distributions in a seminal\npaper of Charikar et al. (ICALP 2002). Surprisingly, we show analytically and\nempirically that \"in practice\" small polylogarithmic-size sketches provide\naccuracy for \"hard\" functions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 19:00:37 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 01:15:54 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Cohen", "Edith", ""], ["Geri", "Ofir", ""], ["Pagh", "Rasmus", ""]]}, {"id": "2004.04858", "submitter": "Massimiliano Rossi", "authors": "Zsuzsanna Lipt\\'ak, Simon J. Puglisi and Massimiliano Rossi", "title": "Pattern Discovery in Colored Strings", "comments": "22 pages, 5 figures, 2 tables, published in ACM Journal of\n  Experimental Algorithmics. This is the journal version of the paper with the\n  same title at SEA 2020 (18th Symposium on Experimental Algorithms, Catania,\n  Italy, June 16-18, 2020)", "journal-ref": null, "doi": "10.1145/3429280", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of identifying patterns of interest in\ncolored strings. A colored string is a string where each position is assigned\none of a finite set of colors. Our task is to find substrings of the colored\nstring that always occur followed by the same color at the same distance. The\nproblem is motivated by applications in embedded systems verification, in\nparticular, assertion mining. The goal there is to automatically find\nproperties of the embedded system from the analysis of its simulation traces.\n  We show that, in our setting, the number of patterns of interest is\nupper-bounded by $\\mathcal{O}(n^2)$, where $n$ is the length of the string. We\nintroduce a baseline algorithm, running in $\\mathcal{O}(n^2)$ time, which\nidentifies all patterns of interest satisfying certain minimality conditions,\nfor all colors in the string. For the case where one is interested in patterns\nrelated to one color only, we also provide a second algorithm which runs in\n$\\mathcal{O}(n^2\\log n)$ time in the worst case but is faster than the baseline\nalgorithm in practice. Both solutions use suffix trees, and the second\nalgorithm also uses an appropriately defined priority queue, which allows us to\nreduce the number of computations. We performed an experimental evaluation of\nthe proposed approaches over both synthetic and real-world datasets, and found\nthat the second algorithm outperforms the first algorithm on all simulated\ndata, while on the real-world data, the performance varies between a slight\nslowdown (on half of the datasets) and a speedup by a factor of up to 11.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 23:51:23 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 15:35:35 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Lipt\u00e1k", "Zsuzsanna", ""], ["Puglisi", "Simon J.", ""], ["Rossi", "Massimiliano", ""]]}, {"id": "2004.04978", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr, Martin Krejca", "title": "A Simplified Run Time Analysis of the Univariate Marginal Distribution\n  Algorithm on LeadingOnes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With elementary means, we prove a stronger run time guarantee for the\nunivariate marginal distribution algorithm (UMDA) optimizing the LeadingOnes\nbenchmark function in the desirable regime with low genetic drift. If the\npopulation size is at least quasilinear, then, with high probability, the UMDA\nsamples the optimum within a number of iterations that is linear in the problem\nsize divided by the logarithm of the UMDA's selection rate. This improves over\nthe previous guarantee, obtained by Dang and Lehre (2015) via the deep\nlevel-based population method, both in terms of the run time and by\ndemonstrating further run time gains from small selection rates. With similar\narguments as in our upper-bound analysis, we also obtain the first lower bound\nfor this problem. Under similar assumptions, we prove that a bound that matches\nour upper bound up to constant factors holds with high probability.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 10:20:05 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Doerr", "Benjamin", ""], ["Krejca", "Martin", ""]]}, {"id": "2004.05018", "submitter": "Daniel Paulusma", "authors": "Nick Brettell, Jake Horsfield, Andrea Munaro, Giacomo Paesani, Daniel\n  Paulusma", "title": "Bounding the Mim-Width of Hereditary Graph Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of NP-hard graph problems become polynomial-time solvable on\ngraph classes where the mim-width is bounded and quickly computable. Hence,\nwhen solving such problems on special graph classes, it is helpful to know\nwhether the graph class under consideration has bounded mim-width. We first\nextend the toolkit for proving (un)boundedness of mim-width of graph classes.\nThis enables us to initiate a systematic study into bounding mim-width from the\nperspective of hereditary graph classes. For a given graph $H$, the class of\n$H$-free graphs has bounded mim-width if and only if it has bounded\nclique-width. We show that the same is not true for $(H_1,H_2)$-free graphs. We\nfind several general classes of $(H_1,H_2)$-free graphs having unbounded\nclique-width, but the mim-width is bounded and quickly computable. We also\nprove a number of new results showing that, for certain $H_1$ and $H_2$, the\nclass of $(H_1,H_2)$-free graphs has unbounded mim-width. Combining these with\nknown results, we present summary theorems of the current state of the art for\nthe boundedness of mim-width for $(H_1,H_2)$-free graphs.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 13:02:31 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 17:59:47 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 01:06:56 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Brettell", "Nick", ""], ["Horsfield", "Jake", ""], ["Munaro", "Andrea", ""], ["Paesani", "Giacomo", ""], ["Paulusma", "Daniel", ""]]}, {"id": "2004.05022", "submitter": "Daniel Paulusma", "authors": "Nick Brettell, Jake Horsfield, Daniel Paulusma", "title": "Colouring $(sP_1+P_5)$-Free Graphs: a Mim-Width Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the class of $(K_t,sP_1+P_5)$-free graphs has bounded mim-width\nfor every $s\\geq 0$ and $t\\geq 1$, and that there is a polynomial-time\nalgorithm that, given a graph in the class, computes a branch decomposition of\nconstant mim-width. A large number of \\NP-complete graph problems become\npolynomial-time solvable on graph classes with bounded mim-width and for which\na branch decomposition is quickly computable. The $k$-Colouring problem is an\nexample of such a problem. For this problem, we may assume that the input graph\nis $K_{k+1}$-free. Then, as a consequence of our result, we obtain a new proof\nfor the known result that for every fixed $k\\geq 1$ and $s\\geq 0$,\n$k$-Colouring is polynomial-time solvable for $(sP_1+P_5)$-free graphs. In\nfact, our findings show that the underlying reason for this polynomial-time\nalgorithm is that the class has bounded mim-width.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 13:12:32 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 19:11:48 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Brettell", "Nick", ""], ["Horsfield", "Jake", ""], ["Paulusma", "Daniel", ""]]}, {"id": "2004.05309", "submitter": "Dominik K\\\"oppl", "authors": "Kazuya Tsuruta and Dominik K\\\"oppl and Yuto Nakashima and Shunsuke\n  Inenaga and Hideo Bannai and Masayuki Takeda", "title": "Grammar-compressed Self-index with Lyndon Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of straight-line programs (SLPs), named the Lyndon\nSLP, inspired by the Lyndon trees (Barcelo, 1990). Based on this SLP, we\npropose a self-index data structure of $O(g)$ words of space that can be built\nfrom a string $T$ in $O(n \\lg n)$ expected time, retrieving the starting\npositions of all occurrences of a pattern $P$ of length $m$ in $O(m + \\lg m \\lg\nn + occ \\lg g)$ time, where $n$ is the length of $T$, $g$ is the size of the\nLyndon SLP for $T$, and $occ$ is the number of occurrences of $P$ in $T$.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 05:27:31 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 06:14:12 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Tsuruta", "Kazuya", ""], ["K\u00f6ppl", "Dominik", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "2004.05345", "submitter": "Qiang Huang", "authors": "Yifan Lei, Qiang Huang, Mohan Kankanhalli, Anthony K. H. Tung", "title": "Locality-Sensitive Hashing Scheme based on Longest Circular Co-Substring", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality-Sensitive Hashing (LSH) is one of the most popular methods for\n$c$-Approximate Nearest Neighbor Search ($c$-ANNS) in high-dimensional spaces.\nIn this paper, we propose a novel LSH scheme based on the Longest Circular\nCo-Substring (LCCS) search framework (LCCS-LSH) with a theoretical guarantee.\nWe introduce a novel concept of LCCS and a new data structure named Circular\nShift Array (CSA) for $k$-LCCS search. The insight of LCCS search framework is\nthat close data objects will have a longer LCCS than the far-apart ones with\nhigh probability. LCCS-LSH is \\emph{LSH-family-independent}, and it supports\n$c$-ANNS with different kinds of distance metrics. We also introduce a\nmulti-probe version of LCCS-LSH and conduct extensive experiments over five\nreal-life datasets. The experimental results demonstrate that LCCS-LSH\noutperforms state-of-the-art LSH schemes.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 09:24:51 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Lei", "Yifan", ""], ["Huang", "Qiang", ""], ["Kankanhalli", "Mohan", ""], ["Tung", "Anthony K. H.", ""]]}, {"id": "2004.05429", "submitter": "Debabrota Basu", "authors": "Naheed Anjum Arafat, Debabrota Basu, Laurent Decreusefond, Stephane\n  Bressan", "title": "Construction and Random Generation of Hypergraphs with Prescribed Degree\n  and Dimension Sequences", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI math.CO stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose algorithms for construction and random generation of hypergraphs\nwithout loops and with prescribed degree and dimension sequences. The objective\nis to provide a starting point for as well as an alternative to Markov chain\nMonte Carlo approaches. Our algorithms leverage the transposition of properties\nand algorithms devised for matrices constituted of zeros and ones with\nprescribed row- and column-sums to hypergraphs. The construction algorithm\nextends the applicability of Markov chain Monte Carlo approaches when the\ninitial hypergraph is not provided. The random generation algorithm allows the\ndevelopment of a self-normalised importance sampling estimator for hypergraph\nproperties such as the average clustering coefficient.We prove the correctness\nof the proposed algorithms. We also prove that the random generation algorithm\ngenerates any hypergraph following the prescribed degree and dimension\nsequences with a non-zero probability. We empirically and comparatively\nevaluate the effectiveness and efficiency of the random generation algorithm.\nExperiments show that the random generation algorithm provides stable and\naccurate estimates of average clustering coefficient, and also demonstrates a\nbetter effective sample size in comparison with the Markov chain Monte Carlo\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 15:44:14 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Arafat", "Naheed Anjum", ""], ["Basu", "Debabrota", ""], ["Decreusefond", "Laurent", ""], ["Bressan", "Stephane", ""]]}, {"id": "2004.05548", "submitter": "Ke Chen", "authors": "Ke Chen and Adrian Dumitrescu", "title": "Multiparty Selection", "comments": "12 pages, 2 figures, and new section for finding an approximate\n  median among $k$ players has been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a sequence $A$ of $n$ numbers and an integer (target) parameter $1\\leq\ni\\leq n$, the (exact) selection problem asks to find the $i$-th smallest\nelement in $A$. An element is said to be $(i,j)$-mediocre if it is neither\namong the top $i$ nor among the bottom $j$ elements of $S$. The approximate\nselection problem asks to find a $(i,j)$-mediocre element for some given $i,j$;\nas such, this variant allows the algorithm to return any element in a\nprescribed range. In the first part, we revisit the selection problem in the\ntwo-party model introduced by Andrew Yao (1979) and then extend our study of\nexact selection to the multiparty model. In the second part, we deduce some\ncommunication complexity benefits that arise in approximate selection. In\nparticular, we present a deterministic protocol for finding an approximate\nmedian among $k$ players.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 05:33:39 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 01:12:41 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Chen", "Ke", ""], ["Dumitrescu", "Adrian", ""]]}, {"id": "2004.05672", "submitter": "Julliano Rosa Nascimento", "authors": "Flavia Bonomo-Braberman, Julliano R. Nascimento, Fabiano S. Oliveira,\n  U\\'everton S. Souza, and Jayme L. Szwarcfiter", "title": "Linear-time Algorithms for Eliminating Claws in Graphs", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since many NP-complete graph problems have been shown polynomial-time\nsolvable when restricted to claw-free graphs, we study the problem of\ndetermining the distance of a given graph to a claw-free graph, considering\nvertex elimination as measure. CLAW-FREE VERTEX DELETION (CFVD) consists of\ndetermining the minimum number of vertices to be removed from a graph such that\nthe resulting graph is claw-free. Although CFVD is NP-complete in general and\nrecognizing claw-free graphs is still a challenge, where the current best\nalgorithm for a graph $G$ has the same running time of the best algorithm for\nmatrix multiplication, we present linear-time algorithms for CFVD on weighted\nblock graphs and weighted graphs with bounded treewidth. Furthermore, we show\nthat this problem can be solved in linear time by a simpler algorithm on\nforests, and we determine the exact values for full $k$-ary trees. On the other\nhand, we show that CLAW-FREE VERTEX DELETION is NP-complete even when the input\ngraph is a split graph. We also show that the problem is hard to approximate\nwithin any constant factor better than $2$, assuming the Unique Games\nConjecture.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 18:49:41 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Bonomo-Braberman", "Flavia", ""], ["Nascimento", "Julliano R.", ""], ["Oliveira", "Fabiano S.", ""], ["Souza", "U\u00e9verton S.", ""], ["Szwarcfiter", "Jayme L.", ""]]}, {"id": "2004.05721", "submitter": "Chunjiang Zhu", "authors": "Chun Jiang Zhu, Song Han, Kam-Yiu Lam", "title": "A Fast Algorithm for Source-wise Round-trip Spanners", "comments": "Accepted and to appear in Theoretical Computer Science (TCS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of fast constructions of source-wise\nround-trip spanners in weighted directed graphs. For a source vertex set\n$S\\subseteq V$ in a graph $G(V,E)$, an $S$-sourcewise round-trip spanner of $G$\nof stretch $k$ is a subgraph $H$ of $G$ such that for every pair of vertices\n$u,v\\in S\\times V$, their round-trip distance in $H$ is at most $k$ times of\ntheir round-trip distance in $G$. We show that for a graph $G(V,E)$ with $n$\nvertices and $m$ edges, an $s$-sized source vertex set $S\\subseteq V$ and an\ninteger $k>1$, there exists an algorithm that in time $O(ms^{1/k}\\log^5n)$\nconstructs an $S$-sourcewise round-trip spanner of stretch $O(k\\log n)$ and\n$O(ns^{1/k}\\log^2n)$ edges with high probability. Compared to the fast\nalgorithms for constructing all-pairs round-trip spanners \\cite{PRS+18,CLR+20},\nour algorithm improve the running time and the number of edges in the spanner\nwhen $k$ is super-constant. Compared with the existing algorithm for\nconstructing source-wise round-trip spanners \\cite{ZL17}, our algorithm\nsignificantly improves their construction time $\\Omega(\\min\\{ms,n^\\omega\\})$\n(where $\\omega \\in [2,2.373)$ and 2.373 is the matrix multiplication exponent)\nto nearly linear $O(ms^{1/k}\\log^5n)$, at the expense of paying an extra\n$O(\\log n)$ in the stretch. As an important building block of the algorithm, we\ndevelop a graph partitioning algorithm to partition $G$ into clusters of\nbounded radius and prove that for every $u,v\\in S\\times V$ at small round-trip\ndistance, the probability of separating them in different clusters is small.\nThe algorithm takes the size of $S$ as input and does not need the knowledge of\n$S$. With the algorithm and a reachability vertex size estimation algorithm, we\nshow that the recursive algorithm for constructing standard round-trip spanners\n\\cite{PRS+18} can be adapted to the source-wise setting.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 23:54:48 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 17:38:11 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Zhu", "Chun Jiang", ""], ["Han", "Song", ""], ["Lam", "Kam-Yiu", ""]]}, {"id": "2004.05738", "submitter": "Mingmou Liu", "authors": "Mingmou Liu, Huacheng Yu", "title": "Lower Bound for Succinct Range Minimum Query", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an integer array $A[1..n]$, the Range Minimum Query problem (RMQ) asks\nto preprocess $A$ into a data structure, supporting RMQ queries: given $a,b\\in\n[1,n]$, return the index $i\\in[a,b]$ that minimizes $A[i]$, i.e.,\n$\\mathrm{argmin}_{i\\in[a,b]} A[i]$. This problem has a classic solution using\n$O(n)$ space and $O(1)$ query time by Gabow, Bentley, Tarjan (STOC, 1984) and\nHarel, Tarjan (SICOMP, 1984). The best known data structure by Fischer, Heun\n(SICOMP, 2011) and Navarro, Sadakane (TALG, 2014) uses $2n+n/(\\frac{\\log\nn}{t})^t+\\tilde{O}(n^{3/4})$ bits and answers queries in $O(t)$ time, assuming\nthe word-size is $w=\\Theta(\\log n)$. In particular, it uses\n$2n+n/\\mathrm{poly}\\log n$ bits of space as long as the query time is a\nconstant.\n  In this paper, we prove the first lower bound for this problem, showing that\n$2n+n/\\mathrm{poly}\\log n$ space is necessary for constant query time. In\ngeneral, we show that if the data structure has query time $O(t)$, then it must\nuse at least $2n+n/(\\log n)^{\\tilde{O}(t^2)}$ space, in the cell-probe model\nwith word-size $w=\\Theta(\\log n)$.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 01:20:51 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Liu", "Mingmou", ""], ["Yu", "Huacheng", ""]]}, {"id": "2004.05813", "submitter": "Somnath Chakraborty", "authors": "Somnath Chakraborty, Hariharan Narayanan", "title": "Learning Mixtures of Spherical Gaussians via Fourier Analysis", "comments": "A few omissions are taken care of, and some more references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we are given independent, identically distributed samples $x_l$\nfrom a mixture $\\mu$ of no more than $k$ of $d$-dimensional spherical gaussian\ndistributions $\\mu_i$ with variance $1$, such that the minimum $\\ell_2$\ndistance between two distinct centers $y_l$ and $y_j$ is greater than $\\sqrt{d}\n\\Delta$ for some $c \\leq \\Delta $, where $c\\in (0,1)$ is a small positive\nuniversal constant. We develop a randomized algorithm that learns the centers\n$y_l$ of the gaussians, to within an $\\ell_2$ distance of $\\delta <\n\\frac{\\Delta\\sqrt{d}}{2}$ and the weights $w_l$ to within $cw_{min}$ with\nprobability greater than $1 - \\exp(-k/c)$. The number of samples and the\ncomputational time is bounded above by $poly(k, d, \\frac{1}{\\delta})$. Such a\nbound on the sample and computational complexity was previously unknown when\n$\\omega(1) \\leq d \\leq O(\\log k)$. When $d = O(1)$, this follows from work of\nRegev and Vijayaraghavan. These authors also show that the sample complexity of\nlearning a random mixture of gaussians in a ball of radius $\\Theta(\\sqrt{d})$\nin $d$ dimensions, when $d$ is $\\Theta( \\log k)$ is at least $poly(k,\n\\frac{1}{\\delta})$, showing that our result is tight in this case.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 08:06:29 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 09:08:11 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Chakraborty", "Somnath", ""], ["Narayanan", "Hariharan", ""]]}, {"id": "2004.05875", "submitter": "Igor Potapov", "authors": "Argyrios Deligkas and Igor Potapov", "title": "Optimizing Reachability Sets in Temporal Graphs by Delaying", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A temporal graph is a dynamic graph where every edge is assigned a set of\ninteger time labels that indicate at which discrete time step the edge is\navailable. In this paper, we study how changes of the time labels,\ncorresponding to delays on the availability of the edges, affect the\nreachability sets from given sources. The questions about reachability sets are\nmotivated by numerous applications of temporal graphs in network epidemiology,\nwhich aim to minimise the spread of infection, and scheduling problems in\nsupply networks in manufacturing with the opposite objectives of maximising\ncoverage and productivity. We introduce control mechanisms for reachability\nsets that are based on two natural operations of delaying. The first operation,\ntermed merging, is global and batches together consecutive time labels into a\nsingle time label in the whole network simultaneously. This corresponds to\npostponing all events until a particular time. The second, imposes independent\ndelays on the time labels of every edge of the graph. We provide a thorough\ninvestigation of the computational complexity of different objectives related\nto reachability sets when these operations are used. For the merging operation,\ni.e. global lockdown effect, we prove NP-hardness results for several\nminimization and maximization reachability objectives, even for very simple\ngraph structures. For the second operation, independent delays, we prove that\nthe minimization problems are NP-hard when the number of allowed delays is\nbounded. We complement this with a polynomial-time algorithm for minimising the\nreachability set in case of unbounded delays.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 11:36:52 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 16:59:54 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Deligkas", "Argyrios", ""], ["Potapov", "Igor", ""]]}, {"id": "2004.05935", "submitter": "Suman Banerjee", "authors": "Suman Banerjee and Bithika Pal", "title": "First Stretch then Shrink and Bulk: A Two Phase Approach for Enumeration\n  of Maximal $(\\Delta, \\gamma)$\\mbox{-}Cliques of a Temporal Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A \\emph{Temporal Network} (also known as \\emph{Link Stream} or\n\\emph{Time-Varying Graph}) is often used to model a time-varying relationship\namong a group of agents. It is typically represented as a collection of\ntriplets of the form $(u,v,t)$ that denotes the interaction between the agents\n$u$ and $v$ at time $t$. For analyzing the contact patterns of the agents\nforming a temporal network, recently the notion of classical \\textit{clique} of\na \\textit{static graph} has been generalized as \\textit{$\\Delta$\\mbox{-}Clique}\nof a Temporal Network. In the same direction, one of our previous studies\nintroduces the notion of \\textit{$(\\Delta, \\gamma)$\\mbox{-}Clique}, which is\nbasically a \\textit{vertex set}, \\textit{time interval} pair, in which every\npair of the clique vertices are linked at least $\\gamma$ times in every\n$\\Delta$ duration of the time interval. In this paper, we propose a different\nmethodology for enumerating all the maximal $(\\Delta, \\gamma)$\\mbox{-}Cliques\nof a given temporal network. The proposed methodology is broadly divided into\ntwo phases. In the first phase, each temporal link is processed for\nconstructing $(\\Delta, \\gamma)$\\mbox{-}Clique(s) with maximum duration. In the\nsecond phase, these initial cliques are expanded by vertex addition to form the\nmaximal cliques. From the experimentation carried out on $5$ real\\mbox{-}world\ntemporal network datasets, we observe that the proposed methodology enumerates\nall the maximal $(\\Delta,\\gamma)$\\mbox{-}Cliques efficiently, particularly when\nthe dataset is sparse. As a special case ($\\gamma=1$), the proposed methodology\nis also able to enumerate $(\\Delta,1) \\equiv \\Delta$\\mbox{-}cliques with much\nless time compared to the existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 18:42:47 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Banerjee", "Suman", ""], ["Pal", "Bithika", ""]]}, {"id": "2004.05954", "submitter": "Thomas Lidbetter Dr", "authors": "Felix Happach, Lisa Hellerstein and Thomas Lidbetter", "title": "A General Framework for Approximating Min Sum Ordering Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a large family of problems in which an ordering (or, more\nprecisely, a chain of subsets) of a finite set must be chosen to minimize some\nweighted sum of costs. This family includes variations of Min Sum Set Cover\n(MSSC), several scheduling and search problems, and problems in Boolean\nfunction evaluation. We define a new problem, called the Min Sum Ordering\nProblem (MSOP) which generalizes all these problems using a cost and a weight\nfunction defined on subsets of a finite set. Assuming a polynomial time\n$\\alpha$-approximation algorithm for the problem of finding a subset whose\nratio of weight to cost is maximal, we show that under very minimal\nassumptions, there is a polynomial time $4 \\alpha$-approximation algorithm for\nMSOP. This approximation result generalizes a proof technique used for several\ndistinct problems in the literature. We apply this to obtain a number of new\napproximation results.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 14:09:45 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 21:42:41 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 19:16:10 GMT"}, {"version": "v4", "created": "Fri, 25 Jun 2021 00:07:33 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Happach", "Felix", ""], ["Hellerstein", "Lisa", ""], ["Lidbetter", "Thomas", ""]]}, {"id": "2004.05961", "submitter": "Rahul Vaze", "authors": "Akhil Bhimaraju, Debanuj Nayak, Rahul Vaze", "title": "Non-clairvoyant Scheduling of Coflows", "comments": "To Appear in Proc. WiOpt 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coflow scheduling problem is considered: given an input/output switch\nwith each port having a fixed capacity, find a scheduling algorithm that\nminimizes the weighted sum of the coflow completion times respecting the port\ncapacities, where each flow of a coflow has a demand per input/output port, and\ncoflow completion time is the finishing time of the last flow of the coflow.\nThe objective of this paper is to present theoretical guarantees on\napproximating the sum of coflow completion time in the non-clairvoyant setting,\nwhere on a coflow arrival, only the number of flows, and their input-output\nport is revealed, while the critical demand volumes for each flow on the\nrespective input-output port is unknown. The main result of this paper is to\nshow that the proposed BlindFlow algorithm is $8p$-approximate, where $p$ is\nthe largest number of input-output port pairs that a coflow uses. This result\nholds even in the online case, where coflows arrive over time and the scheduler\nhas to use only causal information. Simulations reveal that the experimental\nperformance of BlindFlow is far better than the theoretical guarantee.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 14:25:43 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Bhimaraju", "Akhil", ""], ["Nayak", "Debanuj", ""], ["Vaze", "Rahul", ""]]}, {"id": "2004.05975", "submitter": "Uri Stemmer", "authors": "Avinatan Hassidim, Haim Kaplan, Yishay Mansour, Yossi Matias, Uri\n  Stemmer", "title": "Adversarially Robust Streaming Algorithms via Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A streaming algorithm is said to be adversarially robust if its accuracy\nguarantees are maintained even when the data stream is chosen maliciously, by\nan adaptive adversary. We establish a connection between adversarial robustness\nof streaming algorithms and the notion of differential privacy. This connection\nallows us to design new adversarially robust streaming algorithms that\noutperform the current state-of-the-art constructions for many interesting\nregimes of parameters.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 14:49:38 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Hassidim", "Avinatan", ""], ["Kaplan", "Haim", ""], ["Mansour", "Yishay", ""], ["Matias", "Yossi", ""], ["Stemmer", "Uri", ""]]}, {"id": "2004.06036", "submitter": "George Mertzios", "authors": "Argyrios Deligkas, George B. Mertzios, Paul G. Spirakis, and Viktor\n  Zamaraev", "title": "Exact and Approximate Algorithms for Computing a Second Hamiltonian\n  Cycle", "comments": "28 pages, 4 algorithms, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the following total functional problem: Given a\ncubic Hamiltonian graph $G$ and a Hamiltonian cycle $C_0$ of $G$, how can we\ncompute a second Hamiltonian cycle $C_1 \\neq C_0$ of $G$? Cedric Smith proved\nin 1946, using a non-constructive parity argument, that such a second\nHamiltonian cycle always exists. Our main result is an algorithm which computes\nthe second Hamiltonian cycle in time $O(n \\cdot 2^{(0.3-\\varepsilon)n})$ time,\nfor some positive constant $\\varepsilon>0$, and in polynomial space, thus\nimproving the state of the art running time for solving this problem. Our\nalgorithm is based on a fundamental structural property of Thomason's lollipop\nalgorithm, which we prove here for the first time. In the direction of\napproximating the length of a second cycle in a Hamiltonian graph $G$ with a\ngiven Hamiltonian cycle $C_0$ (where we may not have guarantees on the\nexistence of a second Hamiltonian cycle), we provide a linear-time algorithm\ncomputing a second cycle with length at least $n - 4\\alpha\n(\\sqrt{n}+2\\alpha)+8$, where $\\alpha = \\frac{\\Delta-2}{\\delta-2}$ and\n$\\delta,\\Delta$ are the minimum and the maximum degree of the graph,\nrespectively. This approximation result also improves the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 16:11:58 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 01:58:09 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Deligkas", "Argyrios", ""], ["Mertzios", "George B.", ""], ["Spirakis", "Paul G.", ""], ["Zamaraev", "Viktor", ""]]}, {"id": "2004.06167", "submitter": "Geoffrey Ramseyer", "authors": "Ashish Goel, Geoffrey Ramseyer", "title": "Continuous Credit Networks and Layer 2 Blockchains: Monotonicity and\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve transaction rates, many cryptocurrencies have implemented\nso-called ''Layer-2'' transaction protocols, where payments are routed across\nnetworks of private payment channels. However, for a given transaction, not\nevery network state provides a feasible route to perform the payment; in this\ncase, the transaction must be put on the public ledger. The payment channel\nnetwork thus multiplies the transaction rate of the overall system; the less\nfrequently it fails, the higher the multiplier.\n  We build on earlier work on credit networks and show that this network\nliquidity problem is connected to the combinatorics of graphical matroids.\nEarlier work could only analyze the (unnatural) scenario where transactions had\ndiscrete sizes.\n  Superficially, it might seem like the continuous case would be harder to\nexamine. However, removing this assumption lets us make progress in two\nimportant directions. First, we give a partial answer to the ``monotonicity\nconjecture'' that previous work left open. This conjecture asks that the\nnetwork's performance not degrade as capacity on any edge increases. And\nsecond, we construct here a network state sampling procedure with much faster\nasymptotic performance than off-the-shelf Markov chains ($O(\\vert E\\vert\n\\beta(\\vert E\\vert))$, where $\\beta(x)$ is the complexity of solving a linear\nprogram on $x$ constraints.)\n  We obtain our results by mapping the underlying graphs to convex bodies and\nthen showing that the liquidity and sampling problems reduce to bounding and\ncomputing the volumes of these bodies. The transformation relies crucially on\nthe combinatorial properties of the underlying graphic matroid, as do the\nproofs of monotonicity and fast sampling.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 16:12:51 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 21:36:43 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2020 19:02:22 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Goel", "Ashish", ""], ["Ramseyer", "Geoffrey", ""]]}, {"id": "2004.06263", "submitter": "Huang Lingxiao", "authors": "Lingxiao Huang and Nisheeth K. Vishnoi", "title": "Coresets for Clustering in Euclidean Spaces: Importance Sampling is\n  Nearly Optimal", "comments": "Full version of STOC 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a collection of $n$ points in $\\mathbb{R}^d$, the goal of the\n$(k,z)$-clustering problem is to find a subset of $k$ \"centers\" that minimizes\nthe sum of the $z$-th powers of the Euclidean distance of each point to the\nclosest center. Special cases of the $(k,z)$-clustering problem include the\n$k$-median and $k$-means problems. Our main result is a unified two-stage\nimportance sampling framework that constructs an $\\varepsilon$-coreset for the\n$(k,z)$-clustering problem. Compared to the results for $(k,z)$-clustering in\n[Feldman and Langberg, STOC 2011], our framework saves a $\\varepsilon^2 d$\nfactor in the coreset size. Compared to the results for $(k,z)$-clustering in\n[Sohler and Woodruff, FOCS 2018], our framework saves a\n$\\operatorname{poly}(k)$ factor in the coreset size and avoids the\n$\\exp(k/\\varepsilon)$ term in the construction time. Specifically, our coreset\nfor $k$-median ($z=1$) has size $\\tilde{O}(\\varepsilon^{-4} k)$ which, when\ncompared to the result in [Sohler and Woodruff, STOC 2018], saves a $k$ factor\nin the coreset size. Our algorithmic results rely on a new dimensionality\nreduction technique that connects two well-known shape fitting problems:\nsubspace approximation and clustering, and may be of independent interest. We\nalso provide a size lower bound of $\\Omega\\left(k\\cdot \\min \\left\\{2^{z/20},d\n\\right\\}\\right)$ for a $0.01$-coreset for $(k,z)$-clustering, which has a\nlinear dependence of size on $k$ and an exponential dependence on $z$ that\nmatches our algorithmic results.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 01:48:16 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 15:50:47 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 03:32:37 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Huang", "Lingxiao", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "2004.06278", "submitter": "Bernard Widynski", "authors": "Bernard Widynski", "title": "Squares: A Fast Counter-Based RNG", "comments": "A software package with example programs is available at\n  http://squaresrng.wixsite.com/rand", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a new counter-based random number generator (RNG)\nbased on John von Neumann's middle square. We've discovered that only four\nrounds of squaring are sufficient to provide satisfactory random data. This\nappears to be one of the fastest counter-based RNGs.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 02:58:14 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 02:39:05 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 02:51:36 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Widynski", "Bernard", ""]]}, {"id": "2004.06340", "submitter": "Marc Hellmuth", "authors": "Dulce I. Valdivia, Manuela Gei{\\ss}, Maribel Hern\\'andez Rosales,\n  Peter F. Stadler and Marc Hellmuth", "title": "Hierarchical and Modularly-Minimal Vertex Colorings", "comments": "arXiv admin note: text overlap with arXiv:1906.10031", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cographs are exactly the hereditarily well-colored graphs, i.e., the graphs\nfor which a greedy vertex coloring of every induced subgraph uses only the\nminimally necessary number of colors $\\chi(G)$. We show that greedy colorings\nare a special case of the more general hierarchical vertex colorings, which\nrecently were introduced in phylogenetic combinatorics. Replacing cotrees by\nmodular decomposition trees generalizes the concept of hierarchical colorings\nto arbitrary graphs. We show that every graph has a modularly-minimal coloring\n$\\sigma$ satisfying $|\\sigma(M)|=\\chi(M)$ for every strong module $M$ of $G$.\nThis, in particular, shows that modularly-minimal colorings provide a useful\ndevice to design efficient coloring algorithms for certain hereditary graph\nclasses. For cographs, the hierarchical colorings coincide with the\nmodularly-minimal coloring. As a by-product, we obtain a simple linear-time\nalgorithm to compute a modularly-minimal coloring of $P_4$-sparse graphs.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 07:52:53 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Valdivia", "Dulce I.", ""], ["Gei\u00df", "Manuela", ""], ["Rosales", "Maribel Hern\u00e1ndez", ""], ["Stadler", "Peter F.", ""], ["Hellmuth", "Marc", ""]]}, {"id": "2004.06367", "submitter": "Aleksandar Shurbevski", "authors": "Yuui Tamura, Yuhei Nishiyama, Chenxi Wang, Yanming Sun, Aleksandar\n  Shurbevski, Hiroshi Nagamochi, Tatsuya Akutsu", "title": "Enumerating Chemical Graphs with Mono-block 2-Augmented Tree Structure\n  from Given Upper and Lower Bounds on Path Frequencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a problem of enumerating chemical graphs from given constraints\nconcerning their structures, which has an important application to a novel\nmethod for the inverse QSAR/QSPR recently proposed. In this paper, the\nstructure of a chemical graph is specified by a feature vector each of whose\nentries represents the frequency of a prescribed path. We call a graph a\n2-augmented tree if it is obtained from a tree (an acyclic graph) by adding\nedges between two pairs of nonadjacent vertices. Given a set of feature vectors\nas the interval between upper and lower bounds of feature vectors, we design an\nefficient algorithm for enumerating chemical 2-augmented trees that satisfy the\npath frequency specified by some feature vector in the set. We implemented the\nproposed algorithm and conducted some computational experiments.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 09:05:17 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Tamura", "Yuui", ""], ["Nishiyama", "Yuhei", ""], ["Wang", "Chenxi", ""], ["Sun", "Yanming", ""], ["Shurbevski", "Aleksandar", ""], ["Nagamochi", "Hiroshi", ""], ["Akutsu", "Tatsuya", ""]]}, {"id": "2004.06436", "submitter": "Yael Hitron", "authors": "Yael Hitron, Merav Parter", "title": "Round-Efficient Distributed Byzantine Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first round efficient algorithms for several fundamental\ndistributed tasks in the presence of a Byzantine edge. Our algorithms work in\nthe CONGEST model of distributed computing. In the \\emph{Byzantine Broadcast}\nproblem, given is a network $G=(V,E)$ with an unknown Byzantine edge $e'$.\nThere is a source node $s$ holding an initial message $m_0$, and the goal is\nfor all the nodes in the network to receive a copy of $m_0$, while ignoring all\nother messages. Perhaps surprisingly, to the best of our knowledge, all\nexisting algorithms for the problem either assume that the Byzantine behavior\nis probabilistic, use polynomially large messages or else suffer from a large\nround complexity.\n  We give an $\\widetilde{O}(D^2)$-round \\footnote{The notion $\\widetilde{O}$\nhides poly-logarithmic terms, and the notion $\\widehat{O}$ hides a\nmultiplicative factor of an $2^{O(\\sqrt{\\log n})}$ term.} algorithm for the\nByzantine Broadcast problem, where $D$ is the diameter of the graph. The\ncommunication graph is required to be $3$-edge connected, which is known to be\na necessary condition. We also provide a Leader Election algorithm in the\npresence of a Byzantine edge with the same round complexity of\n$\\widetilde{O}(D^2)$ rounds. We use these algorithms to provide the efficient\nconstruction of \\emph{Byzantine cycle covers} which serve the basis for (i)\nByzantine BFS algorithms and (ii) a general compiler for algorithms in the\npresence of a Byzantine edge.\n  We hope that the tools provided in this paper will pave the way towards\nobtaining \\textbf{round-efficient algorithms} for many more distributed\nproblems in the presence of Byzantine edges and nodes.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 11:56:33 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Hitron", "Yael", ""], ["Parter", "Merav", ""]]}, {"id": "2004.06474", "submitter": "Armen Allahverdyan", "authors": "Weibing Deng, R. Xie, S. Deng, and Armen E. Allahverdyan", "title": "Two halves of a meaningful text are statistically different", "comments": "15 pages and 14 tables", "journal-ref": null, "doi": "10.1088/1742-5468/abe947", "report-no": null, "categories": "cs.CL cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Which statistical features distinguish a meaningful text (possibly written in\nan unknown system) from a meaningless set of symbols? Here we answer this\nquestion by comparing features of the first half of a text to its second half.\nThis comparison can uncover hidden effects, because the halves have the same\nvalues of many parameters (style, genre {\\it etc}). We found that the first\nhalf has more different words and more rare words than the second half. Also,\nwords in the first half are distributed less homogeneously over the text in the\nsense of of the difference between the frequency and the inverse spatial\nperiod. These differences hold for the significant majority of several hundred\nrelatively short texts we studied. The statistical significance is confirmed\nvia the Wilcoxon test. Differences disappear after random permutation of words\nthat destroys the linear structure of the text. The differences reveal a\ntemporal asymmetry in meaningful texts, which is confirmed by showing that\ntexts are much better compressible in their natural way (i.e. along the\nnarrative) than in the word-inverted form. We conjecture that these results\nconnect the semantic organization of a text (defined by the flow of its\nnarrative) to its statistical features.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 20:00:12 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Deng", "Weibing", ""], ["Xie", "R.", ""], ["Deng", "S.", ""], ["Allahverdyan", "Armen E.", ""]]}, {"id": "2004.06521", "submitter": "Ashley Montanaro", "authors": "Cezar-Mihail Alexandru, Ella Bridgett-Tomkinson, Noah Linden, Joseph\n  MacManus, Ashley Montanaro and Hannah Morris", "title": "Quantum speedups of some general-purpose numerical optimisation\n  algorithms", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give quantum speedups of several general-purpose numerical optimisation\nmethods for minimising a function $f:\\mathbb{R}^n \\to \\mathbb{R}$. First, we\nshow that many techniques for global optimisation under a Lipschitz constraint\ncan be accelerated near-quadratically. Second, we show that backtracking line\nsearch, an ingredient in quasi-Newton optimisation algorithms, can be\naccelerated up to quadratically. Third, we show that a component of the\nNelder-Mead algorithm can be accelerated by up to a multiplicative factor of\n$O(\\sqrt{n})$. Fourth, we show that a quantum gradient computation algorithm of\nGily\\'en et al. can be used to approximately compute gradients in the framework\nof stochastic gradient descent. In each case, our results are based on applying\nexisting quantum algorithms to accelerate specific components of the classical\nalgorithms, rather than developing new quantum techniques.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 14:04:47 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Alexandru", "Cezar-Mihail", ""], ["Bridgett-Tomkinson", "Ella", ""], ["Linden", "Noah", ""], ["MacManus", "Joseph", ""], ["Montanaro", "Ashley", ""], ["Morris", "Hannah", ""]]}, {"id": "2004.06690", "submitter": "Robin Fritsch", "authors": "Robin Fritsch", "title": "Online Graph Exploration on Trees, Unicyclic Graphs and Cactus Graphs", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": "10.1016/j.ipl.2021.106096", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of exploring all vertices of an undirected weighted\ngraph that is initially unknown to the searcher. An edge of the graph is only\nrevealed when the searcher visits one of its endpoints. Beginning at some start\nnode, the searcher's goal is to visit every vertex of the graph before\nreturning to the start node on a tour as short as possible. We prove that the\nNearest Neighbor algorithm's competitive ratio on trees with $n$ vertices is\n$\\Theta(\\log n)$, i.e. no better than on general graphs. Furthermore, we\nexamine the algorithm Blocking for a range of parameters not considered\npreviously and prove it is 3-competitive on unicyclic graphs as well as\n$5/2+\\sqrt{2}\\approx 3.91$-competitive on cactus graphs. The best known lower\nbound for these two graph classes is 2.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 17:47:31 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 23:07:48 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Fritsch", "Robin", ""]]}, {"id": "2004.06828", "submitter": "Shyam Narayanan", "authors": "Shyam Narayanan", "title": "Improved Algorithms for Population Recovery from the Deletion Channel", "comments": "30 pages. To appear in Symposium on Discrete Algorithms (SODA), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The population recovery problem asks one to recover an unknown distribution\nover $n$-bit strings given access to independent noisy samples of strings drawn\nfrom the distribution. Recently, Ban et al. [BCF+19] studied the problem where\nthe noise is induced through the deletion channel. This problem generalizes the\nfamous trace reconstruction problem, where one wishes to learn a single string\nunder the deletion channel.\n  Ban et al. showed how to learn $\\ell$-sparse distributions over strings using\n$\\exp\\big(n^{1/2} \\cdot (\\log n)^{O(\\ell)}\\big)$ samples. In this work, we\nlearn the distribution using only $\\exp\\big(\\tilde{O}(n^{1/3}) \\cdot\n\\ell^2\\big)$ samples, by developing a higher-moment analog of the algorithms of\n[DOS17, NP17], which solve trace reconstruction in\n$\\exp\\big(\\tilde{O}(n^{1/3})\\big)$ samples. We also give the first algorithm\nwith a runtime subexponential in $n$, solving population recovery in\n$\\exp\\big(\\tilde{O}(n^{1/3}) \\cdot \\ell^3\\big)$ samples and time.\n  Notably, our dependence on $n$ nearly matches the upper bound of [DOS17,\nNP17] when $\\ell = O(1)$, and we reduce the dependence on $\\ell$ from doubly to\nsingly exponential. Therefore, we are able to learn large mixtures of strings:\nwhile Ban et al.'s algorithm can only learn a mixture of $O(\\log n/\\log \\log\nn)$ strings with a subexponential number of samples, we are able to learn a\nmixture of $n^{o(1)}$ strings in $\\exp\\big(n^{1/3 + o(1)}\\big)$ samples and\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 23:03:38 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 04:13:03 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Narayanan", "Shyam", ""]]}, {"id": "2004.06830", "submitter": "Huanyu Zhang", "authors": "Jayadev Acharya, Ziteng Sun, Huanyu Zhang", "title": "Differentially Private Assouad, Fano, and Le Cam", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Le Cam's method, Fano's inequality, and Assouad's lemma are three widely used\ntechniques to prove lower bounds for statistical estimation tasks. We propose\ntheir analogues under central differential privacy. Our results are simple,\neasy to apply and we use them to establish sample complexity bounds in several\nestimation tasks. We establish the optimal sample complexity of discrete\ndistribution estimation under total variation distance and $\\ell_2$ distance.\nWe also provide lower bounds for several other distribution classes, including\nproduct distributions and Gaussian mixtures that are tight up to logarithmic\nfactors. The technical component of our paper relates coupling between\ndistributions to the sample complexity of estimation under differential\nprivacy.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 23:10:38 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 14:57:18 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 03:03:57 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Acharya", "Jayadev", ""], ["Sun", "Ziteng", ""], ["Zhang", "Huanyu", ""]]}, {"id": "2004.06898", "submitter": "Ankit Garg", "authors": "Ankit Garg, Neeraj Kayal, and Chandan Saha", "title": "Learning sums of powers of low-degree polynomials in the non-degenerate\n  case", "comments": "Fixed a minor bug in the statement and proof of Corollary 3.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop algorithms for writing a polynomial as sums of powers of low\ndegree polynomials. Consider an $n$-variate degree-$d$ polynomial $f$ which can\nbe written as $$f = c_1Q_1^{m} + \\ldots + c_s Q_s^{m},$$ where each $c_i\\in\n\\mathbb{F}^{\\times}$, $Q_i$ is a homogeneous polynomial of degree $t$, and $t m\n= d$. In this paper, we give a $\\text{poly}((ns)^t)$-time learning algorithm\nfor finding the $Q_i$'s given (black-box access to) $f$, if the $Q_i's$ satisfy\ncertain non-degeneracy conditions and $n$ is larger than $d^2$. The set of\ndegenerate $Q_i$'s (i.e., inputs for which the algorithm does not work) form a\nnon-trivial variety and hence if the $Q_i$'s are chosen according to any\nreasonable (full-dimensional) distribution, then they are non-degenerate with\nhigh probability (if $s$ is not too large).\n  Our algorithm is based on a scheme for obtaining a learning algorithm for an\narithmetic circuit model from a lower bound for the same model, provided\ncertain non-degeneracy conditions hold. The scheme reduces the learning problem\nto the problem of decomposing two vector spaces under the action of a set of\nlinear operators, where the spaces and the operators are derived from the input\ncircuit and the complexity measure used in a typical lower bound proof. The\nnon-degeneracy conditions are certain restrictions on how the spaces decompose.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 06:18:41 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 08:57:22 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Garg", "Ankit", ""], ["Kayal", "Neeraj", ""], ["Saha", "Chandan", ""]]}, {"id": "2004.07118", "submitter": "Marc Hellmuth", "authors": "Tom Hartmann, Max Bannach, Martin Middendorf, Peter F. Stadler,\n  Nicolas Wieseke and Marc Hellmuth", "title": "Complete Edge-Colored Permutation Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of complete edge-colored permutation graphs as\ncomplete graphs that are the edge-disjoint union of \"classical\" permutation\ngraphs. We show that a graph $G=(V,E)$ is a complete edge-colored permutation\ngraph if and only if each monochromatic subgraph of $G$ is a \"classical\"\npermutation graph and $G$ does not contain a triangle with~$3$ different\ncolors. Using the modular decomposition as a framework we demonstrate that\ncomplete edge-colored permutation graphs are characterized in terms of their\nstrong prime modules, which induce also complete edge-colored permutation\ngraphs. This leads to an $\\mathcal{O}(|V|^2)$-time recognition algorithm. We\nshow, moreover, that complete edge-colored permutation graphs form a superclass\nof so-called symbolic ultrametrics and that the coloring of such graphs is\nalways a Gallai coloring.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 14:34:06 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Hartmann", "Tom", ""], ["Bannach", "Max", ""], ["Middendorf", "Martin", ""], ["Stadler", "Peter F.", ""], ["Wieseke", "Nicolas", ""], ["Hellmuth", "Marc", ""]]}, {"id": "2004.07151", "submitter": "Ewan Davies", "authors": "Ewan Davies, Ross J. Kang, Fran\\c{c}ois Pirot and Jean-S\\'ebastien\n  Sereni", "title": "An algorithmic framework for colouring locally sparse graphs", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an algorithmic framework for graph colouring that reduces the\nproblem to verifying a local probabilistic property of the independent sets.\n  With this we give, for any fixed $k\\ge 3$ and $\\varepsilon>0$, a randomised\npolynomial-time algorithm for colouring graphs of maximum degree $\\Delta$ in\nwhich each vertex is contained in at most $t$ copies of a cycle of length $k$,\nwhere $1/2\\le t\\le \\Delta^\\frac{2\\varepsilon}{1+2\\varepsilon}/(\\log\\Delta)^2$,\nwith $\\lfloor(1+\\varepsilon)\\Delta/\\log(\\Delta/\\sqrt t)\\rfloor$ colours.\n  This generalises and improves upon several notable results including those of\nKim (1995) and Alon, Krivelevich and Sudakov (1999), and more recent ones of\nMolloy (2019) and Achlioptas, Iliopoulos and Sinclair (2019). This bound on the\nchromatic number is tight up to an asymptotic factor $2$ and it coincides with\na famous algorithmic barrier to colouring random graphs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 15:30:38 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Davies", "Ewan", ""], ["Kang", "Ross J.", ""], ["Pirot", "Fran\u00e7ois", ""], ["Sereni", "Jean-S\u00e9bastien", ""]]}, {"id": "2004.07214", "submitter": "Oscar Defrain", "authors": "Marthe Bonamy, Oscar Defrain, Piotr Micek, Lhouari Nourine", "title": "Enumerating minimal dominating sets in the (in)comparability graphs of\n  bounded dimension posets", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enumerating minimal transversals in a hypergraph is a notoriously hard\nproblem. It can be reduced to enumerating minimal dominating sets in a graph,\nin fact even to enumerating minimal dominating sets in an incomparability\ngraph. We provide an output-polynomial time algorithm for incomparability\ngraphs whose underlying posets have bounded dimension. Through a different\nproof technique, we also provide an output-polynomial algorithm for their\ncomplements, i.e., for comparability graphs of bounded dimension posets.\n  Our algorithm for incomparability graphs is based on flashlight search and\nrelies on the geometrical representation of incomparability graphs with bounded\ndimension, as given by Golumbic et al. in 1983. It runs with polynomial delay\nand only needs polynomial space. Our algorithm for comparability graphs is\nbased on the flipping method introduced by Golovach et al. in 2015. It performs\nin incremental-polynomial time and requires exponential space.\n  In addition, we show how to improve the flipping method so that it requires\nonly polynomial space. Since the flipping method is a key tool for the best\nknown algorithms enumerating minimal dominating sets in a number of graph\nclasses, this yields direct improvements on the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 17:15:46 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Bonamy", "Marthe", ""], ["Defrain", "Oscar", ""], ["Micek", "Piotr", ""], ["Nourine", "Lhouari", ""]]}, {"id": "2004.07217", "submitter": "Xianghui Zhong", "authors": "Xianghui Zhong", "title": "Slightly Improved Upper Bound on the Integrality Ratio for the $s-t$\n  Path TSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the integrality ratio of the standard LP\nrelaxation for the metric $s-t$ Path TSP. We make a near-optimal choice for an\nauxiliary function used in the analysis of Traub and Vygen which leads to an\nimproved upper bound on the integrality ratio of 1.5273.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 17:17:41 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 21:20:36 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Zhong", "Xianghui", ""]]}, {"id": "2004.07220", "submitter": "Thuy Duong Vuong", "authors": "Nima Anari, Kuikui Liu, Shayan Oveis Gharan, Cynthia Vinzant, Thuy\n  Duong Vuong", "title": "Log-Concave Polynomials IV: Approximate Exchange, Tight Mixing Times,\n  and Near-Optimal Sampling of Forests", "comments": "Updated with fast sampling for forests, and optimization for general\n  strongly Rayleigh or log concave functions via simple algorithms. To appear\n  in STOC'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove tight mixing time bounds for natural random walks on bases of\nmatroids, determinantal distributions, and more generally distributions\nassociated with log-concave polynomials. For a matroid of rank $k$ on a ground\nset of $n$ elements, or more generally distributions associated with\nlog-concave polynomials of homogeneous degree $k$ on $n$ variables, we show\nthat the down-up random walk, started from an arbitrary point in the support,\nmixes in time $O(k\\log k)$. Our bound has no dependence on $n$ or the starting\npoint, unlike the previous analyses [ALOV19,CGM19], and is tight up to constant\nfactors. The main new ingredient is a property we call approximate exchange, a\ngeneralization of well-studied exchange properties for matroids and valuated\nmatroids, which may be of independent interest. In particular, given function\n$\\mu: {[n] \\choose k} \\to \\mathbb{R}_{\\geq 0},$ our approximate exchange\nproperty implies that a simple local search algorithm gives a\n$k^{O(k)}$-approximation of $\\max_{S} \\mu(S)$ when $\\mu$ is generated by a\nlog-concave polynomial, and that greedy gives the same approximation ratio when\n$\\mu$ is strongly Rayleigh.\n  As an application, we show how to leverage down-up random walks to\napproximately sample random forests or random spanning trees in a graph with\n$n$ edges in time $O(n\\log^2 n).$ The best known result for sampling random\nforest was a FPAUS with high polynomial runtime recently found by \\cite{ALOV19,\nCGM19}. For spanning tree, we improve on the almost-linear time algorithm by\n[Sch18]. Our analysis works on weighted graphs too, and is the first to achieve\nnearly-linear running time for these problems.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 17:22:01 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 07:01:39 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Anari", "Nima", ""], ["Liu", "Kuikui", ""], ["Gharan", "Shayan Oveis", ""], ["Vinzant", "Cynthia", ""], ["Vuong", "Thuy Duong", ""]]}, {"id": "2004.07286", "submitter": "Jay Tenenbaum", "authors": "Haim Kaplan, Jay Tenenbaum", "title": "Locality Sensitive Hashing for Set-Queries, Motivated by Group\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality Sensitive Hashing (LSH) is an effective method to index a set of\npoints such that we can efficiently find the nearest neighbors of a query\npoint. We extend this method to our novel Set-query LSH (SLSH), such that it\ncan find the nearest neighbors of a set of points, given as a query.\n  Let $ s(x,y) $ be the similarity between two points $ x $ and $ y $. We\ndefine a similarity between a set $ Q$ and a point $ x $ by aggregating the\nsimilarities $ s(p,x) $ for all $ p\\in Q $. For example, we can take $ s(p,x) $\nto be the angular similarity between $ p $ and $ x $ (i.e., $1-{\\angle\n(x,p)}/{\\pi}$), and aggregate by arithmetic or geometric averaging, or taking\nthe lowest similarity.\n  We develop locality sensitive hash families and data structures for a large\nset of such arithmetic and geometric averaging similarities, and analyze their\ncollision probabilities. We also establish an analogous framework and hash\nfamilies for distance functions. Specifically, we give a structure for the\neuclidean distance aggregated by either averaging or taking the maximum.\n  We leverage SLSH to solve a geometric extension of the approximate near\nneighbors problem. In this version, we consider a metric for which the unit\nball is an ellipsoid and its orientation is specified with the query.\n  An important application that motivates our work is group recommendation\nsystems. Such a system embeds movies and users in the same feature space, and\nthe task of recommending a movie for a group to watch together, translates to a\nset-query $ Q $ using an appropriate similarity.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 18:41:54 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 07:45:19 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 20:45:03 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Kaplan", "Haim", ""], ["Tenenbaum", "Jay", ""]]}, {"id": "2004.07319", "submitter": "Thomas Bl\\\"asius", "authors": "Thomas Bl\\\"asius, Tobias Friedrich, Andreas G\\\"obel, Jordi Levy, Ralf\n  Rothenberger", "title": "The Impact of Heterogeneity and Geometry on the Proof Complexity of\n  Random Satisfiability", "comments": "50 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG cs.DM cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satisfiability is considered the canonical NP-complete problem and is used as\na starting point for hardness reductions in theory, while in practice heuristic\nSAT solving algorithms can solve large-scale industrial SAT instances very\nefficiently. This disparity between theory and practice is believed to be a\nresult of inherent properties of industrial SAT instances that make them\ntractable. Two characteristic properties seem to be prevalent in the majority\nof real-world SAT instances, heterogeneous degree distribution and locality. To\nunderstand the impact of these two properties on SAT, we study the proof\ncomplexity of random k-SAT models that allow to control heterogeneity and\nlocality. Our findings show that heterogeneity alone does not make SAT easy as\nheterogeneous random k-SAT instances have superpolynomial resolution size. This\nimplies intractability of these instances for modern SAT-solvers. On the other\nhand, modeling locality with an underlying geometry leads to small\nunsatisfiable subformulas, which can be found within polynomial time.\n  A key ingredient for the result on geometric random k-SAT can be found in the\ncomplexity of higher-order Voronoi diagrams. As an additional technical\ncontribution, we show a linear upper bound on the number of non-empty Voronoi\nregions, that holds for points with random positions in a very general setting.\nIn particular, it covers arbitrary p-norms, higher dimensions, and weights\naffecting the area of influence of each point multiplicatively. This is in\nstark contrast to quadratic lower bounds for the worst case.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 20:10:48 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Friedrich", "Tobias", ""], ["G\u00f6bel", "Andreas", ""], ["Levy", "Jordi", ""], ["Rothenberger", "Ralf", ""]]}, {"id": "2004.07346", "submitter": "Mark Sellke", "authors": "S\\'ebastien Bubeck, Yuval Rabani, Mark Sellke", "title": "Online Multiserver Convex Chasing and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of $k$-chasing of convex functions, a simultaneous\ngeneralization of both the famous k-server problem in $R^d$, and of the problem\nof chasing convex bodies and functions. Aside from fundamental interest in this\ngeneral form, it has natural applications to online $k$-clustering problems\nwith objectives such as $k$-median or $k$-means. We show that this problem\nexhibits a rich landscape of behavior. In general, if both $k > 1$ and $d > 1$\nthere does not exist any online algorithm with bounded competitiveness. By\ncontrast, we exhibit a class of nicely behaved functions (which include in\nparticular the above-mentioned clustering problems), for which we show that\ncompetitive online algorithms exist, and moreover with dimension-free\ncompetitive ratio. We also introduce a parallel question of top-$k$ action\nregret minimization in the realm of online convex optimization. There, too, a\nmuch rougher landscape emerges for $k > 1$. While it is possible to achieve\nvanishing regret, unlike the top-one action case the rate of vanishing does not\nspeed up for strongly convex functions. Moreover, vanishing regret necessitates\nboth intractable computations and randomness. Finally we leave open whether\nalmost dimension-free regret is achievable for $k > 1$ and general convex\nlosses. As evidence that it might be possible, we prove dimension-free regret\nfor linear losses via an information-theoretic argument.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 21:17:10 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Rabani", "Yuval", ""], ["Sellke", "Mark", ""]]}, {"id": "2004.07403", "submitter": "Nisheeth Vishnoi", "authors": "Jonathan Leake and Nisheeth K. Vishnoi", "title": "On the computability of continuous maximum entropy distributions with\n  applications", "comments": "50 pages, STOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate a study of the following problem: Given a continuous domain\n$\\Omega$ along with its convex hull $\\mathcal{K}$, a point $A \\in \\mathcal{K}$\nand a prior measure $\\mu$ on $\\Omega$, find the probability density over\n$\\Omega$ whose marginal is $A$ and that minimizes the KL-divergence to $\\mu$.\nThis framework gives rise to several extremal distributions that arise in\nmathematics, quantum mechanics, statistics, and theoretical computer science.\nOur technical contributions include a polynomial bound on the norm of the\noptimizer of the dual problem that holds in a very general setting and relies\non a \"balance\" property of the measure $\\mu$ on $\\Omega$, and exact algorithms\nfor evaluating the dual and its gradient for several interesting settings of\n$\\Omega$ and $\\mu$. Together, along with the ellipsoid method, these results\nimply polynomial-time algorithms to compute such KL-divergence minimizing\ndistributions in several cases. Applications of our results include: 1) an\noptimization characterization of the Goemans-Williamson measure that is used to\nround a positive semidefinite matrix to a vector, 2) the computability of the\nentropic barrier for polytopes studied by Bubeck and Eldan, and 3) a\npolynomial-time algorithm to compute the barycentric quantum entropy of a\ndensity matrix that was proposed as an alternative to von Neumann entropy in\nthe 1970s: this corresponds to the case when $\\Omega$ is the set of rank one\nprojections matrices and $\\mu$ corresponds to the Haar measure on the unit\nsphere. Our techniques generalize to the setting of Hermitian rank $k$\nprojections using the Harish-Chandra-Itzykson-Zuber formula, and are applicable\neven beyond, to adjoint orbits of compact Lie groups.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 00:41:40 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Leake", "Jonathan", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "2004.07444", "submitter": "Oliver Serang", "authors": "Patrick Kreitzberg, Jake Pennington, Kyle Lucke, Oliver Serang", "title": "Fast exact computation of the $k$ most abundant isotope peaks with\n  layer-ordered heaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theoretical computation of isotopic distribution of compounds is crucial\nin many important applications of mass spectrometry, especially as machine\nprecision grows. A considerable amount of good tools have been created in the\nlast decade for doing so. In this paper we present a novel algorithm for\ncalculating the top $k$ peaks of a given compound. The algorithm takes\nadvantage of layer-ordered heaps used in an optimal method of selection on\n$X+Y$ and is able to efficiently calculate the top $k$ peaks on very large\nmolecules. Among its peers, this algorithm shows a significant speedup on\nmolecules whose elements have many isotopes. The algorithm obtains a speedup of\nmore than 31x when compared to $\\textsc{IsoSpec}$ on \\ch{Au2Ca10Ga10Pd76} when\ncomputing 47409787 peaks, which covers 0.999 of the total abundance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 03:55:30 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Kreitzberg", "Patrick", ""], ["Pennington", "Jake", ""], ["Lucke", "Kyle", ""], ["Serang", "Oliver", ""]]}, {"id": "2004.07447", "submitter": "Daniel Halpern", "authors": "Vasilis Gkatzelis, Daniel Halpern, and Nisarg Shah", "title": "Resolving the Optimal Metric Distortion Conjecture", "comments": "An extended abstract of this paper appears in the Proceedings of FOCS\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following metric distortion problem: there are two finite sets\nof points, $V$ and $C$, that lie in the same metric space, and our goal is to\nchoose a point in $C$ whose total distance from the points in $V$ is as small\nas possible. However, rather than having access to the underlying distance\nmetric, we only know, for each point in $V$, a ranking of its distances to the\npoints in $C$. We propose algorithms that choose a point in $C$ using only\nthese rankings as input and we provide bounds on their \\emph{distortion}\n(worst-case approximation ratio). A prominent motivation for this problem comes\nfrom voting theory, where $V$ represents a set of voters, $C$ represents a set\nof candidates, and the rankings correspond to ordinal preferences of the\nvoters. A major conjecture in this framework is that the optimal deterministic\nalgorithm has distortion $3$. We resolve this conjecture by providing a\npolynomial-time algorithm that achieves distortion $3$, matching a known lower\nbound. We do so by proving a novel lemma about matching voters to candidates,\nwhich we refer to as the \\emph{ranking-matching lemma}. This lemma induces a\nfamily of novel algorithms, which may be of independent interest, and we show\nthat a special algorithm in this family achieves distortion $3$. We also\nprovide more refined, parameterized, bounds using the notion of\n$\\alpha$-decisiveness, which quantifies the extent to which a voter may prefer\nher top choice relative to all others. Finally, we introduce a new randomized\nalgorithm with improved distortion compared to known results, and also provide\nimproved lower bounds on the distortion of all deterministic and randomized\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 04:13:06 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 16:52:59 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Gkatzelis", "Vasilis", ""], ["Halpern", "Daniel", ""], ["Shah", "Nisarg", ""]]}, {"id": "2004.07470", "submitter": "Shunhua Jiang", "authors": "Shunhua Jiang, Zhao Song, Omri Weinstein, Hengjie Zhang", "title": "Faster Dynamic Matrix Inverse for Faster LPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Motivated by recent Linear Programming solvers, we design dynamic data\nstructures for maintaining the inverse of an $n\\times n$ real matrix under\n$\\textit{low-rank}$ updates, with polynomially faster amortized running time.\nOur data structure is based on a recursive application of the Woodbury-Morrison\nidentity for implementing $\\textit{cascading}$ low-rank updates, combined with\nrecent sketching technology. Our techniques and amortized analysis of\nmulti-level partial updates, may be of broader interest to dynamic matrix\nproblems.\n  This data structure leads to the fastest known LP solver for general (dense)\nlinear programs, improving the running time of the recent algorithms of (Cohen\net al.'19, Lee et al.'19, Brand'20) from $O^*(n^{2+ \\max\\{\\frac{1}{6},\n\\omega-2, \\frac{1-\\alpha}{2}\\}})$ to $O^*(n^{2+\\max\\{\\frac{1}{18}, \\omega-2,\n\\frac{1-\\alpha}{2}\\}})$, where $\\omega$ and $\\alpha$ are the fast matrix\nmultiplication exponent and its dual. Hence, under the common belief that\n$\\omega \\approx 2$ and $\\alpha \\approx 1$, our LP solver runs in\n$O^*(n^{2.055})$ time instead of $O^*(n^{2.16})$.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 06:02:37 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Jiang", "Shunhua", ""], ["Song", "Zhao", ""], ["Weinstein", "Omri", ""], ["Zhang", "Hengjie", ""]]}, {"id": "2004.07492", "submitter": "Daniel Paulusma", "authors": "Hans Bodlaender, Nick Brettell, Matthew Johnson, Giacomo Paesani,\n  Daniel Paulusma, and Erik Jan van Leeuwen", "title": "Steiner Trees for Hereditary Graph Classes: a Treewidth Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical problems (Edge) Steiner Tree and Vertex Steiner\nTree after restricting the input to some class of graphs characterized by a\nsmall set of forbidden induced subgraphs. We show a dichotomy for the former\nproblem restricted to $(H_1,H_2)$-free graphs and a dichotomy for the latter\nproblem restricted to $H$-free graphs. We find that there exists an infinite\nfamily of graphs $H$ such that Vertex Steiner Tree is polynomial-time solvable\nfor $H$-free graphs, whereas there exist only two graphs $H$ for which this\nholds for Edge Steiner Tree. We also find that Edge Steiner Tree is\npolynomial-time solvable for $(H_1,H_2)$-free graphs if and only if the\ntreewidth of the class of $(H_1,H_2)$-free graphs is bounded (subject to P\n$\\neq$ NP). To obtain the latter result, we determine all pairs $(H_1,H_2)$ for\nwhich the class of $(H_1,H_2)$-free graphs has bounded treewidth.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 07:27:08 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 19:03:54 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Bodlaender", "Hans", ""], ["Brettell", "Nick", ""], ["Johnson", "Matthew", ""], ["Paesani", "Giacomo", ""], ["Paulusma", "Daniel", ""], ["van Leeuwen", "Erik Jan", ""]]}, {"id": "2004.07558", "submitter": "Tillmann Miltzow", "authors": "Mikkel Abrahamsen, Tillmann Miltzow, Nadja Seiferth", "title": "Framework for $\\exists \\mathbb{R}$-Completeness of Two-Dimensional\n  Packing Problems", "comments": "98 pages, 60 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DM cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We show that many natural two-dimensional packing problems are\nalgorithmically equivalent to finding real roots of multivariate polynomials. A\ntwo-dimensional packing problem is defined by the type of pieces, containers,\nand motions that are allowed. The aim is to decide if a given set of pieces can\nbe placed inside a given container. The pieces must be placed so that they are\npairwise interior-disjoint, and only motions of the allowed type can be used to\nmove them there. We establish a framework which enables us to show that for\nmany combinations of allowed pieces, containers, and motions, the resulting\nproblem is $\\exists\\mathbb R$-complete. This means that the problem is\nequivalent (under polynomial time reductions) to deciding whether a given\nsystem of polynomial equations and inequalities with integer coefficients has a\nreal solution. We consider packing problems where only translations are allowed\nas the motions, and problems where arbitrary rigid motions are allowed, i.e.,\nboth translations and rotations. When rotations are allowed, we show that the\nfollowing combinations of allowed pieces and containers are $\\exists\\mathbb\nR$-complete:\n  $\\bullet$ simple polygons, each of which has at most 8 corners, into a\nsquare,\n  $\\bullet$ convex objects bounded by line segments and hyperbolic curves into\na square,\n  $\\bullet$ convex polygons into a container bounded by line segments and\nhyperbolic curves.\n  Restricted to translations, we show that the following combinations are\n$\\exists\\mathbb R$-complete:\n  $\\bullet$ objects bounded by segments and hyperbolic curves into a square,\n  $\\bullet$ convex polygons into a container bounded by segments and hyperbolic\ncurves.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 09:58:07 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Abrahamsen", "Mikkel", ""], ["Miltzow", "Tillmann", ""], ["Seiferth", "Nadja", ""]]}, {"id": "2004.07566", "submitter": "Esther Galby", "authors": "Esther Galby and Andrea Munaro", "title": "Approximating Independent Set and Dominating Set on VPG graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Independent Set and Dominating Set restricted to VPG graphs (or,\nequivalently, string graphs). We show that they both remain $\\mathsf{NP}$-hard\non $B_0$-VPG graphs admitting a representation such that each grid-edge belongs\nto at most one path and each horizontal path has length at most two. On the\nother hand, combining the well-known Baker's shifting technique with bounded\nmim-width arguments, we provide simple PTASes on VPG graphs admitting a\nrepresentation such that each grid-edge belongs to at most $t$ paths and the\nlength of the horizontal part of each path is at most $c$, for some $c \\geq 1$.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 10:08:04 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Galby", "Esther", ""], ["Munaro", "Andrea", ""]]}, {"id": "2004.07572", "submitter": "Ofer Neiman", "authors": "Michael Elkin and Ofer Neiman", "title": "Centralized, Parallel, and Distributed Multi-Source Shortest Paths via\n  Hopsets and Rectangular Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an undirected weighted graph $G = (V,E,w)$. We study the problem of\ncomputing $(1+\\epsilon)$-approximate shortest paths for $S \\times V$, for a\nsubset $S \\subseteq V$ of $|S| = n^r$ sources, for some $0 < r \\le 1$. We\ndevise a significantly improved algorithm for this problem in the entire range\nof parameter $r$, in both the classical centralized and the parallel (PRAM)\nmodels of computation, and in a wide range of $r$ in the distributed (Congested\nClique) model. Specifically, our centralized algorithm for this problem\nrequires time $\\tilde{O}(|E| \\cdot n^{o(1)} + n^{\\omega(r)})$, where\n$n^{\\omega(r)}$ is the time required to multiply an $n^r \\times n$ matrix by an\n$n \\times n$ one. Our PRAM algorithm has polylogarithmic time $(\\log\nn)^{O(1/\\rho)}$, and its work complexity is $\\tilde{O}(|E| \\cdot n^\\rho +\nn^{\\omega(r)})$, for any arbitrarily small constant $\\rho >0$.\n  In particular, for $r \\le 0.313\\ldots$, our centralized algorithm computes $S\n\\times V$ $(1+\\epsilon)$-approximate shortest paths in $n^{2 + o(1)}$ time. Our\nPRAM polylogarithmic-time algorithm has work complexity $O(|E| \\cdot n^\\rho +\nn^{2+o(1)})$, for any arbitrarily small constant $\\rho >0$. Previously existing\nsolutions either require centralized time/parallel work of $O(|E| \\cdot |S|)$\nor provide much weaker approximation guarantees.\n  In the Congested Clique model, our algorithm solves the problem in\npolylogarithmic time for $|S| = n^r$ sources, for $r \\le 0.655$, while previous\nstate-of-the-art algorithms did so only for $r \\le 1/2$. Moreover, it improves\nprevious bounds for all $r > 1/2$. For unweighted graphs, the running time is\nimproved further to $poly(\\log\\log n)$.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 10:23:56 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 14:20:50 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Elkin", "Michael", ""], ["Neiman", "Ofer", ""]]}, {"id": "2004.07574", "submitter": "Frank Vallentin", "authors": "S. Thomas McCormick, Britta Peis, Robert Scheidweiler, Frank Vallentin", "title": "A polynomial time algorithm for solving the closest vector problem in\n  zonotopal lattices", "comments": "12 pages, v3: Comments of referees incorporated, to appear in SIAM\n  Journal on Discrete Mathematics (SIDMA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.MG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we give a polynomial time algorithm for solving the closest\nvector problem in the class of zonotopal lattices. The Voronoi cell of a\nzonotopal lattice is a zonotope, i.e. a projection of a regular cube. Examples\nof zonotopal lattices include lattices of Voronoi's first kind and tensor\nproducts of root lattices of type A. The combinatorial structure of zonotopal\nlattices can be described by regular matroids/totally unimodular matrices. We\nobserve that a linear algebra version of the minimum mean cycle canceling\nmethod can be applied for efficiently solving the closest vector problem in a\nzonotopal lattice if the lattice is given as the integral kernel of a totally\nunimodular matrix.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 10:29:01 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 12:58:30 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 18:29:08 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["McCormick", "S. Thomas", ""], ["Peis", "Britta", ""], ["Scheidweiler", "Robert", ""], ["Vallentin", "Frank", ""]]}, {"id": "2004.07630", "submitter": "Michael Bekos", "authors": "Michael A. Bekos, Michael Kaufmann, Fabian Klute, Sergey Pupyrev,\n  Chrysanthi Raftopoulou, Torsten Ueckerdt", "title": "Four Pages Are Indeed Necessary for Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An embedding of a graph in a book consists of a linear order of its vertices\nalong the spine of the book and of an assignment of its edges to the pages of\nthe book, so that no two edges on the same page cross. The book thickness of a\ngraph is the minimum number of pages over all its book embeddings. Accordingly,\nthe book thickness of a class of graphs is the maximum book thickness over all\nits members. In this paper, we address a long-standing open problem regarding\nthe exact book thickness of the class of planar graphs, which previously was\nknown to be either three or four. We settle this problem by demonstrating\nplanar graphs that require four pages in any of their book embeddings, thus\nestablishing that the book thickness of the class of planar graphs is four.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 12:32:08 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Bekos", "Michael A.", ""], ["Kaufmann", "Michael", ""], ["Klute", "Fabian", ""], ["Pupyrev", "Sergey", ""], ["Raftopoulou", "Chrysanthi", ""], ["Ueckerdt", "Torsten", ""]]}, {"id": "2004.07650", "submitter": "Xiaorui Sun", "authors": "Wenyu Jin, Xiaorui Sun", "title": "Fully Dynamic $c$-Edge Connectivity in Subpolynomial Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic fully dynamic algorithm for $c$-edge connectivity\nproblem with $n^{o(1)}$ worst case update and query time for any positive\ninteger $c = (\\log n)^{o(1)}$ for a graph with $n$ vertices. Previously, only\npolylogarithmic, $O(\\sqrt{n})$, and $O(n^{2/3})$ worst case update time\nalgorithms were known for fully dynamic $1$, $2$ and $3$-edge connectivity\nproblems respectively.\n  Our techniques include a multi-level $c$-edge connectivity sparsifier, an\nonline-batch update algorithm for the sparsifier, and a general approach to\nturn an online-batch dynamic algorithm with small amortized update time into a\nfully dynamic algorithm with worst case update time.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 13:38:53 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Jin", "Wenyu", ""], ["Sun", "Xiaorui", ""]]}, {"id": "2004.07659", "submitter": "Sitan Chen", "authors": "Sitan Chen, Ankur Moitra", "title": "Algorithmic Foundations for the Diffraction Limit", "comments": "55 pages, 5 figures, v2: improved lower bound going beyond the Abbe\n  limit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.ST physics.optics stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For more than a century and a half it has been widely-believed (but was never\nrigorously shown) that the physics of diffraction imposes certain fundamental\nlimits on the resolution of an optical system. However our understanding of\nwhat exactly can and cannot be resolved has never risen above heuristic\narguments which, even worse, appear contradictory. In this work we remedy this\ngap by studying the diffraction limit as a statistical inverse problem and,\nbased on connections to provable algorithms for learning mixture models, we\nrigorously prove upper and lower bounds on the statistical and algorithmic\ncomplexity needed to resolve closely spaced point sources. In particular we\nshow that there is a phase transition where the sample complexity goes from\npolynomial to exponential. Surprisingly, we show that this does not occur at\nthe Abbe limit, which has long been presumed to be the true diffraction limit.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 13:50:53 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 18:56:48 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Chen", "Sitan", ""], ["Moitra", "Ankur", ""]]}, {"id": "2004.07671", "submitter": "Daniel Neuen", "authors": "Martin Grohe, Daniel Neuen, Daniel Wiebking", "title": "Isomorphism Testing for Graphs Excluding Small Minors", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that there is a graph isomorphism test running in time\n$n^{\\operatorname{polylog}(h)}$ on $n$-vertex graphs excluding some $h$-vertex\ngraph as a minor. Previously known bounds were $n^{\\operatorname{poly}(h)}$\n(Ponomarenko, 1988) and $n^{\\operatorname{polylog}(n)}$ (Babai, STOC 2016). For\nthe algorithm we combine recent advances in the group-theoretic graph\nisomorphism machinery with new graph-theoretic arguments.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 14:09:02 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Grohe", "Martin", ""], ["Neuen", "Daniel", ""], ["Wiebking", "Daniel", ""]]}, {"id": "2004.07718", "submitter": "Shaofeng Jiang", "authors": "Vladimir Braverman, Shaofeng H.-C. Jiang, Robert Krauthgamer, and Xuan\n  Wu", "title": "Coresets for Clustering in Excluded-minor Graphs and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coresets are modern data-reduction tools that are widely used in data\nanalysis to improve efficiency in terms of running time, space and\ncommunication complexity. Our main result is a fast algorithm to construct a\nsmall coreset for k-Median in (the shortest-path metric of) an excluded-minor\ngraph. Specifically, we give the first coreset of size that depends only on\n$k$, $\\epsilon$ and the excluded-minor size, and our running time is\nquasi-linear (in the size of the input graph).\n  The main innovation in our new algorithm is that is iterative; it first\nreduces the $n$ input points to roughly $O(\\log n)$ reweighted points, then to\n$O(\\log\\log n)$, and so forth until the size is independent of $n$. Each step\nin this iterative size reduction is based on the importance sampling framework\nof Feldman and Langberg (STOC 2011), with a crucial adaptation that reduces the\nnumber of \\emph{distinct points}, by employing a terminal embedding (where low\ndistortion is guaranteed only for the distance from every terminal to all other\npoints). Our terminal embedding is technically involved and relies on\nshortest-path separators, a standard tool in planar and excluded-minor graphs.\n  Furthermore, our new algorithm is applicable also in Euclidean metrics, by\nsimply using a recent terminal embedding result of Narayanan and Nelson, (STOC\n2019), which extends the Johnson-Lindenstrauss Lemma. We thus obtain an\nefficient coreset construction in high-dimensional Euclidean spaces, thereby\nmatching and simplifying state-of-the-art results (Sohler and Woodruff, FOCS\n2018; Huang and Vishnoi, STOC 2020).\n  In addition, we also employ terminal embedding with additive distortion to\nobtain small coresets in graphs with bounded highway dimension, and use\napplications of our coresets to obtain improved approximation schemes, e.g., an\nimproved PTAS for planar k-Median via a new centroid set.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 15:53:59 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 13:29:20 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Braverman", "Vladimir", ""], ["Jiang", "Shaofeng H. -C.", ""], ["Krauthgamer", "Robert", ""], ["Wu", "Xuan", ""]]}, {"id": "2004.07823", "submitter": "Kazuya Haraguchi", "authors": "Kazuya Haraguchi, Hiroshi Nagamochi", "title": "Polynomial-delay Enumeration Algorithms in Set Systems", "comments": "arXiv admin note: substantial text overlap with arXiv:2004.01904", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a set system $(V, {\\mathcal C}\\subseteq 2^V)$ on a finite set $V$\nof elements, where we call a set $C\\in {\\mathcal C}$ a component. We assume\nthat two oracles $\\mathrm{L}_1$ and $\\mathrm{L}_2$ are available, where given\ntwo subsets $X,Y\\subseteq V$, $\\mathrm{L}_1$ returns a maximal component $C\\in\n{\\mathcal C}$ with $X\\subseteq C\\subseteq Y$; and given a set $Y\\subseteq V$,\n$\\mathrm{L}_2$ returns all maximal components $C\\in {\\mathcal C}$ with\n$C\\subseteq Y$. Given a set $I$ of attributes and a function $\\sigma:V\\to 2^I$\nin a transitive system, a component $C\\in {\\mathcal C}$ is called a solution if\nthe set of common attributes in $C$ is inclusively maximal; i.e.,\n$\\bigcap_{v\\in C}\\sigma(v)\\supsetneq \\bigcap_{v\\in X}\\sigma(v)$ for any\ncomponent $X\\in{\\mathcal C}$ with $C\\subsetneq X$. We prove that there exists\nan algorithm of enumerating all solutions (or all components) in delay bounded\nby a polynomial with respect to the input size and the running times of the\noracles.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 03:11:21 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Haraguchi", "Kazuya", ""], ["Nagamochi", "Hiroshi", ""]]}, {"id": "2004.07839", "submitter": "Eliad Tsfadia", "authors": "Haim Kaplan, Yishay Mansour, Uri Stemmer, Eliad Tsfadia", "title": "Private Learning of Halfspaces: Simplifying the Construction and\n  Reducing the Sample Complexity", "comments": "Accepted to NeurIPS 2020. In this version we added a new section\n  about our new method for privately optimizing high-dimensional functions.\n  arXiv admin note: text overlap with arXiv:1902.10731", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a differentially private learner for halfspaces over a finite grid\n$G$ in $\\mathbb{R}^d$ with sample complexity $\\approx d^{2.5}\\cdot\n2^{\\log^*|G|}$, which improves the state-of-the-art result of [Beimel et al.,\nCOLT 2019] by a $d^2$ factor. The building block for our learner is a new\ndifferentially private algorithm for approximately solving the linear\nfeasibility problem: Given a feasible collection of $m$ linear constraints of\nthe form $Ax\\geq b$, the task is to privately identify a solution $x$ that\nsatisfies most of the constraints. Our algorithm is iterative, where each\niteration determines the next coordinate of the constructed solution $x$.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 16:12:10 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 09:44:34 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Kaplan", "Haim", ""], ["Mansour", "Yishay", ""], ["Stemmer", "Uri", ""], ["Tsfadia", "Eliad", ""]]}, {"id": "2004.07869", "submitter": "Sitan Chen", "authors": "Sebastien Bubeck, Sitan Chen, Jerry Li", "title": "Entanglement is Necessary for Optimal Quantum Property Testing", "comments": "31 pages, comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a surge of progress in recent years in developing algorithms\nfor testing and learning quantum states that achieve optimal copy complexity.\nUnfortunately, they require the use of entangled measurements across many\ncopies of the underlying state and thus remain outside the realm of what is\ncurrently experimentally feasible. A natural question is whether one can match\nthe copy complexity of such algorithms using only independent---but possibly\nadaptively chosen---measurements on individual copies.\n  We answer this in the negative for arguably the most basic quantum testing\nproblem: deciding whether a given $d$-dimensional quantum state is equal to or\n$\\epsilon$-far in trace distance from the maximally mixed state. While it is\nknown how to achieve optimal $O(d/\\epsilon^2)$ copy complexity using entangled\nmeasurements, we show that with independent measurements,\n$\\Omega(d^{4/3}/\\epsilon^2)$ is necessary, even if the measurements are chosen\nadaptively. This resolves a question of Wright. To obtain this lower bound, we\ndevelop several new techniques, including a chain-rule style proof of\nPaninski's lower bound for classical uniformity testing, which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 18:28:39 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Bubeck", "Sebastien", ""], ["Chen", "Sitan", ""], ["Li", "Jerry", ""]]}, {"id": "2004.07886", "submitter": "Uthaipon Tantipongpipat", "authors": "Vivek Madan, Aleksandar Nikolov, Mohit Singh, Uthaipon Tantipongpipat", "title": "Maximizing Determinants under Matroid Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given vectors $v_1,\\dots,v_n\\in\\mathbb{R}^d$ and a matroid $M=([n],I)$, we\nstudy the problem of finding a basis $S$ of $M$ such that $\\det(\\sum_{i \\in\nS}v_i v_i^\\top)$ is maximized. This problem appears in a diverse set of areas\nsuch as experimental design, fair allocation of goods, network design, and\nmachine learning. The current best results include an $e^{2k}$-estimation for\nany matroid of rank $k$ and a $(1+\\epsilon)^d$-approximation for a uniform\nmatroid of rank $k\\ge d+\\frac d\\epsilon$, where the rank $k\\ge d$ denotes the\ndesired size of the optimal set. Our main result is a new approximation\nalgorithm with an approximation guarantee that depends only on the dimension\n$d$ of the vectors and not on the size $k$ of the output set. In particular, we\nshow an $(O(d))^{d}$-estimation and an $(O(d))^{d^3}$-approximation for any\nmatroid, giving a significant improvement over prior work when $k\\gg d$.\n  Our result relies on the existence of an optimal solution to a convex\nprogramming relaxation for the problem which has sparse support; in particular,\nno more than $O(d^2)$ variables of the solution have fractional values. The\nsparsity results rely on the interplay between the first-order optimality\nconditions for the convex program and matroid theory. We believe that the\ntechniques introduced to show sparsity of optimal solutions to convex programs\nwill be of independent interest. We also give a randomized algorithm that\nrounds a sparse fractional solution to a feasible integral solution to the\noriginal problem. To show the approximation guarantee, we utilize recent works\non strongly log-concave polynomials and show new relationships between\ndifferent convex programs studied for the problem. Finally, we use the\nestimation algorithm and sparsity results to give an efficient deterministic\napproximation algorithm with an approximation guarantee that depends solely on\nthe dimension $d$.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 19:16:38 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Madan", "Vivek", ""], ["Nikolov", "Aleksandar", ""], ["Singh", "Mohit", ""], ["Tantipongpipat", "Uthaipon", ""]]}, {"id": "2004.07946", "submitter": "Noam Touitou", "authors": "Yossi Azar, Noam Touitou", "title": "Beyond Tree Embeddings -- a Deterministic Framework for Network Design\n  with Deadlines or Delay", "comments": "41 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider network design problems with deadline or delay. All previous\nresults for these models are based on randomized embedding of the graph into a\ntree (HST) and then solving the problem on this tree. We show that this is not\nnecessary. In particular, we design a deterministic framework for these\nproblems which is not based on embedding. This enables us to provide\ndeterministic $\\text{poly-log}(n)$-competitive algorithms for Steiner tree,\ngeneralized Steiner tree, node weighted Steiner tree, (non-uniform) facility\nlocation and directed Steiner tree with deadlines or with delay (where $n$ is\nthe number of nodes). Our deterministic algorithms also give improved\nguarantees over some previous randomized results. In addition, we show a lower\nbound of $\\text{poly-log}(n)$ for some of these problems, which implies that\nour framework is optimal up to the power of the poly-log. Our algorithms and\ntechniques differ significantly from those in all previous considerations of\nthese problems.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 20:44:15 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Azar", "Yossi", ""], ["Touitou", "Noam", ""]]}, {"id": "2004.07986", "submitter": "Zhao Song", "authors": "Zhao Song, David P. Woodruff, Peilin Zhong", "title": "Average Case Column Subset Selection for Entrywise $\\ell_1$-Norm Loss", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We study the column subset selection problem with respect to the entrywise\n$\\ell_1$-norm loss. It is known that in the worst case, to obtain a good\nrank-$k$ approximation to a matrix, one needs an arbitrarily large\n$n^{\\Omega(1)}$ number of columns to obtain a $(1+\\epsilon)$-approximation to\nthe best entrywise $\\ell_1$-norm low rank approximation of an $n \\times n$\nmatrix. Nevertheless, we show that under certain minimal and realistic\ndistributional settings, it is possible to obtain a\n$(1+\\epsilon)$-approximation with a nearly linear running time and\npoly$(k/\\epsilon)+O(k\\log n)$ columns. Namely, we show that if the input matrix\n$A$ has the form $A = B + E$, where $B$ is an arbitrary rank-$k$ matrix, and\n$E$ is a matrix with i.i.d. entries drawn from any distribution $\\mu$ for which\nthe $(1+\\gamma)$-th moment exists, for an arbitrarily small constant $\\gamma >\n0$, then it is possible to obtain a $(1+\\epsilon)$-approximate column subset\nselection to the entrywise $\\ell_1$-norm in nearly linear time. Conversely we\nshow that if the first moment does not exist, then it is not possible to obtain\na $(1+\\epsilon)$-approximate subset selection algorithm even if one chooses any\n$n^{o(1)}$ columns. This is the first algorithm of any kind for achieving a\n$(1+\\epsilon)$-approximation for entrywise $\\ell_1$-norm loss low rank\napproximation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 22:57:06 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Song", "Zhao", ""], ["Woodruff", "David P.", ""], ["Zhong", "Peilin", ""]]}, {"id": "2004.08039", "submitter": "William Kuszmaul", "authors": "Michael A. Bender, Tsvi Kopelowitz, William Kuszmaul, Seth Pettie", "title": "Contention Resolution Without Collision Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the contention resolution problem on a shared\ncommunication channel that does not support collision detection. A shared\ncommunication channel is a multiple access channel, which consists of a\nsequence of synchronized time slots. Players on the channel may attempt to\nbroadcast a packet (message) in any time slot. A player's broadcast succeeds if\nno other player broadcasts during that slot. If two or more players broadcast\nin the same time slot, then the broadcasts collide and both broadcasts fail.\nThe lack of collision detection means that a player monitoring the channel\ncannot differentiate between the case of two or more players broadcasting in\nthe same slot (a collision) and zero players broadcasting. In the\ncontention-resolution problem, players arrive on the channel over time, and\neach player has one packet to transmit. The goal is to coordinate the players\nso that each player is able to successfully transmit its packet within\nreasonable time. However, the players can only communicate via the shared\nchannel by choosing to either broadcast or not. A contention-resolution\nprotocol is measured in terms of its throughput (channel utilization). Previous\nwork on contention resolution that achieved constant throughput assumed that\neither players could detect collisions, or the players' arrival pattern is\ngenerated by a memoryless (non-adversarial) process. The foundational question\nanswered by this paper is whether collision detection is a luxury or necessity\nwhen the objective is to achieve constant throughput. We show that even without\ncollision detection, one can solve contention resolution, achieving constant\nthroughput, with high probability.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 02:33:19 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 12:57:17 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Bender", "Michael A.", ""], ["Kopelowitz", "Tsvi", ""], ["Kuszmaul", "William", ""], ["Pettie", "Seth", ""]]}, {"id": "2004.08238", "submitter": "Wei-Chang Yeh", "authors": "Wei-Chang Yeh", "title": "Novel Binary-Addition Tree Algorithm (BAT) for Binary-State Network\n  Reliability Problem", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PF eess.SP math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network structures and models have been widely adopted, e.g., for Internet of\nThings, wireless sensor networks, smart grids, transportation networks,\ncommunication networks, social networks, and computer grid systems. Network\nreliability is an effective and popular technique to estimate the probability\nthat the network is still functioning. Networks composed of binary-state (e.g.,\nworking or failed) components (arcs and/or nodes) are called binary-state\nnetworks. The binary-state network is the fundamental type of network; thus,\nthere is always a need for a more efficient algorithm to calculate the network\nreliability. Thus, a novel binary-addition tree (BAT) algorithm that employs\nbinary addition for finding all the possible state vectors and the path-based\nlayered-search algorithm for filtering out all the connected vectors is\nproposed for calculating the binary-state network reliability. According to the\ntime complexity and numerical examples, the efficiency of the proposed BAT is\nhigher than those of traditional algorithms for solving the binary-state\nnetwork reliability problem.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 03:51:48 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Yeh", "Wei-Chang", ""]]}, {"id": "2004.08255", "submitter": "Zhiwei Ckhen", "authors": "Zhiwei Chen, Aoqian Zhang", "title": "A Survey of Approximate Quantile Computation on Large-scale Data\n  (Technical Report)", "comments": null, "journal-ref": "IEEE Access 8 (2020): 34585-34597", "doi": "10.1109/ACCESS.2020.2974919", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data volume grows extensively, data profiling helps to extract metadata of\nlarge-scale data. However, one kind of metadata, order statistics, is difficult\nto be computed because they are not mergeable or incremental. Thus, the\nlimitation of time and memory space does not support their computation on\nlarge-scale data. In this paper, we focus on an order statistic, quantiles, and\npresent a comprehensive analysis of studies on approximate quantile\ncomputation. Both deterministic algorithms and randomized algorithms that\ncompute approximate quantiles over streaming models or distributed models are\ncovered. Then, multiple techniques for improving the efficiency and performance\nof approximate quantile algorithms in various scenarios, such as skewed data\nand high-speed data streams, are presented. Finally, we conclude with coverage\nof existing packages in different languages and with a brief discussion of the\nfuture direction in this area.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 14:10:00 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Chen", "Zhiwei", ""], ["Zhang", "Aoqian", ""]]}, {"id": "2004.08324", "submitter": "Ignasi Sau", "authors": "Ignasi Sau, U\\'everton S. Souza", "title": "Hitting forbidden induced subgraphs on bounded treewidth graphs", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a fixed graph $H$, the $H$-IS-Deletion problem asks, given a graph $G$,\nfor the minimum size of a set $S \\subseteq V(G)$ such that $G\\setminus S$ does\nnot contain $H$ as an induced subgraph. Motivated by previous work about\nhitting (topological) minors and subgraphs on bounded treewidth graphs, we are\ninterested in determining, for a fixed graph $H$, the smallest function\n$f_H(t)$ such that $H$-IS-Deletion can be solved in time $f_H(t) \\cdot\nn^{O(1)}$ assuming the Exponential Time Hypothesis (ETH), where $t$ and $n$\ndenote the treewidth and the number of vertices of the input graph,\nrespectively.\n  We show that $f_H(t) = 2^{O(t^{h-2})}$ for every graph $H$ on $h \\geq 3$\nvertices, and that $f_H(t) = 2^{O(t)}$ if $H$ is a clique or an independent\nset. We present a number of lower bounds by generalizing a reduction of Cygan\net al. [MFCS 2014] for the subgraph version. In particular, we show that when\n$H$ deviates slightly from a clique, the function $f_H(t)$ suffers a sharp\njump: if $H$ is obtained from a clique of size $h$ by removing one edge, then\n$f_H(t) = 2^{\\Theta(t^{h-2})}$. We also show that $f_H(t) = 2^{\\Omega(t^{h})}$\nwhen $H=K_{h,h}$, and this reduction answers an open question of Mi. Pilipczuk\n[MFCS 2011] about the function $f_{C_4}(t)$ for the subgraph version.\n  Motivated by Cygan et al. [MFCS 2014], we also consider the colorful variant\nof the problem, where each vertex of $G$ is colored with some color from $V(H)$\nand we require to hit only induced copies of $H$ with matching colors. In this\ncase, we determine, under the ETH, the function $f_H(t)$ for every connected\ngraph $H$ on $h$ vertices: if $h\\leq 2$ the problem can be solved in polynomial\ntime; if $h\\geq 3$, $f_H(t) = 2^{\\Theta(t)}$ if $H$ is a clique, and $f_H(t) =\n2^{\\Theta(t^{h-2})}$ otherwise.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 16:12:38 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Sau", "Ignasi", ""], ["Souza", "U\u00e9verton S.", ""]]}, {"id": "2004.08338", "submitter": "Luca Elias Sch\\\"afer", "authors": "Simon Busam and Luca E. Sch\\\"afer and Stefan Ruzika", "title": "The two player shortest path network interdiction problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study a biobjective extension of the shortest path\nnetwork interdiction problem. Each arc in the network is associated with two\ninteger length values and two players compute their respective shortest paths\nfrom source to sink independently from each other while an interdictor tries to\nlengthen both shortest paths by removing arcs. We show that this problem is\nintractable and that deciding whether a feasible interdiction strategy is\nefficient, is NP- complete. We provide a solution procedure to solve the\nproblem on two-terminal series-parallel graphs in pseudopolynomial time.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 16:42:33 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2020 11:43:53 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Busam", "Simon", ""], ["Sch\u00e4fer", "Luca E.", ""], ["Ruzika", "Stefan", ""]]}, {"id": "2004.08350", "submitter": "Philip Wellnitz", "authors": "Panagiotis Charalampopoulos, Tomasz Kociumaka, Philip Wellnitz", "title": "Faster Approximate Pattern Matching: A Unified Approach", "comments": "74 pages, 7 figures, FOCS'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate pattern matching is a natural and well-studied problem on\nstrings: Given a text $T$, a pattern $P$, and a threshold $k$, find (the\nstarting positions of) all substrings of $T$ that are at distance at most $k$\nfrom $P$. We consider the two most fundamental string metrics: the Hamming\ndistance and the edit distance. Under the Hamming distance, we search for\nsubstrings of $T$ that have at most $k$ mismatches with $P$, while under the\nedit distance, we search for substrings of $T$ that can be transformed to $P$\nwith at most $k$ edits.\n  Exact occurrences of $P$ in $T$ have a very simple structure: If we assume\nfor simplicity that $|T| \\le 3|P|/2$ and trim $T$ so that $P$ occurs both as a\nprefix and as a suffix of $T$, then both $P$ and $T$ are periodic with a common\nperiod. However, an analogous characterization for the structure of occurrences\nwith up to $k$ mismatches was proved only recently by Bringmann et al.\n[SODA'19]: Either there are $O(k^2)$ $k$-mismatch occurrences of $P$ in $T$, or\nboth $P$ and $T$ are at Hamming distance $O(k)$ from strings with a common\nperiod $O(m/k)$. We tighten this characterization by showing that there are\n$O(k)$ $k$-mismatch occurrences in the case when the pattern is not\n(approximately) periodic, and we lift it to the edit distance setting, where we\ntightly bound the number of $k$-edit occurrences by $O(k^2)$ in the\nnon-periodic case. Our proofs are constructive and let us obtain a unified\nframework for approximate pattern matching for both considered distances. We\nshowcase the generality of our framework with results for the fully-compressed\nsetting (where $T$ and $P$ are given as a straight-line program) and for the\ndynamic setting (where we extend a data structure of Gawrychowski et al.\n[SODA'18]).\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 17:13:13 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 10:26:00 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Charalampopoulos", "Panagiotis", ""], ["Kociumaka", "Tomasz", ""], ["Wellnitz", "Philip", ""]]}, {"id": "2004.08375", "submitter": "William Maxwell", "authors": "Glencora Borradaile, Erin Wolf Chambers, David Eppstein, William\n  Maxwell, Amir Nayyeri", "title": "Low-stretch spanning trees of graphs with bounded width", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of low-stretch spanning trees in graphs of bounded\nwidth: bandwidth, cutwidth, and treewidth. We show that any simple connected\ngraph $G$ with a linear arrangement of bandwidth $b$ can be embedded into a\ndistribution $\\mathcal T$ of spanning trees such that the expected stretch of\neach edge of $G$ is $O(b^2)$. Our proof implies a linear time algorithm for\nsampling from $\\mathcal T$. Therefore, we have a linear time algorithm that\nfinds a spanning tree of $G$ with average stretch $O(b^2)$ with high\nprobability. We also describe a deterministic linear-time algorithm for\ncomputing a spanning tree of $G$ with average stretch $O(b^3)$. For graphs of\ncutwidth $c$, we construct a spanning tree with stretch $O(c^2)$ in linear\ntime. Finally, when $G$ has treewidth $k$ we provide a dynamic programming\nalgorithm computing a minimum stretch spanning tree of $G$ that runs in\npolynomial time with respect to the number of vertices of $G$.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 17:52:49 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Borradaile", "Glencora", ""], ["Chambers", "Erin Wolf", ""], ["Eppstein", "David", ""], ["Maxwell", "William", ""], ["Nayyeri", "Amir", ""]]}, {"id": "2004.08381", "submitter": "Aleksandar Shurbevski", "authors": "Kyousuke Yamashita, Ryuji Masui, Xiang Zhou, Chenxi Wang, Aleksandar\n  Shurbevski, Hiroshi Nagamochi and Tatsuya Akutsu", "title": "Enumerating Chemical Graphs with Two Disjoint Cycles Satisfying Given\n  Path Frequency Specifications", "comments": "arXiv admin note: text overlap with arXiv:2004.06367", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Enumerating chemical graphs satisfying given constraints is a fundamental\nproblem in mathematical and computational chemistry, and plays an essential\npart in a recently proposed framework for the inverse QSAR/QSPR. In this paper,\nconstraints are given by feature vectors each of which consists of the\nfrequencies of paths in a given set of paths. We consider the problem of\nenumerating chemical graphs that satisfy the path frequency constraints, which\nare given by a pair of feature vectors specifying upper and lower bounds of the\nfrequency of each path. We design a branch-and-bound algorithm for enumerating\nchemical graphs of bi-block 2-augmented structure, that is, graphs that contain\ntwo edge-disjoint cycles. We present some computational experiments with an\nimplementation of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 10:51:11 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Yamashita", "Kyousuke", ""], ["Masui", "Ryuji", ""], ["Zhou", "Xiang", ""], ["Wang", "Chenxi", ""], ["Shurbevski", "Aleksandar", ""], ["Nagamochi", "Hiroshi", ""], ["Akutsu", "Tatsuya", ""]]}, {"id": "2004.08432", "submitter": "Jan van den Brand", "authors": "Aaron Bernstein, Jan van den Brand, Maximilian Probst Gutenberg,\n  Danupon Nanongkai, Thatchaphol Saranurak, Aaron Sidford, He Sun", "title": "Fully-Dynamic Graph Sparsifiers Against an Adaptive Adversary", "comments": "Abstract shortened due to arXiv character limit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing dynamic graph algorithms against an adaptive adversary is a major\ngoal in the field of dynamic graph algorithms. While a few such algorithms are\nknown for spanning trees, matchings, and single-source shortest paths, very\nlittle was known for an important primitive like graph sparsifiers. The\nchallenge is how to approximately preserve so much information about the graph\n(e.g., all-pairs distances and all cuts) without revealing the algorithms'\nunderlying randomness to the adaptive adversary.\n  In this paper we present the first non-trivial efficient adaptive algorithms\nfor maintaining spanners and cut sparisifers. These algorithms in turn imply\nimprovements over existing algorithms for other problems. Our first algorithm\nmaintains a polylog$(n)$-spanner of size $\\tilde O(n)$ in polylog$(n)$\namortized update time. The second algorithm maintains an $O(k)$-approximate cut\nsparsifier of size $\\tilde O(n)$ in $\\tilde O(n^{1/k})$ amortized update time,\nfor any $k\\ge1$, which is polylog$(n)$ time when $k=\\log(n)$. The third\nalgorithm maintains a polylog$(n)$-approximate spectral sparsifier in\npolylog$(n)$ amortized update time. The amortized update time of both\nalgorithms can be made worst-case by paying some sub-polynomial factors. Prior\nto our result, there were near-optimal algorithms against oblivious adversaries\n(e.g. Baswana et al. [TALG'12] and Abraham et al. [FOCS'16]), but the only\nnon-trivial adaptive dynamic algorithm requires $O(n)$ amortized update time to\nmaintain $3$- and $5$-spanner of size $O(n^{1+1/2})$ and $O(n^{1+1/3})$,\nrespectively [Ausiello et al. ESA'05].\n  Our results are based on two novel techniques. The first technique, is a\ngeneric black-box reduction that allows us to assume that the graph undergoes\nonly edge deletions and, more importantly, remains an expander with\nalmost-uniform degree. The second technique we call proactive resampling. [...]\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 19:51:47 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 20:52:31 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 09:27:32 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Bernstein", "Aaron", ""], ["Brand", "Jan van den", ""], ["Gutenberg", "Maximilian Probst", ""], ["Nanongkai", "Danupon", ""], ["Saranurak", "Thatchaphol", ""], ["Sidford", "Aaron", ""], ["Sun", "He", ""]]}, {"id": "2004.08434", "submitter": "Cameron Musco", "authors": "Cameron Musco and Christopher Musco", "title": "Projection-Cost-Preserving Sketches: Proof Strategies and Constructions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we illustrate how common matrix approximation methods, such as\nrandom projection and random sampling, yield projection-cost-preserving\nsketches, as introduced in [FSS13, CEM+15]. A projection-cost-preserving sketch\nis a matrix approximation which, for a given parameter $k$, approximately\npreserves the distance of the target matrix to all $k$-dimensional subspaces.\nSuch sketches have applications to scalable algorithms for linear algebra, data\nscience, and machine learning. Our goal is to simplify the presentation of\nproof techniques introduced in [CEM+15] and [CMM17] so that they can serve as a\nguide for future work. We also refer the reader to [CYD19], which gives a\nsimilar simplified exposition of the proof covered in Section 2.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 19:56:46 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Musco", "Cameron", ""], ["Musco", "Christopher", ""]]}, {"id": "2004.08454", "submitter": "Alexander Wein", "authors": "Justin Holmgren, Alexander S. Wein", "title": "Counterexamples to the Low-Degree Conjecture", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conjecture of Hopkins (2018) posits that for certain high-dimensional\nhypothesis testing problems, no polynomial-time algorithm can outperform\nso-called \"simple statistics\", which are low-degree polynomials in the data.\nThis conjecture formalizes the beliefs surrounding a line of recent work that\nseeks to understand statistical-versus-computational tradeoffs via the\nlow-degree likelihood ratio. In this work, we refute the conjecture of Hopkins.\nHowever, our counterexample crucially exploits the specifics of the noise\noperator used in the conjecture, and we point out a simple way to modify the\nconjecture to rule out our counterexample. We also give an example illustrating\nthat (even after the above modification), the symmetry assumption in the\nconjecture is necessary. These results do not undermine the low-degree\nframework for computational lower bounds, but rather aim to better understand\nwhat class of problems it is applicable to.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 21:08:11 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Holmgren", "Justin", ""], ["Wein", "Alexander S.", ""]]}, {"id": "2004.08604", "submitter": "Massimo Cafaro", "authors": "Italo Epicoco, Catiuscia Melle, Massimo Cafaro, Marco Pulimeno and\n  Giuseppe Morleo", "title": "UDDSketch: Accurate Tracking of Quantiles in Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present UDDSketch (Uniform DDSketch), a novel sketch for fast and accurate\ntracking of quantiles in data streams. This sketch is heavily inspired by the\nrecently introduced DDSketch, and is based on a novel bucket collapsing\nprocedure that allows overcoming the intrinsic limits of the corresponding\nDDSketch procedures. Indeed, the DDSketch bucket collapsing procedure does not\nallow the derivation of formal guarantees on the accuracy of quantile\nestimation for data which does not follow a sub-exponential distribution. On\nthe contrary, UDDSketch is designed so that accuracy guarantees can be given\nover the full range of quantiles and for arbitrary distribution in input.\nMoreover, our algorithm fully exploits the budgeted memory adaptively in order\nto guarantee the best possible accuracy over the full range of quantiles.\nExtensive experimental results on synthetic datasets confirm the validity of\nour approach.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 12:18:32 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Epicoco", "Italo", ""], ["Melle", "Catiuscia", ""], ["Cafaro", "Massimo", ""], ["Pulimeno", "Marco", ""], ["Morleo", "Giuseppe", ""]]}, {"id": "2004.08634", "submitter": "Zhuan Khye Koh", "authors": "Daniel Dadush, Zhuan Khye Koh, Bento Natura, L\\'aszl\\'o A. V\\'egh", "title": "An Accelerated Newton-Dinkelbach Method and its Application to Two\n  Variables Per Inequality Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an accelerated, or 'look-ahead' version of the Newton-Dinkelbach\nmethod, a well-known technique for solving fractional and parametric\noptimization problems. This acceleration halves the Bregman divergence between\nthe current iterate and the optimal solution within every two iterations. Using\nthe Bregman divergence as a potential in conjunction with combinatorial\narguments, we obtain strongly polynomial algorithms in three applications\ndomains: (i) For linear fractional combinatorial optimization, we show a\nconvergence bound of $O(m \\log m)$ iterations; the previous best bound was\n$O(m^2 \\log m)$ by Wang et al. (2006). (ii) We obtain a strongly polynomial\nlabel-correcting algorithm for solving linear feasibility systems with two\nvariables per inequality (2VPI). For a 2VPI system with $n$ variables and $m$\nconstraints, our algorithm runs in $O(mn)$ iterations. Every iteration takes\n$O(mn)$ time for general 2VPI systems, and $O(m + n \\log n)$ time for the\nspecial case of deterministic Markov Decision Processes (DMDPs). This extends\nand strengthens a previous result by Madani (2002) that showed a weakly\npolynomial bound for a variant of the Newton-Dinkelbach method for solving\nDMDPs. (iii) We give a simplified variant of the parametric submodular function\nminimization result by Goemans et al. (2017).\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 14:42:16 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 13:47:09 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 10:48:35 GMT"}, {"version": "v4", "created": "Fri, 21 May 2021 16:23:54 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Dadush", "Daniel", ""], ["Koh", "Zhuan Khye", ""], ["Natura", "Bento", ""], ["V\u00e9gh", "L\u00e1szl\u00f3 A.", ""]]}, {"id": "2004.08636", "submitter": "Jacob Turner", "authors": "Jacob Turner", "title": "Mapping Matchings to Minimum Vertex Covers: K\\H{o}nig's Theorem\n  Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a celebrated result in early combinatorics that, in bipartite graphs,\nthe size of maximum matching is equal to the size of a minimum vertex cover.\nK\\H{o}nig's proof of this fact gave an algorithm for finding a minimum vertex\ncover from a maximum matching. In this paper, we revisit the connection this\nalgorithm induces between the two types of structures. We find that all minimum\nvertex covers can be found by applying this algorithm to some matching and then\nclassify which matchings give minimum vertex covers when this algorithm is\napplied to them.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 14:54:18 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Turner", "Jacob", ""]]}, {"id": "2004.08645", "submitter": "Florian H\\\"orsch", "authors": "Florian H\\\"orsch, Zolt\\'an Szigeti", "title": "The $(2,k)$-connectivity augmentation problem: Algorithmic aspects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Durand de Gevigney and Szigeti \\cite{DgGSz} have recently given a min-max\ntheorem for the $(2,k)$-connectivity augmentation problem. This article\nprovides an $O(n^3(m+ n \\textrm{ }log\\textrm{ }n))$ algorithm to find an\noptimal solution for this problem.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 15:42:01 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 09:36:50 GMT"}, {"version": "v3", "created": "Sat, 3 Apr 2021 09:09:31 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["H\u00f6rsch", "Florian", ""], ["Szigeti", "Zolt\u00e1n", ""]]}, {"id": "2004.08681", "submitter": "Jacob Bringewatt", "authors": "Jacob Bringewatt and Michael Jarret", "title": "Effective gaps are not effective: quasipolynomial classical simulation\n  of obstructed stoquastic Hamiltonians", "comments": "12 pages, 6 figures, added footnotes (v2), updated following peer\n  review (v3)", "journal-ref": "Phys. Rev. Lett. 125, 170504 (2020)", "doi": "10.1103/PhysRevLett.125.170504", "report-no": null, "categories": "quant-ph cond-mat.str-el cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All known examples confirming the possibility of an exponential separation\nbetween classical simulation algorithms and stoquastic adiabatic quantum\ncomputing (AQC) exploit symmetries that constrain adiabatic dynamics to\neffective, symmetric subspaces. The symmetries produce large effective\neigenvalue gaps, which in turn make adiabatic computation efficient. We present\na classical algorithm to efficiently sample from the effective subspace of a\n$k$-local stoquastic Hamiltonian $H$, without a priori knowledge of its\nsymmetries (or near-symmetries). Our algorithm maps any $k$-local Hamiltonian\nto a graph $G=(V,E)$ with $\\lvert V \\rvert = O\\left(\\mathrm{poly}(n)\\right)$\nwhere $n$ is the number of qubits. Given the well-known result of Babai, we\nexploit graph isomorphism to study the automorphisms of $G$ and arrive at an\nalgorithm quasi-polynomial in $\\lvert V\\rvert$ for producing samples from the\neffective subspace eigenstates of $H$. Our results rule out exponential\nseparations between stoquastic AQC and classical computation that arise from\nhidden symmetries in $k$-local Hamiltonians. Furthermore, our graph\nrepresentation of $H$ is not limited to stoquastic Hamiltonians and may rule\nout corresponding obstructions in non-stoquastic cases, or be useful in\nstudying additional properties of $k$-local Hamiltonians.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 18:25:04 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 01:24:52 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 20:11:24 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Bringewatt", "Jacob", ""], ["Jarret", "Michael", ""]]}, {"id": "2004.08703", "submitter": "Soheil Behnezhad", "authors": "Soheil Behnezhad, Mahsa Derakhshan", "title": "Stochastic Weighted Matching: $(1-\\epsilon)$ Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V, E)$ be a given edge-weighted graph and let its {\\em realization}\n$\\mathcal{G}$ be a random subgraph of $G$ that includes each edge $e \\in E$\nindependently with probability $p$. In the {\\em stochastic matching} problem,\nthe goal is to pick a sparse subgraph $Q$ of $G$ without knowing the\nrealization $\\mathcal{G}$, such that the maximum weight matching among the\nrealized edges of $Q$ (i.e. graph $Q \\cap \\mathcal{G}$) in expectation\napproximates the maximum weight matching of the whole realization\n$\\mathcal{G}$.\n  In this paper, we prove that for any desirably small $\\epsilon \\in (0, 1)$,\nevery graph $G$ has a subgraph $Q$ that guarantees a\n$(1-\\epsilon)$-approximation and has maximum degree only $O_{\\epsilon, p}(1)$.\nThat is, the maximum degree of $Q$ depends only on $\\epsilon$ and $p$ (both of\nwhich are known to be necessary) and not for example on the number of nodes in\n$G$, the edge-weights, etc.\n  The stochastic matching problem has been studied extensively on both weighted\nand unweighted graphs. Previously, only existence of (close to)\nhalf-approximate subgraphs was known for weighted graphs [Yamaguchi and\nMaehara, SODA'18; Behnezhad et al., SODA'19]. Our result substantially improves\nover these works, matches the state-of-the-art for unweighted graphs [Behnezhad\net al., STOC'20], and essentially settles the approximation factor.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 20:48:18 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Behnezhad", "Soheil", ""], ["Derakhshan", "Mahsa", ""]]}, {"id": "2004.08777", "submitter": "Yinzhan Xu", "authors": "Bryce Sandlund, Yinzhan Xu", "title": "Faster Dynamic Range Mode", "comments": "To appear in ICALP'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the dynamic range mode problem, we are given a sequence $a$ of length\nbounded by $N$ and asked to support element insertion, deletion, and queries\nfor the most frequent element of a contiguous subsequence of $a$. In this work,\nwe devise a deterministic data structure that handles each operation in\nworst-case $\\tilde{O}(N^{0.655994})$ time, thus breaking the $O(N^{2/3})$\nper-operation time barrier for this problem. The data structure is achieved by\ncombining the ideas in Williams and Xu (SODA 2020) for batch range mode with a\nnovel data structure variant of the Min-Plus product.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 06:14:42 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Sandlund", "Bryce", ""], ["Xu", "Yinzhan", ""]]}, {"id": "2004.08818", "submitter": "Bart M. P. Jansen", "authors": "Bart M.P. Jansen and Jari J.H. de Kroon", "title": "Preprocessing Vertex-Deletion Problems: Characterizing Graph Properties\n  by Low-Rank Adjacencies", "comments": "To appear in the Proceedings of SWAT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the $\\Pi$-free Deletion problem parameterized by the size of a\nvertex cover, for a range of graph properties $\\Pi$. Given an input graph $G$,\nthis problem asks whether there is a subset of at most $k$ vertices whose\nremoval ensures the resulting graph does not contain a graph from $\\Pi$ as\ninduced subgraph. Many vertex-deletion problems such as Perfect Deletion,\nWheel-free Deletion, and Interval Deletion fit into this framework. We\nintroduce the concept of characterizing a graph property $\\Pi$ by low-rank\nadjacencies, and use it as the cornerstone of a general kernelization theorem\nfor $\\Pi$-Free Deletion parameterized by the size of a vertex cover. The\nresulting framework captures problems such as AT-Free Deletion, Wheel-free\nDeletion, and Interval Deletion. Moreover, our new framework shows that the\nvertex-deletion problem to perfect graphs has a polynomial kernel when\nparameterized by vertex cover, thereby resolving an open question by Fomin et\nal. [JCSS 2014]. Our main technical contribution shows how linear-algebraic\ndependence of suitably defined vectors over $\\mathbb{F}_2$ implies\ngraph-theoretic statements about the presence of forbidden induced subgraphs.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 11:21:22 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["de Kroon", "Jari J. H.", ""]]}, {"id": "2004.08828", "submitter": "Amir Kafshdar Goharshady", "authors": "Ali Asadi, Krishnendu Chatterjee, Amir Kafshdar Goharshady, Kiarash\n  Mohammadi, Andreas Pavlogiannis", "title": "Faster Algorithms for Quantitative Analysis of Markov Chains and Markov\n  Decision Processes with Small Treewidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete-time Markov Chains (MCs) and Markov Decision Processes (MDPs) are\ntwo standard formalisms in system analysis. Their main associated quantitative\nobjectives are hitting probabilities, discounted sum, and mean payoff. Although\nthere are many techniques for computing these objectives in general MCs/MDPs,\nthey have not been thoroughly studied in terms of parameterized algorithms,\nparticularly when treewidth is used as the parameter. This is in sharp contrast\nto qualitative objectives for MCs, MDPs and graph games, for which\ntreewidth-based algorithms yield significant complexity improvements.\n  In this work, we show that treewidth can also be used to obtain faster\nalgorithms for the quantitative problems. For an MC with $n$ states and $m$\ntransitions, we show that each of the classical quantitative objectives can be\ncomputed in $O((n+m)\\cdot t^2)$ time, given a tree decomposition of the MC that\nhas width $t$. Our results also imply a bound of $O(\\kappa\\cdot (n+m)\\cdot\nt^2)$ for each objective on MDPs, where $\\kappa$ is the number of\nstrategy-iteration refinements required for the given input and objective.\nFinally, we make an experimental evaluation of our new algorithms on\nlow-treewidth MCs and MDPs obtained from the DaCapo benchmark suite. Our\nexperimental results show that on MCs and MDPs with small treewidth, our\nalgorithms outperform existing well-established methods by one or more orders\nof magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 12:03:17 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Asadi", "Ali", ""], ["Chatterjee", "Krishnendu", ""], ["Goharshady", "Amir Kafshdar", ""], ["Mohammadi", "Kiarash", ""], ["Pavlogiannis", "Andreas", ""]]}, {"id": "2004.08949", "submitter": "Andris Ambainis", "authors": "Andris Ambainis and Nikita Larka", "title": "Quantum algorithms for computational geometry problems", "comments": "10 pages", "journal-ref": "Proceedings of TQC'2020, LIPICS, vol. 158, article no. 9", "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study quantum algorithms for problems in computational geometry, such as\nPOINT-ON-3-LINES problem. In this problem, we are given a set of lines and we\nare asked to find a point that lies on at least $3$ of these lines.\nPOINT-ON-3-LINES and many other computational geometry problems are known to be\n3SUM-HARD. That is, solving them classically requires time\n$\\Omega(n^{2-o(1)})$, unless there is faster algorithm for the well known 3SUM\nproblem (in which we are given a set $S$ of $n$ integers and have to determine\nif there are $a, b, c \\in S$ such that $a + b + c = 0$). Quantumly, 3SUM can be\nsolved in time $O(n \\log n)$ using Grover's quantum search algorithm. This\nleads to a question: can we solve POINT-ON-3-LINES and other 3SUM-HARD problems\nin $O(n^c)$ time quantumly, for $c<2$? We answer this question affirmatively,\nby constructing a quantum algorithm that solves POINT-ON-3-LINES in time\n$O(n^{1 + o(1)})$. The algorithm combines recursive use of amplitude\namplification with geometrical ideas. We show that the same ideas give $O(n^{1\n+ o(1)})$ time algorithm for many 3SUM-HARD geometrical problems.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 20:01:50 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ambainis", "Andris", ""], ["Larka", "Nikita", ""]]}, {"id": "2004.08959", "submitter": "James Trimble", "authors": "James Trimble", "title": "An Algorithm for the Exact Treedepth Problem", "comments": "14 pages, 6 figures, 2 tables. This is an extended version of a paper\n  to appear in the proceedings of SEA 2020. The arXiv version is the conference\n  version plus Appendix A (a correctness proof)", "journal-ref": null, "doi": "10.4230/LIPIcs.SEA.2020.19", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm for the minimum-depth elimination tree problem,\nwhich is equivalent to the optimal treedepth decomposition problem. Our\nalgorithm makes use of two cheaply-computed lower bound functions to prune the\nsearch tree, along with symmetry-breaking and domination rules. We present an\nempirical study showing that the algorithm outperforms the current\nstate-of-the-art solver (which is based on a SAT encoding) by orders of\nmagnitude on a range of graph classes.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 20:47:39 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Trimble", "James", ""]]}, {"id": "2004.08993", "submitter": "Kiran Tomlinson", "authors": "Patty Commins, David Liben-Nowell, Tina Liu, Kiran Tomlinson", "title": "Summarizing Diverging String Sequences, with Applications to\n  Chain-Letter Petitions", "comments": "18 pages, 6 figures. Accepted to Combinatorial Pattern Matching (CPM)\n  2020", "journal-ref": null, "doi": "10.4230/LIPIcs.CPM.2020.11", "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms to find optimal alignments among strings, or to find a\nparsimonious summary of a collection of strings, are well studied in a variety\nof contexts, addressing a wide range of interesting applications. In this\npaper, we consider chain letters, which contain a growing sequence of\nsignatories added as the letter propagates. The unusual constellation of\nfeatures exhibited by chain letters (one-ended growth, divergence, and\nmutation) make their propagation, and thus the corresponding reconstruction\nproblem, both distinctive and rich. Here, inspired by these chain letters, we\nformally define the problem of computing an optimal summary of a set of\ndiverging string sequences. From a collection of these sequences of names, with\neach sequence noisily corresponding to a branch of the unknown tree $T$\nrepresenting the letter's true dissemination, can we efficiently and accurately\nreconstruct a tree $T' \\approx T$? In this paper, we give efficient exact\nalgorithms for this summarization problem when the number of sequences is\nsmall; for larger sets of sequences, we prove hardness and provide an efficient\nheuristic algorithm. We evaluate this heuristic on synthetic data sets chosen\nto emulate real chain letters, showing that our algorithm is competitive with\nor better than previous approaches, and that it also comes close to finding the\ntrue trees in these synthetic datasets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 00:00:45 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Commins", "Patty", ""], ["Liben-Nowell", "David", ""], ["Liu", "Tina", ""], ["Tomlinson", "Kiran", ""]]}, {"id": "2004.09051", "submitter": "Zhijing Mou", "authors": "Z. George Mou", "title": "Black-White Array: A New Data Structure for Dynamic Data Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new array based data structure named black-white array (BWA) is introduced\nas an effective and efficient alternative to the list or tree based data\nstructures for dynamic data set. It consists of two sub-arrays, one white and\none black of half of the size of the white. Both of them are conceptually\npartitioned into segments of different ranks with the sizes grow in geometric\nsequence. The layout of BWA allows easy calculation of the meta-data about the\nsegments, which are used extensively in the algorithms for the basic operations\nof the dynamic sets. The insertion of a sequence of unordered numbers into BWA\ntakes amortized time logarithmic to the length of the sequence. It is also\nproven that when the searched or deleted value is present in the BWA, the\nasymptotic amortized cost for the operations is O(log(n)); otherwise, the time\nwill fall somewhere between O(log(n)) and O(log^2(n)). It is shown that the\nstate variable total, which records the number of values in the BWA captures\nthe dynamics of state transition of BWA. This fact is exploited to produce\nconcise, easy- to-understand, and efficient coding for the operations. As it\nuses arrays as the underlying structure for dynamic set, a BWA need neither the\nspace to store the pointers referencing other data nodes nor the time to chase\nthe pointers as with any linked data structures. A C++ implementation of the\nBWA is completed. The performance data were gathered and plotted, which\nconfirmed the theoretic analysis. The testing results showed that the amortized\ntime for the insert, search, and delete operations is all just between 105.949\nand 5720.49 nanoseconds for BWAs of sizes ranging from 210 to 229 under various\nconditions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 04:53:27 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Mou", "Z. George", ""]]}, {"id": "2004.09079", "submitter": "Micha{\\l} Derezi\\'nski", "authors": "Nima Anari and Micha{\\l} Derezi\\'nski", "title": "Isotropy and Log-Concave Polynomials: Accelerated Sampling and\n  High-Precision Counting of Matroid Bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a notion of isotropy for discrete set distributions. If $\\mu$ is a\ndistribution over subsets $S$ of a ground set $[n]$, we say that $\\mu$ is in\nisotropic position if $P[e \\in S]$ is the same for all $e\\in [n]$. We design a\nnew approximate sampling algorithm that leverages isotropy for the class of\ndistributions $\\mu$ that have a log-concave generating polynomial; this class\nincludes determinantal point processes, strongly Rayleigh distributions, and\nuniform distributions over matroid bases. We show that when $\\mu$ is in\napproximately isotropic position, the running time of our algorithm depends\npolynomially on the size of the set $S$, and only logarithmically on $n$. When\n$n$ is much larger than the size of $S$, this is significantly faster than\nprior algorithms, and can even be sublinear in $n$. We then show how to\ntransform a non-isotropic $\\mu$ into an equivalent approximately isotropic form\nwith a polynomial-time preprocessing step, accelerating subsequent sampling\ntimes. The main new ingredient enabling our algorithms is a class of negative\ndependence inequalities that may be of independent interest.\n  As an application of our results, we show how to approximately count bases of\na matroid of rank $k$ over a ground set of $n$ elements to within a factor of\n$1+\\epsilon$ in time $ O((n+1/\\epsilon^2)\\cdot poly(k, \\log n))$. This is the\nfirst algorithm that runs in nearly linear time for fixed rank $k$, and\nachieves an inverse polynomially low approximation error.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 06:31:47 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Anari", "Nima", ""], ["Derezi\u0144ski", "Micha\u0142", ""]]}, {"id": "2004.09083", "submitter": "Zongchen Chen", "authors": "Zongchen Chen, Kuikui Liu, Eric Vigoda", "title": "Rapid Mixing of Glauber Dynamics up to Uniqueness via Contraction", "comments": "Section 7 and Appendix E are updated to fix a small error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math-ph math.MP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For general antiferromagnetic 2-spin systems, including the hardcore model\nand the antiferromagnetic Ising model, there is an $\\mathsf{FPTAS}$ for the\npartition function on graphs of maximum degree $\\Delta$ when the infinite\nregular tree lies in the uniqueness region by Li et al. (2013). Moreover, in\nthe tree non-uniqueness region, Sly (2010) showed that there is no\n$\\mathsf{FPRAS}$ to estimate the partition function unless\n$\\mathsf{NP}=\\mathsf{RP}$. The algorithmic results follow from the correlation\ndecay approach due to Weitz (2006) or the polynomial interpolation approach\ndeveloped by Barvinok (2016). However the running time is only polynomial for\nconstant $\\Delta$. For the hardcore model, recent work of Anari et al. (2020)\nestablishes rapid mixing of the simple single-site Markov chain known as the\nGlauber dynamics in the tree uniqueness region. Our work simplifies their\nanalysis of the Glauber dynamics by considering the total pairwise influence of\na fixed vertex $v$ on other vertices, as opposed to the total influence on $v$,\nthereby extending their work to all 2-spin models and improving the mixing\ntime.\n  More importantly our proof ties together the three disparate algorithmic\napproaches: we show that contraction of the tree recursions with a suitable\npotential function, which is the primary technique for establishing efficiency\nof Weitz's correlation decay approach and Barvinok's polynomial interpolation\napproach, also establishes rapid mixing of the Glauber dynamics. We emphasize\nthat this connection holds for all 2-spin models (both antiferromagnetic and\nferromagnetic), and existing proofs for correlation decay or polynomial\ninterpolation immediately imply rapid mixing of Glauber dynamics. Our proof\nutilizes that the graph partition function divides that of Weitz's\nself-avoiding walk trees, leading to new tools for analyzing influence of\nvertices.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 06:46:14 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 02:25:04 GMT"}, {"version": "v3", "created": "Sun, 18 Jul 2021 22:38:39 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Chen", "Zongchen", ""], ["Liu", "Kuikui", ""], ["Vigoda", "Eric", ""]]}, {"id": "2004.09099", "submitter": "Christian Schulz", "authors": "Monika Henzinger, Shahbaz Khan, Richard Paul, Christian Schulz", "title": "Dynamic Matching Algorithms in Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, significant advances have been made in the design and\nanalysis of fully dynamic maximal matching algorithms. However, these\ntheoretical results have received very little attention from the practical\nperspective. Few of the algorithms are implemented and tested on real datasets,\nand their practical potential is far from understood. In this paper, we attempt\nto bridge the gap between theory and practice that is currently observed for\nthe fully dynamic maximal matching problem. We engineer several algorithms and\nempirically study those algorithms on an extensive set of dynamic instances.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 07:30:15 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Henzinger", "Monika", ""], ["Khan", "Shahbaz", ""], ["Paul", "Richard", ""], ["Schulz", "Christian", ""]]}, {"id": "2004.09129", "submitter": "Sagnik Mukhopadhyay", "authors": "Michal Dory, Yuval Efron, Sagnik Mukhopadhyay, Danupon Nanongkai", "title": "Distributed Weighted Min-Cut in Nearly-Optimal Time", "comments": "Major changes: (i) The fragment decomposition technique is\n  simplified, (ii) Introduction and technical overview have been redone, and\n  (iii) The technical sections have been made simpler for better readability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimum-weight cut (min-cut) is a basic measure of a network's connectivity\nstrength. While the min-cut can be computed efficiently in the sequential\nsetting [Karger STOC'96], there was no efficient way for a distributed network\nto compute its own min-cut without limiting the input structure or dropping the\noutput quality: In the standard CONGEST model, existing algorithms with\nnearly-optimal time (e.g. [Ghaffari, Kuhn, DISC'13; Nanongkai, Su, DISC'14])\ncan guarantee a solution that is $(1+\\epsilon)$-approximation at best while the\nexact $\\tilde O(n^{0.8}D^{0.2} + n^{0.9})$-time algorithm [Ghaffari, Nowicki,\nThorup, SODA'20] works only on *simple* networks (no weights and no parallel\nedges). Here $n$ and $D$ denote the network's number of vertices and\nhop-diameter, respectively. For the weighted case, the best bound was $\\tilde\nO(n)$ [Daga, Henzinger, Nanongkai, Saranurak, STOC'19].\n  In this paper, we provide an *exact* $\\tilde O(\\sqrt n + D)$-time algorithm\nfor computing min-cut on *weighted* networks. Our result improves even the\nprevious algorithm that works only on simple networks. Its time complexity\nmatches the known lower bound up to polylogarithmic factors. At the heart of\nour algorithm are a clever routing trick and two structural lemmas regarding\nthe structure of a minimum cut of a graph. These two structural lemmas\nconsiderably strengthen and generalize the framework of Mukhopadhyay-Nanongkai\n[STOC'20] and can be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 08:44:11 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 17:23:41 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Dory", "Michal", ""], ["Efron", "Yuval", ""], ["Mukhopadhyay", "Sagnik", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "2004.09163", "submitter": "Tim Zeitz", "authors": "Alexander Kleff, Frank Schulz, Jakob Wagenblatt, Tim Zeitz", "title": "Efficient Route Planning with Temporary Driving Bans, Road Closures, and\n  Rated Parking Areas", "comments": "17 pages, 5 figures, full version of SEA2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of planning routes in road networks when certain streets\nor areas are closed at certain times. For heavy vehicles, such areas may be\nvery large since many European countries impose temporary driving bans during\nthe night or on weekends. In this setting, feasible routes may require waiting\nat parking areas, and several feasible routes with different trade-offs between\nwaiting and driving detours around closed areas may exist. We propose a novel\nmodel in which driving and waiting are assigned abstract costs, and waiting\ncosts are location-dependent to reflect the different quality of the parking\nareas. Our goal is to find Pareto-optimal routes with regards to arrival time\nat the destination and total cost. We investigate the complexity of the model\nand determine a necessary constraint on the cost parameters such that the\nproblem is solvable in polynomial time. We present a thoroughly engineered\nimplementation and perform experiments on a production-grade real world data\nset. The experiments show that our implementation can answer realistic queries\nin around a second or less which makes it feasible for practical application.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 09:43:54 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Kleff", "Alexander", ""], ["Schulz", "Frank", ""], ["Wagenblatt", "Jakob", ""], ["Zeitz", "Tim", ""]]}, {"id": "2004.09425", "submitter": "Pawe{\\l} Rz\\k{a}\\.zewski", "authors": "Maria Chudnovsky, Jason King, Micha{\\l} Pilipczuk, Pawe{\\l}\n  Rz\\k{a}\\.zewski, Sophie Spirkl", "title": "Finding large $H$-colorable subgraphs in hereditary graph classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the \\textsc{Max Partial $H$-Coloring} problem: given a graph $G$,\nfind the largest induced subgraph of $G$ that admits a homomorphism into $H$,\nwhere $H$ is a fixed pattern graph without loops. Note that when $H$ is a\ncomplete graph on $k$ vertices, the problem reduces to finding the largest\ninduced $k$-colorable subgraph, which for $k=2$ is equivalent (by\ncomplementation) to \\textsc{Odd Cycle Transversal}.\n  We prove that for every fixed pattern graph $H$ without loops, \\textsc{Max\nPartial $H$-Coloring} can be solved:\n  $\\bullet$ in $\\{P_5,F\\}$-free graphs in polynomial time, whenever $F$ is a\nthreshold graph;\n  $\\bullet$ in $\\{P_5,\\textrm{bull}\\}$-free graphs in polynomial time;\n  $\\bullet$ in $P_5$-free graphs in time $n^{\\mathcal{O}(\\omega(G))}$;\n  $\\bullet$ in $\\{P_6,\\textrm{1-subdivided claw}\\}$-free graphs in time\n$n^{\\mathcal{O}(\\omega(G)^3)}$.\n  Here, $n$ is the number of vertices of the input graph $G$ and $\\omega(G)$ is\nthe maximum size of a clique in~$G$. Furthermore, combining the mentioned\nalgorithms for $P_5$-free and for $\\{P_6,\\textrm{1-subdivided claw}\\}$-free\ngraphs with a simple branching procedure, we obtain subexponential-time\nalgorithms for \\textsc{Max Partial $H$-Coloring} in these classes of graphs.\nFinally, we show that even a restricted variant of \\textsc{Max Partial\n$H$-Coloring} is $\\mathsf{NP}$-hard in the considered subclasses of $P_5$-free\ngraphs, if we allow loops on $H$.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 16:29:59 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 13:52:25 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Chudnovsky", "Maria", ""], ["King", "Jason", ""], ["Pilipczuk", "Micha\u0142", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""], ["Spirkl", "Sophie", ""]]}, {"id": "2004.09454", "submitter": "Yuan Zhou", "authors": "Nikolai Karpov, Qin Zhang, Yuan Zhou", "title": "Collaborative Top Distribution Identifications with Limited Interaction", "comments": "Accepted for presentation at FOCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem in this paper: given a set of $n$\ndistributions, find the top-$m$ ones with the largest means. This problem is\nalso called {\\em top-$m$ arm identifications} in the literature of\nreinforcement learning, and has numerous applications. We study the problem in\nthe collaborative learning model where we have multiple agents who can draw\nsamples from the $n$ distributions in parallel. Our goal is to characterize the\ntradeoffs between the running time of learning process and the number of rounds\nof interaction between agents, which is very expensive in various scenarios. We\ngive optimal time-round tradeoffs, as well as demonstrate complexity\nseparations between top-$1$ arm identification and top-$m$ arm identifications\nfor general $m$ and between fixed-time and fixed-confidence variants. As a\nbyproduct, we also give an algorithm for selecting the distribution with the\n$m$-th largest mean in the collaborative learning model.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 17:11:20 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 02:31:05 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Karpov", "Nikolai", ""], ["Zhang", "Qin", ""], ["Zhou", "Yuan", ""]]}, {"id": "2004.09640", "submitter": "Xiaoqi Tan", "authors": "Xiaoqi Tan and Bo Sun and Alberto Leon-Garcia and Yuan Wu and Danny\n  H.K. Tsang", "title": "Mechanism Design for Online Resource Allocation: A Unified Approach", "comments": "49 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the mechanism design for online resource allocation in a\nstrategic setting. In this setting, a single supplier allocates\ncapacity-limited resources to requests that arrive in a sequential and\narbitrary manner. Each request is associated with an agent who may act\nselfishly to misreport the requirement and valuation of her request. The\nsupplier charges payment from agents whose requests are satisfied, but incurs a\nload-dependent supply cost. The goal is to design an incentive compatible\nonline mechanism, which determines not only the resource allocation of each\nrequest, but also the payment of each agent, so as to (approximately) maximize\nthe social welfare (i.e., aggregate valuations minus supply cost). We study\nthis problem under the framework of competitive analysis. The major\ncontribution of this paper is the development of a unified approach that\nachieves the best-possible competitive ratios for setups with different supply\ncosts. Specifically, we show that when there is no supply cost or the supply\ncost function is linear, our model is essentially a standard 0-1 knapsack\nproblem, for which our approach achieves logarithmic competitive ratios that\nmatch the state-of-the-art (which is optimal). For the more challenging setup\nwhen the supply cost is strictly-convex, we provide online mechanisms, for the\nfirst time, that lead to the optimal competitive ratios as well. To the best of\nour knowledge, this is the first approach that unifies the characterization of\noptimal competitive ratios in online resource allocation for different setups\nincluding zero, linear and strictly-convex supply costs.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 21:18:08 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Tan", "Xiaoqi", ""], ["Sun", "Bo", ""], ["Leon-Garcia", "Alberto", ""], ["Wu", "Yuan", ""], ["Tsang", "Danny H. K.", ""]]}, {"id": "2004.09682", "submitter": "Janardhan Kulkarni", "authors": "Sami Davies, Janardhan Kulkarni, Thomas Rothvoss, Jakub Tarnawski,\n  Yihao Zhang", "title": "Scheduling with Communication Delays via LP Hierarchies and Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classic problem of scheduling jobs with precedence\nconstraints on identical machines to minimize makespan, in the presence of\ncommunication delays. In this setting, denoted by $\\mathsf{P} \\mid\n\\mathsf{prec}, c \\mid C_{\\mathsf{max}}$, if two dependent jobs are scheduled on\ndifferent machines, then at least $c$ units of time must pass between their\nexecutions. Despite its relevance to many applications, this model remains one\nof the most poorly understood in scheduling theory. Even for a special case\nwhere an unlimited number of machines is available, the best known\napproximation ratio is $2/3 \\cdot (c+1)$, whereas Graham's greedy list\nscheduling algorithm already gives a $(c+1)$-approximation in that setting. An\noutstanding open problem in the top-10 list by Schuurman and Woeginger and its\nrecent update by Bansal asks whether there exists a constant-factor\napproximation algorithm.\n  In this work we give a polynomial-time $O(\\log c \\cdot \\log m)$-approximation\nalgorithm for this problem, where $m$ is the number of machines and $c$ is the\ncommunication delay. Our approach is based on a Sherali-Adams lift of a linear\nprogramming relaxation and a randomized clustering of the semimetric space\ninduced by this lift.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 00:00:52 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Davies", "Sami", ""], ["Kulkarni", "Janardhan", ""], ["Rothvoss", "Thomas", ""], ["Tarnawski", "Jakub", ""], ["Zhang", "Yihao", ""]]}, {"id": "2004.09784", "submitter": "Paul D\\\"utting", "authors": "Paul D\\\"utting, Thomas Kesselheim, Brendan Lucier", "title": "An O(log log m) Prophet Inequality for Subadditive Combinatorial\n  Auctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prophet inequalities compare the expected performance of an online algorithm\nfor a stochastic optimization problem to the expected optimal solution in\nhindsight. They are a major alternative to classic worst-case competitive\nanalysis, of particular importance in the design and analysis of simple\n(posted-price) incentive compatible mechanisms with provable approximation\nguarantees.\n  A central open problem in this area concerns subadditive combinatorial\nauctions. Here $n$ agents with subadditive valuation functions compete for the\nassignment of $m$ items. The goal is to find an allocation of the items that\nmaximizes the total value of the assignment. The question is whether there\nexists a prophet inequality for this problem that significantly beats the best\nknown approximation factor of $O(\\log m)$.\n  We make major progress on this question by providing an $O(\\log \\log m)$\nprophet inequality. Our proof goes through a novel primal-dual approach. It is\nalso constructive, resulting in an online policy that takes the form of static\nand anonymous item prices that can be computed in polynomial time given\nappropriate query access to the valuations. As an application of our approach,\nwe construct a simple and incentive compatible mechanism based on posted prices\nthat achieves an $O(\\log \\log m)$ approximation to the optimal revenue for\nsubadditive valuations under an item-independence assumption.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 07:22:07 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["D\u00fctting", "Paul", ""], ["Kesselheim", "Thomas", ""], ["Lucier", "Brendan", ""]]}, {"id": "2004.09885", "submitter": "Yixin Cao", "authors": "Yixin Cao", "title": "Enumerating Maximal Induced Subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$, the maximal induced subgraphs problem asks to enumerate\nall maximal induced subgraphs of $G$ that belong to a certain hereditary graph\nclass. While its optimization version, known as the minimum vertex deletion\nproblem in literature, has been intensively studied, enumeration algorithms are\nknown for a few simple graph classes, e.g., independent sets, cliques, and\nforests, until very recently [Conte and Uno, STOC 2019]. There is also a\nconnected variation of this problem, where one is concerned with only those\ninduced subgraphs that are connected. We introduce two new approaches, which\nenable us to develop algorithms that solve both variations for a number of\nimportant graph classes. A general technique that has been proved very powerful\nin enumeration algorithms is to build a solution map, i.e., a multiple digraph\non all the solutions of the problem, and the key of this approach is to make\nthe solution map strongly connected, so that a simple traversal of the solution\nmap solves the problem. We introduce retaliation-free paths to certificate\nstrong connectedness of the solution map we build. Generalizing the idea of\nCohen, Kimelfeld, and Sagiv [JCSS 2008], we introduce the $t$-restricted\nversion, $t$ being a positive integer, of the maximal (connected) induced\nsubgraphs problem, and show that it is equivalent to the original problem in\nterms of solvability in incremental polynomial time. Moreover, we give\nreductions between the two variations, so that it suffices to solve one of the\nvariations for each class we study. Our work also leads to direct and simpler\nproofs of several important known results.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 10:23:49 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Cao", "Yixin", ""]]}, {"id": "2004.09938", "submitter": "Ryan Mann", "authors": "Ryan L. Mann, Luke Mathieson, Catherine Greenhill", "title": "On the Parameterised Complexity of Induced Multipartite Graph Parameters", "comments": "9 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a family of graph parameters, called induced multipartite graph\nparameters, and study their computational complexity. First, we consider the\nfollowing decision problem: an instance is an induced multipartite graph\nparameter $p$ and a given graph $G$, and for natural numbers $k\\geq2$ and\n$\\ell$, we must decide whether the maximum value of $p$ over all induced\n$k$-partite subgraphs of $G$ is at most $\\ell$. We prove that this problem is\nW[1]-hard. Next, we consider a variant of this problem, where we must decide\nwhether the given graph $G$ contains a sufficiently large induced $k$-partite\nsubgraph $H$ such that $p(H)\\leq\\ell$. We show that for certain parameters this\nproblem is para-NP-hard, while for others it is fixed-parameter tractable.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 12:15:06 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Mann", "Ryan L.", ""], ["Mathieson", "Luke", ""], ["Greenhill", "Catherine", ""]]}, {"id": "2004.10019", "submitter": "Yuan Zhou", "authors": "Zihan Zhang, Yuan Zhou, Xiangyang Ji", "title": "Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage\n  Decomposition", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the reinforcement learning problem in the setting of finite-horizon\nepisodic Markov Decision Processes (MDPs) with $S$ states, $A$ actions, and\nepisode length $H$. We propose a model-free algorithm UCB-Advantage and prove\nthat it achieves $\\tilde{O}(\\sqrt{H^2SAT})$ regret where $T = KH$ and $K$ is\nthe number of episodes to play. Our regret bound improves upon the results of\n[Jin et al., 2018] and matches the best known model-based algorithms as well as\nthe information theoretic lower bound up to logarithmic factors. We also show\nthat UCB-Advantage achieves low local switching cost and applies to concurrent\nreinforcement learning, improving upon the recent results of [Bai et al.,\n2019].\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 14:00:06 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 13:35:38 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Zhang", "Zihan", ""], ["Zhou", "Yuan", ""], ["Ji", "Xiangyang", ""]]}, {"id": "2004.10090", "submitter": "Hu Ding", "authors": "Hu Ding", "title": "A Sub-linear Time Framework for Geometric Optimization with Outliers in\n  High Dimensions", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.03796", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world problems can be formulated as geometric optimization problems\nin high dimensions, especially in the fields of machine learning and data\nmining. Moreover, we often need to take into account of outliers when\noptimizing the objective functions. However, the presence of outliers could\nmake the problems to be much more challenging than their vanilla versions. In\nthis paper, we study the fundamental minimum enclosing ball (MEB) with outliers\nproblem first; partly inspired by the core-set method from B\\u{a}doiu and\nClarkson, we propose a sub-linear time bi-criteria approximation algorithm\nbased on two novel techniques, the Uniform-Adaptive Sampling method and\nSandwich Lemma. To the best of our knowledge, our result is the first\nsub-linear time algorithm, which has the sample size ({\\em i.e.,} the number of\nsampled points) independent of both the number of input points $n$ and\ndimensionality $d$, for MEB with outliers in high dimensions. Furthermore, we\nobserve that these two techniques can be generalized to deal with a broader\nrange of geometric optimization problems with outliers in high dimensions,\nincluding flat fitting, $k$-center clustering, and SVM with outliers, and\ntherefore achieve the sub-linear time algorithms for these problems\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 04:11:27 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 01:02:11 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Ding", "Hu", ""]]}, {"id": "2004.10163", "submitter": "Jonathan Schneider", "authors": "Allen Liu, Renato Paes Leme, Martin Pal, Jon Schneider,\n  Balasubramanian Sivan", "title": "Variable Decomposition for Prophet Inequalities and Optimal Ordering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new decomposition technique for random variables that maps a\ngeneric instance of the prophet inequalities problem to a new instance where\nall but a constant number of variables have a tractable structure that we refer\nto as $(\\varepsilon, \\delta)$-smallness. Using this technique, we make progress\non several outstanding problems in the area:\n  - We show that, even in the case of non-identical distributions, it is\npossible to achieve (arbitrarily close to) the optimal approximation ratio of\n$\\beta \\approx 0.745$ as long as we are allowed to remove a small constant\nnumber of distributions.\n  - We show that for frequent instances of prophet inequalities (where each\ndistribution reoccurs some number of times), it is possible to achieve the\noptimal approximation ratio of $\\beta$ (improving over the previous best-known\nbound of $0.738$).\n  - We give a new, simpler proof of Kertz's optimal approximation guarantee of\n$\\beta \\approx 0.745$ for prophet inequalities with i.i.d. distributions. The\nproof is primal-dual and simultaneously produces upper and lower bounds.\n  - Using this decomposition in combination with a novel convex programming\nformulation, we construct the first Efficient PTAS for the Optimal Ordering\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 17:18:16 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 17:13:40 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Liu", "Allen", ""], ["Leme", "Renato Paes", ""], ["Pal", "Martin", ""], ["Schneider", "Jon", ""], ["Sivan", "Balasubramanian", ""]]}, {"id": "2004.10319", "submitter": "Gramoz Goranci", "authors": "Sebastian Forster, Gramoz Goranci, Monika Henzinger", "title": "Dynamic Maintenance of Low-Stretch Probabilistic Tree Embeddings with\n  Applications", "comments": "abstract shortened to respect the arXiv limit of 1920 characters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first non-trivial fully dynamic probabilistic tree embedding\nalgorithm for weighted graphs undergoing edge insertions and deletions. We\nobtain a trade-off between amortized update time and expected stretch against\nan oblivious adversary. At the two extremes of this trade-off, we can maintain\na tree of expected stretch $ O (\\log^4 n) $ with update time $ m^{1/2 + o(1)} $\nor a tree of expected stretch $ n^{o(1)} $ with update time $ n^{o(1)} $ (for\nedge weights polynomial in $ n $). A guarantee of the latter type has so far\nonly been known for maintaining tree embeddings with average (instead of\nexpected) stretch [Chechik/Zhang, SODA '20].\n  Our main result has direct implications to fully dynamic approximate distance\noracles and fully dynamic buy-at-bulk network design. For dynamic distance\noracles, our result is the first to break the $ O (\\sqrt{m}) $ update-time\nbarrier. For buy-at-bulk network design, a problem which also in the static\nsetting heavily relies on probabilistic tree embeddings, we give the first\nnon-trivial dynamic algorithm. As probabilistic tree embeddings are an\nimportant tool in static approximation algorithms, further applications of our\nresult in dynamic approximation algorithms are conceivable.\n  From a technical perspective, we obtain our main result by first designing a\ndecremental algorithm for probabilistic low-diameter decompositions via a\ncareful combination of Bartal's ball-growing approach [FOCS '96] with the\npruning framework of Chechik and Zhang [SODA '20]. We then extend this to a\nfully dynamic algorithm by enriching a well-known 'decremental to fully\ndynamic' reduction with a new bootstrapping idea to recursively employ a fully\ndynamic algorithm instead of a static one in this reduction.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 22:01:29 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 12:41:50 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Forster", "Sebastian", ""], ["Goranci", "Gramoz", ""], ["Henzinger", "Monika", ""]]}, {"id": "2004.10332", "submitter": "Ran Ben Basat", "authors": "Ran Ben Basat, Gil Einziger, Michael Mitzenmacher, Shay Vargaftik", "title": "Faster and More Accurate Measurement through Additive-Error Counters", "comments": "To appear in IEEE INFOCOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counters are a fundamental building block for networking applications such as\nload balancing, traffic engineering, and intrusion detection, which require\nestimating flow sizes and identifying heavy hitter flows. Existing works\nsuggest replacing counters with shorter multiplicative error \\emph{estimators}\nthat improve the accuracy by fitting more of them within a given space.\nHowever, such estimators impose a computational overhead that degrades the\nmeasurement throughput. Instead, we propose \\emph{additive} error estimators,\nwhich are simpler, faster, and more accurate when used for network measurement.\nOur solution is rigorously analyzed and empirically evaluated against several\nother measurement algorithms on real Internet traces. For a given error target,\nwe improve the speed of the uncompressed solutions by $5\\times$-$30\\times$, and\nthe space by up to $4\\times$. Compared with existing state-of-the-art\nestimators, our solution is $ 9\\times$-$35\\times$ faster while being\nconsiderably more accurate.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 22:45:47 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Basat", "Ran Ben", ""], ["Einziger", "Gil", ""], ["Mitzenmacher", "Michael", ""], ["Vargaftik", "Shay", ""]]}, {"id": "2004.10358", "submitter": "Ying Cao", "authors": "Ying Cao, Bo Sun, Danny H.K. Tsang", "title": "Optimal Online Algorithms for One-Way Trading and Online Knapsack\n  Problems: A Unified Competitive Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study two canonical online optimization problems under capacity/budget\nconstraints: the fractional one-way trading problem (OTP) and the integral\nonline knapsack problem (OKP) under an infinitesimal assumption. Under the\ncompetitive analysis framework, it is well-known that both problems have the\nsame optimal competitive ratio. However, these two problems are investigated by\ndistinct approaches under separate contexts in the literature. There is a gap\nin understanding the connection between these two problems and the nature of\ntheir online algorithm design. This paper provides a unified framework for the\nonline algorithm design, analysis and optimality proof for both problems. We\nfind that the infinitesimal assumption of the OKP is the key that connects the\nOTP in the analysis of online algorithms and the construction of worst-case\ninstances. With this unified understanding, our framework shows its potential\nfor analyzing other extensions of OKP and OTP in a more systematic manner.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 01:34:22 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 04:06:54 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Cao", "Ying", ""], ["Sun", "Bo", ""], ["Tsang", "Danny H. K.", ""]]}, {"id": "2004.10596", "submitter": "Amit Saha", "authors": "Arpita Sanyal (Bhaduri), Amit Saha, Debasri Saha, Banani Saha and\n  Amlan Chakrabarti", "title": "Circuit Design for Clique Problem and Its Implementation on Quantum\n  Computer", "comments": "25 pages, 18 figures. arXiv admin note: text overlap with\n  arXiv:1805.10224 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding cliques in a graph has several applications for its pattern matching\nability. $k$-clique problem, a special case of clique problem, determines\nwhether an arbitrary graph contains a clique of size $k$, has already been\naddressed in quantum domain. A variant of $k$-clique problem that lists all\ncliques of size $k$, has also popular modern-day applications. Albeit, the\nimplementation of such variant of $k$-clique problem in quantum setting still\nremains untouched. In this paper, apart from theoretical solution of such\n$k$-clique problem, practical quantum gate-based implementation has been\naddressed using Grover's algorithm. This approach is further extended to design\ncircuit for the maximum clique problem in classical-quantum hybrid\narchitecture. The algorithm automatically generates the circuit for any given\nundirected and unweighted graph and any given $k$, which makes our approach\ngeneralized in nature. The proposed approach of solving $k$-clique problem has\nexhibited a reduction of qubit cost and circuit depth as compared to the\nstate-of-the-art approach, for a small $k$ with respect to a large graph. A\nframework that can map the automated generated circuit for clique problem to\nquantum devices is also proposed. An analysis of the experimental results is\ndemonstrated using IBM's Qiskit.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 04:29:35 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 11:03:36 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 18:20:17 GMT"}, {"version": "v4", "created": "Wed, 7 Jul 2021 18:59:30 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Sanyal", "Arpita", "", "Bhaduri"], ["Saha", "Amit", ""], ["Saha", "Debasri", ""], ["Saha", "Banani", ""], ["Chakrabarti", "Amlan", ""]]}, {"id": "2004.10776", "submitter": "David Stalfa", "authors": "Biswaroop Maiti, Rajmohan Rajaraman, David Stalfa, Zoya Svitkina,\n  Aravindan Vijayaraghavan", "title": "Scheduling Precedence-Constrained Jobs on Related Machines with\n  Communication Delay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of scheduling $n$ precedence-constrained jobs on $m$\nuniformly-related machines in the presence of an arbitrary, fixed communication\ndelay $\\rho$. We consider a model that allows job duplication, i.e. processing\nof the same job on multiple machines, which, as we show, can reduce the length\nof a schedule (i.e., its makespan) by a logarithmic factor. Our main result is\nan $O(\\log m \\log \\rho / \\log \\log \\rho)$-approximation algorithm for\nminimizing makespan, assuming the minimum makespan is at least $\\rho$. Our\nalgorithm is based on rounding a linear programming relaxation for the problem,\nwhich includes carefully designed constraints capturing the interaction among\ncommunication delay, precedence requirements, varying speeds, and job\nduplication. Our result builds on two previous lines of work, one with\ncommunication delay but identical machines (Lepere, Rapine 2002) and the other\nwith uniformly-related machines but no communication delay (Chudak, Shmoys\n1999). We next show that the integrality gap of our mathematical program is\n$\\Omega(\\sqrt{\\log \\rho})$. Our gap construction employs expander graphs and\nexploits a property of robust expansion and its generalization to paths of\nlonger length. Finally, we quantify the advantage of duplication in scheduling\nwith communication delay. We show that the best schedule without duplication\ncan have makespan $\\Omega(\\rho/\\log \\rho)$ or $\\Omega(\\log m/\\log\\log m)$ or\n$\\Omega(\\log n/\\log \\log n)$ times that of an optimal schedule allowing\nduplication. Nevertheless, we present a polynomial time algorithm to transform\nany schedule to a schedule without duplication at the cost of a $O(\\log^2 n\n\\log m)$ factor increase in makespan. Together with our makespan approximation\nalgorithm for schedules allowing duplication, this also yields a\npolylogarithmic-approximation algorithm for the setting where duplication is\nnot allowed.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 18:23:13 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Maiti", "Biswaroop", ""], ["Rajaraman", "Rajmohan", ""], ["Stalfa", "David", ""], ["Svitkina", "Zoya", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "2004.10805", "submitter": "Antonio Blanca", "authors": "Antonio Blanca, Zongchen Chen, Daniel \\v{S}tefankovi\\v{c} and Eric\n  Vigoda", "title": "Hardness of Identity Testing for Restricted Boltzmann Machines and Potts\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study identity testing for restricted Boltzmann machines (RBMs), and more\ngenerally for undirected graphical models. Given sample access to the Gibbs\ndistribution corresponding to an unknown or hidden model $M^*$ and given an\nexplicit model $M$, can we distinguish if either $M = M^*$ or if they are\n(statistically) far apart? Daskalakis et al. (2018) presented a polynomial-time\nalgorithm for identity testing for the ferromagnetic (attractive) Ising model.\nIn contrast, for the antiferromagnetic (repulsive) Ising model, Bez\\'akov\\'a et\nal. (2019) proved that unless $RP=NP$ there is no identity testing algorithm\nwhen $\\beta d=\\omega(\\log{n})$, where $d$ is the maximum degree of the visible\ngraph and $\\beta$ is the largest edge weight in absolute value.\n  We prove analogous hardness results for RBMs (i.e., mixed Ising models on\nbipartite graphs), even when there are no latent variables or an external\nfield. Specifically, we show that if $RP \\neq NP$, then when $\\beta\nd=\\omega(\\log{n})$ there is no polynomial-time algorithm for identity testing\nfor RBMs; when $\\beta d =O(\\log{n})$ there is an efficient identity testing\nalgorithm that utilizes the structure learning algorithm of Klivans and Meka\n(2017). In addition, we prove similar lower bounds for purely ferromagnetic\nRBMs with inconsistent external fields, and for the ferromagnetic Potts model.\nPrevious hardness results for identity testing of Bez\\'akov\\'a et al. (2019)\nutilized the hardness of finding the maximum cuts, which corresponds to the\nground states of the antiferromagnetic Ising model. Since RBMs are on bipartite\ngraphs such an approach is not feasible. We instead introduce a general\nmethodology to reduce from the corresponding approximate counting problem and\nutilize the phase transition that is exhibited by RBMs and the mean-field Potts\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 19:21:01 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Blanca", "Antonio", ""], ["Chen", "Zongchen", ""], ["\u0160tefankovi\u010d", "Daniel", ""], ["Vigoda", "Eric", ""]]}, {"id": "2004.10873", "submitter": "Guilherme De Castro Mendes Gomes", "authors": "Guilherme C. M. Gomes, S\\'ergio H. Nogueira, Vinicius F. dos Santos", "title": "Some results on Vertex Separator Reconfiguration", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first results on the complexity of the reconfiguration of\nvertex separators under the three most popular rules: token addition/removal,\ntoken jumping, and token sliding. We show that, aside from some trivially\nnegative instances, the first two rules are equivalent to each other and that,\neven if only on a subclass of bipartite graphs, TJ is not equivalent to the\nother two unless $\\mathsf{NP} = \\mathsf{PSPACE}$; we do this by showing a\nrelationship between separators and independent sets in this subclass of\nbipartite graphs. In terms of polynomial time algorithms, we show that every\nclass with a polynomially bounded number of minimal vertex separators admits an\nefficient algorithm under token jumping, then turn our attention to two classes\nthat do not meet this condition: $\\{3P_1, diamond\\}$-free and series-parallel\ngraphs. For the first, we describe a novel characterization, which we use to\nshow that reconfiguring vertex separators under token jumping is always\npossible and that, under token sliding, it can be done in polynomial time; for\nseries-parallel graphs, we also prove that reconfiguration is always possible\nunder TJ and exhibit a polynomial time algorithm to construct the\nreconfiguration sequence.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 21:41:12 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Gomes", "Guilherme C. M.", ""], ["Nogueira", "S\u00e9rgio H.", ""], ["Santos", "Vinicius F. dos", ""]]}, {"id": "2004.10898", "submitter": "Badrish Chandramouli", "authors": "Zongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke, Yinan\n  Li, Umar Farooq Minhas, Per-{\\AA}ke Larson, Donald Kossmann, Rajeev Acharya", "title": "Qd-tree: Learning Data Layouts for Big Data Analytics", "comments": "ACM SIGMOD 2020", "journal-ref": null, "doi": "10.1145/3318464.3389770", "report-no": null, "categories": "cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corporations today collect data at an unprecedented and accelerating scale,\nmaking the need to run queries on large datasets increasingly important.\nTechnologies such as columnar block-based data organization and compression\nhave become standard practice in most commercial database systems. However, the\nproblem of best assigning records to data blocks on storage is still open. For\nexample, today's systems usually partition data by arrival time into row\ngroups, or range/hash partition the data based on selected fields. For a given\nworkload, however, such techniques are unable to optimize for the important\nmetric of the number of blocks accessed by a query. This metric directly\nrelates to the I/O cost, and therefore performance, of most analytical queries.\nFurther, they are unable to exploit additional available storage to drive this\nmetric down further.\n  In this paper, we propose a new framework called a query-data routing tree,\nor qd-tree, to address this problem, and propose two algorithms for their\nconstruction based on greedy and deep reinforcement learning techniques.\nExperiments over benchmark and real workloads show that a qd-tree can provide\nphysical speedups of more than an order of magnitude compared to current\nblocking schemes, and can reach within 2X of the lower bound for data skipping\nbased on selectivity, while providing complete semantic descriptions of created\nblocks.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 23:42:59 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Yang", "Zongheng", ""], ["Chandramouli", "Badrish", ""], ["Wang", "Chi", ""], ["Gehrke", "Johannes", ""], ["Li", "Yinan", ""], ["Minhas", "Umar Farooq", ""], ["Larson", "Per-\u00c5ke", ""], ["Kossmann", "Donald", ""], ["Acharya", "Rajeev", ""]]}, {"id": "2004.10969", "submitter": "Samson Zhou", "authors": "Sepideh Mahabadi, Ilya Razenshteyn, David P. Woodruff, Samson Zhou", "title": "Non-Adaptive Adaptive Sampling on Turnstile Streams", "comments": "To appear at STOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive sampling is a useful algorithmic tool for data summarization\nproblems in the classical centralized setting, where the entire dataset is\navailable to the single processor performing the computation. Adaptive sampling\nrepeatedly selects rows of an underlying matrix\n$\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$, where $n\\gg d$, with probabilities\nproportional to their distances to the subspace of the previously selected\nrows. Intuitively, adaptive sampling seems to be limited to trivial multi-pass\nalgorithms in the streaming model of computation due to its inherently\nsequential nature of assigning sampling probabilities to each row only after\nthe previous iteration is completed. Surprisingly, we show this is not the case\nby giving the first one-pass algorithms for adaptive sampling on turnstile\nstreams and using space $\\text{poly}(d,k,\\log n)$, where $k$ is the number of\nadaptive sampling rounds to be performed.\n  Our adaptive sampling procedure has a number of applications to various data\nsummarization problems that either improve state-of-the-art or have only been\npreviously studied in the more relaxed row-arrival model. We give the first\nrelative-error algorithms for column subset selection, subspace approximation,\nprojective clustering, and volume maximization on turnstile streams that use\nspace sublinear in $n$. We complement our volume maximization algorithmic\nresults with lower bounds that are tight up to lower order terms, even for\nmulti-pass algorithms. By a similar construction, we also obtain lower bounds\nfor volume maximization in the row-arrival model, which we match with\ncompetitive upper bounds.\n  See paper for full abstract.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 05:00:21 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Mahabadi", "Sepideh", ""], ["Razenshteyn", "Ilya", ""], ["Woodruff", "David P.", ""], ["Zhou", "Samson", ""]]}, {"id": "2004.11146", "submitter": "Morgan Barbier", "authors": "Morgan Barbier, Hayat Cheballah, Jean-Marie Le Bars", "title": "On the computation of the M{\\\"o}bius transform", "comments": null, "journal-ref": "Theoretical Computer Science, Elsevier, 2019, Theoretical Computer\n  Science, 809, pp.171-188", "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The M{\\\"o}bius transform is a crucial transformation into the Boolean world;\nit allows to change the Boolean representation between the True Table and\nAlgebraic Normal Form. In this work, we introduce a new algebraic point of view\nof this transformation based on the polynomial form of Boolean functions. It\nappears that we can perform a new notion: the M{\\\"o}bius computation variable\nby variable and new computation properties. As a consequence, we propose new\nalgorithms which can produce a huge speed up of the M{\\\"o}bius computation for\nsub-families of Boolean function. Furthermore we compute directly the\nM{\\\"o}bius transformation of some particular Boolean functions. Finally, we\nshow that for some of them the Hamming weight is directly related to the\nalgebraic degree of specific factors.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 15:21:38 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Barbier", "Morgan", ""], ["Cheballah", "Hayat", ""], ["Bars", "Jean-Marie Le", ""]]}, {"id": "2004.11166", "submitter": "Yutaro Yamaguchi", "authors": "Yuya Masumura, Taihei Oki, Yutaro Yamaguchi", "title": "Dynamic Programming Approach to the Generalized Minimum Manhattan\n  Network Problem", "comments": "A preliminary version will appear in ISCO 2020; 32 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the generalized minimum Manhattan network (GMMN) problem: given a\nset $P$ of pairs of two points in the Euclidean plane $\\mathbb{R}^2$, we are\nrequired to find a minimum-length geometric network which consists of\naxis-aligned segments and contains a shortest path in the $L_1$ metric (a\nso-called Manhattan path) for each pair in $P$. This problem commonly\ngeneralizes several NP-hard network design problems that admit constant-factor\napproximation algorithms, such as the rectilinear Steiner arborescence (RSA)\nproblem, and it is open whether so does the GMMN problem.\n  As a bottom-up exploration, Schnizler (2015) focused on the intersection\ngraphs of the rectangles defined by the pairs in $P$, and gave a\npolynomial-time dynamic programming algorithm for the GMMN problem whose input\nis restricted so that both the treewidth and the maximum degree of its\nintersection graph are bounded by constants. In this paper, as the first\nattempt to remove the degree bound, we provide a polynomial-time algorithm for\nthe star case, and extend it to the general tree case based on an improved\ndynamic programming approach.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 13:57:22 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 01:03:48 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Masumura", "Yuya", ""], ["Oki", "Taihei", ""], ["Yamaguchi", "Yutaro", ""]]}, {"id": "2004.11173", "submitter": "Ignasi Sau", "authors": "Victor A. Campos, Guilherme C. M. Gomes, Allen Ibiapina, Raul Lopes,\n  Ignasi Sau, Ana Silva", "title": "Coloring Problems on Bipartite Graphs of Small Diameter", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a number of coloring problems restricted to bipartite graphs\nwith bounded diameter. First, we investigate the $k$-List Coloring, List\n$k$-Coloring, and $k$-Precoloring Extension problems on bipartite graphs with\ndiameter at most $d$, proving NP-completeness in most cases, and leaving open\nonly the List $3$-Coloring and $3$-Precoloring Extension problems when $d=3$.\n  Some of these results are obtained through a proof that the Surjective\n$C_6$-Homomorphism problem is NP-complete on bipartite graphs with diameter at\nmost four. Although the latter result has been already proved [Vikas, 2017], we\npresent ours as an alternative simpler one. As a byproduct, we also get that\n$3$-Biclique Partition is NP-complete. An attempt to prove this result was\npresented in [Fleischner, Mujuni, Paulusma, and Szeider, 2009], but there was a\nflaw in their proof, which we identify and discuss here.\n  Finally, we prove that the $3$-Fall Coloring problem is NP-complete on\nbipartite graphs with diameter at most four, and prove that NP-completeness for\ndiameter three would also imply NP-completeness of $3$-Precoloring Extension on\ndiameter three, thus closing the previously mentioned open cases. This would\nalso answer a question posed in [Kratochv\\'il, Tuza, and Voigt, 2002].\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 14:08:40 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 22:10:54 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Campos", "Victor A.", ""], ["Gomes", "Guilherme C. M.", ""], ["Ibiapina", "Allen", ""], ["Lopes", "Raul", ""], ["Sau", "Ignasi", ""], ["Silva", "Ana", ""]]}, {"id": "2004.11315", "submitter": "Christian Schulz", "authors": "Wolfgang Ost, Christian Schulz, Darren Strash", "title": "Engineering Data Reduction for Nested Dissection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications rely on time-intensive matrix operations, such as\nfactorization, which can be sped up significantly for large sparse matrices by\ninterpreting the matrix as a sparse graph and computing a node ordering that\nminimizes the so-called fill-in. In this paper, we engineer new data reduction\nrules for the minimum fill-in problem, which significantly reduce the size of\nthe graph while producing an equivalent (or near-equivalent) instance. By\napplying both new and existing data reduction rules exhaustively before nested\ndissection, we obtain improved quality and at the same time large improvements\nin running time on a variety of instances. Our overall algorithm outperforms\nthe state-of-the-art significantly: it not only yields better elimination\norders, but it does so significantly faster than previously possible. For\nexample, on road networks, where nested dissection algorithms are typically\nused as a preprocessing step for shortest path computations, our algorithms are\non average six times faster than Metis while computing orderings with less\nfill-in.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 16:51:55 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Ost", "Wolfgang", ""], ["Schulz", "Christian", ""], ["Strash", "Darren", ""]]}, {"id": "2004.11439", "submitter": "Kaushik Mondal", "authors": "Anisur Rahaman Molla, Kaushik Mondal, William K. Moses Jr", "title": "Efficient Dispersion on an Anonymous Ring in the Presence of Weak\n  Byzantine Robots", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of dispersion of mobile robots on a graph asks that $n$ robots\ninitially placed arbitrarily on the nodes of an $n$-node anonymous graph,\nautonomously move to reach a final configuration where exactly each node has at\nmost one robot on it. This problem is of significant interest due to its\nrelationship to other fundamental robot coordination problems, such as\nexploration, scattering, load balancing, relocation of self-driving electric\ncars to recharge stations, etc. The robots have unique IDs, typically in the\nrange $[1,poly(n)]$ and limited memory, whereas the graph is anonymous, i.e.,\nthe nodes do not have identifiers. The objective is to simultaneously minimize\ntwo performance metrics: (i) time to achieve dispersion and (ii) memory\nrequirement at each robot. This problem has been relatively well-studied when\nrobots are non-faulty.\n  In this paper, we introduce the notion of Byzantine faults to this problem,\ni.e., we formalize the problem of dispersion in the presence of up to $f$\nByzantine robots. We then study the problem on a ring while simultaneously\noptimizing the time complexity of algorithms and the memory requirement per\nrobot. Specifically, we design deterministic algorithms that attempt to match\nthe time lower bound ($\\Omega(n)$ rounds) and memory lower bound ($\\Omega(\\log\nn)$ bits per robot).\n  Our main result is a deterministic algorithm that is both time and memory\noptimal, i.e., $O(n)$ rounds and $O(\\log n)$ bits of memory required per robot,\nsubject to certain constraints. We subsequently provide results that require\nless assumptions but are either only time or memory optimal but not both. We\nalso provide a primitive, utilized often, that takes robots initially gathered\nat a node of the ring and disperses them in a time and memory optimal manner\nwithout additional assumptions required.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 19:51:50 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 09:43:02 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Molla", "Anisur Rahaman", ""], ["Mondal", "Kaushik", ""], ["Moses", "William K.", "Jr"]]}, {"id": "2004.11445", "submitter": "Mina Dalirrooyfard", "authors": "Mina Dalirrooyfard, Virginia Vassilevska Williams", "title": "Conditionally optimal approximation algorithms for the girth of a\n  directed graph", "comments": "To appear in ICALP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that a better than $2$-approximation algorithm for the girth in\ndense directed unweighted graphs needs $n^{3-o(1)}$ time unless one uses fast\nmatrix multiplication. Meanwhile, the best known approximation factor for a\ncombinatorial algorithm running in $O(mn^{1-\\epsilon})$ time (by Chechik et\nal.) is $3$. Is the true answer $2$ or $3$?\n  The main result of this paper is a (conditionally) tight approximation\nalgorithm for directed graphs. First, we show that under a popular hardness\nassumption, any algorithm, even one that exploits fast matrix multiplication,\nwould need to take at least $mn^{1-o(1)}$ time for some sparsity $m$ if it\nachieves a $(2-\\epsilon)$-approximation for any $\\epsilon>0$. Second we give a\n$2$-approximation algorithm for the girth of unweighted graphs running in\n$\\tilde{O}(mn^{3/4})$ time, and a $(2+\\epsilon)$-approximation algorithm (for\nany $\\epsilon>0$) that works in weighted graphs and runs in $\\tilde{O}(m\\sqrt\nn)$ time. Our algorithms are combinatorial.\n  We also obtain a $(4+\\epsilon)$-approximation of the girth running in\n$\\tilde{O}(mn^{\\sqrt{2}-1})$ time, improving upon the previous best\n$\\tilde{O}(m\\sqrt n)$ running time by Chechik et al. Finally, we consider the\ncomputation of roundtrip spanners. We obtain a $(5+\\epsilon)$-approximate\nroundtrip spanner on $\\tilde{O}(n^{1.5}/\\epsilon^2)$ edges in $\\tilde{O}(m\\sqrt\nn/\\epsilon^2)$ time. This improves upon the previous approximation factor\n$(8+\\epsilon)$ of Chechik et al. for the same running time.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 20:07:46 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 01:18:50 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Dalirrooyfard", "Mina", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "2004.11568", "submitter": "Ryan Mann", "authors": "Ryan L. Mann, Tyler Helmuth", "title": "Efficient Algorithms for Approximating Quantum Partition Functions", "comments": "7 pages, 0 figures, published version", "journal-ref": "Journal of Mathematical Physics 62, 022201 (2021)", "doi": "10.1063/5.0013689", "report-no": null, "categories": "cs.DS cs.CC math.CO quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a polynomial-time approximation algorithm for partition\nfunctions of quantum spin models at high temperature. Our algorithm is based on\nthe quantum cluster expansion of Neto\\v{c}n\\'y and Redig and the cluster\nexpansion approach to designing algorithms due to Helmuth, Perkins, and Regts.\nSimilar results have previously been obtained by related methods, and our main\ncontribution is a simple and slightly sharper analysis for the case of pairwise\ninteractions on bounded-degree graphs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 07:21:43 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 13:59:44 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Mann", "Ryan L.", ""], ["Helmuth", "Tyler", ""]]}, {"id": "2004.11621", "submitter": "Meirav Zehavi", "authors": "Fedor V. Fomin, Daniel Lokshtanov, Ivan Mihajlin, Saket Saurabh,\n  Meirav Zehavi", "title": "Computation of Hadwiger Number and Related Contraction Problems: Tight\n  Lower Bounds", "comments": "Accepted to ICALP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the Hadwiger number of an $n$-vertex graph $G$ (the maximum\nsize of a clique minor in $G$) cannot be computed in time $n^{o(n)}$, unless\nthe Exponential Time Hypothesis (ETH) fails. This resolves a well-known open\nquestion in the area of exact exponential algorithms. The technique developed\nfor resolving the Hadwiger number problem has a wider applicability. We use it\nto rule out the existence of $n^{o(n)}$-time algorithms (up to ETH) for a large\nclass of computational problems concerning edge contractions in graphs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 09:42:58 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Lokshtanov", "Daniel", ""], ["Mihajlin", "Ivan", ""], ["Saurabh", "Saket", ""], ["Zehavi", "Meirav", ""]]}, {"id": "2004.11642", "submitter": "Anindya De", "authors": "Anindya De, Elchanan Mossel and Joe Neeman", "title": "Robust testing of low-dimensional functions", "comments": "We significantly strengthen the results of the previous version. This\n  includes the first fully noise tolerant testers for linear juntas as well as\n  subclasses such as any function of constantly many halfspaces", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural problem in high-dimensional inference is to decide if a classifier\n$f:\\mathbb{R}^n \\rightarrow \\{-1,1\\}$ depends on a small number of linear\ndirections of its input data. Call a function $g: \\mathbb{R}^n \\rightarrow\n\\{-1,1\\}$, a linear $k$-junta if it is completely determined by some\n$k$-dimensional subspace of the input space. A recent work of the authors\nshowed that linear $k$-juntas are testable. Thus there exists an algorithm to\ndistinguish between: 1. $f: \\mathbb{R}^n \\rightarrow \\{-1,1\\}$ which is a\nlinear $k$-junta with surface area $s$, 2. $f$ is $\\epsilon$-far from any\nlinear $k$-junta with surface area $(1+\\epsilon)s$, where the query complexity\nof the algorithm is independent of the ambient dimension $n$.\n  Following the surge of interest in noise-tolerant property testing, in this\npaper we prove a noise-tolerant (or robust) version of this result. Namely, we\ngive an algorithm which given any $c>0$, $\\epsilon>0$, distinguishes between 1.\n$f: \\mathbb{R}^n \\rightarrow \\{-1,1\\}$ has correlation at least $c$ with some\nlinear $k$-junta with surface area $s$. 2. $f$ has correlation at most\n$c-\\epsilon$ with any linear $k$-junta with surface area at most $s$. The query\ncomplexity of our tester is $k^{\\mathsf{poly}(s/\\epsilon)}$.\n  Using our techniques, we also obtain a fully noise tolerant tester with the\nsame query complexity for any class $\\mathcal{C}$ of linear $k$-juntas with\nsurface area bounded by $s$. As a consequence, we obtain a fully noise tolerant\ntester with query complexity $k^{O(\\mathsf{poly}(\\log k/\\epsilon))}$ for the\nclass of intersection of $k$-halfspaces (for constant $k$) over the Gaussian\nspace. Our query complexity is independent of the ambient dimension $n$.\nPreviously, no non-trivial noise tolerant testers were known even for a single\nhalfspace.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 10:23:12 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 22:59:17 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["De", "Anindya", ""], ["Mossel", "Elchanan", ""], ["Neeman", "Joe", ""]]}, {"id": "2004.11666", "submitter": "Christian Schulz", "authors": "Monika Henzinger, Alexander Noe, Christian Schulz", "title": "Faster Parallel Multiterminal Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an improved branch-and-bound solver for the multiterminal cut\nproblem, based on the recent work of Henzinger et al.. We contribute new,\nhighly effective data reduction rules to transform the graph into a smaller\nequivalent instance. In addition, we present a local search algorithm that can\nsignificantly improve a given solution to the multiterminal cut problem. Our\nexact algorithm is able to give exact solutions to more and harder problems\ncompared to the state-of-the-art algorithm by Henzinger et al.; and give better\nsolutions for more than two third of the problems that are too large to be\nsolved to optimality. Additionally, we give an inexact heuristic algorithm that\ncomputes high-quality solutions for very hard instances in reasonable time.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 11:36:24 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Henzinger", "Monika", ""], ["Noe", "Alexander", ""], ["Schulz", "Christian", ""]]}, {"id": "2004.11731", "submitter": "Martijn Van Ee", "authors": "Martijn van Ee", "title": "A 12/7-approximation algorithm for the discrete Bamboo Garden Trimming\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the discrete Bamboo Garden Trimming problem (BGT), where we are\ngiven n bamboos with different growth rates. At the end of each day, one can\ncut down one bamboo to height zero. The goal in BGT is to make a perpetual\nschedule of cuts such that the height of the tallest bamboo ever is minimized.\nHere, we improve the current best approximation guarantee by designing a\n12/7-approximation algorithm. This result is based on a reduction to the\nPinwheel Scheduling problem. We show that a guarantee of 12/7 is essentially\nthe best we can hope for if our algorithm is based on this type of reduction.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 13:17:17 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["van Ee", "Martijn", ""]]}, {"id": "2004.11761", "submitter": "R.B. Sandeep", "authors": "D\\'aniel Marx, R. B. Sandeep", "title": "Incompressibility of H-free edge modification problems: Towards a\n  dichotomy", "comments": "56 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$ and an integer $k$, the $H$-free Edge Editing problem is to\nfind whether there exists at most $k$ pairs of vertices in $G$ such that\nchanging the adjacency of the pairs in $G$ results in a graph without any\ninduced copy of $H$. The existence of polynomial kernels for $H$-free Edge\nEditing received significant attention in the parameterized complexity\nliterature. Nontrivial polynomial kernels are known to exist for some graphs\n$H$ with at most 4 vertices, but starting from 5 vertices, polynomial kernels\nare known only if $H$ is either complete or empty. This suggests the conjecture\nthat there is no other $H$ with at least 5 vertices were $H$-free Edge Editing\nadmits a polynomial kernel. Towards this goal, we obtain a set $\\mathcal{H}$ of\nnine 5-vertex graphs such that if for every $H\\in\\mathcal{H}$, $H$-free Edge\nEditing is incompressible and the complexity assumption $NP \\not\\subseteq\ncoNP/poly$ holds, then $H$-free Edge Editing is incompressible for every graph\n$H$ with at least five vertices that is neither complete nor empty. That is,\nproving incompressibility for these nine graphs would give a complete\nclassification of the kernelization complexity of $H$-free Edge Editing for\nevery $H$ with at least 5 vertices.\n  We obtain similar result also for $H$-free Edge Deletion. Here the picture is\nmore complicated due to the existence of another infinite family of graphs $H$\nwhere the problem is trivial (graphs with exactly one edge). We obtain a larger\nset $\\mathcal{H}$ of nineteen graphs whose incompressibility would give a\ncomplete classification of the kernelization complexity of $H$-free Edge\nDeletion for every graph $H$ with at least 5 vertices. Analogous results follow\nalso for the $H$-free Edge Completion problem by simple complementation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 14:02:55 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 09:02:11 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Marx", "D\u00e1niel", ""], ["Sandeep", "R. B.", ""]]}, {"id": "2004.11860", "submitter": "Max Hahn-Klimroth", "authors": "Oliver Gebhard, Max Hahn-Klimroth, Olaf Parczyk, Manuel Penschuck,\n  Maurice Rolvien, Jonathan Scarlett, Nelvin Tan", "title": "Near optimal sparsity-constrained group testing: improved bounds and\n  algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in noiseless non-adaptive group testing have led to a precise\nasymptotic characterization of the number of tests required for\nhigh-probability recovery in the sublinear regime $k = n^{\\theta}$ (with\n$\\theta \\in (0,1)$), with $n$ individuals among which $k$ are infected.\nHowever, the required number of tests may increase substantially under\nreal-world practical constraints, notably including bounds on the maximum\nnumber $\\Delta$ of tests an individual can be placed in, or the maximum number\n$\\Gamma$ of individuals in a given test. While previous works have given\nrecovery guarantees for these settings, significant gaps remain between the\nachievability and converse bounds. In this paper, we substantially or\ncompletely close several of the most prominent gaps. In the case of\n$\\Delta$-divisible items, we show that the definite defectives (DD) algorithm\ncoupled with a random regular design is asymptotically optimal in dense scaling\nregimes, and optimal to within a factor of $e$ more generally; we establish\nthis by strengthening both the best known achievability and converse bounds. In\nthe case of $\\Gamma$-sized tests, we provide a comprehensive analysis of the\nregime $\\Gamma = \\Theta(1)$, and again establish a precise threshold proving\nthe asymptotic optimality of DD equipped with a tailored pooling scheme.\nFinally, for each of these two settings, we provide near-optimal adaptive\nalgorithms based on sequential splitting, and provably demonstrate gaps between\nthe performance of optimal adaptive and non-adaptive algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 17:10:23 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 10:52:50 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Gebhard", "Oliver", ""], ["Hahn-Klimroth", "Max", ""], ["Parczyk", "Olaf", ""], ["Penschuck", "Manuel", ""], ["Rolvien", "Maurice", ""], ["Scarlett", "Jonathan", ""], ["Tan", "Nelvin", ""]]}, {"id": "2004.11937", "submitter": "Dimitrios Thilikos", "authors": "Mamadou Moustapha Kant\\'e, Christophe Paul, Dimitrios M. Thilikos", "title": "A linear fixed parameter tractable algorithm for connected pathwidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph parameter of pathwidth can be seen as a measure of the topological\nresemblance of a graph to a path. A popular definition of pathwidth is given in\nterms of node search where we are given a system of tunnels that is\ncontaminated by some infectious substance and we are looking for a search\nstrategy that, at each step, either places a searcher on a vertex or removes a\nsearcher from a vertex and where an edge is cleaned when both endpoints are\nsimultaneously occupied by searchers. It was proved that the minimum number of\nsearchers required for a successful cleaning strategy is equal to the pathwidth\nof the graph plus one. Two desired characteristics for a cleaning strategy is\nto be monotone (no recontamination occurs) and connected (clean territories\nalways remain connected). Under these two demands, the number of searchers is\nequivalent to a variant of pathwidth called {\\em connected pathwidth}. We prove\nthat connected pathwidth is fixed parameter tractable, in particular we design\na $2^{O(k^2)}\\cdot n$ time algorithm that checks whether the connected\npathwidth of $G$ is at most $k.$ This resolves an open question by\n[Dereniowski, Osula, and Rz{\\k{a}}{\\.{z}}ewski, Finding small-width connected\npath-decompositions in polynomial time. Theor. Comput. Sci., 794:85-100, 2019].\nFor our algorithm, we enrich the typical sequence technique that is able to\ndeal with the connectivity demand. Typical sequences have been introduced in\n[Bodlaender and Kloks. Efficient and constructive algorithms for the pathwidth\nand treewidth of graphs. J. Algorithms, 21(2):358-402, 1996] for the design of\nlinear parameterized algorithms for treewidth and pathwidth. The proposed\nextension is based on an encoding of the connectivity property that is quite\nversatile and may be adapted so to deliver linear parameterized algorithms for\nthe connected variants of other width parameters as well.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 18:33:39 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Kant\u00e9", "Mamadou Moustapha", ""], ["Paul", "Christophe", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "2004.12002", "submitter": "Jay Mardia", "authors": "Jay Mardia, Hilal Asi, Kabir Aladin Chandrasekher", "title": "Finding Planted Cliques in Sublinear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the planted clique problem in which a clique of size $k$ is planted\nin an Erd\\H{o}s-R\\'enyi graph of size $n$ and one wants to recover this planted\nclique. For $k=\\Omega(\\sqrt{n})$, polynomial time algorithms can find the\nplanted clique. The fastest such algorithms run in time linear $O(n^2)$ (or\nnearly linear) in the size of the input [FR10,DGGP14,DM15a]. In this work, we\ninitiate the development of sublinear time algorithms that find the planted\nclique when $k=\\omega(\\sqrt{n \\log \\log n})$. Our algorithms can recover the\nclique in time\n$\\widetilde{O}\\left(n+(\\frac{n}{k})^{3}\\right)=\\widetilde{O}\\left(n^{\\frac{3}{2}}\\right)$\nwhen $k=\\Omega(\\sqrt{n\\log n})$, and in time\n$\\widetilde{O}\\left(n^2/\\exp{\\left(\\frac{k^2}{24n}\\right)}\\right)$ for\n$\\omega(\\sqrt{n\\log \\log n})=k=o(\\sqrt{n\\log{n}})$. An ${\\Omega}(n)$ running\ntime lower bound for the planted clique recovery problem follows easily from\nthe results of [RS19] and therefore our recovery algorithms are optimal\nwhenever $k = \\Omega(n^{\\frac{2}{3}})$. As the lower bound of [RS19] builds on\npurely information theoretic arguments, it cannot provide a detection lower\nbound stronger than $\\widetilde{\\Omega}(\\frac{n^2}{k^2})$. Since our algorithms\nfor $k = \\Omega(\\sqrt{n \\log n})$ run in time\n$\\widetilde{O}\\left(\\frac{n^3}{k^3} + n\\right)$, we show stronger lower bounds\nbased on computational hardness assumptions. With a slightly different notion\nof the planted clique problem we show that the Planted Clique Conjecture\nimplies the following. A natural family of non-adaptive algorithms---which\nincludes our algorithms for clique detection---cannot reliably solve the\nplanted clique detection problem in time $O\\left(\n\\frac{n^{3-\\delta}}{k^3}\\right)$ for any constant $\\delta>0$. Thus we provide\nevidence that if detecting small cliques is hard, it is also likely that\ndetecting large cliques is not \\textit{too} easy.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 21:44:16 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 22:54:45 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Mardia", "Jay", ""], ["Asi", "Hilal", ""], ["Chandrasekher", "Kabir Aladin", ""]]}, {"id": "2004.12063", "submitter": "Alexander Wein", "authors": "David Gamarnik, Aukosh Jagannath, Alexander S. Wein", "title": "Low-Degree Hardness of Random Optimization Problems", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math-ph math.MP math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding nearly optimal solutions of optimization\nproblems with random objective functions. Such problems arise widely in the\ntheory of random graphs, theoretical computer science, and statistical physics.\nTwo concrete problems we consider are (a) optimizing the Hamiltonian of a\nspherical or Ising p-spin glass model, and (b) finding a large independent set\nin a sparse Erdos-Renyi graph. Two families of algorithms are considered: (a)\nlow-degree polynomials of the input---a general framework that captures methods\nsuch as approximate message passing and local algorithms on sparse graphs,\namong others; and (b) the Langevin dynamics algorithm, a canonical Monte Carlo\nanalogue of the gradient descent algorithm (applicable only for the spherical\np-spin glass Hamiltonian).\n  We show that neither family of algorithms can produce nearly optimal\nsolutions with high probability. Our proof uses the fact that both models are\nknown to exhibit a variant of the overlap gap property (OGP) of near-optimal\nsolutions. Specifically, for both models, every two solutions whose objectives\nare above a certain threshold are either close or far from each other. The crux\nof our proof is the stability of both algorithms: a small perturbation of the\ninput induces a small perturbation of the output. By an interpolation argument,\nsuch a stable algorithm cannot overcome the OGP barrier.\n  The stability of the Langevin dynamics is an immediate consequence of the\nwell-posedness of stochastic differential equations. The stability of\nlow-degree polynomials is established using concepts from Gaussian and Boolean\nFourier analysis, including noise sensitivity, hypercontractivity, and total\ninfluence.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 05:45:59 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Gamarnik", "David", ""], ["Jagannath", "Aukosh", ""], ["Wein", "Alexander S.", ""]]}, {"id": "2004.12143", "submitter": "Blerina Sinaimeri", "authors": "Yishu Wang and Arnaud Mary and Marie-France Sagot and Blerina\n  Sinaimeri", "title": "A general framework for enumerating equivalence classes of solutions", "comments": "36 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a problem has more than one solution, it is often important, depending\non the underlying context, to enumerate (i.e., to list) them all. Even when the\nenumeration can be done in polynomial delay, that is, spending no more than\npolynomial time to go from one solution to the next, this can be costly as the\nnumber of solutions themselves may be huge, including sometimes exponential.\nFurthermore, depending on the application, many of these solutions can be\nconsidered equivalent. The problem of an efficient enumeration of the\nequivalence classes or of one representative per class (without generating all\nthe solutions), although identified as a need in many areas, has been addressed\nonly for very few specific cases. In this paper, we provide a general framework\nthat solves this problem in polynomial delay for a wide variety of contexts,\nincluding optimization ones that can be addressed by dynamic programming\nalgorithms, and for certain types of equivalence relations between solutions.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 13:30:59 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 18:47:14 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 10:29:41 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Wang", "Yishu", ""], ["Mary", "Arnaud", ""], ["Sagot", "Marie-France", ""], ["Sinaimeri", "Blerina", ""]]}, {"id": "2004.12166", "submitter": "\\'Edouard Bonnet", "authors": "\\'Edouard Bonnet, St\\'ephan Thomass\\'e, Xuan Thang Tran, R\\'emi\n  Watrigant", "title": "An algorithmic weakening of the Erd\\H{o}s-Hajnal conjecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the approximability of the Maximum Independent Set (MIS) problem in\n$H$-free graphs (that is, graphs which do not admit $H$ as an induced\nsubgraph). As one motivation we investigate the following conjecture: for every\nfixed graph $H$, there exists a constant $\\delta > 0$ such that MIS can be\n$n^{1 - \\delta}$-approximated in $H$-free graphs, where $n$ denotes the number\nof vertices of the input graph. We first prove that a constructive version of\nthe celebrated Erd\\H{o}s-Hajnal conjecture implies ours. We then prove that the\nset of graphs $H$ satisfying our conjecture is closed under the so-called graph\nsubstitution. This, together with the known polynomial-time algorithms for MIS\nin $H$-free graphs (e.g. $P_6$-free and fork-free graphs), implies that our\nconjecture holds for many graphs $H$ for which the Erd\\H{o}s-Hajnal conjecture\nis still open. We then focus on improving the constant $\\delta$ for some graph\nclasses: we prove that the classical Local Search algorithm provides an\n$OPT^{1-\\frac{1}{t}}$-approximation in $K_{t,t}$-free graphs (hence a\n$\\sqrt{OPT}$-approximation in $C_4$-free graphs), and, while there is a simple\n$\\sqrt{n}$-approximation in triangle-free graphs, it cannot be improved to\n$n^{\\frac{1}{4}-\\varepsilon}$ for any $\\varepsilon > 0$ unless $NP \\subseteq\nBPP$. More generally, we show that there is a constant $c$ such that MIS in\ngraphs of girth $\\gamma$ cannot be $n^{\\frac{c}{\\gamma}}$-approximated. Up to a\nconstant factor in the exponent, this matches the ratio of a known\napproximation algorithm by Monien and Speckenmeyer, and by Murphy. To the best\nof our knowledge, this is the first strong (i.e., $\\Omega(n^\\delta)$ for some\n$\\delta > 0$) inapproximability result for Maximum Independent Set in a proper\nhereditary class.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 15:11:59 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Thomass\u00e9", "St\u00e9phan", ""], ["Tran", "Xuan Thang", ""], ["Watrigant", "R\u00e9mi", ""]]}, {"id": "2004.12222", "submitter": "Robert Ganian", "authors": "Eduard Eiben, Robert Ganian, Thekla Hamm, Fabian Klute, Martin\n  N\\\"ollenburg", "title": "Extending Partial 1-Planar Drawings", "comments": "A shortened version of this article has been accepted for\n  presentation and publication at the 47th International Colloquium on\n  Automata, Languages and Programming (ICALP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic extension problems of partial graph representations such as\nplanar graph drawings or geometric intersection representations are of growing\ninterest in topological graph theory and graph drawing. In such an extension\nproblem, we are given a tuple $(G,H,\\mathcal{H})$ consisting of a graph $G$, a\nconnected subgraph $H$ of $G$ and a drawing $\\mathcal{H}$ of $H$, and the task\nis to extend $\\mathcal{H}$ into a drawing of $G$ while maintaining some desired\nproperty of the drawing, such as planarity.\n  In this paper we study the problem of extending partial 1-planar drawings,\nwhich are drawings in the plane that allow each edge to have at most one\ncrossing. In addition we consider the subclass of IC-planar drawings, which are\n1-planar drawings with independent crossings. Recognizing 1-planar graphs as\nwell as IC-planar graphs is \\NP-complete and the \\NP-completeness easily\ncarries over to the extension problem. Therefore, our focus lies on\nestablishing the tractability of such extension problems in a weaker sense than\npolynomial-time tractability. Here, we show that both problems are\nfixed-parameter tractable when parameterized by the number of edges missing\nfrom $H$, i.e., the edge deletion distance between $H$ and $G$. The second part\nof the paper then turns to a more powerful parameterization which is based on\nmeasuring the vertex+edge deletion distance between the partial and complete\ndrawing, i.e., the minimum number of vertices and edges that need to be deleted\nto obtain $H$ from $G$.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 19:54:54 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Eiben", "Eduard", ""], ["Ganian", "Robert", ""], ["Hamm", "Thekla", ""], ["Klute", "Fabian", ""], ["N\u00f6llenburg", "Martin", ""]]}, {"id": "2004.12223", "submitter": "Avah Banerjee", "authors": "Avah Banerjee, Guoli Ding", "title": "Online MinCut: Competitive and Regret Analysis", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the mincut problem in the online setting. We consider\ntwo distinct models: A) competitive analysis and B) regret analysis. In the\ncompetitive setting we consider the vertex arrival model; whenever a new vertex\narrives it's neighborhood with respect to the set of known vertices is\nrevealed. An online algorithm must make an irrevocable decision to determine\nthe side of the cut that the vertex must belong to in order to minimize the\nsize of the final cut. Various models are considered. 1) For classical and\nadvice models we give tight bounds on the competitive ratio of deterministic\nalgorithms. 2) Next we consider few semi-adversarial inputs: random order of\narrival with adversarially generated and sparse graphs. 3) Lastly we derive\nsome structural properties of \\mc-type problems with respect to greedy\nstrategies.\n  Finally we consider a non-stationary regret setting with a variational budget\n$V_T$ and give tights bounds on the regret function. Specifically, we show that\nif $V_T$ is sublinear in $T$ (number of rounds) then there is a deterministic\nalgorithm achieving a sublinear regret bound ($O(V_T)$). Further, this is\noptimal, even if randomization is allowed.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 19:57:35 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 15:58:40 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Banerjee", "Avah", ""], ["Ding", "Guoli", ""]]}, {"id": "2004.12224", "submitter": "Ariel Kulik", "authors": "Yaron Fairstein, Ariel Kulik, Joseph (Seffi) Naor, Danny Raz and Hadas\n  Shachnai", "title": "An Almost Optimal Approximation Algorithm for Monotone Submodular\n  Multiple Knapsack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of maximizing a monotone submodular function subject to\na Multiple Knapsack constraint. The input is a set $I$ of items, each has a\nnon-negative weight, and a set of bins of arbitrary capacities. Also, we are\ngiven a submodular, monotone and non-negative function $f$ over subsets of the\nitems. The objective is to find a packing of a subset of items $A \\subseteq I$\nin the bins such that $f(A)$ is maximized.\n  Our main result is an almost optimal polynomial time\n$(1-e^{-1}-\\varepsilon)$-approximation algorithm for the problem, for any\n$\\varepsilon>0$. The algorithm relies on a structuring technique which converts\na general multiple knapsack constraint to a constraint in which the bins are\npartitioned into groups of exponentially increasing cardinalities, each\nconsisting of bins of uniform capacity. We derive the result by combining\nstructuring with a refined analysis of techniques for submodular optimization\nsubject to knapsack constraints.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 19:58:00 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 10:09:30 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2021 13:36:08 GMT"}, {"version": "v4", "created": "Fri, 16 Apr 2021 13:26:26 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Fairstein", "Yaron", "", "Seffi"], ["Kulik", "Ariel", "", "Seffi"], ["Joseph", "", "", "Seffi"], ["Naor", "", ""], ["Raz", "Danny", ""], ["Shachnai", "Hadas", ""]]}, {"id": "2004.12258", "submitter": "Vadim Grinberg", "authors": "Uriel Feige, Vadim Grinberg", "title": "How to hide a clique?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the well known planted clique problem, a clique (or alternatively, an\nindependent set) of size $k$ is planted at random in an Erdos-Renyi random\n$G(n, p)$ graph, and the goal is to design an algorithm that finds the maximum\nclique (or independent set) in the resulting graph. We introduce a variation on\nthis problem, where instead of planting the clique at random, the clique is\nplanted by an adversary who attempts to make it difficult to find the maximum\nclique in the resulting graph. We show that for the standard setting of the\nparameters of the problem, namely, a clique of size $k = \\sqrt{n}$ planted in a\nrandom $G(n, \\frac{1}{2})$ graph, the known polynomial time algorithms can be\nextended (in a non-trivial way) to work also in the adversarial setting. In\ncontrast, we show that for other natural settings of the parameters, such as\nplanting an independent set of size $k=\\frac{n}{2}$ in a $G(n, p)$ graph with\n$p = n^{-\\frac{1}{2}}$, there is no polynomial time algorithm that finds an\nindependent set of size $k$, unless NP has randomized polynomial time\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 00:30:37 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 06:07:42 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Feige", "Uriel", ""], ["Grinberg", "Vadim", ""]]}, {"id": "2004.12424", "submitter": "Yajun Yang", "authors": "Yajun Yang, Hang Zhang, Hong Gao, Qinghua Hu, Xin Wang", "title": "An Efficient Index Method for the Optimal Route Query over Multi-Cost\n  Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart city has been consider the wave of the future and the route\nrecommendation in networks is a fundamental problem in it. Most existing\napproaches for the shortest route problem consider that there is only one kind\nof cost in networks. However, there always are several kinds of cost in\nnetworks and users prefer to select an optimal route under the global\nconsideration of these kinds of cost. In this paper, we study the problem of\nfinding the optimal route in the multi-cost networks. We prove this problem is\nNP-hard and the existing index techniques cannot be used to this problem. We\npropose a novel partition-based index with contour skyline techniques to find\nthe optimal route. We propose a vertex-filtering algorithm to facilitate the\nquery processing. We conduct extensive experiments on six real-life networks\nand the experimental results show that our method has an improvement in\nefficiency by an order of magnitude compared to the previous heuristic\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 16:04:53 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Yang", "Yajun", ""], ["Zhang", "Hang", ""], ["Gao", "Hong", ""], ["Hu", "Qinghua", ""], ["Wang", "Xin", ""]]}, {"id": "2004.12465", "submitter": "Mingmou Liu", "authors": "Mingmou Liu, Yitong Yin, Huacheng Yu", "title": "Succinct Filters for Sets of Unknown Sizes", "comments": "Full version of the paper accepted to ICALP 2020", "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2020.79", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The membership problem asks to maintain a set $S\\subseteq[u]$, supporting\ninsertions and membership queries, i.e., testing if a given element is in the\nset. A data structure that computes exact answers is called a dictionary. When\na (small) false positive rate $\\epsilon$ is allowed, the data structure is\ncalled a filter.\n  The space usages of the standard dictionaries or filters usually depend on\nthe upper bound on the size of $S$, while the actual set can be much smaller.\n  Pagh, Segev and Wieder (FOCS'13) were the first to study filters with varying\nspace usage based on the current $|S|$. They showed in order to match the space\nwith the current set size $n=|S|$, any filter data structure must use\n$(1-o(1))n(\\log(1/\\epsilon)+(1-O(\\epsilon))\\log\\log n)$ bits, in contrast to\nthe well-known lower bound of $N\\log(1/\\epsilon)$ bits, where $N$ is an upper\nbound on $|S|$. They also presented a data structure with almost optimal space\nof $(1+o(1))n(\\log(1/\\epsilon)+O(\\log\\log n))$ bits provided that\n$n>u^{0.001}$, with expected amortized constant insertion time and worst-case\nconstant lookup time.\n  In this work, we present a filter data structure with improvements in two\naspects:\n  - it has constant worst-case time for all insertions and lookups with high\nprobability;\n  - it uses space $(1+o(1))n(\\log (1/\\epsilon)+\\log\\log n)$ bits when\n$n>u^{0.001}$, achieving optimal leading constant for all $\\epsilon=o(1)$.\n  We also present a dictionary that uses $(1+o(1))n\\log(u/n)$ bits of space,\nmatching the optimal space in terms of the current size, and performs all\noperations in constant time with high probability.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 20:05:56 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Liu", "Mingmou", ""], ["Yin", "Yitong", ""], ["Yu", "Huacheng", ""]]}, {"id": "2004.12496", "submitter": "Amit Levi", "authors": "Xi Chen, Rajesh Jayaram, Amit Levi, Erik Waingarten", "title": "Learning and Testing Junta Distributions with Subcube Conditioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problems of learning and testing junta distributions on\n$\\{-1,1\\}^n$ with respect to the uniform distribution, where a distribution $p$\nis a $k$-junta if its probability mass function $p(x)$ depends on a subset of\nat most $k$ variables. The main contribution is an algorithm for finding\nrelevant coordinates in a $k$-junta distribution with subcube conditioning\n[BC18, CCKLW20]. We give two applications:\n  1. An algorithm for learning $k$-junta distributions with\n$\\tilde{O}(k/\\epsilon^2) \\log n + O(2^k/\\epsilon^2)$ subcube conditioning\nqueries, and\n  2. An algorithm for testing $k$-junta distributions with $\\tilde{O}((k +\n\\sqrt{n})/\\epsilon^2)$ subcube conditioning queries.\n  All our algorithms are optimal up to poly-logarithmic factors.\n  Our results show that subcube conditioning, as a natural model for accessing\nhigh-dimensional distributions, enables significant savings in learning and\ntesting junta distributions compared to the standard sampling model. This\naddresses an open question posed by Aliakbarpour, Blais, and Rubinfeld [ABR17].\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 22:52:53 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Chen", "Xi", ""], ["Jayaram", "Rajesh", ""], ["Levi", "Amit", ""], ["Waingarten", "Erik", ""]]}, {"id": "2004.12532", "submitter": "Alek Westover", "authors": "William Kuszmaul, Alek Westover", "title": "In-Place Parallel-Partition Algorithms using Exclusive-Read-and-Write\n  Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an in-place algorithm for the partition problem that has linear\nwork and polylogarithmic span. The algorithm uses only exclusive read/write\nshared variables, and can be implemented using parallel-for-loops without any\nadditional concurrency considerations (i.e., the algorithm is EREW). A key\nfeature of the algorithm is that it exhibits provably optimal cache behavior,\nup to small-order factors.\n  We also present a second in-place EREW algorithm for the partition problem\nthat has linear work and span $O(\\log n \\cdot \\log \\log n)$, which is within an\n$O(\\log\\log n)$ factor of the optimal span. By using this low-span algorithm as\na subroutine within the cache-friendly algorithm, we are able to obtain a\nsingle EREW algorithm that combines their theoretical guarantees: the algorithm\nachieves span $O(\\log n \\cdot \\log \\log n)$ and optimal cache behavior. As an\nimmediate consequence, we also get an in-place EREW quicksort algorithm with\nwork $O(n \\log n)$, span $O(\\log^2 n \\cdot \\log \\log n)$.\n  Whereas the standard EREW algorithm for parallel partitioning is\nmemory-bandwidth bound on large numbers of cores, our cache-friendly algorithm\nis able to achieve near-ideal scaling in practice by avoiding the\nmemory-bandwidth bottleneck. The algorithm's performance is comparable to that\nof the Blocked Strided Algorithm of Francis, Pannan, Frias, and Petit, which is\nthe previous state-of-the art for parallel EREW sorting algorithms, but which\nlacks theoretical guarantees on its span and cache behavior.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 01:19:11 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 20:17:39 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Kuszmaul", "William", ""], ["Westover", "Alek", ""]]}, {"id": "2004.12590", "submitter": "Dominik K\\\"oppl", "authors": "Dominik K\\\"oppl, Daiki Hashimoto, Diptarama Hendrian, and Ayumi\n  Shinohara", "title": "In-Place Bijective Burrows-Wheeler Transforms", "comments": "In proceedings of CPM 2020", "journal-ref": null, "doi": "10.4230/LIPIcs.CPM.2020.23", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the most well-known variants of the Burrows-Wheeler transform (BWT)\n[Burrows and Wheeler, 1994] is the bijective BWT (BBWT) [Gil and Scott, arXiv\n2012], which applies the extended BWT (EBWT) [Mantaci et al., TCS 2007] to the\nmultiset of Lyndon factors of a given text. Since the EBWT is invertible, the\nBBWT is a bijective transform in the sense that the inverse image of the EBWT\nrestores this multiset of Lyndon factors such that the original text can be\nobtained by sorting these factors in non-increasing order. In this paper, we\npresent algorithms constructing or inverting the BBWT in-place using quadratic\ntime. We also present conversions from the BBWT to the BWT, or vice versa,\neither (a) in-place using quadratic time, or (b) in the run-length compressed\nsetting using $O(n \\lg r / \\lg \\lg r)$ time with $O(r \\lg n)$ bits of words,\nwhere $r$ is the sum of character runs in the BWT and the BBWT.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 05:52:58 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["K\u00f6ppl", "Dominik", ""], ["Hashimoto", "Daiki", ""], ["Hendrian", "Diptarama", ""], ["Shinohara", "Ayumi", ""]]}, {"id": "2004.12633", "submitter": "Sayan Bandyapadhyay", "authors": "Sayan Bandyapadhyay", "title": "On Perturbation Resilience of Non-Uniform $k$-Center", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Non-Uniform $k$-center (NUkC) problem has recently been formulated by\nChakrabarty, Goyal and Krishnaswamy [ICALP, 2016] as a generalization of the\nclassical $k$-center clustering problem. In NUkC, given a set of $n$ points $P$\nin a metric space and non-negative numbers $r_1, r_2, \\ldots , r_k$, the goal\nis to find the minimum dilation $\\alpha$ and to choose $k$ balls centered at\nthe points of $P$ with radius $\\alpha\\cdot r_i$ for $1\\le i\\le k$, such that\nall points of $P$ are contained in the union of the chosen balls. They showed\nthat the problem is NP-hard to approximate within any factor even in tree\nmetrics. On the other hand, they designed a \"bi-criteria\" constant\napproximation algorithm that uses a constant times $k$ balls. Surprisingly, no\ntrue approximation is known even in the special case when the $r_i$'s belong to\na fixed set of size 3. In this paper, we study the NUkC problem under\nperturbation resilience, which was introduced by Bilu and Linial\n[Combinatorics, Probability and Computing, 2012]. We show that the problem\nunder 2-perturbation resilience is polynomial time solvable when the $r_i$'s\nbelong to a constant sized set. However, we show that perturbation resilience\ndoes not help in the general case. In particular, our findings imply that even\nwith perturbation resilience one cannot hope to find any \"good\" approximation\nfor the problem.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 08:23:01 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Bandyapadhyay", "Sayan", ""]]}, {"id": "2004.12646", "submitter": "Yi Li", "authors": "Yi Li and David Woodruff", "title": "Input-Sparsity Low Rank Approximation in Schatten Norm", "comments": "Submitted to ICML 2020. In this version, we state the result for\n  rectangular matrices in the abstract and introduction and give an example\n  comparing to low-rank approximation to A^TA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first input-sparsity time algorithms for the rank-$k$ low rank\napproximation problem in every Schatten norm. Specifically, for a given\n$n\\times n$ matrix $A$, our algorithm computes $Y,Z\\in \\mathbb{R}^{n\\times k}$,\nwhich, with high probability, satisfy $\\|A-YZ^T\\|_p \\leq\n(1+\\epsilon)\\|A-A_k\\|_p$, where $\\|M\\|_p = \\left (\\sum_{i=1}^n \\sigma_i(M)^p\n\\right )^{1/p}$ is the Schatten $p$-norm of a matrix $M$ with singular values\n$\\sigma_1(M), \\ldots, \\sigma_n(M)$, and where $A_k$ is the best rank-$k$\napproximation to $A$. Our algorithm runs in time\n$\\tilde{O}(\\operatorname{nnz}(A) +\nmn^{\\alpha_p}\\operatorname{poly}(k/\\epsilon))$, where $\\alpha_p = 0$ for $p\\in\n[1,2)$ and $\\alpha_p = (\\omega-1)(1-2/p)$ for $p>2$ and $\\omega \\approx 2.374$\nis the exponent of matrix multiplication. For the important case of $p = 1$,\nwhich corresponds to the more \"robust\" nuclear norm, we obtain\n$\\tilde{O}(\\operatorname{nnz}(A) + m \\cdot \\operatorname{poly}(k/\\epsilon))$\ntime, which was previously only known for the Frobenius norm ($p = 2$).\nMoreover, since $\\alpha_p < \\omega - 1$ for every $p$, our algorithm has a\nbetter dependence on $n$ than that in the singular value decomposition for\nevery $p$. Crucial to our analysis is the use of dimensionality reduction for\nKy-Fan $p$-norms.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 08:49:28 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 03:05:56 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 11:31:20 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Li", "Yi", ""], ["Woodruff", "David", ""]]}, {"id": "2004.12667", "submitter": "Paritosh Garg", "authors": "Paritosh Garg, Sagar Kale, Lars Rohwedder, Ola Svensson", "title": "Robust Algorithms under Adversarial Injections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study streaming and online algorithms in the context of\nrandomness in the input. For several problems, a random order of the input\nsequence---as opposed to the worst-case order---appears to be a necessary evil\nin order to prove satisfying guarantees. However, algorithmic techniques that\nwork under this assumption tend to be vulnerable to even small changes in the\ndistribution. For this reason, we propose a new \\emph{adversarial injections}\nmodel, in which the input is ordered randomly, but an adversary may inject\nmisleading elements at arbitrary positions. We believe that studying algorithms\nunder this much weaker assumption can lead to new insights and, in particular,\nmore robust algorithms. We investigate two classical combinatorial-optimization\nproblems in this model: Maximum matching and cardinality constrained monotone\nsubmodular function maximization. Our main technical contribution is a novel\nstreaming algorithm for the latter that computes a $0.55$-approximation. While\nthe algorithm itself is clean and simple, an involved analysis shows that it\nemulates a subdivision of the input stream which can be used to greatly limit\nthe power of the adversary.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 09:28:33 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Garg", "Paritosh", ""], ["Kale", "Sagar", ""], ["Rohwedder", "Lars", ""], ["Svensson", "Ola", ""]]}, {"id": "2004.12683", "submitter": "Astrid Pieterse", "authors": "Eva-Maria C. Hols, Stefan Kratsch, Astrid Pieterse", "title": "Approximate Turing Kernelization for Problems Parameterized by Treewidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the notion of lossy kernelization, introduced by Lokshtanov et al.\n[STOC 2017], to approximate Turing kernelization. An $\\alpha$-approximate\nTuring kernel for a parameterized optimization problem is a polynomial-time\nalgorithm that, when given access to an oracle that outputs $c$-approximate\nsolutions in $O(1)$ time, obtains an $(\\alpha \\cdot c)$-approximate solution to\nthe considered problem, using calls to the oracle of size at most $f(k)$ for\nsome function $f$ that only depends on the parameter.\n  Using this definition, we show that Independent Set parameterized by\ntreewidth $\\ell$ has a $(1+\\varepsilon)$-approximate Turing kernel with\n$O(\\frac{\\ell^2}{\\varepsilon})$ vertices, answering an open question posed by\nLokshtanov et al. [STOC 2017]. Furthermore, we give\n$(1+\\varepsilon)$-approximate Turing kernels for the following graph problems\nparameterized by treewidth: Vertex Cover, Edge Clique Cover, Edge-Disjoint\nTriangle Packing and Connected Vertex Cover.\n  We generalize the result for Independent Set and Vertex Cover, by showing\nthat all graph problems that we will call \"friendly\" admit\n$(1+\\varepsilon)$-approximate Turing kernels of polynomial size when\nparameterized by treewidth. We use this to obtain approximate Turing kernels\nfor Vertex-Disjoint $H$-packing for connected graphs $H$, Clique Cover,\nFeedback Vertex Set and Edge Dominating Set.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 10:00:50 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Hols", "Eva-Maria C.", ""], ["Kratsch", "Stefan", ""], ["Pieterse", "Astrid", ""]]}, {"id": "2004.12692", "submitter": "Dimitrios Thilikos", "authors": "Ignasi Sau, Giannos Stamoulis, Dimitrios M. Thilikos", "title": "k-apices of minor-closed graph classes. II. Parameterized algorithms", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let ${\\cal G}$ be a minor-closed graph class. We say that a graph $G$ is a\n$k$-apex of ${\\cal G}$ if $G$ contains a set $S$ of at most $k$ vertices such\nthat $G\\setminus S$ belongs to ${\\cal G}$. We denote by ${\\cal A}_k ({\\cal G})$\nthe set of all graphs that are $k$-apices of ${\\cal G}.$ In the first paper of\nthis series we obtained upper bounds on the size of the graphs in the\nminor-obstruction set of ${\\cal A}_k ({\\cal G})$, i.e., the minor-minimal set\nof graphs not belonging to ${\\cal A}_k ({\\cal G}).$ In this article we provide\nan algorithm that, given a graph $G$ on $n$ vertices, runs in $2^{{\\sf\npoly}(k)}\\cdot n^3$-time and either returns a set $S$ certifying that $G \\in\n{\\cal A}_k ({\\cal G})$, or reports that $G \\notin {\\cal A}_k ({\\cal G})$. Here\n${\\sf poly}$ is a polynomial function whose degree depends on the maximum size\nof a minor-obstruction of ${\\cal G}.$ In the special case where ${\\cal G}$\nexcludes some apex graph as a minor, we give an alternative algorithm running\nin $2^{{\\sf poly}(k)}\\cdot n^2$-time.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 10:26:00 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 10:26:19 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Sau", "Ignasi", ""], ["Stamoulis", "Giannos", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "2004.12865", "submitter": "Ignasi Sau", "authors": "Marin Bougeret, Bart M. P. Jansen, Ignasi Sau", "title": "Bridge-Depth Characterizes which Structural Parameterizations of Vertex\n  Cover Admit a Polynomial Kernel", "comments": "35 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the kernelization complexity of structural parameterizations of the\nVertex Cover problem. Here, the goal is to find a polynomial-time preprocessing\nalgorithm that can reduce any instance $(G,k)$ of the Vertex Cover problem to\nan equivalent one, whose size is polynomial in the size of a pre-determined\ncomplexity parameter of $G$. A long line of previous research deals with\nparameterizations based on the number of vertex deletions needed to reduce $G$\nto a member of a simple graph class $\\mathcal{F}$, such as forests, graphs of\nbounded tree-depth, and graphs of maximum degree two. We set out to find the\nmost general graph classes $\\mathcal{F}$ for which Vertex Cover parameterized\nby the vertex-deletion distance of the input graph to $\\mathcal{F}$, admits a\npolynomial kernelization. We give a complete characterization of the\nminor-closed graph families $\\mathcal{F}$ for which such a kernelization\nexists. We introduce a new graph parameter called bridge-depth, and prove that\na polynomial kernelization exists if and only if $\\mathcal{F}$ has bounded\nbridge-depth. The proof is based on an interesting connection between\nbridge-depth and the size of minimal blocking sets in graphs, which are vertex\nsets whose removal decreases the independence number.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 15:19:29 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Bougeret", "Marin", ""], ["Jansen", "Bart M. P.", ""], ["Sau", "Ignasi", ""]]}, {"id": "2004.12881", "submitter": "Shay Golan", "authors": "Shay Golan, Tomasz Kociumaka, Tsvi Kopelowitz, Ely Porat", "title": "The Streaming k-Mismatch Problem: Tradeoffs between Space and Total Time", "comments": "Extended abstract to appear in CPM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the $k$-mismatch problem in the streaming model on a pattern of\nlength $m$ and a streaming text of length $n$, both over a size-$\\sigma$\nalphabet. The current state-of-the-art algorithm for the streaming $k$-mismatch\nproblem, by Clifford et al. [SODA 2019], uses $\\tilde O(k)$ space and $\\tilde\nO\\big(\\sqrt k\\big)$ worst-case time per character. The space complexity is\nknown to be (unconditionally) optimal, and the worst-case time per character\nmatches a conditional lower bound. However, there is a gap between the total\ntime cost of the algorithm, which is $\\tilde O(n\\sqrt k)$, and the fastest\nknown offline algorithm, which costs $\\tilde O\\big(n + \\min\\big(\\frac{nk}{\\sqrt\nm},\\sigma n\\big)\\big)$ time. Moreover, it is not known whether improvements\nover the $\\tilde O(n\\sqrt k)$ total time are possible when using more than\n$O(k)$ space.\n  We address these gaps by designing a randomized streaming algorithm for the\n$k$-mismatch problem that, given an integer parameter $k\\le s \\le m$, uses\n$\\tilde O(s)$ space and costs $\\tilde O\\big(n+\\min\\big(\\frac\n{nk^2}m,\\frac{nk}{\\sqrt s},\\frac{\\sigma nm}s\\big)\\big)$ total time. For $s=m$,\nthe total runtime becomes $\\tilde O\\big(n + \\min\\big(\\frac{nk}{\\sqrt m},\\sigma\nn\\big)\\big)$, which matches the time cost of the fastest offline algorithm.\nMoreover, the worst-case time cost per character is still $\\tilde O\\big(\\sqrt\nk\\big)$.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 15:41:49 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Golan", "Shay", ""], ["Kociumaka", "Tomasz", ""], ["Kopelowitz", "Tsvi", ""], ["Porat", "Ely", ""]]}, {"id": "2004.12890", "submitter": "Diptarka Chakraborty", "authors": "Diptarka Chakraborty and Keerti Choudhary", "title": "New Extremal bounds for Reachability and Strong-Connectivity Preservers\n  under failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the question of computing sparse subgraphs for any\ninput directed graph $G=(V,E)$ on $n$ vertices and $m$ edges, that preserves\nreachability and/or strong connectivity structures.\n  We show $O(n+\\min\\{|{\\cal P}|\\sqrt{n},n\\sqrt{|{\\cal P}|}\\})$ bound on a\nsubgraph that is an $1$-fault-tolerant reachability preserver for a given\nvertex-pair set ${\\cal P}\\subseteq V\\times V$, i.e., it preserves reachability\nbetween any pair of vertices in ${\\cal P}$ under single edge (or vertex)\nfailure. Our result is a significant improvement over the previous best $O(n\n|{\\cal P}|)$ bound obtained as a corollary of single-source reachability\npreserver construction. We prove our upper bound by exploiting the special\nstructure of single fault-tolerant reachability preserver for any pair, and\nthen considering the interaction among such structures for different pairs.\n  In the lower bound side, we show that a 2-fault-tolerant reachability\npreserver for a vertex-pair set ${\\cal P}\\subseteq V\\times V$ of size\n$\\Omega(n^\\epsilon)$, for even any arbitrarily small $\\epsilon$, requires at\nleast $\\Omega(n^{1+\\epsilon/8})$ edges. This refutes the existence of\nlinear-sized dual fault-tolerant preservers for reachability for any polynomial\nsized vertex-pair set.\n  We also present the first sub-quadratic bound of at most $\\tilde{O}(k 2^k\nn^{2-1/k})$ size, for strong-connectivity preservers of directed graphs under\n$k$ failures. To the best of our knowledge no non-trivial bound for this\nproblem was known before, for a general $k$. We get our result by adopting the\ncolor-coding technique of Alon, Yuster, and Zwick [JACM'95].\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 15:51:45 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Chakraborty", "Diptarka", ""], ["Choudhary", "Keerti", ""]]}, {"id": "2004.12893", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne and Karl Wimmer", "title": "Testing Data Binnings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the question of data quantization and \"binning,\" we revisit the\nproblem of identity testing of discrete probability distributions. Identity\ntesting (a.k.a. one-sample testing), a fundamental and by now well-understood\nproblem in distribution testing, asks, given a reference distribution (model)\n$\\mathbf{q}$ and samples from an unknown distribution $\\mathbf{p}$, both over\n$[n]=\\{1,2,\\dots,n\\}$, whether $\\mathbf{p}$ equals $\\mathbf{q}$, or is\nsignificantly different from it.\n  In this paper, we introduce the related question of 'identity up to binning,'\nwhere the reference distribution $\\mathbf{q}$ is over $k \\ll n$ elements: the\nquestion is then whether there exists a suitable binning of the domain $[n]$\ninto $k$ intervals such that, once \"binned,\" $\\mathbf{p}$ is equal to\n$\\mathbf{q}$. We provide nearly tight upper and lower bounds on the sample\ncomplexity of this new question, showing both a quantitative and qualitative\ndifference with the vanilla identity testing one, and answering an open\nquestion of Canonne (2019). Finally, we discuss several extensions and related\nresearch directions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 15:54:25 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["Wimmer", "Karl", ""]]}, {"id": "2004.12990", "submitter": "Alfonso Cevallos", "authors": "Alfonso Cevallos, Alistair Stewart", "title": "A verifiably secure and proportional committee election rule", "comments": "26 pages. This is an update version of a paper titled \"Validator\n  Selection in Nominated Proof of Stake\"; this version provides further results\n  on the verifiability of the guarantees of the winning committee", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The property of proportional representation in approval-based committee\nelections has appeared in the social choice literature for over a century, and\nis typically understood as avoiding the underrepresentation of minorities.\nHowever, we argue that the security of some distributed systems is directly\nlinked to the opposite goal of avoiding the overrepresentation of any minority,\na goal which leads us to an optimization objective known as Maximin Support,\nclosely related to the axiom of Proportional Justified Representation (PJR). We\nprovide an inapproximability result for this objective, and propose a new\nelection rule inspired in Phragmen's methods that achieves a) a constant-factor\napproximation guarantee for the objective, and b) the PJR property.\nFurthermore, a structural property allows one to quickly verify that the\nwinning committee satisfies the two aforementioned properties, even if the\nalgorithm is executed by an untrusted party who only communicates the output.\nFinally, we present an efficient post-computation that, when paired with any\napproximation algorithm for Maximin Support, returns a new solution that a)\npreserves the approximation guarantee, b) satisfies PJR, and c) can be\nefficiently verified to satisfy PJR.\n  Our work is motivated by an application on blockchains that implement\nNominated Proof-of-Stake (NPoS), where the community must elect a committee of\nvalidators to participate in its consensus protocol, and where fighting\noverrepresentation protects the system against attacks by an adversarial\nminority. Our election rule enables a validator election protocol with formal\nand verifiable guarantees on security and proportionality. We propose a\nspecific protocol that can be successfully implemented in spite of the\nstringent time constraints of a blockchain architecture, and that will be the\nbasis for an implementation in the Polkadot network, launched in 2020.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 17:53:10 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 13:16:07 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Cevallos", "Alfonso", ""], ["Stewart", "Alistair", ""]]}, {"id": "2004.13018", "submitter": "Nima Anari", "authors": "Nima Anari and Thuy-Duong Vuong", "title": "An Extension of Pl\\\"ucker Relations with Applications to Subdeterminant\n  Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a matrix $A$ and $k\\geq 0$, we study the problem of finding the\n$k\\times k$ submatrix of $A$ with the maximum determinant in absolute value.\nThis problem is motivated by the question of computing the determinant-based\nlower bound of [LSV86] on hereditary discrepancy, which was later shown to be\nan approximate upper bound as well [Mat13]. The special case where $k$\ncoincides with one of the dimensions of $A$ has been extensively studied.\n[Nik15] gave a $2^{O(k)}$-approximation algorithm for this special case,\nmatching known lower bounds; he also raised as an open problem the question of\ndesigning approximation algorithms for the general case.\n  We make progress towards answering this question by giving the first\nefficient approximation algorithm for general $k\\times k$ subdeterminant\nmaximization with an approximation ratio that depends only on $k$. Our\nalgorithm finds a $k^{O(k)}$-approximate solution by performing a simple local\nsearch. Our main technical contribution, enabling the analysis of the\napproximation ratio, is an extension of Pl\\\"ucker relations for the\nGrassmannian, which may be of independent interest; Pl\\\"ucker relations are\nquadratic polynomial equations involving the set of $k\\times k$ subdeterminants\nof a $k\\times n$ matrix. We find an extension of these relations to $k\\times k$\nsubdeterminants of general $m\\times n$ matrices.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 19:59:24 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 05:42:04 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Anari", "Nima", ""], ["Vuong", "Thuy-Duong", ""]]}, {"id": "2004.13086", "submitter": "Andrzej Lingas", "authors": "Andrzej Lingas and Mia Persson", "title": "Computing the Boolean product of two n\\times n Boolean matrices using\n  O(n^2) mechanical operation", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of determining the Boolean product of two n\\times n\nBoolean matrices in an unconventional computational model allowing for\nmechanical operations. We show that O(n^2) operations are sufficient to compute\nthe product in this model.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 18:41:14 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 12:13:25 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Lingas", "Andrzej", ""], ["Persson", "Mia", ""]]}, {"id": "2004.13197", "submitter": "Mayank Goswami", "authors": "Michael A. Bender, Mayank Goswami, Dzejla Mededovic, Pablo Montes,\n  Kostas Tsichlas", "title": "Batched Predecessor and Sorting with Size-Priced Information in External\n  Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the unit-cost comparison model, a black box takes an input two items and\noutputs the result of the comparison. Problems like sorting and searching have\nbeen studied in this model, and it has been generalized to include the concept\nof priced information, where different pairs of items (say database records)\nhave different comparison costs. These comparison costs can be arbitrary (in\nwhich case no algorithm can be close to optimal (Charikar et al. STOC 2000)),\nstructured (for example, the comparison cost may depend on the length of the\ndatabases (Gupta et al. FOCS 2001)), or stochastic (Angelov et al. LATIN 2008).\nMotivated by the database setting where the cost depends on the sizes of the\nitems, we consider the problems of sorting and batched predecessor where two\nnon-uniform sets of items $A$ and $B$ are given as input.\n  (1) In the RAM setting, we consider the scenario where both sets have $n$\nkeys each. The cost to compare two items in $A$ is $a$, to compare an item of\n$A$ to an item of $B$ is $b$, and to compare two items in $B$ is $c$. We give\nupper and lower bounds for the case $a \\le b \\le c$. Notice that the case $b=1,\na=c=\\infty$ is the famous ``nuts and bolts'' problem.\n  (2) In the Disk-Access Model (DAM), where transferring elements between disk\nand internal memory is the main bottleneck, we consider the scenario where\nelements in $B$ are larger than elements in $A$. The larger items take more\nI/Os to be brought into memory, consume more space in internal memory, and are\nrequired in their entirety for comparisons.\n  We first give output-sensitive lower and upper bounds on the batched\npredecessor problem, and use these to derive bounds on the complexity of\nsorting in the two models. Our bounds are tight in most cases, and require\nnovel generalizations of the classical lower bound techniques in external\nmemory to accommodate the non-uniformity of keys.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 22:17:19 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Bender", "Michael A.", ""], ["Goswami", "Mayank", ""], ["Mededovic", "Dzejla", ""], ["Montes", "Pablo", ""], ["Tsichlas", "Kostas", ""]]}, {"id": "2004.13202", "submitter": "Bohan Fan", "authors": "Bohan Fan, Diego Ihara Centurion, Neshat Mohammadi, Francesco Sgherzi,\n  Anastasios Sidiropoulos, Mina Valizadeh", "title": "Learning Lines with Ordinal Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding a mapping $f$ from a set of points into the\nreal line, under ordinal triple constraints. An ordinal constraint for a triple\nof points $(u,v,w)$ asserts that $|f(u)-f(v)|<|f(u)-f(w)|$. We present an\napproximation algorithm for the dense case of this problem. Given an instance\nthat admits a solution that satisfies $(1-\\varepsilon)$-fraction of all\nconstraints, our algorithm computes a solution that satisfies\n$(1-O(\\varepsilon^{1/8}))$-fraction of all constraints, in time $O(n^7) +\n(1/\\varepsilon)^{O(1/\\varepsilon^{1/8})} n$.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 22:45:04 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 20:18:26 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Fan", "Bohan", ""], ["Centurion", "Diego Ihara", ""], ["Mohammadi", "Neshat", ""], ["Sgherzi", "Francesco", ""], ["Sidiropoulos", "Anastasios", ""], ["Valizadeh", "Mina", ""]]}, {"id": "2004.13312", "submitter": "Ilya Sergey", "authors": "Kiran Gopinathan and Ilya Sergey", "title": "Certifying Certainty and Uncertainty in Approximate Membership Query\n  Structures -- Extended Version", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Approximate Membership Query structures (AMQs) rely on randomisation for\ntime- and space-efficiency, while introducing a possibility of false positive\nand false negative answers. Correctness proofs of such structures involve\nsubtle reasoning about bounds on probabilities of getting certain outcomes.\nBecause of these subtleties, a number of unsound arguments in such proofs have\nbeen made over the years. In this work, we address the challenge of building\nrigorous and reusable computer-assisted proofs about probabilistic\nspecifications of AMQs. We describe the framework for systematic decomposition\nof AMQs and their properties into a series of interfaces and reusable\ncomponents. We implement our framework as a library in the Coq proof assistant\nand showcase it by encoding in it a number of non-trivial AMQs, such as Bloom\nfilters, counting filters, quotient filters and blocked constructions, and\nmechanising the proofs of their probabilistic specifications. We demonstrate\nhow AMQs encoded in our framework guarantee the absence of false negatives by\nconstruction. We also show how the proofs about probabilities of false\npositives for complex AMQs can be obtained by means of verified reduction to\nthe implementations of their simpler counterparts. Finally, we provide a\nlibrary of domain-specific theorems and tactics that allow a high degree of\nautomation in probabilistic proofs.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 05:48:09 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Gopinathan", "Kiran", ""], ["Sergey", "Ilya", ""]]}, {"id": "2004.13389", "submitter": "Tatiana Starikovskaya", "authors": "Garance Gourdel, Tomasz Kociumaka, Jakub Radoszewski, Tatiana\n  Starikovskaya", "title": "Approximating longest common substring with $k$ mismatches: Theory and\n  practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the problem of the longest common substring with $k$ mismatches we are\ngiven two strings $X, Y$ and must find the maximal length $\\ell$ such that\nthere is a length-$\\ell$ substring of $X$ and a length-$\\ell$ substring of $Y$\nthat differ in at most $k$ positions. The length $\\ell$ can be used as a robust\nmeasure of similarity between $X, Y$. In this work, we develop new\napproximation algorithms for computing $\\ell$ that are significantly more\nefficient that previously known solutions from the theoretical point of view.\nOur approach is simple and practical, which we confirm via an experimental\nevaluation, and is probably close to optimal as we demonstrate via a\nconditional lower bound.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 09:40:13 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Gourdel", "Garance", ""], ["Kociumaka", "Tomasz", ""], ["Radoszewski", "Jakub", ""], ["Starikovskaya", "Tatiana", ""]]}, {"id": "2004.13442", "submitter": "James Stewart", "authors": "Andreas Galanis, Leslie Ann Goldberg, James Stewart", "title": "Fast algorithms for general spin systems on bipartite expanders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A spin system is a framework in which the vertices of a graph are assigned\nspins from a finite set. The interactions between neighbouring spins give rise\nto weights, so a spin assignment can also be viewed as a weighted graph\nhomomorphism. The problem of approximating the partition function (the\naggregate weight of spin assignments) or of sampling from the resulting\nprobability distribution is typically intractable for general graphs.\n  In this work, we consider arbitrary spin systems on bipartite expander\n$\\Delta$-regular graphs, including the canonical class of bipartite random\n$\\Delta$-regular graphs. We develop fast approximate sampling and counting\nalgorithms for general spin systems whenever the degree and the spectral gap of\nthe graph are sufficiently large. Our approach generalises the techniques of\nJenseen et al. and Chen et al. by showing that typical configurations on\nbipartite expanders correspond to \"bicliques\" of the spin system; then, using\nsuitable polymer models, we show how to sample such configurations and\napproximate the partition function in $\\tilde{O}(n^2)$ time, where $n$ is the\nsize of the graph.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 11:48:17 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 19:39:45 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 16:06:53 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Galanis", "Andreas", ""], ["Goldberg", "Leslie Ann", ""], ["Stewart", "James", ""]]}, {"id": "2004.13491", "submitter": "Malte Renken", "authors": "Till Fluschnik, Hendrik Molter, Rolf Niedermeier, Malte Renken,\n  Philipp Zschoche", "title": "As Time Goes By: Reflections on Treewidth for Temporal Graphs", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-42071-0_6", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treewidth is arguably the most important structural graph parameter leading\nto algorithmically beneficial graph decompositions. Triggered by a strongly\ngrowing interest in temporal networks (graphs where edge sets change over\ntime), we discuss fresh algorithmic views on temporal tree decompositions and\ntemporal treewidth. We review and explain some of the recent work together with\nsome encountered pitfalls, and we point out challenges for future research.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 13:26:15 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Fluschnik", "Till", ""], ["Molter", "Hendrik", ""], ["Niedermeier", "Rolf", ""], ["Renken", "Malte", ""], ["Zschoche", "Philipp", ""]]}, {"id": "2004.13673", "submitter": "Ofer Magen", "authors": "Shiri Chechik and Ofer Magen", "title": "Near Optimal Algorithm for the Directed Single Source Replacement Paths\n  Problem", "comments": "38 pages, 9 figures, to appear in the Proceedings of the 47th\n  International Colloquium on Automata, Languages and Programming (ICALP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Single Source Replacement Paths (SSRP) problem we are given a graph $G\n= (V, E)$, and a shortest paths tree $\\widehat{K}$ rooted at a node $s$, and\nthe goal is to output for every node $t \\in V$ and for every edge $e$ in\n$\\widehat{K}$ the length of the shortest path from $s$ to $t$ avoiding $e$.\n  We present an $\\tilde{O}(m\\sqrt{n} + n^2)$ time randomized combinatorial\nalgorithm for unweighted directed graphs. Previously such a bound was known in\nthe directed case only for the seemingly easier problem of replacement path\nwhere both the source and the target nodes are fixed.\n  Our new upper bound for this problem matches the existing conditional\ncombinatorial lower bounds. Hence, (assuming these conditional lower bounds)\nour result is essentially optimal and completes the picture of the SSRP problem\nin the combinatorial setting.\n  Our algorithm extends to the case of small, rational edge weights. We\nstrengthen the existing conditional lower bounds in this case by showing that\nany $O(mn^{1/2-\\epsilon})$ time (combinatorial or algebraic) algorithm for some\nfixed $\\epsilon >0$ yields a truly subcubic algorithm for the weighted All\nPairs Shortest Paths problem (previously such a bound was known only for the\ncombinatorial setting).\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 17:20:55 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Chechik", "Shiri", ""], ["Magen", "Ofer", ""]]}, {"id": "2004.13748", "submitter": "Sitan Chen", "authors": "Sitan Chen, Raghu Meka", "title": "Learning Polynomials of Few Relevant Dimensions", "comments": "64 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial regression is a basic primitive in learning and statistics. In its\nmost basic form the goal is to fit a degree $d$ polynomial to a response\nvariable $y$ in terms of an $n$-dimensional input vector $x$. This is extremely\nwell-studied with many applications and has sample and runtime complexity\n$\\Theta(n^d)$. Can one achieve better runtime if the intrinsic dimension of the\ndata is much smaller than the ambient dimension $n$? Concretely, we are given\nsamples $(x,y)$ where $y$ is a degree at most $d$ polynomial in an unknown\n$r$-dimensional projection (the relevant dimensions) of $x$. This can be seen\nboth as a generalization of phase retrieval and as a special case of learning\nmulti-index models where the link function is an unknown low-degree polynomial.\nNote that without distributional assumptions, this is at least as hard as junta\nlearning.\n  In this work we consider the important case where the covariates are\nGaussian. We give an algorithm that learns the polynomial within accuracy\n$\\epsilon$ with sample complexity that is roughly $N = O_{r,d}(n\n\\log^2(1/\\epsilon) (\\log n)^d)$ and runtime $O_{r,d}(N n^2)$. Prior to our\nwork, no such results were known even for the case of $r=1$. We introduce a new\nfiltered PCA approach to get a warm start for the true subspace and use\ngeodesic SGD to boost to arbitrary accuracy; our techniques may be of\nindependent interest, especially for problems dealing with subspace recovery or\nanalyzing SGD on manifolds.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 18:00:18 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Chen", "Sitan", ""], ["Meka", "Raghu", ""]]}, {"id": "2004.13891", "submitter": "Jakub Tarnawski", "authors": "Janardhan Kulkarni, Shi Li, Jakub Tarnawski, Minwei Ye", "title": "Hierarchy-Based Algorithms for Minimizing Makespan under Precedence and\n  Communication Constraints", "comments": null, "journal-ref": "Proc. of ACM-SIAM Symposium on Discrete Algorithms (SODA), 2020,\n  pages 2770-2789", "doi": "10.1137/1.9781611975994.169", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classic problem of scheduling jobs with precedence\nconstraints on a set of identical machines to minimize the makespan objective\nfunction. Understanding the exact approximability of the problem when the\nnumber of machines is a constant is a well-known question in scheduling theory.\nIndeed, an outstanding open problem from the classic book of Garey and Johnson\nasks whether this problem is NP-hard even in the case of 3 machines and\nunit-length jobs. In a recent breakthrough, Levey and Rothvoss gave a\n$(1+\\epsilon)$-approximation algorithm, which runs in nearly quasi-polynomial\ntime, for the case when job have unit lengths. However, a substantially more\ndifficult case where jobs have arbitrary processing lengths has remained open.\n  We make progress on this more general problem. We show that there exists a\n$(1+\\epsilon)$-approximation algorithm (with similar running time as that of\nLevey and Rothvoss) for the non-migratory setting: when every job has to be\nscheduled entirely on a single machine, but within a machine the job need not\nbe scheduled during consecutive time steps. Further, we also show that our\nalgorithmic framework generalizes to another classic scenario where, along with\nthe precedence constraints, the jobs also have communication delay constraints.\nBoth of these fundamental problems are highly relevant to the practice of\ndatacenter scheduling.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 23:28:59 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Kulkarni", "Janardhan", ""], ["Li", "Shi", ""], ["Tarnawski", "Jakub", ""], ["Ye", "Minwei", ""]]}, {"id": "2004.13933", "submitter": "Neeldhara Misra", "authors": "Chinmay Sonar, Palash Dey, Neeldhara Misra", "title": "On the complexity of Winner Verification and Candidate Winner for\n  Multiwinner Voting Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Chamberlin-Courant and Monroe rules are fundamental and well-studied\nrules in the literature of multi-winner elections. The problem of determining\nif there exists a committee of size k that has a Chamberlin-Courant\n(respectively, Monroe) score of at most r is known to be NP-complete. We\nconsider the following natural problems in this setting: a) given a committee S\nof size k as input, is it an optimal k-sized committee, and b) given a\ncandidate c and a committee size k, does there exist an optimal k-sized\ncommittee that contains c?\n  In this work, we resolve the complexity of both problems for the\nChamberlin-Courant and Monroe voting rules in the settings of rankings as well\nas approval ballots. We show that verifying if a given committee is optimal is\ncoNP-complete whilst the latter problem is complete for $\\Theta_{2}^{P}$. We\nalso demonstrate efficient algorithms for the second problem when the input\nconsists of single-peaked rankings. Our contribution fills an essential gap in\nthe literature for these important multi-winner rules.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 02:53:24 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Sonar", "Chinmay", ""], ["Dey", "Palash", ""], ["Misra", "Neeldhara", ""]]}, {"id": "2004.13978", "submitter": "Yash Khanna", "authors": "Yash Khanna, Anand Louis", "title": "Planted Models for the Densest $k$-Subgraph Problem", "comments": "31 pages. To appear in FSTTCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph $G$, the Densest $k$-subgraph problem (DkS) asks to\ncompute a set $S \\subset V$ of cardinality $\\left\\lvert S\\right\\rvert \\leq k$\nsuch that the weight of edges inside $S$ is maximized. This is a fundamental\nNP-hard problem whose approximability, inspite of many decades of research, is\nyet to be settled. The current best known approximation algorithm due to\nBhaskara et al. (2010) computes a $\\mathcal{O}\\left({n^{1/4 +\n\\epsilon}}\\right)$ approximation in time\n$n^{\\mathcal{O}\\left(1/\\epsilon\\right)}$, for any $\\epsilon > 0$.\n  We ask what are some \"easier\" instances of this problem? We propose some\nnatural semi-random models of instances with a planted dense subgraph, and\nstudy approximation algorithms for computing the densest subgraph in them.\nThese models are inspired by the semi-random models of instances studied for\nvarious other graph problems such as the independent set problem, graph\npartitioning problems etc. For a large range of parameters of these models, we\nget significantly better approximation factors for the Densest $k$-subgraph\nproblem. Moreover, our algorithm recovers a large part of the planted solution.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 06:38:37 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 15:28:26 GMT"}, {"version": "v3", "created": "Sat, 7 Nov 2020 15:39:14 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Khanna", "Yash", ""], ["Louis", "Anand", ""]]}, {"id": "2004.14064", "submitter": "Andrzej Lingas", "authors": "Miros{\\l}aw Kowaluk and Andrzej Lingas", "title": "Quantum and approximation algorithms for maximum witnesses of Boolean\n  matrix products", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding maximum (or minimum) witnesses of the Boolean product\nof two Boolean matrices (MW for short) has a number of important applications,\nin particular the all-pairs lowest common ancestor (LCA) problem in directed\nacyclic graphs (dags). The best known upper time-bound on the MW problem for\nn\\times n Boolean matrices of the form O(n^{2.575}) has not been substantially\nimproved since 2006. In order to obtain faster algorithms for this problem, we\nstudy quantum algorithms for MW and approximation algorithms for MW (in the\nstandard computational model). Some of our quantum algorithms are input or\noutput sensitive. Our fastest quantum algorithm for the MW problem, and\nconsequently for the related problems, runs in time\n\\tilde{O}(n^{2+\\lambda/2})=\\tilde{O}(n^{2.434}), where \\lambda satisfies the\nequation \\omega(1, \\lambda, 1) = 1 + 1.5 \\, \\lambda and \\omega(1, \\lambda, 1)\nis the exponent of the multiplication of an n \\times n^{\\lambda}$ matrix by an\nn^{\\lambda} \\times n matrix. Next, we consider a relaxed version of the MW\nproblem (in the standard model) asking for reporting a witness of bounded rank\n(the maximum witness has rank 1) for each non-zero entry of the matrix product.\nFirst, by adapting the fastest known algorithm for maximum witnesses, we obtain\nan algorithm for the relaxed problem that reports for each non-zero entry of\nthe product matrix a witness of rank at most \\ell in time\n\\tilde{O}((n/\\ell)n^{\\omega(1,\\log_n \\ell,1)}). Then, by reducing the relaxed\nproblem to the so called k-witness problem, we provide an algorithm that\nreports for each non-zero entry C[i,j] of the product matrix C a witness of\nrank O(\\lceil W_C(i,j)/k\\rceil ), where W_C(i,j) is the number of witnesses for\nC[i,j], with high probability. The algorithm runs in\n\\tilde{O}(n^{\\omega}k^{0.4653} +n^2k) time, where \\omega=\\omega(1,1,1).\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 10:34:54 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 20:13:54 GMT"}, {"version": "v3", "created": "Sun, 30 May 2021 20:15:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kowaluk", "Miros\u0142aw", ""], ["Lingas", "Andrzej", ""]]}, {"id": "2004.14102", "submitter": "Mateusz Lewandowski", "authors": "Marek Karpinski, Mateusz Lewandowski, Syed Mohammad Meesum, Matthias\n  Mnich", "title": "Dense Steiner problems: Approximation algorithms and inapproximability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Steiner Tree problem is a classical problem in combinatorial\noptimization: the goal is to connect a set $T$ of terminals in a graph $G$ by a\ntree of minimum size. Karpinski and Zelikovsky (1996) studied the\n$\\delta$-dense version of {\\sc Steiner Tree}, where each terminal has at least\n$\\delta |V(G)\\setminus T|$ neighbours outside $T$, for a fixed $\\delta > 0$.\nThey gave a PTAS for this problem.\n  We study a generalization of pairwise $\\delta$-dense {\\sc Steiner Forest},\nwhich asks for a minimum-size forest in $G$ in which the nodes in each terminal\nset $T_1,\\dots,T_k$ are connected, and every terminal in $T_i$ has at least\n$\\delta |T_j|$ neighbours in $T_j$, and at least $\\delta|S|$ nodes in $S =\nV(G)\\setminus (T_1\\cup\\dots\\cup T_k)$, for each $i, j$ in $\\{1,\\dots, k\\}$ with\n$i\\neq j$. Our first result is a polynomial-time approximation scheme for all\n$\\delta > 1/2$. Then, we show a $(\\frac{13}{12}+\\varepsilon)$-approximation\nalgorithm for $\\delta = 1/2$ and any $\\varepsilon > 0$. We also consider the\n$\\delta$-dense Group Steiner Tree problem as defined by Hauptmann and show that\nthe problem is $\\mathsf{APX}$-hard.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 11:53:55 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Karpinski", "Marek", ""], ["Lewandowski", "Mateusz", ""], ["Meesum", "Syed Mohammad", ""], ["Mnich", "Matthias", ""]]}, {"id": "2004.14304", "submitter": "Calum MacRury", "authors": "Allan Borodin, Calum MacRury, Akash Rakheja", "title": "Bipartite Stochastic Matching: Online, Random Order, and I.I.D. Models", "comments": "We corrected an error regarding a previous claim involving the\n  non-committal benchmark, and also added in some previously omitted proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the context of stochastic probing with commitment, we consider the\nonline stochastic matching problem; that is, the one sided online bipartite\nmatching problem where edges adjacent to an online node must be probed to\ndetermine if they exist, based on known edge probabilities. If a probed edge\nexists, it must be used in the matching (if possible). We study this problem in\nthe generality of a patience (or budget) constraint which limits the number of\nprobes that can be made to edges adjacent to an online node. The patience\nconstraint results in modelling and computational efficiency issues that are\nnot encountered in the special cases of unit patience and full (i.e.,\nunlimited) patience. The stochastic matching problem leads to a variety of\nsettings. Our main contribution is to provide a new LP relaxation and a unified\napproach for establishing new and improved competitive bounds in three\ndifferent input model settings (namely, adversarial, random order, and known\ni.i.d.). In all these settings, the algorithm does not have any control on the\nordering of the online nodes. We establish competitive bounds in these\nsettings, all of which generalize the standard non-stochastic setting when\nedges do not need to be probed (i.e., exist with certainty). All of our\ncompetitive ratios hold for arbitrary edge probabilities and patience\nconstraints:\n  (1) A $1-1/e$ ratio when the stochastic graph is known, offline vertices are\nweighted and online arrivals are adversarial.\n  (2) A $1-1/e$ ratio when the stochastic graph is known, edges are weighted,\nand online arrivals are given in random order (i.e., in ROM, the random order\nmodel).\n  (3) A $1-1/e$ ratio when online arrivals are drawn i.i.d. from a known\nstochastic type graph and edges are weighted.\n  (4) A (tight) $1/e$ ratio when the stochastic graph is unknown, edges are\nweighted and online arrivals are given in random order.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 16:24:59 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 17:01:50 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 15:38:00 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Borodin", "Allan", ""], ["MacRury", "Calum", ""], ["Rakheja", "Akash", ""]]}, {"id": "2004.14351", "submitter": "Konstantin Gorbunov", "authors": "K. Yu. Gorbunov and V. A. Lyubetsky", "title": "An Almost Exact Linear Complexity Algorithm of the Shortest\n  Transformation of Chain-Cycle Graphs", "comments": "16 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A \"genome structure\" is a labeled directed graph with vertices of degree 1 or\n2. A set of operations over such graphs is fixed, and each of the operations\nhas a certain cost, a strictly positive number. The transformation problem\nconsists in the following: for given structures a and b and given costs, find a\nminimum total cost sequence of operations transforming a into b (\"the shortest\ntransformation of a into b\"). Each operation corresponds to an \"event\", the\nlatter being a change in the graph caused by executing one of the operations\nover it. The possibility of assigning different costs is important in\napplications, since it allows to distinguish between frequent and rare events.\nApparently, arbitrary costs make the problem NP-hard, which results in\nnontriviality of passing from one restriction on costs to another, if the\nproblem is solved by a linear or at least polynomial algorithm (assuming that P\nis not equal NP). We propose a novel linear time and space algorithm which\nconstructs a sequence of operations transforming a to b with total cost close\nor equal to the absolute minimum. Namely, if all the so-called DCJ operations\nhave the same cost w and if deletions and insertions have costs either both\nlarger or both less than w, then the algorithm outputs a transformation of a\ninto b with the total cost differing from the absolute minimum by at most 2w\n(in the former case) or equal to it (in the latter case). In some cases, the\nalgorithm outputs an exact solution, e.g., in the case of circular genome\nstructures. The condition on the costs of deletions and insertions can be\nomitted (although the proof described below does not include this general\ncase).\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 17:28:58 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Gorbunov", "K. Yu.", ""], ["Lyubetsky", "V. A.", ""]]}, {"id": "2004.14473", "submitter": "Thibaut Vidal", "authors": "Thibaut Vidal, Rafael Martinelli, Tuan Anh Pham, Minh Ho\\`ang H\\`a", "title": "Arc Routing with Time-Dependent Travel Times and Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle routing algorithms usually reformulate the road network into a\ncomplete graph in which each arc represents the shortest path between two\nlocations. Studies on time-dependent routing followed this model and therefore\ndefined the speed functions on the complete graph. We argue that this model is\noften inadequate, in particular for arc routing problems involving services on\nedges of a road network. To fill this gap, we formally define the\ntime-dependent capacitated arc routing problem (TDCARP), with travel and\nservice speed functions given directly at the network level. Under these\nassumptions, the quickest path between locations can change over time, leading\nto a complex problem that challenges the capabilities of current solution\nmethods. We introduce effective algorithms for preprocessing quickest paths in\na closed form, efficient data structures for travel time queries during routing\noptimization, as well as heuristic and exact solution approaches for the\nTDCARP. Our heuristic uses the hybrid genetic search principle with tailored\nsolution-decoding algorithms and lower bounds for filtering moves. Our\nbranch-and-price algorithm exploits dedicated pricing routines, heuristic\ndominance rules and completion bounds to find optimal solutions for problem\ncounting up to 75 services. Based on these algorithms, we measure the benefits\nof time-dependent routing optimization for different levels of travel-speed\ndata accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 21:01:16 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Vidal", "Thibaut", ""], ["Martinelli", "Rafael", ""], ["Pham", "Tuan Anh", ""], ["H\u00e0", "Minh Ho\u00e0ng", ""]]}, {"id": "2004.14574", "submitter": "Gorka Kobeaga", "authors": "Gorka Kobeaga, Mar\\'ia Merino, Jose A. Lozano", "title": "On Solving Cycle Problems with Branch-and-Cut: Extending Shrinking and\n  Exact Subcycle Elimination Separation Algorithms", "comments": "23 pages + 3 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend techniques developed in the context of the\nTravelling Salesperson Problem for cycle problems. Particularly, we study the\nshrinking of support graphs and the exact algorithms for subcycle elimination\nseparation problems. The efficient application of the considered techniques has\nproved to be essential in the Travelling Salesperson Problem when solving large\nsize problems by Branch-and-Cut, and this has been the motivation behind this\nwork. Regarding the shrinking of support graphs, we prove the validity of the\nPadberg-Rinaldi general shrinking rules and the Crowder-Padberg subcycle-safe\nshrinking rules. Concerning the subcycle separation problems, we extend two\nexact separation algorithms, the Dynamic Hong and the Extended\nPadberg-Gr\\\"otschel algorithms, which are shown to be superior to the ones used\nso far in the literature of cycle problems.\n  The proposed techniques are empirically tested in 24 subcycle elimination\nproblem instances generated by solving the Orienteering Problem (involving up\nto 15112 vertices) with Branch-and-Cut. The experiments suggest the relevance\nof the proposed techniques for cycle problems. The obtained average speedup for\nthe subcycle separation problems in the Orienteering Problem when the proposed\ntechniques are used together is around 50 times in medium-sized instances and\naround 250 times in large-sized instances.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 03:52:49 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 14:50:22 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2020 17:27:54 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Kobeaga", "Gorka", ""], ["Merino", "Mar\u00eda", ""], ["Lozano", "Jose A.", ""]]}, {"id": "2004.14632", "submitter": "Benjamin Aram Berendsohn", "authors": "Benjamin Aram Berendsohn and L\\'aszl\\'o Kozma", "title": "Geometric group testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Group testing is concerned with identifying $t$ defective items in a set of\n$m$ items, where each test reports whether a specific subset of items contains\nat least one defective. In non-adaptive group testing, the subsets to be tested\nare fixed in advance. By testing multiple items at once, the required number of\ntests can be made much smaller than $m$. In fact, for $t \\in \\mathcal{O}(1)$,\nthe optimal number of (non-adaptive) tests is known to be $\\Theta(\\log{m})$.\n  In this paper, we consider the problem of non-adaptive group testing in a\ngeometric setting, where the items are points in $d$-dimensional Euclidean\nspace and the tests are axis-parallel boxes (hyperrectangles). We present upper\nand lower bounds on the required number of tests under this geometric\nconstraint. In contrast to the general, combinatorial case, the bounds in our\ngeometric setting are polynomial in $m$. For instance, our results imply that\nidentifying a defective pair in a set of $m$ points in the plane always\nrequires $\\Omega(m^{3/5})$ tests, and there exist configurations of $m$ points\nfor which $\\mathcal{O}(m^{2/3})$ tests are sufficient, whereas to identify a\nsingle defective point in the plane, $\\Theta(m^{1/2})$ tests are always\nnecessary and sometimes sufficient.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 08:17:29 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 13:40:16 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 13:43:23 GMT"}, {"version": "v4", "created": "Wed, 2 Dec 2020 12:14:41 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Berendsohn", "Benjamin Aram", ""], ["Kozma", "L\u00e1szl\u00f3", ""]]}, {"id": "2004.14650", "submitter": "Richard Santiago", "authors": "Richard Santiago and Yuichi Yoshida", "title": "Weakly Submodular Function Maximization Using Local Submodularity Ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weak submodularity is a natural relaxation of the diminishing return\nproperty, which is equivalent to submodularity. Weak submodularity has been\nused to show that many (monotone) functions that arise in practice can be\nefficiently maximized with provable guarantees. In this work we introduce two\nnatural generalizations of weak submodularity for non-monotone functions. We\nshow that an efficient randomized greedy algorithm has provable approximation\nguarantees for maximizing these functions subject to a cardinality constraint.\nWe then provide a more refined analysis that takes into account that the weak\nsubmodularity parameter may change (sometimes improving) throughout the\nexecution of the algorithm. This leads to improved approximation guarantees in\nsome settings. We provide applications of our results for monotone and\nnon-monotone maximization problems.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 09:15:33 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 16:34:17 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Santiago", "Richard", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "2004.14692", "submitter": "S. Akshay", "authors": "Kuldeep S. Meel and S. Akshay", "title": "Sparse Hashing for Scalable Approximate Model Counting: Theory and\n  Practice", "comments": "Full version of paper accepted in LICS2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a CNF formula F on n variables, the problem of model counting or #SAT\nis to compute the number of satisfying assignments of F . Model counting is a\nfundamental but hard problem in computer science with varied applications.\nRecent years have witnessed a surge of effort towards developing efficient\nalgorithmic techniques that combine the classical 2-universal hashing with the\nremarkable progress in SAT solving over the past decade. These techniques\naugment the CNF formula F with random XOR constraints and invoke an NP oracle\nrepeatedly on the resultant CNF-XOR formulas. In practice, calls to the NP\noracle calls are replaced a SAT solver whose runtime performance is adversely\naffected by size of XOR constraints. The standard construction of 2-universal\nhash functions chooses every variable with probability p = 1/2 leading to XOR\nconstraints of size n/2 in expectation. Consequently, the challenge is to\ndesign sparse hash functions where variables can be chosen with smaller\nprobability and lead to smaller sized XOR constraints.\n  In this paper, we address this challenge from theoretical and practical\nperspectives. First, we formalize a relaxation of universal hashing, called\nconcentrated hashing and establish a novel and beautiful connection between\nconcentration measures of these hash functions and isoperimetric inequalities\non boolean hypercubes. This allows us to obtain (log m) tight bounds on\nvariance and dispersion index and show that p = O( log(m)/m ) suffices for\ndesign of sparse hash functions from {0, 1}^n to {0, 1}^m. We then use sparse\nhash functions belonging to this concentrated hash family to develop new\napproximate counting algorithms. A comprehensive experimental evaluation of our\nalgorithm on 1893 benchmarks demonstrates that usage of sparse hash functions\ncan lead to significant speedups.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 11:17:26 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Meel", "Kuldeep S.", ""], ["Akshay", "S.", ""]]}, {"id": "2004.14715", "submitter": "Christoph Durr", "authors": "Christoph D\\\"urr (RO), Shahin Kamali", "title": "Randomized Two-Valued Bounded Delay Online Buffer Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the bounded delay buffer management problem unit size packets arrive\nonline to be sent over a network link. The objective is to maximize the total\nweight of packets sent before their deadline. In this paper we are interested\nin the two-valued variant of the problem, where every packet has either low (1)\nor high priority weight ($\\alpha$ > 1). We show that its randomized competitive\nratio against an oblivious adversary is 1 + ($\\alpha$ -- 1)/($\\alpha$ 2 +\n$\\alpha$).\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 12:12:48 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 09:10:14 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["D\u00fcrr", "Christoph", "", "RO"], ["Kamali", "Shahin", ""]]}, {"id": "2004.14724", "submitter": "Niels Gr\\\"uttemeier", "authors": "Niels Gr\\\"uttemeier and Christian Komusiewicz", "title": "Learning Bayesian Networks Under Sparsity Constraints: A Parameterized\n  Complexity Analysis", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning the structure of an optimal Bayesian network\n$D$ when additional constraints are posed on the DAG $D$ or on its moralized\ngraph. More precisely, we consider the constraint that the moralized graph can\nbe transformed to a graph from a sparse graph class $\\Pi$ by at most $k$ vertex\ndeletions. We show that for $\\Pi$ being the graphs with maximum degree $1$, an\noptimal network can be computed in polynomial time when $k$ is constant,\nextending previous work that gave an algorithm with such a running time for\n$\\Pi$ being the class of edgeless graphs [Korhonen & Parviainen, NIPS 2015]. We\nthen show that further extensions or improvements are presumably impossible.\nFor example, we show that when $\\Pi$ is the set of graphs with maximum degree\n$2$ or when $\\Pi$ is the set of graphs in which each component has size at most\nthree, then learning an optimal network is NP-hard even if $k=0$. Finally, we\nshow that learning an optimal network with at most $k$ edges in the moralized\ngraph presumably has no $f(k)\\cdot |I|^{\\mathcal{O}(1)}$-time algorithm and\nthat, in contrast, an optimal network with at most $k$ arcs in the DAG $D$ can\nbe computed in $2^{\\mathcal{O}(k)}\\cdot\n  |I|^{\\mathcal{O}(1)}$ time where $|I|$ is the total input size.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 12:31:13 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 08:41:55 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Gr\u00fcttemeier", "Niels", ""], ["Komusiewicz", "Christian", ""]]}, {"id": "2004.14789", "submitter": "\\'Edouard Bonnet", "authors": "\\'Edouard Bonnet, Eun Jung Kim, St\\'ephan Thomass\\'e, R\\'emi Watrigant", "title": "Twin-width I: tractable FO model checking", "comments": "48 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by a width invariant defined on permutations by Guillemot and Marx\n[SODA '14], we introduce the notion of twin-width on graphs and on matrices.\nProper minor-closed classes, bounded rank-width graphs, map graphs, $K_t$-free\nunit $d$-dimensional ball graphs, posets with antichains of bounded size, and\nproper subclasses of dimension-2 posets all have bounded twin-width. On all\nthese classes (except map graphs without geometric embedding) we show how to\ncompute in polynomial time a sequence of $d$-contractions, witness that the\ntwin-width is at most $d$. We show that FO model checking, that is deciding if\na given first-order formula $\\phi$ evaluates to true for a given binary\nstructure $G$ on a domain $D$, is FPT in $|\\phi|$ on classes of bounded\ntwin-width, provided the witness is given. More precisely, being given a\n$d$-contraction sequence for $G$, our algorithm runs in time $f(d,|\\phi|) \\cdot\n|D|$ where $f$ is a computable but non-elementary function. We also prove that\nbounded twin-width is preserved by FO interpretations and transductions\n(allowing operations such as squaring or complementing a graph). This unifies\nand significantly extends the knowledge on fixed-parameter tractability of FO\nmodel checking on non-monotone classes, such as the FPT algorithm on\nbounded-width posets by Gajarsk\\'y et al. [FOCS '15].\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:05:41 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 13:47:30 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Kim", "Eun Jung", ""], ["Thomass\u00e9", "St\u00e9phan", ""], ["Watrigant", "R\u00e9mi", ""]]}, {"id": "2004.14891", "submitter": "Sagar Kale", "authors": "Monika Henzinger and Sagar Kale", "title": "Fully-Dynamic Coresets", "comments": "Added missed important reference. Abstract is shortened", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With input sizes becoming massive, coresets -- small yet representative\nsummary of the input -- are relevant more than ever. A weighted set $C_w$ that\nis a subset of the input is an $\\varepsilon$-coreset if the cost of any\nfeasible solution $S$ with respect to $C_w$ is within $[1 {\\pm} \\varepsilon]$\nof the cost of $S$ with respect to the original input. We give a very general\ntechnique to compute coresets in the fully-dynamic setting where input points\ncan be added or deleted. Given a static $\\varepsilon$-coreset algorithm that\nruns in time $t(n, \\varepsilon, \\lambda)$ and computes a coreset of size $s(n,\n\\varepsilon, \\lambda)$, where $n$ is the number of input points and $1\n{-}\\lambda$ is the success probability, we give a fully-dynamic algorithm that\ncomputes an $\\varepsilon$-coreset with worst-case update time $O((\\log n) \\cdot\nt(s(n, \\varepsilon/\\log n, \\lambda/n), \\varepsilon/\\log n, \\lambda/n) )$ (this\nbound is stated informally), where the success probability is $1{-}\\lambda$.\nOur technique is a fully-dynamic analog of the merge-and-reduce technique that\napplies to insertion-only setting. Although our space usage is $O(n)$, we work\nin the presence of an adaptive adversary, and we show that $\\Omega(n)$ space is\nrequired when adversary is adaptive. As a consequence, we get fully-dynamic\n$\\varepsilon$-coreset algorithms for $k$-median and $k$-means with worst-case\nupdate time $O(\\varepsilon^{-2}k^2\\log^5 n \\log^3 k)$ and coreset size\n$O(\\varepsilon^{-2}k\\log n \\log^2 k)$ ignoring $\\log \\log n$ and\n$\\log(1/\\varepsilon)$ factors and assuming that $\\varepsilon, \\lambda =\n\\Omega(1/$poly$(n))$. These are the first fully-dynamic algorithms for\n$k$-median and $k$-means with worst-case update times $O($poly$(k, \\log n,\n\\varepsilon^{-1}))$. We also give conditional lower bound on update/query time\nfor any fully-dynamic $(4 - \\delta)$-approximation algorithm for $k$-means.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:45:21 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 13:46:13 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 10:47:29 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Henzinger", "Monika", ""], ["Kale", "Sagar", ""]]}, {"id": "2004.14995", "submitter": "Hao Zheng", "authors": "Hao Zheng, Andrew Price, Chris Myers", "title": "Using Decision Diagrams to Compactly Represent the State Space for\n  Explicit Model Checking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The enormous number of states reachable during explicit model checking is the\nmain bottleneck for scalability. This paper presents approaches of using\ndecision diagrams to represent very large state space compactly and\nefficiently. This is possible for asynchronous systems as two system states\nconnected by a transition often share many same local portions. Using decision\ndiagrams can significantly reduce memory demand by not using memory to store\nthe redundant information among different states. This paper considers\nmulti-value decision diagrams for this purpose. Additionally, a technique to\nreduce the runtime overhead of using these diagrams is also described.\nExperimental results and comparison with the state compression method as\nimplemented in the model checker SPIN show that the approaches presented in\nthis paper are memory efficient for storing large state space with acceptable\nruntime overhead.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:37:56 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Zheng", "Hao", ""], ["Price", "Andrew", ""], ["Myers", "Chris", ""]]}]