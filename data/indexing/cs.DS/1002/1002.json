[{"id": "1002.0012", "submitter": "Jacques Carette", "authors": "Jacques Carette, James H. Davenport", "title": "The Power of Vocabulary: The Case of Cyclotomic Polynomials", "comments": "7 pages. Submitted to ISSAC 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DS cs.MS math.AC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe that the vocabulary used to construct the \"answer\" to problems in\ncomputer algebra can have a dramatic effect on the computational complexity of\nsolving that problem. We recall a formalization of this observation and explain\nthe classic example of sparse polynomial arithmetic. For this case, we show\nthat it is possible to extend the vocabulary so as reap the benefits of\nconciseness whilst avoiding the obvious pitfall of repeating the problem\nstatement as the \"solution\".\n  It is possible to extend the vocabulary either by irreducible cyclotomics or\nby $x^n-1$: we look at the options and suggest that the pragmatist might opt\nfor both.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2010 21:23:05 GMT"}], "update_date": "2010-02-02", "authors_parsed": [["Carette", "Jacques", ""], ["Davenport", "James H.", ""]]}, {"id": "1002.0046", "submitter": "Yann Ponty", "authors": "Olivier Bodini (LIP6), Yann Ponty (LIX)", "title": "Multi-dimensional Boltzmann Sampling of Languages", "comments": "12pp", "journal-ref": "AOFA'10, Vienne : Autriche (2010)", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the uniform random generation of words from a\ncontext-free language (over an alphabet of size $k$), while constraining every\nletter to a targeted frequency of occurrence. Our approach consists in a\nmultidimensional extension of Boltzmann samplers \\cite{Duchon2004}. We show\nthat, under mostly \\emph{strong-connectivity} hypotheses, our samplers return a\nword of size in $[(1-\\varepsilon)n, (1+\\varepsilon)n]$ and exact frequency in\n$\\mathcal{O}(n^{1+k/2})$ expected time. Moreover, if we accept tolerance\nintervals of width in $\\Omega(\\sqrt{n})$ for the number of occurrences of each\nletters, our samplers perform an approximate-size generation of words in\nexpected $\\mathcal{O}(n)$ time. We illustrate these techniques on the\ngeneration of Tetris tessellations with uniform statistics in the different\ntypes of tetraminoes.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2010 06:24:13 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2010 05:38:44 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2010 06:59:29 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["Bodini", "Olivier", "", "LIP6"], ["Ponty", "Yann", "", "LIX"]]}, {"id": "1002.0286", "submitter": "Gregory Gutin", "authors": "R. Crowston, G. Gutin, M. Jones, E.J. Kim, I.Z. Ruzsa", "title": "Systems of Linear Equations over $\\mathbb{F}_2$ and Problems\n  Parameterized Above Average", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-13731-0_17", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the problem Max Lin, we are given a system $Az=b$ of $m$ linear equations\nwith $n$ variables over $\\mathbb{F}_2$ in which each equation is assigned a\npositive weight and we wish to find an assignment of values to the variables\nthat maximizes the excess, which is the total weight of satisfied equations\nminus the total weight of falsified equations. Using an algebraic approach, we\nobtain a lower bound for the maximum excess.\n  Max Lin Above Average (Max Lin AA) is a parameterized version of Max Lin\nintroduced by Mahajan et al. (Proc. IWPEC'06 and J. Comput. Syst. Sci. 75,\n2009). In Max Lin AA all weights are integral and we are to decide whether the\nmaximum excess is at least $k$, where $k$ is the parameter.\n  It is not hard to see that we may assume that no two equations in $Az=b$ have\nthe same left-hand side and $n={\\rm rank A}$. Using our maximum excess results,\nwe prove that, under these assumptions, Max Lin AA is fixed-parameter tractable\nfor a wide special case: $m\\le 2^{p(n)}$ for an arbitrary fixed function\n$p(n)=o(n)$.\n  Max $r$-Lin AA is a special case of Max Lin AA, where each equation has at\nmost $r$ variables. In Max Exact $r$-SAT AA we are given a multiset of $m$\nclauses on $n$ variables such that each clause has $r$ variables and asked\nwhether there is a truth assignment to the $n$ variables that satisfies at\nleast $(1-2^{-r})m + k2^{-r}$ clauses. Using our maximum excess results, we\nprove that for each fixed $r\\ge 2$, Max $r$-Lin AA and Max Exact $r$-SAT AA can\nbe solved in time $2^{O(k \\log k)}+m^{O(1)}.$ This improves\n$2^{O(k^2)}+m^{O(1)}$-time algorithms for the two problems obtained by Gutin et\nal. (IWPEC 2009) and Alon et al. (SODA 2010), respectively.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2010 16:49:29 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2010 09:44:07 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Crowston", "R.", ""], ["Gutin", "G.", ""], ["Jones", "M.", ""], ["Kim", "E. J.", ""], ["Ruzsa", "I. Z.", ""]]}, {"id": "1002.0484", "submitter": "Aubin Jarry", "authors": "Aubin Jarry, Pierre Leone and Jose Rolim", "title": "VRAC: Theory #1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to make full use of geographic routing techniques developed for\nsensor networks, nodes must be localized. However, traditional localization and\nvirtual localization techniques are dependent either on expensive and sometimes\nunavailable hardware (e.g. GPS) or on sophisticated localization calculus (e.g.\ntriangulation) which are both error-prone and with a costly overhead.\n  Instead of actually localizing nodes in the physical two-dimensional\nEuclidean space, we use directly the raw distance to a set of anchors to\nproduce multi-dimensional coordinates. We prove that the image of the physical\ntwo-dimensional Euclidean space is a two-dimensional surface, and we show that\nit is possible to adapt geographic routing strategies on this surface, simply,\nefficiently and successfully.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2010 13:31:53 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Jarry", "Aubin", ""], ["Leone", "Pierre", ""], ["Rolim", "Jose", ""]]}, {"id": "1002.0562", "submitter": "Philipp Zumstein", "authors": "Michael Hoffmann, Ji\\v{r}\\'i Matou\\v{s}ek, Yoshio Okamoto, Philipp\n  Zumstein", "title": "Minimum and maximum against k lies", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": "10.1007/978-3-642-13731-0_14", "report-no": null, "categories": "cs.DS cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neat 1972 result of Pohl asserts that [3n/2]-2 comparisons are sufficient,\nand also necessary in the worst case, for finding both the minimum and the\nmaximum of an n-element totally ordered set. The set is accessed via an oracle\nfor pairwise comparisons. More recently, the problem has been studied in the\ncontext of the Renyi-Ulam liar games, where the oracle may give up to k false\nanswers. For large k, an upper bound due to Aigner shows that (k+O(\\sqrt{k}))n\ncomparisons suffice. We improve on this by providing an algorithm with at most\n(k+1+C)n+O(k^3) comparisons for some constant C. The known lower bounds are of\nthe form (k+1+c_k)n-D, for some constant D, where c_0=0.5, c_1=23/32=0.71875,\nand c_k=\\Omega(2^{-5k/4}) as k goes to infinity.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2010 18:54:35 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Hoffmann", "Michael", ""], ["Matou\u0161ek", "Ji\u0159\u00ed", ""], ["Okamoto", "Yoshio", ""], ["Zumstein", "Philipp", ""]]}, {"id": "1002.0580", "submitter": "Joachim Spoerhase", "authors": "Joachim Spoerhase", "title": "An Optimal Algorithm for the Indirect Covering Subtree Problem", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the indirect covering subtree problem (Kim et al., 1996). The\ninput is an edge weighted tree graph along with customers located at the nodes.\nEach customer is associated with a radius and a penalty. The goal is to locate\na tree-shaped facility such that the sum of setup and penalty cost is\nminimized. The setup cost equals the sum of edge lengths taken by the facility\nand the penalty cost is the sum of penalties of all customers whose distance to\nthe facility exceeds their radius. The indirect covering subtree problem\ngeneralizes the single maximum coverage location problem on trees where the\nfacility is a node rather than a subtree. Indirect covering subtree can be\nsolved in $O(n\\log^2 n)$ time (Kim et al., 1996). A slightly faster algorithm\nfor single maximum coverage location with a running time of\n$O(n\\log^2n/\\log\\log n)$ has been provided (Spoerhase and Wirth, 2009). We\nachieve time $O(n\\log n)$ for indirect covering subtree thereby providing the\nfastest known algorithm for both problems. Our result implies also faster\nalgorithms for competitive location problems such as $(1,X)$-medianoid and\n$(1,p)$-centroid on trees. We complement our result by a lower bound of\n$\\Omega(n\\log n)$ for single maximum coverage location and $(1,X)$-medianoid on\na real-number RAM model showing that our algorithm is optimal in running time.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2010 20:15:56 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Spoerhase", "Joachim", ""]]}, {"id": "1002.0874", "submitter": "Fabio Vandin", "authors": "Roberto Grossi, Andrea Pietracaprina, Nadia Pisanti, Geppino Pucci,\n  Eli Upfal, Fabio Vandin", "title": "MADMX: A Novel Strategy for Maximal Dense Motif Extraction", "comments": "A preliminary version of this work was presented in WABI 2009. 10\n  pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop, analyze and experiment with a new tool, called MADMX, which\nextracts frequent motifs, possibly including don't care characters, from\nbiological sequences. We introduce density, a simple and flexible measure for\nbounding the number of don't cares in a motif, defined as the ratio of solid\n(i.e., different from don't care) characters to the total length of the motif.\nBy extracting only maximal dense motifs, MADMX reduces the output size and\nimproves performance, while enhancing the quality of the discoveries. The\nefficiency of our approach relies on a newly defined combining operation,\ndubbed fusion, which allows for the construction of maximal dense motifs in a\nbottom-up fashion, while avoiding the generation of nonmaximal ones. We provide\nexperimental evidence of the efficiency and the quality of the motifs returned\nby MADMX\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2010 01:20:12 GMT"}], "update_date": "2010-02-05", "authors_parsed": [["Grossi", "Roberto", ""], ["Pietracaprina", "Andrea", ""], ["Pisanti", "Nadia", ""], ["Pucci", "Geppino", ""], ["Upfal", "Eli", ""], ["Vandin", "Fabio", ""]]}, {"id": "1002.1021", "submitter": "Robert Geisberger", "authors": "Robert Geisberger", "title": "Heuristic Contraction Hierarchies with Approximation Guarantee", "comments": "7 pages, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new heuristic point-to-point routing algorithm based on\ncontraction hierarchies (CH). Given an epsilon >= 0, we can prove that the\nlength of the path computed by our algorithm is at most (1+epsilon) times the\nlength of the optimal (shortest) path. CH is based on node contraction:\nremoving nodes from a network and adding shortcut edges to preserve shortest\npath distances. Our algorithm tries to avoid shortcuts even when a replacement\npath is epsilon times longer.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2010 15:27:41 GMT"}], "update_date": "2010-02-05", "authors_parsed": [["Geisberger", "Robert", ""]]}, {"id": "1002.1092", "submitter": "Pat Morin", "authors": "Prosenjit Bose, Luc Devroye, Karim Douieb, Vida Dujmovic, James King,\n  and Pat Morin", "title": "Odds-On Trees", "comments": "19 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let R^d -> A be a query problem over R^d for which there exists a data\nstructure S that can compute P(q) in O(log n) time for any query point q in\nR^d. Let D be a probability measure over R^d representing a distribution of\nqueries. We describe a data structure called the odds-on tree, of size\nO(n^\\epsilon) that can be used as a filter that quickly computes P(q) for some\nquery values q in R^d and relies on S for the remaining queries. With an\nodds-on tree, the expected query time for a point drawn according to D is\nO(H*+1), where H* is a lower-bound on the expected cost of any linear decision\ntree that solves P.\n  Odds-on trees have a number of applications, including distribution-sensitive\ndata structures for point location in 2-d, point-in-polytope testing in d\ndimensions, ray shooting in simple polygons, ray shooting in polytopes,\nnearest-neighbour queries in R^d, point-location in arrangements of hyperplanes\nin R^d, and many other geometric searching problems that can be solved in the\nlinear-decision tree model. A standard lifting technique extends these results\nto algebraic decision trees of constant degree. A slightly different version of\nodds-on trees yields similar results for orthogonal searching problems that can\nbe solved in the comparison tree model.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2010 01:29:24 GMT"}], "update_date": "2010-02-08", "authors_parsed": [["Bose", "Prosenjit", ""], ["Devroye", "Luc", ""], ["Douieb", "Karim", ""], ["Dujmovic", "Vida", ""], ["King", "James", ""], ["Morin", "Pat", ""]]}, {"id": "1002.1104", "submitter": "Fabio Vandin", "authors": "Adam Kirsch, Michael Mitzenmacher, Andrea Pietracaprina, Geppino\n  Pucci, Eli Upfal, Fabio Vandin", "title": "An Efficient Rigorous Approach for Identifying Statistically Significant\n  Frequent Itemsets", "comments": "A preliminary version of this work was presented in ACM PODS 2009. 20\n  pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As advances in technology allow for the collection, storage, and analysis of\nvast amounts of data, the task of screening and assessing the significance of\ndiscovered patterns is becoming a major challenge in data mining applications.\nIn this work, we address significance in the context of frequent itemset\nmining. Specifically, we develop a novel methodology to identify a meaningful\nsupport threshold s* for a dataset, such that the number of itemsets with\nsupport at least s* represents a substantial deviation from what would be\nexpected in a random dataset with the same number of transactions and the same\nindividual item frequencies. These itemsets can then be flagged as\nstatistically significant with a small false discovery rate. We present\nextensive experimental results to substantiate the effectiveness of our\nmethodology.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2010 23:33:47 GMT"}], "update_date": "2010-02-08", "authors_parsed": [["Kirsch", "Adam", ""], ["Mitzenmacher", "Michael", ""], ["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""], ["Upfal", "Eli", ""], ["Vandin", "Fabio", ""]]}, {"id": "1002.1183", "submitter": "Lucas Gerin", "authors": "Lucas Gerin (MODAL'X)", "title": "Random sampling of lattice paths with constraints, via transportation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a Monte Carlo Markov Chain (MCMC) procedure for the random\nsampling of some one-dimensional lattice paths with constraints, for various\nconstraints. We show that an approach inspired by optimal transport allows us\nto bound efficiently the mixing time of the associated Markov chain. The\nalgorithm is robust and easy to implement, and samples an \"almost\" uniform path\nof length $n$ in $n^{3+\\eps}$ steps. This bound makes use of a certain\ncontraction property of the Markov chain, and is also used to derive a bound\nfor the running time of Propp-Wilson's CFTP algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2010 10:25:33 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2010 12:51:28 GMT"}], "update_date": "2010-07-28", "authors_parsed": [["Gerin", "Lucas", "", "MODAL'X"]]}, {"id": "1002.1292", "submitter": "Igor Nor", "authors": "Igor Nor, Danny Hermelin, Sylvain Charlat, Jan Engelstadter, Max\n  Reuter, Olivier Duron, Marie-France Sagot", "title": "Mod/Resc Parsimony Inference", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": "10.1007/978-3-642-13509-5_19", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address in this paper a new computational biology problem that aims at\nunderstanding a mechanism that could potentially be used to genetically\nmanipulate natural insect populations infected by inherited, intra-cellular\nparasitic bacteria. In this problem, that we denote by \\textsc{Mod/Resc\nParsimony Inference}, we are given a boolean matrix and the goal is to find two\nother boolean matrices with a minimum number of columns such that an\nappropriately defined operation on these matrices gives back the input. We show\nthat this is formally equivalent to the \\textsc{Bipartite Biclique Edge Cover}\nproblem and derive some complexity results for our problem using this\nequivalence. We provide a new, fixed-parameter tractability approach for\nsolving both that slightly improves upon a previously published algorithm for\nthe \\textsc{Bipartite Biclique Edge Cover}. Finally, we present experimental\nresults where we applied some of our techniques to a real-life data set.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2010 18:15:15 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2010 01:26:05 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Nor", "Igor", ""], ["Hermelin", "Danny", ""], ["Charlat", "Sylvain", ""], ["Engelstadter", "Jan", ""], ["Reuter", "Max", ""], ["Duron", "Olivier", ""], ["Sagot", "Marie-France", ""]]}, {"id": "1002.1679", "submitter": "Max Alekseyev", "authors": "Max A. Alekseyev", "title": "On the intersections of Fibonacci, Pell, and Lucas numbers", "comments": null, "journal-ref": "INTEGERS 11(3), 2011, pp. 239-259", "doi": "10.1515/INTEG.2011.021", "report-no": null, "categories": "math.NT cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe how to compute the intersection of two Lucas sequences of the\nforms $\\{U_n(P,\\pm 1) \\}_{n=0}^{\\infty}$ or $\\{V_n(P,\\pm 1) \\}_{n=0}^{\\infty}$\nwith $P\\in\\mathbb{Z}$ that includes sequences of Fibonacci, Pell, Lucas, and\nLucas-Pell numbers. We prove that such an intersection is finite except for the\ncase $U_n(1,-1)$ and $U_n(3,1)$ and the case of two $V$-sequences when the\nproduct of their discriminants is a perfect square. Moreover, the intersection\nin these cases also forms a Lucas sequence. Our approach relies on solving\nhomogeneous quadratic Diophantine equations and Thue equations. In particular,\nwe prove that 0, 1, 2, and 5 are the only numbers that are both Fibonacci and\nPell, and list similar results for many other pairs of Lucas sequences. We\nfurther extend our results to Lucas sequences with arbitrary initial terms.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2010 20:08:08 GMT"}, {"version": "v2", "created": "Fri, 5 Nov 2010 05:13:03 GMT"}], "update_date": "2011-06-14", "authors_parsed": [["Alekseyev", "Max A.", ""]]}, {"id": "1002.1751", "submitter": "Bruno Ribeiro", "authors": "Bruno Ribeiro and Don Towsley", "title": "Estimating and Sampling Graphs with Multidimensional Random Walks", "comments": null, "journal-ref": null, "doi": null, "report-no": "UMass Amherst Technical Report UM-CS-2010-011 (extended)", "categories": "cs.DS cs.NI", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Estimating characteristics of large graphs via sampling is a vital part of\nthe study of complex networks. Current sampling methods such as (independent)\nrandom vertex and random walks are useful but have drawbacks. Random vertex\nsampling may require too many resources (time, bandwidth, or money). Random\nwalks, which normally require fewer resources per sample, can suffer from large\nestimation errors in the presence of disconnected or loosely connected graphs.\nIn this work we propose a new $m$-dimensional random walk that uses $m$\ndependent random walkers. We show that the proposed sampling method, which we\ncall Frontier sampling, exhibits all of the nice sampling properties of a\nregular random walk. At the same time, our simulations over large real world\ngraphs show that, in the presence of disconnected or loosely connected\ncomponents, Frontier sampling exhibits lower estimation errors than regular\nrandom walks. We also show that Frontier sampling is more suitable than random\nvertex sampling to sample the tail of the degree distribution of the graph.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2010 15:00:13 GMT"}, {"version": "v2", "created": "Tue, 7 Sep 2010 17:04:26 GMT"}], "update_date": "2010-09-08", "authors_parsed": [["Ribeiro", "Bruno", ""], ["Towsley", "Don", ""]]}, {"id": "1002.2084", "submitter": "Iftah Gamzu", "authors": "Iftah Gamzu, Danny Segev", "title": "A Sublogarithmic Approximation for Highway and Tollbooth Pricing", "comments": "14 pages, 3 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An instance of the tollbooth problem consists of an undirected network and a\ncollection of single-minded customers, each of which is interested in\npurchasing a fixed path subject to an individual budget constraint. The\nobjective is to assign a per-unit price to each edge in a way that maximizes\nthe collective revenue obtained from all customers. The revenue generated by\nany customer is equal to the overall price of the edges in her desired path,\nwhen this cost falls within her budget; otherwise, that customer will not\npurchase any edge.\n  Our main result is a deterministic algorithm for the tollbooth problem on\ntrees whose approximation ratio is O(log m / log log m), where m denotes the\nnumber of edges in the underlying graph. This finding improves on the currently\nbest performance guarantees for trees, due to Elbassioni et al. (SAGT '09), as\nwell as for paths (commonly known as the highway problem), due to Balcan and\nBlum (EC '06). An additional interesting consequence is a computational\nseparation between tollbooth pricing on trees and the original prototype\nproblem of single-minded unlimited supply pricing, under a plausible hardness\nhypothesis due to Demaine et al. (SODA '06).\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2010 12:21:26 GMT"}], "update_date": "2010-02-11", "authors_parsed": [["Gamzu", "Iftah", ""], ["Segev", "Danny", ""]]}, {"id": "1002.2147", "submitter": "Rico Zenklusen", "authors": "Fabrizio Grandoni, Rico Zenklusen", "title": "Optimization with More than One Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural way to deal with multiple, partially conflicting objectives is\nturning all the objectives but one into budget constraints. Some classical\npolynomial-time optimization problems, such as spanning tree and forest,\nshortest path, (perfect) matching, independent set (basis) in a matroid or in\nthe intersection of two matroids, become NP-hard even with one budget\nconstraint. Still, for most of these problems deterministic and randomized\npolynomial-time approximation schemes are known. In the case of two or more\nbudgets, typically only multi-criteria approximation schemes are available,\nwhich return slightly infeasible solutions. Not much is known however for the\ncase of strict budget constraints: filling this gap is the main goal of this\npaper.\n  We show that shortest path, perfect matching, and spanning tree (and hence\nmatroid basis and matroid intersection basis) are inapproximable already with\ntwo budget constraints. For the remaining problems, whose set of solutions\nforms an independence system, we present deterministic and randomized\npolynomial-time approximation schemes for a constant number k of budget\nconstraints. Our results are based on a variety of techniques:\n  1. We present a simple and powerful mechanism to transform multi-criteria\napproximation schemes into pure approximation schemes.\n  2. We show that points in low dimensional faces of any matroid polytope are\nalmost integral, an interesting result on its own. This gives a deterministic\napproximation scheme for k-budgeted matroid independent set.\n  3. We present a deterministic approximation scheme for 2-budgeted matching.\nThe backbone of this result is a purely topological property of curves in R^2.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2010 17:48:15 GMT"}], "update_date": "2010-02-11", "authors_parsed": [["Grandoni", "Fabrizio", ""], ["Zenklusen", "Rico", ""]]}, {"id": "1002.2222", "submitter": "Rdv Ijcsis", "authors": "Mosleh M. Abu Alhaj, M. Halaiyqah, Muhannad A. Abu Hashem, Adnan A.\n  Hnaif, O. Abouabdalla, and Ahmed M. Manasrah", "title": "An innovative platform to improve the performance of exact string\n  matching algorithms", "comments": "IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS January 2010, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 7, No. 1, pp. 280-283, January 2010, USA", "doi": null, "report-no": "Journal of Computer Science, ISSN 1947 5500", "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact String Matching is an essential issue in many computer science\napplications. Unfortunately, the performance of Exact String Matching\nalgorithms, namely, executing time, does not address the needs of these\napplications. This paper proposes a general platform for improving the existing\nExact String Matching algorithms executing time, called the PXSMAlg platform.\nThe function of this platform is to parallelize the Exact String Matching\nalgorithms using the MPI model over the Master or Slaves paradigms. The PXSMAlg\nplatform parallelization process is done by dividing the Text into several\nparts and working on these parts simultaneously. This improves the executing\ntime of the Exact String Matching algorithms. We have simulated the PXSMAlg\nplatform in order to show its competence, through applying the Quick Search\nalgorithm on the PXSMAlg platform. The simulation result showed significant\nimprovement in the Quick Search executing time, and therefore extreme\ncompetence in the PXSMAlg platform.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2010 20:31:03 GMT"}], "update_date": "2010-02-12", "authors_parsed": [["Alhaj", "Mosleh M. Abu", ""], ["Halaiyqah", "M.", ""], ["Hashem", "Muhannad A. Abu", ""], ["Hnaif", "Adnan A.", ""], ["Abouabdalla", "O.", ""], ["Manasrah", "Ahmed M.", ""]]}, {"id": "1002.2259", "submitter": "Nikhil Bansal", "authors": "Nikhil Bansal", "title": "Constructive Algorithms for Discrepancy Minimization", "comments": "Fixed a previous erroneous claim about a logarithmic approximation\n  for hereditary discrepancy. Other minor updates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set system (V,S), V={1,...,n} and S={S1,...,Sm}, the minimum\ndiscrepancy problem is to find a 2-coloring of V, such that each set is colored\nas evenly as possible. In this paper we give the first polynomial time\nalgorithms for discrepancy minimization that achieve bounds similar to those\nknown existentially using the so-called Entropy Method. We also give a first\napproximation-like result for discrepancy. The main idea in our algorithms is\nto produce a coloring over time by letting the color of the elements perform a\nrandom walk (with tiny increments) starting from 0 until they reach $-1$ or\n$+1$. At each time step the random hops for various elements are correlated\nusing the solution to a semidefinite program, where this program is determined\nby the current state and the entropy method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2010 02:48:06 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2010 20:35:21 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2010 21:35:18 GMT"}, {"version": "v4", "created": "Mon, 9 Aug 2010 01:32:16 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Bansal", "Nikhil", ""]]}, {"id": "1002.2440", "submitter": "Bernhard von Stengel", "authors": "Christoph Ambuehl, Bernd Gaertner, Bernhard von Stengel", "title": "Optimal Lower Bounds for Projective List Update Algorithms", "comments": "Version 3 same as version 2, but date in LaTeX \\today macro replaced\n  by March 8, 2012", "journal-ref": "ACM Transactions on Algorithms (TALG) Volume 9, Issue 4, September\n  2013, Article 31, 18 pages", "doi": "10.1145/2500120", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The list update problem is a classical online problem, with an optimal\ncompetitive ratio that is still open, known to be somewhere between 1.5 and\n1.6. An algorithm with competitive ratio 1.6, the smallest known to date, is\nCOMB, a randomized combination of BIT and the TIMESTAMP algorithm TS. This and\nalmost all other list update algorithms, like MTF, are projective in the sense\nthat they can be defined by looking only at any pair of list items at a time.\nProjectivity (also known as \"list factoring\") simplifies both the description\nof the algorithm and its analysis, and so far seems to be the only way to\ndefine a good online algorithm for lists of arbitrary length. In this paper we\ncharacterize all projective list update algorithms and show that their\ncompetitive ratio is never smaller than 1.6 in the partial cost model.\nTherefore, COMB is a best possible projective algorithm in this model.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2010 21:48:07 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2012 08:21:54 GMT"}, {"version": "v3", "created": "Thu, 8 Mar 2012 19:00:15 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Ambuehl", "Christoph", ""], ["Gaertner", "Bernd", ""], ["von Stengel", "Bernhard", ""]]}, {"id": "1002.2580", "submitter": "Rodrigo Silveira", "authors": "Chris Gray, Frank Kammer, Maarten Loffler, Rodrigo I. Silveira", "title": "Removing Local Extrema from Imprecise Terrains", "comments": "21 pages, revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider imprecise terrains, that is, triangulated terrains\nwith a vertical error interval in the vertices. In particular, we study the\nproblem of removing as many local extrema (minima and maxima) as possible from\nthe terrain. We show that removing only minima or only maxima can be done\noptimally in O(n log n) time, for a terrain with n vertices. Interestingly,\nhowever, removing both the minima and maxima simultaneously is NP-hard, and is\neven hard to approximate within a factor of O(log log n) unless P=NP. Moreover,\nwe show that even a simplified version of the problem where vertices can have\nonly two different heights is already NP-hard, a result we obtain by proving\nhardness of a special case of 2-Disjoint Connected Subgraphs, a problem that\nhas lately received considerable attention from the graph-algorithms community.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2010 15:56:49 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2011 10:56:27 GMT"}], "update_date": "2011-08-15", "authors_parsed": [["Gray", "Chris", ""], ["Kammer", "Frank", ""], ["Loffler", "Maarten", ""], ["Silveira", "Rodrigo I.", ""]]}, {"id": "1002.2746", "submitter": "David Doty", "authors": "David Doty, Lila Kari, Benoit Masson", "title": "Negative Interactions in Irreversible Self-Assembly", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-18305-8_4", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of negative (i.e., repulsive) interaction the\nabstract Tile Assembly Model defined by Winfree. Winfree postulated negative\ninteractions to be physically plausible in his Ph.D. thesis, and Reif, Sahu,\nand Yin explored their power in the context of reversible attachment\noperations. We explore the power of negative interactions with irreversible\nattachments, and we achieve two main results. Our first result is an\nimpossibility theorem: after t steps of assembly, Omega(t) tiles will be\nforever bound to an assembly, unable to detach. Thus negative glue strengths do\nnot afford unlimited power to reuse tiles. Our second result is a positive one:\nwe construct a set of tiles that can simulate a Turing machine with space bound\ns and time bound t, while ensuring that no intermediate assembly grows larger\nthan O(s), rather than O(s * t) as required by the standard Turing machine\nsimulation with tiles.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2010 02:56:05 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Doty", "David", ""], ["Kari", "Lila", ""], ["Masson", "Benoit", ""]]}, {"id": "1002.2975", "submitter": "Kevin E. Bassler", "authors": "Charo I. Del Genio, Hyunju Kim, Zoltan Toroczkai, and Kevin E. Bassler", "title": "Efficient and exact sampling of simple graphs with given arbitrary\n  degree sequence", "comments": "8 pages, 3 figures", "journal-ref": "PLoS ONE 5(4), e10012 (2010).", "doi": "10.1371/journal.pone.0010012", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniform sampling from graphical realizations of a given degree sequence is a\nfundamental component in simulation-based measurements of network observables,\nwith applications ranging from epidemics, through social networks to Internet\nmodeling. Existing graph sampling methods are either link-swap based\n(Markov-Chain Monte Carlo algorithms) or stub-matching based (the Configuration\nModel). Both types are ill-controlled, with typically unknown mixing times for\nlink-swap methods and uncontrolled rejections for the Configuration Model. Here\nwe propose an efficient, polynomial time algorithm that generates statistically\nindependent graph samples with a given, arbitrary, degree sequence. The\nalgorithm provides a weight associated with each sample, allowing the\nobservable to be measured either uniformly over the graph ensemble, or,\nalternatively, with a desired distribution. Unlike other algorithms, this\nmethod always produces a sample, without back-tracking or rejections. Using a\ncentral limit theorem-based reasoning, we argue, that for large N, and for\ndegree sequences admitting many realizations, the sample weights are expected\nto have a lognormal distribution. As examples, we apply our algorithm to\ngenerate networks with degree sequences drawn from power-law distributions and\nfrom binomial distributions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2010 22:54:36 GMT"}], "update_date": "2010-04-14", "authors_parsed": [["Del Genio", "Charo I.", ""], ["Kim", "Hyunju", ""], ["Toroczkai", "Zoltan", ""], ["Bassler", "Kevin E.", ""]]}, {"id": "1002.3102", "submitter": "Tanmoy Chakraborty", "authors": "Tanmoy Chakraborty, Eyal Even-Dar, Sudipto Guha, Yishay Mansour, S.\n  Muthukrishnan", "title": "Selective Call Out and Real Time Bidding", "comments": "24 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ads on the Internet are increasingly sold via ad exchanges such as\nRightMedia, AdECN and Doubleclick Ad Exchange. These exchanges allow real-time\nbidding, that is, each time the publisher contacts the exchange, the exchange\n``calls out'' to solicit bids from ad networks. This aspect of soliciting bids\nintroduces a novel aspect, in contrast to existing literature. This suggests\ndeveloping a joint optimization framework which optimizes over the allocation\nand well as solicitation. We model this selective call out as an online\nrecurrent Bayesian decision framework with bandwidth type constraints. We\nobtain natural algorithms with bounded performance guarantees for several\nnatural optimization criteria. We show that these results hold under different\ncall out constraint models, and different arrival processes. Interestingly, the\npaper shows that under MHR assumptions, the expected revenue of generalized\nsecond price auction with reserve is constant factor of the expected welfare.\nAlso the analysis herein allow us prove adaptivity gap type results for the\nadwords problem.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2010 15:19:52 GMT"}, {"version": "v2", "created": "Tue, 10 Aug 2010 16:37:45 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Chakraborty", "Tanmoy", ""], ["Even-Dar", "Eyal", ""], ["Guha", "Sudipto", ""], ["Mansour", "Yishay", ""], ["Muthukrishnan", "S.", ""]]}, {"id": "1002.3511", "submitter": "Yakov Nekrich", "authors": "Marek Karpinski, J. Ian Munro, Yakov Nekrich", "title": "Range Reporting for Moving Points on a Grid", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a new data structure that supports orthogonal range\nreporting queries on a set of points that move along linear trajectories on a\n$U\\times U$ grid. The assumption that points lie on a $U\\times U$ grid enables\nus to significantly decrease the query time in comparison to the standard\nkinetic model. Our data structure answers queries in $O(\\sqrt{\\log U/\\log \\log\nU}+k)$ time, where $k$ denotes the number of points in the answer. The above\nimproves over the $\\Omega(\\log n)$ lower bound that is valid in the\ninfinite-precision kinetic model. The methods used in this paper could be also\nof independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2010 13:29:47 GMT"}], "update_date": "2010-02-19", "authors_parsed": [["Karpinski", "Marek", ""], ["Munro", "J. Ian", ""], ["Nekrich", "Yakov", ""]]}, {"id": "1002.3518", "submitter": "Nikolaos Fountoulakis", "authors": "Nikolaos Fountoulakis and Konstantinos Panagiotou", "title": "Rumor Spreading on Random Regular Graphs and Expanders", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Broadcasting algorithms are important building blocks of distributed systems.\nIn this work we investigate the typical performance of the classical and\nwell-studied push model. Assume that initially one node in a given network\nholds some piece of information. In each round, every one of the informed nodes\nchooses independently a neighbor uniformly at random and transmits the message\nto it.\n  In this paper we consider random networks where each vertex has degree d,\nwhich is at least 3, i.e., the underlying graph is drawn uniformly at random\nfrom the set of all d-regular graphs with n vertices. We show that with\nprobability 1 - o(1) the push model broadcasts the message to all nodes within\n(1 + o(1))C_d ln n rounds, where C_d = 1/ ln(2(1-1/d)) - 1/(d ln(1 - 1/d)). In\nparticular, we can characterize precisely the effect of the node degree to the\ntypical broadcast time of the push model. Moreover, we consider pseudo-random\nregular networks, where we assume that the degree of each node is very large.\nThere we show that the broadcast time is (1+o(1))C ln n with probability 1 -\no(1), where C= 1/ ln 2 + 1, is the limit of C_d as d grows.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2010 13:50:07 GMT"}], "update_date": "2010-02-19", "authors_parsed": [["Fountoulakis", "Nikolaos", ""], ["Panagiotou", "Konstantinos", ""]]}, {"id": "1002.3541", "submitter": "Ilan Newman", "authors": "Ilan Newman and Yuri Rabinovich", "title": "Finite Volume Spaces and Sparsification", "comments": "previous revision was the wrong file: the new revision: changed\n  (extended considerably) the treatment of finite volumes (see revised\n  abstract). Inserted new applications for the sparsification techniques", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study finite $d$-volumes - the high dimensional\ngeneralization of finite metric spaces. Having developed a suitable\ncombinatorial machinery, we define $\\ell_1$-volumes and show that they contain\nEuclidean volumes and hypertree volumes. We show that they can approximate any\n$d$-volume with $O(n^d)$ multiplicative distortion. On the other hand, contrary\nto Bourgain's theorem for $d=1$, there exists a $2$-volume that on $n$ vertices\nthat cannot be approximated by any $\\ell_1$-volume with distortion smaller than\n$\\tilde{\\Omega}(n^{1/5})$.\n  We further address the problem of $\\ell_1$-dimension reduction in the context\nof $\\ell_1$ volumes, and show that this phenomenon does occur, although not to\nthe same striking degree as it does for Euclidean metrics and volumes. In\nparticular, we show that any $\\ell_1$ metric on $n$ points can be $(1+\n\\epsilon)$-approximated by a sum of $O(n/\\epsilon^2)$ cut metrics, improving\nover the best previously known bound of $O(n \\log n)$ due to Schechtman.\n  In order to deal with dimension reduction, we extend the techniques and ideas\nintroduced by Karger and Bencz{\\'u}r, and Spielman et al.~in the context of\ngraph Sparsification, and develop general methods with a wide range of\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2010 15:11:15 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2010 08:22:18 GMT"}, {"version": "v3", "created": "Mon, 2 Aug 2010 12:04:52 GMT"}], "update_date": "2010-08-03", "authors_parsed": [["Newman", "Ilan", ""], ["Rabinovich", "Yuri", ""]]}, {"id": "1002.3724", "submitter": "Francesco Silvestri", "authors": "Sara Nasso (1), Francesco Silvestri (1), Francesco Tisiot (1), Barbara\n  Di Camillo (1), Andrea Pietracaprina (1) and Gianna Maria Toffolo (1) ((1)\n  Department of Information Engineering, University of Padova)", "title": "An Optimized Data Structure for High Throughput 3D Proteomics Data:\n  mzRTree", "comments": "Paper details: 10 pages, 7 figures, 2 tables. To be published in\n  Journal of Proteomics. Source code available at\n  http://www.dei.unipd.it/mzrtree", "journal-ref": "Journal of Proteomics 73(6) (2010) 1176-1182", "doi": "10.1016/j.jprot.2010.02.006", "report-no": null, "categories": "cs.CE cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an emerging field, MS-based proteomics still requires software tools for\nefficiently storing and accessing experimental data. In this work, we focus on\nthe management of LC-MS data, which are typically made available in standard\nXML-based portable formats. The structures that are currently employed to\nmanage these data can be highly inefficient, especially when dealing with\nhigh-throughput profile data. LC-MS datasets are usually accessed through 2D\nrange queries. Optimizing this type of operation could dramatically reduce the\ncomplexity of data analysis. We propose a novel data structure for LC-MS\ndatasets, called mzRTree, which embodies a scalable index based on the R-tree\ndata structure. mzRTree can be efficiently created from the XML-based data\nformats and it is suitable for handling very large datasets. We experimentally\nshow that, on all range queries, mzRTree outperforms other known structures\nused for LC-MS data, even on those queries these structures are optimized for.\nBesides, mzRTree is also more space efficient. As a result, mzRTree reduces\ndata analysis computational costs for very large profile datasets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2010 17:17:02 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2010 08:18:47 GMT"}], "update_date": "2010-04-27", "authors_parsed": [["Nasso", "Sara", ""], ["Silvestri", "Francesco", ""], ["Tisiot", "Francesco", ""], ["Di Camillo", "Barbara", ""], ["Pietracaprina", "Andrea", ""], ["Toffolo", "Gianna Maria", ""]]}, {"id": "1002.3763", "submitter": "Juli\\'an Mestre", "authors": "Jian Li and Juli\\'an Mestre", "title": "Improved bounds for stochastic matching", "comments": "This paper has been withdrawn due to new merged paper arXiv:1008.5356", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This results in this paper have been merged with the result in\narXiv:1003.0167. The authors would like to withdraw this version. Please see\narXiv:1008.5356 for the merged version.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2010 15:44:03 GMT"}, {"version": "v2", "created": "Mon, 6 Sep 2010 07:30:45 GMT"}], "update_date": "2010-09-07", "authors_parsed": [["Li", "Jian", ""], ["Mestre", "Juli\u00e1n", ""]]}, {"id": "1002.3864", "submitter": "Prahladh Harsha", "authors": "Prahladh Harsha, Moses Charikar, Matthew Andrews, Sanjeev Arora,\n  Subhash Khot, Dana Moshkovitz, Lisa Zhang, Ashkan Aazami, Dev Desai, Igor\n  Gorodezky, Geetha Jagannathan, Alexander S. Kulikov, Darakhshan J. Mir,\n  Alantha Newman, Aleksandar Nikolov, David Pritchard and Gwen Spencer", "title": "Limits of Approximation Algorithms: PCPs and Unique Games (DIMACS\n  Tutorial Lecture Notes)", "comments": "74 pages, lecture notes", "journal-ref": null, "doi": null, "report-no": "DIMACS Technical Report 2010-02", "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are the lecture notes for the DIMACS Tutorial \"Limits of Approximation\nAlgorithms: PCPs and Unique Games\" held at the DIMACS Center, CoRE Building,\nRutgers University on 20-21 July, 2009. This tutorial was jointly sponsored by\nthe DIMACS Special Focus on Hardness of Approximation, the DIMACS Special Focus\non Algorithmic Foundations of the Internet, and the Center for Computational\nIntractability with support from the National Security Agency and the National\nScience Foundation.\n  The speakers at the tutorial were Matthew Andrews, Sanjeev Arora, Moses\nCharikar, Prahladh Harsha, Subhash Khot, Dana Moshkovitz and Lisa Zhang. The\nsribes were Ashkan Aazami, Dev Desai, Igor Gorodezky, Geetha Jagannathan,\nAlexander S. Kulikov, Darakhshan J. Mir, Alantha Newman, Aleksandar Nikolov,\nDavid Pritchard and Gwen Spencer.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2010 10:26:50 GMT"}], "update_date": "2010-03-30", "authors_parsed": [["Harsha", "Prahladh", ""], ["Charikar", "Moses", ""], ["Andrews", "Matthew", ""], ["Arora", "Sanjeev", ""], ["Khot", "Subhash", ""], ["Moshkovitz", "Dana", ""], ["Zhang", "Lisa", ""], ["Aazami", "Ashkan", ""], ["Desai", "Dev", ""], ["Gorodezky", "Igor", ""], ["Jagannathan", "Geetha", ""], ["Kulikov", "Alexander S.", ""], ["Mir", "Darakhshan J.", ""], ["Newman", "Alantha", ""], ["Nikolov", "Aleksandar", ""], ["Pritchard", "David", ""], ["Spencer", "Gwen", ""]]}, {"id": "1002.3866", "submitter": "Frederique Bassino", "authors": "Fr\\'ed\\'erique Bassino (LIPN), Mathilde Bouvel (LIAFA), Adeline\n  Pierrot (LIAFA), Dominique Rossin (LIX)", "title": "Deciding the finiteness of the number of simple permutations contained\n  in a wreath-closed class is polynomial", "comments": null, "journal-ref": "Pure Mathematics and Applications 21, 2 (2010) 119-135", "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm running in time O(n ln n) which decides if a\nwreath-closed permutation class Av(B) given by its finite basis B contains a\nfinite number of simple permutations. The method we use is based on an article\nof Brignall, Ruskuc and Vatter which presents a decision procedure (of high\ncomplexity) for solving this question, without the assumption that Av(B) is\nwreath-closed. Using combinatorial, algorithmic and language theoretic\narguments together with one of our previous results on pin-permutations, we are\nable to transform the problem into a co-finiteness problem in a complete\ndeterministic automaton.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2010 10:53:57 GMT"}, {"version": "v2", "created": "Fri, 4 Feb 2011 16:04:26 GMT"}, {"version": "v3", "created": "Fri, 4 Mar 2011 20:30:53 GMT"}], "update_date": "2011-03-30", "authors_parsed": [["Bassino", "Fr\u00e9d\u00e9rique", "", "LIPN"], ["Bouvel", "Mathilde", "", "LIAFA"], ["Pierrot", "Adeline", "", "LIAFA"], ["Rossin", "Dominique", "", "LIX"]]}, {"id": "1002.4002", "submitter": "William Jackson", "authors": "A. K. Ojha, A.K. Das", "title": "Multi-Objective Geometric Programming Problem Being Cost Coefficients as\n  Continous Function with Weighted Mean Method", "comments": null, "journal-ref": "Journal of Computing, Volume 2, Issue 2, February 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric programming problems occur frequently in engineering design and\nmanagement. In multiobjective optimization, the trade-off information between\ndifferent objective functions is probably the most important piece of\ninformation in a solution process to reach the most preferred solution. In this\npaper we have discussed the basic concepts and principles of multiple objective\noptimization problems and developed a solution procedure to solve this\noptimization problem where the cost coefficients are continuous functions using\nweighted method to obtain the non-inferior solutions.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2010 19:36:55 GMT"}], "update_date": "2010-03-25", "authors_parsed": [["Ojha", "A. K.", ""], ["Das", "A. K.", ""]]}, {"id": "1002.4005", "submitter": "William Jackson", "authors": "Rio G. L. D'Souza, K. Chandra Sekaran, A. Kandasamy", "title": "Improved NSGA-II Based on a Novel Ranking Scheme", "comments": null, "journal-ref": "Journal of Computing, Volume 2, Issue 2, February 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-dominated Sorting Genetic Algorithm (NSGA) has established itself as a\nbenchmark algorithm for Multiobjective Optimization. The determination of\npareto-optimal solutions is the key to its success. However the basic algorithm\nsuffers from a high order of complexity, which renders it less useful for\npractical applications. Among the variants of NSGA, several attempts have been\nmade to reduce the complexity. Though successful in reducing the runtime\ncomplexity, there is scope for further improvements, especially considering\nthat the populations involved are frequently of large size. We propose a\nvariant which reduces the run-time complexity using the simple principle of\nspace-time trade-off. The improved algorithm is applied to the problem of\nclassifying types of leukemia based on microarray data. Results of comparative\ntests are presented showing that the improved algorithm performs well on large\npopulations.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2010 19:43:45 GMT"}], "update_date": "2010-03-25", "authors_parsed": [["D'Souza", "Rio G. L.", ""], ["Sekaran", "K. Chandra", ""], ["Kandasamy", "A.", ""]]}, {"id": "1002.4034", "submitter": "Shi Li", "authors": "Shi Li", "title": "On constant factor approximation for earth mover distance over doubling\n  metrics", "comments": "Extended abstract. An older version submitted to ICALP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a metric space $(X,d_X)$, the earth mover distance between two\ndistributions over $X$ is defined as the minimum cost of a bipartite matching\nbetween the two distributions. The doubling dimension of a metric $(X, d_X)$ is\nthe smallest value $\\alpha$ such that every ball in $X$ can be covered by\n$2^\\alpha$ ball of half the radius. We study efficient algorithms for\napproximating earth mover distance over metrics with bounded doubling\ndimension.\n  Given a metric $(X, d_X)$, with $|X| = n$, we can use $\\tilde O(n^2)$\npreprocessing time to create a data structure of size $\\tilde O(n^{1 + \\e})$,\nsuch that subsequently queried EMDs can be $O(\\alpha_X/\\e)$-approximated in\n$\\tilde O(n)$ time.\n  We also show a weaker form of sketching scheme, which we call \"encoding\nscheme\". Given $(X, d_X)$, by using $\\tilde O(n^2)$ preprocessing time, every\nsubsequent distribution $\\mu$ over $X$ can be encoded into $F(\\mu)$ in $\\tilde\nO(n^{1 + \\e})$ time. Given $F(\\mu)$ and $F(\\nu)$, the EMD between $\\mu$ and\n$\\nu$ can be $O(\\alpha_X/\\e)$-approximated in $\\tilde O(n^\\e)$ time.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2010 01:26:19 GMT"}], "update_date": "2010-02-23", "authors_parsed": [["Li", "Shi", ""]]}, {"id": "1002.4182", "submitter": "Gokarna Sharma", "authors": "Gokarna Sharma, Brett Estrade and Costas Busch", "title": "Window-Based Greedy Contention Management for Transactional Memory", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider greedy contention managers for transactional memory for M x N\nexecution windows of transactions with M threads and N transactions per thread.\nAssuming that each transaction conflicts with at most C other transactions\ninside the window, a trivial greedy contention manager can schedule them within\nCN time. In this paper, we show that there are much better schedules. We\npresent and analyze two new randomized greedy contention management algorithms.\nThe first algorithm Offline-Greedy produces a schedule of length O(C + N\nlog(MN)) with high probability, and gives competitive ratio O(log(MN)) for C <=\nN log(MN). The offline algorithm depends on knowing the conflict graph. The\nsecond algorithm Online-Greedy produces a schedule of length O(C log(MN) + N\nlog^2(MN)) with high probability which is only a O(log(NM)) factor worse, but\ndoes not require knowledge of the conflict graph. We also give an adaptive\nversion which achieves similar worst-case performance and C is determined on\nthe fly under execution. Our algorithms provide new tradeoffs for greedy\ntransaction scheduling that parameterize window sizes and transaction conflicts\nwithin the window.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2010 20:02:24 GMT"}], "update_date": "2010-02-23", "authors_parsed": [["Sharma", "Gokarna", ""], ["Estrade", "Brett", ""], ["Busch", "Costas", ""]]}, {"id": "1002.4248", "submitter": "\\\"Ozg\\\"ur  \\\"Ozkan", "authors": "John Iacono and \\\"Ozg\\\"ur \\\"Ozkan", "title": "Mergeable Dictionaries", "comments": "34 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data structure is presented for the Mergeable Dictionary abstract data\ntype, which supports the following operations on a collection of disjoint sets\nof totally ordered data: Predecessor-Search, Split and Merge. While\nPredecessor-Search and Split work in the normal way, the novel operation is\nMerge. While in a typical mergeable dictionary (e.g. 2-4 Trees), the Merge\noperation can only be performed on sets that span disjoint intervals in\nkeyspace, the structure here has no such limitation, and permits the merging of\narbitrarily interleaved sets. Tarjan and Brown present a data structure which\ncan handle arbitrary Merge operations in O(log n) amortized time per operation\nif the set of operations is restricted to exclude the Split operation. In the\npresence of Split operations, the amortized time complexity of their structure\nbecomes \\Omega(n). A data structure which supports both Split and Merge\noperations in O(log^2 n) amortized time per operation was given by Farach and\nThorup. In contrast, our data structure supports all operations, including\nSplit and Merge, in O(log n) amortized time, thus showing that interleaved\nMerge operations can be supported at no additional cost vis-a-vis disjoint\nMerge operations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2010 05:49:00 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2012 20:08:09 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Iacono", "John", ""], ["\u00d6zkan", "\u00d6zg\u00fcr", ""]]}, {"id": "1002.4330", "submitter": "Robert Geisberger", "authors": "Jonathan Dees, Robert Geisberger, Peter Sanders, Roland Bader", "title": "Defining and Computing Alternative Routes in Road Networks", "comments": "10 pages, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every human likes choices. But today's fast route planning algorithms usually\ncompute just a single route between source and target. There are beginnings to\ncompute alternative routes, but this topic has not been studied thoroughly.\nOften, the aspect of meaningful alternative routes is neglected from a human\npoint of view. We fill in this gap by suggesting mathematical definitions for\nsuch routes. As a second contribution we propose heuristics to compute them, as\nthis is NP-hard in general.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2010 13:43:34 GMT"}], "update_date": "2010-02-24", "authors_parsed": [["Dees", "Jonathan", ""], ["Geisberger", "Robert", ""], ["Sanders", "Peter", ""], ["Bader", "Roland", ""]]}, {"id": "1002.4464", "submitter": "Frank Dehne", "authors": "Frank Dehne and Hamidreza Zaboli", "title": "Deterministic Sample Sort For GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and evaluate GPU Bucket Sort, a parallel deterministic sample sort\nalgorithm for many-core GPUs. Our method is considerably faster than Thrust\nMerge (Satish et.al., Proc. IPDPS 2009), the best comparison-based sorting\nalgorithm for GPUs, and it is as fast as the new randomized sample sort for\nGPUs by Leischner et.al. (to appear in Proc. IPDPS 2010). Our deterministic\nsample sort has the advantage that bucket sizes are guaranteed and therefore\nits running time does not have the input data dependent fluctuations that can\noccur for randomized sample sort.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2010 04:27:14 GMT"}], "update_date": "2010-02-25", "authors_parsed": [["Dehne", "Frank", ""], ["Zaboli", "Hamidreza", ""]]}, {"id": "1002.4482", "submitter": "Frank Dehne", "authors": "Frank Dehne and Kumanan Yogaratnam", "title": "Exploring the Limits of GPUs With Parallel Graph Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the limits of graphics processors (GPUs) for\ngeneral purpose parallel computing by studying problems that require highly\nirregular data access patterns: parallel graph algorithms for list ranking and\nconnected components. Such graph problems represent a worst case scenario for\ncoalescing parallel memory accesses on GPUs which is critical for good GPU\nperformance. Our experimental study indicates that PRAM algorithms are a good\nstarting point for developing efficient parallel GPU methods but require\nnon-trivial modifications to ensure good GPU performance. We present a set of\nguidelines that help algorithm designers adapt PRAM graph algorithms for\nparallel GPU computation. We point out that the study of parallel graph\nalgorithms for GPUs is of wider interest for discrete and combinatorial\nproblems in general because many of these problems require similar irregular\ndata access patterns.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2010 06:37:29 GMT"}], "update_date": "2010-02-25", "authors_parsed": [["Dehne", "Frank", ""], ["Yogaratnam", "Kumanan", ""]]}, {"id": "1002.5034", "submitter": "Seeun Umboh", "authors": "Eric Bach, Shuchi Chawla, Seeun Umboh", "title": "Threshold rules for online sample selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following sample selection problem. We observe in an online\nfashion a sequence of samples, each endowed by a quality. Our goal is to either\nselect or reject each sample, so as to maximize the aggregate quality of the\nsubsample selected so far. There is a natural trade-off here between the rate\nof selection and the aggregate quality of the subsample. We show that for a\nnumber of such problems extremely simple and oblivious \"threshold rules\" for\nselection achieve optimal tradeoffs between rate of selection and aggregate\nquality in a probabilistic sense. In some cases we show that the same threshold\nrule is optimal for a large class of quality distributions and is thus\noblivious in a strong sense.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2010 17:52:28 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2010 03:32:33 GMT"}], "update_date": "2010-07-20", "authors_parsed": [["Bach", "Eric", ""], ["Chawla", "Shuchi", ""], ["Umboh", "Seeun", ""]]}]