[{"id": "0904.0228", "submitter": "Genady Grabarnik", "authors": "Genady Grabarnik, Aaron Kershenbaum (IBM TJ Watson Research)", "title": "Safe Reasoning Over Ontologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As ontologies proliferate and automatic reasoners become more powerful, the\nproblem of protecting sensitive information becomes more serious. In\nparticular, as facts can be inferred from other facts, it becomes increasingly\nlikely that information included in an ontology, while not itself deemed\nsensitive, may be able to be used to infer other sensitive information.\n  We first consider the problem of testing an ontology for safeness defined as\nits not being able to be used to derive any sensitive facts using a given\ncollection of inference rules. We then consider the problem of optimizing an\nontology based on the criterion of making as much useful information as\npossible available without revealing any sensitive facts.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2009 18:46:04 GMT"}], "update_date": "2009-04-02", "authors_parsed": [["Grabarnik", "Genady", "", "IBM TJ Watson Research"], ["Kershenbaum", "Aaron", "", "IBM TJ Watson Research"]]}, {"id": "0904.0292", "submitter": "Huy Nguyen", "authors": "Khanh Do Ba, Huy L Nguyen, Huy N Nguyen, Ronitt Rubinfeld", "title": "Sublinear Time Algorithms for Earth Mover's Distance", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the Earth Mover's Distance (EMD) between\nprobability distributions when given access only to samples. We give closeness\ntesters and additive-error estimators over domains in $[0, \\Delta]^d$, with\nsample complexities independent of domain size - permitting the testability\neven of continuous distributions over infinite domains. Instead, our algorithms\ndepend on other parameters, such as the diameter of the domain space, which may\nbe significantly smaller. We also prove lower bounds showing the dependencies\non these parameters to be essentially optimal. Additionally, we consider\nwhether natural classes of distributions exist for which there are algorithms\nwith better dependence on the dimension, and show that for highly clusterable\ndata, this is indeed the case. Lastly, we consider a variant of the EMD,\ndefined over tree metrics instead of the usual L1 metric, and give optimal\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2009 02:17:43 GMT"}], "update_date": "2009-04-03", "authors_parsed": [["Ba", "Khanh Do", ""], ["Nguyen", "Huy L", ""], ["Nguyen", "Huy N", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "0904.0352", "submitter": "Rami Puzis", "authors": "Shlomi Dolev, Yuval Elovici, Rami Puzis, Polina Zilberman", "title": "Incremental Deployment of Network Monitors Based on Group Betweenness\n  Centrality", "comments": null, "journal-ref": "Information Processing Letters, 109(20), 1172-1176 (2009)", "doi": "10.1016/j.ipl.2009.07.019", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications we are required to increase the deployment of a\ndistributed monitoring system on an evolving network. In this paper we present\na new method for finding candidate locations for additional deployment in the\nnetwork. This method is based on the Group Betweenness Centrality (GBC) measure\nthat is used to estimate the influence of a group of nodes over the information\nflow in the network. The new method assists in finding the location of k\nadditional monitors in the evolving network, such that the portion of\nadditional traffic covered is at least (1-1/e) of the optimal.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2009 09:32:51 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2009 10:01:36 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 13:32:31 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Dolev", "Shlomi", ""], ["Elovici", "Yuval", ""], ["Puzis", "Rami", ""], ["Zilberman", "Polina", ""]]}, {"id": "0904.0583", "submitter": "Karthekeyan Chandrasekaran", "authors": "Karthekeyan Chandrasekaran, Daniel Dadush, Santosh Vempala", "title": "Thin Partitions: Isoperimetric Inequalities and Sampling Algorithms for\n  some Nonconvex Families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.FA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Star-shaped bodies are an important nonconvex generalization of convex bodies\n(e.g., linear programming with violations). Here we present an efficient\nalgorithm for sampling a given star-shaped body. The complexity of the\nalgorithm grows polynomially in the dimension and inverse polynomially in the\nfraction of the volume taken up by the kernel of the star-shaped body. The\nanalysis is based on a new isoperimetric inequality. Our main technical\ncontribution is a tool for proving such inequalities when the domain is not\nconvex. As a consequence, we obtain a polynomial algorithm for computing the\nvolume of such a set as well. In contrast, linear optimization over star-shaped\nsets is NP-hard.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2009 14:44:41 GMT"}], "update_date": "2009-04-06", "authors_parsed": [["Chandrasekaran", "Karthekeyan", ""], ["Dadush", "Daniel", ""], ["Vempala", "Santosh", ""]]}, {"id": "0904.0727", "submitter": "Dimitrios Thilikos", "authors": "Hans L. Bodlaender, Fedor V. Fomin, Daniel Lokshtanov, Eelko Penninkx,\n  Saket Saurabh and Dimitrios M. Thilikos", "title": "(Meta) Kernelization", "comments": "Complete version of the paper of FOCS 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a parameterized problem, every instance I comes with a positive integer k.\nThe problem is said to admit a polynomial kernel if, in polynomial time, one\ncan reduce the size of the instance I to a polynomial in k, while preserving\nthe answer. In this work we give two meta-theorems on kernelzation. The first\ntheorem says that all problems expressible in Counting Monadic Second Order\nLogic and satisfying a coverability property admit a polynomial kernel on\ngraphs of bounded genus. Our second result is that all problems that have\nfinite integer index and satisfy a weaker coverability property admit a linear\nkernel on graphs of bounded genus. These theorems unify and extend all\npreviously known kernelization results for planar graph problems.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2009 16:11:07 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2013 19:21:16 GMT"}, {"version": "v3", "created": "Wed, 25 Sep 2013 08:05:29 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Bodlaender", "Hans L.", ""], ["Fomin", "Fedor V.", ""], ["Lokshtanov", "Daniel", ""], ["Penninkx", "Eelko", ""], ["Saurabh", "Saket", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "0904.0859", "submitter": "David Pritchard", "authors": "David Pritchard, Deeparnab Chakrabarty", "title": "Approximability of Sparse Integer Programs", "comments": "Version submitted to Algorithmica special issue on ESA 2009. Previous\n  conference version: http://dx.doi.org/10.1007/978-3-642-04128-0_8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The main focus of this paper is a pair of new approximation algorithms for\ncertain integer programs. First, for covering integer programs {min cx: Ax >=\nb, 0 <= x <= d} where A has at most k nonzeroes per row, we give a\nk-approximation algorithm. (We assume A, b, c, d are nonnegative.) For any k >=\n2 and eps>0, if P != NP this ratio cannot be improved to k-1-eps, and under the\nunique games conjecture this ratio cannot be improved to k-eps. One key idea is\nto replace individual constraints by others that have better rounding\nproperties but the same nonnegative integral solutions; another critical\ningredient is knapsack-cover inequalities. Second, for packing integer programs\n{max cx: Ax <= b, 0 <= x <= d} where A has at most k nonzeroes per column, we\ngive a (2k^2+2)-approximation algorithm. Our approach builds on the iterated LP\nrelaxation framework. In addition, we obtain improved approximations for the\nsecond problem when k=2, and for both problems when every A_{ij} is small\ncompared to b_i. Finally, we demonstrate a 17/16-inapproximability for covering\ninteger programs with at most two nonzeroes per column.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2009 07:31:43 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2009 05:21:35 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2009 23:31:06 GMT"}, {"version": "v4", "created": "Sun, 13 Sep 2009 17:16:39 GMT"}, {"version": "v5", "created": "Tue, 9 Feb 2010 13:08:53 GMT"}], "update_date": "2010-02-09", "authors_parsed": [["Pritchard", "David", ""], ["Chakrabarty", "Deeparnab", ""]]}, {"id": "0904.1002", "submitter": "Wolfgang Bein", "authors": "Wolfgang Bein, Leah Epstein, Lawrence L. Larmore, John Noga", "title": "A Program to Determine the Exact Competitive Ratio of List s-Batching\n  with Unit Jobs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online list s-batch problem, where all the jobs have\nprocessing time 1 and we seek to minimize the sum of the completion times of\nthe jobs. We give a Java program which is used to verify that the\ncompetitiveness of this problem is 619/583.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2009 20:08:48 GMT"}], "update_date": "2009-04-08", "authors_parsed": [["Bein", "Wolfgang", ""], ["Epstein", "Leah", ""], ["Larmore", "Lawrence L.", ""], ["Noga", "John", ""]]}, {"id": "0904.1113", "submitter": "Bodo Manthey", "authors": "David Arthur and Bodo Manthey and Heiko R\\\"oglin", "title": "k-Means has Polynomial Smoothed Complexity", "comments": "Full version of FOCS 2009 paper. The argument has been improved and\n  the restriction to at least three dimensions could be dropped", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-means method is one of the most widely used clustering algorithms,\ndrawing its popularity from its speed in practice. Recently, however, it was\nshown to have exponential worst-case running time. In order to close the gap\nbetween practical performance and theoretical analysis, the k-means method has\nbeen studied in the model of smoothed analysis. But even the smoothed analyses\nso far are unsatisfactory as the bounds are still super-polynomial in the\nnumber n of data points.\n  In this paper, we settle the smoothed running time of the k-means method. We\nshow that the smoothed number of iterations is bounded by a polynomial in n and\n1/\\sigma, where \\sigma is the standard deviation of the Gaussian perturbations.\nThis means that if an arbitrary input data set is randomly perturbed, then the\nk-means method will run in expected polynomial time on that input set.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2009 11:21:23 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2009 08:53:03 GMT"}], "update_date": "2009-08-07", "authors_parsed": [["Arthur", "David", ""], ["Manthey", "Bodo", ""], ["R\u00f6glin", "Heiko", ""]]}, {"id": "0904.1242", "submitter": "Kang Ning", "authors": "Kang Ning and Hon Wai Leong", "title": "The Distribution and Deposition Algorithm for Multiple Sequences Sets", "comments": "15 pages, 7 figures, extended version of conference paper presented\n  on GIW 2006, revised version accepted by Journal of Combinatorial\n  Optimization.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequences set is a mathematical model used in many applications. As the\nnumber of the sequences becomes larger, single sequence set model is not\nappropriate for the rapidly increasing problem sizes. For example, more and\nmore text processing applications separate a single big text file into multiple\nfiles before processing. For these applications, the underline mathematical\nmodel is multiple sequences sets (MSS). Though there is increasing use of MSS,\nthere is little research on how to process MSS efficiently. To process multiple\nsequences sets, sequences are first distributed to different sets, and then\nsequences for each set are processed. Deriving effective algorithm for MSS\nprocessing is both interesting and challenging. In this paper, we have defined\nthe cost functions and performance ratio for analysis of the quality of\nsynthesis sequences. Based on these, the problem of Process of Multiple\nSequences Sets (PMSS) is formulated. We have first proposed two greedy\nalgorithms for the PMSS problem, which are based on generalization of\nalgorithms for single sequences set. Then based on the analysis of the\ncharacteristics of multiple sequences sets, we have proposed the Distribution\nand Deposition (DDA) algorithm and DDA* algorithm for PMSS problem. In DDA\nalgorithm, the sequences are first distributed to multiple sets according to\ntheir alphabet contents; then sequences in each set are deposited by the\ndeposition algorithm. The DDA* algorithm differs from the DDA algorithm in that\nthe DDA* algorithm distributes sequences by clustering based on sequence\nprofiles. Experiments show that DDA and DDA* always output results with smaller\ncosts than other algorithms, and DDA* outperforms DDA in most instances. The\nDDA and DDA* algorithms are also efficient both in time and space.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2009 23:14:06 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2009 03:28:21 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2010 23:02:08 GMT"}, {"version": "v4", "created": "Fri, 30 Apr 2010 02:29:55 GMT"}], "update_date": "2010-05-03", "authors_parsed": [["Ning", "Kang", ""], ["Leong", "Hon Wai", ""]]}, {"id": "0904.1284", "submitter": "Manabu Inuma", "authors": "Manabu Inuma, Akira Otsuka, Hideki Imai", "title": "Theoretical framework for constructing matching algorithms in biometric\n  authentication systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a theoretical framework to construct matching\nalgorithms for any biometric authentication systems. Conventional matching\nalgorithms are not necessarily secure against strong intentional impersonation\nattacks such as wolf attacks. The wolf attack is an attempt to impersonate a\ngenuine user by presenting a \"wolf\" to a biometric authentication system\nwithout the knowledge of a genuine user's biometric sample. A wolf is a sample\nwhich can be accepted as a match with multiple templates. The wolf attack\nprobability (WAP) is the maximum success probability of the wolf attack, which\nwas proposed by Une, Otsuka, Imai as a measure for evaluating security of\nbiometric authentication systems. We present a principle for construction of\nsecure matching algorithms against the wolf attack for any biometric\nauthentication systems. The ideal matching algorithm determines a threshold for\neach input value depending on the entropy of the probability distribution of\nthe (Hamming) distances. Then we show that if the information about the\nprobability distribution for each input value is perfectly given, then our\nmatching algorithm is secure against the wolf attack. Our generalized matching\nalgorithm gives a theoretical framework to construct secure matching\nalgorithms. How lower WAP is achievable depends on how accurately the entropy\nis estimated. Then there is a trade-off between the efficiency and the\nachievable WAP. Almost every conventional matching algorithm employs a fixed\nthreshold and hence it can be regarded as an efficient but insecure instance of\nour theoretical framework. Daugman's IrisCode recognition algorithm proposed\ncan also be regarded as a non-optimal instance of our framework.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2009 08:48:30 GMT"}], "update_date": "2009-04-09", "authors_parsed": [["Inuma", "Manabu", ""], ["Otsuka", "Akira", ""], ["Imai", "Hideki", ""]]}, {"id": "0904.1366", "submitter": "Jian Li", "authors": "Jian Li, Barna Saha, Amol Deshpande", "title": "A Unified Approach to Ranking in Probabilistic Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dramatic growth in the number of application domains that naturally\ngenerate probabilistic, uncertain data has resulted in a need for efficiently\nsupporting complex querying and decision-making over such data. In this paper,\nwe present a unified approach to ranking and top-k query processing in\nprobabilistic databases by viewing it as a multi-criteria optimization problem,\nand by deriving a set of features that capture the key properties of a\nprobabilistic dataset that dictate the ranked result. We contend that a single,\nspecific ranking function may not suffice for probabilistic databases, and we\ninstead propose two parameterized ranking functions, called PRF-w and PRF-e,\nthat generalize or can approximate many of the previously proposed ranking\nfunctions. We present novel generating functions-based algorithms for\nefficiently ranking large datasets according to these ranking functions, even\nif the datasets exhibit complex correlations modeled using probabilistic\nand/xor trees or Markov networks. We further propose that the parameters of the\nranking function be learned from user preferences, and we develop an approach\nto learn those parameters. Finally, we present a comprehensive experimental\nstudy that illustrates the effectiveness of our parameterized ranking\nfunctions, especially PRF-e, at approximating other ranking functions and the\nscalability of our proposed algorithms for exact or approximate ranking.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2009 15:30:58 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2009 04:42:10 GMT"}, {"version": "v3", "created": "Tue, 14 Dec 2010 19:25:42 GMT"}, {"version": "v4", "created": "Wed, 15 Dec 2010 21:12:37 GMT"}], "update_date": "2010-12-17", "authors_parsed": [["Li", "Jian", ""], ["Saha", "Barna", ""], ["Deshpande", "Amol", ""]]}, {"id": "0904.1630", "submitter": "Aaron Sterling", "authors": "Aaron Sterling", "title": "Self-Assembly of a Statistically Self-Similar Fractal", "comments": "I am withdrawing all work I would like to polish before resubmitting,\n  including this paper. Several typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate existence of a tile assembly system that self-assembles the\nstatistically self-similar Sierpinski Triangle in the Winfree-Rothemund Tile\nAssembly Model. This appears to be the first paper that considers self-assembly\nof a random fractal, instead of a deterministic fractal or a finite, bounded\nshape. Our technical contributions include a way to remember, and use,\nunboundedly-long prefixes of an infinite coding sequence at each stage of\nfractal construction; a tile assembly mechanism for nested recursion; and a\ndefinition of \"almost-everywhere local determinism,\" to describe a tileset\nwhose assembly is locally determined, conditional upon a zeta-dimension zero\nset of (infinitely many) \"input\" tiles. This last is similar to the definition\nof randomized computation for Turing machines, in which an algorithm is\ndeterministic relative to an oracle sequence of coin flips that provides advice\nbut does not itself compute. Keywords: tile self-assembly, statistically\nself-similar Sierpinski Triangle.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2009 03:19:04 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2009 15:58:20 GMT"}, {"version": "v3", "created": "Wed, 20 Jul 2011 14:00:06 GMT"}], "update_date": "2011-07-21", "authors_parsed": [["Sterling", "Aaron", ""]]}, {"id": "0904.1645", "submitter": "Cedric Chauve", "authors": "Cedric Chauve, A\\\"ida Ouangraoua (LaBRI)", "title": "A 3-approximation algorithm for computing a parsimonious first\n  speciation in the gene duplication model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem: from a given set of gene families trees on\na set of genomes, find a first speciation, that splits these genomes into two\nsubsets, that minimizes the number of gene duplications that happened before\nthis speciation. We call this problem the Minimum Duplication Bipartition\nProblem. Using a generalization of the Minimum Edge-Cut Problem, known as\nSubmodular Function Minimization, we propose a polynomial time and space\n3-approximation algorithm for the Minimum Duplication Bipartition Problem.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2009 06:28:07 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2009 06:29:22 GMT"}], "update_date": "2009-04-15", "authors_parsed": [["Chauve", "Cedric", "", "LaBRI"], ["Ouangraoua", "A\u00efda", "", "LaBRI"]]}, {"id": "0904.1705", "submitter": "Giorgio Lucarelli", "authors": "Evripidis Bampis, Alexander Kononov, Giorgio Lucarelli, Ioannis Milis", "title": "Bounded Max-Colorings of Graphs", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a bounded max-coloring of a vertex/edge weighted graph, each color class\nis of cardinality at most $b$ and of weight equal to the weight of the heaviest\nvertex/edge in this class. The bounded max-vertex/edge-coloring problems ask\nfor such a coloring minimizing the sum of all color classes' weights.\n  In this paper we present complexity results and approximation algorithms for\nthose problems on general graphs, bipartite graphs and trees. We first show\nthat both problems are polynomial for trees, when the number of colors is\nfixed, and $H_b$ approximable for general graphs, when the bound $b$ is fixed.\nFor the bounded max-vertex-coloring problem, we show a 17/11-approximation\nalgorithm for bipartite graphs, a PTAS for trees as well as for bipartite\ngraphs when $b$ is fixed. For unit weights, we show that the known 4/3 lower\nbound for bipartite graphs is tight by providing a simple 4/3 approximation\nalgorithm. For the bounded max-edge-coloring problem, we prove approximation\nfactors of $3-2/\\sqrt{2b}$, for general graphs, $\\min\\{e, 3-2/\\sqrt{b}\\}$, for\nbipartite graphs, and 2, for trees. Furthermore, we show that this problem is\nNP-complete even for trees. This is the first complexity result for\nmax-coloring problems on trees.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2009 15:36:02 GMT"}], "update_date": "2009-04-13", "authors_parsed": [["Bampis", "Evripidis", ""], ["Kononov", "Alexander", ""], ["Lucarelli", "Giorgio", ""], ["Milis", "Ioannis", ""]]}, {"id": "0904.1920", "submitter": "Zhilin Wu", "authors": "Zhilin Wu (CASIA Liama), Stephane Grumbach (INRIA Liama)", "title": "Feasibility of Motion Planning on Acyclic and Strongly Connected\n  Directed Graphs", "comments": "19 pages, 9 figures, algorithm2e.sty", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion planning is a fundamental problem of robotics with applications in\nmany areas of computer science and beyond. Its restriction to graphs has been\ninvestigated in the literature for it allows to concentrate on the\ncombinatorial problem abstracting from geometric considerations. In this paper,\nwe consider motion planning over directed graphs, which are of interest for\nasymmetric communication networks. Directed graphs generalize undirected\ngraphs, while introducing a new source of complexity to the motion planning\nproblem: moves are not reversible. We first consider the class of acyclic\ndirected graphs and show that the feasibility can be solved in time linear in\nthe product of the number of vertices and the number of arcs. We then turn to\nstrongly connected directed graphs. We first prove a structural theorem for\ndecomposing strongly connected directed graphs into strongly biconnected\ncomponents.Based on the structural decomposition, we give an algorithm for the\nfeasibility of motion planning on strongly connected directed graphs, and show\nthat it can also be decided in time linear in the product of the number of\nvertices and the number of arcs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2009 07:32:43 GMT"}], "update_date": "2009-04-14", "authors_parsed": [["Wu", "Zhilin", "", "CASIA Liama"], ["Grumbach", "Stephane", "", "INRIA Liama"]]}, {"id": "0904.2027", "submitter": "Jelani Nelson", "authors": "Jelani Nelson, David P. Woodruff", "title": "A Near-Optimal Algorithm for L1-Difference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first L_1-sketching algorithm for integer vectors which produces\nnearly optimal sized sketches in nearly linear time. This answers the first\nopen problem in the list of open problems from the 2006 IITK Workshop on\nAlgorithms for Data Streams. Specifically, suppose Alice receives a vector x in\n{-M,...,M}^n and Bob receives y in {-M,...,M}^n, and the two parties share\nrandomness. Each party must output a short sketch of their vector such that a\nthird party can later quickly recover a (1 +/- eps)-approximation to ||x-y||_1\nwith 2/3 probability given only the sketches. We give a sketching algorithm\nwhich produces O(eps^{-2}log(1/eps)log(nM))-bit sketches in O(n*log^2(nM))\ntime, independent of eps. The previous best known sketching algorithm for L_1\nis due to [Feigenbaum et al., SICOMP 2002], which achieved the optimal sketch\nlength of O(eps^{-2}log(nM)) bits but had a running time of O(n*log(nM)/eps^2).\nNotice that our running time is near-linear for every eps, whereas for\nsufficiently small values of eps, the running time of the previous algorithm\ncan be as large as quadratic. Like their algorithm, our sketching procedure\nalso yields a small-space, one-pass streaming algorithm which works even if the\nentries of x,y are given in arbitrary order.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2009 22:54:26 GMT"}], "update_date": "2009-04-15", "authors_parsed": [["Nelson", "Jelani", ""], ["Woodruff", "David P.", ""]]}, {"id": "0904.2058", "submitter": "Chandan Saha", "authors": "Chandan Saha, Ramprasad Saptharishi, Nitin Saxena", "title": "The Power of Depth 2 Circuits over Algebras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of polynomial identity testing (PIT) for depth 2\narithmetic circuits over matrix algebra. We show that identity testing of depth\n3 (Sigma-Pi-Sigma) arithmetic circuits over a field F is polynomial time\nequivalent to identity testing of depth 2 (Pi-Sigma) arithmetic circuits over\nU_2(F), the algebra of upper-triangular 2 x 2 matrices with entries from F.\nSuch a connection is a bit surprising since we also show that, as computational\nmodels, Pi-Sigma circuits over U_2(F) are strictly `weaker' than Sigma-Pi-Sigma\ncircuits over F.\n  The equivalence further shows that PIT of depth 3 arithmetic circuits reduces\nto PIT of width-2 planar commutative Algebraic Branching Programs (ABP). Thus,\nidentity testing for commutative ABPs is interesting even in the case of\nwidth-2.\n  Further, we give a deterministic polynomial time identity testing algorithm\nfor a Pi-Sigma circuit over any constant dimensional commutative algebra over\nF. While over commutative algebras of polynomial dimension, identity testing is\nat least as hard as that of Sigma-Pi-Sigma circuits over F.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2009 06:36:37 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Saha", "Chandan", ""], ["Saptharishi", "Ramprasad", ""], ["Saxena", "Nitin", ""]]}, {"id": "0904.2129", "submitter": "Tamara Mchedlidze David", "authors": "Tamara Mchedlidze, Antonios Symvonis", "title": "Crossing-Optimal Acyclic HP-Completion for Outerplanar st-Digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an embedded planar acyclic digraph G, we define the problem of acyclic\nhamiltonian path completion with crossing minimization (Acyclic-HPCCM) to be\nthe problem of determining a hamiltonian path completion set of edges such\nthat, when these edges are embedded on G, they create the smallest possible\nnumber of edge crossings and turn G to a hamiltonian acyclic digraph. Our\nresults include: 1. We provide a characterization under which a planar\nst-digraph G is hamiltonian. 2. For an outerplanar st-digraph G, we define the\nst-polygon decomposition of G and, based on its properties, we develop a\nlinear-time algorithm that solves the Acyclic-HPCCM problem. 3. For the class\nof planar st-digraphs, we establish an equivalence between the Acyclic-HPCCM\nproblem and the problem of determining an upward 2-page topological book\nembedding with minimum number of spine crossings. We infer (based on this\nequivalence) for the class of outerplanar st-digraphs an upward topological\n2-page book embedding with minimum number of spine crossings. To the best of\nour knowledge, it is the first time that edge-crossing minimization is studied\nin conjunction with the acyclic hamiltonian completion problem and the first\ntime that an optimal algorithm with respect to spine crossing minimization is\npresented for upward topological book embeddings.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2009 14:29:56 GMT"}], "update_date": "2009-04-15", "authors_parsed": [["Mchedlidze", "Tamara", ""], ["Symvonis", "Antonios", ""]]}, {"id": "0904.2203", "submitter": "Imran Pirwani", "authors": "Imran A. Pirwani (1), Mohammad R. Salavatipour (1) ((1) Department of\n  Computing Science, University of Alberta, Edmonton, Canada)", "title": "A Weakly-Robust PTAS for Minimum Clique Partition in Unit Disk Graphs", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": "10.1007/978-3-642-13731-0_19", "report-no": null, "categories": "cs.CG cs.DC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of partitioning the set of vertices of a given unit\ndisk graph (UDG) into a minimum number of cliques. The problem is NP-hard and\nvarious constant factor approximations are known, with the current best ratio\nof 3. Our main result is a {\\em weakly robust} polynomial time approximation\nscheme (PTAS) for UDGs expressed with edge-lengths, it either (i) computes a\nclique partition or (ii) gives a certificate that the graph is not a UDG; for\nthe case (i) that it computes a clique partition, we show that it is guaranteed\nto be within $(1+\\eps)$ ratio of the optimum if the input is UDG; however if\nthe input is not a UDG it either computes a clique partition as in case (i)\nwith no guarantee on the quality of the clique partition or detects that it is\nnot a UDG. Noting that recognition of UDG's is NP-hard even if we are given\nedge lengths, our PTAS is a weakly-robust algorithm. Our algorithm can be\ntransformed into an $O(\\frac{\\log^* n}{\\eps^{O(1)}})$ time distributed PTAS.\n  We consider a weighted version of the clique partition problem on vertex\nweighted UDGs that generalizes the problem. We note some key distinctions with\nthe unweighted version, where ideas useful in obtaining a PTAS breakdown. Yet,\nsurprisingly, it admits a $(2+\\eps)$-approximation algorithm for the weighted\ncase where the graph is expressed, say, as an adjacency matrix. This improves\non the best known 8-approximation for the {\\em unweighted} case for UDGs\nexpressed in standard form.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2009 20:44:57 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2009 22:50:13 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Pirwani", "Imran A.", ""], ["Salavatipour", "Mohammad R.", ""]]}, {"id": "0904.2310", "submitter": "Marek Karpinski", "authors": "Piotr Berman, Marek Karpinski, Andrzej Lingas", "title": "Exact and Approximation Algorithms for Geometric and Capacitated Set\n  Cover Problems with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First, we study geometric variants of the standard set cover motivated by\nassignment of directional antenna and shipping with deadlines, providing the\nfirst known polynomial-time exact solutions. Next, we consider the following\ngeneral capacitated set cover problem. There is given a set of elements with\nreal weights and a family S of sets of elements. One can use a set if it is a\nsubset of one of the sets on our lists and the sum of weights is at most one.\nThe goal is to cover all the elements with the allowed sets.<br>We show that\nany polynomial-time algorithm that approximates the un-capacitated version of\nthe set cover problem with ratio r can be converted to an approximation\nalgorithm for the capacitated version with ratio r + 1.357.In particular, the\ncomposition of these two results yields a polynomial-time approximation\nalgorithm for the problem of covering a set of customers represented by a\nweighted n-point set with a minimum number of antennas of variable angular\nrange and fixed capacity with ratio 2.357. Finally, we provide a PTAS for the\ndual problem where the number of sets (e.g., antennas) to use is fixed and the\ntask is to minimize the maximum set load, in case the sets correspond to line\nintervals or arcs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2009 13:11:33 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Berman", "Piotr", ""], ["Karpinski", "Marek", ""], ["Lingas", "Andrzej", ""]]}, {"id": "0904.2400", "submitter": "Patrick Briest", "authors": "Patrick Briest, Shuchi Chawla, Robert Kleinberg, and S. Matthew\n  Weinberg", "title": "Pricing Randomized Allocations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized mechanisms, which map a set of bids to a probability distribution\nover outcomes rather than a single outcome, are an important but ill-understood\narea of computational mechanism design. We investigate the role of randomized\noutcomes (henceforth, \"lotteries\") in the context of a fundamental and\narchetypical multi-parameter mechanism design problem: selling heterogeneous\nitems to unit-demand bidders. To what extent can a seller improve her revenue\nby pricing lotteries rather than items, and does this modification of the\nproblem affect its computational tractability? Our results show that the\nanswers to these questions hinge on whether consumers can purchase only one\nlottery (the buy-one model) or purchase any set of lotteries and receive an\nindependent sample from each (the buy-many model). In the buy-one model, there\nis a polynomial-time algorithm to compute the revenue-maximizing envy-free\nprices (thus overcoming the inapproximability of the corresponding item pricing\nproblem) and the revenue of the optimal lottery system can exceed the revenue\nof the optimal item pricing by an unbounded factor as long as the number of\nitem types exceeds 4. In the buy-many model with n item types, the profit\nachieved by lottery pricing can exceed item pricing by a factor of O(log n) but\nnot more, and optimal lottery pricing cannot be approximated within a factor of\nO(n^eps) for some eps>0, unless NP has subexponential-time randomized\nalgorithms. Our lower bounds rely on a mixture of geometric and algebraic\ntechniques, whereas the upper bounds use a novel rounding scheme to transform a\nmechanism with randomized outcomes into one with deterministic outcomes while\nlosing only a bounded amount of revenue.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2009 16:30:59 GMT"}], "update_date": "2009-04-17", "authors_parsed": [["Briest", "Patrick", ""], ["Chawla", "Shuchi", ""], ["Kleinberg", "Robert", ""], ["Weinberg", "S. Matthew", ""]]}, {"id": "0904.2576", "submitter": "Anna Adamaszek", "authors": "Anna Adamaszek, Artur Czumaj, Andrzej Lingas", "title": "PTAS for k-tour cover problem on the plane for moderately large values\n  of k", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let P be a set of n points in the Euclidean plane and let O be the origin\npoint in the plane. In the k-tour cover problem (called frequently the\ncapacitated vehicle routing problem), the goal is to minimize the total length\nof tours that cover all points in P, such that each tour starts and ends in O\nand covers at most k points from P.\n  The k-tour cover problem is known to be NP-hard. It is also known to admit\nconstant factor approximation algorithms for all values of k and even a\npolynomial-time approximation scheme (PTAS) for small values of k, i.e.,\nk=O(log n / log log n).\n  We significantly enlarge the set of values of k for which a PTAS is provable.\nWe present a new PTAS for all values of k <= 2^{log^{\\delta}n}, where \\delta =\n\\delta(\\epsilon). The main technical result proved in the paper is a novel\nreduction of the k-tour cover problem with a set of n points to a small set of\ninstances of the problem, each with O((k/\\epsilon)^O(1)) points.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2009 20:12:33 GMT"}], "update_date": "2009-04-20", "authors_parsed": [["Adamaszek", "Anna", ""], ["Czumaj", "Artur", ""], ["Lingas", "Andrzej", ""]]}, {"id": "0904.2712", "submitter": "Mingyu Xiao", "authors": "Mingyu Xiao", "title": "New Branching Rules: Improvements on Independent Set and Vertex Cover in\n  Sparse Graphs", "comments": "The paper was presented at the 2nd annual meeting of asian\n  association for algorithms and computation (AAAC 2009), April 11-12, 2009,\n  Hangzhou, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an $O^*(1.0919^n)$-time algorithm for finding a maximum\nindependent set in an $n$-vertex graph with degree bounded by 3, which improves\nthe previously known algorithm of running time $O^*(1.0977^n)$ by Bourgeois,\nEscoffier and Paschos [IWPEC 2008]. We also present an $O^*(1.1923^k)$-time\nalgorithm to decide if a graph with degree bounded by 3 has a vertex cover of\nsize $k$, which improves the previously known algorithm of running time\n$O^*(1.1939^k)$ by Chen, Kanj and Xia [ISAAC 2003].\n  Two new branching techniques, \\emph{branching on a bottle} and\n\\emph{branching on a 4-cycle}, are introduced, which help us to design simple\nand fast algorithms for the maximum independent set and minimum vertex cover\nproblems and avoid tedious branching rules.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2009 14:19:12 GMT"}], "update_date": "2009-04-20", "authors_parsed": [["Xiao", "Mingyu", ""]]}, {"id": "0904.2728", "submitter": "Cl\\'emence Magnien", "authors": "Clemence Magnien (1), Matthieu Latapy (1) and Michel Habib (2) ((1)\n  LIP6 (CNRS - UPMC), (2) LIAFA (CNRS - Universite Paris Diderot))", "title": "Fast Computation of Empirically Tight Bounds for the Diameter of Massive\n  Graphs", "comments": "ACM Journal of Experimental Algorithmics (JEA), 13, 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diameter of a graph is among its most basic parameters. Since a few\nyears, it moreover became a key issue to compute it for massive graphs in the\ncontext of complex network analysis. However, known algorithms, including the\nones producing approximate values, have too high a time and/or space complexity\nto be used in such cases. We propose here a new approach relying on very simple\nand fast algorithms that compute (upper and lower) bounds for the diameter. We\nshow empirically that, on various real-world cases representative of complex\nnetworks studied in the literature, the obtained bounds are very tight (and\neven equal in some cases). This leads to rigorous and very accurate estimations\nof the actual diameter in cases which were previously untractable in practice.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2009 15:44:49 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Magnien", "Clemence", ""], ["Latapy", "Matthieu", ""], ["Habib", "Michel", ""]]}, {"id": "0904.2785", "submitter": "Daniel Kral", "authors": "Daniel Kral", "title": "Decomposition width - a new width parameter for matroids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new width parameter for matroids called decomposition width\nand prove that every matroid property expressible in the monadic second order\nlogic can be computed in linear time for matroids with bounded decomposition\nwidth if their decomposition is given. Since decompositions of small width for\nour new notion can be computed in polynomial time for matroids of bounded\nbranch-width represented over finite fields, our results include recent\nalgorithmic results of Hlineny [J. Combin. Theory Ser. B 96 (2006), 325-351] in\nthis area and extend his results to matroids not necessarily representable over\nfinite fields.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2009 21:09:35 GMT"}], "update_date": "2009-04-21", "authors_parsed": [["Kral", "Daniel", ""]]}, {"id": "0904.3060", "submitter": "Yingyu Zhang", "authors": "Heping Hu, Yingyu Zhang, Zhengding Lu", "title": "An efficient quantum search engine on unsorted database", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding one or more desired items out of an\nunsorted database. Patel has shown that if the database permits quantum\nqueries, then mere digitization is sufficient for efficient search for one\ndesired item. The algorithm, called factorized quantum search algorithm,\npresented by him can locate the desired item in an unsorted database using\n$O(log_{4}N)$ queries to factorized oracles. But the algorithm requires that\nall the property values must be distinct from each other. In this paper, we\ndiscuss how to make a database satisfy the requirements, and present a quantum\nsearch engine based on the algorithm. Our goal is achieved by introducing\nauxiliary files for the property values that are not distinct, and converting\nevery complex query request into a sequence of calls to factorized quantum\nsearch algorithm. The query complexity of our algorithm is $O(P*Q*M*log_{4}N)$,\nwhere P is the number of the potential simple query requests in the complex\nquery request, Q is the maximum number of calls to the factorized quantum\nsearch algorithm of the simple queries, M is the number of the auxiliary files\nfor the property on which our algorithm are searching for desired items. This\nimplies that to manage an unsorted database on an actual quantum computer is\npossible and efficient.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2009 15:43:06 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Hu", "Heping", ""], ["Zhang", "Yingyu", ""], ["Lu", "Zhengding", ""]]}, {"id": "0904.3062", "submitter": "Mikl\\'os Cs\\H{u}r\\\"os", "authors": "Miklos Csuros", "title": "Approximate counting with a floating-point counter", "comments": "Updated content (fixed errors in the previous version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory becomes a limiting factor in contemporary applications, such as\nanalyses of the Webgraph and molecular sequences, when many objects need to be\ncounted simultaneously. Robert Morris [Communications of the ACM, 21:840--842,\n1978] proposed a probabilistic technique for approximate counting that is\nextremely space-efficient. The basic idea is to increment a counter containing\nthe value $X$ with probability $2^{-X}$. As a result, the counter contains an\napproximation of $\\lg n$ after $n$ probabilistic updates stored in $\\lg\\lg n$\nbits. Here we revisit the original idea of Morris, and introduce a binary\nfloating-point counter that uses a $d$-bit significand in conjunction with a\nbinary exponent. The counter yields a simple formula for an unbiased estimation\nof $n$ with a standard deviation of about $0.6\\cdot n2^{-d/2}$, and uses\n$d+\\lg\\lg n$ bits.\n  We analyze the floating-point counter's performance in a general framework\nthat applies to any probabilistic counter, and derive practical formulas to\nassess its accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2009 15:53:33 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2009 05:10:03 GMT"}], "update_date": "2009-08-24", "authors_parsed": [["Csuros", "Miklos", ""]]}, {"id": "0904.3093", "submitter": "Petteri Kaski", "authors": "Andreas Bj\\\"orklund and Thore Husfeldt and Petteri Kaski and Mikko\n  Koivisto", "title": "Counting Paths and Packings in Halves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that one can count $k$-edge paths in an $n$-vertex graph and\n$m$-set $k$-packings on an $n$-element universe, respectively, in time ${n\n\\choose k/2}$ and ${n \\choose mk/2}$, up to a factor polynomial in $n$, $k$,\nand $m$; in polynomial space, the bounds hold if multiplied by $3^{k/2}$ or\n$5^{mk/2}$, respectively. These are implications of a more general result:\ngiven two set families on an $n$-element universe, one can count the disjoint\npairs of sets in the Cartesian product of the two families with $\\nO(n \\ell)$\nbasic operations, where $\\ell$ is the number of members in the two families and\ntheir subsets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2009 19:46:39 GMT"}], "update_date": "2009-04-21", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""], ["Husfeldt", "Thore", ""], ["Kaski", "Petteri", ""], ["Koivisto", "Mikko", ""]]}, {"id": "0904.3151", "submitter": "Takeaki Uno", "authors": "Takeaki Uno, Masashi Sugiyama, Koji Tsuda", "title": "Efficient Construction of Neighborhood Graphs by the Multiple Sorting\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neighborhood graphs are gaining popularity as a concise data representation\nin machine learning. However, naive graph construction by pairwise distance\ncalculation takes $O(n^2)$ runtime for $n$ data points and this is\nprohibitively slow for millions of data points. For strings of equal length,\nthe multiple sorting method (Uno, 2008) can construct an $\\epsilon$-neighbor\ngraph in $O(n+m)$ time, where $m$ is the number of $\\epsilon$-neighbor pairs in\nthe data. To introduce this remarkably efficient algorithm to continuous\ndomains such as images, signals and texts, we employ a random projection method\nto convert vectors to strings. Theoretical results are presented to elucidate\nthe trade-off between approximation quality and computation time. Empirical\nresults show the efficiency of our method in comparison to fast nearest\nneighbor alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2009 01:03:06 GMT"}], "update_date": "2009-04-22", "authors_parsed": [["Uno", "Takeaki", ""], ["Sugiyama", "Masashi", ""], ["Tsuda", "Koji", ""]]}, {"id": "0904.3169", "submitter": "Christoph Durr", "authors": "Christoph Durr and Flavio Guinez and Martin Matamala", "title": "Reconstructing 3-colored grids from horizontal and vertical projections\n  is NP-hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of coloring a grid using k colors with the\nrestriction that in each row and each column has an specific number of cells of\neach color. In an already classical result, Ryser obtained a necessary and\nsufficient condition for the existence of such a coloring when two colors are\nconsidered. This characterization yields a linear time algorithm for\nconstructing such a coloring when it exists. Gardner et al. showed that for\nk>=7 the problem is NP-hard. Afterward Chrobak and Durr improved this result,\nby proving that it remains NP-hard for k>=4. We solve the gap by showing that\nfor 3 colors the problem is already NP-hard. Besides we also give some results\non tiling tomography problems.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2009 08:55:44 GMT"}], "update_date": "2009-04-22", "authors_parsed": [["Durr", "Christoph", ""], ["Guinez", "Flavio", ""], ["Matamala", "Martin", ""]]}, {"id": "0904.3183", "submitter": "Fredrik Kuivinen", "authors": "Fredrik Kuivinen", "title": "On the Complexity of Submodular Function Minimisation on Diamonds", "comments": "31 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $(L; \\sqcap, \\sqcup)$ be a finite lattice and let $n$ be a positive\ninteger. A function $f : L^n \\to \\mathbb{R}$ is said to be submodular if\n$f(\\tup{a} \\sqcap \\tup{b}) + f(\\tup{a} \\sqcup \\tup{b}) \\leq f(\\tup{a}) +\nf(\\tup{b})$ for all $\\tup{a}, \\tup{b} \\in L^n$. In this paper we study\nsubmodular functions when $L$ is a diamond. Given oracle access to $f$ we are\ninterested in finding $\\tup{x} \\in L^n$ such that $f(\\tup{x}) = \\min_{\\tup{y}\n\\in L^n} f(\\tup{y})$ as efficiently as possible.\n  We establish a min--max theorem, which states that the minimum of the\nsubmodular function is equal to the maximum of a certain function defined over\na certain polyhedron; and a good characterisation of the minimisation problem,\ni.e., we show that given an oracle for computing a submodular $f : L^n \\to\n\\mathbb{Z}$ and an integer $m$ such that $\\min_{\\tup{x} \\in L^n} f(\\tup{x}) =\nm$, there is a proof of this fact which can be verified in time polynomial in\n$n$ and $\\max_{\\tup{t} \\in L^n} \\log |f(\\tup{t})|$; and a pseudo-polynomial\ntime algorithm for the minimisation problem, i.e., given an oracle for\ncomputing a submodular $f : L^n \\to \\mathbb{Z}$ one can find $\\min_{\\tup{t} \\in\nL^n} f(\\tup{t})$ in time bounded by a polynomial in $n$ and $\\max_{\\tup{t} \\in\nL^n} |f(\\tup{t})|$.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2009 12:41:07 GMT"}], "update_date": "2009-04-22", "authors_parsed": [["Kuivinen", "Fredrik", ""]]}, {"id": "0904.3251", "submitter": "Petteri Kaski", "authors": "Andreas Bj\\\"orklund and Thore Husfeldt and Petteri Kaski and Mikko\n  Koivisto", "title": "On evaluation of permanents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the time and space complexity of matrix permanents over rings and\nsemirings.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2009 13:37:21 GMT"}], "update_date": "2009-04-22", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""], ["Husfeldt", "Thore", ""], ["Kaski", "Petteri", ""], ["Koivisto", "Mikko", ""]]}, {"id": "0904.3310", "submitter": "Shariq Bashir Mr.", "authors": "Shariq Bashir, Abdul Rauf Baig", "title": "FastLMFI: An Efficient Approach for Local Maximal Patterns Propagation\n  and Maximal Patterns Superset Checking", "comments": "8 Pages, In the proceedings of 4th ACS/IEEE International Conference\n  on Computer Systems and Applications 2006, March 8, 2006, Dubai/Sharjah, UAE,\n  2006, Page(s) 452-459", "journal-ref": null, "doi": "10.1109/AICCSA.2006.205130", "report-no": null, "categories": "cs.DB cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximal frequent patterns superset checking plays an important role in the\nefficient mining of complete Maximal Frequent Itemsets (MFI) and maximal search\nspace pruning. In this paper we present a new indexing approach, FastLMFI for\nlocal maximal frequent patterns (itemset) propagation and maximal patterns\nsuperset checking. Experimental results on different sparse and dense datasets\nshow that our work is better than the previous well known progressive focusing\ntechnique. We have also integrated our superset checking approach with an\nexisting state of the art maximal itemsets algorithm Mafia, and compare our\nresults with current best maximal itemsets algorithms afopt-max and FP\n(zhu)-max. Our results outperform afopt-max and FP (zhu)-max on dense (chess\nand mushroom) datasets on almost all support thresholds, which shows the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2009 18:33:04 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Bashir", "Shariq", ""], ["Baig", "Abdul Rauf", ""]]}, {"id": "0904.3312", "submitter": "Shariq Bashir Mr.", "authors": "Shariq Bashir, and Abdul Rauf Baig", "title": "HybridMiner: Mining Maximal Frequent Itemsets Using Hybrid Database\n  Representation Approach", "comments": "8 Pages In the proceedings of 9th IEEE-INMIC 2005, Karachi, Pakistan,\n  2005", "journal-ref": null, "doi": "10.1109/INMIC.2005.334484", "report-no": null, "categories": "cs.DB cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel hybrid (arraybased layout and vertical\nbitmap layout) database representation approach for mining complete Maximal\nFrequent Itemset (MFI) on sparse and large datasets. Our work is novel in terms\nof scalability, item search order and two horizontal and vertical projection\ntechniques. We also present a maximal algorithm using this hybrid database\nrepresentation approach. Different experimental results on real and sparse\nbenchmark datasets show that our approach is better than previous state of art\nmaximal algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2009 18:38:25 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Bashir", "Shariq", ""], ["Baig", "Abdul Rauf", ""]]}, {"id": "0904.3316", "submitter": "Shariq Bashir Mr.", "authors": "Shariq Bashir, and Abdul Rauf Baig", "title": "Ramp: Fast Frequent Itemset Mining with Efficient Bit-Vector Projection\n  Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining frequent itemset using bit-vector representation approach is very\nefficient for dense type datasets, but highly inefficient for sparse datasets\ndue to lack of any efficient bit-vector projection technique. In this paper we\npresent a novel efficient bit-vector projection technique, for sparse and dense\ndatasets. To check the efficiency of our bit-vector projection technique, we\npresent a new frequent itemset mining algorithm Ramp (Real Algorithm for Mining\nPatterns) build upon our bit-vector projection technique. The performance of\nthe Ramp is compared with the current best (all, maximal and closed) frequent\nitemset mining algorithms on benchmark datasets. Different experimental results\non sparse and dense datasets show that mining frequent itemset using Ramp is\nfaster than the current best algorithms, which show the effectiveness of our\nbit-vector projection idea. We also present a new local maximal frequent\nitemsets propagation and maximal itemset superset checking approach FastLMFI,\nbuild upon our PBR bit-vector projection technique. Our different computational\nexperiments suggest that itemset maximality checking using FastLMFI is fast and\nefficient than a previous will known progressive focusing approach.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2009 18:49:13 GMT"}], "update_date": "2009-04-22", "authors_parsed": [["Bashir", "Shariq", ""], ["Baig", "Abdul Rauf", ""]]}, {"id": "0904.3319", "submitter": "Shariq Bashir Mr.", "authors": "Shariq Bashir, Zahoor Jan, Abdul Rauf Baig", "title": "Fast Algorithms for Mining Interesting Frequent Itemsets without Minimum\n  Support", "comments": "25 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real world datasets are sparse, dirty and contain hundreds of items. In such\nsituations, discovering interesting rules (results) using traditional frequent\nitemset mining approach by specifying a user defined input support threshold is\nnot appropriate. Since without any domain knowledge, setting support threshold\nsmall or large can output nothing or a large number of redundant uninteresting\nresults. Recently a novel approach of mining only N-most/Top-K interesting\nfrequent itemsets has been proposed, which discovers the top N interesting\nresults without specifying any user defined support threshold. However, mining\ninteresting frequent itemsets without minimum support threshold are more costly\nin terms of itemset search space exploration and processing cost. Thereby, the\nefficiency of their mining highly depends upon three main factors (1) Database\nrepresentation approach used for itemset frequency counting, (2) Projection of\nrelevant transactions to lower level nodes of search space and (3) Algorithm\nimplementation technique. Therefore, to improve the efficiency of mining\nprocess, in this paper we present two novel algorithms called (N-MostMiner and\nTop-K-Miner) using the bit-vector representation approach which is very\nefficient in terms of itemset frequency counting and transactions projection.\nIn addition to this, several efficient implementation techniques of N-MostMiner\nand Top-K-Miner are also present which we experienced in our implementation.\nOur experimental results on benchmark datasets suggest that the NMostMiner and\nTop-K-Miner are very efficient in terms of processing time as compared to\ncurrent best algorithms BOMO and TFP.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2009 19:07:35 GMT"}], "update_date": "2009-04-22", "authors_parsed": [["Bashir", "Shariq", ""], ["Jan", "Zahoor", ""], ["Baig", "Abdul Rauf", ""]]}, {"id": "0904.3320", "submitter": "Shariq Bashir Mr.", "authors": "Shariq Bashir, Saad Razzaq, Umer Maqbool, Sonya Tahir, Abdul Rauf Baig", "title": "Using Association Rules for Better Treatment of Missing Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of training data for knowledge discovery in databases (KDD) and\ndata mining depends upon many factors, but handling missing values is\nconsidered to be a crucial factor in overall data quality. Today real world\ndatasets contains missing values due to human, operational error, hardware\nmalfunctioning and many other factors. The quality of knowledge extracted,\nlearning and decision problems depend directly upon the quality of training\ndata. By considering the importance of handling missing values in KDD and data\nmining tasks, in this paper we propose a novel Hybrid Missing values Imputation\nTechnique (HMiT) using association rules mining and hybrid combination of\nk-nearest neighbor approach. To check the effectiveness of our HMiT missing\nvalues imputation technique, we also perform detail experimental results on\nreal world datasets. Our results suggest that the HMiT technique is not only\nbetter in term of accuracy but it also take less processing time as compared to\ncurrent best missing values imputation technique based on k-nearest neighbor\napproach, which shows the effectiveness of our missing values imputation\ntechnique.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2009 19:09:57 GMT"}], "update_date": "2009-04-22", "authors_parsed": [["Bashir", "Shariq", ""], ["Razzaq", "Saad", ""], ["Maqbool", "Umer", ""], ["Tahir", "Sonya", ""], ["Baig", "Abdul Rauf", ""]]}, {"id": "0904.3321", "submitter": "Shariq Bashir Mr.", "authors": "Shariq Bashir, Saad Razzaq, Umer Maqbool, Sonya Tahir, Abdul Rauf Baig", "title": "Introducing Partial Matching Approach in Association Rules for Better\n  Treatment of Missing Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handling missing values in training datasets for constructing learning models\nor extracting useful information is considered to be an important research task\nin data mining and knowledge discovery in databases. In recent years, lot of\ntechniques are proposed for imputing missing values by considering attribute\nrelationships with missing value observation and other observations of training\ndataset. The main deficiency of such techniques is that, they depend upon\nsingle approach and do not combine multiple approaches, that why they are less\naccurate. To improve the accuracy of missing values imputation, in this paper\nwe introduce a novel partial matching concept in association rules mining,\nwhich shows better results as compared to full matching concept that we\ndescribed in our previous work. Our imputation technique combines the partial\nmatching concept in association rules with k-nearest neighbor approach. Since\nthis is a hybrid technique, therefore its accuracy is much better than as\ncompared to those techniques which depend upon single approach. To check the\nefficiency of our technique, we also provide detail experimental results on\nnumber of benchmark datasets which show better results as compared to previous\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2009 19:16:00 GMT"}], "update_date": "2009-04-22", "authors_parsed": [["Bashir", "Shariq", ""], ["Razzaq", "Saad", ""], ["Maqbool", "Umer", ""], ["Tahir", "Sonya", ""], ["Baig", "Abdul Rauf", ""]]}, {"id": "0904.3351", "submitter": "Majid Fozunbal", "authors": "Majid Fozunbal", "title": "A Subsequence-Histogram Method for Generic Vocabulary Recognition over\n  Deletion Channels", "comments": "5 pages, International Symposium on Information Theory (ISIT) 2009", "journal-ref": null, "doi": null, "report-no": "HP Labs Technical Report HPL-2009-2", "categories": "cs.IT cs.DS math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recognizing a vocabulary--a collection of words\n(sequences) over a finite alphabet--from a potential subsequence of one of its\nwords. We assume the given subsequence is received through a deletion channel\nas a result of transmission of a random word from one of the two generic\nunderlying vocabularies. An exact maximum a posterior (MAP) solution for this\nproblem counts the number of ways a given subsequence can be derived from\nparticular subsets of candidate vocabularies, requiring exponential time or\nspace.\n  We present a polynomial approximation algorithm for this problem. The\nalgorithm makes no prior assumption about the rules and patterns governing the\nstructure of vocabularies. Instead, through off-line processing of\nvocabularies, it extracts data regarding regularity patterns in the\nsubsequences of each vocabulary. In the recognition phase, the algorithm just\nuses this data, called subsequence-histogram, to decide in favor of one of the\nvocabularies. We provide examples to demonstrate the performance of the\nalgorithm and show that it can achieve the same performance as MAP in some\nsituations.\n  Potential applications include bioinformatics, storage systems, and search\nengines.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2009 21:47:19 GMT"}], "update_date": "2009-04-23", "authors_parsed": [["Fozunbal", "Majid", ""]]}, {"id": "0904.3503", "submitter": "Ferdinando Cicalese", "authors": "Ferdinando Cicalese, Tobias Jacobs, Eduardo Laber and Marco Molinaro", "title": "On the Complexity of Searching in Trees: Average-case Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the average-case analysis: A function w : V -> Z+ is given which\ndefines the likelihood for a node to be the one marked, and we want the\nstrategy that minimizes the expected number of queries. Prior to this paper,\nvery little was known about this natural question and the complexity of the\nproblem had remained so far an open question.\n  We close this question and prove that the above tree search problem is\nNP-complete even for the class of trees with diameter at most 4. This results\nin a complete characterization of the complexity of the problem with respect to\nthe diameter size. In fact, for diameter not larger than 3 the problem can be\nshown to be polynomially solvable using a dynamic programming approach.\n  In addition we prove that the problem is NP-complete even for the class of\ntrees of maximum degree at most 16. To the best of our knowledge, the only\nknown result in this direction is that the tree search problem is solvable in\nO(|V| log|V|) time for trees with degree at most 2 (paths).\n  We match the above complexity results with a tight algorithmic analysis. We\nfirst show that a natural greedy algorithm attains a 2-approximation.\nFurthermore, for the bounded degree instances, we show that any optimal\nstrategy (i.e., one that minimizes the expected number of queries) performs at\nmost O(\\Delta(T) (log |V| + log w(T))) queries in the worst case, where w(T) is\nthe sum of the likelihoods of the nodes of T and \\Delta(T) is the maximum\ndegree of T. We combine this result with a non-trivial exponential time\nalgorithm to provide an FPTAS for trees with bounded degree.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2009 17:09:15 GMT"}, {"version": "v2", "created": "Tue, 12 May 2009 14:09:25 GMT"}, {"version": "v3", "created": "Sun, 9 Aug 2009 22:00:39 GMT"}], "update_date": "2009-08-10", "authors_parsed": [["Cicalese", "Ferdinando", ""], ["Jacobs", "Tobias", ""], ["Laber", "Eduardo", ""], ["Molinaro", "Marco", ""]]}, {"id": "0904.3741", "submitter": "David Eppstein", "authors": "David Eppstein and Emma S. Spiro", "title": "The h-Index of a Graph and its Application to Dynamic Subgraph\n  Statistics", "comments": "To appear at Algorithms and Data Structures Symposium, Banff, Canada,\n  August 2009. 18 pages, 4 figures. Includes six pages of appendices that will\n  not be included in the conference proceedings version", "journal-ref": "J. Graph Algorithms & Applications 16(2): 543-567, 2012", "doi": "10.7155/jgaa.00273", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a data structure that maintains the number of triangles in a\ndynamic undirected graph, subject to insertions and deletions of edges and of\ndegree-zero vertices. More generally it can be used to maintain the number of\ncopies of each possible three-vertex subgraph in time O(h) per update, where h\nis the h-index of the graph, the maximum number such that the graph contains\n$h$ vertices of degree at least h. We also show how to maintain the h-index\nitself, and a collection of h high-degree vertices in the graph, in constant\ntime per update. Our data structure has applications in social network analysis\nusing the exponential random graph model (ERGM); its bound of O(h) time per\nedge is never worse than the Theta(sqrt m) time per edge necessary to list all\ntriangles in a static graph, and is strictly better for graphs obeying a power\nlaw degree distribution. In order to better understand the behavior of the\nh-index statistic and its implications for the performance of our algorithms,\nwe also study the behavior of the h-index on a set of 136 real-world networks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2009 18:37:36 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Eppstein", "David", ""], ["Spiro", "Emma S.", ""]]}, {"id": "0904.3756", "submitter": "Michael Goodrich", "authors": "Wenliang Du, David Eppstein, Michael T. Goodrich, George S. Lueker", "title": "On the Approximability of Geometric and Geographic Generalization and\n  the Min-Max Bin Covering Problem", "comments": "18 pages. Expanded version of paper appearing in 2009 Algorithms and\n  Data Structures Symposium (formerly WADS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of abstracting a table of data about individuals so that\nno selection query can identify fewer than k individuals. We show that it is\nimpossible to achieve arbitrarily good polynomial-time approximations for a\nnumber of natural variations of the generalization technique, unless P = NP,\neven when the table has only a single quasi-identifying attribute that\nrepresents a geographic or unordered attribute:\n  Zip-codes: nodes of a planar graph generalized into connected subgraphs\n  GPS coordinates: points in R2 generalized into non-overlapping rectangles\n  Unordered data: text labels that can be grouped arbitrarily. In addition to\nimpossibility results, we provide approximation algorithms for these difficult\nsingle-attribute generalization problems, which, of course, apply to\nmultiple-attribute instances with one that is quasi-identifying. We show\ntheoretically and experimentally that our approximation algorithms can come\nreasonably close to optimal solutions. Incidentally, the generalization problem\nfor unordered data can be viewed as a novel type of bin packing\nproblem--min-max bin covering--which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2009 21:06:58 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2009 22:59:44 GMT"}, {"version": "v3", "created": "Tue, 12 May 2009 16:16:10 GMT"}], "update_date": "2009-05-12", "authors_parsed": [["Du", "Wenliang", ""], ["Eppstein", "David", ""], ["Goodrich", "Michael T.", ""], ["Lueker", "George S.", ""]]}, {"id": "0904.3761", "submitter": "Charalampos Tsourakakis", "authors": "Charalampos E. Tsourakakis, Mihail N. Kolountzakis, Gary L. Miller", "title": "Approximate Triangle Counting", "comments": "1) 16 pages, 2 figures, under submission 2) Removed the erroneous\n  random projection part. Thanks to Ioannis Koutis for pointing out the error.\n  3) Added experimental session", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Triangle counting is an important problem in graph mining. Clustering\ncoefficients of vertices and the transitivity ratio of the graph are two\nmetrics often used in complex network analysis. Furthermore, triangles have\nbeen used successfully in several real-world applications. However, exact\ntriangle counting is an expensive computation. In this paper we present the\nanalysis of a practical sampling algorithm for counting triangles in graphs.\nOur analysis yields optimal values for the sampling rate, thus resulting in\ntremendous speedups ranging from \\emph{2800}x to \\emph{70000}x when applied to\nreal-world networks. At the same time the accuracy of the estimation is\nexcellent.\n  Our contributions include experimentation on graphs with several millions of\nnodes and edges, where we show how practical our proposed method is. Finally,\nour algorithm's implementation is a part of the \\pegasus library (Code and\ndatasets are available at (http://www.cs.cmu.edu/~ctsourak/).) a Peta-Graph\nMining library implemented in Hadoop, the open source version of Mapreduce.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2009 14:21:13 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2009 09:02:34 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Tsourakakis", "Charalampos E.", ""], ["Kolountzakis", "Mihail N.", ""], ["Miller", "Gary L.", ""]]}, {"id": "0904.3898", "submitter": "Manfred Kufleitner", "authors": "Mahmoud Fouz, Manfred Kufleitner, Bodo Manthey and Nima Zeini Jahromi", "title": "On Smoothed Analysis of Quicksort and Hoare's Find", "comments": "To be presented at the 15th Int. Computing and Combinatorics\n  Conference (COCOON 2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a smoothed analysis of Hoare's find algorithm and we revisit the\nsmoothed analysis of quicksort.\n  Hoare's find algorithm - often called quickselect - is an easy-to-implement\nalgorithm for finding the k-th smallest element of a sequence. While the\nworst-case number of comparisons that Hoare's find needs is quadratic, the\naverage-case number is linear. We analyze what happens between these two\nextremes by providing a smoothed analysis of the algorithm in terms of two\ndifferent perturbation models: additive noise and partial permutations.\n  Moreover, we provide lower bounds for the smoothed number of comparisons of\nquicksort and Hoare's find for the median-of-three pivot rule, which usually\nyields faster algorithms than always selecting the first element: The pivot is\nthe median of the first, middle, and last element of the sequence. We show that\nmedian-of-three does not yield a significant improvement over the classic rule:\nthe lower bounds for the classic rule carry over to median-of-three.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2009 16:12:40 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2009 09:02:30 GMT"}], "update_date": "2009-04-27", "authors_parsed": [["Fouz", "Mahmoud", ""], ["Kufleitner", "Manfred", ""], ["Manthey", "Bodo", ""], ["Jahromi", "Nima Zeini", ""]]}, {"id": "0904.4057", "submitter": "Salah A. Aly", "authors": "Zhenning Kong, Salah A. Aly, Emina Soljanin", "title": "Decentralized Coding Algorithms for Distributed Storage in Wireless\n  Sensor Networks", "comments": "Accepted for publication in IEEE JSAC, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider large-scale wireless sensor networks with $n$ nodes, out of which\nk are in possession, (e.g., have sensed or collected in some other way) k\ninformation packets. In the scenarios in which network nodes are vulnerable\nbecause of, for example, limited energy or a hostile environment, it is\ndesirable to disseminate the acquired information throughout the network so\nthat each of the n nodes stores one (possibly coded) packet so that the\noriginal k source packets can be recovered, locally and in a computationally\nsimple way from any k(1 + \\epsilon) nodes for some small \\epsilon > 0. We\ndevelop decentralized Fountain codes based algorithms to solve this problem.\nUnlike all previously developed schemes, our algorithms are truly distributed,\nthat is, nodes do not know n, k or connectivity in the network, except in their\nown neighborhoods, and they do not maintain any routing tables.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2009 20:51:56 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2009 16:11:07 GMT"}], "update_date": "2009-10-27", "authors_parsed": [["Kong", "Zhenning", ""], ["Aly", "Salah A.", ""], ["Soljanin", "Emina", ""]]}, {"id": "0904.4061", "submitter": "Zhifeng Sun", "authors": "Agnes Chan, Rajmohan Rajaraman, Zhifeng Sun, Feng Zhu", "title": "Approximation Algorithms for Key Management in Secure Multicast", "comments": "COCOON 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data dissemination and publish-subscribe systems that guarantee the\nprivacy and authenticity of the participants rely on symmetric key\ncryptography. An important problem in such a system is to maintain the shared\ngroup key as the group membership changes. We consider the problem of\ndetermining a key hierarchy that minimizes the average communication cost of an\nupdate, given update frequencies of the group members and an edge-weighted\nundirected graph that captures routing costs. We first present a\npolynomial-time approximation scheme for minimizing the average number of\nmulticast messages needed for an update. We next show that when routing costs\nare considered, the problem is NP-hard even when the underlying routing network\nis a tree network or even when every group member has the same update\nfrequency. Our main result is a polynomial time constant-factor approximation\nalgorithm for the general case where the routing network is an arbitrary\nweighted graph and group members have nonuniform update frequencies.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2009 15:28:58 GMT"}], "update_date": "2009-04-28", "authors_parsed": [["Chan", "Agnes", ""], ["Rajaraman", "Rajmohan", ""], ["Sun", "Zhifeng", ""], ["Zhu", "Feng", ""]]}, {"id": "0904.4193", "submitter": "Shahar Dobzinski", "authors": "Shahar Dobzinski, Shaddin Dughmi", "title": "On the Power of Randomization in Algorithmic Mechanism Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many settings the power of truthful mechanisms is severely bounded. In\nthis paper we use randomization to overcome this problem. In particular, we\nconstruct an FPTAS for multi-unit auctions that is truthful in expectation,\nwhereas there is evidence that no polynomial-time truthful deterministic\nmechanism provides an approximation ratio better than 2.\n  We also show for the first time that truthful in expectation polynomial-time\nmechanisms are \\emph{provably} stronger than polynomial-time universally\ntruthful mechanisms. Specifically, we show that there is a setting in which:\n(1) there is a non-polynomial time truthful mechanism that always outputs the\noptimal solution, and that (2) no universally truthful randomized mechanism can\nprovide an approximation ratio better than 2 in polynomial time, but (3) an\nFPTAS that is truthful in expectation exists.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2009 15:58:08 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2009 18:30:27 GMT"}], "update_date": "2009-08-24", "authors_parsed": [["Dobzinski", "Shahar", ""], ["Dughmi", "Shaddin", ""]]}, {"id": "0904.4458", "submitter": "Michael Goodrich", "authors": "Michael T. Goodrich", "title": "Learning Character Strings via Mastermind Queries, with a Case Study\n  Involving mtDNA", "comments": "Full version of related paper appearing in IEEE Symposium on Security\n  and Privacy 2009, \"The Mastermind Attack on Genomic Data.\" This version\n  corrects the proofs of what are now Theorems 2 and 4.", "journal-ref": null, "doi": "10.1109/TIT.2012.2208581", "report-no": null, "categories": "cs.DS cs.CR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the degree to which a character string, $Q$, leaks details about\nitself any time it engages in comparison protocols with a strings provided by a\nquerier, Bob, even if those protocols are cryptographically guaranteed to\nproduce no additional information other than the scores that assess the degree\nto which $Q$ matches strings offered by Bob. We show that such scenarios allow\nBob to play variants of the game of Mastermind with $Q$ so as to learn the\ncomplete identity of $Q$. We show that there are a number of efficient\nimplementations for Bob to employ in these Mastermind attacks, depending on\nknowledge he has about the structure of $Q$, which show how quickly he can\ndetermine $Q$. Indeed, we show that Bob can discover $Q$ using a number of\nrounds of test comparisons that is much smaller than the length of $Q$, under\nreasonable assumptions regarding the types of scores that are returned by the\ncryptographic protocols and whether he can use knowledge about the distribution\nthat $Q$ comes from. We also provide the results of a case study we performed\non a database of mitochondrial DNA, showing the vulnerability of existing\nreal-world DNA data to the Mastermind attack.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2009 18:12:24 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2010 05:31:22 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Goodrich", "Michael T.", ""]]}, {"id": "0904.4530", "submitter": "Fei Li", "authors": "Fei Li, Zhi Zhang", "title": "Online Maximizing Weighted Throughput In A Fading Channel", "comments": "9 pages, appears in ISIT 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online scheduling weighted packets with time constraints over a\nfading channel. Packets arrive at the transmitter in an online manner. Each\npacket has a value and a deadline by which it should be sent. The fade state of\nthe channel determines the throughput obtained per unit of time and the\nchannel's quality may change over time. In this paper, we design online\nalgorithms to maximize weighted throughput, defined as the total value of the\npackets sent by their respective deadlines. Competitive ratio is employed to\nmeasure an online algorithm's performance. For this problem and one of its\nvariants, we present two online algorithms with competitive ratios 2.618 and 2\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2009 04:11:35 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Li", "Fei", ""], ["Zhang", "Zhi", ""]]}, {"id": "0904.4615", "submitter": "Maria Potop-Butucaru", "authors": "Swan Dubois (INRIA Rocquencourt, LIP6), Maria Potop-Butucaru (INRIA\n  Rocquencourt, LIP6), S\\'ebastien Tixeuil (LIP6)", "title": "Dynamic FTSS in Asynchronous Systems: the Case of Unison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed fault-tolerance can mask the effect of a limited number of\npermanent faults, while self-stabilization provides forward recovery after an\narbitrary number of transient fault hit the system. FTSS protocols combine the\nbest of both worlds since they are simultaneously fault-tolerant and\nself-stabilizing. To date, FTSS solutions either consider static (i.e. fixed\npoint) tasks, or assume synchronous scheduling of the system components. In\nthis paper, we present the first study of dynamic tasks in asynchronous\nsystems, considering the unison problem as a benchmark. Unison can be seen as a\nlocal clock synchronization problem as neighbors must maintain digital clocks\nat most one time unit away from each other, and increment their own clock value\ninfinitely often. We present many impossibility results for this difficult\nproblem and propose a FTSS solution when the problem is solvable that exhibits\noptimal fault containment.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2009 14:04:59 GMT"}, {"version": "v2", "created": "Thu, 10 Feb 2011 16:21:28 GMT"}], "update_date": "2011-02-11", "authors_parsed": [["Dubois", "Swan", "", "INRIA Rocquencourt, LIP6"], ["Potop-Butucaru", "Maria", "", "INRIA\n  Rocquencourt, LIP6"], ["Tixeuil", "S\u00e9bastien", "", "LIP6"]]}, {"id": "0904.4670", "submitter": "Michael Goodrich", "authors": "Mikhail J. Atallah, Marina Blanton, Michael T. Goodrich, Stanislas\n  Polu", "title": "Discrepancy-Sensitive Dynamic Fractional Cascading, Dominated Maxima\n  Searching, and 2-d Nearest Neighbors in Any Minkowski Metric", "comments": "Expanded version of a paper that appeared in WADS 2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a discrepancy-sensitive approach to dynamic fractional\ncascading. We provide an efficient data structure for dominated maxima\nsearching in a dynamic set of points in the plane, which in turn leads to an\nefficient dynamic data structure that can answer queries for nearest neighbors\nusing any Minkowski metric. We provide an efficient data structure for\ndominated maxima searching in a dynamic set of points in the plane, which in\nturn leads to an efficient dynamic data structure that can answer queries for\nnearest neighbors using any Minkowski metric.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2009 18:15:55 GMT"}], "update_date": "2009-04-30", "authors_parsed": [["Atallah", "Mikhail J.", ""], ["Blanton", "Marina", ""], ["Goodrich", "Michael T.", ""], ["Polu", "Stanislas", ""]]}, {"id": "0904.4911", "submitter": "Michael Goodrich", "authors": "Michael T. Goodrich", "title": "On the Algorithmic Complexity of the Mastermind Game with Black-Peg\n  Results", "comments": "Expanded version with a figure showing the Mastermind game", "journal-ref": "Information Processing Letters, Volume 109, 675-678, 2009", "doi": "10.1016/j.ipl.2009.02.021", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the algorithmic complexity of the Mastermind game,\nwhere results are single-color black pegs. This differs from the usual\ndual-color version of the game, but better corresponds to applications in\ngenetics. We show that it is NP-complete to determine if a sequence of\nsingle-color Mastermind results have a satisfying vector. We also show how to\ndevise efficient algorithms for discovering a hidden vector through\nsingle-color queries. Indeed, our algorithm improves a previous method of\nChvatal by almost a factor of 2.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2009 18:31:15 GMT"}], "update_date": "2009-05-13", "authors_parsed": [["Goodrich", "Michael T.", ""]]}]