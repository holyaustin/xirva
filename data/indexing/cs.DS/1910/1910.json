[{"id": "1910.00081", "submitter": "Nitant Upasani", "authors": "Nitant Upasani, Krishnendra Shekhawat and Garv Sachdeva", "title": "Automated Generation of Dimensioned Rectangular Floorplans", "comments": null, "journal-ref": null, "doi": "10.1016/j.autcon.2020.103149", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a methodology for the automated construction of\nrectangular floorplans (RFPs) while addressing dimensional constraints and\nadjacency relations. Here, adjacency relations are taken in the form of a\ndimensionless rectangular arrangement (RA) ensuring the existence of a RFP,\nwhile dimensional constraints are given in terms of minimum width and aspect\nratio range for each room. A linear optimization model is then presented to\nobtain a feasible dimensioned RFP for user-defined constraints. A GUI is also\ndeveloped for the automated generation of RFPs. The proposed model is able to\ngenerate feasible solutions for every possible RA in a reasonable amount of\ntime. From the architectural perspective, this work can be seen as a\nre-generation of well-known architectural plans with modified dimensions. In\nthe end, the regeneration of existing legacy RFPs (corresponding to the\nuser-defined dimensions) has been demonstrated, taking their image as input.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 20:11:34 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Upasani", "Nitant", ""], ["Shekhawat", "Krishnendra", ""], ["Sachdeva", "Garv", ""]]}, {"id": "1910.00152", "submitter": "Tianyi Lin", "authors": "Tianyi Lin, Nhat Ho, Marco Cuturi and Michael I. Jordan", "title": "On the Complexity of Approximating Multimarginal Optimal Transport", "comments": "Improve the paper significantly; 39 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of approximating the multimarginal optimal transport\n(MOT) distance, a generalization of the classical optimal transport distance,\nconsidered here between $m$ discrete probability distributions supported each\non $n$ support points. First, we show that the standard linear programming (LP)\nrepresentation of the MOT problem is not a minimum-cost flow problem when $m\n\\geq 3$. This negative result implies that some combinatorial algorithms, e.g.,\nnetwork simplex method, are not suitable for approximating the MOT problem,\nwhile the worst-case complexity bound for the deterministic interior-point\nalgorithm remains a quantity of $\\tilde{O}(n^{3m})$. We then propose two simple\nand \\textit{deterministic} algorithms for approximating the MOT problem. The\nfirst algorithm, which we refer to as \\textit{multimarginal Sinkhorn}\nalgorithm, is a provably efficient multimarginal generalization of the Sinkhorn\nalgorithm. We show that it achieves a complexity bound of\n$\\tilde{O}(m^3n^m\\varepsilon^{-2})$ for a tolerance $\\varepsilon \\in (0, 1)$.\nThis provides a first \\textit{near-linear time} complexity bound guarantee for\napproximating the MOT problem and matches the best known complexity bound for\nthe Sinkhorn algorithm in the classical OT setting when $m = 2$. The second\nalgorithm, which we refer to as \\textit{accelerated multimarginal Sinkhorn}\nalgorithm, achieves the acceleration by incorporating an estimate sequence and\nthe complexity bound is $\\tilde{O}(m^3n^{m+1/3}\\varepsilon^{-4/3})$. This bound\nis better than that of the first algorithm in terms of $1/\\varepsilon$, and\naccelerated alternating minimization\nalgorithm~\\citep{Tupitsa-2020-Multimarginal} in terms of $n$. Finally, we\ncompare our new algorithms with the commercial LP solver \\textsc{Gurobi}.\nPreliminary results on synthetic data and real images demonstrate the\neffectiveness and efficiency of our algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 23:43:43 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 23:00:46 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Lin", "Tianyi", ""], ["Ho", "Nhat", ""], ["Cuturi", "Marco", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1910.00223", "submitter": "Alexander Rusciano", "authors": "James Demmel, Laura Grigori, Alexander Rusciano", "title": "An improved analysis and unified perspective on deterministic and\n  randomized low rank matrix approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Generalized LU-Factorization (\\textbf{GLU}) for low-rank\nmatrix approximation. We relate this to past approaches and extensively analyze\nits approximation properties. The established deterministic guarantees are\ncombined with sketching ensembles satisfying Johnson-Lindenstrauss properties\nto present complete bounds. Particularly good performance is shown for the\nsub-sampled randomized Hadamard transform (SRHT) ensemble. Moreover, the\nfactorization is shown to unify and generalize many past algorithms. It also\nhelps to explain the effect of sketching on the growth factor during Gaussian\nElimination.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 06:57:24 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Demmel", "James", ""], ["Grigori", "Laura", ""], ["Rusciano", "Alexander", ""]]}, {"id": "1910.00277", "submitter": "Ren\\'e van Bevern", "authors": "Matthias Bentert and Ren\\'e van Bevern and Till Fluschnik and Andr\\'e\n  Nichterlein and Rolf Niedermeier", "title": "Polynomial-Time Data Reduction for Weighted Problems Beyond Additive\n  Goal Functions", "comments": "This revision contains a new geometrical interpretation of the\n  approach", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with NP-hard problems, kernelization is a fundamental notion for\npolynomial-time data reduction with performance guarantees: in polynomial time,\na problem instance is reduced to an equivalent instance with size upper-bounded\nby a function of a parameter chosen in advance. Kernelization for weighted\nproblems particularly requires to also shrink weights. Marx and V\\'egh [ACM\nTrans. Algorithms 2015] and Etscheid et al. [J. Comput. Syst. Sci. 2017] used a\ntechnique of Frank and Tardos [Combinatorica 1987] to obtain polynomial-size\nkernels for weighted problems, mostly with additive goal functions. We\ncharacterize the function types that the technique is applicable to, which\nturns out to contain many non-additive functions. Using this insight, we\nsystematically obtain kernelization results for natural problems in graph\npartitioning, network design, facility location, scheduling, vehicle routing,\nand computational social choice, thereby improving and generalizing results\nfrom the literature.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 09:45:08 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 14:33:30 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 04:09:49 GMT"}, {"version": "v4", "created": "Wed, 13 Jan 2021 11:29:56 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Bentert", "Matthias", ""], ["van Bevern", "Ren\u00e9", ""], ["Fluschnik", "Till", ""], ["Nichterlein", "Andr\u00e9", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "1910.00308", "submitter": "Martin Schirneck", "authors": "Thomas Bl\\\"asius, Tobias Friedrich, Martin Schirneck", "title": "The Minimization of Random Hypergraphs", "comments": "28 pages, 2 figures; Changes: binomial characterization unified,\n  improvement of the Chernoff-Hoeffding theorem extended to case x --> p", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the maximum-entropy model $\\mathcal{B}_{n,m,p}$ for random\n$n$-vertex, $m$-edge multi-hypergraphs with expected edge size $pn$. We show\nthat the expected size of the minimization of $\\mathcal{B}_{n,m,p}$, i.e., the\nnumber of its inclusion-wise minimal edges, undergoes a phase transition with\nrespect to $m$. If $m$ is at most $1/(1-p)^{(1-p)n}$, then the minimization is\nof size $\\Theta(m)$. Beyond that point, for $\\alpha$ such that $m =\n1/(1-p)^{\\alpha n}$ and $\\mathrm{H}$ being the entropy function, it is\n$\\Theta(1) \\cdot \\min\\!\\left(1, \\, \\frac{1}{(\\alpha\\,{-}\\,(1-p))\n\\sqrt{(1\\,{-}\\,\\alpha) n}}\\right) \\cdot 2^{(\\mathrm{H}(\\alpha) + (1-\\alpha)\n\\log_2 p) n}.$ This implies that the maximum expected size over all $m$ is\n$\\Theta((1+p)^n/\\sqrt{n})$. Our structural findings have algorithmic\nimplications for minimizing an input hypergraph, which in turn has applications\nin the profiling of relational databases as well as for the Orthogonal Vectors\nproblem studied in fine-grained complexity. The main technical tool is an\nimprovement of the Chernoff--Hoeffding inequality, which we make tight up to\nconstant factors. We show that for a binomial variable $X \\sim\n\\mathrm{Bin}(n,p)$ and real number $0 < x \\le p$, it holds that $\\mathrm{P}[X\n\\le xn] = \\Theta(1) \\cdot \\min\\!\\left(1, \\, \\frac{1}{(p-x) \\sqrt{xn}}\\right)\n\\cdot 2^{-\\!\\mathrm{D}(x \\,{\\|}\\, p) n}$, where $\\mathrm{D}$ denotes the\nKullback--Leibler divergence between Bernoulli distributions. The result\nremains true if $x$ depends on $n$ as long as it is bounded away from $0$.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 11:23:19 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 15:32:14 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 11:17:31 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Friedrich", "Tobias", ""], ["Schirneck", "Martin", ""]]}, {"id": "1910.00400", "submitter": "Guido Cantelmo Dr", "authors": "Guido Cantelmo, Moeid Qurashi, A. Arun Prakash, Constantinos Antoniou,\n  Francesco Viti", "title": "Incorporating Trip Chaining within Online Demand Estimation", "comments": "copyright 2019. This manuscript version is made available under the\n  CC-BY-NC-ND 2.0 license http://creativecommons.org/licenses/by-nc-nd/2.0\n  Article presented at 23rd International Symposium on Transportation and\n  Traffic Theory, ISTTT 23, 24-26 July 2019, Lausanne, Switzerland", "journal-ref": "2019 Published to Transportation Research Part B: Methodological", "doi": "10.1016/j.trb.2019.05.010", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-dependent Origin-Destination (OD) demand flows are fundamental inputs\nfor Dynamic Traffic Assignment (DTA) systems and real-time traffic management.\nThis work introduces a novel state-space framework to estimate these demand\nflows in an online context. Specifically, we propose to explicitly include\ntrip-chaining behavior within the state-space formulation, which is solved\nusing the well-established Kalman Filtering technique. While existing works\nalready consider structural information and recursive behavior within the\nonline demand estimation problem, this information has been always considered\nat the OD level. In this study, we introduce this structural information by\nexplicitly representing trip-chaining within the estimation framework. The\nadvantage is twofold. First, all trips belonging to the same tour can be\njointly calibrated. Second, given the estimation during a certain time\ninterval, a prediction of the structural deviation over the whole day can be\nobtained without the need to run additional simulations. The effectiveness of\nthe proposed methodology is demonstrated first on a toy network and then on a\nlarge real-world network. Results show that the model improves the prediction\nperformance with respect to a conventional Kalman Filtering approach. We also\nshow that, on the basis of the estimation of the morning commute, the model can\nbe used to predict the evening commute without need of running additional\nsimulations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 14:04:18 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 19:07:52 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 16:17:45 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Cantelmo", "Guido", ""], ["Qurashi", "Moeid", ""], ["Prakash", "A. Arun", ""], ["Antoniou", "Constantinos", ""], ["Viti", "Francesco", ""]]}, {"id": "1910.00440", "submitter": "Tim Hartmann", "authors": "Jan Dreier, Janosch Fuchs, Tim A. Hartmann, Philipp Kuinke, Peter\n  Rossmanith, Bjoern Tauer, Hung-Lung Wang", "title": "The Complexity of Packing Edge-Disjoint Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study the complexity of Path Packing. Given a graph $G$ and\na list of paths, the task is to embed the paths edge-disjoint in $G$. This\ngeneralizes the well known Hamiltonian-Path problem.\n  Since Hamiltonian Path is efficiently solvable for graphs of small treewidth,\nwe study how this result translates to the much more general Path Packing. On\nthe positive side, we give an FPT-algorithm on trees for the number of paths as\nparameter. Further, we give an XP-algorithm with the combined parameters\nmaximal degree, number of connected components and number of nodes of degree at\nleast three. Surprisingly the latter is an almost tight result by runtime and\nparameterization. We show an ETH lower bound almost matching our runtime.\nMoreover, if two of the three values are constant and one is unbounded the\nproblem becomes NP-hard.\n  Further, we study restrictions to the given list of paths. On the positive\nside, we present an FPT-algorithm parameterized by the sum of the lengths of\nthe paths. Packing paths of length two is polynomial time solvable, while\npacking paths of length three is NP-hard. Finally, even the spacial case EPC\nwhere the paths have to cover every edge in $G$ exactly once is already NP-hard\nfor two paths on 4-regular graphs.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 14:26:28 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Dreier", "Jan", ""], ["Fuchs", "Janosch", ""], ["Hartmann", "Tim A.", ""], ["Kuinke", "Philipp", ""], ["Rossmanith", "Peter", ""], ["Tauer", "Bjoern", ""], ["Wang", "Hung-Lung", ""]]}, {"id": "1910.00494", "submitter": "Alane Lima", "authors": "Alane M. de Lima, Murilo V. G. da Silva and Andr\\'e L. Vignatti", "title": "Estimating the Percolation Centrality of Large Networks through\n  Pseudo-dimension Theory", "comments": "Submitted to ACM SIGKDD Conference on Knowledge Discovery and Data\n  Mining (KDD), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate the problem of estimating the percolation\ncentrality of every vertex in a graph. This centrality measure quantifies the\nimportance of each vertex in a graph going through a contagious process. It is\nan open problem whether the percolation centrality can be computed in\n$\\mathcal{O}(n^{3-c})$ time, for any constant $c>0$. In this paper we present a\n$\\mathcal{O}(m \\log^2 n)$ randomized approximation algorithm for the\npercolation centrality for every vertex of $G$, generalizing techniques\ndeveloped by Riondato, Upfal e Kornaropoulos (this complexity is reduced to\n$\\mathcal{O}((m+n) \\log n)$ for unweighted graphs). The estimation obtained by\nthe algorithm is within $\\epsilon$ of the exact value with probability\n$1-\\delta$, for {\\it fixed} constants $0 < \\epsilon,\\delta \\leq 1$. In fact, we\nshow in our experimental analysis that in the case of real world complex\nnetworks, the output produced by our algorithm is significantly closer to the\nexact values than its guarantee in terms of theoretical worst case analysis.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 15:50:51 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 14:51:41 GMT"}, {"version": "v3", "created": "Sun, 16 Feb 2020 01:40:52 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["de Lima", "Alane M.", ""], ["da Silva", "Murilo V. G.", ""], ["Vignatti", "Andr\u00e9 L.", ""]]}, {"id": "1910.00510", "submitter": "Lou Sala\\\"un", "authors": "Lou Sala\\\"un, Marceau Coupechoux, Chung Shue Chen", "title": "Joint Subcarrier and Power Allocation in NOMA: Optimal and Approximate\n  Algorithms", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2020.2982786", "report-no": null, "categories": "math.OC cs.CC cs.DS eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-orthogonal multiple access (NOMA) is a promising technology to increase\nthe spectral efficiency and enable massive connectivity in 5G and future\nwireless networks. In contrast to orthogonal schemes, such as OFDMA, NOMA\nmultiplexes several users on the same frequency and time resource. Joint\nsubcarrier and power allocation problems (JSPA) in NOMA are NP-hard to solve in\ngeneral. In this family of problems, we consider the weighted sum-rate (WSR)\nobjective function as it can achieve various tradeoffs between sum-rate\nperformance and user fairness. Because of JSPA's intractability, a common\napproach in the literature is to solve separately the power control and\nsubcarrier allocation (also known as user selection) problems, therefore\nachieving sub-optimal result. In this work, we first improve the computational\ncomplexity of existing single-carrier power control and user selection schemes.\nThese improved procedures are then used as basic building blocks to design new\nalgorithms, namely Opt-JSPA, $\\varepsilon$-JSPA and Grad-JSPA. Opt-JSPA\ncomputes an optimal solution with lower complexity than current optimal schemes\nin the literature. It can be used as a benchmark for optimal WSR performance in\nsimulations. However, its pseudo-polynomial time complexity remains impractical\nfor real-world systems with low latency requirements. To further reduce the\ncomplexity, we propose a fully polynomial-time approximation scheme called\n$\\varepsilon$-JSPA. Since, no approximation has been studied in the literature,\n$\\varepsilon$-JSPA stands out by allowing to control a tight trade-off between\nperformance guarantee and complexity. Finally, Grad-JSPA is a heuristic based\non gradient descent. Numerical results show that it achieves near-optimal WSR\nwith much lower complexity than existing optimal methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 15:59:38 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 11:22:52 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2020 09:43:22 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Sala\u00fcn", "Lou", ""], ["Coupechoux", "Marceau", ""], ["Chen", "Chung Shue", ""]]}, {"id": "1910.00517", "submitter": "Juho Lauri", "authors": "Marco Grassia, Juho Lauri, Sourav Dutta, Deepak Ajwani", "title": "Learning Multi-Stage Sparsification for Maximum Clique Enumeration", "comments": "Appeared at the Data Science Meets Optimization Workshop (DSO) at\n  IJCAI'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-stage learning approach for pruning the search space of\nmaximum clique enumeration, a fundamental computationally difficult problem\narising in various network analysis tasks. In each stage, our approach learns\nthe characteristics of vertices in terms of various neighborhood features and\nleverage them to prune the set of vertices that are likely not contained in any\nmaximum clique. Furthermore, we demonstrate that our approach is domain\nindependent -- the same small set of features works well on graph instances\nfrom different domain. Compared to the state-of-the-art heuristics and\npreprocessing strategies, the advantages of our approach are that (i) it does\nnot require any estimate on the maximum clique size at runtime and (ii) we\ndemonstrate it to be effective also for dense graphs. In particular, for dense\ngraphs, we typically prune around 30 \\% of the vertices resulting in speedups\nof up to 53 times for state-of-the-art solvers while generally preserving the\nsize of the maximum clique (though some maximum cliques may be lost). For large\nreal-world sparse graphs, we routinely prune over 99 \\% of the vertices\nresulting in several tenfold speedups at best, typically with no impact on\nsolution quality.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 18:39:04 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Grassia", "Marco", ""], ["Lauri", "Juho", ""], ["Dutta", "Sourav", ""], ["Ajwani", "Deepak", ""]]}, {"id": "1910.00551", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Nicolas Flammarion, Martin J. Wainwright, Peter L.\n  Bartlett", "title": "An Efficient Sampling Algorithm for Non-smooth Composite Potentials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sampling from a density of the form $p(x) \\propto\n\\exp(-f(x)- g(x))$, where $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is a smooth\nand strongly convex function and $g: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is a\nconvex and Lipschitz function. We propose a new algorithm based on the\nMetropolis-Hastings framework, and prove that it mixes to within TV distance\n$\\varepsilon$ of the target density in at most $O(d \\log (d/\\varepsilon))$\niterations. This guarantee extends previous results on sampling from\ndistributions with smooth log densities ($g = 0$) to the more general composite\nnon-smooth case, with the same mixing time up to a multiple of the condition\nnumber. Our method is based on a novel proximal-based proposal distribution\nthat can be efficiently computed for a large class of non-smooth functions $g$.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 17:25:55 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Mou", "Wenlong", ""], ["Flammarion", "Nicolas", ""], ["Wainwright", "Martin J.", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1910.00581", "submitter": "Sebastian Siebertz", "authors": "Daniel Lokshtanov and Amer E. Mouawad and Fahad Panolan and Sebastian\n  Siebertz", "title": "On the Parameterized Complexity of Reconfiguration of Connected\n  Dominating Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a reconfiguration version of an optimization problem $\\mathcal{Q}$ the\ninput is an instance of $\\mathcal{Q}$ and two feasible solutions $S$ and $T$.\nThe objective is to determine whether there exists a step-by-step\ntransformation between $S$ and $T$ such that all intermediate steps also\nconstitute feasible solutions. In this work, we study the parameterized\ncomplexity of the \\textsc{Connected Dominating Set Reconfiguration} problem\n(\\textsc{CDS-R)}. It was shown in previous work that the \\textsc{Dominating Set\nReconfiguration} problem (\\textsc{DS-R}) parameterized by $k$, the maximum\nallowed size of a dominating set in a reconfiguration sequence, is\nfixed-parameter tractable on all graphs that exclude a biclique $K_{d,d}$ as a\nsubgraph, for some constant $d \\geq 1$. We show that the additional\nconnectivity constraint makes the problem much harder, namely, that\n\\textsc{CDS-R} is \\textsf{W}$[1]$-hard parameterized by $k+\\ell$, the maximum\nallowed size of a dominating set plus the length of the reconfiguration\nsequence, already on $5$-degenerate graphs. On the positive side, we show that\n\\textsc{CDS-R} parameterized by $k$ is fixed-parameter tractable, and in fact\nadmits a polynomial kernel on planar graphs.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 09:38:50 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Mouawad", "Amer E.", ""], ["Panolan", "Fahad", ""], ["Siebertz", "Sebastian", ""]]}, {"id": "1910.00692", "submitter": "Raunak Kumar", "authors": "Raunak Kumar (1), Paul Liu (2), Moses Charikar (2), Austin R. Benson\n  (1) ((1) Cornell University, (2) Stanford University)", "title": "Retrieving Top Weighted Triangles in Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern counting in graphs is a fundamental primitive for many network\nanalysis tasks, and a number of methods have been developed for scaling\nsubgraph counting to large graphs. Many real-world networks carry a natural\nnotion of strength of connection between nodes, which are often modeled by a\nweighted graph, but existing scalable graph algorithms for pattern mining are\ndesigned for unweighted graphs. Here, we develop a suite of deterministic and\nrandom sampling algorithms that enable the fast discovery of the 3-cliques\n(triangles) with the largest weight in a graph, where weight is measured by a\ngeneralized mean of a triangle's edges. For example, one of our proposed\nalgorithms can find the top-1000 weighted triangles of a weighted graph with\nbillions of edges in thirty seconds on a commodity server, which is orders of\nmagnitude faster than existing \"fast\" enumeration schemes. Our methods thus\nopen the door towards scalable pattern mining in weighted graphs.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 22:01:12 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Kumar", "Raunak", "", "Cornell University"], ["Liu", "Paul", "", "Stanford University"], ["Charikar", "Moses", "", "Stanford University"], ["Benson", "Austin R.", "", "Cornell University"]]}, {"id": "1910.00704", "submitter": "Philippe Trempe", "authors": "Philippe Trempe", "title": "Spherical k-Nearest Neighbors Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geospatial interpolation is a challenging task due to real world data often\nbeing sparse, heterogeneous and inconsistent. For that matter, this work\npresents SkNNI, a spherical interpolation algorithm capable of working with\nsuch challenging geospatial data. This work also presents NDDNISD an accurate\nand efficient interpolation function for SkNNI which shines due to its spatial\nawareness in terms of proximity and distribution of observation neighbors.\nSkNNI's open source implementation is also discussed and illustrated with a\nsimple usage example.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 22:55:07 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Trempe", "Philippe", ""]]}, {"id": "1910.00773", "submitter": "Kyle Fox", "authors": "Kyle Fox and Xinyi Li", "title": "Approximating the Geometric Edit Distance", "comments": "16 pages, ISAAC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edit distance is a measurement of similarity between two sequences such as\nstrings, point sequences, or polygonal curves. Many matching problems from a\nvariety of areas, such as signal analysis, bioinformatics, etc., need to be\nsolved in a geometric space. Therefore, the geometric edit distance (GED) has\nbeen studied. In this paper, we describe the first strictly sublinear\napproximate near-linear time algorithm for computing the GED of two point\nsequences in constant dimensional Euclidean space. Specifically, we present a\nrandomized (O(n\\log^2n)) time (O(\\sqrt n))-approximation algorithm. Then, we\ngeneralize our result to give a randomized $\\alpha$-approximation algorithm for\nany $\\alpha\\in [\\sqrt{\\log n}, \\sqrt{n / \\log n}]$, running in time\n$O(n^2/\\alpha^2 \\log n)$. Both algorithms are Monte Carlo and return\napproximately optimal solutions with high probability.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 04:34:11 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 17:45:31 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Fox", "Kyle", ""], ["Li", "Xinyi", ""]]}, {"id": "1910.00788", "submitter": "Hossein Esfandiari", "authors": "Hossein Esfandiari, Vahab Mirrokni, Peilin Zhong", "title": "Streaming Balanced Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering of data points in metric space is among the most fundamental\nproblems in computer science with plenty of applications in data mining,\ninformation retrieval and machine learning. Due to the necessity of clustering\nof large datasets, several streaming algorithms have been developed for\ndifferent variants of clustering problems such as $k$-median and $k$-means\nproblems. However, despite the importance of the context, the current\nunderstanding of balanced clustering (or more generally capacitated clustering)\nin the streaming setting is very limited. The only previously known streaming\napproximation algorithm for capacitated clustering requires three passes and\nonly handles insertions.\n  In this work, we develop \\emph{the first single pass streaming algorithm} for\na general class of clustering problems that includes capacitated $k$-median and\ncapacitated $k$-means in Euclidean space, using only poly$( k d \\log \\Delta)$\nspace, where $k$ is the number of clusters, $d$ is the dimension and $\\Delta$\nis the maximum relative range of a coordinate. (Note that $d\\log \\Delta$ is the\nspace required to represent one point.) This algorithm only violates the\ncapacity constraint by a $1+\\epsilon$ factor. Interestingly, unlike the\nprevious algorithm, our algorithm handles both insertions and deletions of\npoints. To provide this result we define a decomposition of the space via some\ncurved half-spaces. We used this decomposition to design a strong coreset of\nsize poly$( k d \\log \\Delta)$ for balanced clustering. Then, we show that this\ncoreset is implementable in the streaming and distributed settings.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 05:59:51 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Esfandiari", "Hossein", ""], ["Mirrokni", "Vahab", ""], ["Zhong", "Peilin", ""]]}, {"id": "1910.00831", "submitter": "Isaac Goldstein", "authors": "Isaac Goldstein, Moshe Lewenstein, Ely Porat", "title": "On the Hardness of Set Disjointness and Set Intersection with Bounded\n  Universe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the SetDisjointness problem, a collection of $m$ sets $S_1,S_2,...,S_m$\nfrom some universe $U$ is preprocessed in order to answer queries on the\nemptiness of the intersection of some two query sets from the collection. In\nthe SetIntersection variant, all the elements in the intersection of the query\nsets are required to be reported. These are two fundamental problems that were\nconsidered in several papers from both the upper bound and lower bound\nperspective.\n  Several conditional lower bounds for these problems were proven for the\ntradeoff between preprocessing and query time or the tradeoff between space and\nquery time. Moreover, there are several unconditional hardness results for\nthese problems in some specific computational models. The fundamental nature of\nthe SetDisjointness and SetIntersection problems makes them useful for proving\nthe conditional hardness of other problems from various areas. However, the\nuniverse of the elements in the sets may be very large, which may cause the\nreduction to some other problems to be inefficient and therefore it is not\nuseful for proving their conditional hardness.\n  In this paper, we prove the conditional hardness of SetDisjointness and\nSetIntersection with bounded universe. This conditional hardness is shown for\nboth the interplay between preprocessing and query time and the interplay\nbetween space and query time. Moreover, we present several applications of\nthese new conditional lower bounds. These applications demonstrates the\nstrength of our new conditional lower bounds as they exploit the limited\nuniverse size. We believe that this new framework of conditional lower bounds\nwith bounded universe can be useful for further significant applications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 08:31:52 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Goldstein", "Isaac", ""], ["Lewenstein", "Moshe", ""], ["Porat", "Ely", ""]]}, {"id": "1910.00868", "submitter": "Joan Boyar", "authors": "Joan Boyar, Kim S. Larsen, Denis Pankratov", "title": "Advice Complexity of Adaptive Priority Algorithms", "comments": "IMADA-preprint-cs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The priority model was introduced by Borodin, Rackoff, and Nielsen (2003) to\ncapture greedy-like algorithms. Motivated by the success of advice complexity\nin the area of online algorithms, Borodin et al. (2020) extended the fixed\npriority model to include an advice tape oracle. They also developed a\nreduction-based framework for proving lower bounds on the amount of advice\nrequired to achieve certain approximation ratios in this rather powerful model.\nIn order to capture most of the algorithms that are considered greedy-like, the\neven stronger model of adaptive priority algorithms is needed. We extend the\nadaptive priority model to include an advice tape oracle. We show how to modify\nthe reduction-based framework from the fixed priority case, making it\napplicable to the more powerful adaptive priority algorithms. The framework\nprovides a template, where one can obtain a lower bound relatively easily by\nexhibiting \"gadget patterns\" fulfilling given criteria. In the process, we\nsimplify the proof that the framework works, and we strengthen all the earlier\nlower bounds by a factor two.\n  As a motivating example, we present a purely combinatorial adaptive priority\nalgorithm with advice for Minimum Vertex Cover on triangle-free graphs of\nmaximum degree three. Our algorithm achieves optimality and uses at most 7n/22\nbits of advice. Known results imply that no adaptive priority algorithm without\nadvice can achieve optimality without advice, and we prove that 7n/22 is fewer\nbits than an online algorithm with advice needs to reach optimality.\n  Furthermore, we show connections between exact algorithms and priority\nalgorithms with advice. Priority algorithms with advice that achieve optimality\ncan be used to define corresponding exact algorithms, priority exact\nalgorithms. The lower bound templates for advice-based adaptive algorithms\nimply lower bounds on exact algorithms designed in this way.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 10:37:28 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 07:34:12 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 10:43:06 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Boyar", "Joan", ""], ["Larsen", "Kim S.", ""], ["Pankratov", "Denis", ""]]}, {"id": "1910.00887", "submitter": "Benjamin Bergougnoux", "authors": "Benjamin Bergougnoux, Charis Papadopoulos and Jan Arne Telle", "title": "Node Multiway Cut and Subset Feedback Vertex Set on Graphs of Bounded\n  Mim-width", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two weighted graph problems Node Multiway Cut (NMC) and Subset Feedback\nVertex Set (SFVS) both ask for a vertex set of minimum total weight, that for\nNMC disconnects a given set of terminals, and for SFVS intersects all cycles\ncontaining a vertex of a given set. We design a meta-algorithm that allows to\nsolve both problems in time $2^{O(rw^3)}\\cdot n^{4}$, $2^{O(q^2\\log(q))}\\cdot\nn^{4}$, and $n^{O(k^2)}$ where $rw$ is the rank-width, $q$ the\n$\\mathbb{Q}$-rank-width, and $k$ the mim-width of a given decomposition. This\nanswers in the affirmative an open question raised by Jaffke et al.\n(Algorithmica, 2019) concerning an XP algorithm for SFVS parameterized by\nmim-width.\n  By a unified algorithm, this solves both problems in polynomial-time on the\nfollowing graph classes: Interval, Permutation, and Bi-Interval graphs,\nCircular Arc and Circular Permutation graphs, Convex graphs, $k$-Polygon,\nDilworth-$k$ and Co-$k$-Degenerate graphs for fixed $k$; and also on Leaf Power\ngraphs if a leaf root is given as input, on $H$-Graphs for fixed $H$ if an\n$H$-representation is given as input, and on arbitrary powers of graphs in all\nthe above classes. Prior to our results, only SFVS was known to be tractable\nrestricted only on Interval and Permutation graphs, whereas all other results\nare new.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 11:45:52 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 09:23:29 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2019 11:33:46 GMT"}, {"version": "v4", "created": "Tue, 7 Jan 2020 14:45:29 GMT"}, {"version": "v5", "created": "Tue, 3 Mar 2020 10:32:58 GMT"}, {"version": "v6", "created": "Tue, 22 Sep 2020 11:27:13 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Bergougnoux", "Benjamin", ""], ["Papadopoulos", "Charis", ""], ["Telle", "Jan Arne", ""]]}, {"id": "1910.00901", "submitter": "Elazar Goldenberg", "authors": "Elazar Goldenberg, Robert Krauthgamer and Barna Saha", "title": "Sublinear Algorithms for Gap Edit Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edit distance is a way of quantifying how similar two strings are to one\nanother by counting the minimum number of character insertions, deletions, and\nsubstitutions required to transform one string into the other. A simple dynamic\nprogramming computes the edit distance between two strings of length $n$ in\n$O(n^2)$ time, and a more sophisticated algorithm runs in time $O(n+t^2)$ when\nthe edit distance is $t$ [Landau, Myers and Schmidt, SICOMP 1998]. In pursuit\nof obtaining faster running time, the last couple of decades have seen a flurry\nof research on approximating edit distance, including polylogarithmic\napproximation in near-linear time [Andoni, Krauthgamer and Onak, FOCS 2010],\nand a constant-factor approximation in subquadratic time [Chakrabarty, Das,\nGoldenberg, Kouck\\'y and Saks, FOCS 2018].\n  We study sublinear-time algorithms for small edit distance, which was\ninvestigated extensively because of its numerous applications. Our main result\nis an algorithm for distinguishing whether the edit distance is at most $t$ or\nat least $t^2$ (the quadratic gap problem) in time\n$\\tilde{O}(\\frac{n}{t}+t^3)$. This time bound is sublinear roughly for all $t$\nin $[\\omega(1), o(n^{1/3})]$, which was not known before. The best previous\nalgorithms solve this problem in sublinear time only for $t=\\omega(n^{1/3})$\n[Andoni and Onak, STOC 2009].\n  Our algorithm is based on a new approach that adaptively switches between\nuniform sampling and reading contiguous blocks of the input strings. In\ncontrast, all previous algorithms choose which coordinates to query\nnon-adaptively. Moreover, it can be extended to solve the $t$ vs\n$t^{2-\\epsilon}$ gap problem in time $\\tilde{O}(\\frac{n}{t^{1-\\epsilon}}+t^3)$.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 12:18:47 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Goldenberg", "Elazar", ""], ["Krauthgamer", "Robert", ""], ["Saha", "Barna", ""]]}, {"id": "1910.01047", "submitter": "Markus Hecher", "authors": "Johannes Klaus Fichte, Markus Hecher, Andreas Pfandler", "title": "Lower Bounds for QBFs of Bounded Treewidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DM cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of deciding the validity (QSAT) of quantified Boolean formulas\n(QBF) is a vivid research area in both theory and practice. In the field of\nparameterized algorithmics, the well-studied graph measure treewidth turned out\nto be a successful parameter. A well-known result by Chen in parameterized\ncomplexity is that QSAT when parameterized by the treewidth of the primal graph\nof the input formula together with the quantifier depth of the formula is\nfixed-parameter tractable. More precisely, the runtime of such an algorithm is\npolynomial in the formula size and exponential in the treewidth, where the\nexponential function in the treewidth is a tower, whose height is the\nquantifier depth. A natural question is whether one can significantly improve\nthese results and decrease the tower while assuming the Exponential Time\nHypothesis (ETH). In the last years, there has been a growing interest in the\nquest of establishing lower bounds under ETH, showing mostly problem-specific\nlower bounds up to the third level of the polynomial hierarchy. Still, an\nimportant question is to settle this as general as possible and to cover the\nwhole polynomial hierarchy. In this work, we show lower bounds based on the ETH\nfor arbitrary QBFs parameterized by treewidth (and quantifier depth). More\nformally, we establish lower bounds for QSAT and treewidth, namely, that under\nETH there cannot be an algorithm that solves QSAT of quantifier depth i in\nruntime significantly better than i-fold exponential in the treewidth and\npolynomial in the input size. In doing so, we provide a versatile reduction\ntechnique to compress treewidth that encodes the essence of dynamic programming\non arbitrary tree decompositions. Further, we describe a general methodology\nfor a more fine-grained analysis of problems parameterized by treewidth that\nare at higher levels of the polynomial hierarchy.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 16:09:22 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 21:05:13 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Fichte", "Johannes Klaus", ""], ["Hecher", "Markus", ""], ["Pfandler", "Andreas", ""]]}, {"id": "1910.01071", "submitter": "Ueverton Souza", "authors": "Gabriel L. Duarte, Daniel Lokshtanov, Lehilton L. C. Pedrosa, Rafael\n  C. S. Schouery and U\\'everton S. Souza", "title": "Computing the largest bond of a graph", "comments": "An extended abstract of this paper was published in IPEC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bond of a graph $G$ is an inclusion-wise minimal disconnecting set of $G$,\ni.e., bonds are cut-sets that determine cuts $[S,V\\setminus S]$ of $G$ such\nthat $G[S]$ and $G[V\\setminus S]$ are both connected. Given $s,t\\in V(G)$, an\n$st$-bond of $G$ is a bond whose removal disconnects $s$ and $t$. Contrasting\nwith the large number of studies related to maximum cuts, there are very few\nresults regarding the largest bond of general graphs. In this paper, we aim to\nreduce this gap on the complexity of computing the largest bond and the largest\n$st$-bond of a graph. Although cuts and bonds are similar, we remark that\ncomputing the largest bond of a graph tends to be harder than computing its\nmaximum cut. We show that {\\sc Largest Bond} remains NP-hard even for planar\nbipartite graphs, and it does not admit a constant-factor approximation\nalgorithm, unless $P = NP$. We also show that {\\sc Largest Bond} and {\\sc\nLargest $st$-Bond} on graphs of clique-width $w$ cannot be solved in time\n$f(w)\\times n^{o(w)}$ unless the Exponential Time Hypothesis fails, but they\ncan be solved in time $f(w)\\times n^{O(w)}$. In addition, we show that both\nproblems are fixed-parameter tractable when parameterized by the size of the\nsolution, but they do not admit polynomial kernels unless NP $\\subseteq$\ncoNP/poly.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 16:41:57 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Duarte", "Gabriel L.", ""], ["Lokshtanov", "Daniel", ""], ["Pedrosa", "Lehilton L. C.", ""], ["Schouery", "Rafael C. S.", ""], ["Souza", "U\u00e9verton S.", ""]]}, {"id": "1910.01073", "submitter": "Haotian Jiang", "authors": "Haotian Jiang, Janardhan Kulkarni, Sahil Singla", "title": "Online Geometric Discrepancy for Stochastic Arrivals with Applications\n  to Envy Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a unit interval $[0,1]$ in which $n$ points arrive one-by-one\nindependently and uniformly at random. On arrival of a point, the problem is to\nimmediately and irrevocably color it in $\\{+1,-1\\}$ while ensuring that every\ninterval $[a,b] \\subseteq [0,1]$ is nearly-balanced. We define\n\\emph{discrepancy} as the largest imbalance of any interval during the entire\nprocess. If all the arriving points were known upfront then we can color them\nalternately to achieve a discrepancy of $1$. What is the minimum possible\nexpected discrepancy when we color the points online?\n  We show that the discrepancy of the above problem is sub-polynomial in $n$\nand that no algorithm can achieve a constant discrepancy. This is a substantial\nimprovement over the trivial random coloring that only gets an\n$\\widetilde{O}(\\sqrt n)$ discrepancy. We then obtain similar results for a\nnatural generalization of this problem to $2$-dimensions where the points\narrive uniformly at random in a unit square. This generalization allows us to\nimprove recent results of Benade et al.\\cite{BenadeKPP-EC18} for the online\nenvy minimization problem when the arrivals are stochastic.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 16:43:57 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Jiang", "Haotian", ""], ["Kulkarni", "Janardhan", ""], ["Singla", "Sahil", ""]]}, {"id": "1910.01099", "submitter": "Florent Foucaud", "authors": "Florent Foucaud and Herv\\'e Hocquard and Dimitri Lajou and Valia\n  Mitsou and Th\\'eo Pierron", "title": "Parameterized complexity of edge-coloured and signed graph homomorphism\n  problems", "comments": "18 pages, 8 figures, 1 table. To appear in proceedings of IPEC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of graph modification problems for homomorphism-based\nproperties of edge-coloured graphs. A homomorphism from an edge-coloured graph\n$G$ to an edge-coloured graph $H$ is a vertex-mapping from $G$ to $H$ that\npreserves adjacencies and edge-colours. We consider the property of having a\nhomomorphism to a fixed edge-coloured graph $H$. Given an edge-coloured graph\n$G$, can we perform $k$ graph operations so that the resulting graph has a\nhomomorphism to $H$? The operations we consider are vertex-deletion,\nedge-deletion and switching (an operation that permutes the colours of the\nedges incident to a given vertex). Switching plays an important role in the\ntheory of signed graphs, that are $2$-edge-coloured graphs whose colours are\n$+$ and $-$. We denote the corresponding problems (parameterized by $k$) by\nVERTEX DELETION $H$-COLOURING, EDGE DELETION $H$-COLOURING and SWITCHING\n$H$-COLOURING. These generalise $H$-COLOURING (where one has to decide if an\ninput graph admits a homomorphism to $H$). Our main focus is when $H$ has order\nat most $2$, a case that includes standard problems such as VERTEX COVER, ODD\nCYCLE TRANSVERSAL and EDGE BIPARTIZATION. For such a graph $H$, we give a\nP/NP-complete complexity dichotomy for all three studied problems. Then, we\naddress their parameterized complexity. We show that all VERTEX DELETION\n$H$-COLOURING and EDGE DELETION $H$-COLOURING problems for such $H$ are FPT.\nThis is in contrast with the fact that already for some $H$ of order~$3$,\nunless P=NP, none of the three considered problems is in XP. We show that the\nsituation is different for SWITCHING $H$-COLOURING: there are three\n$2$-edge-coloured graphs $H$ of order $2$ for which this is W-hard, and\nassuming the ETH, admits no algorithm in time $f(k)n^{o(k)}$ for inputs of size\n$n$. For the other cases, SWITCHING $H$-COLOURING is FPT.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 17:31:50 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Foucaud", "Florent", ""], ["Hocquard", "Herv\u00e9", ""], ["Lajou", "Dimitri", ""], ["Mitsou", "Valia", ""], ["Pierron", "Th\u00e9o", ""]]}, {"id": "1910.01147", "submitter": "Serikzhan Kazi", "authors": "Meng He and Serikzhan Kazi", "title": "Path and Ancestor Queries on Trees with Multidimensional Weight Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider an ordinal tree $T$ on $n$ nodes, with each node assigned a\n  $d$-dimensional weight vector $\\pnt{w} \\in \\{1,2,\\ldots,n\\}^d,$ where $d \\in\n\\mathbb{N}$ is a constant.\n  We study path queries as generalizations of well-known {\\textit{orthogonal\nrange queries}}, with one of the dimensions being tree topology rather than a\nlinear order. Since in our definitions $d$ only represents the number of\ndimensions of the weight vector without taking the tree topology into account,\na path query in a tree with $d$-dimensional weight vectors generalize the\ncorresponding $(d+1)$-dimensional orthogonal range query.\n  We solve {\\textit{ancestor dominance reporting}} problem as a direct\ngeneralization of dominance reporting problem, %in time $\\O((\\lg^{d-1}\nn)/(\\lg\\lg n)^{d-2}+k)$ in time $\\O(\\lg^{d-1}{n}+k)$ %and space of $\\O(n(\\lg\nn)^{d-1}/(\\lg \\lg n)^{d-2})$ words, and space of $\\O(n\\lg^{d-2}n)$ words, where\n$k$ is the size of the output, for $d \\geq 2.$\n  We also achieve a tradeoff of $\\O(n\\lg^{d-2+\\eps}{n})$ words of space, with\nquery time of $\\O((\\lg^{d-1} n)/(\\lg\\lg n)^{d-2}+k),$ for the same problem,\nwhen $d \\geq 3.$\n  We solve {\\textit{path successor problem}} in $\\O(n\\lg^{d-1}{n})$ words of\nspace and time $\\O(\\lg^{d-1+\\eps}{n})$ for $d \\geq 1$ and an arbitrary constant\n$\\eps > 0.$ We propose a solution to {\\textit{path counting problem}}, with\n$\\O(n(\\lg{n}/\\lg\\lg{n})^{d-1})$ words of space and $\\O((\\lg{n}/\\lg\\lg{n})^{d})$\nquery time, for $d \\geq 1.$\n  Finally, we solve {\\textit{path reporting problem}} in\n$\\O(n\\lg^{d-1+\\eps}{n})$ words of space and\n$\\O((\\lg^{d-1}{n})/(\\lg\\lg{n})^{d-2}+k)$ query time, for $d \\geq 2.$\n  These results match or nearly match the best tradeoffs of the respective\nrange queries. We are also the first to solve path successor even for $d = 1$.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 18:09:04 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["He", "Meng", ""], ["Kazi", "Serikzhan", ""]]}, {"id": "1910.01293", "submitter": "Frank Stephan", "authors": "Gordon Hoi, Sanjay Jain and Frank Stephan", "title": "A Fast Exponential Time Algorithm for Max Hamming Distance X3SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X3SAT is the problem of whether one can satisfy a given set of clauses with\nup to three literals such that in every clause, exactly one literal is true and\nthe others are false. A related question is to determine the maximal Hamming\ndistance between two solutions of the instance. Dahll\\\"of provided an algorithm\nfor Maximum Hamming Distance XSAT, which is more complicated than the same\nproblem for X3SAT, with a runtime of $O(1.8348^n)$; Fu, Zhou and Yin considered\nMaximum Hamming Distance for X3SAT and found for this problem an algorithm with\nruntime $O(1.6760^n)$. In this paper, we propose an algorithm in $O(1.3298^n)$\ntime to solve the Max Hamming Distance X3SAT problem; the algorithm actually\ncounts for each $k$ the number of pairs of solutions which have Hamming\nDistance $k$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 03:45:17 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Hoi", "Gordon", ""], ["Jain", "Sanjay", ""], ["Stephan", "Frank", ""]]}, {"id": "1910.01296", "submitter": "Shinsaku Sakaue", "authors": "Shinsaku Sakaue and Naoki Marumo", "title": "Best-first Search Algorithm for Non-convex Sparse Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-convex sparse minimization (NSM), or $\\ell_0$-constrained minimization of\nconvex loss functions, is an important optimization problem that has many\nmachine learning applications. NSM is generally NP-hard, and so to exactly\nsolve NSM is almost impossible in polynomial time. As regards the case of\nquadratic objective functions, exact algorithms based on quadratic\nmixed-integer programming (MIP) have been studied, but no existing exact\nmethods can handle more general objective functions including Huber and\nlogistic losses; this is unfortunate since those functions are prevalent in\npractice. In this paper, we consider NSM with $\\ell_2$-regularized convex\nobjective functions and develop an algorithm by leveraging the efficiency of\nbest-first search (BFS). Our BFS can compute solutions with objective errors at\nmost $\\Delta\\ge0$, where $\\Delta$ is a controllable hyper-parameter that\nbalances the trade-off between the guarantee of objective errors and\ncomputation cost. Experiments demonstrate that our BFS is useful for solving\nmoderate-size NSM instances with non-quadratic objectives and that BFS is also\nfaster than the MIP-based method when applied to quadratic objectives.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 04:17:52 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Sakaue", "Shinsaku", ""], ["Marumo", "Naoki", ""]]}, {"id": "1910.01317", "submitter": "Haizi Yu", "authors": "Haizi Yu, Igor Mineyev, Lav R. Varshney", "title": "Orbit Computation for Atomically Generated Subgroups of Isometries of\n  $\\mathbb{Z}^n$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isometries are ubiquitous in nature; isometries of discrete (quantized)\nobjects---abstracted as the group of isometries of $\\mathbb{Z}^n$ denoted by\n$\\mathsf{ISO}(\\mathbb{Z}^n)$---are important concepts in the computational\nworld. In this paper, we compute various isometric invariances which\nmathematically are orbit-computation problems under various isometry-subgroup\nactions $H \\curvearrowright \\mathbb{Z}^n, H \\leq \\mathsf{ISO}(\\mathbb{Z}^n)$.\nOne computational challenge here is about the \\emph{infinite}: in general, we\ncan have an infinite subgroup acting on $\\mathbb{Z}^n$, resulting in possibly\nan infinite number of orbits of possibly infinite size. In practice, we\nrestrict the set of orbits (a partition of $\\mathbb{Z}^n$) to a finite subset\n$Z \\subseteq \\mathbb{Z}^n$ (a partition of $Z$), where $Z$ is specified a\npriori by an application domain or a data set. Our main contribution is an\nefficient algorithm to solve this \\emph{restricted} orbit-computation problem\nin the special case of \\emph{atomically generated subgroups}---a new notion\npartially motivated from interpretable AI. The atomic property is key to\npreserving the \\emph{semidirect-product structure}---the core structure we\nleverage to make our algorithm outperform generic approaches. Besides\nalgorithmic merit, our approach enables \\emph{parallel-computing}\nimplementations in many subroutines, which can further benefit from hardware\nboosts. Moreover, our algorithm works efficiently for \\emph{any} finite subset\n($Z$) regardless of the shape (continuous/discrete, (non)convex) or location;\nso it is application-independent.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 06:25:31 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 22:55:40 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Yu", "Haizi", ""], ["Mineyev", "Igor", ""], ["Varshney", "Lav R.", ""]]}, {"id": "1910.01327", "submitter": "Wanrong Zhang", "authors": "Rachel Cummings, Sara Krehbiel, Yuliia Lut, Wanrong Zhang", "title": "Privately detecting changes in unknown distributions", "comments": null, "journal-ref": "Proceedings of the International Conference on Machine Learning\n  (2020) Pages 958-968", "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The change-point detection problem seeks to identify distributional changes\nin streams of data. Increasingly, tools for change-point detection are applied\nin settings where data may be highly sensitive and formal privacy guarantees\nare required, such as identifying disease outbreaks based on hospital records,\nor IoT devices detecting activity within a home. Differential privacy has\nemerged as a powerful technique for enabling data analysis while preventing\ninformation leakage about individuals. Much of the prior work on change-point\ndetection---including the only private algorithms for this problem---requires\ncomplete knowledge of the pre-change and post-change distributions. However,\nthis assumption is not realistic for many practical applications of interest.\nThis work develops differentially private algorithms for solving the\nchange-point problem when the data distributions are unknown. Additionally, the\ndata may be sampled from distributions that change smoothly over time, rather\nthan fixed pre-change and post-change distributions. We apply our algorithms to\ndetect changes in the linear trends of such data streams. Finally, we also\nprovide experimental results to empirically validate the performance of our\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 07:05:59 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 22:35:49 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Cummings", "Rachel", ""], ["Krehbiel", "Sara", ""], ["Lut", "Yuliia", ""], ["Zhang", "Wanrong", ""]]}, {"id": "1910.01552", "submitter": "Vladimir Kolmogorov", "authors": "Vladimir Kolmogorov", "title": "Extensions of the Algorithmic Lovasz Local Lemma", "comments": "Superseded by arXiv:2008.05569", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider recent formulations of the algorithmic Lovasz Local Lemma by\nAchlioptas-Iliopoulos-Kolmogorov [2] and by Achlioptas-Iliopoulos-Sinclair [3].\nThese papers analyze a random walk algorithm for finding objects that avoid\nundesired \"bad events\" (or \"flaws\"), and prove that under certain conditions\nthe algorithm is guaranteed to find a \"flawless\" object quickly. We show that\nconditions proposed in these papers are incomparable, and introduce a new\nfamily of conditions that includes those in [2, 3] as special cases. We also\nconsider another condition that appeared in [3] in the context of sparse k-SAT\nformulas. This condition imposes a constraint for each variable of the problem,\nwhereas traditional LLL formulations impose a constraint for each clause.\nAchlioptas et al. handled the variable-based condition via a reduction to a\ndifferent condition and then applying a single-clause backtracking algorithm.\nWe propose a new condition that directly captures the sparse k-SAT application\nconsidered in [3], and allows the use of the standard local search algorithm\n(which offers important advantages such as parallelization). Finally, we extend\nour previous notion of \"commutativity\" from [20] and prove several implications\nof commutativity using some new tools that we develop. In particular, we\nsimplify the result of Iliopoulos [16] about approximating the LLL\ndistribution.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 15:31:06 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 14:18:59 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 04:12:42 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Kolmogorov", "Vladimir", ""]]}, {"id": "1910.01749", "submitter": "Cl\\'ement Canonne", "authors": "Omri Ben-Eliezer, Cl\\'ement L. Canonne, Shoham Letzter, and Erik\n  Waingarten", "title": "Finding monotone patterns in sublinear time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding monotone subsequences in an array from the\nviewpoint of sublinear algorithms. For fixed $k \\in \\mathbb{N}$ and\n$\\varepsilon > 0$, we show that the non-adaptive query complexity of finding a\nlength-$k$ monotone subsequence of $f \\colon [n] \\to \\mathbb{R}$, assuming that\n$f$ is $\\varepsilon$-far from free of such subsequences, is $\\Theta((\\log\nn)^{\\lfloor \\log_2 k \\rfloor})$. Prior to our work, the best algorithm for this\nproblem, due to Newman, Rabinovich, Rajendraprasad, and Sohler (2017), made\n$(\\log n)^{O(k^2)}$ non-adaptive queries; and the only lower bound known, of\n$\\Omega(\\log n)$ queries for the case $k = 2$, followed from that on testing\nmonotonicity due to Erg\\\"un, Kannan, Kumar, Rubinfeld, and Viswanathan (2000)\nand Fischer (2004).\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 22:27:56 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Ben-Eliezer", "Omri", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Letzter", "Shoham", ""], ["Waingarten", "Erik", ""]]}, {"id": "1910.01783", "submitter": "Ueverton Souza", "authors": "St\\'ephane Bessy, Marin Bougeret, Alan D.A. Carneiro, F\\'abio Protti,\n  U\\'everton S. Souza", "title": "Width Parameterizations for Knot-free Vertex Deletion on Digraphs", "comments": "An extended abstract of this paper was published in IPEC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A knot in a directed graph $G$ is a strongly connected subgraph $Q$ of $G$\nwith at least two vertices, such that no vertex in $V(Q)$ is an in-neighbor of\na vertex in $V(G)\\setminus V(Q)$. Knots are important graph structures, because\nthey characterize the existence of deadlocks in a classical distributed\ncomputation model, the so-called OR-model. Deadlock detection is correlated\nwith the recognition of knot-free graphs as well as deadlock resolution is\nclosely related to the {\\sc Knot-Free Vertex Deletion (KFVD)} problem, which\nconsists of determining whether an input graph $G$ has a subset $S \\subseteq\nV(G)$ of size at most $k$ such that $G[V\\setminus S]$ contains no knot. In this\npaper we focus on graph width measure parameterizations for {\\sc KFVD}. First,\nwe show that: (i) {\\sc KFVD} parameterized by the size of the solution $k$ is\nW[1]-hard even when $p$, the length of a longest directed path of the input\ngraph, as well as $\\kappa$, its Kenny-width, are bounded by constants, and we\nremark that {\\sc KFVD} is para-NP-hard even considering many directed width\nmeasures as parameters, but in FPT when parameterized by clique-width; (ii)\n{\\sc KFVD} can be solved in time $2^{O(tw)}\\times n$, but assuming ETH it\ncannot be solved in $2^{o(tw)}\\times n^{O(1)}$, where $tw$ is the treewidth of\nthe underlying undirected graph. Finally, since the size of a minimum directed\nfeedback vertex set ($dfv$) is an upper bound for the size of a minimum\nknot-free vertex deletion set, we investigate parameterization by $dfv$ and we\nshow that (iii) {\\sc KFVD} can be solved in FPT-time parameterized by either\n$dfv+\\kappa$ or $dfv+p$; and it admits a Turing kernel by the distance to a DAG\nhaving an Hamiltonian path.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 02:21:17 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Bessy", "St\u00e9phane", ""], ["Bougeret", "Marin", ""], ["Carneiro", "Alan D. A.", ""], ["Protti", "F\u00e1bio", ""], ["Souza", "U\u00e9verton S.", ""]]}, {"id": "1910.01788", "submitter": "Ruosong Wang", "authors": "Zhao Song, Ruosong Wang, Lin F. Yang, Hongyang Zhang, Peilin Zhong", "title": "Efficient Symmetric Norm Regression via Linear Sketching", "comments": "To appear in NeurIPS 2019. Fixed accidental missorting of author\n  names", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide efficient algorithms for overconstrained linear regression\nproblems with size $n \\times d$ when the loss function is a symmetric norm (a\nnorm invariant under sign-flips and coordinate-permutations). An important\nclass of symmetric norms are Orlicz norms, where for a function $G$ and a\nvector $y \\in \\mathbb{R}^n$, the corresponding Orlicz norm $\\|y\\|_G$ is defined\nas the unique value $\\alpha$ such that $\\sum_{i=1}^n G(|y_i|/\\alpha) = 1$. When\nthe loss function is an Orlicz norm, our algorithm produces a $(1 +\n\\varepsilon)$-approximate solution for an arbitrarily small constant\n$\\varepsilon > 0$ in input-sparsity time, improving over the previously\nbest-known algorithm which produces a $d \\cdot \\mathrm{polylog} n$-approximate\nsolution. When the loss function is a general symmetric norm, our algorithm\nproduces a $\\sqrt{d} \\cdot \\mathrm{polylog} n \\cdot\n\\mathrm{mmc}(\\ell)$-approximate solution in input-sparsity time, where\n$\\mathrm{mmc}(\\ell)$ is a quantity related to the symmetric norm under\nconsideration. To the best of our knowledge, this is the first input-sparsity\ntime algorithm with provable guarantees for the general class of symmetric norm\nregression problem. Our results shed light on resolving the universal sketching\nproblem for linear regression, and the techniques might be of independent\ninterest to numerical linear algebra problems more broadly.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 02:51:57 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 05:56:03 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Song", "Zhao", ""], ["Wang", "Ruosong", ""], ["Yang", "Lin F.", ""], ["Zhang", "Hongyang", ""], ["Zhong", "Peilin", ""]]}, {"id": "1910.01934", "submitter": "Rajesh Chitnis", "authors": "Rajesh Chitnis, Andreas Emil Feldmann", "title": "FPT Inapproximability of Directed Cut and Connectivity Problems", "comments": "Extended abstract in IPEC 2019. arXiv admin note: text overlap with\n  arXiv:1707.06499", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (see paper for full abstract)\n  Cut problems and connectivity problems on digraphs are two well-studied\nclasses of problems from the viewpoint of parameterized complexity. After a\nseries of papers over the last decade, we now have (almost) tight bounds for\nthe running time of several standard variants of these problems parameterized\nby two parameters: the number $k$ of terminals and the size $p$ of the\nsolution. When there is evidence of FPT intractability, then the next natural\nalternative is to consider FPT approximations. In this paper, we show two types\nof results for several directed cut and connectivity problems, building on\nexisting results from the literature: first is to circumvent the hardness\nresults for these problems by designing FPT approximation algorithms, or\nalternatively strengthen the existing hardness results by creating\n\"gap-instances\" under stronger hypotheses such as the (Gap-)Exponential Time\nHypothesis (ETH).\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 11:54:59 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Chitnis", "Rajesh", ""], ["Feldmann", "Andreas Emil", ""]]}, {"id": "1910.02048", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, \\.Ismail \\.Ilkan Ceylan", "title": "The Dichotomy of Evaluating Homomorphism-Closed Queries on Probabilistic\n  Graphs", "comments": "30 pages. Journal version of the ICDT'20 paper\n  https://drops.dagstuhl.de/opus/volltexte/2020/11939/. Submitted to LMCS. The\n  previous version (version 2) was the same as the ICDT'20 paper with some\n  minor formatting tweaks and 7 extra pages of technical appendix", "journal-ref": null, "doi": "10.4230/LIPIcs.ICDT.2020.5", "report-no": null, "categories": "cs.DB cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of probabilistic query evaluation on probabilistic\ngraphs, namely, tuple-independent probabilistic databases on signatures of\narity two. Our focus is the class of queries that is closed under\nhomomorphisms, or equivalently, the infinite unions of conjunctive queries. Our\nmain result states that all unbounded queries from this class are #P-hard for\nprobabilistic query evaluation. As bounded queries from this class are\nequivalent to a union of conjunctive queries, they are already classified by\nthe dichotomy of Dalvi and Suciu (2012). Hence, our result and theirs imply a\ncomplete data complexity dichotomy, between polynomial time and #P-hardness,\nfor evaluating infinite unions of conjunctive queries over probabilistic\ngraphs. This dichotomy covers in particular all fragments of infinite unions of\nconjunctive queries such as negation-free (disjunctive) Datalog, regular path\nqueries, and a large class of ontology-mediated queries on arity-two\nsignatures. Our result is shown by reducing from counting the valuations of\npositive partitioned 2-DNF formulae for some queries, or from the\nsource-to-target reliability problem in an undirected graph for other queries,\ndepending on properties of minimal models. The presented dichotomy result\napplies to even a special case of probabilistic query evaluation called\ngeneralized model counting, where fact probabilities must be 0, 0.5, or 1.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 17:19:35 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 11:37:45 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 10:32:42 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Amarilli", "Antoine", ""], ["Ceylan", "\u0130smail \u0130lkan", ""]]}, {"id": "1910.02057", "submitter": "Siddharth Gupta", "authors": "Giordano Da Lozzo, David Eppstein, Michael T. Goodrich, Siddharth\n  Gupta", "title": "C-Planarity Testing of Embedded Clustered Graphs with Bounded Dual\n  Carving-Width", "comments": "Extended version of the paper \"C-Planarity Testing of Embedded\n  Clustered Graphs with Bounded Dual Carving-Width\" to appear in the\n  Proceedings of the 14th International Symposium on Parameterized and Exact\n  Computation (IPEC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a clustered graph, i.e, a graph whose vertex set is recursively\npartitioned into clusters, the C-Planarity Testing problem asks whether it is\npossible to find a planar embedding of the graph and a representation of each\ncluster as a region homeomorphic to a closed disk such that 1. the subgraph\ninduced by each cluster is drawn in the interior of the corresponding disk, 2.\neach edge intersects any disk at most once, and 3. the nesting between clusters\nis reflected by the representation, i.e., child clusters are properly contained\nin their parent cluster. The computational complexity of this problem, whose\nstudy has been central to the theory of graph visualization since its\nintroduction in 1995 [Qing-Wen Feng, Robert F. Cohen, and Peter Eades.\nPlanarity for clustered graphs. ESA'95], has only been recently settled\n[Radoslav Fulek and Csaba D. T\\'oth. Atomic Embeddability, Clustered Planarity,\nand Thickenability. To appear at SODA'20]. Before such a breakthrough, the\ncomplexity question was still unsolved even when the graph has a prescribed\nplanar embedding, i.e, for embedded clustered graphs.\n  We show that the C-Planarity Testing problem admits a single-exponential\nsingle-parameter FPT algorithm for embedded clustered graphs, when\nparameterized by the carving-width of the dual graph of the input. This is the\nfirst FPT algorithm for this long-standing open problem with respect to a\nsingle notable graph-width parameter. Moreover, in the general case, the\npolynomial dependency of our FPT algorithm is smaller than the one of the\nalgorithm by Fulek and T\\'oth. To further strengthen the relevance of this\nresult, we show that the C-Planarity Testing problem retains its computational\ncomplexity when parameterized by several other graph-width parameters, which\nmay potentially lead to faster algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 17:32:37 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Da Lozzo", "Giordano", ""], ["Eppstein", "David", ""], ["Goodrich", "Michael T.", ""], ["Gupta", "Siddharth", ""]]}, {"id": "1910.02063", "submitter": "Quanquan C. Liu", "authors": "Sayan Bhattacharya, Fabrizio Grandoni, Janardhan Kulkarni, Quanquan C.\n  Liu, Shay Solomon", "title": "Fully Dynamic $(\\Delta+1)$-Coloring in Constant Update Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of (vertex) $(\\Delta+1)$-coloring a graph of maximum degree\n$\\Delta$ has been extremely well-studied over the years in various settings and\nmodels. Surprisingly, for the dynamic setting, almost nothing was known until\nrecently. In SODA'18, Bhattacharya, Chakrabarty, Henzinger and Nanongkai\ndevised a randomized data structure for maintaining a $(\\Delta+1)$-coloring\nwith $O(\\log \\Delta)$ expected amortized update time. In this paper, we present\na $(\\Delta+1)$-coloring data structure that achieves a constant amortized\nupdate time and show that this time bound holds not only in expectation but\nalso with high probability.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 17:43:21 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Bhattacharya", "Sayan", ""], ["Grandoni", "Fabrizio", ""], ["Kulkarni", "Janardhan", ""], ["Liu", "Quanquan C.", ""], ["Solomon", "Shay", ""]]}, {"id": "1910.02123", "submitter": "Wolfgang Mulzer", "authors": "\\'Edouard Bonnet, Sergio Cabello, Wolfgang Mulzer", "title": "Maximum Matchings in Geometric Intersection Graphs", "comments": "26 pages, this version fixes an error in the statements of Corollary\n  24 and Theorem 26", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be an intersection graph of $n$ geometric objects in the plane. We\nshow that a maximum matching in $G$ can be found in\n$O(\\rho^{3\\omega/2}n^{\\omega/2})$ time with high probability, where $\\rho$ is\nthe density of the geometric objects and $\\omega>2$ is a constant such that $n\n\\times n$ matrices can be multiplied in $O(n^\\omega)$ time.\n  The same result holds for any subgraph of $G$, as long as a geometric\nrepresentation is at hand. For this, we combine algebraic methods, namely\ncomputing the rank of a matrix via Gaussian elimination, with the fact that\ngeometric intersection graphs have small separators.\n  We also show that in many interesting cases, the maximum matching problem in\na general geometric intersection graph can be reduced to the case of bounded\ndensity. In particular, a maximum matching in the intersection graph of any\nfamily of translates of a convex object in the plane can be found in\n$O(n^{\\omega/2})$ time with high probability, and a maximum matching in the\nintersection graph of a family of planar disks with radii in $[1, \\Psi]$ can be\nfound in $O(\\Psi^6\\log^{11} n + \\Psi^{12 \\omega} n^{\\omega/2})$ time with high\nprobability.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 19:58:10 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 10:24:37 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Cabello", "Sergio", ""], ["Mulzer", "Wolfgang", ""]]}, {"id": "1910.02151", "submitter": "Gonzalo Navarro", "authors": "Tomasz Kociumaka, Gonzalo Navarro, Nicola Prezza", "title": "Towards a Definitive Compressibility Measure for Repetitive Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike in statistical compression, where Shannon's entropy is a definitive\nlower bound, no such clear measure exists for the compressibility of repetitive\nsequences. Since statistical entropy does not capture repetitiveness, ad-hoc\nmeasures like the size $z$ of the Lempel--Ziv parse are frequently used to\nestimate it. The size $b \\le z$ of the smallest bidirectional macro scheme\ncaptures better what can be achieved via copy-paste processes, though it is\nNP-complete to compute and it is not monotonic upon symbol appends. Recently, a\nmore principled measure, the size $\\gamma$ of the smallest string\n\\emph{attractor}, was introduced. The measure $\\gamma \\le b$ lower bounds all\nthe previous relevant ones, yet length-$n$ strings can be represented and\nefficiently indexed within space $O(\\gamma\\log\\frac{n}{\\gamma})$, which also\nupper bounds most measures. While $\\gamma$ is certainly a better measure of\nrepetitiveness than $b$, it is also NP-complete to compute and not monotonic,\nand it is unknown if one can always represent a string in $o(\\gamma\\log n)$\nspace.\n  In this paper, we study an even smaller measure, $\\delta \\le \\gamma$, which\ncan be computed in linear time, is monotonic, and allows encoding every string\nin $O(\\delta\\log\\frac{n}{\\delta})$ space because $z =\nO(\\delta\\log\\frac{n}{\\delta})$. We show that $\\delta$ better captures the\ncompressibility of repetitive strings. Concretely, we show that (1) $\\delta$\ncan be strictly smaller than $\\gamma$, by up to a logarithmic factor; (2) there\nare string families needing $\\Omega(\\delta\\log\\frac{n}{\\delta})$ space to be\nencoded, so this space is optimal for every $n$ and $\\delta$; (3) one can build\nrun-length context-free grammars of size $O(\\delta\\log\\frac{n}{\\delta})$,\nwhereas the smallest (non-run-length) grammar can be up to $\\Theta(\\log\nn/\\log\\log n)$ times larger; and (4) within $O(\\delta\\log\\frac{n}{\\delta})$\nspace we can not only...\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 21:17:16 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 20:06:38 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2021 12:58:27 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Kociumaka", "Tomasz", ""], ["Navarro", "Gonzalo", ""], ["Prezza", "Nicola", ""]]}, {"id": "1910.02179", "submitter": "Thiago Serra", "authors": "Thiago Serra, Teng Huang, Arvind Raghunathan, David Bergman", "title": "Template-based Minor Embedding for Adiabatic Quantum Optimization", "comments": "INFORMS Journal on Computing (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum Annealing (QA) can be used to quickly obtain near-optimal solutions\nfor Quadratic Unconstrained Binary Optimization (QUBO) problems. In QA\nhardware, each decision variable of a QUBO should be mapped to one or more\nadjacent qubits in such a way that pairs of variables defining a quadratic term\nin the objective function are mapped to some pair of adjacent qubits. However,\nqubits have limited connectivity in existing QA hardware. This has spurred work\non preprocessing algorithms for embedding the graph representing problem\nvariables with quadratic terms into the hardware graph representing qubits\nadjacencies, such as the Chimera graph in hardware produced by D-Wave Systems.\nIn this paper, we use integer linear programming to search for an embedding of\nthe problem graph into certain classes of minors of the Chimera graph, which we\ncall template embeddings. One of these classes corresponds to complete\nbipartite graphs, for which we show the limitation of the existing approach\nbased on minimum Odd Cycle Transversals (OCTs). One of the formulations\npresented is exact, and thus can be used to certify the absence of a minor\nembedding using that template. On an extensive test set consisting of random\ngraphs from five different classes of varying size and sparsity, we can embed\nmore graphs than a state-of-the-art OCT-based approach, our approach scales\nbetter with the hardware size, and the runtime is generally orders of magnitude\nsmaller.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 00:18:26 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 20:45:14 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Serra", "Thiago", ""], ["Huang", "Teng", ""], ["Raghunathan", "Arvind", ""], ["Bergman", "David", ""]]}, {"id": "1910.02248", "submitter": "Fr\\'ed\\'eric Protin", "authors": "Fr\\'ed\\'eric Protin", "title": "A new algorithm for graph center computation and graph partitioning\n  according to the distance to the center", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for finding the center of a graph, as well as the\nrank of each node in the hierarchy of distances to the center. In other words,\nour algorithm allows to partition the graph according to nodes distance to the\ncenter. Moreover, the algorithm is parallelizable. We compare the performances\nof our algorithm with the ones of Floyd-Warshall algorithm, which is\ntraditionally used for these purposes. We show that, for a large variety of\ngraphs, our algorithm outperforms the Floyd-Warshall algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 11:01:20 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Protin", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1910.02351", "submitter": "Aditya Kumar", "authors": "Evandro Menezes, Sebastian Pop, Aditya Kumar", "title": "Clustering case statements for indirect branch predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an O(nlogn) algorithm to compile a switch statement into jump\ntables. To generate jump tables that can be efficiently predicted by current\nhardware branch predictors, we added an upper bound on the number of entries\nfor each table. This modification of the previously best known algorithm\nreduces the complexity from O(n^2) to O(nlogn).\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 01:10:57 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 16:05:42 GMT"}, {"version": "v3", "created": "Sat, 2 Nov 2019 14:22:44 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Menezes", "Evandro", ""], ["Pop", "Sebastian", ""], ["Kumar", "Aditya", ""]]}, {"id": "1910.02438", "submitter": "Edoardo Galimberti", "authors": "Francesco Bonchi, Edoardo Galimberti, Aristides Gionis, Bruno\n  Ordozgoiti, Giancarlo Ruffo", "title": "Discovering Polarized Communities in Signed Networks", "comments": null, "journal-ref": "CIKM 2019, November 3-7, 2019, Beijing, China", "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signed networks contain edge annotations to indicate whether each interaction\nis friendly (positive edge) or antagonistic (negative edge). The model is\nsimple but powerful and it can capture novel and interesting structural\nproperties of real-world phenomena. The analysis of signed networks has many\napplications from modeling discussions in social media, to mining user reviews,\nand to recommending products in e-commerce sites. In this paper we consider the\nproblem of discovering polarized communities in signed networks. In particular,\nwe search for two communities (subsets of the network vertices) where within\ncommunities there are mostly positive edges while across communities there are\nmostly negative edges. We formulate this novel problem as a \"discrete\neigenvector\" problem, which we show to be NP-hard. We then develop two\nintuitive spectral algorithms: one deterministic, and one randomized with\nquality guarantee $\\sqrt{n}$ (where $n$ is the number of vertices in the\ngraph), tight up to constant factors. We validate our algorithms against\nnon-trivial baselines on real-world signed networks. Our experiments confirm\nthat our algorithms produce higher quality solutions, are much faster and can\nscale to much larger networks than the baselines, and are able to detect\nground-truth polarized communities.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 12:44:56 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Bonchi", "Francesco", ""], ["Galimberti", "Edoardo", ""], ["Gionis", "Aristides", ""], ["Ordozgoiti", "Bruno", ""], ["Ruffo", "Giancarlo", ""]]}, {"id": "1910.02459", "submitter": "Massimo Cafaro", "authors": "Massimo Cafaro, Catiuscia Melle, Marco Pulimeno, Italo Epicoco", "title": "Fast Detection of Outliers in Data Streams with the $Q_n$ Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FQN (Fast $Q_n$), a novel algorithm for fast detection of outliers\nin data streams. The algorithm works in the sliding window model, checking if\nan item is an outlier by cleverly computing the $Q_n$ scale estimator in the\ncurrent window. We thoroughly compare our algorithm for online $Q_n$ with the\nstate of the art competing algorithm by Nunkesser et al, and show that FQN (i)\nis faster, (ii) its computational complexity does not depend on the input\ndistribution and (iii) it requires less space. Extensive experimental results\non synthetic datasets confirm the validity of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 15:02:58 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 18:47:51 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Cafaro", "Massimo", ""], ["Melle", "Catiuscia", ""], ["Pulimeno", "Marco", ""], ["Epicoco", "Italo", ""]]}, {"id": "1910.02470", "submitter": "Guohui Lin", "authors": "Yong Chen, Zhi-Zhong Chen, Guohui Lin, Yao Xu, An Zhang", "title": "Approximation algorithms for maximally balanced connected graph\n  partition", "comments": "23 pages, 7 figures, accepted for presentation at COCOA 2019 (Xiamen,\n  China)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a simple connected graph $G = (V, E)$, we seek to partition the vertex\nset $V$ into $k$ non-empty parts such that the subgraph induced by each part is\nconnected, and the partition is maximally balanced in the way that the maximum\ncardinality of these $k$ parts is minimized. We refer this problem to as {\\em\nmin-max balanced connected graph partition} into $k$ parts and denote it as\n{\\sc $k$-BGP}. The general vertex-weighted version of this problem on trees has\nbeen studied since about four decades ago, which admits a linear time exact\nalgorithm; the vertex-weighted {\\sc $2$-BGP} and {\\sc $3$-BGP} admit a\n$5/4$-approximation and a $3/2$-approximation, respectively; but no\napproximability result exists for {\\sc $k$-BGP} when $k \\ge 4$, except a\ntrivial $k$-approximation. In this paper, we present another\n$3/2$-approximation for our cardinality {\\sc $3$-BGP} and then extend it to\nbecome a $k/2$-approximation for {\\sc $k$-BGP}, for any constant $k \\ge 3$.\nFurthermore, for {\\sc $4$-BGP}, we propose an improved $24/13$-approximation.\nTo these purposes, we have designed several local improvement operations, which\ncould be useful for related graph partition problems.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 16:24:36 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Chen", "Yong", ""], ["Chen", "Zhi-Zhong", ""], ["Lin", "Guohui", ""], ["Xu", "Yao", ""], ["Zhang", "An", ""]]}, {"id": "1910.02478", "submitter": "Matthew Francis-Landau", "authors": "Matthew Francis-Landau, Benjamin Van Durme", "title": "Exact and/or Fast Nearest Neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior methods for retrieval of nearest neighbors in high dimensions are fast\nand approximate--providing probabilistic guarantees of returning the correct\nanswer--or slow and exact performing an exhaustive search. We present Certified\nCosine, a novel approach to nearest-neighbors which takes advantage of\nstructure present in the cosine similarity distance metric to offer\ncertificates. When a certificate is constructed, it guarantees that the nearest\nneighbor set is correct, possibly avoiding an exhaustive search. Certified\nCosine's certificates work with high dimensional data and outperform previous\nexact nearest neighbor methods on these datasets.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 16:58:58 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 02:17:40 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Francis-Landau", "Matthew", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "1910.02569", "submitter": "Zhiyi Huang", "authors": "Zhiyi Huang, Runzhou Tao", "title": "Understanding Zadimoghaddam's Edge-weighted Online Matching Algorithm:\n  Unweighted Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article identifies a key algorithmic ingredient in the edge-weighted\nonline matching algorithm by Zadimoghaddam (2017) and presents a simplified\nalgorithm and its analysis to demonstrate how it works in the unweighted case.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 01:03:59 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Huang", "Zhiyi", ""], ["Tao", "Runzhou", ""]]}, {"id": "1910.02598", "submitter": "Dominique Orban", "authors": "Alexis Montoison and Dominique Orban", "title": "BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a\n  Quasi-Minimum Error Property", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.18287.59042", "report-no": "Cahier du GERAD G-2019-71", "categories": "math.NA cs.DS cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce an iterative method named BiLQ for solving general square linear\nsystems Ax = b based on the Lanczos biorthogonalization process defined by\nleast-norm subproblems, and that is a natural companion to BiCG and QMR.\nWhereas the BiCG (Fletcher, 1976), CGS (Sonneveld, 1989) and BiCGSTAB (van der\nVorst, 1992) iterates may not exist when the tridiagonal projection of A is\nsingular, BiLQ is reliable on compatible systems even if A is ill-conditioned\nor rank deficient. As in the symmetric case, the BiCG residual is often smaller\nthan the BiLQ residual and, when the BiCG iterate exists, an inexpensive\ntransfer from the BiLQ iterate is possible. Although the Euclidean norm of the\nBiLQ error is usually not monotonic, it is monotonic in a different norm that\ndepends on the Lanczos vectors. We establish a similar property for the QMR\n(Freund and Nachtigal, 1991) residual. BiLQ combines with QMR to take advantage\nof two initial vectors and solve a system and an adjoint system simultaneously\nat a cost similar to that of applying either method. We derive an analogous\ncombination of USYMLQ and USYMQR based on the orthogonal tridiagonalization\nprocess (Saunders, Simon, and Yip, 1988). The resulting combinations, named\nBiLQR and TriLQR, may be used to estimate integral functionals involving the\nsolution of a primal and an adjoint system. We compare BiLQR and TriLQR with\nMinres-qlp on a related augmented system, which performs a comparable amount of\nwork and requires comparable storage. In our experiments, BiLQR terminates\nearlier than TriLQR and MINRES-QLP in terms of residual and error of the primal\nand adjoint systems.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 04:03:41 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Montoison", "Alexis", ""], ["Orban", "Dominique", ""]]}, {"id": "1910.02611", "submitter": "Gaurav Gupta", "authors": "Gaurav Gupta, Minghao Yan, Benjamin Coleman, R. A. Leo Elworth, Tharun\n  Medini, Todd Treangen, Anshumali Shrivastava", "title": "RAMBO: Repeated And Merged BloOm Filter for Ultra-fast Multiple Set\n  Membership Testing (MSMT) on Large-Scale Data", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Set Membership Testing (MSMT) is a well-known problem in a variety\nof search and query applications. Given a dataset of K different sets and a\nquery q, it aims to find all of the sets containing the query. Trivially, an\nMSMT instance can be reduced to K membership testing instances, each with the\nsame q, leading to O(K) query time with a simple array of Bloom Filters. We\npropose a data-structure called RAMBO (Repeated And Merged BloOm Filter) that\nachieves O(\\sqrt{K} log K) query time in expectation with an additional\nworst-case memory cost factor of O(log K) beyond the array of Bloom Filters.\nDue to this, RAMBO is a very fast and accurate data-structure. Apart from being\nembarrassingly parallel, supporting cheap updates for streaming inputs, zero\nfalse-negative rate, and low false-positive rate, RAMBO beats the\nstate-of-the-art approaches for genome indexing methods: COBS (Compact\nbit-sliced signature index), Sequence Bloom Trees (a Bloofi based\nimplementation), HowDeSBT, SSBT, and document indexing methods like BitFunnel.\nThe proposed data-structure is simply a count-min sketch type arrangement of a\nmembership testing utility (Bloom Filter in our case). It indexes k-grams and\nprovides an approximate membership testing based search utility. The simplicity\nof the algorithm and embarrassingly parallel architecture allows us to index a\n170 TB genome dataset in a mere 14 hours on a cluster of 100 nodes while\ncompeting methods require weeks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 05:15:27 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 20:30:47 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Gupta", "Gaurav", ""], ["Yan", "Minghao", ""], ["Coleman", "Benjamin", ""], ["Elworth", "R. A. Leo", ""], ["Medini", "Tharun", ""], ["Treangen", "Todd", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1910.02639", "submitter": "Aur\\'elien Cavelan", "authors": "Aur\\'elien Cavelan, Rub\\'en M. Cabez\\'on, Jonas H. M. Korndorfer and\n  Florina M. Ciorba", "title": "Finding Neighbors in a Forest: A b-tree for Smoothed Particle\n  Hydrodynamics Simulations", "comments": "Adding a few references in Related Work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the exact close neighbors of each fluid element in mesh-free\ncomputational hydrodynamical methods, such as the Smoothed Particle\nHydrodynamics (SPH), often becomes a main bottleneck for scaling their\nperformance beyond a few million fluid elements per computing node. Tree\nstructures are particularly suitable for SPH simulation codes, which rely on\nfinding the exact close neighbors of each fluid element (or SPH particle). In\nthis work we present a novel tree structure, named \\textit{$b$-tree}, which\nfeatures an adaptive branching factor to reduce the depth of the neighbor\nsearch. Depending on the particle spatial distribution, finding neighbors using\n\\tree has an asymptotic best case complexity of $O(n)$, as opposed to $O(n \\log\nn)$ for other classical tree structures such as octrees and quadtrees. We also\npresent the proposed tree structure as well as the algorithms to build it and\nto find the exact close neighbors of all particles. We assess the scalability\nof the proposed tree-based algorithms through an extensive set of performance\nexperiments in a shared-memory system. Results show that b-tree is up to\n$12\\times$ faster for building the tree and up to $1.6\\times$ faster for\nfinding the exact neighbors of all particles when compared to its octree form.\nMoreover, we apply b-tree to a SPH code and show its usefulness over the\nexisting octree implementation, where b-tree is up to $5\\times$ faster for\nfinding the exact close neighbors compared to the legacy code.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 07:20:05 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 13:20:04 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Cavelan", "Aur\u00e9lien", ""], ["Cabez\u00f3n", "Rub\u00e9n M.", ""], ["Korndorfer", "Jonas H. M.", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1910.02665", "submitter": "Jason Li", "authors": "Jason Li", "title": "Faster Minimum k-cut of a Simple Graph", "comments": "FOCS 2019, 29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the (exact, minimum) $k$-cut problem: given a graph and an\ninteger $k$, delete a minimum-weight set of edges so that the remaining graph\nhas at least $k$ connected components. This problem is a natural generalization\nof the global minimum cut problem, where the goal is to break the graph into\n$k=2$ pieces.\n  Our main result is a (combinatorial) $k$-cut algorithm on simple graphs that\nruns in $n^{(1+o(1))k}$ time for any constant $k$, improving upon the\npreviously best $n^{(2\\omega/3+o(1))k}$ time algorithm of Gupta et\nal.~[FOCS'18] and the previously best $n^{(1.981+o(1))k}$ time combinatorial\nalgorithm of Gupta et al.~[STOC'19]. For combinatorial algorithms, this\nalgorithm is optimal up to $o(1)$ factors assuming recent hardness conjectures:\nwe show by a straightforward reduction that $k$-cut on even a simple graph is\nas hard as $(k-1)$-clique, establishing a lower bound of $n^{(1-o(1))k}$ for\n$k$-cut. This settles, up to lower-order factors, the complexity of $k$-cut on\na simple graph for combinatorial algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 08:28:29 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Li", "Jason", ""]]}, {"id": "1910.02851", "submitter": "Giovanni Schmid PhD.", "authors": "Ferdinando Montecuollo and Giovannni Schmid", "title": "ER-index: a referential index for encrypted genomic databases", "comments": "27 pages with detailed pseudocodes", "journal-ref": "Information Systems 96 (2021) 101668", "doi": "10.1016/j.is.2020.101668", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huge DBMSs storing genomic information are being created and engineerized for\ndoing large-scale, comprehensive and in-depth analysis of human beings and\ntheir diseases. However, recent regulations like the GDPR require that\nsensitive data are stored and elaborated thanks to privacy-by-design methods\nand software. We designed and implemented ER-index, a new full-text index in\nminute space which was optimized for compressing and encrypting collections of\ngenomic sequences, and for performing on them fast pattern-search queries. Our\nnew index complements the E2FM-index, which was introduced to compress and\nencrypt collections of nucleotide sequences without relying on a reference\nsequence. When used on collections of highly similar sequences, the ER-index\nallows to obtain compression ratios which are an order of magnitude smaller\nthan those achieved with the E2FM-index, but maintaining its very good search\nperformance. Moreover, thanks to the ER-index multi-user and multiple-keys\nencryption model, a single index can store the sequences related to a\npopulation of individuals so that users may perform search operations only on\nthe sequences to which they were granted access. The ER-index C++ source code\nplus scripts and data to assess the tool performance are available at:\nhttps://github.com/EncryptedIndexes/erindex.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 15:19:44 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 01:34:41 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Montecuollo", "Ferdinando", ""], ["Schmid", "Giovannni", ""]]}, {"id": "1910.02861", "submitter": "Steven Herbert", "authors": "Steven Herbert and Sathyawageeswar Subramanian", "title": "Spectral sparsification of matrix inputs as a preprocessing step for\n  quantum algorithms", "comments": "10 pages, 1 figure. Preliminary version, comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the potential utility of classical techniques of spectral\nsparsification of graphs as a preprocessing step for digital quantum\nalgorithms, in particular, for Hamiltonian simulation. Our results indicate\nthat spectral sparsification of a graph with $n$ nodes through a sampling\nmethod, e.g.\\ as in \\cite{Spielman2011resistances} using effective resistances,\ngives, with high probability, a locally computable matrix $\\tilde H$ with row\nsparsity at most $\\mathcal{O}(\\text{poly}\\log n)$. For a symmetric matrix $H$\nof size $n$ with $m$ non-zero entries, a one-time classical runtime overhead of\n$\\mathcal{O}(m||H||t\\log n/\\epsilon)$ expended in spectral sparsification is\nthen found to be useful as a way to obtain a sparse matrix $\\tilde H$ that can\nbe used to approximate time evolution $e^{itH}$ under the Hamiltonian $H$ to\nprecision $\\epsilon$. Once such a sparsifier is obtained, it could be used with\na variety of quantum algorithms in the query model that make crucial use of row\nsparsity. We focus on the case of efficient quantum algorithms for sparse\nHamiltonian simulation, since Hamiltonian simulation underlies, as a key\nsubroutine, several quantum algorithms, including quantum phase estimation and\nrecent ones for linear algebra. Finally, we also give two simple quantum\nalgorithms to estimate the row sparsity of an input matrix, which achieve a\nquery complexity of $\\mathcal{O}(n^{3/2})$ as opposed to $\\mathcal{O}(n^2)$\nthat would be required by any classical algorithm for the task.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 15:40:36 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Herbert", "Steven", ""], ["Subramanian", "Sathyawageeswar", ""]]}, {"id": "1910.03241", "submitter": "Jianshu Li", "authors": "Jianshu Li, Xiaoyin Hu, Junjie Luo, Jinchuan Cui", "title": "A Fast Exact Algorithm for Airplane Refueling Problem", "comments": "16 pages, 8 figures, 1 table. Accepted by COCOA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the airplane refueling problem, where we have a fleet of\nairplanes that can refuel each other. Each airplane is characterized by\nspecific fuel tank volume and fuel consumption rate, and the goal is to find a\ndrop out order of airplanes that last airplane in the air can reach as far as\npossible. This problem is equivalent to the scheduling problem $1||\\sum w_j (-\n\\frac{1}{C_j})$. Based on the dominance properties among jobs, we reveal some\nstructural properties of the problem and propose a recursive algorithm to solve\nthe problem exactly. The running time of our algorithm is directly related to\nthe number of schedules that do not violate the dominance properties. An\nexperimental study shows our algorithm outperforms state of the art exact\nalgorithms and is efficient on larger instances.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 07:05:26 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Li", "Jianshu", ""], ["Hu", "Xiaoyin", ""], ["Luo", "Junjie", ""], ["Cui", "Jinchuan", ""]]}, {"id": "1910.03249", "submitter": "Arne Schmidt", "authors": "S\\'andor P. Fekete, Jonas Grosse-Holz, Phillip Keldenich, Arne Schmidt", "title": "Parallel Online Algorithms for the Bin Packing Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study \\emph{parallel} online algorithms:\n  For some fixed integer $k$, a collective of $k$ parallel processes that\nperform online decisions on the same sequence of events forms a $k$-\\emph{copy\nalgorithm}.\n  For any given time and input sequence, the overall performance is determined\nby the best of the $k$ individual total results.\n  Problems of this type have been considered for online makespan minimization;\nthey are also related to optimization with \\emph{advice} on future events,\ni.e., a number of bits available in advance.\n  We develop \\textsc{Predictive Harmonic}$_3$ (PH3), a relatively simple family\nof $k$-copy algorithms for the online Bin Packing Problem, whose joint\ncompetitive factor converges to 1.5 for increasing $k$. In particular, we show\nthat $k=6$ suffices to guarantee a factor of $1.5714$ for PH3, which is better\nthan $1.57829$, the performance of the best known 1-copy algorithm\n\\textsc{Advanced Harmonic}, while $k=11$ suffices to achieve a factor of\n$1.5406$, beating the known lower bound of $1.54278$ for a single online\nalgorithm.\n  In the context of online optimization with advice, our approach implies that\n4 bits suffice to achieve a factor better than this bound of $1.54278$, which\nis considerably less than the previous bound of 15 bits.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 07:22:45 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Fekete", "S\u00e1ndor P.", ""], ["Grosse-Holz", "Jonas", ""], ["Keldenich", "Phillip", ""], ["Schmidt", "Arne", ""]]}, {"id": "1910.03287", "submitter": "Zhiyi Huang", "authors": "Zhiyi Huang", "title": "Understanding Zadimoghaddam's Edge-weighted Online Matching Algorithm:\n  Weighted Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a simplification of Zadimoghaddam's algorithm for the\nedge-weighted online bipartite matching problem, under the online primal dual\nframework. In doing so, we obtain an improved competitive ratio of $0.514$. We\nfirst combine the online correlated selection (OCS), an ingredient distilled\nfrom Zadimoghaddam (2017) by Huang and Tao (2019), and an interpretation of the\nedge-weighted online bipartite matching problem by Devanur et al. (2016) which\nwe will refer to as the complementary cumulative distribution function (CCDF)\nviewpoint, to derive an online primal dual algorithm that is\n$0.505$-competitive. Then, we design an improved OCS which gives the $0.514$\nratio.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 09:04:22 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Huang", "Zhiyi", ""]]}, {"id": "1910.03332", "submitter": "Xiaowei Wu", "authors": "Monika Henzinger and Xiaowei Wu", "title": "Upper and Lower Bounds for Fully Retroactive Graph Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classic dynamic data structure problems maintain a data structure subject to\na sequence S of updates and they answer queries using the latest version of the\ndata structure, i.e., the data structure after processing the whole sequence.\nTo handle operations that change the sequence S of updates, Demaine et al.\n(TALG 2007) introduced retroactive data structures. A retroactive operation\nmodifies the update sequence S in a given position t, called time, and either\ncreates or cancels an update in S at time t. A partially retroactive data\nstructure restricts queries to be executed exclusively in the latest version of\nthe data structure. A fully retroactive data structure supports queries at any\ntime t: a query at time t is answered using only the updates of S up to time t.\nIf the sequence S only consists of insertions, the resulting data structure is\nan incremental retroactive data structure. While efficient retroactive data\nstructures have been proposed for classic data structures, e.g., stack,\npriority queue and binary search tree, the retroactive version of graph\nproblems are rarely studied.\n  In this paper we study retroactive graph problems including connectivity,\nminimum spanning forest (MSF), maximum degree, etc. We provide fully\nretroactive data structures for maintaining the maximum degree, connectivity\nand MSF in $\\tilde{O}(n)$ time per operation. We also give an algorithm for the\nincremental fully retroactive connectivity with $\\tilde{O}(1)$ time per\noperation. We compliment our algorithms with almost tight hardness results. We\nshow that under the OMv conjecture (proposed by Henzinger et al. (STOC 2015)),\nthere does not exist fully retroactive data structures maintaining connectivity\nor MSF, or incremental fully retroactive data structure maintaining the maximum\ndegree with $O(n^{1-\\epsilon})$ time per operation, for any constant $\\epsilon\n> 0$.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 10:48:29 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 07:47:44 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Henzinger", "Monika", ""], ["Wu", "Xiaowei", ""]]}, {"id": "1910.03367", "submitter": "Markus Sinnl", "authors": "Markus Sinnl", "title": "A note on computational approaches for the antibandwidth problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we consider the antibandwidth problem, also known as dual\nbandwidth problem, separation problem and maximum differential coloring\nproblem. Given a labeled graph (i.e., a numbering of the vertices of a graph),\nthe antibandwidth of a node is defined as the minimum absolute difference of\nits labeling to the labeling of all its adjacent vertices. The goal in the\nantibandwidth problem is to find a labeling maximizing the antibandwidth. The\nproblem is NP-hard in general graphs and has applications in diverse areas like\nscheduling, radio frequency assignment, obnoxious facility location and\nmap-coloring. There has been much work on deriving theoretical bounds for the\nproblem and also in the design of metaheuristics in recent years. However, the\noptimality gaps between the best known solution values and reported upper\nbounds for the HarwellBoeing Matrix-instances, which are the commonly used\nbenchmark instances for this problem, are often very large (e.g., up to 577%).\nThe upper bounds reported in literature are based on the theoretical bounds\ninvolving simple graph characteristics, i.e., size, order and degree, and a\nmixed-integer programming (MIP) model. We present new MIP models for the\nproblem, together with valid inequalities, and design a branch-and-cut\nalgorithm and an iterative solution algorithm based on them. These algorithms\nalso include two starting heuristics and a primal heuristic. We also present a\nconstraint programming approach, and calculate upper bounds based on the\nstability number and chromatic number. Our computational study shows that the\ndeveloped approaches allow to find the proven optimal solution for eight\ninstances from literature, where the optimal solution was unknown and also\nprovide reduced gaps for eleven additional instances, including improved\nsolution values for seven instances, the largest optimality gap is now 46%.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 12:38:35 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Sinnl", "Markus", ""]]}, {"id": "1910.03409", "submitter": "Du\\v{s}an Knop", "authors": "Matthias Bentert, Klaus Heeger, Du\\v{s}an Knop", "title": "Length-Bounded Cuts: Proper Interval Graphs and Structural Parameters", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the presented paper we study the Length-Bounded Cut problem for special\ngraph classes as well as from a parameterized-complexity viewpoint. Here, we\nare given a graph $G$, two vertices $s$ and $t$, and positive integers $\\beta$\nand $\\lambda$. The task is to find a set of edges $F$ of size at most $\\beta$\nsuch that every $s$-$t$-path of length at most $\\lambda$ in $G$ contains some\nedge in $F$.\n  Bazgan et al. conjectured that Length-Bounded Cut admits a polynomial-time\nalgorithm if the input graph $G$ is a~proper interval graph. We confirm this\nconjecture by showing a dynamic-programming based polynomial-time algorithm. We\nstrengthen the W[1]-hardness result of Dvo\\v{r}\\'ak and Knop. Our reduction is\nshorter, seems simpler to describe, and the target of the reduction has\nstronger structural properties. Consequently, we give W[1]-hardness for the\ncombined parameter pathwidth and maximum degree of the input graph. Finally, we\nprove that Length-Bounded Cut is W[1]-hard for the feedback vertex number. Both\nour hardness results complement known XP algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 14:09:50 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Bentert", "Matthias", ""], ["Heeger", "Klaus", ""], ["Knop", "Du\u0161an", ""]]}, {"id": "1910.03438", "submitter": "Laurent Viennot", "authors": "Guillaume Ducoffe, Michel Habib (IRIF (UMR\\_8243)), Laurent Viennot\n  (GANG)", "title": "Fast Diameter Computation within Split Graphs", "comments": null, "journal-ref": "COCOA 2019 - 13th Annual International Conference on Combinatorial\n  Optimization and Applications, Dec 2019, Xiamen, China", "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When can we compute the diameter of a graph in quasi linear time? We address\nthis question for the class of split graphs, that we observe to be the hardest\ninstances for deciding whether the diameter is at most two. We stress that\nalthough the diameter of a non-complete split graph canonly be either 2 or 3,\nunder the Strong Exponential-Time Hypothesis (SETH) we cannot compute the\ndiameter of an $n$-vertex $m$-edge split graph in less than quadratic time --\nin the size $n + m$ of the input. Therefore it is worth to study the complexity\nof diameter computation on subclasses of split graphs, in order to better\nunderstand the complexity border. Specifically, we consider the split graphs\nwith bounded clique-interval number and their complements, with the former\nbeing a natural variation of the concept of interval number for split graphs\nthat we introduce in this paper. We first discuss the relations between the\nclique-interval number and other graph invariants such as the classic interval\nnumber of graphs, the treewidth, the VC-dimension and the stabbing number of a\nrelated hypergraph. Then, in part based on these above relations, we almost\ncompletely settle the complexity of diameter computation on these subclasses of\nsplit graphs:$\\bullet$ For the $k$-clique-interval split graphs, we can compute\ntheir diameter in truly subquadratic time if $k = \\mathcal{O}(1)$, and even in\nquasi linear time if $k = o(log n)$ and in addition a corresponding ordering of\nthe vertices in the clique is given. However, under SETH this cannot be done\nintruly subquadratic time for any $k = \\omega(log n)$.$\\bullet$ For the\ncomplements of $k$-clique-interval split graphs, we can compute their diameter\nin truly subquadratic time if $k = \\mathcal{O}(1)$, and even in time\n$\\mathcal{O}(km)$ if a corresponding ordering of the vertices in the stable set\nis given. Again this latter result is optimal under SETH up to polylogarithmic\nfactors.Our findings raise the question whether a $k$-clique interval ordering\ncan always be computed in quasi linear time. We prove that it is the case for\n$k = 1$ and for some subclasses such as boundedtreewidth split graphs,\nthreshold graphs and comparability split graphs. Finally, we prove thatsome\nimportant subclasses of split graphs -- including the ones mentioned above --\nhave a bounded clique-interval number.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 14:57:36 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 11:22:44 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 13:10:28 GMT"}, {"version": "v4", "created": "Fri, 21 May 2021 07:26:55 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Ducoffe", "Guillaume", "", "IRIF"], ["Habib", "Michel", "", "IRIF"], ["Viennot", "Laurent", "", "GANG"]]}, {"id": "1910.03553", "submitter": "Ananda Theertha Suresh", "authors": "Ananda Theertha Suresh", "title": "Differentially private anonymized histograms", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a dataset of label-count pairs, an anonymized histogram is the multiset\nof counts. Anonymized histograms appear in various potentially sensitive\ncontexts such as password-frequency lists, degree distribution in social\nnetworks, and estimation of symmetric properties of discrete distributions.\nMotivated by these applications, we propose the first differentially private\nmechanism to release anonymized histograms that achieves near-optimal privacy\nutility trade-off both in terms of number of items and the privacy parameter.\nFurther, if the underlying histogram is given in a compact format, the proposed\nalgorithm runs in time sub-linear in the number of items. For anonymized\nhistograms generated from unknown discrete distributions, we show that the\nreleased histogram can be directly used for estimating symmetric properties of\nthe underlying distribution.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 17:24:32 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 20:54:42 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Suresh", "Ananda Theertha", ""]]}, {"id": "1910.03578", "submitter": "Luca Ferrari", "authors": "Giulio Cerbai, Lapo Cioni, Luca Ferrari", "title": "Stack Sorting with Increasing and Decreasing Stacks", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a sorting machine consisting of $k+1$ stacks in series: the\nfirst $k$ stacks can only contain elements in decreasing order from top to\nbottom, while the last one has the opposite restriction. This device\ngeneralizes \\cite{SM}, which studies the case $k=1$. Here we show that, for\n$k=2$, the set of sortable permutations is a class with infinite basis, by\nexplicitly finding an antichain of minimal nonsortable permutations. This\nconstruction can easily be adapted to each $k \\ge 3$. Next we describe an\noptimal sorting algorithm, again for the case $k=2$. We then analyze two types\nof left-greedy sorting procedures, obtaining complete results in one case and\nonly some partial results in the other one. We close the paper by discussing a\nfew open questions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 09:12:47 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Cerbai", "Giulio", ""], ["Cioni", "Lapo", ""], ["Ferrari", "Luca", ""]]}, {"id": "1910.03640", "submitter": "Jaein Lim", "authors": "Jaein Lim, Panagiotis Tsiotras", "title": "MAMS-A*: Multi-Agent Multi-Scale A*", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multi-scale forward search algorithm for distributed agents to\nsolve single-query shortest path planning problems. Each agent first builds a\nrepresentation of its own search space of the common environment as a\nmulti-resolution graph, it communicates with the other agents the result of its\nlocal search, and it uses received information from other agents to refine its\nown graph and update the local inconsistency conditions. As a result, all\nagents attain a common subgraph that includes a provably optimal path in the\nmost informative graph available among all agents, if one exists, without\nnecessarily communicating the entire graph. We prove the completeness and\noptimality of the proposed algorithm, and present numerical results supporting\nthe advantages of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 18:51:15 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Lim", "Jaein", ""], ["Tsiotras", "Panagiotis", ""]]}, {"id": "1910.03645", "submitter": "Francesco Gullo", "authors": "Edoardo Galimberti, Martino Ciaperoni, Alain Barrat, Francesco Bonchi,\n  Ciro Cattuto, Francesco Gullo", "title": "Span-core Decomposition for Temporal Networks: Algorithms and\n  Applications", "comments": "ACM Transactions on Knowledge Discovery from Data (TKDD), 2020. arXiv\n  admin note: substantial text overlap with arXiv:1808.09376", "journal-ref": "ACM Transactions on Knowledge Discovery from Data 15 (1):2 (2020)", "doi": "10.1145/3418226", "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When analyzing temporal networks, a fundamental task is the identification of\ndense structures (i.e., groups of vertices that exhibit a large number of\nlinks), together with their temporal span (i.e., the period of time for which\nthe high density holds). In this paper we tackle this task by introducing a\nnotion of temporal core decomposition where each core is associated with two\nquantities, its coreness, which quantifies how densely it is connected, and its\nspan, which is a temporal interval: we call such cores \\emph{span-cores}.\n  For a temporal network defined on a discrete temporal domain $T$, the total\nnumber of time intervals included in $T$ is quadratic in $|T|$, so that the\ntotal number of span-cores is potentially quadratic in $|T|$ as well. Our first\nmain contribution is an algorithm that, by exploiting containment properties\namong span-cores, computes all the span-cores efficiently. Then, we focus on\nthe problem of finding only the \\emph{maximal span-cores}, i.e., span-cores\nthat are not dominated by any other span-core by both their coreness property\nand their span. We devise a very efficient algorithm that exploits theoretical\nfindings on the maximality condition to directly extract the maximal ones\nwithout computing all span-cores.\n  Finally, as a third contribution, we introduce the problem of \\emph{temporal\ncommunity search}, where a set of query vertices is given as input, and the\ngoal is to find a set of densely-connected subgraphs containing the query\nvertices and covering the whole underlying temporal domain $T$. We derive a\nconnection between this problem and the problem of finding (maximal)\nspan-cores. Based on this connection, we show how temporal community search can\nbe solved in polynomial-time via dynamic programming, and how the maximal\nspan-cores can be profitably exploited to significantly speed-up the basic\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 12:44:35 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 22:14:07 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Galimberti", "Edoardo", ""], ["Ciaperoni", "Martino", ""], ["Barrat", "Alain", ""], ["Bonchi", "Francesco", ""], ["Cattuto", "Ciro", ""], ["Gullo", "Francesco", ""]]}, {"id": "1910.03651", "submitter": "Mirza Galib Anwarul Husain Baig", "authors": "Mirza Galib Anwarul Husain Baig and Deepanjan Kesh", "title": "Improved Bounds for Two Query Adaptive Bitprobe Schemes Storing Five\n  Elements", "comments": "This paper is accepted in the proceeding of COCOA 2019. arXiv admin\n  note: substantial text overlap with arXiv:1810.13331", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study two-bitprobe adaptive schemes storing five elements.\nFor these class of schemes, the best known lower bound is m^{1/2} due to Alon\nand Feige [SODA 2009]. Recently, it was proved by Kesh [FSTTCS 2018] that\ntwo-bitprobe adaptive schemes storing three elements will take at least m^{2/3}\nspace, which also puts a lower bound on schemes storing five elements. In this\nwork, we have improved the lower bound to m^{3/4}. We also present a scheme for\nthe same that takes O(m^{5/6}) space. This improves upon the\nO(m^{18/19})-scheme due to Garg [Ph.D. Thesis] and the O(m^{10/11})-scheme due\nto Baig et al. [WALCOM 2019].\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 09:48:43 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 23:51:57 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Baig", "Mirza Galib Anwarul Husain", ""], ["Kesh", "Deepanjan", ""]]}, {"id": "1910.03798", "submitter": "Hossein Esfandiari", "authors": "Hossein Esfandiari, MohammadTaghi HajiAghayi, Brendan Lucier, Michael\n  Mitzenmacher", "title": "Prophets, Secretaries, and Maximizing the Probability of Choosing the\n  Best", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose a customer is faced with a sequence of fluctuating prices, such as\nfor airfare or a product sold by a large online retailer. Given distributional\ninformation about what price they might face each day, how should they choose\nwhen to purchase in order to maximize the likelihood of getting the best price\nin retrospect? This is related to the classical secretary problem, but with\nvalues drawn from known distributions. In their pioneering work, Gilbert and\nMosteller [\\textit{J. Amer. Statist. Assoc. 1966}] showed that when the values\nare drawn i.i.d., there is a thresholding algorithm that selects the best value\nwith probability approximately $0.5801$. However, the more general problem with\nnon-identical distributions has remained unsolved.\n  In this paper we provide an algorithm for the case of non-identical\ndistributions that selects the maximum element with probability $1/e$, and we\nshow that this is tight. We further show that if the observations arrive in a\nrandom order, this barrier of $1/e$ can be broken using a static threshold\nalgorithm, and we show that our success probability is the best possible for\nany single-threshold algorithm under random observation order. Moreover, we\nprove that one can achieve a strictly better success probability using more\ngeneral multi-threshold algorithms, unlike the non-random-order case. Along the\nway, we show that the best achievable success probability for the random-order\ncase matches that of the i.i.d.\\ case, which is approximately $0.5801$, under a\n\"no-superstars\" condition that no single distribution is very likely ex ante to\ngenerate the maximum value. We also extend our results to the problem of\nselecting one of the $k$ best values.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 05:36:04 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Esfandiari", "Hossein", ""], ["HajiAghayi", "MohammadTaghi", ""], ["Lucier", "Brendan", ""], ["Mitzenmacher", "Michael", ""]]}, {"id": "1910.04126", "submitter": "Tal Wagner", "authors": "Arturs Backurs, Yihe Dong, Piotr Indyk, Ilya Razenshteyn, Tal Wagner", "title": "Scalable Nearest Neighbor Search for Optimal Transport", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Optimal Transport (a.k.a. Wasserstein) distance is an increasingly\npopular similarity measure for rich data domains, such as images or text\ndocuments. This raises the necessity for fast nearest neighbor search\nalgorithms according to this distance, which poses a substantial computational\nbottleneck on massive datasets. In this work we introduce Flowtree, a fast and\naccurate approximation algorithm for the Wasserstein-$1$ distance. We formally\nanalyze its approximation factor and running time. We perform extensive\nexperimental evaluation of nearest neighbor search algorithms in the $W_1$\ndistance on real-world dataset. Our results show that compared to previous\nstate of the art, Flowtree achieves up to $7.4$ times faster running time.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 17:12:41 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 14:54:37 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 00:48:34 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 02:54:09 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Backurs", "Arturs", ""], ["Dong", "Yihe", ""], ["Indyk", "Piotr", ""], ["Razenshteyn", "Ilya", ""], ["Wagner", "Tal", ""]]}, {"id": "1910.04134", "submitter": "Canh Pham Pham", "authors": "Canh V. Pham, Hieu V. Duong, and My T. Thai", "title": "Cost-aware Targeted Viral Marketing: Approximation with Less Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cost-aware Targeted Viral Marketing (CTVM), a generalization of Influence\nMaximization (IM), has received a lot of attentions recently due to its\ncommercial values. Previous approximation algorithms for this problem required\na large number of samples to ensure approximate guarantee. In this paper, we\npropose an efficient approximation algorithm which uses fewer samples but\nprovides the same theoretical guarantees based on generating and using\nimportant samples in its operation. Experiments on real social networks show\nthat our proposed method outperforms the state-of-the-art algorithm which\nprovides the same approximation ratio in terms of the number of required\nsamples and running time.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 03:26:23 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 16:51:07 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Pham", "Canh V.", ""], ["Duong", "Hieu V.", ""], ["Thai", "My T.", ""]]}, {"id": "1910.04278", "submitter": "Kyle Fox", "authors": "Erin W. Chambers and Jeff Erickson and Kyle Fox and Amir Nayyeri", "title": "Minimum Cuts in Surface Graphs", "comments": "Unifies and improves upon contributions by different subsets of the\n  authors that appeared in SoCG 2009, SODA 2011, and SODA 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe algorithms to efficiently compute minimum $(s,t)$-cuts and global\nminimum cuts of undirected surface-embedded graphs. Given an edge-weighted\nundirected graph $G$ with $n$ vertices embedded on an orientable surface of\ngenus $g$, our algorithms can solve either problem in $g^{O(g)} n \\log \\log n$\nor $2^{O(g)} n \\log n$ time, whichever is better. When $g$ is a constant, our\n$g^{O(g)} n \\log \\log n$ time algorithms match the best running times known for\ncomputing minimum cuts in planar graphs.\n  Our algorithms for minimum cuts rely on reductions to the problem of finding\na minimum-weight subgraph in a given $\\mathbb{Z}_2$-homology class, and we give\nefficient algorithms for this latter problem as well. If $G$ is embedded on a\nsurface with $b$ boundary components, these algorithms run in $(g + b)^{O(g +\nb)} n \\log \\log n$ and $2^{O(g + b)} n \\log n$ time. We also prove that finding\na minimum-weight subgraph homologous to a single input cycle is NP-hard,\nshowing it is likely impossible to improve upon the exponential dependencies on\n$g$ for this latter problem.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 22:15:08 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Chambers", "Erin W.", ""], ["Erickson", "Jeff", ""], ["Fox", "Kyle", ""], ["Nayyeri", "Amir", ""]]}, {"id": "1910.04320", "submitter": "Zhongyang Li", "authors": "Zhongyang Li", "title": "Exact Recovery of Community Detection in k-partite Graph Models", "comments": "70 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the vertex classification problem on a graph whose vertices are in\n$k\\ (k\\geq 2)$ different communities, edges are only allowed between distinct\ncommunities, and the number of vertices in different communities are not\nnecessarily equal. The observation is a weighted adjacency matrix, perturbed by\na scalar multiple of the Gaussian Orthogonal Ensemble (GOE), or Gaussian\nUnitary Ensemble (GUE) matrix. For the exact recovery of the maximum likelihood\nestimation (MLE) with various weighted adjacency matrices, we prove sharp\nthresholds of the intensity $\\sigma$ of the Gaussian perturbation. These\nweighted adjacency matrices may be considered as natural models for the\nelectric network. Surprisingly, these thresholds of $\\sigma$ do not depend on\nwhether the sample space for MLE is restricted to such classifications that the\nnumber of vertices in each group is equal to the true value. In contrast to the\n$\\ZZ_2$-synchronization, a new complex version of the semi-definite programming\n(SDP) is designed to efficiently implement the community detection problem when\nthe number of communities $k$ is greater than 2, and a common region\n(independent of $k$) for $\\sigma$ such that SDP exactly recovers the true\nclassification is obtained.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 01:27:11 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 03:27:11 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Li", "Zhongyang", ""]]}, {"id": "1910.04640", "submitter": "Giovanni Schmid PhD.", "authors": "Ferdinando Montecuollo, Giovannni Schmid, Roberto Tagliaferri", "title": "E2FM: an encrypted and compressed full-text index for collections of\n  genomic sequences", "comments": "23 pages with pseudo-code and experimental results", "journal-ref": "Bioinformatics, 33(18), 2017, 2808-2817", "doi": "10.1093/bioinformatics/btx313", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next Generation Sequencing (NGS) platforms and, more generally,\nhigh-throughput technologies are giving rise to an exponential growth in the\nsize of nucleotide sequence databases. Moreover, many emerging applications of\nnucleotide datasets -- as those related to personalized medicine -- require the\ncompliance with regulations about the storage and processing of sensitive data.\nWe have designed and carefully engineered E2FM-index, a new full-text index in\nminute space which was optimized for compressing and encrypting nucleotide\nsequence collections in FASTA format and for performing fast pattern-search\nqueries. E2FM-index allows to build self-indexes which occupy till to 1/20 of\nthe storage required by the input FASTA file, thus permitting to save about 95%\nof storage when indexing collections of highly similar sequences; moreover, it\ncan exactly search the built indexes for patterns in times ranging from few\nmilliseconds to a few hundreds milliseconds, depending on pattern length.\nSupplementary material and supporting datasets are available through\nBioinformatics Online and https://figshare.com/s/6246ee9c1bd730a8bf6e.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 15:19:19 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Montecuollo", "Ferdinando", ""], ["Schmid", "Giovannni", ""], ["Tagliaferri", "Roberto", ""]]}, {"id": "1910.04728", "submitter": "Jialin Ding", "authors": "Darryl Ho, Jialin Ding, Sanchit Misra, Nesime Tatbul, Vikram Nathan,\n  Vasimuddin Md, Tim Kraska", "title": "LISA: Towards Learned DNA Sequence Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next-generation sequencing (NGS) technologies have enabled affordable\nsequencing of billions of short DNA fragments at high throughput, paving the\nway for population-scale genomics. Genomics data analytics at this scale\nrequires overcoming performance bottlenecks, such as searching for short DNA\nsequences over long reference sequences. In this paper, we introduce LISA\n(Learned Indexes for Sequence Analysis), a novel learning-based approach to DNA\nsequence search. As a first proof of concept, we focus on accelerating one of\nthe most essential flavors of the problem, called exact search. LISA builds on\nand extends FM-index, which is the state-of-the-art technique widely deployed\nin genomics tool-chains. Initial experiments with human genome datasets\nindicate that LISA achieves up to a factor of 4X performance speedup against\nits traditional counterpart.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 17:41:53 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Ho", "Darryl", ""], ["Ding", "Jialin", ""], ["Misra", "Sanchit", ""], ["Tatbul", "Nesime", ""], ["Nathan", "Vikram", ""], ["Md", "Vasimuddin", ""], ["Kraska", "Tim", ""]]}, {"id": "1910.04848", "submitter": "Xiao-Yue Gong", "authors": "James B. Orlin, Xiao-Yue Gong", "title": "A Fast Max Flow Algorithm", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2013, Orlin proved that the max flow problem could be solved in $O(nm)$\ntime. His algorithm ran in $O(nm + m^{1.94})$ time, which was the fastest for\ngraphs with fewer than $n^{1.06}$ arcs. If the graph was not sufficiently\nsparse, the fastest running time was an algorithm due to King, Rao, and Tarjan.\nWe describe a new variant of the excess scaling algorithm for the max flow\nproblem whose running time strictly dominates the running time of the algorithm\nby King et al. Moreover, for graphs in which $m = O(n \\log n)$, the running\ntime of our algorithm dominates that of King et al. by a factor of $O(\\log\\log\nn)$.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 20:58:12 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Orlin", "James B.", ""], ["Gong", "Xiao-Yue", ""]]}, {"id": "1910.04881", "submitter": "Ruslan Shaydulin", "authors": "Ruslan Shaydulin, Yuri Alexeev", "title": "Evaluating Quantum Approximate Optimization Algorithm: A Case Study", "comments": null, "journal-ref": null, "doi": "10.1109/IGSC48788.2019.8957201", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum Approximate Optimization Algorithm (QAOA) is one of the most\npromising quantum algorithms for the Noisy Intermediate-Scale Quantum (NISQ)\nera. Quantifying the performance of QAOA in the near-term regime is of utmost\nimportance. We perform a large-scale numerical study of the approximation\nratios attainable by QAOA is the low- to medium-depth regime. To find good QAOA\nparameters we perform 990 million 10-qubit QAOA circuit evaluations. We find\nthat the approximation ratio increases only marginally as the depth is\nincreased, and the gains are offset by the increasing complexity of optimizing\nvariational parameters. We observe a high variation in approximation ratios\nattained by QAOA, including high variations within the same class of problem\ninstances. We observe that the difference in approximation ratios between\nproblem instances increases as the similarity between instances decreases. We\nfind that optimal QAOA parameters concentrate for instances in out benchmark,\nconfirming the previous findings for a different class of problems.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 21:14:22 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Shaydulin", "Ruslan", ""], ["Alexeev", "Yuri", ""]]}, {"id": "1910.04911", "submitter": "Yinzhan Xu", "authors": "Virginia Vassilevska Williams, Yinzhan Xu", "title": "Truly Subcubic Min-Plus Product for Less Structured Matrices, with\n  Applications", "comments": "To appear in SODA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to get truly subcubic algorithms for Min-Plus\nproduct for less structured inputs than what was previously known, and to apply\nthem to versions of All-Pairs Shortest Paths (APSP) and other problems. The\nresults are as follows:\n  (1) Our main result is the first truly subcubic algorithm for the Min-Plus\nproduct of two $n\\times n$ matrices $A$ and $B$ with $\\text{polylog}(n)$ bit\ninteger entries, where $B$ has a partitioning into $n^{\\epsilon}\\times\nn^{\\epsilon}$ blocks (for any $\\epsilon>0$) where each block is at most\n$n^\\delta$-far (for $\\delta<3-\\omega$, where $2\\leq \\omega<2.373$) in\n$\\ell_\\infty$ norm from a constant rank integer matrix. This result presents\nthe most general case to date of Min-Plus product that is solvable in truly\nsubcubic time.\n  (2) The first application of our main result is a truly subcubic algorithm\nfor APSP in a new type of geometric graph. Our result extends the result of\nChan'10 in the case of integer edge weights by allowing the weights to differ\nfrom a function of the end-point identities by at most $n^\\delta$ for small\n$\\delta$.\n  (3) In the second application we consider a batch version of the range mode\nproblem in which one is given a length $n$ sequence and $n$ contiguous\nsubsequences, and one is asked to compute the range mode of each subsequence.\nWe give the first $O(n^{1.5-\\epsilon})$ time for $\\epsilon>0$ algorithm for\nthis batch range mode problem.\n  (4) Our final application is to the Maximum Subarray problem: given an\n$n\\times n$ integer matrix, find the contiguous subarray of maximum entry sum.\nWe show that Maximum Subarray can be solved in truly subcubic,\n$O(n^{3-\\epsilon})$ (for $\\epsilon>0$) time, as long as the entries are no\nlarger than $O(n^{0.62})$ in absolute value.\n  We also improve all the known conditional hardness results for the\n$d$-dimensional variant of Maximum Subarray.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 23:24:33 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Williams", "Virginia Vassilevska", ""], ["Xu", "Yinzhan", ""]]}, {"id": "1910.04959", "submitter": "Hossein Esfandiari", "authors": "Hossein Esfandiari, Amin Karbasi, Abbas Mehrabian, Vahab Mirrokni", "title": "Regret Bounds for Batched Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present simple and efficient algorithms for the batched stochastic\nmulti-armed bandit and batched stochastic linear bandit problems. We prove\nbounds for their expected regrets that improve over the best-known regret\nbounds for any number of batches. In particular, our algorithms in both\nsettings achieve the optimal expected regrets by using only a logarithmic\nnumber of batches. We also study the batched adversarial multi-armed bandit\nproblem for the first time and find the optimal regret, up to logarithmic\nfactors, of any algorithm with predetermined batch sizes.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 03:53:28 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 16:44:52 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Esfandiari", "Hossein", ""], ["Karbasi", "Amin", ""], ["Mehrabian", "Abbas", ""], ["Mirrokni", "Vahab", ""]]}, {"id": "1910.04974", "submitter": "Amyra Meidiana", "authors": "Amyra Meidiana, Seok-Hee Hong, Peter Eades, Daniel Keim", "title": "A Quality Metric for Symmetric Graph Drawings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry is an important aesthetic criteria in graph drawing and network\nvisualisation. Symmetric graph drawings aim to faithfully represent\nautomorphisms of graphs as geometric symmetries in a drawing.\n  In this paper, we design and implement a framework for quality metrics that\nmeasure symmetry, that is, how faithfully a drawing of a graph displays\nautomorphisms as geometric symmetries. The quality metrics are based on\ngeometry (i.e. Euclidean distance) as well as mathematical group theory (i.e.\norbits of automorphisms).\n  More specifically, we define two varieties of symmetry quality metrics: (1)\nfor displaying a single automorphism as a symmetry (axial or rotational) and\n(2) for displaying a group of automorphisms (cyclic or dihedral). We also\npresent algorithms to compute the symmetric quality metrics in O(n log n) time\nfor rotational symmetry and axial symmetry.\n  We validate our symmetry quality metrics using deformation experiments. We\nthen use the metrics to evaluate a number of established graph drawing layouts\nto compare how faithfully they display automorphisms of a graph as geometric\nsymmetries.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 05:17:46 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Meidiana", "Amyra", ""], ["Hong", "Seok-Hee", ""], ["Eades", "Peter", ""], ["Keim", "Daniel", ""]]}, {"id": "1910.05184", "submitter": "Amanda Streib", "authors": "Sarah Miracle, Amanda Pascoe Streib, and Noah Streib", "title": "Iterated Decomposition of Biased Permutations Via New Bounds on the\n  Spectral Gap of Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectral gap of a Markov chain can be bounded by the spectral gaps of\nconstituent \"restriction\" chains and a \"projection\" chain, and the strength of\nsuch a bound is the content of various decomposition theorems. In this paper,\nwe introduce a new parameter that allows us to improve upon these bounds. We\nfurther define a notion of orthogonality between the restriction chains and\n\"complementary\" restriction chains. This leads to a new Complementary\nDecomposition theorem, which does not require analyzing the projection chain.\nFor $\\epsilon$-orthogonal chains, this theorem may be iterated $O(1/\\epsilon)$\ntimes while only giving away a constant multiplicative factor on the overall\nspectral gap. As an application, we provide a $1/n$-orthogonal decomposition of\nthe nearest neighbor Markov chain over $k$-class biased monotone permutations\non [$n$], as long as the number of particles in each class is at least $C\\log\nn$. This allows us to apply the Complementary Decomposition theorem iteratively\n$n$ times to prove the first polynomial bound on the spectral gap when $k$ is\nas large as $\\Theta(n/\\log n)$. The previous best known bound assumed $k$ was\nat most a constant.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 13:40:42 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Miracle", "Sarah", ""], ["Streib", "Amanda Pascoe", ""], ["Streib", "Noah", ""]]}, {"id": "1910.05254", "submitter": "Konrad Dabrowski", "authors": "Konrad K. Dabrowski and Matthew Johnson and Giacomo Paesani and\n  Dani\\\"el Paulusma and Viktor Zamaraev", "title": "On the Price of Independence for Vertex Cover, Feedback Vertex Set and\n  Odd Cycle Transversal", "comments": "25 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $vc(G)$, $fvs(G)$ and $oct(G)$, respectively, denote the size of a\nminimum vertex cover, minimum feedback vertex set and minimum odd cycle\ntransversal in a graph $G$. One can ask, when looking for these sets in a\ngraph, how much bigger might they be if we require that they are independent;\nthat is, what is the price of independence? If $G$ has a vertex cover, feedback\nvertex set or odd cycle transversal that is an independent set, then we let\n$ivc(G)$, $ifvs(G)$ or $ioct(G)$, respectively, denote the minimum size of such\na set. Similar to a recent study on the price of connectivity (Hartinger et al.\nEuJC 2016), we investigate for which graphs $H$ the values of $ivc(G)$,\n$ifvs(G)$ and $ioct(G)$ are bounded in terms of $vc(G)$, $fvs(G)$ and $oct(G)$,\nrespectively, when the graph $G$ belongs to the class of $H$-free graphs. We\nfind complete classifications for vertex cover and feedback vertex set and an\nalmost complete classification for odd cycle transversal (subject to three\nnon-equivalent open cases). We also investigate for which graphs $H$ the values\nof $ivc(G)$, $ifvs(G)$ and $ioct(G)$ are equal to $vc(G)$, $fvs(G)$ and\n$oct(G)$, respectively, when the graph $G$ belongs to the class of $H$-free\ngraphs. We find a complete classification for vertex cover and almost complete\nclassifications for feedback vertex set (subject to one open case) and odd\ncycle transversal (subject to three open cases).\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 15:41:10 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Dabrowski", "Konrad K.", ""], ["Johnson", "Matthew", ""], ["Paesani", "Giacomo", ""], ["Paulusma", "Dani\u00ebl", ""], ["Zamaraev", "Viktor", ""]]}, {"id": "1910.05295", "submitter": "Nikitas Rontsis", "authors": "Nikitas Rontsis, Paul J. Goulart", "title": "Optimal Approximation of Doubly Stochastic Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the least-squares approximation of a matrix C in the set of\ndoubly stochastic matrices with the same sparsity pattern as C. Our approach is\nbased on applying the well-known Alternating Direction Method of Multipliers\n(ADMM) to a reformulation of the original problem. Our resulting algorithm\nrequires an initial Cholesky factorization of a positive definite matrix that\nhas the same sparsity pattern as C + I followed by simple iterations whose\ncomplexity is linear in the number of nonzeros in C, thus ensuring excellent\nscalability and speed. We demonstrate the advantages of our approach in a\nseries of experiments on problems with up to 82 million nonzeros; these include\nnormalizing large scale matrices arising from the 3D structure of the human\ngenome, clustering applications, and the SuiteSparse matrix library. Overall,\nour experiments illustrate the outstanding scalability of our algorithm;\nmatrices with millions of nonzeros can be approximated in a few seconds on\nmodest desktop computing hardware.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 16:41:06 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Rontsis", "Nikitas", ""], ["Goulart", "Paul J.", ""]]}, {"id": "1910.05385", "submitter": "Soheil Behnezhad", "authors": "Soheil Behnezhad, Laxman Dhulipala, Hossein Esfandiari, Jakub\n  {\\L}\\k{a}cki, Vahab Mirrokni", "title": "Near-Optimal Massively Parallel Graph Connectivity", "comments": "A preliminary version of this paper is to appear in the proceedings\n  of The 60th Annual IEEE Symposium on Foundations of Computer Science (FOCS\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the connected components of a graph, apart from being a\nfundamental problem with countless applications, is a key primitive for many\nother algorithms. In this paper, we consider this problem in parallel settings.\nParticularly, we focus on the Massively Parallel Computations (MPC) model,\nwhich is the standard theoretical model for modern parallel frameworks such as\nMapReduce, Hadoop, or Spark. We consider the truly sublinear regime of MPC for\ngraph problems where the space per machine is $n^\\delta$ for some desirably\nsmall constant $\\delta \\in (0, 1)$.\n  We present an algorithm that for graphs with diameter $D$ in the wide range\n$[\\log^{\\epsilon} n, n]$, takes $O(\\log D)$ rounds to identify the connected\ncomponents and takes $O(\\log \\log n)$ rounds for all other graphs. The\nalgorithm is randomized, succeeds with high probability, does not require prior\nknowledge of $D$, and uses an optimal total space of $O(m)$. We complement this\nby showing a conditional lower-bound based on the widely believed TwoCycle\nconjecture that $\\Omega(\\log D)$ rounds are indeed necessary in this setting.\n  Studying parallel connectivity algorithms received a resurgence of interest\nafter the pioneering work of Andoni et al. [FOCS 2018] who presented an\nalgorithm with $O(\\log D \\cdot \\log \\log n)$ round-complexity. Our algorithm\nimproves this result for the whole range of values of $D$ and almost settles\nthe problem due to the conditional lower-bound.\n  Additionally, we show that with minimal adjustments, our algorithm can also\nbe implemented in a variant of the (CRCW) PRAM in asymptotically the same\nnumber of rounds.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 19:51:13 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 23:51:43 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Behnezhad", "Soheil", ""], ["Dhulipala", "Laxman", ""], ["Esfandiari", "Hossein", ""], ["\u0141\u0105cki", "Jakub", ""], ["Mirrokni", "Vahab", ""]]}, {"id": "1910.05422", "submitter": "Cenk Baykal", "authors": "Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman,\n  Daniela Rus", "title": "SiPPing Neural Networks: Sensitivity-informed Provable Pruning of Neural\n  Networks", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a pruning algorithm that provably sparsifies the parameters of a\ntrained model in a way that approximately preserves the model's predictive\naccuracy. Our algorithm uses a small batch of input points to construct a\ndata-informed importance sampling distribution over the network's parameters,\nand adaptively mixes a sampling-based and deterministic pruning procedure to\ndiscard redundant weights. Our pruning method is simultaneously computationally\nefficient, provably accurate, and broadly applicable to various network\narchitectures and data distributions. Our empirical comparisons show that our\nalgorithm reliably generates highly compressed networks that incur minimal loss\nin performance relative to that of the original network. We present\nexperimental results that demonstrate our algorithm's potential to unearth\nessential network connections that can be trained successfully in isolation,\nwhich may be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 21:40:59 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 01:03:33 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Baykal", "Cenk", ""], ["Liebenwein", "Lucas", ""], ["Gilitschenski", "Igor", ""], ["Feldman", "Dan", ""], ["Rus", "Daniela", ""]]}, {"id": "1910.05583", "submitter": "Marcel Ausloos", "authors": "Olgica Nedic, Ivana Drvenica, Marcel Ausloos, and Aleksandar Dekanski", "title": "Efficiency in managing peer-review of scientific manuscripts -- editors'\n  perspective", "comments": "28 refs. ; 4 Tables; 5 figures; 21 pages", "journal-ref": "Journal of Serbian Chemical Society 83(12), 1391-1405 (2018)", "doi": "10.2298/JSC180531066N", "report-no": null, "categories": "cs.DL cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to introduce a model for measuring the\nefficiency in managing peer-review of scientific manuscripts by editors. The\napproach employed is based on the assumption that the editorial aim is to\nmanage publication with high efficiency, employing the least amount of\neditorial resources. Efficiency is defined in this research as a measure based\non 7 variables. An on-line survey was constructed and editors of journals\noriginating from Serbia regularly publishing articles in the field of chemistry\nwere invited to participate. An evaluation of the model is given based on\nresponses from 24 journals and 50 editors. With this investigation we aimed to\ncontribute to our understanding of the peer-review process and, possibly, offer\na tool to improve the \"efficiency\" in journal editing. The proposed protocol\nmay be adapted by other journals in order to assess the managing potential of\neditors.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 15:54:46 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Nedic", "Olgica", ""], ["Drvenica", "Ivana", ""], ["Ausloos", "Marcel", ""], ["Dekanski", "Aleksandar", ""]]}, {"id": "1910.05589", "submitter": "Ioannis Katsikarelis", "authors": "Ioannis Katsikarelis, Michael Lampis, Vangelis Th. Paschos", "title": "Improved (In-)Approximability Bounds for d-Scattered Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $d$-Scattered Set problem we are asked to select at least $k$ vertices\nof a given graph, so that the distance between any pair is at least $d$. We\nstudy the problem's (in-)approximability and offer improvements and extensions\nof known results for Independent Set, of which the problem is a generalization.\n  Specifically, we show:\n  - A lower bound of $\\Delta^{\\lfloor d/2\\rfloor-\\epsilon}$ on the\napproximation ratio of any polynomial-time algorithm for graphs of maximum\ndegree $\\Delta$ and an improved upper bound of $O(\\Delta^{\\lfloor d/2\\rfloor})$\non the approximation ratio of any greedy scheme for this problem.\n  - A polynomial-time $2\\sqrt{n}$-approximation for bipartite graphs and even\nvalues of $d$, that matches the known lower bound by considering the only\nremaining case.\n  - A lower bound on the complexity of any $\\rho$-approximation algorithm of\n(roughly) $2^{\\frac{n^{1-\\epsilon}}{\\rho d}}$ for even $d$ and\n$2^{\\frac{n^{1-\\epsilon}}{\\rho(d+\\rho)}}$ for odd $d$ (under the randomized\nETH), complemented by $\\rho$-approximation algorithms of running times that\n(almost) match these bounds.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 16:13:59 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Katsikarelis", "Ioannis", ""], ["Lampis", "Michael", ""], ["Paschos", "Vangelis Th.", ""]]}, {"id": "1910.05645", "submitter": "Doron Mu", "authors": "Shiri Chechik and Doron Mukhtar", "title": "Reachability and Shortest Paths in the Broadcast CONGEST Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the time complexity of the single-source reachability\nproblem and the single-source shortest path problem for directed unweighted\ngraphs in the Broadcast CONGEST model. We focus on the case where the diameter\n$D$ of the underlying network is constant. We show that for the case where $D =\n1$ there is, quite surprisingly, a very simple algorithm that solves the\nreachability problem in $1$(!) round. In contrast, for networks with $D = 2$,\nwe show that any distributed algorithm (possibly randomized) for this problem\nrequires $\\Omega(\\sqrt{n/ \\log{n}}\\,)$ rounds. Our results therefore completely\nresolve (up to a small polylogarithmic factor) the complexity of the\nsingle-source reachability problem for a wide range of diameters. Furthermore,\nwe show that when $D = 1$, it is even possible to get a $3$-approximation for\nthe all-pairs shortest path problem (for directed unweighted graphs) in just\n$2$ rounds. We also prove a stronger lower bound of $\\Omega(\\sqrt{n}\\,)$ for\nthe single-source shortest path problem for unweighted directed graphs that\nholds even when the diameter of the underlying network is $2$. As far as we\nknow this is the first lower bound that achieves $\\Omega(\\sqrt{n}\\,)$ for this\nproblem.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 21:11:52 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Chechik", "Shiri", ""], ["Mukhtar", "Doron", ""]]}, {"id": "1910.05646", "submitter": "Samson Zhou", "authors": "Dmitrii Avdiukhin, Grigory Yaroslavtsev, Samson Zhou", "title": "\"Bring Your Own Greedy\"+Max: Near-Optimal $1/2$-Approximations for\n  Submodular Knapsack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of selecting a small-size representative summary of a large\ndataset is a cornerstone of machine learning, optimization and data science.\nMotivated by applications to recommendation systems and other scenarios with\nquery-limited access to vast amounts of data, we propose a new rigorous\nalgorithmic framework for a standard formulation of this problem as a\nsubmodular maximization subject to a linear (knapsack) constraint. Our\nframework is based on augmenting all partial Greedy solutions with the best\nadditional item. It can be instantiated with negligible overhead in any model\nof computation, which allows the classic \\greedy algorithm and its variants to\nbe implemented. We give such instantiations in the offline (Greedy+Max),\nmulti-pass streaming (Sieve+Max) and distributed (Distributed+Max) settings.\nOur algorithms give ($1/2-\\epsilon$)-approximation with most other key\nparameters of interest being near-optimal. Our analysis is based on a new set\nof first-order linear differential inequalities and their robust approximate\nversions. Experiments on typical datasets (movie recommendations, influence\nmaximization) confirm scalability and high quality of solutions obtained via\nour framework. Instance-specific approximations are typically in the 0.6-0.7\nrange and frequently beat even the $(1-1/e) \\approx 0.63$ worst-case barrier\nfor polynomial-time algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 21:20:10 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Avdiukhin", "Dmitrii", ""], ["Yaroslavtsev", "Grigory", ""], ["Zhou", "Samson", ""]]}, {"id": "1910.05686", "submitter": "Samson Zhou", "authors": "Grigory Yaroslavtsev, Samson Zhou", "title": "Fast Fourier Sparsity Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A function $f : \\mathbb{F}_2^n \\to \\mathbb{R}$ is $s$-sparse if it has at\nmost $s$ non-zero Fourier coefficients. Motivated by applications to fast\nsparse Fourier transforms over $\\mathbb{F}_2^n$, we study efficient algorithms\nfor the problem of approximating the $\\ell_2$-distance from a given function to\nthe closest $s$-sparse function. While previous works (e.g., Gopalan et al.\nSICOMP 2011) study the problem of distinguishing $s$-sparse functions from\nthose that are far from $s$-sparse under Hamming distance, to the best of our\nknowledge no prior work has explicitly focused on the more general problem of\ndistance estimation in the $\\ell_2$ setting, which is particularly\nwell-motivated for noisy Fourier spectra. Given the focus on efficiency, our\nmain result is an algorithm that solves this problem with query complexity\n$\\mathcal{O}(s)$ for constant accuracy and error parameters, which is only\nquadratically worse than applicable lower bounds.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 04:58:24 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Yaroslavtsev", "Grigory", ""], ["Zhou", "Samson", ""]]}, {"id": "1910.05699", "submitter": "Dhawal Jethwani", "authors": "Dhawal Jethwani, Fran\\c{c}ois Le Gall, Sanjay K. Singh", "title": "Quantum-Inspired Classical Algorithms for Singular Value Transformation", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent breakthrough by Tang (STOC 2019) showed how to \"dequantize\" the\nquantum algorithm for recommendation systems by Kerenidis and Prakash (ITCS\n2017). The resulting algorithm, classical but \"quantum-inspired\", efficiently\ncomputes a low-rank approximation of the users' preference matrix. Subsequent\nworks have shown how to construct efficient quantum-inspired algorithms for\napproximating the pseudo-inverse of a low-rank matrix as well, which can be\nused to (approximately) solve low-rank linear systems of equations. In the\npresent paper, we pursue this line of research and develop quantum-inspired\nalgorithms for a large class of matrix transformations that are defined via the\nsingular value decomposition of the matrix. In particular, we obtain classical\nalgorithms with complexity polynomially related (in most parameters) to the\ncomplexity of the best quantum algorithms for singular value transformation\nrecently developed by Chakraborty, Gily\\'{e}n and Jeffery (ICALP 2019) and\nGily\\'{e}n, Su, Low and Wiebe (STOC19).\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 06:39:48 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 13:04:51 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 10:46:57 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Jethwani", "Dhawal", ""], ["Gall", "Fran\u00e7ois Le", ""], ["Singh", "Sanjay K.", ""]]}, {"id": "1910.05811", "submitter": "Fan Ding", "authors": "Fan Ding, Hanjing Wang, Ashish Sabharwal, Yexiang Xue", "title": "Towards Efficient Discrete Integration via Adaptive Quantile Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete integration in a high dimensional space of n variables poses\nfundamental challenges. The WISH algorithm reduces the intractable discrete\nintegration problem into n optimization queries subject to randomized\nconstraints, obtaining a constant approximation guarantee. The optimization\nqueries are expensive, which limits the applicability of WISH. We propose\nAdaWISH, which is able to obtain the same guarantee but accesses only a small\nsubset of queries of WISH. For example, when the number of function values is\nbounded by a constant, AdaWISH issues only O(log n) queries. The key idea is to\nquery adaptively, taking advantage of the shape of the weight function being\nintegrated. In general, we prove that AdaWISH has a regret of only O(log n)\nrelative to an idealistic oracle that issues queries at data-dependent optimal\npoints. Experimentally, AdaWISH gives precise estimates for discrete\nintegration problems, of the same quality as that of WISH and better than\nseveral competing approaches, on a variety of probabilistic inference\nbenchmarks. At the same time, it saves substantially on the number of\noptimization queries compared to WISH. On a suite of UAI inference challenge\nbenchmarks, it saves 81.5% of WISH queries while retaining the quality of\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 18:45:10 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 23:24:40 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ding", "Fan", ""], ["Wang", "Hanjing", ""], ["Sabharwal", "Ashish", ""], ["Xue", "Yexiang", ""]]}, {"id": "1910.05971", "submitter": "Yongzhe Zhang", "authors": "Yongzhe Zhang, Ariful Azad, Zhenjiang Hu", "title": "FastSV: A Distributed-Memory Connected Component Algorithm with Fast\n  Convergence", "comments": "SIAM Conference on Parallel Processing for Scientific Computing\n  (PP20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new distributed-memory algorithm called FastSV for\nfinding connected components in an undirected graph. Our algorithm simplifies\nthe classic Shiloach-Vishkin algorithm and employs several novel and efficient\nhooking strategies for faster convergence. We map different steps of FastSV to\nlinear algebraic operations and implement them with the help of scalable graph\nlibraries. FastSV uses sparse operations to avoid redundant work and optimized\nMPI communication to avoid bottlenecks. The resultant algorithm shows\nhigh-performance and scalability as it can find the connected components of a\nhyperlink graph with over 134B edges in 30 seconds using 262K cores on a Cray\nXC40 supercomputer. FastSV outperforms the state-of-the-art algorithm by an\naverage speedup of 2.21x (max 4.27x) on a variety of real-world graphs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 08:13:41 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 20:33:37 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Zhang", "Yongzhe", ""], ["Azad", "Ariful", ""], ["Hu", "Zhenjiang", ""]]}, {"id": "1910.06151", "submitter": "Ewin Tang", "authors": "Nai-Hui Chia, Andr\\'as Gily\\'en, Tongyang Li, Han-Hsuan Lin, Ewin\n  Tang, Chunhao Wang", "title": "Sampling-based sublinear low-rank matrix arithmetic framework for\n  dequantizing quantum machine learning", "comments": "79 pages, 1 figure. revised to add more connection to QSVT, improve\n  existing results, and clarify exposition", "journal-ref": null, "doi": "10.1145/3357713.3384314", "report-no": null, "categories": "cs.DS cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithmic framework for quantum-inspired classical algorithms\non close-to-low-rank matrices, generalizing the series of results started by\nTang's breakthrough quantum-inspired algorithm for recommendation systems\n[STOC'19]. Motivated by quantum linear algebra algorithms and the quantum\nsingular value transformation (SVT) framework of Gily\\'en et al. [STOC'19], we\ndevelop classical algorithms for SVT that run in time independent of input\ndimension, under suitable quantum-inspired sampling assumptions. Our results\ngive compelling evidence that in the corresponding QRAM data structure input\nmodel, quantum SVT does not yield exponential quantum speedups. Since the\nquantum SVT framework generalizes essentially all known techniques for quantum\nlinear algebra, our results, combined with sampling lemmas from previous work,\nsuffice to generalize all recent results about dequantizing quantum machine\nlearning algorithms. In particular, our classical SVT framework recovers and\noften improves the dequantization results on recommendation systems, principal\ncomponent analysis, supervised clustering, support vector machines, low-rank\nregression, and semidefinite program solving. We also give additional\ndequantization results on low-rank Hamiltonian simulation and discriminant\nanalysis. Our improvements come from identifying the key feature of the\nquantum-inspired input model that is at the core of all prior quantum-inspired\nresults: $\\ell^2$-norm sampling can approximate matrix products in time\nindependent of their dimension. We reduce all our main results to this fact,\nmaking our exposition concise, self-contained, and intuitive.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 14:04:30 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 16:54:14 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Chia", "Nai-Hui", ""], ["Gily\u00e9n", "Andr\u00e1s", ""], ["Li", "Tongyang", ""], ["Lin", "Han-Hsuan", ""], ["Tang", "Ewin", ""], ["Wang", "Chunhao", ""]]}, {"id": "1910.06169", "submitter": "Giorgio Vinciguerra", "authors": "Paolo Ferragina and Giorgio Vinciguerra", "title": "The PGM-index: a multicriteria, compressed and learned approach to data\n  indexing", "comments": "We remark to the reader that this paper is an extended and improved\n  version of our previous paper titled \"Superseding traditional indexes by\n  orchestrating learning and geometry\" (arXiv:1903.00507)", "journal-ref": "PVLDB, 13(8): 1162-1175, 2020", "doi": "10.14778/3389133.3389135", "report-no": null, "categories": "cs.DS cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent introduction of learned indexes has shaken the foundations of the\ndecades-old field of indexing data structures. Combining, or even replacing,\nclassic design elements such as B-tree nodes with machine learning models has\nproven to give outstanding improvements in the space footprint and time\nefficiency of data systems. However, these novel approaches are based on\nheuristics, thus they lack any guarantees both in their time and space\nrequirements. We propose the Piecewise Geometric Model index (shortly,\nPGM-index), which achieves guaranteed I/O-optimality in query operations,\nlearns an optimal number of linear models, and its peculiar recursive\nconstruction makes it a purely learned data structure, rather than a hybrid of\ntraditional and learned indexes (such as RMI and FITing-tree). We show that the\nPGM-index improves the space of the FITing-tree by 63.3% and of the B-tree by\nmore than four orders of magnitude, while achieving their same or even better\nquery time efficiency. We complement this result by proposing three variants of\nthe PGM-index. First, we design a compressed PGM-index that further reduces its\nspace footprint by exploiting the repetitiveness at the level of the learned\nlinear models it is composed of. Second, we design a PGM-index that adapts\nitself to the distribution of the queries, thus resulting in the first known\ndistribution-aware learned index to date. Finally, given its flexibility in the\noffered space-time trade-offs, we propose the multicriteria PGM-index that\nefficiently auto-tune itself in a few seconds over hundreds of millions of keys\nto the possibly evolving space-time constraints imposed by the application of\nuse.\n  We remark to the reader that this paper is an extended and improved version\nof our previous paper titled \"Superseding traditional indexes by orchestrating\nlearning and geometry\" (arXiv:1903.00507).\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 14:25:25 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Ferragina", "Paolo", ""], ["Vinciguerra", "Giorgio", ""]]}, {"id": "1910.06185", "submitter": "Shaohua Li", "authors": "Qilong Feng, Shaohua Li, Xiangzhong Meng, Jianxin Wang", "title": "An Improved FPT Algorithm for the Flip Distance Problem", "comments": "A preliminary version of this paper appeared at MFCS 2017", "journal-ref": "In proceedings of the 42nd International Symposium on Mathematical\n  Foundations of Computer Science, {MFCS} 2017", "doi": "10.4230/LIPIcs.MFCS.2017.65", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $\\cal P$ of points in the Euclidean plane and two triangulations\nof $\\cal P$, the flip distance between these two triangulations is the minimum\nnumber of flips required to transform one triangulation into the other.\nParameterized Flip Distance problem is to decide if the flip distance between\ntwo given triangulations is equal to a given integer $k$. The previous best FPT\nalgorithm runs in time $O^{*}(k\\cdot c^{k})$ ($c\\leq 2\\times 14^{11}$), where\neach step has fourteen possible choices, and the length of the action sequence\nis bounded by $11k$. By applying the backtracking strategy and analyzing the\nunderlying property of the flip sequence, each step of our algorithm has only\nfive possible choices. Based on an auxiliary graph $G$, we prove that the\nlength of the action sequence for our algorithm is bounded by $2|G|$. As a\nresult, we present an FPT algorithm running in time $O^{*}(k\\cdot 32^{k})$.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 14:51:57 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Feng", "Qilong", ""], ["Li", "Shaohua", ""], ["Meng", "Xiangzhong", ""], ["Wang", "Jianxin", ""]]}, {"id": "1910.06416", "submitter": "Sebastiano Vigna", "authors": "Emmanuel Esposito, Thomas Mueller Graf, Sebastiano Vigna", "title": "RecSplit: Minimal Perfect Hashing via Recursive Splitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A minimal perfect hash function bijectively maps a key set $S$ out of a\nuniverse $U$ into the first $|S|$ natural numbers. Minimal perfect hash\nfunctions are used, for example, to map irregularly-shaped keys, such as\nstring, in a compact space so that metadata can then be simply stored in an\narray. While it is known that just $1.44$ bits per key are necessary to store a\nminimal perfect function, no published technique can go below $2$ bits per key\nin practice. We propose a new technique for storing minimal perfect hash\nfunctions with expected linear construction time and expected constant lookup\ntime that makes it possible to build for the first time, for example,\nstructures which need $1.56$ bits per key, that is, within $8.3$% of the lower\nbound, in less than $2$ ms per key. We show that instances of our construction\nare able to simultaneously beat the construction time, space usage and lookup\ntime of the state-of-the-art data structure reaching $2$ bits per key.\nMoreover, we provide parameter choices giving structures which are competitive\nwith alternative, larger-size data structures in terms of space and lookup\ntime. The construction of our data structures can be easily parallelized or\nmapped on distributed computational units (e.g., within the MapReduce\nframework), and structures larger than the available RAM can be directly built\nin mass storage.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 20:50:22 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 11:47:44 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Esposito", "Emmanuel", ""], ["Graf", "Thomas Mueller", ""], ["Vigna", "Sebastiano", ""]]}, {"id": "1910.06437", "submitter": "Sebastiano Vigna", "authors": "Sebastiano Vigna", "title": "It is high time we let go of the Mersenne Twister", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the Mersenne Twister made his first appearance in 1997 it was a powerful\nexample of how linear maps on $\\mathbf F_2$ could be used to generate\npseudorandom numbers. In particular, the easiness with which generators with\nlong periods could be defined gave the Mersenne Twister a large following, in\nspite of the fact that such long periods are not a measure of quality, and they\nrequire a large amount of memory. Even at the time of its publication, several\ndefects of the Mersenne Twister were predictable, but they were somewhat\nobscured by other interesting properties. Today the Mersenne Twister is the\ndefault generator in C compilers, the Python language, the Maple mathematical\ncomputation system, and in many other environments. Nonetheless, knowledge\naccumulated in the last $20$ years suggests that the Mersenne Twister has, in\nfact, severe defects, and should never be used as a general-purpose\npseudorandom number generator. Many of these results are folklore, or are\nscattered through very specialized literature. This paper surveys these results\nfor the non-specialist, providing new, simple, understandable examples, and it\nis intended as a guide for the final user, or for language implementors, so\nthat they can take an informed decision about whether to use the Mersenne\nTwister or not.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 21:44:14 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 14:50:14 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Vigna", "Sebastiano", ""]]}, {"id": "1910.06517", "submitter": "Yujia Jin", "authors": "Yujia Jin, Aaron Sidford", "title": "Principal Component Projection and Regression in Nearly Linear Time\n  through Asymmetric SVRG", "comments": "37 pages, 3 figures; to appear in NeurIPS '19 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a data matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times d}$, principal\ncomponent projection (PCP) and principal component regression (PCR), i.e.\nprojection and regression restricted to the top-eigenspace of $\\mathbf{A}$, are\nfundamental problems in machine learning, optimization, and numerical analysis.\nIn this paper we provide the first algorithms that solve these problems in\nnearly linear time for fixed eigenvalue distribution and large n. This improves\nupon previous methods which have superlinear running times when both the number\nof top eigenvalues and inverse gap between eigenspaces is large. We achieve our\nresults by applying rational approximations to reduce PCP and PCR to solving\nasymmetric linear systems which we solve by a variant of SVRG. We corroborate\nthese findings with preliminary empirical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 04:02:53 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Jin", "Yujia", ""], ["Sidford", "Aaron", ""]]}, {"id": "1910.06920", "submitter": "Sadra Mohammadshirazi", "authors": "Sadra Mohammadshirazi, Alireza Bagheri", "title": "Apply Sorting Algorithms to FAST Problem", "comments": "9 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FAST problem is finding minimum feedback arc set problem in tournaments. In\nthis paper we present some algorithms that are similar to sorting algorithms\nfor FAST problem and we analyze them. We present Pseudo_InsertionSort algorithm\nfor FAST problem and we show that average number of all backward edges in\noutput of that is equal to ((n^2-5n+8)/4)-2^(1-n). We introduce\nPseudo_MergeSort algorithm and we find the probability of being backward for an\nedge. Finally we introduce other algorithms for this problem.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 16:45:59 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Mohammadshirazi", "Sadra", ""], ["Bagheri", "Alireza", ""]]}, {"id": "1910.07087", "submitter": "Saurabh Sawlani", "authors": "Digvijay Boob, Yu Gao, Richard Peng, Saurabh Sawlani, Charalampos E.\n  Tsourakakis, Di Wang and Junxing Wang", "title": "Flowless: Extracting Densest Subgraphs Without Flow Computations", "comments": "fixed incorrect email ID in V2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple and computationally efficient method for dense subgraph\ndiscovery, which is a classic problem both in theory and in practice. It is\nwell known that dense subgraphs can have strong correlation with structures of\ninterest in real-world networks across various domains ranging from social\nnetworks to biological systems [Gionis and Tsourakakis `15].\n  For the densest subgraph problem, Charikar's greedy algorithm [Asashiro `00,\nCharikar `00] is very simple, and can typically find result of quality much\nbetter than the provable factor 2-approximation, which makes it very popular in\npractice. However, it is also known to give suboptimal output in many\nreal-world examples. On the other hand, finding the exact optimal solution\nrequires the computation of maximum flow. Despite the existence of highly\noptimized maximum flow solvers, such computation still incurs prohibitive\ncomputational costs for the massive graphs arising in modern data science\napplications.\n  We devise an iterative algorithm which naturally generalizes the greedy\nalgorithm of Charikar. Our algorithm draws insights from the iterative\napproaches from convex optimization, and also exploits the dual interpretation\nof the densest subgraph problem. We have empirical evidence that our algorithm\nis much more robust against the structural heterogeneities in real-world\ndatasets, and converges to the optimal subgraph density even when the simple\ngreedy algorithm fails. On the other hand, in instances where Charikar's\nalgorithm performs well, our algorithm is able to quickly verify its\noptimality. Furthermore, we demonstrate that our method is significantly faster\nthan the maximum flow based exact optimal algorithm. We conduct experiments on\ndatasets from broad domains, and our algorithm achieves $\\sim$145$\\times$\nspeedup on average to find subgraphs whose density is at least 90\\% of the\noptimal value.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 22:27:27 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 03:37:04 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Boob", "Digvijay", ""], ["Gao", "Yu", ""], ["Peng", "Richard", ""], ["Sawlani", "Saurabh", ""], ["Tsourakakis", "Charalampos E.", ""], ["Wang", "Di", ""], ["Wang", "Junxing", ""]]}, {"id": "1910.07145", "submitter": "Travis Gagie", "authors": "Travis Gagie, Tomohiro I, Giovanni Manzini, Gonzalo Navarro, Hiroshi\n  Sakamoto, Louisa Seelbach Benkner and Yoshimasa Takabatake", "title": "Practical Random Access to SLP-Compressed Texts", "comments": "Accepted to SPIRE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grammar-based compression is a popular and powerful approach to compressing\nrepetitive texts but until recently its relatively poor time-space trade-offs\nduring real-life construction made it impractical for truly massive datasets\nsuch as genomic databases. In a recent paper (SPIRE 2019) we showed how simple\npre-processing can dramatically improve those trade-offs, and in this paper we\nturn our attention to one of the features that make grammar-based compression\nso attractive: the possibility of supporting fast random access. This is an\nessential primitive in many algorithms that process grammar-compressed texts\nwithout decompressing them and so many theoretical bounds have been published\nabout it, but experimentation has lagged behind. We give a new encoding of\ngrammars that is about as small as the practical state of the art (Maruyama et\nal., SPIRE 2013) but with significantly faster queries.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 03:14:03 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 01:05:37 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 13:51:01 GMT"}, {"version": "v4", "created": "Sun, 19 Jul 2020 16:05:36 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Gagie", "Travis", ""], ["I", "Tomohiro", ""], ["Manzini", "Giovanni", ""], ["Navarro", "Gonzalo", ""], ["Sakamoto", "Hiroshi", ""], ["Benkner", "Louisa Seelbach", ""], ["Takabatake", "Yoshimasa", ""]]}, {"id": "1910.07159", "submitter": "Girija Limaye", "authors": "Prem Krishnaa, Girija Limaye, Meghana Nasre, Prajakta Nimbhorkar", "title": "Envy-freeness and Relaxed Stability under lower quotas", "comments": "31 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of matchings under two-sided preferences in the\npresence of maximum as well as minimum quota requirements for the agents. This\nsetting, studied as the Hospital Residents with Lower Quotas (HRLQ) in\nliterature, models important real world problems like assigning medical interns\n(residents) to hospitals, and teaching assistants to instructors where a\nminimum guarantee is essential. When there are no minimum quotas, stability is\nthe de-facto notion of optimality. However, in the presence of minimum quotas,\nensuring stability and simultaneously satisfying lower quotas is not an\nattainable goal in many instances.\n  To address this, a relaxation of stability known as envy-freeness, is\nproposed in literature. In our work, we thoroughly investigate envy-freeness\nfrom a computational view point. Our results show that computing envy-free\nmatchings that match maximum number of agents is computationally hard and also\nhard to approximate up to a constant factor. Additionally, it is known that\nenvy-free matchings satisfying lower-quotas may not exist. To circumvent these\ndrawbacks, we propose a new notion called relaxed stability. We show that\nrelaxed stable matchings are guaranteed to exist even in the presence of\nlower-quotas. Despite the computational intractability of finding a largest\nmatching that is feasible and relaxed stable, we give efficient algorithms that\ncompute a constant factor approximation to this matching in terms of size.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 03:56:28 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 04:17:41 GMT"}, {"version": "v3", "created": "Sat, 16 May 2020 12:01:01 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Krishnaa", "Prem", ""], ["Limaye", "Girija", ""], ["Nasre", "Meghana", ""], ["Nimbhorkar", "Prajakta", ""]]}, {"id": "1910.07283", "submitter": "Matteo Dell'Amico Ph.D.", "authors": "Matteo Dell'Amico", "title": "FISHDBC: Flexible, Incremental, Scalable, Hierarchical Density-Based\n  Clustering for Arbitrary Data and Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FISHDBC is a flexible, incremental, scalable, and hierarchical density-based\nclustering algorithm. It is flexible because it empowers users to work on\narbitrary data, skipping the feature extraction step that usually transforms\nraw data in numeric arrays letting users define an arbitrary distance function\ninstead. It is incremental and scalable: it avoids the $\\mathcal O(n^2)$\nperformance of other approaches in non-metric spaces and requires only\nlightweight computation to update the clustering when few items are added. It\nis hierarchical: it produces a \"flat\" clustering which can be expanded to a\ntree structure, so that users can group and/or divide clusters in sub- or\nsuper-clusters when data exploration requires so. It is density-based and\napproximates HDBSCAN*, an evolution of DBSCAN.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 11:06:23 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Dell'Amico", "Matteo", ""]]}, {"id": "1910.07305", "submitter": "Yasuaki Kobayashi", "authors": "Yasuaki Kobayashi, Kensuke Kojima, Norihide Matsubara, Taiga Sone,\n  Akihiro Yamamoto", "title": "Algorithms and Hardness Results for the Maximum Balanced Connected\n  Subgraph Problem", "comments": "accepted at COCOA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Balanced Connected Subgraph problem (BCS) was recently introduced by\nBhore et al. (CALDAM 2019). In this problem, we are given a graph $G$ whose\nvertices are colored by red or blue. The goal is to find a maximum connected\nsubgraph of $G$ having the same number of blue vertices and red vertices. They\nshowed that this problem is NP-hard even on planar graphs, bipartite graphs,\nand chordal graphs. They also gave some positive results: BCS can be solved in\n$O(n^3)$ time for trees and $O(n + m)$ time for split graphs and properly\ncolored bipartite graphs, where $n$ is the number of vertices and $m$ is the\nnumber of edges.\n  In this paper, we show that BCS can be solved in $O(n^2)$ time for trees and\n$O(n^3)$ time for interval graphs. The former result can be extended to bounded\ntreewidth graphs. We also consider a weighted version of BCS (WBCS). We prove\nthat this variant is weakly NP-hard even on star graphs and strongly NP-hard\neven on split graphs and properly colored bipartite graphs, whereas the\nunweighted counterpart is tractable on those graph classes. Finally, we\nconsider an exact exponential-time algorithm for general graphs. We show that\nBCS can be solved in $2^{n/2}n^{O(1)}$ time. This algorithm is based on a\nvariant of Dreyfus-Wagner algorithm for the Steiner tree problem.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 12:07:06 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 01:18:24 GMT"}, {"version": "v3", "created": "Sat, 8 Feb 2020 23:52:56 GMT"}, {"version": "v4", "created": "Mon, 9 Mar 2020 21:12:43 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Kobayashi", "Yasuaki", ""], ["Kojima", "Kensuke", ""], ["Matsubara", "Norihide", ""], ["Sone", "Taiga", ""], ["Yamamoto", "Akihiro", ""]]}, {"id": "1910.07616", "submitter": "Ali Vakilian", "authors": "Chandra Chekuri and Alina Ene and Ali Vakilian", "title": "Node-Weighted Network Design in Planar and Minor-Closed Families of\n  Graphs", "comments": "This paper builds upon an earlier version with results on\n  edge-connectivity that appeared in ICALP'12 and extends its result to the\n  setting with element-connectivity requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider node-weighted survivable network design (SNDP) in planar graphs\nand minor-closed families of graphs. The input consists of a node-weighted\nundirected graph $G=(V,E)$ and integer connectivity requirements $r(uv)$ for\neach unordered pair of nodes $uv$. The goal is to find a minimum weighted\nsubgraph $H$ of $G$ such that $H$ contains $r(uv)$ disjoint paths between $u$\nand $v$ for each node pair $uv$. Three versions of the problem are\nedge-connectivity SNDP (EC-SNDP), element-connectivity SNDP (Elem-SNDP) and\nvertex-connectivity SNDP (VC-SNDP) depending on whether the paths are required\nto be edge, element or vertex disjoint respectively. Our main result is an\n$O(k)$-approximation algorithm for EC-SNDP and Elem-SNDP when the input graph\nis planar or more generally if it belongs to a proper minor-closed family of\ngraphs; here $k=\\max_{uv} r(uv)$ is the maximum connectivity requirement. This\nimproves upon the $O(k \\log n)$-approximation known for node-weighted EC-SNDP\nand Elem-SNDP in general graphs [Nutov, TALG'12]. We also obtain an $O(1)$\napproximation for node-weighted VC-SNDP when the connectivity requirements are\nin $\\{0,1,2\\}$; for higher connectivity our result for Elem-SNDP can be used in\na black-box fashion to obtain a logarithmic factor improvement over currently\nknown general graph results. Our results are inspired by, and generalize, the\nwork of [Demaine, Hajiaghayi and Klein, TALG'14] who obtained constant factor\napproximations for node-weighted Steiner tree and Steiner forest problems in\nplanar graphs and proper minor-closed families of graphs via a primal-dual\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 21:14:47 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Chekuri", "Chandra", ""], ["Ene", "Alina", ""], ["Vakilian", "Ali", ""]]}, {"id": "1910.07782", "submitter": "Lum Ramabaja", "authors": "Lum Ramabaja, Arber Avdullahu", "title": "The Distributed Bloom Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Distributed Bloom Filter is a space-efficient, probabilistic data\nstructure designed to perform more efficient set reconciliations in distributed\nsystems. It guarantees eventual consistency of states between nodes in a\nsystem, while still keeping bloom filter sizes as compact as possible. The\neventuality can be tweaked as desired, by tweaking the distributed bloom\nfilter's parameters. The scalability, as well as accuracy of the data structure\nis made possible by combining two novel ideas: The first idea introduces a new,\ncomputationally inexpensive way for populating bloom filters, making it\npossible to quickly compute new bloom filters when interacting with peers. The\nsecond idea introduces the concept of unique bloom filter mappings between\npeers. By applying these two simple ideas, one can achieve incredibly\nbandwidth-efficient set reconciliation in networks. Instead of trying to\nminimize the false positive rate of a single bloom filter, we use the unique\nbloom filter mappings to increase the probability for an element to propagate\nthrough a network. We compare the standard bloom filter with the distributed\nbloom filter and show that even with a false positive rate of 50%, i.e. even\nwith a very small bloom filter size, the distributed bloom filter still manages\nto reach complete set reconciliation across the network in a highly\nspace-efficient, as well as time-efficient way.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 09:21:17 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 11:57:57 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Ramabaja", "Lum", ""], ["Avdullahu", "Arber", ""]]}, {"id": "1910.07819", "submitter": "Balanand Jha", "authors": "Balanand Jha, David Fern\\'andez-Baca, Akshay Deepak, Kumar Abhishek", "title": "EvoZip: Efficient Compression of Large Collections of Evolutionary Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic trees represent evolutionary relationships among sets of\norganisms. Popular phylogenetic reconstruction approaches typically yield\nhundreds to thousands of trees on a common leafset. Storing and sharing such\nlarge collection of trees requires considerable amount of space and bandwidth.\nFurthermore, the huge size of phylogenetic tree databases can make search and\nretrieval operations time-consuming. Phylogenetic compression techniques are\nspecialized compression techniques that exploit redundant topological\ninformation to achieve better compression of phylogenetic trees. Here, we\npresent EvoZip, a new approach for phylogenetic tree compression. On average,\nEvoZip achieves 71.6% better compression and takes 80.71% less compression time\nand 60.47% less decompression time than TreeZip, the current state-of-the-art\nalgorithm for phylogenetic tree compression. While EvoZip is based on TreeZip,\nit betters TreeZip due to (a) an improved bipartition and support list encoding\nscheme, (b) use of Deflate compression algorithm, and (c) use of an efficient\ntree reconstruction algorithm. EvoZip is freely available online for use by the\nscientific community.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 10:41:47 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Jha", "Balanand", ""], ["Fern\u00e1ndez-Baca", "David", ""], ["Deepak", "Akshay", ""], ["Abhishek", "Kumar", ""]]}, {"id": "1910.07849", "submitter": "Lukas Barth", "authors": "Lukas Barth, Dorothea Wagner", "title": "Engineering Top-Down Weight-Balanced Trees", "comments": "Accepted for publication at ALENEX 2020", "journal-ref": null, "doi": "10.1137/1.9781611976007.13", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight-balanced trees are a popular form of self-balancing binary search\ntrees. Their popularity is due to desirable guarantees, for example regarding\nthe required work to balance annotated trees.\n  While usual weight-balanced trees perform their balancing operations in a\nbottom-up fashion after a modification to the tree is completed, there exists a\ntop-down variant which performs these balancing operations during descend. This\nvariant has so far received only little attention. We provide an in-depth\nanalysis and engineering of these top-down weight-balanced trees, demonstrating\ntheir superior performance. We also gaining insights into how the balancing\nparameters necessary for a weight-balanced tree should be chosen - with the\nsurprising observation that it is often beneficial to choose parameters which\nare not feasible in the sense of the correctness proofs for the rebalancing\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 12:15:09 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 10:22:03 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Barth", "Lukas", ""], ["Wagner", "Dorothea", ""]]}, {"id": "1910.07944", "submitter": "Dekel Tsur", "authors": "Dekel Tsur", "title": "Faster parameterized algorithm for Bicluter Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Bicluter Editing problem the input is a graph $G$ and an integer $k$,\nand the goal is to decide whether $G$ can be transformed into a bicluster graph\nby adding and removing at most $k$ edges. In this paper we give an algorithm\nfor Bicluster Editing whose running time is $O^*(3.116^k)$.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 14:44:20 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Tsur", "Dekel", ""]]}, {"id": "1910.07950", "submitter": "Richard Peng", "authors": "Yu Gao and Jason Li and Danupon Nanongkai and Richard Peng and\n  Thatchaphol Saranurak and Sorrachai Yingchareonthawornchai", "title": "Deterministic Graph Cuts in Subquadratic Time: Sparse, Balanced, and\n  k-Vertex", "comments": "This manuscript is the merge of several results. Parts of it were\n  submitted to FOCS'19 and SODA'20. Part of it has since been subsumed by a new\n  result involving a subset of the authors, arXiv:1910.08025. It's uploaded in\n  its current form due to its significant technical overlap with the improved\n  result. We expect to upload splitted, more up to date, versions of this in\n  the near future", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study deterministic algorithms for computing graph cuts, with focus on two\nfundamental problems: balanced sparse cut and $k$-vertex connectivity for small\n$k$ ($k=O(\\polylog n)$). Both problems can be solved in near-linear time with\nrandomized algorithms, but their previous deterministic counterparts take at\nleast quadratic time. In this paper, we break this bound for both problems.\nInterestingly, achieving this for one problem crucially relies on doing so for\nthe other.\n  In particular, via a divide-and-conquer argument, a variant of the\ncut-matching game by [Khandekar et al.`07], and the local vertex connectivity\nalgorithm of [Nanongkai et al. STOC'19], we give a subquadratic time algorithm\nfor $k$-vertex connectivity using a subquadratic time algorithm for computing\nbalanced sparse cuts on sparse graphs. To achieve the latter, we improve the\npreviously best $mn$ bound for approximating balanced sparse cut for the whole\nrange of $m$. This starts from (1) breaking the $n^3$ barrier on dense graphs\nto $n^{\\omega + o(1)}$ (where $\\omega < 2.372$) using the the PageRank matrix,\nbut without explicitly sweeping to find sparse cuts; to (2) getting the $\\tilde\nO(m^{1.58})$ bound by combining the $J$-trees by [Madry FOCS `10] with the\n$n^{\\omega + o(1)}$ bound above, and finally; to (3) getting the $m^{1.5 +\no(1)}$ bound by recursively invoking the second bound in conjunction with\nexpander-based graph sparsification. Interestingly, our final $m^{1.5 + o(1)}$\nbound lands at a natural stopping point in the sense that polynomially breaking\nit would lead to a breakthrough for the dynamic connectivity problem.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 14:58:16 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 17:28:16 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Gao", "Yu", ""], ["Li", "Jason", ""], ["Nanongkai", "Danupon", ""], ["Peng", "Richard", ""], ["Saranurak", "Thatchaphol", ""], ["Yingchareonthawornchai", "Sorrachai", ""]]}, {"id": "1910.08025", "submitter": "Thatchaphol Saranurak", "authors": "Julia Chuzhoy, Yu Gao, Jason Li, Danupon Nanongkai, Richard Peng,\n  Thatchaphol Saranurak", "title": "A Deterministic Algorithm for Balanced Cut with Applications to Dynamic\n  Connectivity, Flows, and Beyond", "comments": "Improved presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical Minimum Balanced Cut problem: given a graph $G$,\ncompute a partition of its vertices into two subsets of roughly equal volume,\nwhile minimizing the number of edges connecting the subsets. We present the\nfirst {\\em deterministic, almost-linear time} approximation algorithm for this\nproblem. Specifically, our algorithm, given an $n$-vertex $m$-edge graph $G$\nand any parameter $1\\leq r\\leq O(\\log n)$, computes a $(\\log\nm)^{r^2}$-approximation for Minimum Balanced Cut on $G$, in time $O\\left (\nm^{1+O(1/r)+o(1)}\\cdot (\\log m)^{O(r^2)}\\right )$. In particular, we obtain a\n$(\\log m)^{1/\\epsilon}$-approximation in time $m^{1+O(1/\\sqrt{\\epsilon})}$ for\nany constant $\\epsilon$, and a $(\\log m)^{f(m)}$-approximation in time\n$m^{1+o(1)}$, for any slowly growing function $m$. We obtain deterministic\nalgorithms with similar guarantees for the Sparsest Cut and the\nLowest-Conductance Cut problems.\n  Our algorithm for the Minimum Balanced Cut problem in fact provides a\nstronger guarantee: it either returns a balanced cut whose value is close to a\ngiven target value, or it certifies that such a cut does not exist by\nexhibiting a large subgraph of $G$ that has high conductance. We use this\nalgorithm to obtain deterministic algorithms for dynamic connectivity and\nminimum spanning forest, whose worst-case update time on an $n$-vertex graph is\n$n^{o(1)}$, thus resolving a major open problem in the area of dynamic graph\nalgorithms. Our work also implies deterministic algorithms for a host of\nadditional problems, whose time complexities match, up to subpolynomial in $n$\nfactors, those of known randomized algorithms. The implications include\nalmost-linear time deterministic algorithms for solving Laplacian systems and\nfor approximating maximum flows in undirected graphs.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 16:52:19 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 17:40:42 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Chuzhoy", "Julia", ""], ["Gao", "Yu", ""], ["Li", "Jason", ""], ["Nanongkai", "Danupon", ""], ["Peng", "Richard", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "1910.08033", "submitter": "Yin Tat Lee", "authors": "Yin Tat Lee and Aaron Sidford", "title": "Solving Linear Programs with Sqrt(rank) Linear System Solves", "comments": "The merged version of abs/1312.6677 and abs/1312.6713. It contains\n  several new results beyond these prior submissions, including a nearly\n  optimal self-concordant barrier and its relation to Lewis weight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm that given a linear program with $n$ variables, $m$\nconstraints, and constraint matrix $A$, computes an $\\epsilon$-approximate\nsolution in $\\tilde{O}(\\sqrt{rank(A)}\\log(1/\\epsilon))$ iterations with high\nprobability. Each iteration of our method consists of solving $\\tilde{O}(1)$\nlinear systems and additional nearly linear time computation, improving by a\nfactor of $\\tilde{\\Omega}((m/rank(A))^{1/2})$ over the previous fastest method\nwith this iteration cost due to Renegar (1988). Further, we provide a\ndeterministic polynomial time computable $\\tilde{O}(rank(A))$-self-concordant\nbarrier function for the polytope, resolving an open question of Nesterov and\nNemirovski (1994) on the theory of \"universal barriers\" for interior point\nmethods.\n  Applying our techniques to the linear program formulation of maximum flow\nyields an $\\tilde{O}(|E|\\sqrt{|V|}\\log(U))$ time algorithm for solving the\nmaximum flow problem on directed graphs with $|E|$ edges, $|V|$ vertices, and\ninteger capacities of size at most $U$. This improves upon the previous fastest\npolynomial running time of\n$O(|E|\\min\\{|E|^{1/2},|V|^{2/3}\\}\\log(|V|^{2}/|E|)\\log(U))$ achieved by\nGoldberg and Rao (1998). In the special case of solving dense directed unit\ncapacity graphs our algorithm improves upon the previous fastest running times\nof $O(|E|\\min\\{|E|^{1/2},|V|^{2/3}\\})$ achieved by Even and Tarjan (1975) and\nKarzanov (1973) and of $\\tilde{O}(|E|^{10/7})$ achieved more recently by\nM\\k{a}dry (2013).\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 16:59:29 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 18:42:17 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Lee", "Yin Tat", ""], ["Sidford", "Aaron", ""]]}, {"id": "1910.08283", "submitter": "Muhammad Irfan Yousuf Dr.", "authors": "Muhammad Irfan Yousuf, Raheel Anwar", "title": "Weighted Edge Sampling for Static Graphs", "comments": "9 pages, 3 figures, Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Sampling provides an efficient yet inexpensive solution for analyzing\nlarge graphs. While extracting small representative subgraphs from large\ngraphs, the challenge is to capture the properties of the original graph.\nSeveral sampling algorithms have been proposed in previous studies, but they\nlack in extracting good samples. In this paper, we propose a new sampling\nmethod called Weighted Edge Sampling. In this method, we give equal weight to\nall the edges in the beginning. During the sampling process, we sample an edge\nwith the probability proportional to its weight. When an edge is sampled, we\nincrease the weight of its neighboring edges and this increases their\nprobability to be sampled. Our method extracts the neighborhood of a sampled\nedge more efficiently than previous approaches. We evaluate the efficacy of our\nsampling approach empirically using several real-world data sets and compare it\nwith some of the previous approaches. We find that our method produces samples\nthat better match the original graphs. We also calculate the Root Mean Square\nError and Kolmogorov Smirnov distance to compare the results quantitatively.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 06:51:46 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Yousuf", "Muhammad Irfan", ""], ["Anwar", "Raheel", ""]]}, {"id": "1910.08360", "submitter": "Simon Pukrop", "authors": "Alexander M\\\"acker, Friedhelm Meyer auf der Heide, Simon Pukrop", "title": "Approximating Weighted Completion Time for Order Scheduling with Setup\n  Times", "comments": "This paper was accepted for puplication in the proceedings of the\n  International Conference on Current Trends in Theory and Practice of Computer\n  Science (SOFSEM 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a scheduling problem in which jobs need to be processed on a single\nmachine. Each job has a weight and is composed of several operations belonging\nto different families. The machine needs to perform a setup between the\nprocessing of operations of different families. A job is completed when its\nlatest operation completes and the goal is to minimize the total weighted\ncompletion time of all jobs.\n  We study this problem from the perspective of approximability and provide\nconstant factor approximations as well as an inapproximability result. Prior to\nthis work, only the NP-hardness of the unweighted case and the polynomial\nsolvability of a certain special case were known.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 12:04:53 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 12:56:26 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["M\u00e4cker", "Alexander", ""], ["der Heide", "Friedhelm Meyer auf", ""], ["Pukrop", "Simon", ""]]}, {"id": "1910.08657", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Nick Duffield, Ryan A. Rossi", "title": "Temporal Network Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal networks representing a stream of timestamped edges are seemingly\nubiquitous in the real-world. However, the massive size and continuous nature\nof these networks make them fundamentally challenging to analyze and leverage\nfor descriptive and predictive modeling tasks. In this work, we propose a\ngeneral framework for temporal network sampling with unbiased estimation. We\ndevelop online, single-pass sampling algorithms and unbiased estimators for\ntemporal network sampling. The proposed algorithms enable fast, accurate, and\nmemory-efficient statistical estimation of temporal network patterns and\nproperties. In addition, we propose a temporally decaying sampling algorithm\nwith unbiased estimators for studying networks that evolve in continuous time,\nwhere the strength of links is a function of time, and the motif patterns are\ntemporally-weighted. In contrast to the prior notion of a $\\bigtriangleup\nt$-temporal motif, the proposed formulation and algorithms for counting\ntemporally weighted motifs are useful for forecasting tasks in networks such as\npredicting future links, or a future time-series variable of nodes and links.\nFinally, extensive experiments on a variety of temporal networks from different\ndomains demonstrate the effectiveness of the proposed algorithms. A detailed\nablation study is provided to understand the impact of the various components\nof the proposed framework.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 23:10:18 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 02:47:15 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Duffield", "Nick", ""], ["Rossi", "Ryan A.", ""]]}, {"id": "1910.08889", "submitter": "Rakesh Venkat", "authors": "Anand Louis, Rakesh Venkat", "title": "Planted Models for $k$-way Edge and Vertex Expansion", "comments": "An extended abstract of this paper has been accepted to FSTTCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graph partitioning problems are a central topic of study in algorithms and\ncomplexity theory. Edge expansion and vertex expansion, two popular graph\npartitioning objectives, seek a $2$-partition of the vertex set of the graph\nthat minimizes the considered objective. However, for many natural\napplications, one might require a graph to be partitioned into $k$ parts, for\nsome $k \\geq 2$. For a $k$-partition $S_1, \\ldots, S_k$ of the vertex set of a\ngraph $G = (V,E)$, the $k$-way edge expansion (resp. vertex expansion) of\n$\\{S_1, \\ldots, S_k\\}$ is defined as $\\max_{i \\in [k]} \\Phi(S_i)$, and the\nbalanced $k$-way edge expansion (resp. vertex expansion) of $G$ is defined as\n\\[ \\min_{ \\{S_1, \\ldots, S_k\\} \\in \\mathcal{P}_k} \\max_{i \\in [k]} \\Phi(S_i) \\,\n, \\] where $\\mathcal{P}_k$ is the set of all balanced $k$-partitions of $V$\n(i.e each part of a $k$-partition in $\\mathcal{P}_k$ should have cardinality\n$|V|/k$), and $\\Phi(S)$ denotes the edge expansion (resp. vertex expansion) of\n$S \\subset V$. We study a natural planted model for graphs where the vertex set\nof a graph has a $k$-partition $S_1, \\ldots, S_k$ such that the graph induced\non each $S_i$ has large expansion, but each $S_i$ has small edge expansion\n(resp. vertex expansion) in the graph. We give bi-criteria approximation\nalgorithms for computing the balanced $k$-way edge expansion (resp. vertex\nexpansion) of instances in this planted model.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 04:05:25 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Louis", "Anand", ""], ["Venkat", "Rakesh", ""]]}, {"id": "1910.08920", "submitter": "Mike Cinkoske", "authors": "Jeremiah Blocki, Mike Cinkoske", "title": "A New Connection Between Node and Edge Depth Robust Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a directed acyclic graph (DAG) $G = (V,E)$, we say that $G$ is\n$(e,d)$-depth-robust (resp. $(e,d)$-edge-depth-robust) if for any set $S\n\\subset V$ (resp. $S \\subseteq E$) of at most $|S| \\leq e$ nodes (resp. edges)\nthe graph $G-S$ contains a directed path of length $d$. While edge-depth-robust\ngraphs are potentially easier to construct many applications in cryptography\nrequire node depth-robust graphs with small indegree. We create a graph\nreduction that transforms an $(e, d)$-edge-depth-robust graph with $m$ edges\ninto a $(e/2,d)$-depth-robust graph with $O(m)$ nodes and constant indegree.\nOne immediate consequence of this result is the first construction of a\nprovably $(\\frac{n \\log \\log n}{\\log n}, \\frac{n}{(\\log n)^{1 + \\log \\log\nn}})$-depth-robust graph with constant indegree, where previous constructions\nfor $e =\\frac{n \\log \\log n}{\\log n}$ had $d = O(n^{1-\\epsilon})$. Our\nreduction crucially relies on ST-Robust graphs, a new graph property we\nintroduce which may be of independent interest. We say that a directed, acyclic\ngraph with $n$ inputs and $n$ outputs is $(k_1, k_2)$-ST-Robust if we can\nremove any $k_1$ nodes and there exists a subgraph containing at least $k_2$\ninputs and $k_2$ outputs such that each of the $k_2$ inputs is connected to all\nof the $k_2$ outputs. If the graph if $(k_1,n-k_1)$-ST-Robust for all $k_1 \\leq\nn$ we say that the graph is maximally ST-robust. We show how to construct\nmaximally ST-robust graphs with constant indegree and $O(n)$ nodes. Given a\nfamily $ \\mathbb{M}$ of ST-robust graphs and an arbitrary $(e,\nd)$-edge-depth-robust graph $G$ we construct a new constant-indegree graph $\n\\mathrm{Reduce}(G, \\mathbb{M})$ by replacing each node in $G$ with an ST-robust\ngraph from $ \\mathbb{M}$. We also show that ST-robust graphs can be used to\nconstruct (tight) proofs-of-space and (asymptotically) improved wide-block\nlabeling functions.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 07:36:37 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 20:16:38 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 01:43:56 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Blocki", "Jeremiah", ""], ["Cinkoske", "Mike", ""]]}, {"id": "1910.09005", "submitter": "Jacob Holm", "authors": "Jacob Holm and Eva Rotenberg", "title": "Worst-Case Polylog Incremental SPQR-trees: Embeddings, Planarity, and\n  Triconnectivity", "comments": "Accepted for publication at SODA'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that every labelled planar graph $G$ can be assigned a canonical\nembedding $\\phi(G)$, such that for any planar $G'$ that differs from $G$ by the\ninsertion or deletion of one edge, the number of local changes to the\ncombinatorial embedding needed to get from $\\phi(G)$ to $\\phi(G')$ is $O(\\log\nn)$.\n  In contrast, there exist embedded graphs where $\\Omega(n)$ changes are\nnecessary to accommodate one inserted edge. We provide a matching lower bound\nof $\\Omega(\\log n)$ local changes, and although our upper bound is worst-case,\nour lower bound hold in the amortized case as well.\n  Our proof is based on BC trees and SPQR trees, and we develop\n\\emph{pre-split} variants of these for general graphs, based on a novel biased\nheavy-path decomposition, where the structural changes corresponding to edge\ninsertions and deletions in the underlying graph consist of at most $O(\\log n)$\nbasic operations of a particularly simple form.\n  As a secondary result, we show how to maintain the pre-split trees under edge\ninsertions in the underlying graph deterministically in worst case $O(\\log^3\nn)$ time. Using this, we obtain deterministic data structures for incremental\nplanarity testing, incremental planar embedding, and incremental\ntriconnectivity, that each have worst case $O(\\log^3 n)$ update and query time,\nanswering an open question by La Poutr\\'e and Westbrook from 1998.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 16:00:27 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Holm", "Jacob", ""], ["Rotenberg", "Eva", ""]]}, {"id": "1910.09020", "submitter": "Mohammed Alser", "authors": "Mohammed Alser, Taha Shahroodi, Juan Gomez-Luna, Can Alkan, and Onur\n  Mutlu", "title": "SneakySnake: A Fast and Accurate Universal Genome Pre-Alignment Filter\n  for CPUs, GPUs, and FPGAs", "comments": "To appear in Bioinformatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.AR cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: We introduce SneakySnake, a highly parallel and highly accurate\npre-alignment filter that remarkably reduces the need for computationally\ncostly sequence alignment. The key idea of SneakySnake is to reduce the\napproximate string matching (ASM) problem to the single net routing (SNR)\nproblem in VLSI chip layout. In the SNR problem, we are interested in finding\nthe optimal path that connects two terminals with the least routing cost on a\nspecial grid layout that contains obstacles. The SneakySnake algorithm quickly\nsolves the SNR problem and uses the found optimal path to decide whether or not\nperforming sequence alignment is necessary. Reducing the ASM problem into SNR\nalso makes SneakySnake efficient to implement on CPUs, GPUs, and FPGAs.\nResults: SneakySnake significantly improves the accuracy of pre-alignment\nfiltering by up to four orders of magnitude compared to the state-of-the-art\npre-alignment filters, Shouji, GateKeeper, and SHD. For short sequences,\nSneakySnake accelerates Edlib (state-of-the-art implementation of Myers's\nbit-vector algorithm) and Parasail (state-of-the-art sequence aligner with a\nconfigurable scoring function), by up to 37.7x and 43.9x (>12x on average),\nrespectively, with its CPU implementation, and by up to 413x and 689x (>400x on\naverage), respectively, with FPGA and GPU acceleration. For long sequences, the\nCPU implementation of SneakySnake accelerates Parasail and KSW2 (sequence\naligner of minimap2) by up to 979x (276.9x on average) and 91.7x (31.7x on\naverage), respectively. As SneakySnake does not replace sequence alignment,\nusers can still obtain all capabilities (e.g., configurable scoring functions)\nof the aligner of their choice, unlike existing acceleration efforts that\nsacrifice some aligner capabilities. Availability:\nhttps://github.com/CMU-SAFARI/SneakySnake\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 16:48:05 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 10:47:00 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 20:38:49 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Alser", "Mohammed", ""], ["Shahroodi", "Taha", ""], ["Gomez-Luna", "Juan", ""], ["Alkan", "Can", ""], ["Mutlu", "Onur", ""]]}, {"id": "1910.09071", "submitter": "Mehdi Soleimanifar", "authors": "Aram Harrow, Saeed Mehraban, Mehdi Soleimanifar", "title": "Classical algorithms, correlation decay, and complex zeros of partition\n  functions of quantum many-body systems", "comments": "54 pages, 4 figures", "journal-ref": "Proc of STOC 2020, pp 378-386", "doi": "10.1145/3357713.3384322", "report-no": "MIT-CTP/5288", "categories": "quant-ph cond-mat.stat-mech cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a quasi-polynomial time classical algorithm that\nestimates the partition function of quantum many-body systems at temperatures\nabove the thermal phase transition point. It is known that in the worst case,\nthe same problem is NP-hard below this point. Together with our work, this\nshows that the transition in the phase of a quantum system is also accompanied\nby a transition in the hardness of approximation. We also show that in a system\nof n particles above the phase transition point, the correlation between two\nobservables whose distance is at least log(n) decays exponentially. We can\nimprove the factor of log(n) to a constant when the Hamiltonian has commuting\nterms or is on a 1D chain. The key to our results is a characterization of the\nphase transition and the critical behavior of the system in terms of the\ncomplex zeros of the partition function. Our work extends a seminal work of\nDobrushin and Shlosman on the equivalence between the decay of correlations and\nthe analyticity of the free energy in classical spin models. On the algorithmic\nside, our result extends the scope of a recent approach due to Barvinok for\nsolving classical counting problems to quantum many-body systems.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 22:08:20 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Harrow", "Aram", ""], ["Mehraban", "Saeed", ""], ["Soleimanifar", "Mehdi", ""]]}, {"id": "1910.09131", "submitter": "Zhenwei Dai", "authors": "Zhenwei Dai, Anshumali Shrivastava", "title": "Adaptive Learned Bloom Filter (Ada-BF): Efficient Utilization of the\n  Classifier", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work suggests improving the performance of Bloom filter by\nincorporating a machine learning model as a binary classifier. However, such\nlearned Bloom filter does not take full advantage of the predicted probability\nscores. We proposed new algorithms that generalize the learned Bloom filter by\nusing the complete spectrum of the scores regions. We proved our algorithms\nhave lower False Positive Rate (FPR) and memory usage compared with the\nexisting approaches to learned Bloom filter. We also demonstrated the improved\nperformance of our algorithms on real-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 03:21:24 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Dai", "Zhenwei", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1910.09606", "submitter": "Jesse Bartels", "authors": "Jesse Bartels, Jon Stephens, Saumya Debray", "title": "Representing and Reasoning about Dynamic Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic code, i.e., code that is created or modified at runtime, is\nubiquitous in today's world. The behavior of dynamic code can depend on the\nlogic of the dynamic code generator in subtle and non-obvious ways, with\nsignificant security implications, e.g., JIT compiler bugs can lead to\nexploitable vulnerabilities in the resulting JIT-compiled code. Existing\napproaches to program analysis do not provide adequate support for reasoning\nabout such behavioral relationships. This paper takes a first step in\naddressing this problem by describing a program representation and a new notion\nof dependency that allows us to reason about dependency and information flow\nrelationships between the dynamic code generator and the generated dynamic\ncode. Experimental results show that analyses based on these concepts are able\nto capture properties of dynamic code that cannot be identified using\ntraditional program analyses.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 19:00:48 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 20:45:03 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 17:14:04 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Bartels", "Jesse", ""], ["Stephens", "Jon", ""], ["Debray", "Saumya", ""]]}, {"id": "1910.09791", "submitter": "Ei Ando", "authors": "Ei Ando", "title": "The Distribution Function of the Longest Path Length in Constant\n  Treewidth DAGs with Random Edge Length", "comments": "40 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about the length $X_{\\rm MAX}$ of the longest path in directed\nacyclic graph (DAG) $G=(V,E)$ that have random edge lengths, where $|V|=n$ and\n$|E|=m$. Especially, when the edge lengths are mutually independent and\nuniformly distributed, the problem of computing the distribution function\n$\\Pr[X_{\\rm MAX}\\le x]$ is known to be $#$P-hard even in case $G$ is a directed\npath. This is because $\\Pr[X_{\\rm MAX}\\le x]$ is equal to the volume of the\nknapsack polytope, an $m$-dimensional unit hypercube truncated by a halfspace.\nIn this paper, we show that there is a {\\em deterministic} fully polynomial\ntime approximation scheme (FPTAS) for computing $\\Pr[X_{\\rm MAX}\\le x]$ in case\nthe treewidth of $G$ is bounded by a constant $k$, where there may be\nexponentially many $s-t$ paths in $G$. The running time of our algorithm is\n$O((3k+2)^2 n(\\frac{(6k+6)mn}{\\epsilon})^{9k^2+15k+6})$ to achieve a\nmultiplicative approximation ratio $1+\\epsilon$. On the way to show our FPTAS,\nwe show a fundamental formula that represents $\\Pr[X_{\\rm MAX}\\le x]$ by at\nmost $n-1$ repetition of definite integrals. This also leads us to more\nresults. In case the edge lengths obey the mutually independent standard\nexponential distribution, we show that there exists a $((6k+4)mn)^{O(k)}$ time\nexact algorithm. We also show, for random edge lengths satisfying certain\nconditions, that computing $\\Pr[X_{\\rm MAX}\\le x]$ is fixed parameter tractable\nif we choose treewidth $k$, the additive error $\\epsilon'$ and $x$ as the\nparameters.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 06:56:53 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 03:19:37 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 15:50:44 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ando", "Ei", ""]]}, {"id": "1910.09812", "submitter": "Tobias Z\\\"undorf", "authors": "Moritz Baum, Julian Dibbelt, Andreas Gemsa, Dorothea Wagner, Tobias\n  Z\\\"undorf", "title": "Shortest Feasible Paths with Charging Stops for Battery Electric\n  Vehicles", "comments": "43 pages, 12 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of minimizing overall trip time for battery electric\nvehicles in road networks. As battery capacity is limited, stops at charging\nstations may be inevitable. Careful route planning is crucial, since charging\nstations are scarce and recharging is time-consuming. We extend the Constrained\nShortest Path problem for electric vehicles with realistic models of charging\nstops, including varying charging power and battery swapping stations. While\nthe resulting problem is NP-hard, we propose a combination of algorithmic\ntechniques to achieve good performance in practice. Extensive experimental\nevaluation shows that our approach (CHArge) enables computation of optimal\nsolutions on realistic inputs, even of continental scale. Finally, we\ninvestigate heuristic variants of CHArge that derive high-quality routes in\nwell below a second on sensible instances.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 08:05:46 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Baum", "Moritz", ""], ["Dibbelt", "Julian", ""], ["Gemsa", "Andreas", ""], ["Wagner", "Dorothea", ""], ["Z\u00fcndorf", "Tobias", ""]]}, {"id": "1910.09985", "submitter": "Ruslan Shaydulin", "authors": "Hayato Ushijima-Mwesigwa, Ruslan Shaydulin, Christian F. A. Negre,\n  Susan M. Mniszewski, Yuri Alexeev, Ilya Safro", "title": "Multilevel Combinatorial Optimization Across Quantum Architectures", "comments": null, "journal-ref": null, "doi": "10.1145/3425607", "report-no": "LA-UR-19-30113", "categories": "quant-ph cs.DS physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging quantum processors provide an opportunity to explore new approaches\nfor solving traditional problems in the post Moore's law supercomputing era.\nHowever, the limited number of qubits makes it infeasible to tackle massive\nreal-world datasets directly in the near future, leading to new challenges in\nutilizing these quantum processors for practical purposes. Hybrid\nquantum-classical algorithms that leverage both quantum and classical types of\ndevices are considered as one of the main strategies to apply quantum computing\nto large-scale problems. In this paper, we advocate the use of multilevel\nframeworks for combinatorial optimization as a promising general paradigm for\ndesigning hybrid quantum-classical algorithms. In order to demonstrate this\napproach, we apply this method to two well-known combinatorial optimization\nproblems, namely, the Graph Partitioning Problem, and the Community Detection\nProblem. We develop hybrid multilevel solvers with quantum local search on\nD-Wave's quantum annealer and IBM's gate-model based quantum processor. We\ncarry out experiments on graphs that are orders of magnitudes larger than the\ncurrent quantum hardware size, and we observe results comparable to\nstate-of-the-art solvers in terms of quality of the solution.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 13:56:43 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 17:16:51 GMT"}, {"version": "v3", "created": "Sun, 16 Feb 2020 23:42:56 GMT"}, {"version": "v4", "created": "Wed, 3 Jun 2020 01:59:24 GMT"}, {"version": "v5", "created": "Tue, 22 Sep 2020 14:53:30 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ushijima-Mwesigwa", "Hayato", ""], ["Shaydulin", "Ruslan", ""], ["Negre", "Christian F. A.", ""], ["Mniszewski", "Susan M.", ""], ["Alexeev", "Yuri", ""], ["Safro", "Ilya", ""]]}, {"id": "1910.10113", "submitter": "Alessandra Tappini", "authors": "Giuseppe Liotta, Ignaz Rutter, Alessandra Tappini", "title": "Simultaneous FPQ-Ordering and Hybrid Planarity Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the interplay between embedding constrained planarity and hybrid\nplanarity testing. We consider a constrained planarity testing problem, called\n1-Fixed Constrained Planarity, and prove that this problem can be solved in\nquadratic time for biconnected graphs. Our solution is based on a new\ndefinition of fixedness that makes it possible to simplify and extend known\ntechniques about Simultaneous PQ-Ordering. We apply these results to different\nvariants of hybrid planarity testing, including a relaxation of NodeTrix\nPlanarity with fixed sides, that allows rows and columns to be independently\npermuted.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:55:19 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Liotta", "Giuseppe", ""], ["Rutter", "Ignaz", ""], ["Tappini", "Alessandra", ""]]}, {"id": "1910.10116", "submitter": "Gergely Odor", "authors": "Gergely Odor, Patrick Thiran", "title": "Sequential metric dimension for random graphs", "comments": "Typos, errors corrected. Summary of results and simulations added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the localization game on a graph, the goal is to find a fixed but unknown\ntarget node $v^\\star$ with the least number of distance queries possible. In\nthe $j^{th}$ step of the game, the player queries a single node $v_j$ and\nreceives, as an answer to their query, the distance between the nodes $v_j$ and\n$v^\\star$. The sequential metric dimension (SMD) is the minimal number of\nqueries that the player needs to guess the target with absolute certainty, no\nmatter where the target is.\n  The term SMD originates from the related notion of metric dimension (MD),\nwhich can be defined the same way as the SMD, except that the player's queries\nare non-adaptive. In this work, we extend the results of arXiv:1208.3801 on the\nMD of Erd\\H{o}s-R\\'enyi graphs to the SMD. We find that, in connected\nErd\\H{o}s-R\\'enyi graphs, the MD and the SMD are a constant factor apart. For\nthe lower bound we present a clean analysis by combining tools developed for\nthe MD and a novel coupling argument. For the upper bound we show that a\nstrategy that greedily minimizes the number of candidate targets in each step\nuses asymptotically optimal queries in Erd\\H{o}s-R\\'enyi graphs. Connections\nwith source localization, binary search on graphs and the birthday problem are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 17:08:46 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 08:02:03 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Odor", "Gergely", ""], ["Thiran", "Patrick", ""]]}, {"id": "1910.10348", "submitter": "Thomas Kalinowski", "authors": "Saman Eskandarzadeh, Thomas Kalinowski, Hamish Waterer", "title": "Maintenance scheduling in a railway corricdor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a novel scheduling problem which is motivated by an\napplication in the Australian railway industry. Given a set of maintenance jobs\nand a set of train paths over a railway corridor with bidirectional traffic, we\nseek a schedule of jobs such that a minimum number of train paths are cancelled\ndue to conflict with the job schedule. We show that the problem is NP-complete\nin general. In a special case of the problem when every job under any schedule\njust affects one train path, and the speed of trains is bounded from above and\nbelow, we show that the problem can be solved in polynomial time. Moreover, in\nanother special case of the problem where the traffic is unidirectional, we\nshow that the problem can be solved in time $O(n^4)$.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 04:44:04 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Eskandarzadeh", "Saman", ""], ["Kalinowski", "Thomas", ""], ["Waterer", "Hamish", ""]]}, {"id": "1910.10359", "submitter": "Richard Peng", "authors": "Yang P. Liu and Richard Peng and Mark Sellke", "title": "Vertex Sparsifiers for c-Edge Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show the existence of O(f(c)k) sized vertex sparsifiers that preserve all\nedge-connectivity values up to c between a set of k terminal vertices, where\nf(c) is a function that only depends on c, the edge-connectivity value. This\nconstruction is algorithmic: we also provide an algorithm whose running time\ndepends linearly on k, but exponentially in c. It implies that for constant\nvalues of c, an offline sequence of edge insertions/deletions and\nc-edge-connectivity queries can be answered in polylog time per operation.\nThese results are obtained by combining structural results about minimum\nterminal separating cuts in undirected graphs with recent developments in\nexpander decomposition based methods for finding small vertex/edge cuts in\ngraphs.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 05:20:07 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Liu", "Yang P.", ""], ["Peng", "Richard", ""], ["Sellke", "Mark", ""]]}, {"id": "1910.10364", "submitter": "Vinod Reddy I", "authors": "I.Vinod Reddy", "title": "Parameterized Coloring Problems on Threshold Graphs", "comments": "12pages, latest version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study several coloring problems on graphs from the\nviewpoint of parameterized complexity. We show that Precoloring Extension is\nfixed-parameter tractable (FPT) parameterized by distance to clique and\nEquitable Coloring is FPT parameterized by the distance to threshold graphs. We\nalso study the List k-Coloring and show that the problem is NP-complete on\nsplit graphs and it is FPT parameterized by solution size on split graphs.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 05:43:01 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 10:22:37 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 05:49:12 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Reddy", "I. Vinod", ""]]}, {"id": "1910.10406", "submitter": "Tetsuo Yokoyama", "authors": "Hiroki Masuda, Tetsuo Yokoyama", "title": "Analyzing Trade-offs in Reversible Linear and Binary Search Algorithms", "comments": "Proceedings of the Third Workshop on Software Foundations for Data\n  Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reversible algorithms are algorithms in which each step represents a partial\ninjective function; they are useful for performance optimization in reversible\nsystems. In this study, using Janus, a reversible imperative high-level\nprogramming language, we have developed reversible linear and binary search\nalgorithms. We have analyzed the non-trivial space-time trade-offs between\nthem, focusing on the memory usage disregarding original inputs and outputs,\nthe size of the output garbage disregarding the original inputs, and the\nmaximum amount of traversal of the input. The programs in this study can easily\nbe adapted to other reversible programming languages. Our analysis reveals that\nthe change of the output data and/or the data structure affects the design of\nefficient reversible algorithms. For example, the number of input data\ntraversals depends on whether the search has succeeded or failed, while it\nexpectedly never changes in corresponding irreversible linear and binary\nsearches. Our observations indicate the importance of the selection of data\nstructures and what is regarded as the output with the aim of the reversible\nalgorithm design.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 08:24:15 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Masuda", "Hiroki", ""], ["Yokoyama", "Tetsuo", ""]]}, {"id": "1910.10412", "submitter": "Guillaume Ducoffe", "authors": "Feodor F. Dragan and Guillaume Ducoffe", "title": "A story of diameter, radius and Helly property", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is Helly if every family of pairwise intersecting balls has a\nnonempty common intersection. Motivated by previous work on dually chordal\ngraphs and graphs of bounded distance VC-dimension we prove several new results\non the complexity of computing the diameter and the radius on Helly graphs and\nrelated graph classes.\n  * First, we present algorithms which given an $n$-vertex $m$-edge Helly graph\n$G$ as input, compute w.h.p. its radius and its diameter in time $\\tilde{\\cal\nO}(m\\sqrt{n})$. Our algorithms are based on the Helly property and on several\nimplications of the unimodality of the eccentricity function in Helly graphs:\nevery vertex of locally minimum eccentricity is a central vertex.\n  * Then, we focus on $C_4$-free Helly graphs, which include, amongst other\nsubclasses, bridged Helly graphs and so, chordal Helly graphs and hereditary\nHelly graphs. For the $C_4$-free Helly graphs, we present linear-time\nalgorithms for computing the eccentricity of all vertices. Doing so, we\ngeneralize previous results on strongly chordal graphs to a much larger\nsubclass.\n  * Finally, we derive from our findings on chordal Helly graphs a more general\none-to-many reduction from diameter computation on chordal graphs to either\ndiameter computation on split graphs or the {\\sc Disjoint Set} problem.\nTherefore, split graphs are in some sense the {\\em only} hard instances for\ndiameter computation on chordal graphs. As a byproduct of our reduction the\neccentricity of all vertices in a chordal graph can be approximated in ${\\cal\nO}(m\\log{n})$ time with an additive one-sided error of at most one, and on any\nsubclass of chordal graphs with constant VC-dimension the diameter can be\ncomputed in truly subquadratic time.\n  These above results are a new step toward better understanding the role of\nabstract geometric properties in the fast computation of metric graph\ninvariants.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 08:44:08 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 13:42:45 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Dragan", "Feodor F.", ""], ["Ducoffe", "Guillaume", ""]]}, {"id": "1910.10571", "submitter": "Deeksha Adil", "authors": "Deeksha Adil and Sushant Sachdeva", "title": "Faster p-norm minimizing flows, via smoothed q-norm problems", "comments": "ACM-SIAM Symposium on Discrete Algorithms (SODA 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present faster high-accuracy algorithms for computing $\\ell_p$-norm\nminimizing flows. On a graph with $m$ edges, our algorithm can compute a\n$(1+1/\\text{poly}(m))$-approximate unweighted $\\ell_p$-norm minimizing flow\nwith $pm^{1+\\frac{1}{p-1}+o(1)}$ operations, for any $p \\ge 2,$ giving the best\nbound for all $p\\gtrsim 5.24.$ Combined with the algorithm from the work of\nAdil et al. (SODA '19), we can now compute such flows for any $2\\le p\\le\nm^{o(1)}$ in time at most $O(m^{1.24}).$ In comparison, the previous best\nrunning time was $\\Omega(m^{1.33})$ for large constant $p.$ For\n$p\\sim\\delta^{-1}\\log m,$ our algorithm computes a $(1+\\delta)$-approximate\nmaximum flow on undirected graphs using $m^{1+o(1)}\\delta^{-1}$ operations,\nmatching the current best bound, albeit only for unit-capacity graphs.\n  We also give an algorithm for solving general $\\ell_{p}$-norm regression\nproblems for large $p.$ Our algorithm makes\n$pm^{\\frac{1}{3}+o(1)}\\log^2(1/\\varepsilon)$ calls to a linear solver. This\ngives the first high-accuracy algorithm for computing weighted $\\ell_{p}$-norm\nminimizing flows that runs in time $o(m^{1.5})$ for some $p=m^{\\Omega(1)}.$ Our\nkey technical contribution is to show that smoothed $\\ell_p$-norm problems\nintroduced by Adil et al., are interreducible for different values of $p.$ No\nsuch reduction is known for standard $\\ell_p$-norm problems.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 14:12:50 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 09:35:24 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Adil", "Deeksha", ""], ["Sachdeva", "Sushant", ""]]}, {"id": "1910.10593", "submitter": "Petar Veli\\v{c}kovi\\'c", "authors": "Petar Veli\\v{c}kovi\\'c, Rex Ying, Matilde Padovano, Raia Hadsell,\n  Charles Blundell", "title": "Neural Execution of Graph Algorithms", "comments": "To appear at ICLR 2020. 13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) are a powerful representational tool for solving\nproblems on graph-structured inputs. In almost all cases so far, however, they\nhave been applied to directly recovering a final solution from raw inputs,\nwithout explicit guidance on how to structure their problem-solving. Here,\ninstead, we focus on learning in the space of algorithms: we train several\nstate-of-the-art GNN architectures to imitate individual steps of classical\ngraph algorithms, parallel (breadth-first search, Bellman-Ford) as well as\nsequential (Prim's algorithm). As graph algorithms usually rely on making\ndiscrete decisions within neighbourhoods, we hypothesise that\nmaximisation-based message passing neural networks are best-suited for such\nobjectives, and validate this claim empirically. We also demonstrate how\nlearning in the space of algorithms can yield new opportunities for positive\ntransfer between tasks---showing how learning a shortest-path algorithm can be\nsubstantially improved when simultaneously learning a reachability algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 14:50:45 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 16:47:33 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Veli\u010dkovi\u0107", "Petar", ""], ["Ying", "Rex", ""], ["Padovano", "Matilde", ""], ["Hadsell", "Raia", ""], ["Blundell", "Charles", ""]]}, {"id": "1910.10631", "submitter": "Dominik Kempa", "authors": "Dominik Kempa, Tomasz Kociumaka", "title": "Resolution of the Burrows-Wheeler Transform Conjecture", "comments": "50 pages, full version of a paper accepted to FOCS 2020", "journal-ref": null, "doi": "10.1109/FOCS46700.2020.00097", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Burrows-Wheeler Transform (BWT) is an invertible text transformation that\npermutes symbols of a text according to the lexicographical order of its\nsuffixes. BWT is the main component of popular lossless compression programs\n(such as bzip2) as well as recent powerful compressed indexes (such as\n$r$-index [Gagie et al., J. ACM, 2020]), central in modern bioinformatics. The\ncompression ratio of BWT is quantified by the number $r$ of equal-letter runs.\nDespite the practical significance of BWT, no non-trivial bound on the value of\n$r$ is known. This is in contrast to nearly all other known compression\nmethods, whose sizes have been shown to be either always within a ${\\rm\npolylog}\\,n$ factor (where $n$ is the length of text) from $z$, the size of\nLempel-Ziv (LZ77) parsing of the text, or significantly larger in the worst\ncase (by a $n^{\\varepsilon}$ factor for $\\varepsilon > 0$). In this paper, we\nshow that $r = \\mathcal{O}(z \\log^2n)$ holds for every text. This result has\nnumerous implications for text indexing and data compression; for example: (1)\nit proves that many results related to BWT automatically apply to methods based\non LZ77, e.g., it is possible to obtain functionality of the suffix tree in\n$\\mathcal{O}(z\\,{\\rm polylog}\\,n)$ space; (2) it shows that many text\nprocessing tasks can be solved in the optimal time assuming the text is\ncompressible using LZ77 by a sufficiently large ${\\rm polylog}\\,n$ factor; (3)\nit implies the first non-trivial relation between the number of runs in the BWT\nof the text and its reverse. In addition, we provide an $\\mathcal{O}(z\\,{\\rm\npolylog}\\,n)$-time algorithm converting the LZ77 parsing into the run-length\ncompressed BWT. To achieve this, we develop a number of new data structures and\ntechniques of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 15:54:53 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 07:00:47 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 12:18:41 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Kempa", "Dominik", ""], ["Kociumaka", "Tomasz", ""]]}, {"id": "1910.10665", "submitter": "Daniel Vaz", "authors": "Parinya Chalermsook, Syamantak Das, Bundit Laekhanukit, Daniel Vaz", "title": "Mimicking Networks Parameterized by Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G=(V,E)$, capacities $w(e)$ on edges, and a subset of\nterminals $\\mathcal{T} \\subseteq V: |\\mathcal{T}| = k$, a mimicking network for\n$(G,\\mathcal{T})$ is a graph $(H,w')$ that contains copies of $\\mathcal{T}$ and\npreserves the value of minimum cuts separating any subset $A, B \\subseteq\n\\mathcal{T}$ of terminals. Mimicking networks of size $2^{2^k}$ are known to\nexist and can be constructed algorithmically, while the best known lower bound\nis $2^{\\Omega(k)}$; therefore, an exponential size is required if one aims at\npreserving cuts exactly. In this paper, we study mimicking networks that\npreserve connectivity of the graph exactly up to the value of $c$, where $c$ is\na parameter. This notion of mimicking network is sufficient for some\napplications, as we will elaborate. We first show that a mimicking of size $3^c\n\\cdot k$ exists, that is, we can preserve cuts with small capacity using a\nnetwork of size linear in $k$. Next, we show an algorithm that finds such a\nmimicking network in time $2^{O(c^2)} \\operatorname{poly}(m)$.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 16:52:10 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Chalermsook", "Parinya", ""], ["Das", "Syamantak", ""], ["Laekhanukit", "Bundit", ""], ["Vaz", "Daniel", ""]]}, {"id": "1910.11039", "submitter": "Alexander van der Grinten", "authors": "Alexander van der Grinten, Henning Meyerhenke", "title": "Scaling Betweenness Approximation to Billions of Edges by MPI-based\n  Adaptive Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Betweenness centrality is one of the most popular vertex centrality measures\nin network analysis. Hence, many (sequential and parallel) algorithms to\ncompute or approximate betweenness have been devised. Recent algorithmic\nadvances have made it possible to approximate betweenness very efficiently on\nshared-memory architectures. Yet, the best shared-memory algorithms can still\ntake hours of running time for large graphs, especially for graphs with a high\ndiameter or when a small relative error is required.\n  In this work, we present an MPI-based generalization of the state-of-the-art\nshared-memory algorithm for betweenness approximation. This algorithm is based\non adaptive sampling; our parallelization strategy can be applied in the same\nmanner to adaptive sampling algorithms for other problems. In experiments on a\n16-node cluster, our MPI-based implementation is by a factor of 16.1x faster\nthan the state-of-the-art shared-memory implementation when considering our\nparallelization focus -- the adaptive sampling phase -- only. For the complete\nalgorithm, we obtain an average (geom. mean) speedup factor of 7.4x over the\nstate of the art. For some previously very challenging inputs, this speedup is\nmuch higher. As a result, our algorithm is the first to approximate betweenness\ncentrality on graphs with several billion edges in less than ten minutes with\nhigh accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 11:48:28 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["van der Grinten", "Alexander", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1910.11041", "submitter": "Steven Kelk", "authors": "Mark Jones, Philippe Gambette, Leo van Iersel, Remie Janssen, Steven\n  Kelk, Fabio Pardi, Celine Scornavacca", "title": "Cutting an alignment with Ockham's razor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate different parsimony-based approaches towards\nfinding recombination breakpoints in a multiple sequence alignment. This\nrecombination detection task is crucial in order to avoid errors in\nevolutionary analyses caused by mixing together portions of sequences which had\na different evolution history. Following an overview of the field of\nrecombination detection, we formulate four computational problems for this task\nwith different objective functions. The four problems aim to minimize (1) the\ntotal homoplasy of all blocks (2) the maximum homoplasy per block (3) the total\nhomoplasy ratio of all blocks and (4) the maximum homoplasy ratio per block. We\ndescribe algorithms for each of these problems, which are fixed-parameter\ntractable (FPT) when the characters are binary. We have implemented and tested\nthe algorithms on simulated data, showing that minimizing the total homoplasy\ngives, in most cases, the most accurate results. Our implementation and\nexperimental data have been made publicly available. Finally, we also consider\nthe problem of combining blocks into non-contiguous blocks consisting of at\nmost p contiguous parts. Fixing the homoplasy h of each block to 0, we show\nthat this problem is NP-hard when p >= 3, but polynomial-time solvable for p =\n2. Furthermore, the problem is FPT with parameter h for binary characters when\np = 2. A number of interesting problems remain open.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 11:57:29 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Jones", "Mark", ""], ["Gambette", "Philippe", ""], ["van Iersel", "Leo", ""], ["Janssen", "Remie", ""], ["Kelk", "Steven", ""], ["Pardi", "Fabio", ""], ["Scornavacca", "Celine", ""]]}, {"id": "1910.11069", "submitter": "Lorenz H\\\"ubschle-Schneider", "authors": "Lorenz H\\\"ubschle-Schneider and Peter Sanders", "title": "Communication-Efficient (Weighted) Reservoir Sampling from Fully\n  Distributed Data Streams", "comments": "A previous version of this paper was titled \"Communication-Efficient\n  (Weighted) Reservoir Sampling\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider communication-efficient weighted and unweighted (uniform) random\nsampling from distributed data streams presented as a sequence of mini-batches\nof items. This is a natural model for distributed streaming computation, and\nour goal is to showcase its usefulness. We present and analyze fully\ndistributed, communication-efficient algorithms for both versions of the\nproblem. An experimental evaluation of weighted reservoir sampling on up to 256\nnodes (5120 processors) shows good speedups, while theoretical analysis\npromises further scaling to much larger machines.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 13:21:00 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 13:54:24 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 13:53:52 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["H\u00fcbschle-Schneider", "Lorenz", ""], ["Sanders", "Peter", ""]]}, {"id": "1910.11142", "submitter": "Yury Maximov", "authors": "Valerii Likhosherstov, Yury Maximov, Michael Chertkov", "title": "Tractable Minor-free Generalization of Planar Zero-field Ising Models", "comments": "32 pages. arXiv admin note: substantial text overlap with\n  arXiv:1906.06431, arXiv:1812.09587", "journal-ref": null, "doi": "10.1088/1742-5468/abcaf1", "report-no": null, "categories": "cs.DS math.ST physics.data-an stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new family of zero-field Ising models over $N$ binary\nvariables/spins obtained by consecutive \"gluing\" of planar and $O(1)$-sized\ncomponents and subsets of at most three vertices into a tree. The\npolynomial-time algorithm of the dynamic programming type for solving exact\ninference (computing partition function) and exact sampling (generating i.i.d.\nsamples) consists in a sequential application of an efficient (for planar) or\nbrute-force (for $O(1)$-sized) inference and sampling to the components as a\nblack box. To illustrate the utility of the new family of tractable graphical\nmodels, we first build a polynomial algorithm for inference and sampling of\nzero-field Ising models over $K_{3,3}$-minor-free topologies and over\n$K_{5}$-minor-free topologies -- both are extensions of the planar zero-field\nIsing models -- which are neither genus - nor treewidth-bounded. Second, we\ndemonstrate empirically an improvement in the approximation quality of the\nNP-hard problem of inference over the square-grid Ising model in a\nnode-dependent non-zero \"magnetic\" field.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 10:39:48 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Likhosherstov", "Valerii", ""], ["Maximov", "Yury", ""], ["Chertkov", "Michael", ""]]}, {"id": "1910.11529", "submitter": "Palash Dey", "authors": "Palash Dey and Sourav Medya", "title": "Manipulating Node Similarity Measures in Networks", "comments": "To appear as a full paper in AAMAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Node similarity measures quantify how similar a pair of nodes are in a\nnetwork. These similarity measures turn out to be an important fundamental tool\nfor many real world applications such as link prediction in networks,\nrecommender systems etc. An important class of similarity measures are local\nsimilarity measures. Two nodes are considered similar under local similarity\nmeasures if they have large overlap between their neighboring set of nodes.\nManipulating node similarity measures via removing edges is an important\nproblem. This type of manipulation, for example, hinders effectiveness of link\nprediction in terrorists networks. Fortunately, all the popular computational\nproblems formulated around manipulating similarity measures turn out to be\nNP-hard. We, in this paper, provide fine grained complexity results of these\nproblems through the lens of parameterized complexity. In particular, we show\nthat some of these problems are fixed parameter tractable (FPT) with respect to\nvarious natural parameters whereas other problems remain intractable W[1]-hard\nand W[2]-hard in particular). Finally we show the effectiveness of our proposed\nFPT algorithms on real world datasets as well as synthetic networks generated\nusing Barabasi-Albert and Erdos-Renyi models.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 05:16:55 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 01:47:35 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Dey", "Palash", ""], ["Medya", "Sourav", ""]]}, {"id": "1910.11564", "submitter": "Mitsuru Funakoshi", "authors": "Mitsuru Funakoshi and Julian Pape-Lange", "title": "Non-Rectangular Convolutions and (Sub-)Cadences with Three Elements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrete acyclic convolution computes the 2n-1 sums sum_{i+j=k; (i,j) in\n[0,1,2,...,n-1]^2} (a_i b_j) in O(n log n) time. By using suitable offsets and\nsetting some of the variables to zero, this method provides a tool to calculate\nall non-zero sums sum_{i+j=k; (i,j) in (P cap Z^2)} (a_i b_j) in a rectangle P\nwith perimeter p in O(p log p) time.\n  This paper extends this geometric interpretation in order to allow arbitrary\nconvex polygons P with k vertices and perimeter p. Also, this extended\nalgorithm only needs O(k + p(log p)^2 log k) time.\n  Additionally, this paper presents fast algorithms for counting sub-cadences\nand cadences with 3 elements using this extended method.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 08:17:10 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Funakoshi", "Mitsuru", ""], ["Pape-Lange", "Julian", ""]]}, {"id": "1910.11749", "submitter": "Jonathan Blanchette", "authors": "Jonathan Blanchette, Robert Lagani\\`ere", "title": "A Curious Link Between Prime Numbers, the Maundy Cake Problem and\n  Parallel Sorting", "comments": "10 pages, 2 figures, 3 tables, final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new theoretical algorithms that sums the n-ary comparators output\nin order to get the permutation indices in order to sort a sequence. By\nanalysing the parallel ranking algorithm, we found that the special comparators\nnumber of elements it processes divide the number of elements to be sorted.\nUsing the divide and conquer method, we can express the sorting problem into\nsumming output of comparators taking a prime number of elements, given that\nthis prime number divides the initial disordered sequence length. The number of\nsums is directly related to the Maundy cake problem. Furthermore, we provide a\nnew sequence that counts the number of comparators used in the algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 14:17:30 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 16:58:01 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 15:27:33 GMT"}, {"version": "v4", "created": "Mon, 4 Nov 2019 14:44:21 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Blanchette", "Jonathan", ""], ["Lagani\u00e8re", "Robert", ""]]}, {"id": "1910.11753", "submitter": "Abhiruk Lahiri", "authors": "L Sunil Chandran, Abhiruk Lahiri and Nitin Singh", "title": "Improved Approximation for Maximum Edge Colouring Problem", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The anti-Ramsey number, $ar(G, H)$ is the minimum integer $k$ such that in\nany edge colouring of $G$ with $k$ colours there is a rainbow subgraph\nisomorphic to $H$, i.e., a copy of $H$ with each of its edges assigned a\ndifferent colour. The notion was introduced by Erd{\\\"{o}}s and Simonovits in\n1973. Since then the parameter has been studied extensively in combinatorics,\nalso the particular case when $H$ is a star graph. Recently this case received\nthe attention of researchers from the algorithm community because of its\napplications in interface modelling of wireless networks. To the algorithm\ncommunity, the problem is known as maximum edge $q$-colouring problem.\n  In this paper, we study the maximum edge $2$-colouring problem from the\napproximation algorithm point of view. The case $q=2$ is particularly\ninteresting due to its application in real-life problems. Algorithmically, this\nproblem is known to be NP-hard for $q\\ge 2$. For the case of $q=2$, it is also\nknown that no polynomial-time algorithm can approximate to a factor less than\n$3/2$ assuming the unique games conjecture. Feng et al. showed a\n$2$-approximation algorithm for this problem. Later Adamaszek and Popa\npresented a $5/3$-approximation algorithm with the additional assumption that\nthe input graph has a perfect matching. Note that the obvious but the only\nknown algorithm issues different colours to the edges of a maximum matching\n(say $M$) and different colours to the connected components of $G \\setminus M$.\nIn this article, we give a new analysis of the aforementioned algorithm leading\nto an improved approximation bound for triangle-free graphs with perfect\nmatching. We also show a new lower bound when the input graph is triangle-free.\nThe contribution of the paper is a completely new, deeper and closer analysis\nof how the optimum achieves a higher number of colours than the matching based\nalgorithm, mentioned above.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 14:23:06 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Chandran", "L Sunil", ""], ["Lahiri", "Abhiruk", ""], ["Singh", "Nitin", ""]]}, {"id": "1910.11754", "submitter": "Diego Pennino", "authors": "Diego Pennino, Maurizio Pizzonia and Alessio Papi", "title": "Overlay Indexes: Efficiently Supporting Aggregate Range Queries and\n  Authenticated Data Structures in Off-the-Shelf Databases", "comments": null, "journal-ref": "IEEE Access, Volume 7, pages 175642 - 175670, year 2019", "doi": "10.1109/ACCESS.2019.2957346", "report-no": null, "categories": "cs.DB cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commercial off-the-shelf DataBase Management Systems (DBMSes) are highly\noptimized to process a wide range of queries by means of carefully designed\nindexing and query planning. However, many aggregate range queries are usually\nperformed by DBMSes using sequential scans, and certain needs, like storing\nAuthenticated Data Structures (ADS), are not supported at all. Theoretically,\nthese needs could be efficiently fulfilled adopting specific kinds of indexing,\nwhich however are normally ruled-out in DBMSes design.\n  We introduce the concept of overlay index: an index that is meant to be\nstored in a standard database, alongside regular data and managed by regular\nsoftware, to complement DBMS capabilities. We show a data structure, that we\ncall DB-tree, that realizes an overlay index to support a wide range of custom\naggregate range queries as well as ADSes, efficiently. All DB-trees operations\ncan be performed by executing a small number of queries to the DBMS, that can\nbe issued in parallel in one or two query rounds, and involves a logarithmic\namount of data. We experimentally evaluate the efficiency of DB-trees showing\nthat our approach is effective, especially if data updates are limited.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 14:24:35 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Pennino", "Diego", ""], ["Pizzonia", "Maurizio", ""], ["Papi", "Alessio", ""]]}, {"id": "1910.11782", "submitter": "Maurizio Patrignani", "authors": "Walter Didimo and Giuseppe Liotta and Giacomo Ortali and Maurizio\n  Patrignani", "title": "Optimal Orthogonal Drawings of Planar 3-Graphs in Linear Time", "comments": "40 pages, 32 figures, full version of SODA 2020 final submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A planar orthogonal drawing $\\Gamma$ of a planar graph $G$ is a geometric\nrepresentation of $G$ such that the vertices are drawn as distinct points of\nthe plane, the edges are drawn as chains of horizontal and vertical segments,\nand no two edges intersect except at their common end-points. A bend of\n$\\Gamma$ is a point of an edge where a horizontal and a vertical segment meet.\n$\\Gamma$ is bend-minimum if it has the minimum number of bends over all\npossible planar orthogonal drawings of $G$. This paper addresses a long\nstanding, widely studied, open question: Given a planar 3-graph $G$ (i.e., a\nplanar graph with vertex degree at most three), what is the best computational\nupper bound to compute a bend-minimum planar orthogonal drawing of $G$ in the\nvariable embedding setting? In this setting the algorithm can choose among the\nexponentially many planar embeddings of $G$ the one that leads to an orthogonal\ndrawing with the minimum number of bends. We answer the question by describing\nan $O(n)$-time algorithm that computes a bend-minimum planar orthogonal drawing\nof $G$ with at most one bend per edge, where $n$ is the number of vertices of\n$G$. The existence of an orthogonal drawing algorithm that simultaneously\nminimizes the total number of bends and the number of bends per edge was\npreviously unknown.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 15:08:35 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Didimo", "Walter", ""], ["Liotta", "Giuseppe", ""], ["Ortali", "Giacomo", ""], ["Patrignani", "Maurizio", ""]]}, {"id": "1910.11850", "submitter": "Pasin Manurangsi", "authors": "Pasin Manurangsi", "title": "Tight Running Time Lower Bounds for Strong Inapproximability of Maximum\n  $k$-Coverage, Unique Set Cover and Related Problems (via $t$-Wise Agreement\n  Testing Theorem)", "comments": "To appear in SODA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show, assuming the (randomized) Gap Exponential Time Hypothesis (Gap-ETH),\nthat the following tasks cannot be done in $T(k) \\cdot N^{o(k)}$-time for any\nfunction $T$ where $N$ denote the input size:\n  - $\\left(1 - \\frac{1}{e} + \\epsilon\\right)$-approximation for Max\n$k$-Coverage for any $\\epsilon > 0$,\n  - $\\left(1 + \\frac{2}{e} - \\epsilon\\right)$-approximation for $k$-Median (in\ngeneral metrics) for any constant $\\epsilon > 0$.\n  - $\\left(1 + \\frac{8}{e} - \\epsilon\\right)$-approximation for $k$-Mean (in\ngeneral metrics) for any constant $\\epsilon > 0$.\n  - Any constant factor approximation for $k$-Unique Set Cover, $k$-Nearest\nCodeword Problem and $k$-Closest Vector Problem.\n  - $(1 + \\delta)$-approximation for $k$-Minimum Distance Problem and\n$k$-Shortest Vector Problem for some $\\delta > 0$.\n  Since these problems can be trivially solved in $N^{O(k)}$ time, our running\ntime lower bounds are essentially tight. In terms of approximation ratios, Max\n$k$-Coverage is well-known to admit polynomial-time $\\left(1 -\n\\frac{1}{e}\\right)$-approximation algorithms, and, recently, it was shown that\n$k$-Median and $k$-Mean are approximable to within factors of $\\left(1 +\n\\frac{2}{e}\\right)$ and $\\left(1 + \\frac{8}{e}\\right)$ respectively in FPT time\n[Cohen-Addad et al. 2019]; hence, our inapproximability ratios are also tight\nfor these three problems. For the remaining problems, no non-trivial FPT\napproximation algorithms are known.\n  The starting point of all our hardness results mentioned above is the Label\nCover problem (with projection constraints). We show that Label Cover cannot be\napproximated to within any constant factor in $T(k) \\cdot N^{o(k)}$ time, where\n$N$ and $k$ denote the size of the input and the number of nodes on the side\nwith the larger alphabet respectively. With this hardness, the above results\nfollow immediately from known reductions...\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 17:10:48 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Manurangsi", "Pasin", ""]]}, {"id": "1910.11921", "submitter": "Cyrus Rashtchian", "authors": "Sivaramakrishnan Natarajan Ramamoorthy, Cyrus Rashtchian", "title": "Equivalence of Systematic Linear Data Structures and Matrix Rigidity", "comments": "23 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Dvir, Golovnev, and Weinstein have shown that sufficiently strong\nlower bounds for linear data structures would imply new bounds for rigid\nmatrices. However, their result utilizes an algorithm that requires an $NP$\noracle, and hence, the rigid matrices are not explicit. In this work, we derive\nan equivalence between rigidity and the systematic linear model of data\nstructures. For the $n$-dimensional inner product problem with $m$ queries, we\nprove that lower bounds on the query time imply rigidity lower bounds for the\nquery set itself. In particular, an explicit lower bound of\n$\\omega\\left(\\frac{n}{r}\\log m\\right)$ for $r$ redundant storage bits would\nyield better rigidity parameters than the best bounds due to Alon, Panigrahy,\nand Yekhanin. We also prove a converse result, showing that rigid matrices\ndirectly correspond to hard query sets for the systematic linear model. As an\napplication, we prove that the set of vectors obtained from rank one binary\nmatrices is rigid with parameters matching the known results for explicit sets.\nThis implies that the vector-matrix-vector problem requires query time\n$\\Omega(n^{3/2}/r)$ for redundancy $r \\geq \\sqrt{n}$ in the systematic linear\nmodel, improving a result of Chakraborty, Kamma, and Larsen. Finally, we prove\na cell probe lower bound for the vector-matrix-vector problem in the high error\nregime, improving a result of Chattopadhyay, Kouck\\'{y}, Loff, and\nMukhopadhyay.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 20:14:00 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Ramamoorthy", "Sivaramakrishnan Natarajan", ""], ["Rashtchian", "Cyrus", ""]]}, {"id": "1910.11957", "submitter": "Jan van den Brand", "authors": "Jan van den Brand", "title": "A Deterministic Linear Program Solver in Current Matrix Multiplication\n  Time", "comments": "Appeared in SODA'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interior point algorithms for solving linear programs have been studied\nextensively for a long time [e.g. Karmarkar 1984; Lee, Sidford FOCS'14; Cohen,\nLee, Song STOC'19]. For linear programs of the form $\\min_{Ax=b, x \\ge 0}\nc^\\top x$ with $n$ variables and $d$ constraints, the generic case $d =\n\\Omega(n)$ has recently been settled by Cohen, Lee and Song [STOC'19]. Their\nalgorithm can solve linear programs in $\\tilde O(n^\\omega \\log(n/\\delta))$\nexpected time, where $\\delta$ is the relative accuracy. This is essentially\noptimal as all known linear system solvers require up to $O(n^{\\omega})$ time\nfor solving $Ax = b$. However, for the case of deterministic solvers, the best\nupper bound is Vaidya's 30 years old $O(n^{2.5} \\log(n/\\delta))$ bound\n[FOCS'89]. In this paper we show that one can also settle the deterministic\nsetting by derandomizing Cohen et al.'s $\\tilde{O}(n^\\omega \\log(n/\\delta))$\ntime algorithm. This allows for a strict $\\tilde{O}(n^\\omega \\log(n/\\delta))$\ntime bound, instead of an expected one, and a simplified analysis, reducing the\nlength of their proof of their central path method by roughly half.\nDerandomizing this algorithm was also an open question asked in Song's PhD\nThesis.\n  The main tool to achieve our result is a new data-structure that can maintain\nthe solution to a linear system in subquadratic time. More accurately we are\nable to maintain $\\sqrt{U}A^\\top(AUA^\\top)^{-1}A\\sqrt{U}\\:v$ in subquadratic\ntime under $\\ell_2$ multiplicative changes to the diagonal matrix $U$ and the\nvector $v$. This type of change is common for interior point algorithms.\nPrevious algorithms [e.g. Vaidya STOC'89; Lee, Sidford FOCS'15; Cohen, Lee,\nSong STOC'19] required $\\Omega(n^2)$ time for this task. [...]\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 23:02:01 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 14:26:10 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Brand", "Jan van den", ""]]}, {"id": "1910.11993", "submitter": "Oliver Serang", "authors": "Patrick Kreitzberg, Kyle Lucke, Oliver Serang", "title": "Selection on $X_1+X_2+\\cdots + X_m$ with layer-ordered heaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selection on $X_1+X_2+\\cdots + X_m$ is an important problem with many\napplications in areas such as max-convolution, max-product Bayesian inference,\ncalculating most probable isotopes, and computing non-parametric test\nstatistics, among others. Faster-than-na\\\"{i}ve approaches exist for $m=2$:\nFrederickson (1993) published the optimal algorithm with runtime $O(k)$ and\nKaplan \\emph{et al.} (2018) has since published a much simpler algorithm which\nmakes use of Chazelle's soft heaps (2003). No fast methods exist for $m>2$.\nJohnson \\& Mizoguchi (1978) introduced a method to compute the single $k^{th}$\nvalue when $m>2$, but that method runs in $O(m\\cdot n^{\\lceil\\frac{m}{2}\\rceil}\n\\log(n))$ time and is inefficient when $m \\gg 1$ and $k \\ll\nn^{\\lceil\\frac{m}{2}\\rceil}$.\n  In this paper, we introduce the first efficient methods, both in theory and\npractice, for problems with $m>2$. We introduce the ``layer-ordered heap,'' a\nsimple special class of heap with which we produce a new, fast selection\nalgorithm on the Cartesian product. Using this new algorithm to perform\n$k$-selection on the Cartesian product of $m$ arrays of length $n$ has runtime\n$\\in o(k\\cdot m)$. We also provide implementations of the algorithms proposed\nand evaluate their performance in practice.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 04:17:37 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 01:16:24 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Kreitzberg", "Patrick", ""], ["Lucke", "Kyle", ""], ["Serang", "Oliver", ""]]}, {"id": "1910.12050", "submitter": "Yunus Esencayi", "authors": "Yunus Esencayi (SUNY at Buffalo), Marco Gaboardi (Boston University),\n  Shi Li (SUNY at Buffalo), Di Wang (SUNY at Buffalo)", "title": "Facility Location Problem in Differential Privacy Model Revisited", "comments": "Accepted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the uncapacitated facility location problem in the\nmodel of differential privacy (DP) with uniform facility cost. Specifically, we\nfirst show that, under the hierarchically well-separated tree (HST) metrics and\nthe super-set output setting that was introduced in Gupta et. al., there is an\n$\\epsilon$-DP algorithm that achieves an $O(\\frac{1}{\\epsilon})$(expected\nmultiplicative) approximation ratio; this implies an $O(\\frac{\\log\nn}{\\epsilon})$ approximation ratio for the general metric case, where $n$ is\nthe size of the input metric. These bounds improve the best-known results given\nby Gupta et. al. In particular, our approximation ratio for HST-metrics is\nindependent of $n$, and the ratio for general metrics is independent of the\naspect ratio of the input metric. On the negative side, we show that the\napproximation ratio of any $\\epsilon$-DP algorithm is lower bounded by\n$\\Omega(\\frac{1}{\\sqrt{\\epsilon}})$, even for instances on HST metrics with\nuniform facility cost, under the super-set output setting. The lower bound\nshows that the dependence of the approximation ratio for HST metrics on\n$\\epsilon$ can not be removed or greatly improved. Our novel methods and\ntechniques for both the upper and lower bound may find additional applications.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 11:29:29 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Esencayi", "Yunus", "", "SUNY at Buffalo"], ["Gaboardi", "Marco", "", "Boston University"], ["Li", "Shi", "", "SUNY at Buffalo"], ["Wang", "Di", "", "SUNY at Buffalo"]]}, {"id": "1910.12172", "submitter": "Dhruv Rohatgi", "authors": "Dhruv Rohatgi", "title": "Near-Optimal Bounds for Online Caching with Machine Learned Advice", "comments": "18 pages; accepted to SODA'20; added references and acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the model of online caching with machine learned advice, introduced by\nLykouris and Vassilvitskii, the goal is to solve the caching problem with an\nonline algorithm that has access to next-arrival predictions: when each input\nelement arrives, the algorithm is given a prediction of the next time when the\nelement will reappear. The traditional model for online caching suffers from an\n$\\Omega(\\log k)$ competitive ratio lower bound (on a cache of size $k$). In\ncontrast, the augmented model admits algorithms which beat this lower bound\nwhen the predictions have low error, and asymptotically match the lower bound\nwhen the predictions have high error, even if the algorithms are oblivious to\nthe prediction error. In particular, Lykouris and Vassilvitskii showed that\nthere is a prediction-augmented caching algorithm with a competitive ratio of\n$O(1+\\min(\\sqrt{\\eta/OPT}, \\log k))$ when the overall $\\ell_1$ prediction error\nis bounded by $\\eta$, and $OPT$ is the cost of the optimal offline algorithm.\n  The dependence on $k$ in the competitive ratio is optimal, but the dependence\non $\\eta/OPT$ may be far from optimal. In this work, we make progress towards\nclosing this gap. Our contributions are twofold. First, we provide an improved\nalgorithm with a competitive ratio of $O(1 + \\min((\\eta/OPT)/k, 1) \\log k)$.\nSecond, we provide a lower bound of $\\Omega(\\log \\min((\\eta/OPT)/(k \\log k),\nk))$.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 03:38:16 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 20:11:34 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Rohatgi", "Dhruv", ""]]}, {"id": "1910.12310", "submitter": "Laxman Dhulipala", "authors": "Laxman Dhulipala, Charlie McGuffey, Hongbo Kang, Yan Gu, Guy E.\n  Blelloch, Phillip B. Gibbons, Julian Shun", "title": "Sage: Parallel Semi-Asymmetric Graph Algorithms for NVRAMs", "comments": "This is an extended version of a paper in PVLDB (to be presented at\n  VLDB'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile main memory (NVRAM) technologies provide an attractive set of\nfeatures for large-scale graph analytics, including byte-addressability, low\nidle power, and improved memory-density. NVRAM systems today have an order of\nmagnitude more NVRAM than traditional memory (DRAM). NVRAM systems could\ntherefore potentially allow very large graph problems to be solved on a single\nmachine, at a modest cost. However, a significant challenge in achieving high\nperformance is in accounting for the fact that NVRAM writes can be much more\nexpensive than NVRAM reads.\n  In this paper, we propose an approach to parallel graph analytics using the\nParallel Semi-Asymmetric Model (PSAM), in which the graph is stored as a\nread-only data structure (in NVRAM), and the amount of mutable memory is kept\nproportional to the number of vertices. Similar to the popular semi-external\nand semi-streaming models for graph analytics, the PSAM approach assumes that\nthe vertices of the graph fit in a fast read-write memory (DRAM), but the edges\ndo not. In NVRAM systems, our approach eliminates writes to the NVRAM, among\nother benefits.\n  To experimentally study this new setting, we develop Sage, a parallel\nsemi-asymmetric graph engine with which we implement provably-efficient (and\noften work-optimal) PSAM algorithms for over a dozen fundamental graph\nproblems. We experimentally study Sage using a 48-core machine on the largest\npublicly-available real-world graph (the Hyperlink Web graph with over 3.5\nbillion vertices and 128 billion edges) equipped with Optane DC Persistent\nMemory, and show that Sage outperforms the fastest prior systems designed for\nNVRAM. Importantly, we also show that Sage nearly matches the fastest prior\nsystems running solely in DRAM, by effectively hiding the costs of repeatedly\naccessing NVRAM versus DRAM.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 17:46:20 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 02:34:14 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Dhulipala", "Laxman", ""], ["McGuffey", "Charlie", ""], ["Kang", "Hongbo", ""], ["Gu", "Yan", ""], ["Blelloch", "Guy E.", ""], ["Gibbons", "Phillip B.", ""], ["Shun", "Julian", ""]]}, {"id": "1910.12340", "submitter": "William Kuszmaul", "authors": "Tim Kaler, William Kuszmaul, Tao B. Schardl, Daniele Vettorel", "title": "Cilkmem: Algorithms for Analyzing the Memory High-Water Mark of\n  Fork-Join Parallel Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software engineers designing recursive fork-join programs destined to run on\nmassively parallel computing systems must be cognizant of how their program's\nmemory requirements scale in a many-processor execution. Although tools exist\nfor measuring memory usage during one particular execution of a parallel\nprogram, such tools cannot bound the worst-case memory usage over all possible\nparallel executions.\n  This paper introduces Cilkmem, a tool that analyzes the execution of a\ndeterministic Cilk program to determine its $p$-processor memory high-water\nmark (MHWM), which is the worst-case memory usage of the program over \\emph{all\npossible} $p$-processor executions. Cilkmem employs two new algorithms for\ncomputing the $p$-processor MHWM. The first algorithm calculates the exact\n$p$-processor MHWM in $O(T_1 \\cdot p)$ time, where $T_1$ is the total work of\nthe program. The second algorithm solves, in $O(T_1)$ time, the approximate\nthreshold problem, which asks, for a given memory threshold $M$, whether the\n$p$-processor MHWM exceeds $M/2$ or whether it is guaranteed to be less than\n$M$. Both algorithms are memory efficient, requiring $O(p \\cdot D)$ and $O(D)$\nspace, respectively, where $D$ is the maximum call-stack depth of the program's\nexecution on a single thread.\n  Our empirical studies show that Cilkmem generally exhibits low overheads.\nAcross ten application benchmarks from the Cilkbench suite, the exact algorithm\nincurs a geometric-mean multiplicative overhead of $1.54$ for $p=128$, whereas\nthe approximation-threshold algorithm incurs an overhead of $1.36$ independent\nof $p$. In addition, we use Cilkmem to reveal and diagnose a previously unknown\nissue in a large image-alignment program contributing to unexpectedly high\nmemory usage under parallel executions.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 20:22:26 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Kaler", "Tim", ""], ["Kuszmaul", "William", ""], ["Schardl", "Tao B.", ""], ["Vettorel", "Daniele", ""]]}, {"id": "1910.12375", "submitter": "Michael Walter", "authors": "Peter B\\\"urgisser and Cole Franks and Ankit Garg and Rafael Oliveira\n  and Michael Walter and Avi Wigderson", "title": "Towards a theory of non-commutative optimization: geodesic first and\n  second order methods for moment maps and polytopes", "comments": "108 pages, long version of paper presented at FOCS 2019. v3: various\n  typos fixed", "journal-ref": "60th IEEE Symposium on Foundations of Computer Science (FOCS'19),\n  pp. 845-861, 2019", "doi": "10.1109/FOCS.2019.00055", "report-no": null, "categories": "math.OC cs.DS math-ph math.AG math.MP math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper initiates a systematic development of a theory of non-commutative\noptimization. It aims to unify and generalize a growing body of work from the\npast few years which developed and analyzed algorithms for natural geodesically\nconvex optimization problems on Riemannian manifolds that arise from the\nsymmetries of non-commutative groups. These algorithms minimize the moment map\n(a non-commutative notion of the usual gradient) and test membership in null\ncones and moment polytopes (a vast class of polytopes, typically of exponential\nvertex and facet complexity, which arise from this a priori non-convex,\nnon-linear setting). This setting captures a diverse set of problems in\ndifferent areas of computer science, mathematics, and physics. Several of them\nwere solved efficiently for the first time using non-commutative methods; the\ncorresponding algorithms also lead to solutions of purely structural problems\nand to many new connections between disparate fields.\n  In the spirit of standard convex optimization, we develop two general methods\nin the geodesic setting, a first order and a second order method, which\nrespectively receive first and second order information on the \"derivatives\" of\nthe function to be optimized. These in particular subsume all past results. The\nmain technical work goes into identifying the key parameters of the underlying\ngroup actions which control convergence to the optimum in each of these\nmethods. These non-commutative analogues of \"smoothness\" are far more complex\nand require significant algebraic and analytic machinery. Despite this\ncomplexity, the way in which these parameters control convergence in both\nmethods is quite simple and elegant. We show how to bound these parameters and\nhence obtain efficient algorithms for null cone membership in several concrete\nsituations. Our work points to intriguing open problems and suggests further\nresearch directions.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 23:07:15 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 15:05:22 GMT"}, {"version": "v3", "created": "Mon, 26 Jul 2021 18:50:08 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["B\u00fcrgisser", "Peter", ""], ["Franks", "Cole", ""], ["Garg", "Ankit", ""], ["Oliveira", "Rafael", ""], ["Walter", "Michael", ""], ["Wigderson", "Avi", ""]]}, {"id": "1910.12414", "submitter": "Lin Chen", "authors": "Lin Chen, Hossein Esfandiari, Thomas Fu, Vahab S. Mirrokni", "title": "Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss\n  and Beyond", "comments": "Accepted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing approximate nearest neighbors in high dimensional spaces is a\ncentral problem in large-scale data mining with a wide range of applications in\nmachine learning and data science. A popular and effective technique in\ncomputing nearest neighbors approximately is the locality-sensitive hashing\n(LSH) scheme. In this paper, we aim to develop LSH schemes for distance\nfunctions that measure the distance between two probability distributions,\nparticularly for f-divergences as well as a generalization to capture mutual\ninformation loss. First, we provide a general framework to design LHS schemes\nfor f-divergence distance functions and develop LSH schemes for the generalized\nJensen-Shannon divergence and triangular discrimination in this framework. We\nshow a two-sided approximation result for approximation of the generalized\nJensen-Shannon divergence by the Hellinger distance, which may be of\nindependent interest. Next, we show a general method of reducing the problem of\ndesigning an LSH scheme for a Krein kernel (which can be expressed as the\ndifference of two positive definite kernels) to the problem of maximum inner\nproduct search. We exemplify this method by applying it to the mutual\ninformation loss, due to its several important applications such as model\ncompression.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 03:07:29 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Chen", "Lin", ""], ["Esfandiari", "Hossein", ""], ["Fu", "Thomas", ""], ["Mirrokni", "Vahab S.", ""]]}, {"id": "1910.12490", "submitter": "Wasim Huleihel", "authors": "Wasim Huleihel, Arya Mazumdar, Muriel M\\'edard, and Soumyabrata Pal", "title": "Same-Cluster Querying for Overlapping Clusters", "comments": "43 pages, accepted at NeurIPS'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overlapping clusters are common in models of many practical data-segmentation\napplications. Suppose we are given $n$ elements to be clustered into $k$\npossibly overlapping clusters, and an oracle that can interactively answer\nqueries of the form \"do elements $u$ and $v$ belong to the same cluster?\" The\ngoal is to recover the clusters with minimum number of such queries. This\nproblem has been of recent interest for the case of disjoint clusters. In this\npaper, we look at the more practical scenario of overlapping clusters, and\nprovide upper bounds (with algorithms) on the sufficient number of queries. We\nprovide algorithmic results under both arbitrary (worst-case) and statistical\nmodeling assumptions. Our algorithms are parameter free, efficient, and work in\nthe presence of random noise. We also derive information-theoretic lower bounds\non the number of queries needed, proving that our algorithms are order optimal.\nFinally, we test our algorithms over both synthetic and real-world data,\nshowing their practicality and effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 08:06:20 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Huleihel", "Wasim", ""], ["Mazumdar", "Arya", ""], ["M\u00e9dard", "Muriel", ""], ["Pal", "Soumyabrata", ""]]}, {"id": "1910.12526", "submitter": "Tim Zeitz", "authors": "Ben Strasser, Tim Zeitz", "title": "A Fast and Tight Heuristic for A* in Road Networks", "comments": "16 pages, 3 figures; v2: Camera ready for SEA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study exact, efficient and practical algorithms for route planning in\nlarge road networks. Routing applications often require integrating the current\ntraffic situation, planning ahead with traffic predictions for the future,\nrespecting forbidden turns, and many other features depending on the exact\napplication. While Dijkstra's algorithm can be used to solve these problems, it\nis too slow for many applications. A* is a classical approach to accelerate\nDijkstra's algorithm. A* can support many extended scenarios without much\nadditional implementation complexity. However, A*'s performance depends on the\navailability of a good heuristic that estimates distances. Computing tight\ndistance estimates is a challenge on its own. On road networks, shortest paths\ncan also be quickly computed using hierarchical speedup techniques. They\nachieve speed and exactness but sacrifice A*'s flexibility. Extending them to\ncertain practical applications can be hard. In this paper, we present an\nalgorithm to efficiently extract distance estimates for A* from Contraction\nHierarchies (CH), a hierarchical technique. We call our heuristic\nCH-Potentials. Our approach allows decoupling the supported extensions from the\nhierarchical speed-up technique. Additionally, we describe A* optimizations to\naccelerate the processing of low degree nodes, which often occur in road\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 10:00:18 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 08:04:23 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Strasser", "Ben", ""], ["Zeitz", "Tim", ""]]}, {"id": "1910.12726", "submitter": "Tim Zeitz", "authors": "Ben Strasser, Dorothea Wagner, Tim Zeitz", "title": "Space-Efficient, Fast and Exact Routing in Time-Dependent Road Networks", "comments": "32 pages, 11 figures, published in MDPI Algorithms and a short\n  version at ESA 2020", "journal-ref": null, "doi": "10.3390/a14030090", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of quickly computing point-to-point shortest paths in\nmassive road networks with traffic predictions. Incorporating traffic\npredictions into routing allows, for example, to avoid commuter traffic\ncongestions. Existing techniques follow a two-phase approach: In a\npreprocessing step, an index is built. The index depends on the road network\nand the traffic patterns but not on the path start and end. The latter are the\ninput of the query phase, in which shortest paths are computed. All existing\ntechniques have large index size, slow query running times or may compute\nsuboptimal paths. In this work, we introduce CATCHUp (Customizable Approximated\nTime-dependent Contraction Hierarchies through Unpacking), the first algorithm\nthat simultaneously achieves all three objectives.The core idea of CATCHUp is\nto store paths instead of travel times at shortcuts. Shortcut travel times are\nderived lazily from the stored paths. We perform an experimental study on a set\nof real world instances and compare our approach with state-of-the-art\ntechniques. Our approach achieves the fastest preprocessing, competitive query\nrunning times and up to 38 times smaller indexes than competing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 14:48:42 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 10:39:01 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Strasser", "Ben", ""], ["Wagner", "Dorothea", ""], ["Zeitz", "Tim", ""]]}, {"id": "1910.12747", "submitter": "Laurent Feuilloley", "authors": "Laurent Feuilloley", "title": "Introduction to local certification", "comments": "Last update: minor editing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed graph algorithm is basically an algorithm where every node of a\ngraph can look at its neighborhood at some distance in the graph and chose its\noutput. As distributed environment are subject to faults, an important issue is\nto be able to check that the output is correct, or in general that the network\nis in proper configuration with respect to some predicate. One would like this\nchecking to be very local, to avoid using too much resources. Unfortunately\nmost predicates cannot be checked this way, and that is where certification\ncomes into play. Local certification (also known as proof-labeling schemes,\nlocally checkable proofs or distributed verification) consists in assigning\nlabels to the nodes, that certify that the configuration is correct. There are\nseveral point of view on this topic: it can be seen as a part of\nself-stabilizing algorithms, as labeling problem, or as a non-deterministic\ndistributed decision.\n  This paper is an introduction to the domain of local certification, giving an\noverview of the history, the techniques and the current research directions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 15:16:15 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 15:30:14 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 16:19:39 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 17:08:12 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Feuilloley", "Laurent", ""]]}, {"id": "1910.12848", "submitter": "Zeev Nutov", "authors": "Guy Kortsarz and Zeev Nutov", "title": "Bounded Degree Group Steiner Tree Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two problems that seek a subtree $T$ of a graph $G=(V,E)$ such that\n$T$ satisfies a certain property and has minimal maximum degree.\n  - In the Min-Degree Group Steiner Tree problem we are given a collection\n${\\cal S}$ of groups (subsets of $V$) and $T$ should contain a node from every\ngroup.\n  - In the Min-Degree Steiner $k$-Tree problem we are given a set $R$ of\nterminals and an integer $k$, and $T$ should contain at least $k$ terminals.\n  We show that if the former problem admits approximation ratio $\\rho$ then the\nlater problem admits approximation ratio $\\rho \\cdot O(\\log k)$. For bounded\ntreewidth graphs, we obtain approximation ratio $O(\\log^3 n)$ for Min-Degree\nGroup Steiner Tree.\n  In the more general Bounded Degree Group Steiner Tree problem we are also\ngiven edge costs and degree bounds $\\{b(v):v \\in V\\}$, and $T$ should obey the\ndegree constraints $deg_T(v) \\leq b(v)$ for all $v \\in V$. We give a bicriteria\n$(O(\\log N \\log |{\\cal S}|),O(\\log^2 n))$-approximation algorithm for this\nproblem on tree inputs, where $N$ is the size of the largest group,\ngeneralizing the approximation of Garg, Konjevod, and Ravi for the case without\ndegree bounds.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:55:16 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Kortsarz", "Guy", ""], ["Nutov", "Zeev", ""]]}, {"id": "1910.13011", "submitter": "Pedro Ribeiro", "authors": "Pedro Ribeiro, Pedro Paredes, Miguel E.P. Silva, David Aparicio,\n  Fernando Silva", "title": "A Survey on Subgraph Counting: Concepts, Algorithms and Applications to\n  Network Motifs and Graphlets", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing subgraph frequencies is a fundamental task that lies at the core of\nseveral network analysis methodologies, such as network motifs and\ngraphlet-based metrics, which have been widely used to categorize and compare\nnetworks from multiple domains. Counting subgraphs is however computationally\nvery expensive and there has been a large body of work on efficient algorithms\nand strategies to make subgraph counting feasible for larger subgraphs and\nnetworks.\n  This survey aims precisely to provide a comprehensive overview of the\nexisting methods for subgraph counting. Our main contribution is a general and\nstructured review of existing algorithms, classifying them on a set of key\ncharacteristics, highlighting their main similarities and differences. We\nidentify and describe the main conceptual approaches, giving insight on their\nadvantages and limitations, and provide pointers to existing implementations.\nWe initially focus on exact sequential algorithms, but we also do a thorough\nsurvey on approximate methodologies (with a trade-off between accuracy and\nexecution time) and parallel strategies (that need to deal with an unbalanced\nsearch space).\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 00:01:24 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Ribeiro", "Pedro", ""], ["Paredes", "Pedro", ""], ["Silva", "Miguel E. P.", ""], ["Aparicio", "David", ""], ["Silva", "Fernando", ""]]}, {"id": "1910.13123", "submitter": "Marc Hellmuth", "authors": "Manuel Lafond and Marc Hellmuth", "title": "Reconstruction of time-consistent species trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The history of gene families -- which are equivalent to event-labeled gene\ntrees -- can to some extent be reconstructed from empirically estimated\nevolutionary event-relations containing pairs of orthologous, paralogous or\nxenologous genes. The question then arises as whether inferred event-labeled\ngene trees are \"biologically feasible\" which is the case if one can find a\nspecies tree with which the gene tree can be reconciled in a time-consistent\nway.\n  In this contribution, we consider event-labeled gene trees that contain\nspeciation, duplication as well as horizontal gene transfer and we assume that\nthe species tree is unknown. We provide a cubic-time algorithm to decide\nwhether a \"time-consistent\" binary species for a given event-labeled gene tree\nexists and, in the affirmative case, to construct the species tree within the\nsame time-complexity.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 07:50:53 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Lafond", "Manuel", ""], ["Hellmuth", "Marc", ""]]}, {"id": "1910.13292", "submitter": "Luis Miralles PHD", "authors": "Luis Miralles, M. Atif Qureshi, Brian Mac Namee", "title": "Real-time Bidding campaigns optimization using attribute selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-Time Bidding is nowadays one of the most promising systems in the online\nadvertising ecosystem. In the presented study, the performance of RTB campaigns\nis improved by optimising the parameters of the users' profiles and the\npublishers' websites. Most studies about optimising RTB campaigns are focused\non the bidding strategy. In contrast, the objective of our research consists of\noptimising RTB campaigns by finding out configurations that maximise both the\nnumber of impressions and their average profitability. The experiments\ndemonstrate that, when the number of required visits by advertisers is low, it\nis easy to find configurations with high average profitability, but as the\nrequired number of visits increases, the average profitability tends to go\ndown. Additionally, configuration optimisation has been combined with other\ninteresting strategies to increase, even more, the campaigns' profitability.\nAlong with parameter configuration the study considers the following\ncomplementary strategies to increase profitability: i) selecting multiple\nconfigurations with a small number of visits instead of a unique configuration\nwith a large number, ii) discarding visits according to the thresholds of cost\nand profitability, iii) analysing a reduced space of the dataset and\nextrapolating the solution, and iv) increasing the search space by including\nsolutions below the required number of visits. The developed campaign\noptimisation methodology could be offered by RTB platforms to advertisers to\nmake their campaigns more profitable.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 14:30:15 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Miralles", "Luis", ""], ["Qureshi", "M. Atif", ""], ["Mac Namee", "Brian", ""]]}, {"id": "1910.13297", "submitter": "Felix Hommelsheim", "authors": "David Adjiashvili, Felix Hommelsheim and Moritz M\\\"uhlenthaler", "title": "Flexible Graph Connectivity: Approximating Network Design Problems\n  Between 1- and 2-connectivity", "comments": "Improved presentation of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph connectivity and network design problems are among the most fundamental\nproblems in combinatorial optimization. The minimum spanning tree problem, the\ntwo edge-connected spanning subgraph problem (2-ECSS) and the tree augmentation\nproblem (TAP) are all examples of fundamental well-studied network design tasks\nthat postulate different initial states of the network and different\nassumptions on the reliability of network components. In this paper we motivate\nand study \\emph{Flexible Graph Connectivity} (FGC), a problem that mixes\ntogether both the modeling power and the complexities of all aforementioned\nproblems and more. In a nutshell, FGC asks to design a connected network, while\nallowing to specify different reliability levels for individual edges. While\nthis non-uniform nature of the problem makes it appealing from the modeling\nperspective, it also renders most existing algorithmic tools for dealing with\nnetwork design problems unfit for approximating FGC.\n  In this paper we develop a general algorithmic approach for approximating FGC\nthat yields approximation algorithms with ratios that are very close to the\nbest known bounds for many special cases, such as 2-ECSS and TAP. Our algorithm\nand analysis combine various techniques including a weight-scaling algorithm, a\ncharging argument that uses a variant of exchange bijections between spanning\ntrees and a factor revealing min-max-min optimization problem.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 14:38:22 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 12:06:07 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Adjiashvili", "David", ""], ["Hommelsheim", "Felix", ""], ["M\u00fchlenthaler", "Moritz", ""]]}, {"id": "1910.13367", "submitter": "Caleb Ju", "authors": "Caleb Ju and Edgar Solomonik", "title": "Derivation and Analysis of Fast Bilinear Algorithms for Convolution", "comments": "34 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DM cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of convolution in applications within signal processing, deep\nneural networks, and numerical solvers has motivated the development of\nnumerous fast convolution algorithms. In many of these problems, convolution is\nperformed on terabytes or petabytes of data, so even constant factors of\nimprovement can significantly reduce the computation time. We leverage the\nformalism of bilinear algorithms to describe and analyze all of the most\npopular approaches. This unified lens permits us to study the relationship\nbetween different variants of convolution as well as to derive error bounds and\nanalyze the cost of the various algorithms. We provide new derivations, which\npredominantly leverage matrix and tensor algebra, to describe the Winograd\nfamily of convolution algorithms as well as reductions between 1D and\nmultidimensional convolution. We provide cost and error bounds as well as\nexperimental numerical studies. Our experiments for two of these algorithms,\nthe overlap-add approach and Winograd convolution algorithm with polynomials of\ndegree greater than one, show that fast convolution algorithms can rival the\naccuracy of the fast Fourier transform (FFT) without using complex arithmetic.\nThese algorithms can be used for convolution problems with multidimensional\ninputs or for filters larger than size of four, extending the state-of-the-art\nin Winograd-based convolution algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 16:32:31 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 14:37:58 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Ju", "Caleb", ""], ["Solomonik", "Edgar", ""]]}, {"id": "1910.13386", "submitter": "Changyong Hu", "authors": "Changyong Hu, Vijay K. Garg", "title": "NC Algorithms for Popular Matchings in One-Sided Preference Systems and\n  Related Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popular matching problem is of matching a set of applicants to a set of\nposts, where each applicant has a preference list, ranking a non-empty subset\nof posts in the order of preference, possibly with ties. A matching M is\npopular if there is no other matching M' such that more applicants prefer M' to\nM. We give the first NC algorithm to solve the popular matching problem without\nties. We also give an NC algorithm that solves the maximum-cardinality popular\nmatching problem. No NC or RNC algorithms were known for the matching problem\nin preference systems prior to this work. Moreover, we give an NC algorithm for\na weaker version of the stable matching problem, that is, the problem of\nfinding the \"next\" stable matching given a stable matching.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 16:07:40 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 18:45:37 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Hu", "Changyong", ""], ["Garg", "Vijay K.", ""]]}, {"id": "1910.13479", "submitter": "Isamu Furuya", "authors": "Isamu Furuya", "title": "Practical Repetition-Aware Grammar Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of grammar compression is to construct a small sized context free\ngrammar which uniquely generates the input text data. Among grammar compression\nmethods, RePair is known for its good practical compression performance.\nMR-RePair was recently proposed as an improvement to RePair for constructing\nsmall-sized context free grammar for repetitive text data. However, a compact\nencoding scheme has not been discussed for MR-RePair. We propose a practical\nencoding method for MR-RePair and show its effectiveness through comparative\nexperiments. Moreover, we extend MR-RePair to run-length context free grammar\nand design a novel variant for it called RL-MR-RePair. We experimentally\ndemonstrate that a compression scheme consisting of RL-MR-RePair and the\nproposed encoding method show good performance on real repetitive datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 18:58:48 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Furuya", "Isamu", ""]]}, {"id": "1910.13533", "submitter": "William Kuszmaul", "authors": "William Kuszmaul", "title": "Achieving Optimal Backlog in the Vanilla Multi-Processor Cup Game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In each step of the $p$-processor cup game on $n$ cups, a filler distributes\nup to $p$ units of water among the cups, subject only to the constraint that no\ncup receives more than $1$ unit of water; an emptier then removes up to $1$\nunit of water from each of $p$ cups. Designing strategies for the emptier that\nminimize backlog (i.e., the height of the fullest cup) is important for\napplications in processor scheduling, buffer management in networks, quality of\nservice guarantees, and deamortization. We prove that the greedy algorithm\n(i.e., the empty-from-fullest-cups algorithm) achieves backlog $O(\\log n)$ for\nany $p \\ge 1$. This resolves a long-standing open problem for $p > 1$, and is\nasymptotically optimal as long as $n \\ge 2p$. If the filler is an oblivious\nadversary, then we prove that there is a randomized emptying algorithm that\nachieve backlog $O(\\log p + \\log \\log n)$ with probability $1 -\n2^{-\\operatorname{polylog}(n)}$ for $2^{\\operatorname{polylog}(n)}$ steps. This\nis known to be asymptotically optimal when $n$ is sufficiently large relative\nto $p$. The analysis of the randomized algorithm can also be reinterpreted as a\nsmoothed analysis of the deterministic greedy algorithm. Previously, the only\nknown bound on backlog for $p > 1$, and the only known randomized guarantees\nfor any $p$ (including when $p = 1$), required the use of resource\naugmentation, meaning that the filler can only distribute at most $p(1 -\n\\epsilon)$ units of water in each step, and that the emptier is then permitted\nto remove $1 + \\delta$ units of water from each of $p$ cups, for some\n$\\epsilon, \\delta > 0$.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 21:11:08 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Kuszmaul", "William", ""]]}, {"id": "1910.13543", "submitter": "Young Kun Ko", "authors": "Young Kun Ko, Omri Weinstein", "title": "An Adaptive Step Toward the Multiphase Conjecture", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2010, P\\v{a}tra\\c{s}cu proposed the following three-phase dynamic problem,\nas a candidate for proving polynomial lower bounds on the operational time of\ndynamic data structures:\n  I: Preprocess a collection of sets $\\vec{S} = S_1, \\ldots , S_k \\subseteq\n[n]$, where $k=\\operatorname{poly}(n)$.\n  II: A set $T\\subseteq [n]$ is revealed, and the data structure updates its\nmemory.\n  III: An index $i \\in [k]$ is revealed, and the data structure must determine\nif $S_i\\cap T=^? \\emptyset$.\n  P\\v{a}tra\\c{s}cu conjectured that any data structure for the Multiphase\nproblem must make $n^\\epsilon$ cell-probes in either Phase II or III, and\nshowed that this would imply similar unconditional lower bounds on many\nimportant dynamic data structure problems. Alas, there has been almost no\nprogress on this conjecture in the past decade since its introduction. We show\nan $\\tilde{\\Omega}(\\sqrt{n})$ cell-probe lower bound on the Multiphase problem\nfor data structures with general (adaptive) updates, and queries with unbounded\nbut \"layered\" adaptivity. This result captures all known set-intersection data\nstructures and significantly strengthens previous Multiphase lower bounds,\nwhich only captured non-adaptive data structures.\n  Our main technical result is a communication lower bound on a 4-party variant\nof P\\v{a}tra\\c{s}cu's Number-On-Forehead Multiphase game, using information\ncomplexity techniques. We also show that a lower bound on P\\v{a}tra\\c{s}cu's\noriginal NOF game would imply a polynomial ($n^{1+\\epsilon}$) lower bound on\nthe number of wires of any constant-depth circuit with arbitrary gates\ncomputing a random $\\tilde{O}(n)\\times n$ linear operator $x \\mapsto Ax$, a\nlong-standing open problem in circuit complexity. This suggests that the NOF\nconjecture is much stronger than its data structure counterpart.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 21:30:14 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Ko", "Young Kun", ""], ["Weinstein", "Omri", ""]]}, {"id": "1910.13618", "submitter": "Chen Dan", "authors": "Chen Dan, Hong Wang, Hongyang Zhang, Yuchen Zhou, Pradeep Ravikumar", "title": "Optimal Analysis of Subset-Selection Based L_p Low Rank Approximation", "comments": "20 pages, accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the low rank approximation problem of any given matrix $A$ over\n$\\mathbb{R}^{n\\times m}$ and $\\mathbb{C}^{n\\times m}$ in entry-wise $\\ell_p$\nloss, that is, finding a rank-$k$ matrix $X$ such that $\\|A-X\\|_p$ is\nminimized. Unlike the traditional $\\ell_2$ setting, this particular variant is\nNP-Hard. We show that the algorithm of column subset selection, which was an\nalgorithmic foundation of many existing algorithms, enjoys approximation ratio\n$(k+1)^{1/p}$ for $1\\le p\\le 2$ and $(k+1)^{1-1/p}$ for $p\\ge 2$. This improves\nupon the previous $O(k+1)$ bound for $p\\ge 1$\n\\cite{chierichetti2017algorithms}. We complement our analysis with lower\nbounds; these bounds match our upper bounds up to constant $1$ when $p\\geq 2$.\nAt the core of our techniques is an application of \\emph{Riesz-Thorin\ninterpolation theorem} from harmonic analysis, which might be of independent\ninterest to other algorithmic designs and analysis more broadly.\n  As a consequence of our analysis, we provide better approximation guarantees\nfor several other algorithms with various time complexity. For example, to make\nthe algorithm of column subset selection computationally efficient, we analyze\na polynomial time bi-criteria algorithm which selects $O(k\\log m)$ columns. We\nshow that this algorithm has an approximation ratio of $O((k+1)^{1/p})$ for\n$1\\le p\\le 2$ and $O((k+1)^{1-1/p})$ for $p\\ge 2$. This improves over the\nbest-known bound with an $O(k+1)$ approximation ratio. Our bi-criteria\nalgorithm also implies an exact-rank method in polynomial time with a slightly\nlarger approximation ratio.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 01:42:20 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Dan", "Chen", ""], ["Wang", "Hong", ""], ["Zhang", "Hongyang", ""], ["Zhou", "Yuchen", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1910.13765", "submitter": "Antonio Di Stasio", "authors": "Antonio Di Stasio, Aniello Murano, Giuseppe Perelli, Moshe Y. Vardi", "title": "Solving Parity Games Using An Automata-Based Algorithm", "comments": null, "journal-ref": "LNCS 9705, 2016, pp. 64-76", "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Parity games are abstract infinite-round games that take an important role in\nformal verification. In the basic setting, these games are two-player,\nturn-based, and played under perfect information on directed graphs, whose\nnodes are labeled with priorities. The winner of a play is determined according\nto the parities (even or odd) of the minimal priority occurring infinitely\noften in that play. The problem of finding a winning strategy in parity games\nis known to be in UPTime $\\cap$ CoUPTime and deciding whether a polynomial time\nsolution exists is a long-standing open question. In the last two decades, a\nvariety of algorithms have been proposed. Many of them have been also\nimplemented in a platform named PGSolver. This has enabled an empirical\nevaluation of these algorithms and a better understanding of their relative\nmerits. In this paper, we further contribute to this subject by implementing,\nfor the first time, an algorithm based on alternating automata. More precisely,\nwe consider an algorithm introduced by Kupferman and Vardi that solves a parity\ngame by solving the emptiness problem of a corresponding alternating parity\nautomaton. Our empirical evaluation demonstrates that this algorithm\noutperforms other algorithms when the game has a a small number of priorities\nrelative to the size of the game. In many concrete applications, we do indeed\nend up with parity games where the number of priorities is relatively small.\nThis makes the new algorithm quite useful in practice.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 10:48:04 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Di Stasio", "Antonio", ""], ["Murano", "Aniello", ""], ["Perelli", "Giuseppe", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "1910.13830", "submitter": "Tharun Kumar Reddy Medini", "authors": "Tharun Medini, Qixuan Huang, Yiqiu Wang, Vijai Mohan, Anshumali\n  Shrivastava", "title": "Extreme Classification in Log Memory using Count-Min Sketch: A Case\n  Study of Amazon Search with 50M Products", "comments": "Published at NeurIPS 2019. arXiv admin note: text overlap with\n  arXiv:1810.04254", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, it has been shown that many hard AI tasks, especially in\nNLP, can be naturally modeled as extreme classification problems leading to\nimproved precision. However, such models are prohibitively expensive to train\ndue to the memory blow-up in the last layer. For example, a reasonable softmax\nlayer for the dataset of interest in this paper can easily reach well beyond\n100 billion parameters (>400 GB memory). To alleviate this problem, we present\nMerged-Average Classifiers via Hashing (MACH), a generic K-classification\nalgorithm where memory provably scales at O(logK) without any strong assumption\non the classes. MACH is subtly a count-min sketch structure in disguise, which\nuses universal hashing to reduce classification with a large number of classes\nto few embarrassingly parallel and independent classification tasks with a\nsmall (constant) number of classes. MACH naturally provides a technique for\nzero communication model parallelism. We experiment with 6 datasets; some\nmulticlass and some multilabel, and show consistent improvement over respective\nstate-of-the-art baselines. In particular, we train an end-to-end deep\nclassifier on a private product search dataset sampled from Amazon Search\nEngine with 70 million queries and 49.46 million products. MACH outperforms, by\na significant margin,the state-of-the-art extreme classification models\ndeployed on commercial search engines: Parabel and dense embedding models. Our\nlargest model has 6.4 billion parameters and trains in less than 35 hours on a\nsingle p3.16x machine. Our training times are 7-10x faster, and our memory\nfootprints are 2-4x smaller than the best baselines. This training time is also\nsignificantly lower than the one reported by Google's mixture of experts (MoE)\nlanguage model on a comparable model size and hardware.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 19:41:28 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Medini", "Tharun", ""], ["Huang", "Qixuan", ""], ["Wang", "Yiqiu", ""], ["Mohan", "Vijai", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1910.13874", "submitter": "Eugenio Angriman", "authors": "Eugenio Angriman, Alexander van der Grinten, Aleksandar Bojchevski,\n  Daniel Z\\\"ugner, Stephan G\\\"unnemann, Henning Meyerhenke", "title": "Group Centrality Maximization for Large-scale Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of vertex centrality measures is a key aspect of network analysis.\nNaturally, such centrality measures have been generalized to groups of\nvertices; for popular measures it was shown that the problem of finding the\nmost central group is $\\mathcal{NP}$-hard. As a result, approximation\nalgorithms to maximize group centralities were introduced recently. Despite a\nnearly-linear running time, approximation algorithms for group betweenness and\n(to a lesser extent) group closeness are rather slow on large networks due to\nhigh constant overheads.\n  That is why we introduce GED-Walk centrality, a new submodular group\ncentrality measure inspired by Katz centrality. In contrast to closeness and\nbetweenness, it considers walks of any length rather than shortest paths, with\nshorter walks having a higher contribution. We define algorithms that (i)\nefficiently approximate the GED-Walk score of a given group and (ii)\nefficiently approximate the (proved to be $\\mathcal{NP}$-hard) problem of\nfinding a group with highest GED-Walk score.\n  Experiments on several real-world datasets show that scores obtained by\nGED-Walk improve performance on common graph mining tasks such as collective\nclassification and graph-level classification. An evaluation of empirical\nrunning times demonstrates that maximizing GED-Walk (in approximation) is two\norders of magnitude faster compared to group betweenness approximation and for\ngroup sizes $\\leq 100$ one to two orders faster than group closeness\napproximation. For graphs with tens of millions of edges, approximate GED-Walk\nmaximization typically needs less than one minute. Furthermore, our experiments\nsuggest that the maximization algorithms scale linearly with the size of the\ninput graph and the size of the group.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 14:21:37 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Angriman", "Eugenio", ""], ["van der Grinten", "Alexander", ""], ["Bojchevski", "Aleksandar", ""], ["Z\u00fcgner", "Daniel", ""], ["G\u00fcnnemann", "Stephan", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1910.13900", "submitter": "Deeparnab Chakrabarty", "authors": "Deeparnab Chakrabarty and Paul de Supinski", "title": "On a Decentralized $(\\Delta{+}1)$-Graph Coloring Algorithm", "comments": "To appear in 3rd SIAM Symposium on Simplicity in Algorithms (SOSA),\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a decentralized graph coloring model where each vertex only knows\nits own color and whether some neighbor has the same color as it. The\nnetworking community has studied this model extensively due to its applications\nto channel selection, rate adaptation, etc. Here, we analyze variants of a\nsimple algorithm of Bhartia et al. [Proc., ACM MOBIHOC, 2016]. In particular,\nwe introduce a variant which requires only $O(n\\log\\Delta)$ expected\nrecolorings that generalizes the coupon collector problem. Finally, we show\nthat the $O(n\\Delta)$ bound Bhartia et al. achieve for their algorithm still\nholds and is tight in adversarial scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 14:46:03 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 13:47:47 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["de Supinski", "Paul", ""]]}, {"id": "1910.13971", "submitter": "Larkin Flodin", "authors": "Larkin Flodin, Venkata Gandikota, Arya Mazumdar", "title": "Superset Technique for Approximate Recovery in One-Bit Compressed\n  Sensing", "comments": "17 pages, 2 figures. Number of measurements needed in Theorem 4 and\n  Corollaries 5-6 were incorrect in previous versions, and are now corrected.\n  More details are discussed in Remarks 1 and 2", "journal-ref": "In Advances in Neural Information Processing Systems, pp.\n  10387-10396. 2019", "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-bit compressed sensing (1bCS) is a method of signal acquisition under\nextreme measurement quantization that gives important insights on the limits of\nsignal compression and analog-to-digital conversion. The setting is also\nequivalent to the problem of learning a sparse hyperplane-classifier. In this\npaper, we propose a novel approach for signal recovery in nonadaptive 1bCS that\nmatches the sample complexity of the current best methods. We construct 1bCS\nmatrices that are universal - i.e. work for all signals under a model - and at\nthe same time recover very general random sparse signals with high probability.\nIn our approach, we divide the set of samples (measurements) into two parts,\nand use the first part to recover the superset of the support of a sparse\nvector. The second set of measurements is then used to approximate the signal\nwithin the superset. While support recovery in 1bCS is well-studied, recovery\nof superset of the support requires fewer samples, and to our knowledge has not\nbeen previously considered for the purpose of approximate recovery of signals.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 16:38:33 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 20:27:56 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 20:54:17 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Flodin", "Larkin", ""], ["Gandikota", "Venkata", ""], ["Mazumdar", "Arya", ""]]}, {"id": "1910.13984", "submitter": "Ali Vakilian", "authors": "Piotr Indyk, Ali Vakilian, Yang Yuan", "title": "Learning-Based Low-Rank Approximations", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a \"learning-based\" algorithm for the low-rank decomposition\nproblem: given an $n \\times d$ matrix $A$, and a parameter $k$, compute a\nrank-$k$ matrix $A'$ that minimizes the approximation loss $\\|A-A'\\|_F$. The\nalgorithm uses a training set of input matrices in order to optimize its\nperformance. Specifically, some of the most efficient approximate algorithms\nfor computing low-rank approximations proceed by computing a projection $SA$,\nwhere $S$ is a sparse random $m \\times n$ \"sketching matrix\", and then\nperforming the singular value decomposition of $SA$. We show how to replace the\nrandom matrix $S$ with a \"learned\" matrix of the same sparsity to reduce the\nerror.\n  Our experiments show that, for multiple types of data sets, a learned sketch\nmatrix can substantially reduce the approximation loss compared to a random\nmatrix $S$, sometimes by one order of magnitude. We also study mixed matrices\nwhere only some of the rows are trained and the remaining ones are random, and\nshow that matrices still offer improved performance while retaining worst-case\nguarantees.\n  Finally, to understand the theoretical aspects of our approach, we study the\nspecial case of $m=1$. In particular, we give an approximation algorithm for\nminimizing the empirical loss, with approximation factor depending on the\nstable rank of matrices in the training set. We also show generalization bounds\nfor the sketch matrix learning problem.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 16:54:50 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Indyk", "Piotr", ""], ["Vakilian", "Ali", ""], ["Yuan", "Yang", ""]]}, {"id": "1910.14105", "submitter": "Albert Pern\\'ia V\\'azquez", "authors": "Albert Pern\\'ia V\\'azquez, N\\'uria Valls Canudas, Elisabet Golobardes\n  Rib\\'e, Alessandro Camboni, Xavier Vilas\\'is-Cardona", "title": "Use of R-trees to improve reconstruction time in pixel trackers", "comments": "9 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ins-det cs.DS hep-ex", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computing time is becoming a key issue for tracking algorithms both online\nand off-line. Programming using adequate data structures can largely improve\nthe efficiency of the reconstruction in terms of time response. We propose\nusing one such data structure, called R-tree, that performs a fast, flexible\nand custom spatial indexing of the hits based on a neighbourhood organisation.\nThe overhead required to prepare the data structure shows to be largely\ncompensated by the efficiency in the search of hits that are candidate to\nbelong to the same track when events present a large number of hits. The study,\nincluding different indexing approaches, is performed for a generic pixel\ntracker largely inspired in the upgrade of the LHCb vertex locator with a\nbackwards reconstruction algorithm of the cellular automaton type.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 19:46:02 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 19:15:10 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["V\u00e1zquez", "Albert Pern\u00eda", ""], ["Canudas", "N\u00faria Valls", ""], ["Rib\u00e9", "Elisabet Golobardes", ""], ["Camboni", "Alessandro", ""], ["Vilas\u00eds-Cardona", "Xavier", ""]]}, {"id": "1910.14153", "submitter": "Sergey Pupyrev", "authors": "Sergey Pupyrev", "title": "Improved Bounds for Track Numbers of Planar Graphs", "comments": null, "journal-ref": "Journal of Graph Algorithms and Applications, 24(3):323-341, 2020", "doi": "10.7155/jgaa.00536", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A track layout of a graph consists of a vertex coloring and a total order of\neach color class, such that no two edges cross between any two color classes.\nThe track number of a graph is the minimum number of colors required by a track\nlayout of the graph.\n  This paper improves lower and upper bounds on the track number of several\nfamilies of planar graphs. We prove that every planar graph has track number at\nmost $225$ and every planar $3$-tree has track number at most $25$. Then we\nshow that there exist outerplanar graphs whose track number is $5$, which leads\nto the best known lower bound of $8$ for planar graphs. Finally, we investigate\nleveled planar graphs and tighten bounds on the track number of weakly leveled\ngraphs, Halin graphs, and X-trees.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 21:55:35 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 22:58:45 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Pupyrev", "Sergey", ""]]}, {"id": "1910.14154", "submitter": "Christoph Grunau", "authors": "Christoph Grunau, Slobodan Mitrovi\\'c, Ronitt Rubinfeld, Ali Vakilian", "title": "Improved Local Computation Algorithm for Set Cover via Sparsification", "comments": "To appear in ACM-SIAM Symposium on Discrete Algorithms (SODA 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a Local Computation Algorithm (LCA) for the set cover problem.\nGiven a set system where each set has size at most $s$ and each element is\ncontained in at most $t$ sets, the algorithm reports whether a given set is in\nsome fixed set cover whose expected size is $O(\\log{s})$ times the minimum\nfractional set cover value. Our algorithm requires $s^{O(\\log{s})} t^{O(\\log{s}\n\\cdot (\\log \\log{s} + \\log \\log{t}))}$ queries. This result improves upon the\napplication of the reduction of [Parnas and Ron, TCS'07] on the result of [Kuhn\net al., SODA'06], which leads to a query complexity of $(st)^{O(\\log{s} \\cdot\n\\log{t})}$.\n  To obtain this result, we design a parallel set cover algorithm that admits\nan efficient simulation in the LCA model by using a sparsification technique\nintroduced in [Ghaffari and Uitto, SODA'19] for the maximal independent set\nproblem. The parallel algorithm adds a random subset of the sets to the\nsolution in a style similar to the PRAM algorithm of [Berger et al., FOCS'89].\nHowever, our algorithm differs in the way that it never revokes its decisions,\nwhich results in a fewer number of adaptive rounds. This requires a novel\napproximation analysis which might be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 22:07:55 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 13:39:13 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Grunau", "Christoph", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Rubinfeld", "Ronitt", ""], ["Vakilian", "Ali", ""]]}, {"id": "1910.14166", "submitter": "Charlie Dickens", "authors": "Graham Cormode, Charlie Dickens", "title": "Iterative Hessian Sketch in Input Sparsity Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable algorithms to solve optimization and regression tasks even\napproximately, are needed to work with large datasets. In this paper we study\nefficient techniques from matrix sketching to solve a variety of convex\nconstrained regression problems. We adopt \"Iterative Hessian Sketching\" (IHS)\nand show that the fast CountSketch and sparse Johnson-Lindenstrauss Transforms\nyield state-of-the-art accuracy guarantees under IHS, while drastically\nimproving the time cost. As a result, we obtain significantly faster algorithms\nfor constrained regression, for both sparse and dense inputs. Our empirical\nresults show that we can summarize data roughly 100x faster for sparse data,\nand, surprisingly, 10x faster on dense data! Consequently, solutions accurate\nto within machine precision of the optimal solution can be found much faster\nthan the previous state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 22:40:07 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Cormode", "Graham", ""], ["Dickens", "Charlie", ""]]}, {"id": "1910.14276", "submitter": "Yang P. Liu", "authors": "Yang P. Liu, Aaron Sidford", "title": "Faster Energy Maximization for Faster Maximum Flow", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide an algorithm which given any $m$-edge $n$-vertex\ndirected graph with integer capacities at most $U$ computes a maximum $s$-$t$\nflow for any vertices $s$ and $t$ in $m^{11/8+o(1)}U^{1/4}$ time with high\nprobability. This running time improves upon the previous best of\n$\\tilde{O}(m^{10/7} U^{1/7})$ (M\\k{a}dry 2016), $\\tilde{O}(m \\sqrt{n} \\log U)$\n(Lee Sidford 2014), and $O(mn)$ (Orlin 2013) when the graph is not too dense or\nhas large capacities.\n  We achieve this result by leveraging recent advances in solving undirected\nflow problems on graphs. We show that in the maximum flow framework of\n(M\\k{a}dry 2016) the problem of optimizing the amount of perturbation of the\ncentral path needed to maximize energy and thereby reduce congestion can be\nefficiently reduced to a smoothed $\\ell_2$-$\\ell_p$ flow optimization problem,\nwhich can be solved approximately via recent work (Kyng, Peng, Sachdeva, Wang\n2019). Leveraging this new primitive, we provide a new long-step interior point\nmethod for maximum flow with faster convergence and simpler analysis that no\nlonger needs global potential functions involving energy as in previous methods\n(M\\k{a}dry 2013, M\\k{a}dry 2016).\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 06:42:12 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Liu", "Yang P.", ""], ["Sidford", "Aaron", ""]]}, {"id": "1910.14344", "submitter": "Sebastian Forster", "authors": "Sebastian Forster, Danupon Nanongkai, Thatchaphol Saranurak, Liu Yang,\n  Sorrachai Yingchareonthawornchai", "title": "Computing and Testing Small Connectivity in Near-Linear Time and Queries\n  via Fast Local Cut Algorithms", "comments": "This paper resulted from a merge of two papers submitted to arXiv\n  (arXiv:1904.08382 and arXiv:1905.05329) and will be presented at the 31st\n  Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2020). Abstract\n  shortened to respect arXiv's limit of 1920 characters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Consider the following \"local\" cut-detection problem in a directed graph: We\nare given a seed vertex $x$ and need to remove at most $k$ edges so that at\nmost $\\nu$ edges can be reached from $x$ (a \"local\" cut) or output $\\bot$ to\nindicate that no such cut exists. If we are given query access to the input\ngraph, then this problem can in principle be solved without reading the whole\ngraph and with query complexity depending on $k$ and $\\nu$. In this paper we\nconsider a slack variant of this problem where, when such a cut exists, we can\noutput a cut with up to $O(k\\nu)$ edges reachable from $x$.\n  We present a simple randomized algorithm spending $O(k^2\\nu)$ time and\n$O(k\\nu)$ queries for the above variant, improving in particular a previous\ntime bound of $O(k^{O(k)}\\nu)$ by Chechik et al. [SODA '17]. We also extend our\nalgorithm to handle an approximate variant. We demonstrate that these local\nalgorithms are versatile primitives for designing substantially improved\nalgorithms for classic graph problems by providing the following three\napplications. (Throughout, $\\tilde O(T)$ hides $\\operatorname{polylog}(T)$.)\n(1) A randomized algorithm for the classic $k$-vertex connectivity problem that\ntakes near-linear time when $k=O(\\operatorname{polylog}(n))$, namely $\\tilde\nO(m+nk^3)$ time in undirected graphs. For directed graphs our $\\tilde\nO(mk^2)$-time algorithm is near-linear when $k=O(\\operatorname{polylog}(n))$.\nOur techniques also yield an improved approximation scheme. (2) Property\ntesting algorithms for $k$-edge and -vertex connectivity with query\ncomplexities that are near-linear in $k$, exponentially improving the\nstate-of-the-art. This resolves two open problems, one by Goldreich and Ron\n[STOC '97] and one by Orenstein and Ron [Theor. Comput Sci. '11]. (3) A faster\nalgorithm for computing the maximal $k$-edge connected subgraphs, improving\nprior work of Chechik et al. [SODA '17].\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 10:09:17 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Forster", "Sebastian", ""], ["Nanongkai", "Danupon", ""], ["Saranurak", "Thatchaphol", ""], ["Yang", "Liu", ""], ["Yingchareonthawornchai", "Sorrachai", ""]]}, {"id": "1910.14387", "submitter": "Thomas Hujsa", "authors": "Raymond Devillers, Evgeny Erofeev, Thomas Hujsa", "title": "Efficient Synthesis of Weighted Marked Graphs with Circular Reachability\n  Graph, and Beyond", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous studies, several methods have been developed to synthesise Petri\nnets from labelled transition systems (LTS), often with structural constraints\non the net and on the LTS. In this paper, we focus on Weighted Marked Graphs\n(WMGs) and Choice-Free (CF) Petri nets, two weighted subclasses of nets in\nwhich each place has at most one output; WMGs have the additional constraint\nthat each place has at most one input. We provide new conditions for checking\nthe existence of a WMG whose reachability graph is isomorphic to a given\ncircular LTS, i.e. forming a single cycle; we develop two new polynomial-time\nsynthesis algorithms dedicated to these constraints: the first one is LTS-based\n(classical synthesis) while the second one is vector-based (weak synthesis) and\nmore efficient in general. We show that our conditions also apply to CF\nsynthesis in the case of three-letter alphabets, and we discuss the\ndifficulties in extending them to CF synthesis over arbitrary alphabets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 11:37:52 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Devillers", "Raymond", ""], ["Erofeev", "Evgeny", ""], ["Hujsa", "Thomas", ""]]}, {"id": "1910.14508", "submitter": "Marcel Wild", "authors": "Marcel Wild", "title": "ALLSAT compressed with wildcards: Frequent Set Mining", "comments": "I solicite the help of FSM practitioners for the final version of\n  this article!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like any simplicial complex the simplicial complex of all frequent sets can\nbe compressed with wildcards once the maximal frequent sets (=facets) are\nknown. Namely, the task (a particular kind of ALLSAT problem) is achieved by\nthe author's recent algorithm Facets-To-Faces. But how to get the facets in the\nfirst place? The novel algorithm Find-All-Facets determines all facets of any\n(decidable) finite simplicial complex by replacing costly hypergraph\ndualization (Dualize+Advance and its variants) with the cheaper calculation of\nthe minimal members of certain set families. The latter task is sped up by\nVertical Layout. While all of this concerns arbitrary simplicial complexes, the\nimpact to Frequent Set Mining (FSM) seems particularly high.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 14:44:42 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 17:03:58 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Wild", "Marcel", ""]]}, {"id": "1910.14610", "submitter": "Haoqian Li", "authors": "Haoqian Li", "title": "A Survey of Adwords Problem With Small Bids In a Primal-dual Setting:\n  Greedy Algorithm, Ranking Algorithm and Primal-dual Training-based Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Adwords problem has always been an interesting internet advertising\nproblem. There are many ways to solve the Adwords problem with the adversarial\norder model, including the Greedy Algorithm, the Balance Algorithm, and the\nScale-bid Algorithm, which is also known as MSVV. In this survey, I will review\nthe Adwords problem with different input models under a primal-dual setting,\nand with the small-bid assumption. In the first section, I will focus on\nAdwords with adversarial order model, and use duality to prove the efficiency\nof the Greedy Algorithm and the MSVV algorithm. Next, I will look at a\nprimal-dual training-based algorithm for the Adwords problem with the IID\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 16:59:14 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Li", "Haoqian", ""]]}, {"id": "1910.14638", "submitter": "Bree Cummins", "authors": "Robert Nerem, Peter Crawford-Kahrl, Bree Cummins, Tomas Gedeon", "title": "A poset metric from the directed maximum common edge subgraph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the directed maximum common edge subgraph problem (DMCES) for the\nclass of directed graphs that are finite, weakly connected, oriented, and\nsimple. We use DMCES to define a metric on partially ordered sets that can be\nrepresented as weakly connected directed acyclic graphs. While most existing\nmetrics assume that the underlying sets of the partial order are identical, and\nonly the relationships between elements can differ, the metric defined here\nallows the partially ordered sets to be different. The proof that there is a\nmetric based on DMCES involves the extension of the concept of line digraphs.\nAlthough this extension can be used to compute the metric by a reduction to the\nmaximum clique problem, it is computationally feasible only for sparse graphs.\nWe provide an alternative techniques for computing the metric for directed\ngraphs that have the additional property of being transitively closed.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:28:21 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 17:21:35 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Nerem", "Robert", ""], ["Crawford-Kahrl", "Peter", ""], ["Cummins", "Bree", ""], ["Gedeon", "Tomas", ""]]}]