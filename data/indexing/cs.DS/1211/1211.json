[{"id": "1211.0157", "submitter": "Partha Sarathi Mandal Dr.", "authors": "Barun Gorain and Partha Sarathi Mandal", "title": "Optimal Covering with Mobile Sensors in an Unbounded Region", "comments": "7 pages, submitted in the Eighth International Conference on Wireless\n  Communication and Sensor Networks (WCSN-2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covering a bounded region with minimum number of homogeneous sensor nodes is\na NP-complete problem \\cite{Li09}. In this paper we have proposed an {\\it id}\nbased distributed algorithm for optimal coverage in an unbounded region. The\nproposed algorithm guarantees maximum spreading in $O(\\sqrt{n})$ rounds without\ncreating any coverage hole. The algorithm executes in synchronous rounds\nwithout exchanging any message.\n  We have also explained how our proposed algorithm can achieve optimal energy\nconsumption and handle random sensor node deployment for optimal spreading.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 12:11:23 GMT"}], "update_date": "2012-11-02", "authors_parsed": [["Gorain", "Barun", ""], ["Mandal", "Partha Sarathi", ""]]}, {"id": "1211.0177", "submitter": "Hao-Hsiang Hung", "authors": "Hao-Hsiang Hung", "title": "Improved Time Complexity of Bandwidth Approximation in Dense Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Given a graph $G=(V, E)$ and and a proper labeling $f$ from $V$ to $\\{1, ...,\nn\\}$, we define $B(f)$ as the maximum absolute difference between $f(u)$ and\n$f(v)$ where $(u,v)\\in E$. The bandwidth of $G$ is the minimum $B(f)$ for all\n$f$. Say $G$ is $\\delta$-dense if its minimum degree is $\\delta n$. In this\npaper, we investigate the trade-off between the approximation ratio and the\ntime complexity of the classical approach of Karpinski {et al}.\\cite{Karpin97},\nand present a faster randomized algorithm for approximating the bandwidth of\n$\\delta$-dense graphs. In particular, by removing the polylog factor of the\ntime complexity required to enumerate all possible placements for balls to\nbins, we reduce the time complexity from $O(n^6\\cdot (\\log n)^{O(1)})$ to\n$O(n^{4+o(1)})$. In advance, we reformulate the perfect matching phase of the\nalgorithm with a maximum flow problem of smaller size and reduce the time\ncomplexity to $O(n^2\\log\\log n)$. We also extend the graph classes could be\napplied by the original approach: we show that the algorithm remains polynomial\ntime as long as $\\delta$ is $O({(\\log\\log n)}^2 / {\\log n})$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 13:46:43 GMT"}], "update_date": "2012-11-02", "authors_parsed": [["Hung", "Hao-Hsiang", ""]]}, {"id": "1211.0243", "submitter": "Shi Li", "authors": "Shi Li, Ola Svensson", "title": "Approximating $k$-Median via Pseudo-Approximation", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approximation algorithm for $k$-median that achieves an\napproximation guarantee of\n  $1+\\sqrt{3}+\\epsilon$, improving upon the decade-old ratio of $3+\\epsilon$.\nOur approach is based on two components, each of which, we believe, is of\nindependent interest.\n  First, we show that in order to give an $\\alpha$-approximation algorithm for\n$k$-median, it is sufficient to give a \\emph{pseudo-approximation algorithm}\nthat finds an $\\alpha$-approximate solution by opening $k+O(1)$ facilities.\nThis is a rather surprising result as there exist instances for which opening\n$k+1$ facilities may lead to a significant smaller cost than if only $k$\nfacilities were opened.\n  Second, we give such a pseudo-approximation algorithm with $\\alpha=\n1+\\sqrt{3}+\\epsilon$. Prior to our work, it was not even known whether opening\n$k + o(k)$ facilities would help improve the approximation ratio.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 18:02:58 GMT"}], "update_date": "2012-11-02", "authors_parsed": [["Li", "Shi", ""], ["Svensson", "Ola", ""]]}, {"id": "1211.0270", "submitter": "Hjalte Wedel Vildh{\\o}j", "authors": "Philip Bille, Inge Li Goertz, Benjamin Sach, Hjalte Wedel Vildh{\\o}j", "title": "Time-Space Trade-Offs for Longest Common Extensions", "comments": "A preliminary version of this paper appeared in the proceedings of\n  the 23rd Annual Symposium on Combinatorial Pattern Matching (CPM 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the longest common extension (LCE) problem, that is, preprocess a\nstring $T$ into a compact data structure that supports fast LCE queries. An LCE\nquery takes a pair $(i,j)$ of indices in $T$ and returns the length of the\nlongest common prefix of the suffixes of $T$ starting at positions $i$ and $j$.\nWe study the time-space trade-offs for the problem, that is, the space used for\nthe data structure vs. the worst-case time for answering an LCE query. Let $n$\nbe the length of $T$. Given a parameter $\\tau$, $1 \\leq \\tau \\leq n$, we show\nhow to achieve either $O(\\infrac{n}{\\sqrt{\\tau}})$ space and $O(\\tau)$ query\ntime, or $O(\\infrac{n}{\\tau})$ space and $O(\\tau \\log({|\\LCE(i,j)|}/{\\tau}))$\nquery time, where $|\\LCE(i,j)|$ denotes the length of the LCE returned by the\nquery. These bounds provide the first smooth trade-offs for the LCE problem and\nalmost match the previously known bounds at the extremes when $\\tau=1$ or\n$\\tau=n$. We apply the result to obtain improved bounds for several\napplications where the LCE problem is the computational bottleneck, including\napproximate string matching and computing palindromes. We also present an\nefficient technique to reduce LCE queries on two strings to one string.\nFinally, we give a lower bound on the time-space product for LCE data\nstructures in the non-uniform cell probe model showing that our second\ntrade-off is nearly optimal.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 19:55:11 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2013 07:33:24 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Bille", "Philip", ""], ["Goertz", "Inge Li", ""], ["Sach", "Benjamin", ""], ["Vildh\u00f8j", "Hjalte Wedel", ""]]}, {"id": "1211.0297", "submitter": "Paraskevas Lekeas", "authors": "Paraskevas V. Lekeas", "title": "A Note on Circular Arc Online Coloring using First Fit", "comments": "9 pages, 3 figures, Figure 3 corrected from previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Raman (2007), using a column construction technique it is proved that\nevery interval graph can be colored online with First Fit with at most $8w(G)$\ncolors, where $w(G)$ is the size of the maximum clique of $G$. Since the column\nconstruction can not be adapted to circular arc graphs we give a different\nproof to establish an upper bound of $9w(G)$ for online coloring a circular arc\ngraph $G$ with the First Fit algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 20:17:37 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2013 14:50:19 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Lekeas", "Paraskevas V.", ""]]}, {"id": "1211.0303", "submitter": "Yann Ponty", "authors": "Andy Lorenz, Yann Ponty (LIX, INRIA Saclay - Ile de France)", "title": "Non-redundant random generation algorithms for weighted context-free\n  languages", "comments": "arXiv admin note: text overlap with arXiv:1012.4560", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the non-redundant random generation of $k$ words of length $n$ in\na context-free language. Additionally, we want to avoid a predefined set of\nwords. We study a rejection-based approach, whose worst-case time complexity is\nshown to grow exponentially with $k$ for some specifications and in the limit\ncase of a coupon collector. We propose two algorithms respectively based on the\nrecursive method and on an unranking approach. We show how careful\nimplementations of these algorithms allow for a non-redundant generation of $k$\nwords of length $n$ in $\\mathcal{O}(k\\cdot n\\cdot \\log{n})$ arithmetic\noperations, after a precomputation of $\\Theta(n)$ numbers. The overall\ncomplexity is therefore dominated by the generation of $k$ words, and the\nnon-redundancy comes at a negligible cost.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 20:33:15 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Lorenz", "Andy", "", "LIX, INRIA Saclay - Ile de France"], ["Ponty", "Yann", "", "LIX, INRIA Saclay - Ile de France"]]}, {"id": "1211.0361", "submitter": "Jae Young Park", "authors": "Anna C. Gilbert, Jae Young Park, Michael B. Wakin", "title": "Sketched SVD: Recovering Spectral Features from Compressive Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a streaming data model in which n sensors observe individual\nstreams of data, presented in a turnstile model. Our goal is to analyze the\nsingular value decomposition (SVD) of the matrix of data defined implicitly by\nthe stream of updates. Each column i of the data matrix is given by the stream\nof updates seen at sensor i. Our approach is to sketch each column of the\nmatrix, forming a \"sketch matrix\" Y, and then to compute the SVD of the sketch\nmatrix. We show that the singular values and right singular vectors of Y are\nclose to those of X, with small relative error. We also believe that this bound\nis of independent interest in non-streaming and non-distributed data collection\nsettings.\n  Assuming that the data matrix X is of size Nxn, then with m linear\nmeasurements of each column of X, we obtain a smaller matrix Y with dimensions\nmxn. If m = O(k \\epsilon^{-2} (log(1/\\epsilon) + log(1/\\delta)), where k\ndenotes the rank of X, then with probability at least 1-\\delta, the singular\nvalues \\sigma'_j of Y satisfy the following relative error result\n  (1-\\epsilon)^(1/2)<= \\sigma'_j/\\sigma_j <= (1 + \\epsilon)^(1/2) as compared\nto the singular values \\sigma_j of the original matrix X. Furthermore, the\nright singular vectors v'_j of Y satisfy\n  ||v_j-v_j'||_2 <= min(sqrt{2},\n(\\epsilon\\sqrt{1+\\epsilon})/(\\sqrt{1-\\epsilon}) max_{i\\neq j}\n(\\sqrt{2}\\sigma_i\\sigma_j)/(min_{c\\in[-1,1]}(|\\sigma^2_i-\\sigma^2_j(1+c\\epsilon)|)))\nas compared to the right singular vectors v_j of X. We apply this result to\nobtain a streaming graph algorithm to approximate the eigenvalues and\neigenvectors of the graph Laplacian in the case where the graph has low rank\n(many connected components).\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 03:57:59 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Gilbert", "Anna C.", ""], ["Park", "Jae Young", ""], ["Wakin", "Michael B.", ""]]}, {"id": "1211.0391", "submitter": "Andreas Bjorklund", "authors": "Andreas Bj\\\"orklund", "title": "Below All Subsets for Some Permutational Counting Problems", "comments": "Corrected several technical errors, added comment on how to use the\n  algorithm for ATSP, and changed title slightly to a more adequate one", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the two problems of computing the permanent of an $n\\times n$\nmatrix of $\\operatorname{poly}(n)$-bit integers and counting the number of\nHamiltonian cycles in a directed $n$-vertex multigraph with\n$\\operatorname{exp}(\\operatorname{poly}(n))$ edges can be reduced to relatively\nfew smaller instances of themselves. In effect we derive the first\ndeterministic algorithms for these two problems that run in $o(2^n)$ time in\nthe worst case. Classic $\\operatorname{poly}(n)2^n$ time algorithms for the two\nproblems have been known since the early 1960's. Our algorithms run in\n$2^{n-\\Omega(\\sqrt{n/\\log n})}$ time.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 08:27:13 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2012 20:23:36 GMT"}, {"version": "v3", "created": "Mon, 26 Aug 2013 06:21:53 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""]]}, {"id": "1211.0479", "submitter": "Stefan Szeider", "authors": "Christer B\\\"ackstr\\\"om, Peter Jonsson, Sebastian Ordyniak, Stefan\n  Szeider", "title": "Parameterized Complexity and Kernel Bounds for Hard Planning Problems", "comments": "This is the full version of a paper that will appear in the Proc. of\n  CIAC 2013", "journal-ref": "Proceedings of CIAC 2013, LNCS 7878, pp. 13-24, 2013", "doi": "10.1007/978-3-642-38233-8_2", "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The propositional planning problem is a notoriously difficult computational\nproblem. Downey et al. (1999) initiated the parameterized analysis of planning\n(with plan length as the parameter) and B\\\"ackstr\\\"om et al. (2012) picked up\nthis line of research and provided an extensive parameterized analysis under\nvarious restrictions, leaving open only one stubborn case. We continue this\nwork and provide a full classification. In particular, we show that the case\nwhen actions have no preconditions and at most $e$ postconditions is\nfixed-parameter tractable if $e\\leq 2$ and W[1]-complete otherwise. We show\nfixed-parameter tractability by a reduction to a variant of the Steiner Tree\nproblem; this problem has been shown fixed-parameter tractable by Guo et al.\n(2007). If a problem is fixed-parameter tractable, then it admits a\npolynomial-time self-reduction to instances whose input size is bounded by a\nfunction of the parameter, called the kernel. For some problems, this function\nis even polynomial which has desirable computational implications. Recent\nresearch in parameterized complexity has focused on classifying fixed-parameter\ntractable problems on whether they admit polynomial kernels or not. We revisit\nall the previously obtained restrictions of planning that are fixed-parameter\ntractable and show that none of them admits a polynomial kernel unless the\npolynomial hierarchy collapses to its third level.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 15:59:54 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2013 12:23:42 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["B\u00e4ckstr\u00f6m", "Christer", ""], ["Jonsson", "Peter", ""], ["Ordyniak", "Sebastian", ""], ["Szeider", "Stefan", ""]]}, {"id": "1211.0589", "submitter": "Shayan Oveis Gharan", "authors": "Russell Lyons and Shayan Oveis Gharan", "title": "Sharp Bounds on Random Walk Eigenvalues via Spectral Embedding", "comments": null, "journal-ref": "IMRN (2017), rnx082", "doi": null, "report-no": null, "categories": "math.PR cs.DM cs.DS math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral embedding of graphs uses the top k non-trivial eigenvectors of the\nrandom walk matrix to embed the graph into R^k. The primary use of this\nembedding has been for practical spectral clustering algorithms [SM00,NJW02].\nRecently, spectral embedding was studied from a theoretical perspective to\nprove higher order variants of Cheeger's inequality [LOT12,LRTV12].\n  We use spectral embedding to provide a unifying framework for bounding all\nthe eigenvalues of graphs. For example, we show that for any finite graph with\nn vertices and all k >= 2, the k-th largest eigenvalue is at most\n1-Omega(k^3/n^3), which extends the only other such result known, which is for\nk=2 only and is due to [LO81]. This upper bound improves to 1-Omega(k^2/n^2) if\nthe graph is regular. We generalize these results, and we provide sharp bounds\non the spectral measure of various classes of graphs, including\nvertex-transitive graphs and infinite graphs, in terms of specific graph\nparameters like the volume growth.\n  As a consequence, using the entire spectrum, we provide (improved) upper\nbounds on the return probabilities and mixing time of random walks with\nconsiderably shorter and more direct proofs. Our work introduces spectral\nembedding as a new tool in analyzing reversible Markov chains. Furthermore,\nbuilding on [Lyo05], we design a local algorithm to approximate the number of\nspanning trees of massive graphs.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2012 02:03:41 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 01:45:36 GMT"}, {"version": "v3", "created": "Fri, 13 Jan 2017 20:42:13 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Lyons", "Russell", ""], ["Gharan", "Shayan Oveis", ""]]}, {"id": "1211.0616", "submitter": "Amit Daniely", "authors": "Amit Daniely and Nati Linial and Shai Shalev-Shwartz", "title": "The complexity of learning halfspaces using generalized linear methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular learning algorithms (E.g. Regression, Fourier-Transform based\nalgorithms, Kernel SVM and Kernel ridge regression) operate by reducing the\nproblem to a convex optimization problem over a vector space of functions.\nThese methods offer the currently best approach to several central problems\nsuch as learning half spaces and learning DNF's. In addition they are widely\nused in numerous application domains. Despite their importance, there are still\nvery few proof techniques to show limits on the power of these algorithms.\n  We study the performance of this approach in the problem of (agnostically and\nimproperly) learning halfspaces with margin $\\gamma$. Let $\\mathcal{D}$ be a\ndistribution over labeled examples. The $\\gamma$-margin error of a hyperplane\n$h$ is the probability of an example to fall on the wrong side of $h$ or at a\ndistance $\\le\\gamma$ from it. The $\\gamma$-margin error of the best $h$ is\ndenoted $\\mathrm{Err}_\\gamma(\\mathcal{D})$. An $\\alpha(\\gamma)$-approximation\nalgorithm receives $\\gamma,\\epsilon$ as input and, using i.i.d. samples of\n$\\mathcal{D}$, outputs a classifier with error rate $\\le\n\\alpha(\\gamma)\\mathrm{Err}_\\gamma(\\mathcal{D}) + \\epsilon$. Such an algorithm\nis efficient if it uses $\\mathrm{poly}(\\frac{1}{\\gamma},\\frac{1}{\\epsilon})$\nsamples and runs in time polynomial in the sample size.\n  The best approximation ratio achievable by an efficient algorithm is\n$O\\left(\\frac{1/\\gamma}{\\sqrt{\\log(1/\\gamma)}}\\right)$ and is achieved using an\nalgorithm from the above class. Our main result shows that the approximation\nratio of every efficient algorithm from this family must be $\\ge\n\\Omega\\left(\\frac{1/\\gamma}{\\mathrm{poly}\\left(\\log\\left(1/\\gamma\\right)\\right)}\\right)$,\nessentially matching the best known upper bound.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2012 15:14:40 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2012 15:06:46 GMT"}, {"version": "v3", "created": "Sun, 20 Oct 2013 17:46:03 GMT"}, {"version": "v4", "created": "Sat, 10 May 2014 11:15:05 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Daniely", "Amit", ""], ["Linial", "Nati", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1211.0729", "submitter": "Zhijie Wang", "authors": "Jack Wang", "title": "A Simple Algorithm for Computing BOCP", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we devise a concise algorithm for computing BOCP. Our method\nis simple, easy-to-implement but without loss of efficiency. Given two\ncircular-arc polygons with $m$ and $n$ edges respectively, our method runs in\n$O(m+n+(l+k)\\log l)$ time, using $O(m+n+k)$ space, where $k$ is the number of\nintersections, and $l$ is the number of {edge}s. Our algorithm has the power to\napproximate to linear complexity when $k$ and $l$ are small. The superiority of\nthe proposed algorithm is also validated through empirical study.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2012 22:38:11 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2012 15:41:44 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2013 14:13:05 GMT"}, {"version": "v4", "created": "Thu, 21 Feb 2013 16:31:32 GMT"}, {"version": "v5", "created": "Mon, 18 Mar 2013 19:04:22 GMT"}, {"version": "v6", "created": "Thu, 4 Jul 2013 06:03:22 GMT"}, {"version": "v7", "created": "Sat, 13 Sep 2014 05:39:04 GMT"}, {"version": "v8", "created": "Tue, 3 Jul 2018 13:27:03 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Wang", "Jack", ""]]}, {"id": "1211.0752", "submitter": "Cheng Wang", "authors": "Cheng Wang", "title": "Faster Approximation of Max Flow for Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I extend the methods in \"Electrical Flows, Laplacian Systems, and Faster\nApproximation of Maximum Flow in Undirected Graphs, with Paul Christiano,\nJonathan Kelner, Daniel Spielman, and Shang-Hua Teng\" to directed graphs with a\nvariation of the framework in the paper, which lead to an algorithm that\napproximately solves the directed max flow problem in nearly-linear time.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 03:34:45 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2012 02:58:59 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Wang", "Cheng", ""]]}, {"id": "1211.0877", "submitter": "Justin Hsu", "authors": "Justin Hsu and Aaron Roth and Jonathan Ullman", "title": "Differential Privacy for the Analyst via Private Equilibrium Computation", "comments": null, "journal-ref": null, "doi": "10.1145/2488608.2488651", "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new mechanisms for answering exponentially many queries from multiple\nanalysts on a private database, while protecting differential privacy both for\nthe individuals in the database and for the analysts. That is, our mechanism's\nanswer to each query is nearly insensitive to changes in the queries asked by\nother analysts. Our mechanism is the first to offer differential privacy on the\njoint distribution over analysts' answers, providing privacy for data analysts\neven if the other data analysts collude or register multiple accounts. In some\nsettings, we are able to achieve nearly optimal error rates (even compared to\nmechanisms which do not offer analyst privacy), and we are able to extend our\ntechniques to handle non-linear queries. Our analysis is based on a novel view\nof the private query-release problem as a two-player zero-sum game, which may\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 14:43:00 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2013 00:36:45 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Hsu", "Justin", ""], ["Roth", "Aaron", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1211.0952", "submitter": "Wolfgang Mulzer", "authors": "Kenneth L. Clarkson, Wolfgang Mulzer, C. Seshadhri", "title": "Self-improving Algorithms for Coordinate-Wise Maxima and Convex Hulls", "comments": "39 pages, 17 figures; thoroughly revised presentation; preliminary\n  versions appeared at SODA 2010 and SoCG 2012", "journal-ref": "SIAM Journal on Computing (SICOMP), 43(2), 2014, pp. 617-653", "doi": "10.1137/12089702X", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the coordinate-wise maxima and the convex hull of a planar point set\nare probably the most classic problems in computational geometry. We consider\nthese problems in the self-improving setting. Here, we have $n$ distributions\n$\\mathcal{D}_1, \\ldots, \\mathcal{D}_n$ of planar points. An input point set\n$(p_1, \\ldots, p_n)$ is generated by taking an independent sample $p_i$ from\neach $\\mathcal{D}_i$, so the input is distributed according to the product\n$\\mathcal{D} = \\prod_i \\mathcal{D}_i$. A self-improving algorithm repeatedly\ngets inputs from the distribution $\\mathcal{D}$ (which is a priori unknown),\nand it tries to optimize its running time for $\\mathcal{D}$. The algorithm uses\nthe first few inputs to learn salient features of the distribution\n$\\mathcal{D}$, before it becomes fine-tuned to $\\mathcal{D}$. Let\n$\\text{OPTMAX}_\\mathcal{D}$ (resp. $\\text{OPTCH}_\\mathcal{D}$) be the expected\ndepth of an \\emph{optimal} linear comparison tree computing the maxima (resp.\nconvex hull) for $\\mathcal{D}$. Our maxima algorithm eventually achieves\nexpected running time $O(\\text{OPTMAX}_\\mathcal{D} + n)$. Furthermore, we give\na self-improving algorithm for convex hulls with expected running time\n$O(\\text{OPTCH}_\\mathcal{D} + n\\log\\log n)$.\n  Our results require new tools for understanding linear comparison trees. In\nparticular, we convert a general linear comparison tree to a restricted version\nthat can then be related to the running time of our algorithms. Another\ninteresting feature is an interleaved search procedure to determine the\nlikeliest point to be extremal with minimal computation. This allows our\nalgorithms to be competitive with the optimal algorithm for $\\mathcal{D}$.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 17:57:14 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2014 14:47:38 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Clarkson", "Kenneth L.", ""], ["Mulzer", "Wolfgang", ""], ["Seshadhri", "C.", ""]]}, {"id": "1211.0975", "submitter": "Moritz Hardt", "authors": "Moritz Hardt and Aaron Roth", "title": "Beyond Worst-Case Analysis in Private Singular Vector Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider differentially private approximate singular vector computation.\nKnown worst-case lower bounds show that the error of any differentially private\nalgorithm must scale polynomially with the dimension of the singular vector. We\nare able to replace this dependence on the dimension by a natural parameter\nknown as the coherence of the matrix that is often observed to be significantly\nsmaller than the dimension both theoretically and empirically. We also prove a\nmatching lower bound showing that our guarantee is nearly optimal for every\nsetting of the coherence parameter. Notably, we achieve our bounds by giving a\nrobust analysis of the well-known power iteration algorithm, which may be of\nindependent interest. Our algorithm also leads to improvements in worst-case\nsettings and to better low-rank approximations in the spectral norm.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 19:11:30 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Hardt", "Moritz", ""], ["Roth", "Aaron", ""]]}, {"id": "1211.0978", "submitter": "Ignasi Sau", "authors": "Valentin Garnero and Ignasi Sau", "title": "A Linear Kernel for Planar Total Dominating Set", "comments": "33 pages, 13 figures", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, Vol. 20 no.\n  1, Discrete Algorithms (May 16, 2018) dmtcs:4487", "doi": "10.23638/DMTCS-20-1-14", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A total dominating set of a graph $G=(V,E)$ is a subset $D \\subseteq V$ such\nthat every vertex in $V$ is adjacent to some vertex in $D$. Finding a total\ndominating set of minimum size is NP-hard on planar graphs and W[2]-complete on\ngeneral graphs when parameterized by the solution size. By the meta-theorem of\nBodlaender et al. [J. ACM, 2016], there exists a linear kernel for Total\nDominating Set on graphs of bounded genus. Nevertheless, it is not clear how\nsuch a kernel can be effectively constructed, and how to obtain explicit\nreduction rules with reasonably small constants. Following the approach of\nAlber et al. [J. ACM, 2004], we provide an explicit kernel for Total Dominating\nSet on planar graphs with at most $410k$ vertices, where $k$ is the size of the\nsolution. This result complements several known constructive linear kernels on\nplanar graphs for other domination problems such as Dominating Set, Edge\nDominating Set, Efficient Dominating Set, Connected Dominating Set, or Red-Blue\nDominating Set.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 19:16:18 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2014 14:12:22 GMT"}, {"version": "v3", "created": "Fri, 28 Apr 2017 22:22:06 GMT"}, {"version": "v4", "created": "Tue, 12 Dec 2017 08:19:34 GMT"}, {"version": "v5", "created": "Mon, 7 May 2018 09:46:25 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Garnero", "Valentin", ""], ["Sau", "Ignasi", ""]]}, {"id": "1211.0986", "submitter": "Jelani Nelson", "authors": "Jelani Nelson, Eric Price, Mary Wootters", "title": "New constructions of RIP matrices with fast multiplication and fewer\n  rows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In compressed sensing, the \"restricted isometry property\" (RIP) is a\nsufficient condition for the efficient reconstruction of a nearly k-sparse\nvector x in C^d from m linear measurements Phi x. It is desirable for m to be\nsmall, and for Phi to support fast matrix-vector multiplication. In this work,\nwe give a randomized construction of RIP matrices Phi in C^{m x d}, preserving\nthe L_2 norms of all k-sparse vectors with distortion 1+eps, where the\nmatrix-vector multiply Phi x can be computed in nearly linear time. The number\nof rows m is on the order of eps^{-2}klog dlog^2(klog d). Previous analyses of\nconstructions of RIP matrices supporting fast matrix-vector multiplies, such as\nthe sampled discrete Fourier matrix, required m to be larger by roughly a log k\nfactor.\n  Supporting fast matrix-vector multiplication is useful for iterative recovery\nalgorithms which repeatedly multiply by Phi or Phi^*. Furthermore, our\nconstruction, together with a connection between RIP matrices and the\nJohnson-Lindenstrauss lemma in [Krahmer-Ward, SIAM. J. Math. Anal. 2011],\nimplies fast Johnson-Lindenstrauss embeddings with asymptotically fewer rows\nthan previously known.\n  Our approach is a simple twist on previous constructions. Rather than\nchoosing the rows for the embedding matrix to be rows sampled from some larger\nstructured matrix (such as the discrete Fourier transform or a random circulant\nmatrix), we instead choose each row of the embedding matrix to be a linear\ncombination of a small number of rows of the original matrix, with random sign\nflips as coefficients. The main tool in our analysis is a recent bound for the\nsupremum of certain types of Rademacher chaos processes in\n[Krahmer-Mendelson-Rauhut, arXiv:1207.0235].\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 19:58:27 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Nelson", "Jelani", ""], ["Price", "Eric", ""], ["Wootters", "Mary", ""]]}, {"id": "1211.0995", "submitter": "Jelani Nelson", "authors": "Jelani Nelson, Huy L. Nguyen", "title": "Sparsity Lower Bounds for Dimensionality Reducing Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give near-tight lower bounds for the sparsity required in several\ndimensionality reducing linear maps. First, consider the JL lemma which states\nthat for any set of n vectors in R there is a matrix A in R^{m x d} with m =\nO(eps^{-2}log n) such that mapping by A preserves pairwise Euclidean distances\nof these n vectors up to a 1 +/- eps factor. We show that there exists a set of\nn vectors such that any such matrix A with at most s non-zero entries per\ncolumn must have s = Omega(eps^{-1}log n/log(1/eps)) as long as m <\nO(n/log(1/eps)). This bound improves the lower bound of Omega(min{eps^{-2},\neps^{-1}sqrt{log_m d}}) by [Dasgupta-Kumar-Sarlos, STOC 2010], which only held\nagainst the stronger property of distributional JL, and only against a certain\nrestricted class of distributions. Meanwhile our lower bound is against the JL\nlemma itself, with no restrictions. Our lower bound matches the sparse\nJohnson-Lindenstrauss upper bound of [Kane-Nelson, SODA 2012] up to an\nO(log(1/eps)) factor.\n  Next, we show that any m x n matrix with the k-restricted isometry property\n(RIP) with constant distortion must have at least Omega(klog(n/k)) non-zeroes\nper column if the number of the rows is the optimal value m = O(klog (n/k)),\nand if k < n/polylog n. This improves the previous lower bound of Omega(min{k,\nn/m}) by [Chandar, 2010] and shows that for virtually all k it is impossible to\nhave a sparse RIP matrix with an optimal number of rows.\n  Lastly, we show that any oblivious distribution over subspace embedding\nmatrices with 1 non-zero per column and preserving all distances in a d\ndimensional-subspace up to a constant factor with constant probability must\nhave at least Omega(d^2) rows. This matches one of the upper bounds in\n[Nelson-Nguyen, 2012] and shows the impossibility of obtaining the best of both\nof constructions in that work, namely 1 non-zero per column and \\~O(d) rows.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 20:42:03 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Nelson", "Jelani", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "1211.1002", "submitter": "Jelani Nelson", "authors": "Jelani Nelson, Huy L. Nguyen", "title": "OSNAP: Faster numerical linear algebra algorithms via sparser subspace\n  embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An \"oblivious subspace embedding (OSE)\" given some parameters eps,d is a\ndistribution D over matrices B in R^{m x n} such that for any linear subspace W\nin R^n with dim(W) = d it holds that Pr_{B ~ D}(forall x in W ||B x||_2 in (1\n+/- eps)||x||_2) > 2/3 We show an OSE exists with m = O(d^2/eps^2) and where\nevery B in the support of D has exactly s=1 non-zero entries per column. This\nimproves previously best known bound in [Clarkson-Woodruff, arXiv:1207.6365].\nOur quadratic dependence on d is optimal for any OSE with s=1 [Nelson-Nguyen,\n2012]. We also give two OSE's, which we call Oblivious Sparse\nNorm-Approximating Projections (OSNAPs), that both allow the parameter settings\nm = \\~O(d/eps^2) and s = polylog(d)/eps, or m = O(d^{1+gamma}/eps^2) and\ns=O(1/eps) for any constant gamma>0. This m is nearly optimal since m >= d is\nrequired simply to no non-zero vector of W lands in the kernel of B. These are\nthe first constructions with m=o(d^2) to have s=o(d). In fact, our OSNAPs are\nnothing more than the sparse Johnson-Lindenstrauss matrices of [Kane-Nelson,\nSODA 2012]. Our analyses all yield OSE's that are sampled using either\nO(1)-wise or O(log d)-wise independent hash functions, which provides some\nefficiency advantages over previous work for turnstile streaming applications.\nOur main result is essentially a Bai-Yin type theorem in random matrix theory\nand is likely to be of independent interest: i.e. we show that for any U in\nR^{n x d} with orthonormal columns and random sparse B, all singular values of\nBU lie in [1-eps, 1+eps] with good probability.\n  Plugging OSNAPs into known algorithms for numerical linear algebra problems\nsuch as approximate least squares regression, low rank approximation, and\napproximating leverage scores implies faster algorithms for all these problems.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 20:53:18 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Nelson", "Jelani", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "1211.1041", "submitter": "Moritz Hardt", "authors": "Moritz Hardt and Ankur Moitra", "title": "Algorithms and Hardness for Robust Subspace Recovery", "comments": "Appeared in Proceedings of COLT 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a fundamental problem in unsupervised learning called\n\\emph{subspace recovery}: given a collection of $m$ points in $\\mathbb{R}^n$,\nif many but not necessarily all of these points are contained in a\n$d$-dimensional subspace $T$ can we find it? The points contained in $T$ are\ncalled {\\em inliers} and the remaining points are {\\em outliers}. This problem\nhas received considerable attention in computer science and in statistics. Yet\nefficient algorithms from computer science are not robust to {\\em adversarial}\noutliers, and the estimators from robust statistics are hard to compute in high\ndimensions.\n  Are there algorithms for subspace recovery that are both robust to outliers\nand efficient? We give an algorithm that finds $T$ when it contains more than a\n$\\frac{d}{n}$ fraction of the points. Hence, for say $d = n/2$ this estimator\nis both easy to compute and well-behaved when there are a constant fraction of\noutliers. We prove that it is Small Set Expansion hard to find $T$ when the\nfraction of errors is any larger, thus giving evidence that our estimator is an\n{\\em optimal} compromise between efficiency and robustness.\n  As it turns out, this basic problem has a surprising number of connections to\nother areas including small set expansion, matroid theory and functional\nanalysis that we make use of here.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 21:39:22 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2012 14:32:57 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2013 21:51:26 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Hardt", "Moritz", ""], ["Moitra", "Ankur", ""]]}, {"id": "1211.1056", "submitter": "Moritz Hardt", "authors": "Moritz Hardt and David P. Woodruff", "title": "How Robust are Linear Sketches to Adaptive Inputs?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear sketches are powerful algorithmic tools that turn an n-dimensional\ninput into a concise lower-dimensional representation via a linear\ntransformation. Such sketches have seen a wide range of applications including\nnorm estimation over data streams, compressed sensing, and distributed\ncomputing. In almost any realistic setting, however, a linear sketch faces the\npossibility that its inputs are correlated with previous evaluations of the\nsketch. Known techniques no longer guarantee the correctness of the output in\nthe presence of such correlations. We therefore ask: Are linear sketches\ninherently non-robust to adaptively chosen inputs? We give a strong affirmative\nanswer to this question. Specifically, we show that no linear sketch\napproximates the Euclidean norm of its input to within an arbitrary\nmultiplicative approximation factor on a polynomial number of adaptively chosen\ninputs. The result remains true even if the dimension of the sketch is d = n -\no(n) and the sketch is given unbounded computation time. Our result is based on\nan algorithm with running time polynomial in d that adaptively finds a\ndistribution over inputs on which the sketch is incorrect with constant\nprobability. Our result implies several corollaries for related problems\nincluding lp-norm estimation and compressed sensing. Notably, we resolve an\nopen problem in compressed sensing regarding the feasibility of l2/l2-recovery\nguarantees in the presence of computationally bounded adversaries.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 22:18:13 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Hardt", "Moritz", ""], ["Woodruff", "David P.", ""]]}, {"id": "1211.1109", "submitter": "Raghu Meka", "authors": "Daniel Kane, Raghu Meka", "title": "A PRG for Lipschitz Functions of Polynomials with Applications to\n  Sparsest Cut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give improved pseudorandom generators (PRGs) for Lipschitz functions of\nlow-degree polynomials over the hypercube. These are functions of the form\npsi(P(x)), where P is a low-degree polynomial and psi is a function with small\nLipschitz constant. PRGs for smooth functions of low-degree polynomials have\nreceived a lot of attention recently and play an important role in constructing\nPRGs for the natural class of polynomial threshold functions. In spite of the\nrecent progress, no nontrivial PRGs were known for fooling Lipschitz functions\nof degree O(log n) polynomials even for constant error rate. In this work, we\ngive the first such generator obtaining a seed-length of (log\nn)\\tilde{O}(d^2/eps^2) for fooling degree d polynomials with error eps.\nPrevious generators had an exponential dependence on the degree.\n  We use our PRG to get better integrality gap instances for sparsest cut, a\nfundamental problem in graph theory with many applications in graph\noptimization. We give an instance of uniform sparsest cut for which a powerful\nsemi-definite relaxation (SDP) first introduced by Goemans and Linial and\nstudied in the seminal work of Arora, Rao and Vazirani has an integrality gap\nof exp(\\Omega((log log n)^{1/2})). Understanding the performance of the\nGoemans-Linial SDP for uniform sparsest cut is an important open problem in\napproximation algorithms and metric embeddings and our work gives a\nnear-exponential improvement over previous lower bounds which achieved a gap of\n\\Omega(log log n).\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 04:51:27 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Kane", "Daniel", ""], ["Meka", "Raghu", ""]]}, {"id": "1211.1149", "submitter": "Jian Li", "authors": "Jian Li, Wen Yuan", "title": "Stochastic Combinatorial Optimization via Poisson Approximation", "comments": "42 pages, 1 figure, Preliminary version appears in the Proceeding of\n  the 45th ACM Symposium on the Theory of Computing (STOC13)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study several stochastic combinatorial problems, including the expected\nutility maximization problem, the stochastic knapsack problem and the\nstochastic bin packing problem. A common technical challenge in these problems\nis to optimize some function of the sum of a set of random variables. The\ndifficulty is mainly due to the fact that the probability distribution of the\nsum is the convolution of a set of distributions, which is not an easy\nobjective function to work with. To tackle this difficulty, we introduce the\nPoisson approximation technique. The technique is based on the Poisson\napproximation theorem discovered by Le Cam, which enables us to approximate the\ndistribution of the sum of a set of random variables using a compound Poisson\ndistribution.\n  We first study the expected utility maximization problem introduced recently\n[Li and Despande, FOCS11]. For monotone and Lipschitz utility functions, we\nobtain an additive PTAS if there is a multidimensional PTAS for the\nmulti-objective version of the problem, strictly generalizing the previous\nresult.\n  For the stochastic bin packing problem (introduced in [Kleinberg, Rabani and\nTardos, STOC97]), we show there is a polynomial time algorithm which uses at\nmost the optimal number of bins, if we relax the size of each bin and the\noverflow probability by eps.\n  For stochastic knapsack, we show a 1+eps-approximation using eps extra\ncapacity, even when the size and reward of each item may be correlated and\ncancelations of items are allowed. This generalizes the previous work [Balghat,\nGoel and Khanna, SODA11] for the case without correlation and cancelation. Our\nalgorithm is also simpler. We also present a factor 2+eps approximation\nalgorithm for stochastic knapsack with cancelations. the current known\napproximation factor of 8 [Gupta, Krishnaswamy, Molinaro and Ravi, FOCS11].\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 09:12:49 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2013 09:11:44 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Li", "Jian", ""], ["Yuan", "Wen", ""]]}, {"id": "1211.1299", "submitter": "Ren\\'e van Bevern", "authors": "Ren\\'e van Bevern, Rodney G. Downey, Michael R. Fellows, Serge\n  Gaspers, Frances A. Rosamond", "title": "Myhill-Nerode methods for hypergraphs", "comments": "A preliminary version of this article appeared in the proceedings of\n  ISAAC 2013. This extended and revised version contains the full proof\n  details, more figures, and corollaries to make the application of the\n  Myhill-Nerode theorem for hypergraphs easier in an algorithmic setting.\n  Moreover, it provides a fix to the proof of the Myhill-Nerode theorem for\n  graphs in the books of Downey and Fellows", "journal-ref": "Algorithmica 73(4):696-729, 2015", "doi": "10.1007/s00453-015-9977-x", "report-no": null, "categories": "cs.DM cs.DS cs.FL math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an analog of the Myhill-Nerode methods from formal language theory\nfor hypergraphs and use it to derive the following results for two NP-hard\nhypergraph problems:\n  * We provide an algorithm for testing whether a hypergraph has cutwidth at\nmost k that runs in linear time for constant k. In terms of parameterized\ncomplexity theory, the problem is fixed-parameter linear parameterized by k.\n  * We show that it is not expressible in monadic second-order logic whether a\nhypergraph has bounded (fractional, generalized) hypertree width. The proof\nleads us to conjecture that, in terms of parameterized complexity theory, these\nproblems are W[1]-hard parameterized by the incidence treewidth (the treewidth\nof the incidence graph).\n  Thus, in the form of the Myhill-Nerode theorem for hypergraphs, we obtain a\nmethod to derive linear-time algorithms and to obtain indicators for\nintractability for hypergraph problems parameterized by incidence treewidth.\n  In an appendix, we point out an error and a fix to the proof of the\nMyhill-Nerode theorem for graphs in Downey and Fellow's book on parameterized\ncomplexity.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 16:36:55 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2013 15:26:06 GMT"}, {"version": "v3", "created": "Thu, 25 Jul 2013 09:06:54 GMT"}, {"version": "v4", "created": "Wed, 5 Feb 2014 19:58:02 GMT"}, {"version": "v5", "created": "Tue, 6 Jan 2015 15:25:36 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["van Bevern", "Ren\u00e9", ""], ["Downey", "Rodney G.", ""], ["Fellows", "Michael R.", ""], ["Gaspers", "Serge", ""], ["Rosamond", "Frances A.", ""]]}, {"id": "1211.1319", "submitter": "L\\'aszl\\'o Kozma", "authors": "Laszlo Kozma, Shay Moran", "title": "Shattering, Graph Orientations, and Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a connection between two seemingly disparate fields: VC-theory and\ngraph theory. This connection yields natural correspondences between\nfundamental concepts in VC-theory, such as shattering and VC-dimension, and\nwell-studied concepts of graph theory related to connectivity, combinatorial\noptimization, forbidden subgraphs, and others.\n  In one direction, we use this connection to derive results in graph theory.\nOur main tool is a generalization of the Sauer-Shelah Lemma. Using this tool we\nobtain a series of inequalities and equalities related to properties of\norientations of a graph. Some of these results appear to be new, for others we\ngive new and simple proofs.\n  In the other direction, we present new illustrative examples of\nshattering-extremal systems - a class of set-systems in VC-theory whose\nunderstanding is considered by some authors to be incomplete. These examples\nare derived from properties of orientations related to distances and flows in\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 17:36:15 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Kozma", "Laszlo", ""], ["Moran", "Shay", ""]]}, {"id": "1211.1340", "submitter": "Maxim Vashkevich", "authors": "Maxim Vashkevich and Alexander Petrovsky", "title": "Derivation of fast DCT algorithms using algebraic technique based on\n  Galois theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents an algebraic technique for derivation of fast discrete\ncosine transform (DCT) algorithms. The technique is based on the algebraic\nsignal processing theory (ASP). In ASP a DCT associates with a polynomial\nalgebra C[x]/p(x). A fast algorithm is obtained as a stepwise decomposition of\nC[x]/p(x). In order to reveal the connection between derivation of fast DCT\nalgorithms and Galois theory we define polynomial algebra over the field of\nrational numbers Q instead of complex C. The decomposition of Q[x]/p(x)\nrequires the extension of the base field Q to splitting field E of polynomial\np(x). Galois theory is used to find intermediate subfields L_i in which\npolynomial p(x) is factored. Based on this factorization fast DCT algorithm is\nderived.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 18:20:57 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Vashkevich", "Maxim", ""], ["Petrovsky", "Alexander", ""]]}, {"id": "1211.1505", "submitter": "Jesper Nederlof", "authors": "Hans L. Bodlaender, Marek Cygan, Stefan Kratsch and Jesper Nederlof", "title": "Solving weighted and counting variants of connectivity problems\n  parameterized by treewidth deterministically in single exponential time", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that many local graph problems, like Vertex Cover and\nDominating Set, can be solved in 2^{O(tw)}|V|^{O(1)} time for graphs G=(V,E)\nwith a given tree decomposition of width tw. However, for nonlocal problems,\nlike the fundamental class of connectivity problems, for a long time we did not\nknow how to do this faster than tw^{O(tw)}|V|^{O(1)}. Recently, Cygan et al.\n(FOCS 2011) presented Monte Carlo algorithms for a wide range of connectivity\nproblems running in time $c^{tw}|V|^{O(1)} for a small constant c, e.g., for\nHamiltonian Cycle and Steiner tree. Naturally, this raises the question whether\nrandomization is necessary to achieve this runtime; furthermore, it is\ndesirable to also solve counting and weighted versions (the latter without\nincurring a pseudo-polynomial cost in terms of the weights).\n  We present two new approaches rooted in linear algebra, based on matrix rank\nand determinants, which provide deterministic c^{tw}|V|^{O(1)} time algorithms,\nalso for weighted and counting versions. For example, in this time we can solve\nthe traveling salesman problem or count the number of Hamiltonian cycles. The\nrank-based ideas provide a rather general approach for speeding up even\nstraightforward dynamic programming formulations by identifying \"small\" sets of\nrepresentative partial solutions; we focus on the case of expressing\nconnectivity via sets of partitions, but the essential ideas should have\nfurther applications. The determinant-based approach uses the matrix tree\ntheorem for deriving closed formulas for counting versions of connectivity\nproblems; we show how to evaluate those formulas via dynamic programming.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 10:38:52 GMT"}], "update_date": "2012-11-08", "authors_parsed": [["Bodlaender", "Hans L.", ""], ["Cygan", "Marek", ""], ["Kratsch", "Stefan", ""], ["Nederlof", "Jesper", ""]]}, {"id": "1211.1506", "submitter": "Jesper Nederlof", "authors": "Marek Cygan, Stefan Kratsch and Jesper Nederlof", "title": "Fast Hamiltonicity checking via bases of perfect matchings", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an even integer t \\geq 2, the Matchings Connecivity matrix H_t is a\nmatrix that has rows and columns both labeled by all perfect matchings of the\ncomplete graph K_t on t vertices; an entry H_t[M_1,M_2] is 1 if M_1\\cup M_2 is\na Hamiltonian cycle and 0 otherwise. Motivated by the computational study of\nthe Hamiltonicity problem, we present three results on the structure of H_t: We\nfirst show that H_t has rank at most 2^{t/2-1} over GF(2) via an appropriate\nfactorization that explicitly provides families of matchings X_t forming bases\nfor H_t. Second, we show how to quickly change representation between such\nbases. Third, we notice that the sets of matchings X_t induce permutation\nmatrices within H_t.\n  Subsequently, we use the factorization to obtain an 1.888^n n^{O(1)} time\nMonte Carlo algorithm that solves the Hamiltonicity problem in directed\nbipartite graphs. Our algorithm as well counts the number of Hamiltonian cycles\nmodulo two in directed bipartite or undirected graphs in the same time bound.\nMoreover, we use the fast basis change algorithm from the second result to\npresent a Monte Carlo algorithm that given an undirected graph on n vertices\nalong with a path decomposition of width at most pw decides Hamiltonicity in\n(2+\\sqrt{2})^{pw}n^{O(1)} time. Finally, we use the third result to show that\nfor every \\epsilon >0 this cannot be improved to\n(2+\\sqrt{2}-\\epsilon)^{pw}n^{O(1)} time unless the Strong Exponential Time\nHypothesis fails, i.e., a faster algorithm for this problem would imply the\nbreakthrough result of a (2-\\epsilon)^n time algorithm for CNF-Sat.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 10:39:24 GMT"}], "update_date": "2012-11-08", "authors_parsed": [["Cygan", "Marek", ""], ["Kratsch", "Stefan", ""], ["Nederlof", "Jesper", ""]]}, {"id": "1211.1661", "submitter": "Mark Korenblit", "authors": "Mark Korenblit and Vadim E. Levit", "title": "A One-Vertex Decomposition Algorithm for Generating Algebraic\n  Expressions of Square Rhomboids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper investigates relationship between algebraic expressions and graphs.\nWe consider a digraph called a square rhomboid that is an example of\nnon-series-parallel graphs. Our intention is to simplify the expressions of\nsquare rhomboids and eventually find their shortest representations. With that\nend in view, we describe the new algorithm for generating square rhomboid\nexpressions, which improves on our previous algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 20:25:20 GMT"}], "update_date": "2012-11-08", "authors_parsed": [["Korenblit", "Mark", ""], ["Levit", "Vadim E.", ""]]}, {"id": "1211.1716", "submitter": "James Voss", "authors": "Mikhail Belkin, Luis Rademacher, James Voss", "title": "Blind Signal Separation in the Presence of Gaussian Noise", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prototypical blind signal separation problem is the so-called cocktail\nparty problem, with n people talking simultaneously and n different microphones\nwithin a room. The goal is to recover each speech signal from the microphone\ninputs. Mathematically this can be modeled by assuming that we are given\nsamples from an n-dimensional random variable X=AS, where S is a vector whose\ncoordinates are independent random variables corresponding to each speaker. The\nobjective is to recover the matrix A^{-1} given random samples from X. A range\nof techniques collectively known as Independent Component Analysis (ICA) have\nbeen proposed to address this problem in the signal processing and machine\nlearning literature. Many of these techniques are based on using the kurtosis\nor other cumulants to recover the components.\n  In this paper we propose a new algorithm for solving the blind signal\nseparation problem in the presence of additive Gaussian noise, when we are\ngiven samples from X=AS+\\eta, where \\eta is drawn from an unknown, not\nnecessarily spherical n-dimensional Gaussian distribution. Our approach is\nbased on a method for decorrelating a sample with additive Gaussian noise under\nthe assumption that the underlying distribution is a linear transformation of a\ndistribution with independent components. Our decorrelation routine is based on\nthe properties of cumulant tensors and can be combined with any standard\ncumulant-based method for ICA to get an algorithm that is provably robust in\nthe presence of Gaussian noise. We derive polynomial bounds for the sample\ncomplexity and error propagation of our method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 22:50:51 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2013 04:43:53 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Belkin", "Mikhail", ""], ["Rademacher", "Luis", ""], ["Voss", "James", ""]]}, {"id": "1211.1722", "submitter": "Ilias Diakonikolas", "authors": "Anindya De, Ilias Diakonikolas, Rocco A. Servedio", "title": "Inverse problems in approximate uniform generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of \\emph{inverse} problems in approximate uniform\ngeneration, focusing on uniform generation of satisfying assignments of various\ntypes of Boolean functions. In such an inverse problem, the algorithm is given\nuniform random satisfying assignments of an unknown function $f$ belonging to a\nclass $\\C$ of Boolean functions, and the goal is to output a probability\ndistribution $D$ which is $\\epsilon$-close, in total variation distance, to the\nuniform distribution over $f^{-1}(1)$.\n  Positive results: We prove a general positive result establishing sufficient\nconditions for efficient inverse approximate uniform generation for a class\n$\\C$. We define a new type of algorithm called a \\emph{densifier} for $\\C$, and\nshow (roughly speaking) how to combine (i) a densifier, (ii) an approximate\ncounting / uniform generation algorithm, and (iii) a Statistical Query learning\nalgorithm, to obtain an inverse approximate uniform generation algorithm. We\napply this general result to obtain a poly$(n,1/\\eps)$-time algorithm for the\nclass of halfspaces; and a quasipoly$(n,1/\\eps)$-time algorithm for the class\nof $\\poly(n)$-size DNF formulas.\n  Negative results: We prove a general negative result establishing that the\nexistence of certain types of signature schemes in cryptography implies the\nhardness of certain inverse approximate uniform generation problems. This\nimplies that there are no {subexponential}-time inverse approximate uniform\ngeneration algorithms for 3-CNF formulas; for intersections of two halfspaces;\nfor degree-2 polynomial threshold functions; and for monotone 2-CNF formulas.\n  Finally, we show that there is no general relationship between the complexity\nof the \"forward\" approximate uniform generation problem and the complexity of\nthe inverse problem for a class $\\C$ -- it is possible for either one to be\neasy while the other is hard.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 23:12:00 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["De", "Anindya", ""], ["Diakonikolas", "Ilias", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1211.1857", "submitter": "Herman Haverkort", "authors": "Herman Haverkort and Jeffrey Janssen", "title": "Simple I/O-efficient flow accumulation on grid terrains", "comments": "This paper is an exact copy of the paper that appeared in the\n  abstract collection of the Workshop on Massive Data Algorithms, Aarhus, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The flow accumulation problem for grid terrains takes as input a matrix of\nflow directions, that specifies for each cell of the grid to which of its eight\nneighbours any incoming water would flow. The problem is to compute, for each\ncell c, from how many cells of the terrain water would reach c. We show that\nthis problem can be solved in O(scan(N)) I/Os for a terrain of N cells. Taking\nconstant factors in the I/O-efficiency into account, our algorithm may be an\norder of magnitude faster than the previously known algorithm that is based on\ntime-forward processing and needs O(sort(N)) I/Os.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2012 14:13:04 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["Haverkort", "Herman", ""], ["Janssen", "Jeffrey", ""]]}, {"id": "1211.1909", "submitter": "Arnab Bhattacharyya", "authors": "Arnab Bhattacharyya, Mark Braverman, Bernard Chazelle, Huy L. Nguyen", "title": "On the Convergence of the Hegselmann-Krause System", "comments": "9 pages, 1 figure, to appear in ITCS '13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study convergence of the following discrete-time non-linear dynamical\nsystem: n agents are located in R^d and at every time step, each moves\nsynchronously to the average location of all agents within a unit distance of\nit. This popularly studied system was introduced by Krause to model the\ndynamics of opinion formation and is often referred to as the Hegselmann-Krause\nmodel. We prove the first polynomial time bound for the convergence of this\nsystem in arbitrary dimensions. This improves on the bound of n^{O(n)}\nresulting from a more general theorem of Chazelle. Also, we show a quadratic\nlower bound and improve the upper bound for one-dimensional systems to O(n^3).\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2012 17:18:07 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Braverman", "Mark", ""], ["Chazelle", "Bernard", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "1211.2066", "submitter": "Herman Haverkort", "authors": "Herman Haverkort", "title": "I/O-optimal algorithms on grid graphs", "comments": "12 pages' extended abstract plus 12 pages' appendix with details,\n  proofs and calculations. Has not been published in and is currently not under\n  review of any conference or journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph of which the n vertices form a regular two-dimensional grid,\nand in which each (possibly weighted and/or directed) edge connects a vertex to\none of its eight neighbours, the following can be done in O(scan(n)) I/Os,\nprovided M = Omega(B^2): computation of shortest paths with non-negative edge\nweights from a single source, breadth-first traversal, computation of a minimum\nspanning tree, topological sorting, time-forward processing (if the input is a\nplane graph), and an Euler tour (if the input graph is a tree). The\nminimum-spanning tree algorithm is cache-oblivious. The best previously\npublished algorithms for these problems need Theta(sort(n)) I/Os. Estimates of\nthe actual I/O volume show that the new algorithms may often be very efficient\nin practice.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2012 08:07:28 GMT"}], "update_date": "2012-11-12", "authors_parsed": [["Haverkort", "Herman", ""]]}, {"id": "1211.2227", "submitter": "Joseph Anderson", "authors": "Joseph Anderson, Navin Goyal, Luis Rademacher", "title": "Efficient learning of simplices", "comments": "New author added to this version, Joseph Anderson. New results:\n  reductions from learning a simplex and a linearly transformed l_p ball to ICA\n  (sections 7 and 8)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show an efficient algorithm for the following problem: Given uniformly\nrandom points from an arbitrary n-dimensional simplex, estimate the simplex.\nThe size of the sample and the number of arithmetic operations of our algorithm\nare polynomial in n. This answers a question of Frieze, Jerrum and Kannan\n[FJK]. Our result can also be interpreted as efficiently learning the\nintersection of n+1 half-spaces in R^n in the model where the intersection is\nbounded and we are given polynomially many uniform samples from it. Our proof\nuses the local search technique from Independent Component Analysis (ICA), also\nused by [FJK]. Unlike these previous algorithms, which were based on analyzing\nthe fourth moment, ours is based on the third moment.\n  We also show a direct connection between the problem of learning a simplex\nand ICA: a simple randomized reduction to ICA from the problem of learning a\nsimplex. The connection is based on a known representation of the uniform\nmeasure on a simplex. Similar representations lead to a reduction from the\nproblem of learning an affine transformation of an n-dimensional l_p ball to\nICA.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2012 20:47:23 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2013 22:54:40 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2013 02:52:50 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Anderson", "Joseph", ""], ["Goyal", "Navin", ""], ["Rademacher", "Luis", ""]]}, {"id": "1211.2322", "submitter": "Charo Del Genio", "authors": "Charo I. Del Genio and Thilo Gross", "title": "Graph isomorphism and automorphism problems are polynomial", "comments": "After appearance of the manuscript on arXiv we received communication\n  from Dr. Matthew Anderson with a counter example. Although we performed\n  extensive tests prior to uploading, the counter example is correct and there\n  seems to be an error in our procedure. We would like to beg the pardon of any\n  colleagues who spent time reading the preprint and once again thank Dr.\n  Anderson for his communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex questions in biology, physics, and mathematics can be mapped to\nthe graph isomorphism problem and the closely related graph automorphism\nproblem. In particular, these problems appear in the context of network\nvisualization, computational logic, structure recognition, and dynamics of\ncomplex systems. Both problems have previously been suspected, but not proven,\nto be NP-complete. In this paper we propose an algorithm that solves both graph\nautomorphism and isomorphism problems in polynomial time. The algorithm can be\neasily implemented and thus opens up a wide range of applications.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2012 11:50:27 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2012 16:27:32 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Del Genio", "Charo I.", ""], ["Gross", "Thilo", ""]]}, {"id": "1211.2384", "submitter": "George Mertzios", "authors": "George B. Mertzios and Paul G. Spirakis", "title": "Strong Bounds for Evolution in Undirected Graphs", "comments": "28 pages, 18 fugures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the generalized Moran process, as introduced by Lieberman\net al. [Nature, 433:312-316, 2005]. We introduce the parameterized notions of\nselective amplifiers and selective suppressors of evolution, i.e. of networks\n(graphs) with many \"strong starts\" and many \"weak starts\" for the mutant,\nrespectively. We first prove the existence of strong selective amplifiers and\nof (quite) strong selective suppressors. Furthermore we provide strong upper\nbounds and almost tight lower bounds (by proving the \"Thermal Theorem\") for the\ntraditional notion of fixation probability of Lieberman et al., i.e. assuming a\nrandom initial placement of the mutant.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2012 08:04:50 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 18:10:43 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Mertzios", "George B.", ""], ["Spirakis", "Paul G.", ""]]}, {"id": "1211.2636", "submitter": "M. Oguzhan Kulekci", "authors": "M. Oguzhan Kulekci", "title": "A memory versus compression ratio trade-off in PPM via compressed\n  context modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its introduction prediction by partial matching (PPM) has always been a\nde facto gold standard in lossless text compression, where many variants\nimproving the compression ratio and speed have been proposed. However, reducing\nthe high space requirement of PPM schemes did not gain that much attention.\nThis study focuses on reducing the memory consumption of PPM via the recently\nproposed compressed context modeling that uses the compressed representations\nof contexts in the statistical model. Differently from the classical context\ndefinition as the string of the preceding characters at a particular position,\nCCM considers context as the amount of preceding information that is actually\nthe bit stream composed by compressing the previous symbols. We observe that by\nusing the CCM, the data structures, particularly the context trees, can be\nimplemented in smaller space, and present a trade-off between the compression\nratio and the space requirement. The experiments conducted showed that this\ntrade-off is especially beneficial in low orders with approximately 20 - 25\npercent gain in memory by a sacrifice of up to nearly 7 percent loss in\ncompression ratio.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 14:36:14 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Kulekci", "M. Oguzhan", ""]]}, {"id": "1211.2662", "submitter": "Arash Rafiey", "authors": "Arash Rafiey", "title": "Recognizing Interval Bigraphs by Forbidden Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let H be a connected bipartite graph with n nodes and m edges. We give an\nO(nm) time algorithm to decide whether H is an interval bigraph. The best known\nalgorithm has time complexity O(nm^6(m + n) \\log n) and it was developed in\n1997 [18]. Our approach is based on an ordering characterization of interval\nbigraphs introduced by Hell and Huang [13]. We transform the problem of finding\nthe desired ordering to choosing strong components of a pair-digraph without\ncreating conflicts. We make use of the structure of the pair-digraph as well as\ndecomposition of bigraph H based on the special components of the pair-digraph.\nThis way we make explicit what the difficult cases are and gain efficiency by\nisolating such situations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 15:38:44 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2013 01:24:15 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2016 02:37:29 GMT"}, {"version": "v4", "created": "Sat, 19 May 2018 06:16:15 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Rafiey", "Arash", ""]]}, {"id": "1211.2664", "submitter": "Cl\\'ement Canonne", "authors": "Clement Canonne, Dana Ron, Rocco A. Servedio", "title": "Testing probability distributions using conditional samples", "comments": "Significant changes on Section 9 (detailing and expanding the proof\n  of Theorem 16). Several clarifications and typos fixed in various places", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a new framework for property testing of probability distributions,\nby considering distribution testing algorithms that have access to a\nconditional sampling oracle.* This is an oracle that takes as input a subset $S\n\\subseteq [N]$ of the domain $[N]$ of the unknown probability distribution $D$\nand returns a draw from the conditional probability distribution $D$ restricted\nto $S$. This new model allows considerable flexibility in the design of\ndistribution testing algorithms; in particular, testing algorithms in this\nmodel can be adaptive.\n  We study a wide range of natural distribution testing problems in this new\nframework and some of its variants, giving both upper and lower bounds on query\ncomplexity. These problems include testing whether $D$ is the uniform\ndistribution $\\mathcal{U}$; testing whether $D = D^\\ast$ for an explicitly\nprovided $D^\\ast$; testing whether two unknown distributions $D_1$ and $D_2$\nare equivalent; and estimating the variation distance between $D$ and the\nuniform distribution. At a high level our main finding is that the new\n\"conditional sampling\" framework we consider is a powerful one: while all the\nproblems mentioned above have $\\Omega(\\sqrt{N})$ sample complexity in the\nstandard model (and in some cases the complexity must be almost linear in $N$),\nwe give $\\mathrm{poly}(\\log N, 1/\\varepsilon)$-query algorithms (and in some\ncases $\\mathrm{poly}(1/\\varepsilon)$-query algorithms independent of $N$) for\nall these problems in our conditional sampling setting.\n  *Independently from our work, Chakraborty et al. also considered this\nframework. We discuss their work in Subsection [1.4].\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 15:39:28 GMT"}, {"version": "v2", "created": "Fri, 16 Jan 2015 18:23:16 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Canonne", "Clement", ""], ["Ron", "Dana", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1211.2670", "submitter": "Andreas Wiese", "authors": "Aris Anagnostopoulos, Fabrizio Grandoni, Stefano Leonardi, and Andreas\n  Wiese", "title": "A Mazing 2+eps Approximation for Unsplittable Flow on a Path", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the unsplittable flow on a path problem (UFP) where we are given a\npath with non-negative edge capacities and tasks, which are characterized by a\nsubpath, a demand, and a profit. The goal is to find the most profitable subset\nof tasks whose total demand does not violate the edge capacities. This problem\nnaturally arises in many settings such as bandwidth allocation, resource\nconstrained scheduling, and interval packing.\n  A natural task classification defines the size of a task i to be the ratio\ndelta between the demand of i and the minimum capacity of any edge used by i.\nIf all tasks have sufficiently small delta, the problem is already well\nunderstood and there is a 1+eps approximation. For the complementary\nsetting---instances whose tasks all have large delta---much remains unknown,\nand the best known polynomial-time procedure gives only (for any constant\ndelta>0) an approximation ratio of 6+eps.\n  In this paper we present a polynomial time 1+eps approximation for the latter\nsetting. Key to this result is a complex geometrically inspired dynamic\nprogram. Here each task is represented as a segment underneath the capacity\ncurve, and we identify a proper maze-like structure so that each passage of the\nmaze is crossed by only O(1) tasks in the computed solution. In combination\nwith the known PTAS for delta-small tasks, our result implies a 2+eps\napproximation for UFP, improving on the previous best 7+eps approximation\n[Bonsma et al., FOCS 2011]. We remark that our improved approximation factor\nmatches the best known approximation ratio for the considerably easier special\ncase of uniform edge capacities.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 15:56:03 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Anagnostopoulos", "Aris", ""], ["Grandoni", "Fabrizio", ""], ["Leonardi", "Stefano", ""], ["Wiese", "Andreas", ""]]}, {"id": "1211.2687", "submitter": "Varun Gupta", "authors": "Varun Gupta, Ana Radovanovic", "title": "Online Stochastic Bin Packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of packing Virtual Machines on physical servers in\nthe cloud, we study the problem of one-dimensional online stochastic bin\npacking. Items with sizes i.i.d. from an unknown distribution with integral\nsupport arrive as a stream and must be packed on arrival in bins of size B,\nalso an integer. The size of an item is known when it arrives and the goal is\nto minimize the waste, defined to be the total unused space in non-empty bins.\nWhile there are many heuristics for online stochastic bin packing, all such\nheuristics are either optimal for only certain classes of item size\ndistributions, or rely on learning the distribution. The state-of-the-art Sum\nof Squares heuristic (Csirik et al.) obtains sublinear (in number of items\nseen) waste for distributions where the expected waste for the optimal offline\nalgorithm is sublinear, but has a constant factor larger waste for\ndistributions with linear waste under OPT. Csirik et al. solved this problem by\nlearning the distribution and solving an LP to inject phantom jobs in the\narrival stream.\n  We present two distribution-agnostic bin packing heuristics that achieve\nadditive O(sqrt{n}) waste compared to OPT for all distributions. Our algorithms\nare gradient descent on suitably defined Lagrangian relaxations of the bin\npacking Linear Program. The first algorithm is very similar to the SS\nalgorithm, but conceptually packs the bins top-down instead of bottom-up. This\nmotivates our second heuristic that uses a different Lagrangian relaxation to\npack bins bottom-up.\n  Next, we consider the more general problem of online stochastic bin packing\nwith item departures where the time requirement of an item is only revealed\nwhen the item departs. Our algorithms extend as is to the case of item\ndepartures. We also briefly revisit the Best Fit heuristic which has not been\nstudied in the scenario of item departures yet.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 16:35:25 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Gupta", "Varun", ""], ["Radovanovic", "Ana", ""]]}, {"id": "1211.2696", "submitter": "Diodato Ferraioli", "authors": "Diodato Ferraioli and Carmine Ventre", "title": "Metastability of Asymptotically Well-Behaved Potential Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main criticisms to game theory concerns the assumption of full\nrationality. Logit dynamics is a decentralized algorithm in which a level of\nirrationality (a.k.a. \"noise\") is introduced in players' behavior. In this\ncontext, the solution concept of interest becomes the logit equilibrium, as\nopposed to Nash equilibria. Logit equilibria are distributions over strategy\nprofiles that possess several nice properties, including existence and\nuniqueness. However, there are games in which their computation may take time\nexponential in the number of players. We therefore look at an approximate\nversion of logit equilibria, called metastable distributions, introduced by\nAuletta et al. [SODA 2012]. These are distributions that remain stable (i.e.,\nplayers do not go too far from it) for a super-polynomial number of steps\n(rather than forever, as for logit equilibria). The hope is that these\ndistributions exist and can be reached quickly by logit dynamics.\n  We identify a class of potential games, called asymptotically well-behaved,\nfor which the behavior of the logit dynamics is not chaotic as the number of\nplayers increases so to guarantee meaningful asymptotic results. We prove that\nany such game admits distributions which are metastable no matter the level of\nnoise present in the system, and the starting profile of the dynamics. These\ndistributions can be quickly reached if the rationality level is not too big\nwhen compared to the inverse of the maximum difference in potential. Our proofs\nbuild on results which may be of independent interest, including some spectral\ncharacterizations of the transition matrix defined by logit dynamics for\ngeneric games and the relationship of several convergence measures for Markov\nchains.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 17:06:11 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2013 15:24:01 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2014 12:24:52 GMT"}, {"version": "v4", "created": "Tue, 4 Nov 2014 19:10:19 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Ferraioli", "Diodato", ""], ["Ventre", "Carmine", ""]]}, {"id": "1211.2713", "submitter": "Richard Peng", "authors": "Mu Li, Gary L. Miller, Richard Peng", "title": "Iterative Row Sampling", "comments": "26 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant interest and progress recently in algorithms that\nsolve regression problems involving tall and thin matrices in input sparsity\ntime. These algorithms find shorter equivalent of a n*d matrix where n >> d,\nwhich allows one to solve a poly(d) sized problem instead. In practice, the\nbest performances are often obtained by invoking these routines in an iterative\nfashion. We show these iterative methods can be adapted to give theoretical\nguarantees comparable and better than the current state of the art.\n  Our approaches are based on computing the importances of the rows, known as\nleverage scores, in an iterative manner. We show that alternating between\ncomputing a short matrix estimate and finding more accurate approximate\nleverage scores leads to a series of geometrically smaller instances. This\ngives an algorithm that runs in $O(nnz(A) + d^{\\omega + \\theta} \\epsilon^{-2})$\ntime for any $\\theta > 0$, where the $d^{\\omega + \\theta}$ term is comparable\nto the cost of solving a regression problem on the small approximation. Our\nresults are built upon the close connection between randomized matrix\nalgorithms, iterative methods, and graph sparsification.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 17:58:38 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2013 19:52:00 GMT"}], "update_date": "2013-04-05", "authors_parsed": [["Li", "Mu", ""], ["Miller", "Gary L.", ""], ["Peng", "Richard", ""]]}, {"id": "1211.2756", "submitter": "Anton Korobeynikov", "authors": "Sergey I. Nikolenko, Anton I. Korobeynikov and Max A. Alekseyev", "title": "BayesHammer: Bayesian clustering for error correction in single-cell\n  sequencing", "comments": null, "journal-ref": "BMC Genomics 14(Suppl 1) (2013), pp. S7", "doi": "10.1186/1471-2164-14-S1-S7", "report-no": null, "categories": "q-bio.QM cs.CE cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error correction of sequenced reads remains a difficult task, especially in\nsingle-cell sequencing projects with extremely non-uniform coverage. While\nexisting error correction tools designed for standard (multi-cell) sequencing\ndata usually come up short in single-cell sequencing projects, algorithms\nactually used for single-cell error correction have been so far very\nsimplistic.\n  We introduce several novel algorithms based on Hamming graphs and Bayesian\nsubclustering in our new error correction tool BayesHammer. While BayesHammer\nwas designed for single-cell sequencing, we demonstrate that it also improves\non existing error correction tools for multi-cell sequencing data while working\nmuch faster on real-life datasets. We benchmark BayesHammer on both $k$-mer\ncounts and actual assembly results with the SPAdes genome assembler.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 19:52:34 GMT"}], "update_date": "2013-01-31", "authors_parsed": [["Nikolenko", "Sergey I.", ""], ["Korobeynikov", "Anton I.", ""], ["Alekseyev", "Max A.", ""]]}, {"id": "1211.2926", "submitter": "M. Oguzhan Kulekci", "authors": "M. Oguzhan Kulekci", "title": "Enumeration of sequences with large alphabets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study focuses on efficient schemes for enumerative coding of\n$\\sigma$--ary sequences by mainly borrowing ideas from \\\"Oktem & Astola's\n\\cite{Oktem99} hierarchical enumerative coding and Schalkwijk's\n\\cite{Schalkwijk72} asymptotically optimal combinatorial code on binary\nsequences. By observing that the number of distinct $\\sigma$--dimensional\nvectors having an inner sum of $n$, where the values in each dimension are in\nrange $[0...n]$ is $K(\\sigma,n) = \\sum_{i=0}^{\\sigma-1} {{n-1} \\choose\n{\\sigma-1-i}} {{\\sigma} \\choose {i}}$, we propose representing $C$ vector via\nenumeration, and present necessary algorithms to perform this task. We prove\n$\\log K(\\sigma,n)$ requires approximately $ (\\sigma -1) \\log (\\sigma-1) $ less\nbits than the naive $(\\sigma-1)\\lceil \\log (n+1) \\rceil$ representation for\nrelatively large $n$, and examine the results for varying alphabet sizes\nexperimentally. We extend the basic scheme for the enumerative coding of\n$\\sigma$--ary sequences by introducing a new method for large alphabets. We\nexperimentally show that the newly introduced technique is superior to the\nbasic scheme by providing experiments on DNA sequences.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 09:30:08 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Kulekci", "M. Oguzhan", ""]]}, {"id": "1211.2960", "submitter": "Hamid Allouch HA", "authors": "Hamid Allouch, Idriss Chana and Mostafa Belkasmi", "title": "Iterative decoding of Generalized Parallel Concatenated Block codes\n  using cyclic permutations", "comments": "8 pages, 14 figures, IJCSI journal link:\n  http://www.ijcsi.org/articles/Iterative-decoding-of-generalized-parallel-concatenated-block-codes-using-cyclic-permutations.php", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 5, No 1, September 2012 (ISSN (Online): 1694-0814)", "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Iterative decoding techniques have gain popularity due to their performance\nand their application in most communications systems. In this paper, we present\na new application of our iterative decoder on the GPCB (Generalized Parallel\nConcatenated Block codes) which uses cyclic permutations. We introduce a new\nvariant of the component decoder. After extensive simulation; the obtained\nresult is very promising compared with several existing methods. We evaluate\nthe effects of various parameters component codes, interleaver size, block\nsize, and the number of iterations. Three interesting results are obtained; the\nfirst one is that the performances in terms of BER (Bit Error Rate) of the new\nconstituent decoder are relatively similar to that of original one. Secondly\nour turbo decoding outperforms another turbo decoder for some linear block\ncodes. Thirdly the proposed iterative decoding of GPCB-BCH (75, 51) is about\n2.1dB from its Shannon limit.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 11:44:56 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Allouch", "Hamid", ""], ["Chana", "Idriss", ""], ["Belkasmi", "Mostafa", ""]]}, {"id": "1211.3201", "submitter": "Chaitanya Swamy", "authors": "Hadi Minooei and Chaitanya Swamy", "title": "Truthful Mechanism Design for Multidimensional Covering Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate {\\em multidimensional covering mechanism-design} problems,\nwherein there are $m$ items that need to be covered and $n$ agents who provide\ncovering objects, with each agent $i$ having a private cost for the covering\nobjects he provides. The goal is to select a set of covering objects of minimum\ntotal cost that together cover all the items.\n  We focus on two representative covering problems: uncapacitated facility\nlocation (\\ufl) and vertex cover (\\vcp). For multidimensional \\ufl, we give a\nblack-box method to transform any {\\em Lagrangian-multiplier-preserving}\n$\\rho$-approximation algorithm for \\ufl to a truthful-in-expectation,\n$\\rho$-approx. mechanism. This yields the first result for multidimensional\n\\ufl, namely a truthful-in-expectation 2-approximation mechanism.\n  For multidimensional \\vcp (\\mvcp), we develop a {\\em decomposition method}\nthat reduces the mechanism-design problem into the simpler task of constructing\n{\\em threshold mechanisms}, which are a restricted class of truthful\nmechanisms, for simpler (in terms of graph structure or problem dimension)\ninstances of \\mvcp. By suitably designing the decomposition and the threshold\nmechanisms it uses as building blocks, we obtain truthful mechanisms with the\nfollowing approximation ratios ($n$ is the number of nodes): (1) $O(r^2\\log n)$\nfor $r$-dimensional \\vcp; and (2) $O(r\\log n)$ for $r$-dimensional \\vcp on any\nproper minor-closed family of graphs (which improves to $O(\\log n)$ if no two\nneighbors of a node belong to the same player). These are the first truthful\nmechanisms for \\mvcp with non-trivial approximation guarantees.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 04:26:50 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2012 03:51:54 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2013 04:54:38 GMT"}, {"version": "v4", "created": "Tue, 10 Sep 2013 19:35:11 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Minooei", "Hadi", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "1211.3299", "submitter": "Bodo Manthey", "authors": "Tobias Brunsch, Kamiel Cornelissen, Bodo Manthey, Heiko R\\\"oglin", "title": "Smoothed Analysis of Belief Propagation for Minimum-Cost Flow and\n  Matching", "comments": "To be presented at WALCOM 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief propagation (BP) is a message-passing heuristic for statistical\ninference in graphical models such as Bayesian networks and Markov random\nfields. BP is used to compute marginal distributions or maximum likelihood\nassignments and has applications in many areas, including machine learning,\nimage processing, and computer vision. However, the theoretical understanding\nof the performance of BP is unsatisfactory.\n  Recently, BP has been applied to combinatorial optimization problems. It has\nbeen proved that BP can be used to compute maximum-weight matchings and\nminimum-cost flows for instances with a unique optimum. The number of\niterations needed for this is pseudo-polynomial and hence BP is not efficient\nin general.\n  We study belief propagation in the framework of smoothed analysis and prove\nthat with high probability the number of iterations needed to compute\nmaximum-weight matchings and minimum-cost flows is bounded by a polynomial if\nthe weights/costs of the edges are randomly perturbed. To prove our upper\nbounds, we use an isolation lemma by Beier and V\\\"{o}cking (SIAM J. Comput.\n2006) for matching and generalize an isolation lemma for min-cost flow by\nGamarnik, Shah, and Wei (Operations Research, 2012). We also prove almost\nmatching lower tail bounds for the number of iterations that BP needs to\nconverge.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 13:23:00 GMT"}], "update_date": "2012-11-15", "authors_parsed": [["Brunsch", "Tobias", ""], ["Cornelissen", "Kamiel", ""], ["Manthey", "Bodo", ""], ["R\u00f6glin", "Heiko", ""]]}, {"id": "1211.3412", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed and Jennifer Neville and Ramana Kompella", "title": "Network Sampling: From Static to Streaming Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network sampling is integral to the analysis of social, information, and\nbiological networks. Since many real-world networks are massive in size,\ncontinuously evolving, and/or distributed in nature, the network structure is\noften sampled in order to facilitate study. For these reasons, a more thorough\nand complete understanding of network sampling is critical to support the field\nof network science. In this paper, we outline a framework for the general\nproblem of network sampling, by highlighting the different objectives,\npopulation and units of interest, and classes of network sampling methods. In\naddition, we propose a spectrum of computational models for network sampling\nmethods, ranging from the traditionally studied model based on the assumption\nof a static domain to a more challenging model that is appropriate for\nstreaming domains. We design a family of sampling methods based on the concept\nof graph induction that generalize across the full spectrum of computational\nmodels (from static to streaming) while efficiently preserving many of the\ntopological properties of the input graphs. Furthermore, we demonstrate how\ntraditional static sampling algorithms can be modified for graph streams for\neach of the three main classes of sampling methods: node, edge, and\ntopology-based sampling. Our experimental results indicate that our proposed\nfamily of sampling methods more accurately preserves the underlying properties\nof the graph for both static and streaming graphs. Finally, we study the impact\nof network sampling algorithms on the parameter estimation and performance\nevaluation of relational classification algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 01:48:37 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Neville", "Jennifer", ""], ["Kompella", "Ramana", ""]]}, {"id": "1211.3642", "submitter": "Keisuke Goto", "authors": "Keisuke Goto and Hideo Bannai", "title": "Simpler and Faster Lempel Ziv Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new, simple, and efficient approach for computing the Lempel-Ziv\n(LZ77) factorization of a string in linear time, based on suffix arrays.\nComputational experiments on various data sets show that our approach\nconstantly outperforms the currently fastest algorithm LZ OG (Ohlebusch and Gog\n2011), and can be up to 2 to 3 times faster in the processing after obtaining\nthe suffix array, while requiring the same or a little more space.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 16:22:40 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2013 08:31:01 GMT"}], "update_date": "2013-01-21", "authors_parsed": [["Goto", "Keisuke", ""], ["Bannai", "Hideo", ""]]}, {"id": "1211.4150", "submitter": "Aaron Roth", "authors": "Morteza Zadimoghaddam and Aaron Roth", "title": "Efficiently Learning from Revealed Preference", "comments": "Extended abstract appears in WINE 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the revealed preferences problem from a learning\nperspective. Every day, a price vector and a budget is drawn from an unknown\ndistribution, and a rational agent buys his most preferred bundle according to\nsome unknown utility function, subject to the given prices and budget\nconstraint. We wish not only to find a utility function which rationalizes a\nfinite set of observations, but to produce a hypothesis valuation function\nwhich accurately predicts the behavior of the agent in the future. We give\nefficient algorithms with polynomial sample-complexity for agents with linear\nvaluation functions, as well as for agents with linearly separable, concave\nvaluation functions with bounded second derivative.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2012 19:30:52 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Zadimoghaddam", "Morteza", ""], ["Roth", "Aaron", ""]]}, {"id": "1211.4370", "submitter": "Elahe Moghimi", "authors": "Elahe Moghimi Hanjani and Mahdi Javanmard", "title": "An Algorithm for Optimized Searching using NON-Overlapping Iterative\n  Neighbor intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have attempted in this paper to reduce the number of checked condition\nthrough saving frequency of the tandem replicated words, and also using\nnon-overlapping iterative neighbor intervals on plane sweep algorithm. The\nessential idea of non-overlapping iterative neighbor search in a document lies\nin focusing the search not on the full space of solutions but on a smaller\nsubspace considering non-overlapping intervals defined by the solutions.\nSubspace is defined by the range near the specified minimum keyword. We\nrepeatedly pick a range up and flip the unsatisfied keywords, so the relevant\nranges are detected. The proposed method tries to improve the plane sweep\nalgorithm by efficiently calculating the minimal group of words and enumerating\nintervals in a document which contain the minimum frequency keyword. It\ndecreases the number of comparison and creates the best state of optimized\nsearch algorithm especially in a high volume of data. Efficiency and\nreliability are also increased compared to the previous modes of the technical\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 11:49:44 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Hanjani", "Elahe Moghimi", ""], ["Javanmard", "Mahdi", ""]]}, {"id": "1211.4521", "submitter": "Tyler Clemons Mr", "authors": "Tyler Clemons, S. M. Faisal, Shirish Tatikonda, Charu Aggarawl, and\n  Srinivasan Parthasarathy", "title": "Hash in a Flash: Hash Tables for Solid State Devices", "comments": "16 pages 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, information retrieval algorithms have taken center stage for\nextracting important data in ever larger datasets. Advances in hardware\ntechnology have lead to the increasingly wide spread use of flash storage\ndevices. Such devices have clear benefits over traditional hard drives in terms\nof latency of access, bandwidth and random access capabilities particularly\nwhen reading data. There are however some interesting trade-offs to consider\nwhen leveraging the advanced features of such devices. On a relative scale\nwriting to such devices can be expensive. This is because typical flash devices\n(NAND technology) are updated in blocks. A minor update to a given block\nrequires the entire block to be erased, followed by a re-writing of the block.\nOn the other hand, sequential writes can be two orders of magnitude faster than\nrandom writes. In addition, random writes are degrading to the life of the\nflash drive, since each block can support only a limited number of erasures.\nTF-IDF can be implemented using a counting hash table. In general, hash tables\nare a particularly challenging case for the flash drive because this data\nstructure is inherently dependent upon the randomness of the hash function, as\nopposed to the spatial locality of the data. This makes it difficult to avoid\nthe random writes incurred during the construction of the counting hash table\nfor TF-IDF. In this paper, we will study the design landscape for the\ndevelopment of a hash table for flash storage devices. We demonstrate how to\neffectively design a hash table with two related hash functions, one of which\nexhibits a data placement property with respect to the other. Specifically, we\nfocus on three designs based on this general philosophy and evaluate the\ntrade-offs among them along the axes of query performance, insert and update\ntimes and I/O time through an implementation of the TF-IDF algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 17:55:01 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Clemons", "Tyler", ""], ["Faisal", "S. M.", ""], ["Tatikonda", "Shirish", ""], ["Aggarawl", "Charu", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1211.4629", "submitter": "Arash Rafiey", "authors": "Arash Rafiey", "title": "Single Exponential FPT Algorithm for Interval Vertex Deletion and\n  Interval Completion Problem", "comments": "There are faster algorithms available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let G be an input graph with n vertices and m edges and let k be a fixed\nparameter. We provide a single exponential FPT algorithm with running time\nO(c^kn(n+m)), c= min {18,k} that turns graph G into an interval graph by\ndeleting at most k vertices from G. This solves an open problem posed by D.Marx\n[19]. We also provide a single exponential FPT algorithm with running time\nO(c^kn(n+m)), c= min {17,k} that turns G into an interval graph by adding at\nmost$k edges. The first FPT algorithm with run time O(k^{2k}n^3m) appeared in\nSTOC 2007 [24]. Our algorithm is the the first single exponential FPT algorithm\nthat improves the running time of the previous algorithm. The algorithms are\nbased on a structural decomposition of G into smaller subgraphs when G is free\nfrom small interval graph obstructions. The decomposition allows us to manage\nthe search tree more efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 23:55:21 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 22:23:45 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Rafiey", "Arash", ""]]}, {"id": "1211.4853", "submitter": "Gwena\\\"el Joret", "authors": "Gwena\\\"el Joret and Adrian Vetta", "title": "Reducing the rank of a matroid", "comments": "v2: Minor changes made following helpful comments by the referees", "journal-ref": "Discrete Mathematics and Theoretical Computer Science,\n  17/2:143--156, 2015", "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the rank reduction problem for matroids: Given a matroid M and an\ninteger k, find a minimum size subset of elements of M whose removal reduces\nthe rank of M by at least k. When M is a graphical matroid this problem is the\nminimum k-cut problem, which admits a 2-approximation algorithm. In this paper\nwe show that the rank reduction problem for transversal matroids is essentially\nat least as hard to approximate as the densest k-subgraph problem. We also\nprove that, while the problem is easily solvable in polynomial time for\npartition matroids, it is NP-hard when considering the intersection of two\npartition matroids. Our proof shows, in particular, that the maximum vertex\ncover problem is NP-hard on bipartite graphs, which answers an open problem of\nB. Simeone.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 20:38:25 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2015 07:49:26 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Joret", "Gwena\u00ebl", ""], ["Vetta", "Adrian", ""]]}, {"id": "1211.5031", "submitter": "Lukasz Kowalik", "authors": "Marcin Kami\\'nski and {\\L}ukasz Kowalik", "title": "Beyond the Vizing's bound for at most seven colors", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be a simple graph of maximum degree $\\Delta$. The edges of $G$\ncan be colored with at most $\\Delta +1$ colors by Vizing's theorem. We study\nlower bounds on the size of subgraphs of $G$ that can be colored with $\\Delta$\ncolors.\n  Vizing's Theorem gives a bound of $\\frac{\\Delta}{\\Delta+1}|E|$. This is known\nto be tight for cliques $K_{\\Delta+1}$ when $\\Delta$ is even. However, for\n$\\Delta=3$ it was improved to $26/31|E|$ by Albertson and Haas [Parsimonious\nedge colorings, Disc. Math. 148, 1996] and later to $6/7|E|$ by Rizzi\n[Approximating the maximum 3-edge-colorable subgraph problem, Disc. Math. 309,\n2009]. It is tight for $B_3$, the graph isomorphic to a $K_4$ with one edge\nsubdivided.\n  We improve previously known bounds for $\\Delta\\in{3,...,7}$, under the\nassumption that for $\\Delta=3,4,6$ graph $G$ is not isomorphic to $B_3$, $K_5$\nand $K_7$, respectively. For $\\Delta \\geq 4$ these are the first results which\nimprove over the Vizing's bound. We also show a new bound for subcubic\nmultigraphs not isomorphic to $K_3$ with one edge doubled.\n  In the second part, we give approximation algorithms for the Maximum\nk-Edge-Colorable Subgraph problem, where given a graph G (without any bound on\nits maximum degree or other restrictions) one has to find a k-edge-colorable\nsubgraph with maximum number of edges. In particular, when G is simple for\nk=3,4,5,6,7 we obtain approximation ratios of 13/15, 9/11, 19/22, 23/27 and\n22/25, respectively. We also present a 7/9-approximation for k=3 when G is a\nmultigraph. The approximation algorithms follow from a new general framework\nthat can be used for any value of k.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 13:52:26 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2012 19:41:38 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2013 07:53:38 GMT"}, {"version": "v4", "created": "Sun, 2 Mar 2014 13:36:27 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Kami\u0144ski", "Marcin", ""], ["Kowalik", "\u0141ukasz", ""]]}, {"id": "1211.5084", "submitter": "Haitao Wang", "authors": "Haitao Wang and Wuzhou Zhang", "title": "On Top-$k$ Weighted SUM Aggregate Nearest and Farthest Neighbors in the\n  $L_1$ Plane", "comments": "24 pages; this version extends our results in the previous version to\n  more general problem settings, and the title has been changed accordingly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study top-$k$ aggregate (or group) nearest neighbor queries\nusing the weighted SUM operator under the $L_1$ metric in the plane. Given a\nset $P$ of $n$ points, for any query consisting of a set $Q$ of $m$ weighted\npoints and an integer $k$, $ 1 \\le k \\le n$, the top-$k$ aggregate nearest\nneighbor query asks for the $k$ points of $P$ whose aggregate distances to $Q$\nare the smallest, where the aggregate distance of each point $p$ of $P$ to $Q$\nis the sum of the weighted distances from $p$ to all points of $Q$. We build an\n$O(n\\log n\\log\\log n)$-size data structure in $O(n\\log n \\log\\log n)$ time,\nsuch that each top-$k$ query can be answered in $O(m\\log m+(k+m)\\log^2 n)$\ntime. We also obtain other results with trade-off between preprocessing and\nquery. Even for the special case where $k=1$, our results are better than the\npreviously best method (in PODS 2012), which requires $O(n\\log^2 n)$\npreprocessing time, $O(n\\log^2 n)$ space, and $O(m^2\\log^3 n)$ query time. In\naddition, for the one-dimensional version of this problem, our approach can\nbuild an $O(n)$-size data structure in $O(n\\log n)$ time that can support\n$O(\\min\\{k,\\log m\\}\\cdot m+k+\\log n)$ time queries. Further, we extend our\ntechniques to the top-$k$ aggregate farthest neighbor queries, with the same\nbounds.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 16:48:06 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2013 22:08:51 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2013 05:48:46 GMT"}, {"version": "v4", "created": "Sun, 18 Aug 2013 17:31:38 GMT"}, {"version": "v5", "created": "Sat, 29 Nov 2014 00:40:32 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Wang", "Haitao", ""], ["Zhang", "Wuzhou", ""]]}, {"id": "1211.5108", "submitter": "Alessio Langiu", "authors": "Maxime Crochemore, Alessio Langiu and Filippo Mignosi", "title": "The Rightmost Equal-Cost Position Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LZ77-based compression schemes compress the input text by replacing factors\nin the text with an encoded reference to a previous occurrence formed by the\ncouple (length, offset). For a given factor, the smallest is the offset, the\nsmallest is the resulting compression ratio. This is optimally achieved by\nusing the rightmost occurrence of a factor in the previous text. Given a cost\nfunction, for instance the minimum number of bits used to represent an integer,\nwe define the Rightmost Equal-Cost Position (REP) problem as the problem of\nfinding one of the occurrences of a factor which cost is equal to the cost of\nthe rightmost one. We present the Multi-Layer Suffix Tree data structure that,\nfor a text of length n, at any time i, it provides REP(LPF) in constant time,\nwhere LPF is the longest previous factor, i.e. the greedy phrase, a reference\nto the list of REP({set of prefixes of LPF}) in constant time and REP(p) in\ntime O(|p| log log n) for any given pattern p.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 18:22:39 GMT"}], "update_date": "2012-11-22", "authors_parsed": [["Crochemore", "Maxime", ""], ["Langiu", "Alessio", ""], ["Mignosi", "Filippo", ""]]}, {"id": "1211.5184", "submitter": "Zhuojie Zhou", "authors": "Zhuojie Zhou, Nan Zhang, Zhiguo Gong, Gautam Das", "title": "Faster Random Walks By Rewiring Online Social Networks On-The-Fly", "comments": "15 pages, 14 figure, technical report for ICDE2013 paper. Appendix\n  has all the theorems' proofs; ICDE'2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many online social networks feature restrictive web interfaces which only\nallow the query of a user's local neighborhood through the interface. To enable\nanalytics over such an online social network through its restrictive web\ninterface, many recent efforts reuse the existing Markov Chain Monte Carlo\nmethods such as random walks to sample the social network and support analytics\nbased on the samples. The problem with such an approach, however, is the large\namount of queries often required (i.e., a long \"mixing time\") for a random walk\nto reach a desired (stationary) sampling distribution.\n  In this paper, we consider a novel problem of enabling a faster random walk\nover online social networks by \"rewiring\" the social network on-the-fly.\nSpecifically, we develop Modified TOpology (MTO)-Sampler which, by using only\ninformation exposed by the restrictive web interface, constructs a \"virtual\"\noverlay topology of the social network while performing a random walk, and\nensures that the random walk follows the modified overlay topology rather than\nthe original one. We show that MTO-Sampler not only provably enhances the\nefficiency of sampling, but also achieves significant savings on query cost\nover real-world online social networks such as Google Plus, Epinion etc.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2012 01:57:20 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Zhou", "Zhuojie", ""], ["Zhang", "Nan", ""], ["Gong", "Zhiguo", ""], ["Das", "Gautam", ""]]}, {"id": "1211.5350", "submitter": "Alessio Langiu", "authors": "Maxime Crochemore, Alessio Langiu and Filippo Mignosi", "title": "Note on the Greedy Parsing Optimality for Dictionary-Based Text\n  Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic dictionary-based compression schemes are the most daily used data\ncompression schemes since they appeared in the foundational papers of Ziv and\nLempel in 1977, commonly referred to as LZ77. Their work is the base of\nDeflate, gZip, WinZip, 7Zip and many others compression software. All of those\ncompression schemes use variants of the greedy approach to parse the text into\ndictionary phrases. Greedy parsing optimality was proved by Cohn et al. (1996)\nfor fixed length code and unbounded dictionaries. The optimality of the greedy\nparsing was never proved for bounded size dictionary which actually all of\nthose schemes require. We define the suffix-closed property for dynamic\ndictionaries and we show that any LZ77-based dictionary, including the bounded\nvariants, satisfy this property. Under this condition we prove the optimality\nof the greedy parsing as a variant of the proof by Cohn et al.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2012 18:25:56 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Crochemore", "Maxime", ""], ["Langiu", "Alessio", ""], ["Mignosi", "Filippo", ""]]}, {"id": "1211.5353", "submitter": "Roberto Konow", "authors": "Roberto Konow and Gonzalo Navarro", "title": "Faster Compact Top-k Document Retrieval", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An optimal index solving top-k document retrieval [Navarro and Nekrich,\nSODA12] takes O(m + k) time for a pattern of length m, but its space is at\nleast 80n bytes for a collection of n symbols. We reduce it to 1.5n to 3n\nbytes, with O(m+(k+log log n) log log n) time, on typical texts. The index is\nup to 25 times faster than the best previous compressed solutions, and requires\nat most 5% more space in practice (and in some cases as little as one half).\nApart from replacing classical by compressed data structures, our main idea is\nto replace suffix tree sampling by frequency thresholding to achieve\ncompression.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2012 18:58:27 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Konow", "Roberto", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1211.5389", "submitter": "Gabriele Fici", "authors": "Gabriele Fici, Thierry Lecroq, Arnaud Lefebvre and Elise Prieur-Gaston", "title": "Algorithms for Computing Abelian Periods of Words", "comments": "Accepted for publication in Discrete Applied Mathematics", "journal-ref": "Discrete Applied Mathematics 163: 287-297 (2014)", "doi": "10.1016/j.dam.2013.08.021", "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constantinescu and Ilie (Bulletin EATCS 89, 167--170, 2006) introduced the\nnotion of an \\emph{Abelian period} of a word. A word of length $n$ over an\nalphabet of size $\\sigma$ can have $\\Theta(n^{2})$ distinct Abelian periods.\nThe Brute-Force algorithm computes all the Abelian periods of a word in time\n$O(n^2 \\times \\sigma)$ using $O(n \\times \\sigma)$ space. We present an off-line\nalgorithm based on a $\\sel$ function having the same worst-case theoretical\ncomplexity as the Brute-Force one, but outperforming it in practice. We then\npresent on-line algorithms that also enable to compute all the Abelian periods\nof all the prefixes of $w$.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2012 22:32:07 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2013 17:24:29 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Fici", "Gabriele", ""], ["Lecroq", "Thierry", ""], ["Lefebvre", "Arnaud", ""], ["Prieur-Gaston", "Elise", ""]]}, {"id": "1211.5414", "submitter": "Daniel Hsu", "authors": "Daniel Hsu and Sham M. Kakade and Tong Zhang", "title": "Analysis of a randomized approximation scheme for matrix multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note gives a simple analysis of a randomized approximation scheme for\nmatrix multiplication proposed by Sarlos (2006) based on a random rotation\nfollowed by uniform column sampling. The result follows from a matrix version\nof Bernstein's inequality and a tail inequality for quadratic forms in\nsubgaussian random vectors.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2012 06:11:54 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""], ["Zhang", "Tong", ""]]}, {"id": "1211.5433", "submitter": "Emanuele Giaquinta", "authors": "Emanuele Giaquinta and Szymon Grabowski and Kimmo Fredriksson", "title": "Approximate pattern matching with k-mismatches in packed text", "comments": "This paper is an extended version of the article that appeared in\n  Information Processing Letters 113(19-21):693-697 (2013),\n  http://dx.doi.org/10.1016/j.ipl.2013.07.002", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given strings $P$ of length $m$ and $T$ of length $n$ over an alphabet of\nsize $\\sigma$, the string matching with $k$-mismatches problem is to find the\npositions of all the substrings in $T$ that are at Hamming distance at most $k$\nfrom $P$. If $T$ can be read only one character at the time the best known\nbounds are $O(n\\sqrt{k\\log k})$ and $O(n + n\\sqrt{k/w}\\log k)$ in the word-RAM\nmodel with word length $w$. In the RAM models (including $AC^0$ and word-RAM)\nit is possible to read up to $\\floor{w / \\log \\sigma}$ characters in constant\ntime if the characters of $T$ are encoded using $\\ceil{\\log \\sigma}$ bits. The\nonly solution for $k$-mismatches in packed text works in $O((n \\log\\sigma/\\log\nn)\\ceil{m \\log (k + \\log n / \\log\\sigma) / w} + n^{\\varepsilon})$ time, for any\n$\\varepsilon > 0$. We present an algorithm that runs in time\n$O(\\frac{n}{\\floor{w/(m\\log\\sigma)}} (1 + \\log \\min(k,\\sigma) \\log m /\n\\log\\sigma))$ in the $AC^0$ model if $m=O(w / \\log\\sigma)$ and $T$ is given\npacked. We also describe a simpler variant that runs in time\n$O(\\frac{n}{\\floor{w/(m\\log\\sigma)}}\\log \\min(m, \\log w / \\log\\sigma))$ in the\nword-RAM model. The algorithms improve the existing bound for $w =\n\\Omega(\\log^{1+\\epsilon}n)$, for any $\\epsilon > 0$. Based on the introduced\ntechnique, we present algorithms for several other approximate matching\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2012 08:30:45 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2013 12:02:24 GMT"}, {"version": "v3", "created": "Wed, 31 Jul 2013 12:45:11 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Giaquinta", "Emanuele", ""], ["Grabowski", "Szymon", ""], ["Fredriksson", "Kimmo", ""]]}, {"id": "1211.5729", "submitter": "Xiaojun Zhu", "authors": "Xiaojun Zhu, Qun Li, Weizhen Mao, Guihai Chen", "title": "Online Vector Scheduling and Generalized Load Balancing", "comments": "This work has been accepted to JPDC. Please refer to\n  http://dx.doi.org/10.1016/j.jpdc.2013.12.006", "journal-ref": null, "doi": "10.1016/j.jpdc.2013.12.006", "report-no": "WM-CS-2012-01", "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a polynomial time reduction from vector scheduling problem (VS) to\ngeneralized load balancing problem (GLB). This reduction gives the first\nnon-trivial online algorithm for VS where vectors come in an online fashion.\nThe online algorithm is very simple in that each vector only needs to minimize\nthe $L_{\\ln(md)}$ norm of the resulting load when it comes, where $m$ is the\nnumber of partitions and $d$ is the dimension of vectors. It has an\napproximation bound of $e\\log(md)$, which is in $O(\\ln(md))$, so it also\nimproves the $O(\\ln^2d)$ bound of the existing polynomial time algorithm for\nVS. Additionally, the reduction shows that GLB does not have constant\napproximation algorithms that run in polynomial time unless $P=NP$.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2012 05:20:29 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2014 08:53:59 GMT"}], "update_date": "2014-01-15", "authors_parsed": [["Zhu", "Xiaojun", ""], ["Li", "Qun", ""], ["Mao", "Weizhen", ""], ["Chen", "Guihai", ""]]}, {"id": "1211.5933", "submitter": "Yixin Cao", "authors": "Yixin Cao and D\\'aniel Marx", "title": "Interval Deletion is Fixed-Parameter Tractable", "comments": "Final version, to appear in ACM Transactions on Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the minimum \\emph{interval deletion} problem, which asks for the\nremoval of a set of at most $k$ vertices to make a graph of $n$ vertices into\nan interval graph. We present a parameterized algorithm of runtime $10^k \\cdot\nn^{O(1)}$ for this problem, that is, we show the problem is fixed-parameter\ntractable.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 12:42:52 GMT"}, {"version": "v2", "created": "Tue, 14 May 2013 14:53:58 GMT"}, {"version": "v3", "created": "Tue, 6 May 2014 19:33:50 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Cao", "Yixin", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1211.6194", "submitter": "EPTCS", "authors": "Alexandre David, Lasse Jacobsen, Morten Jacobsen, Ji\\v{r}\\'i Srba", "title": "A Forward Reachability Algorithm for Bounded Timed-Arc Petri Nets", "comments": "In Proceedings SSV 2012, arXiv:1211.5873", "journal-ref": "EPTCS 102, 2012, pp. 125-140", "doi": "10.4204/EPTCS.102.12", "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timed-arc Petri nets (TAPN) are a well-known time extension of the Petri net\nmodel and several translations to networks of timed automata have been proposed\nfor this model. We present a direct, DBM-based algorithm for forward\nreachability analysis of bounded TAPNs extended with transport arcs, inhibitor\narcs and age invariants. We also give a complete proof of its correctness,\nincluding reduction techniques based on symmetries and extrapolation. Finally,\nwe augment the algorithm with a novel state-space reduction technique\nintroducing a monotonic ordering on markings and prove its soundness even in\nthe presence of monotonicity-breaking features like age invariants and\ninhibitor arcs. We implement the algorithm within the model-checker TAPAAL and\nthe experimental results document an encouraging performance compared to\nverification approaches that translate TAPN models to UPPAAL timed automata.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2012 02:37:14 GMT"}], "update_date": "2012-11-28", "authors_parsed": [["David", "Alexandre", ""], ["Jacobsen", "Lasse", ""], ["Jacobsen", "Morten", ""], ["Srba", "Ji\u0159\u00ed", ""]]}, {"id": "1211.6195", "submitter": "EPTCS", "authors": "Kenneth Y. J{\\o}rgensen, Kim G. Larsen, Ji\\v{r}\\'i Srba", "title": "Time-Darts: A Data Structure for Verification of Closed Timed Automata", "comments": "In Proceedings SSV 2012, arXiv:1211.5873", "journal-ref": "EPTCS 102, 2012, pp. 141-155", "doi": "10.4204/EPTCS.102.13", "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic data structures for model checking timed systems have been subject\nto a significant research, with Difference Bound Matrices (DBMs) still being\nthe preferred data structure in several mature verification tools. In\ncomparison, discretization offers an easy alternative, with all operations\nhaving linear-time complexity in the number of clocks, and yet valid for a\nlarge class of closed systems. Unfortunately, fine-grained discretization\ncauses itself a state-space explosion. We introduce a new data structure called\ntime-darts for the symbolic representation of state-spaces of timed automata.\nCompared with the complete discretization, a single time-dart allows to\nrepresent an arbitrary large set of states, yet the time complexity of\noperations on time-darts remain linear in the number of clocks. We prove the\ncorrectness of the suggested reachability algorithm and perform several\nexperiments in order to compare the performance of time-darts and the complete\ndiscretization. The main conclusion is that in all our experiments the\ntime-dart method outperforms the complete discretization and it scales\nsignificantly better for models with larger constants.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2012 02:37:21 GMT"}], "update_date": "2012-11-28", "authors_parsed": [["J\u00f8rgensen", "Kenneth Y.", ""], ["Larsen", "Kim G.", ""], ["Srba", "Ji\u0159\u00ed", ""]]}, {"id": "1211.6216", "submitter": "Jos\\'e Verschae", "authors": "Nicole Megow and Jos\\'e Verschae", "title": "Dual techniques for scheduling on a machine with varying speed", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study scheduling problems on a machine with varying speed. Assuming a\nknown speed function we ask for a cost-efficient scheduling solution. Our main\nresult is a PTAS for minimizing the total weighted completion time in this\nsetting. This also implies a PTAS for the closely related problem of scheduling\nto minimize generalized global cost functions. The key to our results is a\nre-interpretation of the problem within the well-known two-dimensional Gantt\nchart: instead of the standard approach of scheduling in the {\\em\ntime-dimension}, we construct scheduling solutions in the weight-dimension.\n  We also consider a dynamic problem variant in which deciding upon the speed\nis part of the scheduling problem and we are interested in the tradeoff between\nscheduling cost and speed-scaling cost, which is typically the energy\nconsumption. We observe that the optimal order is independent of the energy\nconsumption and that the problem can be reduced to the setting where the speed\nof the machine is fixed, and thus admits a PTAS. Furthermore, we provide an\nFPTAS for the NP-hard problem variant in which the machine can run only on a\nfixed number of discrete speeds. Finally, we show how our results can be used\nto obtain a~$(2+\\eps)$-approximation for scheduling preemptive jobs with\nrelease dates on multiple identical parallel machines.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2012 05:45:55 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2013 23:22:12 GMT"}, {"version": "v3", "created": "Tue, 4 Mar 2014 21:20:25 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Megow", "Nicole", ""], ["Verschae", "Jos\u00e9", ""]]}, {"id": "1211.6315", "submitter": "Petr  Kuznetsov", "authors": "Petr Kuznetsov and Sathya Peri", "title": "Non-Interference and Local Correctness in Transactional Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactional memory promises to make concurrent programming tractable and\nefficient by allowing the user to assemble sequences of actions in atomic\ntransactions with all-or-nothing semantics. It is believed that, by its very\nvirtue, transactional memory must ensure that all committed transactions\nconstitute a serial execution respecting the real-time order. In contrast,\naborted or incomplete transactions should not \"take effect.\" But what does \"not\ntaking effect\" mean exactly?\n  It seems natural to expect that aborted or incomplete transactions do not\nappear in the global serial execution, and, thus, no committed transaction can\nbe affected by them. We investigate another, less obvious, feature of \"not\ntaking effect\" called non-interference: aborted or incomplete transactions\nshould not force any other transaction to abort. In the strongest form of\nnon-interference that we explore in this paper, by removing a subset of aborted\nor incomplete transactions from the history, we should not be able to turn an\naborted transaction into a committed one without violating the correctness\ncriterion.\n  We show that non-interference is, in a strict sense, not implementable with\nrespect to the popular criterion of opacity that requires all transactions (be\nthey committed, aborted or incomplete) to witness the same global serial\nexecution. In contrast, when we only require local correctness,\nnon-interference is implementable. Informally, a correctness criterion is local\nif it only requires that every transaction can be serialized along with (a\nsubset of) the transactions committed before its last event (aborted or\nincomplete transactions ignored). We give a few examples of local correctness\nproperties, including the recently proposed criterion of virtual world\nconsistency, and present a simple though efficient implementation that\nsatisfies non-interference and local opacity.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2012 14:31:06 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2012 22:15:53 GMT"}, {"version": "v3", "created": "Fri, 11 Jan 2013 12:29:37 GMT"}, {"version": "v4", "created": "Mon, 27 May 2013 13:51:56 GMT"}, {"version": "v5", "created": "Sat, 12 Oct 2013 12:23:21 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Kuznetsov", "Petr", ""], ["Peri", "Sathya", ""]]}, {"id": "1211.6553", "submitter": "Kurt Mehlhorn", "authors": "Kurt Mehlhorn and Adrian Neumann and Jens M. Schmidt", "title": "Certifying 3-Edge-Connectivity", "comments": "29 pages in Algorithmica, 2015", "journal-ref": null, "doi": "10.1007/s00453-015-0075-x", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a certifying algorithm that tests graphs for 3-edge-connectivity;\nthe algorithm works in linear time. If the input graph is not 3-edge-connected,\nthe algorithm returns a 2-edge-cut. If it is 3-edge-connected, it returns a\nconstruction sequence that constructs the input graph from the graph with two\nvertices and three parallel edges using only operations that (obviously)\npreserve 3-edge-connectivity. Additionally, we show how compute and certify the\n3-edge-connected components and a cactus representation of the 2-cuts in linear\ntime. For 3-vertex-connectivity, we show how to compute the 3-vertex-connected\ncomponents of a 2-connected graph.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 09:27:24 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2013 12:04:05 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2015 15:46:05 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Mehlhorn", "Kurt", ""], ["Neumann", "Adrian", ""], ["Schmidt", "Jens M.", ""]]}, {"id": "1211.6656", "submitter": "Bruno Escoffier", "authors": "Bruno Escoffier, EunJung Kim and Vangelis Th. Paschos", "title": "Subexponential and FPT-time Inapproximability of Independent Set and\n  Related Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fixed-parameter algorithms, approximation algorithms and moderately\nexponential algorithms are three major approaches to algorithms design. While\neach of them being very active in its own, there is an increasing attention to\nthe connection between different approaches. In particular, whether Maximum\nIndependent Set would be better approximable once endowed with\nsubexponential-time or FPT-time is a central question. In this paper, we\npresent a strong link between the linear PCP conjecture and the\ninapproximability, thus partially answering this question.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 16:56:19 GMT"}], "update_date": "2012-11-29", "authors_parsed": [["Escoffier", "Bruno", ""], ["Kim", "EunJung", ""], ["Paschos", "Vangelis Th.", ""]]}, {"id": "1211.6969", "submitter": "Abolfazl Ramezanpour", "authors": "A. Ramezanpour", "title": "Computing loop corrections by message passing", "comments": "12 pages, 4 figures, a bit expanded and typos corrected", "journal-ref": "Phys. Rev. E 87, 060103(R) (2013)", "doi": "10.1103/PhysRevE.87.060103", "report-no": null, "categories": "cond-mat.stat-mech cond-mat.dis-nn cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any spanning tree in a loopy interaction graph can be used for communicating\nthe effect of the loopy interactions by introducing messages that are passed\nalong the edges in the spanning tree. This defines an exact mapping of the\nproblem on the loopy interaction graph onto an extended problem on a tree\ninteraction graph, where the thermodynamic quantities can be computed by a\nmessage-passing algorithm based on the Bethe equations. We propose an\napproximation loop correction algorithm for the Ising model relying on the\nabove representation of the problem. The algorithm deals at the same time with\nthe short and long loops, and can be used to obtain upper and lower bounds for\nthe free energy.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 16:31:37 GMT"}, {"version": "v2", "created": "Thu, 9 May 2013 10:17:25 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2013 12:43:10 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Ramezanpour", "A.", ""]]}, {"id": "1211.7110", "submitter": "Henning Ulfarsson", "authors": "Hjalti Magnusson, Henning Ulfarsson", "title": "Algorithms for discovering and proving theorems about permutation\n  patterns", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm, called BiSC, that describes the patterns avoided by\na given set of permutations. It automatically conjectures the statements of\nknown theorems such as the descriptions of stack-sortable (Knuth 1975) and\nWest-2-stack-sortable permutations (West 1990), smooth (Lakshmibai and Sandhya\n1990) and forest-like permutations (Bousquet-Melou and Butler 2007), and simsun\npermutations (Branden and Claesson 2011). The algorithm has also been used to\ndiscover new theorems and conjectures related to Young tableaux,\nWilf-equivalences and sorting devices. We further give algorithms to prove a\ncomplete description of preimages of pattern classes under certain sorting\ndevices. These generalize an algorithm of Claesson and Ulfarsson (2012) and\nallow us to prove a linear time algorithm for finding occurrences of the\npattern 4312.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 22:38:57 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Magnusson", "Hjalti", ""], ["Ulfarsson", "Henning", ""]]}, {"id": "1211.7302", "submitter": "Aaron Roth", "authors": "Zhiyi Huang and Aaron Roth", "title": "Exploiting Metric Structure for Efficient Private Query Release", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of privately answering queries defined on databases\nwhich are collections of points belonging to some metric space. We give simple,\ncomputationally efficient algorithms for answering distance queries defined\nover an arbitrary metric. Distance queries are specified by points in the\nmetric space, and ask for the average distance from the query point to the\npoints contained in the database, according to the specified metric. Our\nalgorithms run efficiently in the database size and the dimension of the space,\nand operate in both the online query release setting, and the offline setting\nin which they must in polynomial time generate a fixed data structure which can\nanswer all queries of interest. This represents one of the first subclasses of\nlinear queries for which efficient algorithms are known for the private query\nrelease problem, circumventing known hardness results for generic linear\nqueries.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 16:28:46 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Huang", "Zhiyi", ""], ["Roth", "Aaron", ""]]}]