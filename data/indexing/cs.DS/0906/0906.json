[{"id": "0906.0080", "submitter": "L.T. Handoko", "authors": "Z. Akbar and L.T. Handoko", "title": "Reverse method for labeling the information from semi-structured web\n  pages", "comments": "5 pages, Proceeding of the 2009 International Conference on Signal\n  Processing Systems pp. 551-555", "journal-ref": null, "doi": "10.1109/ICSPS.2009.86", "report-no": "FISIKALIPI-09017", "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new technique to infer the structure and extract the tokens of\ndata from the semi-structured web sources which are generated using a\nconsistent template or layout with some implicit regularities. The attributes\nare extracted and labeled reversely from the region of interest of targeted\ncontents. This is in contrast with the existing techniques which always\ngenerate the trees from the root. We argue and show that our technique is\nsimpler, more accurate and effective especially to detect the changes of the\ntemplates of targeted web pages.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2009 14:22:04 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2009 17:36:09 GMT"}], "update_date": "2009-08-06", "authors_parsed": [["Akbar", "Z.", ""], ["Handoko", "L. T.", ""]]}, {"id": "0906.0205", "submitter": "Forrest Sheng Bao", "authors": "Yuanlin Zhang and Forrest Sheng Bao", "title": "A Survey of Tree Convex Sets Test", "comments": "13 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Tree convex sets refer to a collection of sets such that each set in the\ncollection is a subtree of a tree whose nodes are the elements of these sets.\nThey extend the concept of row convex sets each of which is an interval over a\ntotal ordering of the elements of those sets. They have been applied to\nidentify tractable Constraint Satisfaction Problems and Combinatorial Auction\nProblems. Recently, polynomial algorithms have been proposed to recognize tree\nconvex sets. In this paper, we review the materials that are the key to a\nlinear recognition algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2009 03:54:42 GMT"}], "update_date": "2009-06-03", "authors_parsed": [["Zhang", "Yuanlin", ""], ["Bao", "Forrest Sheng", ""]]}, {"id": "0906.0231", "submitter": "Kimikazu Kato", "authors": "Kimikazu Kato and Tikara Hosino", "title": "Solving $k$-Nearest Neighbor Problem on Multiple Graphics Processors", "comments": "5 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The recommendation system is a software system to predict customers' unknown\npreferences from known preferences. In the recommendation system, customers'\npreferences are encoded into vectors, and finding the nearest vectors to each\nvector is an essential part. This vector-searching part of the problem is\ncalled a $k$-nearest neighbor problem. We give an effective algorithm to solve\nthis problem on multiple graphics processor units (GPUs).\n  Our algorithm consists of two parts: an $N$-body problem and a partial sort.\nFor a algorithm of the $N$-body problem, we applied the idea of a known\nalgorithm for the $N$-body problem in physics, although another trick is need\nto overcome the problem of small sized shared memory. For the partial sort, we\ngive a novel GPU algorithm which is effective for small $k$. In our partial\nsort algorithm, a heap is accessed in parallel by threads with a low cost of\nsynchronization. Both of these two parts of our algorithm utilize maximal power\nof coalesced memory access, so that a full bandwidth is achieved.\n  By an experiment, we show that when the size of the problem is large, an\nimplementation of the algorithm on two GPUs runs more than 330 times faster\nthan a single core implementation on a latest CPU. We also show that our\nalgorithm scales well with respect to the number of GPUs.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2009 08:14:13 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2009 06:48:21 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2010 02:23:22 GMT"}], "update_date": "2010-07-16", "authors_parsed": [["Kato", "Kimikazu", ""], ["Hosino", "Tikara", ""]]}, {"id": "0906.0328", "submitter": "Andrea Pasquinucci", "authors": "Andrea Pasquinucci", "title": "Rivisiting Token/Bucket Algorithms in New Applications", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a somehow peculiar Token/Bucket problem which at first sight\nlooks confusing and difficult to solve. The winning approach to solve the\nproblem consists in going back to the simple and traditional methods to solve\ncomputer science problems like the one taught to us by Knuth. Somehow the main\ntrick is to be able to specify clearly what needs to be achieved, and then the\nsolution, even if complex, appears almost by itself.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2009 18:06:45 GMT"}], "update_date": "2009-06-02", "authors_parsed": [["Pasquinucci", "Andrea", ""]]}, {"id": "0906.0350", "submitter": "Mugurel Ionut Andreica", "authors": "Mugurel Ionut Andreica, Eliana-Dina Tirsa, Nicolae Tapus, Florin Pop,\n  Ciprian Mihai Dobre", "title": "Towards a Centralized Scheduling Framework for Communication Flows in\n  Distributed Systems", "comments": null, "journal-ref": "Proceedings of the 17th International Conference on Control\n  Systems and Computer Science (CSCS), vol. 1, pp. 441-448, Bucharest, Romania,\n  26-29 May, 2009. (ISSN: 2066-4451)", "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The overall performance of a distributed system is highly dependent on the\ncommunication efficiency of the system. Although network resources (links,\nbandwidth) are becoming increasingly more available, the communication\nperformance of data transfers involving large volumes of data does not\nnecessarily improve at the same rate. This is due to the inefficient usage of\nthe available network resources. A solution to this problem consists of data\ntransfer scheduling techniques, which manage and allocate the network resources\nin an efficient manner. In this paper we present several online and offline\ndata transfer optimization techniques, in the context of a centrally controlled\ndistributed system.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2009 19:30:03 GMT"}], "update_date": "2009-06-02", "authors_parsed": [["Andreica", "Mugurel Ionut", ""], ["Tirsa", "Eliana-Dina", ""], ["Tapus", "Nicolae", ""], ["Pop", "Florin", ""], ["Dobre", "Ciprian Mihai", ""]]}, {"id": "0906.0376", "submitter": "Mugurel Ionut Andreica", "authors": "Mugurel Ionut Andreica, Eliana-Dina Tirsa, Alexandru Costan, Nicolae\n  Tapus", "title": "Offline Algorithms for Several Network Design, Clustering and QoS\n  Optimization Problems", "comments": null, "journal-ref": "Proceedings of the 17th International Conference on Control\n  Systems and Computer Science (CSCS), vol. 1, pp. 273-280, Bucharest, Romania,\n  26-29 May, 2009. (ISSN: 2066-4451)", "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address several network design, clustering and Quality of\nService (QoS) optimization problems and present novel, efficient, offline\nalgorithms which compute optimal or near-optimal solutions. The QoS\noptimization problems consist of reliability improvement (by computing backup\nshortest paths) and network link upgrades (in order to reduce the latency on\nseveral paths). The network design problems consist of determining small\ndiameter networks, as well as very well connected and regular network\ntopologies. The network clustering problems consider only the restricted model\nof static and mobile path networks, for which we were able to develop optimal\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2009 20:31:10 GMT"}], "update_date": "2009-06-03", "authors_parsed": [["Andreica", "Mugurel Ionut", ""], ["Tirsa", "Eliana-Dina", ""], ["Costan", "Alexandru", ""], ["Tapus", "Nicolae", ""]]}, {"id": "0906.0379", "submitter": "Mugurel Ionut Andreica", "authors": "Mugurel Ionut Andreica, Eliana-Dina Tirsa, Nicolae Tapus", "title": "Data Distribution Optimization using Offline Algorithms and a\n  Peer-to-Peer Small Diameter Tree Architecture with Bounded Node Degrees", "comments": null, "journal-ref": "Proc. of the 17th Intl. Conf. on Control Systems and Computer\n  Science (CSCS), vol. 2, pp. 445-452, 2009 (3rd Intl. Workshop on High\n  Performance Grid Middleware - HiPerGrid). (ISSN: 2066-4451)", "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multicast data transfers occur in many distributed systems and applications\n(e.g. IPTV, Grids, content delivery networks). Because of this, efficient\nmulticast data distribution optimization techniques are required. In the first\npart of this paper we present a small diameter, bounded degree, collaborative\npeer-to-peer multicast tree architecture, which supports dynamic node arrivals\nand departures making local decisions only. The architecture is fault tolerant\nand, at low arrival and departure rates, converges towards a theoretically\noptimal structure. In the second part of the paper we consider several offline\ndata distribution optimization problems, for which we present novel and\ntime-efficient algorithmic solutions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2009 21:07:18 GMT"}], "update_date": "2009-06-03", "authors_parsed": [["Andreica", "Mugurel Ionut", ""], ["Tirsa", "Eliana-Dina", ""], ["Tapus", "Nicolae", ""]]}, {"id": "0906.0391", "submitter": "Ilya Volnyansky", "authors": "Ilya Volnyansky, Vladimir Pestov", "title": "Curse of Dimensionality in Pivot-based Indexes", "comments": "9 pp., 4 figures, latex 2e, a revised submission to the 2nd\n  International Workshop on Similarity Search and Applications, 2009", "journal-ref": "Proc. 2nd Int. Workshop on Similarity Search and Applications\n  (SISAP 2009), Prague, Aug. 29-30, 2009, T. Skopal and P. Zezula (eds.), IEEE\n  Computer Society, Los Alamitos--Washington--Tokyo, 2009, pp. 39-46.", "doi": "10.1109/SISAP.2009.9", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We offer a theoretical validation of the curse of dimensionality in the\npivot-based indexing of datasets for similarity search, by proving, in the\nframework of statistical learning, that in high dimensions no pivot-based\nindexing scheme can essentially outperform the linear scan.\n  A study of the asymptotic performance of pivot-based indexing schemes is\nperformed on a sequence of datasets modeled as samples $X_d$ picked in i.i.d.\nfashion from metric spaces $\\Omega_d$. We allow the size of the dataset $n=n_d$\nto be such that $d$, the ``dimension'', is superlogarithmic but subpolynomial\nin $n$. The number of pivots is allowed to grow as $o(n/d)$. We pick the least\nrestrictive cost model of similarity search where we count each distance\ncalculation as a single computation and disregard the rest.\n  We demonstrate that if the intrinsic dimension of the spaces $\\Omega_d$ in\nthe sense of concentration of measure phenomenon is $O(d)$, then the\nperformance of similarity search pivot-based indexes is asymptotically linear\nin $n$.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2009 00:41:46 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2009 23:50:44 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Volnyansky", "Ilya", ""], ["Pestov", "Vladimir", ""]]}, {"id": "0906.0409", "submitter": "Xin Han", "authors": "Xin Han, Francis Y.L. Chin, Hing-Fung Ting, Guochuan Zhang", "title": "A New Upper Bound on 2D Online Bin Packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2D Online Bin Packing is a fundamental problem in Computer Science and\nthe determination of its asymptotic competitive ratio has attracted great\nresearch attention. In a long series of papers, the lower bound of this ratio\nhas been improved from 1.808, 1.856 to 1.907 and its upper bound reduced from\n3.25, 3.0625, 2.8596, 2.7834 to 2.66013. In this paper, we rewrite the upper\nbound record to 2.5545. Our idea for the improvement is as follows. In SODA\n2002 \\cite{SS03}, Seiden and van Stee proposed an elegant algorithm called $H\n\\otimes B$, comprised of the {\\em Harmonic algorithm} $H$ and the {\\em Improved\nHarmonic algorithm} $B$, for the two-dimensional online bin packing problem and\nproved that the algorithm has an asymptotic competitive ratio of at most\n2.66013. Since the best known online algorithm for one-dimensional bin packing\nis the {\\em Super Harmonic algorithm} \\cite{S02}, a natural question to ask is:\ncould a better upper bound be achieved by using the Super Harmonic algorithm\ninstead of the Improved Harmonic algorithm? However, as mentioned in\n\\cite{SS03}, the previous analysis framework does not work. In this paper, we\ngive a positive answer for the above question. A new upper bound of 2.5545 is\nobtained for 2-dimensional online bin packing. The main idea is to develop new\nweighting functions for the Super Harmonic algorithm and propose new techniques\nto bound the total weight in a rectangular bin.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2009 02:30:11 GMT"}], "update_date": "2009-06-03", "authors_parsed": [["Han", "Xin", ""], ["Chin", "Francis Y. L.", ""], ["Ting", "Hing-Fung", ""], ["Zhang", "Guochuan", ""]]}, {"id": "0906.0422", "submitter": "Natalia Vanetik Dr", "authors": "Natalia Vanetik", "title": "Computing the tree number of a cut-outerplanar graph", "comments": "These results are not incorrect but are covered in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the notion of arboricity of a graph is well-known in graph theory, very\nfew results are dedicated to the minimal number of trees covering the edges of\na graph, called the tree number of a graph.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2009 05:48:48 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 19:49:49 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Vanetik", "Natalia", ""]]}, {"id": "0906.0687", "submitter": "Olga Holtz", "authors": "Olga Holtz, Noam Shomron", "title": "Computational Complexity and Numerical Stability of Linear Problems", "comments": "16 pages; updated to reflect referees' remarks; to appear in\n  Proceedings of the 5th European Congress of Mathematics", "journal-ref": "European Congress of Mathematics Amsterdam, 14-18 July, 2008, EMS\n  Publishing House, pp. 381-400", "doi": "10.4171/077-1/16", "report-no": null, "categories": "cs.CC cs.DS cs.NA math.HO math.NA math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey classical and recent developments in numerical linear algebra,\nfocusing on two issues: computational complexity, or arithmetic costs, and\nnumerical stability, or performance under roundoff error. We present a brief\naccount of the algebraic complexity theory as well as the general error\nanalysis for matrix multiplication and related problems. We emphasize the\ncentral role played by the matrix multiplication problem and discuss historical\nand modern approaches to its solution.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2009 10:51:40 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2009 00:59:14 GMT"}], "update_date": "2010-06-22", "authors_parsed": [["Holtz", "Olga", ""], ["Shomron", "Noam", ""]]}, {"id": "0906.0731", "submitter": "Paul Vitanyi", "authors": "Paul M.B. Vitanyi (CWI, Amsterdam)", "title": "Distributed elections in an Archimedean ring of processors", "comments": null, "journal-ref": "16th ACM Symposium on Theory of Computing, Washington D.C., 1984,\n  542 - 547", "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlimited asynchronism is intolerable in real physically distributed computer\nsystems. Such systems, synchronous or not, use clocks and timeouts. Therefore\nthe magnitudes of elapsed absolute time in the system need to satisfy the axiom\nof Archimedes. Under this restriction of asynchronicity logically\ntime-independent solutions can be derived which are nonetheless better (in\nnumber of message passes) than is possible otherwise. The use of clocks by the\nindividual processors, in elections in a ring of asynchronous processors\nwithout central control, allows a deterministic solution which requires but a\nlinear number of message passes. To obtain the result it has to be assumed that\nthe clocks measure finitely proportional absolute time-spans for their time\nunits, that is, the magnitudes of elapsed time in the ring network satisfy the\naxiom of Archimedes. As a result, some basic subtilities associated with\ndistributed computations are highlighted. For instance, the known nonlinear\nlower bound on the required number of message passes is cracked. For the\nsynchronous case, in which the necessary assumptions hold a fortiori, the\nmethod is -asymptotically- the most efficient one yet, and of optimal order of\nmagnitude. The deterministic algorithm is of -asymptotically- optimal bit\ncomplexity, and, in the synchronous case, also yields an optimal method to\ndetermine the ring size. All of these results improve the known ones.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2009 16:07:56 GMT"}], "update_date": "2009-06-04", "authors_parsed": [["Vitanyi", "Paul M. B.", "", "CWI, Amsterdam"]]}, {"id": "0906.0862", "submitter": "Daniel Karapetyan", "authors": "Gregory Gutin and Daniel Karapetyan", "title": "A Memetic Algorithm for the Multidimensional Assignment Problem", "comments": "14 pages", "journal-ref": "Lecture Notes in Computer Science 5752 (2009) 125-129", "doi": "10.1007/978-3-642-03751-1_12", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Multidimensional Assignment Problem (MAP or s-AP in the case of s\ndimensions) is an extension of the well-known assignment problem. The most\nstudied case of MAP is 3-AP, though the problems with larger values of s have\nalso a number of applications. In this paper we propose a memetic algorithm for\nMAP that is a combination of a genetic algorithm with a local search procedure.\nThe main contribution of the paper is an idea of dynamically adjusted\ngeneration size, that yields an outstanding flexibility of the algorithm to\nperform well for both small and large fixed running times. The results of\ncomputational experiments for several instance families show that the proposed\nalgorithm produces solutions of very high quality in a reasonable time and\noutperforms the state-of-the art 3-AP memetic algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2009 09:44:59 GMT"}], "update_date": "2010-03-30", "authors_parsed": [["Gutin", "Gregory", ""], ["Karapetyan", "Daniel", ""]]}, {"id": "0906.1341", "submitter": "Mugurel Ionut Andreica", "authors": "Madalina Ecaterina Andreica, Mugurel Ionut Andreica, Angela Andreica", "title": "Efficient Algorithms for Several Constrained Activity Scheduling\n  Problems in the Time and Space Domains", "comments": null, "journal-ref": "Proceedings of the 33rd American Romanian Academy of Arts and\n  Sciences' International Congress, vol. 1, pp. 59-63, Sibiu, Romania, 2-5\n  June, 2009. (ISBN: 978-2-553-01433-8)", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider several constrained activity scheduling problems in\nthe time and space domains, like finding activity orderings which optimize the\nvalues of several objective functions (time scheduling) or finding optimal\nlocations where certain types of activities will take place (space scheduling).\nWe present novel, efficient algorithmic solutions for all the considered\nproblems, based on the dynamic programming and greedy techniques. In each case\nwe compute exact, optimal solutions.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2009 09:35:01 GMT"}], "update_date": "2009-06-09", "authors_parsed": [["Andreica", "Madalina Ecaterina", ""], ["Andreica", "Mugurel Ionut", ""], ["Andreica", "Angela", ""]]}, {"id": "0906.1343", "submitter": "Mugurel Ionut Andreica", "authors": "Mugurel Ionut Andreica, Madalina Ecaterina Andreica, Daniel Ardelean", "title": "Efficient Algorithms for Several Constrained Resource Allocation,\n  Management and Discovery Problems", "comments": null, "journal-ref": "Proceedings of the 33rd American Romanian Academy of Arts and\n  Sciences' International Congress, vol. 1, pp. 64-68, Sibiu, Romania, 2-5\n  June, 2009. (ISBN: 978-2-553-01433-8)", "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present efficient algorithmic solutions for several\nconstrained resource allocation, management and discovery problems. We consider\nnew types of resource allocation models and constraints, and we present new\ngeometric techniques which are useful when the resources are mapped to points\ninto a multidimensional feature space. We also consider a resource discovery\nproblem for which we present a guessing game theoretical model.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2009 09:48:40 GMT"}], "update_date": "2009-06-09", "authors_parsed": [["Andreica", "Mugurel Ionut", ""], ["Andreica", "Madalina Ecaterina", ""], ["Ardelean", "Daniel", ""]]}, {"id": "0906.1356", "submitter": "Gregory Gutin", "authors": "G. Gutin, E.J. Kim, S. Szeider, A. Yeo", "title": "A Probabilistic Approach to Problems Parameterized Above or Below Tight\n  Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach for establishing fixed-parameter tractability of\nproblems parameterized above tight lower bounds. To illustrate the approach we\nconsider three problems of this type of unknown complexity that were introduced\nby Mahajan, Raman and Sikdar (J. Comput. Syst. Sci. 75, 2009). We show that a\ngeneralization of one of the problems and non-trivial special cases of the\nother two are fixed-parameter tractable.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2009 13:17:51 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2009 18:46:03 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2009 14:05:19 GMT"}], "update_date": "2009-08-18", "authors_parsed": [["Gutin", "G.", ""], ["Kim", "E. J.", ""], ["Szeider", "S.", ""], ["Yeo", "A.", ""]]}, {"id": "0906.1359", "submitter": "Gregory Gutin", "authors": "G. Gutin, D. Karapetyan, and I. Razgon", "title": "Fixed-Parameter Algorithms in Analysis of Heuristics for Extracting\n  Networks in Linear Programs", "comments": null, "journal-ref": "Lecture Notes in Computer Science 5917 (2009) 222-233", "doi": "10.1007/978-3-642-11269-0", "report-no": null, "categories": "cs.DS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of extracting a maximum-size reflected network in a\nlinear program. This problem has been studied before and a state-of-the-art SGA\nheuristic with two variations have been proposed.\n  In this paper we apply a new approach to evaluate the quality of SGA\\@. In\nparticular, we solve majority of the instances in the testbed to optimality\nusing a new fixed-parameter algorithm, i.e., an algorithm whose runtime is\npolynomial in the input size but exponential in terms of an additional\nparameter associated with the given problem.\n  This analysis allows us to conclude that the the existing SGA heuristic, in\nfact, produces solutions of a very high quality and often reaches the optimal\nobjective values. However, SGA contain two components which leave some space\nfor improvement: building of a spanning tree and searching for an independent\nset in a graph. In the hope of obtaining even better heuristic, we tried to\nreplace both of these components with some equivalent algorithms.\n  We tried to use a fixed-parameter algorithm instead of a greedy one for\nsearching of an independent set. But even the exact solution of this subproblem\nimproved the whole heuristic insignificantly. Hence, the crucial part of SGA is\nbuilding of a spanning tree. We tried three different algorithms, and it\nappears that the Depth-First search is clearly superior to the other ones in\nbuilding of the spanning tree for SGA.\n  Thereby, by application of fixed-parameter algorithms, we managed to check\nthat the existing SGA heuristic is of a high quality and selected the component\nwhich required an improvement. This allowed us to intensify the research in a\nproper direction which yielded a superior variation of SGA.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2009 13:59:43 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2009 15:28:15 GMT"}], "update_date": "2010-03-30", "authors_parsed": [["Gutin", "G.", ""], ["Karapetyan", "D.", ""], ["Razgon", "I.", ""]]}, {"id": "0906.1360", "submitter": "S\\'ilvio Duarte Queir\\'os M.", "authors": "Silvio M. Duarte Queiros", "title": "On the effectiveness of a binless entropy estimator for generalised\n  entropic forms", "comments": "10 pages. 5 Figures. 2 Appendices with 2 figures", "journal-ref": "Phys. Rev. E 80, 062101 (2009)", "doi": "10.1103/PhysRevE.80.062101", "report-no": null, "categories": "cs.IT cs.DS math.IT math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript we discuss the effectiveness of the Kozachenko-Leonenko\nentropy estimator when generalised to cope with entropic forms customarily\napplied to study systems evincing asymptotic scale invariance and dependence\n(either linear or non-linear type). We show that when the variables are\nindependently and identically distributed the estimator is only valuable along\nthe whole domain if the data follow the uniform distribution, whereas for other\ndistributions the estimator is only effectual in the limit of the\nBoltzmann-Gibbs-Shanon entropic form. We also analyse the influence of the\ndependence (linear and non-linear) between variables on the accuracy of the\nestimator between variables. As expected in the last case the estimator looses\nefficiency for the Boltzmann-Gibbs-Shanon entropic form as well.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2009 14:37:40 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2009 12:29:12 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2010 00:01:19 GMT"}], "update_date": "2010-01-04", "authors_parsed": [["Queiros", "Silvio M. Duarte", ""]]}, {"id": "0906.1370", "submitter": "Emanuele Viola", "authors": "Emanuele Viola", "title": "Cell-Probe Lower Bounds for Prefix Sums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that to store n bits x so that each prefix-sum query Sum(i) :=\nsum_{k < i} x_k can be answered by non-adaptively probing q cells of log n\nbits, one needs memory > n + n/log^{O(q)} n. Our bound matches a recent upper\nbound of n + n/log^{Omega(q)} n by Patrascu (FOCS 2008), also non-adaptive. We\nalso obtain a n + n/log^{2^{O(q)}} n lower bound for storing a string of\nbalanced brackets so that each Match(i) query can be answered by non-adaptively\nprobing q cells. To obtain these bounds we show that a too efficient data\nstructure allows us to break the correlations between query answers.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2009 17:25:27 GMT"}], "update_date": "2009-06-09", "authors_parsed": [["Viola", "Emanuele", ""]]}, {"id": "0906.1557", "submitter": "Uri Yovel", "authors": "Asaf Levin, Uri Yovel", "title": "Uniform unweighted set cover: The power of non-oblivious local search", "comments": "31 pages, includes figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are given n base elements and a finite collection of subsets of them. The\nsize of any subset varies between p to k (p < k). In addition, we assume that\nthe input contains all possible subsets of size p. Our objective is to find a\nsubcollection of minimum-cardinality which covers all the elements. This\nproblem is known to be NP-hard. We provide two approximation algorithms for it,\none for the generic case, and an improved one for the special case of (p,k) =\n(2,4). The algorithm for the generic case is a greedy one, based on packing\nphases: at each phase we pick a collection of disjoint subsets covering i new\nelements, starting from i = k down to i = p+1. At a final step we cover the\nremaining base elements by the subsets of size p. We derive the exact\nperformance guarantee of this algorithm for all values of k and p, which is\nless than Hk, where Hk is the k'th harmonic number. However, the algorithm\nexhibits the known improvement methods over the greedy one for the unweighted\nk-set cover problem (in which subset sizes are only restricted not to exceed\nk), and hence it serves as a benchmark for our improved algorithm. The improved\nalgorithm for the special case of (p,k) = (2,4) is based on non-oblivious local\nsearch: it starts with a feasible cover, and then repeatedly tries to replace\nsets of size 3 and 4 so as to maximize an objective function which prefers big\nsets over small ones. For this case, our generic algorithm achieves an\nasymptotic approximation ratio of 1.5 + epsilon, and the local search algorithm\nachieves a better ratio, which is bounded by 1.458333... + epsilon.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2009 18:26:42 GMT"}], "update_date": "2009-06-09", "authors_parsed": [["Levin", "Asaf", ""], ["Yovel", "Uri", ""]]}, {"id": "0906.1849", "submitter": "Subhas Ghosh", "authors": "Subhas Kumar Ghosh (Honeywell Technology Solutions) and Janardan Misra\n  (Honeywell Technology Solutions)", "title": "A Randomized Algorithm for 3-SAT", "comments": null, "journal-ref": null, "doi": "10.1007/s11786-010-0036-3", "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose and analyze a simple randomized algorithm to find a\nsatisfiable assignment for a Boolean formula in conjunctive normal form (CNF)\nhaving at most 3 literals in every clause. Given a k-CNF formula phi on n\nvariables, and alpha in{0,1}^n that satisfies phi, a clause of phi is critical\nif exactly one literal of that clause is satisfied under assignment alpha.\nPaturi et. al. (Chicago Journal of Theoretical Computer Science 1999) proposed\na simple randomized algorithm (PPZ) for k-SAT for which success probability\nincreases with the number of critical clauses (with respect to a fixed\nsatisfiable solution of the input formula). Here, we first describe another\nsimple randomized algorithm DEL which performs better if the number of critical\nclauses are less (with respect to a fixed satisfiable solution of the input\nformula). Subsequently, we combine these two simple algorithms such that the\nsuccess probability of the combined algorithm is maximum of the success\nprobabilities of PPZ and DEL on every input instance. We show that when the\naverage number of clauses per variable that appear as unique true literal in\none or more critical clauses in phi is between 1 and 1.9317, combined algorithm\nperforms better than the PPZ algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2009 04:41:52 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Ghosh", "Subhas Kumar", "", "Honeywell Technology Solutions"], ["Misra", "Janardan", "", "Honeywell Technology Solutions"]]}, {"id": "0906.1953", "submitter": "Serge Gaspers", "authors": "Martin F\\\"urer, Serge Gaspers, Shiva Prasad Kasiviswanathan", "title": "An Exponential Time 2-Approximation Algorithm for Bandwidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bandwidth of a graph G on n vertices is the minimum b such that the\nvertices of G can be labeled from 1 to n such that the labels of every pair of\nadjacent vertices differ by at most b.\n  In this paper, we present a 2-approximation algorithm for the bandwidth\nproblem that takes worst-case O(1.9797^n) time and uses polynomial space. This\nimproves both the previous best 2- and 3-approximation algorithms of Cygan et\nal. which have an O(3^n) and O(2^n) worst-case time bounds, respectively. Our\nalgorithm is based on constructing bucket decompositions of the input graph. A\nbucket decomposition partitions the vertex set of a graph into ordered sets\n(called buckets) of (almost) equal sizes such that all edges are either\nincident to vertices in the same bucket or to vertices in two consecutive\nbuckets. The idea is to find the smallest bucket size for which there exists a\nbucket decomposition. The algorithm uses a simple divide-and-conquer strategy\nalong with dynamic programming to achieve this improved time bound.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2009 15:22:57 GMT"}, {"version": "v2", "created": "Sun, 20 Nov 2011 20:52:28 GMT"}, {"version": "v3", "created": "Sun, 29 Apr 2012 21:13:34 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["F\u00fcrer", "Martin", ""], ["Gaspers", "Serge", ""], ["Kasiviswanathan", "Shiva Prasad", ""]]}, {"id": "0906.2020", "submitter": "Ravishankar Krishnaswamy", "authors": "Anupam Gupta, Ravishankar Krishnaswamy, Amit Kumar, Danny Segev", "title": "Scheduling with Outliers", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": "10.1007/978-3-642-03685-9_12", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classical scheduling problems, we are given jobs and machines, and have to\nschedule all the jobs to minimize some objective function. What if each job has\na specified profit, and we are no longer required to process all jobs -- we can\nschedule any subset of jobs whose total profit is at least a (hard) target\nprofit requirement, while still approximately minimizing the objective\nfunction?\n  We refer to this class of problems as scheduling with outliers. This model\nwas initiated by Charikar and Khuller (SODA'06) on the minimum max-response\ntime in broadcast scheduling. We consider three other well-studied scheduling\nobjectives: the generalized assignment problem, average weighted completion\ntime, and average flow time, and provide LP-based approximation algorithms for\nthem. For the minimum average flow time problem on identical machines, we give\na logarithmic approximation algorithm for the case of unit profits based on\nrounding an LP relaxation; we also show a matching integrality gap. For the\naverage weighted completion time problem on unrelated machines, we give a\nconstant factor approximation. The algorithm is based on randomized rounding of\nthe time-indexed LP relaxation strengthened by the knapsack-cover inequalities.\nFor the generalized assignment problem with outliers, we give a simple\nreduction to GAP without outliers to obtain an algorithm whose makespan is\nwithin 3 times the optimum makespan, and whose cost is at most (1 + \\epsilon)\ntimes the optimal cost.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2009 21:22:22 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Gupta", "Anupam", ""], ["Krishnaswamy", "Ravishankar", ""], ["Kumar", "Amit", ""], ["Segev", "Danny", ""]]}, {"id": "0906.2048", "submitter": "Sungjin Im", "authors": "Chandra Chekuri, Sungjin Im and Benjamin Moseley", "title": "Minimizing Maximum Response Time and Delay Factor in Broadcast\n  Scheduling", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online algorithms for pull-based broadcast scheduling. In this\nsetting there are n pages of information at a server and requests for pages\narrive online. When the server serves (broadcasts) a page p, all outstanding\nrequests for that page are satisfied. We study two related metrics, namely\nmaximum response time (waiting time) and maximum delay-factor and their\nweighted versions. We obtain the following results in the worst-case online\ncompetitive model.\n  - We show that FIFO (first-in first-out) is 2-competitive even when the page\nsizes are different. Previously this was known only for unit-sized pages [10]\nvia a delicate argument. Our proof differs from [10] and is perhaps more\nintuitive.\n  - We give an online algorithm for maximum delay-factor that is\nO(1/eps^2)-competitive with (1+\\eps)-speed for unit-sized pages and with\n(2+\\eps)-speed for different sized pages. This improves on the algorithm in\n[12] which required (2+\\eps)-speed and (4+\\eps)-speed respectively. In addition\nwe show that the algorithm and analysis can be extended to obtain the same\nresults for maximum weighted response time and delay factor.\n  - We show that a natural greedy algorithm modeled after LWF\n(Longest-Wait-First) is not O(1)-competitive for maximum delay factor with any\nconstant speed even in the setting of standard scheduling with unit-sized jobs.\nThis complements our upper bound and demonstrates the importance of the\ntradeoff made in our algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2009 07:24:52 GMT"}], "update_date": "2009-06-12", "authors_parsed": [["Chekuri", "Chandra", ""], ["Im", "Sungjin", ""], ["Moseley", "Benjamin", ""]]}, {"id": "0906.2395", "submitter": "Benjmain Moseley", "authors": "Chandra Chekuri, Sungjin Im, Benjamin Moseley", "title": "Longest Wait First for Broadcast Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online algorithms for broadcast scheduling. In the pull-based\nbroadcast model there are $n$ unit-sized pages of information at a server and\nrequests arrive online for pages. When the server transmits a page $p$, all\noutstanding requests for that page are satisfied. The longest-wait-first} (LWF)\nalgorithm is a natural algorithm that has been shown to have good empirical\nperformance. In this paper we make two main contributions to the analysis of\nLWF and broadcast scheduling. \\begin{itemize} \\item We give an intuitive and\neasy to understand analysis of LWF which shows that it is\n$O(1/\\eps^2)$-competitive for average flow-time with $(4+\\eps)$ speed. Using a\nmore involved analysis, we show that LWF is $O(1/\\eps^3)$-competitive for\naverage flow-time with $(3.4+\\epsilon)$ speed. \\item We show that a natural\nextension of LWF is O(1)-speed O(1)-competitive for more general objective\nfunctions such as average delay-factor and $L_k$ norms of delay-factor (for\nfixed $k$). \\end{itemize}\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2009 18:27:07 GMT"}], "update_date": "2009-06-15", "authors_parsed": [["Chekuri", "Chandra", ""], ["Im", "Sungjin", ""], ["Moseley", "Benjamin", ""]]}, {"id": "0906.2448", "submitter": "Karthekeyan Chandrasekaran", "authors": "Karthekeyan Chandrasekaran, Amit Deshpande, Santosh Vempala", "title": "The Limit of Convexity Based Isoperimetry: Sampling Harmonic-Concave\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logconcave functions represent the current frontier of efficient algorithms\nfor sampling, optimization and integration in R^n. Efficient sampling\nalgorithms to sample according to a probability density (to which the other two\nproblems can be reduced) relies on good isoperimetry which is known to hold for\narbitrary logconcave densities. In this paper, we extend this frontier in two\nways: first, we characterize convexity-like conditions that imply good\nisoperimetry, i.e., what condition on function values along every line\nguarantees good isoperimetry? The answer turns out to be the set of\n(1/(n-1))-harmonic concave functions in R^n; we also prove that this is the\nbest possible characterization along every line, of functions having good\nisoperimetry. Next, we give the first efficient algorithm for sampling\naccording to such functions with complexity depending on a smoothness\nparameter. Further, noting that the multivariate Cauchy density is an important\ndistribution in this class, we exploit certain properties of the Cauchy density\nto give an efficient sampling algorithm based on random walks with a mixing\ntime that matches the current best bounds known for sampling logconcave\nfunctions.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2009 05:15:53 GMT"}], "update_date": "2009-06-16", "authors_parsed": [["Chandrasekaran", "Karthekeyan", ""], ["Deshpande", "Amit", ""], ["Vempala", "Santosh", ""]]}, {"id": "0906.2461", "submitter": "Erik Demaine", "authors": "Erik D. Demaine, Martin L. Demaine, Vi Hart, John Iacono, Stefan\n  Langerman, Joseph O'Rourke", "title": "Continuous Blooming of Convex Polyhedra", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct the first two continuous bloomings of all convex polyhedra.\nFirst, the source unfolding can be continuously bloomed. Second, any unfolding\nof a convex polyhedron can be refined (further cut, by a linear number of cuts)\nto have a continuous blooming.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2009 09:27:18 GMT"}], "update_date": "2009-06-16", "authors_parsed": [["Demaine", "Erik D.", ""], ["Demaine", "Martin L.", ""], ["Hart", "Vi", ""], ["Iacono", "John", ""], ["Langerman", "Stefan", ""], ["O'Rourke", "Joseph", ""]]}, {"id": "0906.2466", "submitter": "Iftah Gamzu", "authors": "Chandra Chekuri, Iftah Gamzu", "title": "Truthful Mechanisms via Greedy Iterative Packing", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": "10.1007/978-3-642-03685-9_5", "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important research thread in algorithmic game theory studies the design of\nefficient truthful mechanisms that approximate the optimal social welfare. A\nfundamental question is whether an \\alpha-approximation algorithm translates\ninto an \\alpha-approximate truthful mechanism. It is well-known that plugging\nan \\alpha-approximation algorithm into the VCG technique may not yield a\ntruthful mechanism. Thus, it is natural to investigate properties of\napproximation algorithms that enable their use in truthful mechanisms.\n  The main contribution of this paper is to identify a useful and natural\nproperty of approximation algorithms, which we call loser-independence; this\nproperty is applicable in the single-minded and single-parameter settings.\nIntuitively, a loser-independent algorithm does not change its outcome when the\nbid of a losing agent increases, unless that agent becomes a winner. We\ndemonstrate that loser-independent algorithms can be employed as sub-procedures\nin a greedy iterative packing approach while preserving monotonicity. A greedy\niterative approach provides a good approximation in the context of maximizing a\nnon-decreasing submodular function subject to independence constraints. Our\nframework gives rise to truthful approximation mechanisms for various problems.\nNotably, some problems arise in online mechanism design.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2009 10:39:42 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Chekuri", "Chandra", ""], ["Gamzu", "Iftah", ""]]}, {"id": "0906.2671", "submitter": "Christian David", "authors": "Catherine Doss (LJLL), Mich\\`ele Thieullen (PMA)", "title": "Oscillations and Random Perturbations of a FitzHugh-Nagumo System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stochastic perturbation of a FitzHugh-Nagumo system. We show\nthat it is possible to generate oscillations for values of parameters which do\nnot allow oscillations for the deterministic system. We also study the\nappearance of a new equilibrium point and new bifurcation parameters due to the\nnoisy component.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2009 12:29:32 GMT"}], "update_date": "2009-06-16", "authors_parsed": [["Doss", "Catherine", "", "LJLL"], ["Thieullen", "Mich\u00e8le", "", "PMA"]]}, {"id": "0906.2738", "submitter": "Yakov Nekrich", "authors": "Yakov Nekrich", "title": "Data Structures for Approximate Range Counting", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new data structures for approximately counting the number of\npoints in orthogonal range.\n  There is a deterministic linear space data structure that supports updates in\nO(1) time and approximates the number of elements in a 1-D range up to an\nadditive term $k^{1/c}$ in $O(\\log \\log U\\cdot\\log \\log n)$ time, where $k$ is\nthe number of elements in the answer, $U$ is the size of the universe and $c$\nis an arbitrary fixed constant. We can estimate the number of points in a\ntwo-dimensional orthogonal range up to an additive term $ k^{\\rho}$ in $O(\\log\n\\log U+ (1/\\rho)\\log\\log n)$ time for any $\\rho>0$. We can estimate the number\nof points in a three-dimensional orthogonal range up to an additive term\n$k^{\\rho}$ in $O(\\log \\log U + (\\log\\log n)^3+ (3^v)\\log\\log n)$ time for\n$v=\\log \\frac{1}{\\rho}/\\log {3/2}+2$.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2009 16:25:04 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2009 09:30:57 GMT"}], "update_date": "2009-10-05", "authors_parsed": [["Nekrich", "Yakov", ""]]}, {"id": "0906.2960", "submitter": "Daniel Karapetyan", "authors": "Daniel Karapetyan and Gregory Gutin and Boris Goldengorin", "title": "Empirical evaluation of construction heuristics for the multidimensional\n  assignment problem", "comments": "15 pages", "journal-ref": "Text in Algorithmics 11 (2009) 107-122", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multidimensional assignment problem (MAP) (abbreviated s-AP in the case\nof s dimensions) is an extension of the well-known assignment problem. The most\nstudied case of MAP is 3-AP, though the problems with larger values of s have\nalso a number of applications. In this paper we consider four fast construction\nheuristics for MAP. One of the heuristics is new. A modification of the\nheuristics is proposed to optimize the access to slow computer memory. The\nresults of computational experiments for several instance families are provided\nand discussed.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2009 15:47:17 GMT"}], "update_date": "2012-05-17", "authors_parsed": [["Karapetyan", "Daniel", ""], ["Gutin", "Gregory", ""], ["Goldengorin", "Boris", ""]]}, {"id": "0906.3056", "submitter": "Chi Zhang", "authors": "Chi Zhang, Gang Wang, Xiaoguang Liu, Jing Liu", "title": "Approximating Scheduling Machines with Capacity Constraints", "comments": "this is a correction of paper to appear at FAW2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Scheduling Machines with Capacity Constraints problem, we are given k\nidentical machines, each of which can process at most m_i jobs. M jobs are also\ngiven, where job j has a non-negative processing time length t_j >= 0. The task\nis to find a schedule such that the makespan is minimized and the capacity\nconstraints are met. In this paper, we present a 3-approximation algorithm\nusing an extension of Iterative Rounding Method introduced by Jain. To the best\nof the authors' knowledge, this is the first attempt to apply Iterative\nRounding Method to scheduling problem with capacity constraints.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2009 02:01:26 GMT"}], "update_date": "2009-06-18", "authors_parsed": [["Zhang", "Chi", ""], ["Wang", "Gang", ""], ["Liu", "Xiaoguang", ""], ["Liu", "Jing", ""]]}, {"id": "0906.3074", "submitter": "Miao Song", "authors": "Miao Song", "title": "Feynman Algorithm Implementation for Comparison with Euler in a Uniform\n  Elastic Two-Layer 2D and 3D Object Dynamic Deformation Framework in OpenGL\n  with GUI", "comments": "28 pages, 15 figures; from June 2008; portions of this work have been\n  subsequently published in conferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DS cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implement for comparative purposes the Feynman algorithm within a\nC++-based framework for two-layer uniform facet elastic object for real-time\nsoftbody simulation based on physics modeling methods. To facilitate the\ncomparison, we implement initial timing measurements on the same hardware\nagainst that of Euler integrator in the softbody framework by varying different\nalgorithm parameters. Due to a relatively large number of such variations we\nimplement a GLUI-based user-interface to allow for much more finer control over\nthe simulation process at real-time, which was lacking completely in the\nprevious versions of the framework. We show our currents results based on the\nenhanced framework. The two-layered elastic object consists of inner and outer\nelastic mass-spring surfaces and compressible internal pressure. The density of\nthe inner layer can be set differently from the density of the outer layer; the\nmotion of the inner layer can be opposite to the motion of the outer layer.\nThese special features, which cannot be achieved by a single layered object,\nresult in improved imitation of a soft body, such as tissue's liquid\nnon-uniform deformation. The inertial behavior of the elastic object is well\nillustrated in environments with gravity and collisions with walls, ceiling,\nand floor. The collision detection is defined by elastic collision penalty\nmethod and the motion of the object is guided by the Ordinary Differential\nEquation computation. Users can interact with the modeled objects, deform them,\nand observe the response to their action in real-time and we provide an\nextensible framework and its implementation for comparative studies of\ndifferent physical-based modeling and integration algorithm implementations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2009 05:39:28 GMT"}], "update_date": "2009-06-18", "authors_parsed": [["Song", "Miao", ""]]}, {"id": "0906.3240", "submitter": "Efi Fogel", "authors": "Efi Fogel", "title": "Minkowski Sum Construction and other Applications of Arrangements of\n  Geodesic Arcs on the Sphere", "comments": "A Ph.D. thesis carried out at the Tel-Aviv university. 134 pages\n  long. The advisor was Prof. Dan Halperin", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two exact implementations of efficient output-sensitive algorithms\nthat compute Minkowski sums of two convex polyhedra in 3D. We do not assume\ngeneral position. Namely, we handle degenerate input, and produce exact\nresults. We provide a tight bound on the exact maximum complexity of Minkowski\nsums of polytopes in 3D in terms of the number of facets of the summand\npolytopes. The algorithms employ variants of a data structure that represents\narrangements embedded on two-dimensional parametric surfaces in 3D, and they\nmake use of many operations applied to arrangements in these representations.\nWe have developed software components that support the arrangement\ndata-structure variants and the operations applied to them. These software\ncomponents are generic, as they can be instantiated with any number type.\nHowever, our algorithms require only (exact) rational arithmetic. These\nsoftware components together with exact rational-arithmetic enable a robust,\nefficient, and elegant implementation of the Minkowski-sum constructions and\nthe related applications. These software components are provided through a\npackage of the Computational Geometry Algorithm Library (CGAL) called\nArrangement_on_surface_2. We also present exact implementations of other\napplications that exploit arrangements of arcs of great circles embedded on the\nsphere. We use them as basic blocks in an exact implementation of an efficient\nalgorithm that partitions an assembly of polyhedra in 3D with two hands using\ninfinite translations. This application distinctly shows the importance of\nexact computation, as imprecise computation might result with dismissal of\nvalid partitioning-motions.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2009 15:51:04 GMT"}], "update_date": "2009-12-07", "authors_parsed": [["Fogel", "Efi", ""]]}, {"id": "0906.3483", "submitter": "Mugurel Ionut Andreica", "authors": "Mugurel Ionut Andreica, Nicolae Tapus", "title": "Efficient Offline Algorithmic Techniques for Several Packet Routing\n  Problems in Distributed Systems", "comments": null, "journal-ref": "Acta Universitatis Apulensis - Mathematics-Informatics, no. 18,\n  pp. 111-128, 2009. (ISSN: 1582-5329)", "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider several problems concerning packet routing in\ndistributed systems. Each problem is formulated using terms from Graph Theory\nand for each problem we present efficient, novel, algorithmic techniques for\ncomputing optimal solutions. We address topics like: bottleneck paths (trees),\noptimal paths with non-linear costs, optimal paths with multiple optimization\nobjectives, maintaining aggregate connectivity information under a sequence of\nnetwork link failures, and several others.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2009 17:38:39 GMT"}], "update_date": "2009-06-19", "authors_parsed": [["Andreica", "Mugurel Ionut", ""], ["Tapus", "Nicolae", ""]]}, {"id": "0906.3490", "submitter": "Mugurel Ionut Andreica", "authors": "Mugurel Ionut Andreica, Madalina Ecaterina Andreica, Costel Visan", "title": "Optimal Constrained Resource Allocation Strategies under Low Risk\n  Circumstances", "comments": null, "journal-ref": "Metalurgia International, vol. 14, special issue no. 8, pp.\n  143-154, 2009. (ISSN: 1582-2214)", "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider multiple constrained resource allocation problems,\nwhere the constraints can be specified by formulating activity dependency\nrestrictions or by using game-theoretic models. All the problems are focused on\ngeneric resources, with a few exceptions which consider financial resources in\nparticular. The problems consider low-risk circumstances and the values of the\nuncertain variables which are used by the algorithms are the expected values of\nthe variables. For each of the considered problems we propose novel algorithmic\nsolutions for computing optimal resource allocation strategies. The presented\nsolutions are optimal or near-optimal from the perspective of their time\ncomplexity. The considered problems have applications in a broad range of\ndomains, like workflow scheduling in industry (e.g. in the mining and\nmetallurgical industry) or the financial sector, motion planning, facility\nlocation and data transfer or job scheduling and resource management in Grids,\nclouds or other distributed systems.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2009 17:52:41 GMT"}], "update_date": "2009-06-19", "authors_parsed": [["Andreica", "Mugurel Ionut", ""], ["Andreica", "Madalina Ecaterina", ""], ["Visan", "Costel", ""]]}, {"id": "0906.3527", "submitter": "Serge Gaspers", "authors": "Serge Gaspers, Gregory B. Sorkin", "title": "A universally fastest algorithm for Max 2-Sat, Max 2-CSP, and everything\n  in between", "comments": "40 pages, a preliminary version appeared in the proceedings of SODA\n  2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce \"hybrid\" Max 2-CSP formulas consisting of \"simple\nclauses\", namely conjunctions and disjunctions of pairs of variables, and\ngeneral 2-variable clauses, which can be any integer-valued functions of pairs\nof boolean variables. This allows an algorithm to use both efficient reductions\nspecific to AND and OR clauses, and other powerful reductions that require the\ngeneral CSP setting. We use new reductions introduced here, and recent\nreductions such as \"clause-learning\" and \"2-reductions\" generalized to our\nsetting's mixture of simple and general clauses.\n  Parametrizing an instance by the fraction p of non-simple clauses, we give an\nexact (exponential-time) algorithm that is the fastest known polynomial-space\nalgorithm for p=0 (which includes the well-studied Max 2-Sat problem but also\ninstances with arbitrary mixtures of AND and OR clauses); the only efficient\nalgorithm for mixtures of AND, OR, and general integer-valued clauses; and tied\nfor fastest for general Max 2-CSP (p=1). Since a pure 2-Sat input instance may\nbe transformed to a general CSP instance in the course of being solved, the\nalgorithm's efficiency and generality go hand in hand.\n  Our algorithm analysis and optimization are a variation on the familiar\nmeasure-and-conquer approach, resulting in an optimizing mathematical program\nthat is convex not merely quasi-convex, and thus can be solved efficiently and\nwith a certificate of optimality. We produce a family of running-time\nupper-bound formulas, each optimized for instances with a particular value of p\nbut valid for all instances.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2009 21:29:31 GMT"}], "update_date": "2009-06-22", "authors_parsed": [["Gaspers", "Serge", ""], ["Sorkin", "Gregory B.", ""]]}, {"id": "0906.3966", "submitter": "R Doomun", "authors": "K Pradheep Kumar, A P Shanthi", "title": "Application of non-uniform laxity to EDF for aperiodic tasks to improve\n  task utilisation on multicore platforms", "comments": "7 pages, Journal of Computer Science and Information Security", "journal-ref": "IJCSIS 2009, June Issue, Vol. 2, No. 1", "doi": null, "report-no": null, "categories": "cs.PF cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new scheduler applying the concept of non-uniform\nlaxity to Earliest deadline first (EDF) approach for aperiodic tasks. This\nscheduler improves task utilisation (Execution time / deadline) and also\nincreases the number of tasks that are being scheduled. Laxity is a measure of\nthe spare time permitted for the task before it misses its deadline, and is\ncomputed using the expression (deadline - (current time + execution time)).\nWeight decides the priority of the task and is defined by the expression\n(quantum slice time / allocated time)*total core time for the task. Quantum\nslice time is the time actually used, allocated time is the time allocated by\nthe scheduler, and total core time is the time actually reserved by the core\nfor execution of one quantum of the task. Non-uniform laxity enables scheduling\nof tasks that have higher priority before the normal execution of other tasks\nand is computed by multiplying the weight of the task with its laxity. The\nalgorithm presented in the paper has been simulated on Cheddar, a real time\nscheduling tool and also on SESC, an architectural simulator for multicore\nplatforms, for upto 5000 random task sets, and upto 5000 cores. This scheduler\nimproves task utilisation by 35% and the number of tasks being scheduled by\n36%, compared to conventional EDF.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2009 09:33:17 GMT"}], "update_date": "2009-06-23", "authors_parsed": [["Kumar", "K Pradheep", ""], ["Shanthi", "A P", ""]]}, {"id": "0906.4539", "submitter": "Michael Mahoney", "authors": "Michael W. Mahoney and Hariharan Narayanan", "title": "Learning with Spectral Kernels and Heavy-Tailed Data", "comments": "21 pages. Substantially revised and extended relative to the first\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two ubiquitous aspects of large-scale data analysis are that the data often\nhave heavy-tailed properties and that diffusion-based or spectral-based methods\nare often used to identify and extract structure of interest. Perhaps\nsurprisingly, popular distribution-independent methods such as those based on\nthe VC dimension fail to provide nontrivial results for even simple learning\nproblems such as binary classification in these two settings. In this paper, we\ndevelop distribution-dependent learning methods that can be used to provide\ndimension-independent sample complexity bounds for the binary classification\nproblem in these two popular settings. In particular, we provide bounds on the\nsample complexity of maximum margin classifiers when the magnitude of the\nentries in the feature vector decays according to a power law and also when\nlearning is performed with the so-called Diffusion Maps kernel. Both of these\nresults rely on bounding the annealed entropy of gap-tolerant classifiers in a\nHilbert space. We provide such a bound, and we demonstrate that our proof\ntechnique generalizes to the case when the margin is measured with respect to\nmore general Banach space norms. The latter result is of potential interest in\ncases where modeling the relationship between data elements as a dot product in\na Hilbert space is too restrictive.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2009 18:38:31 GMT"}, {"version": "v2", "created": "Mon, 10 May 2010 17:19:30 GMT"}], "update_date": "2010-05-11", "authors_parsed": [["Mahoney", "Michael W.", ""], ["Narayanan", "Hariharan", ""]]}, {"id": "0906.4692", "submitter": "Rossano Venturini", "authors": "Paolo Ferragina and Igor Nitto and Rossano Venturini", "title": "On optimally partitioning a text to improve its compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the problem of partitioning an input string T in\nsuch a way that compressing individually its parts via a base-compressor C gets\na compressed output that is shorter than applying C over the entire T at once.\nThis problem was introduced in the context of table compression, and then\nfurther elaborated and extended to strings and trees. Unfortunately, the\nliterature offers poor solutions: namely, we know either a cubic-time algorithm\nfor computing the optimal partition based on dynamic programming, or few\nheuristics that do not guarantee any bounds on the efficacy of their computed\npartition, or algorithms that are efficient but work in some specific scenarios\n(such as the Burrows-Wheeler Transform) and achieve compression performance\nthat might be worse than the optimal-partitioning by a $\\Omega(\\sqrt{\\log n})$\nfactor. Therefore, computing efficiently the optimal solution is still open. In\nthis paper we provide the first algorithm which is guaranteed to compute in\n$O(n \\log_{1+\\eps}n)$ time a partition of T whose compressed output is\nguaranteed to be no more than $(1+\\epsilon)$-worse the optimal one, where\n$\\epsilon$ may be any positive constant.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2009 13:36:24 GMT"}], "update_date": "2009-06-26", "authors_parsed": [["Ferragina", "Paolo", ""], ["Nitto", "Igor", ""], ["Venturini", "Rossano", ""]]}, {"id": "0906.4816", "submitter": "Assaf Naor", "authors": "Subhash Khot and Assaf Naor", "title": "Sharp kernel clustering algorithms and their associated Grothendieck\n  inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the kernel clustering problem we are given a (large) $n\\times n$ symmetric\npositive semidefinite matrix $A=(a_{ij})$ with $\\sum_{i=1}^n\\sum_{j=1}^n\na_{ij}=0$ and a (small) $k\\times k$ symmetric positive semidefinite matrix\n$B=(b_{ij})$. The goal is to find a partition $\\{S_1,...,S_k\\}$ of $\\{1,...\nn\\}$ which maximizes $ \\sum_{i=1}^k\\sum_{j=1}^k (\\sum_{(p,q)\\in S_i\\times\nS_j}a_{pq})b_{ij}$.\n  We design a polynomial time approximation algorithm that achieves an\napproximation ratio of $\\frac{R(B)^2}{C(B)}$, where $R(B)$ and $C(B)$ are\ngeometric parameters that depend only on the matrix $B$, defined as follows: if\n$b_{ij} = < v_i, v_j>$ is the Gram matrix representation of $B$ for some\n$v_1,...,v_k\\in \\R^k$ then $R(B)$ is the minimum radius of a Euclidean ball\ncontaining the points $\\{v_1, ..., v_k\\}$. The parameter $C(B)$ is defined as\nthe maximum over all measurable partitions $\\{A_1,...,A_k\\}$ of $\\R^{k-1}$ of\nthe quantity $\\sum_{i=1}^k\\sum_{j=1}^k b_{ij}< z_i,z_j>$, where for $i\\in\n\\{1,...,k\\}$ the vector $z_i\\in \\R^{k-1}$ is the Gaussian moment of $A_i$,\ni.e., $z_i=\\frac{1}{(2\\pi)^{(k-1)/2}}\\int_{A_i}xe^{-\\|x\\|_2^2/2}dx$. We also\nshow that for every $\\eps > 0$, achieving an approximation guarantee of\n$(1-\\e)\\frac{R(B)^2}{C(B)}$ is Unique Games hard.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2009 23:10:44 GMT"}], "update_date": "2009-06-29", "authors_parsed": [["Khot", "Subhash", ""], ["Naor", "Assaf", ""]]}, {"id": "0906.5010", "submitter": "Seshadhri Comandur", "authors": "C. Seshadhri", "title": "Testing cycle-freeness: Finding a certificate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We deal with the problem of designing one-sided error property testers for\ncycle-freeness in bounded degree graphs. Such a property tester always accepts\nforests. Furthermore, when it rejects an input, it provides a short cycle as a\ncertificate. The problem of testing cycle-freeness in this model was first\nconsidered by Goldreich and Ron \\cite{GR97}. They give a constant time tester\nwith two-sided error (it does not provide certificates for rejection) and prove\na $\\Omega(\\sqrt{n})$ lower bound for testers with one-sided error. We design a\nproperty tester with one-sided error whose running time matches this lower\nbound (upto polylogarithmic factors). Interestingly, this has connections to a\nrecent conjecture of Benjamini, Schramm, and Shapira \\cite{BSS08}. The property\nof cycle-freeness is closed under the operation of taking minors. This is the\nfirst example of such a property that has an almost optimal\n$\\otilde(\\sqrt{n})$-time one-sided error tester, but has a constant time\ntwo-sided error tester. It was conjectured in \\cite{BSS08} that this happens\nfor a vast class of minor-closed properties, and this result can seen as the\nfirst indication towards that.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2009 21:06:04 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Seshadhri", "C.", ""]]}, {"id": "0906.5050", "submitter": "Leah Epstein", "authors": "Leah Epstein and Asaf Levin", "title": "AFPTAS results for common variants of bin packing: A new method to\n  handle the small items", "comments": null, "journal-ref": "SIAM Journal on Optimization 20(6): 3121-3145 (2010)", "doi": "10.1137/090767613", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two well-known natural variants of bin packing, and show that\nthese packing problems admit asymptotic fully polynomial time approximation\nschemes (AFPTAS). In bin packing problems, a set of one-dimensional items of\nsize at most 1 is to be assigned (packed) to subsets of sum at most 1 (bins).\nIt has been known for a while that the most basic problem admits an AFPTAS. In\nthis paper, we develop methods that allow to extend this result to other\nvariants of bin packing. Specifically, the problems which we study in this\npaper, for which we design asymptotic fully polynomial time approximation\nschemes, are the following. The first problem is \"Bin packing with cardinality\nconstraints\", where a parameter k is given, such that a bin may contain up to k\nitems. The goal is to minimize the number of bins used. The second problem is\n\"Bin packing with rejection\", where every item has a rejection penalty\nassociated with it. An item needs to be either packed to a bin or rejected, and\nthe goal is to minimize the number of used bins plus the total rejection\npenalty of unpacked items. This resolves the complexity of two important\nvariants of the bin packing problem. Our approximation schemes use a novel\nmethod for packing the small items. This new method is the core of the improved\nrunning times of our schemes over the running times of the previous results,\nwhich are only asymptotic polynomial time approximation schemes (APTAS).\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2009 08:39:18 GMT"}], "update_date": "2012-02-16", "authors_parsed": [["Epstein", "Leah", ""], ["Levin", "Asaf", ""]]}, {"id": "0906.5051", "submitter": "Leah Epstein", "authors": "Leah Epstein and Asaf Levin", "title": "Bin packing with general cost structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the work of Anily et al., we consider a variant of bin packing,\ncalled \"bin packing with general cost structures\" (GCBP) and design an\nasymptotic fully polynomial time approximation scheme (AFPTAS) for this\nproblem. In the classic bin packing problem, a set of one-dimensional items is\nto be assigned to subsets of total size at most 1, that is, to be packed into\nunit sized bins. However, in GCBP, the cost of a bin is not 1 as in classic bin\npacking, but it is a non-decreasing and concave function of the number of items\npacked in it, where the cost of an empty bin is zero. The construction of the\nAFPTAS requires novel techniques for dealing with small items, which are\ndeveloped in this work. In addition, we develop a fast approximation algorithm\nwhich acts identically for all non-decreasing and concave functions, and has an\nasymptotic approximation ratio of 1.5 for all functions simultaneously.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2009 19:48:10 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Epstein", "Leah", ""], ["Levin", "Asaf", ""]]}, {"id": "0906.5062", "submitter": "Simant Dube", "authors": "Simant Dube", "title": "Geometrical Interpretation of the Master Theorem for Divide-and-conquer\n  Recurrences", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide geometrical interpretation of the Master Theorem to solve\ndivide-and-conquer recurrences. We show how different cases of the recurrences\ncorrespond to different kinds of fractal images. Fractal dimension and\nHausdorff measure are shown to be closely related to the solution of such\nrecurrences.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2009 04:43:17 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2009 10:34:53 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Dube", "Simant", ""]]}, {"id": "0906.5070", "submitter": "R Doomun", "authors": "R. Thamilselvan, Dr. P. Balasubramanie", "title": "Integrating Genetic Algorithm, Tabu Search Approach for Job Shop\n  Scheduling", "comments": "6 pages, International Journal of Computer Science and Information\n  Security (IJCSIS)", "journal-ref": "IJCSIS June 2009 Issue, Vol. 2, No. 1", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new algorithm based on integrating Genetic Algorithms\nand Tabu Search methods to solve the Job Shop Scheduling problem. The idea of\nthe proposed algorithm is derived from Genetic Algorithms. Most of the\nscheduling problems require either exponential time or space to generate an\noptimal answer. Job Shop scheduling (JSS) is the general scheduling problem and\nit is a NP-complete problem, but it is difficult to find the optimal solution.\nThis paper applies Genetic Algorithms and Tabu Search for Job Shop Scheduling\nproblem and compares the results obtained by each. With the implementation of\nour approach the JSS problems reaches optimal solution and minimize the\nmakespan.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2009 11:16:30 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Thamilselvan", "R.", ""], ["Balasubramanie", "Dr. P.", ""]]}, {"id": "0906.5089", "submitter": "David Fern\\'andez-Baca", "authors": "Mukul S. Bansal, Jianrong Dong, David Fern\\'andez-Baca", "title": "Comparing and Aggregating Partially Resolved Trees", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define, analyze, and give efficient algorithms for two kinds of distance\nmeasures for rooted and unrooted phylogenies. For rooted trees, our measures\nare based on the topologies the input trees induce on triplets; that is, on\nthree-element subsets of the set of species. For unrooted trees, the measures\nare based on quartets (four-element subsets). Triplet and quartet-based\ndistances provide a robust and fine-grained measure of the similarities between\ntrees. The distinguishing feature of our distance measures relative to\ntraditional quartet and triplet distances is their ability to deal cleanly with\nthe presence of unresolved nodes, also called polytomies. For rooted trees,\nthese are nodes with more than two children; for unrooted trees, they are nodes\nof degree greater than three.\n  Our first class of measures are parametric distances, where there is a\nparameter that weighs the difference between an unresolved triplet/quartet\ntopology and a resolved one. Our second class of measures are based on\nHausdorff distance. Each tree is viewed as a set of all possible ways in which\nthe tree could be refined to eliminate unresolved nodes. The distance between\nthe original (unresolved) trees is then taken to be the Hausdorff distance\nbetween the associated sets of fully resolved trees, where the distance between\ntrees in the sets is the triplet or quartet distance, as appropriate.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2009 15:56:42 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Bansal", "Mukul S.", ""], ["Dong", "Jianrong", ""], ["Fern\u00e1ndez-Baca", "David", ""]]}, {"id": "0906.5106", "submitter": "Shmuel Onn", "authors": "Raymond Hemmecke, Shmuel Onn, Robert Weismantel", "title": "Multicommodity Flow in Polynomial Time", "comments": null, "journal-ref": "Optimization Letters, 5:13--25, 2011", "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multicommodity flow problem is NP-hard already for two commodities over\nbipartite graphs. Nonetheless, using our recent theory of n-fold integer\nprogramming and extensions developed herein, we are able to establish the\nsurprising polynomial time solvability of the problem in two broad situations.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2009 23:56:05 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2009 14:29:56 GMT"}], "update_date": "2011-01-18", "authors_parsed": [["Hemmecke", "Raymond", ""], ["Onn", "Shmuel", ""], ["Weismantel", "Robert", ""]]}]