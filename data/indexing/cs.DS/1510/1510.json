[{"id": "1510.00215", "submitter": "Piotr Skowron", "authors": "Piotr Skowron", "title": "FPT Approximation Schemes for Maximizing Submodular Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the existence of approximation algorithms for maximization of\nsubmodular functions, that run in fixed parameter tractable (FPT) time. Given a\nnon-decreasing submodular set function $v: 2^X \\to \\mathbb{R}$ the goal is to\nselect a subset $S$ of $K$ elements from $X$ such that $v(S)$ is maximized. We\nidentify three properties of set functions, referred to as $p$-separability\nproperties, and we argue that many real-life problems can be expressed as\nmaximization of submodular, $p$-separable functions, with low values of the\nparameter $p$. We present FPT approximation schemes for the minimization and\nmaximization variants of the problem, for several parameters that depend on\ncharacteristics of the optimized set function, such as $p$ and $K$. We confirm\nthat our algorithms are applicable to a broad class of problems, in particular\nto problems from computational social choice, such as item selection or winner\ndetermination under several multiwinner election systems.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 13:12:29 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 05:17:11 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 07:29:09 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Skowron", "Piotr", ""]]}, {"id": "1510.00523", "submitter": "Takahisa Toda", "authors": "Takahisa Toda and Takehide Soh", "title": "Implementing Efficient All Solutions SAT Solvers", "comments": null, "journal-ref": "ACM Journal of Experimental Algorithmics, Vol. 21, No. 1, Article\n  1.12, 2016", "doi": "10.1145/2975585", "report-no": null, "categories": "cs.DS cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All solutions SAT (AllSAT for short) is a variant of propositional\nsatisfiability problem. Despite its significance, AllSAT has been relatively\nunexplored compared to other variants. We thus survey and discuss major\ntechniques of AllSAT solvers. We faithfully implement them and conduct\ncomprehensive experiments using a large number of instances and various types\nof solvers including one of the few public softwares. The experiments reveal\nsolver's characteristics. Our implemented solvers are made publicly available\nso that other researchers can easily develop their solver by modifying our\ncodes and compare it with existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 08:32:59 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Toda", "Takahisa", ""], ["Soh", "Takehide", ""]]}, {"id": "1510.00598", "submitter": "Marin Bougeret", "authors": "Marin Bougeret, Stephane Bessy, Daniel Gon\\c{c}alves, Cristophe Paul", "title": "On independent set on B1-EPG graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the Maximum Independent Set problem (MIS) on\n$B_1$-EPG graphs. EPG (for Edge intersection graphs of Paths on a Grid) was\nintroduced in ~\\cite{edgeintersinglebend} as the class of graphs whose vertices\ncan be represented as simple paths on a rectangular grid so that two vertices\nare adjacent if and only if the corresponding paths share at least one edge of\nthe underlying grid. The restricted class $B_k$-EPG denotes EPG-graphs where\nevery path has at most $k$ bends. The study of MIS on $B_1$-EPG graphs has been\ninitiated in~\\cite{wadsMIS} where authors prove that MIS is NP-complete on\n$B_1$-EPG graphs, and provide a polynomial $4$-approximation. In this article\nwe study the approximability and the fixed parameter tractability of MIS on\n$B_1$-EPG. We show that there is no PTAS for MIS on $B_1$-EPG unless P$=$NP,\neven if there is only one shape of path, and even if each path has its vertical\npart or its horizontal part of length at most $3$. This is optimal, as we show\nthat if all paths have their horizontal part bounded by a constant, then MIS\nadmits a PTAS. Finally, we show that MIS is FPT in the standard\nparameterization on $B_1$-EPG restricted to only three shapes of path, and\n$W_1$-hard on $B_2$-EPG. The status for general $B_1$-EPG (with the four\nshapes) is left open.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 13:55:34 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Bougeret", "Marin", ""], ["Bessy", "Stephane", ""], ["Gon\u00e7alves", "Daniel", ""], ["Paul", "Cristophe", ""]]}, {"id": "1510.00634", "submitter": "Gabriele Fici", "authors": "Gabriele Fici, Thierry Lecroq, Arnaud Lefebvre, \\'Elise Prieur-Gaston,\n  William F. Smyth", "title": "A Note on Easy and Efficient Computation of Full Abelian Periods of a\n  Word", "comments": "Accepted for publication in Discrete Applied Mathematics", "journal-ref": "Discrete Applied Mathematics, 212: 88-95, 2016", "doi": "10.1016/j.dam.2015.09.024", "report-no": null, "categories": "cs.DS cs.DM cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constantinescu and Ilie (Bulletin of the EATCS 89, 167-170, 2006) introduced\nthe idea of an Abelian period with head and tail of a finite word. An Abelian\nperiod is called full if both the head and the tail are empty. We present a\nsimple and easy-to-implement $O(n\\log\\log n)$-time algorithm for computing all\nthe full Abelian periods of a word of length $n$ over a constant-size alphabet.\nExperiments show that our algorithm significantly outperforms the $O(n)$\nalgorithm proposed by Kociumaka et al. (Proc. of STACS, 245-256, 2013) for the\nsame problem.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 16:17:11 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Fici", "Gabriele", ""], ["Lecroq", "Thierry", ""], ["Lefebvre", "Arnaud", ""], ["Prieur-Gaston", "\u00c9lise", ""], ["Smyth", "William F.", ""]]}, {"id": "1510.00738", "submitter": "Daniel Freund", "authors": "Daniel Freund and David P. Williamson", "title": "Rank Aggregation: New Bounds for MCx", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rank aggregation problem has received significant recent attention within\nthe computer science community. Its applications today range far beyond the\noriginal aim of building metasearch engines to problems in machine learning,\nrecommendation systems and more. Several algorithms have been proposed for\nthese problems, and in many cases approximation guarantees have been proven for\nthem. However, it is also known that some Markov chain based algorithms (MC1,\nMC2, MC3, MC4) perform extremely well in practice, yet had no known performance\nguarantees. We prove supra-constant lower bounds on approximation guarantees\nfor all of them. We also raise the lower bound for sorting by Copeland score\nfrom 3/2 to 2 and prove an upper bound of 11, before showing that in particular\nways, MC4 can nevertheless be seen as a generalization of Copeland score.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 21:10:55 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Freund", "Daniel", ""], ["Williamson", "David P.", ""]]}, {"id": "1510.00755", "submitter": "Taylor Arnold", "authors": "Taylor Arnold", "title": "Sparse Density Representations for Simultaneous Inference on Large\n  Spatial Datasets", "comments": "9 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large spatial datasets often represent a number of spatial point processes\ngenerated by distinct entities or classes of events. When crossed with\ncovariates, such as discrete time buckets, this can quickly result in a data\nset with millions of individual density estimates. Applications that require\nsimultaneous access to a substantial subset of these estimates become resource\nconstrained when densities are stored in complex and incompatible formats. We\npresent a method for representing spatial densities along the nodes of sparsely\npopulated trees. Fast algorithms are provided for performing set operations and\nqueries on the resulting compact tree structures. The speed and simplicity of\nthe approach is demonstrated on both real and simulated spatial data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 23:05:48 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Arnold", "Taylor", ""]]}, {"id": "1510.00773", "submitter": "Junjie Ye", "authors": "Junjie Ye", "title": "A Note on Finding Dual Feedback Vertex Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an edge-bicolored graph $G$ where each edge is colored either red or\nblue, a vertex set $S$ is a dual feedback vertex set if $S$ hits all blue\ncycles and red cycles of $G$. In this paper, we show that a dual feedback\nvertex set of size at most $k$ can be found in time $O^*(c_1^k)$ and all\nminimal dual feedback vertex set of size at most $k$ can be enumerated in time\n$O^*(c_2^{k^2 + k})$ by compact representations for constants $c_1$ and $c_2$.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 03:58:48 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Ye", "Junjie", ""]]}, {"id": "1510.00958", "submitter": "Akshar Varma", "authors": "Akshar Varma", "title": "Existence of k-ary Trees: Subtree Sizes, Heights and Depths", "comments": "Revised version, 12 pages (excluding references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rooted tree is an important data structure, and the subtree size, height,\nand depth are naturally defined attributes of every node. We consider the\nproblem of the existence of a k-ary tree given a list of attribute sequences.\nWe give polynomial time (O(nlog(n))) algorithms for the existence of a k-ary\ntree given depth and/or height sequences. Our most significant results are the\nStrong NP-Completeness of the decision problems of existence of k-ary trees\ngiven subtree sizes sequences. We prove this by multi-stage reductions from\nNUMERICAL MATCHING WITH TARGET SUMS. In the process, we also prove a\ngeneralized version of the 3-PARTITION problem to be Strongly NP-Complete. By\nlooking at problems where a combination of attribute sequences are given, we\nare able to draw the boundary between easy and hard problems related to\nexistence of trees given attribute sequences and enhance our understanding of\nwhere the difficulty lies in such problems.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2015 17:50:30 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 03:36:19 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Varma", "Akshar", ""]]}, {"id": "1510.01072", "submitter": "Wolfgang Mulzer", "authors": "Haim Kaplan, Wolfgang Mulzer, Liam Roditty, Paul Seiferth", "title": "Routing in Unit Disk Graphs", "comments": "19 pages, 6 figures; a preliminary version appeared in LATIN 2016", "journal-ref": "Algorithmica, 80(3), 2018, pp. 830-848", "doi": "10.1007/s00453-017-0308-2", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $S \\subset \\mathbb{R}^2$ be a set of $n$ sites. The unit disk graph\n$\\text{UD}(S)$ on $S$ has vertex set $S$ and an edge between two distinct sites\n$s,t \\in S$ if and only if $s$ and $t$ have Euclidean distance $|st| \\leq 1$.\n  A routing scheme $R$ for $\\text{UD}(S)$ assigns to each site $s \\in S$ a\nlabel $\\ell(s)$ and a routing table $\\rho(s)$. For any two sites $s, t \\in S$,\nthe scheme $R$ must be able to route a packet from $s$ to $t$ in the following\nway: given a current site $r$ (initially, $r = s$), a header $h$ (initially\nempty), and the label $\\ell(t)$ of the target, the scheme $R$ consults the\nrouting table $\\rho(r)$ to compute a neighbor $r'$ of $r$, a new header $h'$,\nand the label $\\ell(t')$ of an intermediate target $t'$. (The label of the\noriginal target may be stored at the header $h'$.) The packet is then routed to\n$r'$, and the procedure is repeated until the packet reaches $t$. The resulting\nsequence of sites is called the routing path. The stretch of $R$ is the maximum\nratio of the (Euclidean) length of the routing path produced by $R$ and the\nshortest path in $\\text{UD}(S)$, over all pairs of distinct sites in $S$.\n  For any given $\\varepsilon > 0$, we show how to construct a routing scheme\nfor $\\text{UD}(S)$ with stretch $1+\\varepsilon$ using labels of $O(\\log n)$\nbits and routing tables of $O(\\varepsilon^{-5}\\log^2 n \\log^2 D)$ bits, where\n$D$ is the (Euclidean) diameter of $\\text{UD}(S)$. The header size is $O(\\log n\n\\log D)$ bits.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 09:20:37 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 07:44:00 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Kaplan", "Haim", ""], ["Mulzer", "Wolfgang", ""], ["Roditty", "Liam", ""], ["Seiferth", "Paul", ""]]}, {"id": "1510.01158", "submitter": "Damianos Gavalas", "authors": "Damianos Gavalas, Charalampos Konstantopoulos, Grammati Pantziou", "title": "Design and Management of Vehicle Sharing Systems: A Survey of\n  Algorithmic Approaches", "comments": "38 pages, 3 figures, 4 tables, In: \"Smart Cities and Homes: Key\n  Enabling Technologies\", M.S. Obaidat and P. Nicopolitidis (Eds.), Elsevier\n  Science, in press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle (bike or car) sharing represents an emerging transportation scheme\nwhich may comprise an important link in the green mobility chain of smart city\nenvironments. This chapter offers a comprehensive review of algorithmic\napproaches for the design and management of vehicle sharing systems. Our focus\nis on one-way vehicle sharing systems (wherein customers are allowed to pick-up\na vehicle at any location and return it to any other station) which best suits\ntypical urban journey requirements. Along this line, we present methods dealing\nwith the so-called asymmetric demand-offer problem (i.e. the unbalanced offer\nand demand of vehicles) typically experienced in one-way sharing systems which\nseverely affects their economic viability as it implies that considerable human\n(and financial) resources should be engaged in relocating vehicles to satisfy\ncustomer demand. The chapter covers all planning aspects that affect the\neffectiveness and viability of vehicle sharing systems: the actual system\ndesign (e.g. number and location of vehicle station facilities, vehicle fleet\nsize, vehicles distribution among stations); customer incentivisation schemes\nto motivate customer-based distribution of bicycles/cars (such schemes offer\nmeaningful incentives to users so as to leave their vehicle to a station\ndifferent to that originally intended and satisfy future user demand);\ncost-effective solutions to schedule operator-based repositioning of\nbicycles/cars (by employees explicitly enrolled in vehicle relocation) based on\nthe current and future (predicted) demand patterns (operator-based and\ncustomer-based relocation may be thought as complementary methods to achieve\nthe intended distribution of vehicles among stations).\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 14:02:52 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Gavalas", "Damianos", ""], ["Konstantopoulos", "Charalampos", ""], ["Pantziou", "Grammati", ""]]}, {"id": "1510.01455", "submitter": "Anirban Dasgupta", "authors": "Anirban Dasgupta, Kevin Lang, Lee Rhodes, Justin Thaler", "title": "A Framework for Estimating Stream Expression Cardinalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $m$ distributed data streams $A_1, \\dots, A_m$, we consider the problem\nof estimating the number of unique identifiers in streams defined by set\nexpressions over $A_1, \\dots, A_m$. We identify a broad class of algorithms for\nsolving this problem, and show that the estimators output by any algorithm in\nthis class are perfectly unbiased and satisfy strong variance bounds. Our\nanalysis unifies and generalizes a variety of earlier results in the\nliterature. To demonstrate its generality, we describe several novel sampling\nalgorithms in our class, and show that they achieve a novel tradeoff between\naccuracy, space usage, update speed, and applicability.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 07:04:32 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 02:47:42 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Dasgupta", "Anirban", ""], ["Lang", "Kevin", ""], ["Rhodes", "Lee", ""], ["Thaler", "Justin", ""]]}, {"id": "1510.01518", "submitter": "Georgina Hall", "authors": "Amir Ali Ahmadi, Georgina Hall", "title": "DC Decomposition of Nonconvex Polynomials with Algebraic Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of decomposing a multivariate polynomial as the\ndifference of two convex polynomials. We introduce algebraic techniques which\nreduce this task to linear, second order cone, and semidefinite programming.\nThis allows us to optimize over subsets of valid difference of convex\ndecompositions (dcds) and find ones that speed up the convex-concave procedure\n(CCP). We prove, however, that optimizing over the entire set of dcds is\nNP-hard.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 10:34:19 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 10:10:34 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Ahmadi", "Amir Ali", ""], ["Hall", "Georgina", ""]]}, {"id": "1510.01557", "submitter": "Amer Mouawad", "authors": "Akanksha Agrawal, Daniel Lokshtanov, Amer E. Mouawad, Saket Saurabh", "title": "Simultaneous Feedback Vertex Set: A Parameterized Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a family of graphs $\\mathcal{F}$, a graph $G$, and a positive integer\n$k$, the $\\mathcal{F}$-Deletion problem asks whether we can delete at most $k$\nvertices from $G$ to obtain a graph in $\\mathcal{F}$. $\\mathcal{F}$-Deletion\ngeneralizes many classical graph problems such as Vertex Cover, Feedback Vertex\nSet, and Odd Cycle Transversal. A graph $G = (V, \\cup_{i=1}^{\\alpha} E_{i})$,\nwhere the edge set of $G$ is partitioned into $\\alpha$ color classes, is called\nan $\\alpha$-edge-colored graph. A natural extension of the\n$\\mathcal{F}$-Deletion problem to edge-colored graphs is the\n$\\alpha$-Simultaneous $\\mathcal{F}$-Deletion problem. In the latter problem, we\nare given an $\\alpha$-edge-colored graph $G$ and the goal is to find a set $S$\nof at most $k$ vertices such that each graph $G_i \\setminus S$, where $G_i =\n(V, E_i)$ and $1 \\leq i \\leq \\alpha$, is in $\\mathcal{F}$. In this work, we\nstudy $\\alpha$-Simultaneous $\\mathcal{F}$-Deletion for $\\mathcal{F}$ being the\nfamily of forests. In other words, we focus on the $\\alpha$-Simultaneous\nFeedback Vertex Set ($\\alpha$-SimFVS) problem. Algorithmically, we show that,\nlike its classical counterpart, $\\alpha$-SimFVS parameterized by $k$ is\nfixed-parameter tractable (FPT) and admits a polynomial kernel, for any fixed\nconstant $\\alpha$. In particular, we give an algorithm running in $2^{O(\\alpha\nk)}n^{O(1)}$ time and a kernel with $O(\\alpha k^{3(\\alpha + 1)})$ vertices. The\nrunning time of our algorithm implies that $\\alpha$-SimFVS is FPT even when\n$\\alpha \\in o(\\log n)$. We complement this positive result by showing that for\n$\\alpha \\in O(\\log n)$, where $n$ is the number of vertices in the input graph,\n$\\alpha$-SimFVS becomes W[1]-hard. Our positive results answer one of the open\nproblems posed by Cai and Ye (MFCS 2014).\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 12:49:14 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Agrawal", "Akanksha", ""], ["Lokshtanov", "Daniel", ""], ["Mouawad", "Amer E.", ""], ["Saurabh", "Saket", ""]]}, {"id": "1510.01800", "submitter": "Arthur Flajolet", "authors": "Arthur Flajolet, Patrick Jaillet", "title": "Logarithmic regret bounds for Bandits with Knapsacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal regret bounds for Multi-Armed Bandit problems are now well\ndocumented. They can be classified into two categories based on the growth rate\nwith respect to the time horizon $T$: (i) small, distribution-dependent, bounds\nof order of magnitude $\\ln(T)$ and (ii) robust, distribution-free, bounds of\norder of magnitude $\\sqrt{T}$. The Bandits with Knapsacks model, an extension\nto the framework allowing to model resource consumption, lacks this clear-cut\ndistinction. While several algorithms have been shown to achieve asymptotically\noptimal distribution-free bounds on regret, there has been little progress\ntoward the development of small distribution-dependent regret bounds. We\npartially bridge the gap by designing a general-purpose algorithm with\ndistribution-dependent regret bounds that are logarithmic in the initial\nendowments of resources in several important cases that cover many practical\napplications, including dynamic pricing with limited supply, bid optimization\nin online advertisement auctions, and dynamic procurement.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 01:47:48 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 04:51:23 GMT"}, {"version": "v3", "created": "Sun, 2 Oct 2016 01:36:35 GMT"}, {"version": "v4", "created": "Mon, 10 Apr 2017 19:21:24 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Flajolet", "Arthur", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1510.01814", "submitter": "Kai Zhu", "authors": "Kai Zhu and Lei Ying", "title": "Source Localization in Networks: Trees and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information diffusion in networks can be used to model many real-world\nphenomena, including rumor spreading on online social networks, epidemics in\nhuman beings, and malware on the Internet. Informally speaking, the source\nlocalization problem is to identify a node in the network that provides the\nbest explanation of the observed diffusion. Despite significant efforts and\nsuccesses over last few years, theoretical guarantees of source localization\nalgorithms were established only for tree networks due to the complexity of the\nproblem. This paper presents a new source localization algorithm, called the\nShort-Fat Tree (SFT) algorithm. Loosely speaking, the algorithm selects the\nnode such that the breadth-first search (BFS) tree from the node has the\nminimum depth but the maximum number of leaf nodes. Performance guarantees of\nSFT under the independent cascade (IC) model are established for both tree\nnetworks and the Erdos-Renyi (ER) random graph. On tree networks, SFT is the\nmaximum a posterior (MAP) estimator. On the ER random graph, the following\nfundamental limits have been obtained: $(i)$ when the infection duration\n$<\\frac{2}{3}t_u,$ SFT identifies the source with probability one\nasymptotically, where $t_u=\\left\\lceil\\frac{\\log n}{\\log \\mu}\\right\\rceil+2$\nand $\\mu$ is the average node degree, $(ii)$ when the infection duration\n$>t_u,$ the probability of identifying the source approaches zero\nasymptotically under any algorithm; and $(iii)$ when infection duration $<t_u,$\nthe BFS tree starting from the source is a fat tree. Numerical experiments on\ntree networks, the ER random graphs and real world networks with different\nevaluation metrics show that the SFT algorithm outperforms existing algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 03:58:54 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Zhu", "Kai", ""], ["Ying", "Lei", ""]]}, {"id": "1510.01891", "submitter": "Adam Kurpisz", "authors": "Adam Kurpisz, Samuli Lepp\\\"anen, Monaldo Mastrolilli", "title": "On the Hardest Problem Formulations for the 0/1 Lasserre Hierarchy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lasserre/Sum-of-Squares (SoS) hierarchy is a systematic procedure for\nconstructing a sequence of increasingly tight semidefinite relaxations. It is\nknown that the hierarchy converges to the 0/1 polytope in n levels and captures\nthe convex relaxations used in the best available approximation algorithms for\na wide variety of optimization problems.\n  In this paper we characterize the set of 0/1 integer linear problems and\nunconstrained 0/1 polynomial optimization problems that can still have an\nintegrality gap at level n-1. These problems are the hardest for the Lasserre\nhierarchy in this sense.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 10:57:45 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Kurpisz", "Adam", ""], ["Lepp\u00e4nen", "Samuli", ""], ["Mastrolilli", "Monaldo", ""]]}, {"id": "1510.01963", "submitter": "Renaud Lacour", "authors": "Renaud Lacour, Kathrin Klamroth, Carlos M. Fonseca", "title": "A Box Decomposition Algorithm to Compute the Hypervolume Indicator", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to the computation of the hypervolume indicator,\nbased on partitioning the dominated region into a set of axis-parallel\nhyperrectangles or boxes. We present a nonincremental algorithm and an\nincremental algorithm, which allows insertions of points, whose time\ncomplexities are $O(n^{\\lfloor \\frac{p-1}{2} \\rfloor+1})$ and $O(n^{\\lfloor\n\\frac{p}{2} \\rfloor+1})$, respectively. While the theoretical complexity of\nsuch a method is lower bounded by the complexity of the partition, which is, in\nthe worst-case, larger than the best upper bound on the complexity of the\nhypervolume computation, we show that it is practically efficient. In\nparticular, the nonincremental algorithm competes with the currently most\npractically efficient algorithms. Finally, we prove an enhanced upper bound of\n$O(n^{p-1})$ and a lower bound of $\\Omega (n^{\\lfloor \\frac{p}{2}\\rfloor} \\log\nn )$ for $p \\geq 4$ on the worst-case complexity of the WFG algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 14:39:23 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2015 08:14:58 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Lacour", "Renaud", ""], ["Klamroth", "Kathrin", ""], ["Fonseca", "Carlos M.", ""]]}, {"id": "1510.02004", "submitter": "Nicol\\'as Alejandro Alvarez", "authors": "Nicol\\'as Alvarez and Ver\\'onica Becher", "title": "M. Levin's construction of absolutely normal numbers with very low\n  discrepancy", "comments": "20 pages with references; submitted to AMS Mathematics of Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the currently known constructions of absolutely normal numbers, the one\ngiven by Mordechay Levin in 1979 achieves the lowest discrepancy bound. In this\nwork we analyze this construction in terms of computability and computational\ncomplexity. We show that, under basic assumptions, it yields a computable real\nnumber. The construction does not give the digits of the fractional expansion\nexplicitly, but it gives a sequence of increasing approximations whose limit is\nthe announced absolutely normal number. The $n$-th approximation has an error\nless than $2^{2^{-n}}$. To obtain the $n$-th approximation the construction\nrequires, in the worst case, a number of mathematical operations that is double\nexponential in $n$. We consider variants on the construction that reduce the\ncomputational complexity at the expense of an increment in discrepancy.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 23:57:14 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Alvarez", "Nicol\u00e1s", ""], ["Becher", "Ver\u00f3nica", ""]]}, {"id": "1510.02188", "submitter": "Zhi-Hong Deng", "authors": "Zhi-Hong Deng, Shulei Ma, He Liu", "title": "An Efficient Data Structure for Fast Mining High Utility Itemsets", "comments": "25 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel data structure called PUN-list, which\nmaintains both the utility information about an itemset and utility upper bound\nfor facilitating the processing of mining high utility itemsets. Based on\nPUN-lists, we present a method, called MIP (Mining high utility Itemset using\nPUN-Lists), for fast mining high utility itemsets. The efficiency of MIP is\nachieved with three techniques. First, itemsets are represented by a highly\ncondensed data structure, PUN-list, which avoids costly, repeatedly utility\ncomputation. Second, the utility of an itemset can be efficiently calculated by\nscanning the PUN-list of the itemset and the PUN-lists of long itemsets can be\nfast constructed by the PUN-lists of short itemsets. Third, by employing the\nutility upper bound lying in the PUN-lists as the pruning strategy, MIP\ndirectly discovers high utility itemsets from the search space, called\nset-enumeration tree, without generating numerous candidates. Extensive\nexperiments on various synthetic and real datasets show that PUN-list is very\neffective since MIP is at least an order of magnitude faster than recently\nreported algorithms on average.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 03:04:12 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Deng", "Zhi-Hong", ""], ["Ma", "Shulei", ""], ["Liu", "He", ""]]}, {"id": "1510.02197", "submitter": "Ante \\'Custi\\'c", "authors": "Ante \\'Custi\\'c, Abraham P. Punnen", "title": "A characterization of linearizable instances of the quadratic minimum\n  spanning tree problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate special cases of the quadratic minimum spanning tree problem\n(QMSTP) on a graph $G=(V,E)$ that can be solved as a linear minimum spanning\ntree problem. Characterization of such problems on graphs with special\nproperties are given. This include complete graphs, complete bipartite graphs,\ncactuses among others. Our characterization can be verified in $O(|E|^2)$ time.\nIn the case of complete graphs and when the cost matrix is given in factored\nform, we show that our characterization can be verified in $O(|E|)$ time.\nRelated open problems are also indicated.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 04:46:37 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["\u0106usti\u0107", "Ante", ""], ["Punnen", "Abraham P.", ""]]}, {"id": "1510.02215", "submitter": "Ethan R. Elenberg", "authors": "Ethan R. Elenberg, Karthikeyan Shanmugam, Michael Borokhovich,\n  Alexandros G. Dimakis", "title": "Distributed Estimation of Graph 4-Profiles", "comments": "To appear in part at WWW'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel distributed algorithm for counting all four-node induced\nsubgraphs in a big graph. These counts, called the $4$-profile, describe a\ngraph's connectivity properties and have found several uses ranging from\nbioinformatics to spam detection. We also study the more complicated problem of\nestimating the local $4$-profiles centered at each vertex of the graph. The\nlocal $4$-profile embeds every vertex in an $11$-dimensional space that\ncharacterizes the local geometry of its neighborhood: vertices that connect\ndifferent clusters will have different local $4$-profiles compared to those\nthat are only part of one dense cluster.\n  Our algorithm is a local, distributed message-passing scheme on the graph and\ncomputes all the local $4$-profiles in parallel. We rely on two novel\ntheoretical contributions: we show that local $4$-profiles can be calculated\nusing compressed two-hop information and also establish novel concentration\nresults that show that graphs can be substantially sparsified and still retain\ngood approximation quality for the global $4$-profile.\n  We empirically evaluate our algorithm using a distributed GraphLab\nimplementation that we scaled up to $640$ cores. We show that our algorithm can\ncompute global and local $4$-profiles of graphs with millions of edges in a few\nminutes, significantly improving upon the previous state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 07:49:23 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 08:49:33 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Elenberg", "Ethan R.", ""], ["Shanmugam", "Karthikeyan", ""], ["Borokhovich", "Michael", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1510.02637", "submitter": "Jakub Radoszewski", "authors": "Tomasz Kociumaka, Jakub Radoszewski, Wojciech Rytter", "title": "Efficient Ranking of Lyndon Words and Decoding Lexicographically Minimal\n  de Bruijn Sequence", "comments": "Improved version of a paper presented at CPM 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give efficient algorithms for ranking Lyndon words of length n over an\nalphabet of size {\\sigma}. The rank of a Lyndon word is its position in the\nsequence of lexicographically ordered Lyndon words of the same length. The\noutputs are integers of exponential size, and complexity of arithmetic\noperations on such large integers cannot be ignored. Our model of computations\nis the word-RAM, in which basic arithmetic operations on (large) numbers of\nsize at most {\\sigma}^n take O(n) time. Our algorithm for ranking Lyndon words\nmakes O(n^2) arithmetic operations (this would imply directly cubic time on\nword-RAM). However, using an algebraic approach we are able to reduce the total\ntime complexity on the word-RAM to O(n^2 log {\\sigma}). We also present an\nO(n^3 log^2 {\\sigma})-time algorithm that generates the Lyndon word of a given\nlength and rank in lexicographic order. Finally we use the connections between\nLyndon words and lexicographically minimal de Bruijn sequences (theorem of\nFredricksen and Maiorana) to develop the first polynomial-time algorithm for\ndecoding minimal de Bruijn sequence of any rank n (it determines the position\nof an arbitrary word of length n within the de Bruijn sequence).\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 11:28:49 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Kociumaka", "Tomasz", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""]]}, {"id": "1510.02694", "submitter": "Kurt Mehlhorn", "authors": "Rann Duan and Jugal Garg and Kurt Mehlhorn", "title": "An Improved Combinatorial Polynomial Algorithm for the Linear\n  Arrow-Debreu Market", "comments": "to appear in SODA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an improved combinatorial algorithm for the computation of\nequilibrium prices in the linear Arrow-Debreu model. For a market with $n$\nagents and integral utilities bounded by $U$, the algorithm runs in $O(n^7\n\\log^3 (nU))$ time. This improves upon the previously best algorithm of Ye by a\nfactor of $\\tOmega(n)$. The algorithm refines the algorithm described by Duan\nand Mehlhorn and improves it by a factor of $\\tOmega(n^3)$. The improvement\ncomes from a better understanding of the iterative price adjustment process,\nthe improved balanced flow computation for nondegenerate instances, and a novel\nperturbation technique for achieving nondegeneracy.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 15:05:08 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2015 14:54:30 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Duan", "Rann", ""], ["Garg", "Jugal", ""], ["Mehlhorn", "Kurt", ""]]}, {"id": "1510.02824", "submitter": "Thomas Dybdahl Ahle", "authors": "Thomas D. Ahle and Rasmus Pagh and Ilya Razenshteyn and Francesco\n  Silvestri", "title": "On the Complexity of Inner Product Similarity Join", "comments": "in Proc. 35th ACM Symposium on Principles of Database Systems, 2016", "journal-ref": null, "doi": "10.1145/2902251.2902285", "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of tasks in classification, information retrieval, recommendation\nsystems, and record linkage reduce to the core problem of inner product\nsimilarity join (IPS join): identifying pairs of vectors in a collection that\nhave a sufficiently large inner product. IPS join is well understood when\nvectors are normalized and some approximation of inner products is allowed.\nHowever, the general case where vectors may have any length appears much more\nchallenging. Recently, new upper bounds based on asymmetric locality-sensitive\nhashing (ALSH) and asymmetric embeddings have emerged, but little has been\nknown on the lower bound side. In this paper we initiate a systematic study of\ninner product similarity join, showing new lower and upper bounds. Our main\nresults are:\n  * Approximation hardness of IPS join in subquadratic time, assuming the\nstrong exponential time hypothesis.\n  * New upper and lower bounds for (A)LSH-based algorithms. In particular, we\nshow that asymmetry can be avoided by relaxing the LSH definition to only\nconsider the collision probability of distinct elements.\n  * A new indexing method for IPS based on linear sketches, implying that our\nhardness results are not far from being tight.\n  Our technical contributions include new asymmetric embeddings that may be of\nindependent interest. At the conceptual level we strive to provide greater\nclarity, for example by distinguishing among signed and unsigned variants of\nIPS join and shedding new light on the effect of asymmetry.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 21:10:12 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 09:45:18 GMT"}, {"version": "v3", "created": "Thu, 7 Apr 2016 11:36:25 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Ahle", "Thomas D.", ""], ["Pagh", "Rasmus", ""], ["Razenshteyn", "Ilya", ""], ["Silvestri", "Francesco", ""]]}, {"id": "1510.02882", "submitter": "Dominik K\\\"oppl", "authors": "Dominik K\\\"oppl and Kunihiko Sadakane", "title": "Lempel-Ziv Computation In Compressed Space (LZ-CICS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that both the Lempel Ziv 77- and the 78-factorization of a text of\nlength $n$ on an integer alphabet of size $\\sigma$ can be computed in $O(n \\lg\n\\lg \\sigma)$ time (linear time if we allow randomization) using $O(n \\lg\n\\sigma)$ bits of working space. Given that a compressed representation of the\nsuffix tree is loaded into RAM, we can compute both factorizations in $O(n)$\ntime using $z \\lg n + O(n)$ bits of space, where $z$ is the number of factors.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 07:14:09 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 07:54:57 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2016 09:42:21 GMT"}, {"version": "v4", "created": "Wed, 16 Mar 2016 12:16:29 GMT"}, {"version": "v5", "created": "Sat, 28 May 2016 11:54:34 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["K\u00f6ppl", "Dominik", ""], ["Sadakane", "Kunihiko", ""]]}, {"id": "1510.03019", "submitter": "Tong Yang", "authors": "Tong Yang, Alex X. Liu, Muhammad Shahzad, Yuankun Zhong, Qiaobin Fu,\n  Zi Li, Gaogang Xie and Xiaoming Li", "title": "A Shifting Bloom Filter Framework for Set Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set queries are fundamental operations in computer systems and\napplications.This paper addresses the fundamental problem of designing a\nprobabilistic data structure that can quickly process set queries using a small\namount of memory. We propose a Shifting Bloom Filter (ShBF) framework for\nrepresenting and querying sets. We demonstrate the effectiveness of ShBF using\nthree types of popular set queries: membership, association, and multiplicity\nqueries. The key novelty of ShBF is on encoding the auxiliary information of a\nset element in a location offset. In contrast, prior BF based set data\nstructures allocate additional memory to store auxiliary information. To\nevaluate ShBF in comparison with prior art, we conducted experiments using\nreal-world network traces. Results show that ShBF significantly advances the\nstate-of-the-art on all three types of set queries.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 07:41:22 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2015 15:07:09 GMT"}, {"version": "v3", "created": "Tue, 22 Mar 2016 10:11:21 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Yang", "Tong", ""], ["Liu", "Alex X.", ""], ["Shahzad", "Muhammad", ""], ["Zhong", "Yuankun", ""], ["Fu", "Qiaobin", ""], ["Li", "Zi", ""], ["Xie", "Gaogang", ""], ["Li", "Xiaoming", ""]]}, {"id": "1510.03041", "submitter": "Jean-Florent Raymond", "authors": "Dimitris Chatzidimitriou, Jean-Florent Raymond, Ignasi Sau, Dimitrios\n  M. Thilikos", "title": "Minors in graphs of large ${\\theta}_r$-girth", "comments": "Some of the results of this paper have been presented in WAOA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For every $r \\in \\mathbb{N}$, let $\\theta_r$ denote the graph with two\nvertices and $r$ parallel edges. The $\\theta_r$-girth of a graph $G$ is the\nminimum number of edges of a subgraph of $G$ that can be contracted to\n$\\theta_r$. This notion generalizes the usual concept of girth which\ncorresponds to the case $r=2$. In [Minors in graphs of large girth, Random\nStructures & Algorithms, 22(2):213--225, 2003], K\\\"uhn and Osthus showed that\ngraphs of sufficiently large minimum degree contain clique-minors whose order\nis an exponential function of their girth. We extend this result for the case\nof $\\theta_{r}$-girth and we show that the minimum degree can be replaced by\nsome connectivity measurement. As an application of our results, we prove that,\nfor every fixed $r$, graphs excluding as a minor the disjoint union of $k$\n$\\theta_{r}$'s have treewidth $O(k\\cdot \\log k)$.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 11:46:31 GMT"}, {"version": "v2", "created": "Fri, 8 Jul 2016 10:17:59 GMT"}, {"version": "v3", "created": "Tue, 17 Jan 2017 21:35:59 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Chatzidimitriou", "Dimitris", ""], ["Raymond", "Jean-Florent", ""], ["Sau", "Ignasi", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1510.03152", "submitter": "David Eppstein", "authors": "David Eppstein", "title": "Treetopes and their Graphs", "comments": "16 pages, 8 figures. To appear at 27th ACM-SIAM Symp. on Discrete\n  Algorithms (SODA 2016)", "journal-ref": "Discrete & Computational Geometry 64 (2): 259-289, 2020", "doi": "10.1007/s00454-020-00177-0", "report-no": null, "categories": "cs.CG cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define treetopes, a generalization of the three-dimensional roofless\npolyhedra (Halin graphs) to arbitrary dimensions. Like roofless polyhedra,\ntreetopes have a designated base facet such that every face of dimension\ngreater than one intersects the base in more than one point. We prove an\nequivalent characterization of the 4-treetopes using the concept of clustered\nplanarity from graph drawing, and we use this characterization to recognize the\ngraphs of 4-treetopes in polynomial time. This result provides one of the first\nclasses of 4-polytopes, other than pyramids and stacked polytopes, that can be\nrecognized efficiently from their graphs.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 06:27:17 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Eppstein", "David", ""]]}, {"id": "1510.03252", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Sanjeev Khanna, Yang Li, Val Tannen", "title": "Dynamic Sketching for Graph Optimization Problems with Applications to\n  Cut-Preserving Sketches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new model for sublinear algorithms called\n\\emph{dynamic sketching}. In this model, the underlying data is partitioned\ninto a large \\emph{static} part and a small \\emph{dynamic} part and the goal is\nto compute a summary of the static part (i.e, a \\emph{sketch}) such that given\nany \\emph{update} for the dynamic part, one can combine it with the sketch to\ncompute a given function. We say that a sketch is \\emph{compact} if its size is\nbounded by a polynomial function of the length of the dynamic data,\n(essentially) independent of the size of the static part.\n  A graph optimization problem $P$ in this model is defined as follows. The\ninput is a graph $G(V,E)$ and a set $T \\subseteq V$ of $k$ terminals; the edges\nbetween the terminals are the dynamic part and the other edges in $G$ are the\nstatic part. The goal is to summarize the graph $G$ into a compact sketch (of\nsize poly$(k)$) such that given any set $Q$ of edges between the terminals, one\ncan answer the problem $P$ for the graph obtained by inserting all edges in $Q$\nto $G$, using only the sketch.\n  We study the fundamental problem of computing a maximum matching and prove\ntight bounds on the sketch size. In particular, we show that there exists a\n(compact) dynamic sketch of size $O(k^2)$ for the matching problem and any such\nsketch has to be of size $\\Omega(k^2)$. Our sketch for matchings can be further\nused to derive compact dynamic sketches for other fundamental graph problems\ninvolving cuts and connectivities. Interestingly, our sketch for matchings can\nalso be used to give an elementary construction of a \\emph{cut-preserving\nvertex sparsifier} with space $O(kC^2)$ for $k$-terminal graphs; here $C$ is\nthe total capacity of the edges incident on the terminals. Additionally, we\ngive an improved lower bound (in terms of $C$) of $\\Omega(C/\\log{C})$ on size\nof cut-preserving vertex sparsifiers.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 12:29:40 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Assadi", "Sepehr", ""], ["Khanna", "Sanjeev", ""], ["Li", "Yang", ""], ["Tannen", "Val", ""]]}, {"id": "1510.03339", "submitter": "Kurt Mehlhorn", "authors": "Kurt Mehlhorn and Sanjeev Saxena", "title": "A Still Simpler Way of Introducing the Interior-Point Method for Linear\n  Programming", "comments": "Updates and replaces arXiv:1412.0652", "journal-ref": "Computer Science Review, Volume 22, Pages 1-11 (November 2016)", "doi": "10.1016/j.cosrev.2016.07.001", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear programming is now included in algorithm undergraduate and\npostgraduate courses for computer science majors. We give a self-contained\ntreatment of an interior-point method which is particularly tailored to the\ntypical mathematical background of CS students. In particular, only limited\nknowledge of linear algebra and calculus is assumed.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 15:37:07 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 09:49:50 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Mehlhorn", "Kurt", ""], ["Saxena", "Sanjeev", ""]]}, {"id": "1510.03362", "submitter": "Alejandro Salinger", "authors": "Jan Reineke and Alejandro Salinger", "title": "On the Smoothness of Paging Algorithms", "comments": "Full version of paper presented at WAOA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the smoothness of paging algorithms. How much can the number of page\nfaults increase due to a perturbation of the request sequence? We call a paging\nalgorithm smooth if the maximal increase in page faults is proportional to the\nnumber of changes in the request sequence. We also introduce quantitative\nsmoothness notions that measure the smoothness of an algorithm. We derive lower\nand upper bounds on the smoothness of deterministic and randomized\ndemand-paging and competitive algorithms. Among strongly-competitive\ndeterministic algorithms LRU matches the lower bound, while FIFO matches the\nupper bound.\n  Well-known randomized algorithms like Partition, Equitable, or Mark are shown\nnot to be smooth. We introduce two new randomized algorithms, called\nSmoothed-LRU and LRU-Random. Smoothed- LRU allows to sacrifice competitiveness\nfor smoothness, where the trade-off is controlled by a parameter. LRU-Random is\nat least as competitive as any deterministic algorithm while smoother.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 16:49:13 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Reineke", "Jan", ""], ["Salinger", "Alejandro", ""]]}, {"id": "1510.03367", "submitter": "Peter Huggins PhD", "authors": "Peter Huggins", "title": "Layered Heaps Beating Standard and Fibonacci Heaps in Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classic problem of designing heaps. Standard binary heaps run\nfaster in practice than Fibonacci heaps but have worse time guarantees. Here we\npresent a new type of heap, a layered heap, that runs faster in practice than\nboth standard binary and Fibonacci heaps, but has asymptotic insert times\nbetter than that of binary heaps. Our heap is defined recursively and maximum\nrun time speed up occurs when a recursion depth of 1 is used, i.e. a heap of\nheaps.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 17:03:33 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Huggins", "Peter", ""]]}, {"id": "1510.03564", "submitter": "Gregory Gutin", "authors": "Florian Barbero, Gregory Gutin, Mark Jones, Bin Sheng and Anders Yeo", "title": "Linear-Vertex Kernel for the Problem of Packing $r$-Stars into a Graph\n  without Long Induced Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let integers $r\\ge 2$ and $d\\ge 3$ be fixed. Let ${\\cal G}_d$ be the set of\ngraphs with no induced path on $d$ vertices. We study the problem of packing\n$k$ vertex-disjoint copies of $K_{1,r}$ ($k\\ge 2$) into a graph $G$ from\nparameterized preprocessing, i.e., kernelization, point of view. We show that\nevery graph $G\\in {\\cal G}_d$ can be reduced, in polynomial time, to a graph\n$G'\\in {\\cal G}_d$ with $O(k)$ vertices such that $G$ has at least $k$\nvertex-disjoint copies of $K_{1,r}$ if and only if $G'$ has. Such a result is\nknown for arbitrary graphs $G$ when $r=2$ and we conjecture that it holds for\nevery $r\\ge 2$.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 08:02:41 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Barbero", "Florian", ""], ["Gutin", "Gregory", ""], ["Jones", "Mark", ""], ["Sheng", "Bin", ""], ["Yeo", "Anders", ""]]}, {"id": "1510.03742", "submitter": "Joseph Fitzsimons", "authors": "Liming Zhao, Carlos A. P\\'erez-Delgado and Joseph F. Fitzsimons", "title": "Fast graph operations in quantum computation", "comments": "9 pages, 1 figure. Comments welcome", "journal-ref": "Phys. Rev. A 93, 032314 (2016)", "doi": "10.1103/PhysRevA.93.032314", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The connection between certain entangled states and graphs has been heavily\nstudied in the context of measurement-based quantum computation as a tool for\nunderstanding entanglement. Here we show that this correspondence can be\nharnessed in the reverse direction to yield a graph data structure which allows\nfor more efficient manipulation and comparison of graphs than any possible\nclassical structure. We introduce efficient algorithms for many transformation\nand comparison operations on graphs represented as graph states, and prove that\nno classical data structure can have similar performance for the full set of\noperations studied.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 15:32:47 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Zhao", "Liming", ""], ["P\u00e9rez-Delgado", "Carlos A.", ""], ["Fitzsimons", "Joseph F.", ""]]}, {"id": "1510.03895", "submitter": "Matti Karppa", "authors": "Matti Karppa and Petteri Kaski and Jukka Kohonen", "title": "A faster subquadratic algorithm for finding outlier correlations", "comments": "ACM TALG, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of detecting outlier pairs of strongly correlated\nvariables among a collection of $n$ variables with otherwise weak pairwise\ncorrelations. After normalization, this task amounts to the geometric task\nwhere we are given as input a set of $n$ vectors with unit Euclidean norm and\ndimension $d$, and for some constants $0<\\tau<\\rho<1$, we are asked to find all\nthe outlier pairs of vectors whose inner product is at least $\\rho$ in absolute\nvalue, subject to the promise that all but at most $q$ pairs of vectors have\ninner product at most $\\tau$ in absolute value.\n  Improving on an algorithm of G. Valiant [FOCS 2012; J. ACM 2015], we present\na randomized algorithm that for Boolean inputs ($\\{-1,1\\}$-valued data\nnormalized to unit Euclidean length) runs in time \\[ \\tilde\nO\\bigl(n^{\\max\\,\\{1-\\gamma+M(\\Delta\\gamma,\\gamma),\\,M(1-\\gamma,2\\Delta\\gamma)\\}}+qdn^{2\\gamma}\\bigr)\\,,\n\\] where $0<\\gamma<1$ is a constant tradeoff parameter and $M(\\mu,\\nu)$ is the\nexponent to multiply an $\\lfloor n^\\mu\\rfloor\\times\\lfloor n^\\nu\\rfloor$ matrix\nwith an $\\lfloor n^\\nu\\rfloor\\times \\lfloor n^\\mu\\rfloor$ matrix and\n$\\Delta=1/(1-\\log_\\tau\\rho)$. As corollaries we obtain randomized algorithms\nthat run in time \\[ \\tilde\nO\\bigl(n^{\\frac{2\\omega}{3-\\log_\\tau\\rho}}+qdn^{\\frac{2(1-\\log_\\tau\\rho)}{3-\\log_\\tau\\rho}}\\bigr)\n\\] and in time \\[ \\tilde\nO\\bigl(n^{\\frac{4}{2+\\alpha(1-\\log_\\tau\\rho)}}+qdn^{\\frac{2\\alpha(1-\\log_\\tau\\rho)}{2+\\alpha(1-\\log_\\tau\\rho)}}\\bigr)\\,,\n\\] where $2\\leq\\omega<2.38$ is the exponent for square matrix multiplication\nand $0.3<\\alpha\\leq 1$ is the exponent for rectangular matrix multiplication.\nThe notation $\\tilde O(\\cdot)$ hides polylogarithmic factors in $n$ and $d$\nwhose degree may depend on $\\rho$ and $\\tau$. We present further corollaries\nfor the light bulb problem and for learning sparse Boolean functions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 21:03:27 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 10:57:28 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Karppa", "Matti", ""], ["Kaski", "Petteri", ""], ["Kohonen", "Jukka", ""]]}, {"id": "1510.03945", "submitter": "Jean-Florent Raymond", "authors": "Dimitris Chatzidimitriou and Jean-Florent Raymond and Ignasi Sau and\n  Dimitrios M. Thilikos", "title": "An $O(\\log OPT)$-approximation for covering and packing minor models of\n  ${\\theta}_r$", "comments": "Some of the results of this paper have been presented in WAOA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two graphs $G$ and $H$, we define $\\textsf{v-cover}_{H}(G)$ (resp.\n$\\textsf{e-cover}_{H}(G)$) as the minimum number of vertices (resp. edges)\nwhose removal from $G$ produces a graph without any minor isomorphic to ${H}$.\nAlso $\\textsf{v-pack}_{H}(G)$ (resp. $\\textsf{v-pack}_{H}(G)$) is the maximum\nnumber of vertex- (resp. edge-) disjoint subgraphs of $G$ that contain a minor\nisomaorphic to $H$. We denote by $\\theta_r$ the graph with two vertices and $r$\nparallel edges between them. When $H=\\theta_r$, the parameters\n$\\textsf{v-cover}_{H}$, $\\textsf{e-cover}_{H}$, $\\textsf{v-pack}_{H}$, and\n$\\textsf{v-pack}_{H}$ are NP-hard to compute (for sufficiently big values of\n$r$). Drawing upon combinatorial results in [Minors in graphs of large\n$\\theta_r$-girth, Chatzidimitriou et al., arXiv:1510.03041], we give an\nalgorithmic proof that if $\\textsf{v-pack}_{\\theta_r}(G)\\leq k$, then\n$\\textsf{v-cover}_{\\theta_r}(G) = O(k\\log k)$, and similarly for\n$\\textsf{v-pack}_{\\theta_r}$ and $\\textsf{e-cover}_{\\theta_r}$. In other words,\nthe class of graphs containing ${\\theta_r}$ as a minor has the vertex/edge\nErd\\H{o}s-P\\'osa property, for every positive integer $r$. Using the\nalgorithmic machinery of our proofs, we introduce a unified approach for the\ndesign of an $O(\\log {\\rm OPT})$-approximation algorithm for\n$\\textsf{v-pack}_{\\theta_r}$, $\\textsf{v-cover}_{\\theta_r}$,\n$\\textsf{v-pack}_{\\theta_r}$, and $\\textsf{e-cover}_{\\theta_r}$ that runs in\n$O(n\\cdot \\log(n)\\cdot m)$ steps. Also, we derive several new\nErd\\H{o}s-P\\'osa-type results from the techniques that we introduce.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 01:41:15 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 21:44:13 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Chatzidimitriou", "Dimitris", ""], ["Raymond", "Jean-Florent", ""], ["Sau", "Ignasi", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1510.04099", "submitter": "Chihao Zhang", "authors": "Lingxiao Huang, Pinyan Lu and Chihao Zhang", "title": "Canonical Paths for MCMC: from Art to Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) method is a widely used algorithm design\nscheme with many applications. To make efficient use of this method, the key\nstep is to prove that the Markov chain is rapid mixing. Canonical paths is one\nof the two main tools to prove rapid mixing. However, there are much fewer\nsuccess examples comparing to coupling, the other main tool. The main reason is\nthat there is no systematic approach or general recipe to design canonical\npaths. Building up on a previous exploration by McQuillan, we develop a general\ntheory to design canonical paths for MCMC: We reduce the task of designing\ncanonical paths to solving a set of linear equations, which can be\nautomatically done even by a machine.\n  Making use of this general approach, we obtain fully polynomial-time\nrandomized approximation schemes (FPRAS) for counting the number of\n$b$-matching with $b\\leq 7$ and $b$-edge-cover with $b\\leq 2$. They are natural\ngeneralizations of matchings and edge covers for graphs. No polynomial time\napproximation was previously known for these problems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 14:01:40 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Huang", "Lingxiao", ""], ["Lu", "Pinyan", ""], ["Zhang", "Chihao", ""]]}, {"id": "1510.04121", "submitter": "Igor Potapov", "authors": "Oleksiy Kurganskyy and Igor Potapov", "title": "Reachability problems for PAMs", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Piecewise affine maps (PAMs) are frequently used as a reference model to show\nthe openness of the reachability questions in other systems. The reachability\nproblem for one-dimentional PAM is still open even if we define it with only\ntwo intervals. As the main contribution of this paper we introduce new\ntechniques for solving reachability problems based on p-adic norms and weights\nas well as showing decidability for two classes of maps. Then we show the\nconnections between topological properties for PAM's orbits, reachability\nproblems and representation of numbers in a rational base system. Finally we\nshow a particular instance where the uniform distribution of the original orbit\nmay not remain uniform or even dense after making regular shifts and taking a\nfractional part in that sequence.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 14:40:59 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Kurganskyy", "Oleksiy", ""], ["Potapov", "Igor", ""]]}, {"id": "1510.04149", "submitter": "Saurabh Paul", "authors": "Saurabh Paul, Malik Magdon-Ismail, Petros Drineas", "title": "Column Selection via Adaptive Sampling", "comments": "To Appear in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting a good column (or row) subset of massive data matrices has found\nmany applications in data analysis and machine learning. We propose a new\nadaptive sampling algorithm that can be used to improve any relative-error\ncolumn selection algorithm. Our algorithm delivers a tighter theoretical bound\non the approximation error which we also demonstrate empirically using two well\nknown relative-error column subset selection algorithms. Our experimental\nresults on synthetic and real-world data show that our algorithm outperforms\nnon-adaptive sampling as well as prior adaptive sampling approaches.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 15:23:31 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Paul", "Saurabh", ""], ["Magdon-Ismail", "Malik", ""], ["Drineas", "Petros", ""]]}, {"id": "1510.04249", "submitter": "Mikayel Samvelyan", "authors": "Svetlana Avetisyan, Mikayel Samvelyan, Martun Karapetyan", "title": "Random Irregular Block-hierarchical Networks: Algorithms for Computation\n  of Main Properties", "comments": "The original is in Russian", "journal-ref": "Proceedings of the IX Annual Scientific Conference in\n  Russian-Armenian University (2015) 48-60", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the class of random irregular block-hierarchical networks is\ndefined and algorithms for generation and calculation of network properties are\ndescribed. The algorithms presented for this class of networks are more\nefficient than known algorithms both in computation time and memory usage and\ncan be used to analyze topological properties of such networks. The algorithms\nare implemented in the system created by the authors for the study of\ntopological and statistical properties of random networks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 19:35:53 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2015 17:55:12 GMT"}, {"version": "v3", "created": "Sat, 9 Jan 2016 18:30:44 GMT"}, {"version": "v4", "created": "Wed, 3 Aug 2016 15:46:41 GMT"}, {"version": "v5", "created": "Thu, 15 Dec 2016 12:41:28 GMT"}, {"version": "v6", "created": "Thu, 3 May 2018 14:52:11 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Avetisyan", "Svetlana", ""], ["Samvelyan", "Mikayel", ""], ["Karapetyan", "Martun", ""]]}, {"id": "1510.04590", "submitter": "Zhengyu Wang", "authors": "Zhengyu Wang", "title": "An Improved Randomized Data Structure for Dynamic Graph Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized algorithm for dynamic graph connectivity. With\nfailure probability less than $1/n^c$ (for any constant $c$ we choose), our\nsolution has worst case running time $O(\\log^3 n)$ per edge insertion,\n$O(\\log^4 n)$ per edge deletion, and $O(\\log n/\\log\\log n)$ per query, where\n$n$ is the number of vertices. The previous best algorithm has worst case\nrunning time $O(\\log^4 n)$ per edge insertion and $O(\\log^5 n)$ per edge\ndeletion. The improvement is made by reducing the randomness used in the\nprevious result, so that we save a $\\log n$ factor in update time.\n  Specifically, \\cite{kapron2013dynamic} uses $\\log n$ copies of a data\nstructure in order to boost a success probability from $1/2$ to $1-n^{-c}$. We\nshow that, in fact though, because of the special structure of their algorithm,\nthis boosting via repetition is unnecessary. Rather, we can still obtain the\nsame correctness guarantee with high probability by arguing via a new\ninvariant, without repetition.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 15:41:36 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Wang", "Zhengyu", ""]]}, {"id": "1510.04597", "submitter": "Damien Fay", "authors": "Damien Fay", "title": "Predictive partitioning for efficient BFS traversal in social networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how graph structure can be used to drastically reduce\nthe computational bottleneck of the Breadth First Search algorithm (the\nfoundation of many graph traversal techniques). In particular, we address\nparallel implementations where the bottleneck is the number of messages between\nprocessors emitted at the peak iteration. First, we derive an expression for\nthe expected degree distribution of vertices in the frontier of the algorithm\nwhich is shown to be highly skewed. Subsequently, we derive an expression for\nthe expected message along an edge in a particular iteration. This skew\nsuggests a weighted, iteration based, partition would be advantageous.\nEmploying the METIS algorithm we then show empirically that such partitions can\nreduce the message overhead by up to 50% in some particular instances and in\nthe order of 20% on average. These results have implications for graph\nprocessing in multiprocessor and distributed computing environments.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 15:50:51 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Fay", "Damien", ""]]}, {"id": "1510.04622", "submitter": "Arturs Backurs", "authors": "Amir Abboud, Arturs Backurs, Thomas Dueholm Hansen, Virginia\n  Vassilevska Williams, Or Zamir", "title": "Subtree Isomorphism Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Subtree Isomorphism problem asks whether a given tree is contained in\nanother given tree. The problem is of fundamental importance and has been\nstudied since the 1960s. For some variants, e.g., ordered trees, near-linear\ntime algorithms are known, but for the general case truly subquadratic\nalgorithms remain elusive.\n  Our first result is a reduction from the Orthogonal Vectors problem to\nSubtree Isomorphism, showing that a truly subquadratic algorithm for the latter\nrefutes the Strong Exponential Time Hypothesis (SETH).\n  In light of this conditional lower bound, we focus on natural special cases\nfor which no truly subquadratic algorithms are known. We classify these cases\nagainst the quadratic barrier, showing in particular that:\n  -- Even for binary, rooted trees, a truly subquadratic algorithm refutes\nSETH.\n  -- Even for rooted trees of depth $O(\\log\\log{n})$, where $n$ is the total\nnumber of vertices, a truly subquadratic algorithm refutes SETH.\n  -- For every constant $d$, there is a constant $\\epsilon_d>0$ and a\nrandomized, truly subquadratic algorithm for degree-$d$ rooted trees of depth\nat most $(1+ \\epsilon_d) \\log_{d}{n}$. In particular, there is an $O(\\min\\{\n2.85^h ,n^2 \\})$ algorithm for binary trees of depth $h$.\n  Our reductions utilize new \"tree gadgets\" that are likely useful for future\nSETH-based lower bounds for problems on trees. Our upper bounds apply a\nfolklore result from randomized decision tree complexity.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 16:50:31 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Abboud", "Amir", ""], ["Backurs", "Arturs", ""], ["Hansen", "Thomas Dueholm", ""], ["Williams", "Virginia Vassilevska", ""], ["Zamir", "Or", ""]]}, {"id": "1510.04676", "submitter": "Martin Aum\\\"uller", "authors": "Martin Aum\\\"uller, Martin Dietzfelbinger, and Pascal Klaue", "title": "How Good is Multi-Pivot Quicksort?", "comments": "Submitted to a journal, v2: Fixed statement of Gibb's inequality, v3:\n  Revised version, especially improving on the experiments in Section 9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Pivot Quicksort refers to variants of classical quicksort where in the\npartitioning step $k$ pivots are used to split the input into $k + 1$ segments.\nFor many years, multi-pivot quicksort was regarded as impractical, but in 2009\na 2-pivot approach by Yaroslavskiy, Bentley, and Bloch was chosen as the\nstandard sorting algorithm in Sun's Java 7. In 2014 at ALENEX, Kushagra et al.\nintroduced an even faster algorithm that uses three pivots. This paper studies\nwhat possible advantages multi-pivot quicksort might offer in general. The\ncontributions are as follows: Natural comparison-optimal algorithms for\nmulti-pivot quicksort are devised and analyzed. The analysis shows that the\nbenefits of using multiple pivots with respect to the average comparison count\nare marginal and these strategies are inferior to simpler strategies such as\nthe well known median-of-$k$ approach. A substantial part of the partitioning\ncost is caused by rearranging elements. A rigorous analysis of an algorithm for\nrearranging elements in the partitioning step is carried out, observing mainly\nhow often array cells are accessed during partitioning. The algorithm behaves\nbest if 3 to 5 pivots are used. Experiments show that this translates into good\ncache behavior and is closest to predicting observed running times of\nmulti-pivot quicksort algorithms. Finally, it is studied how choosing pivots\nfrom a sample affects sorting cost. The study is theoretical in the sense that\nalthough the findings motivate design recommendations for multipivot quicksort\nalgorithms that lead to running time improvements over known algorithms in an\nexperimental setting, these improvements are small.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 19:30:29 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2015 09:52:28 GMT"}, {"version": "v3", "created": "Tue, 31 May 2016 13:15:53 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Aum\u00fcller", "Martin", ""], ["Dietzfelbinger", "Martin", ""], ["Klaue", "Pascal", ""]]}, {"id": "1510.04796", "submitter": "Sumit Mishra", "authors": "Sumit Mishra, Samrat Mondal and Sriparna Saha", "title": "Improved Solution to the Non-Domination Level Update Problem", "comments": "18 pages and 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-domination level update problem is to sort the non-dominated fronts after\ninsertion or deletion of a solution. Generally the solution to this problem\nrequires to perform the complete non-dominated sorting which is too expensive\nin terms of number of comparisons. Recently an Efficient Non-domination Level\nUpdate (ENLU) approach is proposed which does not perform the complete sorting.\nFor this purpose, in this paper a space efficient version of ENLU approach is\nproposed without compromising the number of comparisons. However this approach\ndoes not work satisfactorily in all the cases. So we have also proposed another\ntree based approach for solving this non-domination level update problem. In\ncase of insertion, the tree based approach always checks for same number of\nfronts unlike linear approach in which the number of fronts to be checked\ndepends on the inserted solution. The result shows that in case where all the\nsolutions are dominating in nature the maximum number of comparisons using tree\nbased approach is $\\mathcal{O}(\\log N)$ as opposed to $\\mathcal{O}(N)$ in ENLU\napproach. When all the solutions are equally divided into $K$ fronts such that\neach solution in a front is dominated by all the solutions in the previous\nfront then the maximum number of comparisons to find a deleted solution in case\nof tree based approach is $K{-}\\log K$ less than that of ENLU approach. Using\nthese approaches an on-line sorting algorithm is also proposed and the\ncompetitive analysis of this algorithm is also presented.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 06:53:19 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Mishra", "Sumit", ""], ["Mondal", "Samrat", ""], ["Saha", "Sriparna", ""]]}, {"id": "1510.04991", "submitter": "Aviad Rubinstein", "authors": "Aviad Rubinstein", "title": "Honest signaling in zero-sum games is hard, and lying is even harder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that, assuming the exponential time hypothesis, finding an\n\\epsilon-approximately optimal symmetric signaling scheme in a two-player\nzero-sum game requires quasi-polynomial time. This is tight by [Cheng et al.,\nFOCS'15] and resolves an open question of [Dughmi, FOCS'14]. We also prove that\nfinding a multiplicative approximation is NP-hard.\n  We also introduce a new model where a dishonest signaler may publicly commit\nto use one scheme, but post signals according to a different scheme. For this\nmodel, we prove that even finding a (1-2^{-n})-approximately optimal scheme is\nNP-hard.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 19:17:15 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 03:26:14 GMT"}, {"version": "v3", "created": "Tue, 18 Apr 2017 22:39:10 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Rubinstein", "Aviad", ""]]}, {"id": "1510.05043", "submitter": "Sanjoy Dasgupta", "authors": "Sanjoy Dasgupta", "title": "A cost function for similarity-based hierarchical clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of algorithms for hierarchical clustering has been hampered\nby a shortage of precise objective functions. To help address this situation,\nwe introduce a simple cost function on hierarchies over a set of points, given\npairwise similarities between those points. We show that this criterion behaves\nsensibly in canonical instances and that it admits a top-down construction\nprocedure with a provably good approximation ratio.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 22:48:28 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Dasgupta", "Sanjoy", ""]]}, {"id": "1510.05058", "submitter": "Victor Amelkin", "authors": "Victor Amelkin, Ambuj Singh, Petko Bogdanov", "title": "A Distance Measure for the Analysis of Polar Opinion Dynamics in Social\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/ICDE.2017.64", "report-no": null, "categories": "cs.SI cs.DM cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of opinion dynamics in social networks plays an important role in\ntoday's life. For applications such as predicting users' political preference,\nit is particularly important to be able to analyze the dynamics of competing\nopinions. While observing the evolution of polar opinions of a social network's\nusers over time, can we tell when the network \"behaved\" abnormally?\nFurthermore, can we predict how the opinions of the users will change in the\nfuture? Do opinions evolve according to existing network opinion dynamics\nmodels? To answer such questions, it is not sufficient to study individual user\nbehavior, since opinions can spread far beyond users' egonets. We need a method\nto analyze opinion dynamics of all network users simultaneously and capture the\neffect of individuals' behavior on the global evolution pattern of the social\nnetwork.\n  In this work, we introduce Social Network Distance (SND) - a distance measure\nthat quantifies the \"cost\" of evolution of one snapshot of a social network\ninto another snapshot under various models of polar opinion propagation. SND\nhas a rich semantics of a transportation problem, yet, is computable in time\nlinear in the number of users, which makes SND applicable to the analysis of\nlarge-scale online social networks. In our experiments with synthetic and\nreal-world Twitter data, we demonstrate the utility of our distance measure for\nanomalous event detection. It achieves a true positive rate of 0.83, twice as\nhigh as that of alternatives. When employed for opinion prediction in Twitter,\nour method's accuracy is 75.63%, which is 7.5% higher than that of the next\nbest method.\n  Source Code: https://cs.ucsb.edu/~victor/pub/ucsb/dbl/snd/\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 01:12:37 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Amelkin", "Victor", ""], ["Singh", "Ambuj", ""], ["Bogdanov", "Petko", ""]]}, {"id": "1510.05093", "submitter": "Serge Gaspers", "authors": "Manfred Cochefert, Jean-Francois Couturier, Serge Gaspers, Dieter\n  Kratsch", "title": "Faster algorithms to enumerate hypergraph transversals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A transversal of a hypergraph is a set of vertices intersecting each\nhyperedge. We design and analyze new exponential-time algorithms to enumerate\nall inclusion-minimal transversals of a hypergraph. For each fixed k>2, our\nalgorithms for hypergraphs of rank k, where the rank is the maximum size of a\nhyperedge, outperform the previous best. This also implies improved upper\nbounds on the maximum number of minimal transversals in n-vertex hypergraphs of\nrank k>2. Our main algorithm is a branching algorithm whose running time is\nanalyzed with Measure and Conquer. It enumerates all minimal transversals of\nhypergraphs of rank 3 on n vertices in time O(1.6755^n). Our algorithm for\nhypergraphs of rank 4 is based on iterative compression. Our enumeration\nalgorithms improve upon the best known algorithms for counting minimum\ntransversals in hypergraphs of rank k for k>2 and for computing a minimum\ntransversal in hypergraphs of rank k for k>5.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 07:56:35 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Cochefert", "Manfred", ""], ["Couturier", "Jean-Francois", ""], ["Gaspers", "Serge", ""], ["Kratsch", "Dieter", ""]]}, {"id": "1510.05137", "submitter": "Xue Chen", "authors": "Xue Chen", "title": "Integrality Gaps and Approximation Algorithms for Dispersers and\n  Bipartite Expanders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of approximating the quality of a disperser. A bipartite\ngraph $G$ on $([N],[M])$ is a $(\\rho N,(1-\\delta)M)$-disperser if for any\nsubset $S\\subseteq [N]$ of size $\\rho N$, the neighbor set $\\Gamma(S)$ contains\nat least $(1-\\delta)M$ distinct vertices. Our main results are strong\nintegrality gaps in the Lasserre hierarchy and an approximation algorithm for\ndispersers.\n  \\begin{enumerate}\n  \\item For any $\\alpha>0$, $\\delta>0$, and a random bipartite graph $G$ with\nleft degree $D=O(\\log N)$, we prove that the Lasserre hierarchy cannot\ndistinguish whether $G$ is an $(N^{\\alpha},(1-\\delta)M)$-disperser or not an\n$(N^{1-\\alpha},\\delta M)$-disperser.\n  \\item For any $\\rho>0$, we prove that there exist infinitely many constants\n$d$ such that the Lasserre hierarchy cannot distinguish whether a random\nbipartite graph $G$ with right degree $d$ is a $(\\rho N,\n(1-(1-\\rho)^d)M)$-disperser or not a $(\\rho N, (1-\\Omega(\\frac{1-\\rho}{\\rho d +\n1-\\rho}))M)$-disperser. We also provide an efficient algorithm to find a subset\nof size exact $\\rho N$ that has an approximation ratio matching the integrality\ngap within an extra loss of\n$\\frac{\\min\\{\\frac{\\rho}{1-\\rho},\\frac{1-\\rho}{\\rho}\\}}{\\log d}$.\n\\end{enumerate}\n  Our method gives an integrality gap in the Lasserre hierarchy for bipartite\nexpanders with left degree~$D$. $G$ on $([N],[M])$ is a $(\\rho N,a)$-expander\nif for any subset $S\\subseteq [N]$ of size $\\rho N$, the neighbor set\n$\\Gamma(S)$ contains at least $a \\cdot \\rho N$ distinct vertices. We prove that\nfor any constant $\\epsilon>0$, there exist constants $\\epsilon'<\\epsilon,\\rho,$\nand $D$ such that the Lasserre hierarchy cannot distinguish whether a bipartite\ngraph on $([N],[M])$ with left degree $D$ is a $(\\rho N,\n(1-\\epsilon')D)$-expander or not a $(\\rho N, (1-\\epsilon)D)$-expander.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 15:37:42 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Chen", "Xue", ""]]}, {"id": "1510.05179", "submitter": "Jeremy Kepner", "authors": "Karia Dibert, Hayden Jansen, Jeremy Kepner", "title": "Algebraic Conditions for Generating Accurate Adjacency Arrays", "comments": "2015 IEEE MIT Undergraduate Research Technology Conference", "journal-ref": null, "doi": "10.1109/URTC.2015.7563745", "report-no": null, "categories": "cs.DB cs.DS math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data processing systems impose multiple views on data as it is processed by\nthe system. These views include spreadsheets, databases, matrices, and graphs.\nAssociative arrays unify and simplify these different approaches into a common\ntwo-dimensional view of data. Graph construction, a fundamental operation in\nthe data processing pipeline, is typically done by multiplying the incidence\narray representations of a graph, $\\mathbf{E}_\\mathrm{in}$ and\n$\\mathbf{E}_\\mathrm{out}$, to produce an adjacency matrix of the graph that can\nbe processed with a variety of machine learning clustering techniques. This\nwork focuses on establishing the mathematical criteria to ensure that the\nmatrix product $\\mathbf{E}_\\mathrm{out}^\\intercal\\mathbf{E}_\\mathrm{in}$ is the\nadjacency array of the graph. It will then be shown that these criteria are\nalso necessary and sufficient for the remaining nonzero product of incidence\narrays, $\\mathbf{E}_\\mathrm{in}^\\intercal\\mathbf{E}_\\mathrm{out}$ to be the\nadjacency matrices of the reversed graph. Algebraic structures that comply with\nthe criteria will be identified and discussed.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 23:09:49 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Dibert", "Karia", ""], ["Jansen", "Hayden", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1510.05433", "submitter": "Yaroslav Akhremtsev", "authors": "Yaroslav Akhremtsev and Peter Sanders", "title": "Fast Parallel Operations on Search Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using (a,b)-trees as an example, we show how to perform a parallel split with\nlogarithmic latency and parallel join, bulk updates, intersection, union (or\nmerge), and (symmetric) set difference with logarithmic latency and with\ninformation theoretically optimal work. We present both asymptotically optimal\nsolutions and simplified versions that perform well in practice - they are\nseveral times faster than previous implementations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 11:42:00 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 11:43:48 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Akhremtsev", "Yaroslav", ""], ["Sanders", "Peter", ""]]}, {"id": "1510.05503", "submitter": "Stefan Fafianie", "authors": "Stefan Fafianie, Stefan Kratsch, Voung Anh Quyen", "title": "Preprocessing under uncertainty", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study preprocessing for tractable problems when part of the\ninput is unknown or uncertain. This comes up naturally if, e.g., the load of\nsome machines or the congestion of some roads is not known far enough in\nadvance, or if we have to regularly solve a problem over instances that are\nlargely similar, e.g., daily airport scheduling with few charter flights.\nUnlike robust optimization, which also studies settings like this, our goal\nlies not in computing solutions that are (approximately) good for every\ninstantiation. Rather, we seek to preprocess the known parts of the input, to\nspeed up finding an optimal solution once the missing data is known.\n  We present efficient algorithms that given an instance with partially\nuncertain input generate an instance of size polynomial in the amount of\nuncertain data that is equivalent for every instantiation of the unknown part.\nConcretely, we obtain such algorithms for Minimum Spanning Tree, Minimum Weight\nMatroid Basis, and Maximum Cardinality Bipartite Maxing, where respectively the\nweight of edges, weight of elements, and the availability of vertices is\nunknown for part of the input. Furthermore, we show that there are tractable\nproblems, such as Small Connected Vertex Cover, for which one cannot hope to\nobtain similar results.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 14:51:04 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Fafianie", "Stefan", ""], ["Kratsch", "Stefan", ""], ["Quyen", "Voung Anh", ""]]}, {"id": "1510.05886", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Jiao Zhou, Ker-I Ko, Ding-zhu Du", "title": "Approximation Algorithm for Minimum Weight Connected $m$-Fold Dominating\n  Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using connected dominating set (CDS) to serve as a virtual backbone in a\nwireless networks can save energy and reduce interference. Since nodes may fail\ndue to accidental damage or energy depletion, it is desirable that the virtual\nbackbone has some fault-tolerance. A $k$-connected $m$-fold dominating set\n($(k,m)$-CDS) of a graph $G$ is a node set $D$ such that every node in\n$V\\setminus D$ has at least $m$ neighbors in $D$ and the subgraph of $G$\ninduced by $D$ is $k$-connected. Using $(k,m)$-CDS can tolerate the failure of\n$\\min\\{k-1,m-1\\}$ nodes. In this paper, we study Minimum Weight $(1,m)$-CDS\nproblem ($(1,m)$-MWCDS), and present an\n$(H(\\delta+m)+2H(\\delta-1))$-approximation algorithm, where $\\delta$ is the\nmaximum degree of the graph and $H(\\cdot)$ is the Harmonic number. Notice that\nthere is a $1.35\\ln n$-approximation algorithm for the $(1,1)$-MWCDS problem,\nwhere $n$ is the number of nodes in the graph. Though our constant in $O(\\ln\n\\cdot)$ is larger than 1.35, $n$ is replaced by $\\delta$. Such a replacement\nenables us to obtain a $(6.67+\\varepsilon)$-approximation for the $(1,m)$-MWCDS\nproblem on unit disk graphs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 13:32:28 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 00:23:25 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Zhang", "Zhao", ""], ["Zhou", "Jiao", ""], ["Ko", "Ker-I", ""], ["Du", "Ding-zhu", ""]]}, {"id": "1510.06051", "submitter": "Marie-Louise Lackner", "authors": "Michael H. Albert, Marie-Louise Lackner, Martin Lackner and Vincent\n  Vatter", "title": "The Complexity of Pattern Matching for $321$-Avoiding and Skew-Merged\n  Permutations", "comments": null, "journal-ref": "Discrete Mathematics & Theoretical Computer Science, Vol. 18 no.\n  2, Permutation Patterns 2015, Permutation Patterns (December 21, 2016)\n  dmtcs:2607", "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Permutation Pattern Matching problem, asking whether a pattern\npermutation $\\pi$ is contained in a permutation $\\tau$, is known to be\nNP-complete. In this paper we present two polynomial time algorithms for\nspecial cases. The first algorithm is applicable if both $\\pi$ and $\\tau$ are\n$321$-avoiding; the second is applicable if $\\pi$ and $\\tau$ are skew-merged.\nBoth algorithms have a runtime of $O(kn)$, where $k$ is the length of $\\pi$ and\n$n$ the length of $\\tau$.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 20:43:18 GMT"}, {"version": "v2", "created": "Sun, 18 Dec 2016 22:43:30 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Albert", "Michael H.", ""], ["Lackner", "Marie-Louise", ""], ["Lackner", "Martin", ""], ["Vatter", "Vincent", ""]]}, {"id": "1510.06073", "submitter": "David Woodruff", "authors": "Kenneth L. Clarkson and David P. Woodruff", "title": "Input Sparsity and Hardness for Robust Subspace Approximation", "comments": "paper appeared in FOCS, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the subspace approximation problem, we seek a k-dimensional subspace F of\nR^d that minimizes the sum of p-th powers of Euclidean distances to a given set\nof n points a_1, ..., a_n in R^d, for p >= 1. More generally than minimizing\nsum_i dist(a_i,F)^p,we may wish to minimize sum_i M(dist(a_i,F)) for some loss\nfunction M(), for example, M-Estimators, which include the Huber and Tukey loss\nfunctions. Such subspaces provide alternatives to the singular value\ndecomposition (SVD), which is the p=2 case, finding such an F that minimizes\nthe sum of squares of distances. For p in [1,2), and for typical M-Estimators,\nthe minimizing $F$ gives a solution that is more robust to outliers than that\nprovided by the SVD. We give several algorithmic and hardness results for these\nrobust subspace approximation problems.\n  We think of the n points as forming an n x d matrix A, and letting nnz(A)\ndenote the number of non-zero entries of A. Our results hold for p in [1,2). We\nuse poly(n) to denote n^{O(1)} as n -> infty. We obtain: (1) For minimizing\nsum_i dist(a_i,F)^p, we give an algorithm running in O(nnz(A) +\n(n+d)poly(k/eps) + exp(poly(k/eps))), (2) we show that the problem of\nminimizing sum_i dist(a_i, F)^p is NP-hard, even to output a\n(1+1/poly(d))-approximation, answering a question of Kannan and Vempala, and\ncomplementing prior results which held for p >2, (3) For loss functions for a\nwide class of M-Estimators, we give a problem-size reduction: for a parameter\nK=(log n)^{O(log k)}, our reduction takes O(nnz(A) log n + (n+d) poly(K/eps))\ntime to reduce the problem to a constrained version involving matrices whose\ndimensions are poly(K eps^{-1} log n). We also give bicriteria solutions, (4)\nOur techniques lead to the first O(nnz(A) + poly(d/eps)) time algorithms for\n(1+eps)-approximate regression for a wide class of convex M-Estimators.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 21:58:12 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Clarkson", "Kenneth L.", ""], ["Woodruff", "David P.", ""]]}, {"id": "1510.06257", "submitter": "Nicola Prezza", "authors": "Nicola Prezza and Alberto Policriti", "title": "Computing LZ77 in Run-Compressed Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that the LZ77 factorization of a text T {\\in\\Sigma^n}\ncan be computed in O(R log n) bits of working space and O(n log R) time, R\nbeing the number of runs in the Burrows-Wheeler transform of T reversed. For\nextremely repetitive inputs, the working space can be as low as O(log n) bits:\nexponentially smaller than the text itself. As a direct consequence of our\nresult, we show that a class of repetition-aware self-indexes based on a\ncombination of run-length encoded BWT and LZ77 can be built in asymptotically\noptimal O(R + z) words of working space, z being the size of the LZ77 parsing.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 14:05:24 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Prezza", "Nicola", ""], ["Policriti", "Alberto", ""]]}, {"id": "1510.06452", "submitter": "Stefan Schneider", "authors": "Marvin K\\\"unnemann, Daniel Moeller, Ramamohan Paturi, Stefan Schneider", "title": "Subquadratic Algorithms for Succinct Stable Matching", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-34171-2_21", "report-no": null, "categories": "cs.DS cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the stable matching problem when the preference lists are not\ngiven explicitly but are represented in a succinct way and ask whether the\nproblem becomes computationally easier and investigate other implications. We\ngive subquadratic algorithms for finding a stable matching in special cases of\nnatural succinct representations of the problem, the $d$-attribute, $d$-list,\ngeometric, and single-peaked models. We also present algorithms for verifying a\nstable matching in the same models. We further show that for $d = \\omega(\\log\nn)$ both finding and verifying a stable matching in the $d$-attribute and\n$d$-dimensional geometric models requires quadratic time assuming the Strong\nExponential Time Hypothesis. This suggests that these succinct models are not\nsignificantly simpler computationally than the general case for sufficiently\nlarge $d$.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 22:51:47 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2015 23:39:24 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 21:19:52 GMT"}, {"version": "v4", "created": "Mon, 18 Jul 2016 18:30:42 GMT"}, {"version": "v5", "created": "Tue, 20 Dec 2016 06:01:55 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["K\u00fcnnemann", "Marvin", ""], ["Moeller", "Daniel", ""], ["Paturi", "Ramamohan", ""], ["Schneider", "Stefan", ""]]}, {"id": "1510.06492", "submitter": "Linus Hermansson", "authors": "Linus Hermansson, Fredrik D. Johansson, and Osamu Watanabe", "title": "Generalized Shortest Path Kernel on Graphs", "comments": "Short version presented at Discovery Science 2015 in Banff", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of classifying graphs using graph kernels. We define\na new graph kernel, called the generalized shortest path kernel, based on the\nnumber and length of shortest paths between nodes. For our example\nclassification problem, we consider the task of classifying random graphs from\ntwo well-known families, by the number of clusters they contain. We verify\nempirically that the generalized shortest path kernel outperforms the original\nshortest path kernel on a number of datasets. We give a theoretical analysis\nfor explaining our experimental results. In particular, we estimate\ndistributions of the expected feature vectors for the shortest path kernel and\nthe generalized shortest path kernel, and we show some evidence explaining why\nour graph kernel outperforms the shortest path kernel for our graph\nclassification problem.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 05:49:31 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Hermansson", "Linus", ""], ["Johansson", "Fredrik D.", ""], ["Watanabe", "Osamu", ""]]}, {"id": "1510.06535", "submitter": "Uri Zwick", "authors": "Thomas Dueholm Hansen, Haim Kaplan, Robert E. Tarjan, Uri Zwick", "title": "Hollow Heaps", "comments": "27 pages, 7 figures, preliminary version appeared in ICALP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the hollow heap, a very simple data structure with the same\namortized efficiency as the classical Fibonacci heap. All heap operations\nexcept delete and delete-min take $O(1)$ time, worst case as well as amortized;\ndelete and delete-min take $O(\\log n)$ amortized time on a heap of $n$ items.\nHollow heaps are by far the simplest structure to achieve this. Hollow heaps\ncombine two novel ideas: the use of lazy deletion and re-insertion to do\ndecrease-key operations, and the use of a dag (directed acyclic graph) instead\nof a tree or set of trees to represent a heap. Lazy deletion produces hollow\nnodes (nodes without items), giving the data structure its name.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 09:09:11 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Hansen", "Thomas Dueholm", ""], ["Kaplan", "Haim", ""], ["Tarjan", "Robert E.", ""], ["Zwick", "Uri", ""]]}, {"id": "1510.07185", "submitter": "Amr Elmasry", "authors": "Omar Darwish, Amr Elmasry, Jyrki Katajainen", "title": "Memory-Adjustable Navigation Piles with Applications to Sorting and\n  Convex Hulls", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider space-bounded computations on a random-access machine (RAM) where\nthe input is given on a read-only random-access medium, the output is to be\nproduced to a write-only sequential-access medium, and the available workspace\nallows random reads and writes but is of limited capacity. The length of the\ninput is $N$ elements, the length of the output is limited by the computation,\nand the capacity of the workspace is $O(S)$ bits for some predetermined\nparameter $S$. We present a state-of-the-art priority queue---called an\nadjustable navigation pile---for this restricted RAM model. Under some\nreasonable assumptions, our priority queue supports $\\mathit{minimum}$ and\n$\\mathit{insert}$ in $O(1)$ worst-case time and $\\mathit{extract}$ in $O(N/S +\n\\lg{} S)$ worst-case time for any $S \\geq \\lg{} N$. We show how to use this\ndata structure to sort $N$ elements and to compute the convex hull of $N$\npoints in the two-dimensional Euclidean space in $O(N^2/S + N \\lg{} S)$\nworst-case time for any $S \\geq \\lg{} N$. Following a known lower bound for the\nspace-time product of any branching program for finding unique elements, both\nour sorting and convex-hull algorithms are optimal. The adjustable navigation\npile has turned out to be useful when designing other space-efficient\nalgorithms, and we expect that it will find its way to yet other applications.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 20:56:29 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Darwish", "Omar", ""], ["Elmasry", "Amr", ""], ["Katajainen", "Jyrki", ""]]}, {"id": "1510.07188", "submitter": "Yinglei Song", "authors": "Yinglei Song", "title": "On the Dominating Set Problem in Random Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the {\\sc Dominating Set} problem in random graphs. In\na random graph, each pair of vertices are joined by an edge with a probability\nof $p$, where $p$ is a positive constant less than $1$. We show that, given a\nrandom graph in $n$ vertices, a minimum dominating set in the graph can be\ncomputed in expected $2^{O(\\log_{2}^{2}{n})}$ time. For the parameterized\ndominating set problem, we show that it cannot be solved in expected\n$O(f(k)n^{c})$ time unless the minimum dominating set problem can be\napproximated within a ratio of $o(\\log_{2}n)$ in expected polynomial time,\nwhere $f(k)$ is a function of the parameter $k$ and $c$ is a constant\nindependent of $n$ and $k$. In addition, we show that the parameterized\ndominating set problem can be solved in expected $O(f(k)n^{c})$ time when the\nprobability $p$ depends on $n$ and equals to $\\frac{1}{g(n)}$, where $g(n)< n$\nis a monotonously increasing function of $n$ and its value approaches infinity\nwhen $n$ approaches infinity.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 22:43:25 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Song", "Yinglei", ""]]}, {"id": "1510.07254", "submitter": "Jian-Jia Chen", "authors": "Jian-Jia Chen", "title": "Federated Scheduling Admits No Constant Speedup Factors for\n  Constrained-Deadline DAG Task Systems", "comments": "in Real-Time Systems Journal 2016", "journal-ref": null, "doi": "10.1007/s11241-016-9255-2", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the federated scheduling approaches in multiprocessor systems, a task\neither 1) is restricted to execute sequentially on a single processor or 2) has\nexclusive access to the assigned processors. There have been several positive\nresults to conduct good federated scheduling policies, which have constant\nspeedup factors with respect to any optimal federated scheduling algorithm.\nThis paper answers an open question: \"For constrained-deadline task systems\nwith directed acyclic graph (DAG) dependency structures, do federated\nscheduling policies have a constant speedup factor with respect to any optimal\nscheduling algorithm?\" The answer is \"No!\" This paper presents an example,\nwhich demonstrates that any federated scheduling algorithm has a speedup factor\nof at least $\\Omega(\\min\\{M, N\\})$ with respect to any optimal scheduling\nalgorithm, where $N$ is the number of tasks and $M$ is the number of\nprocessors.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 14:46:12 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 19:56:30 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Chen", "Jian-Jia", ""]]}, {"id": "1510.07363", "submitter": "Hadi Pouransari", "authors": "Hadi Pouransari, Pieter Coulier, Eric Darve", "title": "Fast hierarchical solvers for sparse matrices using extended\n  sparsification and low-rank approximation", "comments": null, "journal-ref": null, "doi": "10.1137/15M1046939", "report-no": null, "categories": "math.NA cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inversion of sparse matrices with standard direct solve schemes is robust,\nbut computationally expensive. Iterative solvers, on the other hand,\ndemonstrate better scalability; but, need to be used with an appropriate\npreconditioner (e.g., ILU, AMG, Gauss-Seidel, etc.) for proper convergence. The\nchoice of an effective preconditioner is highly problem dependent. We propose a\nnovel fully algebraic sparse matrix solve algorithm, which has linear\ncomplexity with the problem size. Our scheme is based on the Gauss elimination.\nFor a given matrix, we approximate the LU factorization with a tunable accuracy\ndetermined a priori. This method can be used as a stand-alone direct solver\nwith linear complexity and tunable accuracy, or it can be used as a black-box\npreconditioner in conjunction with iterative methods such as GMRES. The\nproposed solver is based on the low-rank approximation of fill-ins generated\nduring the elimination. Similar to H-matrices, fill-ins corresponding to blocks\nthat are well-separated in the adjacency graph are represented via a\nhierarchical structure. The linear complexity of the algorithm is guaranteed if\nthe blocks corresponding to well-separated clusters of variables are\nnumerically low-rank.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 04:39:37 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 19:44:50 GMT"}, {"version": "v3", "created": "Thu, 15 Dec 2016 02:24:36 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Pouransari", "Hadi", ""], ["Coulier", "Pieter", ""], ["Darve", "Eric", ""]]}, {"id": "1510.07420", "submitter": "Nikesh Dattani", "authors": "Richard Tanburn (Oxford University), Oliver Lunt (Oxford University),\n  Nikesh S. Dattani (Kyoto University)", "title": "Crushing runtimes in adiabatic quantum computation with Energy Landscape\n  Manipulation (ELM): Application to Quantum Factoring", "comments": "Feedback Encouraged", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DM cs.DS math.NT", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We introduce two methods for speeding up adiabatic quantum computations by\nincreasing the energy between the ground and first excited states. Our methods\nare even more general. They can be used to shift a Hamiltonian's density of\nstates away from the ground state, so that fewer states occupy the low-lying\nenergies near the minimum, hence allowing for faster adiabatic passages to find\nthe ground state with less risk of getting caught in an undesired low-lying\nexcited state during the passage. Even more generally, our methods can be used\nto transform a discrete optimization problem into a new one whose unique\nminimum still encodes the desired answer, but with the objective function's\nvalues forming a different landscape. Aspects of the landscape such as the\nobjective function's range, or the values of certain coefficients, or how many\ndifferent inputs lead to a given output value, can be decreased *or* increased.\nOne of the many examples for which these methods are useful is in finding the\nground state of a Hamiltonian using NMR: If it is difficult to find a molecule\nsuch that the distances between the spins match the interactions in the\nHamiltonian, the interactions in the Hamiltonian can be changed without at all\nchanging the ground state. We apply our methods to an AQC algorithm for integer\nfactorization, and the first method reduces the maximum runtime in our example\nby up to 754%, and the second method reduces the maximum runtime of another\nexample by up to 250%. These two methods may also be combined.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 09:33:43 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Tanburn", "Richard", "", "Oxford University"], ["Lunt", "Oliver", "", "Oxford University"], ["Dattani", "Nikesh S.", "", "Kyoto University"]]}, {"id": "1510.07462", "submitter": "Szabolcs Iv\\'an", "authors": "Kitti Gelle, Szabolcs Ivan", "title": "Recognizing Union-Find trees is NP-complete", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disjoint-Set forests, consisting of Union-Find trees are data structures\nhaving a widespread practical application due to their efficiency. Despite them\nbeing well-known, no exact structural characterization of these trees is known\n(such a characterization exists for Union trees which are constructed without\nusing path compression). In this paper we provide such a characterization and\nshow that the decision problem whether a given tree is a Union-Find tree is\n$\\NP$-complete.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 12:59:22 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2017 17:44:15 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Gelle", "Kitti", ""], ["Ivan", "Szabolcs", ""]]}, {"id": "1510.07565", "submitter": "Andreas Pavlogiannis", "authors": "Krishnendu Chatterjee and Amir Kafshdar Goharshady and Rasmus\n  Ibsen-Jensen and Andreas Pavlogiannis", "title": "Algorithms for Algebraic Path Properties in Concurrent Systems of\n  Constant Treewidth Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study algorithmic questions for concurrent systems where the transitions\nare labeled from a complete, closed semiring, and path properties are algebraic\nwith semiring operations. The algebraic path properties can model dataflow\nanalysis problems, the shortest path problem, and many other natural problems\nthat arise in program analysis. We consider that each component of the\nconcurrent system is a graph with constant treewidth, a property satisfied by\nthe controlflow graphs of most programs. We allow for multiple possible\nqueries, which arise naturally in demand driven dataflow analysis. The study of\nmultiple queries allows us to consider the tradeoff between the resource usage\nof the one-time preprocessing and for each individual query. The traditional\napproach constructs the product graph of all components and applies the\nbest-known graph algorithm on the product. In this approach, even the answer to\na single query requires the transitive closure, which provides no room for\ntradeoff between preprocessing and query time.\n  Our main contributions are algorithms that significantly improve the\nworst-case running time of the traditional approach, and provide various\ntradeoffs depending on the number of queries. For example, in a concurrent\nsystem of two components, the traditional approach requires hexic time in the\nworst case for answering one query as well as computing the transitive closure,\nwhereas we show that with one-time preprocessing in almost cubic time, each\nsubsequent query can be answered in at most linear time, and even the\ntransitive closure can be computed in almost quartic time. Furthermore, we\nestablish conditional optimality results showing that the worst-case running\ntime of our algorithms cannot be improved without achieving major breakthroughs\nin graph algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 17:50:00 GMT"}], "update_date": "2015-11-27", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Goharshady", "Amir Kafshdar", ""], ["Ibsen-Jensen", "Rasmus", ""], ["Pavlogiannis", "Andreas", ""]]}, {"id": "1510.07676", "submitter": "Daniel Lokshtanov", "authors": "Mithilesh Kumar and Daniel Lokshtanov", "title": "Faster Exact and Parameterized Algorithm for Feedback Vertex Set in\n  Tournaments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tournament is a directed graph T such that every pair of vertices are\nconnected by an arc. A feedback vertex set is a set S of vertices in T such\nthat T - S is acyclic. In this article we consider the Feedback Vertex Set\nproblem in tournaments. Here input is a tournament T and integer k, and the\ntask is to determine whether T has a feedback vertex set of size at most k. We\ngive a new algorithm for Feedback Vertex Set in Tournaments. The running time\nof our algorithm is upper bounded by O(1.618^k + n^{O(1)}) and by O(1.46^n).\nThus our algorithm simultaneously improves over the fastest known parameterized\nalgorithm for the problem by Dom et al. running in time O(2^kk^{O(1)} +\nn^{O(1)}), and the fastest known exact exponential time algorithm by Gaspers\nand Mnich with running time O(1.674^n).\n  On the way to prove our main result we prove a new partitioning theorem for\nundirected graphs. In particular we show that the vertices of any undirected\nm-edge graph of maximum degree d can be colored white or black in such a way\nthat for each of the two colors, the number of edges with both endpoints of\nthat color is between m/4-d/2 and m/4+d/4.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 20:54:31 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Kumar", "Mithilesh", ""], ["Lokshtanov", "Daniel", ""]]}, {"id": "1510.07758", "submitter": "David Fern\\'andez-Baca", "authors": "Yun Deng and David Fern\\'andez-Baca", "title": "Fast Compatibility Testing for Rooted Phylogenetic Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following basic problem in phylogenetic tree construction.\nLet $\\mathcal{P} = \\{T_1, \\ldots, T_k\\}$ be a collection of rooted phylogenetic\ntrees over various subsets of a set of species. The tree compatibility problem\nasks whether there is a tree $T$ with the following property: for each $i \\in\n\\{1, \\dots, k\\}$, $T_i$ can be obtained from the restriction of $T$ to the\nspecies set of $T_i$ by contracting zero or more edges. If such a tree $T$\nexists, we say that $\\mathcal{P}$ is compatible.\n  We give a $\\tilde{O}(M_\\mathcal{P})$ algorithm for the tree compatibility\nproblem, where $M_\\mathcal{P}$ is the total number of nodes and edges in\n$\\mathcal{P}$. Unlike previous algorithms for this problem, the running time of\nour method does not depend on the degrees of the nodes in the input trees.\nThus, it is equally fast on highly resolved and highly unresolved trees.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 02:59:29 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Deng", "Yun", ""], ["Fern\u00e1ndez-Baca", "David", ""]]}, {"id": "1510.07768", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Aditya Bhaskara, Silvio Lattanzi, Vahab Mirrokni,\n  Lorenzo Orecchia", "title": "Expanders via Local Edge Flips", "comments": "To appear in the proceedings of the 27th ACM-SIAM Symposium on\n  Discrete Algorithms (SODA) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing distributed and scalable algorithms to improve network connectivity\nis a central topic in peer-to-peer networks. In this paper we focus on the\nfollowing well-known problem: given an $n$-node $d$-regular network for\n$d=\\Omega(\\log n)$, we want to design a decentralized, local algorithm that\ntransforms the graph into one that has good connectivity properties (low\ndiameter, expansion, etc.) without affecting the sparsity of the graph. To this\nend, Mahlmann and Schindelhauer introduced the random \"flip\" transformation,\nwhere in each time step, a random pair of vertices that have an edge decide to\n`swap a neighbor'. They conjectured that performing $O(n d)$ such flips at\nrandom would convert any connected $d$-regular graph into a $d$-regular\nexpander graph, with high probability. However, the best known upper bound for\nthe number of steps is roughly $O(n^{17} d^{23})$, obtained via a delicate\nMarkov chain comparison argument.\n  Our main result is to prove that a natural instantiation of the random flip\nproduces an expander in at most $O(n^2 d^2 \\sqrt{\\log n})$ steps, with high\nprobability. Our argument uses a potential-function analysis based on the\nmatrix exponential, together with the recent beautiful results on the\nhigher-order Cheeger inequality of graphs. We also show that our technique can\nbe used to analyze another well-studied random process known as the `random\nswitch', and show that it produces an expander in $O(n d)$ steps with high\nprobability.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 04:13:17 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Bhaskara", "Aditya", ""], ["Lattanzi", "Silvio", ""], ["Mirrokni", "Vahab", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1510.07773", "submitter": "Wenbin Chen", "authors": "Wenbin Chen", "title": "An O(log k log^2 n)-competitive Randomized Algorithm for the k-Sever\n  Problem", "comments": "arXiv admin note: text overlap with arXiv:1410.4955", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that there is an O(log k log^2 n)-competitive\nrandomized algorithm for the k-sever problem on any metric space with n points,\nwhich improved the previous best competitive ratio O(log^2 k log^3 n log log n)\nby Nikhil Bansal et al. (FOCS 2011, pages 267-276).\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 05:15:02 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Chen", "Wenbin", ""]]}, {"id": "1510.07825", "submitter": "Agnis \\=Ari\\c{n}\\v{s}", "authors": "Agnis \\=Ari\\c{n}\\v{s}", "title": "Span-program-based quantum algorithms for graph bipartiteness and\n  connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Span program is a linear-algebraic model of computation which can be used to\ndesign quantum algorithms. For any Boolean function there exists a span program\nthat leads to a quantum algorithm with optimal quantum query complexity. In\ngeneral, finding such span programs is not an easy task. In this work, given a\nquery access to the adjacency matrix of a simple graph $G$ with $n$ vertices,\nwe provide two new span-program-based quantum algorithms: an algorithm for\ntesting if the graph is bipartite that uses $O(n\\sqrt{n})$ quantum queries; an\nalgorithm for testing if the graph is connected that uses $O(n\\sqrt{n})$\nquantum queries.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 09:41:13 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["\u0100ri\u0146\u0161", "Agnis", ""]]}, {"id": "1510.07971", "submitter": "Elisabetta Bergamini", "authors": "Elisabetta Bergamini and Henning Meyerhenke", "title": "Approximating Betweenness Centrality in Fully-dynamic Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1504.07091,\n  arXiv:1409.6241", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Betweenness is a well-known centrality measure that ranks the nodes of a\nnetwork according to their participation in shortest paths. Since an exact\ncomputation is prohibitive in large networks, several approximation algorithms\nhave been proposed. Besides that, recent years have seen the publication of\ndynamic algorithms for efficient recomputation of betweenness in networks that\nchange over time. In this paper we propose the first betweenness centrality\napproximation algorithms with a provable guarantee on the maximum approximation\nerror for dynamic networks. Several new intermediate algorithmic results\ncontribute to the respective approximation algorithms: (i) new upper bounds on\nthe vertex diameter, (ii) the first fully-dynamic algorithm for updating an\napproximation of the vertex diameter in undirected graphs, and (iii) an\nalgorithm with lower time complexity for updating single-source shortest paths\nin unweighted graphs after a batch of edge actions. Using approximation, our\nalgorithms are the first to make in-memory computation of betweenness in\ndynamic networks with millions of edges feasible. Our experiments show that our\nalgorithms can achieve substantial speedups compared to recomputation, up to\nseveral orders of magnitude. Moreover, the approximation accuracy is usually\nsignificantly better than the theoretical guarantee in terms of absolute error.\nMore importantly, for reasonably small approximation error thresholds, the rank\nof nodes is well preserved, in particular for nodes with high betweenness.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 16:29:20 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Bergamini", "Elisabetta", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1510.08154", "submitter": "Akanksha Agrawal", "authors": "Akanksha Agrawal, Sudeshna Kolay, Daniel Lokshtanov, Saket Saurabh", "title": "A faster FPT Algorithm and a smaller Kernel for Block Graph Vertex\n  Deletion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph $G$ is called a \\emph{block graph} if each maximal $2$-connected\ncomponent of $G$ is a clique. In this paper we study the Block Graph Vertex\nDeletion from the perspective of fixed parameter tractable (FPT) and\nkernelization algorithm. In particular, an input to Block Graph Vertex Deletion\nconsists of a graph $G$ and a positive integer $k$ and the objective to check\nwhether there exists a subset $S \\subseteq V(G)$ of size at most $k$ such that\nthe graph induced on $V(G)\\setminus S$ is a block graph. In this paper we give\nan FPT algorithm with running time $4^kn^{O(1)}$ and a polynomial kernel of\nsize $O(k^4)$ for Block Graph Vertex Deletion. The running time of our FPT\nalgorithm improves over the previous best algorithm for the problem that ran in\ntime $10^kn^{O(1)}$ and the size of our kernel reduces over the previously\nknown kernel of size $O(k^9)$. Our results are based on a novel connection\nbetween Block Graph Vertex Deletion and the classical {\\sc Feedback Vertex Set}\nproblem in graphs without induced $C_4$ and $K_4-e$. To achieve our results we\nalso obtain an algorithm for {\\sc Weighted Feedback Vertex Set} running in time\n$3.618^kn^{O(1)}$ and improving over the running time of previously known\nalgorithm with running time $5^kn^{O(1)}$.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 01:37:26 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Agrawal", "Akanksha", ""], ["Kolay", "Sudeshna", ""], ["Lokshtanov", "Daniel", ""], ["Saurabh", "Saket", ""]]}, {"id": "1510.08274", "submitter": "Fabrizio Frati", "authors": "Patrizio Angelini and Giordano Da Lozzo and Giuseppe Di Battista and\n  Fabrizio Frati and Maurizio Patrignani and Ignaz Rutter", "title": "Beyond Level Planarity", "comments": "Appears in the Proceedings of the 24th International Symposium on\n  Graph Drawing and Network Visualization (GD 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we settle the computational complexity of two open problems\nrelated to the extension of the notion of level planarity to surfaces different\nfrom the plane. Namely, we show that the problems of testing the existence of a\nlevel embedding of a level graph on the surface of the rolling cylinder or on\nthe surface of the torus, respectively known by the name of $\\textit{Cyclic\nLevel Planarity}$ and $\\textit{Torus Level Planarity}$, are polynomial-time\nsolvable.\n  Moreover, we show a complexity dichotomy for testing the\n$\\textit{Simultaneous Level Planarity}$ of a set of level graphs, with respect\nto both the number of level graphs and the number of levels.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 12:00:16 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 13:56:43 GMT"}, {"version": "v3", "created": "Tue, 23 Aug 2016 08:00:17 GMT"}, {"version": "v4", "created": "Mon, 29 Aug 2016 10:12:20 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Angelini", "Patrizio", ""], ["Da Lozzo", "Giordano", ""], ["Di Battista", "Giuseppe", ""], ["Frati", "Fabrizio", ""], ["Patrignani", "Maurizio", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1510.08276", "submitter": "Yixin Cao", "authors": "Jie You and Jianxin Wang and Yixin Cao", "title": "Approximate Association via Dissociation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vertex set $X$ of a graph $G$ is an association set if each component of $G\n- X$ is a clique, or a dissociation set if each component of $G - X$ is a\nsingle vertex or a single edge. Interestingly, $G - X$ is then precisely a\ngraph containing no induced $P_3$'s or containing no $P_3$'s, respectively. We\nobserve some special structures and show that if none of them exists, then the\nminimum association set problem can be reduced to the minimum (weighted)\ndissociation set problem. This yields the first nontrivial approximation\nalgorithm for association set, and its approximation ratio is 2.5, matching the\nbest result of the closely related cluster editing problem. The reduction is\nbased on a combinatorial study of modular decomposition of graphs free of these\nspecial structures. Further, a novel algorithmic use of modular decomposition\nenables us to implement this approach in $O(m n + n^2)$ time.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 12:01:50 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["You", "Jie", ""], ["Wang", "Jianxin", ""], ["Cao", "Yixin", ""]]}, {"id": "1510.08546", "submitter": "Xiao Liu", "authors": "Seth Gilbert, Xiao Liu, Haifeng Yu", "title": "On Differentially Private Online Collaborative Recommendation Systems", "comments": "35 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In collaborative recommendation systems, privacy may be compromised, as\nusers' opinions are used to generate recommendations for others. In this paper,\nwe consider an online collaborative recommendation system, and we measure\nusers' privacy in terms of the standard differential privacy. We give the first\nquantitative analysis of the trade-offs between recommendation quality and\nusers' privacy in such a system by showing a lower bound on the best achievable\nprivacy for any non-trivial algorithm, and proposing a near-optimal algorithm.\nFrom our results, we find that there is actually little trade-off between\nrecommendation quality and privacy for any non-trivial algorithm. Our results\nalso identify the key parameters that determine the best achievable privacy.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 02:28:46 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Gilbert", "Seth", ""], ["Liu", "Xiao", ""], ["Yu", "Haifeng", ""]]}, {"id": "1510.08551", "submitter": "Zhenbo Wang", "authors": "Kameng Nip, Zhenbo Wang and Zizhuo Wang", "title": "Scheduling under Linear Constraints", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a parallel machine scheduling problem in which the processing\ntimes of jobs are not given in advance but are determined by a system of linear\nconstraints. The objective is to minimize the makespan, i.e., the maximum job\ncompletion time among all feasible choices. This novel problem is motivated by\nvarious real-world application scenarios. We discuss the computational\ncomplexity and algorithms for various settings of this problem. In particular,\nwe show that if there is only one machine with an arbitrary number of linear\nconstraints, or there is an arbitrary number of machines with no more than two\nlinear constraints, or both the number of machines and the number of linear\nconstraints are fixed constants, then the problem is polynomial-time solvable\nvia solving a series of linear programming problems. If both the number of\nmachines and the number of constraints are inputs of the problem instance, then\nthe problem is NP-Hard. We further propose several approximation algorithms for\nthe latter case.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 02:48:28 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Nip", "Kameng", ""], ["Wang", "Zhenbo", ""], ["Wang", "Zizhuo", ""]]}, {"id": "1510.08663", "submitter": "Tony Guttmann", "authors": "Andrew Elvey Price and Anthony J Guttmann", "title": "Permutations sortable by two stacks in series", "comments": "17 pages, 7 figures. Improved analysis, updated references", "journal-ref": "Advances in Applied Mathematics (2017): 81-96", "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of the number of permutations that can be sorted by\ntwo stacks in series. We do this by first counting all such permutations of\nlength less than 20 exactly, then using a numerical technique to obtain\nnineteen further coefficients approximately. Analysing these coefficients by a\nvariety of methods we conclude that the OGF behaves as $$S(z) \\sim A (1 - \\mu\n\\cdot z)^\\gamma,$$ where $\\mu =12.45 \\pm 0.15,$ $\\gamma= 1.5 \\pm 0.3,$ and $A\n\\approx 0.02$.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 12:19:42 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 01:11:31 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Price", "Andrew Elvey", ""], ["Guttmann", "Anthony J", ""]]}, {"id": "1510.08748", "submitter": "Frederik Rye Skjoldjensen", "authors": "Philip Bille, Inge Li G{\\o}rtz, Frederik Rye Skjoldjensen", "title": "Subsequence Automata with Default Transitions", "comments": "Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $S$ be a string of length $n$ with characters from an alphabet of size\n$\\sigma$. The \\emph{subsequence automaton} of $S$ (often called the\n\\emph{directed acyclic subsequence graph}) is the minimal deterministic finite\nautomaton accepting all subsequences of $S$. A straightforward construction\nshows that the size (number of states and transitions) of the subsequence\nautomaton is $O(n\\sigma)$ and that this bound is asymptotically optimal.\n  In this paper, we consider subsequence automata with \\emph{default\ntransitions}, that is, special transitions to be taken only if none of the\nregular transitions match the current character, and which do not consume the\ncurrent character. We show that with default transitions, much smaller\nsubsequence automata are possible, and provide a full trade-off between the\nsize of the automaton and the \\emph{delay}, i.e., the maximum number of\nconsecutive default transitions followed before consuming a character.\n  Specifically, given any integer parameter $k$, $1 < k \\leq \\sigma$, we\npresent a subsequence automaton with default transitions of size\n$O(nk\\log_{k}\\sigma)$ and delay $O(\\log_k \\sigma)$. Hence, with $k = 2$ we\nobtain an automaton of size $O(n \\log \\sigma)$ and delay $O(\\log \\sigma)$. On\nthe other extreme, with $k = \\sigma$, we obtain an automaton of size $O(n\n\\sigma)$ and delay $O(1)$, thus matching the bound for the standard subsequence\nautomaton construction. Finally, we generalize the result to multiple strings.\nThe key component of our result is a novel hierarchical automata construction\nof independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 15:47:24 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 09:42:20 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Bille", "Philip", ""], ["G\u00f8rtz", "Inge Li", ""], ["Skjoldjensen", "Frederik Rye", ""]]}, {"id": "1510.08865", "submitter": "Kai Wei", "authors": "Kai Wei, Rishabh Iyer, Shengjie Wang, Wenruo Bai, Jeff Bilmes", "title": "Mixed Robust/Average Submodular Partitioning: Fast Algorithms,\n  Guarantees, and Applications to Parallel Machine Learning and Multi-Label\n  Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two mixed robust/average-case submodular partitioning problems that\nwe collectively call Submodular Partitioning. These problems generalize both\npurely robust instances of the problem (namely max-min submodular fair\nallocation (SFA) and min-max submodular load balancing (SLB) and also\ngeneralize average-case instances (that is the submodular welfare problem (SWP)\nand submodular multiway partition (SMP). While the robust versions have been\nstudied in the theory community, existing work has focused on tight\napproximation guarantees, and the resultant algorithms are not, in general,\nscalable to very large real-world applications. This is in contrast to the\naverage case, where most of the algorithms are scalable. In the present paper,\nwe bridge this gap, by proposing several new algorithms (including those based\non greedy, majorization-minimization, minorization-maximization, and relaxation\nalgorithms) that not only scale to large sizes but that also achieve\ntheoretical approximation guarantees close to the state-of-the-art, and in some\ncases achieve new tight bounds. We also provide new scalable algorithms that\napply to additive combinations of the robust and average-case extreme\nobjectives. We show that these problems have many applications in machine\nlearning (ML). This includes: 1) data partitioning and load balancing for\ndistributed machine algorithms on parallel machines; 2) data clustering; and 3)\nmulti-label image segmentation with (only) Boolean submodular functions via\npixel partitioning. We empirically demonstrate the efficacy of our algorithms\non real-world problems involving data partitioning for distributed optimization\nof standard machine learning objectives (including both convex and deep neural\nnetwork objectives), and also on purely unsupervised (i.e., no supervised or\nsemi-supervised learning, and no interactive segmentation) image segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 20:07:32 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 04:00:41 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Wei", "Kai", ""], ["Iyer", "Rishabh", ""], ["Wang", "Shengjie", ""], ["Bai", "Wenruo", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1510.08892", "submitter": "Meirav Zehavi", "authors": "Meirav Zehavi", "title": "A Randomized Algorithm for Long Directed Cycle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a directed graph $G$ and a parameter $k$, the {\\sc Long Directed Cycle\n(LDC)} problem asks whether $G$ contains a simple cycle on at least $k$\nvertices, while the {\\sc $k$-Path} problems asks whether $G$ contains a simple\npath on exactly $k$ vertices. Given a deterministic (randomized) algorithm for\n{\\sc $k$-Path} as a black box, which runs in time $t(G,k)$, we prove that {\\sc\nLDC} can be solved in deterministic time $O^*(\\max\\{t(G,2k),4^{k+o(k)}\\})$\n(randomized time $O^*(\\max\\{t(G,2k),4^k\\})$). In particular, we get that {\\sc\nLDC} can be solved in randomized time $O^*(4^k)$.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 20:33:08 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Zehavi", "Meirav", ""]]}, {"id": "1510.08896", "submitter": "Cameron Musco", "authors": "Chi Jin and Sham M. Kakade and Cameron Musco and Praneeth Netrapalli\n  and Aaron Sidford", "title": "Robust Shift-and-Invert Preconditioning: Faster and More Sample\n  Efficient Algorithms for Eigenvector Computation", "comments": "Manuscript outdated. Updated version at arxiv:1605.08754", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide faster algorithms and improved sample complexities for\napproximating the top eigenvector of a matrix.\n  Offline Setting: Given an $n \\times d$ matrix $A$, we show how to compute an\n$\\epsilon$ approximate top eigenvector in time $\\tilde O ( [nnz(A) + \\frac{d\n\\cdot sr(A)}{gap^2}]\\cdot \\log 1/\\epsilon )$ and $\\tilde O([\\frac{nnz(A)^{3/4}\n(d \\cdot sr(A))^{1/4}}{\\sqrt{gap}}]\\cdot \\log1/\\epsilon )$. Here $sr(A)$ is the\nstable rank and $gap$ is the multiplicative eigenvalue gap. By separating the\n$gap$ dependence from $nnz(A)$ we improve on the classic power and Lanczos\nmethods. We also improve prior work using fast subspace embeddings and\nstochastic optimization, giving significantly improved dependencies on $sr(A)$\nand $\\epsilon$. Our second running time improves this further when $nnz(A) \\le\n\\frac{d\\cdot sr(A)}{gap^2}$.\n  Online Setting: Given a distribution $D$ with covariance matrix $\\Sigma$ and\na vector $x_0$ which is an $O(gap)$ approximate top eigenvector for $\\Sigma$,\nwe show how to refine to an $\\epsilon$ approximation using $\\tilde\nO(\\frac{v(D)}{gap^2} + \\frac{v(D)}{gap \\cdot \\epsilon})$ samples from $D$. Here\n$v(D)$ is a natural variance measure. Combining our algorithm with previous\nwork to initialize $x_0$, we obtain a number of improved sample complexity and\nruntime results. For general distributions, we achieve asymptotically optimal\naccuracy as a function of sample size as the number of samples grows large.\n  Our results center around a robust analysis of the classic method of\nshift-and-invert preconditioning to reduce eigenvector computation to\napproximately solving a sequence of linear systems. We then apply fast SVRG\nbased approximate system solvers to achieve our claims. We believe our results\nsuggest the general effectiveness of shift-and-invert based approaches and\nimply that further computational gains may be reaped in practice.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 20:47:27 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 02:40:00 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Jin", "Chi", ""], ["Kakade", "Sham M.", ""], ["Musco", "Cameron", ""], ["Netrapalli", "Praneeth", ""], ["Sidford", "Aaron", ""]]}, {"id": "1510.09037", "submitter": "Krist\\'of Tak\\'acs", "authors": "Krist\\'of Tak\\'acs", "title": "Multiple sequence alignment for short sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple sequence alignment (MSA) has been one of the most important problems\nin bioinformatics for more decades and it is still heavily examined by many\nmathematicians and biologists. However, mostly because of the practical\nmotivation of this problem, the research on this topic is focused on aligning\nlong sequences. It is understandable, since the sequences that need to be\naligned (usually DNA or protein sequences) are generally quite long (e. g., at\nleast 30-40 characters). Nevertheless, it is a challenging question that\nexactly where MSA starts to become a real hard problem (since it is known that\nMSA is NP-complete [2]), and the key to answer this question is to examine\nshort sequences. If the optimal alignment for short sequences could be\ndetermined in polynomial time, then these results may help to develop faster or\nmore accurate heuristic algorithms for aligning long sequences. In this work,\nit is shown that for length-1 sequences using arbitrary metric, as well as for\nlength-2 sequences using unit metric, the optimum of the MSA problem can be\nachieved by the trivial alignment.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 10:16:44 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2015 09:58:02 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Tak\u00e1cs", "Krist\u00f3f", ""]]}]