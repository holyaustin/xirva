[{"id": "1803.00008", "submitter": "Gautam Kamath", "authors": "Jayadev Acharya, Gautam Kamath, Ziteng Sun, Huanyu Zhang", "title": "INSPECTRE: Privately Estimating the Unseen", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop differentially private methods for estimating various\ndistributional properties. Given a sample from a discrete distribution $p$,\nsome functional $f$, and accuracy and privacy parameters $\\alpha$ and\n$\\varepsilon$, the goal is to estimate $f(p)$ up to accuracy $\\alpha$, while\nmaintaining $\\varepsilon$-differential privacy of the sample.\n  We prove almost-tight bounds on the sample size required for this problem for\nseveral functionals of interest, including support size, support coverage, and\nentropy. We show that the cost of privacy is negligible in a variety of\nsettings, both theoretically and experimentally. Our methods are based on a\nsensitivity analysis of several state-of-the-art methods for estimating these\nproperties with sublinear sample complexities.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 19:00:00 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Acharya", "Jayadev", ""], ["Kamath", "Gautam", ""], ["Sun", "Ziteng", ""], ["Zhang", "Huanyu", ""]]}, {"id": "1803.00673", "submitter": "Kai Wang", "authors": "Kai Wang", "title": "An efficient algorithm to test forcibly-connectedness of graphical\n  degree sequences", "comments": "20 pages, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm to test whether a given graphical degree sequence is\nforcibly connected or not and prove its correctness. We also outline the\nextensions of the algorithm to test whether a given graphical degree sequence\nis forcibly $k$-connected or not for every fixed $k\\ge 2$. We show through\nexperimental evaluations that the algorithm is efficient on average, though its\nworst case run time is probably exponential. We also adapt Ruskey et al's\nclassic algorithm to enumerate zero-free graphical degree sequences of length\n$n$ and Barnes and Savage's classic algorithm to enumerate graphical partitions\nof even integer $n$ by incorporating our testing algorithm into theirs and then\nobtain some enumerative results about forcibly connected graphical degree\nsequences of given length $n$ and forcibly connected graphical partitions of\ngiven even integer $n$. Based on these enumerative results we make some\nconjectures such as: when $n$ is large, (1) almost all zero-free graphical\ndegree sequences of length $n$ are forcibly connected; (2) almost none of the\ngraphical partitions of even $n$ are forcibly connected.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 00:38:09 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Wang", "Kai", ""]]}, {"id": "1803.00786", "submitter": "Magnus M. Halldorsson", "authors": "Ravi B. Boppana, Magn\\'us M. Halld\\'orsson and Dror Rawitz", "title": "Simple and Local Independent Set Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We bound the performance guarantees that follow from Tur\\'an-like bounds for\nunweighted and weighted independent sets in bounded-degree graphs. In\nparticular, a randomized approach of Boppana forms a simple 1-round distributed\nalgorithm, as well as a streaming and preemptive online algorithm. We show it\ngives a tight $(\\Delta+1)/2$-approximation in unweighted graphs of maximum\ndegree $\\Delta$, which is best possible for 1-round distributed algorithms. For\nweighted graphs, it gives only a $\\Delta$-approximation, but a simple\nmodification results in an asymptotic expected $0.529 \\Delta$-approximation.\nThis compares with a recent, more complex $\\Delta$-approximation~\\cite{BCGS17},\nwhich holds deterministically.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 09:58:26 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Boppana", "Ravi B.", ""], ["Halld\u00f3rsson", "Magn\u00fas M.", ""], ["Rawitz", "Dror", ""]]}, {"id": "1803.00796", "submitter": "Karl Bringmann", "authors": "Amir Abboud, Arturs Backurs, Karl Bringmann, Marvin K\\\"unnemann", "title": "Fine-Grained Complexity of Analyzing Compressed Data: Quantifying\n  Improvements over Decompress-And-Solve", "comments": "Presented at FOCS'17. Full version. 63 pages", "journal-ref": null, "doi": "10.1109/FOCS.2017.26", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we analyze data without decompressing it? As our data keeps growing,\nunderstanding the time complexity of problems on compressed inputs, rather than\nin convenient uncompressed forms, becomes more and more relevant. Suppose we\nare given a compression of size $n$ of data that originally has size $N$, and\nwe want to solve a problem with time complexity $T(\\cdot)$. The naive strategy\nof \"decompress-and-solve\" gives time $T(N)$, whereas \"the gold standard\" is\ntime $T(n)$: to analyze the compression as efficiently as if the original data\nwas small.\n  We restrict our attention to data in the form of a string (text, files,\ngenomes, etc.) and study the most ubiquitous tasks. While the challenge might\nseem to depend heavily on the specific compression scheme, most methods of\npractical relevance (Lempel-Ziv-family, dictionary methods, and others) can be\nunified under the elegant notion of Grammar Compressions. A vast literature,\nacross many disciplines, established this as an influential notion for\nAlgorithm design.\n  We introduce a framework for proving (conditional) lower bounds in this\nfield, allowing us to assess whether decompress-and-solve can be improved, and\nby how much. Our main results are:\n  - The $O(nN\\sqrt{\\log{N/n}})$ bound for LCS and the $O(\\min\\{N \\log N, nM\\})$\nbound for Pattern Matching with Wildcards are optimal up to $N^{o(1)}$ factors,\nunder the Strong Exponential Time Hypothesis. (Here, $M$ denotes the\nuncompressed length of the compressed pattern.)\n  - Decompress-and-solve is essentially optimal for Context-Free Grammar\nParsing and RNA Folding, under the $k$-Clique conjecture.\n  - We give an algorithm showing that decompress-and-solve is not optimal for\nDisjointness.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 10:32:07 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Abboud", "Amir", ""], ["Backurs", "Arturs", ""], ["Bringmann", "Karl", ""], ["K\u00fcnnemann", "Marvin", ""]]}, {"id": "1803.00804", "submitter": "Karl Bringmann", "authors": "Karl Bringmann, Philip Wellnitz", "title": "Clique-Based Lower Bounds for Parsing Tree-Adjoining Grammars", "comments": "Presented at CPM'17. 15 pages", "journal-ref": null, "doi": "10.4230/LIPIcs.CPM.2017.12", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-adjoining grammars are a generalization of context-free grammars that\nare well suited to model human languages and are thus popular in computational\nlinguistics. In the tree-adjoining grammar recognition problem, given a grammar\n$\\Gamma$ and a string $s$ of length $n$, the task is to decide whether $s$ can\nbe obtained from $\\Gamma$. Rajasekaran and Yooseph's parser (JCSS'98) solves\nthis problem in time $O(n^{2\\omega})$, where $\\omega < 2.373$ is the matrix\nmultiplication exponent. The best algorithms avoiding fast matrix\nmultiplication take time $O(n^6)$.\n  The first evidence for hardness was given by Satta (J. Comp. Linguist.'94):\nFor a more general parsing problem, any algorithm that avoids fast matrix\nmultiplication and is significantly faster than $O(|\\Gamma| n^6)$ in the case\nof $|\\Gamma| = \\Theta(n^{12})$ would imply a breakthrough for Boolean matrix\nmultiplication.\n  Following an approach by Abboud et al. (FOCS'15) for context-free grammar\nrecognition, in this paper we resolve many of the disadvantages of the previous\nlower bound. We show that, even on constant-size grammars, any improvement on\nRajasekaran and Yooseph's parser would imply a breakthrough for the $k$-Clique\nproblem. This establishes tree-adjoining grammar parsing as a practically\nrelevant problem with the unusual running time of $n^{2\\omega}$, up to lower\norder factors.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 10:56:49 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Bringmann", "Karl", ""], ["Wellnitz", "Philip", ""]]}, {"id": "1803.00807", "submitter": "Niels Gr\\\"uttemeier", "authors": "Niels Gr\\\"uttemeier and Christian Komusiewicz", "title": "On the Relation of Strong Triadic Closure and Cluster Deletion", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the parameterized and classical complexity of two related problems\non undirected graphs $G=(V,E)$. In Strong Triadic Closure we aim to label the\nedges in $E$ as strong and weak such that at most~$k$ edges are weak and $G$\ncontains no induced $P_3$ with two strong edges. In Cluster Deletion, we aim to\ndestroy all induced $P_3$s by a minimum number of edge deletions. We first show\nthat Strong Triadic Closure admits a $4k$-vertex kernel. Then, we study\nparameterization by $\\ell:=|E|-k$ and show that both problems are\nfixed-parameter tractable and unlikely to admit a polynomial kernel with\nrespect to $\\ell$. Finally, we give a dichotomy of the classical complexity of\nboth problems on $H$-free graphs for all $H$ of order four.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 11:11:44 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 11:14:25 GMT"}, {"version": "v3", "created": "Tue, 6 Aug 2019 13:26:52 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Gr\u00fcttemeier", "Niels", ""], ["Komusiewicz", "Christian", ""]]}, {"id": "1803.00829", "submitter": "Liren Shan", "authors": "Liren Shan, Huan Li, Zhongzhi Zhang", "title": "Independence number and the number of maximum independent sets in\n  pseudofractal scale-free web and Sierpi\\'nski gasket", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a fundamental subject of theoretical computer science, the maximum\nindependent set (MIS) problem not only is of purely theoretical interest, but\nalso has found wide applications in various fields. However, for a general\ngraph determining the size of a MIS is NP-hard, and exact computation of the\nnumber of all MISs is even more difficult. It is thus of significant interest\nto seek special graphs for which the MIS problem can be exactly solved. In this\npaper, we address the MIS problem in the pseudofractal scale-free web and the\nSierpi\\'nski gasket, which have the same number of vertices and edges. For both\ngraphs, we determine exactly the independence number and the number of all\npossible MISs. The independence number of the pseudofractal scale-free web is\nas twice as the one of the Sierpi\\'nski gasket. Moreover, the pseudofractal\nscale-free web has a unique MIS, while the number of MISs in the Sierpi\\'nski\ngasket grows exponentially with the number of vertices.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 12:52:03 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Shan", "Liren", ""], ["Li", "Huan", ""], ["Zhang", "Zhongzhi", ""]]}, {"id": "1803.00849", "submitter": "Karl Bringmann", "authors": "Karl Bringmann, Sergio Cabello, Michael T.M. Emmerich", "title": "Maximum Volume Subset Selection for Anchored Boxes", "comments": "Presented at SoCG'17. Full Version. 24 pages", "journal-ref": null, "doi": "10.4230/LIPIcs.SoCG.2017.22", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $B$ be a set of $n$ axis-parallel boxes in $\\mathbb{R}^d$ such that each\nbox has a corner at the origin and the other corner in the positive quadrant of\n$\\mathbb{R}^d$, and let $k$ be a positive integer. We study the problem of\nselecting $k$ boxes in $B$ that maximize the volume of the union of the\nselected boxes. This research is motivated by applications in skyline queries\nfor databases and in multicriteria optimization, where the problem is known as\nthe hypervolume subset selection problem. It is known that the problem can be\nsolved in polynomial time in the plane, while the best known running time in\nany dimension $d \\ge 3$ is $\\Omega\\big(\\binom{n}{k}\\big)$. We show that:\n  - The problem is NP-hard already in 3 dimensions.\n  - In 3 dimensions, we break the bound $\\Omega\\big(\\binom{n}{k}\\big)$, by\nproviding an $n^{O(\\sqrt{k})}$ algorithm.\n  - For any constant dimension $d$, we present an efficient polynomial-time\napproximation scheme.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 13:58:44 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Bringmann", "Karl", ""], ["Cabello", "Sergio", ""], ["Emmerich", "Michael T. M.", ""]]}, {"id": "1803.00904", "submitter": "Aviad Rubinstein", "authors": "Aviad Rubinstein", "title": "Hardness of Approximate Nearest Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove conditional near-quadratic running time lower bounds for approximate\nBichromatic Closest Pair with Euclidean, Manhattan, Hamming, or edit distance.\nSpecifically, unless the Strong Exponential Time Hypothesis (SETH) is false,\nfor every $\\delta>0$ there exists a constant $\\epsilon>0$ such that computing a\n$(1+\\epsilon)$-approximation to the Bichromatic Closest Pair requires\n$n^{2-\\delta}$ time. In particular, this implies a near-linear query time for\nApproximate Nearest Neighbor search with polynomial preprocessing time.\n  Our reduction uses the Distributed PCP framework of [ARW'17], but obtains\nimproved efficiency using Algebraic Geometry (AG) codes. Efficient PCPs from AG\ncodes have been constructed in other settings before [BKKMS'16, BCGRS'17], but\nour construction is the first to yield new hardness results.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 15:44:01 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Rubinstein", "Aviad", ""]]}, {"id": "1803.00925", "submitter": "Marcin Pilipczuk", "authors": "Krzysztof Kiljan and Marcin Pilipczuk", "title": "Experimental Evaluation of Parameterized Algorithms for Feedback Vertex\n  Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feedback Vertex Set is a classic combinatorial optimization problem that asks\nfor a minimum set of vertices in a given graph whose deletion makes the graph\nacyclic. From the point of view of parameterized algorithms and fixed-parameter\ntractability, Feedback Vertex Set is one of the landmark problems: a long line\nof study resulted in multiple algorithmic approaches and deep understanding of\nthe combinatorics of the problem. Because of its central role in parameterized\ncomplexity, the first edition of the Parameterized Algorithms and Computational\nExperiments Challenge (PACE) in 2016 featured Feedback Vertex Set as the\nproblem of choice in one of its tracks. The results of PACE 2016 on one hand\nshowed large discrepancy between performance of different classic approaches to\nthe problem, and on the other hand indicated a new approach based on\nhalf-integral relaxations of the problem as probably the most efficient\napproach to the problem. In this paper we provide an exhaustive experimental\nevaluation of fixed-parameter and branching algorithms for Feedback Vertex Set.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 16:03:37 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 11:15:59 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Kiljan", "Krzysztof", ""], ["Pilipczuk", "Marcin", ""]]}, {"id": "1803.00926", "submitter": "Buddhima Gamlath", "authors": "Buddhima Gamlath, Sangxia Huang, Ola Svensson", "title": "Semi-Supervised Algorithms for Approximately Optimal and Accurate\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study $k$-means clustering in a semi-supervised setting. Given an oracle\nthat returns whether two given points belong to the same cluster in a fixed\noptimal clustering, we investigate the following question: how many oracle\nqueries are sufficient to efficiently recover a clustering that, with\nprobability at least $(1 - \\delta)$, simultaneously has a cost of at most $(1 +\n\\epsilon)$ times the optimal cost and an accuracy of at least $(1 - \\epsilon)$?\n  We show how to achieve such a clustering on $n$ points with $O{((k^2 \\log n)\n\\cdot m{(Q, \\epsilon^4, \\delta / (k\\log n))})}$ oracle queries, when the $k$\nclusters can be learned with an $\\epsilon'$ error and a failure probability\n$\\delta'$ using $m(Q, \\epsilon',\\delta')$ labeled samples in the supervised\nsetting, where $Q$ is the set of candidate cluster centers. We show that $m(Q,\n\\epsilon', \\delta')$ is small both for $k$-means instances in Euclidean space\nand for those in finite metric spaces. We further show that, for the Euclidean\n$k$-means instances, we can avoid the dependency on $n$ in the query complexity\nat the expense of an increased dependency on $k$: specifically, we give a\nslightly more involved algorithm that uses $O(k^4/(\\epsilon^2 \\delta) +\n(k^{9}/\\epsilon^4) \\log(1/\\delta) + k \\cdot m(\\mathbb{R}^r, \\epsilon^4/k,\n\\delta))$ oracle queries.\n  We also show that the number of queries needed for $(1 - \\epsilon)$-accuracy\nin Euclidean $k$-means must linearly depend on the dimension of the underlying\nEuclidean space, and for finite metric space $k$-means, we show that it must at\nleast be logarithmic in the number of candidate centers. This shows that our\nquery complexities capture the right dependencies on the respective parameters.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 16:07:59 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 08:25:45 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Gamlath", "Buddhima", ""], ["Huang", "Sangxia", ""], ["Svensson", "Ola", ""]]}, {"id": "1803.00927", "submitter": "Marcin Pilipczuk", "authors": "Micha{\\l} Ziobro and Marcin Pilipczuk", "title": "Finding Hamiltonian Cycle in Graphs of Bounded Treewidth: Experimental\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of treewidth, introduced by Robertson and Seymour in their seminal\nGraph Minors series, turned out to have tremendous impact on graph\nalgorithmics. Many hard computational problems on graphs turn out to be\nefficiently solvable in graphs of bounded treewidth: graphs that can be sweeped\nwith separators of bounded size. These efficient algorithms usually follow the\ndynamic programming paradigm.\n  In the recent years, we have seen a rapid and quite unexpected development of\ninvolved techniques for solving various computational problems in graphs of\nbounded treewidth. One of the most surprising directions is the development of\nalgorithms for connectivity problems that have only single-exponential\ndependency (i.e., $2^{O(t)}$) on the treewidth in the running time bound, as\nopposed to slightly superexponential (i.e., $2^{O(t \\log t)}$) stemming from\nmore naive approaches. In this work, we perform a thorough experimental\nevaluation of these approaches in the context of one of the most classic\nconnectivity problem, namely Hamiltonian Cycle.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 16:08:57 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 11:12:15 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Ziobro", "Micha\u0142", ""], ["Pilipczuk", "Marcin", ""]]}, {"id": "1803.00937", "submitter": "Marcin Pilipczuk", "authors": "Shaohua Li and Marcin Pilipczuk", "title": "An improved FPT algorithm for Independent Feedback Vertex Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Independent Feedback Vertex Set problem - a variant of the\nclassic Feedback Vertex Set problem where, given a graph $G$ and an integer\n$k$, the problem is to decide whether there exists a vertex set $S\\subseteq\nV(G)$ such that $G\\setminus S$ is a forest and $S$ is an independent set of\nsize at most $k$. We present an $O^\\ast((1+\\varphi^{2})^{k})$-time FPT\nalgorithm for this problem, where $\\varphi<1.619$ is the golden ratio,\nimproving the previous fastest $O^\\ast(4.1481^{k})$-time algorithm given by\nAgrawal et al [IPEC 2016]. The exponential factor in our time complexity bound\nmatches the fastest deterministic FPT algorithm for the classic Feedback Vertex\nSet problem. On the technical side, the main novelty is a refined measure of an\ninput instance in a branching process, that allows for a simpler and more\nconcise description and analysis of the algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 16:25:40 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 11:00:54 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Li", "Shaohua", ""], ["Pilipczuk", "Marcin", ""]]}, {"id": "1803.00938", "submitter": "Karl Bringmann", "authors": "Karl Bringmann, Marvin K\\\"unnemann", "title": "Multivariate Fine-Grained Complexity of Longest Common Subsequence", "comments": "Presented at SODA'18. Full Version. 66 pages", "journal-ref": null, "doi": "10.1137/1.9781611975031.79", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the classic combinatorial pattern matching problem of finding a\nlongest common subsequence (LCS). For strings $x$ and $y$ of length $n$, a\ntextbook algorithm solves LCS in time $O(n^2)$, but although much effort has\nbeen spent, no $O(n^{2-\\varepsilon})$-time algorithm is known. Recent work\nindeed shows that such an algorithm would refute the Strong Exponential Time\nHypothesis (SETH) [Abboud, Backurs, Vassilevska Williams + Bringmann,\nK\\\"unnemann FOCS'15].\n  Despite the quadratic-time barrier, for over 40 years an enduring scientific\ninterest continued to produce fast algorithms for LCS and its variations.\nParticular attention was put into identifying and exploiting input parameters\nthat yield strongly subquadratic time algorithms for special cases of interest,\ne.g., differential file comparison. This line of research was successfully\npursued until 1990, at which time significant improvements came to a halt. In\nthis paper, using the lens of fine-grained complexity, our goal is to (1)\njustify the lack of further improvements and (2) determine whether some special\ncases of LCS admit faster algorithms than currently known.\n  To this end, we provide a systematic study of the multivariate complexity of\nLCS, taking into account all parameters previously discussed in the literature:\nthe input size $n:=\\max\\{|x|,|y|\\}$, the length of the shorter string\n$m:=\\min\\{|x|,|y|\\}$, the length $L$ of an LCS of $x$ and $y$, the numbers of\ndeletions $\\delta := m-L$ and $\\Delta := n-L$, the alphabet size, as well as\nthe numbers of matching pairs $M$ and dominant pairs $d$. For any class of\ninstances defined by fixing each parameter individually to a polynomial in\nterms of the input size, we prove a SETH-based lower bound matching one of\nthree known algorithms. Specifically, we determine the optimal running time for\nLCS under SETH as $(n+\\min\\{d, \\delta \\Delta, \\delta m\\})^{1\\pm o(1)}$.\n  [...]\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 16:26:13 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Bringmann", "Karl", ""], ["K\u00fcnnemann", "Marvin", ""]]}, {"id": "1803.01237", "submitter": "Vipul Harsh", "authors": "Vipul Harsh, Laxmikant Kale, Edgar Solomonik", "title": "Histogram Sort with Sampling", "comments": "12 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To minimize data movement, state-of-the-art parallel sorting algorithms use\ntechniques based on sampling and histogramming to partition keys prior to\nredistribution. Sampling enables partitioning to be done using a representative\nsubset of the keys, while histogramming enables evaluation and iterative\nimprovement of a given partition. We introduce Histogram sort with sampling\n(HSS), which combines sampling and iterative histogramming to find high quality\npartitions with minimal data movement and high practical performance. Compared\nto the best known (recently introduced) algorithm for finding these partitions,\nour algorithm requires a factor of {\\Theta}(log(p)/ log log(p)) less\ncommunication, and substantially less when compared to standard variants of\nSample sort and Histogram sort. We provide a distributed memory implementation\nof the proposed algorithm, compare its performance to two existing\nimplementations, and provide a brief application study showing benefit of the\nnew algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 20:51:38 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 03:47:49 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Harsh", "Vipul", ""], ["Kale", "Laxmikant", ""], ["Solomonik", "Edgar", ""]]}, {"id": "1803.01276", "submitter": "Miguel Mosteiro", "authors": "Austin Halper, Miguel A. Mosteiro, Yulia Rossikova, and Prudence W. H.\n  Wong", "title": "Station Assignment with Reallocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a dynamic allocation problem that arises in various scenarios where\nmobile clients joining and leaving the system have to communicate with static\nstations via radio transmissions. Restrictions are a maximum delay, or laxity,\nbetween consecutive client transmissions and a maximum bandwidth that a station\ncan share among its clients. We study the problem of assigning clients to\nstations so that every client transmits to some station, satisfying those\nrestrictions. We consider reallocation algorithms, where clients are revealed\nat its arrival time, the departure time is unknown until they leave, and\nclients may be reallocated to another station, but at a cost proportional to\nthe reciprocal of the client laxity. We present negative results for previous\nrelated protocols that motivate the study; we introduce new protocols that\nexpound trade-offs between station usage and reallocation cost; we determine\nexperimentally a classification of the clients attempting to balance those\nopposite goals; we prove theoretically bounds on our performance metrics; and\nwe show through simulations that, for realistic scenarios, our protocols behave\nmuch better than our theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 00:55:39 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Halper", "Austin", ""], ["Mosteiro", "Miguel A.", ""], ["Rossikova", "Yulia", ""], ["Wong", "Prudence W. H.", ""]]}, {"id": "1803.01281", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Siddharth Samsi, William Arcand, David Bestor, Bill\n  Bergeron, Tim Davis, Vijay Gadepally, Michael Houle, Matthew Hubbell, Hayden\n  Jananthan, Michael Jones, Anna Klein, Peter Michaleas, Roger Pearce, Lauren\n  Milechin, Julie Mullen, Andrew Prout, Antonio Rosa, Geoff Sanders, Charles\n  Yee, Albert Reuther", "title": "Design, Generation, and Validation of Extreme Scale Power-Law Graphs", "comments": "8 pages, 6 figures, IEEE IPDPS 2018 Graph Algorithm Building Blocks\n  (GABB) workshop", "journal-ref": null, "doi": "10.1109/IPDPSW.2018.00055", "report-no": null, "categories": "cs.DC cs.DM cs.DS cs.PF math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive power-law graphs drive many fields: metagenomics, brain mapping,\nInternet-of-things, cybersecurity, and sparse machine learning. The development\nof novel algorithms and systems to process these data requires the design,\ngeneration, and validation of enormous graphs with exactly known properties.\nSuch graphs accelerate the proper testing of new algorithms and systems and are\na prerequisite for success on real applications. Many random graph generators\ncurrently exist that require realizing a graph in order to know its exact\nproperties: number of vertices, number of edges, degree distribution, and\nnumber of triangles. Designing graphs using these random graph generators is a\ntime-consuming trial-and-error process. This paper presents a novel approach\nthat uses Kronecker products to allow the exact computation of graph properties\nprior to graph generation. In addition, when a real graph is desired, it can be\ngenerated quickly in memory on a parallel computer with no-interprocessor\ncommunication. To test this approach, graphs with $10^{12}$ edges are generated\non a 40,000+ core supercomputer in 1 second and exactly agree with those\npredicted by the theory. In addition, to demonstrate the extensibility of this\napproach, decetta-scale graphs with up to $10^{30}$ edges are simulated in a\nfew minutes on a laptop.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 01:47:49 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Kepner", "Jeremy", ""], ["Samsi", "Siddharth", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Davis", "Tim", ""], ["Gadepally", "Vijay", ""], ["Houle", "Michael", ""], ["Hubbell", "Matthew", ""], ["Jananthan", "Hayden", ""], ["Jones", "Michael", ""], ["Klein", "Anna", ""], ["Michaleas", "Peter", ""], ["Pearce", "Roger", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Sanders", "Geoff", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1803.01285", "submitter": "Maximilien Burq", "authors": "Itai Ashlagi, Maximilien Burq, Patrick Jaillet, Amin Saberi", "title": "Maximizing Efficiency in Dynamic Matching Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of matching agents who arrive at a marketplace over time\nand leave after d time periods. Agents can only be matched while they are\npresent in the marketplace. Each pair of agents can yield a different match\nvalue, and the planner's goal is to maximize the total value over a finite time\nhorizon. We study matching algorithms that perform well over any sequence of\narrivals when there is no a priori information about the match values or\narrival times.\n  Our main contribution is a 1/4-competitive algorithm. The algorithm randomly\nselects a subset of agents who will wait until right before their departure to\nget matched, and maintains a maximum-weight matching with respect to the other\nagents. The primal-dual analysis of the algorithm hinges on a careful\ncomparison between the initial dual value associated with an agent when it\nfirst arrives, and the final value after d time steps.\n  It is also shown that no algorithm is 1/2-competitive. We extend the model to\nthe case in which departure times are drawn i.i.d from a distribution with\nnon-decreasing hazard rate, and establish a 1/8-competitive algorithm in this\nsetting. Finally we show on real-world data that a modified version of our\nalgorithm performs well in practice.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 02:36:15 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Ashlagi", "Itai", ""], ["Burq", "Maximilien", ""], ["Jaillet", "Patrick", ""], ["Saberi", "Amin", ""]]}, {"id": "1803.01362", "submitter": "Adri\\'an G\\'omez-Brand\\'on", "authors": "Nieves R. Brisaboa, Travis Gagie, Adri\\'an G\\'omez-Brand\\'on and\n  Gonzalo Navarro", "title": "Two-Dimensional Block Trees", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Block Tree (BT) is a novel compact data structure designed to compress\nsequence collections. It obtains compression ratios close to Lempel-Ziv and\nsupports efficient direct access to any substring. The BT divides the text\nrecursively into fixed-size blocks and those appearing earlier are represented\nwith pointers. On repetitive collections, a few blocks can represent all the\nothers, and thus the BT reduces the size by orders of magnitude. In this paper\nwe extend the BT to two dimensions, to exploit repetitiveness in collections of\nimages, graphs, and maps. This two-dimensional Block Tree divides the image\nregularly into subimages and replaces some of them by pointers to other\noccurrences thereof. We develop a specific variant aimed at compressing the\nadjacency matrices of Web graphs, obtaining space reductions of up to 50\\%\ncompared with the $k^2$-tree, which is the best alternative supporting direct\nand reverse navigation in the graph.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 14:46:32 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Gagie", "Travis", ""], ["G\u00f3mez-Brand\u00f3n", "Adri\u00e1n", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1803.01474", "submitter": "Michael Mitzenmacher", "authors": "Michael Mitzenmacher", "title": "Optimizing Learned Bloom Filters by Sandwiching", "comments": "Short note. Related to arXiv:1802.00884 ; will probably combine them\n  into a larger submission at some point in the future", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a simple method for improving the performance of the recently\nintroduced learned Bloom filters, by showing that they perform better when the\nlearned function is sandwiched between two Bloom filters.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 02:50:14 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Mitzenmacher", "Michael", ""]]}, {"id": "1803.01481", "submitter": "Shengjun Wu", "authors": "Yunkai Wang, Shengjun Wu and Wei Wang", "title": "Controlled quantum search on structured databases", "comments": "10 pages, 18 figures", "journal-ref": "Phys. Rev. Research 1, 033016 (2019)", "doi": "10.1103/PhysRevResearch.1.033016", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present quantum algorithms to search for marked vertices in structured\ndatabases with low connectivity. Adopting a multi-stage search process, we\nachieve a success probability close to $100\\%$ on Cayley trees with large\nbranching factors. We find that the number of stages required is given by the\nheight of the Cayley tree. At each stage, the jumping rate should be chosen as\ndifferent values. The dominant term of the runtime in the search process is\nproportional to $N^{(2r-1)/2r}$ for the Cayley tree of height $r$ with $N$\nvertices. We further find that one can control the number of stages by\nadjusting the weight of the edges in the graphs. The multi-stage search process\ncan be merged into a single stage, and then an optimal runtime proportional to\n$\\sqrt{N}$ is achieved, yielding a substantial speedup. The search process is\nquite robust under various kinds of small perturbations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 03:36:31 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Wang", "Yunkai", ""], ["Wu", "Shengjun", ""], ["Wang", "Wei", ""]]}, {"id": "1803.01695", "submitter": "Nicola Prezza", "authors": "Dominik Kempa, Alberto Policriti, Nicola Prezza, Eva Rotenberg", "title": "String Attractors: Verification and Optimization", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.ESA.2018.52", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  String attractors [STOC 2018] are combinatorial objects recently introduced\nto unify all known dictionary compression techniques in a single theory. A set\n$\\Gamma\\subseteq [1..n]$ is a $k$-attractor for a string $S\\in[1..\\sigma]^n$ if\nand only if every distinct substring of $S$ of length at most $k$ has an\noccurrence straddling at least one of the positions in $\\Gamma$. Finding the\nsmallest $k$-attractor is NP-hard for $k\\geq3$, but polylogarithmic\napproximations can be found using reductions from dictionary compressors. It is\neasy to reduce the $k$-attractor problem to a set-cover instance where string's\npositions are interpreted as sets of substrings. The main result of this paper\nis a much more powerful reduction based on the truncated suffix tree. Our new\ncharacterization of the problem leads to more efficient algorithms for string\nattractors: we show how to check the validity and minimality of a $k$-attractor\nin near-optimal time and how to quickly compute exact and approximate\nsolutions. For example, we prove that a minimum $3$-attractor can be found in\noptimal $O(n)$ time when $\\sigma\\in O(\\sqrt[3+\\epsilon]{\\log n})$ for any\nconstant $\\epsilon>0$, and $2.45$-approximation can be computed in $O(n)$ time\non general alphabets. To conclude, we introduce and study the complexity of the\nclosely-related sharp-$k$-attractor problem: to find the smallest set of\npositions capturing all distinct substrings of length exactly $k$. We show that\nthe problem is in P for $k=1,2$ and is NP-complete for constant $k\\geq 3$.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 14:50:27 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 11:53:34 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Kempa", "Dominik", ""], ["Policriti", "Alberto", ""], ["Prezza", "Nicola", ""], ["Rotenberg", "Eva", ""]]}, {"id": "1803.01723", "submitter": "Nicola Prezza", "authors": "Nicola Prezza", "title": "Optimal Substring-Equality Queries with Applications to Sparse Text\n  Indexing", "comments": "Refactored according to TALG's reviews. New w.h.p. bounds and Las\n  Vegas algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of encoding a string of length $n$ from an integer\nalphabet of size $\\sigma$ so that access and substring equality queries (that\nis, determining the equality of any two substrings) can be answered\nefficiently. Any uniquely-decodable encoding supporting access must take\n$n\\log\\sigma + \\Theta(\\log (n\\log\\sigma))$ bits. We describe a new data\nstructure matching this lower bound when $\\sigma\\leq n^{O(1)}$ while supporting\nboth queries in optimal $O(1)$ time. Furthermore, we show that the string can\nbe overwritten in-place with this structure. The redundancy of $\\Theta(\\log n)$\nbits and the constant query time break exponentially a lower bound that is\nknown to hold in the read-only model. Using our new string representation, we\nobtain the first in-place subquadratic (indeed, even sublinear in some cases)\nalgorithms for several string-processing problems in the restore model: the\ninput string is rewritable and must be restored before the computation\nterminates. In particular, we describe the first in-place subquadratic Monte\nCarlo solutions to the sparse suffix sorting, sparse LCP array construction,\nand suffix selection problems. With the sole exception of suffix selection, our\nalgorithms are also the first running in sublinear time for small enough sets\nof input suffixes. Combining these solutions, we obtain the first\nsublinear-time Monte Carlo algorithm for building the sparse suffix tree in\ncompact space. We also show how to derandomize our algorithms using small\nspace. This leads to the first Las Vegas in-place algorithm computing the full\nLCP array in $O(n\\log n)$ time and to the first Las Vegas in-place algorithms\nsolving the sparse suffix sorting and sparse LCP array construction problems in\n$O(n^{1.5}\\sqrt{\\log \\sigma})$ time. Running times of these Las Vegas\nalgorithms hold in the worst case with high probability.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 15:22:58 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 12:49:04 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Prezza", "Nicola", ""]]}, {"id": "1803.02032", "submitter": "Adam Gustafson", "authors": "Adam Gustafson, Hariharan Narayanan", "title": "John's Walk", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CG cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an affine-invariant random walk for drawing uniform random samples\nfrom a convex body $\\mathcal{K} \\subset \\mathbb{R}^n$ that uses maximum volume\ninscribed ellipsoids, known as John's ellipsoids, for the proposal\ndistribution. Our algorithm makes steps using uniform sampling from the John's\nellipsoid of the symmetrization of $\\mathcal{K}$ at the current point. We show\nthat from a warm start, the random walk mixes in $\\widetilde{O}(n^7)$ steps\nwhere the log factors depend only on constants associated with the warm start\nand desired total variation distance to uniformity. We also prove polynomial\nmixing bounds starting from any fixed point $x$ such that for any chord $pq$ of\n$\\mathcal{K}$ containing $x$, $\\left|\\log \\frac{|p-x|}{|q-x|}\\right|$ is\nbounded above by a polynomial in $n$.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 07:01:51 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 19:36:44 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Gustafson", "Adam", ""], ["Narayanan", "Hariharan", ""]]}, {"id": "1803.02270", "submitter": "Lin Yang", "authors": "Vladimir Braverman, Emanuele Viola, David Woodruff, Lin F. Yang", "title": "Revisiting Frequency Moment Estimation in Random Order Streams", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit one of the classic problems in the data stream literature, namely,\nthat of estimating the frequency moments $F_p$ for $0 < p < 2$ of an underlying\n$n$-dimensional vector presented as a sequence of additive updates in a stream.\nIt is well-known that using $p$-stable distributions one can approximate any of\nthese moments up to a multiplicative $(1+\\epsilon)$-factor using\n$O(\\epsilon^{-2} \\log n)$ bits of space, and this space bound is optimal up to\na constant factor in the turnstile streaming model. We show that surprisingly,\nif one instead considers the popular random-order model of insertion-only\nstreams, in which the updates to the underlying vector arrive in a random\norder, then one can beat this space bound and achieve $\\tilde{O}(\\epsilon^{-2}\n+ \\log n)$ bits of space, where the $\\tilde{O}$ hides poly$(\\log(1/\\epsilon) +\n\\log \\log n)$ factors. If $\\epsilon^{-2} \\approx \\log n$, this represents a\nroughly quadratic improvement in the space achievable in turnstile streams. Our\nalgorithm is in fact deterministic, and we show our space bound is optimal up\nto poly$(\\log(1/\\epsilon) + \\log \\log n)$ factors for deterministic algorithms\nin the random order model. We also obtain a similar improvement in space for $p\n= 2$ whenever $F_2 \\gtrsim \\log n\\cdot F_1$.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 16:01:11 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Braverman", "Vladimir", ""], ["Viola", "Emanuele", ""], ["Woodruff", "David", ""], ["Yang", "Lin F.", ""]]}, {"id": "1803.02423", "submitter": "Daniel Sussman", "authors": "Daniel L. Sussman, Youngser Park, Carey E. Priebe, Vince Lyzinski", "title": "Matched Filters for Noisy Induced Subgraph Detection", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding the vertex correspondence between two noisy graphs\nwith different number of vertices where the smaller graph is still large has\nmany applications in social networks, neuroscience, and computer vision. We\npropose a solution to this problem via a graph matching matched filter:\ncentering and padding the smaller adjacency matrix and applying graph matching\nmethods to align it to the larger network. The centering and padding schemes\ncan be incorporated into any algorithm that matches using adjacency matrices.\nUnder a statistical model for correlated pairs of graphs, which yields a noisy\ncopy of the small graph within the larger graph, the resulting optimization\nproblem can be guaranteed to recover the true vertex correspondence between the\nnetworks.\n  However, there are currently no efficient algorithms for solving this\nproblem. To illustrate the possibilities and challenges of such problems, we\nuse an algorithm that can exploit a partially known correspondence and show via\nvaried simulations and applications to {\\it Drosophila} and human connectomes\nthat this approach can achieve good performance.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 20:55:17 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 21:28:38 GMT"}, {"version": "v3", "created": "Mon, 1 Jul 2019 20:23:00 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Sussman", "Daniel L.", ""], ["Park", "Youngser", ""], ["Priebe", "Carey E.", ""], ["Lyzinski", "Vince", ""]]}, {"id": "1803.02540", "submitter": "Adam Sealfon", "authors": "Shafi Goldwasser, Rafail Ostrovsky, Alessandra Scafuro and Adam\n  Sealfon", "title": "Population stability: regulating size in the presence of an adversary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new coordination problem in distributed computing that we call\nthe population stability problem. A system of agents each with limited memory\nand communication, as well as the ability to replicate and self-destruct, is\nsubjected to attacks by a worst-case adversary that can at a bounded rate (1)\ndelete agents chosen arbitrarily and (2) insert additional agents with\narbitrary initial state into the system. The goal is perpetually to maintain a\npopulation whose size is within a constant factor of the target size $N$. The\nproblem is inspired by the ability of complex biological systems composed of a\nmultitude of memory-limited individual cells to maintain a stable population\nsize in an adverse environment. Such biological mechanisms allow organisms to\nheal after trauma or to recover from excessive cell proliferation caused by\ninflammation, disease, or normal development.\n  We present a population stability protocol in a communication model that is a\nsynchronous variant of the population model of Angluin et al. In each round,\npairs of agents selected at random meet and exchange messages, where at least a\nconstant fraction of agents is matched in each round. Our protocol uses\nthree-bit messages and $\\omega(\\log^2 N)$ states per agent. We emphasize that\nour protocol can handle an adversary that can both insert and delete agents, a\nsetting in which existing approximate counting techniques do not seem to apply.\nThe protocol relies on a novel coloring strategy in which the population size\nis encoded in the variance of the distribution of colors. Individual agents can\nlocally obtain a weak estimate of the population size by sampling from the\ndistribution, and make individual decisions that robustly maintain a stable\nglobal population size.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 06:48:35 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Goldwasser", "Shafi", ""], ["Ostrovsky", "Rafail", ""], ["Scafuro", "Alessandra", ""], ["Sealfon", "Adam", ""]]}, {"id": "1803.02565", "submitter": "Takuro Fukunaga", "authors": "Yasushi Kawase, Hanna Sumita, Takuro Fukunaga", "title": "Submodular maximization with uncertain knapsack capacity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the maximization problem of monotone submodular functions under\nan uncertain knapsack constraint. Specifically, the problem is discussed in the\nsituation that the knapsack capacity is not given explicitly and can be\naccessed only through an oracle that answers whether or not the current\nsolution is feasible when an item is added to the solution. Assuming that\ncancellation of the last item is allowed when it overflows the knapsack\ncapacity, we discuss the robustness ratios of adaptive policies for this\nproblem, which are the worst case ratios of the objective values achieved by\nthe output solutions to the optimal objective values. We present a randomized\npolicy of robustness ratio $(1-1/e)/2$, and a deterministic policy of\nrobustness ratio $2(1-1/e)/21$. We also consider a universal policy that\nchooses items following a precomputed sequence. We present a randomized\nuniversal policy of robustness ratio $(1-1/\\sqrt[4]{e})/2$. When the\ncancellation is not allowed, no randomized adaptive policy achieves a constant\nrobustness ratio. Because of this hardness, we assume that a probability\ndistribution of the knapsack capacity is given, and consider computing a\nsequence of items that maximizes the expected objective value. We present a\npolynomial-time randomized algorithm of approximation ratio\n$(1-1/\\sqrt[4]{e})/4-\\epsilon$ for any small constant $\\epsilon >0$.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 09:06:21 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Kawase", "Yasushi", ""], ["Sumita", "Hanna", ""], ["Fukunaga", "Takuro", ""]]}, {"id": "1803.02576", "submitter": "Tirso Varela Rodeiro", "authors": "Nieves R. Brisaboa, Guillermo de Bernardo, Gonzalo Navarro, Tirso V.\n  Rodeiro and Diego Seco", "title": "Compact Representations of Event Sequences", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new technique for the efficient management of large sequences\nof multidimensional data, which takes advantage of regularities that arise in\nreal-world datasets and supports different types of aggregation queries. More\nimportantly, our representation is flexible in the sense that the relevant\ndimensions and queries may be used to guide the construction process, easily\nproviding a space-time tradeoff depending on the relevant queries in the\ndomain. We provide two alternative representations for sequences of\nmultidimensional data and describe the techniques to efficiently store the\ndatasets and to perform aggregation queries over the compressed representation.\nWe perform experimental evaluation on realistic datasets, showing the space\nefficiency and query capabilities of our proposal.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 09:57:23 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["de Bernardo", "Guillermo", ""], ["Navarro", "Gonzalo", ""], ["Rodeiro", "Tirso V.", ""], ["Seco", "Diego", ""]]}, {"id": "1803.02661", "submitter": "Haim Avron", "authors": "Liron Mor-Yosef and Haim Avron", "title": "Sketching for Principal Component Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component regression (PCR) is a useful method for regularizing\nlinear regression. Although conceptually simple, straightforward\nimplementations of PCR have high computational costs and so are inappropriate\nwhen learning with large scale data. In this paper, we propose efficient\nalgorithms for computing approximate PCR solutions that are, on one hand, high\nquality approximations to the true PCR solutions (when viewed as minimizer of a\nconstrained optimization problem), and on the other hand entertain rigorous\nrisk bounds (when viewed as statistical estimators). In particular, we propose\nan input sparsity time algorithms for approximate PCR. We also consider\ncomputing an approximate PCR in the streaming model, and kernel PCR. Empirical\nresults demonstrate the excellent performance of our proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 14:09:10 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 07:45:19 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Mor-Yosef", "Liron", ""], ["Avron", "Haim", ""]]}, {"id": "1803.02750", "submitter": "Carlos Baquero", "authors": "Vitor Enes, Paulo S\\'ergio Almeida, Carlos Baquero, Jo\\~ao Leit\\~ao", "title": "Efficient Synchronization of State-based CRDTs", "comments": "To be published at the 35th IEEE International Conference on Data\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To ensure high availability in large scale distributed systems, Conflict-free\nReplicated Data Types (CRDTs) relax consistency by allowing immediate query and\nupdate operations at the local replica, with no need for remote\nsynchronization. State-based CRDTs synchronize replicas by periodically sending\ntheir full state to other replicas, which can become extremely costly as the\nCRDT state grows. Delta-based CRDTs address this problem by producing small\nincremental states (deltas) to be used in synchronization instead of the full\nstate. However, current synchronisation algorithms for Delta-based CRDTs induce\nredundant wasteful delta propagation, performing worse than expected, and\nsurprisingly, no better than State-based. In this paper we: 1) identify two\nsources of inefficiency in current synchronization algorithms for delta-based\nCRDTs; 2) bring the concept of join decomposition to state-based CRDTs; 3)\nexploit join decompositions to obtain optimal deltas and 4) improve the\nefficiency of synchronization algorithms; and finally, 5) evaluate the improved\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 16:28:03 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 16:00:46 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2019 12:37:17 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Enes", "Vitor", ""], ["Almeida", "Paulo S\u00e9rgio", ""], ["Baquero", "Carlos", ""], ["Leit\u00e3o", "Jo\u00e3o", ""]]}, {"id": "1803.02807", "submitter": "Simone Faro", "authors": "Simone Faro and Arianna Pavone", "title": "Flexible and Efficient Algorithms for Abelian Matching in Strings", "comments": "This is a short preliminary version of a full paper submitted to an\n  international journal. Most examples, details, lemmas and theorems have been\n  omitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abelian pattern matching problem consists in finding all substrings of a\ntext which are permutations of a given pattern. This problem finds application\nin many areas and can be solved in linear time by a naive sliding window\napproach. In this short communication we present a new class of algorithms\nbased on a new efficient fingerprint computation approach, called\nHeap-Counting, which turns out to be fast, flexible and easy to be implemented.\nIt can be proved that our solutions have a linear worst case time complexity\nand, in addition, we present an extensive experimental evaluation which shows\nthat our newly presented algorithms are among the most efficient and flexible\nsolutions in practice for the abelian matching problem in strings.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 18:34:11 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Faro", "Simone", ""], ["Pavone", "Arianna", ""]]}, {"id": "1803.02815", "submitter": "Gautam Kamath", "authors": "Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Jacob\n  Steinhardt, Alistair Stewart", "title": "Sever: A Robust Meta-Algorithm for Stochastic Optimization", "comments": "To appear in ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimensions, most machine learning methods are brittle to even a small\nfraction of structured outliers. To address this, we introduce a new\nmeta-algorithm that can take in a base learner such as least squares or\nstochastic gradient descent, and harden the learner to be resistant to\noutliers. Our method, Sever, possesses strong theoretical guarantees yet is\nalso highly scalable -- beyond running the base learner itself, it only\nrequires computing the top singular vector of a certain $n \\times d$ matrix. We\napply Sever on a drug design dataset and a spam classification dataset, and\nfind that in both cases it has substantially greater robustness than several\nbaselines. On the spam dataset, with $1\\%$ corruptions, we achieved $7.4\\%$\ntest error, compared to $13.4\\%-20.5\\%$ for the baselines, and $3\\%$ error on\nthe uncorrupted dataset. Similarly, on the drug design dataset, with $10\\%$\ncorruptions, we achieved $1.42$ mean-squared error test error, compared to\n$1.51$-$2.33$ for the baselines, and $1.23$ error on the uncorrupted dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 18:47:48 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 20:51:06 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kamath", "Gautam", ""], ["Kane", "Daniel M.", ""], ["Li", "Jerry", ""], ["Steinhardt", "Jacob", ""], ["Stewart", "Alistair", ""]]}, {"id": "1803.02862", "submitter": "Guohui Lin", "authors": "Yinhui Cai, Guangting Chen, Yong Chen, Randy Goebel, Guohui Lin,\n  Longcheng Liu and An Zhang", "title": "Approximation algorithms for two-machine flow-shop scheduling with a\n  conflict graph", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path cover is a well-known intractable problem that finds a minimum number of\nvertex disjoint paths in a given graph to cover all the vertices. We show that\na variant, where the objective function is not the number of paths but the\nnumber of length-$0$ paths (that is, isolated vertices), turns out to be\npolynomial-time solvable. We further show that another variant, where the\nobjective function is the total number of length-$0$ and length-$1$ paths, is\nalso polynomial-time solvable. Both variants find applications in approximating\nthe two-machine flow-shop scheduling problem in which job processing has\nconstraints that are formulated as a conflict graph. For the unit jobs, we\npresent a $4/3$-approximation algorithm for the scheduling problem with an\narbitrary conflict graph, based on the exact algorithm for the variants of the\npath cover problem. For the arbitrary jobs while the conflict graph is the\nunion of two disjoint cliques, that is, all the jobs can be partitioned into\ntwo groups such that the jobs in a group are pairwise conflicting, we present a\nsimple $3/2$-approximation algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 20:12:47 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Cai", "Yinhui", ""], ["Chen", "Guangting", ""], ["Chen", "Yong", ""], ["Goebel", "Randy", ""], ["Lin", "Guohui", ""], ["Liu", "Longcheng", ""], ["Zhang", "An", ""]]}, {"id": "1803.02977", "submitter": "Richard Barnes", "authors": "Richard Barnes", "title": "Accelerating a fluvial incision and landscape evolution model with\n  parallelism", "comments": "13 pages, 12 figures, 2 tables", "journal-ref": null, "doi": "10.1016/j.geomorph.2019.01.002", "report-no": null, "categories": "cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving inverse problems and achieving statistical rigour in landscape\nevolution models requires running many model realizations. Parallel computation\nis necessary to achieve this in a reasonable time. However, no previous\nalgorithm is well-suited to leveraging modern parallelism. Here, I describe an\nalgorithm that can utilize the parallel potential of GPUs, many-core\nprocessors, and SIMD instructions, in addition to working well in serial. The\nnew algorithm runs 43x faster (70s vs. 3,000s on a 10,000x10,000 input) than\nthe previous state of the art and exhibits sublinear scaling with input size. I\nalso identify methods for using multidirectional flow routing and quickly\neliminating landscape depressions and local minima. Tips for parallelization\nand a step-by-step guide to achieving it are given to help others achieve good\nperformance with their own code. Complete, well-commented, easily adaptable\nsource code for all versions of the algorithm is available as a supplement and\non Github.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 05:58:54 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Barnes", "Richard", ""]]}, {"id": "1803.03060", "submitter": "Jakub Kozik", "authors": "Lech Duraj, Grzegorz Gutowski, Jakub Kozik", "title": "A note on two-colorability of nonuniform hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a hypergraph $H$, let $q(H)$ denote the expected number of monochromatic\nedges when the color of each vertex in $H$ is sampled uniformly at random from\nthe set of size 2. Let $s_{\\min}(H)$ denote the minimum size of an edge in $H$.\nErd\\H{o}s asked in 1963 whether there exists an unbounded function $g(k)$ such\nthat any hypergraph $H$ with $s_{\\min}(H) \\geq k$ and $q(H) \\leq g(k)$ is two\ncolorable. Beck in 1978 answered this question in the affirmative for a\nfunction $g(k) = \\Theta(\\log^* k)$. We improve this result by showing that, for\nan absolute constant $\\delta>0$, a version of random greedy coloring procedure\nis likely to find a proper two coloring for any hypergraph $H$ with\n$s_{\\min}(H) \\geq k$ and $q(H) \\leq \\delta \\cdot \\log k$.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 12:16:41 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Duraj", "Lech", ""], ["Gutowski", "Grzegorz", ""], ["Kozik", "Jakub", ""]]}, {"id": "1803.03239", "submitter": "Michael P. Kim", "authors": "Michael P. Kim and Omer Reingold and Guy N. Rothblum", "title": "Fairness Through Computationally-Bounded Awareness", "comments": "Appears at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of fair classification within the versatile framework of\nDwork et al. [ITCS '12], which assumes the existence of a metric that measures\nsimilarity between pairs of individuals. Unlike earlier work, we do not assume\nthat the entire metric is known to the learning algorithm; instead, the learner\ncan query this arbitrary metric a bounded number of times. We propose a new\nnotion of fairness called metric multifairness and show how to achieve this\nnotion in our setting. Metric multifairness is parameterized by a similarity\nmetric $d$ on pairs of individuals to classify and a rich collection ${\\cal C}$\nof (possibly overlapping) \"comparison sets\" over pairs of individuals. At a\nhigh level, metric multifairness guarantees that similar subpopulations are\ntreated similarly, as long as these subpopulations are identified within the\nclass ${\\cal C}$.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 18:23:17 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 18:23:37 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kim", "Michael P.", ""], ["Reingold", "Omer", ""], ["Rothblum", "Guy N.", ""]]}, {"id": "1803.03241", "submitter": "Pravesh K Kothari", "authors": "Adam Klivans and Pravesh K. Kothari and Raghu Meka", "title": "Efficient Algorithms for Outlier-Robust Regression", "comments": "27 pages. Appeared in COLT 2018. This update removes Lemma 6.2 that\n  erroneously claimed an information-theoretic lower bound on error rate as a\n  function of fraction of outliers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first polynomial-time algorithm for performing linear or\npolynomial regression resilient to adversarial corruptions in both examples and\nlabels.\n  Given a sufficiently large (polynomial-size) training set drawn i.i.d. from\ndistribution D and subsequently corrupted on some fraction of points, our\nalgorithm outputs a linear function whose squared error is close to the squared\nerror of the best-fitting linear function with respect to D, assuming that the\nmarginal distribution of D over the input space is \\emph{certifiably\nhypercontractive}. This natural property is satisfied by many well-studied\ndistributions such as Gaussian, strongly log-concave distributions and, uniform\ndistribution on the hypercube among others. We also give a simple statistical\nlower bound showing that some distributional assumption is necessary to succeed\nin this setting.\n  These results are the first of their kind and were not known to be even\ninformation-theoretically possible prior to our work.\n  Our approach is based on the sum-of-squares (SoS) method and is inspired by\nthe recent applications of the method for parameter recovery problems in\nunsupervised learning. Our algorithm can be seen as a natural convex relaxation\nof the following conceptually simple non-convex optimization problem: find a\nlinear function and a large subset of the input corrupted sample such that the\nleast squares loss of the function over the subset is minimized over all\npossible large subsets.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 18:30:31 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 04:04:29 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2020 15:42:45 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Klivans", "Adam", ""], ["Kothari", "Pravesh K.", ""], ["Meka", "Raghu", ""]]}, {"id": "1803.03242", "submitter": "Gal Yona", "authors": "Guy N. Rothblum and Gal Yona", "title": "Probably Approximately Metric-Fair Learning", "comments": "Published in International Conference on Machine Learning (ICML) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seminal work of Dwork {\\em et al.} [ITCS 2012] introduced a metric-based\nnotion of individual fairness. Given a task-specific similarity metric, their\nnotion required that every pair of similar individuals should be treated\nsimilarly. In the context of machine learning, however, individual fairness\ndoes not generalize from a training set to the underlying population. We show\nthat this can lead to computational intractability even for simple\nfair-learning tasks.\n  With this motivation in mind, we introduce and study a relaxed notion of {\\em\napproximate metric-fairness}: for a random pair of individuals sampled from the\npopulation, with all but a small probability of error, if they are similar then\nthey should be treated similarly. We formalize the goal of achieving\napproximate metric-fairness simultaneously with best-possible accuracy as\nProbably Approximately Correct and Fair (PACF) Learning. We show that\napproximate metric-fairness {\\em does} generalize, and leverage these\ngeneralization guarantees to construct polynomial-time PACF learning algorithms\nfor the classes of linear and logistic predictors.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 18:35:08 GMT"}, {"version": "v2", "created": "Sun, 1 Jul 2018 07:01:34 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Rothblum", "Guy N.", ""], ["Yona", "Gal", ""]]}, {"id": "1803.03248", "submitter": "Yannic Maus", "authors": "Mohsen Ghaffari, Juho Hirvonen, Fabian Kuhn, Yannic Maus", "title": "Improved Distributed $\\Delta$-Coloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized distributed algorithm that computes a\n$\\Delta$-coloring in any non-complete graph with maximum degree $\\Delta \\geq 4$\nin $O(\\log \\Delta) + 2^{O(\\sqrt{\\log\\log n})}$ rounds, as well as a randomized\nalgorithm that computes a $\\Delta$-coloring in $O((\\log \\log n)^2)$ rounds when\n$\\Delta \\in [3, O(1)]$. Both these algorithms improve on an $O(\\log^3 n/\\log\n\\Delta)$-round algorithm of Panconesi and Srinivasan~[STOC'1993], which has\nremained the state of the art for the past 25 years. Moreover, the latter\nalgorithm gets (exponentially) closer to an $\\Omega(\\log\\log n)$ round lower\nbound of Brandt et al.~[STOC'16].\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 18:46:32 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 10:56:05 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Hirvonen", "Juho", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""]]}, {"id": "1803.03290", "submitter": "Chen Yuan", "authors": "Yiting Zhao, Chen Yuan, Guangyi Liu, Ilya Grinberg", "title": "Graph-based Preconditioning Conjugate Gradient Algorithm for N-1\n  Contingency Analysis", "comments": "5 pages, 8 figures, Proc. of 2018 IEEE Power and Energy Society\n  General Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contingency analysis (CA) plays a critical role to guarantee operation\nsecurity in the modern power systems. With the high penetration of renewable\nenergy, a real-time and comprehensive N-1 CA is needed as a power system\nanalysis tool to ensure system security. In this paper, a graph-based\npreconditioning conjugate gradient (GPCG) approach is proposed for the nodal\nparallel computing in N-1 CA. To pursue a higher performance in the practical\napplication, the coefficient matrix of the base case is used as the incomplete\nLU (ILU) preconditioner for each N-1 scenario. Additionally, the re-dispatch\nstrategy is employed to handle the islanding issues in CA. Finally, computation\nperformance of the proposed GPCG approach is tested on a real provincial system\nin China.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 20:11:28 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Zhao", "Yiting", ""], ["Yuan", "Chen", ""], ["Liu", "Guangyi", ""], ["Grinberg", "Ilya", ""]]}, {"id": "1803.03300", "submitter": "Chen Yuan", "authors": "Chen Yuan, Yuqi Zhou, Guofang Zhang, Guangyi Liu, Renchang Dai, Xi\n  Chen, Zhiwei Wang", "title": "Exploration of Graph Computing in Power System State Estimation", "comments": "5 pages, 2 figures, Proc. of 2018 IEEE Power and Energy Society\n  General Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DM cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased complexity of power systems due to the integration of\nsmart grid technologies and renewable energy resources, more frequent changes\nhave been introduced to system status, and the traditional serial mode of state\nestimation algorithm cannot well meet the restrict time-constrained requirement\nfor the future dynamic power grid, even with advanced computer hardware. To\nguarantee the grid reliability and minimize the impacts caused by system status\nfluctuations, a fast, even SCADA-rate, state estimator is urgently needed. In\nthis paper, a graph based power system modeling is firstly explored and a graph\ncomputing based state estimation is proposed to speed up its performance. The\npower system is represented by a graph, which is a collection of vertices and\nedges, and the measurements are attributes of vertices and edges. Each vertex\ncan independently implement local computation, like formulations of the\nnode-based H matrix, gain matrix and righthand-side (RHS) vector, only with the\ninformation on its connected edges and neighboring vertices. Then, by taking\nadvantages of graph database, these node-based data are conveniently collected\nand stored in the compressed sparse row (CSR) format avoiding the complexity\nand heaviness introduced by the sparse matrices. With communications and\nsynchronization, centralized computation of solving the weighted least square\n(WLS) state estimation is completed with hierarchical parallel computing. The\nproposed strategy is implemented on a graph database platform. The testing\nresults of IEEE 14-bus, IEEE 118-bus systems and a provincial system in China\nverify the accuracy and high-performance of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 20:49:51 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Yuan", "Chen", ""], ["Zhou", "Yuqi", ""], ["Zhang", "Guofang", ""], ["Liu", "Guangyi", ""], ["Dai", "Renchang", ""], ["Chen", "Xi", ""], ["Wang", "Zhiwei", ""]]}, {"id": "1803.03358", "submitter": "Junjie Ye", "authors": "Yixin Cao, Ashutosh Rai, R. B. Sandeep, and Junjie Ye", "title": "A Polynomial Kernel for Diamond-Free Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $H$-free editing problem asks whether we can edit at most $k$ edges to\nmake a graph contain no induced copy of the fixed graph $H$. We obtain a\npolynomial kernel for this problem when $H$ is a diamond. The incompressibility\ndichotomy for $H$ being a 3-connected graph and the classical complexity\ndichotomy suggest that except for $H$ being a complete/empty graph, $H$-free\nediting problems admit polynomial kernels only for a few small graphs $H$.\nTherefore, we believe that our result is an essential step toward a complete\ndichotomy on the compressibility of $H$-free editing. Additionally, we give a\ncubic-vertex kernel for the diamond-free edge deletion problem, which is far\nsimpler than the previous kernel of the same size for the problem.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 02:31:16 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 01:43:10 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Cao", "Yixin", ""], ["Rai", "Ashutosh", ""], ["Sandeep", "R. B.", ""], ["Ye", "Junjie", ""]]}, {"id": "1803.03482", "submitter": "Marc Shapiro", "authors": "Marc Shapiro (Regal, DELYS), Annette Bieniusa, Peter Zeller, Gustavo\n  Petri (IRIF)", "title": "Ensuring referential integrity under causal consistency", "comments": null, "journal-ref": "PaPoC 2018 - 5th W. on Principles and Practice of Consistency for\n  Distributed Data, Apr 2016, Porto, Portugal", "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referential integrity (RI) is an important correctness property of a shared,\ndistributed object storage system. It is sometimes thought that enforcing RI\nrequires a strong form of consistency. In this paper, we argue that causal\nconsistency suffices to maintain RI. We support this argument with pseudocode\nfor a reference CRDT data type that maintains RI under causal consistency.\nQuickCheck has not found any errors in the model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 12:08:46 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Shapiro", "Marc", "", "Regal, DELYS"], ["Bieniusa", "Annette", "", "IRIF"], ["Zeller", "Peter", "", "IRIF"], ["Petri", "Gustavo", "", "IRIF"]]}, {"id": "1803.03528", "submitter": "Francois Queyroi", "authors": "Fran\\c{c}ois Queyroi (GC)", "title": "Biological and Shortest-Path Routing Procedures for Transportation\n  Network Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of efficient transportation networks is an important challenge in\nmany research areas. Among the most promising recent methods, biological\nrouting mimic local rules found in nature. However comparisons with other\nmethods are rare. In this paper we define a common framework to compare network\ndesign method. We use it to compare biological and a shortest-path routing\napproaches. We find that biological routing explore a more efficient set of\nsolution when looking to design a network for uniformly distributed transfers.\nHowever, the difference between the two approaches is not as important for a\nskewed distribution of transfers.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 15:21:43 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Queyroi", "Fran\u00e7ois", "", "GC"]]}, {"id": "1803.03530", "submitter": "Kuan Cheng", "authors": "Kuan Cheng, Bernhard Haeupler, Xin Li, Amirbehshad Shahrasbi, Ke Wu", "title": "Synchronization Strings: Efficient and Fast Deterministic Constructions\n  over Small Alphabets", "comments": "29 pages. arXiv admin note: substantial text overlap with\n  arXiv:1710.07356", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronization strings are recently introduced by Haeupler and Shahrasbi\n(STOC 2017) in the study of codes for correcting insertion and deletion errors\n(insdel codes). They showed that for any parameter $\\varepsilon>0$,\nsynchronization strings of arbitrary length exist over an alphabet whose size\ndepends only on $\\varepsilon$. Specifically, they obtained an alphabet size of\n$O(\\varepsilon^{-4})$, which left an open question on where the minimal size of\nsuch alphabets lies between $\\Omega(\\varepsilon^{-1})$ and\n$O(\\varepsilon^{-4})$. In this work, we partially bridge this gap by providing\nan improved lower bound of $\\Omega(\\varepsilon^{-3/2})$, and an improved upper\nbound of $O(\\varepsilon^{-2})$. We also provide fast explicit constructions of\nsynchronization strings over small alphabets.\n  Further, along the lines of previous work on similar combinatorial objects,\nwe study the extremal question of the smallest possible alphabet size over\nwhich synchronization strings can exist for some constant $\\varepsilon < 1$. We\nshow that one can construct $\\varepsilon$-synchronization strings over\nalphabets of size four while no such string exists over binary alphabets. This\nreduces the extremal question to whether synchronization strings exist over\nternary alphabets.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 02:45:15 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Cheng", "Kuan", ""], ["Haeupler", "Bernhard", ""], ["Li", "Xin", ""], ["Shahrasbi", "Amirbehshad", ""], ["Wu", "Ke", ""]]}, {"id": "1803.03622", "submitter": "Matthias Rost", "authors": "Matthias Rost and Stefan Schmid", "title": "Virtual Network Embedding Approximations: Leveraging Randomized Rounding", "comments": "Extended version of our paper which will be presented at IFIP\n  Networking 2018; v2 is a major revision of v1 and includes the results of our\n  computational evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Virtual Network Embedding Problem (VNEP) captures the essence of many\nresource allocation problems of today's infrastructure providers, which offer\ntheir physical computation and networking resources to customers. Customers\nrequest resources in the form of Virtual Networks, i.e. as a directed graph\nwhich specifies computational requirements at the nodes and communication\nrequirements on the edges. An embedding of a Virtual Network on the shared\nphysical infrastructure is the joint mapping of (virtual) nodes to physical\nservers together with the mapping of (virtual) edges onto paths in the physical\nnetwork connecting the respective servers.\n  This work initiates the study of approximation algorithms for the VNEP.\nConcretely, we study the offline setting with admission control: given multiple\nrequest graphs the task is to embed the most profitable subset while not\nexceeding resource capacities. Our approximation is based on the randomized\nrounding of Linear Programming (LP) solutions. Interestingly, we uncover that\nthe standard LP formulation for the VNEP exhibits an inherent structural\ndeficit when considering general virtual network topologies: its solutions\ncannot be decomposed into valid embeddings. In turn, focusing on the class of\ncactus request graphs, we devise a novel LP formulation, whose solutions can be\ndecomposed into convex combinations of valid embedding. Proving performance\nguarantees of our rounding scheme, we obtain the first approximation algorithm\nfor the VNEP in the resource augmentation model.\n  We propose two types of rounding heuristics and evaluate their performance in\nan extensive computational study. Our results indicate that randomized rounding\ncan yield good solutions (even without augmentations). Specifically, heuristic\nrounding achieves 73.8% of the baseline's profit, while not exceeding\ncapacities.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 18:15:31 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 12:05:44 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Rost", "Matthias", ""], ["Schmid", "Stefan", ""]]}, {"id": "1803.03628", "submitter": "Ulrich Breunig", "authors": "Ulrich Breunig, Roberto Baldacci, Richard F. Hartl, Thibaut Vidal", "title": "The Electric Two-echelon Vehicle Routing Problem", "comments": null, "journal-ref": null, "doi": "10.1016/j.cor.2018.11.005", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-echelon distribution systems are attractive from an economical standpoint\nand help to keep large vehicles out of city centers. Large trucks can be used\nto deliver goods to intermediate facilities in accessible locations, whereas\nsmaller vehicles allow to reach the final customers. Due to their reduced size\nand emissions, companies consider using an electric fleet of terrestrian or\naerial vehicles for last mile deliveries. Route planning in multi-tier\nlogistics leads to notoriously difficult problems. This difficulty is accrued\nin the presence of an electric fleet, since each vehicle operates on a smaller\nrange, and may require visits to charging stations. To study these challenges,\nwe introduce the Electric Two-echelon Vehicle Routing Problem as a prototypical\nproblem. We propose a large neighbourhood search metaheuristic as well as an\nexact mathematical programming algorithm, which uses decomposition techniques\nto enumerate promising first-level solutions, in conjunction with bounding\nfunctions and route enumeration for the second-level routes. These algorithms\nproduce optimal or near-optimal solutions for the problem, and allow us to\nevaluate the impact of several defining features of optimized battery-powered\ndistribution networks. We created representative E2EVRP benchmark instances to\nsimulate realistic metropolitan areas. In particular, we observe that the\ndetour miles due to recharging decrease proportionally to $1/\\rho^x$ with $x\n\\approx 5/4$ as a function of the charging stations density $\\rho$; e.g., in a\nscenario where the density of charging stations is doubled, recharging detours\nare reduced by 58\\%. Finally, we evaluate the trade-off between battery\ncapacity and detour miles. This estimate is critical for strategic\nfleet-acquisition decisions, in a context where large batteries are generally\nmore costly and less environment-friendly.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 18:28:17 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2018 10:55:01 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2018 13:59:09 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Breunig", "Ulrich", ""], ["Baldacci", "Roberto", ""], ["Hartl", "Richard F.", ""], ["Vidal", "Thibaut", ""]]}, {"id": "1803.03663", "submitter": "Daniel Paulusma", "authors": "Barnaby Martin, Daniel Paulusma, Erik Jan van Leeuwen", "title": "Disconnected Cuts in Claw-free Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A disconnected cut of a connected graph is a vertex cut that itself also\ninduces a disconnected subgraph. The decision problem whether a graph has a\ndisconnected cut is called Disconnected Cut. This problem is closely related to\nseveral homomorphism and contraction problems, and fits in an extensive line of\nresearch on vertex cuts with additional properties. It is known that\nDisconnected Cut is NP-hard on general graphs, while polynomial-time algorithms\nare known for several graph classes. However, the complexity of the problem on\nclaw-free graphs remained an open question. Its connection to the complexity of\nthe problem to contract a claw-free graph to the 4-vertex cycle $C_4$ led Ito\net al. (TCS 2011) to explicitly ask to resolve this open question.\n  We prove that Disconnected Cut is polynomial-time solvable on claw-free\ngraphs, answering the question of Ito et al. The centerpiece of our result is a\nnovel decomposition theorem for claw-free graphs of diameter 2, which we\nbelieve is of independent interest and expands the research line initiated by\nChudnovsky and Seymour (JCTB 2007-2012) and Hermelin et al. (ICALP 2011). On\nour way to exploit this decomposition theorem, we characterize how disconnected\ncuts interact with certain cobipartite subgraphs, and prove two further novel\nalgorithmic results, namely Disconnected Cut is polynomial-time solvable on\ncircular-arc graphs and line graphs.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 19:23:03 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Martin", "Barnaby", ""], ["Paulusma", "Daniel", ""], ["van Leeuwen", "Erik Jan", ""]]}, {"id": "1803.03767", "submitter": "Richard Santiago", "authors": "Richard Santiago, F. Bruce Shepherd", "title": "Multi-Agent Submodular Optimization", "comments": "arXiv admin note: text overlap with arXiv:1612.05222", "journal-ref": "Proceedings of the 21st International Conference on Approximation\n  Algorithms for Combinatorial Optimization Problems (APPROX), 116:23:1-23:20,\n  2018", "doi": "10.4230/LIPIcs.APPROX-RANDOM.2018.23", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen many algorithmic advances in the area of submodular\noptimization: (SO) $\\min/\\max~f(S): S \\in \\mathcal{F}$, where $\\mathcal{F}$ is\na given family of feasible sets over a ground set $V$ and $f:2^V \\rightarrow\n\\mathbb{R}$ is submodular. This progress has been coupled with a wealth of new\napplications for these models. Our focus is on a more general class of\n\\emph{multi-agent submodular optimization} (MASO) which was introduced by Goel\net al. in the minimization setting: $\\min \\sum_i f_i(S_i): S_1 \\uplus S_2\n\\uplus \\cdots \\uplus S_k \\in \\mathcal{F}$. Here we use $\\uplus$ to denote\ndisjoint union and hence this model is attractive where resources are being\nallocated across $k$ agents, each with its own submodular cost function\n$f_i()$. In this paper we explore the extent to which the approximability of\nthe multi-agent problems are linked to their single-agent {\\em primitives},\nreferred to informally as the {\\em multi-agent gap}.\n  We present different reductions that transform a multi-agent problem into a\nsingle-agent one. For maximization we show that (MASO) admits an\n$O(\\alpha)$-approximation whenever (SO) admits an $\\alpha$-approximation over\nthe multilinear formulation, and thus substantially expanding the family of\ntractable models. We also discuss several family classes (such as spanning\ntrees, matroids, and $p$-systems) that have a provable multi-agent gap of 1. In\nthe minimization setting we show that (MASO) has an $O(\\alpha \\cdot \\min \\{k,\n\\log^2 (n)\\})$-approximation whenever (SO) admits an $\\alpha$-approximation\nover the convex formulation. In addition, we discuss the class of \"bounded\nblocker\" families where there is a provably tight O$(\\log n)$ gap between\n(MASO) and (SO).\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 06:22:46 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2018 01:57:00 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Santiago", "Richard", ""], ["Shepherd", "F. Bruce", ""]]}, {"id": "1803.03831", "submitter": "Anne Morvan", "authors": "Rafael Pinot, Anne Morvan, Florian Yger, C\\'edric Gouy-Pailler, Jamal\n  Atif", "title": "Graph-based Clustering under Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the first differentially private clustering method\nfor arbitrary-shaped node clusters in a graph. This algorithm takes as input\nonly an approximate Minimum Spanning Tree (MST) $\\mathcal{T}$ released under\nweight differential privacy constraints from the graph. Then, the underlying\nnonconvex clustering partition is successfully recovered from cutting optimal\ncuts on $\\mathcal{T}$. As opposed to existing methods, our algorithm is\ntheoretically well-motivated. Experiments support our theoretical findings.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 16:26:47 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Pinot", "Rafael", ""], ["Morvan", "Anne", ""], ["Yger", "Florian", ""], ["Gouy-Pailler", "C\u00e9dric", ""], ["Atif", "Jamal", ""]]}, {"id": "1803.03833", "submitter": "Pan Li", "authors": "Pan Li and Olgica Milenkovic", "title": "Submodular Hypergraphs: p-Laplacians, Cheeger Inequalities and Spectral\n  Clustering", "comments": "A short version of this paper is presented in ICML 2018. This version\n  includes the definition of a sequence of eigenvalues for 1-Laplacian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce submodular hypergraphs, a family of hypergraphs that have\ndifferent submodular weights associated with different cuts of hyperedges.\nSubmodular hypergraphs arise in clustering applications in which higher-order\nstructures carry relevant information. For such hypergraphs, we define the\nnotion of p-Laplacians and derive corresponding nodal domain theorems and k-way\nCheeger inequalities. We conclude with the description of algorithms for\ncomputing the spectra of 1- and 2-Laplacians that constitute the basis of new\nspectral hypergraph clustering methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 16:42:05 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 19:36:36 GMT"}, {"version": "v3", "created": "Mon, 24 Sep 2018 23:45:45 GMT"}, {"version": "v4", "created": "Thu, 11 Oct 2018 16:17:55 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Li", "Pan", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1803.03839", "submitter": "Kunihiro Wasa", "authors": "Kunihiro Wasa and Takeaki Uno", "title": "Efficient Enumeration of Bipartite Subgraphs in Graphs", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-94776-1_38", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph enumeration problems ask to output all subgraphs of an input graph\nthat belongs to the specified graph class or satisfy the given constraint.\nThese problems have been widely studied in theoretical computer science. As\nfar, many efficient enumeration algorithms for the fundamental substructures\nsuch as spanning trees, cycles, and paths, have been developed. This paper\naddresses the enumeration problem of bipartite subgraphs. Even though bipartite\ngraphs are quite fundamental and have numerous applications in both theory and\napplication, its enumeration algorithms have not been intensively studied, to\nthe best of our knowledge. We propose the first non-trivial algorithms for\nenumerating all bipartite subgraphs in a given graph. As the main results, we\ndevelop two efficient algorithms: the one enumerates all bipartite induced\nsubgraphs of a graph with degeneracy $k$ in $O(k)$ time per solution. The other\nenumerates all bipartite subgraphs in $O(1)$ time per solution.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 17:08:51 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Wasa", "Kunihiro", ""], ["Uno", "Takeaki", ""]]}, {"id": "1803.03922", "submitter": "Yuechao Pan", "authors": "Yuechao Pan, Roger Pearce and John D. Owens", "title": "Scalable Breadth-First Search on a GPU Cluster", "comments": "12 pages, 13 figures. To appear at IPDPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On a GPU cluster, the ratio of high computing power to communication\nbandwidth makes scaling breadth-first search (BFS) on a scale-free graph\nextremely challenging. By separating high and low out-degree vertices, we\npresent an implementation with scalable computation and a model for scalable\ncommunication for BFS and direction-optimized BFS. Our communication model uses\nglobal reduction for high-degree vertices, and point-to-point transmission for\nlow-degree vertices. Leveraging the characteristics of degree separation, we\nreduce the graph size to one third of the conventional edge list\nrepresentation. With several other optimizations, we observe linear weak\nscaling as we increase the number of GPUs, and achieve 259.8 GTEPS on a\nscale-33 Graph500 RMAT graph with 124 GPUs on the latest CORAL early access\nsystem.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 08:18:51 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 06:37:57 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Pan", "Yuechao", ""], ["Pearce", "Roger", ""], ["Owens", "John D.", ""]]}, {"id": "1803.03945", "submitter": "Eleni Bakali", "authors": "Alexandros Angelopoulos, Eleni Bakali", "title": "Exact uniform sampling over catalan structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new framework for creating elegant algorithms for exact uniform\nsampling of important Catalan structures, such as triangulations of convex\npolygons, Dyck words, monotonic lattice paths and mountain ranges. Along with\nsampling, we obtain optimal coding, and optimal number of random bits required\nfor the algorithm. The framework is based on an original two-parameter\nrecursive relation, where Ballot and Catalan numbers appear and which may be\nregarded as to demonstrate a generalized reduction argument. We then describe\n(a) a unique $n\\times n$ matrix to be used for any of the problems -the common\npre-processing step of our framework- and (b) a linear height tree, where\nleaves correspond one by one to all distinct solutions of each problem;\nsampling is essentially done by selecting a path from the root to a leaf - the\nmain algorithm. Our main algorithm is linear for a number of the problems\nmentioned.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 11:26:25 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Angelopoulos", "Alexandros", ""], ["Bakali", "Eleni", ""]]}, {"id": "1803.04282", "submitter": "Andrej Sajenko", "authors": "Frank Kammer, Andrej Sajenko", "title": "Linear-Time In-Place DFS and BFS on the Word RAM", "comments": "Short version of this paper is accepted to CIAC2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an in-place depth first search (DFS) and an in-place breadth first\nsearch (BFS) that runs on a word RAM in linear time such that, if the adjacency\narrays of the input graph are given in a sorted order, the input is restored\nafter running the algorithm. To obtain our results we use properties of the\nrepresentation used to store the given graph and show several linear-time\nin-place graph transformations from one representation into another.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 14:29:54 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 16:16:50 GMT"}, {"version": "v3", "created": "Sat, 3 Nov 2018 01:14:37 GMT"}, {"version": "v4", "created": "Tue, 29 Jan 2019 10:22:12 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Kammer", "Frank", ""], ["Sajenko", "Andrej", ""]]}, {"id": "1803.04292", "submitter": "Bertil Chapuis", "authors": "Bertil Chapuis, Benoit Garbinato", "title": "Geodabs: Trajectory Indexing Meets Fingerprinting at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding trajectories and discovering motifs that are similar in large\ndatasets is a central problem for a wide range of applications. Solutions\naddressing this problem usually rely on spatial indexing and on the computation\nof a similarity measure in polynomial time. Although effective in the context\nof sparse trajectory datasets, this approach is too expensive in the context of\ndense datasets, where many trajectories potentially match with a given query.\nIn this paper, we apply fingerprinting, a copy-detection mechanism used in the\ncontext of textual data, to trajectories. To this end, we fingerprint\ntrajectories with geodabs, a construction based on geohash aimed at trajectory\nfingerprinting. We demonstrate that by relying on the properties of a space\nfilling curve geodabs can be used to build sharded inverted indexes. We show\nhow normalization affects precision and recall, two key measures in information\nretrieval. We then demonstrate that the probabilistic nature of fingerprinting\nhas a marginal effect on the quality of the results. Finally, we evaluate our\nmethod in terms of performances and show that, in contrast with existing\nmethods, it is not affected by the density of the trajectory dataset and that\nit can be efficiently distributed.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 14:48:06 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Chapuis", "Bertil", ""], ["Garbinato", "Benoit", ""]]}, {"id": "1803.04327", "submitter": "Martin Milani\\v{c}", "authors": "Nina Chiarelli, Tatiana Romina Hartinger, Valeria Alejandra Leoni,\n  Maria In\\'es Lopez Pujato, Martin Milani\\v{c}", "title": "New Algorithms for Weighted $k$-Domination and Total $k$-Domination\n  Problems in Proper Interval Graphs", "comments": "Extended abstract in ISCO 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a positive integer $k$, a $k$-dominating set in a graph $G$ is a set of\nvertices such that every vertex not in the set has at least $k$ neighbors in\nthe set. A total $k$-dominating set, also known as a $k$-tuple total dominating\nset, is a set of vertices such that every vertex of the graph has at least $k$\nneighbors in the set. The problems of finding the minimum size of a\n$k$-dominating, respectively total $k$-dominating set, in a given graph, are\nreferred to as $k$-domination, respectively total $k$-domination. These\ngeneralizations of the classical domination and total domination problems are\nknown to be NP-hard in the class of chordal graphs, and, more specifically,\neven in the classes of split graphs (both problems) and undirected path graphs\n(in the case of total $k$-domination). On the other hand, it follows from\nrecent work of Kang et al.~(2017) that these two families of problems are\nsolvable in time $\\mathcal{O}(|V(G)|^{6k+4})$ in the class of interval graphs.\nWe develop faster algorithms for $k$-domination and total $k$-domination in the\nclass of proper interval graphs, by means of reduction to a single shortest\npath computation in a derived directed acyclic graph with\n$\\mathcal{O}(|V(G)|^{2k})$ nodes and $\\mathcal{O}(|V(G)|^{4k})$ arcs. We show\nthat a suitable implementation, which avoids constructing all arcs of the\ndigraph, leads to a running time of $\\mathcal{O}(|V(G)|^{3k})$. The algorithms\nare also applicable to the weighted case.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 15:55:49 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 21:36:42 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 05:31:40 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Chiarelli", "Nina", ""], ["Hartinger", "Tatiana Romina", ""], ["Leoni", "Valeria Alejandra", ""], ["Pujato", "Maria In\u00e9s Lopez", ""], ["Milani\u010d", "Martin", ""]]}, {"id": "1803.04340", "submitter": "Alastair Abbott", "authors": "Alastair A. Abbott, Cristian S. Calude, Michael J. Dinneen, Richard\n  Hua", "title": "A Hybrid Quantum-Classical Paradigm to Mitigate Embedding Costs in\n  Quantum Annealing", "comments": "30 pages, 6 figures", "journal-ref": "International Journal of Quantum Information 17(5), 1950042 (2019)", "doi": "10.1142/S0219749919500424", "report-no": "CDMTCS Research Report 520", "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite rapid recent progress towards the development of quantum computers\ncapable of providing computational advantages over classical computers, it\nseems likely that such computers will, initially at least, be required to run\nin a hybrid quantum-classical regime. This realisation has led to interest in\nhybrid quantum-classical algorithms allowing, for example, quantum computers to\nsolve large problems despite having very limited numbers of qubits. Here we\npropose a hybrid paradigm for quantum annealers with the goal of mitigating a\ndifferent limitation of such devices: the need to embed problem instances\nwithin the (often highly restricted) connectivity graph of the annealer. This\nembedding process can be costly to perform and may destroy any computational\nspeedup. In order to solve many practical problems, it is moreover necessary to\nperform many, often related, such embeddings. We will show how, for such\nproblems, a raw speedup that is negated by the embedding time can nonetheless\nbe exploited to give a real speedup. As a proof-of-concept example we present\nan in-depth case study of a simple problem based on the maximum weight\nindependent set problem. Although we do not observe a quantum speedup\nexperimentally, the advantage of the hybrid approach is robustly verified,\nshowing how a potential quantum speedup may be exploited and encouraging\nfurther efforts to apply the approach to problems of more practical interest.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 16:06:12 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 12:59:04 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 13:57:38 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Abbott", "Alastair A.", ""], ["Calude", "Cristian S.", ""], ["Dinneen", "Michael J.", ""], ["Hua", "Richard", ""]]}, {"id": "1803.04354", "submitter": "Houda Khrouf", "authors": "Houda Khrouf and Rapha\\\"el Troncy", "title": "Topical Community Detection in Event-based Social Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based services have recently witnessed a rapid growth driving the way\npeople explore and share information of interest. They host a huge amount of\nusers' activities including explicit RSVP, shared photos, comments and social\nconnections. Exploiting these activities to detect communities of similar users\nis a challenging problem. In reality, a community in event-based social network\n(ESBN) is a group of users not only sharing common events and friends, but also\nhaving similar topical interests. However, such community could not be detected\nby most of existing methods which mainly draw on link analysis in the network.\nTo address this problem, there is a need to capitalize on the semantics of\nshared objects along with the structural properties, and to generate\noverlapping communities rather than disjoint ones. In this paper, we propose to\nleverage the users' activities around events with the aim to detect communities\nbased on topical clustering and link analysis that maximize a new form of\nsemantic modularity. We particularly highlight the difference between online\nand offline social interactions, and the influence of event categories to\ndetect communities. Experimental results on real datasets showed that our\napproach was able to detect semantically meaningful communities compared with\nexisting state of the art methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 16:23:46 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Khrouf", "Houda", ""], ["Troncy", "Rapha\u00ebl", ""]]}, {"id": "1803.04452", "submitter": "Matthias Rost", "authors": "Matthias Rost and Stefan Schmid", "title": "(FPT-)Approximation Algorithms for the Virtual Network Embedding Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many resource allocation problems in the cloud can be described as a basic\nVirtual Network Embedding Problem (VNEP): finding mappings of request graphs\n(describing the workloads) onto a substrate graph (describing the physical\ninfrastructure). In the offline setting, the two natural objectives are profit\nmaximization, i.e., embedding a maximal number of request graphs subject to the\nresource constraints, and cost minimization, i.e., embedding all requests at\nminimal overall cost. The VNEP can be seen as a generalization of classic\nrouting and call admission problems, in which requests are arbitrary graphs\nwhose communication endpoints are not fixed. Due to its applications, the\nproblem has been studied intensively in the networking community. However, the\nunderlying algorithmic problem is hardly understood.\n  This paper presents the first fixed-parameter tractable approximation\nalgorithms for the VNEP. Our algorithms are based on randomized rounding. Due\nto the flexible mapping options and the arbitrary request graph topologies, we\nshow that a novel linear program formulation is required. Only using this novel\nformulation the computation of convex combinations of valid mappings is\nenabled, as the formulation needs to account for the structure of the request\ngraphs. Accordingly, to capture the structure of request graphs, we introduce\nthe graph-theoretic notion of extraction orders and extraction width and show\nthat our algorithms have exponential runtime in the request graphs' maximal\nwidth. Hence, for request graphs of fixed extraction width, we obtain the first\npolynomial-time approximations.\n  Studying the new notion of extraction orders we show that (i) computing\nextraction orders of minimal width is NP-hard and (ii) that computing\ndecomposable LP solutions is in general NP-hard, even when restricting request\ngraphs to planar ones.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 18:44:46 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Rost", "Matthias", ""], ["Schmid", "Stefan", ""]]}, {"id": "1803.04555", "submitter": "Nil Mamano", "authors": "David Eppstein, Michael T. Goodrich, and Nil Mamano", "title": "Reactive Proximity Data Structures for Graphs", "comments": "Full version of the paper in 13th Latin American Theoretical\n  INformatics Symposium (LATIN 2018), April 16-10, 2018, Buenos Aires,\n  Argentina. 2nd version: improved writeup and slight improvements in the\n  runtimes via a more detailed analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider data structures for graphs where we maintain a subset of the\nnodes called sites, and allow proximity queries, such as asking for the closest\nsite to a query node, and update operations that enable or disable nodes as\nsites. We refer to a data structure that can efficiently react to such updates\nas reactive. We present novel reactive proximity data structures for graphs of\npolynomial expansion, i.e., the class of graphs with small separators, such as\nplanar graphs and road networks. Our data structures can be used in several\nlogistical problems and geographic information systems dealing with real-time\ndata, such as emergency dispatching. We experimentally compare our data\nstructure to Dijkstra's algorithm in a system emulating random queries in a\nreal road network.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 22:21:43 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 11:46:24 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Eppstein", "David", ""], ["Goodrich", "Michael T.", ""], ["Mamano", "Nil", ""]]}, {"id": "1803.04578", "submitter": "Tigran Tonoyan", "authors": "Magnus M. Halldorsson and Guy Kortsarz and Pradipta Mitra and Tigran\n  Tonoyan", "title": "Spanning Trees With Edge Conflicts and Wireless Connectivity", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of finding a spanning tree along with a partition of\nthe tree edges into fewest number of feasible sets, where constraints on the\nedges define feasibility. The motivation comes from wireless networking, where\nwe seek to model the irregularities seen in actual wireless environments. Not\nall node pairs may be able to communicate, even if geographically close ---\nthus, the available pairs are modeled with a link graph $\\mathcal{L}=(V,E)$.\nAlso, signal attenuation need not follow a nice geometric formulas --- hence,\ninterference is modeled by a conflict (hyper)graph $\\mathcal{C}=(E,F)$ on the\nlinks. The objective is to maximize the efficiency of the communication, or\nequivalently minimizing the length of a schedule of the tree edges in the form\nof a coloring.\n  We find that in spite of all this generality, the problem can be approximated\nlinearly in terms of a versatile parameter, the inductive independence of the\ninterference graph. Specifically, we give a simple algorithm that attains a\n$O(\\rho \\log n)$-approximation, where $n$ is the number of nodes and $\\rho$ is\nthe inductive independence, and show that near-linear dependence on $\\rho$ is\nalso necessary. We also treat an extension to Steiner trees, modeling\nmulticasting, and obtain a comparable result.\n  Our results suggest that several canonical assumptions of geometry,\nregularity and \"niceness\" in wireless settings can sometimes be relaxed without\na significant hit in algorithm performance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 23:54:34 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Halldorsson", "Magnus M.", ""], ["Kortsarz", "Guy", ""], ["Mitra", "Pradipta", ""], ["Tonoyan", "Tigran", ""]]}, {"id": "1803.04660", "submitter": "Laurent Viennot", "authors": "Feodor Dragan, Michel Habib (IRIF, GANG), Laurent Viennot (IRIF, GANG)", "title": "Revisiting Radius, Diameter, and all Eccentricity Computation in Graphs\n  through Certificates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce notions of certificates allowing to bound eccentricities in a\ngraph. In particular , we revisit radius (minimum eccentricity) and diameter\n(maximum eccentricity) computation and explain the efficiency of practical\nradius and diameter algorithms by the existence of small certificates for\nradius and diameter plus few additional properties. We show how such\ncomputation is related to covering a graph with certain balls or complementary\nof balls. We introduce several new algorithmic techniques related to\neccentricity computation and propose algorithms for radius, diameter and all\neccentricities with theoretical guarantees with respect to certain graph\nparameters. This is complemented by experimental results on various real-world\ngraphs showing that these parameters appear to be low in practice. We also\nobtain refined results in the case where the input graph has low doubling\ndimension, has low hyperbolicity, or is chordal.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 07:19:41 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Dragan", "Feodor", "", "IRIF, GANG"], ["Habib", "Michel", "", "IRIF, GANG"], ["Viennot", "Laurent", "", "IRIF, GANG"]]}, {"id": "1803.04744", "submitter": "Lars Rohwedder", "authors": "Klaus Jansen and Lars Rohwedder", "title": "On Integer Programming, Discrepancy, and Convolution", "comments": "A preliminary version appeared in the proceedings of ITCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integer programs with a constant number of constraints are solvable in\npseudo-polynomial time. We give a new algorithm with a better pseudo-polynomial\nrunning time than previous results. Moreover, we establish a strong connection\nto the problem (min, +)-convolution. (min, +)-convolution has a trivial\nquadratic time algorithm and it has been conjectured that this cannot be\nimproved significantly. We show that further improvements to our\npseudo-polynomial algorithm for any fixed number of constraints are equivalent\nto improvements for (min, +)-convolution. This is a strong evidence that our\nalgorithm's running time is the best possible. We also present a faster\nspecialized algorithm for testing feasibility of an integer program with few\nconstraints and for this we also give a tight lower bound, which is based on\nthe SETH.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 12:12:36 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 11:35:07 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2019 08:28:18 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Jansen", "Klaus", ""], ["Rohwedder", "Lars", ""]]}, {"id": "1803.04756", "submitter": "Laure Daviaud", "authors": "Laure Daviaud and Marcin Jurdzinski and Ranko Lazic", "title": "A pseudo-quasi-polynomial algorithm for solving mean-payoff parity games", "comments": null, "journal-ref": null, "doi": "10.1145/3209108.3209162", "report-no": null, "categories": "cs.GT cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a mean-payoff parity game, one of the two players aims both to achieve a\nqualitative parity objective and to minimize a quantitative long-term average\nof payoffs (aka. mean payoff). The game is zero-sum and hence the aim of the\nother player is to either foil the parity objective or to maximize the mean\npayoff.\n  Our main technical result is a pseudo-quasi-polynomial algorithm for solving\nmean-payoff parity games. All algorithms for the problem that have been\ndeveloped for over a decade have a pseudo-polynomial and an exponential factors\nin their running times; in the running time of our algorithm the latter is\nreplaced with a quasi-polynomial one. By the results of Chatterjee and Doyen\n(2012) and of Schewe, Weinert, and Zimmermann (2018), our main technical result\nimplies that there are pseudo-quasi-polynomial algorithms for solving parity\nenergy games and for solving parity games with weights.\n  Our main conceptual contributions are the definitions of strategy\ndecompositions for both players, and a notion of progress measures for\nmean-payoff parity games that generalizes both parity and energy progress\nmeasures. The former provides normal forms for and succinct representations of\nwinning strategies, and the latter enables the application to mean-payoff\nparity games of the order-theoretic machinery that underpins a recent\nquasi-polynomial algorithm for solving parity games.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 12:49:21 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 11:22:19 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 21:13:39 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Daviaud", "Laure", ""], ["Jurdzinski", "Marcin", ""], ["Lazic", "Ranko", ""]]}, {"id": "1803.04865", "submitter": "Raphael Kramer", "authors": "Raphael Kramer and Manuel Iori and Thibaut Vidal", "title": "Mathematical models and search algorithms for the capacitated $p$-center\n  problem", "comments": "29 pages, 7 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacitated p-center problem requires to select p facilities from a set\nof candidates to service a number of customers, subject to facility capacity\nconstraints, with the aim of minimizing the maximum distance between a customer\nand its associated facility. The problem is well known in the field of facility\nlocation, because of the many applications that it can model. In this paper, we\nsolve it by means of search algorithms that iteratively seek the optimal\ndistance by solving tailored subproblems. We present different mathematical\nformulations for the subproblems and improve them by means of several valid\ninequalities, including an effective one based on a 0-1 disjunction and the\nsolution of subset sum problems. We also develop an alternative search strategy\nthat finds a balance between the traditional sequential search and binary\nsearch. This strategy limits the number of feasible subproblems to be solved\nand, at the same time, avoids large overestimates of the solution value, which\nare detrimental for the search. We evaluate the proposed techniques by means of\nextensive computational experiments on benchmark instances from the literature\nand new larger test sets. All instances from the literature with up to 402\nvertices and integer distances are solved to proven optimality, including 13\nopen cases, and feasible solutions are found in 10 minutes for instances with\nup to 3038 vertices.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 15:02:57 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Kramer", "Raphael", ""], ["Iori", "Manuel", ""], ["Vidal", "Thibaut", ""]]}, {"id": "1803.04940", "submitter": "Ohad Trabelsi", "authors": "Lior Kamma and Ohad Trabelsi", "title": "Nearly Optimal Time Bounds for kPath in Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give almost tight conditional lower bounds on the running time of the\nkHyperPath problem. Given an $r$-uniform hypergraph for some integer $r$,\nkHyperPath seeks a tight path of length $k$. That is, a sequence of $k$ nodes\nsuch that every consecutive $r$ of them constitute a hyperedge in the graph.\nThis problem is a natural generalization of the extensively-studied kPath\nproblem in graphs. We show that solving kHyperPath in time\n$O^*(2^{(1-\\gamma)k})$ where $\\gamma>0$ is independent of $r$ is probably\nimpossible. Specifically, it implies that Set Cover on $n$ elements can be\nsolved in time $O^*(2^{(1 - \\delta)n})$ for some $\\delta>0$. The only known\nlower bound for the kPath problem is $2^{\\Omega(k)} poly(n)$ where $n$ is the\nnumber of nodes assuming the Exponential Time Hypothesis (ETH), and finding any\nconditional lower bound with an explicit constant in the exponent has been an\nimportant open problem.\n  We complement our lower bound with an almost tight upper bound. Formally, for\nevery integer $r\\geq 3$ we give algorithms that solve kHyperPath and\nkHyperCycle on $r$-uniform hypergraphs with $n$ nodes and $m$ edges in time\n$2^k m \\cdot poly(n)$ and $2^k m^2 poly(n)$ respectively, and that is even for\nthe directed version of these problems. To the best of our knowledge, this is\nthe first algorithm for kHyperPath. The fastest algorithms known for kPath run\nin time $2^k poly(n)$ for directed graphs (Williams, 2009), and in time $1.66^k\npoly(n)$ for undirected graphs (Bj\\\"orklund \\etal, 2014).\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 17:12:25 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 13:19:37 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Kamma", "Lior", ""], ["Trabelsi", "Ohad", ""]]}, {"id": "1803.05001", "submitter": "Miguel A. Mosteiro", "authors": "Lucas Farach-Colton, Martin Farach-Colton, Leslie Ann Goldberg, John\n  Lapinskas, Reut Levi, Moti Medina and Miguel A. Mosteiro", "title": "Improved Distortion and Spam Resistance for PageRank", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking functions are an integral component of web searching, and are thus\nsubject to link spamming. Quite a lot of practical and theoretical knowledge\nhas accumulated about which types of ranking functions are susceptible to which\ntypes of spam attacks, but there is no general characterization of the class of\nspam attacks nor of the susceptibility of a ranking function to arbitrary spam\nattacks.\n  Our first contribution is a characterization of the class of spam attacks and\na definition of the spam resistance of a ranking function against all possible\nlink-spam attacks. We show how this formal notion of spam resistance matches\nthe intuition of practitioners. This definition allows us to evaluate novel\nranking functions.\n  A natural way to resist spam is to design a ranking function that heavily\nfavors trusted nodes, which are nodes known not be compromised by the spammer.\nHowever, this introduces local distortions into the ranking function. We\nformalize this notion of distortion and show that it matches the intuitive\nnotion used by practitioners. We know of no ranking functions in the literature\nthat combine high spam resistance and low distortion; we consider several\nwell-known ranking functions (including the original form of PageRank) and show\nthat they fail on one or both counts.\n  Finally, we introduce a new form of PageRank known as Min-PPR. We prove that\nMin-PPR has low distortion and high spam resistance. A secondary benefit is\nthat Min-PPR comes with an explicit cost function on nodes that shows how\nimportant they are to the spammer; thus a ranker can focus their spam-detection\ncapacity on these vulnerable nodes. Both Min-PPR and its associated cost\nfunction are straightforward to compute.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 18:52:21 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 10:57:38 GMT"}, {"version": "v3", "created": "Mon, 4 Nov 2019 19:10:02 GMT"}, {"version": "v4", "created": "Wed, 14 Oct 2020 19:24:12 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Farach-Colton", "Lucas", ""], ["Farach-Colton", "Martin", ""], ["Goldberg", "Leslie Ann", ""], ["Lapinskas", "John", ""], ["Levi", "Reut", ""], ["Medina", "Moti", ""], ["Mosteiro", "Miguel A.", ""]]}, {"id": "1803.05084", "submitter": "Scott Freitas", "authors": "Scott Freitas, Hanghang Tong, Nan Cao, Yinglong Xia", "title": "Local Partition in Rich Graphs", "comments": "Under KDD 2018 review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local graph partitioning is a key graph mining tool that allows researchers\nto identify small groups of interrelated nodes (e.g. people) and their\nconnective edges (e.g. interactions). Because local graph partitioning is\nprimarily focused on the network structure of the graph (vertices and edges),\nit often fails to consider the additional information contained in the\nattributes. In this paper we propose---(i) a scalable algorithm to improve\nlocal graph partitioning by taking into account both the network structure of\nthe graph and the attribute data and (ii) an application of the proposed local\ngraph partitioning algorithm (AttriPart) to predict the evolution of local\ncommunities (LocalForecasting). Experimental results show that our proposed\nAttriPart algorithm finds up to 1.6$\\times$ denser local partitions, while\nrunning approximately 43$\\times$ faster than traditional local partitioning\ntechniques (PageRank-Nibble). In addition, our LocalForecasting algorithm shows\na significant improvement in the number of nodes and edges correctly predicted\nover baseline methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 00:25:37 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Freitas", "Scott", ""], ["Tong", "Hanghang", ""], ["Cao", "Nan", ""], ["Xia", "Yinglong", ""]]}, {"id": "1803.05159", "submitter": "Stanislaw Gorlow", "authors": "Pedro J. Villasana T. and Stanislaw Gorlow and Arvind T. Hariraman", "title": "Multiplicative Updates for Convolutional NMF Under $\\beta$-Divergence", "comments": null, "journal-ref": "Optim Lett (2019)", "doi": "10.1007/s11590-019-01434-9", "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we generalize the convolutional NMF by taking the\n$\\beta$-divergence as the contrast function and present the correct\nmultiplicative updates for its factors in closed form. The new updates unify\nthe $\\beta$-NMF and the convolutional NMF. We state why almost all of the\nexisting updates are inexact and approximative w.r.t. the convolutional data\nmodel. We show that our updates are stable and that their convergence\nperformance is consistent across the most common values of $\\beta$.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 08:11:07 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 14:55:30 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["T.", "Pedro J. Villasana", ""], ["Gorlow", "Stanislaw", ""], ["Hariraman", "Arvind T.", ""]]}, {"id": "1803.05361", "submitter": "Yangguang Shi", "authors": "Yuval Emek, Shay Kutten, Ron Lavi, Yangguang Shi", "title": "Approximating Generalized Network Design under (Dis)economies of Scale\n  with Applications to Energy Efficiency", "comments": "39 pages, 1 figure. An extended abstract of this paper is to appear\n  in the 50th Annual ACM Symposium on the Theory of Computing (STOC 2018)", "journal-ref": null, "doi": "10.1145/3188745.3188812", "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a generalized network design (GND) problem, a set of resources are\nassigned to multiple communication requests. Each request contributes its\nweight to the resources it uses and the total load on a resource is then\ntranslated to the cost it incurs via a resource specific cost function. For\nexample, a request may be to establish a virtual circuit, thus contributing to\nthe load on each edge in the circuit. Motivated by energy efficiency\napplications, recently, there is a growing interest in GND using cost functions\nthat exhibit (dis)economies of scale ((D)oS), namely, cost functions that\nappear subadditive for small loads and superadditive for larger loads.\n  The current paper advances the existing literature on approximation\nalgorithms for GND problems with (D)oS cost functions in various aspects: (1)\nwe present a generic approximation framework that yields approximation results\nfor a much wider family of requests in both directed and undirected graphs; (2)\nour framework allows for unrelated weights, thus providing the first\nnon-trivial approximation for the problem of scheduling unrelated parallel\nmachines with (D)oS cost functions; (3) our framework is fully combinatorial\nand runs in strongly polynomial time; (4) the family of (D)oS cost functions\nconsidered in the current paper is more general than the one considered in the\nexisting literature, providing a more accurate abstraction for practical energy\nconservation scenarios; and (5) we obtain the first approximation ratio for GND\nwith (D)oS cost functions that depends only on the parameters of the resources'\ntechnology and does not grow with the number of resources, the number of\nrequests, or their weights. The design of our framework relies heavily on\nRoughgarden's smoothness toolbox (JACM 2015), thus demonstrating the possible\nusefulness of this toolbox in the area of approximation algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 15:39:58 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Emek", "Yuval", ""], ["Kutten", "Shay", ""], ["Lavi", "Ron", ""], ["Shi", "Yangguang", ""]]}, {"id": "1803.05465", "submitter": "Siddharth Gupta", "authors": "Giordano Da Lozzo, David Eppstein, Michael T. Goodrich, and Siddharth\n  Gupta", "title": "Subexponential-Time and FPT Algorithms for Embedded Flat Clustered\n  Planarity", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The C-Planarity problem asks for a drawing of a $\\textit{clustered graph}$,\ni.e., a graph whose vertices belong to properly nested clusters, in which each\ncluster is represented by a simple closed region with no edge-edge crossings,\nno region-region crossings, and no unnecessary edge-region crossings. We study\nC-Planarity for $\\textit{embedded flat clustered graphs}$, graphs with a fixed\ncombinatorial embedding whose clusters partition the vertex set. Our main\nresult is a subexponential-time algorithm to test C-Planarity for these graphs\nwhen their face size is bounded. Furthermore, we consider a variation of the\nnotion of $\\textit{embedded tree decomposition}$ in which, for each face,\nincluding the outer face, there is a bag that contains every vertex of the\nface. We show that C-Planarity is fixed-parameter tractable with the\nembedded-width of the underlying graph and the number of disconnected clusters\nas parameters.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 18:28:22 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Da Lozzo", "Giordano", ""], ["Eppstein", "David", ""], ["Goodrich", "Michael T.", ""], ["Gupta", "Siddharth", ""]]}, {"id": "1803.05499", "submitter": "Gabriele Castellano", "authors": "Gabriele Castellano, Flavio Esposito and Fulvio Risso", "title": "A Distributed Architecture for Edge Service Orchestration with\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Network Function Virtualization paradigm is attracting the interest of\nservice providers, that may greatly benefit from its flexibility and\nscalability properties. However, the diversity of possible orchestrated\nservices, rises the necessity of adopting specific orchestration strategies for\neach service request that are unknown a priori. This paper presents Senate, a\ndistributed architecture that enables precise orchestration of heterogeneous\nservices over a common edge infrastructure. To assign shared resources to\nservice orchestrators, Senate uses the Distributed Orchestration Resource\nAssignment (DORA), an approximation algorithm that we designed to guarantee\nboth a bound on convergence time and an optimal (1-1/e)-approximation with\nrespect to the Pareto optimal resource assignment. We evaluate advantages of\nservice orchestration with Senate and performance of DORA through a prototype\nimplementation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 20:21:29 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Castellano", "Gabriele", ""], ["Esposito", "Flavio", ""], ["Risso", "Fulvio", ""]]}, {"id": "1803.05501", "submitter": "Michal Feldman", "authors": "Alon Eden and Uriel Feige and Michal Feldman", "title": "Max-Min Greedy Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bipartite graph $G(U,V;E)$ that admits a perfect matching is given. One\nplayer imposes a permutation $\\pi$ over $V$, the other player imposes a\npermutation $\\sigma$ over $U$. In the greedy matching algorithm, vertices of\n$U$ arrive in order $\\sigma$ and each vertex is matched to the lowest (under\n$\\pi$) yet unmatched neighbor in $V$ (or left unmatched, if all its neighbors\nare already matched). The obtained matching is maximal, thus matches at least a\nhalf of the vertices. The max-min greedy matching problem asks: suppose the\nfirst (max) player reveals $\\pi$, and the second (min) player responds with the\nworst possible $\\sigma$ for $\\pi$, does there exist a permutation $\\pi$\nensuring to match strictly more than a half of the vertices? Can such a\npermutation be computed in polynomial time?\n  The main result of this paper is an affirmative answer for this question: we\nshow that there exists a polytime algorithm to compute $\\pi$ for which for\nevery $\\sigma$ at least $\\rho > 0.51$ fraction of the vertices of $V$ are\nmatched. We provide additional lower and upper bounds for special families of\ngraphs, including regular and Hamiltonian. Interestingly, even for regular\ngraphs with arbitrarily large degree (implying a large number of disjoint\nperfect matchings), there is no $\\pi$ ensuring to match more than a fraction\n$8/9$ of the vertices.\n  The max-min greedy matching problem solves an open problem regarding the\nwelfare guarantees attainable by pricing in sequential markets with binary\nunit-demand valuations. In addition, it has implications for the size of the\nunique stable matching in markets with global preferences, subject to the graph\nstructure.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 20:28:59 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Eden", "Alon", ""], ["Feige", "Uriel", ""], ["Feldman", "Michal", ""]]}, {"id": "1803.05652", "submitter": "Samson Zhou", "authors": "Jeremiah Blocki, Venkata Gandikota, Elena Grigorescu, Samson Zhou", "title": "Relaxed Locally Correctable Codes in Computationally Bounded Channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error-correcting codes that admit local decoding and correcting algorithms\nhave been the focus of much recent research due to their numerous theoretical\nand practical applications. An important goal is to obtain the best possible\ntradeoffs between the number of queries the algorithm makes to its oracle (the\nlocality of the task), and the amount of redundancy in the encoding (the\ninformation rate).\n  In Hamming's classical adversarial channel model, the current tradeoffs are\ndramatic, allowing either small locality, but superpolynomial blocklength, or\nsmall blocklength, but high locality. However, in the computationally bounded,\nadversarial channel model, proposed by Lipton (STACS 1994), constructions of\nlocally decodable codes suddenly exhibit small locality and small blocklength,\nbut these constructions require strong trusted setup assumptions e.g.,\nOstrovsky, Pandey and Sahai (ICALP 2007) construct private locally decodable\ncodes in the setting where the sender and receiver already share a symmetric\nkey.\n  We study variants of locally decodable and locally correctable codes in\ncomputationally bounded, adversarial channels, in a setting with no public-key\nor private-key cryptographic setup. The only setup assumption we require is the\nselection of the public parameters (seed) for a collision-resistant hash\nfunction. Specifically, we provide constructions of relaxed locally correctable\nand relaxed locally decodable codes over the binary alphabet, with constant\ninformation rate, and poly-logarithmic locality.\n  Our constructions, which compare favorably with their classical analogues in\nthe computationally unbounded Hamming channel, crucially employ\ncollision-resistant hash functions and local expander graphs, extending ideas\nfrom recent cryptographic constructions of memory-hard functions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 09:23:58 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 20:45:49 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Blocki", "Jeremiah", ""], ["Gandikota", "Venkata", ""], ["Grigorescu", "Elena", ""], ["Zhou", "Samson", ""]]}, {"id": "1803.05825", "submitter": "Shay Solomon", "authors": "Noam Solomon and Shay Solomon", "title": "A Generalized Matching Reconfiguration Problem", "comments": "Major revision, different title. Abstract truncated to fit arXiv\n  limits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal in {\\em reconfiguration problems} is to compute a {\\em gradual\ntransformation} between two feasible solutions of a problem such that all\nintermediate solutions are also feasible. In the {\\em Matching Reconfiguration\nProblem} (MRP), proposed in a pioneering work by Ito et al.\\ from 2008, we are\ngiven a graph $G$ and two matchings $M$ and $M'$, and we are asked whether\nthere is a sequence of matchings in $G$ starting with $M$ and ending at $M'$,\neach resulting from the previous one by either adding or deleting a single edge\nin $G$, without ever going through a matching of size $< \\min\\{|M|,|M'|\\}-1$.\nIto et al.\\ gave a polynomial time algorithm for the problem.\n  In this paper we introduce a natural generalization of the MRP that depends\non an integer parameter $\\Delta \\ge 1$: here we are allowed to make $\\Delta$\nchanges to the current solution rather than 1 at each step of the\n{transformation procedure}. There is always a valid sequence of matchings\ntransforming $M$ to $M'$ if $\\Delta$ is sufficiently large, and naturally we\nwould like to minimize $\\Delta$. We first devise an optimal transformation\nprocedure for unweighted matching with $\\Delta = 3$, and then extend it to\nweighted matchings to achieve asymptotically optimal guarantees. The running\ntime of these procedures is linear.\n  We further demonstrate the applicability of this generalized problem to\ndynamic graph matchings. In this area, the number of changes to the maintained\nmatching per update step (the \\emph{recourse bound}) is an important quality\nmeasure. Nevertheless, the \\emph{worst-case} recourse bounds of almost all\nknown dynamic matching algorithms are prohibitively large, much larger than the\ncorresponding update times. We fill in this gap via a surprisingly simple\nblack-box reduction: Any dynamic algorithm for maintaining [...]\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 15:59:45 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 21:17:04 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 21:42:16 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Solomon", "Noam", ""], ["Solomon", "Shay", ""]]}, {"id": "1803.05866", "submitter": "Katharina Huber", "authors": "Manuel Lafond and Nadia El-Mabrouk and Katharina T. Huber and Vincent\n  Moulton", "title": "The complexity of comparing multiply-labelled trees by extending\n  phylogenetic-tree metrics", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multilabeled tree (or MUL-tree) is a rooted tree in which every leaf is\nlabelled by an element from some set, but in which more than one leaf may be\nlabelled by the same element of that set. In phylogenetics, such trees are used\nin biogeographical studies, to study the evolution of gene families, and also\nwithin approaches to construct phylogenetic networks. A multilabelled tree in\nwhich no leaf-labels are repeated is called a phylogenetic tree, and one in\nwhich every label is the same is also known as a tree-shape. In this paper, we\nconsider the complexity of computing metrics on MUL-trees that are obtained by\nextending metrics on phylogenetic trees. In particular, by restricting our\nattention to tree shapes, we show that computing the metric extension on\nMUL-trees is NP complete for two well-known metrics on phylogenetic trees,\nnamely, the path-difference and Robinson Foulds distances. We also show that\nthe extension of the Robinson Foulds distance is fixed parameter tractable with\nrespect to the distance parameter. The path distance complexity result allows\nus to also answer an open problem concerning the complexity of solving the\nquadratic assignment problem for two matrices that are a Robinson similarity\nand a Robinson dissimilarity, which we show to be NP-complete. We conclude by\nconsidering the maximum agreement subtree (MAST) distance on phylogenetic trees\nto MUL-trees. Although its extension to MUL-trees can be computed in polynomial\ntime, we show that computing its natural generalization to more than two\nMUL-trees is NP-complete, although fixed-parameter tractable in the maximum\ndegree when the number of given trees is bounded.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 17:00:24 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Lafond", "Manuel", ""], ["El-Mabrouk", "Nadia", ""], ["Huber", "Katharina T.", ""], ["Moulton", "Vincent", ""]]}, {"id": "1803.05948", "submitter": "Sebastian Wild", "authors": "Sebastian Wild", "title": "Average Cost of QuickXsort with Pivot Sampling", "comments": "updated to final version accepted for AofA 2018", "journal-ref": null, "doi": "10.4230/lipics.aofa.2018.36", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  QuickXsort is a strategy to combine Quicksort with another sorting method X,\nso that the result has essentially the same comparison cost as X in isolation,\nbut sorts in place even when X requires a linear-size buffer. We solve the\nrecurrence for QuickXsort precisely up to the linear term including the\noptimization to choose pivots from a sample of k elements. This allows to\nimmediately obtain overall average costs using only the average costs of\nsorting method X (as if run in isolation). We thereby extend and greatly\nsimplify the analysis of QuickHeapsort and QuickMergesort with practically\nefficient pivot selection, and give the first tight upper bounds including the\nlinear term for such methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 18:58:15 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 17:34:25 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Wild", "Sebastian", ""]]}, {"id": "1803.06010", "submitter": "Shannon McCurdy", "authors": "Shannon R. McCurdy", "title": "Ridge Regression and Provable Deterministic Ridge Leverage Score\n  Sampling", "comments": "24 pages, 15 figures. Minor changes such as typos fixed, some\n  background discussion added, references added", "journal-ref": "Advances in Neural Information Processing Systems (NeurIPS 2018),\n  Montreal, Canada", "doi": null, "report-no": null, "categories": "math.ST cs.DS stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ridge leverage scores provide a balance between low-rank approximation and\nregularization, and are ubiquitous in randomized linear algebra and machine\nlearning. Deterministic algorithms are also of interest in the moderately big\ndata regime, because deterministic algorithms provide interpretability to the\npractitioner by having no failure probability and always returning the same\nresults.\n  We provide provable guarantees for deterministic column sampling using ridge\nleverage scores. The matrix sketch returned by our algorithm is a column subset\nof the original matrix, yielding additional interpretability. Like the\nrandomized counterparts, the deterministic algorithm provides (1 + {\\epsilon})\nerror column subset selection, (1 + {\\epsilon}) error projection-cost\npreservation, and an additive-multiplicative spectral bound. We also show that\nunder the assumption of power-law decay of ridge leverage scores, this\ndeterministic algorithm is provably as accurate as randomized algorithms.\n  Lastly, ridge regression is frequently used to regularize ill-posed linear\nleast-squares problems. While ridge regression provides shrinkage for the\nregression coefficients, many of the coefficients remain small but non-zero.\nPerforming ridge regression with the matrix sketch returned by our algorithm\nand a particular regularization parameter forces coefficients to zero and has a\nprovable (1 + {\\epsilon}) bound on the statistical risk. As such, it is an\ninteresting alternative to elastic net regularization.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 21:35:55 GMT"}, {"version": "v2", "created": "Sun, 23 Dec 2018 21:06:18 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["McCurdy", "Shannon R.", ""]]}, {"id": "1803.06074", "submitter": "Haruka Mizuta", "authors": "Tesshu Hanaka and Takehiro Ito and Haruka Mizuta and Benjamin Moore\n  and Naomi Nishimura and Vijay Subramanya and Akira Suzuki and Krishna\n  Vaidyanathan", "title": "Reconfiguring spanning and induced subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph reconfiguration is a family of problems focusing on the reachability\nof the solution space in which feasible solutions are subgraphs, represented\neither as sets of vertices or sets of edges, satisfying a prescribed graph\nstructure property. Although there has been previous work that can be\ncategorized as subgraph reconfiguration, most of the related results appear\nunder the name of the property under consideration; for example, independent\nset, clique, and matching. In this paper, we systematically clarify the\ncomplexity status of subgraph reconfiguration with respect to graph structure\nproperties.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 04:54:55 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Hanaka", "Tesshu", ""], ["Ito", "Takehiro", ""], ["Mizuta", "Haruka", ""], ["Moore", "Benjamin", ""], ["Nishimura", "Naomi", ""], ["Subramanya", "Vijay", ""], ["Suzuki", "Akira", ""], ["Vaidyanathan", "Krishna", ""]]}, {"id": "1803.06102", "submitter": "Petr Golovach", "authors": "Fedor V. Fomin, Petr A. Golovach, and Fahad Panolan", "title": "Parameterized Low-Rank Binary Matrix Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a number of algorithmic results for the following family of\nproblems: For a given binary m\\times n matrix A and integer k, decide whether\nthere is a \"simple\" binary matrix B which differs from A in at most k entries.\nFor an integer r, the \"simplicity\" of B is characterized as follows.\n  - Binary r-Means: Matrix B has at most r different columns. This problem is\nknown to be NP-complete already for r=2. We show that the problem is solvable\nin time 2^{O(k\\log k)}\\cdot(nm)^{O(1)} and thus is fixed-parameter tractable\nparameterized by k. We prove that the problem admits a polynomial kernel when\nparameterized by r and k but it has no polynomial kernel when parameterized by\nk only unless NP\\subseteq coNP/poly. We also complement these result by showing\nthat when being parameterized by r and k, the problem admits an algorithm of\nrunning time 2^{O(r\\cdot \\sqrt{k\\log{(k+r)}})}(nm)^{O(1)}, which is\nsubexponential in k for r\\in O(k^{1/2 -\\epsilon}) for any \\epsilon>0.\n  - Low GF(2)-Rank Approximation: Matrix B is of GF(2)-rank at most r. This\nproblem is known to be NP-complete already for r=1. It also known to be\nW[1]-hard when parameterized by k. Interestingly, when parameterized by r and\nk, the problem is not only fixed-parameter tractable, but it is solvable in\ntime 2^{O(r^{ 3/2}\\cdot \\sqrt{k\\log{k}})}(nm)^{O(1)}, which is subexponential\nin k.\n  - Low Boolean-Rank Approximation: Matrix B is of Boolean rank at most r. The\nproblem is known to be NP-complete for k=0 as well as for r=1. We show that it\nis solvable in subexponential in k time 2^{O(r2^r\\cdot \\sqrt{k\\log\nk})}(nm)^{O(1)}.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 07:47:27 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Panolan", "Fahad", ""]]}, {"id": "1803.06114", "submitter": "Yuko Kuroki", "authors": "Yuko Kuroki and Tomomi Matsui", "title": "A constant-ratio approximation algorithm for a class of hub-and-spoke\n  network design problems and metric labeling problems: star metric case", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportation networks frequently employ hub-and-spoke network architectures\nto route flows between many origin and destination pairs. Hub facilities work\nas switching points for flows in large networks. In this study, we deal with a\nproblem, called the single allocation hub-and-spoke network design problem. In\nthe problem, the goal is to allocate each non-hub node to exactly one of given\nhub nodes so as to minimize the total transportation cost. The problem is\nessentially equivalent to another combinatorial optimization problem, called\nthe metric labeling problem. The metric labeling problem was first introduced\nby Kleinberg and Tardos in 2002, motivated by application to segmentation\nproblems in computer vision and related areas. In this study, we deal with the\ncase where the set of hubs forms a star, which arises especially in\ntelecommunication networks. We propose a polynomial-time randomized\napproximation algorithm for the problem, whose approximation ratio is less than\n5.281. Our algorithms solve a linear relaxation problem and apply dependent\nrounding procedures.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 09:01:30 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Kuroki", "Yuko", ""], ["Matsui", "Tomomi", ""]]}, {"id": "1803.06324", "submitter": "J\\'er\\'emie Chalopin", "authors": "J\\'er\\'emie Chalopin, Victor Chepoi, Feodor F. Dragan, Guillaume\n  Ducoffe, Abdulhakeem Mohammed, Yann Vax\\`es", "title": "Fast approximation and exact computation of negative curvature\n  parameters of graphs", "comments": null, "journal-ref": null, "doi": "10.1007/s00454-019-00107-9", "report-no": null, "categories": "cs.DS cs.CG cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study Gromov hyperbolicity and related parameters, that\nrepresent how close (locally) a metric space is to a tree from a metric point\nof view. The study of Gromov hyperbolicity for geodesic metric spaces can be\nreduced to the study of graph hyperbolicity. The main contribution of this\npaper is a new characterization of the hyperbolicity of graphs. This\ncharacterization has algorithmic implications in the field of large-scale\nnetwork analysis. A sharp estimate of graph hyperbolicity is useful, e.g., in\nembedding an undirected graph into hyperbolic space with minimum distortion\n[Verbeek and Suri, SoCG'14]. The hyperbolicity of a graph can be computed in\npolynomial-time, however it is unlikely that it can be done in subcubic time.\nThis makes this parameter difficult to compute or to approximate on large\ngraphs. Using our new characterization of graph hyperbolicity, we provide a\nsimple factor 8 approximation algorithm for computing the hyperbolicity of an\n$n$-vertex graph $G=(V,E)$ in optimal time $O(n^2)$ (assuming that the input is\nthe distance matrix of the graph). This algorithm leads to constant factor\napproximations of other graph-parameters related to hyperbolicity (thinness,\nslimness, and insize). We also present the first efficient algorithms for exact\ncomputation of these parameters. All of our algorithms can be used to\napproximate the hyperbolicity of a geodesic metric space.\n  We also show that a similar characterization of hyperbolicity holds for all\ngeodesic metric spaces endowed with a geodesic spanning tree. Along the way, we\nprove that any complete geodesic metric space $(X,d)$ has such a geodesic\nspanning tree. We hope that this fundamental result can be useful in other\ncontexts.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 17:25:06 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 11:58:41 GMT"}, {"version": "v3", "created": "Mon, 3 Jun 2019 10:07:13 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Chalopin", "J\u00e9r\u00e9mie", ""], ["Chepoi", "Victor", ""], ["Dragan", "Feodor F.", ""], ["Ducoffe", "Guillaume", ""], ["Mohammed", "Abdulhakeem", ""], ["Vax\u00e8s", "Yann", ""]]}, {"id": "1803.06416", "submitter": "Rachel Cummings", "authors": "Rachel Cummings, Sara Krehbiel, Kevin A. Lai, Uthaipon Tantipongpipat", "title": "Differential Privacy for Growing Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the design of differentially private algorithms for adaptive\nanalysis of dynamically growing databases, where a database accumulates new\ndata entries while the analysis is ongoing. We provide a collection of tools\nfor machine learning and other types of data analysis that guarantee\ndifferential privacy and accuracy as the underlying databases grow arbitrarily\nlarge. We give both a general technique and a specific algorithm for adaptive\nanalysis of dynamically growing databases. Our general technique is illustrated\nby two algorithms that schedule black box access to some algorithm that\noperates on a fixed database to generically transform private and accurate\nalgorithms for static databases into private and accurate algorithms for\ndynamically growing databases. These results show that almost any private and\naccurate algorithm can be rerun at appropriate points of data growth with\nminimal loss of accuracy, even when data growth is unbounded. Our specific\nalgorithm directly adapts the private multiplicative weights algorithm to the\ndynamic setting, maintaining the accuracy guarantee of the static setting\nthrough unbounded data growth. Along the way, we develop extensions of several\nother differentially private algorithms to the dynamic setting, which may be of\nindependent interest for future work on the design of differentially private\nalgorithms for growing databases.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 22:05:44 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Cummings", "Rachel", ""], ["Krehbiel", "Sara", ""], ["Lai", "Kevin A.", ""], ["Tantipongpipat", "Uthaipon", ""]]}, {"id": "1803.06418", "submitter": "Andrew Nystrom", "authors": "Andrew Nystrom and John Hughes", "title": "Leveraging Sparsity to Speed Up Polynomial Feature Expansions of CSR\n  Matrices Using $K$-Simplex Numbers", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An algorithm is provided for performing polynomial feature expansions that\nboth operates on and produces compressed sparse row (CSR) matrices. Previously,\nno such algorithm existed, and performing polynomial expansions on CSR matrices\nrequired an intermediate densification step. The algorithm performs a\n$K$-degree expansion by using a bijective function involving $K$-simplex\nnumbers of column indices in the original matrix to column indices in the\nexpanded matrix. Not only is space saved by operating in CSR format, but the\nbijective function allows for only the nonzero elements to be iterated over and\nmultiplied together during the expansion, greatly improving average time\ncomplexity. For a vector of dimensionality $D$ and density $0 \\le d \\le 1$, the\nalgorithm has average time complexity $\\Theta(d^KD^K)$ where $K$ is the\npolynomial-feature order; this is an improvement by a factor $d^K$ over the\nstandard method. This work derives the required function for the cases of $K=2$\nand $K=3$ and shows its use in the $K=2$ algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 22:24:32 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 03:46:50 GMT"}, {"version": "v3", "created": "Mon, 10 Sep 2018 04:04:09 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Nystrom", "Andrew", ""], ["Hughes", "John", ""]]}, {"id": "1803.06521", "submitter": "Sitan Chen", "authors": "Sitan Chen, Ankur Moitra", "title": "Beyond the Low-Degree Algorithm: Mixtures of Subcubes and Their\n  Applications", "comments": "62 pages; to appear in STOC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of learning mixtures of $k$ subcubes over\n$\\{0,1\\}^n$, which contains many classic learning theory problems as a special\ncase (and is itself a special case of others). We give a surprising $n^{O(\\log\nk)}$-time learning algorithm based on higher-order multilinear moments. It is\nnot possible to learn the parameters because the same distribution can be\nrepresented by quite different models. Instead, we develop a framework for\nreasoning about how multilinear moments can pinpoint essential features of the\nmixture, like the number of components.\n  We also give applications of our algorithm to learning decision trees with\nstochastic transitions (which also capture interesting scenarios where the\ntransitions are deterministic but there are latent variables). Using our\nalgorithm for learning mixtures of subcubes, we can approximate the Bayes\noptimal classifier within additive error $\\epsilon$ on $k$-leaf decision trees\nwith at most $s$ stochastic transitions on any root-to-leaf path in $n^{O(s +\n\\log k)}\\cdot\\text{poly}(1/\\epsilon)$ time. In this stochastic setting, the\nclassic Occam algorithms for learning decision trees with zero stochastic\ntransitions break down, while the low-degree algorithm of Linial et al.\ninherently has a quasipolynomial dependence on $1/\\epsilon$.\n  In contrast, as we will show, mixtures of $k$ subcubes are uniquely\ndetermined by their degree $2 \\log k$ moments and hence provide a useful\nabstraction for simultaneously achieving the polynomial dependence on\n$1/\\epsilon$ of the classic Occam algorithms for decision trees and the\nflexibility of the low-degree algorithm in being able to accommodate stochastic\ntransitions. Using our multilinear moment techniques, we also give the first\nimproved upper and lower bounds since the work of Feldman et al. for the\nrelated but harder problem of learning mixtures of binary product\ndistributions.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 15:26:04 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 17:43:48 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Chen", "Sitan", ""], ["Moitra", "Ankur", ""]]}, {"id": "1803.06644", "submitter": "Haris Aziz", "authors": "Haris Aziz and Jerome Lang and Jerome Monnot", "title": "Computing and Testing Pareto Optimal Committees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting a set of alternatives based on the preferences of agents is an\nimportant problem in committee selection and beyond. Among the various criteria\nput forth for the desirability of a committee, Pareto optimality is a minimal\nand important requirement. As asking agents to specify their preferences over\nexponentially many subsets of alternatives is practically infeasible, we assume\nthat each agent specifies a weak order on single alternatives, from which a\npreference relation over subsets is derived using some preference extension. We\nconsider five prominent extensions (responsive, downward lexicographic, upward\nlexicographic, best, and worst). For each of them, we consider the\ncorresponding Pareto optimality notion, and we study the complexity of\ncomputing and verifying Pareto optimal outcomes. We also consider strategic\nissues: for four of the set extensions, we present a linear-time, Pareto\noptimal and strategyproof algorithm that even works for weak preferences.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 11:35:36 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Aziz", "Haris", ""], ["Lang", "Jerome", ""], ["Monnot", "Jerome", ""]]}, {"id": "1803.06816", "submitter": "Katsuhisa Yamanaka", "authors": "Katsuhisa Yamanaka and Takashi Horiyama and J. Mark Keil and David\n  Kirkpatrick and Yota Otachi and Toshiki Saitoh and Ryuhei Uehara and Yushi\n  Uno", "title": "Swapping Colored Tokens on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the computational complexity of the following problem. We are\ngiven a graph in which each vertex has an initial and a target color. Each pair\nof adjacent vertices can swap their current colors. Our goal is to perform the\nminimum number of swaps so that the current and target colors agree at each\nvertex. When the colors are chosen from {1,2,...,c}, we call this problem\nc-Colored Token Swapping since the current color of a vertex can be seen as a\ncolored token placed on the vertex. We show that c-Colored Token Swapping is\nNP-complete for c = 3 even if input graphs are restricted to connected planar\nbipartite graphs of maximum degree 3. We then show that 2-Colored Token\nSwapping can be solved in polynomial time for general graphs and in linear time\nfor trees. Besides, we show that, the problem for complete graphs is\nfixed-parameter tractable when parameterized by the number of colors, while it\nis known to be NP-complete when the number of colors is unbounded.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 06:53:35 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Yamanaka", "Katsuhisa", ""], ["Horiyama", "Takashi", ""], ["Keil", "J. Mark", ""], ["Kirkpatrick", "David", ""], ["Otachi", "Yota", ""], ["Saitoh", "Toshiki", ""], ["Uehara", "Ryuhei", ""], ["Uno", "Yushi", ""]]}, {"id": "1803.06858", "submitter": "Daniel Wiebking", "authors": "Martin Grohe, Daniel Neuen, Pascal Schweitzer, Daniel Wiebking", "title": "An improved isomorphism test for bounded-tree-width graphs", "comments": "34 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new fpt algorithm testing isomorphism of $n$-vertex graphs of tree\nwidth $k$ in time $2^{k\\operatorname{polylog} (k)}\\operatorname{poly} (n)$,\nimproving the fpt algorithm due to Lokshtanov, Pilipczuk, Pilipczuk, and\nSaurabh (FOCS 2014), which runs in time $2^{\\mathcal{O}(k^5\\log\nk)}\\operatorname{poly} (n)$. Based on an improved version of the\nisomorphism-invariant graph decomposition technique introduced by Lokshtanov et\nal., we prove restrictions on the structure of the automorphism groups of\ngraphs of tree width $k$. Our algorithm then makes heavy use of the group\ntheoretic techniques introduced by Luks (JCSS 1982) in his isomorphism test for\nbounded degree graphs and Babai (STOC 2016) in his quasipolynomial isomorphism\ntest. In fact, we even use Babai's algorithm as a black box in one place.\n  We also give a second algorithm which, at the price of a slightly worse\nrunning time $2^{\\mathcal{O}(k^2 \\log k)}\\operatorname{poly} (n)$, avoids the\nuse of Babai's algorithm and, more importantly, has the additional benefit that\nit can also used as a canonization algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 10:28:48 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Grohe", "Martin", ""], ["Neuen", "Daniel", ""], ["Schweitzer", "Pascal", ""], ["Wiebking", "Daniel", ""]]}, {"id": "1803.06878", "submitter": "Tom\\'a\\v{s} Masa\\v{r}\\'ik", "authors": "Du\\v{s}an Knop, Tom\\'a\\v{s} Masa\\v{r}\\'ik, Tom\\'a\\v{s} Toufar", "title": "Parameterized Complexity of Fair Vertex Evaluation Problems", "comments": "25 pages, 5 figures, presented at MFCS 2019", "journal-ref": null, "doi": "10.4230/LIPIcs.MFCS.2019.33", "report-no": null, "categories": "cs.CC cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prototypical graph problem is centered around a graph-theoretic property\nfor a set of vertices and a solution to it is a set of vertices for which the\ndesired property holds. The task is to decide whether, in the given graph,\nthere exists a solution of a certain quality, where we use size as a quality\nmeasure. In this work, we are changing the measure to the fair measure\n[Lin&Sahni: Fair edge deletion problems. IEEE Trans. Comput. 89]. The measure\nis k if the number of solution neighbors does not exceed k for any vertex in\nthe graph. One possible way to study graph problems is by defining the property\nin a certain logic. For a given objective an evaluation problem is to find a\nset (of vertices) that simultaneously minimizes the assumed measure and\nsatisfies an appropriate formula.\n  In the presented paper we show that there is an FPT algorithm for the MSO\nFair Vertex Evaluation problem for formulas with one free variable\nparameterized by the twin cover number of the input graph. Here, the free\nvariable corresponds to the solution sought. One may define an extended variant\nof MSO Fair Vertex Evaluation for formulas with l free variables; here we\nmeasure a maximum number of neighbors in each of the l sets. However, such\nvariant is W[1]-hard for parameter l even on graphs with twin cover one.\nFurthermore, we study the Fair Vertex Cover (Fair VC) problem. Fair VC is among\nthe simplest problems with respect to the demanded property (i.e., the rest\nforms an edgeless graph). On the negative side, Fair VC is W[1]-hard when\nparameterized by both treedepth and feedback vertex set of the input graph. On\nthe positive side, we provide an FPT algorithm for the parameter modular width.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 11:49:02 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 21:53:20 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 17:10:55 GMT"}, {"version": "v4", "created": "Tue, 9 Jun 2020 10:56:41 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Knop", "Du\u0161an", ""], ["Masa\u0159\u00edk", "Tom\u00e1\u0161", ""], ["Toufar", "Tom\u00e1\u0161", ""]]}, {"id": "1803.06914", "submitter": "Shariefuddin Pirzada", "authors": "Koko K. Kayibi, S. Pirzada and Carrie Rutherford", "title": "Mixing Time of Markov chain of the Knapsack Problem", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To find the number of assignments of zeros and ones satisfying a specific\nKnapsack Problem is $\\#P$ hard, so only approximations are envisageable. A\nMarkov chain allowing uniform sampling of all possible solutions is given by\nLuby, Randall and Sinclair. In 2005, Morris and Sinclair, by using a flow\nargument, have shown that the mixing time of this Markov chain is\n$\\mathcal{O}(n^{9/2+\\epsilon})$, for any $\\epsilon > 0$. By using a canonical\npath argument on the distributive lattice structure of the set of solutions, we\nobtain an improved bound, the mixing time is given as $\\tau_{_{x}}(\\epsilon)\n\\leq n^{3} \\ln (16 \\epsilon^{-1})$.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 15:14:52 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Kayibi", "Koko K.", ""], ["Pirzada", "S.", ""], ["Rutherford", "Carrie", ""]]}, {"id": "1803.06977", "submitter": "Laurent Viennot", "authors": "Siddharth Gupta (BGU), Adrian Kosowski (GANG), Laurent Viennot (GANG,\n  IRIF, LINCS)", "title": "Exploiting Hopsets: Improved Distance Oracles for Graphs of Constant\n  Highway Dimension and Beyond", "comments": null, "journal-ref": "46th International Colloquium on Automata, Languages, and\n  Programming (ICALP), Jul 2019, Patras, Greece", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For fixed $h \\geq 2$, we consider the task of adding to a graph $G$ a set of\nweighted shortcut edges on the same vertex set, such that the length of a\nshortest $h$-hop path between any pair of vertices in the augmented graph is\nexactly the same as the original distance between these vertices in $G$. A set\nof shortcut edges with this property is called an exact $h$-hopset and may be\napplied in processing distance queries on graph $G$. In particular, a\n$2$-hopset directly corresponds to a distributed distance oracle known as a hub\nlabeling. In this work, we explore centralized distance oracles based on\n$3$-hopsets and display their advantages in several practical scenarios. In\nparticular, for graphs of constant highway dimension, and more generally for\ngraphs of constant skeleton dimension, we show that $3$-hopsets require\nexponentially fewer shortcuts per node than any previously described distance\noracle while incurring only a quadratic increase in the query decoding time,\nand actually offer a speedup when compared to simple oracles based on a direct\napplication of $2$-hopsets. Finally, we consider the problem of computing\nminimum-size $h$-hopset (for any $h \\geq 2$) for a given graph $G$, showing a\npolylogarithmic-factor approximation for the case of unique shortest path\ngraphs. When $h=3$, for a given bound on the space used by the distance oracle,\nwe provide a construction of hopsets achieving polylog approximation both for\nspace and query time compared to the optimal $3$-hopset oracle given the space\nbound.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 15:06:49 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 09:16:43 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Gupta", "Siddharth", "", "BGU"], ["Kosowski", "Adrian", "", "GANG"], ["Viennot", "Laurent", "", "GANG,\n  IRIF, LINCS"]]}, {"id": "1803.07097", "submitter": "Ryo Ashida", "authors": "Ryo Ashida and Kotaro Nakagawa", "title": "$\\tilde{O}(n^{1/3})$-Space Algorithm for the Grid Graph Reachability\n  Problem", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.SoCG.2018.5", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The directed graph reachability problem takes as input an $n$-vertex directed\ngraph $G=(V,E)$, and two distinguished vertices $s$ and $t$. The problem is to\ndetermine whether there exists a path from $s$ to $t$ in $G$. This is a\ncanonical complete problem for class NL. Asano et al. proposed an\n$\\tilde{O}(\\sqrt{n})$ space and polynomial time algorithm for the directed grid\nand planar graph reachability problem. The main result of this paper is to show\nthat the directed graph reachability problem restricted to grid graphs can be\nsolved in polynomial time using only $\\tilde{O}(n^{1/3})$ space.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 18:06:52 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 06:17:14 GMT"}, {"version": "v3", "created": "Fri, 20 Sep 2019 07:25:38 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Ashida", "Ryo", ""], ["Nakagawa", "Kotaro", ""]]}, {"id": "1803.07199", "submitter": "Ali Dasdan", "authors": "Ali Dasdan", "title": "Twelve Simple Algorithms to Compute Fibonacci Numbers", "comments": "33 pages, 29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fibonacci numbers are a sequence of integers in which every number after\nthe first two, 0 and 1, is the sum of the two preceding numbers. These numbers\nare well known and algorithms to compute them are so easy that they are often\nused in introductory algorithms courses. In this paper, we present twelve of\nthese well-known algorithms and some of their properties. These algorithms,\nthough very simple, illustrate multiple concepts from the algorithms field, so\nwe highlight them. We also present the results of a small-scale experimental\ncomparison of their runtimes on a personal laptop. Finally, we provide a list\nof homework questions for the students. We hope that this paper can serve as a\nuseful resource for the students learning the basics of algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 00:04:27 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 16:11:40 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Dasdan", "Ali", ""]]}, {"id": "1803.07226", "submitter": "Jinshi Yu", "authors": "Jinshi Yu, Guoxu Zhou, Andrzej Cichocki, Shengli Xie", "title": "Learning the Hierarchical Parts of Objects by Deep Non-Smooth\n  Nonnegative Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonsmooth Nonnegative Matrix Factorization (nsNMF) is capable of producing\nmore localized, less overlapped feature representations than other variants of\nNMF while keeping satisfactory fit to data. However, nsNMF as well as other\nexisting NMF methods is incompetent to learn hierarchical features of complex\ndata due to its shallow structure. To fill this gap, we propose a deep nsNMF\nmethod coined by the fact that it possesses a deeper architecture compared with\nstandard nsNMF. The deep nsNMF not only gives parts-based features due to the\nnonnegativity constraints, but also creates higher-level, more abstract\nfeatures by combing lower-level ones. The in-depth description of how deep\narchitecture can help to efficiently discover abstract features in dnsNMF is\npresented. And we also show that the deep nsNMF has close relationship with the\ndeep autoencoder, suggesting that the proposed model inherits the major\nadvantages from both deep learning and NMF. Extensive experiments demonstrate\nthe standout performance of the proposed method in clustering analysis.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 02:39:44 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Yu", "Jinshi", ""], ["Zhou", "Guoxu", ""], ["Cichocki", "Andrzej", ""], ["Xie", "Shengli", ""]]}, {"id": "1803.07639", "submitter": "Srinivasan Parthasarathy", "authors": "Srinivasan Parthasarathy", "title": "Adaptive Greedy Algorithms for Stochastic Set Cover Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study adaptive greedy algorithms for the problems of stochastic set cover\nwith perfect and imperfect coverages. In stochastic set cover with perfect\ncoverage, we are given a set of items and a ground set B. Evaluating an item\nreveals its state which is a random subset of B drawn from the state\ndistribution of the item. Every element in B is assumed to be present in the\nstate of some item with probability 1. For this problem, we show that the\nadaptive greedy algorithm has an approximation ratio of H(|B|), the |B|th\nHarmonic number. In stochastic set cover with imperfect coverage, an element in\nthe ground set need not be present in the state of any item. We show a\nreduction from this problem to the former problem; the adaptive greedy\nalgorithm for the reduced instance has an approxiation ratio of H(|E|), where E\nis the set of pairs (F, e) such that the state of item F contains e with\npositive probability.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 20:30:55 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 11:55:59 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 04:47:14 GMT"}, {"version": "v4", "created": "Thu, 29 Mar 2018 12:17:17 GMT"}, {"version": "v5", "created": "Fri, 6 Apr 2018 18:41:50 GMT"}, {"version": "v6", "created": "Thu, 14 Jun 2018 15:22:31 GMT"}, {"version": "v7", "created": "Fri, 15 Jun 2018 19:39:55 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Parthasarathy", "Srinivasan", ""]]}, {"id": "1803.07813", "submitter": "Krzysztof Domino", "authors": "Krzysztof Domino, Adam Glos", "title": "Introducing higher order correlations to marginals' subset of\n  multivariate data by means of Archimedean copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the algorithm that alters the subset of marginals\nof multivariate standard distributed data into such modelled by an Archimedean\ncopula. Proposed algorithm leaves a correlation matrix almost unchanged, but\nintroduces a higher order correlation into a subset of marginals. Our data\ntransformation algorithm can be used to analyse whether particular machine\nlearning algorithm, especially a dimensionality reduction one, utilises higher\norder correlations or not. We present an exemplary application on two features\nselection algorithms, mention that features selection is one of the approaches\nto dimensionality reduction. To measure higher order correlation, we use\nmultivariate higher order cumulants, hence to utilises higher order\ncorrelations be to use the Joint Skewness Band Selection (JSBS) algorithm that\nuses third-order multivariate cumulant. We show the robust performance of the\nJSBS in contrary to the poor performance of the Maximum Ellipsoid Volume (MEV)\nalgorithm that does not utilise such higher order correlations. With this\nresult, we confirm the potential application of our data generation algorithm\nto analyse a performance of various dimensionality reduction algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 09:42:39 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 10:44:30 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Domino", "Krzysztof", ""], ["Glos", "Adam", ""]]}, {"id": "1803.08037", "submitter": "Pedro Felzenszwalb", "authors": "Pedro F. Felzenszwalb", "title": "Similar Elements and Metric Labeling on Complete Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem that involves finding similar elements in a collection\nof sets. The problem is motivated by applications in machine learning and\npattern recognition. We formulate the similar elements problem as an\noptimization and give an efficient approximation algorithm that finds a\nsolution within a factor of 2 of the optimal. The similar elements problem is a\nspecial case of the metric labeling problem and we also give an efficient\n2-approximation algorithm for the metric labeling problem on complete graphs.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 17:55:02 GMT"}, {"version": "v2", "created": "Sat, 24 Mar 2018 13:38:50 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Felzenszwalb", "Pedro F.", ""]]}, {"id": "1803.08273", "submitter": "Chunhao Wang", "authors": "Chunhao Wang and Leonard Wossnig", "title": "A quantum algorithm for simulating non-sparse Hamiltonians", "comments": "19 pages, 1 figure, presentation improved", "journal-ref": "Quantum Information and Computation, Vol. 20, No. 7 & 8 (2020)\n  597-615", "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a quantum algorithm for simulating the dynamics of Hamiltonians\nthat are not necessarily sparse. Our algorithm is based on the input model\nwhere the entries of the Hamiltonian are stored in a data structure in a\nquantum random access memory (qRAM) which allows for the efficient preparation\nof states that encode the rows of the Hamiltonian. We use a linear combination\nof quantum walks to achieve poly-logarithmic dependence on precision. The time\ncomplexity of our algorithm, measured in terms of the circuit depth, is\n$O(t\\sqrt{N}\\|H\\|\\,\\mathrm{polylog}(N, t\\|H\\|, 1/\\epsilon))$, where $t$ is the\nevolution time, $N$ is the dimension of the system, and $\\epsilon$ is the error\nin the final state, which we call precision. Our algorithm can be directly\napplied as a subroutine for unitary implementation and quantum linear systems\nsolvers, achieving $\\widetilde{O}(\\sqrt{N})$ dependence for both applications.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 09:16:02 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 22:16:53 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Wang", "Chunhao", ""], ["Wossnig", "Leonard", ""]]}, {"id": "1803.08617", "submitter": "Yihan Sun", "authors": "Naama Ben-David and Guy E. Blelloch and Yihan Sun and Yuanhao Wei", "title": "Multiversion Concurrency with Bounded Delay and Precise Garbage\n  Collection", "comments": null, "journal-ref": null, "doi": "10.1145/3323165.3323185", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are interested in bounding the number of instructions taken\nto process transactions. The main result is a multiversion transactional system\nthat supports constant delay (extra instructions beyond running in isolation)\nfor all read-only transactions, delay equal to the number of processes for\nwriting transactions that are not concurrent with other writers, and\nlock-freedom for concurrent writers. The system supports precise garbage\ncollection in that versions are identified for collection as soon as the last\ntransaction releases them. As far as we know these are first results that bound\ndelays for multiple readers and even a single writer. The approach is\nparticularly useful in situations where read-transactions dominate write\ntransactions, or where write transactions come in as streams or batches and can\nbe processed by a single writer (possibly in parallel).\n  The approach is based on using functional data structures to support multiple\nversions, and an efficient solution to the Version Maintenance (VM) problem for\nacquiring, updating and releasing versions. Our solution to the VM problem is\nprecise, safe and wait-free (PSWF).\n  We experimentally validate our approach by applying it to balanced tree data\nstructures for maintaining ordered maps. We test the transactional system using\nmultiple algorithms for the VM problem, including our PSWF VM algorithm, and\nimplementations with weaker guarantees based on epochs, hazard pointers, and\nread-copy-update. To evaluate the functional data structure for concurrency and\nmulti-versioning, we implement batched updates for functional tree structures\nand compare the performance with state-of-the-art concurrent data structures\nfor balanced trees. The experiments indicate our approach works well in\npractice over a broad set of criteria.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 00:22:27 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 04:34:03 GMT"}, {"version": "v3", "created": "Wed, 15 May 2019 22:35:09 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Ben-David", "Naama", ""], ["Blelloch", "Guy E.", ""], ["Sun", "Yihan", ""], ["Wei", "Yuanhao", ""]]}, {"id": "1803.08621", "submitter": "Yihan Sun", "authors": "Yihan Sun and Guy E. Blelloch", "title": "Parallel Range, Segment and Rectangle Queries with Augmented Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The range, segment and rectangle query problems are fundamental problems in\ncomputational geometry, and have extensive applications in many domains.\nDespite the significant theoretical work on these problems, efficient\nimplementations can be complicated. We know of very few practical\nimplementations of the algorithms in parallel, and most implementations do not\nhave tight theoretical bounds. We focus on simple and efficient parallel\nalgorithms and implementations for these queries, which have tight worst-case\nbound in theory and good parallel performance in practice. We propose to use a\nsimple framework (the augmented map) to model the problem. Based on the\naugmented map interface, we develop both multi-level tree structures and\nsweepline algorithms supporting range, segment and rectangle queries in two\ndimensions. For the sweepline algorithms, we propose a parallel paradigm and\nshow corresponding cost bounds. All of our data structures are work-efficient\nto build in theory and achieve a low parallel depth. The query time is almost\nlinear to the output size.\n  We have implemented all the data structures described in the paper using a\nparallel augmented map library. Based on the library each data structure only\nrequires about 100 lines of C++ code. We test their performance on large data\nsets (up to $10^8$ elements) and a machine with 72-cores (144 hyperthreads).\nThe parallel construction achieves 32-68x speedup. Speedup numbers on queries\nare up to 126-fold. Our sequential implementation outperforms the CGAL library\nby at least 2x in both construction and queries. Our sequential implementation\ncan be slightly slower than the R-tree in the Boost library in some cases\n(0.6-2.5x), but has significantly better query performance (1.6-1400x) than\nBoost.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 00:48:06 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 20:56:08 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Sun", "Yihan", ""], ["Blelloch", "Guy E.", ""]]}, {"id": "1803.08700", "submitter": "Nicolas Tremblay", "authors": "Nicolas Tremblay, Simon Barthelm\\'e, Pierre-Olivier Amblard", "title": "Determinantal Point Processes for Coresets", "comments": null, "journal-ref": "Journal of Machine Learning Research 20 (2019) 1-70", "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When faced with a data set too large to be processed all at once, an obvious\nsolution is to retain only part of it. In practice this takes a wide variety of\ndifferent forms, and among them \"coresets\" are especially appealing. A coreset\nis a (small) weighted sample of the original data that comes with the following\nguarantee: a cost function can be evaluated on the smaller set instead of the\nlarger one, with low relative error. For some classes of problems, and via a\ncareful choice of sampling distribution (based on the so-called \"sensitivity\"\nmetric), iid random sampling has turned to be one of the most successful\nmethods for building coresets efficiently. However, independent samples are\nsometimes overly redundant, and one could hope that enforcing diversity would\nlead to better performance. The difficulty lies in proving coreset properties\nin non-iid samples. We show that the coreset property holds for samples formed\nwith determinantal point processes (DPP). DPPs are interesting because they are\na rare example of repulsive point processes with tractable theoretical\nproperties, enabling us to prove general coreset theorems. We apply our results\nto both the k-means and the linear regression problems, and give extensive\nempirical evidence that the small additional computational cost of DPP sampling\ncomes with superior performance over its iid counterpart. Of independent\ninterest, we also provide analytical formulas for the sensitivity in the linear\nregression and 1-means cases.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 09:17:48 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 15:58:34 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 08:18:36 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Tremblay", "Nicolas", ""], ["Barthelm\u00e9", "Simon", ""], ["Amblard", "Pierre-Olivier", ""]]}, {"id": "1803.08777", "submitter": "Rajesh Jayaram", "authors": "Rajesh Jayaram, David P. Woodruff", "title": "Data Streams with Bounded Deletions", "comments": "To appear, PODS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two prevalent models in the data stream literature are the insertion-only and\nturnstile models. Unfortunately, many important streaming problems require a\n$\\Theta(\\log(n))$ multiplicative factor more space for turnstile streams than\nfor insertion-only streams. This complexity gap often arises because the\nunderlying frequency vector $f$ is very close to $0$, after accounting for all\ninsertions and deletions to items. Signal detection in such streams is\ndifficult, given the large number of deletions.\n  In this work, we propose an intermediate model which, given a parameter\n$\\alpha \\geq 1$, lower bounds the norm $\\|f\\|_p$ by a $1/\\alpha$-fraction of\nthe $L_p$ mass of the stream had all updates been positive. Here, for a vector\n$f$, $\\|f\\|_p = \\left (\\sum_{i=1}^n |f_i|^p \\right )^{1/p}$, and the value of\n$p$ we choose depends on the application. This gives a fluid medium between\ninsertion only streams (with $\\alpha = 1$), and turnstile streams (with $\\alpha\n= \\text{poly}(n)$), and allows for analysis in terms of $\\alpha$.\n  We show that for streams with this $\\alpha$-property, for many fundamental\nstreaming problems we can replace a $O(\\log(n))$ factor in the space usage for\nalgorithms in the turnstile model with a $O(\\log(\\alpha))$ factor. This is true\nfor identifying heavy hitters, inner product estimation, $L_0$ estimation,\n$L_1$ estimation, $L_1$ sampling, and support sampling. For each problem, we\ngive matching or nearly matching lower bounds for $\\alpha$-property streams. We\nnote that in practice, many important turnstile data streams are in fact\n$\\alpha$-property streams for small values of $\\alpha$. For such applications,\nour results represent significant improvements in efficiency for all the\naforementioned problems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 13:03:24 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Jayaram", "Rajesh", ""], ["Woodruff", "David P.", ""]]}, {"id": "1803.08917", "submitter": "Zeyuan Allen-Zhu", "authors": "Dan Alistarh, Zeyuan Allen-Zhu, Jerry Li", "title": "Byzantine Stochastic Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of distributed stochastic optimization in an\nadversarial setting where, out of the $m$ machines which allegedly compute\nstochastic gradients every iteration, an $\\alpha$-fraction are Byzantine, and\ncan behave arbitrarily and adversarially. Our main result is a variant of\nstochastic gradient descent (SGD) which finds $\\varepsilon$-approximate\nminimizers of convex functions in $T = \\tilde{O}\\big( \\frac{1}{\\varepsilon^2 m}\n+ \\frac{\\alpha^2}{\\varepsilon^2} \\big)$ iterations. In contrast, traditional\nmini-batch SGD needs $T = O\\big( \\frac{1}{\\varepsilon^2 m} \\big)$ iterations,\nbut cannot tolerate Byzantine failures. Further, we provide a lower bound\nshowing that, up to logarithmic factors, our algorithm is\ninformation-theoretically optimal both in terms of sampling complexity and time\ncomplexity.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 17:58:54 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Alistarh", "Dan", ""], ["Allen-Zhu", "Zeyuan", ""], ["Li", "Jerry", ""]]}, {"id": "1803.09040", "submitter": "Liwei Zeng", "authors": "Liwei Zeng, Sunil Chopra, Karen Smilowitz", "title": "A Bounded Formulation for The School Bus Scheduling Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new formulation for the school bus scheduling problem\n(SBSP) which optimizes school start times and bus operation times to minimize\ntransportation cost. Our goal is to minimize the number of buses to serve all\nbus routes such that each route arrives in a time window before school starts.\nWe present a new time-indexed integer linear programming (ILP) formulation for\nthis problem. Based on a strengthened version of the linear relaxation of the\nILP, we develop a dependent randomized rounding algorithm that yields\nnear-optimal solutions for large-scale problem instances. We also generalize\nour methodologies to solve a robust version of the SBSP.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 02:39:10 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 00:59:36 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Zeng", "Liwei", ""], ["Chopra", "Sunil", ""], ["Smilowitz", "Karen", ""]]}, {"id": "1803.09289", "submitter": "Mordecai Golin", "authors": "Di Chen and Mordecai J. Golin", "title": "Minmax Centered k-Partitioning of Trees and Applications to Sink\n  Evacuation with Dynamic Confluent Flows", "comments": "Substantially Revised and expanded version of arXiv:1607.08041", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $T=(V,E)$ be a tree with associated costs on its subtrees. A minmax\n$k$-partition of $T$ is a partition into $k$ subtrees, minimizing the maximum\ncost of a subtree over all possible partitions. In the centered version of the\nproblem, the cost of a subtree cost is defined as the minimum cost of\n\"servicing\" that subtree using a center located within it.\n  The problem motivating this work was the sink-evacuation problem on trees,\ni.e., finding a collection of $k$-sinks that minimize the time required by a\nconfluent dynamic network flow to evacuate all supplies to sinks.\n  This paper provides the first polynomial-time algorithm for solving this\nproblem, running in $O\\Bigl(\\max(k,\\log n) k n \\log^4 n\\Bigr)$ time. The\ntechnique developed can be used to solve any Minmax Centered $k$-Partitioning\nproblem on trees in which the servicing costs satisfy some very general\nconditions. Solutions can be found for both the discrete case, in which centers\nmust be on vertices, and the continuous case, in which centers may also be\nplaced on edges. The technique developed also improves previous results for\nfinding a minmax cost $k$-partition of a tree given the location of the sinks\nin advance.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 16:27:39 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Chen", "Di", ""], ["Golin", "Mordecai J.", ""]]}, {"id": "1803.09353", "submitter": "Thodoris Lykouris", "authors": "Thodoris Lykouris, Vahab Mirrokni, Renato Paes Leme", "title": "Stochastic bandits robust to adversarial corruptions", "comments": "To appear in STOC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new model of stochastic bandits with adversarial corruptions\nwhich aims to capture settings where most of the input follows a stochastic\npattern but some fraction of it can be adversarially changed to trick the\nalgorithm, e.g., click fraud, fake reviews and email spam. The goal of this\nmodel is to encourage the design of bandit algorithms that (i) work well in\nmixed adversarial and stochastic models, and (ii) whose performance\ndeteriorates gracefully as we move from fully stochastic to fully adversarial\nmodels.\n  In our model, the rewards for all arms are initially drawn from a\ndistribution and are then altered by an adaptive adversary. We provide a simple\nalgorithm whose performance gracefully degrades with the total corruption the\nadversary injected in the data, measured by the sum across rounds of the\nbiggest alteration the adversary made in the data in that round; this total\ncorruption is denoted by $C$. Our algorithm provides a guarantee that retains\nthe optimal guarantee (up to a logarithmic term) if the input is stochastic and\nwhose performance degrades linearly to the amount of corruption $C$, while\ncrucially being agnostic to it. We also provide a lower bound showing that this\nlinear degradation is necessary if the algorithm achieves optimal performance\nin the stochastic setting (the lower bound works even for a known amount of\ncorruption, a special case in which our algorithm achieves optimal performance\nwithout the extra logarithm).\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 21:48:53 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Lykouris", "Thodoris", ""], ["Mirrokni", "Vahab", ""], ["Leme", "Renato Paes", ""]]}, {"id": "1803.09370", "submitter": "Saket Saurabh", "authors": "Sushmita Gupta, Pranabendu Misra, Saket Saurabh and Meirav Zehavi", "title": "Popular Matching in Roommates Setting is NP-hard", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An input to the Popular Matching problem, in the roommates setting, consists\nof a graph $G$ and each vertex ranks its neighbors in strict order, known as\nits preference. In the Popular Matching problem the objective is to test\nwhether there exists a matching $M^\\star$ such that there is no matching $M$\nwhere more people are happier with $M$ than with $M^\\star$. In this paper we\nsettle the computational complexity of the Popular Matching problem in the\nroommates setting by showing that the problem is NP-complete. Thus, we resolve\nan open question that has been repeatedly, explicitly asked over the last\ndecade.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 23:53:25 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Gupta", "Sushmita", ""], ["Misra", "Pranabendu", ""], ["Saurabh", "Saket", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1803.09435", "submitter": "Suthee Ruangwises", "authors": "Suthee Ruangwises, Toshiya Itoh", "title": "Unpopularity Factor in the Marriage and Roommates Problems", "comments": "A preliminary version of this paper has appeared at CSR 2019", "journal-ref": "Theory of Computing Systems, 65(3): 579-592 (2021)", "doi": "10.1007/s00224-020-09978-5", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $A$ of $n$ people, with each person having a preference list that\nranks a subset of $A$ as his/her acceptable partners in order of preference, we\nconsider the Roommates Problem (RP) and the Marriage Problem (MP) of matching\npeople with their partners. In RP there is no further restriction, while in MP\nonly people of opposite genders can be acceptable partners. For a pair of\nmatchings $X$ and $Y$, let $\\phi(X,Y)$ denote the number of people who prefer a\nperson they get matched by $X$ to a person they get matched by $Y$, and define\nan unpopularity factor $u(M)$ of a matching $M$ to be the maximum ratio\n$\\phi(M',M) / \\phi(M,M')$ among all other possible matchings $M'$. In this\npaper, we develop an algorithm to compute the unpopularity factor of a given\nmatching in $O(m\\sqrt{n}\\log^2 n)$ time for RP and in $O(m\\sqrt{n}\\log n)$ time\nfor MP, where $m$ is the total length of people's preference lists. We also\ngeneralize the notion of unpopularity factor to a weighted setting where people\nare given different voting weights and show that our algorithm can be slightly\nmodified to support that setting with the same running time.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 06:41:07 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 15:14:44 GMT"}, {"version": "v3", "created": "Sat, 16 Mar 2019 06:09:25 GMT"}, {"version": "v4", "created": "Tue, 30 Jul 2019 09:26:23 GMT"}, {"version": "v5", "created": "Fri, 25 Oct 2019 08:54:06 GMT"}, {"version": "v6", "created": "Tue, 3 Nov 2020 11:35:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ruangwises", "Suthee", ""], ["Itoh", "Toshiya", ""]]}, {"id": "1803.09483", "submitter": "Dimitrios Thilikos", "authors": "Petr A. Golovach and Dimitrios M. Thilikos", "title": "Clustering to Given Connectivities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a general variant of the graph clustering problem where the\ncriterion of density for the clusters is (high) connectivity. In {\\sc\nClustering to Given Connectivities}, we are given an $n$-vertex graph $G$, an\ninteger $k$, and a sequence $\\Lambda=\\langle\n\\lambda_{1},\\ldots,\\lambda_{t}\\rangle$ of positive integers and we ask whether\nit is possible to remove at most $k$ edges from $G$ such that the resulting\nconnected components are {\\sl exactly} $t$ and their corresponding edge\nconnectivities are lower-bounded by the numbers in $\\Lambda$. We prove that\nthis problem, parameterized by $k$, is fixed parameter tractable i.e., can be\nsolved by an $f(k)\\cdot n^{O(1)}$-step algorithm, for some function $f$ that\ndepends only on the parameter $k$. Our algorithm uses the recursive\nunderstanding technique that is especially adapted so to deal with the fact\nthat, in out setting, we do not impose any restriction to the connectivity\ndemands in $\\Lambda$.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 09:38:02 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 07:17:25 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Golovach", "Petr A.", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1803.09517", "submitter": "Nicola Prezza", "authors": "Gonzalo Navarro, Carlos Ochoa, Nicola Prezza", "title": "On the Approximation Ratio of Ordered Parsings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shannon's entropy is a clear lower bound for statistical compression. The\nsituation is not so well understood for dictionary-based compression. A\nplausible lower bound is $b$, the least number of phrases of a general\nbidirectional parse of a text, where phrases can be copied from anywhere else\nin the text. Since computing $b$ is NP-complete, a popular gold standard is\n$z$, the number of phrases in the Lempel-Ziv parse of the text, which is the\noptimal one when phrases can be copied only from the left. While $z$ can be\ncomputed in linear time with a greedy algorithm, almost nothing has been known\nfor decades about its approximation ratio with respect to $b$. In this paper we\nprove that $z=O(b\\log(n/b))$, where $n$ is the text length. We also show that\nthe bound is tight as a function of $n$, by exhibiting a text family where $z =\n\\Omega(b\\log n)$. Our upper bound is obtained by building a run-length\ncontext-free grammar based on a locally consistent parsing of the text. Our\nlower bound is obtained by relating $b$ with $r$, the number of equal-letter\nruns in the Burrows-Wheeler transform of the text. We proceed by observing that\nLempel-Ziv is just one particular case of greedy parses, meaning that the\noptimal value of $z$ is obtained by scanning the text and maximizing the phrase\nlength at each step, and of ordered parses, meaning that there is an increasing\norder between phrases and their sources. As a new example of ordered greedy\nparses, we introduce {\\em lexicographical} parses, where phrases can only be\ncopied from lexicographically smaller text locations. We prove that the size\n$v$ of the optimal lexicographical parse is also obtained greedily in $O(n)$\ntime, that $v=O(b\\log(n/b))$, and that there exists a text family where $v =\n\\Omega(b\\log n)$.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 11:34:13 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 19:03:40 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Navarro", "Gonzalo", ""], ["Ochoa", "Carlos", ""], ["Prezza", "Nicola", ""]]}, {"id": "1803.09520", "submitter": "Nicola Prezza", "authors": "Gonzalo Navarro and Nicola Prezza", "title": "Universal Compressed Text Indexing", "comments": "Fixed with reviewer's comments", "journal-ref": null, "doi": "10.1016/j.tcs.2018.09.007", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of repetitive datasets has lately generated a lot of interest in\ncompressed self-indexes based on dictionary compression, a rich and\nheterogeneous family that exploits text repetitions in different ways. For each\nsuch compression scheme, several different indexing solutions have been\nproposed in the last two decades. To date, the fastest indexes for repetitive\ntexts are based on the run-length compressed Burrows-Wheeler transform and on\nthe Compact Directed Acyclic Word Graph. The most space-efficient indexes, on\nthe other hand, are based on the Lempel-Ziv parsing and on grammar compression.\nIndexes for more universal schemes such as collage systems and macro schemes\nhave not yet been proposed. Very recently, Kempa and Prezza [STOC 2018] showed\nthat all dictionary compressors can be interpreted as approximation algorithms\nfor the smallest string attractor, that is, a set of text positions capturing\nall distinct substrings. Starting from this observation, in this paper we\ndevelop the first universal compressed self-index, that is, the first indexing\ndata structure based on string attractors, which can therefore be built on top\nof any dictionary-compressed text representation. Let $\\gamma$ be the size of a\nstring attractor for a text of length $n$. Our index takes\n$O(\\gamma\\log(n/\\gamma))$ words of space and supports locating the $occ$\noccurrences of any pattern of length $m$ in $O(m\\log n + occ\\log^{\\epsilon}n)$\ntime, for any constant $\\epsilon>0$. This is, in particular, the first index\nfor general macro schemes and collage systems. Our result shows that the\nrelation between indexing and compression is much deeper than what was\npreviously thought: the simple property standing at the core of all dictionary\ncompressors is sufficient to support fast indexed queries.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 11:39:19 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 10:55:27 GMT"}, {"version": "v3", "created": "Thu, 6 Sep 2018 08:08:17 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Navarro", "Gonzalo", ""], ["Prezza", "Nicola", ""]]}, {"id": "1803.09675", "submitter": "Andrej Sajenko", "authors": "Frank Kammer, Andrej Sajenko", "title": "Extra Space during Initialization of Succinct Data Structures and\n  Dynamical Initializable Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many succinct data structures on the word RAM require precomputed tables to\nstart operating. Usually, the tables can be constructed in sublinear time. In\nthis time, most of a data structure is not initialized, i.e., there is plenty\nof unused space allocated for the data structure. We present a general\nframework to store temporarily extra buffers between the real data so that the\ndata can be processed immediately, stored first in the buffers, and then moved\ninto the real data structure after finishing the tables. As an application, we\napply our framework to Dodis, Patrascu, and Thorup's data structure (STOC 2010)\nthat emulates c-ary memory and to Farzan and Munro's succinct encoding of\narbitrary graphs (TCS 2013). We also use our framework to present an in-place\ndynamical initializable array.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 15:43:50 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 14:15:12 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Kammer", "Frank", ""], ["Sajenko", "Andrej", ""]]}, {"id": "1803.09785", "submitter": "Daniel Karapetyan Dr", "authors": "Daniel Karapetyan and Andrew J. Parkes and Thomas St\\\"utzle", "title": "Algorithm Configuration: Learning policies for the quick termination of\n  poor performers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way to speed up the algorithm configuration task is to use short runs\ninstead of long runs as much as possible, but without discarding the\nconfigurations that eventually do well on the long runs. We consider the\nproblem of selecting the top performing configurations of the Conditional\nMarkov Chain Search (CMCS), a general algorithm schema that includes, for\nexamples, VNS. We investigate how the structure of performance on short tests\nlinks with those on long tests, showing that significant differences arise\nbetween test domains. We propose a \"performance envelope\" method to exploit the\nlinks; that learns when runs should be terminated, but that automatically\nadapts to the domain.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 18:38:35 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Karapetyan", "Daniel", ""], ["Parkes", "Andrew J.", ""], ["St\u00fctzle", "Thomas", ""]]}, {"id": "1803.09930", "submitter": "Hung Ngo", "authors": "Hung Q. Ngo", "title": "Worst-Case Optimal Join Algorithms: Techniques, Results, and Open\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Worst-case optimal join algorithms are the class of join algorithms whose\nruntime match the worst-case output size of a given join query. While the first\nprovably worst-case optimal join algorithm was discovered relatively recently,\nthe techniques and results surrounding these algorithms grow out of decades of\nresearch from a wide range of areas, intimately connecting graph theory,\nalgorithms, information theory, constraint satisfaction, database theory, and\ngeometric inequalities. These ideas are not just paperware: in addition to\nacademic project implementations, two variations of such algorithms are the\nwork-horse join algorithms of commercial database and data analytics engines.\n  This paper aims to be a brief introduction to the design and analysis of\nworst-case optimal join algorithms. We discuss the key techniques for proving\nruntime and output size bounds. We particularly focus on the fascinating\nconnection between join algorithms and information theoretic inequalities, and\nthe idea of how one can turn a proof into an algorithm. Finally, we conclude\nwith a representative list of fundamental open problems in this area.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 07:13:49 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 21:05:41 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 04:17:42 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Ngo", "Hung Q.", ""]]}, {"id": "1803.09952", "submitter": "Aris Pagourtzis", "authors": "Nikolaos Melissinos and Aris Pagourtzis", "title": "A Faster FPTAS for the Subset-Sums Ratio Problem", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-94776-1_50", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Subset-Sums Ratio problem (SSR) is an optimization problem in which,\ngiven a set of integers, the goal is to find two subsets such that the ratio of\ntheir sums is as close to 1 as possible. In this paper we develop a new FPTAS\nfor the SSR problem which builds on techniques proposed in [D. Nanongkai,\nSimple FPTAS for the subset-sums ratio problem, Inf. Proc. Lett. 113 (2013)].\nOne of the key improvements of our scheme is the use of a dynamic programming\ntable in which one dimension represents the difference of the sums of the two\nsubsets. This idea, together with a careful choice of a scaling parameter,\nyields an FPTAS that is several orders of magnitude faster than the best\ncurrently known scheme of [C. Bazgan, M. Santha, Z. Tuza, Efficient\napproximation algorithms for the Subset-Sums Equality problem, J. Comp. System\nSci. 64 (2) (2002)].\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 08:24:48 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Melissinos", "Nikolaos", ""], ["Pagourtzis", "Aris", ""]]}, {"id": "1803.10150", "submitter": "Ellen Vitercik", "authors": "Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik", "title": "Learning to Branch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree search algorithms, such as branch-and-bound, are the most widely used\ntools for solving combinatorial and nonconvex problems. For example, they are\nthe foremost method for solving (mixed) integer programs and constraint\nsatisfaction problems. Tree search algorithms recursively partition the search\nspace to find an optimal solution. In order to keep the tree size small, it is\ncrucial to carefully decide, when expanding a tree node, which question\n(typically variable) to branch on at that node in order to partition the\nremaining space. Numerous partitioning techniques (e.g., variable selection)\nhave been proposed, but there is no theory describing which technique is\noptimal. We show how to use machine learning to determine an optimal weighting\nof any set of partitioning procedures for the instance distribution at hand\nusing samples from the distribution. We provide the first sample complexity\nguarantees for tree search algorithm configuration. These guarantees bound the\nnumber of samples sufficient to ensure that the empirical performance of an\nalgorithm over the samples nearly matches its expected performance on the\nunknown instance distribution. This thorough theoretical investigation\nnaturally gives rise to our learning algorithm. Via experiments, we show that\nlearning an optimal weighting of partitioning procedures can dramatically\nreduce tree size, and we prove that this reduction can even be exponential.\nThrough theory and experiments, we show that learning to branch is both\npractical and hugely beneficial.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 15:47:24 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 20:53:11 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Dick", "Travis", ""], ["Sandholm", "Tuomas", ""], ["Vitercik", "Ellen", ""]]}, {"id": "1803.10172", "submitter": "Daniele Calandriello", "authors": "Daniele Calandriello, Alessandro Lazaric and Michal Valko", "title": "Distributed Adaptive Sampling for Kernel Matrix Approximation", "comments": "Presented at AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most kernel-based methods, such as kernel or Gaussian process regression,\nkernel PCA, ICA, or $k$-means clustering, do not scale to large datasets,\nbecause constructing and storing the kernel matrix $\\mathbf{K}_n$ requires at\nleast $\\mathcal{O}(n^2)$ time and space for $n$ samples. Recent works show that\nsampling points with replacement according to their ridge leverage scores (RLS)\ngenerates small dictionaries of relevant points with strong spectral\napproximation guarantees for $\\mathbf{K}_n$. The drawback of RLS-based methods\nis that computing exact RLS requires constructing and storing the whole kernel\nmatrix. In this paper, we introduce SQUEAK, a new algorithm for kernel\napproximation based on RLS sampling that sequentially processes the dataset,\nstoring a dictionary which creates accurate kernel matrix approximations with a\nnumber of points that only depends on the effective dimension $d_{eff}(\\gamma)$\nof the dataset. Moreover since all the RLS estimations are efficiently\nperformed using only the small dictionary, SQUEAK is the first RLS sampling\nalgorithm that never constructs the whole matrix $\\mathbf{K}_n$, runs in linear\ntime $\\widetilde{\\mathcal{O}}(nd_{eff}(\\gamma)^3)$ w.r.t. $n$, and requires\nonly a single pass over the dataset. We also propose a parallel and distributed\nversion of SQUEAK that linearly scales across multiple machines, achieving\nsimilar accuracy in as little as\n$\\widetilde{\\mathcal{O}}(\\log(n)d_{eff}(\\gamma)^3)$ time.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 16:39:00 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Calandriello", "Daniele", ""], ["Lazaric", "Alessandro", ""], ["Valko", "Michal", ""]]}, {"id": "1803.10184", "submitter": "Hamid Hoorfar", "authors": "Hamid Hoorfar and Alireza Bagheri", "title": "A New Optimal Algorithm for Computing the Visibility Area of a simple\n  Polygon from a Viewpoint", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a simple polygon $ \\mathcal {P} $ of $ n $ vertices in the Plane. We\nstudy the problem of computing the visibility area from a given viewpoint $ q $\ninside $ \\mathcal {P} $ where only sub-linear variables are allowed for working\nspace. Without any memory-constrained, this problem was previously solved in $\nO(n) $-time and $ O(n) $-variables space. In a newer research, the visibility\narea of a point be computed in $ O(n) $-time, using $ O(\\sqrt{n}) $ variables\nfor working space. In this paper, we present an optimal-time algorithm, using $\nO(c/\\log n) $ variables space for computing visibility area, where $ c<n $ is\nthe number of critical vertices. We keep the algorithm in the linear-time and\nreduce space as much as possible.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 17:07:26 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 15:07:44 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2018 17:28:27 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Hoorfar", "Hamid", ""], ["Bagheri", "Alireza", ""]]}, {"id": "1803.10266", "submitter": "Vitaly Feldman", "authors": "Cynthia Dwork and Vitaly Feldman", "title": "Privacy-preserving Prediction", "comments": "Accepted for presentation at Conference on Learning Theory (COLT)\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring differential privacy of models learned from sensitive user data is\nan important goal that has been studied extensively in recent years. It is now\nknown that for some basic learning problems, especially those involving\nhigh-dimensional data, producing an accurate private model requires much more\ndata than learning without privacy. At the same time, in many applications it\nis not necessary to expose the model itself. Instead users may be allowed to\nquery the prediction model on their inputs only through an appropriate\ninterface. Here we formulate the problem of ensuring privacy of individual\npredictions and investigate the overheads required to achieve it in several\nstandard models of classification and regression.\n  We first describe a simple baseline approach based on training several models\non disjoint subsets of data and using standard private aggregation techniques\nto predict. We show that this approach has nearly optimal sample complexity for\n(realizable) PAC learning of any class of Boolean functions. At the same time,\nwithout strong assumptions on the data distribution, the aggregation step\nintroduces a substantial overhead. We demonstrate that this overhead can be\navoided for the well-studied class of thresholds on a line and for a number of\nstandard settings of convex regression. The analysis of our algorithm for\nlearning thresholds relies crucially on strong generalization guarantees that\nwe establish for all differentially private prediction algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 18:40:05 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 00:36:56 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Dwork", "Cynthia", ""], ["Feldman", "Vitaly", ""]]}, {"id": "1803.10332", "submitter": "Jafar Fathali", "authors": "Jafar Fathali, Mehdi Zaferanieh", "title": "The balanced 2-median and 2-maxian problems on a tree", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the facility location problems with balancing on\nallocation clients to servers. Two bi-objective models are considered, in which\none objective is the traditional p-median or p-maxian objective and the second\nis to minimize the maximum demand volume allocated to any facility. An edge\ndeletion method with time complexity O(n^2) is presented for the balanced\n$2$-median problem on a tree. For the balanced 2-maxian problem, it is shown\nthe optimal solution is two end vertices of the diameter of the tree, which can\nbe obtained in a linear time.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 21:12:15 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Fathali", "Jafar", ""], ["Zaferanieh", "Mehdi", ""]]}, {"id": "1803.10366", "submitter": "Gautam Goel", "authors": "Niangjun Chen, Gautam Goel, and Adam Wierman", "title": "Smoothed Online Convex Optimization in High Dimensions via Online\n  Balanced Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Smoothed Online Convex Optimization, a version of online convex\noptimization where the learner incurs a penalty for changing her actions\nbetween rounds. Given a $\\Omega(\\sqrt{d})$ lower bound on the competitive ratio\nof any online algorithm, where $d$ is the dimension of the action space, we ask\nunder what conditions this bound can be beaten. We introduce a novel\nalgorithmic framework for this problem, Online Balanced Descent (OBD), which\nworks by iteratively projecting the previous point onto a carefully chosen\nlevel set of the current cost function so as to balance the switching costs and\nhitting costs. We demonstrate the generality of the OBD framework by showing\nhow, with different choices of \"balance,\" OBD can improve upon state-of-the-art\nperformance guarantees for both competitive ratio and regret, in particular,\nOBD is the first algorithm to achieve a dimension-free competitive ratio, $3 +\nO(1/\\alpha)$, for locally polyhedral costs, where $\\alpha$ measures the\n\"steepness\" of the costs. We also prove bounds on the dynamic regret of OBD\nwhen the balance is performed in the dual space that are dimension-free and\nimply that OBD has sublinear static regret.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 00:39:33 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 13:14:17 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Chen", "Niangjun", ""], ["Goel", "Gautam", ""], ["Wierman", "Adam", ""]]}, {"id": "1803.10633", "submitter": "S\\'andor Kisfaludi-Bak", "authors": "Mark de Berg, Hans L. Bodlaender, S\\'andor Kisfaludi-Bak, D\\'aniel\n  Marx, Tom C. van der Zanden", "title": "A Framework for ETH-Tight Algorithms and Lower Bounds in Geometric\n  Intersection Graphs", "comments": "41 pages. v4 corrects a small mistake in the conference version of\n  Theorem 1 by slightly restricting its scope and adding Lemma 4 to its proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithmic and lower-bound framework that facilitates the\nconstruction of subexponential algorithms and matching conditional complexity\nbounds. It can be applied to intersection graphs of similarly-sized fat\nobjects, yielding algorithms with running time $2^{O(n^{1-1/d})}$ for any fixed\ndimension $d \\geq 2$ for many well known graph problems, including Independent\nSet, $r$-Dominating Set for constant $r$, and Steiner Tree. For most problems,\nwe get improved running times compared to prior work; in some cases, we give\nthe first known subexponential algorithm in geometric intersection graphs.\nAdditionally, most of the obtained algorithms are representation-agnostic,\ni.e., they work on the graph itself and do not require the geometric\nrepresentation. Our algorithmic framework is based on a weighted separator\ntheorem and various treewidth techniques.\n  The lower bound framework is based on a constructive embedding of graphs into\nd-dimensional grids, and it allows us to derive matching\n$2^{\\Omega(n^{1-1/d})}$ lower bounds under the Exponential Time Hypothesis even\nin the much more restricted class of $d$-dimensional induced grid graphs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 14:09:16 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 09:22:02 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 07:29:07 GMT"}, {"version": "v4", "created": "Thu, 17 Dec 2020 10:52:46 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["de Berg", "Mark", ""], ["Bodlaender", "Hans L.", ""], ["Kisfaludi-Bak", "S\u00e1ndor", ""], ["Marx", "D\u00e1niel", ""], ["van der Zanden", "Tom C.", ""]]}, {"id": "1803.10639", "submitter": "Hasan Abasi", "authors": "Hasan Abasi, Nader H. Bshouty", "title": "On Learning Graphs with Edge-Detecting Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a general graph $G=(V,E)$ using\nedge-detecting queries, where the number of vertices $|V|=n$ is given to the\nlearner. The information theoretic lower bound gives $m\\log n$ for the number\nof queries, where $m=|E|$ is the number of edges. In case the number of edges\n$m$ is also given to the learner, Angluin-Chen's Las Vegas algorithm\n\\cite{AC08} runs in $4$ rounds and detects the edges in $O(m\\log n)$ queries.\nIn the other harder case where the number of edges $m$ is unknown, their\nalgorithm runs in $5$ rounds and asks $O(m\\log n+\\sqrt{m}\\log^2 n)$ queries.\nThere have been two open problems: \\emph{(i)} can the number of queries be\nreduced to $O(m\\log n)$ in the second case, and, \\emph{(ii)} can the number of\nrounds be reduced without substantially increasing the number of queries (in\nboth cases). For the first open problem (when $m$ is unknown) we give two\nalgorithms. The first is an $O(1)$-round Las Vegas algorithm that asks $m\\log\nn+\\sqrt{m}(\\log^{[k]}n)\\log n$ queries for any constant $k$ where\n$\\log^{[k]}n=\\log \\stackrel{k}{\\cdots} \\log n$. The second is an\n$O(\\log^*n)$-round Las Vegas algorithm that asks $O(m\\log n)$ queries. This\nsolves the first open problem for any practical $n$, for example,\n$n<2^{65536}$. We also show that no deterministic algorithm can solve this\nproblem in a constant number of rounds. To solve the second problem we study\nthe case when $m$ is known. We first show that any non-adaptive Monte Carlo\nalgorithm (one-round) must ask at least $\\Omega(m^2\\log n)$ queries, and any\ntwo-round Las Vegas algorithm must ask at least $m^{4/3-o(1)}\\log n$ queries on\naverage. We then give two two-round Monte Carlo algorithms, the first asks\n$O(m^{4/3}\\log n)$ queries for any $n$ and $m$, and the second asks $O(m\\log\nn)$ queries when $n>2^m$. Finally, we give a $3$-round Monte Carlo algorithm\nthat asks $O(m\\log n)$ queries for any $n$ and $m$.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 14:20:17 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Abasi", "Hasan", ""], ["Bshouty", "Nader H.", ""]]}, {"id": "1803.10846", "submitter": "Yu Cheng", "authors": "Yu Cheng, Rong Ge", "title": "Non-Convex Matrix Completion Against a Semi-Random Adversary", "comments": "added references and fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion is a well-studied problem with many machine learning\napplications. In practice, the problem is often solved by non-convex\noptimization algorithms. However, the current theoretical analysis for\nnon-convex algorithms relies heavily on the assumption that every entry is\nobserved with exactly the same probability $p$, which is not realistic in\npractice.\n  In this paper, we investigate a more realistic semi-random model, where the\nprobability of observing each entry is at least $p$. Even with this mild\nsemi-random perturbation, we can construct counter-examples where existing\nnon-convex algorithms get stuck in bad local optima.\n  In light of the negative results, we propose a pre-processing step that tries\nto re-weight the semi-random input, so that it becomes \"similar\" to a random\ninput. We give a nearly-linear time algorithm for this problem, and show that\nafter our pre-processing, all the local minima of the non-convex objective can\nbe used to approximately recover the underlying ground-truth matrix.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 20:46:27 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 19:11:49 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Cheng", "Yu", ""], ["Ge", "Rong", ""]]}, {"id": "1803.10923", "submitter": "Tasuku Soma", "authors": "Kaito Fujii, Tasuku Soma, Yuichi Yoshida", "title": "Polynomial-Time Algorithms for Submodular Laplacian Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be an undirected graph, $L_G\\in \\mathbb{R}^{V \\times V}$ be the\nassociated Laplacian matrix, and $b \\in \\mathbb{R}^V$ be a vector. Solving the\nLaplacian system $L_G x = b$ has numerous applications in theoretical computer\nscience, machine learning, and network analysis. Recently, the notion of the\nLaplacian operator $L_F:\\mathbb{R}^V \\to 2^{\\mathbb{R}^V}$ for a submodular\ntransformation $F:2^V \\to \\mathbb{R}_+^E$ was introduced, which can handle\nundirected graphs, directed graphs, hypergraphs, and joint distributions in a\nunified manner. In this study, we show that the submodular Laplacian system\n$L_F( x) \\ni b$ can be solved in polynomial time. Furthermore, we also prove\nthat even when the submodular Laplacian system has no solution, we can solve\nits regression form in polynomial time. Finally, we discuss potential\napplications of submodular Laplacian systems in machine learning and network\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 04:40:35 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Fujii", "Kaito", ""], ["Soma", "Tasuku", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1803.10983", "submitter": "Christine Dahn", "authors": "Christine Dahn, Nils M. Kriege, and Petra Mutzel", "title": "A Fixed-Parameter Algorithm for the Max-Cut Problem on Embedded 1-Planar\n  Graphs", "comments": "conference version from IWOCA 2018", "journal-ref": null, "doi": "10.1007/978-3-319-94667-2_12", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a fixed-parameter tractable algorithm for the \\textsc{Max-Cut}\nproblem on embedded 1-planar graphs parameterized by the crossing number $k$ of\nthe given embedding. A graph is called 1-planar if it can be drawn in the plane\nwith at most one crossing per edge. Our algorithm recursively reduces a\n1-planar graph to at most $3^k$ planar graphs, using edge removal and node\ncontraction. The \\textsc{Max-Cut} problem is then solved on the planar graphs\nusing established polynomial-time algorithms. We show that a maximum cut in the\ngiven 1-planar graph can be derived from the solutions for the planar graphs.\nOur algorithm computes a maximum cut in an embedded 1-planar graph with $n$\nnodes and $k$ edge crossings in time $\\mathcal{O}(3^k \\cdot n^{3/2} \\log n)$.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 09:39:14 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 09:41:28 GMT"}], "update_date": "2018-12-08", "authors_parsed": [["Dahn", "Christine", ""], ["Kriege", "Nils M.", ""], ["Mutzel", "Petra", ""]]}, {"id": "1803.11105", "submitter": "Ran Bittmann", "authors": "Ran M. Bittmann, Philippe Nemery, Xingtian Shi, Michael Kemelmakher,\n  Mengjiao Wang", "title": "Frequent Item-set Mining without Ubiquitous Items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent Item-set Mining (FIM), sometimes called Market Basket Analysis (MBA)\nor Association Rule Learning (ARL), are Machine Learning (ML) methods for\ncreating rules from datasets of transactions of items. Most methods identify\nitems likely to appear together in a transaction based on the support (i.e. a\nminimum number of relative co-occurrence of the items) for that hypothesis.\nAlthough this is a good indicator to measure the relevance of the assumption\nthat these items are likely to appear together, the phenomenon of very frequent\nitems, referred to as ubiquitous items, is not addressed in most algorithms.\nUbiquitous items have the same entropy as infrequent items, and not\ncontributing significantly to the knowledge. On the other hand, they have\nstrong effect on the performance of the algorithms and sometimes preventing the\nconvergence of the FIM algorithms and thus the provision of meaningful results.\nThis paper discusses the phenomenon of ubiquitous items and demonstrates how\nignoring these has a dramatic effect on the computation performances but with a\nlow and controlled effect on the significance of the results.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 14:52:33 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Bittmann", "Ran M.", ""], ["Nemery", "Philippe", ""], ["Shi", "Xingtian", ""], ["Kemelmakher", "Michael", ""], ["Wang", "Mengjiao", ""]]}, {"id": "1803.11132", "submitter": "Alexander Wein", "authors": "Afonso S. Bandeira, Amelia Perry, Alexander S. Wein", "title": "Notes on computational-to-statistical gaps: predictions using\n  statistical physics", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In these notes we describe heuristics to predict computational-to-statistical\ngaps in certain statistical problems. These are regimes in which the underlying\nstatistical problem is information-theoretically possible although no efficient\nalgorithm exists, rendering the problem essentially unsolvable for large\ninstances. The methods we describe here are based on mature, albeit\nnon-rigorous, tools from statistical physics.\n  These notes are based on a lecture series given by the authors at the Courant\nInstitute of Mathematical Sciences in New York City, on May 16th, 2017.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 16:10:04 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 04:10:11 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Bandeira", "Afonso S.", ""], ["Perry", "Amelia", ""], ["Wein", "Alexander S.", ""]]}, {"id": "1803.11159", "submitter": "Zhize Li", "authors": "Zhize Li, Tianyi Zhang, Shuyu Cheng, Jun Zhu, Jian Li", "title": "Stochastic Gradient Hamiltonian Monte Carlo with Variance Reduction for\n  Bayesian Inference", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-based Monte Carlo sampling algorithms, like Langevin dynamics and\nHamiltonian Monte Carlo, are important methods for Bayesian inference. In\nlarge-scale settings, full-gradients are not affordable and thus stochastic\ngradients evaluated on mini-batches are used as a replacement. In order to\nreduce the high variance of noisy stochastic gradients, Dubey et al. [2016]\napplied the standard variance reduction technique on stochastic gradient\nLangevin dynamics and obtained both theoretical and experimental improvements.\nIn this paper, we apply the variance reduction tricks on Hamiltonian Monte\nCarlo and achieve better theoretical convergence results compared with the\nvariance-reduced Langevin dynamics. Moreover, we apply the symmetric splitting\nscheme in our variance-reduced Hamiltonian Monte Carlo algorithms to further\nimprove the theoretical results. The experimental results are also consistent\nwith the theoretical results. As our experiment shows, variance-reduced\nHamiltonian Monte Carlo demonstrates better performance than variance-reduced\nLangevin dynamics in Bayesian regression and classification tasks on real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 17:06:26 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 19:03:50 GMT"}, {"version": "v3", "created": "Fri, 21 Jun 2019 18:47:35 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Li", "Zhize", ""], ["Zhang", "Tianyi", ""], ["Cheng", "Shuyu", ""], ["Zhu", "Jun", ""], ["Li", "Jian", ""]]}, {"id": "1803.11245", "submitter": "Travis Gagie", "authors": "Christina Boucher, Travis Gagie, Alan Kuhnle, Ben Langmead, Giovanni\n  Manzini and Taher Mun", "title": "Prefix-Free Parsing for Building Big BWTs", "comments": "Preliminary version appeared at WABI '18; full version submitted to a\n  journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput sequencing technologies have led to explosive growth of\ngenomic databases; one of which will soon reach hundreds of terabytes. For many\napplications we want to build and store indexes of these databases but\nconstructing such indexes is a challenge. Fortunately, many of these genomic\ndatabases are highly-repetitive---a characteristic that can be exploited to\nease the computation of the Burrows-Wheeler Transform (BWT), which underlies\nmany popular indexes. In this paper, we introduce a preprocessing algorithm,\nreferred to as {\\em prefix-free parsing}, that takes a text $T$ as input, and\nin one-pass generates a dictionary $D$ and a parse $P$ of $T$ with the property\nthat the BWT of $T$ can be constructed from $D$ and $P$ using workspace\nproportional to their total size and $O (|T|)$-time. Our experiments show that\n$D$ and $P$ are significantly smaller than $T$ in practice, and thus, can fit\nin a reasonable internal memory even when $T$ is very large. In particular, we\nshow that with prefix-free parsing we can build an 131-megabyte run-length\ncompressed FM-index (restricted to support only counting and not locating) for\n1000 copies of human chromosome 19 in 2 hours using 21 gigabytes of memory,\nsuggesting that we can build a 6.73 gigabyte index for 1000 complete\nhuman-genome haplotypes in approximately 102 hours using about 1 terabyte of\nmemory.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 20:36:11 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 17:07:15 GMT"}, {"version": "v3", "created": "Mon, 14 May 2018 15:05:06 GMT"}, {"version": "v4", "created": "Fri, 16 Nov 2018 16:35:53 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Boucher", "Christina", ""], ["Gagie", "Travis", ""], ["Kuhnle", "Alan", ""], ["Langmead", "Ben", ""], ["Manzini", "Giovanni", ""], ["Mun", "Taher", ""]]}, {"id": "1803.11427", "submitter": "Jean Cardinal", "authors": "Jean Cardinal, Stefan Langerman, Pablo P\\'erez-Lantero", "title": "On the Diameter of Tree Associahedra", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a natural notion of search trees on graphs, which we show is\nubiquitous in various areas of discrete mathematics and computer science.\nSearch trees on graphs can be modified by local operations called rotations,\nwhich generalize rotations in binary search trees. The rotation graph of search\ntrees on a graph $G$ is the skeleton of a polytope called the graph\nassociahedron of $G$.\n  We consider the case where the graph $G$ is a tree. We construct a family of\ntrees $G$ on $n$ vertices and pairs of search trees on $G$ such that the\nminimum number of rotations required to transform one search tree into the\nother is $\\Omega (n\\log n)$. This implies that the worst-case diameter of tree\nassociahedra is $\\Theta (n\\log n)$, which answers a question from Thibault\nManneville and Vincent Pilaud. The proof relies on a notion of projection of a\nsearch tree which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 11:59:08 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Cardinal", "Jean", ""], ["Langerman", "Stefan", ""], ["P\u00e9rez-Lantero", "Pablo", ""]]}]