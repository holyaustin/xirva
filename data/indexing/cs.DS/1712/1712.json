[{"id": "1712.00214", "submitter": "Toshiya Itoh", "authors": "Toshiya Itoh and Yoshinori Takei", "title": "On the Hardness of Deriving the Arithmetic Mean Component Competitive\n  Ratio", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the multi-objective time series search problem, Hasegawa and Itoh\n[Theoretical Computer Science, Vo.718, pp.58-66, 2018] presented the best\npossible online algorithm balanced price policy (BPP for short) for any\nmonotone function $f: R^k \\to R$. Specifically, the competitive ratio with\nrespect to the monotone function $f(c_{1},\\ldots,c_{k})=(c_{1}+\\cdots+c_{k})/k$\nis referred to as the arithmetic mean component competitive ratio. Hasegawa and\nItoh derived the closed formula of the arithmetic mean component competitive\nratio for $k=2$, but it has not been known for any integer $k \\geq 3$. In this\npaper, we show that it is NP-hard to derive closed formulas of the arithmetic\nmean component competitive ratio for general integer $k\\geq 2$. On the the\nhand, we derive closed formulas of the arithmetic mean component competitive\nratio for $k=3$ and $k=4$.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 07:22:47 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 23:27:51 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Itoh", "Toshiya", ""], ["Takei", "Yoshinori", ""]]}, {"id": "1712.00519", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr", "title": "An Elementary Analysis of the Probability That a Binomial Random\n  Variable Exceeds Its Expectation", "comments": "v2: Minor change in the presentation of previous works (took into\n  account the new version of Pel[16]). v3: Minor change in the presentation of\n  previous works (the proof of Lemma 6.4 in [RT11] gives a significantly\n  stronger result than what is stated in the Lemma itself). v4: Minor changes\n  (typos, mentioned the work of Slud)", "journal-ref": "Statistics and Probability Letters, 139:67-74, 2018", "doi": "10.1016/j.spl.2018.03.016", "report-no": null, "categories": "math.PR cs.DS cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an elementary proof of the fact that a binomial random variable $X$\nwith parameters $n$ and $0.29/n \\le p < 1$ with probability at least $1/4$\nstrictly exceeds its expectation. We also show that for $1/n \\le p < 1 - 1/n$,\n$X$ exceeds its expectation by more than one with probability at least\n$0.0370$. Both probabilities approach $1/2$ when $np$ and $n(1-p)$ tend to\ninfinity.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 23:24:52 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 12:22:42 GMT"}, {"version": "v3", "created": "Thu, 21 Dec 2017 15:39:10 GMT"}, {"version": "v4", "created": "Thu, 4 Jan 2018 12:49:19 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Doerr", "Benjamin", ""]]}, {"id": "1712.00615", "submitter": "Nader Bshouty", "authors": "Nader H. Bshouty and Vivian E. Bshouty-Hurani and George Haddad and\n  Thomas Hashem and Fadi Khoury and Omar Sharafy", "title": "Adaptive Group Testing Algorithms to Estimate the Number of Defectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the number of defective items in adaptive\nGroup testing by using a minimum number of queries. We improve the existing\nalgorithm and prove a lower bound that show that, for constant estimation, the\nnumber of tests in our algorithm is optimal.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 14:57:35 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Bshouty", "Nader H.", ""], ["Bshouty-Hurani", "Vivian E.", ""], ["Haddad", "George", ""], ["Hashem", "Thomas", ""], ["Khoury", "Fadi", ""], ["Sharafy", "Omar", ""]]}, {"id": "1712.00751", "submitter": "Marcel Wild", "authors": "Marcel Wild", "title": "ALLSAT compressed with wildcards: An invitation for C-programmers", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model set of a general Boolean function in CNF is calculated in a\ncompressed format, using novel wildcards. This method can be explained in very\nvisual ways. Preliminary comparison with existing methods (BDD's and\nMathematica's ESOP command) looks promising but our algorithm begs for a C\nencoding which would render it comparable in more systematic ways.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 10:59:34 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 09:07:48 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Wild", "Marcel", ""]]}, {"id": "1712.00870", "submitter": "Laurence Boxer", "authors": "Laurence Boxer", "title": "Coarse Grained Parallel Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the running time of the Saukas-Song algorithm for selection on a\ncoarse grained multicomputer without expressing the running time in terms of\ncommunication rounds. This shows that while in the best case the Saukas-Song\nalgorithm runs in asymptotically optimal time, in general it does not. We\npropose other algorithms for coarse grained selection that have optimal\nexpected running time.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 01:43:34 GMT"}, {"version": "v2", "created": "Sun, 14 Oct 2018 17:19:51 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 22:41:22 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Boxer", "Laurence", ""]]}, {"id": "1712.00918", "submitter": "Anindya De", "authors": "Anindya De", "title": "Boolean function analysis meets stochastic optimization: An\n  approximation scheme for stochastic knapsack", "comments": "To appear in SODA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic knapsack problem is the stochastic variant of the classical\nknapsack problem in which the algorithm designer is given a a knapsack with a\ngiven capacity and a collection of items where each item is associated with a\nprofit and a probability distribution on its size. The goal is to select a\nsubset of items with maximum profit and violate the capacity constraint with\nprobability at most $p$ (referred to as the overflow probability). While\nseveral approximation algorithms have been developed for this problem, most of\nthese algorithms relax the capacity constraint of the knapsack. In this paper,\nwe design efficient approximation schemes for this problem without relaxing the\ncapacity constraint.\n  (i) Our first result is in the case when item sizes are Bernoulli random\nvariables. In this case, we design a (nearly) fully polynomial time\napproximation scheme (FPTAS) which only relaxes the overflow probability. (ii)\nOur second result generalizes the first result to the case when all the item\nsizes are supported on a (common) set of constant size. (iii) Our third result\nis in the case when item sizes are so-called \"hypercontractive\" random\nvariables i.e., random variables whose second and fourth moments are within\nconstant factors of each other. In other words, the kurtosis of the random\nvariable is upper bounded by a constant.\n  Crucially, all of our algorithms meet the capacity constraint exactly, a\nresult which was previously known only when the item sizes were Poisson or\nGaussian random variables. Our results rely on new connections between Boolean\nfunction analysis and stochastic optimization. We believe that these ideas and\ntechniques may prove to be useful in other stochastic optimization problems as\nwell.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 06:09:50 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["De", "Anindya", ""]]}, {"id": "1712.01139", "submitter": "Eylon Yogev", "authors": "Merav Parter and Eylon Yogev", "title": "Distributed Algorithms Made Secure: A Graph Theoretic Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the area of distributed graph algorithms a number of network's entities\nwith local views solve some computational task by exchanging messages with\ntheir neighbors. Quite unfortunately, an inherent property of most existing\ndistributed algorithms is that throughout the course of their execution, the\nnodes get to learn not only their own output but rather learn quite a lot on\nthe inputs or outputs of many other entities. This leakage of information might\nbe a major obstacle in settings where the output (or input) of network's\nindividual is a private information. In this paper, we introduce a new\nframework for \\emph{secure distributed graph algorithms} and provide the first\n\\emph{general compiler} that takes any \"natural\" non-secure distributed\nalgorithm that runs in $r$ rounds, and turns it into a secure algorithm that\nruns in $\\widetilde{O}(r \\cdot D \\cdot poly(\\Delta))$ rounds where $\\Delta$ is\nthe maximum degree in the graph and $D$ is its diameter. The security of the\ncompiled algorithm is information-theoretic but holds only against a\nsemi-honest adversary that controls a single node in the network.\n  This compiler is made possible due to a new combinatorial structure called\n\\emph{private neighborhood trees}: a collection of $n$ trees\n$T(u_1),\\ldots,T(u_n)$, one for each vertex $u_i \\in V(G)$, such that each tree\n$T(u_i)$ spans the neighbors of $u_i$ {\\em without going through $u_i$}.\nIntuitively, each tree $T(u_i)$ allows all neighbors of $u_i$ to exchange a\n\\emph{secret} that is hidden from $u_i$, which is the basic graph\ninfrastructure of the compiler. In a $(d,c)$-private neighborhood trees each\ntree $T(u_i)$ has depth at most $d$ and each edge $e \\in G$ appears in at most\n$c$ different trees. We show a construction of private neighborhood trees with\n$d=\\widetilde{O}(\\Delta \\cdot D)$ and $c=\\widetilde{O}(D)$.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 15:08:38 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 11:50:03 GMT"}, {"version": "v3", "created": "Thu, 2 Aug 2018 18:39:42 GMT"}, {"version": "v4", "created": "Sun, 9 Dec 2018 12:49:29 GMT"}, {"version": "v5", "created": "Sun, 13 Jan 2019 20:28:17 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Parter", "Merav", ""], ["Yogev", "Eylon", ""]]}, {"id": "1712.01197", "submitter": "Sandor P. Fekete", "authors": "Aaron T. Becker, Erik D. Demaine, S\\'andor P. Fekete, Jarrett\n  Lonsforda, Rose Morris-Wright", "title": "Particle Computation: Complexity, Algorithms, and Logic", "comments": "27 pages, 19 figures, full version that combines three previous\n  conference articles", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.CG cs.DC cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate algorithmic control of a large swarm of mobile particles (such\nas robots, sensors, or building material) that move in a 2D workspace using a\nglobal input signal (such as gravity or a magnetic field). We show that a maze\nof obstacles to the environment can be used to create complex systems. We\nprovide a wide range of results for a wide range of questions. These can be\nsubdivided into external algorithmic problems, in which particle configurations\nserve as input for computations that are performed elsewhere, and internal\nlogic problems, in which the particle configurations themselves are used for\ncarrying out computations. For external algorithms, we give both negative and\npositive results. If we are given a set of stationary obstacles, we prove that\nit is NP-hard to decide whether a given initial configuration of unit-sized\nparticles can be transformed into a desired target configuration. Moreover, we\nshow that finding a control sequence of minimum length is PSPACE-complete. We\nalso work on the inverse problem, providing constructive algorithms to design\nworkspaces that efficiently implement arbitrary permutations between different\nconfigurations. For internal logic, we investigate how arbitrary computations\ncan be implemented. We demonstrate how to encode dual-rail logic to build a\nuniversal logic gate that concurrently evaluates and, nand, nor, and or\noperations. Using many of these gates and appropriate interconnects, we can\nevaluate any logical expression. However, we establish that simulating the full\nrange of complex interactions present in arbitrary digital circuits encounters\na fundamental difficulty: a fan-out gate cannot be generated. We resolve this\nmissing component with the help of 2x1 particles, which can create fan-out\ngates that produce multiple copies of the inputs. Using these gates we provide\nrules for replicating arbitrary digital circuits.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 17:03:10 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Becker", "Aaron T.", ""], ["Demaine", "Erik D.", ""], ["Fekete", "S\u00e1ndor P.", ""], ["Lonsforda", "Jarrett", ""], ["Morris-Wright", "Rose", ""]]}, {"id": "1712.01208", "submitter": "Tim Kraska", "authors": "Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, Neoklis Polyzotis", "title": "The Case for Learned Index Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexes are models: a B-Tree-Index can be seen as a model to map a key to the\nposition of a record within a sorted array, a Hash-Index as a model to map a\nkey to a position of a record within an unsorted array, and a BitMap-Index as a\nmodel to indicate if a data record exists or not. In this exploratory research\npaper, we start from this premise and posit that all existing index structures\ncan be replaced with other types of models, including deep-learning models,\nwhich we term learned indexes. The key idea is that a model can learn the sort\norder or structure of lookup keys and use this signal to effectively predict\nthe position or existence of records. We theoretically analyze under which\nconditions learned indexes outperform traditional index structures and describe\nthe main challenges in designing learned index structures. Our initial results\nshow, that by using neural nets we are able to outperform cache-optimized\nB-Trees by up to 70% in speed while saving an order-of-magnitude in memory over\nseveral real-world data sets. More importantly though, we believe that the idea\nof replacing core components of a data management system through learned models\nhas far reaching implications for future systems designs and that this work\njust provides a glimpse of what might be possible.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 17:18:41 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 19:42:46 GMT"}, {"version": "v3", "created": "Mon, 30 Apr 2018 07:54:41 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Kraska", "Tim", ""], ["Beutel", "Alex", ""], ["Chi", "Ed H.", ""], ["Dean", "Jeffrey", ""], ["Polyzotis", "Neoklis", ""]]}, {"id": "1712.01231", "submitter": "Mohamad Elmasri", "authors": "Mohamad Elmasri", "title": "Sub-clustering in decomposable graphs and size-varying junction trees", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel representation of decomposable graphs based on\nsemi-latent tree-dependent bipartite graphs. The novel representation has two\nmain benefits. First, it enables a form of sub-clustering within maximal\ncliques of the graph, adding informational richness to the general use of\ndecomposable graphs that could be harnessed in applications with behavioural\ntype of data. Second, it allows for a new node-driven Markov chain Monte Carlo\nsampler of decomposable graphs that can easily parallelize and scale. The\nproposed sampler also benefits from the computational efficiency of\njunction-tree-based samplers of decomposable graphs.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 18:12:24 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Elmasri", "Mohamad", ""]]}, {"id": "1712.01241", "submitter": "Alex Wang", "authors": "Abhratanu Dutta, Aravindan Vijayaraghavan, Alex Wang", "title": "Clustering Stable Instances of Euclidean k-means", "comments": "23 pages, 2 figures, appearing in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Euclidean k-means problem is arguably the most widely-studied clustering\nproblem in machine learning. While the k-means objective is NP-hard in the\nworst-case, practitioners have enjoyed remarkable success in applying\nheuristics like Lloyd's algorithm for this problem. To address this disconnect,\nwe study the following question: what properties of real-world instances will\nenable us to design efficient algorithms and prove guarantees for finding the\noptimal clustering? We consider a natural notion called additive perturbation\nstability that we believe captures many practical instances. Stable instances\nhave unique optimal k-means solutions that do not change even when each point\nis perturbed a little (in Euclidean distance). This captures the property that\nthe k-means optimal solution should be tolerant to measurement errors and\nuncertainty in the points. We design efficient algorithms that provably recover\nthe optimal clustering for instances that are additive perturbation stable.\nWhen the instance has some additional separation, we show an efficient\nalgorithm with provable guarantees that is also robust to outliers. We\ncomplement these results by studying the amount of stability in real datasets\nand demonstrating that our algorithm performs well on these benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 18:33:31 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Dutta", "Abhratanu", ""], ["Vijayaraghavan", "Aravindan", ""], ["Wang", "Alex", ""]]}, {"id": "1712.01524", "submitter": "Janardhan Kulkarni", "authors": "Bolin Ding, Janardhan Kulkarni, Sergey Yekhanin", "title": "Collecting Telemetry Data Privately", "comments": "To appear in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collection and analysis of telemetry data from users' devices is\nroutinely performed by many software companies. Telemetry collection leads to\nimproved user experience but poses significant risks to users' privacy. Locally\ndifferentially private (LDP) algorithms have recently emerged as the main tool\nthat allows data collectors to estimate various population statistics, while\npreserving privacy. The guarantees provided by such algorithms are typically\nvery strong for a single round of telemetry collection, but degrade rapidly\nwhen telemetry is collected regularly. In particular, existing LDP algorithms\nare not suitable for repeated collection of counter data such as daily app\nusage statistics. In this paper, we develop new LDP mechanisms geared towards\nrepeated collection of counter data, with formal privacy guarantees even after\nbeing executed for an arbitrarily long period of time. For two basic analytical\ntasks, mean estimation and histogram estimation, our LDP mechanisms for\nrepeated data collection provide estimates with comparable or even the same\naccuracy as existing single-round LDP collection mechanisms. We conduct\nempirical evaluation on real-world counter datasets to verify our theoretical\nresults. Our mechanisms have been deployed by Microsoft to collect telemetry\nacross millions of devices.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 08:32:18 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Ding", "Bolin", ""], ["Kulkarni", "Janardhan", ""], ["Yekhanin", "Sergey", ""]]}, {"id": "1712.01725", "submitter": "Christian Sohler", "authors": "David Cohen-Steiner, Weihao Kong, Christian Sohler, Gregory Valiant", "title": "Approximating the Spectrum of a Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectrum of a network or graph $G=(V,E)$ with adjacency matrix $A$,\nconsists of the eigenvalues of the normalized Laplacian $L= I - D^{-1/2} A\nD^{-1/2}$. This set of eigenvalues encapsulates many aspects of the structure\nof the graph, including the extent to which the graph posses community\nstructures at multiple scales. We study the problem of approximating the\nspectrum $\\lambda = (\\lambda_1,\\dots,\\lambda_{|V|})$, $0 \\le \\lambda_1,\\le\n\\dots, \\le \\lambda_{|V|}\\le 2$ of $G$ in the regime where the graph is too\nlarge to explicitly calculate the spectrum. We present a sublinear time\nalgorithm that, given the ability to query a random node in the graph and\nselect a random neighbor of a given node, computes a succinct representation of\nan approximation $\\widetilde \\lambda = (\\widetilde \\lambda_1,\\dots,\\widetilde\n\\lambda_{|V|})$, $0 \\le \\widetilde \\lambda_1,\\le \\dots, \\le \\widetilde\n\\lambda_{|V|}\\le 2$ such that $\\|\\widetilde \\lambda - \\lambda\\|_1 \\le \\epsilon\n|V|$. Our algorithm has query complexity and running time $exp(O(1/\\epsilon))$,\nindependent of the size of the graph, $|V|$. We demonstrate the practical\nviability of our algorithm on 15 different real-world graphs from the Stanford\nLarge Network Dataset Collection, including social networks, academic\ncollaboration graphs, and road networks. For the smallest of these graphs, we\nare able to validate the accuracy of our algorithm by explicitly calculating\nthe true spectrum; for the larger graphs, such a calculation is computationally\nprohibitive.\n  In addition we study the implications of our algorithm to property testing in\nthe bounded degree graph model.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 16:02:55 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Cohen-Steiner", "David", ""], ["Kong", "Weihao", ""], ["Sohler", "Christian", ""], ["Valiant", "Gregory", ""]]}, {"id": "1712.01774", "submitter": "Stefan Bamberger", "authors": "Stefan Bamberger, Felix Krahmer", "title": "Optimal Fast Johnson-Lindenstrauss Embeddings for Large Data Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Johnson-Lindenstrauss embeddings are widely used to reduce the dimension and\nthus the processing time of data. To reduce the total complexity, also fast\nalgorithms for applying these embeddings are necessary. To date, such fast\nalgorithms are only available either for a non-optimal embedding dimension or\nup to a certain threshold on the number of data points.\n  We address a variant of this problem where one aims to simultaneously embed\nlarger subsets of the data set. Our method follows an approach by Nelson: A\nsubsampled Hadamard transform maps points into a space of lower, but not\noptimal dimension. Subsequently, a random matrix with independent entries\nprojects to an optimal embedding dimension.\n  For subsets whose size scales at least polynomially in the ambient dimension,\nthe complexity of this method comes close to the number of operations just to\nread the data under mild assumptions on the size of the data set that are\nconsiderably less restrictive than in previous works. We also prove a lower\nbound showing that subsampled Hadamard matrices alone cannot reach an optimal\nembedding dimension. Hence, the second embedding cannot be omitted.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 17:26:27 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 14:39:59 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Bamberger", "Stefan", ""], ["Krahmer", "Felix", ""]]}, {"id": "1712.01779", "submitter": "Ran Ben Basat", "authors": "Eran Assaf, Ran Ben Basat, Gil Einziger, Roy Friedman", "title": "Pay for a Sliding Bloom Filter and Get Counting, Distinct Elements, and\n  Entropy for Free", "comments": "To appear in IEEE INFOCOM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many networking applications, recent data is more significant than older\ndata, motivating the need for sliding window solutions. Various capabilities,\nsuch as DDoS detection and load balancing, require insights about multiple\nmetrics including Bloom filters, per-flow counting, count distinct and entropy\nestimation.\n  In this work, we present a unified construction that solves all the above\nproblems in the sliding window model. Our single solution offers a better space\nto accuracy tradeoff than the state-of-the-art for each of these individual\nproblems! We show this both analytically and by running multiple real Internet\nbackbone and datacenter packet traces.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 17:30:37 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Assaf", "Eran", ""], ["Basat", "Ran Ben", ""], ["Einziger", "Gil", ""], ["Friedman", "Roy", ""]]}, {"id": "1712.01781", "submitter": "Michael Mitzenmacher", "authors": "Michael Mitzenmacher", "title": "Arithmetic Progression Hypergraphs: Examining the Second Moment Method", "comments": "10 pages, to be submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many data structure settings, it has been shown that using \"double\nhashing\" in place of standard hashing, by which we mean choosing multiple hash\nvalues according to an arithmetic progression instead of choosing each hash\nvalue independently, has asymptotically negligible difference in performance.\nWe attempt to extend these ideas beyond data structure settings by considering\nhow threshold arguments based on second moment methods can be generalized to\n\"arithmetic progression\" versions of problems. With this motivation, we define\na novel \"quasi-random\" hypergraph model, random arithmetic progression (AP)\nhypergraphs, which is based on edges that form arithmetic progressions and\nunifies many previous problems. Our main result is to show that second moment\narguments for 3-NAE-SAT and 2-coloring of 3-regular hypergraphs extend to the\ndouble hashing setting. We leave several open problems related to these\nquasi-random hypergraphs and the thresholds of associated problem variations.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 17:41:32 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Mitzenmacher", "Michael", ""]]}, {"id": "1712.01834", "submitter": "Diptarka Chakraborty", "authors": "Diptarka Chakraborty, Debarati Das, Michal Kouck\\'y, Nitin Saurabh", "title": "Optimal Quasi-Gray Codes: The Alphabet Matters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A quasi-Gray code of dimension $n$ and length $\\ell$ over an alphabet\n$\\Sigma$ is a sequence of distinct words $w_1,w_2,\\dots,w_\\ell$ from $\\Sigma^n$\nsuch that any two consecutive words differ in at most $c$ coordinates, for some\nfixed constant $c>0$. In this paper we are interested in the read and write\ncomplexity of quasi-Gray codes in the bit-probe model, where we measure the\nnumber of symbols read and written in order to transform any word $w_i$ into\nits successor $w_{i+1}$.\n  We present construction of quasi-Gray codes of dimension $n$ and length $3^n$\nover the ternary alphabet $\\{0,1,2\\}$ with worst-case read complexity $O(\\log\nn)$ and write complexity $2$. This generalizes to arbitrary odd-size alphabets.\nFor the binary alphabet, we present quasi-Gray codes of dimension $n$ and\nlength at least $2^n - 20n$ with worst-case read complexity $6+\\log n$ and\nwrite complexity $2$. This complements a recent result by Raskin [Raskin '17]\nwho shows that any quasi-Gray code over binary alphabet of length $2^n$ has\nread complexity $\\Omega(n)$.\n  Our results significantly improve on previously known constructions and for\nthe odd-size alphabets we break the $\\Omega(n)$ worst-case barrier for\nspace-optimal (non-redundant) quasi-Gray codes with constant number of writes.\nWe obtain our results via a novel application of algebraic tools together with\nthe principles of catalytic computation [Buhrman et al. '14, Ben-Or and Cleve\n'92, Barrington '89, Coppersmith and Grossman '75].\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 14:43:45 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 12:44:18 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Chakraborty", "Diptarka", ""], ["Das", "Debarati", ""], ["Kouck\u00fd", "Michal", ""], ["Saurabh", "Nitin", ""]]}, {"id": "1712.01971", "submitter": "Yi Li", "authors": "Yi Li and Vasileios Nakos", "title": "Deterministic Heavy Hitters with Sublinear Query Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the classic problem of finding heavy hitters in the\nturnstile streaming model. We give the first deterministic linear sketch that\nhas $O(\\epsilon^{-2} \\log n \\cdot \\log^*(\\epsilon^{-1}))$ rows and answers\nqueries in sublinear time. The number of rows is only a factor of\n$\\log^*(\\epsilon^{-1})$ more than that used by the state-of-the-art algorithm\nprior to our paper due to Nelson, Nguyen and Woodruff (RANDOM'12). Their\nalgorithm runs in time at least linear in the universe size $n$, which is\nhighly undesirable in streaming applications. Our approach is based on an\niterative procedure, where most unrecovered heavy hitters are identified in\neach iteration. Although this technique has been extensively employed in the\nrelated problem of sparse recovery, this is the first time, to the best of our\nknowledge, that it has been used in the context of $\\ell_1$ heavy hitters.\n  Along the way, we also give sublinear time algorithms for the closely related\nproblems of combinatorial group testing and $\\ell_1/\\ell_1$ compressed sensing,\nmatching the space usage of previous (super-)linear time algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 23:42:02 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 00:20:54 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Li", "Yi", ""], ["Nakos", "Vasileios", ""]]}, {"id": "1712.02076", "submitter": "Gal Shahaf", "authors": "Michael Schapira, Gal Shahaf", "title": "Oblivious Routing via Random Walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present novel oblivious routing algorithms for both splittable and\nunsplittable multicommodity flow. Our algorithm for minimizing congestion for\n\\emph{unsplittable} multicommodity flow is the first oblivious routing\nalgorithm for this setting. As an intermediate step towards this algorithm, we\npresent a novel generalization of Valiant's classical load balancing scheme for\npacket-switched networks to arbitrary graphs, which is of independent interest.\nOur algorithm for minimizing congestion for \\emph{splittable} multicommodity\nflow improves upon the state-of-the-art, in terms of both running time and\nperformance, for graphs that exhibit good expansion guarantees. Our algorithms\nrely on diffusing traffic via iterative applications of the random walk\noperator. Consequently, the performance guarantees of our algorithms are\nderived from the convergence of the random walk operator to the stationary\ndistribution and are expressed in terms of the spectral gap of the graph (which\ndominates the mixing time).\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 07:56:43 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Schapira", "Michael", ""], ["Shahaf", "Gal", ""]]}, {"id": "1712.02103", "submitter": "Lei Shang", "authors": "Lei Shang", "title": "Exact Algorithms With Worst-case Guarantee For Scheduling: From Theory\n  to Practice", "comments": "156 pages, PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This PhD thesis summarizes research works on the design of exact algorithms\nthat provide a worst-case (time or space) guarantee for NP-hard scheduling\nproblems. Both theoretical and practical aspects are considered with three main\nresults reported. The first one is about a Dynamic Programming algorithm which\nsolves the F3Cmax problem in O*(3^n) time and space. The algorithm is easily\ngeneralized to other flowshop problems and single machine scheduling problems.\nThe second contribution is about a search tree method called Branch & Merge\nwhich solves the 1||SumTi problem with the time complexity converging to\nO*(2^n) and in polynomial space. Our third contribution aims to improve the\npractical efficiency of exact search tree algorithms solving scheduling\nproblems. First we realized that a better way to implement the idea of Branch &\nMerge is to use a technique called Memorization. By the finding of a new\nalgorithmic paradox and the implementation of a memory cleaning strategy, the\nmethod succeeded to solve instances with 300 more jobs with respect to the\nstate-of-the-art algorithm for the 1||SumTi problem. Then the treatment is\nextended to another three problems 1|ri|SumCi, 1|dtilde|SumwiCi and F2||SumCi.\nThe results of the four problems all together show the power of the\nMemorization paradigm when applied on sequencing problems. We name it Branch &\nMemorize to promote a systematic consideration of Memorization as an essential\nbuilding block in branching algorithms like Branch and Bound. The method can\nsurely also be used to solve other problems, which are not necessarily\nscheduling problems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 09:42:34 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Shang", "Lei", ""]]}, {"id": "1712.02176", "submitter": "Stefan Weltge", "authors": "Alfonso Cevallos, Stefan Weltge, Rico Zenklusen", "title": "Lifting Linear Extension Complexity Bounds to the Mixed-Integer Setting", "comments": "A conference version of this paper will be presented at SODA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed-integer mathematical programs are among the most commonly used models\nfor a wide set of problems in Operations Research and related fields. However,\nthere is still very little known about what can be expressed by small\nmixed-integer programs. In particular, prior to this work, it was open whether\nsome classical problems, like the minimum odd-cut problem, can be expressed by\na compact mixed-integer program with few (even constantly many) integer\nvariables. This is in stark contrast to linear formulations, where recent\nbreakthroughs in the field of extended formulations have shown that many\npolytopes associated to classical combinatorial optimization problems do not\neven admit approximate extended formulations of sub-exponential size.\n  We provide a general framework for lifting inapproximability results of\nextended formulations to the setting of mixed-integer extended formulations,\nand obtain almost tight lower bounds on the number of integer variables needed\nto describe a variety of classical combinatorial optimization problems. Among\nthe implications we obtain, we show that any mixed-integer extended formulation\nof sub-exponential size for the matching polytope, cut polytope, traveling\nsalesman polytope or dominant of the odd-cut polytope, needs $ \\Omega(n/\\log n)\n$ many integer variables, where $ n $ is the number of vertices of the\nunderlying graph. Conversely, the above-mentioned polyhedra admit\npolynomial-size mixed-integer formulations with only $ O(n) $ or $ O(n \\log n)\n$ (for the traveling salesman polytope) many integer variables.\n  Our results build upon a new decomposition technique that, for any convex set\n$ C $, allows for approximating any mixed-integer description of $ C $ by the\nintersection of $ C $ with the union of a small number of affine subspaces.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 13:15:32 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Cevallos", "Alfonso", ""], ["Weltge", "Stefan", ""], ["Zenklusen", "Rico", ""]]}, {"id": "1712.02302", "submitter": "Henry Cohn", "authors": "Jonah Blasiak, Thomas Church, Henry Cohn, Joshua A. Grochow, Chris\n  Umans", "title": "Which groups are amenable to proving exponent two for matrix\n  multiplication?", "comments": "23 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GR cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cohn-Umans group-theoretic approach to matrix multiplication suggests\nembedding matrix multiplication into group algebra multiplication, and bounding\n$\\omega$ in terms of the representation theory of the host group. This\nframework is general enough to capture the best known upper bounds on $\\omega$\nand is conjectured to be powerful enough to prove $\\omega = 2$, although\nfinding a suitable group and constructing such an embedding has remained\nelusive. Recently it was shown, by a generalization of the proof of the Cap Set\nConjecture, that abelian groups of bounded exponent cannot prove $\\omega = 2$\nin this framework, which ruled out a family of potential constructions in the\nliterature.\n  In this paper we study nonabelian groups as potential hosts for an embedding.\nWe prove two main results:\n  (1) We show that a large class of nonabelian groups---nilpotent groups of\nbounded exponent satisfying a mild additional condition---cannot prove $\\omega\n= 2$ in this framework. We do this by showing that the shrinkage rate of powers\nof the augmentation ideal is similar to the shrinkage rate of the number of\nfunctions over $(\\mathbb{Z}/p\\mathbb{Z})^n$ that are degree $d$ polynomials;\nour proof technique can be seen as a generalization of the polynomial method\nused to resolve the Cap Set Conjecture.\n  (2) We show that symmetric groups $S_n$ cannot prove nontrivial bounds on\n$\\omega$ when the embedding is via three Young subgroups---subgroups of the\nform $S_{k_1} \\times S_{k_2} \\times \\dotsb \\times S_{k_\\ell}$---which is a\nnatural strategy that includes all known constructions in $S_n$.\n  By developing techniques for negative results in this paper, we hope to\ncatalyze a fruitful interplay between the search for constructions proving\nbounds on $\\omega$ and methods for ruling them out.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 17:38:48 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Blasiak", "Jonah", ""], ["Church", "Thomas", ""], ["Cohn", "Henry", ""], ["Grochow", "Joshua A.", ""], ["Umans", "Chris", ""]]}, {"id": "1712.02371", "submitter": "M\\'arcia Cappelle", "authors": "M\\'arcia R. Cappelle, Les Foulds and Humberto J. Longo", "title": "A note on searching sorted unbalanced three-dimensional arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the problem of searching sequentially for a desired real value (a\nkey) within a sorted unbalanced three-dimensional finite real array. This\nclassic problem can be viewed as determining the correct dimensional threshold\nfunction from a finite class of such functions within the array, based on\nsequential queries that take the form of point samples. This note addresses the\nchallenge of constructing algorithms that require the minimum number of queries\nnecessary in the worst case, to search for a given key in arrays that have\nthree dimensions with sizes that are not necessarily equal.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 19:03:12 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Cappelle", "M\u00e1rcia R.", ""], ["Foulds", "Les", ""], ["Longo", "Humberto J.", ""]]}, {"id": "1712.02447", "submitter": "Daniel Paulusma", "authors": "Konrad Dabrowski, Daniel Paulusma", "title": "On Colouring $(2P_2,H)$-Free and $(P_5,H)$-Free Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Colouring problem asks whether the vertices of a graph can be coloured\nwith at most $k$ colours for a given integer $k$ in such a way that no two\nadjacent vertices receive the same colour. A graph is $(H_1,H_2)$-free if it\nhas no induced subgraph isomorphic to $H_1$ or $H_2$. A connected graph $H_1$\nis almost classified if Colouring on $(H_1,H_2)$-free graphs is known to be\npolynomial-time solvable or NP-complete for all but finitely many connected\ngraphs $H_2$. We show that every connected graph $H_1$ apart from the claw\n$K_{1,3}$ and the $5$-vertex path $P_5$ is almost classified. We also prove a\nnumber of new hardness results for Colouring on $(2P_2,H)$-free graphs. This\nenables us to list all graphs $H$ for which the complexity of Colouring is open\non $(2P_2,H)$-free graphs and all graphs $H$ for which the complexity of\nColouring is open on $(P_5,H)$-free graphs. In fact we show that these two\nlists coincide. Moreover, we show that the complexities of Colouring for\n$(2P_2,H)$-free graphs and for $(P_5,H)$-free graphs are the same for all known\ncases.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 23:45:33 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Dabrowski", "Konrad", ""], ["Paulusma", "Daniel", ""]]}, {"id": "1712.02485", "submitter": "Jelena Diakonikolas", "authors": "Jelena Diakonikolas and Lorenzo Orecchia", "title": "The Approximate Duality Gap Technique: A Unified Theory of First-Order\n  Methods", "comments": "In SIAM Journal on Optimization. The most recent version corrected a\n  few typos", "journal-ref": null, "doi": "10.1137/18M1172314", "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general technique for the analysis of first-order methods. The\ntechnique relies on the construction of a duality gap for an appropriate\napproximation of the objective function, where the function approximation\nimproves as the algorithm converges. We show that in continuous time\nenforcement of an invariant that this approximate duality gap decreases at a\ncertain rate exactly recovers a wide range of first-order continuous-time\nmethods. We characterize the discretization errors incurred by different\ndiscretization methods, and show how iteration-complexity-optimal methods for\nvarious classes of problems cancel out the discretization error. The techniques\nare illustrated on various classes of problems -- including convex minimization\nfor Lipschitz-continuous objectives, smooth convex minimization, composite\nminimization, smooth and strongly convex minimization, solving variational\ninequalities with monotone operators, and convex-concave saddle-point\noptimization -- and naturally extend to other settings.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 03:56:01 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 19:27:41 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 00:54:37 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Diakonikolas", "Jelena", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1712.02827", "submitter": "Panagiotis Strouthopoulos", "authors": "Panagiotis Strouthopoulos, Apostolos Papadopoulos", "title": "Core Discovery in Hidden Graphs", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive network exploration is an important research direction with many\napplications. In such a setting, the network is, usually, modeled as a graph\n$G$, whereas any structural information of interest is extracted by inspecting\nthe way nodes are connected together. In the case where the adjacency matrix or\nthe adjacency list of $G$ is available, one can directly apply graph mining\nalgorithms to extract useful knowledge. However, there are cases where this is\nnot possible because the graph is \\textit{hidden} or \\textit{implicit}, meaning\nthat the edges are not recorded explicitly in the form of an adjacency\nrepresentation. In such a case, the only alternative is to pose a sequence of\n\\textit{edge probing queries} asking for the existence or not of a particular\ngraph edge. However, checking all possible node pairs is costly (quadratic on\nthe number of nodes). Thus, our objective is to pose as few edge probing\nqueries as possible, since each such query is expected to be costly. In this\nwork, we center our focus on the \\textit{core decomposition} of a hidden graph.\nIn particular, we provide an efficient algorithm to detect the maximal subgraph\nof $S_k$ of $G$ where the induced degree of every node $u \\in S_k$ is at least\n$k$. Performance evaluation results demonstrate that significant performance\nimprovements are achieved in comparison to baseline approaches.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 19:32:06 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Strouthopoulos", "Panagiotis", ""], ["Papadopoulos", "Apostolos", ""]]}, {"id": "1712.02870", "submitter": "Sounaka Mishra", "authors": "Sounaka Mishra and Shijin Rajakrishnan", "title": "The Complexity of Maximum $k$-Order Bounded Component Set Problem", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G=(V, E)$ and a positive integer $k$, in Maximum $k$-Order\nBounded Component Set (Max-$k$-OBCS), it is required to find a vertex set $S\n\\subseteq V$ of maximum size such that each component in the induced graph\n$G[S]$ has at most $k$ vertices. We prove that for constant $k$, Max-$k$-OBCS\nis hard to approximate within a factor of $n^{1 -\\epsilon}$, for any $\\epsilon\n> 0$, unless $\\mathsf{P} = \\mathsf{NP}$. This is an improvement on the previous\nlower bound of $\\sqrt{n}$ for Max-2-OBCS due to Orlovich et al. We provide\nlower bounds on the approximability when $k$ is not a constant as well.\nMax-$k$-OBCS can be seen as a generalization of Maximum Independent Set\n(Max-IS). We generalize Tur\\'an's greedy algorithm for Max-IS and prove that it\napproximates Max-$k$-OBCS within a factor of $(2k - 1)\\overline{d} + k$, where\n$\\overline{d}$ is the average degree of the input graph $G$. This approximation\nfactor is a generalization of Tur\\'an's approximation factor for Max-IS.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 09:52:09 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 15:04:37 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 03:33:38 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Mishra", "Sounaka", ""], ["Rajakrishnan", "Shijin", ""]]}, {"id": "1712.02965", "submitter": "Steven Kelk", "authors": "Janosch D\\\"ocker, Leo van Iersel, Steven Kelk, Simone Linz", "title": "Deciding the existence of a cherry-picking sequence is hard on two trees", "comments": "Fixed some tiny things. Accepted for journal publication", "journal-ref": "Discrete Applied Mathematics, 260:131-143, 2019", "doi": "10.1016/j.dam.2019.01.031", "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we show that deciding whether two rooted binary phylogenetic trees on\nthe same set of taxa permit a cherry-picking sequence, a special type of\nelimination order on the taxa, is NP-complete. This improves on an earlier\nresult which proved hardness for eight or more trees. Via a known equivalence\nbetween cherry-picking sequences and temporal phylogenetic networks, our result\nproves that it is NP-complete to determine the existence of a temporal\nphylogenetic network that contains topological embeddings of both trees. The\nhardness result also greatly strengthens previous inapproximability results for\nthe minimum temporal-hybridization number problem. This is the optimization\nversion of the problem where we wish to construct a temporal phylogenetic\nnetwork that topologically embeds two given rooted binary phylogenetic trees\nand that has a minimum number of indegree-2 nodes, which represent events such\nas hybridization and horizontal gene transfer. We end on a positive note,\npointing out that fixed parameter tractability results in this area are likely\nto ensure the continued relevance of the temporal phylogenetic network model.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 06:53:22 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 17:02:50 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["D\u00f6cker", "Janosch", ""], ["van Iersel", "Leo", ""], ["Kelk", "Steven", ""], ["Linz", "Simone", ""]]}, {"id": "1712.02977", "submitter": "Toru Niina", "authors": "Toru Niina", "title": "Periortree: An Extention of R-Tree for Periodic Boundary Conditions", "comments": "a very preliminary draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Searching spatial data is an important operation for scientific simulations\nwhich are performed mostly with periodic boundary conditions. An R-Tree is a\nwell known tree data structure used to contain spatial objects and it is\ncapable of answering to spatial searching queries in an efficient way. In this\npaper, a novel method to construct an R-Tree considering periodic boundary\nconditions is presented. Unlike existing methods, the proposed method works\nwithout any kind of extra objects or queries. Moreover, because this method\nreduces the volume of bounding box for each node under the periodic boundary\nconditions, it is expected to increase the overall efficiency. While the\nextension of an R-Tree is presented in this work, this method is not only\napplicable to an R-Tree but also to other data structures that use axis-aligned\nbounding boxes with periodic boundary conditions. The implementation is\navailable on GitHub.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 08:29:16 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Niina", "Toru", ""]]}, {"id": "1712.03158", "submitter": "Thijs Laarhoven", "authors": "Thijs Laarhoven", "title": "Graph-based time-space trade-offs for approximate near neighbors", "comments": "26 pages, 4 figures", "journal-ref": "34th International Symposium on Computational Geometry (SoCG), pp.\n  57:1-57:14, 2018", "doi": "10.4230/LIPIcs.SoCG.2018.57", "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take a first step towards a rigorous asymptotic analysis of graph-based\napproaches for finding (approximate) nearest neighbors in high-dimensional\nspaces, by analyzing the complexity of (randomized) greedy walks on the\napproximate near neighbor graph. For random data sets of size $n = 2^{o(d)}$ on\nthe $d$-dimensional Euclidean unit sphere, using near neighbor graphs we can\nprovably solve the approximate nearest neighbor problem with approximation\nfactor $c > 1$ in query time $n^{\\rho_q + o(1)}$ and space $n^{1 + \\rho_s +\no(1)}$, for arbitrary $\\rho_q, \\rho_s \\geq 0$ satisfying \\begin{align} (2c^2 -\n1) \\rho_q + 2 c^2 (c^2 - 1) \\sqrt{\\rho_s (1 - \\rho_s)} \\geq c^4. \\end{align}\nGraph-based near neighbor searching is especially competitive with hash-based\nmethods for small $c$ and near-linear memory, and in this regime the asymptotic\nscaling of a greedy graph-based search matches the recent optimal hash-based\ntrade-offs of Andoni-Laarhoven-Razenshteyn-Waingarten [SODA'17]. We further\nstudy how the trade-offs scale when the data set is of size $n =\n2^{\\Theta(d)}$, and analyze asymptotic complexities when applying these results\nto lattice sieving.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 16:26:22 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Laarhoven", "Thijs", ""]]}, {"id": "1712.03349", "submitter": "Sankardeep Chakraborty", "authors": "Sankardeep Chakraborty, Seungbum Jo, Srinivasa Rao Satti", "title": "Improved Space-efficient Linear Time Algorithms for Some Classical Graph\n  Problems", "comments": "This short article appeared in CTW 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note provides space-efficient linear time algorithms for computing\nbridges, topological sorting, and strongly connected components improving on\nseveral recent results of Elmasry et al. [STACS'15], Banerjee et al.\n[COCOON'16] and Chakraborty et al. [ISAAC'16]. En route, we also provide\nanother DFS implementation with weaker input graph representation assumption\nwithout compromising on the time and space bounds of the earlier results of\nBanerjee et al. [COCOON'16] and Kammer et al. [MFCS'16].\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 07:11:11 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Chakraborty", "Sankardeep", ""], ["Jo", "Seungbum", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "1712.03371", "submitter": "Adam Kasperski", "authors": "Adam Kasperski, Pawel Zielinski", "title": "Risk averse single machine scheduling - complexity and approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a class of single machine scheduling problems is considered. It\nis assumed that job processing times and due dates can be uncertain and they\nare specified in the form of discrete scenario set. A probability distribution\nin the scenario set is known. In order to choose a schedule some risk criteria\nsuch as the value at risk (VaR) an conditional value at risk (CVaR) are used.\nVarious positive and negative complexity results are provided for basic single\nmachine scheduling problems. In this paper new complexity results are shown and\nsome known complexity results are strengthen.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 10:28:26 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Kasperski", "Adam", ""], ["Zielinski", "Pawel", ""]]}, {"id": "1712.03554", "submitter": "Hector J. Garcia", "authors": "H\\'ector J. Garc\\'ia, Igor L. Markov", "title": "Simulation of Quantum Circuits via Stabilizer Frames", "comments": "15 pages, 18 figures, 3 tables", "journal-ref": "IEEE Transactions on Computers, vol. 64, no. 8, 2015", "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generic quantum-circuit simulation appears intractable for conventional\ncomputers and may be unnecessary because useful quantum circuits exhibit\nsignificant structure that can be exploited during simulation. For example,\nGottesman and Knill identified an important subclass, called stabilizer\ncircuits, which can be simulated efficiently using group-theory techniques and\ninsights from quantum physics. Realistic circuits enriched with quantum\nerror-correcting codes and fault-tolerant procedures are dominated by\nstabilizer subcircuits and contain a relatively small number of non-Clifford\ncomponents. Therefore, we develop new data structures and algorithms that\nfacilitate parallel simulation of such circuits. Stabilizer frames offer more\ncompact storage than previous approaches but require more sophisticated\nbookkeeping. Our implementation, called Quipu, simulates certain quantum\narithmetic circuits (e.g., reversible ripple-carry adders) in polynomial time\nand space for equal superpositions of $n$-qubits. On such instances, known\nlinear-algebraic simulation techniques, such as the (state-of-the-art)\nBDD-based simulator QuIDDPro, take exponential time. We simulate quantum\nFourier transform and quantum fault-tolerant circuits using Quipu, and the\nresults demonstrate that our stabilizer-based technique empirically outperforms\nQuIDDPro in all cases. While previous high-performance, structure-aware\nsimulations of quantum circuits were difficult to parallelize, we demonstrate\nthat Quipu can be parallelized with a nontrivial computational speedup.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 16:18:19 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Garc\u00eda", "H\u00e9ctor J.", ""], ["Markov", "Igor L.", ""]]}, {"id": "1712.03693", "submitter": "David Harvey", "authors": "David Harvey and Joris van der Hoeven", "title": "Faster integer and polynomial multiplication using cyclotomic\n  coefficient rings", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DS math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm that computes the product of two n-bit integers in\nO(n log n (4\\sqrt 2)^{log^* n}) bit operations. Previously, the best known\nbound was O(n log n 6^{log^* n}). We also prove that for a fixed prime p,\npolynomials in F_p[X] of degree n may be multiplied in O(n log n 4^{log^* n})\nbit operations; the previous best bound was O(n log n 8^{log^* n}).\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 09:37:57 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Harvey", "David", ""], ["van der Hoeven", "Joris", ""]]}, {"id": "1712.03714", "submitter": "Yann Strozecki", "authors": "Arnaud Mary and Yann Strozecki", "title": "Efficient enumeration of solutions produced by closure operations", "comments": "30 pages, 1 figure. Long version of the article arXiv:1509.05623 of\n  the same name which appeared in STACS 2016. Final version for DMTCS journal", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, Vol. 21 no. 3\n  , Discrete Algorithms (June 13, 2019) dmtcs:5549", "doi": "10.23638/DMTCS-21-3-22", "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of generating all elements obtained by\nthe saturation of an initial set by some operations. More precisely, we prove\nthat we can generate the closure of a boolean relation (a set of boolean\nvectors) by polymorphisms with a polynomial delay. Therefore we can compute\nwith polynomial delay the closure of a family of sets by any set of \"set\noperations\": union, intersection, symmetric difference, subsets, supersets\n$\\dots$). To do so, we study the $Membership_{\\mathcal{F}}$ problem: for a set\nof operations $\\mathcal{F}$, decide whether an element belongs to the closure\nby $\\mathcal{F}$ of a family of elements. In the boolean case, we prove that\n$Membership_{\\mathcal{F}}$ is in P for any set of boolean operations\n$\\mathcal{F}$. When the input vectors are over a domain larger than two\nelements, we prove that the generic enumeration method fails, since\n$Membership_{\\mathcal{F}}$ is NP-hard for some $\\mathcal{F}$. We also study the\nproblem of generating minimal or maximal elements of closures and prove that\nsome of them are related to well known enumeration problems such as the\nenumeration of the circuits of a matroid or the enumeration of maximal\nindependent sets of a hypergraph. This article improves on previous works of\nthe same authors.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 10:52:13 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 19:59:50 GMT"}, {"version": "v3", "created": "Thu, 18 Apr 2019 10:24:03 GMT"}, {"version": "v4", "created": "Wed, 5 Jun 2019 12:49:09 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Mary", "Arnaud", ""], ["Strozecki", "Yann", ""]]}, {"id": "1712.04043", "submitter": "Iyad Kanj", "authors": "Eduard Eiben, Iyad Kanj", "title": "How to navigate through obstacles?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of obstacles and two points, is there a path between the two\npoints that does not cross more than $k$ different obstacles? This is a\nfundamental problem that has undergone a tremendous amount of work. It is known\nto be NP-hard, even when the obstacles are very simple geometric shapes (e.g.,\nunit-length line segments). The problem can be generalized into the following\ngraph problem: Given a planar graph $G$ whose vertices are colored by color\nsets, two designated vertices $s, t \\in V(G)$, and $k \\in \\mathbb{N}$, is there\nan $s$-$t$ path in $G$ that uses at most $k$ colors? If each obstacle is\nconnected, the resulting graph satisfies the color-connectivity property,\nnamely that each color induces a connected subgraph.\n  We study the complexity and design algorithms for the above graph problem\nwith an eye on its geometric applications. We prove that without the\ncolor-connectivity property, the problem is W[SAT]-hard parameterized by $k$. A\ncorollary of this result is that, unless W[2] $=$ FPT, the problem cannot be\napproximated in FPT time to within a factor that is a function of $k$. By\ndescribing a generic plane embedding of the graph instances, we show that our\nhardness results translate to the geometric instances of the problem.\n  We then focus on graphs satisfying the color-connectivity property. By\nexploiting the planarity of the graph and the connectivity of the colors, we\ndevelop topological results to \"represent\" the valid $s$-$t$ paths containing\nsubsets of colors from any vertex $v$. We employ these results to design an FPT\nalgorithm for the problem parameterized by both $k$ and the treewidth of the\ngraph, and extend this result to obtain an FPT algorithm for the\nparameterization by both $k$ and the length of the path. The latter result\ndirectly implies previous FPT results for various obstacle shapes, such as unit\ndisks and fat regions.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 21:46:36 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Eiben", "Eduard", ""], ["Kanj", "Iyad", ""]]}, {"id": "1712.04127", "submitter": "Simone Linz", "authors": "Janosch D\\\"ocker and Simone Linz", "title": "On the existence of a cherry-picking sequence", "comments": "Accepted for publication in Theoretical Computer Science", "journal-ref": "Theoretical Computer Science, 714:36-50, 2018", "doi": "10.1016/j.tcs.2017.12.005", "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the minimum number of reticulation events that is required to\nsimultaneously embed a collection P of rooted binary phylogenetic trees into a\nso-called temporal network has been characterized in terms of cherry-picking\nsequences. Such a sequence is a particular ordering on the leaves of the trees\nin P. However, it is well-known that not all collections of phylogenetic trees\nhave a cherry-picking sequence. In this paper, we show that the problem of\ndeciding whether or not P has a cherry-picking sequence is NP-complete for when\nP contains at least eight rooted binary phylogenetic trees. Moreover, we use\nautomata theory to show that the problem can be solved in polynomial time if\nthe number of trees in P and the number of cherries in each such tree are\nbounded by a constant.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 04:42:44 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["D\u00f6cker", "Janosch", ""], ["Linz", "Simone", ""]]}, {"id": "1712.04131", "submitter": "Simone Linz", "authors": "Simone Linz and Charles Semple", "title": "Attaching leaves and picking cherries to characterise the hybridisation\n  number for a set of phylogenies", "comments": null, "journal-ref": "Advances in Applied Mathematics, 105:102-129, 2019", "doi": "10.1016/j.aam.2019.01.004", "report-no": null, "categories": "q-bio.PE cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the last decade, we have seen much progress towards characterising\nand computing the minimum hybridisation number for a set P of rooted\nphylogenetic trees. Roughly speaking, this minimum quantifies the number of\nhybridisation events needed to explain a set of phylogenetic trees by\nsimultaneously embedding them into a phylogenetic network. From a mathematical\nviewpoint, the notion of agreement forests is the underpinning concept for\nalmost all results that are related to calculating the minimum hybridisation\nnumber for when |P|=2. However, despite various attempts, characterising this\nnumber in terms of agreement forests for |P|>2 remains elusive. In this paper,\nwe characterise the minimum hybridisation number for when P is of arbitrary\nsize and consists of not necessarily binary trees. Building on our previous\nwork on cherry-picking sequences, we first establish a new characterisation to\ncompute the minimum hybridisation number in the space of tree-child networks.\nSubsequently, we show how this characterisation extends to the space of all\nrooted phylogenetic networks. Moreover, we establish a particular hardness\nresult that gives new insight into some of the limitations of agreement\nforests.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 05:00:34 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 00:59:14 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Linz", "Simone", ""], ["Semple", "Charles", ""]]}, {"id": "1712.04146", "submitter": "Salman Salloum", "authors": "Salman Salloum and Yulin He and Joshua Zhexue Huang and Xiaoliang\n  Zhang and Tamer Z. Emara and Chenghao Wei and Heping He", "title": "A Random Sample Partition Data Model for Big Data Analysis", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TII.2019.2912723", "report-no": null, "categories": "cs.DC cs.DS physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data sets must be carefully partitioned into statistically similar data\nsubsets that can be used as representative samples for big data analysis tasks.\nIn this paper, we propose the random sample partition (RSP) data model to\nrepresent a big data set as a set of non-overlapping data subsets, called RSP\ndata blocks, where each RSP data block has a probability distribution similar\nto the whole big data set. Under this data model, efficient block level\nsampling is used to randomly select RSP data blocks, replacing expensive record\nlevel sampling to select sample data from a big distributed data set on a\ncomputing cluster. We show how RSP data blocks can be employed to estimate\nstatistics of a big data set and build models which are equivalent to those\nbuilt from the whole big data set. In this approach, analysis of a big data set\nbecomes analysis of few RSP data blocks which have been generated in advance on\nthe computing cluster. Therefore, the new method for data analysis based on RSP\ndata blocks is scalable to big data.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 06:49:28 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 10:59:15 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Salloum", "Salman", ""], ["He", "Yulin", ""], ["Huang", "Joshua Zhexue", ""], ["Zhang", "Xiaoliang", ""], ["Emara", "Tamer Z.", ""], ["Wei", "Chenghao", ""], ["He", "Heping", ""]]}, {"id": "1712.04159", "submitter": "Niek Tax", "authors": "Niek Tax, Marlon Dumas", "title": "Mining Non-Redundant Local Process Models From Sequence Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential pattern mining techniques extract patterns corresponding to\nfrequent subsequences from a sequence database. A practical limitation of these\ntechniques is that they overload the user with too many patterns. Local Process\nModel (LPM) mining is an alternative approach coming from the field of process\nmining. While in traditional sequential pattern mining, a pattern describes one\nsubsequence, an LPM captures a set of subsequences. Also, while traditional\nsequential patterns only match subsequences that are observed in the sequence\ndatabase, an LPM may capture subsequences that are not explicitly observed, but\nthat are related to observed subsequences. In other words, LPMs generalize the\nbehavior observed in the sequence database. These properties make it possible\nfor a set of LPMs to cover the behavior of a much larger set of sequential\npatterns. Yet, existing LPM mining techniques still suffer from the pattern\nexplosion problem because they produce sets of redundant LPMs. In this paper,\nwe propose several heuristics to mine a set of non-redundant LPMs either from a\nset of redundant LPMs or from a set of sequential patterns. We empirically\ncompare the proposed heuristics between them and against existing (local)\nprocess mining techniques in terms of coverage, redundancy, and complexity of\nthe produced sets of LPMs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 08:03:50 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 06:51:54 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Tax", "Niek", ""], ["Dumas", "Marlon", ""]]}, {"id": "1712.04196", "submitter": "Deena P. Francis", "authors": "Deena P. Francis and Kumudha Raimond", "title": "Empirical Evaluation of Kernel PCA Approximation Methods in\n  Classification Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel Principal Component Analysis (KPCA) is a popular dimensionality\nreduction technique with a wide range of applications. However, it suffers from\nthe problem of poor scalability. Various approximation methods have been\nproposed in the past to overcome this problem. The Nystr\\\"om method, Randomized\nNonlinear Component Analysis (RNCA) and Streaming Kernel Principal Component\nAnalysis (SKPCA) were proposed to deal with the scalability issue of KPCA.\nDespite having theoretical guarantees, their performance in real world learning\ntasks have not been explored previously. In this work the evaluation of SKPCA,\nRNCA and Nystr\\\"om method for the task of classification is done for several\nreal world datasets. The results obtained indicate that SKPCA based features\ngave much better classification accuracy when compared to the other methods for\na very large dataset.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 10:02:08 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Francis", "Deena P.", ""], ["Raimond", "Kumudha", ""]]}, {"id": "1712.04217", "submitter": "Andreas Alpers", "authors": "Andreas Alpers and Peter Gritzmann", "title": "Dynamic Discrete Tomography", "comments": "In Press", "journal-ref": "Inverse Problems, 2018", "doi": "10.1088/1361-6420/aaa202", "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reconstructing the paths of a set of points over\ntime, where, at each of a finite set of moments in time the current positions\nof points in space are only accessible through some small number of their\nX-rays. This particular particle tracking problem, with applications, e.g., in\nplasma physics, is the basic problem in dynamic discrete tomography. We\nintroduce and analyze various different algorithmic models. In particular, we\ndetermine the computational complexity of the problem (and various of its\nrelatives) and derive algorithms that can be used in practice. As a byproduct\nwe provide new results on constrained variants of min-cost flow and matching\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 10:37:35 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 12:39:02 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Alpers", "Andreas", ""], ["Gritzmann", "Peter", ""]]}, {"id": "1712.04264", "submitter": "Muhammad Farhan", "authors": "Muhammad Farhan, Juvaria Tariq, Arif Zaman, Mudassir Shabbir, Imdad\n  Ullah Khan", "title": "Efficient Approximation Algorithms for String Kernel Based Sequence\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence classification algorithms, such as SVM, require a definition of\ndistance (similarity) measure between two sequences. A commonly used notion of\nsimilarity is the number of matches between $k$-mers ($k$-length subsequences)\nin the two sequences. Extending this definition, by considering two $k$-mers to\nmatch if their distance is at most $m$, yields better classification\nperformance. This, however, makes the problem computationally much more\ncomplex. Known algorithms to compute this similarity have computational\ncomplexity that render them applicable only for small values of $k$ and $m$. In\nthis work, we develop novel techniques to efficiently and accurately estimate\nthe pairwise similarity score, which enables us to use much larger values of\n$k$ and $m$, and get higher predictive accuracy. This opens up a broad avenue\nof applying this classification approach to audio, images, and text sequences.\nOur algorithm achieves excellent approximation performance with theoretical\nguarantees. In the process we solve an open combinatorial problem, which was\nposed as a major hindrance to the scalability of existing solutions. We give\nanalytical bounds on quality and runtime of our algorithm and report its\nempirical performance on real world biological and music sequences datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 12:33:58 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Farhan", "Muhammad", ""], ["Tariq", "Juvaria", ""], ["Zaman", "Arif", ""], ["Shabbir", "Mudassir", ""], ["Khan", "Imdad Ullah", ""]]}, {"id": "1712.04544", "submitter": "Mark Scanlon", "authors": "David Lillis, Frank Breitinger and Mark Scanlon", "title": "Hierarchical Bloom Filter Trees for Approximate Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bytewise approximate matching algorithms have in recent years shown\nsignificant promise in de- tecting files that are similar at the byte level.\nThis is very useful for digital forensic investigators, who are regularly faced\nwith the problem of searching through a seized device for pertinent data. A\ncommon scenario is where an investigator is in possession of a collection of\n\"known-illegal\" files (e.g. a collection of child abuse material) and wishes to\nfind whether copies of these are stored on the seized device. Approximate\nmatching addresses shortcomings in traditional hashing, which can only find\nidentical files, by also being able to deal with cases of merged files,\nembedded files, partial files, or if a file has been changed in any way.\n  Most approximate matching algorithms work by comparing pairs of files, which\nis not a scalable approach when faced with large corpora. This paper\ndemonstrates the effectiveness of using a \"Hierarchical Bloom Filter Tree\"\n(HBFT) data structure to reduce the running time of\ncollection-against-collection matching, with a specific focus on the MRSH-v2\nalgorithm. Three experiments are discussed, which explore the effects of\ndifferent configurations of HBFTs. The proposed approach dramatically reduces\nthe number of pairwise comparisons required, and demonstrates substantial speed\ngains, while maintaining effectiveness.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 21:44:18 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Lillis", "David", ""], ["Breitinger", "Frank", ""], ["Scanlon", "Mark", ""]]}, {"id": "1712.04581", "submitter": "Nikhil Bansal", "authors": "Nikhil Bansal and Anupam Gupta", "title": "Potential-Function Proofs for First-Order Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note discusses proofs for convergence of first-order methods based on\nsimple potential-function arguments. We cover methods like gradient descent\n(for both smooth and non-smooth settings), mirror descent, and some accelerated\nvariants.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 01:10:13 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 21:55:56 GMT"}, {"version": "v3", "created": "Sun, 2 Jun 2019 19:36:25 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Bansal", "Nikhil", ""], ["Gupta", "Anupam", ""]]}, {"id": "1712.04637", "submitter": "Sanjeev Saxena", "authors": "Sanjeev Saxena", "title": "Ellipsoid Method for Linear Programming made simple", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, ellipsoid method for linear programming is derived using only\nminimal knowledge of algebra and matrices. Unfortunately, most authors first\ndescribe the algorithm, then later prove its correctness, which requires a good\nknowledge of linear algebra.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 07:35:17 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Saxena", "Sanjeev", ""]]}, {"id": "1712.04886", "submitter": "Dominik Kempa", "authors": "Dominik Kempa", "title": "Optimal Construction of Compressed Indexes for Highly Repetitive Texts", "comments": null, "journal-ref": null, "doi": "10.1137/1.9781611975482.82", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose algorithms that, given the input string of length $n$ over integer\nalphabet of size $\\sigma$, construct the Burrows-Wheeler transform (BWT), the\npermuted longest-common-prefix (PLCP) array, and the LZ77 parsing in\n$O(n/\\log_{\\sigma}n+r\\,{\\rm polylog}\\,n)$ time and working space, where $r$ is\nthe number of runs in the BWT of the input. These are the essential components\nof many compressed indexes such as compressed suffix tree, FM-index, and\ngrammar and LZ77-based indexes, but also find numerous applications in sequence\nanalysis and data compression. The value of $r$ is a common measure of\nrepetitiveness that is significantly smaller than $n$ if the string is highly\nrepetitive. Since just accessing every symbol of the string requires\n$\\Omega(n/\\log_{\\sigma}n)$ time, the presented algorithms are time and space\noptimal for inputs satisfying the assumption $n/r\\in\\Omega({\\rm polylog}\\,n)$\non the repetitiveness. For such inputs our result improves upon the currently\nfastest general algorithms of Belazzougui (STOC 2014) and Munro et al. (SODA\n2017) which run in $O(n)$ time and use $O(n/\\log_{\\sigma} n)$ working space. We\nalso show how to use our techniques to obtain optimal solutions on highly\nrepetitive data for other fundamental string processing problems such as:\nLyndon factorization, construction of run-length compressed suffix arrays, and\nsome classical \"textbook\" problems such as computing the longest substring\noccurring at least some fixed number of times.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 17:56:24 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 13:05:52 GMT"}, {"version": "v3", "created": "Sat, 27 Jan 2018 21:30:19 GMT"}, {"version": "v4", "created": "Sat, 17 Mar 2018 23:40:12 GMT"}, {"version": "v5", "created": "Mon, 9 Apr 2018 15:51:48 GMT"}, {"version": "v6", "created": "Sat, 21 Apr 2018 19:20:09 GMT"}, {"version": "v7", "created": "Fri, 17 May 2019 15:38:16 GMT"}, {"version": "v8", "created": "Mon, 20 May 2019 01:00:47 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Kempa", "Dominik", ""]]}, {"id": "1712.05010", "submitter": "Panos Giannopoulos", "authors": "\\'Edouard Bonnet, Panos Giannopoulos, Eun Jung Kim, Pawe{\\l}\n  Rz\\k{a}\\.zewski, Florian Sikora", "title": "QPTAS and Subexponential Algorithm for Maximum Clique on Disk Graphs", "comments": "20 pages, 12 figures, to appear at SoCG 2018. This is the full\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A (unit) disk graph is the intersection graph of closed (unit) disks in the\nplane. Almost three decades ago, an elegant polynomial-time algorithm was found\nfor \\textsc{Maximum Clique} on unit disk graphs [Clark, Colbourn, Johnson;\nDiscrete Mathematics '90]. Since then, it has been an intriguing open question\nwhether or not tractability can be extended to general disk graphs. We show the\nrather surprising structural result that a disjoint union of cycles is the\ncomplement of a disk graph if and only if at most one of those cycles is of odd\nlength. From that, we derive the first QPTAS and subexponential algorithm\nrunning in time $2^{\\tilde{O}(n^{2/3})}$ for \\textsc{Maximum Clique} on disk\ngraphs. In stark contrast, \\textsc{Maximum Clique} on intersection graphs of\nfilled ellipses or filled triangles is unlikely to have such algorithms, even\nwhen the ellipses are close to unit disks. Indeed, we show that there is a\nconstant approximation which is not attainable even in time\n$2^{n^{1-\\varepsilon}}$, unless the Exponential Time Hypothesis fails.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 21:10:13 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 16:25:35 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Giannopoulos", "Panos", ""], ["Kim", "Eun Jung", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""], ["Sikora", "Florian", ""]]}, {"id": "1712.05012", "submitter": "Morad Behandish", "authors": "Pouya Tavousi, Morad Behandish, Horea T. Ilies, and Kazem Kazerounian", "title": "Protofold II: Enhanced Model and Implementation for Kinetostatic Protein\n  Folding", "comments": "Shorter versions were presented in two conference papers in ASME\n  International Design Engineering Technical Conferences (IDETC'2013)", "journal-ref": "ASME Transactions, Journal of Nanotechnology in Engineering and\n  Medicine, 6(3), p.034601, 2016", "doi": "10.1115/1.4032759", "report-no": "CDL-TR-16-03", "categories": "cs.CE cs.DC cs.DS cs.RO q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reliable prediction of 3D protein structures from sequence data remains a\nbig challenge due to both theoretical and computational difficulties. We have\npreviously shown that our kinetostatic compliance method (KCM) implemented into\nthe Protofold package can overcome some of the key difficulties faced by other\nde novo structure prediction methods, such as the very small time steps\nrequired by the molecular dynamics (MD) approaches or the very large number of\nsamples needed by the Monte Carlo (MC) sampling techniques. In this article, we\nimprove the free energy formulation used in Protofold by including the\ntypically underrated entropic effects, imparted due to differences in\nhydrophobicity of the chemical groups, which dominate the folding of most\nwater-soluble proteins. In addition to the model enhancement, we revisit the\nnumerical implementation by redesigning the algorithms and introducing\nefficient data structures that reduce the expected complexity from quadratic to\nlinear. Moreover, we develop and optimize parallel implementations of the\nalgorithms on both central and graphics processing units (CPU/GPU) achieving\nspeed-ups up to two orders of magnitude on the GPU. Our simulations are\nconsistent with the general behavior observed in the folding process in aqueous\nsolvent, confirming the effectiveness of model improvements. We report on the\nfolding process at multiple levels; namely, the formation of secondary\nstructural elements and tertiary interactions between secondary elements or\nacross larger domains. We also observe significant enhancements in running\ntimes that make the folding simulation tractable for large molecules.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 11:36:29 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Tavousi", "Pouya", ""], ["Behandish", "Morad", ""], ["Ilies", "Horea T.", ""], ["Kazerounian", "Kazem", ""]]}, {"id": "1712.05020", "submitter": "Trevor Brown", "authors": "Trevor Brown", "title": "B-slack trees: Highly Space Efficient B-trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  B-slack trees, a subclass of B-trees that have substantially better\nworst-case space complexity, are introduced. They store $n$ keys in height\n$O(\\log_b n)$, where $b$ is the maximum node degree. Updates can be performed\nin $O(\\log_{\\frac b 2} n)$ amortized time. A relaxed balance version, which is\nwell suited for concurrent implementation, is also presented.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 21:51:02 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 16:07:02 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Brown", "Trevor", ""]]}, {"id": "1712.05218", "submitter": "Thomas Bosman", "authors": "Thomas Bosman, Martijn van Ee, Yang Jiao, Alberto\n  Marchetti-Spaccamela, R. Ravi, Leen Stougie", "title": "Approximation Algorithms for Replenishment Problems with Fixed Turnover\n  Times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study a class of optimization problems we coin replenishment\nproblems with fixed turnover times: a very natural model that has received\nlittle attention in the literature. Nodes with capacity for storing a certain\ncommodity are located at various places; at each node the commodity depletes\nwithin a certain time, the turnover time, which is constant but can vary\nbetween locations. Nodes should never run empty, and to prevent this we may\nschedule nodes for replenishment every day. The natural feature that makes this\nproblem interesting is that we may schedule a replenishment (well) before a\nnode becomes empty, but then the next replenishment will be due earlier also.\nThis added workload needs to be balanced against the cost of routing vehicles\nto do the replenishments. In this paper, we focus on the aspect of minimizing\nrouting costs. However, the framework of recurring tasks, in which the next job\nof a task must be done within a fixed amount of time after the previous one is\nmuch more general and gives an adequate model for many practical situations.\n  Note that our problem has an infinite time horizon. However, it can be fully\ncharacterized by a compact input, containing only the location of each store\nand a turnover time. This makes determining its computational complexity highly\nchallenging and indeed it remains essentially unresolved. We study the problem\nfor two objectives: min-avg minimizes the average tour length and min-max\nminimizes the maximum tour length over all days. For min-max we derive a\nlogarithmic factor approximation for the problem on general metrics and a\n6-approximation for the problem on trees, for which we have a proof of\nNP-hardness. For min-avg we present a logarithmic approximation on general\nmetrics, 2-approximation for trees, and a pseudopolynomial time algorithm for\nthe line. Many intriguing problems remain open.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 13:35:09 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Bosman", "Thomas", ""], ["van Ee", "Martijn", ""], ["Jiao", "Yang", ""], ["Marchetti-Spaccamela", "Alberto", ""], ["Ravi", "R.", ""], ["Stougie", "Leen", ""]]}, {"id": "1712.05222", "submitter": "Georgia Kouyialis", "authors": "Georgia Kouyialis and Ruth Misener", "title": "Symmetry Detection for Quadratically Constrained Quadratic Programs\n  Using Binary Layered Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry in mathematical programming may lead to a multiplicity of solutions.\nIn nonconvex optimisation, it can negatively affect the performance of the\nbranch-and-bound algorithm. Symmetry may induce large search trees with\nmultiple equivalent solutions, i.e. with the same optimal value. Dealing with\nsymmetry requires detecting and classifying it first. This work develops\nmethods for detecting groups of symmetry in the formulation of quadratically\nconstrained quadratic optimisation problems via adjacency matrices. Using graph\ntheory, we transform these matrices into Binary Layered Graphs (BLG) and enter\nthem into the software package nauty. Nauty generates important symmetric\nproperties of the original problem.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 13:51:52 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 17:32:00 GMT"}, {"version": "v3", "created": "Tue, 23 Jan 2018 12:11:57 GMT"}, {"version": "v4", "created": "Sun, 20 Jan 2019 21:23:05 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Kouyialis", "Georgia", ""], ["Misener", "Ruth", ""]]}, {"id": "1712.05450", "submitter": "Morteza Zadimoghaddam", "authors": "Nitish Korula, Vahab Mirrokni, Morteza Zadimoghaddam", "title": "Online Submodular Welfare Maximization: Greedy Beats 1/2 in Random Order", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the Submodular Welfare Maximization (SWM) problem, the input consists of a\nset of $n$ items, each of which must be allocated to one of $m$ agents. Each\nagent $\\ell$ has a valuation function $v_\\ell$, where $v_\\ell(S)$ denotes the\nwelfare obtained by this agent if she receives the set of items $S$. The\nfunctions $v_\\ell$ are all submodular; as is standard, we assume that they are\nmonotone and $v_\\ell(\\emptyset) = 0$. The goal is to partition the items into\n$m$ disjoint subsets $S_1, S_2, \\ldots S_m$ in order to maximize the social\nwelfare, defined as $\\sum_{\\ell = 1}^m v_\\ell(S_\\ell)$.\n  In this paper, we consider the online version of SWM. Here, items arrive one\nat a time in an online manner; when an item arrives, the algorithm must make an\nirrevocable decision about which agent to assign it to before seeing any\nsubsequent items. This problem is motivated by applications to Internet\nadvertising, where user ad impressions must be allocated to advertisers whose\nvalue is a submodular function of the set of users / impressions they receive.\nIn the random order model, the adversary can construct a worst-case set of\nitems and valuations, but does not control the order in which the items arrive;\ninstead, they are assumed to arrive in a random order. Obtaining a competitive\nratio of $1/2 + \\Omega(1)$ for the random order model has been an important\nopen problem for several years. We solve this open problem by demonstrating\nthat the greedy algorithm has a competitive ratio of at least $0.505$ for the\nOnline Submodular Welfare Maximization problem in the random order model. For\nspecial cases of submodular functions including weighted matching, weighted\ncoverage functions and a broader class of \"second-order supermodular\"\nfunctions, we provide a different analysis that gives a competitive ratio of\n$0.51$.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 20:47:22 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Korula", "Nitish", ""], ["Mirrokni", "Vahab", ""], ["Zadimoghaddam", "Morteza", ""]]}, {"id": "1712.05517", "submitter": "Takeyuki Tamura", "authors": "Tatsuya Akutsu, Jesper Jansson, Ruiming Li, Atsuhiro Takasu, Takeyuki\n  Tamura", "title": "New and Improved Algorithms for Unordered Tree Inclusion", "comments": "22 pages, 9 figures. To appear in Theoretical Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tree inclusion problem is, given two node-labeled trees $P$ and $T$ (the\n``pattern tree'' and the ``target tree''), to locate every minimal subtree in\n$T$ (if any) that can be obtained by applying a sequence of node insertion\noperations to $P$. Although the ordered tree inclusion problem is solvable in\npolynomial time, the unordered tree inclusion problem is NP-hard. The currently\nfastest algorithm for the latter is a classic algorithm by Kilpel\\\"{a}inen and\nMannila from 1995 that runs in $O(2^{2d} mn)$ time, where $m$ and $n$ are the\nsizes of the pattern and target trees, respectively, and $d$ is the degree of\nthe pattern tree. Here, we develop a new algorithm that runs in $O(2^{d} mn^2)$\ntime, improving the exponential factor from $2^{2d}$ to $2^d$ by considering a\nparticular type of ancestor-descendant relationships that is suitable for\ndynamic programming. We also study restricted variants of the unordered tree\ninclusion problem.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 03:15:35 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 03:58:09 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Akutsu", "Tatsuya", ""], ["Jansson", "Jesper", ""], ["Li", "Ruiming", ""], ["Takasu", "Atsuhiro", ""], ["Tamura", "Takeyuki", ""]]}, {"id": "1712.05822", "submitter": "Markus Lohrey", "authors": "Markus Lohrey, Carl Philipp Reh, Kurt Sieber", "title": "Optimal top dag compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that for a given ordered node-labelled tree of size $n$ and with\n$s$ many different node labels, one can construct in linear time a top dag of\nheight $O(\\log n)$ and size $O(n / \\log_\\sigma n) \\cap O(d \\cdot \\log n)$,\nwhere $\\sigma = \\max\\{ 2, s\\}$ and $d$ is the size of the minimal dag. The size\nbound $O(n / \\log_\\sigma n)$ is optimal and improves on previous bounds.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 19:46:47 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Lohrey", "Markus", ""], ["Reh", "Carl Philipp", ""], ["Sieber", "Kurt", ""]]}, {"id": "1712.05825", "submitter": "Nate Veldt", "authors": "Nate Veldt, David Gleich, Anthony Wirth", "title": "Unifying Sparsest Cut, Cluster Deletion, and Modularity Clustering\n  Objectives with Correlation Clustering", "comments": null, "journal-ref": null, "doi": "10.1145/3178876.3186110", "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph clustering, or community detection, is the task of identifying groups\nof closely related objects in a large network. In this paper we introduce a new\ncommunity-detection framework called LambdaCC that is based on a specially\nweighted version of correlation clustering. A key component in our methodology\nis a clustering resolution parameter, $\\lambda$, which implicitly controls the\nsize and structure of clusters formed by our framework. We show that, by\nincreasing this parameter, our objective effectively interpolates between two\ndifferent strategies in graph clustering: finding a sparse cut and forming\ndense subgraphs. Our methodology unifies and generalizes a number of other\nimportant clustering quality functions including modularity, sparsest cut, and\ncluster deletion, and places them all within the context of an optimization\nproblem that has been well studied from the perspective of approximation\nalgorithms. Our approach is particularly relevant in the regime of finding\ndense clusters, as it leads to a 2-approximation for the cluster deletion\nproblem. We use our approach to cluster several graphs, including large\ncollaboration networks and social networks.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 20:04:39 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 00:29:32 GMT"}, {"version": "v3", "created": "Sat, 14 Jul 2018 00:29:25 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Veldt", "Nate", ""], ["Gleich", "David", ""], ["Wirth", "Anthony", ""]]}, {"id": "1712.05876", "submitter": "Zsuzsanna Lipt\\'ak", "authors": "Ferdinando Cicalese and Zsuzsanna Lipt\\'ak and Massimiliano Rossi", "title": "Bubble-Flip---A New Generation Algorithm for Prefix Normal Words", "comments": "30 pages, 3 figures, accepted in Theoret. Comp. Sc.. This is the\n  journal version of the paper with the same title at LATA 2018 (12th\n  International Conference on Language and Automata Theory and Applications,\n  Tel Aviv, April 9-11, 2018)", "journal-ref": null, "doi": "10.1016/j.tcs.2018.06.021", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new recursive generation algorithm for prefix normal words.\nThese are binary strings with the property that no substring has more 1s than\nthe prefix of the same length. The new algorithm uses two operations on binary\nstrings, which exploit certain properties of prefix normal words in a smart\nway. We introduce infinite prefix normal words and show that one of the\noperations used by the algorithm, if applied repeatedly to extend the string,\nproduces an ultimately periodic infinite word, which is prefix normal.\nMoreover, based on the original finite word, we can predict both the length and\nthe density of an ultimate period of this infinite word.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 23:39:45 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 21:08:06 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 18:19:39 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Cicalese", "Ferdinando", ""], ["Lipt\u00e1k", "Zsuzsanna", ""], ["Rossi", "Massimiliano", ""]]}, {"id": "1712.06039", "submitter": "Aditya Potukuchi", "authors": "Swastik Kopparty and Aditya Potukuchi", "title": "Syndrome decoding of Reed-Muller codes and tensor decomposition over\n  finite fields", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reed-Muller codes are some of the oldest and most widely studied\nerror-correcting codes, of interest for both their algebraic structure as well\nas their many algorithmic properties. A recent beautiful result of Saptharishi,\nShpilka and Volk showed that for binary Reed-Muller codes of length $n$ and\ndistance $d = O(1)$, one can correct $\\operatorname{polylog}(n)$ random errors\nin $\\operatorname{poly}(n)$ time (which is well beyond the worst-case error\ntolerance of $O(1)$).\n  In this paper, we consider the problem of `syndrome decoding' Reed-Muller\ncodes from random errors. More specifically, given the\n$\\operatorname{polylog}(n)$-bit long syndrome vector of a codeword corrupted in\n$\\operatorname{polylog}(n)$ random coordinates, we would like to compute the\nlocations of the codeword corruptions. This problem turns out to be equivalent\nto a basic question about computing tensor decomposition of random low-rank\ntensors over finite fields.\n  Our main result is that syndrome decoding of Reed-Muller codes (and the\nequivalent tensor decomposition problem) can be solved efficiently, i.e., in\n$\\operatorname{polylog}(n)$ time. We give two algorithms for this problem:\n  1. The first algorithm is a finite field variant of a classical algorithm for\ntensor decomposition over real numbers due to Jennrich. This also gives an\nalternate proof for the main result of Saptharishi et al.\n  2. The second algorithm is obtained by implementing the steps of the\nBerlekamp-Welch-style decoding algorithm of Saptharishi et al. in\nsublinear-time. The main new ingredient is an algorithm for solving certain\nkinds of systems of polynomial equations.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 01:12:00 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Kopparty", "Swastik", ""], ["Potukuchi", "Aditya", ""]]}, {"id": "1712.06309", "submitter": "Dmitry Gribanov", "authors": "D.V. Gribanov, D.S. Malyshev, P.M. Pardalos, S.I. Veselov", "title": "FPT-algorithms for some problems related to integer programming", "comments": "arXiv admin note: text overlap with arXiv:1710.00321 From author:\n  some minor corrections has been done", "journal-ref": "J Comb Optim (2018) 35: 1128", "doi": "10.1007/s10878-018-0264-z", "report-no": null, "categories": "math.OC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present FPT-algorithms for special cases of the shortest\nlattice vector, integer linear programming, and simplex width computation\nproblems, when matrices included in the problems' formulations are near square.\nThe parameter is the maximum absolute value of rank minors of the corresponding\nmatrices. Additionally, we present FPT-algorithms with respect to the same\nparameter for the problems, when the matrices have no singular rank\nsub-matrices.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 09:30:12 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 04:37:37 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Gribanov", "D. V.", ""], ["Malyshev", "D. S.", ""], ["Pardalos", "P. M.", ""], ["Veselov", "S. I.", ""]]}, {"id": "1712.06473", "submitter": "Pan Peng", "authors": "Gramoz Goranci, Monika Henzinger, Pan Peng", "title": "The Power of Vertex Sparsifiers in Dynamic Graph Algorithms", "comments": "A preliminary version was presented at the 25th Annual European\n  Symposium on Algorithms (ESA 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new algorithmic framework for designing dynamic graph\nalgorithms in minor-free graphs, by exploiting the structure of such graphs and\na tool called vertex sparsification, which is a way to compress large graphs\ninto small ones that well preserve relevant properties among a subset of\nvertices and has previously mainly been used in the design of approximation\nalgorithms.\n  Using this framework, we obtain a Monte Carlo randomized fully dynamic\nalgorithm for $(1+\\varepsilon)$-approximating the energy of electrical flows in\n$n$-vertex planar graphs with $\\tilde{O}(r\\varepsilon^{-2})$ worst-case update\ntime and $\\tilde{O}((r+\\frac{n}{\\sqrt{r}})\\varepsilon^{-2})$ worst-case query\ntime, for any $r$ larger than some constant. For $r=n^{2/3}$, this gives\n$\\tilde{O}(n^{2/3}\\varepsilon^{-2})$ update time and\n$\\tilde{O}(n^{2/3}\\varepsilon^{-2})$ query time. We also extend this algorithm\nto work for minor-free graphs with similar approximation and running time\nguarantees. Furthermore, we illustrate our framework on the all-pairs max flow\nand shortest path problems by giving corresponding dynamic algorithms in\nminor-free graphs with both sublinear update and query times. To the best of\nour knowledge, our results are the first to systematically establish such a\nconnection between dynamic graph algorithms and vertex sparsification.\n  We also present both upper bound and lower bound for maintaining the energy\nof electrical flows in the incremental subgraph model, where updates consist of\nonly vertex activations, which might be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 15:36:10 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Goranci", "Gramoz", ""], ["Henzinger", "Monika", ""], ["Peng", "Pan", ""]]}, {"id": "1712.06481", "submitter": "Ren\\'e van Bevern", "authors": "Matthias Bentert and Ren\\'e van Bevern and Rolf Niedermeier", "title": "Inductive $k$-independent graphs and $c$-colorable subgraphs in\n  scheduling: A review", "comments": null, "journal-ref": "Journal of Scheduling 22(1):3-20, 2019", "doi": "10.1007/s10951-018-0595-8", "report-no": null, "categories": "cs.DM cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inductive $k$-independent graphs generalize chordal graphs and have recently\nbeen advocated in the context of interference-avoiding wireless communication\nscheduling. The NP-hard problem of finding maximum-weight induced $c$-colorable\nsubgraphs, which is a generalization of finding maximum independent sets,\nnaturally occurs when selecting $c$ sets of pairwise non-conflicting jobs\n(modeled as graph vertices). We investigate the parameterized complexity of\nthis problem on inductive $k$-independent graphs. We show that the Independent\nSet problem is W[1]-hard even on 2-simplicial 3-minoes---a subclass of\ninductive 2-independent graphs. In contrast, we prove that the more general\nMaximum $c$-Colorable Subgraph problem is fixed-parameter tractable on\nedge-wise unions of cluster and chordal graphs, which are 2-simplicial. In both\ncases, the parameter is the solution size. Aside from this, we survey other\ngraph classes between inductive 1-inductive and inductive 2-inductive graphs\nwith applications in scheduling.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 15:53:03 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 01:41:03 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 12:29:30 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Bentert", "Matthias", ""], ["van Bevern", "Ren\u00e9", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "1712.06587", "submitter": "Anthony Zaleski", "authors": "Anthony Zaleski", "title": "Solving satisfiability using inclusion-exclusion", "comments": "11 pages, 3 figures, Maple package available on author's site", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using Maple, we implement a SAT solver based on the principle of\ninclusion-exclusion and the Bonferroni inequalities. Using randomly generated\ninput, we investigate the performance of our solver as a function of the number\nof variables and number of clauses. We also test it against Maple's built-in\ntautology procedure. Finally, we implement the Lov\\'asz local lemma with Maple\nand discuss its applicability to SAT.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 22:05:05 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Zaleski", "Anthony", ""]]}, {"id": "1712.06690", "submitter": "Michael O'Brien", "authors": "Michael P. O'Brien and Blair D. Sullivan", "title": "An Experimental Evaluation of a Bounded Expansion Algorithmic Pipeline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work has suggested that the structural restrictions of graphs from\nclasses of bounded expansion--locally dense pockets in a globally sparse\ngraph--naturally coincide with common properties of real-world networks such as\nclustering and heavy-tailed degree distributions. As such, fixed-parameter\ntractable algorithms for bounded expansion classes may offer a promising\nframework for network analysis where other approaches have struggled to scale.\nHowever, there has been little work done in implementing and evaluating the\nperformance of these structure-based algorithms. To this end we introduce\nCONCUSS, a proof-of-concept implementation of a generic algorithmic pipeline\nfor classes of bounded expansion. In particular, we focus on using CONCUSS for\nsubgraph isomorphism counting (also called motif or graphlet counting), which\nhas been used extensively as a tool for analyzing biological and social\nnetworks. Through a broad set of experiments we first evaluate the interactions\nbetween implementation/engineering choices at multiple stages of the pipeline\nand their effects on overall run time. From there, we establish viability of\nthe bounded expansion framework by demonstrating that in some scenarios CONCUSS\nachieves run times competitive with a popular algorithm for subgraph\nisomorphism counting that does not exploit graph structure. Finally, we\nempirically identify two particular ways in which future theoretical advances\ncould alleviate bottlenecks in the algorithmic pipeline.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 21:58:11 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 15:32:39 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["O'Brien", "Michael P.", ""], ["Sullivan", "Blair D.", ""]]}, {"id": "1712.06706", "submitter": "Henning Bruhn", "authors": "Henning Bruhn and Oliver Schaudt", "title": "Fast Algorithms for Delta-Separated Sparsity Projection", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a fast approximation algorithm for the $\\Delta$-separated\nsparsity projection problem. The $\\Delta$-separated sparsity model was\nintroduced by Hegde, Duarte and Cevher (2009) to capture the firing process of\na single Poisson neuron with absolute refractoriness. The running time of our\nprojection algorithm is linear for an arbitrary (but fixed) precision and it is\nboth a head and a tail approximation. This solves a problem of Hegde, Indyk and\nSchmidt (2015). We also describe how our algorithm fits into the approximate\nmodel iterative hard tresholding framework of Hegde, Indyk and Schmidt (2014)\nthat allows to recover $\\Delta$-separated sparse signals from noisy random\nlinear measurements. The resulting recovery algorithm is substantially faster\nthan the existing one, at least for large data sets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 22:48:22 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Bruhn", "Henning", ""], ["Schaudt", "Oliver", ""]]}, {"id": "1712.06763", "submitter": "Fl\\'avio Miyazawa", "authors": "Y. Kohayakawa, F. K. Miyazawa, Y. Wakabayashi", "title": "A tight lower bound for an online hypercube packing problem and bounds\n  for prices of anarchy of a related game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a tight lower bound on the asymptotic performance ratio $\\rho$ of\nthe bounded space online $d$-hypercube bin packing problem, solving an open\nquestion raised in 2005. In the classic $d$-hypercube bin packing problem, we\nare given a sequence of $d$-dimensional hypercubes and we have an unlimited\nnumber of bins, each of which is a $d$-dimensional unit hypercube. The goal is\nto pack (orthogonally) the given hypercubes into the minimum possible number of\nbins, in such a way that no two hypercubes in the same bin overlap. The bounded\nspace online $d$-hypercube bin packing problem is a variant of the\n$d$-hypercube bin packing problem, in which the hypercubes arrive online and\neach one must be packed in an open bin without the knowledge of the next\nhypercubes. Moreover, at each moment, only a constant number of open bins are\nallowed (whenever a new bin is used, it is considered open, and it remains so\nuntil it is considered closed, in which case, it is not allowed to accept new\nhypercubes). Epstein and van Stee [SIAM J. Comput. 35 (2005), no. 2, 431-448]\nshowed that $\\rho$ is $\\Omega(\\log d)$ and $O(d/\\log d)$, and conjectured that\nit is $\\Theta(\\log d)$. We show that $\\rho$ is in fact $\\Theta(d/\\log d)$. To\nobtain this result, we elaborate on some ideas presented by those authors, and\ngo one step further showing how to obtain better (offline) packings of certain\nspecial instances for which one knows how many bins any bounded space algorithm\nhas to use. Our main contribution establishes the existence of such packings,\nfor large enough $d$, using probabilistic arguments. Such packings also lead to\nlower bounds for the prices of anarchy of the selfish $d$-hypercube bin packing\ngame. We present a lower bound of $\\Omega(d/\\log d)$ for the pure price of\nanarchy of this game, and we also give a lower bound of $\\Omega(\\log d)$ for\nits strong price of anarchy.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 03:15:14 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Kohayakawa", "Y.", ""], ["Miyazawa", "F. K.", ""], ["Wakabayashi", "Y.", ""]]}, {"id": "1712.06865", "submitter": "Anup Bhattacharya", "authors": "Nir Ailon, Anup Bhattacharya, Ragesh Jaiswal", "title": "Approximate Correlation Clustering Using Same-Cluster Queries", "comments": "To appear in LATIN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ashtiani et al. (NIPS 2016) introduced a semi-supervised framework for\nclustering (SSAC) where a learner is allowed to make same-cluster queries. More\nspecifically, in their model, there is a query oracle that answers queries of\nthe form given any two vertices, do they belong to the same optimal cluster?.\nAshtiani et al. showed the usefulness of such a query framework by giving a\npolynomial time algorithm for the k-means clustering problem where the input\ndataset satisfies some separation condition. Ailon et al. extended the above\nwork to the approximation setting by giving an efficient (1+\\eps)-approximation\nalgorithm for k-means for any small \\eps > 0 and any dataset within the SSAC\nframework. In this work, we extend this line of study to the correlation\nclustering problem. Correlation clustering is a graph clustering problem where\npairwise similarity (or dissimilarity) information is given for every pair of\nvertices and the objective is to partition the vertices into clusters that\nminimise the disagreement (or maximises agreement) with the pairwise\ninformation given as input. These problems are popularly known as MinDisAgree\nand MaxAgree problems, and MinDisAgree[k] and MaxAgree[k] are versions of these\nproblems where the number of optimal clusters is at most k. There exist\nPolynomial Time Approximation Schemes (PTAS) for MinDisAgree[k] and MaxAgree[k]\nwhere the approximation guarantee is (1+\\eps) for any small \\eps and the\nrunning time is polynomial in the input parameters but exponential in k and\n1/\\eps. We obtain an (1+\\eps)-approximation algorithm for any small \\eps with\nrunning time that is polynomial in the input parameters and also in k and\n1/\\eps. We also give non-trivial upper and lower bounds on the number of\nsame-cluster queries, the lower bound being based on the Exponential Time\nHypothesis (ETH).\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 11:04:48 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Ailon", "Nir", ""], ["Bhattacharya", "Anup", ""], ["Jaiswal", "Ragesh", ""]]}, {"id": "1712.06970", "submitter": "Cl\\'emence Magnien", "authors": "Tiphaine Viard, Cl\\'emence Magnien and Matthieu Latapy", "title": "Enumerating maximal cliques in link streams with durations", "comments": null, "journal-ref": "Information Processing Letters, 133, 44-48, 2018", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link streams model interactions over time, and a clique in a link stream is\ndefined as a set of nodes and a time interval such that all pairs of nodes in\nthis set interact permanently during this time interval. This notion was\nintroduced recently in the case where interactions are instantaneous. We\ngeneralize it to the case of interactions with durations and show that the\ninstantaneous case actually is a particular case of the case with durations. We\npropose an algorithm to detect maximal cliques that improves our previous one\nfor instantaneous link streams, and performs better than the state of the art\nalgorithms in several cases of interest.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 15:09:47 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 13:54:23 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Viard", "Tiphaine", ""], ["Magnien", "Cl\u00e9mence", ""], ["Latapy", "Matthieu", ""]]}, {"id": "1712.06989", "submitter": "Jianbo Ye", "authors": "Jianbo Ye", "title": "A Faster Drop-in Implementation for Leaf-wise Exact Greedy Induction of\n  Decision Tree Using Pre-sorted Deque", "comments": "4 pages, updated with new statistics and fix typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short article presents a new implementation for decision trees. By\nintroducing pre-sorted deques, the leaf-wise greedy tree growing strategy no\nlonger needs to re-sort data at each node, and takes O(kn) time and O(1) extra\nmemory locating the best split and branching. The consistent, superior\nperformance - plus its simplicity and guarantee in producing the same\nclassification results as the standard decision trees - makes the new\nimplementation a drop-in replacement for depth-wise tree induction with strong\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 15:32:37 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 19:43:53 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Ye", "Jianbo", ""]]}, {"id": "1712.06996", "submitter": "Jaroslaw Byrka", "authors": "Jaroslaw Byrka and Aravind Srinivasan", "title": "Approximation algorithms for stochastic and risk-averse optimization", "comments": "Extension of a SODA'07 paper. To appear in SIAM J. Discrete Math", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present improved approximation algorithms in stochastic optimization. We\nprove that the multi-stage stochastic versions of covering integer programs\n(such as set cover and vertex cover) admit essentially the same approximation\nalgorithms as their standard (non-stochastic) counterparts; this improves upon\nwork of Swamy \\& Shmoys which shows an approximability that depends\nmultiplicatively on the number of stages. We also present approximation\nalgorithms for facility location and some of its variants in the $2$-stage\nrecourse model, improving on previous approximation guarantees. We give a\n$2.2975$-approximation algorithm in the standard polynomial-scenario model and\nan algorithm with an expected per-scenario $2.4957$-approximation guarantee,\nwhich is applicable to the more general black-box distribution model.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 09:25:14 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Byrka", "Jaroslaw", ""], ["Srinivasan", "Aravind", ""]]}, {"id": "1712.07041", "submitter": "Anna Paola Muntoni", "authors": "Alfredo Braunstein and Anna Paola Muntoni", "title": "The cavity approach for Steiner trees packing problems", "comments": null, "journal-ref": "J. Stat. Mech. (2018) 123401", "doi": "10.1088/1742-5468/aaeb3f", "report-no": null, "categories": "cs.DS cond-mat.stat-mech", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Belief Propagation approximation, or cavity method, has been recently\napplied to several combinatorial optimization problems in its zero-temperature\nimplementation, the max-sum algorithm. In particular, recent developments to\nsolve the edge-disjoint paths problem and the prize-collecting Steiner tree\nproblem on graphs have shown remarkable results for several classes of graphs\nand for benchmark instances. Here we propose a generalization of these\ntechniques for two variants of the Steiner trees packing problem where multiple\n\"interacting\" trees have to be sought within a given graph. Depending on the\ninteraction among trees we distinguish the vertex-disjoint Steiner trees\nproblem, where trees cannot share nodes, from the edge-disjoint Steiner trees\nproblem, where edges cannot be shared by trees but nodes can be members of\nmultiple trees. Several practical problems of huge interest in network design\ncan be mapped into these two variants, for instance, the physical design of\nVery Large Scale Integration (VLSI) chips. The formalism described here relies\non two components edge-variables that allows us to formulate a massage-passing\nalgorithm for the V-DStP and two algorithms for the E-DStP differing in the\nscaling of the computational time with respect to some relevant parameters. We\nwill show that one of the two formalisms used for the edge-disjoint variant\nallow us to map the max-sum update equations into a weighted maximum matching\nproblem over proper bipartite graphs. We developed a heuristic procedure based\non the max-sum equations that shows excellent performance in synthetic networks\n(in particular outperforming standard multi-step greedy procedures by large\nmargins) and on large benchmark instances of VLSI for which the optimal\nsolution is known, on which the algorithm found the optimum in two cases and\nthe gap to optimality was never larger than 4 %.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 16:48:47 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 10:52:31 GMT"}, {"version": "v3", "created": "Thu, 3 Jan 2019 18:05:13 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Braunstein", "Alfredo", ""], ["Muntoni", "Anna Paola", ""]]}, {"id": "1712.07099", "submitter": "Andreas T\\\"onnis", "authors": "Antonios Antoniadis, Carsten Fischer, Andreas T\\\"onnis", "title": "A Collection of Lower Bounds for Online Matching on the Line", "comments": "to appear in LATIN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the online matching on the line problem, the task is to match a set of\nrequests $R$ online to a given set of servers $S$. The distance metric between\nany two points in $R\\,\\cup\\, S$ is a line metric and the objective for the\nonline algorithm is to minimize the sum of distances between matched\nserver-request pairs. This problem is well-studied and - despite recent\nimprovements - there is still a large gap between the best known lower and\nupper bounds: The best known deterministic algorithm for the problem is\n$O(\\log^2n)$-competitive, while the best known deterministic lower bound is\n$9.001$. The lower and upper bounds for randomized algorithms are $4.5$ and\n$O(\\log n)$ respectively.\n  We prove that any deterministic online algorithm which in each round: $(i)$\nbases the matching decision only on information local to the current request,\nand $(ii)$ is symmetric (in the sense that the decision corresponding to the\nmirror image of some instance $I$ is the mirror image of the decision\ncorresponding to instance $I$), must be $\\Omega(\\log n)$-competitive. We then\nextend the result by showing that it also holds when relaxing the symmetry\nproperty so that the algorithm might prefer one side over the other, but only\nup to some degree. This proves a barrier of $\\Omega(\\log n)$ on the competitive\nratio for a large class of \"natural\" algorithms. This class includes all\ndeterministic online algorithms found in the literature so far.\n  Furthermore, we show that our result can be extended to randomized algorithms\nthat locally induce a symmetric distribution over the chosen servers. The\n$\\Omega(\\log n)$-barrier on the competitive ratio holds for this class of\nalgorithms as well.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 18:35:28 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Antoniadis", "Antonios", ""], ["Fischer", "Carsten", ""], ["T\u00f6nnis", "Andreas", ""]]}, {"id": "1712.07196", "submitter": "Thomas Steinke", "authors": "Vitaly Feldman, Thomas Steinke", "title": "Calibrating Noise to Variance in Adaptive Data Analysis", "comments": "Accepted for presentation at Conference on Learning Theory (COLT)\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets are often used multiple times and each successive analysis may\ndepend on the outcome of previous analyses. Standard techniques for ensuring\ngeneralization and statistical validity do not account for this adaptive\ndependence. A recent line of work studies the challenges that arise from such\nadaptive data reuse by considering the problem of answering a sequence of\n\"queries\" about the data distribution where each query may depend arbitrarily\non answers to previous queries.\n  The strongest results obtained for this problem rely on differential privacy\n-- a strong notion of algorithmic stability with the important property that it\n\"composes\" well when data is reused. However the notion is rather strict, as it\nrequires stability under replacement of an arbitrary data element. The simplest\nalgorithm is to add Gaussian (or Laplace) noise to distort the empirical\nanswers. However, analysing this technique using differential privacy yields\nsuboptimal accuracy guarantees when the queries have low variance. Here we\npropose a relaxed notion of stability that also composes adaptively. We\ndemonstrate that a simple and natural algorithm based on adding noise scaled to\nthe standard deviation of the query provides our notion of stability. This\nimplies an algorithm that can answer statistical queries about the dataset with\nsubstantially improved accuracy guarantees for low-variance queries. The only\nprevious approach that provides such accuracy guarantees is based on a more\ninvolved differentially private median-of-means algorithm and its analysis\nexploits stronger \"group\" stability of the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 20:42:39 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 19:14:14 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Feldman", "Vitaly", ""], ["Steinke", "Thomas", ""]]}, {"id": "1712.07246", "submitter": "Josh Alman", "authors": "Josh Alman and Virginia Vassilevska Williams", "title": "Further limitations of the known approaches for matrix multiplication", "comments": "16 pages. To appear in 9th Innovations in Theoretical Computer\n  Science Conference (ITCS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the techniques behind the current best algorithms for matrix\nmultiplication. Our results are threefold.\n  (1) We provide a unifying framework, showing that all known matrix\nmultiplication running times since 1986 can be achieved from a single very\nnatural tensor - the structural tensor $T_q$ of addition modulo an integer $q$.\n  (2) We show that if one applies a generalization of the known techniques\n(arbitrary zeroing out of tensor powers to obtain independent matrix products\nin order to use the asymptotic sum inequality of Sch\\\"{o}nhage) to an arbitrary\nmonomial degeneration of $T_q$, then there is an explicit lower bound,\ndepending on $q$, on the bound on the matrix multiplication exponent $\\omega$\nthat one can achieve. We also show upper bounds on the value $\\alpha$ that one\ncan achieve, where $\\alpha$ is such that $n\\times n^\\alpha \\times n$ matrix\nmultiplication can be computed in $n^{2+o(1)}$ time.\n  (3) We show that our lower bound on $\\omega$ approaches $2$ as $q$ goes to\ninfinity. This suggests a promising approach to improving the bound on\n$\\omega$: for variable $q$, find a monomial degeneration of $T_q$ which, using\nthe known techniques, produces an upper bound on $\\omega$ as a function of $q$.\nThen, take $q$ to infinity. It is not ruled out, and hence possible, that one\ncan obtain $\\omega=2$ in this way.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 22:40:49 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Alman", "Josh", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "1712.07431", "submitter": "Yakov Nekrich", "authors": "J. Ian Munro and Gonzalo Navarro and Yakov Nekrich", "title": "Text Indexing and Searching in Sublinear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first index that can be built in $o(n)$ time for a text of\nlength $n$, and can also be queried in $o(q)$ time for a pattern of length $q$.\nOn an alphabet of size $\\sigma$, our index uses $O(n\\sqrt{\\log n\\log\\sigma})$\nbits, is built in $O(n((\\log\\log n)^2+\\sqrt{\\log\\sigma})/\\sqrt{\\log_\\sigma n})$\ndeterministic time, and computes the number $\\mathrm{occ}$ of occurrences of\nthe pattern in time $O(q/\\log_\\sigma n+\\log n)$. Each such occurrence can then\nbe found in $O(\\sqrt{\\log n\\log\\sigma})$ time. By slightly increasing the space\nand construction time, to $O(n(\\sqrt{\\log n\\log\\sigma}+\n\\log\\sigma\\log^\\varepsilon n))$ and $O(n\\log^{3/2}\\sigma/\\log^{1/2-\\varepsilon}\nn)$, respectively, for any constant $0<\\varepsilon<1/2$, we can find the\n$\\mathrm{occ}$ pattern occurrences in time $O(q/\\log_\\sigma n +\n\\sqrt{\\log_\\sigma n}\\log\\log n + \\mathrm{occ})$. We build on a novel text\nsampling based on difference covers, which enjoys properties that allow us\nefficiently computing longest common prefixes in constant time. We extend our\nresults to the secondary memory model as well, where we give the first\nconstruction in $o(\\mathit{Sort}(n))$ I/Os of a data structure with suffix\narray functionality; this data structure supports pattern matching queries with\noptimal or nearly-optimal cost.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 11:51:51 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 15:50:47 GMT"}, {"version": "v3", "created": "Mon, 15 Jul 2019 08:15:41 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Munro", "J. Ian", ""], ["Navarro", "Gonzalo", ""], ["Nekrich", "Yakov", ""]]}, {"id": "1712.07504", "submitter": "John Wilmes", "authors": "Daniel \\v{S}tefankovi\\v{c}, Eric Vigoda, and John Wilmes", "title": "On Counting Perfect Matchings in General Graphs", "comments": "To appear in LATIN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting perfect matchings has played a central role in the theory of\ncounting problems. The permanent, corresponding to bipartite graphs, was shown\nto be #P-complete to compute exactly by Valiant (1979), and a fully polynomial\nrandomized approximation scheme (FPRAS) was presented by Jerrum, Sinclair, and\nVigoda (2004) using a Markov chain Monte Carlo (MCMC) approach. However, it has\nremained an open question whether there exists an FPRAS for counting perfect\nmatchings in general graphs. In fact, it was unresolved whether the same Markov\nchain defined by JSV is rapidly mixing in general. In this paper, we show that\nit is not. We prove torpid mixing for any weighting scheme on hole patterns in\nthe JSV chain. As a first step toward overcoming this obstacle, we introduce a\nnew algorithm for counting matchings based on the Gallai-Edmonds decomposition\nof a graph, and give an FPRAS for counting matchings in graphs that are\nsufficiently close to bipartite. In particular, we obtain a fixed-parameter\ntractable algorithm for counting matchings in general graphs, parameterized by\nthe greatest \"order\" of a factor-critical subgraph.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 14:53:56 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["\u0160tefankovi\u010d", "Daniel", ""], ["Vigoda", "Eric", ""], ["Wilmes", "John", ""]]}, {"id": "1712.07697", "submitter": "Iosif Salem", "authors": "Marco Canini (1), Iosif Salem (2), Liron Schiff (3), Elad Michael\n  Schiller (2), Stefan Schmid (4 and 5) ((1) Universit\\'e catholique de\n  Louvain, (2) Chalmers University of Technology, (3) GuardiCore Labs, (4)\n  University of Vienna, (5) Aalborg University)", "title": "Renaissance: Self-Stabilizing Distributed SDN Control Plane", "comments": "v2 includes: refined presentation, simpler notation in Algorithm 2,\n  additional explanations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By introducing programmability, automated verification, and innovative\ndebugging tools, Software-Defined Networks (SDNs) are poised to meet the\nincreasingly stringent dependability requirements of today's communication\nnetworks. However, the design of fault-tolerant SDNs remains an open challenge.\nThis paper considers the design of dependable SDNs through the lenses of\nself-stabilization - a very strong notion of fault-tolerance. In particular, we\ndevelop algorithms for an in-band and distributed control plane for SDNs,\ncalled Renaissance, which tolerate a wide range of (concurrent) controller,\nlink, and communication failures. Our self-stabilizing algorithms ensure that\nafter the occurrence of an arbitrary combination of failures, (i) every\nnon-faulty SDN controller can reach any switch (or another controller) in the\nnetwork within a bounded communication delay (in the presence of a bounded\nnumber of concurrent failures) and (ii) every switch is managed by at least one\ncontroller (as long as at least one controller is not faulty). We evaluate\nRenaissance through a rigorous worst-case analysis as well as a prototype\nimplementation (based on OVS and Floodlight), and we report on our experiments\nusing Mininet.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 20:23:43 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 15:54:23 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Canini", "Marco", "", "4 and 5"], ["Salem", "Iosif", "", "4 and 5"], ["Schiff", "Liron", "", "4 and 5"], ["Schiller", "Elad Michael", "", "4 and 5"], ["Schmid", "Stefan", "", "4 and 5"]]}, {"id": "1712.07906", "submitter": "Ugo Vaccaro", "authors": "Ferdinando Cicalese, Luisa Gargano, Ugo Vaccaro", "title": "Bounds on the Entropy of a Function of a Random Variable and their\n  Applications", "comments": "Accepted for publications to IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the entropy $H(X)$ of a discrete random variable $X$ is\nalways greater than or equal to the entropy $H(f(X))$ of a function $f$ of $X$,\nwith equality if and only if $f$ is one-to-one. In this paper, we give tight\nbounds on $H(f(X))$ when the function $f$ is not one-to-one, and we illustrate\na few scenarios where this matters. As an intermediate step towards our main\nresult, we derive a lower bound on the entropy of a probability distribution,\nwhen only a bound on the ratio between the maximal and minimal probabilities is\nknown. The lower bound improves on previous results in the literature, and it\ncould find applications outside the present scenario.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 12:28:48 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Cicalese", "Ferdinando", ""], ["Gargano", "Luisa", ""], ["Vaccaro", "Ugo", ""]]}, {"id": "1712.08130", "submitter": "Slobodan Mitrovi\\'c", "authors": "Aleksander M\\k{a}dry, Slobodan Mitrovi\\'c, Ludwig Schmidt", "title": "A Fast Algorithm for Separated Sparsity via Perturbed Lagrangians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity-based methods are widely used in machine learning, statistics, and\nsignal processing. There is now a rich class of structured sparsity approaches\nthat expand the modeling power of the sparsity paradigm and incorporate\nconstraints such as group sparsity, graph sparsity, or hierarchical sparsity.\nWhile these sparsity models offer improved sample complexity and better\ninterpretability, the improvements come at a computational cost: it is often\nchallenging to optimize over the (non-convex) constraint sets that capture\nvarious sparsity structures. In this paper, we make progress in this direction\nin the context of separated sparsity -- a fundamental sparsity notion that\ncaptures exclusion constraints in linearly ordered data such as time series.\nWhile prior algorithms for computing a projection onto this constraint set\nrequired quadratic time, we provide a perturbed Lagrangian relaxation approach\nthat computes provably exact projection in only nearly-linear time. Although\nthe sparsity constraint is non-convex, our perturbed Lagrangian approach is\nstill guaranteed to find a globally optimal solution. In experiments, our new\nalgorithms offer a 10$\\times$ speed-up already on moderately-size inputs.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 18:06:34 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["M\u0105dry", "Aleksander", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1712.08147", "submitter": "Andrea Lincoln", "authors": "Andrea Lincoln, Virginia Vassilevska Williams, Ryan Williams", "title": "Tight Hardness for Shortest Cycles and Paths in Sparse Graphs", "comments": "Updated the [AR16] citation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained reductions have established equivalences between many core\nproblems with $\\tilde{O}(n^3)$-time algorithms on $n$-node weighted graphs,\nsuch as Shortest Cycle, All-Pairs Shortest Paths (APSP), Radius, Replacement\nPaths, Second Shortest Paths, and so on. These problems also have\n$\\tilde{O}(mn)$-time algorithms on $m$-edge $n$-node weighted graphs, and such\nalgorithms have wider applicability. Are these $mn$ bounds optimal when $m \\ll\nn^2$?\n  Starting from the hypothesis that the minimum weight $(2\\ell+1)$-Clique\nproblem in edge weighted graphs requires $n^{2\\ell+1-o(1)}$ time, we prove that\nfor all sparsities of the form $m = \\Theta(n^{1+1/\\ell})$, there is no $O(n^2 +\nmn^{1-\\epsilon})$ time algorithm for $\\epsilon>0$ for \\emph{any} of the below\nproblems:\n  Minimum Weight $(2\\ell+1)$-Cycle in a directed weighted graph,\n  Shortest Cycle in a directed weighted graph,\n  APSP in a directed or undirected weighted graph,\n  Radius (or Eccentricities) in a directed or undirected weighted graph,\n  Wiener index of a directed or undirected weighted graph,\n  Replacement Paths in a directed weighted graph,\n  Second Shortest Path in a directed weighted graph,\n  Betweenness Centrality of a given node in a directed weighted graph.\n  That is, we prove hardness for a variety of sparse graph problems from the\nhardness of a dense graph problem. Our results also lead to new conditional\nlower bounds from several related hypothesis for unweighted sparse graph\nproblems including $k$-cycle, shortest cycle, Radius, Wiener index and APSP.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 18:34:13 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 23:01:57 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 21:36:52 GMT"}, {"version": "v4", "created": "Tue, 5 May 2020 19:56:16 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Lincoln", "Andrea", ""], ["Williams", "Virginia Vassilevska", ""], ["Williams", "Ryan", ""]]}, {"id": "1712.08205", "submitter": "Iosif Salem", "authors": "Iosif Salem (1) and Elad Michael Schiller (1) ((1) Chalmers University\n  of Technology)", "title": "Practically-Self-Stabilizing Vector Clocks in the Absence of Execution\n  Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector clock algorithms are basic wait-free building blocks that facilitate\ncausal ordering of events. As wait-free algorithms, they are guaranteed to\ncomplete their operations within a finite number of steps. Stabilizing\nalgorithms allow the system to recover after the occurrence of transient\nfaults, such as soft errors and arbitrary violations of the assumptions\naccording to which the system was designed to behave. We present the first, to\nthe best of our knowledge, stabilizing vector clock algorithm for asynchronous\ncrash-prone message-passing systems that can recover in a wait-free manner\nafter the occurrence of transient faults. In these settings, it is challenging\nto demonstrate a finite and wait-free recovery from (communication and crash\nfailures as well as) transient faults, bound the message and storage sizes,\ndeal with the removal of all stale information without blocking, and deal with\ncounter overflow events (which occur at different network nodes concurrently).\n  We present an algorithm that never violates safety in the absence of\ntransient faults and provides bounded time recovery during fair executions that\nfollow the last transient fault. The novelty is that in the absence of\nexecution fairness, the algorithm guarantees a bound on the number of times in\nwhich the system might violate safety (while existing algorithms might block\nforever due to the presence of both transient faults and crash failures).\n  Since vector clocks facilitate a number of elementary synchronization\nbuilding blocks (without requiring remote replica synchronization) in\nasynchronous systems, we believe that our analytical insights are useful for\nthe design of other systems that cannot guarantee execution fairness.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 20:45:47 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Salem", "Iosif", ""], ["Schiller", "Elad Michael", ""]]}, {"id": "1712.08285", "submitter": "Emanuel Onica", "authors": "Ciprian Amariei, Paul Diac, Emanuel Onica", "title": "Grand Challenge: Optimized Stage Processing for Anomaly Detection on\n  Numerical Data Streams", "comments": null, "journal-ref": "DEBS 2017, Proceedings of the 11th ACM International Conference on\n  Distributed and Event-based Systems, Pages 286-291", "doi": "10.1145/3093742.3095101", "report-no": null, "categories": "cs.PF cs.DS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2017 Grand Challenge focused on the problem of automatic detection of\nanomalies for manufacturing equipment. This paper reports the technical details\nof a solution focused on particular optimizations of the processing stages.\nThese included customized input parsing, fine tuning of a k-means clustering\nalgorithm and probability analysis using a lazy flavor of a Markov chain. We\nhave observed in our custom implementation that carefully tweaking these\nprocessing stages at single node level by leveraging various data stream\ncharacteristics can yield good performance results. We start the paper with\nseveral observations concerning the input data stream, following with our\nsolution description with details on particular optimizations, and we conclude\nwith evaluation and a discussion of obtained results.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 02:31:16 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Amariei", "Ciprian", ""], ["Diac", "Paul", ""], ["Onica", "Emanuel", ""]]}, {"id": "1712.08328", "submitter": "Sanjeev Saxena", "authors": "Sanjeev Saxena", "title": "A simple introduction to Karmarkar's Algorithm for Linear Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An extremely simple, description of Karmarkar's algorithm with very few\ntechnical terms is given.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 07:33:10 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Saxena", "Sanjeev", ""]]}, {"id": "1712.08345", "submitter": "EPTCS", "authors": "Timo Kehrer (Humboldt-University of Berlin), Alice Miller (University\n  of Glasgow)", "title": "Proceedings Third Workshop on Graphs as Models", "comments": null, "journal-ref": "EPTCS 263, 2017", "doi": "10.4204/EPTCS.263", "report-no": null, "categories": "cs.LO cs.DC cs.DS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are used as models in many areas of computer science and computer\nengineering. For example graphs are used to represent syntax, control and data\nflow, dependency, state spaces, models such as UML and other types of\ndomain-specific models, and social network graphs. In all of these examples,\nthe graph serves as an intuitive yet mathematically precise foundation for many\npurposes, both in theory building as well as in practical applications.\nGraph-based models serve as an abstract communication medium and are used to\ndescribe various concepts and phenomena. Moreover, once such graph-based models\nare constructed, they can be analyzed and transformed to verify the correctness\nof static and dynamic properties, to discover new properties, to deeply study a\nparticular domain of interest or to produce new equivalent and/or optimized\nversions of graph-based models.\n  The Graphs as Models (GaM) workshop series combines the strengths of two\npre-existing workshop series: GT-VMT (Graph Transformation and Visual Modelling\nTechniques) and GRAPHITE (Graph Inspection and Traversal Engineering), but also\nsolicits research from other related areas, such as social network analysis.\nGaM offers a platform for exchanging new ideas and results for active\nresearchers in these areas, with a particular aim of boosting inter- and\ntransdisciplinary research exploiting new applications of graphs as models in\nany area of computational science. This year (2017), the third edition of the\nGaM workshop was co-located with the European Joint Conferences on Theory and\nPractice of Software 2017 (ETAPS'17), held in Uppsala, Sweden.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 08:48:17 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Kehrer", "Timo", "", "Humboldt-University of Berlin"], ["Miller", "Alice", "", "University\n  of Glasgow"]]}, {"id": "1712.08362", "submitter": "Daniel Paulusma", "authors": "Matthew Johnson and Giacomo Paesani and Daniel Paulusma", "title": "Connected Vertex Cover for $(sP_1+P_5)$-Free Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Connected Vertex Cover problem is to decide if a graph G has a vertex\ncover of size at most $k$ that induces a connected subgraph of $G$. This is a\nwell-studied problem, known to be NP-complete for restricted graph classes,\nand, in particular, for $H$-free graphs if $H$ is not a linear forest (a graph\nis $H$-free if it does not contain $H$ as an induced subgraph). It is easy to\nsee that Connected Vertex Cover is polynomial-time solvable for $P_4$-free\ngraphs. We continue the search for tractable graph classes: we prove that it is\nalso polynomial-time solvable for $(sP_1+P_5)$-free graphs for every integer\n$s\\geq 0$.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 09:18:52 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 14:43:30 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 16:37:04 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Johnson", "Matthew", ""], ["Paesani", "Giacomo", ""], ["Paulusma", "Daniel", ""]]}, {"id": "1712.08482", "submitter": "Stavros Nikolopoulos D.", "authors": "Maria Chroni, Stavros D. Nikolopoulos, and Leonidas Palios", "title": "Encoding Watermark Numbers as Reducible Permutation Graphs using\n  Self-inverting Permutations", "comments": "27 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1110.1194", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several graph theoretic watermark methods have been proposed to encode\nnumbers as graph structures in software watermarking environments. In this\npaper, we propose an efficient and easily implementable codec system for\nencoding watermark numbers as reducible permutation flow-graphs and, thus, we\nextend the class of graphs used in such a watermarking environment. More\nprecisely, we present an algorithm for encoding a watermark number $w$ as a\nself-inverting permutation $\\pi^*$, an algorithm for encoding the\nself-inverting permutation $\\pi^*$ into a reducible permutation graph\n$F[\\pi^*]$ whose structure resembles the structure of real program graphs, as\nwell as decoding algorithms which extract the permutation $\\pi^*$ from the\nreducible permutation graph $F[\\pi^*]$ and the number $w$ from $\\pi^*$. Both\nthe encoding and the decoding process takes time and space linear in the length\nof the binary representation of $w$. The two main components of our proposed\ncodec system, i.e., the self-inverting permutation $\\pi^*$ and the reducible\npermutation graph $F[\\pi^*]$, incorporate the binary representation of the\nwatermark~$w$ in their structure and possess important structural properties,\nwhich make our system resilient to attacks; to this end, we experimentally\nevaluated our system under edge modification attacks on the graph $F[\\pi^*]$\nand the results show that we can detect such attacks with high probability.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 11:10:34 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Chroni", "Maria", ""], ["Nikolopoulos", "Stavros D.", ""], ["Palios", "Leonidas", ""]]}, {"id": "1712.08558", "submitter": "Venkata Gandikota", "authors": "Karthekeyan Chandrasekaran, Daniel Dadush, Venkata Gandikota, Elena\n  Grigorescu", "title": "Lattice-based Locality Sensitive Hashing is Optimal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality sensitive hashing (LSH) was introduced by Indyk and Motwani (STOC\n`98) to give the first sublinear time algorithm for the c-approximate nearest\nneighbor (ANN) problem using only polynomial space. At a high level, an LSH\nfamily hashes \"nearby\" points to the same bucket and \"far away\" points to\ndifferent buckets. The quality of measure of an LSH family is its LSH exponent,\nwhich helps determine both query time and space usage.\n  In a seminal work, Andoni and Indyk (FOCS `06) constructed an LSH family\nbased on random ball partitioning of space that achieves an LSH exponent of\n1/c^2 for the l_2 norm, which was later shown to be optimal by Motwani, Naor\nand Panigrahy (SIDMA `07) and O'Donnell, Wu and Zhou (TOCT `14). Although\noptimal in the LSH exponent, the ball partitioning approach is computationally\nexpensive. So, in the same work, Andoni and Indyk proposed a simpler and more\npractical hashing scheme based on Euclidean lattices and provided computational\nresults using the 24-dimensional Leech lattice. However, no theoretical\nanalysis of the scheme was given, thus leaving open the question of finding the\nexponent of lattice based LSH.\n  In this work, we resolve this question by showing the existence of lattices\nachieving the optimal LSH exponent of 1/c^2 using techniques from the geometry\nof numbers. At a more conceptual level, our results show that optimal LSH space\npartitions can have periodic structure. Understanding the extent to which\nadditional structure can be imposed on these partitions, e.g. to yield low\nspace and query complexity, remains an important open problem.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 16:39:13 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Chandrasekaran", "Karthekeyan", ""], ["Dadush", "Daniel", ""], ["Gandikota", "Venkata", ""], ["Grigorescu", "Elena", ""]]}, {"id": "1712.08573", "submitter": "Jakub Radoszewski", "authors": "Tomasz Kociumaka, Jakub Radoszewski, and Tatiana Starikovskaya", "title": "Longest common substring with approximately $k$ mismatches", "comments": "extended version of a paper from CPM 2016 with corrected proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the longest common substring problem, we are given two strings of length\n$n$ and must find a substring of maximal length that occurs in both strings. It\nis well known that the problem can be solved in linear time, but the solution\nis not robust and can vary greatly when the input strings are changed even by\none character. To circumvent this, Leimeister and Morgenstern introduced the\nproblem of the longest common substring with $k$ mismatches. Lately, this\nproblem has received a lot of attention in the literature. In this paper, we\nfirst show a conditional lower bound based on the SETH hypothesis implying that\nthere is little hope to improve existing solutions. We then introduce a new but\nclosely related problem of the longest common substring with approximately $k$\nmismatches and use locality-sensitive hashing to show that it admits a solution\nwith strongly subquadratic running time. We also apply these results to obtain\na strongly subquadratic-time 2-approximation algorithm for the longest common\nsubstring with $k$ mismatches problem and show conditional hardness of\nimproving its approximation ratio.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 16:54:53 GMT"}, {"version": "v2", "created": "Sat, 18 Aug 2018 07:05:30 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Kociumaka", "Tomasz", ""], ["Radoszewski", "Jakub", ""], ["Starikovskaya", "Tatiana", ""]]}, {"id": "1712.08685", "submitter": "Nick Duffield", "authors": "Nesreen K. Ahmed and Nick Duffield and Liangzhen Xia", "title": "Sampling for Approximate Bipartite Network Projection", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.IR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite networks manifest as a stream of edges that represent transactions,\ne.g., purchases by retail customers. Many machine learning applications employ\nneighborhood-based measures to characterize the similarity among the nodes,\nsuch as the pairwise number of common neighbors (CN) and related metrics. While\nthe number of node pairs that share neighbors is potentially enormous, only a\nrelatively small proportion of them have many common neighbors. This motivates\nfinding a weighted sampling approach to preferentially sample these node pairs.\nThis paper presents a new sampling algorithm that provides a fixed size\nunbiased estimate of the similarity matrix resulting from a bipartite graph\nstream projection. The algorithm has two components. First, it maintains a\nreservoir of sampled bipartite edges with sampling weights that favor selection\nof high similarity nodes. Second, arriving edges generate a stream of\n\\textsl{similarity updates} based on their adjacency with the current sample.\nThese updates are aggregated in a second reservoir sample-based stream\naggregator to yield the final unbiased estimate. Experiments on real world\ngraphs show that a 10% sample at each stage yields estimates of high similarity\nedges with weighted relative errors of about 1%.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 23:56:44 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 03:46:41 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Duffield", "Nick", ""], ["Xia", "Liangzhen", ""]]}, {"id": "1712.08709", "submitter": "Hongyang Zhang", "authors": "Hongyang Zhang, Huacheng Yu, Ashish Goel", "title": "Pruning based Distance Sketches with Provable Guarantees on Random\n  Graphs", "comments": "Full version for the conference paper to appear in The Web\n  Conference'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS math.PR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the distances between vertices on graphs is one of the most\nfundamental components in network analysis. Since finding shortest paths\nrequires traversing the graph, it is challenging to obtain distance information\non large graphs very quickly. In this work, we present a preprocessing\nalgorithm that is able to create landmark based distance sketches efficiently,\nwith strong theoretical guarantees. When evaluated on a diverse set of social\nand information networks, our algorithm significantly improves over existing\napproaches by reducing the number of landmarks stored, preprocessing time, or\nstretch of the estimated distances.\n  On Erd\\\"{o}s-R\\'{e}nyi graphs and random power law graphs with degree\ndistribution exponent $2 < \\beta < 3$, our algorithm outputs an exact distance\ndata structure with space between $\\Theta(n^{5/4})$ and $\\Theta(n^{3/2})$\ndepending on the value of $\\beta$, where $n$ is the number of vertices. We\ncomplement the algorithm with tight lower bounds for Erdos-Renyi graphs and the\ncase when $\\beta$ is close to two.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 03:55:22 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 06:16:46 GMT"}, {"version": "v3", "created": "Sun, 10 Feb 2019 07:49:20 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Zhang", "Hongyang", ""], ["Yu", "Huacheng", ""], ["Goel", "Ashish", ""]]}, {"id": "1712.08712", "submitter": "Sarath Pattathil", "authors": "Sarath Pattathil, Nikhil Karamchandani, Dhruti Shah", "title": "Persistence of the Jordan center in Random Growing Trees", "comments": "28 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Jordan center of a graph is defined as a vertex whose maximum distance to\nother nodes in the graph is minimal, and it finds applications in facility\nlocation and source detection problems. We study properties of the Jordan\nCenter in the case of random growing trees. In particular, we consider a\nregular tree graph on which an infection starts from a root node and then\nspreads along the edges of the graph according to various random spread models.\nFor the Independent Cascade (IC) model and the discrete Susceptible Infected\n(SI) model, both of which are discrete time models, we show that as the\ninfected subgraph grows with time, the Jordan center persists on a single\nvertex after a finite number of timesteps. Finally, we also study the\ncontinuous time version of the SI model and bound the maximum distance between\nthe Jordan center and the root node at any time.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 04:22:16 GMT"}, {"version": "v2", "created": "Sun, 21 Oct 2018 15:13:17 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Pattathil", "Sarath", ""], ["Karamchandani", "Nikhil", ""], ["Shah", "Dhruti", ""]]}, {"id": "1712.08749", "submitter": "Maxime Crochemore", "authors": "Maxime Crochemore and Luis M. S. Russo", "title": "Cartesian trees and Lyndon trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article describes the structural and algorithmic relations between\nCartesian trees and Lyndon Trees. This leads to a uniform presentation of the\nLyndon table of a word corresponding to the Next Nearest Smaller table of a\nsequence of numbers. It shows how to efficiently compute runs, that is, maximal\nperiodicities occurring in a word.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 10:01:09 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Crochemore", "Maxime", ""], ["Russo", "Luis M. S.", ""]]}, {"id": "1712.08809", "submitter": "Miguel Romero", "authors": "Miguel Romero", "title": "The tractability frontier of well-designed SPARQL queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of query evaluation of SPARQL queries. We focus on\nthe fundamental fragment of well-designed SPARQL restricted to the AND,\nOPTIONAL and UNION operators. Our main result is a structural characterisation\nof the classes of well-designed queries that can be evaluated in polynomial\ntime. In particular, we introduce a new notion of width called domination\nwidth, which relies on the well-known notion of treewidth. We show that, under\nsome complexity theoretic assumptions, the classes of well-designed queries\nthat can be evaluated in polynomial time are precisely those of bounded\ndomination width.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 17:23:08 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 15:28:25 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 09:56:52 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Romero", "Miguel", ""]]}, {"id": "1712.08880", "submitter": "Michael Mahoney", "authors": "Petros Drineas and Michael W. Mahoney", "title": "Lectures on Randomized Numerical Linear Algebra", "comments": "To appear in the edited volume of lectures from the 2016 PCMI summer\n  school", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter is based on lectures on Randomized Numerical Linear Algebra from\nthe 2016 Park City Mathematics Institute summer school on The Mathematics of\nData.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 06:05:57 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Drineas", "Petros", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1712.08963", "submitter": "Jing Tang", "authors": "Jing Tang, Xueyan Tang, Junsong Yuan", "title": "Towards Profit Maximization for Online Social Network Providers", "comments": "INFOCOM 2018 (Full version), 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Social Networks (OSNs) attract billions of users to share information\nand communicate where viral marketing has emerged as a new way to promote the\nsales of products. An OSN provider is often hired by an advertiser to conduct\nviral marketing campaigns. The OSN provider generates revenue from the\ncommission paid by the advertiser which is determined by the spread of its\nproduct information. Meanwhile, to propagate influence, the activities\nperformed by users such as viewing video ads normally induce diffusion cost to\nthe OSN provider. In this paper, we aim to find a seed set to optimize a new\nprofit metric that combines the benefit of influence spread with the cost of\ninfluence propagation for the OSN provider. Under many diffusion models, our\nprofit metric is the difference between two submodular functions which is\nchallenging to optimize as it is neither submodular nor monotone. We design a\ngeneral two-phase framework to select seeds for profit maximization and develop\nseveral bounds to measure the quality of the seed set constructed. Experimental\nresults with real OSN datasets show that our approach can achieve high\napproximation guarantees and significantly outperform the baseline algorithms,\nincluding state-of-the-art influence maximization algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 19:51:10 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Tang", "Jing", ""], ["Tang", "Xueyan", ""], ["Yuan", "Junsong", ""]]}, {"id": "1712.09007", "submitter": "Ger Yang", "authors": "David Liau, Eric Price, Zhao Song, Ger Yang", "title": "Stochastic Multi-armed Bandits in Constant Space", "comments": "AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the stochastic bandit problem in the sublinear space setting,\nwhere one cannot record the win-loss record for all $K$ arms. We give an\nalgorithm using $O(1)$ words of space with regret \\[\n  \\sum_{i=1}^{K}\\frac{1}{\\Delta_i}\\log \\frac{\\Delta_i}{\\Delta}\\log T \\] where\n$\\Delta_i$ is the gap between the best arm and arm $i$ and $\\Delta$ is the gap\nbetween the best and the second-best arms. If the rewards are bounded away from\n$0$ and $1$, this is within an $O(\\log 1/\\Delta)$ factor of the optimum regret\npossible without space constraints.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 05:04:35 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 17:06:53 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Liau", "David", ""], ["Price", "Eric", ""], ["Song", "Zhao", ""], ["Yang", "Ger", ""]]}, {"id": "1712.09121", "submitter": "Jason Li", "authors": "Mohsen Ghaffari, Jason Li", "title": "Improved Distributed Algorithms for Exact Shortest Paths", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing shortest paths is one of the central problems in the theory of\ndistributed computing. For the last few years, substantial progress has been\nmade on the approximate single source shortest paths problem, culminating in an\nalgorithm of Becker et al. [DISC'17] which deterministically computes\n$(1+o(1))$-approximate shortest paths in $\\tilde O(D+\\sqrt n)$ time, where $D$\nis the hop-diameter of the graph. Up to logarithmic factors, this time\ncomplexity is optimal, matching the lower bound of Elkin [STOC'04].\n  The question of exact shortest paths however saw no algorithmic progress for\ndecades, until the recent breakthrough of Elkin [STOC'17], which established a\nsublinear-time algorithm for exact single source shortest paths on undirected\ngraphs. Shortly after, Huang et al. [FOCS'17] provided improved algorithms for\nexact all pairs shortest paths problem on directed graphs.\n  In this paper, we present a new single-source shortest path algorithm with\ncomplexity $\\tilde O(n^{3/4}D^{1/4})$. For polylogarithmic $D$, this improves\non Elkin's $\\tilde{O}(n^{5/6})$ bound and gets closer to the\n$\\tilde{\\Omega}(n^{1/2})$ lower bound of Elkin [STOC'04]. For larger values of\n$D$, we present an improved variant of our algorithm which achieves complexity\n$\\tilde{O}\\left( n^{3/4+o(1)}+ \\min\\{ n^{3/4}D^{1/6},n^{6/7}\\}+D\\right)$, and\nthus compares favorably with Elkin's bound of $\\tilde{O}(n^{5/6} +\nn^{2/3}D^{1/3} + D ) $ in essentially the entire range of parameters. This\nalgorithm provides also a qualitative improvement, because it works for the\nmore challenging case of directed graphs (i.e., graphs where the two directions\nof an edge can have different weights), constituting the first sublinear-time\nalgorithm for directed graphs. Our algorithm also extends to the case of exact\n$\\kappa$-source shortest paths...\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 19:57:16 GMT"}, {"version": "v2", "created": "Sat, 13 Jan 2018 20:20:06 GMT"}, {"version": "v3", "created": "Tue, 3 Apr 2018 00:56:01 GMT"}, {"version": "v4", "created": "Wed, 11 Apr 2018 22:51:31 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Li", "Jason", ""]]}, {"id": "1712.09166", "submitter": "Ran Duan", "authors": "Ran Duan, Haoqing He, Tianyi Zhang", "title": "Near-linear Time Algorithm for Approximate Minimum Degree Spanning Trees", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G = (V, E)$, we wish to compute a spanning tree whose maximum\nvertex degree, i.e. tree degree, is as small as possible. Computing the exact\noptimal solution is known to be NP-hard, since it generalizes the Hamiltonian\npath problem. For the approximation version of this problem, a $\\tilde{O}(mn)$\ntime algorithm that computes a spanning tree of degree at most $\\Delta^* +1$ is\npreviously known [F\\\"urer \\& Raghavachari 1994]; here $\\Delta^*$ denotes the\nminimum tree degree of all the spanning trees. In this paper we give the first\nnear-linear time approximation algorithm for this problem. Specifically\nspeaking, we propose an $\\tilde{O}(\\frac{1}{\\epsilon^7}m)$ time algorithm that\ncomputes a spanning tree with tree degree $(1+\\epsilon)\\Delta^* +\nO(\\frac{1}{\\epsilon^2}\\log n)$ for any constant $\\epsilon \\in (0,\\frac{1}{6})$.\nThus, when $\\Delta^*=\\omega(\\log n)$, we can achieve approximate solutions with\nconstant approximate ratio arbitrarily close to 1 in near-linear time.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 03:12:48 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 12:57:59 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 16:29:21 GMT"}, {"version": "v4", "created": "Sun, 31 May 2020 16:37:18 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Duan", "Ran", ""], ["He", "Haoqing", ""], ["Zhang", "Tianyi", ""]]}, {"id": "1712.09203", "submitter": "Hongyang Zhang", "authors": "Yuanzhi Li, Tengyu Ma, Hongyang Zhang", "title": "Algorithmic Regularization in Over-parameterized Matrix Sensing and\n  Neural Networks with Quadratic Activations", "comments": "COLT 2018 best paper; fixed minor missing steps in the previous\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the gradient descent algorithm provides an implicit\nregularization effect in the learning of over-parameterized matrix\nfactorization models and one-hidden-layer neural networks with quadratic\nactivations. Concretely, we show that given $\\tilde{O}(dr^{2})$ random linear\nmeasurements of a rank $r$ positive semidefinite matrix $X^{\\star}$, we can\nrecover $X^{\\star}$ by parameterizing it by $UU^\\top$ with $U\\in \\mathbb\nR^{d\\times d}$ and minimizing the squared loss, even if $r \\ll d$. We prove\nthat starting from a small initialization, gradient descent recovers\n$X^{\\star}$ in $\\tilde{O}(\\sqrt{r})$ iterations approximately. The results\nsolve the conjecture of Gunasekar et al.'17 under the restricted isometry\nproperty. The technique can be applied to analyzing neural networks with\none-hidden-layer quadratic activations with some technical modifications.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 08:04:43 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 08:21:52 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 03:50:08 GMT"}, {"version": "v4", "created": "Mon, 19 Mar 2018 07:59:32 GMT"}, {"version": "v5", "created": "Thu, 14 Feb 2019 00:24:05 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Li", "Yuanzhi", ""], ["Ma", "Tengyu", ""], ["Zhang", "Hongyang", ""]]}, {"id": "1712.09230", "submitter": "Yota Otachi", "authors": "Masashi Kiyomi, Hirotaka Ono, Yota Otachi, Pascal Schweitzer, Jun\n  Tarui", "title": "Space-Efficient Algorithms for Longest Increasing Subsequence", "comments": "14 pages, STACS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a sequence of integers, we want to find a longest increasing\nsubsequence of the sequence. It is known that this problem can be solved in\n$O(n \\log n)$ time and space. Our goal in this paper is to reduce the space\nconsumption while keeping the time complexity small. For $\\sqrt{n} \\le s \\le\nn$, we present algorithms that use $O(s \\log n)$ bits and $O(\\frac{1}{s} \\cdot\nn^{2} \\cdot \\log n)$ time for computing the length of a longest increasing\nsubsequence, and $O(\\frac{1}{s} \\cdot n^{2} \\cdot \\log^{2} n)$ time for finding\nan actual subsequence. We also show that the time complexity of our algorithms\nis optimal up to polylogarithmic factors in the framework of sequential access\nalgorithms with the prescribed amount of space.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 10:55:12 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Kiyomi", "Masashi", ""], ["Ono", "Hirotaka", ""], ["Otachi", "Yota", ""], ["Schweitzer", "Pascal", ""], ["Tarui", "Jun", ""]]}, {"id": "1712.09379", "submitter": "Anastasios Kyrillidis", "authors": "Rajiv Khanna and Anastasios Kyrillidis", "title": "IHT dies hard: Provable accelerated Iterative Hard Thresholding", "comments": "accepted to AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study --both in theory and practice-- the use of momentum motions in\nclassic iterative hard thresholding (IHT) methods. By simply modifying plain\nIHT, we investigate its convergence behavior on convex optimization criteria\nwith non-convex constraints, under standard assumptions. In diverse scenaria,\nwe observe that acceleration in IHT leads to significant improvements, compared\nto state of the art projected gradient descent and Frank-Wolfe variants. As a\nbyproduct of our inspection, we study the impact of selecting the momentum\nparameter: similar to convex settings, two modes of behavior are observed\n--\"rippling\" and linear-- depending on the level of momentum.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 19:40:47 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 18:01:30 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Khanna", "Rajiv", ""], ["Kyrillidis", "Anastasios", ""]]}, {"id": "1712.09473", "submitter": "Zhao Song", "authors": "Huaian Diao, Zhao Song, Wen Sun, David P. Woodruff", "title": "Sketching for Kronecker Product Regression and P-splines", "comments": "AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  TensorSketch is an oblivious linear sketch introduced in Pagh'13 and later\nused in Pham, Pagh'13 in the context of SVMs for polynomial kernels. It was\nshown in Avron, Nguyen, Woodruff'14 that TensorSketch provides a subspace\nembedding, and therefore can be used for canonical correlation analysis, low\nrank approximation, and principal component regression for the polynomial\nkernel. We take TensorSketch outside of the context of polynomials kernels, and\nshow its utility in applications in which the underlying design matrix is a\nKronecker product of smaller matrices. This allows us to solve Kronecker\nproduct regression and non-negative Kronecker product regression, as well as\nregularized spline regression. Our main technical result is then in extending\nTensorSketch to other norms. That is, TensorSketch only provides input sparsity\ntime for Kronecker product regression with respect to the $2$-norm. We show how\nto solve Kronecker product regression with respect to the $1$-norm in time\nsublinear in the time required for computing the Kronecker product, as well as\nfor more general $p$-norms.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 01:26:52 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Diao", "Huaian", ""], ["Song", "Zhao", ""], ["Sun", "Wen", ""], ["Woodruff", "David P.", ""]]}, {"id": "1712.09494", "submitter": "EPTCS", "authors": "Nathan Cassee (Eindhoven University of Technology, Eindhoven, The\n  Netherlands), Anton Wijs (Eindhoven University of Technology, Eindhoven, The\n  Netherlands)", "title": "Analysing the Performance of GPU Hash Tables for State Space Exploration", "comments": "In Proceedings GaM 2017, arXiv:1712.08345", "journal-ref": "EPTCS 263, 2017, pp. 1-15", "doi": "10.4204/EPTCS.263.1", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, General Purpose Graphics Processors (GPUs) have been\nused to significantly speed up numerous applications. One of the areas in which\nGPUs have recently led to a significant speed-up is model checking. In model\nchecking, state spaces, i.e., large directed graphs, are explored to verify\nwhether models satisfy desirable properties. GPUexplore is a GPU-based model\nchecker that uses a hash table to efficiently keep track of already explored\nstates. As a large number of states is discovered and stored during such an\nexploration, the hash table should be able to quickly handle many inserts and\nqueries concurrently. In this paper, we experimentally compare two different\nhash tables optimised for the GPU, one being the GPUexplore hash table, and the\nother using Cuckoo hashing. We compare the performance of both hash tables\nusing random and non-random data obtained from model checking experiments, to\nanalyse the applicability of the two hash tables for state space exploration.\nWe conclude that Cuckoo hashing is three times faster than GPUexplore hashing\nfor random data, and that Cuckoo hashing is five to nine times faster for\nnon-random data. This suggests great potential to further speed up GPUexplore\nin the near future.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 05:14:23 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Cassee", "Nathan", "", "Eindhoven University of Technology, Eindhoven, The\n  Netherlands"], ["Wijs", "Anton", "", "Eindhoven University of Technology, Eindhoven, The\n  Netherlands"]]}, {"id": "1712.09624", "submitter": "Nicolas Le Scouarnec", "authors": "Nicolas Le Scouarnec", "title": "Cuckoo++ Hash Tables: High-Performance Hash Tables for Networking\n  Applications", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DB cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash tables are an essential data-structure for numerous networking\napplications (e.g., connection tracking, firewalls, network address\ntranslators). Among these, cuckoo hash tables provide excellent performance by\nallowing lookups to be processed with very few memory accesses (2 to 3 per\nlookup). Yet, for large tables, cuckoo hash tables remain memory bound and each\nmemory access impacts performance. In this paper, we propose algorithmic\nimprovements to cuckoo hash tables allowing to eliminate some unnecessary\nmemory accesses; these changes are conducted without altering the properties of\nthe original cuckoo hash table so that all existing theoretical analysis remain\napplicable. On a single core, our hash table achieves 37M lookups per second\nfor positive lookups (i.e., when the key looked up is present in the table),\nand 60M lookups per second for negative lookups, a 50% improvement over the\nimplementation included into the DPDK. On a 18-core, with mostly positive\nlookups, our implementation achieves 496M lookups per second, a 45% improvement\nover DPDK.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 16:40:15 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Scouarnec", "Nicolas Le", ""]]}, {"id": "1712.09630", "submitter": "Per Austrin", "authors": "Per Austrin, Petteri Kaski and Kaie Kubjas", "title": "Tensor network complexity of multilinear maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study tensor networks as a model of arithmetic computation for evaluating\nmultilinear maps. These capture any algorithm based on low border rank tensor\ndecompositions, such as $O(n^{\\omega+\\epsilon})$ time matrix multiplication,\nand in addition many other algorithms such as $O(n \\log n)$ time discrete\nFourier transform and $O^*(2^n)$ time for computing the permanent of a matrix.\nHowever tensor networks sometimes yield faster algorithms than those that\nfollow from low-rank decompositions. For instance the fastest known\n$O(n^{(\\omega +\\epsilon)t})$ time algorithms for counting $3t$-cliques can be\nimplemented with tensor networks, even though the underlying tensor has border\nrank $n^{3t}$ for all $t \\ge 2$. For counting homomorphisms of a general\npattern graph $P$ into a host graph on $n$ vertices we obtain an upper bound of\n$O(n^{(\\omega+\\epsilon)\\operatorname{bw}(P)/2})$ where $\\operatorname{bw}(P)$\nis the branchwidth of $P$. This essentially matches the bound for counting\ncliques, and yields small improvements over previous algorithms for many\nchoices of $P$.\n  While powerful, the model still has limitations, and we are able to show a\nnumber of unconditional lower bounds for various multilinear maps, including:\n  (a) an $\\Omega(n^{\\operatorname{bw}(P)})$ time lower bound for counting\nhomomorphisms from $P$ to an $n$-vertex graph, matching the upper bound if\n$\\omega = 2$. In particular for $P$ a $v$-clique this yields an\n$\\Omega(n^{\\lceil 2v/3 \\rceil})$ time lower bound for counting $v$-cliques, and\nfor $P$ a $k$-uniform $v$-hyperclique we obtain an $\\Omega(n^v)$ time lower\nbound for $k \\ge 3$, ruling out tensor networks as an approach to obtaining\nnon-trivial algorithms for hyperclique counting and the Max-$3$-CSP problem.\n  (b) an $\\Omega(2^{0.918n})$ time lower bound for the permanent of an $n\n\\times n$ matrix.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 17:03:56 GMT"}, {"version": "v2", "created": "Sat, 7 Apr 2018 11:24:27 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2018 12:36:55 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Austrin", "Per", ""], ["Kaski", "Petteri", ""], ["Kubjas", "Kaie", ""]]}, {"id": "1712.09636", "submitter": "Aleksandar Prokopec", "authors": "Aleksandar Prokopec", "title": "Analysis of Concurrent Lock-Free Hash Tries with Constant-Time\n  Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Ctrie is a scalable concurrent non-blocking dictionary data structure, with\ngood cache locality, and non-blocking linearizable iterators. However,\noperations on most existing concurrent hash tries run in O(log n) time. In this\ntechnical report, we extend the standard concurrent hash-tries with an\nauxiliary data structure called a cache. The cache is essentially an array that\nstores pointers to a specific level of the hash trie. We analyze the\nperformance implications of adding a cache, and prove that the running time of\nthe basic operations becomes O(1).\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 17:25:07 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Prokopec", "Aleksandar", ""]]}, {"id": "1712.09666", "submitter": "Anoosheh Heidarzadeh", "authors": "Anoosheh Heidarzadeh and Alex Sprintson and Chanan Singh", "title": "A Fast and Accurate Failure Frequency Approximation for $k$-Terminal\n  Reliability Systems", "comments": "17 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of approximating the failure frequency of\nlarge-scale composite $k$-terminal reliability systems. In such systems, the\nnodes ($k$ of which are terminals) are connected through components which are\nsubject to random failure and repair processes. At any time, a system failure\noccurs if the surviving system fails to connect all the k terminals together.\nWe assume that each component's up-times and down-times follow statistically\nindependent stationary random processes, and these processes are statistically\nindependent across the components. In this setting, the exact computation of\nfailure frequency is known to be computationally intractable (NP-hard). In this\nwork, we present an algorithm to approximate the failure frequency for any\ngiven multiplicative error factor that runs in polynomial time in the number of\n(minimal) cutsets. Moreover, for the special case of all-terminal reliability\nsystems, i.e., where all nodes are terminals, we propose an algorithm for\napproximating the failure frequency within an arbitrary multiplicative error\nthat runs in polynomial time in the number of nodes (which can be much smaller\nthan the number of cutsets). In addition, our simulation results confirm that\nthe proposed method is much faster and more accurate than the Monte Carlo\nsimulation technique for approximating the failure frequency.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 20:05:35 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Heidarzadeh", "Anoosheh", ""], ["Sprintson", "Alex", ""], ["Singh", "Chanan", ""]]}, {"id": "1712.09679", "submitter": "Yuxiang Jiang", "authors": "Yisu Peng, Yuxiang Jiang, Predrag Radivojac", "title": "Enumerating consistent subgraphs of directed acyclic graphs: an insight\n  into biomedical ontologies", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM q-bio.BM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Modern problems of concept annotation associate an object of interest (gene,\nindividual, text document) with a set of interrelated textual descriptors\n(functions, diseases, topics), often organized in concept hierarchies or\nontologies. Most ontologies can be seen as directed acyclic graphs, where nodes\nrepresent concepts and edges represent relational ties between these concepts.\nGiven an ontology graph, each object can only be annotated by a consistent\nsubgraph; that is, a subgraph such that if an object is annotated by a\nparticular concept, it must also be annotated by all other concepts that\ngeneralize it. Ontologies therefore provide a compact representation of a large\nspace of possible consistent subgraphs; however, until now we have not been\naware of a practical algorithm that can enumerate such annotation spaces for a\ngiven ontology. In this work we propose an algorithm for enumerating consistent\nsubgraphs of directed acyclic graphs. The algorithm recursively partitions the\ngraph into strictly smaller graphs until the resulting graph becomes a rooted\ntree (forest), for which a linear-time solution is computed. It then combines\nthe tallies from graphs created in the recursion to obtain the final count. We\nprove the correctness of this algorithm and then apply it to characterize four\nmajor biomedical ontologies. We believe this work provides valuable insights\ninto concept annotation spaces and predictability of ontological annotation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 20:45:31 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Peng", "Yisu", ""], ["Jiang", "Yuxiang", ""], ["Radivojac", "Predrag", ""]]}, {"id": "1712.09738", "submitter": "Neng Huang", "authors": "Xiaoyu He, Neng Huang, Xiaoming Sun", "title": "On the Decision Tree Complexity of String Matching", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  String matching is one of the most fundamental problems in computer science.\nA natural problem is to determine the number of characters that need to be\nqueried (i.e. the decision tree complexity) in a string in order to decide\nwhether this string contains a certain pattern. Rivest showed that for every\npattern $p$, in the worst case any deterministic algorithm needs to query at\nleast $n-|p|+1$ characters, where $n$ is the length of the string and $|p|$ is\nthe length of the pattern. He further conjectured that this bound is tight. By\nusing the adversary method, Tuza disproved this conjecture and showed that more\nthan one half of binary patterns are {\\em evasive}, i.e. any algorithm needs to\nquery all the characters (see Section 1.1 for more details).\n  In this paper, we give a query algorithm which settles the decision tree\ncomplexity of string matching except for a negligible fraction of patterns. Our\nalgorithm shows that Tuza's criteria of evasive patterns are almost complete.\nUsing the algebraic approach of Rivest and Vuillemin, we also give a new\nsufficient condition for the evasiveness of patterns, which is beyond Tuza's\ncriteria. In addition, our result reveals an interesting connection to\n\\emph{Skolem's Problem} in mathematics.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 02:17:49 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 11:33:31 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["He", "Xiaoyu", ""], ["Huang", "Neng", ""], ["Sun", "Xiaoming", ""]]}, {"id": "1712.09856", "submitter": "Nicola Galesi", "authors": "Nicola Galesi and Fariba Ranjbar", "title": "Tight Bounds for Maximal Identifiability of Failure Nodes in Boolean\n  Network Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study maximal identifiability, a measure recently introduced in Boolean\nNetwork Tomography to characterize networks' capability to localize failure\nnodes in end-to-end path measurements. We prove tight upper and lower bounds on\nthe maximal identifiability of failure nodes for specific classes of network\ntopologies, such as trees and $d$-dimensional grids, in both directed and\nundirected cases. We prove that directed $d$-dimensional grids with support $n$\nhave maximal identifiability $d$ using $2d(n-1)+2$ monitors; and in the\nundirected case we show that $2d$ monitors suffice to get identifiability of\n$d-1$. We then study identifiability under embeddings: we establish relations\nbetween maximal identifiability, embeddability and graph dimension when network\ntopologies are model as DAGs. Our results suggest the design of networks over\n$N$ nodes with maximal identifiability $\\Omega(\\log N)$ using $O(\\log N)$\nmonitors and a heuristic to boost maximal identifiability on a given network by\nsimulating $d$-dimensional grids. We provide positive evidence of this\nheuristic through data extracted by exact computation of maximal\nidentifiability on examples of small real networks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 13:42:54 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 07:12:17 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Galesi", "Nicola", ""], ["Ranjbar", "Fariba", ""]]}, {"id": "1712.09948", "submitter": "Charalampos Tsourakakis", "authors": "Cameron Musco, Christopher Musco, Charalampos E. Tsourakakis", "title": "Minimizing Polarization and Disagreement in Social Networks", "comments": "19 pages (accepted, WWW 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of social media and online social networks has been a disruptive\nforce in society. Opinions are increasingly shaped by interactions on online\nsocial media, and social phenomena including disagreement and polarization are\nnow tightly woven into everyday life. In this work we initiate the study of the\nfollowing question: given $n$ agents, each with its own initial opinion that\nreflects its core value on a topic, and an opinion dynamics model, what is the\nstructure of a social network that minimizes {\\em polarization} and {\\em\ndisagreement} simultaneously?\n  This question is central to recommender systems: should a recommender system\nprefer a link suggestion between two online users with similar mindsets in\norder to keep disagreement low, or between two users with different opinions in\norder to expose each to the other's viewpoint of the world, and decrease\noverall levels of polarization? Our contributions include a mathematical\nformalization of this question as an optimization problem and an exact,\ntime-efficient algorithm. We also prove that there always exists a network with\n$O(n/\\epsilon^2)$ edges that is a $(1+\\epsilon)$ approximation to the optimum.\nFor a fixed graph, we additionally show how to optimize our objective function\nover the agents' innate opinions in polynomial time.\n  We perform an empirical study of our proposed methods on synthetic and\nreal-world data that verify their value as mining tools to better understand\nthe trade-off between of disagreement and polarization. We find that there is a\nlot of space to reduce both polarization and disagreement in real-world\nnetworks; for instance, on a Reddit network where users exchange comments on\npolitics, our methods achieve a $\\sim 60\\,000$-fold reduction in polarization\nand disagreement.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 17:33:22 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Musco", "Cameron", ""], ["Musco", "Christopher", ""], ["Tsourakakis", "Charalampos E.", ""]]}, {"id": "1712.10163", "submitter": "Alexander Wein", "authors": "Afonso S. Bandeira, Ben Blum-Smith, Joe Kileel, Amelia Perry, Jonathan\n  Weed, Alexander S. Wein", "title": "Estimation under group actions: recovering orbits from invariants", "comments": "54 pages. This version contains a number of new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.IT math.AC math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by geometric problems in signal processing, computer vision, and\nstructural biology, we study a class of orbit recovery problems where we\nobserve very noisy copies of an unknown signal, each acted upon by a random\nelement of some group (such as Z/p or SO(3)). The goal is to recover the orbit\nof the signal under the group action in the high-noise regime. This generalizes\nproblems of interest such as multi-reference alignment (MRA) and the\nreconstruction problem in cryo-electron microscopy (cryo-EM). We obtain\nmatching lower and upper bounds on the sample complexity of these problems in\nhigh generality, showing that the statistical difficulty is intricately\ndetermined by the invariant theory of the underlying symmetry group.\n  In particular, we determine that for cryo-EM with noise variance $\\sigma^2$\nand uniform viewing directions, the number of samples required scales as\n$\\sigma^6$. We match this bound with a novel algorithm for ab initio\nreconstruction in cryo-EM, based on invariant features of degree at most 3. We\nfurther discuss how to recover multiple molecular structures from heterogeneous\ncryo-EM samples.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 09:53:24 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 17:35:19 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Bandeira", "Afonso S.", ""], ["Blum-Smith", "Ben", ""], ["Kileel", "Joe", ""], ["Perry", "Amelia", ""], ["Weed", "Jonathan", ""], ["Wein", "Alexander S.", ""]]}, {"id": "1712.10197", "submitter": "Bala Krishnamoorthy", "authors": "Ananth Kalyanaraman, Methun Kamruzzaman, and Bala Krishnamoorthy", "title": "Interesting Paths in the Mapper", "comments": "NP-completeness of k-IP shown only for DAGs now; connections to\n  coboundary operations outlined", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Mapper produces a compact summary of high dimensional data as a\nsimplicial complex. We study the problem of quantifying the interestingness of\nsubpopulations in a Mapper, which appear as long paths, flares, or loops.\nFirst, we create a weighted directed graph G using the 1-skeleton of the\nMapper. We use the average values at the vertices of a target function to\ndirect edges (from low to high). The difference between the average values at\nvertices (high-low) is set as the edge's weight. Covariation of the remaining h\nfunctions (independent variables) is captured by a h-bit binary signature\nassigned to the edge. An interesting path in G is a directed path whose edges\nall have the same signature. We define the interestingness score of such a path\nas a sum of its edge weights multiplied by a nonlinear function of their ranks\nin the path.\n  Second, we study three optimization problems on this graph G. In the problem\nMax-IP, we seek an interesting path in G with the maximum interestingness\nscore. We show that Max-IP is NP-complete. For the special case when G is a\ndirected acyclic graph (DAG), we show that Max-IP can be solved in polynomial\ntime - in O(mnd_i) where d_i is the maximum indegree of a vertex in G.\n  In the more general problem IP, the goal is to find a collection of\nedge-disjoint interesting paths such that the overall sum of their\ninterestingness scores is maximized. We also study a variant of IP termed k-IP,\nwhere the goal is to identify a collection of edge-disjoint interesting paths\neach with k edges, and their total interestingness score is maximized. While\nk-IP can be solved in polynomial time for k <= 2, we show k-IP is NP-complete\nfor k >= 3 even when G is a DAG. We develop polynomial time heuristics for IP\nand k-IP on DAGs.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 12:11:23 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 07:17:05 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Kalyanaraman", "Ananth", ""], ["Kamruzzaman", "Methun", ""], ["Krishnamoorthy", "Bala", ""]]}, {"id": "1712.10261", "submitter": "Nikhil Srivastava", "authors": "Charles Carlson and Alexandra Kolla and Nikhil Srivastava and Luca\n  Trevisan", "title": "Optimal Lower Bounds for Sketching Graph Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the space complexity of sketching cuts and Laplacian quadratic forms\nof graphs. We show that any data structure which approximately stores the sizes\nof all cuts in an undirected graph on $n$ vertices up to a $1+\\epsilon$ error\nmust use $\\Omega(n\\log n/\\epsilon^2)$ bits of space in the worst case,\nimproving the $\\Omega(n/\\epsilon^2)$ bound of Andoni et al. and matching the\nbest known upper bound achieved by spectral sparsifiers. Our proof is based on\na rigidity phenomenon for cut (and spectral) approximation which may be of\nindependent interest: any two $d-$regular graphs which approximate each other's\ncuts significantly better than a random graph approximates the complete graph\nmust overlap in a constant fraction of their edges.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 15:44:08 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Carlson", "Charles", ""], ["Kolla", "Alexandra", ""], ["Srivastava", "Nikhil", ""], ["Trevisan", "Luca", ""]]}, {"id": "1712.10273", "submitter": "Noam Touitou", "authors": "Yossi Azar, Noam Touitou", "title": "Improved Online Algorithm for Weighted Flow Time", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss one of the most fundamental scheduling problem of processing jobs\non a single machine to minimize the weighted flow time (weighted response\ntime). Our main result is a $O(\\log P)$-competitive algorithm, where $P$ is the\nmaximum-to-minimum processing time ratio, improving upon the\n$O(\\log^{2}P)$-competitive algorithm of Chekuri, Khanna and Zhu (STOC 2001). We\nalso design a $O(\\log D)$-competitive algorithm, where $D$ is the\nmaximum-to-minimum density ratio of jobs. Finally, we show how to combine these\nresults with the result of Bansal and Dhamdhere (SODA 2003) to achieve a\n$O(\\log(\\min(P,D,W)))$-competitive algorithm (where $W$ is the\nmaximum-to-minimum weight ratio), without knowing $P,D,W$ in advance. As shown\nby Bansal and Chan (SODA 2009), no constant-competitive algorithm is achievable\nfor this problem.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 16:35:59 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 15:43:10 GMT"}, {"version": "v3", "created": "Thu, 16 Aug 2018 00:16:41 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Azar", "Yossi", ""], ["Touitou", "Noam", ""]]}]