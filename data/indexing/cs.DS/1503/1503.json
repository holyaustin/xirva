[{"id": "1503.00049", "submitter": "Ali Alatabbi", "authors": "Ali Alatabbi and Costas S. Iliopoulos and Alessio Langiu and M. Sohel\n  Rahman", "title": "Algorithms for Longest Common Abelian Factors", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of computing the longest common abelian\nfactor (LCAF) between two given strings. We present a simple $O(\\sigma~ n^2)$\ntime algorithm, where $n$ is the length of the strings and $\\sigma$ is the\nalphabet size, and a sub-quadratic running time solution for the binary string\ncase, both having linear space requirement. Furthermore, we present a modified\nalgorithm applying some interesting tricks and experimentally show that the\nresulting algorithm runs faster.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 01:20:55 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Alatabbi", "Ali", ""], ["Iliopoulos", "Costas S.", ""], ["Langiu", "Alessio", ""], ["Rahman", "M. Sohel", ""]]}, {"id": "1503.00067", "submitter": "Tadao Takaoka", "authors": "Tadao Takaoka", "title": "O(1) Time Generation of Adjacent Multiset Combinations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve the problem of designing an O(1) time algorithm for generating\nadjacent multiset combinations in a different approach from Walsh. By the word\nadjacent, we mean that two adjacent multiset combinations are different at two\nplaces by one in their vector forms. Previous O(1) time algorithms for multiset\ncombinations generated non-adjacent multiset combinations. Our algorithm in\nthis paper can be derived from a general framework of combinatorial Gray code,\nwhich we characterise to suit our need for combinations and multiset\ncombinations. The central idea is a twisted lexico tree, which is obtained from\nthe lexicographic tree for the given set of combinatorial objects by twisting\nbranches depending on the parity of each node. An iterative algorithm which\ntraverses this tree will generate the given set of combinatorial objects in\nconstant time as well as with a fixed number of changes from the present\ncombinatorial object to the next.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 04:57:28 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Takaoka", "Tadao", ""]]}, {"id": "1503.00190", "submitter": "Martin Grohe", "authors": "Martin Grohe and Pascal Schweitzer", "title": "Computing with Tangles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tangles of graphs have been introduced by Robertson and Seymour in the\ncontext of their graph minor theory. Tangles may be viewed as describing\n\"k-connected components\" of a graph (though in a twisted way). They play an\nimportant role in graph minor theory. An interesting aspect of tangles is that\nthey cannot only be defined for graphs, but more generally for arbitrary\nconnectivity functions (that is, integer-valued submodular and symmetric set\nfunctions).\n  However, tangles are difficult to deal with algorithmically. To start with,\nit is unclear how to represent them, because they are families of separations\nand as such may be exponentially large. Our first contribution is a data\nstructure for representing and accessing all tangles of a graph up to some\nfixed order.\n  Using this data structure, we can prove an algorithmic version of a very\ngeneral structure theorem due to Carmesin, Diestel, Harman and Hundertmark (for\ngraphs) and Hundertmark (for arbitrary connectivity functions) that yields a\ncanonical tree decomposition whose parts correspond to the maximal tangles.\n(This may be viewed as a generalisation of the decomposition of a graph into\nits 3-connected components.)\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 22:21:30 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 15:31:16 GMT"}, {"version": "v3", "created": "Wed, 2 Mar 2016 19:22:33 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Grohe", "Martin", ""], ["Schweitzer", "Pascal", ""]]}, {"id": "1503.00260", "submitter": "Hubie Chen", "authors": "Hubie Chen", "title": "Parameter Compilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In resolving instances of a computational problem, if multiple instances of\ninterest share a feature in common, it may be fruitful to compile this feature\ninto a format that allows for more efficient resolution, even if the\ncompilation is relatively expensive. In this article, we introduce a formal\nframework for classifying problems according to their compilability. The basic\nobject in our framework is that of a parameterized problem, which here is a\nlanguage along with a parameterization---a map which provides, for each\ninstance, a so-called parameter on which compilation may be performed. Our\nframework is positioned within the paradigm of parameterized complexity, and\nour notions are relatable to established concepts in the theory of\nparameterized complexity. Indeed, we view our framework as playing a unifying\nrole, integrating together parameterized complexity and compilability theory.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 12:23:06 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Chen", "Hubie", ""]]}, {"id": "1503.00278", "submitter": "Othon Michail", "authors": "Othon Michail", "title": "An Introduction to Temporal Graphs: An Algorithmic Perspective", "comments": "42 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A \\emph{temporal graph} is, informally speaking, a graph that changes with\ntime. When time is discrete and only the relationships between the\nparticipating entities may change and not the entities themselves, a temporal\ngraph may be viewed as a sequence $G_1,G_2\\ldots,G_l$ of static graphs over the\nsame (static) set of nodes $V$. Though static graphs have been extensively\nstudied, for their temporal generalization we are still far from having a\nconcrete set of structural and algorithmic principles. Recent research shows\nthat many graph properties and problems become radically different and usually\nsubstantially more difficult when an extra time dimension in added to them.\nMoreover, there is already a rich and rapidly growing set of modern systems and\napplications that can be naturally modeled and studied via temporal graphs.\nThis, further motivates the need for the development of a temporal extension of\ngraph theory. We survey here recent results on temporal graphs and temporal\ngraph problems that have appeared in the Computer Science community.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 14:04:28 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Michail", "Othon", ""]]}, {"id": "1503.00368", "submitter": "Steven Kelk", "authors": "Steven Kelk, Leo van Iersel, Celine Scornavacca", "title": "Phylogenetic incongruence through the lens of Monadic Second Order logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE cs.LO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the field of phylogenetics there is growing interest in measures for\nsummarising the dissimilarity, or 'incongruence', of two or more phylogenetic\ntrees. Many of these measures are NP-hard to compute and this has stimulated a\nconsiderable volume of research into fixed parameter tractable algorithms. In\nthis article we use Monadic Second Order logic (MSOL) to give alternative,\ncompact proofs of fixed parameter tractability for several well-known\nincongruency measures. In doing so we wish to demonstrate the considerable\npotential of MSOL - machinery still largely unknown outside the algorithmic\ngraph theory community - within phylogenetics. A crucial component of this work\nis the observation that many of these measures, when bounded, imply the\nexistence of an 'agreement forest' of bounded size, which in turn implies that\nan auxiliary graph structure, the display graph, has bounded treewidth. It is\nthis bound on treewidth that makes the machinery of MSOL available for proving\nfixed parameter tractability. We give a variety of different MSOL formulations.\nSome are based on explicitly encoding agreement forests, while some only use\nthem implicitly to generate the treewidth bound. Our formulations introduce a\nnumber of \"phylogenetics MSOL primitives\" which will hopefully be of use to\nother researchers.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 22:31:31 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Kelk", "Steven", ""], ["van Iersel", "Leo", ""], ["Scornavacca", "Celine", ""]]}, {"id": "1503.00374", "submitter": "Eugenia-Maria Kontopoulou", "authors": "Christos Boutsidis and Petros Drineas and Prabhanjan Kambadur and\n  Eugenia-Maria Kontopoulou and Anastasios Zouzias", "title": "A Randomized Algorithm for Approximating the Log Determinant of a\n  Symmetric Positive Definite Matrix", "comments": "working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel algorithm for approximating the logarithm of the\ndeterminant of a symmetric positive definite (SPD) matrix. The algorithm is\nrandomized and approximates the traces of a small number of matrix powers of a\nspecially constructed matrix, using the method of Avron and Toledo~\\cite{AT11}.\nFrom a theoretical perspective, we present additive and relative error bounds\nfor our algorithm. Our additive error bound works for any SPD matrix, whereas\nour relative error bound works for SPD matrices whose eigenvalues lie in the\ninterval $(\\theta_1,1)$, with $0<\\theta_1<1$; the latter setting was proposed\nin~\\cite{icml2015_hana15}. From an empirical perspective, we demonstrate that a\nC++ implementation of our algorithm can approximate the logarithm of the\ndeterminant of large matrices very accurately in a matter of seconds.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 23:25:13 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 17:12:16 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Boutsidis", "Christos", ""], ["Drineas", "Petros", ""], ["Kambadur", "Prabhanjan", ""], ["Kontopoulou", "Eugenia-Maria", ""], ["Zouzias", "Anastasios", ""]]}, {"id": "1503.00423", "submitter": "Sam Cole", "authors": "Sam Cole, Shmuel Friedland, Lev Reyzin", "title": "A Simple Spectral Algorithm for Recovering Planted Partitions", "comments": "21 pages + title page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the planted partition model, in which $n = ks$\nvertices of a random graph are partitioned into $k$ \"clusters,\" each of size\n$s$. Edges between vertices in the same cluster and different clusters are\nincluded with constant probability $p$ and $q$, respectively (where $0 \\le q <\np \\le 1$). We give an efficient algorithm that, with high probability, recovers\nthe clusters as long as the cluster sizes are are least $\\Omega(\\sqrt{n})$.\nInformally, our algorithm constructs the projection operator onto the dominant\n$k$-dimensional eigenspace of the graph's adjacency matrix and uses it to\nrecover one cluster at a time. To our knowledge, our algorithm is the first\npurely spectral algorithm which runs in polynomial time and works even when $s\n= \\Theta(\\sqrt n)$, though there have been several non-spectral algorithms\nwhich accomplish this. Our algorithm is also among the simplest of these\nspectral algorithms, and its proof of correctness illustrates the usefulness of\nthe Cauchy integral formula in this domain.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 06:53:39 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 19:03:37 GMT"}, {"version": "v3", "created": "Wed, 2 Nov 2016 00:31:37 GMT"}, {"version": "v4", "created": "Fri, 25 Aug 2017 16:37:23 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Cole", "Sam", ""], ["Friedland", "Shmuel", ""], ["Reyzin", "Lev", ""]]}, {"id": "1503.00516", "submitter": "Johann Bengua", "authors": "Johann A. Bengua, Ho N. Phien, Hoang D. Tuan and Minh N. Do", "title": "Matrix Product State for Feature Extraction of Higher-Order Tensors", "comments": "10 pages, 3 figures, updated introduction, submitted to IEEE\n  Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces matrix product state (MPS) decomposition as a\ncomputational tool for extracting features of multidimensional data represented\nby higher-order tensors. Regardless of tensor order, MPS extracts its relevant\nfeatures to the so-called core tensor of maximum order three which can be used\nfor classification. Mainly based on a successive sequence of singular value\ndecompositions (SVD), MPS is quite simple to implement without any recursive\nprocedure needed for optimizing local tensors. Thus, it leads to substantial\ncomputational savings compared to other tensor feature extraction methods such\nas higher-order orthogonal iteration (HOOI) underlying the Tucker decomposition\n(TD). Benchmark results show that MPS can reduce significantly the feature\nspace of data while achieving better classification performance compared to\nHOOI.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 13:20:25 GMT"}, {"version": "v2", "created": "Mon, 25 May 2015 22:45:24 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 21:29:47 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2016 22:11:39 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Bengua", "Johann A.", ""], ["Phien", "Ho N.", ""], ["Tuan", "Hoang D.", ""], ["Do", "Minh N.", ""]]}, {"id": "1503.00540", "submitter": "Caterina De Bacco", "authors": "Fabrizio Altarelli, Alfredo Braunstein, Luca Dall'Asta, Caterina De\n  Bacco and Silvio Franz", "title": "The edge-disjoint path problem on random graphs by message-passing", "comments": "14 pages, 8 figures", "journal-ref": "PLoS ONE 10(12): e0145222 (2015)", "doi": "10.1371/journal.pone.0145222", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a message-passing algorithm to solve the edge disjoint path\nproblem (EDP) on graphs incorporating under a unique framework both traffic\noptimization and path length minimization. The min-sum equations for this\nproblem present an exponential computational cost in the number of paths. To\novercome this obstacle we propose an efficient implementation by mapping the\nequations onto a weighted combinatorial matching problem over an auxiliary\ngraph. We perform extensive numerical simulations on random graphs of various\ntypes to test the performance both in terms of path length minimization and\nmaximization of the number of accommodated paths. In addition, we test the\nperformance on benchmark instances on various graphs by comparison with\nstate-of-the-art algorithms and results found in the literature. Our\nmessage-passing algorithm always outperforms the others in terms of the number\nof accommodated paths when considering non trivial instances (otherwise it\ngives the same trivial results). Remarkably, the largest improvement in\nperformance with respect to the other methods employed is found in the case of\nbenchmarks with meshes, where the validity hypothesis behind message-passing is\nexpected to worsen. In these cases, even though the exact message-passing\nequations do not converge, by introducing a reinforcement parameter to force\nconvergence towards a sub optimal solution, we were able to always outperform\nthe other algorithms with a peak of 27% performance improvement in terms of\naccommodated paths. On random graphs, we numerically observe two separated\nregimes: one in which all paths can be accommodated and one in which this is\nnot possible. We also investigate the behaviour of both the number of paths to\nbe accommodated and their minimum total length.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 14:27:02 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Altarelli", "Fabrizio", ""], ["Braunstein", "Alfredo", ""], ["Dall'Asta", "Luca", ""], ["De Bacco", "Caterina", ""], ["Franz", "Silvio", ""]]}, {"id": "1503.00617", "submitter": "Martin F\\\"urer", "authors": "Martin F\\\"urer", "title": "Efficient Computation of the Characteristic Polynomial of a Threshold\n  Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient algorithm is presented to compute the characteristic polynomial\nof a threshold graph. Threshold graphs were introduced by Chv\\'atal and Hammer,\nas well as by Henderson and Zalcstein in 1977. A threshold graph is obtained\nfrom a one vertex graph by repeatedly adding either an isolated vertex or a\ndominating vertex, which is a vertex adjacent to all the other vertices.\nThreshold graphs are special kinds of cographs, which themselves are special\nkinds of graphs of clique-width 2. We obtain a running time of $O(n \\log^2 n)$\nfor computing the characteristic polynomial, while the previously fastest\nalgorithm ran in quadratic time. Keywords: Efficient Algorithms, Threshold\nGraphs, Characteristic Polynomial.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 16:57:51 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["F\u00fcrer", "Martin", ""]]}, {"id": "1503.00658", "submitter": "Michael Mitzenmacher", "authors": "Michael Mitzenmacher", "title": "More Analysis of Double Hashing for Balanced Allocations", "comments": "13 pages ; current draft ; will be submitted to conference shortly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With double hashing, for a key $x$, one generates two hash values $f(x)$ and\n$g(x)$, and then uses combinations $(f(x) +i g(x)) \\bmod n$ for $i=0,1,2,...$\nto generate multiple hash values in the range $[0,n-1]$ from the initial two.\nFor balanced allocations, keys are hashed into a hash table where each bucket\ncan hold multiple keys, and each key is placed in the least loaded of $d$\nchoices. It has been shown previously that asymptotically the performance of\ndouble hashing and fully random hashing is the same in the balanced allocation\nparadigm using fluid limit methods. Here we extend a coupling argument used by\nLueker and Molodowitch to show that double hashing and ideal uniform hashing\nare asymptotically equivalent in the setting of open address hash tables to the\nbalanced allocation setting, providing further insight into this phenomenon. We\nalso discuss the potential for and bottlenecks limiting the use this approach\nfor other multiple choice hashing schemes.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 18:48:10 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Mitzenmacher", "Michael", ""]]}, {"id": "1503.00704", "submitter": "Marcin Pilipczuk", "authors": "Marek Cygan and Marcin Pilipczuk and Micha{\\l} Pilipczuk and Erik Jan\n  van Leeuwen and Marcin Wrochna", "title": "Polynomial kernelization for removing induced claws and diamonds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is called (claw,diamond)-free if it contains neither a claw (a\n$K_{1,3}$) nor a diamond (a $K_4$ with an edge removed) as an induced subgraph.\nEquivalently, (claw,diamond)-free graphs can be characterized as line graphs of\ntriangle-free graphs, or as linear dominoes, i.e., graphs in which every vertex\nis in at most two maximal cliques and every edge is in exactly one maximal\nclique.\n  In this paper we consider the parameterized complexity of the\n(claw,diamond)-free Edge Deletion problem, where given a graph $G$ and a\nparameter $k$, the question is whether one can remove at most $k$ edges from\n$G$ to obtain a (claw,diamond)-free graph. Our main result is that this problem\nadmits a polynomial kernel. We complement this finding by proving that, even on\ninstances with maximum degree $6$, the problem is NP-complete and cannot be\nsolved in time $2^{o(k)}\\cdot |V(G)|^{O(1)}$ unless the Exponential Time\nHypothesis fail\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 20:49:51 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Cygan", "Marek", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""], ["van Leeuwen", "Erik Jan", ""], ["Wrochna", "Marcin", ""]]}, {"id": "1503.00778", "submitter": "Tengyu Ma", "authors": "Sanjeev Arora, Rong Ge, Tengyu Ma, Ankur Moitra", "title": "Simple, Efficient, and Neural Algorithms for Sparse Coding", "comments": "37 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding is a basic task in many fields including signal processing,\nneuroscience and machine learning where the goal is to learn a basis that\nenables a sparse representation of a given set of data, if one exists. Its\nstandard formulation is as a non-convex optimization problem which is solved in\npractice by heuristics based on alternating minimization. Re- cent work has\nresulted in several algorithms for sparse coding with provable guarantees, but\nsomewhat surprisingly these are outperformed by the simple alternating\nminimization heuristics. Here we give a general framework for understanding\nalternating minimization which we leverage to analyze existing heuristics and\nto design new ones also with provable guarantees. Some of these algorithms seem\nimplementable on simple neural architectures, which was the original motivation\nof Olshausen and Field (1997a) in introducing sparse coding. We also give the\nfirst efficient algorithm for sparse coding that works almost up to the\ninformation theoretic limit for sparse recovery on incoherent dictionaries. All\nprevious algorithms that approached or surpassed this limit run in time\nexponential in some natural parameter. Finally, our algorithms improve upon the\nsample complexity of existing approaches. We believe that our analysis\nframework will have applications in other settings where simple iterative\nalgorithms are used.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 23:02:56 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Arora", "Sanjeev", ""], ["Ge", "Rong", ""], ["Ma", "Tengyu", ""], ["Moitra", "Ankur", ""]]}, {"id": "1503.00805", "submitter": "Ehsan Emamjomeh-Zadeh", "authors": "Ehsan Emamjomeh-Zadeh, David Kempe, Vikrant Singhal", "title": "Deterministic and Probabilistic Binary Search in Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following natural generalization of Binary Search: in a given\nundirected, positively weighted graph, one vertex is a target. The algorithm's\ntask is to identify the target by adaptively querying vertices. In response to\nquerying a node $q$, the algorithm learns either that $q$ is the target, or is\ngiven an edge out of $q$ that lies on a shortest path from $q$ to the target.\nWe study this problem in a general noisy model in which each query\nindependently receives a correct answer with probability $p > \\frac{1}{2}$ (a\nknown constant), and an (adversarial) incorrect one with probability $1-p$.\n  Our main positive result is that when $p = 1$ (i.e., all answers are\ncorrect), $\\log_2 n$ queries are always sufficient. For general $p$, we give an\n(almost information-theoretically optimal) algorithm that uses, in expectation,\nno more than $(1 - \\delta)\\frac{\\log_2 n}{1 - H(p)} + o(\\log n) + O(\\log^2\n(1/\\delta))$ queries, and identifies the target correctly with probability at\nleas $1-\\delta$. Here, $H(p) = -(p \\log p + (1-p) \\log(1-p))$ denotes the\nentropy. The first bound is achieved by the algorithm that iteratively queries\na 1-median of the nodes not ruled out yet; the second bound by careful repeated\ninvocations of a multiplicative weights algorithm.\n  Even for $p = 1$, we show several hardness results for the problem of\ndetermining whether a target can be found using $K$ queries. Our upper bound of\n$\\log_2 n$ implies a quasipolynomial-time algorithm for undirected connected\ngraphs; we show that this is best-possible under the Strong Exponential Time\nHypothesis (SETH). Furthermore, for directed graphs, or for undirected graphs\nwith non-uniform node querying costs, the problem is PSPACE-complete. For a\nsemi-adaptive version, in which one may query $r$ nodes each in $k$ rounds, we\nshow membership in $\\Sigma_{2k-1}$ in the polynomial hierarchy, and hardness\nfor $\\Sigma_{2k-5}$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 02:19:23 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 08:30:04 GMT"}, {"version": "v3", "created": "Sat, 29 Jul 2017 02:26:17 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Emamjomeh-Zadeh", "Ehsan", ""], ["Kempe", "David", ""], ["Singhal", "Vikrant", ""]]}, {"id": "1503.00827", "submitter": "Ali Sinop", "authors": "Ali Kemal Sinop", "title": "How to Round Subspaces: A New Spectral Clustering Algorithm", "comments": "Appeared in SODA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic problem in spectral clustering is the following. If a solution\nobtained from the spectral relaxation is close to an integral solution, is it\npossible to find this integral solution even though they might be in completely\ndifferent basis? In this paper, we propose a new spectral clustering algorithm.\nIt can recover a $k$-partition such that the subspace corresponding to the span\nof its indicator vectors is $O(\\sqrt{opt})$ close to the original subspace in\nspectral norm with $opt$ being the minimum possible ($opt \\le 1$ always).\nMoreover our algorithm does not impose any restriction on the cluster sizes.\nPreviously, no algorithm was known which could find a $k$-partition closer than\n$o(k \\cdot opt)$.\n  We present two applications for our algorithm. First one finds a disjoint\nunion of bounded degree expanders which approximate a given graph in spectral\nnorm. The second one is for approximating the sparsest $k$-partition in a graph\nwhere each cluster have expansion at most $\\phi_k$ provided $\\phi_k \\le\nO(\\lambda_{k+1})$ where $\\lambda_{k+1}$ is the $(k+1)^{st}$ eigenvalue of\nLaplacian matrix. This significantly improves upon the previous algorithms,\nwhich required $\\phi_k \\le O(\\lambda_{k+1}/k)$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 04:44:04 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2015 22:19:52 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2015 01:02:31 GMT"}, {"version": "v4", "created": "Sat, 18 Jul 2015 05:10:10 GMT"}, {"version": "v5", "created": "Mon, 19 Oct 2015 09:04:08 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Sinop", "Ali Kemal", ""]]}, {"id": "1503.00833", "submitter": "Takehiro Ito", "authors": "Arash Haddadan, Takehiro Ito, Amer E. Mouawad, Naomi Nishimura,\n  Hirotaka Ono, Akira Suzuki, Youcef Tebbal", "title": "The complexity of dominating set reconfiguration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we are given two dominating sets $D_s$ and $D_t$ of a graph $G$\nwhose cardinalities are at most a given threshold $k$. Then, we are asked\nwhether there exists a sequence of dominating sets of $G$ between $D_s$ and\n$D_t$ such that each dominating set in the sequence is of cardinality at most\n$k$ and can be obtained from the previous one by either adding or deleting\nexactly one vertex. This problem is known to be PSPACE-complete in general. In\nthis paper, we study the complexity of this decision problem from the viewpoint\nof graph classes. We first prove that the problem remains PSPACE-complete even\nfor planar graphs, bounded bandwidth graphs, split graphs, and bipartite\ngraphs. We then give a general scheme to construct linear-time algorithms and\nshow that the problem can be solved in linear time for cographs, trees, and\ninterval graphs. Furthermore, for these tractable cases, we can obtain a\ndesired sequence such that the number of additions and deletions is bounded by\n$O(n)$, where $n$ is the number of vertices in the input graph.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 06:44:38 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Haddadan", "Arash", ""], ["Ito", "Takehiro", ""], ["Mouawad", "Amer E.", ""], ["Nishimura", "Naomi", ""], ["Ono", "Hirotaka", ""], ["Suzuki", "Akira", ""], ["Tebbal", "Youcef", ""]]}, {"id": "1503.01058", "submitter": "Aleksandr Cariow", "authors": "Aleksandr Cariow, Galina Cariowa, Bartosz Kubsik", "title": "An algorithm for multiplication of split-octonions", "comments": "14 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:1502.06250", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce efficient algorithm for the multiplication of\nsplit-octonions. The direct multiplication of two split-octonions requires 64\nreal multiplications and 56 real additions. More effective solutions still do\nnot exist. We show how to compute a product of the split-octonions with 28 real\nmultiplications and 92 real additions. During synthesis of the discussed\nalgorithm we use the fact that product of two split-octonions may be\nrepresented as vector-matrix product. The matrix that participates in the\nproduct calculating has unique structural properties that allow performing its\nadvantageous decomposition. Namely this decomposition leads to significant\nreducing of the multiplicative complexity of split-octonions multiplication.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 19:09:30 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Cariow", "Aleksandr", ""], ["Cariowa", "Galina", ""], ["Kubsik", "Bartosz", ""]]}, {"id": "1503.01093", "submitter": "Szymon Grabowski", "authors": "Szymon Grabowski", "title": "A note on the longest common Abelian factor problem", "comments": "v3 is vastly different to the previous one", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abelian string matching problems are becoming an object of considerable\ninterest in last years. Very recently, Alatabbi et al. \\cite{AILR2015}\npresented the first solution for the longest common Abelian factor problem for\na pair of strings, reaching $O(\\sigma n^2)$ time with $O(\\sigma n \\log n)$ bits\nof space, where $n$ is the length of the strings and $\\sigma$ is the alphabet\nsize. In this note we show how the time complexity can be preserved while the\nspace is reduced by a factor of $\\sigma$, and then how the time complexity can\nbe improved, if the alphabet is not too small, when superlinear space is\nallowed.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 20:37:41 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 18:59:52 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2015 19:10:20 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Grabowski", "Szymon", ""]]}, {"id": "1503.01098", "submitter": "Eun Jung Kim", "authors": "Eun Jung Kim, Martin Milanic, Oliver Schaudt", "title": "Recognizing k-equistable graphs in FPT time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph $G = (V,E)$ is called equistable if there exist a positive integer\n$t$ and a weight function $w : V \\to \\mathbb{N}$ such that $S \\subseteq V$ is a\nmaximal stable set of $G$ if and only if $w(S) = t$. Such a function $w$ is\ncalled an equistable function of $G$. For a positive integer $k$, a graph $G =\n(V,E)$ is said to be $k$-equistable if it admits an equistable function which\nis bounded by $k$.\n  We prove that the problem of recognizing $k$-equistable graphs is fixed\nparameter tractable when parameterized by $k$, affirmatively answering a\nquestion of Levit et al. In fact, the problem admits an $O(k^5)$-vertex kernel\nthat can be computed in linear time.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 20:47:07 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Kim", "Eun Jung", ""], ["Milanic", "Martin", ""], ["Schaudt", "Oliver", ""]]}, {"id": "1503.01156", "submitter": "David Felber", "authors": "David Felber and Rafail Ostrovsky", "title": "A randomized online quantile summary in $O(\\frac{1}{\\varepsilon} \\log\n  \\frac{1}{\\varepsilon})$ words", "comments": "slight fixes to version submitted to ICALP 2015--mistake in time\n  complexity, and a few minor numeric miscalculations in section 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A quantile summary is a data structure that approximates to\n$\\varepsilon$-relative error the order statistics of a much larger underlying\ndataset.\n  In this paper we develop a randomized online quantile summary for the cash\nregister data input model and comparison data domain model that uses\n$O(\\frac{1}{\\varepsilon} \\log \\frac{1}{\\varepsilon})$ words of memory. This\nimproves upon the previous best upper bound of $O(\\frac{1}{\\varepsilon}\n\\log^{3/2} \\frac{1}{\\varepsilon})$ by Agarwal et. al. (PODS 2012). Further, by\na lower bound of Hung and Ting (FAW 2010) no deterministic summary for the\ncomparison model can outperform our randomized summary in terms of space\ncomplexity. Lastly, our summary has the nice property that\n$O(\\frac{1}{\\varepsilon} \\log \\frac{1}{\\varepsilon})$ words suffice to ensure\nthat the success probability is $1 - e^{-\\text{poly}(1/\\varepsilon)}$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 22:58:55 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Felber", "David", ""], ["Ostrovsky", "Rafail", ""]]}, {"id": "1503.01192", "submitter": "Amr Elmasry", "authors": "Amr Elmasry", "title": "Counting Inversions Adaptively", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple and efficient algorithm for adaptively counting inversions\nin a sequence of $n$ integers. Our algorithm runs in $O(n + n\n\\sqrt{\\lg{(Inv/n)}})$ time in the word-RAM model of computation, where $Inv$ is\nthe number of inversions.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 01:50:20 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Elmasry", "Amr", ""]]}, {"id": "1503.01203", "submitter": "Serge Gaspers", "authors": "Serge Gaspers and Simon Mackenzie", "title": "On the Number of Minimal Separators in Graphs", "comments": "arXiv admin note: text overlap with arXiv:0909.5278 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the largest number of minimal separators a graph on n vertices\ncan have at most.\n  We give a new proof that this number is in $O( ((1+\\sqrt{5})/2)^n n )$.\n  We prove that this number is in $\\omega( 1.4521^n )$, improving on the\nprevious best lower bound of $\\Omega(3^{n/3}) \\subseteq \\omega( 1.4422^n )$.\n  This gives also an improved lower bound on the number of potential maximal\ncliques in a graph. We would like to emphasize that our proofs are short,\nsimple, and elementary.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 03:05:21 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 08:31:44 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Gaspers", "Serge", ""], ["Mackenzie", "Simon", ""]]}, {"id": "1503.01212", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin, Karthik Sridharan", "title": "Hierarchies of Relaxations for Online Prediction Problems with Evolving\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online prediction where regret of the algorithm is measured against\na benchmark defined via evolving constraints. This framework captures online\nprediction on graphs, as well as other prediction problems with combinatorial\nstructure. A key aspect here is that finding the optimal benchmark predictor\n(even in hindsight, given all the data) might be computationally hard due to\nthe combinatorial nature of the constraints. Despite this, we provide\npolynomial-time \\emph{prediction} algorithms that achieve low regret against\ncombinatorial benchmark sets. We do so by building improper learning algorithms\nbased on two ideas that work together. The first is to alleviate part of the\ncomputational burden through random playout, and the second is to employ\nLasserre semidefinite hierarchies to approximate the resulting integer program.\nInterestingly, for our prediction algorithms, we only need to compute the\nvalues of the semidefinite programs and not the rounded solutions. However, the\nintegrality gap for Lasserre hierarchy \\emph{does} enter the generic regret\nbound in terms of Rademacher complexity of the benchmark set. This establishes\na trade-off between the computation time and the regret bound of the algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 04:05:35 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 00:04:05 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1503.01218", "submitter": "Tasuku Soma", "authors": "Tasuku Soma and Yuichi Yoshida", "title": "Maximizing Monotone Submodular Functions over the Integer Lattice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of maximizing non-negative monotone submodular functions under a\ncertain constraint has been intensively studied in the last decade. In this\npaper, we address the problem for functions defined over the integer lattice.\n  Suppose that a non-negative monotone submodular function $f:\\mathbb{Z}_+^n\n\\to \\mathbb{R}_+$ is given via an evaluation oracle. Assume further that $f$\nsatisfies the diminishing return property, which is not an immediate\nconsequence of submodularity when the domain is the integer lattice. Given\nthis, we design polynomial-time $(1-1/e-\\epsilon)$-approximation algorithms for\na cardinality constraint, a polymatroid constraint, and a knapsack constraint.\nFor a cardinality constraint, we also provide a\n$(1-1/e-\\epsilon)$-approximation algorithm with slightly worse time complexity\nthat does not rely on the diminishing return property.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 04:39:23 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 14:58:14 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Soma", "Tasuku", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1503.01322", "submitter": "Vincent A Traag", "authors": "V.A. Traag", "title": "Faster unfolding of communities: speeding up the Louvain algorithm", "comments": null, "journal-ref": "Phys. Rev. E 92, 032801, (2015)", "doi": "10.1103/PhysRevE.92.032801", "report-no": null, "categories": "physics.soc-ph cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex networks exhibit a modular structure of densely connected groups\nof nodes. Usually, such a modular structure is uncovered by the optimization of\nsome quality function. Although flawed, modularity remains one of the most\npopular quality functions. The Louvain algorithm was originally developed for\noptimizing modularity, but has been applied to a variety of methods. As such,\nspeeding up the Louvain algorithm, enables the analysis of larger graphs in a\nshorter time for various methods. We here suggest to consider moving nodes to a\nrandom neighbor community, instead of the best neighbor community. Although\nincredibly simple, it reduces the theoretical runtime complexity from\n$\\mathcal{O}(m)$ to $\\mathcal{O}(n \\log \\langle k \\rangle)$ in networks with a\nclear community structure. In benchmark networks, it speeds up the algorithm\nroughly 2-3 times, while in some real networks it even reaches 10 times faster\nruntimes. This improvement is due to two factors: (1) a random neighbor is\nlikely to be in a \"good\" community; and (2) random neighbors are likely to be\nhubs, helping the convergence. Finally, the performance gain only slightly\ndiminishes the quality, especially for modularity, thus providing a good\nquality-performance ratio. However, these gains are less pronounced, or even\ndisappear, for some other measures such as significance or surprise.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 14:50:22 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2015 10:32:00 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Traag", "V. A.", ""]]}, {"id": "1503.01334", "submitter": "Davide Orsucci", "authors": "Davide Orsucci, Hans J. Briegel and Vedran Dunjko", "title": "Faster quantum mixing for slowly evolving sequences of Markov chains", "comments": "20 pages, 2 figures", "journal-ref": "Quantum 2, 105 (2018)", "doi": "10.22331/q-2018-11-09-105", "report-no": "LA-UR-17-20510", "categories": "quant-ph cs.AI cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Markov chain methods are remarkably successful in computational physics,\nmachine learning, and combinatorial optimization. The cost of such methods\noften reduces to the mixing time, i.e., the time required to reach the steady\nstate of the Markov chain, which scales as $\\delta^{-1}$, the inverse of the\nspectral gap. It has long been conjectured that quantum computers offer nearly\ngeneric quadratic improvements for mixing problems. However, except in special\ncases, quantum algorithms achieve a run-time of $\\mathcal{O}(\\sqrt{\\delta^{-1}}\n\\sqrt{N})$, which introduces a costly dependence on the Markov chain size $N,$\nnot present in the classical case. Here, we re-address the problem of mixing of\nMarkov chains when these form a slowly evolving sequence. This setting is akin\nto the simulated annealing setting and is commonly encountered in physics,\nmaterial sciences and machine learning. We provide a quantum memory-efficient\nalgorithm with a run-time of $\\mathcal{O}(\\sqrt{\\delta^{-1}} \\sqrt[4]{N})$,\nneglecting logarithmic terms, which is an important improvement for large state\nspaces. Moreover, our algorithms output quantum encodings of distributions,\nwhich has advantages over classical outputs. Finally, we discuss the run-time\nbounds of mixing algorithms and show that, under certain assumptions, our\nalgorithms are optimal.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 15:07:07 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 11:37:31 GMT"}, {"version": "v3", "created": "Tue, 17 Jul 2018 15:20:37 GMT"}, {"version": "v4", "created": "Mon, 29 Oct 2018 23:33:30 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Orsucci", "Davide", ""], ["Briegel", "Hans J.", ""], ["Dunjko", "Vedran", ""]]}, {"id": "1503.01363", "submitter": "Meiram Murzabulatov", "authors": "Piotr Berman, Meiram Murzabulatov, Sofya Raskhodnikova", "title": "Tolerant Testers of Image Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate a systematic study of tolerant testers of image properties or,\nequivalently, algorithms that approximate the distance from a given image to\nthe desired property (that is, the smallest fraction of pixels that need to\nchange in the image to ensure that the image satisfies the desired property).\nImage processing is a particularly compelling area of applications for\nsublinear-time algorithms and, specifically, property testing. However, for\ntesting algorithms to reach their full potential in image processing, they have\nto be tolerant, which allows them to be resilient to noise. Prior to this work,\nonly one tolerant testing algorithm for an image property (image partitioning)\nhas been published.\n  We design efficient approximation algorithms for the following fundamental\nquestions: What fraction of pixels have to be changed in an image so that it\nbecomes a half-plane? a representation of a convex object? a representation of\na connected object? More precisely, our algorithms approximate the distance to\nthree basic properties (being a half-plane, convexity, and connectedness)\nwithin a small additive error $\\epsilon$, after reading a number of pixels\npolynomial in $1/\\epsilon$ and independent of the size of the image. The\nrunning time of the testers for half-plane and convexity is also polynomial in\n$1/\\epsilon$. Tolerant testers for these three properties were not investigated\npreviously. For convexity and connectedness, even the existence of distance\napproximation algorithms with query complexity independent of the input size is\nnot implied by previous work. (It does not follow from the VC-dimension bounds,\nsince VC dimension of convexity and connectedness, even in two dimensions,\ndepends on the input size. It also does not follow from the existence of\nnon-tolerant testers.)\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 16:22:58 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 15:51:54 GMT"}, {"version": "v3", "created": "Mon, 15 Aug 2016 04:26:21 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Berman", "Piotr", ""], ["Murzabulatov", "Meiram", ""], ["Raskhodnikova", "Sofya", ""]]}, {"id": "1503.01382", "submitter": "Gregory Gutin", "authors": "Jason Crampton, Naomi Farley, Gregory Gutin and Mark Jones", "title": "Optimal Constructions for Chain-based Cryptographic Enforcement of\n  Information Flow Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simple security property in an information flow policy can be enforced by\nencrypting data objects and distributing an appropriate secret to each user. A\nuser derives a suitable decryption key from the secret and publicly available\ninformation. A chain-based enforcement scheme provides an alternative method of\ncryptographic enforcement that does not require any public information, the\ntrade-off being that a user may require more than one secret. For a given\ninformation flow policy, there will be many different possible chain-based\nenforcement schemes. In this paper, we provide a polynomial-time algorithm for\nselecting a chain-based scheme which uses the minimum possible number of keys.\nWe also compute the number of secrets that will be required and establish an\nupper bound on the number of secrets required by any user.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 16:51:31 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2015 16:13:50 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Crampton", "Jason", ""], ["Farley", "Naomi", ""], ["Gutin", "Gregory", ""], ["Jones", "Mark", ""]]}, {"id": "1503.01535", "submitter": "Setareh Borjian", "authors": "Setareh Borjian, Vahideh H. Manshadi, Cynthia Barnhart and Patrick\n  Jaillet", "title": "Managing Relocation and Delay in Container Terminals with Flexible\n  Service Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new model and mathematical formulation for planning crane\nmoves in the storage yard of container terminals. Our objective is to develop a\ntool that captures customer centric elements, especially service time, and\nhelps operators to manage costly relocation moves. Our model incorporates\nseveral practical details and provides port operators with expanded\ncapabilities including planning repositioning moves in off-peak hours,\ncontrolling wait times of each customer as well as total service time,\noptimizing the number of relocations and wait time jointly, and optimizing\nsimultaneously the container stacking and retrieval process. We also study a\nclass of flexible service policies which allow for out-of-order retrieval. We\nshow that under such flexible policies, we can decrease the number of\nrelocations and retrieval delays without creating inequities.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 04:37:24 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Borjian", "Setareh", ""], ["Manshadi", "Vahideh H.", ""], ["Barnhart", "Cynthia", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1503.01578", "submitter": "Sanghyuk Chun", "authors": "Sanghyuk Chun, Yung-Kyun Noh, Jinwoo Shin", "title": "Scalable Iterative Algorithm for Robust Subspace Clustering", "comments": "This paper has been withdrawn by the author due to an error in the\n  initialization section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering (SC) is a popular method for dimensionality reduction of\nhigh-dimensional data, where it generalizes Principal Component Analysis (PCA).\nRecently, several methods have been proposed to enhance the robustness of PCA\nand SC, while most of them are computationally very expensive, in particular,\nfor high dimensional large-scale data. In this paper, we develop much faster\niterative algorithms for SC, incorporating robustness using a {\\em non-squared}\n$\\ell_2$-norm objective. The known implementations for optimizing the objective\nwould be costly due to the alternative optimization of two separate objectives:\noptimal cluster-membership assignment and robust subspace selection, while the\nsubstitution of one process to a faster surrogate can cause failure in\nconvergence. To address the issue, we use a simplified procedure requiring\nefficient matrix-vector multiplications for subspace update instead of solving\nan expensive eigenvector problem at each iteration, in addition to release\nnested robust PCA loops. We prove that the proposed algorithm monotonically\nconverges to a local minimum with approximation guarantees, e.g., it achieves\n2-approximation for the robust PCA objective. In our experiments, the proposed\nalgorithm is shown to converge at an order of magnitude faster than known\nalgorithms optimizing the same objective, and have outperforms prior subspace\nclustering methods in accuracy and running time for MNIST dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 08:54:51 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 20:47:35 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Chun", "Sanghyuk", ""], ["Noh", "Yung-Kyun", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1503.01624", "submitter": "Sebastian Deorowicz", "authors": "Sebastian Deorowicz, Agnieszka Danek, Marcin Niemiec", "title": "GDC 2: Compression of large collections of genomes", "comments": null, "journal-ref": "Scientific Reports, Article no. 11565 (2015)", "doi": "10.1038/srep11565", "report-no": null, "categories": "cs.DS cs.CE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fall of prices of the high-throughput genome sequencing changes the\nlandscape of modern genomics. A number of large scale projects aimed at\nsequencing many human genomes are in progress. Genome sequencing also becomes\nan important aid in the personalized medicine. One of the significant side\neffects of this change is a necessity of storage and transfer of huge amounts\nof genomic data. In this paper we deal with the problem of compression of large\ncollections of complete genomic sequences. We propose an algorithm that is able\nto compress the collection of 1092 human diploid genomes about 9,500 times.\nThis result is about 4 times better than what is offered by the other existing\ncompressors. Moreover, our algorithm is very fast as it processes the data with\nspeed 200MB/s on a modern workstation. In a consequence the proposed algorithm\nallows storing the complete genomic collections at low cost, e.g., the examined\ncollection of 1092 human genomes needs only about 700MB when compressed, what\ncan be compared to about 6.7 TB of uncompressed FASTA files. The source code is\navailable at\nhttp://sun.aei.polsl.pl/REFRESH/index.php?page=projects&project=gdc&subpage=about.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 12:38:32 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Deorowicz", "Sebastian", ""], ["Danek", "Agnieszka", ""], ["Niemiec", "Marcin", ""]]}, {"id": "1503.01663", "submitter": "Dan Feldman PhD", "authors": "Dan Feldman, Mikhail Volkov, Daniela Rus", "title": "Dimensionality Reduction of Massive Sparse Datasets Using Coresets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a practical solution with performance guarantees to\nthe problem of dimensionality reduction for very large scale sparse matrices.\nWe show applications of our approach to computing the low rank approximation\n(reduced SVD) of such matrices. Our solution uses coresets, which is a subset\nof $O(k/\\eps^2)$ scaled rows from the $n\\times d$ input matrix, that\napproximates the sub of squared distances from its rows to every\n$k$-dimensional subspace in $\\REAL^d$, up to a factor of $1\\pm\\eps$. An open\ntheoretical problem has been whether we can compute such a coreset that is\nindependent of the input matrix and also a weighted subset of its rows. %An\nopen practical problem has been whether we can compute a non-trivial\napproximation to the reduced SVD of very large databases such as the Wikipedia\ndocument-term matrix in a reasonable time. We answer this question\naffirmatively. % and demonstrate an algorithm that efficiently computes a low\nrank approximation of the entire English Wikipedia. Our main technical result\nis a novel technique for deterministic coreset construction that is based on a\nreduction to the problem of $\\ell_2$ approximation for item frequencies.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 15:39:49 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Feldman", "Dan", ""], ["Volkov", "Mikhail", ""], ["Rus", "Daniela", ""]]}, {"id": "1503.01706", "submitter": "Carsten Grimm", "authors": "Carsten Grimm", "title": "Efficient Farthest-Point Queries in Two-Terminal Series-Parallel\n  Networks", "comments": "To appear in the proceedings of the 41st Workshop on Graph-Theoretic\n  Concepts in Computer Science (WG2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the continuum of points along the edges of a network, i.e., a\nconnected, undirected graph with positive edge weights. We measure the distance\nbetween these points in terms of the weighted shortest path distance, called\nthe network distance. Within this metric space, we study farthest points and\nfarthest distances. We introduce a data structure supporting queries for the\nfarthest distance and the farthest points on two-terminal series-parallel\nnetworks. This data structure supports farthest-point queries in $O(k + \\log\nn)$ time after $O(n \\log p)$ construction time, where $k$ is the number of\nfarthest points, $n$ is the size of the network, and $p$ parallel operations\nare required to generate the network.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 17:43:02 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 13:28:28 GMT"}, {"version": "v3", "created": "Fri, 29 May 2015 07:46:05 GMT"}, {"version": "v4", "created": "Wed, 29 Jul 2015 09:18:56 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Grimm", "Carsten", ""]]}, {"id": "1503.01720", "submitter": "Arnab Bhattacharyya", "authors": "Arnab Bhattacharyya and Kirankumar Shiragur", "title": "How friends and non-determinism affect opinion dynamics", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hegselmann-Krause system (HK system for short) is one of the most popular\nmodels for the dynamics of opinion formation in multiagent systems. Agents are\nmodeled as points in opinion space, and at every time step, each agent moves to\nthe mass center of all the agents within unit distance. The rate of convergence\nof HK systems has been the subject of several recent works. In this work, we\ninvestigate two natural variations of the HK system and their effect on the\ndynamics. In the first variation, we only allow pairs of agents who are friends\nin an underlying social network to communicate with each other. In the second\nvariation, agents may not move exactly to the mass center but somewhere close\nto it. The dynamics of both variants are qualitatively very different from that\nof the classical HK system. Nevertheless, we prove that both these systems\nconverge in polynomial number of non-trivial steps, regardless of the social\nnetwork in the first variant and noise patterns in the second variant.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 18:15:56 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Shiragur", "Kirankumar", ""]]}, {"id": "1503.01752", "submitter": "Yin Tat Lee", "authors": "Yin Tat Lee and Aaron Sidford", "title": "Efficient Inverse Maintenance and Faster Algorithms for Linear\n  Programming", "comments": "In an older version of this paper, we mistakenly claimed an improved\n  running time for Dikin walk by noting solely the improved running time for\n  linear system solving and ignoring the determinant computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the following inverse maintenance problem: given\n$A \\in \\mathbb{R}^{n\\times d}$ and a number of rounds $r$, we receive a\n$n\\times n$ diagonal matrix $D^{(k)}$ at round $k$ and we wish to maintain an\nefficient linear system solver for $A^{T}D^{(k)}A$ under the assumption\n$D^{(k)}$ does not change too rapidly. This inverse maintenance problem is the\ncomputational bottleneck in solving multiple optimization problems. We show how\nto solve this problem with $\\tilde{O}(nnz(A)+d^{\\omega})$ preprocessing time\nand amortized $\\tilde{O}(nnz(A)+d^{2})$ time per round, improving upon previous\nrunning times for solving this problem.\n  Consequently, we obtain the fastest known running times for solving multiple\nproblems including, linear programming and computing a rounding of a polytope.\nIn particular given a feasible point in a linear program with $d$ variables,\n$n$ constraints, and constraint matrix $A\\in\\mathbb{R}^{n\\times d}$, we show\nhow to solve the linear program in time\n$\\tilde{O}(nnz(A)+d^{2})\\sqrt{d}\\log(\\epsilon^{-1}))$. We achieve our results\nthrough a novel combination of classic numerical techniques of low rank update,\npreconditioning, and fast matrix multiplication as well as recent work on\nsubspace embeddings and spectral sparsification that we hope will be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 20:12:13 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 19:48:25 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2015 13:06:14 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Lee", "Yin Tat", ""], ["Sidford", "Aaron", ""]]}, {"id": "1503.01839", "submitter": "Douglas Staple", "authors": "Douglas B. Staple", "title": "The combinatorial algorithm for computing $\\pi(x)$", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes recent advances in the combinatorial method for\ncomputing $\\pi(x)$, the number of primes $\\leq x$. In particular, the memory\nusage has been reduced by a factor of $\\log x$, and modifications for shared-\nand distributed-memory parallelism have been incorporated. The resulting method\ncomputes $\\pi(x)$ with complexity $O(x^{2/3}\\mathrm{log}^{-2}x)$ in time and\n$O(x^{1/3}\\mathrm{log}^{2}x)$ in space. The algorithm has been implemented and\nused to compute $\\pi(10^n)$ for $1 \\leq n \\leq 26$ and $\\pi(2^m)$ for $1\\leq m\n\\leq 86$. The mathematics presented here is consistent with and builds on that\nof previous authors.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 03:13:45 GMT"}, {"version": "v2", "created": "Thu, 28 May 2015 23:59:56 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Staple", "Douglas B.", ""]]}, {"id": "1503.02353", "submitter": "Peter Robinson", "authors": "Gopal Pandurangan, Peter Robinson, Michele Scquizzato", "title": "Fast Distributed Algorithms for Connectivity and MST in Large Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the increasing need to understand the algorithmic foundations of\ndistributed large-scale graph computations, we study a number of fundamental\ngraph problems in a message-passing model for distributed computing where $k\n\\geq 2$ machines jointly perform computations on graphs with $n$ nodes\n(typically, $n \\gg k$). The input graph is assumed to be initially randomly\npartitioned among the $k$ machines, a common implementation in many real-world\nsystems. Communication is point-to-point, and the goal is to minimize the\nnumber of communication rounds of the computation.\n  Our main result is an (almost) optimal distributed randomized algorithm for\ngraph connectivity. Our algorithm runs in $\\tilde{O}(n/k^2)$ rounds\n($\\tilde{O}$ notation hides a $\\poly\\log(n)$ factor and an additive\n$\\poly\\log(n)$ term). This improves over the best previously known bound of\n$\\tilde{O}(n/k)$ [Klauck et al., SODA 2015], and is optimal (up to a\npolylogarithmic factor) in view of an existing lower bound of\n$\\tilde{\\Omega}(n/k^2)$. Our improved algorithm uses a bunch of techniques,\nincluding linear graph sketching, that prove useful in the design of efficient\ndistributed graph algorithms. Using the connectivity algorithm as a building\nblock, we then present fast randomized algorithms for computing minimum\nspanning trees, (approximate) min-cuts, and for many graph verification\nproblems. All these algorithms take $\\tilde{O}(n/k^2)$ rounds, and are optimal\nup to polylogarithmic factors. We also show an almost matching lower bound of\n$\\tilde{\\Omega}(n/k^2)$ rounds for many graph verification problems by\nleveraging lower bounds in random-partition communication complexity.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 01:16:06 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 15:54:33 GMT"}, {"version": "v3", "created": "Wed, 6 Jul 2016 18:53:05 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Pandurangan", "Gopal", ""], ["Robinson", "Peter", ""], ["Scquizzato", "Michele", ""]]}, {"id": "1503.02413", "submitter": "Galia Shabtai", "authors": "Galia Shabtai, Danny Raz, and Yuval Shavitt", "title": "Stochastic Service Placement", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource allocation for cloud services is a complex task due to the diversity\nof the services and the dynamic workloads. One way to address this is by\noverprovisioning which results in high cost due to the unutilized resources. A\nmuch more economical approach, relying on the stochastic nature of the demand,\nis to allocate just the right amount of resources and use additional more\nexpensive mechanisms in case of overflow situations where demand exceeds the\ncapacity. In this paper we study this approach and show both by comprehensive\nanalysis for independent normal distributed demands and simulation on synthetic\ndata that it is significantly better than currently deployed methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 09:59:58 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Shabtai", "Galia", ""], ["Raz", "Danny", ""], ["Shavitt", "Yuval", ""]]}, {"id": "1503.02416", "submitter": "Travis Gagie", "authors": "Travis Gagie", "title": "Approximating LZ77 in Small Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a positive \\(\\epsilon \\leq 1\\) and read-only access to a string \\(S\n[1..n]\\) whose LZ77 parse consists of $z$ phrases, with high probability we can\nbuild an LZ77-like parse of $S$ that consists of $\\Oh{z / \\epsilon}$ phrases\nusing $\\Oh{n^{1 + \\epsilon}}$ time, $\\Oh{n^{1 + \\epsilon} / B}$ I/Os (where $B$\nis the size of a disk block) and $\\Oh{z / \\epsilon}$ space.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 10:17:30 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Gagie", "Travis", ""]]}, {"id": "1503.02504", "submitter": "Vasileios Iliopoulos DR", "authors": "Vasileios Iliopoulos", "title": "The Quicksort algorithm and related topics", "comments": "PhD thesis. Reference [23] was missing in first version. It now reads\n  correctly in page 142, Section 5.6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting algorithms have attracted a great deal of attention and study, as\nthey have numerous applications to Mathematics, Computer Science and related\nfields. In this thesis, we first deal with the mathematical analysis of the\nQuicksort algorithm and its variants. Specifically, we study the time\ncomplexity of the algorithm and we provide a complete demonstration of the\nvariance of the number of comparisons required, a known result but one whose\ndetailed proof is not easy to read out of the literature. We also examine\nvariants of Quicksort, where multiple pivots are chosen for the partitioning of\nthe array.\n  The rest of this work is dedicated to the analysis of finding the true order\nby further pairwise comparisons when a partial order compatible with the true\norder is given in advance. We discuss a number of cases where the partially\nordered sets arise at random. To this end, we employ results from Graph and\nInformation Theory. Finally, we obtain an alternative bound on the number of\nlinear extensions when the partially ordered set arises from a random graph,\nand discuss the possible application of Shellsort in merging chains.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 15:04:30 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 23:14:27 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2015 12:47:21 GMT"}, {"version": "v4", "created": "Fri, 2 Oct 2015 12:51:33 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Iliopoulos", "Vasileios", ""]]}, {"id": "1503.02517", "submitter": "Emmanuel Osegi", "authors": "Ugochi A. Okengwu, Enoch O. Nwachukwu, Emmanuel N. Osegi", "title": "Modified Dijkstra Algorithm with Invention Hierarchies Applied to a\n  Conic Graph", "comments": "Paper Proposals", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  A modified version of the Dijkstra algorithm using an inventive contraction\nhierarchy is proposed. The algorithm considers a directed acyclic graph with a\nconical or semi-circular structure for which a pair of edges is chosen\niteratively from multi-sources. The algorithm obtains minimum paths by using a\ncomparison process. The comparison process follows a mathematical construction\nroutine that considers a forward and backward check such that only paths with\nminimum lengths are selected. In addition, the algorithm automatically invents\na new path by computing the absolute edge difference for the minimum edge pair\nand its succeeding neighbour in O (n) time. The invented path is approximated\nto the hidden path using a fitness criterion. The proposed algorithm extends\nthe multi-source multi-destination problem to include those paths for which a\npath mining redirection from multi-sources to multi-destinations is a minimum.\nThe algorithm has been applied to a hospital locator path finding system and\nthe results were quite satisfactory.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 15:18:32 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2015 02:55:19 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Okengwu", "Ugochi A.", ""], ["Nwachukwu", "Enoch O.", ""], ["Osegi", "Emmanuel N.", ""]]}, {"id": "1503.02577", "submitter": "Helio M. de Oliveira", "authors": "G. Jer\\^onimo da Silva Jr., R.M. Campello de Souza and H.M. de\n  Oliveira", "title": "New Algorithms for Computing a Single Component of the Discrete Fourier\n  Transform", "comments": "4 pages, 3 figures, 1 table. In: 10th International Symposium on\n  Communication Theory and Applications, Ambleside, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the theory and hardware implementation of two new\nalgorithms for computing a single component of the discrete Fourier transform.\nIn terms of multiplicative complexity, both algorithms are more efficient, in\ngeneral, than the well known Goertzel Algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 17:39:44 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Silva", "G. Jer\u00f4nimo da", "Jr."], ["de Souza", "R. M. Campello", ""], ["de Oliveira", "H. M.", ""]]}, {"id": "1503.02592", "submitter": "Jonathan Sorenson", "authors": "Jonathan P. Sorenson", "title": "Two Compact Incremental Prime Sieves", "comments": null, "journal-ref": "LMS J. Comput. Math. 18 (2015) 675-683", "doi": "10.1112/S1461157015000194", "report-no": null, "categories": "cs.DS math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prime sieve is an algorithm that finds the primes up to a bound $n$. We say\nthat a prime sieve is incremental, if it can quickly determine if $n+1$ is\nprime after having found all primes up to $n$. We say a sieve is compact if it\nuses roughly $\\sqrt{n}$ space or less. In this paper we present two new\nresults:\n  (1) We describe the rolling sieve, a practical, incremental prime sieve that\ntakes $O(n\\log\\log n)$ time and $O(\\sqrt{n}\\log n)$ bits of space, and\n  (2) We show how to modify the sieve of Atkin and Bernstein (2004) to obtain a\nsieve that is simultaneously sublinear, compact, and incremental.\n  The second result solves an open problem given by Paul Pritchard in 1994.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 18:02:51 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Sorenson", "Jonathan P.", ""]]}, {"id": "1503.02654", "submitter": "K. Alex Mills", "authors": "K. Alex Mills, R. Chandrasekaran, Neeraj Mittal", "title": "Algorithms for Replica Placement in High-Availability Storage", "comments": "22 pages, 7 figures, 4 algorithm listings", "journal-ref": null, "doi": "10.1007/978-3-319-26626-8_26", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new model of causal failure is presented and used to solve a novel replica\nplacement problem in data centers. The model describes dependencies among\nsystem components as a directed graph. A replica placement is defined as a\nsubset of vertices in such a graph. A criterion for optimizing replica\nplacements is formalized and explained. In this work, the optimization goal is\nto avoid choosing placements in which a single failure event is likely to wipe\nout multiple replicas. Using this criterion, a fast algorithm is given for the\nscenario in which the dependency model is a tree. The main contribution of the\npaper is an $O(n + \\rho \\log \\rho)$ dynamic programming algorithm for placing\n$\\rho$ replicas on a tree with $n$ vertices. This algorithm exhibits the\ninteresting property that only two subproblems need to be recursively\nconsidered at each stage. An $O(n^2 \\rho)$ greedy algorithm is also briefly\nreported.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 19:59:17 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2015 19:32:53 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2015 16:33:41 GMT"}, {"version": "v4", "created": "Fri, 22 May 2015 15:40:48 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Mills", "K. Alex", ""], ["Chandrasekaran", "R.", ""], ["Mittal", "Neeraj", ""]]}, {"id": "1503.02746", "submitter": "Laszlo Babai", "authors": "L\\'aszl\\'o Babai and John Wilmes", "title": "Asymptotic Delsarte cliques in distance-regular graphs", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new bound on the parameter $\\lambda$ (number of common neighbors of\na pair of adjacent vertices) in a distance-regular graph $G$, improving and\ngeneralizing bounds for strongly regular graphs by Spielman (1996) and Pyber\n(2014). The new bound is one of the ingredients of recent progress on the\ncomplexity of testing isomorphism of strongly regular graphs (Babai, Chen, Sun,\nTeng, Wilmes 2013). The proof is based on a clique geometry found by Metsch\n(1991) under certain constraints on the parameters. We also give a simplified\nproof of the following asymptotic consequence of Metsch's result: if $k\\mu =\no(\\lambda^2)$ then each edge of $G$ belongs to a unique maximal clique of size\nasymptotically equal to $\\lambda$, and all other cliques have size\n$o(\\lambda)$. Here $k$ denotes the degree and $\\mu$ the number of common\nneighbors of a pair of vertices at distance 2. We point out that Metsch's\ncliques are \"asymptotically Delsarte\" when $k\\mu = o(\\lambda^2)$, so families\nof distance-regular graphs with parameters satisfying $k\\mu = o(\\lambda^2)$ are\n\"asymptotically Delsarte-geometric.\"\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 01:35:13 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Babai", "L\u00e1szl\u00f3", ""], ["Wilmes", "John", ""]]}, {"id": "1503.02773", "submitter": "Marc Tedder", "authors": "Marc Tedder", "title": "Simpler, Linear-Time Transitive Orientation via Lexicographic\n  Breadth-First Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparability graphs are the undirected graphs whose edges can be directed so\nthat the resulting directed graph is transitive. They are related to posets and\nhave applications in scheduling theory. This paper considers the problem of\nfinding a transitive orientation of a comparability graph, a requirement for\nmany of its applications. A linear-time algorithm is presented based on an\nelegant partition refinement scheme developed elsewhere for the problem. The\nalgorithm is intended as a simpler and more practical alternative to the\nexisting lineartime solution, which is commonly understood to be difficult and\nmainly of theoretical value. It accomplishes this by using Lexicographic\nBreadth-First Search to achieve the same effect as produced by modular\ndecomposition in the earlier linear-time algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 05:16:44 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Tedder", "Marc", ""]]}, {"id": "1503.02774", "submitter": "Andrea Lincoln", "authors": "Adam Hesterberg, Andrea Lincoln, Jayson Lynch", "title": "Improved Connectivity Condition for Byzantine Fault Tolerance", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a network in which some pairs of nodes can communicate freely, and some\nsubsets of the nodes could be faulty and colluding to disrupt communication,\nwhen can messages reliably be sent from one given node to another? We give a\nnew characterization of when the agreement problem can be solved and provide an\nagreement algorithm which can reach agreement when the number of Byzantine\nnodes along each minimal vertex cut is bounded. Our new bound holds for a\nstrict superset of cases than the previously known bound. We show that the new\nbound is tight. Furthermore, we show that this algorithm does not require the\nprocesses to know the graph structure, as the previously known algorithm did.\nFinally, we explore some of the situations in which we can reach agreement if\nwe assume that individual nodes or entire subgraphs are trustworthy.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 05:21:54 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Hesterberg", "Adam", ""], ["Lincoln", "Andrea", ""], ["Lynch", "Jayson", ""]]}, {"id": "1503.02835", "submitter": "R\\'emy Belmonte", "authors": "R\\'emy Belmonte, Yuya Higashikawa, Naoki Katoh and Yoshio Okamoto", "title": "Polynomial-time approximability of the k-Sink Location problem", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dynamic network ${\\cal N} = (G,c,\\tau,S)$ where $G=(V,E)$ is a graph,\nintegers $\\tau(e)$ and $c(e)$ represent, for each edge $e\\in E$, the time\nrequired to traverse edge $e$ and its nonnegative capacity, and the set\n$S\\subseteq V$ is a set of sources. In the $k$-{\\sc Sink Location} problem, one\nis given as input a dynamic network ${\\cal N}$ where every source $u\\in S$ is\ngiven a nonnegative supply value $\\sigma(u)$. The task is then to find a set of\nsinks $X = \\{x_1,\\ldots,x_k\\}$ in $G$ that minimizes the routing time of all\nsupply to $X$. Note that, in the case where $G$ is an undirected graph, the\noptimal position of the sinks in $X$ needs not be at vertices, and can be\nlocated along edges. Hoppe and Tardos showed that, given an instance of\n$k$-{\\sc Sink Location} and a set of $k$ vertices $X\\subseteq V$, one can find\nan optimal routing scheme of all the supply in $G$ to $X$ in polynomial time,\nin the case where graph $G$ is directed. Note that when $G$ is directed, this\nsuffices to obtain polynomial-time solvability of the $k$-{\\sc Sink Location}\nproblem, since any optimal position will be located at vertices of $G$.\nHowever, the computational complexity of the $k$-{\\sc Sink Location} problem on\ngeneral undirected graphs is still open. In this paper, we show that the\n$k$-{\\sc Sink Location} problem admits a fully polynomial-time approximation\nscheme (FPTAS) for every fixed $k$, and that the problem is $W[1]$-hard when\nparameterized by $k$.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 09:52:52 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Belmonte", "R\u00e9my", ""], ["Higashikawa", "Yuya", ""], ["Katoh", "Naoki", ""], ["Okamoto", "Yoshio", ""]]}, {"id": "1503.02868", "submitter": "Aleksandrs Belovs", "authors": "Aleksandrs Belovs and Eric Blais", "title": "Quantum Algorithm for Monotonicity Testing on the Hypercube", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we develop a bounded-error quantum algorithm that makes $\\tilde\nO(n^{1/4}\\varepsilon^{-1/2})$ queries to a Boolean function $f$, accepts a\nmonotone function, and rejects a function that is $\\varepsilon$-far from being\nmonotone. This gives a super-quadratic improvement compared to the best known\nrandomized algorithm for all $\\varepsilon = o(1)$. The improvement is cubic\nwhen $\\varepsilon = 1/\\sqrt{n}$.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 11:30:16 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Belovs", "Aleksandrs", ""], ["Blais", "Eric", ""]]}, {"id": "1503.02880", "submitter": "Mathias Hauptmann", "authors": "Mathias Hauptmann, Marek Karpinski", "title": "On the Approximability of Independent Set Problem on Power Law Graphs", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first nonconstant lower bounds for the approximability of the\nIndependent Set Problem on the Power Law Graphs. These bounds are of the form\n$n^{\\epsilon}$ in the case when the power law exponent satisfies $\\beta <1$. In\nthe case when $\\beta =1$, the lower bound is of the form $\\log (n)^{\\epsilon}$.\nThe embedding technique used in the proof could also be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 12:18:44 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Hauptmann", "Mathias", ""], ["Karpinski", "Marek", ""]]}, {"id": "1503.02920", "submitter": "Chao Xu", "authors": "Jianer Chen and Chao Xu", "title": "Dealing With 4-Variables by Resolution: An Improved MaxSAT Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We study techniques for solving the Maximum Satisfiability problem (MaxSAT).\nOur focus is on variables of degree 4. We identify cases for degree-4 variables\nand show how the resolution principle and the kernelization techniques can be\nnicely integrated to achieve more efficient algorithms for the MaxSAT problem.\nAs a result, we present an algorithm of time $O^*(1.3248^k)$ for the MaxSAT\nproblem, improving the previous best upper bound $O^*(1.358^k)$ by Ivan\nBliznets and Alexander.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 14:21:23 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Chen", "Jianer", ""], ["Xu", "Chao", ""]]}, {"id": "1503.03105", "submitter": "L\\'aszl\\'o Kozma", "authors": "Parinya Chalermsook, Mayank Goswami, Laszlo Kozma, Kurt Mehlhorn,\n  Thatchaphol Saranurak", "title": "Self-Adjusting Binary Search Trees: What Makes Them Tick?", "comments": "Substantial revision, additional results. To be presented at ESA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Splay trees (Sleator and Tarjan) satisfy the so-called access lemma. Many of\nthe nice properties of splay trees follow from it. What makes self-adjusting\nbinary search trees (BSTs) satisfy the access lemma? After each access,\nself-adjusting BSTs replace the search path by a tree on the same set of nodes\n(the after-tree). We identify two simple combinatorial properties of the search\npath and the after-tree that imply the access lemma. Our main result (i)\nimplies the access lemma for all minimally self-adjusting BST algorithms for\nwhich it was known to hold: splay trees and their generalization to the class\nof local algorithms (Subramanian, Georgakopoulos and Mc-Clurkin), as well as\nGreedy BST, introduced by Demaine et al. and shown to satisfy the access lemma\nby Fox, (ii) implies that BST algorithms based on \"strict\" depth-halving\nsatisfy the access lemma, addressing an open question that was raised several\ntimes since 1985, and (iii) yields an extremely short proof for the O(log n log\nlog n) amortized access cost for the path-balance heuristic (proposed by\nSleator), matching the best known bound (Balasubramanian and Raman) to a\nlower-order factor.\n  One of our combinatorial properties is locality. We show that any\nBST-algorithm that satisfies the access lemma via the sum-of-log (SOL)\npotential is necessarily local. The other property states that the sum of the\nnumber of leaves of the after-tree plus the number of side alternations in the\nsearch path must be at least a constant fraction of the length of the search\npath. We show that a weak form of this property is necessary for sequential\naccess to be linear.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 21:46:34 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2015 09:20:57 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Chalermsook", "Parinya", ""], ["Goswami", "Mayank", ""], ["Kozma", "Laszlo", ""], ["Mehlhorn", "Kurt", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "1503.03155", "submitter": "Olivia Simpson", "authors": "Fan Chung, Olivia Simpson", "title": "Computing Heat Kernel Pagerank and a Local Clustering Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heat kernel pagerank is a variation of Personalized PageRank given in an\nexponential formulation. In this work, we present a sublinear time algorithm\nfor approximating the heat kernel pagerank of a graph. The algorithm works by\nsimulating random walks of bounded length and runs in time\n$O\\big(\\frac{\\log(\\epsilon^{-1})\\log\nn}{\\epsilon^3\\log\\log(\\epsilon^{-1})}\\big)$, assuming performing a random walk\nstep and sampling from a distribution with bounded support take constant time.\n  The quantitative ranking of vertices obtained with heat kernel pagerank can\nbe used for local clustering algorithms. We present an efficient local\nclustering algorithm that finds cuts by performing a sweep over a heat kernel\npagerank vector, using the heat kernel pagerank approximation algorithm as a\nsubroutine. Specifically, we show that for a subset $S$ of Cheeger ratio\n$\\phi$, many vertices in $S$ may serve as seeds for a heat kernel pagerank\nvector which will find a cut of conductance $O(\\sqrt{\\phi})$.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 02:47:50 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 17:59:15 GMT"}, {"version": "v3", "created": "Thu, 15 Dec 2016 20:03:08 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Chung", "Fan", ""], ["Simpson", "Olivia", ""]]}, {"id": "1503.03157", "submitter": "Olivia Simpson", "authors": "Fan Chung and Olivia Simpson", "title": "Solving Local Linear Systems with Boundary Conditions Using Heat Kernel\n  Pagerank", "comments": null, "journal-ref": "Internet Mathematics, Vol. 11, Iss. 4-5, 2015 p. 449-471", "doi": "10.1080/15427951.2015.1009522", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient algorithm for solving local linear systems with a\nboundary condition using the Green's function of a connected induced subgraph\nrelated to the system. We introduce the method of using the Dirichlet heat\nkernel pagerank vector to approximate local solutions to linear systems in the\ngraph Laplacian satisfying given boundary conditions over a particular subset\nof vertices. With an efficient algorithm for approximating Dirichlet heat\nkernel pagerank, our local linear solver algorithm computes an approximate\nlocal solution with multiplicative and additive error $\\epsilon$ by performing\n$O(\\epsilon^{-5}s^3\\log(s^3\\epsilon^{-1})\\log n)$ random walk steps, where $n$\nis the number of vertices in the full graph and $s$ is the size of the local\nsystem on the induced subgraph.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 02:58:43 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Chung", "Fan", ""], ["Simpson", "Olivia", ""]]}, {"id": "1503.03165", "submitter": "Ni  Ding Miss", "authors": "Ni Ding, Rodney A. Kennedy and Parastoo Sadeghi", "title": "Iterative Merging Algorithm for Cooperative Data Exchange", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding the minimum sum-rate strategy in\ncooperative data exchange systems that do not allow packet-splitting (NPS-CDE).\nIn an NPS-CDE system, there are a number of geographically close cooperative\nclients who send packets to help the others recover a packet set. A minimum\nsum-rate strategy is the strategy that achieves universal recovery (the\nsituation when all the clients recover the whole packet set) with the the\nminimal sum-rate (the total number of transmissions). We propose an iterative\nmerging (IM) algorithm that recursively merges client sets based on a lower\nestimate of the minimum sum-rate and updates to the value of the minimum\nsum-rate. We also show that a minimum sum-rate strategy can be learned by\nallocating rates for the local recovery in each merged client set in the IM\nalgorithm. We run an experiment to show that the complexity of the IM algorithm\nis lower than that of the existing deterministic algorithm when the number of\nclients is lower than $94$.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 04:00:13 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 11:29:21 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Ding", "Ni", ""], ["Kennedy", "Rodney A.", ""], ["Sadeghi", "Parastoo", ""]]}, {"id": "1503.03351", "submitter": "Chihao Zhang", "authors": "Yitong Yin, Chihao Zhang", "title": "Sampling colorings almost uniformly in sparse random graphs", "comments": "The paper has been withdrawn by the authors since the result has been\n  generalized and incorporated in their new work, arXiv:1507.07225", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of sampling proper $q$-colorings from uniform distribution has\nbeen extensively studied. Most of existing samplers require $q\\ge \\alpha\n\\Delta+\\beta$ for some constants $\\alpha$ and $\\beta$, where $\\Delta$ is the\nmaximum degree of the graph. The problem becomes more challenging when the\nunderlying graph has unbounded degree since even the decision of\n$q$-colorability becomes nontrivial in this situation. The Erd\\H{o}s-R\\'{e}nyi\nrandom graph $\\mathcal{G}(n,d/n)$ is a typical class of such graphs and has\nreceived a lot of recent attention. In this case, the performance of a sampler\nis usually measured by the relation between $q$ and the average degree $d$. We\nare interested in the fully polynomial-time almost uniform sampler (FPAUS) and\nthe state-of-the-art with such sampler for proper $q$-coloring on\n$\\mathcal{G}(n,d/n)$ requires that $q\\ge 5.5d$.\n  In this paper, we design an FPAUS for proper $q$-colorings on\n$\\mathcal{G}(n,d/n)$ by requiring that $q\\ge 3d+O(1)$, which improves the best\nbound for the problem so far. Our sampler is based on the spatial mixing\nproperty of $q$-coloring on random graphs. The core of the sampler is a\ndeterministic algorithm to estimate the marginal probability on blocks, which\nis computed by a novel block version of recursion for $q$-coloring on unbounded\ndegree graphs.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 14:31:49 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2015 01:29:08 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Yin", "Yitong", ""], ["Zhang", "Chihao", ""]]}, {"id": "1503.03465", "submitter": "Daniel Lemire", "authors": "Daniel Lemire and Owen Kaser", "title": "Faster 64-bit universal hashing using carry-less multiplications", "comments": null, "journal-ref": "Journal of Cryptographic Engineering, Volume 6, Issue 3, pp\n  171-185, 2016", "doi": "10.1007/s13389-015-0110-5", "report-no": null, "categories": "cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intel and AMD support the Carry-less Multiplication (CLMUL) instruction set\nin their x64 processors. We use CLMUL to implement an almost universal 64-bit\nhash family (CLHASH). We compare this new family with what might be the fastest\nalmost universal family on x64 processors (VHASH). We find that CLHASH is at\nleast 60% faster. We also compare CLHASH with a popular hash function designed\nfor speed (Google's CityHash). We find that CLHASH is 40% faster than CityHash\non inputs larger than 64 bytes and just as fast otherwise.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 19:47:09 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2015 15:32:20 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2015 20:26:26 GMT"}, {"version": "v4", "created": "Mon, 23 Mar 2015 18:31:47 GMT"}, {"version": "v5", "created": "Fri, 27 Mar 2015 20:45:29 GMT"}, {"version": "v6", "created": "Fri, 26 Jun 2015 01:45:13 GMT"}, {"version": "v7", "created": "Fri, 18 Sep 2015 00:10:17 GMT"}, {"version": "v8", "created": "Wed, 4 Nov 2015 16:34:39 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Lemire", "Daniel", ""], ["Kaser", "Owen", ""]]}, {"id": "1503.03763", "submitter": "Helio M. de Oliveira", "authors": "M.M. Campello de Souza, H.M. de Oliveira, R.M. Campello de Souza and\n  M.M. Vasconcelos", "title": "The Discrete Cosine Transform over Prime Finite Fields", "comments": "5 pages, 1 table, Lecture Notes in Computer Science, LNCS 3124,\n  Heidelberg: Springer Verlag, 2004, vol.1, pp.482-487, 2004", "journal-ref": null, "doi": "10.1007/978-3-540-27824-5_65", "report-no": null, "categories": "cs.DM cs.DS eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines finite field trigonometry as a tool to construct\ntrigonometric digital transforms. In particular, by using properties of the\nk-cosine function over GF(p), the Finite Field Discrete Cosine Transform\n(FFDCT) is introduced. The FFDCT pair in GF(p) is defined, having blocklengths\nthat are divisors of (p+1)/2. A special case is the Mersenne FFDCT, defined\nwhen p is a Mersenne prime. In this instance blocklengths that are powers of\ntwo are possible and radix-2 fast algorithms can be used to compute the\ntransform.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 15:17:31 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["de Souza", "M. M. Campello", ""], ["de Oliveira", "H. M.", ""], ["de Souza", "R. M. Campello", ""], ["Vasconcelos", "M. M.", ""]]}, {"id": "1503.03851", "submitter": "Yury Makarychev", "authors": "Konstantin Makarychev, Yury Makarychev, and Yuan Zhou", "title": "Satisfiability of Ordering CSPs Above Average", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the satisfiability of ordering constraint satisfaction problems\n(CSPs) above average. We prove the conjecture of Gutin, van Iersel, Mnich, and\nYeo that the satisfiability above average of ordering CSPs of arity $k$ is\nfixed-parameter tractable for every $k$. Previously, this was only known for\n$k=2$ and $k=3$. We also generalize this result to more general classes of\nCSPs, including CSPs with predicates defined by linear inequalities.\n  To obtain our results, we prove a new Bonami-type inequality for the\nEfron-Stein decomposition. The inequality applies to functions defined on\narbitrary product probability spaces. In contrast to other variants of the\nBonami Inequality, it does not depend on the mass of the smallest atom in the\nprobability space. We believe that this inequality is of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 19:35:22 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 20:03:34 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""], ["Zhou", "Yuan", ""]]}, {"id": "1503.03877", "submitter": "Ruchi Chaudhary", "authors": "Ruchi Chaudhary, David Fernandez-Baca, and J. Gordon Burleigh", "title": "Constructing and Employing Tree Alignment Graphs for Phylogenetic\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree alignment graphs (TAGs) provide an intuitive data structure for storing\nphylogenetic trees that exhibits the relationships of the individual input\ntrees and can potentially account for nested taxonomic relationships. This\npaper provides a theoretical foundation for the use of TAGs in phylogenetics.\nWe provide a formal definition of TAG that - unlike previous definition - does\nnot depend on the order in which input trees are provided. In the consensus\ncase, when all input trees have the same leaf labels, we describe algorithms\nfor constructing majority-rule and strict consensus trees using the TAG. When\nthe input trees do not have identical sets of leaf labels, we describe how to\ndetermine if the input trees are compatible and, if they are compatible, to\nconstruct a supertree that contains the input trees.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 20:07:05 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Chaudhary", "Ruchi", ""], ["Fernandez-Baca", "David", ""], ["Burleigh", "J. Gordon", ""]]}, {"id": "1503.03905", "submitter": "Jan Vondrak", "authors": "Alina Ene, Jan Vondrak, Yi Wu", "title": "Local Distribution and the Symmetry Gap: Approximability of Multiway\n  Partitioning Problems", "comments": "This is the full version of our SODA 2013 paper. Full proofs have\n  been included and one erroneous claim has been removed (brute-force rounding\n  for Min-CSP problems)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the approximability of multiway partitioning problems, examples of\nwhich include Multiway Cut, Node-weighted Multiway Cut, and Hypergraph Multiway\nCut. We investigate these problems from the point of view of two possible\ngeneralizations: as Min-CSPs, and as Submodular Multiway Partition problems.\nThese two generalizations lead to two natural relaxations, the Basic LP, and\nthe Lovasz relaxation. We show that the Lovasz relaxation gives a\n(2-2/k)-approximation for Submodular Multiway Partition with $k$ terminals,\nimproving a recent 2-approximation. We prove that this factor is optimal in two\nsenses: (1) A (2-2/k-\\epsilon)-approximation for Submodular Multiway Partition\nwith k terminals would require exponentially many value queries. (2) For\nHypergraph Multiway Cut and Node-weighted Multiway Cut with k terminals, both\nspecial cases of Submodular Multiway Partition, we prove that a\n(2-2/k-\\epsilon)-approximation is NP-hard, assuming the Unique Games\nConjecture.\n  Both our hardness results are more general: (1) We show that the notion of\nsymmetry gap, previously used for submodular maximization problems, also\nimplies hardness results for submodular minimization problems. (2) Assuming the\nUnique Games Conjecture, we show that the Basic LP gives an optimal\napproximation for every Min-CSP that includes the Not-Equal predicate.\n  Finally, we connect the two hardness techniques by proving that the\nintegrality gap of the Basic LP coincides with the symmetry gap of the\nmultilinear relaxation (for a related instance). This shows that the appearance\nof the same hardness threshold for a Min-CSP and the related submodular\nminimization problem is not a coincidence.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 22:27:37 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Ene", "Alina", ""], ["Vondrak", "Jan", ""], ["Wu", "Yi", ""]]}, {"id": "1503.03974", "submitter": "Carlo Comin", "authors": "Carlo Comin, Roberto Posenato, Romeo Rizzi", "title": "Hyper Temporal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple Temporal Networks (STNs) provide a powerful and general tool for\nrepresenting conjunctions of maximum delay constraints over ordered pairs of\ntemporal variables. In this paper we introduce Hyper Temporal Networks (HyTNs),\na strict generalization of STNs, to overcome the limitation of considering only\nconjunctions of constraints but maintaining a practical efficiency in the\nconsistency check of the instances. In a Hyper Temporal Network a single\ntemporal hyperarc constraint may be defined as a set of two or more maximum\ndelay constraints which is satisfied when at least one of these delay\nconstraints is satisfied. HyTNs are meant as a light generalization of STNs\noffering an interesting compromise. On one side, there exist practical\npseudo-polynomial time algorithms for checking consistency and computing\nfeasible schedules for HyTNs. On the other side, HyTNs offer a more powerful\nmodel accommodating natural constraints that cannot be expressed by STNs like\nTrigger off exactly delta min before (after) the occurrence of the first (last)\nevent in a set., which are used to represent synchronization events in some\nprocess aware information systems/workflow models proposed in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 07:44:19 GMT"}, {"version": "v2", "created": "Sun, 8 May 2016 17:00:34 GMT"}, {"version": "v3", "created": "Wed, 22 Mar 2017 15:49:44 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Comin", "Carlo", ""], ["Posenato", "Roberto", ""], ["Rizzi", "Romeo", ""]]}, {"id": "1503.04099", "submitter": "Jonathan Spreer", "authors": "Benjamin A. Burton, Cl\\'ement Maria, Jonathan Spreer", "title": "Algorithms and complexity for Turaev-Viro invariants", "comments": "17 pages, 5 figures", "journal-ref": "Journal of Applied and Computational Topology, 2018", "doi": "10.1007/s41468-018-0016-2", "report-no": null, "categories": "math.GT cs.CC cs.DS cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Turaev-Viro invariants are a powerful family of topological invariants\nfor distinguishing between different 3-manifolds. They are invaluable for\nmathematical software, but current algorithms to compute them require\nexponential time.\n  The invariants are parameterised by an integer $r \\geq 3$. We resolve the\nquestion of complexity for $r=3$ and $r=4$, giving simple proofs that computing\nTuraev-Viro invariants for $r=3$ is polynomial time, but for $r=4$ is \\#P-hard.\nMoreover, we give an explicit fixed-parameter tractable algorithm for arbitrary\n$r$, and show through concrete implementation and experimentation that this\nalgorithm is practical---and indeed preferable---to the prior state of the art\nfor real computation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 15:21:06 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Burton", "Benjamin A.", ""], ["Maria", "Cl\u00e9ment", ""], ["Spreer", "Jonathan", ""]]}, {"id": "1503.04169", "submitter": "Dung Nguyen", "authors": "Dung Nguyen, Molham Aref, Martin Bravenboer, George Kollias, Hung Q.\n  Ngo, Christopher R\\'e, Atri Rudra", "title": "Join Processing for Graph Patterns: An Old Dog with New Tricks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Join optimization has been dominated by Selinger-style, pairwise optimizers\nfor decades. But, Selinger-style algorithms are asymptotically suboptimal for\napplications in graphic analytics. This suboptimality is one of the reasons\nthat many have advocated supplementing relational engines with specialized\ngraph processing engines. Recently, new join algorithms have been discovered\nthat achieve optimal worst-case run times for any join or even so-called beyond\nworst-case (or instance optimal) run time guarantees for specialized classes of\njoins. These new algorithms match or improve on those used in specialized\ngraph-processing systems. This paper asks can these new join algorithms allow\nrelational engines to close the performance gap with graph engines?\n  We examine this question for graph-pattern queries or join queries. We find\nthat classical relational databases like Postgres and MonetDB or newer graph\ndatabases/stores like Virtuoso and Neo4j may be orders of magnitude slower than\nthese new approaches compared to a fully featured RDBMS, LogicBlox, using these\nnew ideas. Our results demonstrate that an RDBMS with such new algorithms can\nperform as well as specialized engines like GraphLab -- while retaining a\nhigh-level interface. We hope this adds to the ongoing debate of the role of\ngraph accelerators, new graph systems, and relational systems in modern\nworkloads.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 17:53:48 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 03:12:37 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Nguyen", "Dung", ""], ["Aref", "Molham", ""], ["Bravenboer", "Martin", ""], ["Kollias", "George", ""], ["Ngo", "Hung Q.", ""], ["R\u00e9", "Christopher", ""], ["Rudra", "Atri", ""]]}, {"id": "1503.04177", "submitter": "Dennis Weyland", "authors": "Dennis Weyland", "title": "Some Comments on the Stochastic Eulerian Tour Problem", "comments": "research commentary, 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Stochastic Eulerian Tour Problem was introduced in 2008 as a stochastic\nvariant of the well-known Eulerian Tour Problem. In a follow-up paper the same\nauthors investigated some heuristics for solving the Stochastic Eulerian Tour\nProblem. After a thorough study of these two publications a few issues emerged.\nIn this short research commentary we would like to discuss these issues.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 18:23:17 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Weyland", "Dennis", ""]]}, {"id": "1503.04426", "submitter": "Carlo Comin", "authors": "Carlo Comin, Romeo Rizzi", "title": "An Improved Pseudo-Polynomial Upper Bound for the Value Problem and\n  Optimal Strategy Synthesis in Mean Payoff Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we offer an $O(|V|^2 |E|\\, W)$ pseudo-polynomial time\ndeterministic algorithm for solving the Value Problem and Optimal Strategy\nSynthesis in Mean Payoff Games. This improves by a factor $\\log(|V|\\, W)$ the\nbest previously known pseudo-polynomial time upper bound due to Brim,~\\etal The\nimprovement hinges on a suitable characterization of values, and a description\nof optimal positional strategies, in terms of reweighted Energy Games and Small\nEnergy-Progress Measures.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2015 13:48:06 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 09:09:21 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2015 10:08:35 GMT"}, {"version": "v4", "created": "Mon, 20 Apr 2015 20:57:29 GMT"}, {"version": "v5", "created": "Sun, 20 Dec 2015 16:18:38 GMT"}, {"version": "v6", "created": "Wed, 23 Dec 2015 20:43:43 GMT"}, {"version": "v7", "created": "Sun, 24 Apr 2016 16:35:44 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Comin", "Carlo", ""], ["Rizzi", "Romeo", ""]]}, {"id": "1503.04468", "submitter": "Antonio Manuel Rodriguez-Chia", "authors": "J. Puerto, A.B. Ramos, A.M. Rodriguez-Chia and M.C. Sanchez-Gil", "title": "Ordered Median Hub Location Problems with Capacity Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Single Allocation Ordered Median Hub Location problem is a recent hub\nmodel introduced in Puerto et al. (2011) that provides a unifying analysis of a\nwide class of hub location mod- els. In this paper, we deal with the\ncapacitated version of this problem, presenting two formulations as well as\nsome preprocessing phases for fixing variables. In addition, a strengthening of\none of these formulations is also studied through the use of some fami- lies of\nvalid inequalities. A battery of test problems with data taken from the AP\nlibrary are solved where it is shown that the running times have been\nsignificantly reduced with the improvements presented in the paper.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2015 20:12:43 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Puerto", "J.", ""], ["Ramos", "A. B.", ""], ["Rodriguez-Chia", "A. M.", ""], ["Sanchez-Gil", "M. C.", ""]]}, {"id": "1503.04753", "submitter": "Kin Cheong Sou", "authors": "Kin Cheong Sou", "title": "Minimum Equivalent Precedence Relation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper two related simplification problems for systems of linear\ninequalities describing precedence relation systems are considered. Given a\nprecedence relation system, the first problem seeks a minimum subset of the\nprecedence relations (i.e., inequalities) which has the same solution set as\nthat of the original system. The second problem is the same as the first one\nexcept that the ``subset restriction'' in the first problem is removed. This\npaper establishes that the first problem is NP-hard. However, a sufficient\ncondition is provided under which the first problem is solvable in\npolynomial-time. In addition, a decomposition of the first problem into\nindependent tractable and intractable subproblems is derived. The second\nproblem is shown to be solvable in polynomial-time, with a full\nparameterization of all solutions described. The results in this paper\ngeneralize those in [Moyles and Thompson 1969, Aho, Garey, and Ullman 1972] for\nthe minimum equivalent graph problem and transitive reduction problem, which\nare applicable to unweighted directed graphs.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 14:58:07 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Sou", "Kin Cheong", ""]]}, {"id": "1503.04794", "submitter": "Michael LaPlante", "authors": "Michael LaPlante", "title": "A Polynomial Time Algorithm For Solving Clique Problems", "comments": "23 pages, 35 figures and some blocks of pseudo-code, but the\n  algorithm is explained by page 7 and 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present a single algorithm which solves the clique problems, \"What is the\nlargest size clique?\", \"What are all the maximal cliques?\" and the decision\nproblem, \"Does a clique of size k exist?\" for any given graph in polynomial\ntime. The existence of this algorithm proves that P = NP.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 18:31:48 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["LaPlante", "Michael", ""]]}, {"id": "1503.04843", "submitter": "Thomas Steinke", "authors": "Raef Bassily and Adam Smith and Thomas Steinke and Jonathan Ullman", "title": "More General Queries and Less Generalization Error in Adaptive Data\n  Analysis", "comments": "This paper was merged with another manuscript and is now subsumed by\n  arXiv:1511.02513", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptivity is an important feature of data analysis---typically the choice of\nquestions asked about a dataset depends on previous interactions with the same\ndataset. However, generalization error is typically bounded in a non-adaptive\nmodel, where all questions are specified before the dataset is drawn. Recent\nwork by Dwork et al. (STOC '15) and Hardt and Ullman (FOCS '14) initiated the\nformal study of this problem, and gave the first upper and lower bounds on the\nachievable generalization error for adaptive data analysis.\n  Specifically, suppose there is an unknown distribution $\\mathcal{P}$ and a\nset of $n$ independent samples $x$ is drawn from $\\mathcal{P}$. We seek an\nalgorithm that, given $x$ as input, \"accurately\" answers a sequence of\nadaptively chosen \"queries\" about the unknown distribution $\\mathcal{P}$. How\nmany samples $n$ must we draw from the distribution, as a function of the type\nof queries, the number of queries, and the desired level of accuracy?\n  In this work we make two new contributions towards resolving this question:\n  *We give upper bounds on the number of samples $n$ that are needed to answer\nstatistical queries that improve over the bounds of Dwork et al.\n  *We prove the first upper bounds on the number of samples required to answer\nmore general families of queries. These include arbitrary low-sensitivity\nqueries and the important class of convex risk minimization queries.\n  As in Dwork et al., our algorithms are based on a connection between\ndifferential privacy and generalization error, but we feel that our analysis is\nsimpler and more modular, which may be useful for studying these questions in\nthe future.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 20:48:42 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 02:01:05 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Bassily", "Raef", ""], ["Smith", "Adam", ""], ["Steinke", "Thomas", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1503.04915", "submitter": "EPTCS", "authors": "Jean-Michel Hufflen (FEMTO-ST & University of Franche-Comt\\'e)", "title": "Using Model-Checking Techniques for Component-Based Systems with\n  Reconfigurations", "comments": "In Proceedings FESCA 2015, arXiv:1503.04378", "journal-ref": "EPTCS 178, 2015, pp. 33-46", "doi": "10.4204/EPTCS.178.4", "report-no": null, "categories": "cs.SE cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within a component-based approach allowing dynamic reconfigurations,\nsequences of successive reconfiguration operations are expressed by means of\nreconfiguration paths, possibly infinite. We show that a subclass of such paths\ncan be modelled by finite state automata. This feature allows us to use\ntechniques related to model-checking to prove some architectural, event, and\ntemporal properties related to dynamic reconfiguration. Our method is proved\ncorrect w.r.t. these properties' definition.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 04:00:09 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Hufflen", "Jean-Michel", "", "FEMTO-ST & University of Franche-Comt\u00e9"]]}, {"id": "1503.04963", "submitter": "Janne H. Korhonen", "authors": "Keren Censor-Hillel, Petteri Kaski, Janne H. Korhonen, Christoph\n  Lenzen, Ami Paz, Jukka Suomela", "title": "Algebraic Methods in the Congested Clique", "comments": "This is work is a merger of arxiv:1412.2109 and arxiv:1412.2667", "journal-ref": null, "doi": "10.1007/s00446-016-0270-2", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we use algebraic methods for studying distance computation and\nsubgraph detection tasks in the congested clique model. Specifically, we adapt\nparallel matrix multiplication implementations to the congested clique,\nobtaining an $O(n^{1-2/\\omega})$ round matrix multiplication algorithm, where\n$\\omega < 2.3728639$ is the exponent of matrix multiplication. In conjunction\nwith known techniques from centralised algorithmics, this gives significant\nimprovements over previous best upper bounds in the congested clique model. The\nhighlight results include:\n  -- triangle and 4-cycle counting in $O(n^{0.158})$ rounds, improving upon the\n$O(n^{1/3})$ triangle detection algorithm of Dolev et al. [DISC 2012],\n  -- a $(1 + o(1))$-approximation of all-pairs shortest paths in $O(n^{0.158})$\nrounds, improving upon the $\\tilde{O} (n^{1/2})$-round $(2 +\no(1))$-approximation algorithm of Nanongkai [STOC 2014], and\n  -- computing the girth in $O(n^{0.158})$ rounds, which is the first\nnon-trivial solution in this model.\n  In addition, we present a novel constant-round combinatorial algorithm for\ndetecting 4-cycles.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 09:31:48 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Kaski", "Petteri", ""], ["Korhonen", "Janne H.", ""], ["Lenzen", "Christoph", ""], ["Paz", "Ami", ""], ["Suomela", "Jukka", ""]]}, {"id": "1503.04988", "submitter": "Matthew Sackman", "authors": "Matthew Sackman", "title": "Perfect Consistent Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Consistent Hashing functions are widely used for load balancing across a\nvariety of applications. However, the original presentation and typical\nimplementations of Consistent Hashing rely on randomised allocation of hash\ncodes to keys which results in a flawed and approximately-uniform allocation of\nkeys to hash codes. We analyse the desired properties and present an algorithm\nthat perfectly achieves them without resorting to any random distributions. The\nalgorithm is simple and adds to our understanding of what is necessary to\ncreate a consistent hash function.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 10:46:40 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 10:25:51 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Sackman", "Matthew", ""]]}, {"id": "1503.04990", "submitter": "Michael Bekos", "authors": "Michael A. Bekos, Till Bruckdorfer, Michael Kaufmann, Chrysanthi N.\n  Raftopoulou", "title": "The Book Thickness of 1-Planar Graphs is Constant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a book embedding, the vertices of a graph are placed on the spine of a\nbook and the edges are assigned to pages, so that edges on the same page do not\ncross. In this paper, we prove that every $1$-planar graph (that is, a graph\nthat can be drawn on the plane such that no edge is crossed more than once)\nadmits an embedding in a book with constant number of pages. To the best of our\nknowledge, the best non-trivial previous upper-bound is $O(\\sqrt{n})$, where\n$n$ is the number of vertices of the graph.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 10:49:36 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 09:45:16 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Bekos", "Michael A.", ""], ["Bruckdorfer", "Till", ""], ["Kaufmann", "Michael", ""], ["Raftopoulou", "Chrysanthi N.", ""]]}, {"id": "1503.04994", "submitter": "Elena Dubrova", "authors": "Maxim Teslenko and Elena Dubrova", "title": "A Linear-Time Algorithm for Finding All Double-Vertex Dominators of a\n  Given Vertex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dominators provide a general mechanism for identifying reconverging paths in\ngraphs. This is useful for a number of applications in Computer-Aided Design\n(CAD) including signal probability computation in biased random simulation,\nswitching activity estimation in power and noise analysis, and cut points\nidentification in equivalence checking. However, traditional single-vertex\ndominators are too rare in circuit graphs. In order to handle reconverging\npaths more efficiently, we consider the case of double-vertex dominators which\noccur more frequently. First, we derive a number of specific properties of\ndouble-vertex dominators. Then, we describe a data structure for representing\nall double-vertex dominators of a given vertex in linear space. Finally, we\npresent an algorithm for finding all double-vertex dominators of a given vertex\nin linear time. Our results provide an efficient systematic way of partitioning\nlarge graphs along the reconverging points of the signal flow.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 10:58:18 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Teslenko", "Maxim", ""], ["Dubrova", "Elena", ""]]}, {"id": "1503.05110", "submitter": "Florian Sikora", "authors": "\\'Edouard Bonnet, Florian Sikora", "title": "The Graph Motif problem parameterized by the structure of the input\n  graph", "comments": "24 pages, accepted in DAM, conference version in IPEC 2015", "journal-ref": null, "doi": "10.1016/j.dam.2016.11.016", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Graph Motif problem was introduced in 2006 in the context of biological\nnetworks. It consists of deciding whether or not a multiset of colors occurs in\na connected subgraph of a vertex-colored graph. Graph Motif has been mostly\nanalyzed from the standpoint of parameterized complexity. The main parameters\nwhich came into consideration were the size of the multiset and the number of\ncolors. Though, in the many applications of Graph Motif, the input graph\noriginates from real-life and has structure. Motivated by this prosaic\nobservation, we systematically study its complexity relatively to graph\nstructural parameters. For a wide range of parameters, we give new or improved\nFPT algorithms, or show that the problem remains intractable. For the FPT\ncases, we also give some kernelization lower bounds as well as some ETH-based\nlower bounds on the worst case running time. Interestingly, we establish that\nGraph Motif is W[1]-hard (while in W[P]) for parameter max leaf number, which\nis, to the best of our knowledge, the first problem to behave this way.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 16:17:04 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 16:38:06 GMT"}, {"version": "v3", "created": "Thu, 12 Jan 2017 12:18:02 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Sikora", "Florian", ""]]}, {"id": "1503.05157", "submitter": "Jeremy Debattista", "authors": "Jeremy Debattista, Santiago Londo\\~no, Christoph Lange, S\\\"oren Auer", "title": "Quality Assessment of Linked Datasets using Probabilistic Approximation", "comments": "15 pages, 2 figures, To appear in ESWC 2015 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing application of Linked Open Data, assessing the quality of\ndatasets by computing quality metrics becomes an issue of crucial importance.\nFor large and evolving datasets, an exact, deterministic computation of the\nquality metrics is too time consuming or expensive. We employ probabilistic\ntechniques such as Reservoir Sampling, Bloom Filters and Clustering Coefficient\nestimation for implementing a broad set of data quality metrics in an\napproximate but sufficiently accurate way. Our implementation is integrated in\nthe comprehensive data quality assessment framework Luzzu. We evaluated its\nperformance and accuracy on Linked Open Datasets of broad relevance.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 18:39:22 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Debattista", "Jeremy", ""], ["Londo\u00f1o", "Santiago", ""], ["Lange", "Christoph", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "1503.05225", "submitter": "Amirali Abdullah", "authors": "Amirali Abdullah and Ravi Kumar and Andrew McGregor and Sergei\n  Vassilvitskii and Suresh Venkatasubramanian", "title": "Sketching, Embedding, and Dimensionality Reduction for Information\n  Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information distances like the Hellinger distance and the Jensen-Shannon\ndivergence have deep roots in information theory and machine learning. They are\nused extensively in data analysis especially when the objects being compared\nare high dimensional empirical probability distributions built from data.\nHowever, we lack common tools needed to actually use information distances in\napplications efficiently and at scale with any kind of provable guarantees. We\ncan't sketch these distances easily, or embed them in better behaved spaces, or\neven reduce the dimensionality of the space while maintaining the probability\nstructure of the data.\n  In this paper, we build these tools for information distances---both for the\nHellinger distance and Jensen--Shannon divergence, as well as related measures,\nlike the $\\chi^2$ divergence. We first show that they can be sketched\nefficiently (i.e. up to multiplicative error in sublinear space) in the\naggregate streaming model. This result is exponentially stronger than known\nupper bounds for sketching these distances in the strict turnstile streaming\nmodel. Second, we show a finite dimensionality embedding result for the\nJensen-Shannon and $\\chi^2$ divergences that preserves pair wise distances.\nFinally we prove a dimensionality reduction result for the Hellinger,\nJensen--Shannon, and $\\chi^2$ divergences that preserves the information\ngeometry of the distributions (specifically, by retaining the simplex structure\nof the space). While our second result above already implies that these\ndivergences can be explicitly embedded in Euclidean space, retaining the\nsimplex structure is important because it allows us to continue doing inference\nin the reduced space. In essence, we preserve not just the distance structure\nbut the underlying geometry of the space.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 21:25:22 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Abdullah", "Amirali", ""], ["Kumar", "Ravi", ""], ["McGregor", "Andrew", ""], ["Vassilvitskii", "Sergei", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1503.05638", "submitter": "Y. William Yu", "authors": "Y. William Yu, Noah M. Daniels, David Christian Danko, Bonnie Berger", "title": "Entropy-scaling search of massive biological data", "comments": "Including supplement: 41 pages, 6 figures, 4 tables, 1 box", "journal-ref": "Cell Systems, Volume 1, Issue 2, 130-140, 2015", "doi": "10.1016/j.cels.2015.08.004", "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many datasets exhibit a well-defined structure that can be exploited to\ndesign faster search tools, but it is not always clear when such acceleration\nis possible. Here, we introduce a framework for similarity search based on\ncharacterizing a dataset's entropy and fractal dimension. We prove that\nsearching scales in time with metric entropy (number of covering hyperspheres),\nif the fractal dimension of the dataset is low, and scales in space with the\nsum of metric entropy and information-theoretic entropy (randomness of the\ndata). Using these ideas, we present accelerated versions of standard tools,\nwith no loss in specificity and little loss in sensitivity, for use in three\ndomains---high-throughput drug screening (Ammolite, 150x speedup), metagenomics\n(MICA, 3.5x speedup of DIAMOND [3,700x BLASTX]), and protein structure search\n(esFragBag, 10x speedup of FragBag). Our framework can be used to achieve\n\"compressive omics,\" and the general theory can be readily applied to data\nscience problems outside of biology.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 02:54:21 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 04:31:50 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Yu", "Y. William", ""], ["Daniels", "Noah M.", ""], ["Danko", "David Christian", ""], ["Berger", "Bonnie", ""]]}, {"id": "1503.05670", "submitter": "Yinglei Song", "authors": "Yinglei Song", "title": "Time and Space Efficient Algorithms for RNA Folding with the\n  Four-Russians Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop new algorithms for the basic RNA folding problem.\nGiven an RNA sequence that contains $n$ nucleotides, the goal of the problem is\nto compute a pseudoknot-free secondary structure that maximizes the number of\nbase pairs in the sequence. We show that there exists a dynamic programming\nalgorithm that can solve the problem in time $O(\\frac{n^{3}}{\\log_{2}{n}})$\nwhile using only $O(\\frac{n^{2}}{\\log_{2}{n}})$ memory space. In addition, we\nshow that the time complexity of this algorithm can be further improved to\n$O(\\frac{n^{3}}{\\log_{2}^{2}{n}})$ at the expense of a slightly increased space\ncomplexity. To the best of our knowledge, this is the first algorithm that can\nsolve the problem with traditional dynamic programming techniques in time\n$O(\\frac{n^{3}}{\\log_{2}^{2}{n}})$. In addition, our results improve the best\nknown upper bound of the space complexity for efficiently solving both this\nproblem and the context-free language recognition problem.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 08:21:25 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2015 09:32:05 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Song", "Yinglei", ""]]}, {"id": "1503.05698", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Martin Wimmer, Jakob Gruber, Jesper Larsson Tr\\\"aff, Philippas Tsigas", "title": "The Lock-free $k$-LSM Relaxed Priority Queue", "comments": "Short version as ACM PPoPP'15 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Priority queues are data structures which store keys in an ordered fashion to\nallow efficient access to the minimal (maximal) key. Priority queues are\nessential for many applications, e.g., Dijkstra's single-source shortest path\nalgorithm, branch-and-bound algorithms, and prioritized schedulers.\n  Efficient multiprocessor computing requires implementations of basic data\nstructures that can be used concurrently and scale to large numbers of threads\nand cores. Lock-free data structures promise superior scalability by avoiding\nblocking synchronization primitives, but the \\emph{delete-min} operation is an\ninherent scalability bottleneck in concurrent priority queues. Recent work has\nfocused on alleviating this obstacle either by batching operations, or by\nrelaxing the requirements to the \\emph{delete-min} operation.\n  We present a new, lock-free priority queue that relaxes the \\emph{delete-min}\noperation so that it is allowed to delete \\emph{any} of the $\\rho+1$ smallest\nkeys, where $\\rho$ is a runtime configurable parameter. Additionally, the\nbehavior is identical to a non-relaxed priority queue for items added and\nremoved by the same thread. The priority queue is built from a logarithmic\nnumber of sorted arrays in a way similar to log-structured merge-trees. We\nexperimentally compare our priority queue to recent state-of-the-art lock-free\npriority queues, both with relaxed and non-relaxed semantics, showing high\nperformance and good scalability of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 10:34:38 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Wimmer", "Martin", ""], ["Gruber", "Jakob", ""], ["Tr\u00e4ff", "Jesper Larsson", ""], ["Tsigas", "Philippas", ""]]}, {"id": "1503.05761", "submitter": "Sian-Jheng Lin", "authors": "Sian-Jheng Lin, Tareq Y. Al-Naffouri, Yunghsiang S. Han", "title": "FFT Algorithm for Binary Extension Finite Fields and its Application to\n  Reed-Solomon Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, a new polynomial basis over binary extension fields was proposed\nsuch that the fast Fourier transform (FFT) over such fields can be computed in\nthe complexity of order $\\mathcal{O}(n\\lg(n))$, where $n$ is the number of\npoints evaluated in FFT. In this work, we reformulate this FFT algorithm such\nthat it can be easier understood and be extended to develop frequency-domain\ndecoding algorithms for $(n=2^m,k)$ systematic Reed-Solomon~(RS) codes over\n$\\mathbb{F}_{2^m},m\\in \\mathbb{Z}^+$, with $n-k$ a power of two. First, the\nbasis of syndrome polynomials is reformulated in the decoding procedure so that\nthe new transforms can be applied to the decoding procedure. A fast extended\nEuclidean algorithm is developed to determine the error locator polynomial. The\ncomputational complexity of the proposed decoding algorithm is\n$\\mathcal{O}(n\\lg(n-k)+(n-k)\\lg^2(n-k))$, improving upon the best currently\navailable decoding complexity $\\mathcal{O}(n\\lg^2(n)\\lg\\lg(n))$, and reaching\nthe best known complexity bound that was established by Justesen in 1976.\nHowever, Justesen's approach is only for the codes over some specific fields,\nwhich can apply Cooley-Tucky FFTs. As revealed by the computer simulations, the\nproposed decoding algorithm is $50$ times faster than the conventional one for\nthe $(2^{16},2^{15})$ RS code over $\\mathbb{F}_{2^{16}}$.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 13:39:59 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2015 05:22:42 GMT"}, {"version": "v3", "created": "Sun, 14 Aug 2016 05:09:28 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Lin", "Sian-Jheng", ""], ["Al-Naffouri", "Tareq Y.", ""], ["Han", "Yunghsiang S.", ""]]}, {"id": "1503.05812", "submitter": "Yitong Yin", "authors": "Renjie Song and Yitong Yin and Jinman Zhao", "title": "Counting hypergraph matchings up to uniqueness threshold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of approximately counting matchings in hypergraphs of\nbounded maximum degree and maximum size of hyperedges. With an activity\nparameter $\\lambda$, each matching $M$ is assigned a weight $\\lambda^{|M|}$.\nThe counting problem is formulated as computing a partition function that gives\nthe sum of the weights of all matchings in a hypergraph. This problem unifies\ntwo extensively studied statistical physics models in approximate counting: the\nhardcore model (graph independent sets) and the monomer-dimer model (graph\nmatchings).\n  For this model, the critical activity $\\lambda_c= \\frac{d^d}{k (d-1)^{d+1}}$\nis the threshold for the uniqueness of Gibbs measures on the infinite\n$(d+1)$-uniform $(k+1)$-regular hypertree. Consider hypergraphs of maximum\ndegree at most $k+1$ and maximum size of hyperedges at most $d+1$. We show that\nwhen $\\lambda < \\lambda_c$, there is an FPTAS for computing the partition\nfunction; and when $\\lambda = \\lambda_c$, there is a PTAS for computing the\nlog-partition function. These algorithms are based on the decay of correlation\n(strong spatial mixing) property of Gibbs distributions. When $\\lambda >\n2\\lambda_c$, there is no PRAS for the partition function or the log-partition\nfunction unless NP$=$RP.\n  Towards obtaining a sharp transition of computational complexity of\napproximate counting, we study the local convergence from a sequence of finite\nhypergraphs to the infinite lattice with specified symmetry. We show a\nsurprising connection between the local convergence and the reversibility of a\nnatural random walk. This leads us to a barrier for the hardness result: The\nnon-uniqueness of infinite Gibbs measure is not realizable by any finite\ngadgets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 15:55:36 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 16:40:23 GMT"}, {"version": "v3", "created": "Fri, 6 Jan 2017 08:01:01 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Song", "Renjie", ""], ["Yin", "Yitong", ""], ["Zhao", "Jinman", ""]]}, {"id": "1503.05832", "submitter": "Kevin Sanft", "authors": "Kevin R. Sanft and Hans G. Othmer", "title": "Constant-complexity Stochastic Simulation Algorithm with Optimal Binning", "comments": "The following article has been submitted to The Journal of Chemical\n  Physics. After it is published, it will be found at\n  http://scitation.aip.org/content/aip/journal/jcp", "journal-ref": null, "doi": "10.1063/1.4928635", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the cellular scale, biochemical processes are governed by random\ninteractions between reactant molecules with small copy counts, leading to\nbehavior that is inherently stochastic. Such systems are often modeled as\ncontinuous-time Markov jump processes that can be described by the Chemical\nMaster Equation. Gillespie's Stochastic Simulation Algorithm (SSA) generates\nexact trajectories of these systems. The amount of computational work required\nfor each step of the original SSA is proportional to the number of reaction\nchannels, leading to computational complexity that scales linearly as the\nproblem size increases. The original SSA is therefore inefficient for large\nproblems, which has prompted the development of several alternative\nformulations with improved scaling properties. We describe an exact SSA that\nuses a table data structure with event time binning to achieve constant\ncomputational complexity. Optimal algorithm parameters and binning strategies\nare discussed. We compare the computational efficiency of the algorithm to\nexisting methods and demonstrate excellent scaling for large problems. This\nmethod is well suited for generating exact trajectories of large models that\ncan be described by the Reaction-Diffusion Master Equation arising from\nspatially discretized reaction-diffusion processes.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 16:31:18 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Sanft", "Kevin R.", ""], ["Othmer", "Hans G.", ""]]}, {"id": "1503.05960", "submitter": "Samin Aref", "authors": "Iman Kazemian and Samin Aref", "title": "Hub Location under Uncertainty: a Minimax Regret Model for the\n  Capacitated Problem with Multiple Allocations", "comments": "11 pages, 10 tables, 3 figures, accepted pre-print (author copy) with\n  supplementary data", "journal-ref": "Int. J. Supply Chain and Inventory Management, Vol. 2, No. 1,\n  pp.1-19 (2017)", "doi": "10.1504/IJSCIM.2017.086371", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the capacitated hub location problem is formulated by a minimax\nregret model, which takes into account uncertain setup cost and demand. We\nfocus on hub location with multiple allocations as a strategic problem\nrequiring one definite solution. Investigating how deterministic models may\nlead to sub-optimal solutions, we provide an efficient formulation method for\nthe problem.\n  A computational analysis is performed to investigate the impact of\nuncertainty on the location of hubs. The suggested model is also compared with\nan alternative method, seasonal optimization, in terms of efficiency and\npracticability. The results indicate the importance of incorporating\nstochasticity and variability of parameters in solving practical hub location\nproblems. Applying our method to a case study derived from an industrial food\nproduction company, we solve a logistical problem involving seasonal demand and\nuncertainty. The solution yields a definite hub network configuration to be\nimplemented throughout the planning horizon.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 22:45:19 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2015 03:10:26 GMT"}, {"version": "v3", "created": "Wed, 8 Jun 2016 08:09:09 GMT"}, {"version": "v4", "created": "Sun, 17 Sep 2017 03:09:56 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Kazemian", "Iman", ""], ["Aref", "Samin", ""]]}, {"id": "1503.05977", "submitter": "Yakov Nekrich", "authors": "J. Ian Munro and Yakov Nekrich and Jeffrey Scott Vitter", "title": "Dynamic Data Structures for Document Collections and Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the dynamic indexing problem, we must maintain a changing collection of\ntext documents so that we can efficiently support insertions, deletions, and\npattern matching queries. We are especially interested in developing efficient\ndata structures that store and query the documents in compressed form. All\nprevious compressed solutions to this problem rely on answering rank and select\nqueries on a dynamic sequence of symbols. Because of the lower bound in\n[Fredman and Saks, 1989], answering rank queries presents a bottleneck in\ncompressed dynamic indexing. In this paper we show how this lower bound can be\ncircumvented using our new framework. We demonstrate that the gap between\nstatic and dynamic variants of the indexing problem can be almost closed. Our\nmethod is based on a novel framework for adding dynamism to static compressed\ndata structures. Our framework also applies more generally to dynamizing other\nproblems. We show, for example, how our framework can be applied to develop\ncompressed representations of dynamic graphs and binary relations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 01:31:25 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Munro", "J. Ian", ""], ["Nekrich", "Yakov", ""], ["Vitter", "Jeffrey Scott", ""]]}, {"id": "1503.06271", "submitter": "Mina Ghashami", "authors": "Mina Ghashami and Amirali Abdullah", "title": "Binary Coding in Stream", "comments": "5 figures, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data is becoming ever more ubiquitous, ranging over massive video\nrepositories, document corpuses, image sets and Internet routing history.\nProximity search and clustering are two algorithmic primitives fundamental to\ndata analysis, but suffer from the \"curse of dimensionality\" on these gigantic\ndatasets. A popular attack for this problem is to convert object\nrepresentations into short binary codewords, while approximately preserving\nnear neighbor structure. However, there has been limited research on\nconstructing codewords in the \"streaming\" or \"online\" settings often applicable\nto this scale of data, where one may only make a single pass over data too\nmassive to fit in local memory.\n  In this paper, we apply recent advances in matrix sketching techniques to\nconstruct binary codewords in both streaming and online setting. Our\nexperimental results compete outperform several of the most popularly used\nalgorithms, and we prove theoretical guarantees on performance in the streaming\nsetting under mild assumptions on the data and randomness of the training set.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 06:25:02 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Ghashami", "Mina", ""], ["Abdullah", "Amirali", ""]]}, {"id": "1503.06321", "submitter": "Christian Komusiewicz", "authors": "Danny Hermelin, Moshe Kaspi, Christian Komusiewicz, Barak Navon", "title": "Parameterized Complexity of Critical Node Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following natural graph cut problem called Critical Node Cut\n(CNC): Given a graph $G$ on $n$ vertices, and two positive integers $k$ and\n$x$, determine whether $G$ has a set of $k$ vertices whose removal leaves $G$\nwith at most $x$ connected pairs of vertices. We analyze this problem in the\nframework of parameterized complexity. That is, we are interested in whether or\nnot this problem is solvable in $f(\\kappa) \\cdot n^{O(1)}$ time (i.e., whether\nor not it is fixed-parameter tractable), for various natural parameters\n$\\kappa$. We consider four such parameters:\n  - The size $k$ of the required cut.\n  - The upper bound $x$ on the number of remaining connected pairs.\n  - The lower bound $y$ on the number of connected pairs to be removed.\n  - The treewidth $w$ of $G$.\n  We determine whether or not CNC is fixed-parameter tractable for each of\nthese parameters. We determine this also for all possible aggregations of these\nfour parameters, apart from $w+k$. Moreover, we also determine whether or not\nCNC admits a polynomial kernel for all these parameterizations. That is,\nwhether or not there is an algorithm that reduces each instance of CNC in\npolynomial time to an equivalent instance of size $\\kappa^{O(1)}$, where\n$\\kappa$ is the given parameter.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 17:03:30 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2015 19:29:14 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Hermelin", "Danny", ""], ["Kaspi", "Moshe", ""], ["Komusiewicz", "Christian", ""], ["Navon", "Barak", ""]]}, {"id": "1503.06381", "submitter": "Ellen Vitercik", "authors": "Allison Lewko, Ellen Vitercik", "title": "Balancing Communication for Multi-party Interactive Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider interactive coding in a setting where $n$ parties wish to compute\na joint function of their inputs via an interactive protocol over imperfect\nchannels. We assume that adversarial errors can comprise a\n$\\mathcal{O}(\\frac{1}{n})$ fraction of the total communication, occurring\nanywhere on the communication network. Our goal is to maintain a constant\nmultiplicative overhead in the total communication required, as compared to the\nerror-free setting, and also to balance the workload over the different\nparties. We build upon the prior protocol of Jain, Kalai, and Lewko, but while\nthat protocol relies on a single coordinator to shoulder a heavy burden\nthroughout the protocol, we design a mechanism to pass the coordination duties\nfrom party to party, resulting in a more even distribution of communication\nover the course of the computation.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 03:50:32 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Lewko", "Allison", ""], ["Vitercik", "Ellen", ""]]}, {"id": "1503.06394", "submitter": "Insu Han", "authors": "Insu Han, Dmitry Malioutov, Jinwoo Shin", "title": "Large-scale Log-determinant Computation through Stochastic Chebyshev\n  Expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Logarithms of determinants of large positive definite matrices appear\nubiquitously in machine learning applications including Gaussian graphical and\nGaussian process models, partition functions of discrete graphical models,\nminimum-volume ellipsoids, metric learning and kernel learning. Log-determinant\ncomputation involves the Cholesky decomposition at the cost cubic in the number\nof variables, i.e., the matrix dimension, which makes it prohibitive for\nlarge-scale applications. We propose a linear-time randomized algorithm to\napproximate log-determinants for very large-scale positive definite and general\nnon-singular matrices using a stochastic trace approximation, called the\nHutchinson method, coupled with Chebyshev polynomial expansions that both rely\non efficient matrix-vector multiplications. We establish rigorous additive and\nmultiplicative approximation error bounds depending on the condition number of\nthe input matrix. In our experiments, the proposed algorithm can provide very\nhigh accuracy solutions at orders of magnitude faster time than the Cholesky\ndecomposition and Schur completion, and enables us to compute log-determinants\nof matrices involving tens of millions of variables.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 06:55:12 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Han", "Insu", ""], ["Malioutov", "Dmitry", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1503.06438", "submitter": "Meirav Zehavi", "authors": "Meirav Zehavi", "title": "Maximization Problems Parameterized Using Their Minimization Versions:\n  The Case of Vertex Cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parameterized complexity of problems is often studied with respect to the\nsize of their optimal solutions. However, for a maximization problem, the size\nof the optimal solution can be very large, rendering algorithms parameterized\nby it inefficient. Therefore, we suggest to study the parameterized complexity\nof maximization problems with respect to the size of the optimal solutions to\ntheir minimization versions. We examine this suggestion by considering the\nMaximal Minimal Vertex Cover (MMVC) problem, whose minimization version, Vertex\nCover, is one of the most studied problems in the field of Parameterized\nComplexity. Our main contribution is a parameterized approximation algorithm\nfor MMVC, including its weighted variant. We also give conditional lower bounds\nfor the running times of algorithms for MMVC and its weighted variant.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 15:28:55 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Zehavi", "Meirav", ""]]}, {"id": "1503.06447", "submitter": "Raghu Meka", "authors": "Raghu Meka, Aaron Potechin, Avi Wigderson", "title": "Sum-of-squares lower bounds for planted clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding cliques in random graphs and the closely related \"planted\" clique\nvariant, where a clique of size k is planted in a random G(n, 1/2) graph, have\nbeen the focus of substantial study in algorithm design. Despite much effort,\nthe best known polynomial-time algorithms only solve the problem for k ~\nsqrt(n).\n  In this paper we study the complexity of the planted clique problem under\nalgorithms from the Sum-of-squares hierarchy. We prove the first average case\nlower bound for this model: for almost all graphs in G(n,1/2), r rounds of the\nSOS hierarchy cannot find a planted k-clique unless k > n^{1/2r} (up to\nlogarithmic factors). Thus, for any constant number of rounds planted cliques\nof size n^{o(1)} cannot be found by this powerful class of algorithms. This is\nshown via an integrability gap for the natural formulation of maximum clique\nproblem on random graphs for SOS and Lasserre hierarchies, which in turn follow\nfrom degree lower bounds for the Positivestellensatz proof system.\n  We follow the usual recipe for such proofs. First, we introduce a natural\n\"dual certificate\" (also known as a \"vector-solution\" or \"pseudo-expectation\")\nfor the given system of polynomial equations representing the problem for every\nfixed input graph. Then we show that the matrix associated with this dual\ncertificate is PSD (positive semi-definite) with high probability over the\nchoice of the input graph.This requires the use of certain tools. One is the\ntheory of association schemes, and in particular the eigenspaces and\neigenvalues of the Johnson scheme. Another is a combinatorial method we develop\nto compute (via traces) norm bounds for certain random matrices whose entries\nare highly dependent; we hope this method will be useful elsewhere.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 17:48:28 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Meka", "Raghu", ""], ["Potechin", "Aaron", ""], ["Wigderson", "Avi", ""]]}, {"id": "1503.06483", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Maryam Yammahi, Nima Bari, Roman Vichr, Faisal Alsaby,\n  Simon Y. Berkovich", "title": "Construction of FuzzyFind Dictionary using Golay Coding Transformation\n  for Searching Applications", "comments": null, "journal-ref": null, "doi": "10.14569/IJACSA.2015.060313", "report-no": null, "categories": "cs.DB cs.AI cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching through a large volume of data is very critical for companies,\nscientists, and searching engines applications due to time complexity and\nmemory complexity. In this paper, a new technique of generating FuzzyFind\nDictionary for text mining was introduced. We simply mapped the 23 bits of the\nEnglish alphabet into a FuzzyFind Dictionary or more than 23 bits by using more\nFuzzyFind Dictionary, and reflecting the presence or absence of particular\nletters. This representation preserves closeness of word distortions in terms\nof closeness of the created binary vectors within Hamming distance of 2\ndeviations. This paper talks about the Golay Coding Transformation Hash Table\nand how it can be used on a FuzzyFind Dictionary as a new technology for using\nin searching through big data. This method is introduced by linear time\ncomplexity for generating the dictionary and constant time complexity to access\nthe data and update by new data sets, also updating for new data sets is linear\ntime depends on new data points. This technique is based on searching only for\nletters of English that each segment has 23 bits, and also we have more than\n23-bit and also it could work with more segments as reference table.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 21:46:12 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Kowsari", "Kamran", ""], ["Yammahi", "Maryam", ""], ["Bari", "Nima", ""], ["Vichr", "Roman", ""], ["Alsaby", "Faisal", ""], ["Berkovich", "Simon Y.", ""]]}, {"id": "1503.06567", "submitter": "Andrej Risteski", "authors": "Pranjal Awasthi and Andrej Risteski", "title": "On some provably correct cases of variational inference for topic models", "comments": "46 pages, Compared to previous version: clarified notation, a number\n  of typos fixed throughout paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a very efficient and popular heuristic used in\nvarious forms in the context of latent variable models. It's closely related to\nExpectation Maximization (EM), and is applied when exact EM is computationally\ninfeasible. Despite being immensely popular, current theoretical understanding\nof the effectiveness of variaitonal inference based algorithms is very limited.\nIn this work we provide the first analysis of instances where variational\ninference algorithms converge to the global optimum, in the setting of topic\nmodels.\n  More specifically, we show that variational inference provably learns the\noptimal parameters of a topic model under natural assumptions on the topic-word\nmatrix and the topic priors. The properties that the topic word matrix must\nsatisfy in our setting are related to the topic expansion assumption introduced\nin (Anandkumar et al., 2013), as well as the anchor words assumption in (Arora\net al., 2012c). The assumptions on the topic priors are related to the well\nknown Dirichlet prior, introduced to the area of topic modeling by (Blei et\nal., 2003).\n  It is well known that initialization plays a crucial role in how well\nvariational based algorithms perform in practice. The initializations that we\nuse are fairly natural. One of them is similar to what is currently used in\nLDA-c, the most popular implementation of variational inference for topic\nmodels. The other one is an overlapping clustering algorithm, inspired by a\nwork by (Arora et al., 2014) on dictionary learning, which is very simple and\nefficient.\n  While our primary goal is to provide insights into when variational inference\nmight work in practice, the multiplicative, rather than the additive nature of\nthe variational inference updates forces us to use fairly non-standard proof\narguments, which we believe will be of general interest.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 09:20:39 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2015 11:24:43 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Risteski", "Andrej", ""]]}, {"id": "1503.06610", "submitter": "Yann Strozecki", "authors": "Dominique Barth, Olivier David, Franck Quessette, Vincent Reinhard,\n  Yann Strozecki, Sandrine Vial", "title": "Efficient Generation of Stable Planar Cages for Chemistry", "comments": "17 pages, 7 figures. Accepted at the 14th International Symposium on\n  Experimental Algorithms (SEA 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe an algorithm which generates all colored planar\nmaps with a good minimum sparsity from simple motifs and rules to connect them.\nAn implementation of this algorithm is available and is used by chemists who\nwant to quickly generate all sound molecules they can obtain by mixing some\nbasic components.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 11:54:55 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Barth", "Dominique", ""], ["David", "Olivier", ""], ["Quessette", "Franck", ""], ["Reinhard", "Vincent", ""], ["Strozecki", "Yann", ""], ["Vial", "Sandrine", ""]]}, {"id": "1503.06632", "submitter": "Elena Dubrova", "authors": "Maxim Teslenko and Elena Dubrova", "title": "A Fast Heuristic Algorithm for Redundancy Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Redundancy identification is an important step of the design flow that\ntypically follows logic synthesis and optimization. In addition to reducing\ncircuit area, power consumption, and delay, redundancy removal also improves\ntestability. All commercially available synthesis tools include a redundancy\nremoval engine which is often run multiple times on the same netlist during\noptimization. This paper presents a fast heuristic algorithm for redundancy\nremoval in combinational circuits. Our idea is to provide a quick partial\nsolution which can be used for the intermediate redundancy removal runs instead\nof exact ATPG or SAT-based approaches. The presented approach has a higher\nimplication power than the traditional heuristic algorithms, such as FIRE, e.g.\non average it removes 37% more redundancies than FIRE with no penalty in\nruntime.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 13:25:18 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Teslenko", "Maxim", ""], ["Dubrova", "Elena", ""]]}, {"id": "1503.06725", "submitter": "Charo del Genio", "authors": "Kevin E. Bassler, Charo I. Del Genio, P\\'eter L. Erd\\H{o}s, Istv\\'an\n  Mikl\\'os and Zolt\\'an Toroczkai", "title": "Exact sampling of graphs with prescribed degree correlations", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": "10.1088/1367-2630/17/8/083052", "report-no": null, "categories": "cs.DM cond-mat.stat-mech cs.DS math.CO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world networks exhibit correlations between the node degrees. For\ninstance, in social networks nodes tend to connect to nodes of similar degree.\nConversely, in biological and technological networks, high-degree nodes tend to\nbe linked with low-degree nodes. Degree correlations also affect the dynamics\nof processes supported by a network structure, such as the spread of opinions\nor epidemics. The proper modelling of these systems, i.e., without uncontrolled\nbiases, requires the sampling of networks with a specified set of constraints.\nWe present a solution to the sampling problem when the constraints imposed are\nthe degree correlations. In particular, we develop an efficient and exact\nmethod to construct and sample graphs with a specified joint-degree matrix,\nwhich is a matrix providing the number of edges between all the sets of nodes\nof a given degree, for all degrees, thus completely specifying all pairwise\ndegree correlations, and additionally, the degree sequence itself. Our\nalgorithm always produces independent samples without backtracking. The\ncomplexity of the graph construction algorithm is O(NM) where N is the number\nof nodes and M is the number of edges.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 16:58:16 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2015 18:16:43 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Bassler", "Kevin E.", ""], ["Del Genio", "Charo I.", ""], ["Erd\u0151s", "P\u00e9ter L.", ""], ["Mikl\u00f3s", "Istv\u00e1n", ""], ["Toroczkai", "Zolt\u00e1n", ""]]}, {"id": "1503.06822", "submitter": "Ioannis Papoutsakis", "authors": "Ioannis Papoutsakis", "title": "Tree spanners of bounded degree graphs", "comments": null, "journal-ref": "Discrete Applied Mathematics, Volume 236, 2018, Pages 395-407", "doi": "10.1016/j.dam.2017.10.025", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tree $t$-spanner of a graph $G$ is a spanning tree of $G$ such that the\ndistance between pairs of vertices in the tree is at most $t$ times their\ndistance in $G$. Deciding tree $t$-spanner admissible graphs has been proved to\nbe tractable for $t<3$ and NP-complete for $t>3$, while the complexity status\nof this problem is unresolved when $t=3$. For every $t>2$ and $b>0$, an\nefficient dynamic programming algorithm to decide tree $t$-spanner\nadmissibility of graphs with vertex degrees less than $b$ is presented. Only\nfor $t=3$, the algorithm remains efficient, when graphs $G$ with degrees less\nthan $b\\log |V(G)|$ are examined.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 20:30:00 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2015 18:01:09 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2015 17:42:08 GMT"}, {"version": "v4", "created": "Sun, 17 Apr 2016 08:58:27 GMT"}, {"version": "v5", "created": "Mon, 9 May 2016 16:33:16 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Papoutsakis", "Ioannis", ""]]}, {"id": "1503.06876", "submitter": "Ping Li", "authors": "Ping Li", "title": "Binary and Multi-Bit Coding for Stable Random Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop efficient binary (i.e., 1-bit) and multi-bit coding schemes for\nestimating the scale parameter of $\\alpha$-stable distributions. The work is\nmotivated by the recent work on one scan 1-bit compressed sensing (sparse\nsignal recovery) using $\\alpha$-stable random projections, which requires\nestimating of the scale parameter at bits-level. Our technique can be naturally\napplied to data stream computations for estimating the $\\alpha$-th frequency\nmoment. In fact, the method applies to the general scale family of\ndistributions, not limited to $\\alpha$-stable distributions.\n  Due to the heavy-tailed nature of $\\alpha$-stable distributions, using\ntraditional estimators will potentially need many bits to store each\nmeasurement in order to ensure sufficient accuracy. Interestingly, our paper\ndemonstrates that, using a simple closed-form estimator with merely 1-bit\ninformation does not result in a significant loss of accuracy if the parameter\nis chosen appropriately. For example, when $\\alpha=0+$, 1, and 2, the\ncoefficients of the optimal estimation variances using full (i.e.,\ninfinite-bit) information are 1, 2, and 2, respectively. With the 1-bit scheme\nand appropriately chosen parameters, the corresponding variance coefficients\nare 1.544, $\\pi^2/4$, and 3.066, respectively. Theoretical tail bounds are also\nprovided. Using 2 or more bits per measurements reduces the estimation variance\nand importantly, stabilizes the estimate so that the variance is not sensitive\nto parameters. With look-up tables, the computational cost is minimal.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 00:05:17 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2016 15:23:36 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1503.07093", "submitter": "Roland Marko", "authors": "Marek Karpinski and Roland Mark\\'o", "title": "On the Complexity of Nondeterministically Testable Hypergraph Parameters", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proves the equivalence of the notions of nondeterministic and\ndeterministic parameter testing for uniform dense hypergraphs of arbitrary\norder. It generalizes the result previously known only for the case of simple\ngraphs. By a similar method we establish also the equivalence between\nnondeterministic and deterministic hypergraph property testing, answering the\nopen problem in the area. We introduce a new notion of a cut norm for\nhypergraphs of higher order, and employ regularity techniques combined with the\nultralimit method.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 16:06:23 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Karpinski", "Marek", ""], ["Mark\u00f3", "Roland", ""]]}, {"id": "1503.07170", "submitter": "Jason Eastman", "authors": "Sotiria Lampoudi, Eric Saunders, Jason Eastman", "title": "An Integer Linear Programming Solution to the Telescope Network\n  Scheduling Problem", "comments": "Accepted for publication in the refereed conference proceedings of\n  the International Conference on Operations Research and Enterprise Systems\n  (ICORES 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Telescope networks are gaining traction due to their promise of higher\nresource utilization than single telescopes and as enablers of novel\nastronomical observation modes. However, as telescope network sizes increase,\nthe possibility of scheduling them completely or even semi-manually disappears.\nIn an earlier paper, a step towards software telescope scheduling was made with\nthe specification of the Reservation formalism, through the use of which\nastronomers can express their complex observation needs and preferences. In\nthis paper we build on that work. We present a solution to the discretized\nversion of the problem of scheduling a telescope network. We derive a solvable\ninteger linear programming (ILP) model based on the Reservation formalism. We\nshow computational results verifying its correctness, and confirm that our\nGurobi-based implementation can address problems of realistic size. Finally, we\nextend the ILP model to also handle the novel observation requests that can be\nspecified using the more advanced Compound Reservation formalism.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 20:00:24 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Lampoudi", "Sotiria", ""], ["Saunders", "Eric", ""], ["Eastman", "Jason", ""]]}, {"id": "1503.07192", "submitter": "Hristo Djidjev", "authors": "Guillaume Chapuis and Hristo Djidjev", "title": "Shortest-Path Queries in Planar Graphs on GPU-Accelerated Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an efficient parallel algorithm for answering shortest-path\nqueries in planar graphs and implement it on a multi-node CPU/GPU clusters. The\nalgorithm uses a divide-and-conquer approach for decomposing the input graph\ninto small and roughly equal subgraphs and constructs a distributed data\nstructure containing shortest distances within each of those subgraphs and\nbetween their boundary vertices. For a planar graph with $n$ vertices, that\ndata structure needs $O(n)$ storage per processor and allows queries to be\nanswered in $O(n^{1/4})$ time.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 20:39:43 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Chapuis", "Guillaume", ""], ["Djidjev", "Hristo", ""]]}, {"id": "1503.07444", "submitter": "Karthik C. S.", "authors": "Jean-Daniel Boissonnat, Karthik C. S., and S\\'ebastien Tavenas", "title": "Building Efficient and Compact Data Structures for Simplicial Complexes", "comments": "An extended abstract appeared in the proceedings of SoCG 2015", "journal-ref": null, "doi": "10.1007/s00453-016-0207-y", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Simplex Tree (ST) is a recently introduced data structure that can\nrepresent abstract simplicial complexes of any dimension and allows efficient\nimplementation of a large range of basic operations on simplicial complexes. In\nthis paper, we show how to optimally compress the Simplex Tree while retaining\nits functionalities. In addition, we propose two new data structures called the\nMaximal Simplex Tree (MxST) and the Simplex Array List (SAL). We analyze the\ncompressed Simplex Tree, the Maximal Simplex Tree, and the Simplex Array List\nunder various settings.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 16:26:30 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2015 20:23:57 GMT"}, {"version": "v3", "created": "Tue, 23 Aug 2016 07:38:24 GMT"}, {"version": "v4", "created": "Sat, 5 Nov 2016 12:17:57 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Boissonnat", "Jean-Daniel", ""], ["S.", "Karthik C.", ""], ["Tavenas", "S\u00e9bastien", ""]]}, {"id": "1503.07463", "submitter": "Alexander Barvinok", "authors": "Alexander Barvinok", "title": "Computing the partition function of a polynomial on the Boolean cube", "comments": "The final version of this paper is due to be published in the\n  collection of papers \"A Journey through Discrete Mathematics. A Tribute to\n  Jiri Matousek\" edited by Martin Loebl, Jaroslav Nesetril and Robin Thomas, to\n  be published by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a polynomial f: {-1, 1}^n --> C, we define the partition function as the\naverage of e^{lambda f(x)} over all points x in {-1, 1}^n, where lambda in C is\na parameter. We present a quasi-polynomial algorithm, which, given such f,\nlambda and epsilon >0 approximates the partition function within a relative\nerror of epsilon in N^{O(ln n -ln epsilon)} time provided |lambda| < 1/(2 L\nsqrt{deg f}), where L=L(f) is a parameter bounding the Lipschitz constant of f\nfrom above and N is the number of monomials in f. As a corollary, we obtain a\nquasi-polynomial algorithm, which, given such an f with coefficients +1 and -1\nand such that every variable enters not more than 4 monomials, approximates the\nmaximum of f on {-1, 1}^n within a factor of O(sqrt{deg f}/delta), provided the\nmaximum is N delta for some 0< delta <1. If every variable enters not more than\nk monomials for some fixed k > 4, we are able to establish a similar result\nwhen delta > (k-1)/k.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 17:08:37 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 23:16:38 GMT"}, {"version": "v3", "created": "Wed, 11 May 2016 18:18:29 GMT"}, {"version": "v4", "created": "Tue, 29 Nov 2016 02:22:00 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Barvinok", "Alexander", ""]]}, {"id": "1503.07563", "submitter": "Tsvi Kopelowitz", "authors": "Amihood Amir, Tsvi Kopelowitz, Avivit Levy, Seth Pettie, Ely Porat, B.\n  Riva Shalom", "title": "Mind the Gap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the complexity of the online Dictionary Matching with One Gap\nProblem (DMOG) which is the following. Preprocess a dictionary $D$ of $d$\npatterns, where each pattern contains a special gap symbol that can match any\nstring, so that given a text that arrives online, a character at a time, we can\nreport all of the patterns from $D$ that are suffixes of the text that has\narrived so far, before the next character arrives. In more general versions the\ngap symbols are associated with bounds determining the possible lengths of\nmatching strings. Finding efficient algorithmic solutions for (online) DMOG has\nproven to be a difficult algorithmic challenge. We demonstrate that the\ndifficulty in obtaining efficient solutions for the DMOG problem even, in the\noffline setting, can be traced back to the infamous 3SUM conjecture.\nInterestingly, our reduction deviates from the known reduction paths that\nfollow from 3SUM. In particular, most reductions from 3SUM go through the\nset-disjointness problem, which corresponds to the problem of preprocessing a\ngraph to answer edge-triangles queries. We use a new path of reductions by\nconsidering the complementary, although structurally very different,\nvertex-triangles queries. Using this new path we show a conditional lower bound\nof $\\Omega(\\delta(G_D)+op)$ time per text character, where $G_D$ is a bipartite\ngraph that captures the structure of $D$, $\\delta(G_D)$ is the degeneracy of\nthis graph, and $op$ is the output size. We also provide matching upper-bounds\n(up to sub-polynomial factors) for the vertex-triangles problem, and then\nextend these techniques to the online DMOG problem. In particular, we introduce\nalgorithms whose time cost depends linearly on $\\delta(G_D)$. Our algorithms\nmake use of graph orientations, together with some additional techniques.\nFinally, when $\\delta(G_D)$ is large we are able to obtain even more efficient\nsolutions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 22:02:54 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2015 03:31:44 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Amir", "Amihood", ""], ["Kopelowitz", "Tsvi", ""], ["Levy", "Avivit", ""], ["Pettie", "Seth", ""], ["Porat", "Ely", ""], ["Shalom", "B. Riva", ""]]}, {"id": "1503.07568", "submitter": "Jos\\~A Ignacio Alvarez-Hamelin Phd.", "authors": "Mariano G. Beir\\'o, Sebasti\\'an P. Grynberg, J. Ignacio\n  Alvarez-Hamelin", "title": "Router-level community structure of the Internet Autonomous Systems", "comments": null, "journal-ref": null, "doi": "10.1140/epjds/s13688-015-0048-y", "report-no": null, "categories": "cs.NI cs.DS cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet is composed of routing devices connected between them and\norganized into independent administrative entities: the Autonomous Systems. The\nexistence of different types of Autonomous Systems (like large connectivity\nproviders, Internet Service Providers or universities) together with\ngeographical and economical constraints, turns the Internet into a complex\nmodular and hierarchical network. This organization is reflected in many\nproperties of the Internet topology, like its high degree of clustering and its\nrobustness.\n  In this work, we study the modular structure of the Internet router-level\ngraph in order to assess to what extent the Autonomous Systems satisfy some of\nthe known notions of community structure. We show that the modular structure of\nthe Internet is much richer than what can be captured by the current community\ndetection methods, which are severely affected by resolution limits and by the\nheterogeneity of the Autonomous Systems. Here we overcome this issue by using a\nmultiresolution detection algorithm combined with a small sample of nodes. We\nalso discuss recent work on community structure in the light of our results.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 22:28:21 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 13:31:51 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Beir\u00f3", "Mariano G.", ""], ["Grynberg", "Sebasti\u00e1n P.", ""], ["Alvarez-Hamelin", "J. Ignacio", ""]]}, {"id": "1503.07624", "submitter": "Giri Narasimhan", "authors": "Mario E. Consuegra, Wendy A. Martinez, Giri Narasimhan, Raju\n  Rangaswami, Leo Shao, Giuseppe Vietri", "title": "Analyzing Adaptive Cache Replacement Strategies", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive Replacement Cache (ARC) and CLOCK with Adaptive Replacement (CAR)\nare state-of-the- art \"adaptive\" cache replacement algorithms invented to\nimprove on the shortcomings of classical cache replacement policies such as\nLRU, LFU and CLOCK. By separating out items that have been accessed only once\nand items that have been accessed more frequently, both ARC and CAR are able to\ncontrol the harmful effect of single-access items flooding the cache and\npushing out more frequently accessed items. Both ARC and CAR have been shown to\noutperform their classical and popular counterparts in practice. Both\nalgorithms are complex, yet popular. Even though they can be treated as online\nalgorithms with an \"adaptive\" twist, a theoretical proof of the competitiveness\nof ARC and CAR remained unsolved for over a decade. We show that the\ncompetitiveness ratio of CAR (and ARC) has a lower bound of N + 1 (where N is\nthe size of the cache) and an upper bound of 18N (4N for ARC). If the size of\ncache offered to ARC or CAR is larger than the one provided to OPT, then we\nshow improved competitiveness ratios. The important implication of the above\nresults are that no \"pathological\" worst-case request sequences exist that\ncould deteriorate the performance of ARC and CAR by more than a constant factor\nas compared to LRU.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 05:20:16 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 16:17:53 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Consuegra", "Mario E.", ""], ["Martinez", "Wendy A.", ""], ["Narasimhan", "Giri", ""], ["Rangaswami", "Raju", ""], ["Shao", "Leo", ""], ["Vietri", "Giuseppe", ""]]}, {"id": "1503.07905", "submitter": "Efrosini Sourla MSc", "authors": "Efrosini Sourla, Spyros Sioutas, Kostas Tsichlas and Christos\n  Zaroliagis", "title": "D3-Tree: A Dynamic Distributed Deterministic Load - Balancer for\n  decentralized tree structures", "comments": "32 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose D3-Tree, a dynamic distributed deterministic\nstructure for data management in decentralized networks. We present in brief\nthe theoretical algorithmic analysis, in which our proposed structure is based\non, and we describe thoroughly the key aspects of the implementation.\nConducting experiments, we verify that the implemented structure outperforms\nother well-known hierarchical tree-based structures, since it provides better\ncomplexities regarding load-balancing operations. More specifically, the\nstructure achieves a logarithmic amortized bound, using an efficient\ndeterministic load-balancing mechanism, which is general enough to be applied\nto other hierarchical tree-based structures. Moreover, we investigate the\nstructure's fault tolerance, which hasn't been sufficiently tackled in previous\nwork, both theoretically and through rigorous experimentation. We prove that\nD3-Tree is highly fault tolerant, since, even for massive node failures, it\nachieves a significant success rate in element queries. Afterwards we go one\nstep further, in order to achieve sub-logarithmic complexity and propose the\nART+ structure (Autonomous Range Tree), exploiting the excellent performance of\nD3-Tree. ART+ is a fully dynamic and fault-tolerant structure, which achieves\nsub-logarithmic performance for query and update operations and performs\nload-balancing in sub-logarithmic amortized cost.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 21:14:34 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Sourla", "Efrosini", ""], ["Sioutas", "Spyros", ""], ["Tsichlas", "Kostas", ""], ["Zaroliagis", "Christos", ""]]}, {"id": "1503.07940", "submitter": "Ananda Theertha Suresh", "authors": "Alon Orlitsky and Ananda Theertha Suresh", "title": "Competitive Distribution Estimation", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating an unknown distribution from its samples is a fundamental problem\nin statistics. The common, min-max, formulation of this goal considers the\nperformance of the best estimator over all distributions in a class. It shows\nthat with $n$ samples, distributions over $k$ symbols can be learned to a KL\ndivergence that decreases to zero with the sample size $n$, but grows\nunboundedly with the alphabet size $k$.\n  Min-max performance can be viewed as regret relative to an oracle that knows\nthe underlying distribution. We consider two natural and modest limits on the\noracle's power. One where it knows the underlying distribution only up to\nsymbol permutations, and the other where it knows the exact distribution but is\nrestricted to use natural estimators that assign the same probability to\nsymbols that appeared equally many times in the sample.\n  We show that in both cases the competitive regret reduces to\n$\\min(k/n,\\tilde{\\mathcal{O}}(1/\\sqrt n))$, a quantity upper bounded uniformly\nfor every alphabet size. This shows that distributions can be estimated nearly\nas well as when they are essentially known in advance, and nearly as well as\nwhen they are completely known in advance but need to be estimated via a\nnatural estimator. We also provide an estimator that runs in linear time and\nincurs competitive regret of $\\tilde{\\mathcal{O}}(\\min(k/n,1/\\sqrt n))$, and\nshow that for natural estimators this competitive regret is inevitable. We also\ndemonstrate the effectiveness of competitive estimators using simulations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 01:41:48 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Orlitsky", "Alon", ""], ["Suresh", "Ananda Theertha", ""]]}, {"id": "1503.08019", "submitter": "Mohamad Kazem Shirani Faradonbeh", "authors": "Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, George Michailidis", "title": "Optimality of Fast Matching Algorithms for Random Networks with\n  Applications to Structural Controllability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SY stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network control refers to a very large and diverse set of problems including\ncontrollability of linear time-invariant dynamical systems, where the objective\nis to select an appropriate input to steer the network to a desired state.\nThere are many notions of controllability, one of them being structural\ncontrollability, which is intimately connected to finding maximum matchings on\nthe underlying network topology. In this work, we study fast, scalable\nalgorithms for finding maximum matchings for a large class of random networks.\nFirst, we illustrate that degree distribution random networks are realistic\nmodels for real networks in terms of structural controllability. Subsequently,\nwe analyze a popular, fast and practical heuristic due to Karp and Sipser as\nwell as a simplification of it. For both heuristics, we establish asymptotic\noptimality and provide results concerning the asymptotic size of maximum\nmatchings for an extensive class of random networks.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 10:52:40 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 04:37:16 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Faradonbeh", "Mohamad Kazem Shirani", ""], ["Tewari", "Ambuj", ""], ["Michailidis", "George", ""]]}, {"id": "1503.08078", "submitter": "Robert Ganian", "authors": "Robert Ganian, Martin Kronegger, Andreas Pfandler, Alexandru Popa", "title": "Parameterized Complexity of Asynchronous Border Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarrays are research tools used in gene discovery as well as disease and\ncancer diagnostics. Two prominent but challenging problems related to\nmicroarrays are the Border Minimization Problem (BMP) and the Border\nMinimization Problem with given placement (P-BMP).\n  In this paper we investigate the parameterized complexity of natural variants\nof BMP and P-BMP under several natural parameters. We show that BMP and P-BMP\nare in FPT under the following two combinations of parameters: 1) the size of\nthe alphabet (c), the maximum length of a sequence (string) in the input (l)\nand the number of rows of the microarray (r); and, 2) the size of the alphabet\nand the size of the border length (o). Furthermore, P-BMP is in FPT when\nparameterized by c and l. We complement our tractability results with\ncorresponding hardness results.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 13:50:22 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Ganian", "Robert", ""], ["Kronegger", "Martin", ""], ["Pfandler", "Andreas", ""], ["Popa", "Alexandru", ""]]}, {"id": "1503.08498", "submitter": "Vasileios Iliopoulos DR", "authors": "Vasileios Iliopoulos and David B. Penman", "title": "Dual pivot Quicksort", "comments": "Post print of the article \"Dual pivot Quicksort\" published on 1\n  August of 2012 in the journal of Discrete Mathematics, Algorithms and\n  Applications", "journal-ref": "Discrete Math. Algorithm. Appl. 04, 1250041 (2012) [13 pages]", "doi": "10.1142/S1793830912500413", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyse the dual pivot Quicksort, a variant of the standard\nQuicksort algorithm, in which two pivots are used for the partitioning of the\narray. We are solving recurrences of the expected number of key comparisons and\nexchanges performed by the algorithm, obtaining the exact and asymptotic total\naverage values contributing to its time complexity. Further, we compute the\naverage number of partitioning stages and the variance of the number of key\ncomparisons. In terms of mean values, dual pivot Quicksort does not appear to\nbe faster than ordinary algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 21:37:24 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Iliopoulos", "Vasileios", ""], ["Penman", "David B.", ""]]}, {"id": "1503.08796", "submitter": "Rebecca Hoberg", "authors": "Rebecca Hoberg and Thomas Rothvoss", "title": "A Logarithmic Additive Integrality Gap for Bin Packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For bin packing, the input consists of $n$ items with sizes $s_1,...,s_n \\in\n[0,1]$ which have to be assigned to a minimum number of bins of size 1.\nRecently, the second author gave an LP-based polynomial time algorithm that\nemployed techniques from discrepancy theory to find a solution using at most\n$OPT + O(\\log OPT \\cdot \\log \\log OPT)$ bins.\n  In this paper, we present an approximation algorithm that has an additive gap\nof only $O(\\log OPT)$ bins, which matches certain combinatorial lower bounds.\nAny further improvement would have to use more algebraic structure. Our\nimprovement is based on a combination of discrepancy theory techniques and a\nnovel 2-stage packing: first we pack items into containers; then we pack\ncontainers into bins of size 1. Apart from being more effective, we believe our\nalgorithm is much cleaner than the one of Rothvoss.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 19:02:12 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Hoberg", "Rebecca", ""], ["Rothvoss", "Thomas", ""]]}, {"id": "1503.09168", "submitter": "Adrian Kosowski", "authors": "Jurek Czyzowicz (DII), Leszek Gasieniec, Adrian Kosowski (LIAFA, INRIA\n  Paris-Rocquencourt), Evangelos Kranakis, Paul G. Spirakis (RA-CTI),\n  Przemyslaw Uznanski", "title": "On Convergence and Threshold Properties of Discrete Lotka-Volterra\n  Population Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we focus on a natural class of population protocols whose\ndynamics are modelled by the discrete version of Lotka-Volterra equations. In\nsuch protocols, when an agent $a$ of type (species) $i$ interacts with an agent\n$b$ of type (species) $j$ with $a$ as the initiator, then $b$'s type becomes\n$i$ with probability $P\\_{ij}$. In such an interaction, we think of $a$ as the\npredator, $b$ as the prey, and the type of the prey is either converted to that\nof the predator or stays as is. Such protocols capture the dynamics of some\nopinion spreading models and generalize the well-known Rock-Paper-Scissors\ndiscrete dynamics. We consider the pairwise interactions among agents that are\nscheduled uniformly at random. We start by considering the convergence time and\nshow that any Lotka-Volterra-type protocol on an $n$-agent population converges\nto some absorbing state in time polynomial in $n$, w.h.p., when any pair of\nagents is allowed to interact. By contrast, when the interaction graph is a\nstar, even the Rock-Paper-Scissors protocol requires exponential time to\nconverge. We then study threshold effects exhibited by Lotka-Volterra-type\nprotocols with 3 and more species under interactions between any pair of\nagents. We start by presenting a simple 4-type protocol in which the\nprobability difference of reaching the two possible absorbing states is\nstrongly amplified by the ratio of the initial populations of the two other\ntypes, which are transient, but \"control\" convergence. We then prove that the\nRock-Paper-Scissors protocol reaches each of its three possible absorbing\nstates with almost equal probability, starting from any configuration\nsatisfying some sub-linear lower bound on the initial size of each species.\nThat is, Rock-Paper-Scissors is a realization of a \"coin-flip consensus\" in a\ndistributed system. Some of our techniques may be of independent value.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 19:19:11 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Czyzowicz", "Jurek", "", "DII"], ["Gasieniec", "Leszek", "", "LIAFA, INRIA\n  Paris-Rocquencourt"], ["Kosowski", "Adrian", "", "LIAFA, INRIA\n  Paris-Rocquencourt"], ["Kranakis", "Evangelos", "", "RA-CTI"], ["Spirakis", "Paul G.", "", "RA-CTI"], ["Uznanski", "Przemyslaw", ""]]}]