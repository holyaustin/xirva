[{"id": "1810.00029", "submitter": "Eduardo Laber", "authors": "Eduardo Sany Laber and Lucas Murtinho", "title": "Minimization of Gini impurity via connections with the k-means problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gini impurity is one of the measures used to select attribute in Decision\nTrees/Random Forest construction. In this note we discuss connections between\nthe problem of computing the partition with minimum Weighted Gini impurity and\nthe $k$-means clustering problem. Based on these connections we show that the\ncomputation of the partition with minimum Weighted Gini is a NP-Complete\nproblem and we also discuss how to obtain new algorithms with provable\napproximation for the Gini Minimization problem.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 18:26:43 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Laber", "Eduardo Sany", ""], ["Murtinho", "Lucas", ""]]}, {"id": "1810.00169", "submitter": "Mohammad Noormohammadpour", "authors": "Mohammad Noormohammadpour, Ajitesh Srivastava, Cauligi S. Raghavendra", "title": "On Minimizing the Completion Times of Long Flows over Inter-Datacenter\n  WAN", "comments": "Accepted for publication in IEEE Communications Letters", "journal-ref": "IEEE Communications Letters 22 (2018) 2475 - 2478", "doi": "10.1109/LCOMM.2018.2872980", "report-no": null, "categories": "cs.NI cs.DS cs.PF cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long flows contribute huge volumes of traffic over inter-datacenter WAN. The\nFlow Completion Time (FCT) is a vital network performance metric that affects\nthe running time of distributed applications and the users' quality of\nexperience. Flow routing techniques based on propagation or queuing latency or\ninstantaneous link utilization are insufficient for minimization of the long\nflows' FCT. We propose a routing approach that uses the remaining sizes and\npaths of all ongoing flows to minimize the worst-case completion time of\nincoming flows assuming no knowledge of future flow arrivals. Our approach can\nbe formulated as an NP-Hard graph optimization problem. We propose BWRH, a\nheuristic to quickly generate an approximate solution. We evaluate BWRH against\nseveral real WAN topologies and two different traffic patterns. We see that\nBWRH provides solutions with an average optimality gap of less than $0.25\\%$.\nFurthermore, we show that compared to other popular routing heuristics, BWRH\nreduces the mean and tail FCT by up to $1.46\\times$ and $1.53\\times$,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 07:49:10 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Noormohammadpour", "Mohammad", ""], ["Srivastava", "Ajitesh", ""], ["Raghavendra", "Cauligi S.", ""]]}, {"id": "1810.00270", "submitter": "Bart{\\l}omiej Bosek", "authors": "Bart{\\l}omiej Bosek, Tomasz Krawczyk", "title": "On-line partitioning of width w posets into w^O(log log w) chains", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An on-line chain partitioning algorithm receives the elements of a poset one\nat a time, and when an element is received, irrevocably assigns it to one of\nthe chains. In this paper, we present an on-line algorithm that partitions\nposets of width $w$ into $w^{O(\\log{\\log{w}})}$ chains. This improves over\npreviously best known algorithms using $w^{O(\\log{w})}$ chains by Bosek and\nKrawczyk and by Bosek, Kierstead, Krawczyk, Matecki, and Smith. Our algorithm\nruns in $w^{O(\\sqrt{w})}n$ time, where $w$ is the width and $n$ is the size of\na presented poset.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 21:55:24 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 21:16:07 GMT"}, {"version": "v3", "created": "Sun, 29 Mar 2020 12:51:23 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Bosek", "Bart\u0142omiej", ""], ["Krawczyk", "Tomasz", ""]]}, {"id": "1810.00447", "submitter": "Vahideh Manshadi", "authors": "Dawsen Hwang, Patrick Jaillet, and Vahideh Manshadi", "title": "Online Resource Allocation under Partially Predictable Demand", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For online resource allocation problems, we propose a new demand arrival\nmodel where the sequence of arrivals contains both an adversarial component and\na stochastic one. Our model requires no demand forecasting; however, due to the\npresence of the stochastic component, we can partially predict future demand as\nthe sequence of arrivals unfolds. Under the proposed model, we study the\nproblem of the online allocation of a single resource to two types of\ncustomers, and design online algorithms that outperform existing ones. Our\nalgorithms are adjustable to the relative size of the stochastic component, and\nour analysis reveals that as the portion of the stochastic component grows, the\nloss due to making online decisions decreases. This highlights the value of\n(even partial) predictability in online resource allocation. We impose no\nconditions on how the resource capacity scales with the maximum number of\ncustomers. However, we show that using an adaptive algorithm---which makes\nonline decisions based on observed data---is particularly beneficial when\ncapacity scales linearly with the number of customers. Our work serves as a\nfirst step in bridging the long-standing gap between the two well-studied\napproaches to the design and analysis of online algorithms based on (1)\nadversarial models and (2) stochastic ones. Using novel algorithm design, we\ndemonstrate that even if the arrival sequence contains an adversarial\ncomponent, we can take advantage of the limited information that the data\nreveals to improve allocation decisions. We also study the classical secretary\nproblem under our proposed arrival model, and we show that randomizing over\nmultiple stopping rules may increase the probability of success.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 18:46:15 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Hwang", "Dawsen", ""], ["Jaillet", "Patrick", ""], ["Manshadi", "Vahideh", ""]]}, {"id": "1810.00580", "submitter": "Marcin Bienkowski", "authors": "Marcin Bienkowski and {\\L}ukasz Je\\.z and Pawe{\\l} Schmidt", "title": "Slaying Hydrae: Improved Bounds for Generalized k-Server in Uniform\n  Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized $k$-server problem is an extension of the weighted $k$-server\nproblem, which in turn extends the classic $k$-server problem. In the\ngeneralized $k$-server problem, each of $k$ servers $s_1, \\dots, s_k$ remains\nin its own metric space $M_i$. A request is a tuple $(r_1,\\dots,r_k)$, where\n$r_i \\in M_i$, and to service it, an algorithm needs to move at least one\nserver $s_i$ to the point $r_i$. The objective is to minimize the total\ndistance traveled by all servers.\n  In this paper, we focus on the generalized $k$-server problem for the case\nwhere all $M_i$ are uniform metrics. We show an $O(k^2 \\cdot \\log\nk)$-competitive randomized algorithm improving over a recent result by Bansal\net al. [SODA 2018], who gave an $O(k^3 \\cdot \\log k)$-competitive algorithm. To\nthis end, we define an abstract online problem, called Hydra game, and we show\nthat a randomized solution of low cost to this game implies a randomized\nalgorithm to the generalized $k$-server problem with low competitive ratio.\n  We also show that no randomized algorithm can achieve competitive ratio lower\nthan $\\Omega(k)$, thus improving the lower bound of $\\Omega(k / \\log^2 k)$ by\nBansal et al.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 08:42:17 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 16:30:51 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Bienkowski", "Marcin", ""], ["Je\u017c", "\u0141ukasz", ""], ["Schmidt", "Pawe\u0142", ""]]}, {"id": "1810.00621", "submitter": "Karl Bringmann", "authors": "Karl Bringmann, Bhaskar Ray Chaudhury", "title": "Polyline Simplification has Cubic Complexity", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classic polyline simplification problem we want to replace a given\npolygonal curve $P$, consisting of $n$ vertices, by a subsequence $P'$ of $k$\nvertices from $P$ such that the polygonal curves $P$ and $P'$ are as close as\npossible. Closeness is usually measured using the Hausdorff or Fr\\'echet\ndistance. These distance measures can be applied \"globally\", i.e., to the whole\ncurves $P$ and $P'$, or \"locally\", i.e., to each simplified subcurve and the\nline segment that it was replaced with separately (and then taking the\nmaximum). This gives rise to four problem variants: Global-Hausdorff (known to\nbe NP-hard), Local-Hausdorff (in time $O(n^3)$), Global-Fr\\'echet (in time $O(k\nn^5)$), and Local-Fr\\'echet (in time $O(n^3)$).\n  Our contribution is as follows.\n  - Cubic time for all variants: For Global-Fr\\'echet we design an algorithm\nrunning in time $O(n^3)$. This shows that all three problems (Local-Hausdorff,\nLocal-Fr\\'echet, and Global-Fr\\'echet) can be solved in cubic time. All these\nalgorithms work over a general metric space such as $(\\mathbb{R}^d,L_p)$, but\nthe hidden constant depends on $p$ and (linearly) on $d$.\n  - Cubic conditional lower bound: We provide evidence that in high dimensions\ncubic time is essentially optimal for all three problems (Local-Hausdorff,\nLocal-Fr\\'echet, and Global-Fr\\'echet). Specifically, improving the cubic time\nto $O(n^{3-\\epsilon} \\textrm{poly}(d))$ for polyline simplification over\n$(\\mathbb{R}^d,L_p)$ for $p = 1$ would violate plausible conjectures. We obtain\nsimilar results for all $p \\in [1,\\infty), p \\ne 2$.\n  In total, in high dimensions and over general $L_p$-norms we resolve the\ncomplexity of polyline simplification with respect to Local-Hausdorff,\nLocal-Fr\\'echet, and Global-Fr\\'echet, by providing new algorithms and\nconditional lower bounds.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 11:15:37 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Bringmann", "Karl", ""], ["Chaudhury", "Bhaskar Ray", ""]]}, {"id": "1810.00644", "submitter": "Xiushan Nie", "authors": "Xingbo Liu and Xiushan Nie and Yilong Yin", "title": "Fusion Hashing: A General Framework for Self-improvement of Hashing", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has been widely used for efficient similarity search based on its\nquery and storage efficiency. To obtain better precision, most studies focus on\ndesigning different objective functions with different constraints or penalty\nterms that consider neighborhood information. In this paper, in contrast to\nexisting hashing methods, we propose a novel generalized framework called\nfusion hashing (FH) to improve the precision of existing hashing methods\nwithout adding new constraints or penalty terms. In the proposed FH, given an\nexisting hashing method, we first execute it several times to get several\ndifferent hash codes for a set of training samples. We then propose two novel\nfusion strategies that combine these different hash codes into one set of final\nhash codes. Based on the final hash codes, we learn a simple linear hash\nfunction for the samples that can significantly improve model precision. In\ngeneral, the proposed FH can be adopted in existing hashing method and achieve\nmore precise and stable performance compared to the original hashing method\nwith little extra expenditure in terms of time and space. Extensive experiments\nwere performed based on three benchmark datasets and the results demonstrate\nthe superior performance of the proposed framework\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 12:12:35 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Liu", "Xingbo", ""], ["Nie", "Xiushan", ""], ["Yin", "Yilong", ""]]}, {"id": "1810.00789", "submitter": "Oscar Defrain", "authors": "Marthe Bonamy, Oscar Defrain, Marc Heinrich, Jean-Florent Raymond, and\n  Micha{\\l} Pilipczuk", "title": "Enumerating minimal dominating sets in $K_t$-free graphs and variants", "comments": "26 pages. A preliminary version of this article appeared in the\n  proceedings of the 36th Symposium on Theoretical Aspects of Computer Science\n  (STACS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a long-standing open problem whether the minimal dominating sets of a\ngraph can be enumerated in output-polynomial time. In this paper we investigate\nthis problem in graph classes defined by forbidding an induced subgraph. In\nparticular, we provide output-polynomial time algorithms for $K_t$-free graphs\nand variants. This answers a question of Kant\\'e et al. about enumeration in\nbipartite graphs.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 16:21:29 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 18:31:33 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 12:24:02 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Bonamy", "Marthe", ""], ["Defrain", "Oscar", ""], ["Heinrich", "Marc", ""], ["Raymond", "Jean-Florent", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1810.00980", "submitter": "Paul Liu", "authors": "Paul Liu, Austin Benson, Moses Charikar", "title": "A sampling framework for counting temporal motifs", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pattern counting in graphs is fundamental to network science tasks, and there\nare many scalable methods for approximating counts of small patterns, often\ncalled motifs, in large graphs. However, modern graph datasets now contain\nricher structure, and incorporating temporal information in particular has\nbecome a critical part of network analysis. Temporal motifs, which are\ngeneralizations of small subgraph patterns that incorporate temporal ordering\non edges, are an emerging part of the network analysis toolbox. However, there\nare no algorithms for fast estimation of temporal motifs counts; moreover, we\nshow that even counting simple temporal star motifs is NP-complete. Thus, there\nis a need for fast and approximate algorithms. Here, we present the first\nfrequency estimation algorithms for counting temporal motifs. More\nspecifically, we develop a sampling framework that sits as a layer on top of\nexisting exact counting algorithms and enables fast and accurate\nmemory-efficient estimates of temporal motif counts. Our results show that we\ncan achieve one to two orders of magnitude speedups with minimal and\ncontrollable loss in accuracy on a number of datasets.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 21:05:04 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Liu", "Paul", ""], ["Benson", "Austin", ""], ["Charikar", "Moses", ""]]}, {"id": "1810.01073", "submitter": "N.S Narayanaswamy", "authors": "Manas Jyoti Kashyop and N.S. Narayanaswamy", "title": "Fully dynamic $3/2$ approximate maximum cardinality matching in\n  $O(\\sqrt{n})$ update time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized algorithm to maintain a maximal matching without 3\nlength augmenting paths in the fully dynamic setting. Consequently, we maintain\na $3/2$ approximate maximum cardinality matching. Our algorithm takes expected\namortized $O(\\sqrt{n})$ time where $n$ is the number of vertices in the graph\nwhen the update sequence is generated by an oblivious adversary. Over any\nsequence of $t$ edge insertions and deletions presented by an oblivious\nadversary, the total update time of our algorithm is $O(t\\sqrt{n})$ in\nexpectation and $O(t\\sqrt{n} + n \\log n)$ with high probability. To the best of\nour knowledge, our algorithm is the first one to maintain an approximate\nmatching in which all augmenting paths are of length at least $5$ in\n$o(\\sqrt{m})$ update time.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 05:26:00 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 17:17:32 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Kashyop", "Manas Jyoti", ""], ["Narayanaswamy", "N. S.", ""]]}, {"id": "1810.01136", "submitter": "Marcin Pilipczuk", "authors": "Bart M.P. Jansen and Marcin Pilipczuk and Erik Jan van Leeuwen", "title": "A deterministic polynomial kernel for Odd Cycle Transversal and Vertex\n  Multiway Cut in planar graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that Odd Cycle Transversal and Vertex Multiway Cut admit\ndeterministic polynomial kernels when restricted to planar graphs and\nparameterized by the solution size. This answers a question of Saurabh. On the\nway to these results, we provide an efficient sparsification routine in the\nflavor of the sparsification routine used for the Steiner Tree problem in\nplanar graphs (FOCS 2014). It differs from the previous work because it\npreserves the existence of low-cost subgraphs that are not necessarily Steiner\ntrees in the original plane graph, but structures that turn into (supergraphs\nof) Steiner trees after adding all edges between pairs of vertices that lie on\na common face. We also show connections between Vertex Multiway Cut and the\nVertex Planarization problem, where the existence of a polynomial kernel\nremains an important open problem.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 09:35:49 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 08:12:52 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["Pilipczuk", "Marcin", ""], ["van Leeuwen", "Erik Jan", ""]]}, {"id": "1810.01223", "submitter": "Max A. Deppert", "authors": "Max A. Deppert, Klaus Jansen", "title": "Near-Linear Approximation Algorithms for Scheduling Problems with Batch\n  Setup Times", "comments": null, "journal-ref": null, "doi": "10.1145/3323165.3323200", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the scheduling of $n$ jobs divided into $c$ classes on $m$\nidentical parallel machines. For every class there is a setup time which is\nrequired whenever a machine switches from the processing of one class to\nanother class. The objective is to find a schedule that minimizes the makespan.\nWe give near-linear approximation algorithms for the following problem\nvariants: the non-preemptive context where jobs may not be preempted, the\npreemptive context where jobs may be preempted but not parallelized, as well as\nthe splittable context where jobs may be preempted and parallelized.\n  We present the first algorithm improving the previously best approximation\nratio of $2$ to a better ratio of $3/2$ in the preemptive case. In more detail,\nfor all three flavors we present an approximation ratio $2$ with running time\n$\\mathcal{O}(n)$, ratio $3/2+\\varepsilon$ in time $\\mathcal{O}(n\\log\n1/\\varepsilon)$ as well as a ratio of $3/2$. The $(3/2)$-approximate algorithms\nhave different running times. In the non-preemptive case we get time\n$\\mathcal{O}(n\\log (n+\\Delta))$ where $\\Delta$ is the largest value of the\ninput. The splittable approximation runs in time $\\mathcal{O}(n+c\\log(c+m))$\nwhereas the preemptive algorithm has a running time $\\mathcal{O}(n \\log (c+m))\n\\leq \\mathcal{O}(n \\log n)$. So far, no PTAS is known for the preemptive\nproblem without restrictions, so we make progress towards that question.\nRecently Jansen et al. found an EPTAS for the splittable and non-preemptive\ncase but with impractical running times exponential in $1/\\varepsilon$.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 13:14:46 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 17:01:08 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Deppert", "Max A.", ""], ["Jansen", "Klaus", ""]]}, {"id": "1810.01238", "submitter": "Bhaskar Ray Chaudhury Mr.", "authors": "Karl Bringmann, Bhaskar Ray Chaudhury", "title": "Sketching, Streaming, and Fine-Grained Complexity of (Weighted) LCS", "comments": "To appear in FSTTCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study sketching and streaming algorithms for the Longest Common\nSubsequence problem (LCS) on strings of small alphabet size $|\\Sigma|$. For the\nproblem of deciding whether the LCS of strings $x,y$ has length at least $L$,\nwe obtain a sketch size and streaming space usage of $\\mathcal{O}(L^{|\\Sigma| -\n1} \\log L)$.\n  We also prove matching unconditional lower bounds.\n  As an application, we study a variant of LCS where each alphabet symbol is\nequipped with a weight that is given as input, and the task is to compute a\ncommon subsequence of maximum total weight. Using our sketching algorithm, we\nobtain an $\\mathcal{O}(\\textrm{min}\\{nm, n + m^{{\\lvert \\Sigma\n\\rvert}}\\})$-time algorithm for this problem, on strings $x,y$ of length $n,m$,\nwith $n \\ge m$. We prove optimality of this running time up to lower order\nfactors, assuming the Strong Exponential Time Hypothesis.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 13:42:21 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Bringmann", "Karl", ""], ["Chaudhury", "Bhaskar Ray", ""]]}, {"id": "1810.01542", "submitter": "Daniel Paulusma", "authors": "Walter Kern and Daniel Paulusma", "title": "Contracting to a Longest Path in H-Free Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove two dichotomy results for detecting long paths as patterns in a\ngiven graph. The NP-hard problem Longest Induced Path is to determine the\nlongest induced path in a graph. The NP-hard problem Longest Path\nContractibility is to determine the longest path to which a graph can be\ncontracted to. By combining known results with new results we completely\nclassify the computational complexity of both problems for $H$-free graphs. Our\nmain focus is on the second problem, for which we design a general\ncontractibility technique that enables us to reduce the problem to a matching\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 23:45:12 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Kern", "Walter", ""], ["Paulusma", "Daniel", ""]]}, {"id": "1810.01676", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Jan Studen\\'y, Przemys{\\l}aw Uzna\\'nski", "title": "Approximating Approximate Pattern Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a text $T$ of length $n$ and a pattern $P$ of length $m$, the\napproximate pattern matching problem asks for computation of a particular\n\\emph{distance} function between $P$ and every $m$-substring of $T$. We\nconsider a $(1\\pm\\varepsilon)$ multiplicative approximation variant of this\nproblem, for $\\ell_p$ distance function. In this paper, we describe two\n$(1+\\varepsilon)$-approximate algorithms with a runtime of\n$\\widetilde{O}(\\frac{n}{\\varepsilon})$ for all (constant) non-negative values\nof $p$. For constant $p \\ge 1$ we show a deterministic\n$(1+\\varepsilon)$-approximation algorithm. Previously, such run time was known\nonly for the case of $\\ell_1$ distance, by Gawrychowski and Uzna\\'nski [ICALP\n2018] and only with a randomized algorithm. For constant $0 \\le p \\le 1$ we\nshow a randomized algorithm for the $\\ell_p$, thereby providing a smooth\ntradeoff between algorithms of Kopelowitz and Porat [FOCS~2015, SOSA~2018] for\nHamming distance (case of $p=0$) and of Gawrychowski and Uzna\\'nski for\n$\\ell_1$ distance.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 10:38:59 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 14:21:30 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2019 07:48:46 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Studen\u00fd", "Jan", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1810.01699", "submitter": "Han Peters", "authors": "Han Peters, Guus Regts", "title": "Location of zeros for the partition function of the Ising model on\n  bounded degree graphs", "comments": "24 pages, 3 figures. Made a number of small clarifications,\n  corrections and changes in notation. Results remain unchanged. To appear in\n  the Journal of the London Mathematical Society", "journal-ref": null, "doi": "10.1112/jlms.12286", "report-no": null, "categories": "math.CO cs.DS math-ph math.CV math.DS math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seminal Lee-Yang theorem states that for any graph the zeros of the\npartition function of the ferromagnetic Ising model lie on the unit circle in\n$\\mathbb C$. In fact the union of the zeros of all graphs is dense on the unit\ncircle. In this paper we study the location of the zeros for the class of\ngraphs of bounded maximum degree $d\\geq 3$, both in the ferromagnetic and the\nanti-ferromagnetic case. We determine the location exactly as a function of the\ninverse temperature and the degree $d$. An important step in our approach is to\ntranslate to the setting of complex dynamics and analyze a dynamical system\nthat is naturally associated to the partition function.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 11:55:46 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 08:06:19 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 11:19:10 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Peters", "Han", ""], ["Regts", "Guus", ""]]}, {"id": "1810.01726", "submitter": "Ayush Tulsyan", "authors": "Surender Baswana, Shiv Kumar Gupta, Ayush Tulsyan", "title": "Fault Tolerant and Fully Dynamic DFS in Undirected Graphs: Simple Yet\n  Efficient", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an algorithm for a fault tolerant Depth First Search (DFS) Tree in\nan undirected graph. This algorithm is drastically simpler than the current\nstate-of-the-art algorithms for this problem, uses optimal space and optimal\npreprocessing time, and still achieves better time complexity. This algorithm\nalso leads to a better time complexity for maintaining a DFS tree in a fully\ndynamic environment.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 13:21:14 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 13:35:14 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Baswana", "Surender", ""], ["Gupta", "Shiv Kumar", ""], ["Tulsyan", "Ayush", ""]]}, {"id": "1810.01730", "submitter": "Ben Chugg", "authors": "Ben Chugg, Takanori Maehara", "title": "Submodular Stochastic Probing with Prices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Stochastic Probing with Prices (SPP), a variant of the\nStochastic Probing (SP) model in which we must pay a price to probe an element.\nA SPP problem involves two set systems $(N,\\mathcal{I}_{in})$ and\n$(N,\\mathcal{I}_{out})$ where each $e\\in N$ is active with probability $p_e$.\nTo discover whether $e$ is active, it must be probed by paying the price\n$\\Delta_e$. If it is probed and active, then it is irrevocably added to the\nsolution. Moreover, at all times, the set of probed elements must lie in\n$\\mathcal{I}_{out}$, and the solution (the set of probed and active elements)\nmust lie in $\\mathcal{I}_{in}$. The goal is to maximize a set function $f$\nminus the cost of the probes. We give a bi-criteria approximation algorithm to\nthe online version of this problem, in which the elements are shown to the\nalgorithm in a possibly adversarial order. Our results translate to\nstate-of-the-art approximations for the traditional (online) stochastic probing\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 13:31:07 GMT"}, {"version": "v2", "created": "Sun, 14 Oct 2018 18:05:37 GMT"}, {"version": "v3", "created": "Tue, 8 Jan 2019 09:14:25 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Chugg", "Ben", ""], ["Maehara", "Takanori", ""]]}, {"id": "1810.01776", "submitter": "Renjie Chen", "authors": "Renjie Chen and Craig Gotsman", "title": "Efficient Fastest-Path Computations in Road Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the age of real-time online traffic information and GPS-enabled devices,\nfastest-path computations between two points in a road network modeled as a\ndirected graph, where each directed edge is weighted by a \"travel time\" value,\nare becoming a standard feature of many navigation-related applications. To\nsupport this, very efficient computation of these paths in very large road\nnetworks is critical. Fastest paths may be computed as minimal-cost paths in a\nweighted directed graph, but traditional minimal-cost path algorithms based on\nvariants of the classic Dijkstra algorithm do not scale well, as in the worst\ncase they may traverse the entire graph. A common improvement, which can\ndramatically reduce the number of traversed graph vertices, is the A*\nalgorithm, which requires a good heuristic lower bound on the minimal cost. We\nintroduce a simple, but very effective, heuristic function based on a small\nnumber of values assigned to each graph vertex. The values are based on graph\nseparators and computed efficiently in a preprocessing stage. We present\nexperimental results demonstrating that our heuristic provides estimates of the\nminimal cost which are superior to those of other heuristics. Our experiments\nshow that when used in the A* algorithm, this heuristic can reduce the number\nof vertices traversed by an order of magnitude compared to other heuristics.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 13:47:04 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Chen", "Renjie", ""], ["Gotsman", "Craig", ""]]}, {"id": "1810.01785", "submitter": "John Iacono", "authors": "John Iacono and Stefan Langerman", "title": "Weighted dynamic finger in binary search trees", "comments": "An earlier version of this work appeared in the Proceedings of the\n  Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that the online binary search tree data structure GreedyASS\nperforms asymptotically as well on a sufficiently long sequence of searches as\nany static binary search tree where each search begins from the previous search\n(rather than the root). This bound is known to be equivalent to assigning each\nitem $i$ in the search tree a positive weight $w_i$ and bounding the search\ncost of an item in the search sequence $s_1,\\ldots,s_m$ by $$O\\left(1+ \\log\n\\frac{\\displaystyle \\sum_{\\min(s_{i-1},s_i) \\leq x \\leq\n\\max(s_{i-1},s_i)}w_x}{\\displaystyle \\min(w_{s_i},w_{s_{i-1}})} \\right)$$\namortized. This result is the strongest finger-type bound to be proven for\nbinary search trees. By setting the weights to be equal, one observes that our\nbound implies the dynamic finger bound. Compared to the previous proof of the\ndynamic finger bound for Splay trees, our result is significantly shorter,\nstronger, simpler, and has reasonable constants.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 15:07:51 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Iacono", "John", ""], ["Langerman", "Stefan", ""]]}, {"id": "1810.01834", "submitter": "Ellis Hershkowitz", "authors": "D Ellis Hershkowitz and Gregory Kehne", "title": "Reverse Greedy is Bad for k-Center", "comments": "Fixed a couple minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show the reverse greedy algorithm is between a $(2k-2)$- and a\n$2k$-approximation for $k$-center.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 16:34:34 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 00:50:43 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 19:58:50 GMT"}, {"version": "v4", "created": "Thu, 21 Feb 2019 14:50:06 GMT"}, {"version": "v5", "created": "Thu, 12 Dec 2019 15:49:33 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Hershkowitz", "D Ellis", ""], ["Kehne", "Gregory", ""]]}, {"id": "1810.01946", "submitter": "Laura Toma", "authors": "Herman Haverkort and Laura Toma", "title": "A Comparison of I/O-Efficient Algorithms for Visibility Computation on\n  Massive Grid Terrains", "comments": "38 pages, 12 figures; This is the full version cumulating two\n  conference papers that appeared in ACM SIGSPATIAL GIS 2009 and ACM SIGSPATIAL\n  GIS 2013. Does not contain new results, only contains more detailed\n  pseudocode, analysis, proofs and plots. Written in 2014/2015. Has not been\n  published and is not under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a grid terrain T and a viewpoint v, the viewshed of v is the set of\ngrid points of T that are visible from v. To decide whether a point p is\nvisible one needs to interpolate the elevation of the terrain along the\nline-of-sight vp. Existing viewshed algorithms differ widely in what points\nthey chose to interpolate and how they interpolate the terrain. These choices\ncrucially affect the running time and accuracy of the algorithms. This paper\ndescribes I/O-efficient algorithms for computing visibility maps in a couple of\ndifferent models.\n  First, we describe two algorithms that sweep the terrain by rotating a ray\naround the viewpoint while maintaining the terrain profile along the ray.\nSecond, we describe an algorithm which sweeps the terrain centrifugally,\ngrowing a star-shaped region around the viewpoint while maintaining the\napproximate visible horizon of the terrain within the swept region. Our last\ntwo algorithms are based on computing and merging horizons. All algorithms are\nI/O-efficient in the I/O-model of Aggarwal and Vitter. We present an\nexperimental analysis on large terrains obtained from NASA SRTM data. All our\nalgorithms are scalable to volumes of data that are over 50 times larger than\nmain memory. Our main finding is that, in practice, horizons are significantly\nsmaller than their theoretical worst case bound, which makes horizon-based\napproaches very fast. Our last two algorithms, which compute the most accurate\nviewshed, turn out to be very fast in practice, although their worst-case bound\nis inferior.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 20:20:51 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Haverkort", "Herman", ""], ["Toma", "Laura", ""]]}, {"id": "1810.02099", "submitter": "Solon Pissis", "authors": "Lorraine A.K Ayad, Giulia Bernardini, Roberto Grossi, Costas S.\n  Iliopoulos, Nadia Pisanti, Solon P. Pissis, and Giovanna Rosone", "title": "Longest Property-Preserved Common Factor", "comments": "Extended version of SPIRE 2018 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we introduce a new family of string processing problems. We are\ngiven two or more strings and we are asked to compute a factor common to all\nstrings that preserves a specific property and has maximal length. Here we\nconsider three fundamental string properties: square-free factors, periodic\nfactors, and palindromic factors under three different settings, one per\nproperty. In the first setting, we are given a string $x$ and we are asked to\nconstruct a data structure over $x$ answering the following type of on-line\nqueries: given string $y$, find a longest square-free factor common to $x$ and\n$y$. In the second setting, we are given $k$ strings and an integer $1 < k'\\leq\nk$ and we are asked to find a longest periodic factor common to at least $k'$\nstrings. In the third setting, we are given two strings and we are asked to\nfind a longest palindromic factor common to the two strings. We present\nlinear-time solutions for all settings. We anticipate that our paradigm can be\nextended to other string properties or settings.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 08:39:06 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Ayad", "Lorraine A. K", ""], ["Bernardini", "Giulia", ""], ["Grossi", "Roberto", ""], ["Iliopoulos", "Costas S.", ""], ["Pisanti", "Nadia", ""], ["Pissis", "Solon P.", ""], ["Rosone", "Giovanna", ""]]}, {"id": "1810.02112", "submitter": "Edouard Fouch\\'e", "authors": "Edouard Fouch\\'e and Klemens B\\\"ohm", "title": "Monte Carlo Dependency Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the dependency of variables is a fundamental task in data\nanalysis. Identifying the relevant attributes in databases leads to better data\nunderstanding and also improves the performance of learning algorithms, both in\nterms of runtime and quality. In data streams, dependency monitoring provides\nkey insights into the underlying process, but is challenging. In this paper, we\npropose Monte Carlo Dependency Estimation (MCDE), a theoretical framework to\nestimate multivariate dependency in static and dynamic data. MCDE quantifies\ndependency as the average discrepancy between marginal and conditional\ndistributions via Monte Carlo simulations. Based on this framework, we present\nMann-Whitney P (MWP), a novel dependency estimator. We show that MWP satisfies\na number of desirable properties and can accommodate any kind of numerical\ndata. We demonstrate the superiority of our estimator by comparing it to the\nstate-of-the-art multivariate dependency measures.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 09:16:46 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Fouch\u00e9", "Edouard", ""], ["B\u00f6hm", "Klemens", ""]]}, {"id": "1810.02183", "submitter": "Ilias Zadik", "authors": "Christian Borgs, Jennifer Chayes, Adam Smith, Ilias Zadik", "title": "Revealing Network Structure, Confidentially: Improved Rates for\n  Node-Private Graphon Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR cs.DS math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by growing concerns over ensuring privacy on social networks, we\ndevelop new algorithms and impossibility results for fitting complex\nstatistical models to network data subject to rigorous privacy guarantees. We\nconsider the so-called node-differentially private algorithms, which compute\ninformation about a graph or network while provably revealing almost no\ninformation about the presence or absence of a particular node in the graph.\n  We provide new algorithms for node-differentially private estimation for a\npopular and expressive family of network models: stochastic block models and\ntheir generalization, graphons. Our algorithms improve on prior work, reducing\ntheir error quadratically and matching, in many regimes, the optimal nonprivate\nalgorithm. We also show that for the simplest random graph models ($G(n,p)$ and\n$G(n,m)$), node-private algorithms can be qualitatively more accurate than for\nmore complex models---converging at a rate of $\\frac{1}{\\epsilon^2 n^{3}}$\ninstead of $\\frac{1}{\\epsilon^2 n^2}$. This result uses a new extension lemma\nfor differentially private algorithms that we hope will be broadly useful.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 13:00:27 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Borgs", "Christian", ""], ["Chayes", "Jennifer", ""], ["Smith", "Adam", ""], ["Zadik", "Ilias", ""]]}, {"id": "1810.02227", "submitter": "Jan Wassenberg", "authors": "Jan Wassenberg, Robert Obryk, Jyrki Alakuijala, Emmanuel Mogenet", "title": "Randen - fast backtracking-resistant random generator with\n  AES+Feistel+Reverie", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms that rely on a pseudorandom number generator often lose their\nperformance guarantees when adversaries can predict the behavior of the\ngenerator. To protect non-cryptographic applications against such attacks, we\npropose 'strong' pseudorandom generators characterized by two properties:\ncomputationally indistinguishable from random and backtracking-resistant. Some\nexisting cryptographically secure generators also meet these criteria, but they\nare too slow to be accepted for general-purpose use. We introduce a new\nopen-sourced generator called 'Randen' and show that it is 'strong' in addition\nto outperforming Mersenne Twister, PCG, ChaCha8, ISAAC and Philox in real-world\nbenchmarks. This is made possible by hardware acceleration. Randen is an\ninstantiation of Reverie, a recently published robust sponge-like random\ngenerator, with a new permutation built from an improved generalized Feistel\nstructure with 16 branches. We provide new bounds on active s-boxes for up to\n24 rounds of this construction, made possible by a memory-efficient search\nalgorithm. Replacing existing generators with Randen can protect randomized\nalgorithms such as reservoir sampling from attack. The permutation may also be\nuseful for wide-block ciphers and hashing functions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 14:08:57 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Wassenberg", "Jan", ""], ["Obryk", "Robert", ""], ["Alakuijala", "Jyrki", ""], ["Mogenet", "Emmanuel", ""]]}, {"id": "1810.02270", "submitter": "Yong Tan", "authors": "Yong Tan", "title": "Compound Binary Search Tree and Algorithms", "comments": "8 pages with words into 6300", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Binary Search Tree (BST) is average in computer science which supports a\ncompact data structure in memory and oneself even conducts a row of quick\nalgorithms, by which people often apply it in dynamical circumstance. Besides\nthese edges, it is also with weakness on its own structure specially with poor\nperformance at worst case. In this paper, we will develop this data structure\ninto a synthesis to show a series of novel features residing in. Of that, there\nare new methods invented for raising the performance and efficiency\nnevertheless some existing ones in logarithm or linear time.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 15:11:47 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Tan", "Yong", ""]]}, {"id": "1810.02304", "submitter": "Guillaume Ducoffe", "authors": "Guillaume Ducoffe", "title": "Polynomial-time Recognition of 4-Steiner Powers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k^{th}$-power of a given graph $G=(V,E)$ is obtained from $G$ by adding\nan edge between every two distinct vertices at a distance at most $k$ in $G$.\nWe call $G$ a $k$-Steiner power if it is an induced subgraph of the\n$k^{th}$-power of some tree. Our main contribution is a polynomial-time\nrecognition algorithm of $4$-Steiner powers, thereby extending the\ndecade-year-old results of (Lin, Kearney and Jiang, ISAAC'00) for $k=1,2$ and\n(Chang and Ko, WG'07) for $k=3$.\n  A graph $G$ is termed $k$-leaf power if there is some tree $T$ such that: all\nvertices in $V(G)$ are leaf-nodes of $T$, and $G$ is an induced subgraph of the\n$k^{th}$-power of $T$. As a byproduct of our main result, we give the first\nknown polynomial-time recognition algorithm for $6$-leaf powers.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 16:36:53 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2018 15:25:17 GMT"}, {"version": "v3", "created": "Sun, 3 Feb 2019 16:53:50 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Ducoffe", "Guillaume", ""]]}, {"id": "1810.02348", "submitter": "AmirMahdi Ahmadinejad", "authors": "AmirMahdi Ahmadinejad, Arun Jambulapati, Amin Saberi, and Aaron\n  Sidford", "title": "Perron-Frobenius Theory in Nearly Linear Time: Positive Eigenvectors,\n  M-matrices, Graph Kernels, and Other Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide nearly linear time algorithms for several problems\nclosely associated with the classic Perron-Frobenius theorem, including\ncomputing Perron vectors, i.e. entrywise non-negative eigenvectors of\nnon-negative matrices, and solving linear systems in asymmetric M-matrices, a\ngeneralization of Laplacian systems. The running times of our algorithms depend\nnearly linearly on the input size and polylogarithmically on the desired\naccuracy and problem condition number.\n  Leveraging these results we also provide improved running times for a broader\nrange of problems including computing random walk-based graph kernels,\ncomputing Katz centrality, and more. The running times of our algorithms\nimprove upon previously known results which either depended polynomially on the\ncondition number of the problem, required quadratic time, or only applied to\nspecial cases.\n  We obtain these results by providing new iterative methods for reducing these\nproblems to solving linear systems in Row-Column Diagonally Dominant (RCDD)\nmatrices. Our methods are related to the classic shift-and-invert\npreconditioning technique for eigenvector computation and constitute the first\nalternative to the result in Cohen et al. (2016) for reducing stationary\ndistribution computation and solving directed Laplacian systems to solving RCDD\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 17:51:11 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Ahmadinejad", "AmirMahdi", ""], ["Jambulapati", "Arun", ""], ["Saberi", "Amin", ""], ["Sidford", "Aaron", ""]]}, {"id": "1810.02452", "submitter": "Elham Havvaei", "authors": "David Eppstein and Elham Havvaei", "title": "Parameterized Leaf Power Recognition via Embedding into Graph Products", "comments": null, "journal-ref": "Algorithmica 82 (8): 2337-2359, 2020", "doi": "10.1007/s00453-020-00720-8", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-leaf power graph $G$ of a tree $T$ is a graph whose vertices are the\nleaves of $T$ and whose edges connect pairs of leaves at unweighted distance at\nmost~$k$ in $T$. Recognition of the $k$-leaf power graphs for $k \\geq 7$ is\nstill an open problem. In this paper, we provide two algorithms for this\nproblem for sparse leaf power graphs. Our results shows that the problem of\nrecognizing these graphs is fixed-parameter tractable when parameterized both\nby $k$ and by the degeneracy of the given graph. To prove this, we first\ndescribe how to embed the leaf root of a leaf power graph into a product of the\ngraph with a cycle graph. We bound the treewidth of the resulting product in\nterms of $k$ and the degeneracy of $G$. The first presented algorithm uses\nmethods based on monadic second-order logic (MSO$_2$) to recognize the\nexistence of a leaf power as a subgraph of the product graph. Using the same\nembedding in the product graph, the second algorithm presents a dynamic\nprogramming approach to solve the problem and provide a better dependence on\nthe parameters.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 23:08:03 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 22:49:21 GMT"}, {"version": "v3", "created": "Sun, 31 May 2020 22:43:32 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Eppstein", "David", ""], ["Havvaei", "Elham", ""]]}, {"id": "1810.02711", "submitter": "Maxence Delorme", "authors": "Maxence Delorme, Sergio Garc\\'ia, Jacek Gondzio, Joerg Kalcsics, David\n  Manlove, and William Pettersson", "title": "Mathematical models for stable matching problems with ties and\n  incomplete lists", "comments": "31 pages, 11 tables, 1 figure", "journal-ref": null, "doi": "10.1016/j.ejor.2019.09.006", "report-no": "ERGO-18-024", "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new integer linear programming (ILP) models for NP-hard\noptimisation problems in instances of the Stable Marriage problem with Ties and\nIncomplete lists (SMTI) and its many-to-one generalisation, the Hospitals /\nResidents problem with Ties (HRT). These models can be used to efficiently\nsolve these optimisation problems when applied to (i) instances derived from\nreal-world applications, and (ii) larger instances that are randomly-generated.\nIn the case of SMTI, we consider instances arising from the pairing of children\nwith adoptive families, where preferences are obtained from a quality measure\nof each possible pairing of child to family. In this case we seek a maximum\nweight stable matching. We present new algorithms for preprocessing instances\nof SMTI with ties on both sides, as well as new ILP models. Algorithms based on\nexisting state-of-the-art models only solve 6 of our 22 real-world instances\nwithin an hour per instance, and our new models solve all 22 instances within a\nmean runtime of 60 seconds. For HRT, we consider instances derived from the\nproblem of assigning junior doctors to foundation posts in Scottish hospitals.\nHere we seek a maximum size stable matching. We show how to extend our models\nfor SMTI to the HRT case. For the real instances, we reduce the mean runtime\nfrom an average of 144 seconds when using state-of-the-art methods, to 3\nseconds when using our new ILP-based algorithms. We also show that our models\noutperform considerably state-of-the-art models on larger randomly-generated\ninstances of SMTI and HRT.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 14:23:31 GMT"}, {"version": "v2", "created": "Sun, 17 Mar 2019 16:18:22 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Delorme", "Maxence", ""], ["Garc\u00eda", "Sergio", ""], ["Gondzio", "Jacek", ""], ["Kalcsics", "Joerg", ""], ["Manlove", "David", ""], ["Pettersson", "William", ""]]}, {"id": "1810.02722", "submitter": "Dengwang Tang", "authors": "Dengwang Tang, Vijay G. Subramanian", "title": "Balanced Allocation with Random Walk Based Sampling", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the standard ball-in-bins experiment, a well-known scheme is to sample $d$\nbins independently and uniformly at random and put the ball into the least\nloaded bin. It can be shown that this scheme yields a maximum load of $\\log\\log\nn/\\log d+O(1)$ with high probability.\n  Subsequent work analyzed the model when at each time, $d$ bins are sampled\nthrough some correlated or non-uniform way. However, the case when the sampling\nfor different balls are correlated are rarely investigated. In this paper we\npropose three schemes for the ball-in-bins allocation problem. We assume that\nthere is an underlying $k$-regular graph connecting the bins. The three schemes\nare variants of power-of-$d$ choices, except that the sampling of $d$ bins at\neach time are based on the locations of $d$ independently moving\nnon-backtracking random walkers, with the positions of the random walkers being\nreset when certain events occurs. We show that under some conditions for the\nunderlying graph that can be summarized as the graph having large enough girth,\nall three schemes can perform as well as power-of-$d$, so that the maximum load\nis bounded by $\\log\\log n/\\log d+O(1)$ with high probability.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 14:39:45 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 13:30:11 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Tang", "Dengwang", ""], ["Subramanian", "Vijay G.", ""]]}, {"id": "1810.02899", "submitter": "Ran Ben Basat", "authors": "Ran Ben Basat, Gil Einziger, Isaac Keslassy, Ariel Orda, Shay\n  Vargaftik, Erez Waisbard", "title": "Memento: Making Sliding Windows Efficient for Heavy Hitters", "comments": "This is an extended version of the paper that will appear in ACM\n  CoNEXT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud operators require real-time identification of Heavy Hitters (HH) and\nHierarchical Heavy Hitters (HHH) for applications such as load balancing,\ntraffic engineering, and attack mitigation. However, existing techniques are\nslow in detecting new heavy hitters.\n  In this paper, we make the case for identifying heavy hitters through\n\\textit{sliding windows}. Sliding windows detect heavy hitters quicker and more\naccurately than current methods, but to date had no practical algorithms.\nAccordingly, we introduce, design and analyze the \\textit{Memento} family of\nsliding window algorithms for the HH and HHH problems in the single-device and\nnetwork-wide settings. Using extensive evaluations, we show that our\nsingle-device solutions attain similar accuracy and are by up to $273\\times$\nfaster than existing window-based techniques. Furthermore, we exemplify our\nnetwork-wide HHH detection capabilities on a realistic testbed. To that end, we\nimplemented Memento as an open-source extension to the popular HAProxy cloud\nload-balancer. In our evaluations, using an HTTP flood by 50 subnets, our\nnetwork-wide approach detected the new subnets faster, and reduced the number\nof undetected flood requests by up to $37\\times$ compared to the alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 22:50:24 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 21:28:05 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Basat", "Ran Ben", ""], ["Einziger", "Gil", ""], ["Keslassy", "Isaac", ""], ["Orda", "Ariel", ""], ["Vargaftik", "Shay", ""], ["Waisbard", "Erez", ""]]}, {"id": "1810.03083", "submitter": "Shahin Pourbahrami", "authors": "Shahin Pourbahrami, Leyli Mohammad Khanli", "title": "A Survey of Neighbourhood Construction Models for Categorizing Data\n  Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Finding neighbourhood structures is very useful in extracting valuable\nrelationships among data samples. This paper presents a survey of recent\nneighbourhood construction algorithms for pattern clustering and classifying\ndata points. Extracting neighbourhoods and connections among the points is\nextremely useful for clustering and classifying the data. Many applications\nsuch as detecting social network communities, bundling related edges, and\nsolving location and routing problems all indicate the usefulness of this\nproblem. Finding data point neighbourhood in data mining and pattern\nrecognition should generally improve knowledge extraction from databases.\nSeveral algorithms of data point neighbourhood construction have been proposed\nto analyse the data in this sense. They will be described and discussed from\ndifferent aspects in this paper. Finally, the future challenges concerning the\ntitle of the present paper will be outlined.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 04:05:29 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Pourbahrami", "Shahin", ""], ["Khanli", "Leyli Mohammad", ""]]}, {"id": "1810.03087", "submitter": "Andrei Bulatov", "authors": "Amineh Dadsetan and Andrei A. Bulatov", "title": "Counting homomorphisms in plain exponential time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the counting Graph Homomorphism problem (#GraphHom) the question is: Given\ngraphs G,H, find the number of homomorphisms from G to H. This problem is\ngenerally #P-complete, moreover, Cygan et al. proved that unless the ETH is\nfalse there is no algorithm that solves this problem in time\nO(|V(H)|^{o(|V(G)|)}. This, however, does not rule out the possibility that\nfaster algorithms exist for restricted problems of this kind. Wahlstrom proved\nthat #GraphHom can be solved in plain exponential time, that is, in time\nk^{|V(G)|+V(H)|}\\poly(|V(H)|,|V(G)|) provided H has clique width k. We\ngeneralize this result to a larger class of graphs, and also identify several\nother graph classes that admit a plain exponential algorithm for #GraphHom.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 05:42:30 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Dadsetan", "Amineh", ""], ["Bulatov", "Andrei A.", ""]]}, {"id": "1810.03162", "submitter": "Feras Fattohi", "authors": "Feras Fattohi (Technische Universit\\\"at Berlin)", "title": "Competitive Online Virtual Cluster Embedding Algorithms", "comments": "MSc Thesis at Technische Universit\\\"at Berlin", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the conventional cloud service model, computing resources are allocated\nfor tenants on a pay-per-use basis. However, the performance of applications\nthat communicate inside this network is unpredictable because network resources\nare not guaranteed. To mitigate this issue, the virtual cluster (VC) model has\nbeen developed in which network and compute units are guaranteed. Thereon, many\nalgorithms have been developed that are based on novel extensions of the VC\nmodel in order to solve the online virtual cluster embedding problem (VCE) with\nadditional parameters. In the online VCE, the resource footprint is greedily\nminimized per request which is connected with maximizing the profit for the\nprovider per request. However, this does not imply that a global maximization\nof the profit over the whole sequence of requests is guaranteed. In fact, these\nalgorithms do not even provide a worst case guarantee on a fraction of the\nmaximum achievable profit of a certain sequence of requests. Thus, these online\nalgorithms do not provide a competitive ratio on the profit.\n  In this thesis, two competitive online VCE algorithms and two heuristic\nalgorithms are presented. The competitive online VCE algorithms have different\ncompetitive ratios on the objective function and the capacity constraints\nwhereas the heuristic algorithms do not violate the capacity constraints. The\nworst case competitive ratios are analyzed. After that, the evaluation shows\nthe advantages and disadvantages of these algorithms in several scenarios with\ndifferent request patterns and profit metrics on the fat-tree and MDCube\ndatacenter topologies. The results show that for different scenarios, different\nalgorithms have the best performance with respect to certain metrics.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 15:31:08 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Fattohi", "Feras", "", "Technische Universit\u00e4t Berlin"]]}, {"id": "1810.03224", "submitter": "Huan Li", "authors": "Huan Li, Aaron Schild", "title": "Spectral Subspace Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to spectral sparsification that approximates the\nquadratic form of the pseudoinverse of a graph Laplacian restricted to a\nsubspace. We show that sparsifiers with a near-linear number of edges in the\ndimension of the subspace exist. Our setting generalizes that of Schur\ncomplement sparsifiers. Our approach produces sparsifiers by sampling a\nuniformly random spanning tree of the input graph and using that tree to guide\nan edge elimination procedure that contracts, deletes, and reweights edges. In\nthe context of Schur complement sparsifiers, our approach has two benefits over\nprior work. First, it produces a sparsifier in almost-linear time with no\nruntime dependence on the desired error. We directly exploit this to compute\napproximate effective resistances for a small set of vertex pairs in faster\ntime than prior work (Durfee-Kyng-Peebles-Rao-Sachdeva '17). Secondly, it\nyields sparsifiers that are reweighted minors of the input graph. As a result,\nwe give a near-optimal answer to a variant of the Steiner point removal\nproblem.\n  A key ingredient of our algorithm is a subroutine of independent interest: a\nnear-linear time algorithm that, given a chosen set of vertices, builds a data\nstructure from which we can query a multiplicative approximation to the\ndecrease in the effective resistance between two vertices after identifying all\nvertices in the chosen set to a single vertex with inverse polynomial\nadditional additive error in near-constant time.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 23:17:30 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Li", "Huan", ""], ["Schild", "Aaron", ""]]}, {"id": "1810.03374", "submitter": "Raghu Meka", "authors": "Nikhil Bansal, Raghu Meka", "title": "On the discrepancy of random low degree set systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the celebrated Beck-Fiala conjecture, we consider the random\nsetting where there are $n$ elements and $m$ sets and each element lies in $t$\nrandomly chosen sets. In this setting, Ezra and Lovett showed an $O((t \\log\nt)^{1/2})$ discrepancy bound in the regime when $n \\leq m$ and an $O(1)$ bound\nwhen $n \\gg m^t$.\n  In this paper, we give a tight $O(\\sqrt{t})$ bound for the entire range of\n$n$ and $m$, under a mild assumption that $t = \\Omega (\\log \\log m)^2$. The\nresult is based on two steps. First, applying the partial coloring method to\nthe case when $n = m \\log^{O(1)} m$ and using the properties of the random set\nsystem we show that the overall discrepancy incurred is at most $O(\\sqrt{t})$.\nSecond, we reduce the general case to that of $n \\leq m \\log^{O(1)}m$ using LP\nduality and a careful counting argument.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 11:14:24 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Bansal", "Nikhil", ""], ["Meka", "Raghu", ""]]}, {"id": "1810.03491", "submitter": "Sayan Bhattacharya", "authors": "Sayan Bhattacharya and Janardhan Kulkarni", "title": "An Improved Algorithm for Incremental Cycle Detection and Topological\n  Ordering in Sparse Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of incremental cycle detection and topological\nordering in a directed graph $G = (V, E)$ with $|V| = n$ nodes. In this\nsetting, initially the edge-set $E$ of the graph is empty. Subsequently, at\neach time-step an edge gets inserted into $G$. After every edge-insertion, we\nhave to report if the current graph contains a cycle, and as long as the graph\nremains acyclic, we have to maintain a topological ordering of the node-set\n$V$. Let $m$ be the total number of edges that get inserted into $G$. We\npresent a randomized algorithm for this problem with $\\tilde{O}(m^{4/3})$ total\nexpected update time.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 14:26:48 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Bhattacharya", "Sayan", ""], ["Kulkarni", "Janardhan", ""]]}, {"id": "1810.03513", "submitter": "Robert Gmyr", "authors": "Robert Gmyr and Gopal Pandurangan", "title": "Time-Message Trade-Offs in Distributed Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper focuses on showing time-message trade-offs in distributed\nalgorithms for fundamental problems such as leader election, broadcast,\nspanning tree (ST), minimum spanning tree (MST), minimum cut, and many graph\nverification problems. We consider the synchronous CONGEST distributed\ncomputing model and assume that each node has initial knowledge of itself and\nthe identifiers of its neighbors - the so-called KT1 model - a well-studied\nmodel that also naturally arises in many applications. Recently, it has been\nestablished that one can obtain (almost) singularly optimal algorithms, i.e.,\nalgorithms that have simultaneously optimal time and message complexity (up to\npolylogarithmic factors), for many fundamental problems in the standard KT0\nmodel (where nodes have only local knowledge of themselves and not their\nneighbors). The situation is less clear in the KT1 model. In this paper, we\npresent several new distributed algorithms in the KT1 model that trade off\nbetween time and message complexity.\n  Our distributed algorithms are based on a uniform approach which involves\nconstructing a sparsified spanning subgraph of the original graph - called a\ndanner - that trades off the number of edges with the diameter of the\nsparsifier. In particular, a key ingredient of our approach is a distributed\nrandomized algorithm that, given a graph G and any delta in [0, 1], with high\nprobability constructs a danner that has diameter Otilde(D + n^(1 - delta)) and\nOtilde(min{m, n^(1 + delta)}) edges in Otilde(n^(1 - delta)) rounds while using\nOtilde(min{m, n^(1 + delta)}) messages, where n, m, and D are the number of\nnodes, edges, and the diameter of G, respectively. Using our danner\nconstruction, we present a family of distributed randomized algorithms for\nvarious fundamental problems that exhibit a trade-off between message and time\ncomplexity and that improve over previous results.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 14:59:22 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Gmyr", "Robert", ""], ["Pandurangan", "Gopal", ""]]}, {"id": "1810.03551", "submitter": "Diptarka Chakraborty", "authors": "Diptarka Chakraborty, Debarati Das, Michal Koucky", "title": "Approximate Online Pattern Matching in Sub-linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the approximate pattern matching problem under edit distance. In\nthis problem we are given a pattern $P$ of length $w$ and a text $T$ of length\n$n$ over some alphabet $\\Sigma$, and a positive integer $k$. The goal is to\nfind all the positions $j$ in $T$ such that there is a substring of $T$ ending\nat $j$ which has edit distance at most $k$ from the pattern $P$. Recall, the\nedit distance between two strings is the minimum number of character\ninsertions, deletions, and substitutions required to transform one string into\nthe other. For a position $t$ in $\\{1,...,n\\}$, let $k_t$ be the smallest edit\ndistance between $P$ and any substring of $T$ ending at $t$. In this paper we\ngive a constant factor approximation to the sequence $k_1,k_2,...,k_{n}$. We\nconsider both offline and online settings.\n  In the offline setting, where both $P$ and $T$ are available, we present an\nalgorithm that for all $t$ in $\\{1,...,n\\}$, computes the value of $k_t$\napproximately within a constant factor. The worst case running time of our\nalgorithm is $O(n w^{3/4})$. As a consequence we break the $O(nw)$-time barrier\nfor this problem.\n  In the online setting, we are given $P$ and then $T$ arrives one symbol at a\ntime. We design an algorithm that upon arrival of the $t$-th symbol of $T$\ncomputes $k_t$ approximately within $O(1)$-multiplicative factor and\n$w^{8/9}$-additive error. Our algorithm takes $O(w^{1-(7/54)})$ amortized time\nper symbol arrival and takes $O(w^{1-(1/54)})$ additional space apart from\nstoring the pattern $P$.\n  Both of our algorithms are randomized and produce correct answer with high\nprobability. To the best of our knowledge this is the first worst-case\nsub-linear (in the length of the pattern) time and sub-linear succinct space\nalgorithm for online approximate pattern matching problem.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 16:10:17 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 11:03:00 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Chakraborty", "Diptarka", ""], ["Das", "Debarati", ""], ["Koucky", "Michal", ""]]}, {"id": "1810.03562", "submitter": "Carlos Alejandro Alfaro", "authors": "Carlos A. Alfaro, Sergio L. Perez, Carlos E. Valencia, Marcos C.\n  Vargas", "title": "The equivalence between two classic algorithms for the assignment\n  problem", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a detailed review of two algorithms that solve the minimization case\nof the assignment problem. The Bertsekas' auction algorithm and the Goldberg &\nKennedy algorithm. We will show that these algorithms are equivalent in the\nsense that both perform equivalent steps in the same order. We also present\nexperimental results comparing the performance of three algorithms for the\nassignment problem. They show the auction algorithm performs and scales better\nin practice than algorithms that are harder to implement but have better\ntheoretical time complexity.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 16:23:15 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Alfaro", "Carlos A.", ""], ["Perez", "Sergio L.", ""], ["Valencia", "Carlos E.", ""], ["Vargas", "Marcos C.", ""]]}, {"id": "1810.03664", "submitter": "Diptarka Chakraborty", "authors": "Diptarka Chakraborty, Debarati Das, Elazar Goldenberg, Michal Koucky,\n  Michael Saks", "title": "Approximating Edit Distance Within Constant Factor in Truly\n  Sub-Quadratic Time", "comments": null, "journal-ref": "Journal of the ACM, Volume 67, Issue 6, October 2020, Article No.:\n  36, Page number: 1-22", "doi": "10.1145/3422823", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edit distance is a measure of similarity of two strings based on the minimum\nnumber of character insertions, deletions, and substitutions required to\ntransform one string into the other. The edit distance can be computed exactly\nusing a dynamic programming algorithm that runs in quadratic time. Andoni,\nKrauthgamer, and Onak (2010) gave a nearly linear time algorithm that\napproximates edit distance within an approximation factor $\\text{poly}(\\log\nn)$.\n  In this paper, we provide an algorithm with running time\n$\\tilde{O}(n^{2-2/7})$ that approximates the edit distance within a constant\nfactor.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 19:08:37 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 04:57:03 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Chakraborty", "Diptarka", ""], ["Das", "Debarati", ""], ["Goldenberg", "Elazar", ""], ["Koucky", "Michal", ""], ["Saks", "Michael", ""]]}, {"id": "1810.03792", "submitter": "Pasin Manurangsi", "authors": "Pasin Manurangsi", "title": "A Note on Max $k$-Vertex Cover: Faster FPT-AS, Smaller Approximate\n  Kernel and Improved Approximation", "comments": "An extended abstract of this work will appear in SOSA'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Maximum $k$-Vertex Cover (Max $k$-VC), the input is an edge-weighted graph\n$G$ and an integer $k$, and the goal is to find a subset $S$ of $k$ vertices\nthat maximizes the total weight of edges covered by $S$. Here we say that an\nedge is covered by $S$ iff at least one of its endpoints lies in $S$.\n  We present an FPT approximation scheme (FPT-AS) that runs in\n$(1/\\epsilon)^{O(k)} poly(n)$ time for the problem, which improves upon Gupta\net al.'s $(k/\\epsilon)^{O(k)} poly(n)$-time FPT-AS [SODA'18, FOCS'18]. Our\nalgorithm is simple: just use brute force to find the best $k$-vertex subset\namong the $O(k/\\epsilon)$ vertices with maximum weighted degrees.\n  Our algorithm naturally yields an efficient approximate kernelization scheme\nof $O(k/\\epsilon)$ vertices; previously, an $O(k^5/\\epsilon^2)$-vertex\napproximate kernel is only known for the unweighted version of Max $k$-VC\n[Lokshtanov et al., STOC'17]. Interestingly, this has an application outside of\nparameterized complexity: using our approximate kernelization as a\npreprocessing step, we can directly apply Raghavendra and Tan's SDP-based\nalgorithm for 2SAT with cardinality constraint [SODA'12] to give an\n$0.92$-approximation for Max $k$-VC in polynomial time. This improves upon\nFeige and Langberg's algorithm [J. Algorithms'01] which yields $(0.75 +\n\\delta)$-approximation for some (unspecified) constant $\\delta > 0$.\n  We also consider the minimization version (Min $k$-VC), where the goal is to\nminimize the total weight of edges covered by $S$. We provide an FPT-AS for Min\n$k$-VC with similar running time of $(1/\\epsilon)^{O(k)} poly(n)$, which again\nimproves on a $(k/\\epsilon)^{O(k)} poly(n)$-time FPT-AS of Gupta et al. On the\nother hand, we show that there is unlikely a polynomial size approximate\nkernelization for Min $k$-VC for any factor less than two.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 03:16:12 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Manurangsi", "Pasin", ""]]}, {"id": "1810.03813", "submitter": "Moran Feldman", "authors": "Moran Feldman", "title": "Guess Free Maximization of Submodular and Linear Sums", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing the sum of a monotone submodular\nfunction and a linear function subject to a general solvable polytope\nconstraint. Recently, Sviridenko et al. (2017) described an algorithm for this\nproblem whose approximation guarantee is optimal in some intuitive and formal\nsenses. Unfortunately, this algorithm involves a guessing step which makes it\nless clean and significantly affects its time complexity. In this work we\ndescribe a clean alternative algorithm that uses a novel weighting technique in\norder to avoid the problematic guessing step while keeping the same\napproximation guarantee as the algorithm of Sviridenko et al.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 04:42:04 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Feldman", "Moran", ""]]}, {"id": "1810.04006", "submitter": "Florent Capelli", "authors": "Florent Capelli, Yann Strozecki", "title": "Enumerating models of DNF faster: breaking the dependency on the formula\n  size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study the problem of enumerating the models of DNF\nformulas. The aim is to provide enumeration algorithms with a delay that\ndepends polynomially on the size of each model and not on the size of the\nformula, which can be exponentially larger. We succeed for two subclasses of\nDNF formulas: we provide a constant delay algorithm for $k$-DNF with fixed $k$\nby an appropriate amortization method and we give a quadratic delay algorithm\nfor monotone formulas. We then focus on the \\emph{average delay} of enumeration\nalgorithms and show how to obtain a sublinear delay in the formula size.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 14:06:57 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 06:30:36 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Capelli", "Florent", ""], ["Strozecki", "Yann", ""]]}, {"id": "1810.04312", "submitter": "EPTCS", "authors": "David Hardin (Rockwell Collins), Konrad Slind (Rockwell Collins)", "title": "Using ACL2 in the Design of Efficient, Verifiable Data Structures for\n  High-Assurance Systems", "comments": "In Proceedings ACL2 2018, arXiv:1810.03762", "journal-ref": "EPTCS 280, 2018, pp. 61-76", "doi": "10.4204/EPTCS.280.5", "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification of algorithms and data structures utilized in modern autonomous\nand semi-autonomous vehicles for land, sea, air, and space presents a\nsignificant challenge. Autonomy algorithms, e.g., route planning, pattern\nmatching, and inference, are based on complex data structures such as directed\ngraphs and algebraic data types. Proof techniques for these data structures\nexist, but are oriented to unbounded, functional realizations, which are not\ntypically efficient in either space or time. Autonomous systems designers, on\nthe other hand, generally limit the space and time allocations for any given\nfunction, and require that algorithms deliver results within a finite time, or\nsuffer a watchdog timeout. Furthermore, high-assurance design rules frown on\ndynamic memory allocation, preferring simple array-based data structure\nimplementations.\n  In order to provide efficient implementations of high-level data structures\nused in autonomous systems with the high assurance needed for accreditation, we\nhave developed a verifying compilation technique that supports the \"natural\"\nfunctional proof style, but yet applies to more efficient data structure\nimplementations. Our toolchain features code generation to mainstream\nprogramming languages, as well as GPU-based and hardware-based realizations. We\nbase the Intermediate Verification Language for our toolchain upon higher-order\nlogic; however, we have used ACL2 to develop our efficient yet verifiable data\nstructure design. ACL2 is particularly well-suited for this work, with its\nsophisticated libraries for reasoning about aggregate data structures of\narbitrary size, efficient execution of formal specifications, as well as its\nsupport for \"single-threaded objects\" -- functional datatypes with imperative\n\"under the hood\" implementations.\n  In this paper, we detail our high-assurance data structure design approach,\nincluding examples in ACL2 of common algebraic data types implemented using\nthis design approach, proofs of correctness for those data types carried out in\nACL2, as well as sample ACL2 implementations of relevant algorithms utilizing\nthese efficient, high-assurance data structures.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 00:36:03 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Hardin", "David", "", "Rockwell Collins"], ["Slind", "Konrad", "", "Rockwell Collins"]]}, {"id": "1810.04321", "submitter": "Assaf Naor", "authors": "Subhash Khot and Assaf Naor", "title": "The Andoni--Krauthgamer--Razenshteyn characterization of sketchable\n  norms fails for sketchable metrics", "comments": "To appear in SODA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.FA math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Andoni, Krauthgamer and Razenshteyn (AKR) proved (STOC 2015) that a\nfinite-dimensional normed space $(X,\\|\\cdot\\|_X)$ admits a $O(1)$ sketching\nalgorithm (namely, with $O(1)$ sketch size and $O(1)$ approximation) if and\nonly if for every $\\varepsilon\\in (0,1)$ there exist $\\alpha\\geqslant 1$ and an\nembedding $f:X\\to \\ell_{1-\\varepsilon}$ such that $\\|x-y\\|_X\\leqslant\n\\|f(x)-f(y)\\|_{1-\\varepsilon}\\leqslant \\alpha \\|x-y\\|_X$ for all $x,y\\in X$.\nThe \"if part\" of this theorem follows from a sketching algorithm of Indyk (FOCS\n2000). The contribution of AKR is therefore to demonstrate that the mere\navailability of a sketching algorithm implies the existence of the\naforementioned geometric realization. Indyk's algorithm shows that the \"if\npart\" of the AKR characterization holds true for any metric space whatsoever,\ni.e., the existence of an embedding as above implies sketchability even when\n$X$ is not a normed space. Due to this, a natural question that AKR posed was\nwhether the assumption that the underlying space is a normed space is needed\nfor their characterization of sketchability. We resolve this question by\nproving that for arbitrarily large $n\\in \\mathbb{N}$ there is an $n$-point\nmetric space $(M(n),d_{M(n)})$ which is $O(1)$-sketchable yet for every\n$\\varepsilon\\in (0,\\frac12)$, if $\\alpha(n)\\geqslant 1$ and $f_n:M(n)\\to\n\\ell_{1-\\varepsilon}$ are such that $d_{M(n)}(x,y)\\leqslant\n\\|f_n(x)-f_n(y)\\|_{1-\\varepsilon}\\leqslant \\alpha(n) d_{M(n)}(x,y)$ for all\n$x,y\\in M(n)$, then necessarily $\\lim_{n\\to \\infty} \\alpha(n)= \\infty$.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 00:56:59 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Khot", "Subhash", ""], ["Naor", "Assaf", ""]]}, {"id": "1810.04331", "submitter": "Ali Shameli", "authors": "Itai Ashlagi, Amin Saberi, Ali Shameli", "title": "Assignment Mechanisms under Distributional Constraints", "comments": "26 pages, conference version published in SODA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the assignment problem of objects to agents with heterogeneous\npreferences under distributional constraints. Each agent is associated with a\npublicly known type and has a private ordinal ranking over objects. We are\ninterested in assigning as many agents as possible. Our first contribution is a\ngeneralization of the well-known and widely used serial dictatorship. Our\nmechanism maintains several desirable properties of serial dictatorship,\nincluding strategyproofness, Pareto efficiency, and computational tractability\nwhile satisfying the distributional constraints with a small error. We also\npropose a generalization of the probabilistic serial algorithm, which finds an\nordinally efficient and envy-free assignment, and also satisfies the\ndistributional constraints with a small error. We show, however, that no\nordinally efficient and envy-free mechanism is also weakly strategyproof. Both\nof our algorithms assign at least the same number of students as the optimum\nfractional assignment.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 02:20:02 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 08:09:18 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Ashlagi", "Itai", ""], ["Saberi", "Amin", ""], ["Shameli", "Ali", ""]]}, {"id": "1810.04348", "submitter": "Anirudh Subramanyam", "authors": "Anirudh Subramanyam, Panagiotis P. Repoussis, Chrysanthos E. Gounaris", "title": "Robust optimization of a broad class of heterogeneous vehicle routing\n  problems under demand uncertainty", "comments": "54 pages, 10 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies robust variants of an extended model of the classical\nHeterogeneous Vehicle Routing Problem (HVRP), where a mixed fleet of vehicles\nwith different capacities, availabilities, fixed costs and routing costs is\nused to serve customers with uncertain demand. This model includes, as special\ncases, all variants of the HVRP studied in the literature with fixed and\nunlimited fleet sizes, accessibility restrictions at customer locations, as\nwell as multiple depots. Contrary to its deterministic counterpart, the goal of\nthe robust HVRP is to determine a minimum-cost set of routes and fleet\ncomposition that remains feasible for all demand realizations from a\npre-specified uncertainty set. To solve this problem, we develop robust\nversions of classical node- and edge-exchange neighborhoods that are commonly\nused in local search and establish that efficient evaluation of the local moves\ncan be achieved for five popular classes of uncertainty sets. The proposed\nlocal search is then incorporated in a modular fashion within two metaheuristic\nalgorithms to determine robust HVRP solutions. The quality of the metaheuristic\nsolutions is quantified using an integer programming model that provides lower\nbounds on the optimal solution. An extensive computational study on literature\nbenchmarks shows that the proposed methods allow us to obtain high quality\nrobust solutions for different uncertainty sets and with minor additional\neffort compared to deterministic solutions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 03:11:41 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 23:42:36 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Subramanyam", "Anirudh", ""], ["Repoussis", "Panagiotis P.", ""], ["Gounaris", "Chrysanthos E.", ""]]}, {"id": "1810.04379", "submitter": "Daniel Paulusma", "authors": "Esther Galby, Paloma T. Lima, Daniel Paulusma, Bernard Ries", "title": "Classifying k-Edge Colouring for H-free Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is $H$-free if it does not contain an induced subgraph isomorphic to\n$H$. For every integer $k$ and every graph $H$, we determine the computational\ncomplexity of $k$-Edge Colouring for $H$-free graphs.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 06:03:12 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Galby", "Esther", ""], ["Lima", "Paloma T.", ""], ["Paulusma", "Daniel", ""], ["Ries", "Bernard", ""]]}, {"id": "1810.04449", "submitter": "Christian P. Robert", "authors": "Changye Wu (U Paris Dauphine), Julien Stoehr (U Paris Dauphine), and\n  Christian P. Robert (U Paris Dauphine & U Warwick)", "title": "Faster Hamiltonian Monte Carlo by Learning Leapfrog Scale", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo samplers have become standard algorithms for MCMC\nimplementations, as opposed to more basic versions, but they still require some\namount of tuning and calibration. Exploiting the U-turn criterion of the NUTS\nalgorithm (Hoffman and Gelman, 2014), we propose a version of HMC that relies\non the distribution of the integration time of the associated leapfrog\nintegrator. Using in addition the primal-dual averaging method for tuning the\nstep size of the integrator, we achieve an essentially calibration free version\nof HMC. When compared with the original NUTS on several benchmarks, this\nalgorithm exhibits a significantly improved efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 10:34:48 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 09:46:38 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Wu", "Changye", "", "U Paris Dauphine"], ["Stoehr", "Julien", "", "U Paris Dauphine"], ["Robert", "Christian P.", "", "U Paris Dauphine & U Warwick"]]}, {"id": "1810.04517", "submitter": "Zhize Li", "authors": "Zhize Li, Wei Zhang, Kees Roos", "title": "A Fast Polynomial-time Primal-Dual Projection Algorithm for Linear\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, there are several polynomial algorithms for linear programming\nincluding the ellipsoid method, the interior point method and other variants.\nRecently, Chubanov [Chubanov, 2015] proposed a projection and rescaling\nalgorithm, which has become a potentially \\emph{practical} class of polynomial\nalgorithms for linear feasibility problems and also for the general linear\nprogramming. However, the Chubanov-type algorithms usually perform much better\non the infeasible instances than on the feasible instances in practice. To\nexplain this phenomenon, we derive a new theoretical complexity bound for the\ninfeasible instances based on the condition number, which shows that algorithms\ncan indeed run much faster on infeasible instances in certain situations. In\norder to speed up the feasible instances, we propose a \\emph{Polynomial-time\nPrimal-Dual Projection} algorithm (called PPDP) by explicitly developing the\ndual algorithm. The numerical results validate that our PPDP algorithm achieves\na quite balanced performance between feasible and infeasible instances, and its\nperformance is remarkably better than previous algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 13:27:02 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Li", "Zhize", ""], ["Zhang", "Wei", ""], ["Roos", "Kees", ""]]}, {"id": "1810.04620", "submitter": "R\\'emi Watrigant", "authors": "\\'Edouard Bonnet, Nicolas Bousquet, Pierre Charbit, St\\'ephan\n  Thomass\\'e, R\\'emi Watrigant", "title": "Parameterized Complexity of Independent Set in H-Free Graphs", "comments": "An extended abstract appeared in the proceedings of IPEC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we investigate the complexity of Maximum Independent Set (MIS)\nin the class of $H$-free graphs, that is, graphs excluding a fixed graph as an\ninduced subgraph. Given that the problem remains $NP$-hard for most graphs $H$,\nwe study its fixed-parameter tractability and make progress towards a dichotomy\nbetween $FPT$ and $W[1]$-hard cases. We first show that MIS remains $W[1]$-hard\nin graphs forbidding simultaneously $K_{1, 4}$, any finite set of cycles of\nlength at least $4$, and any finite set of trees with at least two branching\nvertices. In particular, this answers an open question of Dabrowski et al.\nconcerning $C_4$-free graphs. Then we extend the polynomial algorithm of\nAlekseev when $H$ is a disjoint union of edges to an $FPT$ algorithm when $H$\nis a disjoint union of cliques. We also provide a framework for solving several\nother cases, which is a generalization of the concept of \\emph{iterative\nexpansion} accompanied by the extraction of a particular structure using\nRamsey's theorem. Iterative expansion is a maximization version of the\nso-called \\emph{iterative compression}. We believe that our framework can be of\nindependent interest for solving other similar graph problems. Finally, we\npresent positive and negative results on the existence of polynomial (Turing)\nkernels for several graphs $H$.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 16:24:29 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 15:34:23 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Bousquet", "Nicolas", ""], ["Charbit", "Pierre", ""], ["Thomass\u00e9", "St\u00e9phan", ""], ["Watrigant", "R\u00e9mi", ""]]}, {"id": "1810.04670", "submitter": "Ranveer Singh", "authors": "Ranveer Singh, Vivek Vijay, RB Bapat", "title": "Algorithm for $\\mathcal{B}$-partitions, parameterized complexity of the\n  matrix determinant and permanent", "comments": "arXiv admin note: text overlap with arXiv:1701.04420", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every square matrix $A=(a_{uv})\\in \\mathcal{C}^{n\\times n}$ can be\nrepresented as a digraph having $n$ vertices. In the digraph, a block (or\n2-connected component) is a maximally connected subdigraph that has no\ncut-vertex. The determinant and the permanent of a matrix can be calculated in\nterms of the determinant and the permanent of some specific induced subdigraphs\nof the blocks in the digraph. Interestingly, these induced subdigraphs are\nvertex-disjoint and they partition the digraph. Such partitions of the digraph\nare called the $\\mathcal{B}$-partitions. In this paper, first, we develop an\nalgorithm to find the $\\mathcal{B}$-partitions. Next, we analyze the\nparameterized complexity of matrix determinant and permanent, where, the\nparameters are the sizes of blocks and the number of cut-vertices of the\ndigraph. We give a class of combinations of cut-vertices and block sizes for\nwhich the parametrized complexities beat the state of art complexities of the\ndeterminant and the permanent.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 08:03:34 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Singh", "Ranveer", ""], ["Vijay", "Vivek", ""], ["Bapat", "RB", ""]]}, {"id": "1810.04686", "submitter": "Michael Jarret", "authors": "Michael Jarret, Brad Lackey, Aike Liu, Kianna Wan", "title": "Quantum adiabatic optimization without heuristics", "comments": "41 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum adiabatic optimization (QAO) is performed using a time-dependent\nHamiltonian $H(s)$ with spectral gap $\\gamma(s)$. Assuming the existence of an\noracle $\\Gamma$ such that $\\gamma_\\min = \\Theta\\left(\\min_s\\Gamma(s)\\right)$,\nwe provide an algorithm that reliably performs QAO in time\n$O\\left(\\gamma_\\min^{-1}\\right)$ with $O\\left(\\log(\\gamma_\\min^{-1})\\right)$\noracle queries, where $\\gamma_\\min = \\min_s \\gamma(s)$. Our strategy is not\nheuristic and does not require guessing time parameters or annealing paths.\nRather, our algorithm naturally produces an annealing path such that $\\|dH/ds\\|\n\\approx \\gamma(s)$ and chooses its own runtime to be as close as possible to\noptimal while promising convergence to the ground state.\n  We then demonstrate the feasibility of this approach in practice by\nexplicitly constructing a gap oracle $\\Gamma$ for the problem of finding the\nminimum point $m = \\mathrm{argmin}_u W(u)$ of the cost function\n$W:\\mathcal{V}\\longrightarrow [0,1]$, restricting ourselves to computational\nbasis measurements and driving Hamiltonian $H(0)=I -\n|\\mathcal{V}|^{-1}\\sum_{u,v \\in \\mathcal{V}}\\vert{u}\\rangle\\langle{v}\\vert$.\nRequiring only that $W$ have a constant lower bound on its spectral gap and\nupper bound $\\kappa$ on its spectral ratio, our QAO algorithm returns $m$ with\nprobability $(1-\\epsilon)(1-e^{-1/\\epsilon})$ in time\n$\\widetilde{\\mathcal{O}}(\\epsilon^{-1}[\\sqrt{|\\mathcal{V}|} +\n(\\kappa-1)^{2/3}|\\mathcal{V}|^{2/3}])$. This achieves a quantum advantage for\nall $\\kappa$, and recovers Grover scaling up to logarithmic factors when\n$\\kappa \\approx 1$. We implement the algorithm as a subroutine in an\noptimization procedure that produces $m$ with exponentially small failure\nprobability and expected runtime\n$\\widetilde{\\mathcal{O}}(\\epsilon^{-1}[\\sqrt{|\\mathcal{V}|} +\n(\\kappa-1)^{2/3}|\\mathcal{V}|^{2/3}])$ even when $\\kappa$ is not known\nbeforehand.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 18:01:02 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 17:54:02 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Jarret", "Michael", ""], ["Lackey", "Brad", ""], ["Liu", "Aike", ""], ["Wan", "Kianna", ""]]}, {"id": "1810.04870", "submitter": "Aleksandar Ilic", "authors": "Aleksandar Ilic and Milan Basic", "title": "Path matrix and path energy of graphs", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$, we associate a path matrix $P$ whose $(i, j)$ entry\nrepresents the maximum number of vertex disjoint paths between the vertices $i$\nand $j$, with zeros on the main diagonal. In this note, we resolve four\nconjectures from [M. M. Shikare, P. P. Malavadkar, S. C. Patekar, I. Gutman,\n\\emph{On Path Eigenvalues and Path Energy of Graphs}, MATCH Commun. Math.\nComput. Chem. {\\bf 79} (2018), 387--398.] on the path energy of graphs and\nfinally present efficient $O(|E| |V|^3)$ algorithm for computing the path\nmatrix used for verifying computational results.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 07:22:38 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 01:05:11 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Ilic", "Aleksandar", ""], ["Basic", "Milan", ""]]}, {"id": "1810.05129", "submitter": "Pascal Maillard", "authors": "Louigi Addario-Berry, Pascal Maillard", "title": "The algorithmic hardness threshold for continuous random energy models", "comments": "22 pages, 2 figures. Minor additions and modifications in v2, minor\n  corrections in v3 to v5, to appear in Mathematical Statistics and Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cond-mat.dis-nn cond-mat.stat-mech cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove an algorithmic hardness result for finding low-energy states in the\nso-called \\emph{continuous random energy model (CREM)}, introduced by Bovier\nand Kurkova in 2004 as an extension of Derrida's \\emph{generalized random\nenergy model}. The CREM is a model of a random energy landscape $(X_v)_{v \\in\n\\{0,1\\}^N}$ on the discrete hypercube with built-in hierarchical structure, and\ncan be regarded as a toy model for strongly correlated random energy landscapes\nsuch as the family of $p$-spin models including the Sherrington--Kirkpatrick\nmodel. The CREM is parameterized by an increasing function $A:[0,1]\\to[0,1]$,\nwhich encodes the correlations between states.\n  We exhibit an \\emph{algorithmic hardness threshold} $x_*$, which is explicit\nin terms of $A$. More precisely, we obtain two results: First, we show that a\nrenormalization procedure combined with a greedy search yields for any\n$\\varepsilon > 0$ a linear-time algorithm which finds states $v \\in \\{0,1\\}^N$\nwith $X_v \\ge (x_*-\\varepsilon) N$. Second, we show that the value $x_*$ is\nessentially best-possible: for any $\\varepsilon > 0$, any algorithm which finds\nstates $v$ with $X_v \\ge (x_*+\\varepsilon)N$ requires exponentially many\nqueries in expectation and with high probability. We further discuss what\ninsights this study yields for understanding algorithmic hardness thresholds\nfor random instances of combinatorial optimization problems.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 17:21:55 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 11:54:35 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 20:53:42 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Addario-Berry", "Louigi", ""], ["Maillard", "Pascal", ""]]}, {"id": "1810.05143", "submitter": "Yang P. Liu", "authors": "Yang P. Liu, Sushant Sachdeva, Zejun Yu", "title": "Short Cycles via Low-Diameter Decompositions", "comments": "21 pages, SODA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present improved algorithms for short cycle decomposition of a graph.\nShort cycle decompositions were introduced in the recent work of Chu et al, and\nwere used to make progress on several questions in graph sparsification.\n  For all constants $\\delta \\in (0,1]$, we give an $O(mn^\\delta)$ time\nalgorithm that, given a graph $G,$ partitions its edges into cycles of length\n$O(\\log n)^\\frac{1}{\\delta}$, with $O(n)$ extra edges not in any cycle. This\ngives the first subquadratic, in fact almost linear time, algorithm achieving\npolylogarithmic cycle lengths. We also give an $m \\cdot \\exp(O(\\sqrt{\\log n}))$\ntime algorithm that partitions the edges of a graph into cycles of length\n$\\exp(O(\\sqrt{\\log n} \\log\\log n))$, with $O(n)$ extra edges not in any cycle.\nThis improves on the short cycle decomposition algorithms given in Chu et al in\nterms of all parameters, and is significantly simpler.\n  As a result, we obtain faster algorithms and improved guarantees for several\nproblems in graph sparsification -- construction of resistance sparsifiers,\ngraphical spectral sketches, degree preserving sparsifiers, and approximating\nthe effective resistances of all edges.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 17:46:48 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 19:03:04 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Liu", "Yang P.", ""], ["Sachdeva", "Sushant", ""], ["Yu", "Zejun", ""]]}, {"id": "1810.05163", "submitter": "Melissa Lynn", "authors": "Melissa Lynn", "title": "A Constraint Propagation Algorithm for Sums-of-Squares Formulas over the\n  Integers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sums-of-squares formulas over the integers have been studied extensively\nusing their equivalence to consistently signed intercalate matrices. This\nrepresentation, combined with combinatorial arguments, has been used to produce\nsums-of-squares formulas and to show that formulas of certain types cannot\nexist. In this paper, we introduce an algorithm that produces consistently\nsigned intercalate matrices, or proves their nonexistence, extending previous\nmethods beyond what is computationally feasible by hand.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 17:26:19 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Lynn", "Melissa", ""]]}, {"id": "1810.05245", "submitter": "Marco Molinaro", "authors": "Marco Molinaro", "title": "Stochastic $\\ell_p$ Load Balancing and Moment Problems via the\n  $L$-Function Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers stochastic optimization problems whose objective\nfunctions involve powers of random variables. For example, consider the classic\nStochastic lp Load Balancing Problem (SLBp): There are $m$ machines and $n$\njobs, and known independent random variables $Y_{ij}$ decribe the load incurred\non machine $i$ if we assign job $j$ to it. The goal is to assign each jobs to\nmachines in order to minimize the expected $l_p$-norm of the total load on the\nmachines. While convex relaxations represent one of the most powerful\nalgorithmic tools, in problems such as SLBp the main difficulty is to capture\nthe objective function in a way that only depends on each random variable\nseparately.\n  We show how to capture $p$-power-type objectives in such separable way by\nusing the $L$-function method, introduced by Lata{\\l}a to relate the moment of\nsums of random variables to the individual marginals. We show how this quickly\nleads to a constant-factor approximation for very general subset selection\nproblem with $p$-moment objective.\n  Moreover, we give a constant-factor approximation for SLBp, improving on the\nrecent $O(p/\\ln p)$-approximation of [Gupta et al., SODA 18]. Here the\napplication of the method is much more involved. In particular, we need to\nsharply connect the expected $l_p$-norm of a random vector with the $p$-moments\nof its marginals (machine loads), taking into account simultaneously the\ndifferent scales of the loads that are incurred by an unknown assignment.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 21:00:31 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Molinaro", "Marco", ""]]}, {"id": "1810.05303", "submitter": "Yan Gu", "authors": "Guy E. Blelloch and Yan Gu and Julian Shun and Yihan Sun", "title": "Parallelism in Randomized Incremental Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that many sequential randomized incremental algorithms\nare in fact parallel. We consider algorithms for several problems including\nDelaunay triangulation, linear programming, closest pair, smallest enclosing\ndisk, least-element lists, and strongly connected components.\n  We analyze the dependences between iterations in an algorithm, and show that\nthe dependence structure is shallow with high probability, or that by violating\nsome dependences the structure is shallow and the work is not increased\nsignificantly. We identify three types of algorithms based on their dependences\nand present a framework for analyzing each type. Using the framework gives\nwork-efficient polylogarithmic-depth parallel algorithms for most of the\nproblems that we study.\n  This paper shows the first incremental Delaunay triangulation algorithm with\noptimal work and polylogarithmic depth, which is an open problem for over 30\nyears. This result is important since most implementations of parallel Delaunay\ntriangulation use the incremental approach. Our results also improve bounds on\nstrongly connected components and least-elements lists, and significantly\nsimplify parallel algorithms for several problems.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 01:05:11 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Blelloch", "Guy E.", ""], ["Gu", "Yan", ""], ["Shun", "Julian", ""], ["Sun", "Yihan", ""]]}, {"id": "1810.05313", "submitter": "Daniel Lemire", "authors": "Daniel Lemire, Melissa E. O'Neill", "title": "Xorshift1024*, Xorshift1024+, Xorshift128+ and Xoroshiro128+ Fail\n  Statistical Tests for Linearity", "comments": null, "journal-ref": "Computational and Applied Mathematics 350, 2019", "doi": "10.1016/j.cam.2018.10.019", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  L'Ecuyer & Simard's Big Crush statistical test suite has revealed statistical\nflaws in many popular random number generators including Marsaglia's Xorshift\ngenerators. Vigna recently proposed some 64-bit variations on the Xorshift\nscheme that are further scrambled (i.e., Xorshift1024*, Xorshift1024+,\nXorshift128+, Xoroshiro128+). Unlike their unscrambled counterparts, they pass\nBig Crush when interleaving blocks of 32 bits for each 64-bit word (most\nsignificant, least significant, most significant, least significant, etc.). We\nreport that these scrambled generators systematically fail Big\nCrush---specifically the linear-complexity and matrix-rank tests that detect\nlinearity---when taking the 32 lowest-order bits in reverse order from each\n64-bit word.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 01:40:57 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 01:30:24 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Lemire", "Daniel", ""], ["O'Neill", "Melissa E.", ""]]}, {"id": "1810.05692", "submitter": "Aloni Cohen", "authors": "Aloni Cohen, Kobbi Nissim", "title": "Linear Program Reconstruction in Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We briefly report on a successful linear program reconstruction attack\nperformed on a production statistical queries system and using a real dataset.\nThe attack was deployed in test environment in the course of the Aircloak\nChallenge bug bounty program and is based on the reconstruction algorithm of\nDwork, McSherry, and Talwar. We empirically evaluate the effectiveness of the\nalgorithm and a related algorithm by Dinur and Nissim with various dataset\nsizes, error rates, and numbers of queries in a Gaussian noise setting.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 19:27:25 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 18:49:37 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Cohen", "Aloni", ""], ["Nissim", "Kobbi", ""]]}, {"id": "1810.05753", "submitter": "Adri\\'an G\\'omez-Brand\\'on", "authors": "Nieves R. Brisaboa and Travis Gagie and Adri\\'an G\\'omez-Brand\\'on and\n  Gonzalo Navarro and Jos\\'e R. Param\\'a", "title": "Relative compression of trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RCT, a new compact data structure to represent trajectories of\nobjects. It is based on a relative compression technique called Relative\nLempel-Ziv (RLZ), which compresses sequences by applying an LZ77 encoding with\nrespect to an artificial reference. Combined with $O(z)$-sized data structures\non the sequence of phrases that allows to solve trajectory and spatio-temporal\nqueries efficiently. We plan that RCT improves in compression and time\nperformance the previous compressed representations in the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 23:01:24 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Gagie", "Travis", ""], ["G\u00f3mez-Brand\u00f3n", "Adri\u00e1n", ""], ["Navarro", "Gonzalo", ""], ["Param\u00e1", "Jos\u00e9 R.", ""]]}, {"id": "1810.05759", "submitter": "Yuan Wang", "authors": "Yuan Wang and Bei Wang", "title": "Topological Inference of Manifolds with Boundary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of data points sampled from some underlying space, there are two\nimportant challenges in geometric and topological data analysis when dealing\nwith sampled data: reconstruction -- how to assemble discrete samples into\nglobal structures, and inference -- how to extract geometric and topological\ninformation from data that are high-dimensional, incomplete and noisy. Niyogi\net al. (2008) have shown that by constructing an offset of the samples using a\nsuitable offset parameter could provide reconstructions that preserve homotopy\ntypes therefore homology for densely sampled smooth submanifolds of Euclidean\nspace without boundary. Chazal et al. (2009) and Attali et al. (2013) have\nintroduced a parameterized set of sampling conditions that extend the results\nof Niyogi et al. to a large class of compact subsets of Euclidean space. Our\nwork tackles data problems that fill a gap between the work of Niyogi et al.\nand Chazal et al. In particular, we give a probabilistic notion of sampling\nconditions for manifolds with boundary that could not be handled by existing\ntheories. We also give stronger results that relate topological equivalence\nbetween the offset and the manifold as a deformation retract.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 23:25:33 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Wang", "Yuan", ""], ["Wang", "Bei", ""]]}, {"id": "1810.05830", "submitter": "Leslie Ann Goldberg", "authors": "Leslie Ann Goldberg and Mark Jerrum", "title": "Approximating Pairwise Correlations in the Ising Model", "comments": "To Appear in ACM ToCT", "journal-ref": "ACM Trans. Comput. Theory 11 (2019), no. 4, Art. 23", "doi": "10.1145/3337785", "report-no": null, "categories": "cs.DS cs.CC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Ising model, we consider the problem of estimating the covariance of\nthe spins at two specified vertices. In the ferromagnetic case, it is easy to\nobtain an additive approximation to this covariance by repeatedly sampling from\nthe relevant Gibbs distribution. However, we desire a multiplicative\napproximation, and it is not clear how to achieve this by sampling, given that\nthe covariance can be exponentially small. Our main contribution is a fully\npolynomial time randomised approximation scheme (FPRAS) for the covariance. We\nalso show that that the restriction to the ferromagnetic case is essential ---\nthere is no FPRAS for multiplicatively estimating the covariance of an\nantiferromagnetic Ising model unless RP = #P. In fact, we show that even\ndetermining the sign of the covariance is #P-hard in the antiferromagnetic\ncase.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 09:20:08 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 09:14:33 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Goldberg", "Leslie Ann", ""], ["Jerrum", "Mark", ""]]}, {"id": "1810.05957", "submitter": "Kent Quanrud", "authors": "Kent Quanrud", "title": "Approximating optimal transport with linear programs", "comments": "To appear in SOSA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the regime of bounded transportation costs, additive approximations for\nthe optimal transport problem are reduced (rather simply) to relative\napproximations for positive linear programs, resulting in faster additive\napproximation algorithms for optimal transport.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 03:02:25 GMT"}, {"version": "v2", "created": "Sun, 21 Oct 2018 18:15:35 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Quanrud", "Kent", ""]]}, {"id": "1810.06081", "submitter": "Nikhil Vyas", "authors": "Nikhil Vyas", "title": "Super Strong ETH is False for Random $k$-SAT", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been hypothesized that $k$-SAT is hard to solve for randomly chosen\ninstances near the \"critical threshold\", where the clause-to-variable ratio is\n$2^k \\ln 2-\\theta(1)$. Feige's hypothesis for $k$-SAT says that for all\nsufficiently large clause-to-variable ratios, random $k$-SAT cannot be refuted\nin polynomial time. It has also been hypothesized that the worst-case $k$-SAT\nproblem cannot be solved in $2^{n(1-\\omega_k(1)/k)}$ time, as multiple known\nalgorithmic paradigms (backtracking, local search and the polynomial method)\nonly yield an $2^{n(1-1/O(k))}$ time algorithm. This hypothesis has been called\nthe \"Super-Strong ETH\", modeled after the ETH and the Strong ETH.\n  Our main result is a randomized algorithm which refutes the Super-Strong ETH\nfor the case of random $k$-SAT, for any clause-to-variable ratio. Given any\nrandom $k$-SAT instance $F$ with $n$ variables and $m$ clauses, our algorithm\ndecides satisfiability for $F$ in $2^{n(1-\\Omega(\\log k)/k)}$ time, with high\nprobability. It turns out that a well-known algorithm from the literature on\nSAT algorithms does the job: the PPZ algorithm of Paturi, Pudlak, and Zane\n(1998).\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 19:12:39 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Vyas", "Nikhil", ""]]}, {"id": "1810.06230", "submitter": "Yikun Ban", "authors": "Yikun Ban, Xin Liu, Yitao Duan, Xue Liu, Wei Xu", "title": "No Place to Hide: Catching Fraudulent Entities in Tensors", "comments": "Proceedings of the 2019 World Wide Web Conference (WWW '19), May\n  13--17, 2019, San Francisco, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many approaches focus on detecting dense blocks in the tensor of multimodal\ndata to prevent fraudulent entities (e.g., accounts, links) from retweet\nboosting, hashtag hijacking, link advertising, etc. However, no existing method\nis effective to find the dense block if it only possesses high density on a\nsubset of all dimensions in tensors. In this paper, we novelly identify\ndense-block detection with dense-subgraph mining, by modeling a tensor into a\nweighted graph without any density information lost. Based on the weighted\ngraph, which we call information sharing graph (ISG), we propose an algorithm\nfor finding multiple densest subgraphs, D-Spot, that is faster (up to 11x\nfaster than the state-of-the-art algorithm) and can be computed in parallel. In\nan N-dimensional tensor, the entity group found by the ISG+D-Spot is at least\n1/2 of the optimum with respect to density, compared with the 1/N guarantee\nensured by competing methods. We use nine datasets to demonstrate that\nISG+D-Spot becomes new state-of-the-art dense-block detection method in terms\nof accuracy specifically for fraud detection.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 08:58:37 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 06:42:45 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 01:38:25 GMT"}, {"version": "v4", "created": "Sun, 21 Oct 2018 03:12:48 GMT"}, {"version": "v5", "created": "Sat, 23 Feb 2019 09:41:06 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Ban", "Yikun", ""], ["Liu", "Xin", ""], ["Duan", "Yitao", ""], ["Liu", "Xue", ""], ["Xu", "Wei", ""]]}, {"id": "1810.06267", "submitter": "Sagar Kale", "authors": "Sagar Kale", "title": "Small Space Stream Summary for Matroid Center", "comments": "Added explicit running times of the matroid center algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the matroid center problem, which generalizes the $k$-center problem, we\nneed to pick a set of centers that is an independent set of a matroid with rank\n$r$. We study this problem in streaming, where elements of the ground set\narrive in the stream. We first show that any randomized one-pass streaming\nalgorithm that computes a better than $\\Delta$-approximation for\npartition-matroid center must use $\\Omega(r^2)$ bits of space, where $\\Delta$\nis the aspect ratio of the metric and can be arbitrarily large. This shows a\nquadratic separation between matroid center and $k$-center, for which the\nDoubling algorithm gives an $8$-approximation using $O(k)$-space and one pass.\nTo complement this, we give a one-pass algorithm for matroid center that stores\nat most $O(r^2\\log(1/\\varepsilon)/\\varepsilon)$ points (viz., stream summary)\namong which a $(7+\\varepsilon)$-approximate solution exists, which can be found\nby brute force, or a $(17+\\varepsilon)$-approximation can be found with an\nefficient algorithm. If we are allowed a second pass, we can compute a\n$(3+\\varepsilon)$-approximation efficiently; this also achieves almost the\nknown-best approximation ratio (of $3+\\varepsilon$) with total running time of\n$O((nr + r^{3.5})\\log(1/\\varepsilon)/\\varepsilon + r^2(\\log\n\\Delta)/\\varepsilon)$, where $n$ is the number of input points.\n  We also consider the problem of matroid center with $z$ outliers and give a\none-pass algorithm that outputs a set of\n$O((r^2+rz)\\log(1/\\varepsilon)/\\varepsilon)$ points that contains a\n$(15+\\varepsilon)$-approximate solution. Our techniques extend to knapsack\ncenter and knapsack center with outliers in a straightforward way, and we get\nalgorithms that use space linear in the size of a largest feasible set (as\nopposed to quadratic space for matroid center).\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 10:53:14 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 10:07:43 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kale", "Sagar", ""]]}, {"id": "1810.06360", "submitter": "Arne Schmidt", "authors": "S\\'andor P. Fekete and Robert Gmyr and Sabrina Hugo and Phillip\n  Keldenich and Christian Scheffer and Arne Schmidt", "title": "CADbots: Algorithmic Aspects of Manipulating Programmable Matter with\n  Finite Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We contribute results for a set of fundamental problems in the context of\nprogrammable matter by presenting algorithmic methods for evaluating and\nmanipulating a collective of particles by a finite automaton that can neither\nstore significant amounts of data, nor perform complex computations, and is\nlimited to a handful of possible physical operations. We provide a toolbox for\ncarrying out fundamental tasks on a given arrangement of tiles, using the\narrangement itself as a storage device, similar to a higher-dimensional Turing\nmachine with geometric properties. Specific results include time- and\nspace-efficient procedures for bounding, counting, copying, reflecting,\nrotating or scaling a complex given shape.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 14:11:59 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Fekete", "S\u00e1ndor P.", ""], ["Gmyr", "Robert", ""], ["Hugo", "Sabrina", ""], ["Keldenich", "Phillip", ""], ["Scheffer", "Christian", ""], ["Schmidt", "Arne", ""]]}, {"id": "1810.06740", "submitter": "Josh Alman", "authors": "Josh Alman", "title": "An Illuminating Algorithm for the Light Bulb Problem", "comments": "10 pages. To appear in the 2nd Symposium on Simplicity in Algorithms\n  (SOSA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Light Bulb Problem is one of the most basic problems in data analysis.\nOne is given as input $n$ vectors in $\\{-1,1\\}^d$, which are all independently\nand uniformly random, except for a planted pair of vectors with inner product\nat least $\\rho \\cdot d$ for some constant $\\rho > 0$. The task is to find the\nplanted pair. The most straightforward algorithm leads to a runtime of\n$\\Omega(n^2)$. Algorithms based on techniques like Locality-Sensitive Hashing\nachieve runtimes of $n^{2 - O(\\rho)}$; as $\\rho$ gets small, these approach\nquadratic.\n  Building on prior work, we give a new algorithm for this problem which runs\nin time $O(n^{1.582} + nd),$ regardless of how small $\\rho$ is. This matches\nthe best known runtime due to Karppa et al. Our algorithm combines techniques\nfrom previous work on the Light Bulb Problem with the so-called `polynomial\nmethod in algorithm design,' and has a simpler analysis than previous work. Our\nalgorithm is also easily derandomized, leading to a deterministic algorithm for\nthe Light Bulb Problem with the same runtime of $O(n^{1.582} + nd),$ improving\nprevious results.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 22:51:50 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Alman", "Josh", ""]]}, {"id": "1810.06809", "submitter": "Yikun Ban", "authors": "Yikun Ban", "title": "On Finding Dense Subgraphs in Bipartite Graphs: Linear Algorithms", "comments": "Submitted to ICDE 2019, in Oct.2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting dense subgraphs from large graphs is a core component in many\napplications, ranging from social networks mining, bioinformatics. In this\npaper, we focus on mining dense subgraphs in a bipartite graph. The work is\nmotivated by the task of detecting synchronized behavior that can often be\nformulated as mining a bipartite graph formed by the source nodes (followers,\ncustomers) and target nodes (followees, products, etc.) for malicious patterns.\nWe introduce a new restricted biclique problem, Maximal Half Isolated Biclique\n(MHI Biclique), and show that the problem finds immediate applications in fraud\ndetection. We prove that, unlike many other biclique problems such as the\nmaximum edge biclique problem that are known to be NP-Complete, the MHI\nBiclique problem admits a linear time solution. We provide a novel algorithm\nS-tree, and its extension, S-forest, that solves the problem efficiently. We\nalso demonstrate that the algorithms are robust against deliberate camouflaging\nand other perturbations. Furthermore, our approach can automatically combine\nand prioritize multiple features, reducing the need for feature engineering\nwhile maintaining security against unseen attacks. Extensive experiments on\nseveral public and proprietary datasets demonstrate that S-tree/S-forest\noutperforms strong rivals across all configurations, becoming the new state of\nthe art in fraud detection.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 05:01:28 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 01:43:43 GMT"}, {"version": "v3", "created": "Sat, 3 Nov 2018 09:23:15 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Ban", "Yikun", ""]]}, {"id": "1810.06848", "submitter": "Marcin Pilipczuk", "authors": "Stefan Kratsch and Shaohua Li and D\\'aniel Marx and Marcin Pilipczuk\n  and Magnus Wahlstr\\\"om", "title": "Multi-budgeted directed cuts", "comments": "Full version of the paper presented at IPEC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study multi-budgeted variants of the classic minimum cut problem and graph\nseparation problems that turned out to be important in parameterized\ncomplexity: Skew Multicut and Directed Feedback Arc Set. In our generalization,\nwe assign colors $1,2,...,\\ell$ to some edges and give separate budgets\n$k_{1},k_{2},...,k_{\\ell}$. Let $E_{i}$ be the set of edges of color $i$. The\nsolution $C$ for the multi-budgeted variant of a graph separation problem not\nonly needs to satisfy the usual separation requirements, but also needs to\nsatisfy that $|C\\cap E_{i}|\\leq k_{i}$ for every $i\\in \\{1,...,\\ell\\}$.\n  Contrary to the classic minimum cut problem, the multi-budgeted variant turns\nout to be NP-hard even for $\\ell = 2$. We propose FPT algorithms parameterized\nby $k=k_{1}+...+k_{\\ell}$ for all three problems. To this end, we develop a\nbranching procedure for the multi-budgeted minimum cut problem that measures\nthe progress of the algorithm not by reducing $k$ as usual, by but elevating\nthe capacity of some edges and thus increasing the size of maximum\nsource-to-sink flow. Using the fact that a similar strategy is used to\nenumerate all important separators of a given size, we merge this process with\nthe flow-guided branching and show an FPT bound on the number of (appropriately\ndefined) important multi-budgeted separators. This allows us to extend our\nalgorithm to the Skew Multicut and Directed Feedback Arc Set problems.\n  Furthermore, we show connections of the multi-budgeted variants with weighted\nvariants of the directed cut problems and the Chain $\\ell$-SAT problem, whose\nparameterized complexity remains an open problem. We show that these problems\nadmit a bounded-in-parameter number of \"maximally pushed\" solutions (in a\nsimilar spirit as important separators are maximally pushed), giving somewhat\nweak evidence towards their tractability.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 07:35:19 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 17:18:49 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Kratsch", "Stefan", ""], ["Li", "Shaohua", ""], ["Marx", "D\u00e1niel", ""], ["Pilipczuk", "Marcin", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "1810.06864", "submitter": "Marcin Pilipczuk", "authors": "Marek Cygan and Pawe{\\l} Komosa and Daniel Lokshtanov and Micha{\\l}\n  Pilipczuk and Marcin Pilipczuk and Saket Saurabh and Magnus Wahlstr\\\"om", "title": "Randomized contractions meet lean decompositions", "comments": "v2: New co-author (Magnus) and improved results on vertex\n  unbreakability of bags, v3: final changes, including new abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show an algorithm that, given an $n$-vertex graph $G$ and a parameter $k$,\nin time $2^{O(k \\log k)} n^{O(1)}$ finds a tree decomposition of $G$ with the\nfollowing properties:\n  * every adhesion of the tree decomposition is of size at most $k$, and\n  * every bag of the tree decomposition is $(i,i)$-unbreakable in $G$ for every\n$1 \\leq i \\leq k$.\n  Here, a set $X \\subseteq V(G)$ is $(a,b)$-unbreakable in $G$ if for every\nseparation $(A,B)$ of order at most $b$ in $G$, we have $|A \\cap X| \\leq a$ or\n$|B \\cap X| \\leq a$. The resulting tree decomposition has arguably best\npossible adhesion size boundsand unbreakability guarantees. Furthermore, the\nparametric factor in the running time bound is significantly smaller than in\nprevious similar constructions. These improvements allow us to present\nparameterized algorithms for Minimum Bisection, Steiner Cut, and Steiner\nMulticut with improved parameteric factor in the running time bound.\n  The main technical insight is to adapt the notion of lean decompositions of\nThomas and the subsequent construction algorithm of Bellenbaum and Diestel to\nthe parameterized setting.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 08:02:45 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 09:27:09 GMT"}, {"version": "v3", "created": "Sat, 26 Sep 2020 06:43:17 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Cygan", "Marek", ""], ["Komosa", "Pawe\u0142", ""], ["Lokshtanov", "Daniel", ""], ["Pilipczuk", "Micha\u0142", ""], ["Pilipczuk", "Marcin", ""], ["Saurabh", "Saket", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "1810.07259", "submitter": "Manoj Gupta", "authors": "Jayesh Choudhari and Manoj Gupta and Shivdutt Sharma", "title": "Nearly Optimal Space Efficient Algorithm for Depth First Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a space-efficient algorithm for performing depth-first search\ntraversal(DFS) of a graph in $O(m+n\\log^* n)$ time using $O(n)$ bits of space.\nWhile a normal DFS algorithm results in a DFS-tree (in case the graph is\nconnected), our space bounds do not permit us even to store such a tree.\nHowever, our algorithm correctly outputs all edges of the DFS-tree.\n  The previous best algorithm (which used $O(n)$ working space) took $O(m \\log\nn)$ time (Asano, Izumi, Kiyomi, Konagaya, Ono, Otachi, Schweitzer, Tarui,\nUehara (ISAAC 2014) and Elmasry, Hagerup, Krammer (STACS 2015)). The main open\nquestion left behind in this area was to design faster algorithm for DFS using\n$O(n)$ bits of space. Our algorithm answers this open question as it has a\nnearly optimal running time (as the DFS takes $O(m+n)$ time even if there is no\nspace restriction).\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 20:27:46 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Choudhari", "Jayesh", ""], ["Gupta", "Manoj", ""], ["Sharma", "Shivdutt", ""]]}, {"id": "1810.07275", "submitter": "Francesco Pelosin", "authors": "Francesco Pelosin", "title": "Graph Compression Using The Regularity Method", "comments": "37 pages, M.Sc. Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We are living in a world which is getting more and more interconnected and,\nas physiological effect, the interaction between the entities produces more and\nmore information. This high throughput generation calls for techniques able to\nreduce the volume of the data, but still able to preserve the carried\nknowledge. Data compression and summarization techniques are one of the\npossible approaches to face such problems. The aim of this thesis is to devise\na new pipeline for compressing and decompressing a graph by exploiting\nSzemer\\'edi's Regularity Lemma. In particular, it has been developed a\nprocedure called CoDec (Compression-Decompression) which is based on Alon et\nal's constructive version of the Regularity Lemma. We provide an extensive\nexperimental evaluation to measure how robust is the framework as we both\ncorrupt the structures carried by the graph and add noisy edges among them. The\nexperimental results make us confident that our method can be effectively used\nas a graph compression technique able to preserve meaningful patterns of the\noriginal graph.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 15:43:38 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 15:16:57 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Pelosin", "Francesco", ""]]}, {"id": "1810.07323", "submitter": "Rodrigo de Lamare", "authors": "M. Kaloorazi and R. C. de Lamare", "title": "Compressed Randomized UTV Decompositions for Low-Rank Approximations and\n  Big Data Applications", "comments": "13 pages, 3 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix approximations play a fundamental role in numerical linear\nalgebra and signal processing applications. This paper introduces a novel\nrank-revealing matrix decomposition algorithm termed Compressed Randomized UTV\n(CoR-UTV) decomposition along with a CoR-UTV variant aided by the power method\ntechnique. CoR-UTV is primarily developed to compute an approximation to a\nlow-rank input matrix by making use of random sampling schemes. Given a large\nand dense matrix of size $m\\times n$ with numerical rank $k$, where $k \\ll\n\\text{min} \\{m,n\\}$, CoR-UTV requires a few passes over the data, and runs in\n$O(mnk)$ floating-point operations. Furthermore, CoR-UTV can exploit modern\ncomputational platforms and, consequently, can be optimized for maximum\nefficiency. CoR-UTV is simple and accurate, and outperforms reported\nalternative methods in terms of efficiency and accuracy. Simulations with\nsynthetic data as well as real data in image reconstruction and robust\nprincipal component analysis applications support our claims.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 21:33:04 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Kaloorazi", "M.", ""], ["de Lamare", "R. C.", ""]]}, {"id": "1810.07422", "submitter": "Florin Manea", "authors": "Pawe{\\l} Gawrychowski, Florin Manea, Rados{\\l}aw Serafin", "title": "Fast and Longest Rollercoasters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For $k\\geq 3$, a k-rollercoaster is a sequence of numbers whose every maximal\ncontiguous subsequence, that is increasing or decreasing, has length at least\n$k$; $3$-rollercoasters are called simply rollercoasters. Given a sequence of\ndistinct numbers, we are interested in computing its maximum-length (not\nnecessarily contiguous) subsequence that is a $k$-rollercoaster. Biedl et al.\n[ICALP 2018] have shown that each sequence of $n$ distinct real numbers\ncontains a rollercoaster of length at least $\\lceil n/2\\rceil$ for $n>7$, and\nthat a longest rollercoaster contained in such a sequence can be computed in\n$O(n\\log n)$-time. They have also shown that every sequence of $n\\geq\n(k-1)^2+1$ distinct real numbers contains a $k$-rollercoaster of length at\nleast $\\frac{n}{2(k-1)}-\\frac{3k}{2}$, and gave an $O(nk\\log n)$-time algorithm\ncomputing a longest $k$-rollercoaster in a sequence of length $n$.\n  In this paper, we give an $O(nk^2)$-time algorithm computing the length of a\nlongest $k$-rollercoaster contained in a sequence of $n$ distinct real numbers;\nhence, for constant $k$, our algorithm computes the length of a longest\n$k$-rollercoaster in optimal linear time. The algorithm can be easily adapted\nto output the respective $k$-rollercoaster. In particular, this improves the\nresults of Biedl et al. [ICALP 2018], by showing that a longest rollercoaster\ncan be computed in optimal linear time. We also present an algorithm computing\nthe length of a longest $k$-rollercoaster in $O(n \\log^2 n)$-time, that is,\nsubquadratic even for large values of $k\\leq n$. Again, the rollercoaster can\nbe easily retrieved. Finally, we show an $\\Omega(n \\log k)$ lower bound for the\nnumber of comparisons in any comparison-based algorithm computing the length of\na longest $k$-rollercoaster.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 08:12:01 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 14:13:28 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Manea", "Florin", ""], ["Serafin", "Rados\u0142aw", ""]]}, {"id": "1810.07508", "submitter": "Anupam Gupta", "authors": "Niv Buchbinder, Anupam Gupta, Marco Molinaro, Joseph (Seffi) Naor", "title": "$k$-Servers with a Smile: Online Algorithms via Projections", "comments": "To appear in the SODA 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the $k$-server problem on trees and HSTs. We give an algorithm\nbased on Bregman projections. This algorithm has a competitive ratios that\nmatch some of the recent results given by Bubeck et al. (STOC 2018), whose\nalgorithm was based on mirror-descent-based continuous dynamics prescribed via\na differential inclusion.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 12:57:51 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 19:36:52 GMT"}, {"version": "v3", "created": "Thu, 20 Dec 2018 02:05:54 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Buchbinder", "Niv", "", "Seffi"], ["Gupta", "Anupam", "", "Seffi"], ["Molinaro", "Marco", "", "Seffi"], ["Joseph", "", "", "Seffi"], ["Naor", "", ""]]}, {"id": "1810.07510", "submitter": "Kilian Grage", "authors": "Kilian Grage, Klaus Jansen, Kim Manuel Klein", "title": "An EPTAS for machine scheduling with bag-constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine scheduling is a fundamental optimization problem in computer science.\nThe task of scheduling a set of jobs on a given number of machines and\nminimizing the makespan is well studied and among other results, we know that\nEPTAS's for machine scheduling on identical machines exist. Das and Wiese\ninitiated the research on a generalization of makespan minimization, that\nincludes so called bag-constraints. In this variation of machine scheduling the\ngiven set of jobs is partitioned into subsets, so called bags. Given this\npartition a schedule is only considered feasible when on any machine there is\nat most one job from each bag.\n  Das and Wiese showed that this variant of machine scheduling admits a PTAS.\nWe will improve on this result by giving the first EPTAS for the machine\nscheduling problem with bag-constraints. We achieve this result by using new\ninsights on this problem and restrictions given by the bag-constraints. We show\nthat, to gain an approximate solution, we can relax the bag-constraints and\nignore some of the restrictions. Our EPTAS uses a new instance transformation\nthat will allow us to schedule large and small jobs independently of each other\nfor a majority of bags. We also show that it is sufficient to respect the\nbag-constraint only among a constant number of bags, when scheduling large\njobs. With these observations our algorithm will allow for some conflicts when\ncomputing a schedule and we show how to repair the schedule in polynomial-time\nby swapping certain jobs around.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 13:00:20 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Grage", "Kilian", ""], ["Jansen", "Klaus", ""], ["Klein", "Kim Manuel", ""]]}, {"id": "1810.07585", "submitter": "Georgia Avarikioti", "authors": "Georgia Avarikioti, Gerrit Janssen, Yuyi Wang, and Roger Wattenhofer", "title": "Payment Network Design with Fees", "comments": "This paper was accepted at CBT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Payment channels are the most prominent solution to the blockchain\nscalability problem. We introduce the problem of network design with fees for\npayment channels from the perspective of a Payment Service Provider (PSP).\nGiven a set of transactions, we examine the optimal graph structure and fee\nassignment to maximize the PSP's profit. A customer prefers to route\ntransactions through the PSP's network if the cheapest path from sender to\nreceiver is financially interesting, i.e., if the path costs less than the\nblockchain fee. When the graph structure is a tree, and the PSP facilitates all\ntransactions, the problem can be formulated as a linear program. For a path\ngraph, we present a polynomial time algorithm to assign optimal fees. We also\nshow that the star network, where the center is an additional node acting as an\nintermediary, is a near-optimal solution to the network design problem.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 14:36:57 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Avarikioti", "Georgia", ""], ["Janssen", "Gerrit", ""], ["Wang", "Yuyi", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1810.07603", "submitter": "Georgia Avarikioti", "authors": "Georgia Avarikioti, Yuyi Wang, Roger Wattenhofer", "title": "Algorithmic Blockchain Channel Design", "comments": "This paper is accepted in ISAAC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Payment networks, also known as channels, are a most promising solution to\nthe throughput problem of cryptocurrencies. In this paper we study the design\nof capital-efficient payment networks, offline as well as online variants. We\nwant to know how to compute an efficient payment network topology, how capital\nshould be assigned to the individual edges, and how to decide which\ntransactions to accept. Towards this end, we present a flurry of interesting\nresults, basic but generally applicable insights on the one hand, and hardness\nresults and approximation algorithms on the other hand.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 15:14:25 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Avarikioti", "Georgia", ""], ["Wang", "Yuyi", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1810.07717", "submitter": "Carson Kent", "authors": "Jose Blanchet, Arun Jambulapati, Carson Kent and Aaron Sidford", "title": "Towards Optimal Running Times for Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we provide faster algorithms for approximating the optimal\ntransport distance, e.g. earth mover's distance, between two discrete\nprobability distributions $\\mu, \\nu \\in \\Delta^n$. Given a cost function $C :\n[n] \\times [n] \\to \\mathbb{R}_{\\geq 0}$ where $C(i,j) \\leq 1$ quantifies the\npenalty of transporting a unit of mass from $i$ to $j$, we show how to compute\na coupling $X$ between $r$ and $c$ in time $\\widetilde{O}\\left(n^2 /\\epsilon\n\\right)$ whose expected transportation cost is within an additive $\\epsilon$ of\noptimal. This improves upon the previously best known running time for this\nproblem of $\\widetilde{O}\\left(\\text{min}\\left\\{ n^{9/4}/\\epsilon,\nn^2/\\epsilon^2 \\right\\}\\right)$.\n  We achieve our results by providing reductions from optimal transport to\ncanonical optimization problems for which recent algorithmic efforts have\nprovided nearly-linear time algorithms. Leveraging nearly linear time\nalgorithms for solving packing linear programs and for solving the matrix\nbalancing problem, we obtain two separate proofs of our stated running time.\nFurther, one of our algorithms is easily parallelized and can be implemented\nwith depth $\\widetilde{O}(1/\\epsilon)$.\n  Moreover, we show that further algorithmic improvements to our result would\nbe surprising in the sense that any improvement would yield an $o(n^{2.5})$\nalgorithm for \\textit{maximum cardinality bipartite matching}, for which\ncurrently the only known algorithms for achieving such a result are based on\nfast-matrix multiplication.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 18:08:30 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 07:01:38 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2020 07:20:12 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Blanchet", "Jose", ""], ["Jambulapati", "Arun", ""], ["Kent", "Carson", ""], ["Sidford", "Aaron", ""]]}, {"id": "1810.07800", "submitter": "Sarah Berkemer", "authors": "Sarah J. Berkemer, Christian H\\\"oner zu Siederdissen, Peter F. Stadler", "title": "Alignments as Compositional Structures", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Alignments, i.e., position-wise comparisons of two or more strings or ordered\nlists are of utmost practical importance in computational biology and a host of\nother fields, including historical linguistics and emerging areas of research\nin the Digital Humanities. The problem is well-known to be computationally hard\nas soon as the number of input strings is not bounded. Due to its prac- tical\nimportance, a huge number of heuristics have been devised, which have proved\nvery successful in a wide range of applications. Alignments nevertheless have\nreceived hardly any attention as formal, mathematical structures. Here, we\nfocus on the compositional aspects of alignments, which underlie most algo-\nrithmic approaches to computing alignments. We also show that the concepts\nnaturally generalize to finite partially ordered sets and partial maps between\nthem that in some sense preserve the partial orders.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 13:53:27 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Berkemer", "Sarah J.", ""], ["Siederdissen", "Christian H\u00f6ner zu", ""], ["Stadler", "Peter F.", ""]]}, {"id": "1810.07816", "submitter": "Joseph Cheriyan", "authors": "Joe Cheriyan, Jack Dippel, Fabrizio Grandoni, Arindam Khan, Vishnu V.\n  Narayan", "title": "The Matching Augmentation Problem: A $\\frac74$-Approximation Algorithm", "comments": "[v2]: based on a thorough journal review, the submission has been\n  revised to improve the exposition, though the results and proofs are the same\n  (modulo expository improvements); there are several changes, mostly in\n  sections 4, 5, 6; more informal discussion (of the credit scheme) has been\n  added in sections 5.2 and 5.3; nevertheless, sections 4.4 and 5.3 need\n  patience and effort", "journal-ref": "Mathematical Programming (2019)", "doi": "10.1007/s10107-019-01394-z", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a $\\frac74$ approximation algorithm for the matching augmentation\nproblem (MAP): given a multi-graph with edges of cost either zero or one such\nthat the edges of cost zero form a matching, find a 2-edge connected spanning\nsubgraph (2-ECSS) of minimum cost.\n  We first present a reduction of any given MAP instance to a collection of\nwell-structured MAP instances such that the approximation guarantee is\npreserved. Then we present a $\\frac74$ approximation algorithm for a\nwell-structured MAP instance. The algorithm starts with a min-cost 2-edge cover\nand then applies ear-augmentation steps. We analyze the cost of the\near-augmentations using an approach similar to the one proposed by Vempala and\nVetta for the (unweighted) min-size 2-ECSS problem (`Factor 4/3 approximations\nfor minimum 2-connected subgraphs,' APPROX 2000, LNCS 1913, pp.262-273).\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 21:55:38 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 18:03:18 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Cheriyan", "Joe", ""], ["Dippel", "Jack", ""], ["Grandoni", "Fabrizio", ""], ["Khan", "Arindam", ""], ["Narayan", "Vishnu V.", ""]]}, {"id": "1810.07825", "submitter": "David Eppstein", "authors": "David Eppstein and Bruce Reed", "title": "Finding Maximal Sets of Laminar 3-Separators in Planar Graphs in Linear\n  Time", "comments": "25 pages, 10 figures. To appear in the Proceedings of the 30th\n  ACM-SIAM Symposium on Discrete Algorithms (SODA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider decomposing a 3-connected planar graph $G$ using laminar\nseparators of size three. We show how to find a maximal set of laminar\n3-separators in such a graph in linear time. We also discuss how to find\nmaximal laminar set of 3-separators from special families. For example we\ndiscuss non-trivial cuts, ie. cuts which split $G$ into two components of size\nat least two. For any vertex $v$, we also show how to find a maximal set of\n3-separators disjoint from $v$ which are laminar and satisfy: every vertex in a\nseparator $X$ has two neighbours not in the unique component of $G-X$\ncontaining $v$. In all cases, we show how to construct a corresponding tree\ndecomposition of adhesion three. Our new algorithms form an important component\nof recent methods for finding disjoint paths in nonplanar graphs.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 22:46:01 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Eppstein", "David", ""], ["Reed", "Bruce", ""]]}, {"id": "1810.07852", "submitter": "Xiangyu Guo", "authors": "Xiangyu Guo, Shi Li", "title": "Distributed $k$-Clustering for Data with Heavy Noise", "comments": "slightly improve the comm cost over the version accepted into\n  NeurIPS'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the $k$-center/median/means clustering with\noutliers problems (or the $(k, z)$-center/median/means problems) in the\ndistributed setting. Most previous distributed algorithms have their\ncommunication costs linearly depending on $z$, the number of outliers. Recently\nGuha et al. overcame this dependence issue by considering bi-criteria\napproximation algorithms that output solutions with $2z$ outliers. For the case\nwhere $z$ is large, the extra $z$ outliers discarded by the algorithms might be\ntoo large, considering that the data gathering process might be costly. In this\npaper, we improve the number of outliers to the best possible $(1+\\epsilon)z$,\nwhile maintaining the $O(1)$-approximation ratio and independence of\ncommunication cost on $z$. The problems we consider include the $(k, z)$-center\nproblem, and $(k, z)$-median/means problems in Euclidean metrics.\nImplementation of the our algorithm for $(k, z)$-center shows that it\noutperforms many previous algorithms, both in terms of the communication cost\nand quality of the output solution.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 01:04:14 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 02:41:28 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Guo", "Xiangyu", ""], ["Li", "Shi", ""]]}, {"id": "1810.07896", "submitter": "Zhao Song", "authors": "Michael B. Cohen, Yin Tat Lee, Zhao Song", "title": "Solving Linear Programs in the Current Matrix Multiplication Time", "comments": "STOC 2019, JACM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper shows how to solve linear programs of the form $\\min_{Ax=b,x\\geq0}\nc^\\top x$ with $n$ variables in time\n$$O^*((n^{\\omega}+n^{2.5-\\alpha/2}+n^{2+1/6}) \\log(n/\\delta))$$ where $\\omega$\nis the exponent of matrix multiplication, $\\alpha$ is the dual exponent of\nmatrix multiplication, and $\\delta$ is the relative accuracy. For the current\nvalue of $\\omega\\sim2.37$ and $\\alpha\\sim0.31$, our algorithm takes\n$O^*(n^{\\omega} \\log(n/\\delta))$ time. When $\\omega = 2$, our algorithm takes\n$O^*(n^{2+1/6} \\log(n/\\delta))$ time.\n  Our algorithm utilizes several new concepts that we believe may be of\nindependent interest:\n  $\\bullet$ We define a stochastic central path method.\n  $\\bullet$ We show how to maintain a projection matrix\n$\\sqrt{W}A^{\\top}(AWA^{\\top})^{-1}A\\sqrt{W}$ in sub-quadratic time under\n$\\ell_{2}$ multiplicative changes in the diagonal matrix $W$.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 04:37:51 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 00:42:30 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 02:38:41 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Cohen", "Michael B.", ""], ["Lee", "Yin Tat", ""], ["Song", "Zhao", ""]]}, {"id": "1810.07903", "submitter": "Zhihao Gavin Tang", "authors": "Zhiyi Huang, Binghui Peng, Zhihao Gavin Tang, Runzhou Tao, Xiaowei Wu\n  and Yuhao Zhang", "title": "Tight Competitive Ratios of Classic Matching Algorithms in the Fully\n  Online Model", "comments": "To appear SODA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huang et al.~(STOC 2018) introduced the fully online matching problem, a\ngeneralization of the classic online bipartite matching problem in that it\nallows all vertices to arrive online and considers general graphs. They showed\nthat the ranking algorithm by Karp et al.~(STOC 1990) is strictly better than\n$0.5$-competitive and the problem is strictly harder than the online bipartite\nmatching problem in that no algorithms can be $(1-1/e)$-competitive.\n  This paper pins down two tight competitive ratios of classic algorithms for\nthe fully online matching problem. For the fractional version of the problem,\nwe show that a natural instantiation of the water-filling algorithm is\n$2-\\sqrt{2} \\approx 0.585$-competitive, together with a matching hardness\nresult. Interestingly, our hardness result applies to arbitrary algorithms in\nthe edge-arrival models of the online matching problem, improving the\nstate-of-art $\\frac{1}{1+\\ln 2} \\approx 0.5906$ upper bound. For integral\nalgorithms, we show a tight competitive ratio of $\\approx 0.567$ for the\nranking algorithm on bipartite graphs, matching a hardness result by Huang et\nal. (STOC 2018).\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 05:30:48 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Huang", "Zhiyi", ""], ["Peng", "Binghui", ""], ["Tang", "Zhihao Gavin", ""], ["Tao", "Runzhou", ""], ["Wu", "Xiaowei", ""], ["Zhang", "Yuhao", ""]]}, {"id": "1810.08004", "submitter": "Hossein Vahidi", "authors": "Saeed Akhoondian Amiri, Alexandru Popa, Mohammad Roghani, Golnoosh\n  Shahkarami, Reza Soltani, Hossein Vahidi", "title": "Complexity of Computing the Anti-Ramsey Numbers for Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The anti-Ramsey numbers are a fundamental notion in graph theory, introduced\nin 1978, by Erd\\\" os, Simonovits and S\\' os. For given graphs $G$ and $H$ the\n\\emph{anti-Ramsey number} $\\textrm{ar}(G,H)$ is defined to be the maximum\nnumber $k$ such that there exists an assignment of $k$ colors to the edges of\n$G$ in which every copy of $H$ in $G$ has at least two edges with the same\ncolor.\n  There are works on the computational complexity of the problem when $H$ is a\nstar. Along this line of research, we study the complexity of computing the\nanti-Ramsey number $\\textrm{ar}(G,P_k)$, where $P_k$ is a path of length $k$.\nFirst, we observe that when $k = \\Omega(n)$, the problem is hard; hence, the\nchallenging part is the computational complexity of the problem when $k$ is a\nfixed constant.\n  We provide a characterization of the problem for paths of constant length.\nOur first main contribution is to prove that computing $\\textrm{ar}(G,P_k)$ for\nevery integer $k>2$ is NP-hard. We obtain this by providing several structural\nproperties of such coloring in graphs. We investigate further and show that\napproximating $\\textrm{ar}(G,P_3)$ to a factor of $n^{-1/2 - \\epsilon}$ is hard\nalready in $3$-partite graphs, unless P=NP. We also study the exact complexity\nof the precolored version and show that there is no subexponential algorithm\nfor the problem unless ETH fails for any fixed constant $k$.\n  Given the hardness of approximation and parametrization of the problem, it is\nnatural to study the problem on restricted graph families. We introduce the\nnotion of color connected coloring and employing this structural property. We\nobtain a linear time algorithm to compute $\\textrm{ar}(G,P_k)$, for every\ninteger $k$, when the host graph, $G$, is a tree.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 11:59:02 GMT"}, {"version": "v2", "created": "Sat, 9 Mar 2019 12:47:48 GMT"}, {"version": "v3", "created": "Fri, 4 Oct 2019 20:40:36 GMT"}, {"version": "v4", "created": "Thu, 2 Jul 2020 20:36:42 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Amiri", "Saeed Akhoondian", ""], ["Popa", "Alexandru", ""], ["Roghani", "Mohammad", ""], ["Shahkarami", "Golnoosh", ""], ["Soltani", "Reza", ""], ["Vahidi", "Hossein", ""]]}, {"id": "1810.08054", "submitter": "Ryan Rogers", "authors": "Marco Gaboardi and Ryan Rogers and Or Sheffet", "title": "Locally Private Mean Estimation: Z-test and Tight Confidence Intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides tight upper- and lower-bounds for the problem of mean\nestimation under $\\epsilon$-differential privacy in the local model, when the\ninput is composed of $n$ i.i.d. drawn samples from a normal distribution with\nvariance $\\sigma$. Our algorithms result in a $(1-\\beta)$-confidence interval\nfor the underlying distribution's mean $\\mu$ of length $\\tilde O\\left(\n\\frac{\\sigma \\sqrt{\\log(\\frac 1 \\beta)}}{\\epsilon\\sqrt n} \\right)$. In\naddition, our algorithms leverage binary search using local differential\nprivacy for quantile estimation, a result which may be of separate interest.\nMoreover, we prove a matching lower-bound (up to poly-log factors), showing\nthat any one-shot (each individual is presented with a single query) local\ndifferentially private algorithm must return an interval of length\n$\\Omega\\left( \\frac{\\sigma\\sqrt{\\log(1/\\beta)}}{\\epsilon\\sqrt{n}}\\right)$.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 13:48:58 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 01:20:58 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 18:45:51 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Gaboardi", "Marco", ""], ["Rogers", "Ryan", ""], ["Sheffet", "Or", ""]]}, {"id": "1810.08109", "submitter": "Christoph D\\\"urr", "authors": "Spyros Angelopoulos and Christoph D\\\"urr and Shendan Jin", "title": "Best-of-two-worlds analysis of online search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In search problems, a mobile searcher seeks to locate a target that hides in\nsome unknown position of the environment. Such problems are typically\nconsidered to be of an on-line nature, in that the input is unknown to the\nsearcher, and the performance of a search strategy is usually analyzed by means\nof the standard framework of the competitive ratio, which compares the cost\nincurred by the searcher to an optimal strategy that knows the location of the\ntarget. However, one can argue that even for simple search problems,\ncompetitive analysis fails to distinguish between strategies which,\nintuitively, should have different performance in practice.\n  Motivated by the above, in this work we introduce and study measures\nsupplementary to competitive analysis in the context of search problems. In\nparticular, we focus on the well-known problem of linear search, informally\nknown as the cow-path problem, for which there is an infinite number of\nstrategies that achieve an optimal competitive ratio equal to 9. We propose a\nmeasure that reflects the rate at which the line is being explored by the\nsearcher, and which can be seen as an extension of the bijective ratio over an\nuncountable set of requests. Using this measure we show that a natural strategy\nthat explores the line aggressively is optimal among all 9-competitive\nstrategies. This provides, in particular, a strict separation from the\ncompetitively optimal doubling strategy, which is much more conservative in\nterms of exploration. We also provide evidence that this aggressiveness is\nrequisite for optimality, by showing that any optimal strategy must mimic the\naggressive strategy in its first few explorations.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 15:30:39 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Angelopoulos", "Spyros", ""], ["D\u00fcrr", "Christoph", ""], ["Jin", "Shendan", ""]]}, {"id": "1810.08171", "submitter": "Hongyang Zhang", "authors": "Maria-Florina Balcan and Yi Li and David P. Woodruff and Hongyang\n  Zhang", "title": "Testing Matrix Rank, Optimally", "comments": "51 pages. To appear in SODA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for the problem of testing if a matrix $A \\in F^{n \\times n}$\nhas rank at most $d$, or requires changing an $\\epsilon$-fraction of entries to\nhave rank at most $d$, there is a non-adaptive query algorithm making\n$\\widetilde{O}(d^2/\\epsilon)$ queries. Our algorithm works for any field $F$.\nThis improves upon the previous $O(d^2/\\epsilon^2)$ bound (SODA'03), and\nbypasses an $\\Omega(d^2/\\epsilon^2)$ lower bound of (KDD'14) which holds if the\nalgorithm is required to read a submatrix. Our algorithm is the first such\nalgorithm which does not read a submatrix, and instead reads a carefully\nselected non-adaptive pattern of entries in rows and columns of $A$. We\ncomplement our algorithm with a matching query complexity lower bound for\nnon-adaptive testers over any field. We also give tight bounds of\n$\\widetilde{\\Theta}(d^2)$ queries in the sensing model for which query access\ncomes in the form of $\\langle X_i, A\\rangle:=tr(X_i^\\top A)$; perhaps\nsurprisingly these bounds do not depend on $\\epsilon$.\n  We next develop a novel property testing framework for testing numerical\nproperties of a real-valued matrix $A$ more generally, which includes the\nstable rank, Schatten-$p$ norms, and SVD entropy. Specifically, we propose a\nbounded entry model, where $A$ is required to have entries bounded by $1$ in\nabsolute value. We give upper and lower bounds for a wide range of problems in\nthis model, and discuss connections to the sensing model above.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 17:24:52 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Li", "Yi", ""], ["Woodruff", "David P.", ""], ["Zhang", "Hongyang", ""]]}, {"id": "1810.08276", "submitter": "Rudini Sampaio", "authors": "Rafael Araujo, Eurinardo Costa, Sulamita Klein, Rudini Sampaio,\n  Ueverton S. Souza", "title": "FPT algorithms to recognize well covered graphs", "comments": "15 pages, 2 figures", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 21 no.\n  1, ICGT 2018 (April 2, 2019) dmtcs:5342", "doi": "10.23638/DMTCS-21-1-3", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$, let $vc(G)$ and $vc^+(G)$ be the sizes of a minimum and a\nmaximum minimal vertex covers of $G$, respectively. We say that $G$ is well\ncovered if $vc(G)=vc^+(G)$ (that is, all minimal vertex covers have the same\nsize). Determining if a graph is well covered is a coNP-complete problem. In\nthis paper, we obtain $O^*(2^{vc})$-time and $O^*(1.4656^{vc^+})$-time\nalgorithms to decide well coveredness, improving results of Boria et. al.\n(2015). Moreover, using crown decomposition, we show that such problems admit\nkernels having linear number of vertices. In 2018, Alves et. al. (2018) proved\nthat recognizing well covered graphs is coW[2]-hard when the independence\nnumber $\\alpha(G)=n-vc(G)$ is the parameter. Contrasting with such\ncoW[2]-hardness, we present an FPT algorithm to decide well coveredness when\n$\\alpha(G)$ and the degeneracy of the input graph $G$ are aggregate parameters.\nFinally, we use the primeval decomposition technique to obtain a linear time\nalgorithm for extended $P_4$-laden graphs and $(q,q-4)$-graphs, which is FPT\nparameterized by $q$, improving results of Klein et al (2013).\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 21:01:47 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 03:42:41 GMT"}, {"version": "v3", "created": "Wed, 20 Mar 2019 20:31:28 GMT"}, {"version": "v4", "created": "Mon, 1 Apr 2019 15:23:34 GMT"}], "update_date": "2021-01-04", "authors_parsed": [["Araujo", "Rafael", ""], ["Costa", "Eurinardo", ""], ["Klein", "Sulamita", ""], ["Sampaio", "Rudini", ""], ["Souza", "Ueverton S.", ""]]}, {"id": "1810.08345", "submitter": "Zhao Song", "authors": "Rasmus Kyng, Zhao Song", "title": "A Matrix Chernoff Bound for Strongly Rayleigh Distributions and Spectral\n  Sparsifiers from a few Random Spanning Trees", "comments": "FOCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Strongly Rayleigh distributions are a class of negatively dependent\ndistributions of binary-valued random variables [Borcea, Branden, Liggett JAMS\n09]. Recently, these distributions have played a crucial role in the analysis\nof algorithms for fundamental graph problems, e.g. Traveling Salesman Problem\n[Gharan, Saberi, Singh FOCS 11]. We prove a new matrix Chernoff bound for\nStrongly Rayleigh distributions.\n  As an immediate application, we show that adding together the Laplacians of\n$\\epsilon^{-2} \\log^2 n$ random spanning trees gives an $(1\\pm \\epsilon)$\nspectral sparsifiers of graph Laplacians with high probability. Thus, we\npositively answer an open question posed in [Baston, Spielman, Srivastava, Teng\nJACM 13]. Our number of spanning trees for spectral sparsifier matches the\nnumber of spanning trees required to obtain a cut sparsifier in [Fung,\nHariharan, Harvey, Panigraphi STOC 11]. The previous best result was by naively\napplying a classical matrix Chernoff bound which requires $\\epsilon^{-2} n \\log\nn$ spanning trees. For the tree averaging procedure to agree with the original\ngraph Laplacian in expectation, each edge of the tree should be reweighted by\nthe inverse of the edge leverage score in the original graph. We also show that\nwhen using this reweighting of the edges, the Laplacian of single random tree\nis bounded above in the PSD order by the original graph Laplacian times a\nfactor $\\log n$ with high probability, i.e. $L_T \\preceq O(\\log n) L_G$.\n  We show a lower bound that almost matches our last result, namely that in\nsome graphs, with high probability, the random spanning tree is $\\it{not}$\nbounded above in the spectral order by $\\frac{\\log n}{\\log\\log n}$ times the\noriginal graph Laplacian. We also show a lower bound that in $\\epsilon^{-2}\n\\log n$ spanning trees are necessary to get a $(1\\pm \\epsilon)$ spectral\nsparsifier.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 03:27:28 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Kyng", "Rasmus", ""], ["Song", "Zhao", ""]]}, {"id": "1810.08414", "submitter": "Haris Angelidakis", "authors": "Haris Angelidakis, Pranjal Awasthi, Avrim Blum, Vaggos Chatziafratis,\n  Chen Dan", "title": "Bilu-Linial stability, certified algorithms and the Independent Set\n  problem", "comments": "Contains improved algorithms for bounded-degree graphs. To appear in\n  ESA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Maximum Independent Set (MIS) problem under the notion of\nstability introduced by Bilu and Linial (2010): a weighted instance of MIS is\n$\\gamma$-stable if it has a unique optimal solution that remains the unique\noptimum under multiplicative perturbations of the weights by a factor of at\nmost $\\gamma\\geq 1$. The goal then is to efficiently recover the unique optimal\nsolution. In this work, we solve stable instances of MIS on several graphs\nclasses: we solve $\\widetilde{O}(\\Delta/\\sqrt{\\log \\Delta})$-stable instances\non graphs of maximum degree $\\Delta$, $(k - 1)$-stable instances on\n$k$-colorable graphs and $(1 + \\varepsilon)$-stable instances on planar graphs.\nFor general graphs, we present a strong lower bound showing that there are no\nefficient algorithms for $O(n^{\\frac{1}{2} - \\varepsilon})$-stable instances of\nMIS, assuming the planted clique conjecture. We also give an algorithm for\n$(\\varepsilon n)$-stable instances. As a by-product of our techniques, we give\nalgorithms and lower bounds for stable instances of Node Multiway Cut.\nFurthermore, we prove a general result showing that the integrality gap of\nconvex relaxations of several maximization problems reduces dramatically on\nstable instances.\n  Moreover, we initiate the study of certified algorithms, a notion recently\nintroduced by Makarychev and Makarychev (2018), which is a class of\n$\\gamma$-approximation algorithms that satisfy one crucial property: the\nsolution returned is optimal for a perturbation of the original instance. We\nobtain $\\Delta$-certified algorithms for MIS on graphs of maximum degree\n$\\Delta$, and $(1+\\varepsilon)$-certified algorithms on planar graphs. Finally,\nwe analyze the algorithm of Berman and Furer (1994) and prove that it is a\n$\\left(\\frac{\\Delta + 1}{3} + \\varepsilon\\right)$-certified algorithm for MIS\non graphs of maximum degree $\\Delta$ where all weights are equal to 1.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 09:14:13 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 16:58:34 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Angelidakis", "Haris", ""], ["Awasthi", "Pranjal", ""], ["Blum", "Avrim", ""], ["Chatziafratis", "Vaggos", ""], ["Dan", "Chen", ""]]}, {"id": "1810.08544", "submitter": "Udit Agarwal", "authors": "Udit Agarwal and Vijaya Ramachandran", "title": "New and Simplified Distributed Algorithms for Weighted All Pairs\n  Shortest Paths", "comments": "arXiv admin note: text overlap with arXiv:1807.08824", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing all pairs shortest paths (APSP) and\nshortest paths for k sources in a weighted graph in the distributed CONGEST\nmodel. For graphs with non-negative integer edge weights (including zero\nweights) we build on a recent pipelined algorithm to obtain\n$\\tilde{O}(\\lambda^{1/4}\\cdot n^{5/4})$ in graphs with non-negative integer\nedge-weight at most $\\lambda$, and $\\tilde{O}(n \\cdot \\bigtriangleup^{1/3})$\nrounds for shortest path distances at most $\\bigtriangleup$. Additionally, we\nsimplify some of the procedures in the earlier APSP algorithms for non-negative\nedge weights in [HNS17,ARKP18]. We also present results for computing h-hop\nshortest paths and shortest paths from $k$ given sources.\n  In other results, we present a randomized exact APSP algorithm for graphs\nwith arbitrary edge weights that runs in $\\tilde{O}(n^{4/3})$ rounds w.h.p. in\nn, which improves the previous best $\\tilde{O}(n^{3/2})$ bound, which is\ndeterministic. We also present an $\\tilde{O}(n/\\epsilon^2)$-round deterministic\n$(1+\\epsilon)$ approximation algorithm for graphs with non-negative $poly(n)$\ninteger weights (including zero edge-weights), improving results in\n[Nanongkai14,LP15] that hold only for positive integer weights.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 15:53:23 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Agarwal", "Udit", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1810.08671", "submitter": "Josh Alman", "authors": "Josh Alman and Virginia Vassilevska Williams", "title": "Limits on All Known (and Some Unknown) Approaches to Matrix\n  Multiplication", "comments": "32 pages. A preliminary version appeared in the 59th Annual IEEE\n  Symposium on Foundations of Computer Science (FOCS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the known techniques for designing Matrix Multiplication algorithms.\nThe two main approaches are the Laser method of Strassen, and the Group\ntheoretic approach of Cohn and Umans. We define a generalization based on\nzeroing outs which subsumes these two approaches, which we call the Solar\nmethod, and an even more general method based on monomial degenerations, which\nwe call the Galactic method.\n  We then design a suite of techniques for proving lower bounds on the value of\n$\\omega$, the exponent of matrix multiplication, which can be achieved by\nalgorithms using many tensors $T$ and the Galactic method. Some of our\ntechniques exploit `local' properties of $T$, like finding a sub-tensor of $T$\nwhich is so `weak' that $T$ itself couldn't be used to achieve a good bound on\n$\\omega$, while others exploit `global' properties, like $T$ being a monomial\ndegeneration of the structural tensor of a group algebra.\n  Our main result is that there is a universal constant $\\ell>2$ such that a\nlarge class of tensors generalizing the Coppersmith-Winograd tensor $CW_q$\ncannot be used within the Galactic method to show a bound on $\\omega$ better\nthan $\\ell$, for any $q$. We give evidence that previous lower-bounding\ntechniques were not strong enough to show this. We also prove a number of\ncomplementary results along the way, including that for any group $G$, the\nstructural tensor of $\\mathbb{C}[G]$ can be used to recover the best bound on\n$\\omega$ which the Coppersmith-Winograd approach gets using $CW_{|G|-2}$ as\nlong as the asymptotic rank of the structural tensor is not too large.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 20:07:08 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Alman", "Josh", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "1810.08684", "submitter": "Kira Adaricheva V", "authors": "Kira Adaricheva and Taylor Ninesling", "title": "Direct and Binary Direct Bases for One-set Updates of a Closure System", "comments": "17 pages, 1 table, 1 figure, poster session presentation ICFCA-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a concept of a binary-direct implicational basis and show that\nthe shortest binary-direct basis exists and it is known as the $D$-basis\nintroduced in Adaricheva, Nation, Rand [Disc.Appl.Math. 2013]. Using this\nconcept we approach the algorithmic solution to the Singleton Horn Extension\nproblem, as well as the one set removal problem, when the closure system is\ngiven by the canonical direct or binary-direct basis. In this problem, a new\nclosed set is added to or removed from the closure system forcing the re-write\nof a given basis. Our goal is to obtain the same type of implicational basis\nfor the new closure system as was given for original closure system and to make\nthe basis update an optimal process.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 20:35:02 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Adaricheva", "Kira", ""], ["Ninesling", "Taylor", ""]]}, {"id": "1810.08810", "submitter": "Aaron Roth", "authors": "Alexandra Chouldechova, Aaron Roth", "title": "The Frontiers of Fairness in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last few years have seen an explosion of academic and popular interest in\nalgorithmic fairness. Despite this interest and the volume and velocity of work\nthat has been produced recently, the fundamental science of fairness in machine\nlearning is still in a nascent state. In March 2018, we convened a group of\nexperts as part of a CCC visioning workshop to assess the state of the field,\nand distill the most promising research directions going forward. This report\nsummarizes the findings of that workshop. Along the way, it surveys recent\ntheoretical work in the field and points towards promising directions for\nresearch.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 14:24:05 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Chouldechova", "Alexandra", ""], ["Roth", "Aaron", ""]]}, {"id": "1810.08833", "submitter": "Haoyu Zhang", "authors": "Haoyu Zhang, Qin Zhang", "title": "MinJoin: Efficient Edit Similarity Joins via Local Hash Minima", "comments": "Accepted to KDD 2019, full version, 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing similarity joins under edit distance on a\nset of strings. Edit similarity joins is a fundamental problem in databases,\ndata mining and bioinformatics. It finds important applications in data\ncleaning and integration, collaborative filtering, genome sequence assembly,\netc. This problem has attracted significant attention in the past two decades.\nHowever, all previous algorithms either cannot scale well to long strings and\nlarge similarity thresholds, or suffer from imperfect accuracy.\n  In this paper we propose a new algorithm for edit similarity joins using a\nnovel string partition based approach. We show mathematically that with high\nprobability our algorithm achieves a perfect accuracy, and runs in linear time\nplus a data-dependent verification step. Experiments on real world datasets\nshow that our algorithm significantly outperforms the state-of-the-art\nalgorithms for edit similarity joins, and achieves perfect accuracy on all the\ndatasets that we have tested.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 17:54:37 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 05:10:14 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Zhang", "Haoyu", ""], ["Zhang", "Qin", ""]]}, {"id": "1810.08867", "submitter": "Alireza Rezaei", "authors": "Shayan Oveis Gharan, Alireza Rezaei", "title": "A Polynomial Time MCMC Method for Sampling from Continuous DPPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Gibbs sampling algorithm for continuous determinantal point\nprocesses. We show that, given a warm start, the Gibbs sampler generates a\nrandom sample from a continuous $k$-DPP defined on a $d$-dimensional domain by\nonly taking $\\text{poly}(k)$ number of steps. As an application, we design an\nalgorithm to generate random samples from $k$-DPPs defined by a spherical\nGaussian kernel on a unit sphere in $d$-dimensions, $\\mathbb{S}^{d-1}$ in time\npolynomial in $k,d$.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 23:29:40 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Gharan", "Shayan Oveis", ""], ["Rezaei", "Alireza", ""]]}, {"id": "1810.09005", "submitter": "Carlos Cardonha", "authors": "Carlos Cardonha, Lucas C. Villa Real", "title": "Theoretical and Practical Aspects of the Linear Tape Scheduling Problem", "comments": "26 pages, 1 figure, 3 tables, currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic tapes have been playing a key role as means for storage of digital\ndata for decades, and their unsurpassed cost-effectiveness still make them the\ntechnology of choice in several industries, such as media and entertainment.\nTapes are mostly used for cold storage nowadays, and therefore the study of\nscheduling algorithms for read requests tailored for these devices has been\nlargely neglected in the literature. In this article, we investigate the Linear\nTape Scheduling Problem (LTSP), in which read requests associated with files\nstored on a single-tracked magnetic tape should be scheduled in a way that the\nsum of all response times are minimized. LTSP has many similarities with\nclassical combinatorial optimization problems such as the Traveling Repairmen\nProblem and the Dial-a-Ride Problem restricted to the real line; nevertheless,\nsignificant differences on structural properties and strict time-limit\nconstraints of real-world scenarios make LTSP challenging and interesting on\nits own. In this work, we investigate several properties and algorithms for\nLTSP and some of its extensions. The results allowed for the identification of\n3-approximation algorithms for LTSP and efficient exact algorithms for some of\nits special cases. We also show that LTSPR, the version of the problem with\nheterogeneous release times for requests, is NP-complete. OLTSP, the online\nextension of LTSPR, does not admit c-competitive algorithms for any constant\nfactor c, but we nevertheless introduce an algorithm for the problem and show\nthrough extensive computational experiments on synthetic and real-world\ndatasets that different embodiments of the proposed strategy are\ncomputationally efficient and over-perform by orders of magnitude an algorithm\nbeing currently used by real-world tape file systems.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 18:42:32 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Cardonha", "Carlos", ""], ["Real", "Lucas C. Villa", ""]]}, {"id": "1810.09027", "submitter": "Yasamin Nazari", "authors": "Michael Dinitz, Yasamin Nazari", "title": "Massively Parallel Approximate Distance Sketches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data structures that allow efficient distance estimation (distance oracles,\ndistance sketches, etc.) have been extensively studied, and are particularly\nwell studied in centralized models and classical distributed models such as\nCONGEST. We initiate their study in newer (and arguably more realistic) models\nof distributed computation: the Congested Clique model and the Massively\nParallel Computation (MPC) model. We provide efficient constructions in both of\nthese models, but our core results are for MPC. In MPC we give two main\nresults: an algorithm that constructs stretch/space optimal distance sketches\nbut takes a (small) polynomial number of rounds, and an algorithm that\nconstructs distance sketches with worse stretch but that only takes\npolylogarithmic rounds.\n  Along the way, we show that other useful combinatorial structures can also be\ncomputed in MPC. In particular, one key component we use to construct distance\nsketches are an MPC construction of the hopsets of Elkin and Neiman (2016).\nThis result has additional applications such as the first polylogarithmic time\nalgorithm for constant approximate single-source shortest paths for weighted\ngraphs in the low memory MPC setting.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 21:07:24 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 01:46:41 GMT"}, {"version": "v3", "created": "Tue, 21 May 2019 01:38:44 GMT"}, {"version": "v4", "created": "Thu, 12 Sep 2019 15:56:35 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Dinitz", "Michael", ""], ["Nazari", "Yasamin", ""]]}, {"id": "1810.09250", "submitter": "Jelani Nelson", "authors": "Shyam Narayanan, Jelani Nelson", "title": "Optimal terminal dimensionality reduction in Euclidean space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\varepsilon\\in(0,1)$ and $X\\subset\\mathbb R^d$ be arbitrary with $|X|$\nhaving size $n>1$. The Johnson-Lindenstrauss lemma states there exists\n$f:X\\rightarrow\\mathbb R^m$ with $m = O(\\varepsilon^{-2}\\log n)$ such that $$\n\\forall x\\in X\\ \\forall y\\in X, \\|x-y\\|_2 \\le \\|f(x)-f(y)\\|_2 \\le\n(1+\\varepsilon)\\|x-y\\|_2 . $$ We show that a strictly stronger version of this\nstatement holds, answering one of the main open questions of [MMMR18]:\n\"$\\forall y\\in X$\" in the above statement may be replaced with \"$\\forall\ny\\in\\mathbb R^d$\", so that $f$ not only preserves distances within $X$, but\nalso distances to $X$ from the rest of space. Previously this stronger version\nwas only known with the worse bound $m = O(\\varepsilon^{-4}\\log n)$. Our proof\nis via a tighter analysis of (a specific instantiation of) the embedding recipe\nof [MMMR18].\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 13:22:38 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Narayanan", "Shyam", ""], ["Nelson", "Jelani", ""]]}, {"id": "1810.09832", "submitter": "Rediet Abebe", "authors": "Rediet Abebe, Kira Goldner", "title": "Mechanism Design for Social Good", "comments": "AI Matters, 2018", "journal-ref": null, "doi": "10.1145/3284751.328476", "report-no": null, "categories": "cs.GT cs.AI cs.CY cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Across various domains--such as health, education, and housing--improving\nsocietal welfare involves allocating resources, setting policies, targeting\ninterventions, and regulating activities. These solutions have an immense\nimpact on the day-to-day lives of individuals, whether in the form of access to\nquality healthcare, labor market outcomes, or how votes are accounted for in a\ndemocratic society. Problems that can have an out-sized impact on individuals\nwhose opportunities have historically been limited often pose conceptual and\ntechnical challenges, requiring insights from many disciplines. Conversely, the\nlack of interdisciplinary approach can leave these urgent needs unaddressed and\ncan even exacerbate underlying socioeconomic inequalities. To realize the\nopportunities in these domains, we need to correctly set objectives and reason\nabout human behavior and actions. Doing so requires a deep grounding in the\nfield of interest and collaboration with domain experts who understand the\nsocietal implications and feasibility of proposed solutions. These insights can\nplay an instrumental role in proposing algorithmically-informed policies.\n  In this article, we describe the Mechanism Design for Social Good (MD4SG)\nresearch agenda, which involves using insights from algorithms, optimization,\nand mechanism design to improve access to opportunity. The MD4SG research\ncommunity takes an interdisciplinary, multi-stakeholder approach to improve\nsocietal welfare. We discuss three exciting research avenues within MD4SG\nrelated to improving access to opportunity in the developing world, labor\nmarkets and discrimination, and housing. For each of these, we showcase ongoing\nwork, underline new directions, and discuss potential for implementing existing\nwork in practice.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 18:41:52 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Abebe", "Rediet", ""], ["Goldner", "Kira", ""]]}, {"id": "1810.09981", "submitter": "Hanrui Zhang", "authors": "Wei Chen, Shang-Hua Teng, Hanrui Zhang", "title": "A Systematic Framework and Characterization of Influence-Based Network\n  Centrality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a framework for studying the following fundamental\nquestion in network analysis: How should one assess the centralities of nodes\nin an information/influence propagation process over a social network?\n  Our framework systematically extends a family of classical graph-theoretical\ncentrality formulations, including degree centrality, harmonic centrality, and\ntheir \"sphere-of-influence\" generalizations, to influence-based network\ncentralities. We further extend natural group centralities from graph models to\ninfluence models, since group cooperation is essential in social influences.\nThis in turn enables us to assess individuals' centralities in group influence\nsettings by applying the concept of Shapley value from cooperative game theory.\n  Mathematically, using the property that these centrality formulations are\nBayesian, we prove the following characterization theorem: Every\ninfluence-based centrality formulation in this family is the unique Bayesian\ncentrality that conforms with its corresponding graph-theoretical centrality\nformulation. Moreover, the uniqueness is fully determined by the centrality\nformulation on the class of layered graphs, which is derived from a beautiful\nalgebraic structure of influence instances modeled by cascading sequences. Our\nmain mathematical result that layered graphs in fact form a basis for the space\nof influence-cascading-sequence profiles could also be useful in other studies\nof network influences. We further provide an algorithmic framework for\nefficient approximation of these influence-based centrality measures.\n  Our study provides a systematic road map for comparative analyses of\ndifferent influence-based centrality formulations, as well as for transferring\ngraph-theoretical concepts to influence models.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 17:22:26 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Chen", "Wei", ""], ["Teng", "Shang-Hua", ""], ["Zhang", "Hanrui", ""]]}, {"id": "1810.10005", "submitter": "Brad Lackey", "authors": "Brad Lackey", "title": "A belief propagation algorithm based on domain decomposition", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note provides a detailed description and derivation of the domain\ndecomposition algorithm that appears in previous works by the author. Given a\nlarge re-estimation problem, domain decomposition provides an iterative method\nfor assembling Boltzmann distributions associated to small subproblems into an\napproximation of the Bayesian posterior of the whole problem. The algorithm is\namenable to using Boltzmann sampling to approximate these Boltzmann\ndistributions. In previous work, we have shown the capability of heuristic\nversions of this algorithm to solve LDPC decoding and circuit fault diagnosis\nproblems too large to fit on quantum annealing hardware used for sampling.\nHere, we rigorously prove soundness of the method.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 20:22:51 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Lackey", "Brad", ""]]}, {"id": "1810.10044", "submitter": "Charles Carlson", "authors": "Charles Carlson, Alexandra Kolla, Ray Li, Nitya Mani, Benny Sudakov\n  and Luca Trevisan", "title": "Lower bounds for Max-Cut in $H$-free graphs via semidefinite programming", "comments": "21 pages, to be published in LATIN 2020 proceedings, Updated version\n  is rewritten to include additional results along with corrections to original\n  arguments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph $G$, let $f(G)$ denote the size of the maximum cut in $G$. The\nproblem of estimating $f(G)$ as a function of the number of vertices and edges\nof $G$ has a long history and was extensively studied in the last fifty years.\nIn this paper we propose an approach, based on semidefinite programming (SDP),\nto prove lower bounds on $f(G)$. We use this approach to find large cuts in\ngraphs with few triangles and in $K_r$-free graphs.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 18:43:35 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 19:18:46 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Carlson", "Charles", ""], ["Kolla", "Alexandra", ""], ["Li", "Ray", ""], ["Mani", "Nitya", ""], ["Sudakov", "Benny", ""], ["Trevisan", "Luca", ""]]}, {"id": "1810.10046", "submitter": "Jason Altschuler", "authors": "Jason Altschuler, Francis Bach, Alessandro Rudi, Jonathan Weed", "title": "Approximating the Quadratic Transportation Metric in Near-Linear Time", "comments": "unchanged from v1; this article now superseded by arXiv:1812.05189", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the quadratic transportation metric (also called the\n$2$-Wasserstein distance or root mean square distance) between two point\nclouds, or, more generally, two discrete distributions, is a fundamental\nproblem in machine learning, statistics, computer graphics, and theoretical\ncomputer science. A long line of work has culminated in a sophisticated\ngeometric algorithm due to Agarwal and Sharathkumar in 2014, which runs in time\n$\\tilde{O}(n^{3/2})$, where $n$ is the number of points. However, obtaining\nfaster algorithms has proven difficult since the $2$-Wasserstein distance is\nknown to have poor sketching and embedding properties, which limits the\neffectiveness of geometric approaches. In this paper, we give an extremely\nsimple deterministic algorithm with $\\tilde{O}(n)$ runtime by using a\ncompletely different approach based on entropic regularization, approximate\nSinkhorn scaling, and low-rank approximations of Gaussian kernel matrices. We\ngive explicit dependence of our algorithm on the dimension and precision of the\napproximation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 18:50:19 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 18:44:33 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Altschuler", "Jason", ""], ["Bach", "Francis", ""], ["Rudi", "Alessandro", ""], ["Weed", "Jonathan", ""]]}, {"id": "1810.10094", "submitter": "Mostafa Haghir Chehreghani", "authors": "Mostafa Haghir Chehreghani and Albert Bifet and Talel Abdessalem", "title": "Novel Adaptive Algorithms for Estimating Betweenness, Coverage and\n  k-path Centralities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important index widely used to analyze social and information networks is\nbetweenness centrality. In this paper, first given a directed network $G$ and a\nvertex $r\\in V(G)$, we present a novel adaptive algorithm for estimating\nbetweenness score of $r$. Our algorithm first computes two subsets of the\nvertex set of $G$, called $\\mathcal{RF}(r)$ and $\\mathcal{RT}(r)$, that define\nthe sample spaces of the start-points and the end-points of the samples. Then,\nit adaptively samples from $\\mathcal{RF}(r)$ and $\\mathcal{RT}(r)$ and stops as\nsoon as some condition is satisfied. The stopping condition depends on the\nsamples met so far, $|\\mathcal{RF}(r)|$ and $|\\mathcal{RT}(r)|$. We show that\ncompared to the well-known existing methods, our algorithm gives a more\nefficient $(\\lambda,\\delta)$-approximation. Then, we propose a novel algorithm\nfor estimating $k$-path centrality of $r$. Our algorithm is based on computing\ntwo sets $\\mathcal{RF}(r)$ and $\\mathcal{D}(r)$. While $\\mathcal{RF}(r)$\ndefines the sample space of the source vertices of the sampled paths,\n$\\mathcal{D}(r)$ defines the sample space of the other vertices of the paths.\nWe show that in order to give a $(\\lambda,\\delta)$-approximation of the\n$k$-path score of $r$, our algorithm requires considerably less samples.\nMoreover, it processes each sample faster and with less memory. Finally, we\nempirically evaluate our proposed algorithms and show their superior\nperformance. Also, we show that they can be used to efficiently compute\ncentrality scores of a set of vertices.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 21:17:57 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Chehreghani", "Mostafa Haghir", ""], ["Bifet", "Albert", ""], ["Abdessalem", "Talel", ""]]}, {"id": "1810.10132", "submitter": "Gautam Goel", "authors": "Gautam Goel, Adam Wierman", "title": "Smoothed Online Optimization for Regression and Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Online Convex Optimization (OCO) in the setting where the costs\nare $m$-strongly convex and the online learner pays a switching cost for\nchanging decisions between rounds. We show that the recently proposed Online\nBalanced Descent (OBD) algorithm is constant competitive in this setting, with\ncompetitive ratio $3 + O(1/m)$, irrespective of the ambient dimension.\nAdditionally, we show that when the sequence of cost functions is\n$\\epsilon$-smooth, OBD has near-optimal dynamic regret and maintains strong\nper-round accuracy. We demonstrate the generality of our approach by showing\nthat the OBD framework can be used to construct competitive algorithms for a\nvariety of online problems across learning and control, including online\nvariants of ridge regression, logistic regression, maximum likelihood\nestimation, and LQR control.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 23:57:40 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 05:30:46 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Goel", "Gautam", ""], ["Wierman", "Adam", ""]]}, {"id": "1810.10229", "submitter": "Guillaume Ducoffe", "authors": "Guillaume Ducoffe", "title": "Faster approximation algorithms for computing shortest cycles on\n  weighted graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an $n$-vertex $m$-edge graph $G$ with non negative edge-weights, the\ngirth of $G$ is the weight of a shortest cycle in $G$. For any graph $G$ with\npolynomially bounded integer weights, we present a deterministic algorithm that\ncomputes, in $\\tilde{\\cal O}(n^{5/3}+m)$-time, a cycle of weight at most twice\nthe girth of $G$. Our approach combines some new insights on the previous\napproximation algorithms for this problem (Lingas and Lundell, IPL'09; Roditty\nand Tov, TALG'13) with Hitting Set based methods that are used for approximate\ndistance oracles and date back from (Thorup and Zwick, JACM'05). Then, we turn\nour algorithm into a deterministic $(2+\\varepsilon)$-approximation for graphs\nwith arbitrary non negative edge-weights, at the price of a slightly worse\nrunning-time in $\\tilde{\\cal O}(n^{5/3}\\log^{{\\cal O}(1)}{(1/\\varepsilon)}+m)$.\nFinally, if we insist in removing the dependency in the number $m$ of edges, we\ncan transform our algorithms into an $\\tilde{\\cal O}(n^{5/3})$-time randomized\n$4$-approximation for the graphs with non negative edge-weights -- assuming the\nadjacency lists are sorted. Combined with the aforementioned Hitting Set based\nmethods, this algorithm can be derandomized, thereby yielding an $\\tilde{\\cal\nO}(n^{5/3})$-time deterministic $4$-approximation for the graphs with\npolynomially bounded integer weights, and an $\\tilde{\\cal O}(n^{5/3}\\log^{{\\cal\nO}(1)}{(1/\\varepsilon)})$-time deterministic $(4+\\varepsilon)$-approximation\nfor the graphs with non negative edge-weights. To the best of our knowledge,\nthese are the first known subquadratic-time approximation algorithms for\ncomputing the girth of weighted graphs.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 07:47:41 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Ducoffe", "Guillaume", ""]]}, {"id": "1810.10258", "submitter": "Kazuaki Yamaguchi Dr.", "authors": "Satoshi Shimizu, Kazuaki Yamaguchi, Sumio Masuda", "title": "A Maximum Edge-Weight Clique Extraction Algorithm Based on\n  Branch-and-Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum edge-weight clique problem is to find a clique whose sum of\nedge-weight is the maximum for a given edge-weighted undirected graph. The\nproblem is NP-hard and some branch-and-bound algorithms have been proposed. In\nthis paper, we propose a new exact algorithm based on branch-and-bound. It\nassigns edge-weights to vertices and calculates upper bounds using vertex\ncoloring. By some computational experiments, we confirmed our algorithm is\nfaster than previous algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 09:11:23 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Shimizu", "Satoshi", ""], ["Yamaguchi", "Kazuaki", ""], ["Masuda", "Sumio", ""]]}, {"id": "1810.10615", "submitter": "Guilherme Miguel Teixeira Rito", "authors": "Guilherme Rito and Herv\\'e Paulino", "title": "Scheduling computations with provably low synchronization overheads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work Stealing has been a very successful algorithm for scheduling parallel\ncomputations, and is known to achieve high performances even for computations\nexhibiting fine-grained parallelism. We present a variant of \\ws\\ that provably\navoids most synchronization overheads by keeping processors' deques entirely\nprivate by default, and only exposing work when requested by thieves. This is\nthe first paper that obtains bounds on the synchronization overheads that are\n(essentially) independent of the total amount of work, thus corresponding to a\ngreat improvement, in both algorithm design and theory, over state-of-the-art\n\\ws\\ algorithms. Consider any computation with work $T_{1}$ and critical-path\nlength $T_{\\infty}$ executed by $P$ processors using our scheduler. Our\nanalysis shows that the expected execution time is $O\\left(\\frac{T_{1}}{P} +\nT_{\\infty}\\right)$, and the expected synchronization overheads incurred during\nthe execution are at most $O\\left(\\left(C_{CAS} +\nC_{MFence}\\right)PT_{\\infty}\\right)$, where $C_{CAS}$ and $C_{MFence}$\nrespectively denote the maximum cost of executing a Compare-And-Swap\ninstruction and a Memory Fence instruction.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 20:54:48 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 18:32:59 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Rito", "Guilherme", ""], ["Paulino", "Herv\u00e9", ""]]}, {"id": "1810.10631", "submitter": "Tsunehiko Kameda", "authors": "Robert Benkoczi, Binay Bhattacharya, Yuya Higashikawa, Tsunehiko\n  Kameda, and Naoki Katoh", "title": "Minsum $k$-Sink Problem on Path Networks", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of locating a set of $k$ sinks on a path network with\ngeneral edge capacities that minimizes the sum of the evacuation times of all\nevacuees. We first present an $O(kn\\log^4n)$ time algorithm when the edge\ncapacities are non-uniform, where $n$ is the number of vertices. We then\npresent an $O(kn\\log^3 n)$ time algorithm when the edge capacities are uniform.\nWe also present an $O(n\\log n)$ time algorithm for the special case where $k=1$\nand the edge capacities are non-uniform.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 21:28:19 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Benkoczi", "Robert", ""], ["Bhattacharya", "Binay", ""], ["Higashikawa", "Yuya", ""], ["Kameda", "Tsunehiko", ""], ["Katoh", "Naoki", ""]]}, {"id": "1810.10635", "submitter": "Kasper Green Larsen", "authors": "Riko Jacob, Kasper Green Larsen, Jesper Buus Nielsen", "title": "Lower Bounds for Oblivious Data Structures", "comments": "To appear at SODA'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An oblivious data structure is a data structure where the memory access\npatterns reveals no information about the operations performed on it. Such data\nstructures were introduced by Wang et al. [ACM SIGSAC'14] and are intended for\nsituations where one wishes to store the data structure at an untrusted server.\nOne way to obtain an oblivious data structure is simply to run a classic data\nstructure on an oblivious RAM (ORAM). Until very recently, this resulted in an\noverhead of $\\omega(\\lg n)$ for the most natural setting of parameters.\nMoreover, a recent lower bound for ORAMs by Larsen and Nielsen [CRYPTO'18] show\nthat they always incur an overhead of at least $\\Omega(\\lg n)$ if used in a\nblack box manner. To circumvent the $\\omega(\\lg n)$ overhead, researchers have\ninstead studied classic data structure problems more directly and have obtained\nefficient solutions for many such problems such as stacks, queues, deques,\npriority queues and search trees. However, none of these data structures\nprocess operations faster than $\\Theta(\\lg n)$, leaving open the question of\nwhether even faster solutions exist. In this paper, we rule out this\npossibility by proving $\\Omega(\\lg n)$ lower bounds for oblivious stacks,\nqueues, deques, priority queues and search trees.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 21:43:32 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Jacob", "Riko", ""], ["Larsen", "Kasper Green", ""], ["Nielsen", "Jesper Buus", ""]]}, {"id": "1810.10738", "submitter": "Laxman Dhulipala", "authors": "Thomas Tseng, Laxman Dhulipala, Guy Blelloch", "title": "Batch-Parallel Euler Tour Trees", "comments": "This is the full version of the paper appearing in ALENEX 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic trees problem is to maintain a forest undergoing edge insertions\nand deletions while supporting queries for information such as connectivity.\nThere are many existing data structures for this problem, but few of them are\ncapable of exploiting parallelism in the batch-setting, in which large batches\nof edges are inserted or deleted from the forest at once. In this paper, we\ndemonstrate that the Euler tour tree, an existing sequential dynamic trees data\nstructure, can be parallelized in the batch setting. For a batch of $k$ updates\nover a forest of $n$ vertices, our parallel Euler tour trees perform $O(k \\log\n(1 + n/k))$ expected work with $O(\\log n)$ depth with high probability. Our\nwork bound is asymptotically optimal, and we improve on the depth bound\nachieved by Acar et al. for the batch-parallel dynamic trees problem.\n  The main building block for parallelizing Euler tour trees is a\nbatch-parallel skip list data structure, which we believe may be of independent\ninterest. Euler tour trees require a sequence data structure capable of joins\nand splits. Sequentially, balanced binary trees are used, but they are\ndifficult to join or split in parallel. We show that skip lists, on the other\nhand, support batches of joins or splits of size $k$ over $n$ elements with\n$O(k \\log (1 + n/k))$ work in expectation and $O(\\log n)$ depth with high\nprobability. We also achieve the same efficiency bounds for augmented skip\nlists, which allows us to augment our Euler tour trees to support subtree\nqueries.\n  Our data structures achieve between 67--96x self-relative speedup on 72 cores\nwith hyper-threading on large batch sizes. Our data structures also outperform\nthe fastest existing sequential dynamic trees data structures empirically.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 06:30:18 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 08:46:50 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Tseng", "Thomas", ""], ["Dhulipala", "Laxman", ""], ["Blelloch", "Guy", ""]]}, {"id": "1810.10834", "submitter": "Sebastian Lamm", "authors": "Sebastian Lamm, Christian Schulz, Darren Strash, Robert Williger,\n  Huashuo Zhang", "title": "Exactly Solving the Maximum Weight Independent Set Problem on Large\n  Real-World Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One powerful technique to solve NP-hard optimization problems in practice is\nbranch-and-reduce search---which is branch-and-bound that intermixes branching\nwith reductions to decrease the input size. While this technique is known to be\nvery effective in practice for unweighted problems, very little is known for\nweighted problems, in part due to a lack of known effective reductions. In this\nwork, we develop a full suite of new reductions for the maximum weight\nindependent set problem and provide extensive experiments to show their\neffectiveness in practice on real-world graphs of up to millions of vertices\nand edges.\n  Our experiments indicate that our approach is able to outperform existing\nstate-of-the-art algorithms, solving many instances that were previously\ninfeasible. In particular, we show that branch-and-reduce is able to solve a\nlarge number of instances up to two orders of magnitude faster than existing\n(inexact) local search algorithms---and is able to solve the majority of\ninstances within 15 minutes. For those instances remaining infeasible, we show\nthat combining kernelization with local search produces higher-quality\nsolutions than local search alone.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 11:22:37 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Lamm", "Sebastian", ""], ["Schulz", "Christian", ""], ["Strash", "Darren", ""], ["Williger", "Robert", ""], ["Zhang", "Huashuo", ""]]}, {"id": "1810.10860", "submitter": "Romain Aza\\\"is", "authors": "Romain Aza\\\"is, Jean-Baptiste Durand, Christophe Godin", "title": "Approximation of trees by self-nested trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of self-nested trees presents remarkable compression properties\nbecause of the systematic repetition of subtrees in their structure. In this\npaper, we provide a better combinatorial characterization of this specific\nfamily of trees. In particular, we show from both theoretical and practical\nviewpoints that complex queries can be quickly answered in self-nested trees\ncompared to general trees. We also present an approximation algorithm of a tree\nby a self-nested one that can be used in fast prediction of edit distance\nbetween two trees.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 13:14:32 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Aza\u00efs", "Romain", ""], ["Durand", "Jean-Baptiste", ""], ["Godin", "Christophe", ""]]}, {"id": "1810.10900", "submitter": "Will Ma", "authors": "Will Ma, David Simchi-Levi, Chung-Piaw Teo", "title": "On Policies for Single-leg Revenue Management with Limited Demand\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the single-item revenue management problem, with no\ninformation given about the demand trajectory over time. When the item is sold\nthrough accepting/rejecting different fare classes, Ball and Queyranne (2009)\nhave established the tight competitive ratio for this problem using booking\nlimit policies, which raise the acceptance threshold as the remaining inventory\ndwindles. However, when the item is sold through dynamic pricing instead, there\nis the additional challenge that offering a low price may entice high-paying\ncustomers to substitute down. We show that despite this challenge, the same\ncompetitive ratio can still be achieved using a randomized dynamic pricing\npolicy. Our policy incorporates the price-skimming technique from Eren and\nMaglaras (2010), but importantly we show how the randomized price distribution\nshould be stochastically-increased as the remaining inventory dwindles. A key\ntechnical ingredient in our policy is a new \"valuation tracking\" subroutine,\nwhich tracks the possible values for the optimum, and follows the most\n\"inventory-conservative\" control which maintains the desired competitive ratio.\nFinally, we demonstrate the empirical effectiveness of our policy in\nsimulations, where its average-case performance surpasses all naive\nmodifications of the existing policies.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 14:40:02 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 14:56:35 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Ma", "Will", ""], ["Simchi-Levi", "David", ""], ["Teo", "Chung-Piaw", ""]]}, {"id": "1810.10932", "submitter": "Sebastian Forster", "authors": "Aaron Bernstein, Sebastian Forster, Monika Henzinger", "title": "A Deamortization Approach for Dynamic Spanner and Dynamic Maximal\n  Matching", "comments": "A preliminary version of this article was presented at the 30th\n  Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2019). This version\n  fixes an an error in the analysis of the dynamic matching algorithm. Abstract\n  shortened to respect the arXiv limit of 1920 characters", "journal-ref": null, "doi": "10.1137/1.9781611975482.115", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many dynamic graph algorithms have an amortized update time, rather than a\nstronger worst-case guarantee. But amortized data structures are not suitable\nfor real-time systems, where each individual operation has to be executed\nquickly. For this reason, there exist many recent randomized results that aim\nto provide a guarantee stronger than amortized expected. The strongest possible\nguarantee for a randomized algorithm is that it is always correct (Las Vegas),\nand has high-probability worst-case update time, which gives a bound on the\ntime for each individual operation that holds with high probability.\n  In this paper we present the first polylogarithmic high-probability\nworst-case time bounds for the dynamic spanner and the dynamic maximal matching\nproblem. 1. For dynamic spanner, the only known $o(n)$ worst-case bounds were\n$O(n^{3/4})$ high-probability worst-case update time for maintaining a\n3-spanner and $O(n^{5/9})$ for maintaining a 5-spanner. We give a $O(1)^k\n\\log^3(n)$ high-probability worst-case time bound for maintaining a\n$(2k-1)$-spanner, which yields the first worst-case polylog update time for all\nconstant $k$. (All the results above maintain the optimal tradeoff of stretch\n$2k-1$ and $\\tilde{O}(n^{1+1/k})$ edges.) 2. For dynamic maximal matching, or\ndynamic $2$-approximate maximum matching, no algorithm with $o(n)$ worst-case\ntime bound was known and we present an algorithm with $O(\\log^5(n))$\nhigh-probability worst-case time; similar worst-case bounds existed only for\nmaintaining a matching that was $(2+\\epsilon)$-approximate, and hence not\nmaximal.\n  Our results are achieved using a new black-box reduction that converts any\ndata structure with worst-case expected update time into one with a\nhigh-probability worst-case update time: the query time remains the same, while\nthe update time increases by a factor of $O(\\log^2(n))$.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 15:37:58 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 13:35:42 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Bernstein", "Aaron", ""], ["Forster", "Sebastian", ""], ["Henzinger", "Monika", ""]]}, {"id": "1810.10940", "submitter": "Michael Lampis", "authors": "Ararat Harutyunyan, Michael Lampis, Vadim Lozin, J\\'er\\^ome Monnot", "title": "Maximum Independent Sets in Subcubic Graphs: New Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum independent set problem is known to be NP-hard in the class of\nsubcubic graphs, i.e. graphs of vertex degree at most 3. We present a\npolynomial-time solution in a subclass of subcubic graphs generalizing several\npreviously known results.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 15:54:56 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Harutyunyan", "Ararat", ""], ["Lampis", "Michael", ""], ["Lozin", "Vadim", ""], ["Monnot", "J\u00e9r\u00f4me", ""]]}, {"id": "1810.10965", "submitter": "Guillermo de Bernardo", "authors": "Ana Cerdeira-Pena, Guillermo de Bernardo, Antonio Fari\\~na, Jose R.\n  Parama, Fernando Silva-Coira", "title": "Towards a compact representation of temporal rasters", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. Published in SPIRE 2018", "journal-ref": "SPIRE 2018", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big research efforts have been devoted to efficiently manage spatio-temporal\ndata. However, most works focused on vectorial data, and much less, on raster\ndata. This work presents a new representation for raster data that evolve along\ntime named Temporal k^2 raster. It faces the two main issues that arise when\ndealing with spatio-temporal data: the space consumption and the query response\ntimes. It extends a compact data structure for raster data in order to manage\ntime and thus, it is possible to query it directly in compressed form, instead\nof the classical approach that requires a complete decompression before any\nmanipulation. In addition, in the same compressed space, the new data structure\nincludes two indexes: a spatial index and an index on the values of the cells,\nthus becoming a self-index for raster data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 13:18:49 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Cerdeira-Pena", "Ana", ""], ["de Bernardo", "Guillermo", ""], ["Fari\u00f1a", "Antonio", ""], ["Parama", "Jose R.", ""], ["Silva-Coira", "Fernando", ""]]}, {"id": "1810.10982", "submitter": "Andr\\'e Nusser", "authors": "Karl Bringmann, Marvin K\\\"unnemann, Andr\\'e Nusser", "title": "Fr\\'echet Distance Under Translation: Conditional Hardness and an\n  Algorithm via Offline Dynamic Grid Reachability", "comments": "Preprint of SODA 2019 publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The discrete Fr\\'echet distance is a popular measure for comparing polygonal\ncurves. An important variant is the discrete Fr\\'echet distance under\ntranslation, which enables detection of similar movement patterns in different\nspatial domains. For polygonal curves of length $n$ in the plane, the fastest\nknown algorithm runs in time $\\tilde{\\cal O}(n^{5})$ [Ben Avraham, Kaplan,\nSharir '15]. This is achieved by constructing an arrangement of disks of size\n${\\cal O}(n^{4})$, and then traversing its faces while updating reachability in\na directed grid graph of size $N := {\\cal O}(n^2)$, which can be done in time\n$\\tilde{\\cal O}(\\sqrt{N})$ per update [Diks, Sankowski '07]. The contribution\nof this paper is two-fold.\n  First, although it is an open problem to solve dynamic reachability in\ndirected grid graphs faster than $\\tilde{\\cal O}(\\sqrt{N})$, we improve this\npart of the algorithm: We observe that an offline variant of dynamic\n$s$-$t$-reachability in directed grid graphs suffices, and we solve this\nvariant in amortized time $\\tilde{\\cal O}(N^{1/3})$ per update, resulting in an\nimproved running time of $\\tilde{\\cal O}(n^{4.66...})$ for the discrete\nFr\\'echet distance under translation. Second, we provide evidence that\nconstructing the arrangement of size ${\\cal O}(n^{4})$ is necessary in the\nworst case, by proving a conditional lower bound of $n^{4 - o(1)}$ on the\nrunning time for the discrete Fr\\'echet distance under translation, assuming\nthe Strong Exponential Time Hypothesis.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 17:15:56 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 11:21:54 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Bringmann", "Karl", ""], ["K\u00fcnnemann", "Marvin", ""], ["Nusser", "Andr\u00e9", ""]]}, {"id": "1810.11134", "submitter": "Mohamed Masoud", "authors": "Mohamed Masoud, Saeid Belkasim", "title": "Optimizing Capacitated Vehicle Scheduling with Time Windows: A Case\n  Study of RMC Delivery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ready Mixed Concrete Delivery Problem (RMCDP) is a multi-objective\nmulti-constraint dynamic combinatorial optimization problem. From the\noperational research prospective, it is a real life logistic problem that is\nhard to be solved with large instances. In RMCDP, there is a need to optimize\nthe Ready Mixed Concrete ( RMC) delivery by predetermining an optimal schedule\nfor the sites-trips assignments that adheres to strict time, distance, and\ncapacity constraints. This optimization process is subjected to a domain of\nobjectives ranging from achieving maximum revenue to minimizing the operational\ncost. In this paper, we analyze the problem based on realistic assumptions and\nintroduce its theoretical foundation. We derive a complete projection of the\nproblem in graph theory, and prove its NP-Completeness in the complexity\ntheory, which constitutes the base of the proposed approaches. The first\napproach is a graph-based greedy algorithm that deploys dynamic graph weights\nand has polynomial time complexity. The second approach is a heuristic-based\nalgorithm coupled with the dynamic programming and is referred to as Priority\nAlgorithm. This algorithm is carefully designed to address the RMCDP dynamic\ncharacteristic, and satisfies its multi-objectivity. In comparison with the\nstate-of-arts approaches, our algorithm achieves high feasibility rate, lower\ndesign complexity, and significantly lower computational time to find optimal\nor very slightly suboptimal solutions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 23:24:00 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Masoud", "Mohamed", ""], ["Belkasim", "Saeid", ""]]}, {"id": "1810.11152", "submitter": "Haida Zhang", "authors": "Haida Zhang, Zengfeng Huang, Xuemin Lin, Zhe Lin, Wenjie Zhang, Ying\n  Zhang", "title": "Efficient and High-Quality Seeded Graph Matching: Employing High Order\n  Structural Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by many real applications, we study the problem of seeded graph\nmatching. Given two graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$, and a\nsmall set $S$ of pre-matched node pairs $[u, v]$ where $u \\in V_1$ and $v \\in\nV_2$, the problem is to identify a matching between $V_1$ and $V_2$ growing\nfrom $S$, such that each pair in the matching corresponds to the same\nunderlying entity. Recent studies on efficient and effective seeded graph\nmatching have drawn a great deal of attention and many popular methods are\nlargely based on exploring the similarity between local structures to identify\nmatching pairs. While these recent techniques work well on random graphs, their\naccuracy is low over many real networks. Motivated by this, we propose to\nutilize high order neighboring information to improve the matching accuracy. As\na result, a new framework of seeded graph matching is proposed, which employs\nPersonalized PageRank (PPR) to quantify the matching score of each node pair.\nTo further boost the matching accuracy, we propose a novel postponing strategy,\nwhich postpones the selection of pairs that have competitors with similar\nmatching scores. We theoretically prove that the postpone strategy indeed\nsignificantly improves the matching accuracy. To improve the scalability of\nmatching large graphs, we also propose efficient approximation techniques based\non algorithms for computing PPR heavy hitters. Our comprehensive experimental\nstudies on large-scale real datasets demonstrate that, compared with state of\nthe art approaches, our framework not only increases the precision and recall\nboth by a significant margin but also achieves speed-up up to more than one\norder of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 00:38:04 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Zhang", "Haida", ""], ["Huang", "Zengfeng", ""], ["Lin", "Xuemin", ""], ["Lin", "Zhe", ""], ["Zhang", "Wenjie", ""], ["Zhang", "Ying", ""]]}, {"id": "1810.11216", "submitter": "Lisa Wilhelmi", "authors": "Martin Hoefer, Lisa Wilhelmi", "title": "Packing Returning Secretaries", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online secretary problems with returns in combinatorial packing\ndomains with $n$ candidates that arrive sequentially over time in random order.\nThe goal is to accept a feasible packing of candidates of maximum total value.\nIn the first variant, each candidate arrives exactly twice. All $2n$ arrivals\noccur in random order. We propose a simple 0.5-competitive algorithm that can\nbe combined with arbitrary approximation algorithms for the packing domain,\neven when the total value of candidates is a subadditive function. For\nbipartite matching, we obtain an algorithm with competitive ratio at least\n$0.5721 - o(1)$ for growing $n$, and an algorithm with ratio at least $0.5459$\nfor all $n \\ge 1$. We extend all algorithms and ratios to $k \\ge 2$ arrivals\nper candidate.\n  In the second variant, there is a pool of undecided candidates. In each\nround, a random candidate from the pool arrives. Upon arrival a candidate can\nbe either decided (accept/reject) or postponed (returned into the pool). We\nmainly focus on minimizing the expected number of postponements when computing\nan optimal solution. An expected number of $\\Theta(n \\log n)$ is always\nsufficient. For matroids, we show that the expected number can be reduced to\n$O(r \\log (n/r))$, where $r \\le n/2$ is the minimum of the ranks of matroid and\ndual matroid. For bipartite matching, we show a bound of $O(r \\log n)$, where\n$r$ is the size of the optimum matching. For general packing, we show a lower\nbound of $\\Omega(n \\log \\log n)$, even when the size of the optimum is $r =\n\\Theta(\\log n)$.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 07:51:43 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 15:46:10 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Hoefer", "Martin", ""], ["Wilhelmi", "Lisa", ""]]}, {"id": "1810.11232", "submitter": "Stefan Klootwijk", "authors": "Stefan Klootwijk, Bodo Manthey, Sander K. Visser", "title": "Probabilistic Analysis of Optimization Problems on Generalized Random\n  Shortest Path Metrics", "comments": "An extended abstract appeared in the proceedings of WALCOM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple heuristics often show a remarkable performance in practice for\noptimization problems. Worst-case analysis often falls short of explaining this\nperformance. Because of this, \"beyond worst-case analysis\" of algorithms has\nrecently gained a lot of attention, including probabilistic analysis of\nalgorithms.\n  The instances of many optimization problems are essentially a discrete metric\nspace. Probabilistic analysis for such metric optimization problems has\nnevertheless mostly been conducted on instances drawn from Euclidean space,\nwhich provides a structure that is usually heavily exploited in the analysis.\nHowever, most instances from practice are not Euclidean. Little work has been\ndone on metric instances drawn from other, more realistic, distributions. Some\ninitial results have been obtained by Bringmann et al. (Algorithmica, 2013),\nwho have used random shortest path metrics on complete graphs to analyze\nheuristics.\n  The goal of this paper is to generalize these findings to non-complete\ngraphs, especially Erd\\H{o}s-R\\'enyi random graphs. A random shortest path\nmetric is constructed by drawing independent random edge weights for each edge\nin the graph and setting the distance between every pair of vertices to the\nlength of a shortest path between them with respect to the drawn weights. For\nsuch instances, we prove that the greedy heuristic for the minimum distance\nmaximum matching problem, the nearest neighbor and insertion heuristics for the\ntraveling salesman problem, and a trivial heuristic for the $k$-median problem\nall achieve a constant expected approximation ratio. Additionally, we show a\npolynomial upper bound for the expected number of iterations of the 2-opt\nheuristic for the traveling salesman problem.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 09:12:00 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 10:34:58 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Klootwijk", "Stefan", ""], ["Manthey", "Bodo", ""], ["Visser", "Sander K.", ""]]}, {"id": "1810.11262", "submitter": "Igor Sergeev S.", "authors": "Igor S. Sergeev", "title": "Some comments on the structure of the best known networks sorting 16\n  elements", "comments": "7+7 pages, 6 figures (in English and Russian)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an explanation of the structure of the best known sorting networks\nfor 16 elements with respect to the complexity and to the depth due to Green\nand van Voorhis.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 10:56:24 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Sergeev", "Igor S.", ""]]}, {"id": "1810.11291", "submitter": "Spyros Angelopoulos", "authors": "Spyros Angelopoulos and Alejandro Lopez-Ortiz", "title": "Interruptible Algorithms for Multiproblem Solving", "comments": "Extended version of IJCAI 2009 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of designing an interruptible system in\na setting in which $n$ problem instances, all equally important, must be solved\nconcurrently. The system involves scheduling executions of contract algorithms\n(which offer a trade-off between allowable computation time and quality of the\nsolution) in m identical parallel processors. When an interruption occurs, the\nsystem must report a solution to each of the $n$ problem instances. The quality\nof this output is then compared to the best-possible algorithm that has\nforeknowledge of the interruption time and must, likewise, produce solutions to\nall $n$ problem instances. This extends the well-studied setting in which only\none problem instance is queried at interruption time.\n  In this work we first introduce new measures for evaluating the performance\nof interruptible systems in this setting. In particular, we propose the\ndeficiency of a schedule as a performance measure that meets the requirements\nof the problem at hand. We then present a schedule whose performance we prove\nthat is within a small factor from optimal in the general, multiprocessor\nsetting. We also show several lower bounds on the deficiency of schedules on a\nsingle processor. More precisely, we prove a general lower bound of (n+1)/n, an\nimproved lower bound for the two-problem setting (n=2), and a tight lower bound\nfor the class of round-robin schedules. Our techniques can also yield a\nsimpler, alternative proof of the main result of [Bernstein et al, IJCAI 2003]\nconcerning the performance of cyclic schedules in multiprocessor environments.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 12:35:58 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Angelopoulos", "Spyros", ""], ["Lopez-Ortiz", "Alejandro", ""]]}, {"id": "1810.11308", "submitter": "Martin Hirzel", "authors": "Kanat Tangwongsan, Martin Hirzel, Scott Schneider", "title": "Sub-O(log n) Out-of-Order Sliding-Window Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliding-window aggregation summarizes the most recent information in a data\nstream. Users specify how that summary is computed, usually as an associative\nbinary operator because this is the most general known form for which it is\npossible to avoid naively scanning every window. For strictly in-order\narrivals, there are algorithms with $O(1)$ time per window change assuming\nassociative operators. Meanwhile, it is common in practice for streams to have\ndata arriving slightly out of order, for instance, due to clock drifts or\ncommunication delays. Unfortunately, for out-of-order streams, one has to\nresort to latency-prone buffering or pay $O(\\log n)$ time per insert or evict,\nwhere $n$ is the window size.\n  This paper presents the design, analysis, and implementation of FiBA, a novel\nsliding-window aggregation algorithm with an amortized upper bound of $O(\\log\nd)$ time per insert or evict, where $d$ is the distance of the inserted or\nevicted value to the closer end of the window. This means $O(1)$ time for\nin-order arrivals and nearly $O(1)$ time for slightly out-of-order arrivals,\nwith a smooth transition towards $O(\\log n)$ as $d$ approaches $n$. We also\nprove a matching lower bound on running time, showing optimality. Our algorithm\nis as general as the prior state-of-the-art: it requires associativity, but not\ninvertibility nor commutativity. At the heart of the algorithm is a careful\ncombination of finger-searching techniques, lazy rebalancing, and\nposition-aware partial aggregates. We further show how to answer range queries\nthat aggregate subwindows for window sharing. Finally, our experimental\nevaluation shows that FiBA performs well in practice and supports the\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 13:17:20 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Tangwongsan", "Kanat", ""], ["Hirzel", "Martin", ""], ["Schneider", "Scott", ""]]}, {"id": "1810.11319", "submitter": "Ruben Mayer", "authors": "Christian Mayer and Ruben Mayer and Sukanya Bhowmik and Lukas Epple\n  and Kurt Rothermel", "title": "HYPE: Massive Hypergraph Partitioning with Neighborhood Expansion", "comments": "To appear in Proceedings of IEEE 2018 International Conference on Big\n  Data (BigData '18), 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important real-world applications-such as social networks or distributed\ndata bases-can be modeled as hypergraphs. In such a model, vertices represent\nentities-such as users or data records-whereas hyperedges model a group\nmembership of the vertices-such as the authorship in a specific topic or the\nmembership of a data record in a specific replicated shard. To optimize such\napplications, we need an efficient and effective solution to the NP-hard\nbalanced k-way hypergraph partitioning problem. However, existing hypergraph\npartitioners that scale to very large graphs do not effectively exploit the\nhypergraph structure when performing the partitioning decisions. We propose\nHYPE, a hypergraph partitionier that exploits the neighborhood relations\nbetween vertices in the hypergraph using an efficient implementation of\nneighborhood expansion. HYPE improves partitioning quality by up to 95% and\nreduces runtime by up to 39% compared to streaming partitioning.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 13:35:55 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 09:22:01 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 11:42:15 GMT"}, {"version": "v4", "created": "Wed, 14 Nov 2018 10:59:29 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Mayer", "Christian", ""], ["Mayer", "Ruben", ""], ["Bhowmik", "Sukanya", ""], ["Epple", "Lukas", ""], ["Rothermel", "Kurt", ""]]}, {"id": "1810.11332", "submitter": "Arin Chaudhuri", "authors": "Arin Chaudhuri, Wenhao Hu", "title": "A fast algorithm for computing distance correlation", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2019.01.016", "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical dependence measures such as Pearson correlation, Spearman's $\\rho$,\nand Kendall's $\\tau$ can detect only monotonic or linear dependence. To\novercome these limitations, Szekely et al.(2007) proposed distance covariance\nas a weighted $L_2$ distance between the joint characteristic function and the\nproduct of marginal distributions. The distance covariance is $0$ if and only\nif two random vectors ${X}$ and ${Y}$ are independent. This measure has the\npower to detect the presence of a dependence structure when the sample size is\nlarge enough. They further showed that the sample distance covariance can be\ncalculated simply from modified Euclidean distances, which typically requires\n$\\mathcal{O}(n^2)$ cost. The quadratic computing time greatly limits the\napplication of distance covariance to large data. In this paper, we present a\nsimple exact $\\mathcal{O}(n\\log(n))$ algorithm to calculate the sample distance\ncovariance between two univariate random variables. The proposed method\nessentially consists of two sorting steps, so it is easy to implement.\nEmpirical results show that the proposed algorithm is significantly faster than\nstate-of-the-art methods. The algorithm's speed will enable researchers to\nexplore complicated dependence structures in large datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 13:57:23 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 18:40:56 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Chaudhuri", "Arin", ""], ["Hu", "Wenhao", ""]]}, {"id": "1810.11421", "submitter": "Andrew van der Poel", "authors": "Kyle Kloster, Blair D. Sullivan, Andrew van der Poel", "title": "Mining Maximal Induced Bicliques using Odd Cycle Transversals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many common graph data mining tasks take the form of identifying dense\nsubgraphs (e.g. clustering, clique-finding, etc). In biological applications,\nthe natural model for these dense substructures is often a complete bipartite\ngraph (biclique), and the problem requires enumerating all maximal bicliques\n(instead of just identifying the largest or densest). The best known algorithm\nin general graphs is due to Dias et al., and runs in time O(M |V|^4 ), where M\nis the number of maximal induced bicliques (MIBs) in the graph. When the graph\nbeing searched is itself bipartite, Zhang et al. give a faster algorithm where\nthe time per MIB depends on the number of edges in the graph. In this work, we\npresent a new algorithm for enumerating MIBs in general graphs, whose run time\ndepends on how \"close to bipartite\" the input is. Specifically, the runtime is\nparameterized by the size k of an odd cycle transversal (OCT), a vertex set\nwhose deletion results in a bipartite graph. Our algorithm runs in time O(M\n|V||E|k^2 3^(k/3) ), which is an improvement on Dias et al. whenever k <=\n3log_3(|V|). We implement our algorithm alongside a variant of Dias et al.'s in\nopen-source C++ code, and experimentally verify that the OCT-based approach is\nfaster in practice on graphs with a wide variety of sizes, densities, and OCT\ndecompositions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 17:08:43 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 19:21:06 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Kloster", "Kyle", ""], ["Sullivan", "Blair D.", ""], ["van der Poel", "Andrew", ""]]}, {"id": "1810.11655", "submitter": "Sabine Bertram", "authors": "Sabine Bertram and Co-Pierre Georg", "title": "A privacy-preserving system for data ownership using blockchain and\n  distributed databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain has the potential to revolutionize the way we store, use, and\nprocess data. Information on most blockchains can be viewed by every node\nhosting the blockchain, which means that most blockchains cannot handle private\ndata. Decentralized databases exist that guarantee privacy by encrypting user\ndata with the user's private key, but this prevents easy data sharing. However,\nin many real world applications, from student data to medical records, it is\ndesirable that user data is anonymously searchable. In this paper we present a\nnovel system that gives users ownership over their data while at the same time\nenabling them to make their data searchable within previously agreed upon\nlimits. Our system implements a strong notion of ownership using a\nself-sovereign identity system and a weak notion of ownership using multiple\ncentralized databases together with a blockchain and a tumbling process. We\ndiscuss applications of our methods to university's student records and medical\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 14:44:52 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Bertram", "Sabine", ""], ["Georg", "Co-Pierre", ""]]}, {"id": "1810.11656", "submitter": "Kshitij Gajjar", "authors": "Kshitij Gajjar, Jaikumar Radhakrishnan", "title": "Minimizing Branching Vertices in Distance-preserving Subgraphs", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is $\\mathsf{NP}$-hard to determine the minimum number of branching\nvertices needed in a single-source distance-preserving subgraph of an\nundirected graph. We show that this problem can be solved in polynomial time if\nthe input graph is an interval graph.\n  In earlier work, it was shown that every interval graph with $k$ terminal\nvertices admits an all-pairs distance-preserving subgraph with $O(k\\log k)$\nbranching vertices. We consider graphs that can be expressed as the strong\nproduct of two interval graphs, and present a polynomial time algorithm that\ntakes such a graph with $k$ terminals as input, and outputs an all-pairs\ndistance-preserving subgraph of it with $O(k^2)$ branching vertices. This bound\nis tight.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 14:50:18 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Gajjar", "Kshitij", ""], ["Radhakrishnan", "Jaikumar", ""]]}, {"id": "1810.11700", "submitter": "Dimitrios Thilikos", "authors": "Julien Baste and Didem G\\\"oz\\\"upek and Mordechai Shalom and Dimitrios\n  M. Thilikos", "title": "Minimum Reload Cost Graph Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of Reload cost in a graph refers to the cost that occurs while\ntraversing a vertex via two of its incident edges. This cost is uniquely\ndetermined by the colors of the two edges. This concept has various\napplications in transportation networks, communication networks, and energy\ndistribution networks. Various problems using this model are defined and\nstudied in the literature. The problem of finding a spanning tree whose\ndiameter with respect to the reload costs is the smallest possible, the\nproblems of finding a path, trail or walk with minimum total reload cost\nbetween two given vertices, problems about finding a proper edge coloring of a\ngraph such that the total reload cost is minimized, the problem of finding a\nspanning tree such that the sum of the reload costs of all paths between all\npairs of vertices is minimized, and the problem of finding a set of cycles of\nminimum reload cost, that cover all the vertices of a graph, are examples of\nsuch problems. % In this work we focus on the last problem. Noting that a cycle\ncover of a graph is a 2-factor of it, we generalize the problem to that of\nfinding an $r$-factor of minimum reload cost of an edge colored graph. We prove\nseveral NP-hardness results for special cases of the problem. Namely, bounded\ndegree graphs, planar graphs, bounded total cost, and bounded number of\ndistinct costs. For the special case of $r=2$, our results imply an improved\nNP-hardness result. On the positive side, we present a polynomial-time solvable\nspecial case which provides a tight boundary between the polynomial and hard\ncases in terms of $r$ and the maximum degree of the graph. We then investigate\nthe parameterized complexity of the problem, prove W[1]-hardness results and\npresent an FPT algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 20:37:42 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 14:49:14 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Baste", "Julien", ""], ["G\u00f6z\u00fcpek", "Didem", ""], ["Shalom", "Mordechai", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1810.11829", "submitter": "Thodoris Lykouris", "authors": "Avrim Blum, Suriya Gunasekar, Thodoris Lykouris, Nathan Srebro", "title": "On preserving non-discrimination when combining expert advice", "comments": "Appeared in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the interplay between sequential decision making and avoiding\ndiscrimination against protected groups, when examples arrive online and do not\nfollow distributional assumptions. We consider the most basic extension of\nclassical online learning: \"Given a class of predictors that are individually\nnon-discriminatory with respect to a particular metric, how can we combine them\nto perform as well as the best predictor, while preserving non-discrimination?\"\nSurprisingly we show that this task is unachievable for the prevalent notion of\n\"equalized odds\" that requires equal false negative rates and equal false\npositive rates across groups. On the positive side, for another notion of\nnon-discrimination, \"equalized error rates\", we show that running separate\ninstances of the classical multiplicative weights algorithm for each group\nachieves this guarantee. Interestingly, even for this notion, we show that\nalgorithms with stronger performance guarantees than multiplicative weights\ncannot preserve non-discrimination.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 16:28:30 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 21:37:00 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Blum", "Avrim", ""], ["Gunasekar", "Suriya", ""], ["Lykouris", "Thodoris", ""], ["Srebro", "Nathan", ""]]}, {"id": "1810.11863", "submitter": "Amirbehshad Shahrasbi", "authors": "Bernhard Haeupler, Aviad Rubinstein, and Amirbehshad Shahrasbi", "title": "Near-Linear Time Insertion-Deletion Codes and\n  (1+$\\varepsilon$)-Approximating Edit Distance via Indexing", "comments": null, "journal-ref": null, "doi": "10.1145/3313276.3316371", "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce fast-decodable indexing schemes for edit distance which can be\nused to speed up edit distance computations to near-linear time if one of the\nstrings is indexed by an indexing string $I$. In particular, for every length\n$n$ and every $\\varepsilon >0$, one can in near linear time construct a string\n$I \\in \\Sigma'^n$ with $|\\Sigma'| = O_{\\varepsilon}(1)$, such that, indexing\nany string $S \\in \\Sigma^n$, symbol-by-symbol, with $I$ results in a string $S'\n\\in \\Sigma''^n$ where $\\Sigma'' = \\Sigma \\times \\Sigma'$ for which edit\ndistance computations are easy, i.e., one can compute a\n$(1+\\varepsilon)$-approximation of the edit distance between $S'$ and any other\nstring in $O(n \\text{poly}(\\log n))$ time.\n  Our indexing schemes can be used to improve the decoding complexity of\nstate-of-the-art error correcting codes for insertions and deletions. In\nparticular, they lead to near-linear time decoding algorithms for the\ninsertion-deletion codes of [Haeupler, Shahrasbi; STOC `17] and faster decoding\nalgorithms for list-decodable insertion-deletion codes of [Haeupler, Shahrasbi,\nSudan; ICALP `18]. Interestingly, the latter codes are a crucial ingredient in\nthe construction of fast-decodable indexing schemes.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 19:21:33 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 23:37:33 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Rubinstein", "Aviad", ""], ["Shahrasbi", "Amirbehshad", ""]]}, {"id": "1810.11896", "submitter": "Nima Anari", "authors": "Nima Anari, Constantinos Daskalakis, Wolfgang Maass, Christos H.\n  Papadimitriou, Amin Saberi, Santosh Vempala", "title": "Smoothed Analysis of Discrete Tensor Decomposition and Assemblies of\n  Neurons", "comments": "To appear in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze linear independence of rank one tensors produced by tensor powers\nof randomly perturbed vectors. This enables efficient decomposition of sums of\nhigh-order tensors. Our analysis builds upon [BCMV14] but allows for a wider\nrange of perturbation models, including discrete ones. We give an application\nto recovering assemblies of neurons.\n  Assemblies are large sets of neurons representing specific memories or\nconcepts. The size of the intersection of two assemblies has been shown in\nexperiments to represent the extent to which these memories co-occur or these\nconcepts are related; the phenomenon is called association of assemblies. This\nsuggests that an animal's memory is a complex web of associations, and poses\nthe problem of recovering this representation from cognitive data. Motivated by\nthis problem, we study the following more general question: Can we reconstruct\nthe Venn diagram of a family of sets, given the sizes of their $\\ell$-wise\nintersections? We show that as long as the family of sets is randomly\nperturbed, it is enough for the number of measurements to be polynomially\nlarger than the number of nonempty regions of the Venn diagram to fully\nreconstruct the diagram.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 22:15:48 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Anari", "Nima", ""], ["Daskalakis", "Constantinos", ""], ["Maass", "Wolfgang", ""], ["Papadimitriou", "Christos H.", ""], ["Saberi", "Amin", ""], ["Vempala", "Santosh", ""]]}, {"id": "1810.11905", "submitter": "Shanshan Wu", "authors": "Shanshan Wu, Sujay Sanghavi, Alexandros G. Dimakis", "title": "Sparse Logistic Regression Learns All Discrete Pairwise Graphical Models", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the effectiveness of a classical algorithm for recovering the\nMarkov graph of a general discrete pairwise graphical model from i.i.d.\nsamples. The algorithm is (appropriately regularized) maximum conditional\nlog-likelihood, which involves solving a convex program for each node; for\nIsing models this is $\\ell_1$-constrained logistic regression, while for more\ngeneral alphabets an $\\ell_{2,1}$ group-norm constraint needs to be used. We\nshow that this algorithm can recover any arbitrary discrete pairwise graphical\nmodel, and also characterize its sample complexity as a function of model\nwidth, alphabet size, edge parameter accuracy, and the number of variables. We\nshow that along every one of these axes, it matches or improves on all existing\nresults and algorithms for this problem. Our analysis applies a sharp\ngeneralization error bound for logistic regression when the weight vector has\nan $\\ell_1$ constraint (or $\\ell_{2,1}$ constraint) and the sample vector has\nan $\\ell_{\\infty}$ constraint (or $\\ell_{2, \\infty}$ constraint). We also show\nthat the proposed convex programs can be efficiently solved in $\\tilde{O}(n^2)$\nrunning time (where $n$ is the number of variables) under the same statistical\nguarantees. We provide experimental results to support our analysis.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 23:40:42 GMT"}, {"version": "v2", "created": "Sun, 3 Feb 2019 03:40:47 GMT"}, {"version": "v3", "created": "Tue, 18 Jun 2019 20:53:05 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Wu", "Shanshan", ""], ["Sanghavi", "Sujay", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1810.11965", "submitter": "Tu Nguyen", "authors": "Ngoc-Tu Nguyen, Zhi-Li Zhang", "title": "R-BBG$_2$: Recursive Bipartition of Bi-connected Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph $G(V, E)$, it is well known that partitioning a\ngraph $G$ into $q$ connected subgraphs of equal or specificed sizes is in\ngeneral NP-hard problem. On the other hand, it has been shown that the\nq-partition problem is solvable in polynomial time for q-connected graphs. For\nexample, efficient polynomial time algorithms for finding 2-partition\n(bipartition) or 3-partition of 2-connected or 3-connected have been developed\nin the literature. In this paper, we are interested in the following problem:\ngiven a bi-connected graph $G$ of size $n$, can we partition it into two\n(connected) sub-graphs, $G[V_1]$ and $G[V_2]$ of sizes $n_1$ and $n_2$ such as\nboth $G[V_1]$ and $G[V_2]$ are also bi-connected (and $n_1+n_2=n$)? We refer to\nthis problem as the recursive bipartition problem of bi-connected graphs,\ndenoted by R-BBG$_2$. We show that a ploynomial algorithm exists to both decide\nthe recursive bipartion problem R-BBG$_2$ and find the corresponding\nbi-connected subgraphs when such a recursive bipartition exists.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 05:50:29 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Nguyen", "Ngoc-Tu", ""], ["Zhang", "Zhi-Li", ""]]}, {"id": "1810.12030", "submitter": "Joran van Apeldoorn", "authors": "Joran van Apeldoorn and Sander Gribling", "title": "Simon's problem for linear functions", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simon's problem asks the following: determine if a function $f: \\{0,1\\}^n\n\\rightarrow \\{0,1\\}^n$ is one-to-one or if there exists a unique $s \\in\n\\{0,1\\}^n$ such that $f(x) = f(x \\oplus s)$ for all $x \\in \\{0,1\\}^n$, given\nthe promise that exactly one of the two holds. A classical algorithm that can\nsolve this problem for every $f$ requires $2^{\\Omega(n)}$ queries to $f$. Simon\nshowed that there is a quantum algorithm that can solve this promise problem\nfor every $f$ using only $\\mathcal O(n)$ quantum queries to $f$. A matching\nlower bound on the number of quantum queries was given by Koiran et al., even\nfor functions $f: {\\mathbb{F}_p^n} \\to {\\mathbb{F}_p^n}$. We give a short proof\nthat $\\mathcal O(n)$ quantum queries is optimal even when we are additionally\npromised that $f$ is linear. This is somewhat surprising because for linear\nfunctions there even exists a classical $n$-query algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 09:47:30 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 12:55:50 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["van Apeldoorn", "Joran", ""], ["Gribling", "Sander", ""]]}, {"id": "1810.12047", "submitter": "Martin Aum\\\"uller", "authors": "Martin Aum\\\"uller and Nikolaj Hass", "title": "Simple and Fast BlockQuicksort using Lomuto's Partitioning Scheme", "comments": "Accepted at ALENEX 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents simple variants of the BlockQuicksort algorithm described\nby Edelkamp and Weiss (ESA 2016). The simplification is achieved by using\nLomuto's partitioning scheme instead of Hoare's crossing pointer technique to\npartition the input. To achieve a robust sorting algorithm that works well on\nmany different input types, the paper introduces a novel two-pivot variant of\nLomuto's partitioning scheme. A surprisingly simple twist to the generic\ntwo-pivot quicksort approach makes the algorithm robust. The paper provides an\nanalysis of the theoretical properties of the proposed algorithms and compares\nthem to their competitors. The analysis shows that Lomuto-based approaches\nincur a higher average sorting cost than the Hoare-based approach of\nBlockQuicksort. Moreover, the analysis is particularly useful to reason about\npivot choices that suit the two-pivot approach. An extensive experimental study\nshows that, despite their worse theoretical behavior, the simpler variants\nperform as well as the original version of BlockQuicksort.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 10:37:21 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Aum\u00fcller", "Martin", ""], ["Hass", "Nikolaj", ""]]}, {"id": "1810.12065", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li, Zhao Song", "title": "On the Convergence Rate of Training Recurrent Neural Networks", "comments": "V2/V3/V4 polish writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can local-search methods such as stochastic gradient descent (SGD) avoid\nbad local minima in training multi-layer neural networks? Why can they fit\nrandom labels even given non-convex and non-smooth architectures? Most existing\ntheory only covers networks with one hidden layer, so can we go deeper?\n  In this paper, we focus on recurrent neural networks (RNNs) which are\nmulti-layer networks widely used in natural language processing. They are\nharder to analyze than feedforward neural networks, because the $\\textit{same}$\nrecurrent unit is repeatedly applied across the entire time horizon of length\n$L$, which is analogous to feedforward networks of depth $L$. We show when the\nnumber of neurons is sufficiently large, meaning polynomial in the training\ndata size and in $L$, then SGD is capable of minimizing the regression loss in\nthe linear convergence rate. This gives theoretical evidence of how RNNs can\nmemorize data.\n  More importantly, in this paper we build general toolkits to analyze\nmulti-layer networks with ReLU activations. For instance, we prove why ReLU\nactivations can prevent exponential gradient explosion or vanishing, and build\na perturbation theory to analyze first-order approximation of multi-layer\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 11:45:02 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 15:25:15 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 11:47:47 GMT"}, {"version": "v4", "created": "Mon, 27 May 2019 10:08:59 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""], ["Song", "Zhao", ""]]}, {"id": "1810.12086", "submitter": "Davi Castro-Silva", "authors": "Davi Castro-Silva, Eric Gourdin", "title": "A study on load-balanced variants of the bin packing problem", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider several extensions of the fractional bin packing problem, a\nrelaxation of the traditional bin packing problem where the objects may be\nsplit across multiple bins. In these extensions, we introduce load-balancing\nconstraints imposing that the share of each object which is assigned to a same\nbin must be equal. We propose a Mixed-Integer Programming (MIP) formulation and\nshow that the problem becomes NP-hard if we limit to at most 3 the number of\nbins across which each object can be split.\n  We then consider a variant where the balanced allocations of objects to bins\nmay be done in successive rounds; this problem was inspired by\ntelecommunication applications, and may be used to model simple Live Streaming\nnetworks. We show that two rounds are always sufficient to completely assign\nall objects to the bins and then provide an optimal polynomial-time allocation\nalgorithm for this problem.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 13:12:21 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Castro-Silva", "Davi", ""], ["Gourdin", "Eric", ""]]}, {"id": "1810.12130", "submitter": "Tu Nguyen", "authors": "Ngoc-Tu Nguyen, Bing-Hong Liu, Shao-I Chu, and Hao-Zhe Weng", "title": "Challenges, Designs, and Performances of a Distributed Algorithm for\n  Minimum-Latency of Data-Aggregation in Multi-Channel WSNs", "comments": "IEEE Transactions on Network and Service Management (accepted with\n  minor revisions)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In wireless sensor networks (WSNs), the sensed data by sensors need to be\ngathered, so that one very important application is periodical data collection.\nThere is much effort which aimed at the data collection scheduling algorithm\ndevelopment to minimize the latency. Most of previous works investigating the\nminimum latency of data collection issue have an ideal assumption that the\nnetwork is a centralized system, in which the entire network is completely\nsynchronized with full knowledge of components. In addition, most of existing\nworks often assume that any (or no) data in the network are allowed to be\naggregated into one packet and the network models are often treated as tree\nstructures. However, in practical, WSNs are more likely to be distributed\nsystems, since each sensor's knowledge is disjointed to each other, and a fixed\nnumber of data are allowed to to be aggregated into one packet. This is a\nformidable motivation for us to investigate the problem of minimum latency for\nthe data aggregation without data collision in the distributed WSNs when the\nsensors are considered to be assigned the channels and the data are compressed\nwith a flexible aggregation ratio, termed the minimum-latency\ncollision-avoidance multiple-data-aggregation scheduling with multi-channel\n(MLCAMDAS-MC) problem. A new distributed algorithm, termed the distributed\ncollision-avoidance scheduling (DCAS) algorithm, is proposed to address the\nMLCAMDAS-MC. Finally, we provide the theoretical analyses of DCAS and conduct\nextensive simulations to demonstrate the performance of DCAS.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 13:56:03 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Nguyen", "Ngoc-Tu", ""], ["Liu", "Bing-Hong", ""], ["Chu", "Shao-I", ""], ["Weng", "Hao-Zhe", ""]]}, {"id": "1810.12277", "submitter": "Kathryn Nurse", "authors": "Matt DeVos, Kathryn Nurse", "title": "A Maximum Linear Arrangement Problem on Directed Graphs", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new arrangement problem on directed graphs, Maximum Directed\nLinear Arrangement (MaxDLA). This is a directed variant of a similar problem\nfor undirected graphs, in which however one seeks maximum and not minimum; this\nproblem known as the Minimum Linear Arrangement Problem (MinLA) has been much\nstudied in the literature. We establish a number of theorems illustrating the\nbehavior and complexity of MaxDLA. First, we relate MaxDLA to Maximum Directed\nCut (MaxDiCut) by proving that every simple digraph $D$ on $n$ vertices\nsatisfies $\\frac{n}{2}$$maxDiCut(D) \\leq MaxDLA(D) \\leq (n-1)MaxDiCut(D)$.\nNext, we prove that MaxDiCut is NP-Hard for planar digraphs (even with the\nadded restriction of maximum degree 15); it follows from the above bounds that\nMaxDLA is also NP-Hard for planar digraphs. In contrast, Hadlock (1975) and\nDorfman and Orlova (1972) showed that the undirected Maximum Cut problem is\nsolvable in polynomial time on planar graphs.\n  On the positive side, we present a polynomial-time algorithm for solving\nMaxDLA on orientations of trees with degree bounded by a constant, which\ntranslates to a polynomial-time algorithm for solving MinLA on the complements\nof those trees. This pairs with results by Goldberg and Klipker (1976),\nShiloach (1979) and Chung (1984) solving MinLA in polynomial time on trees.\nFinally, analogues of Harper's famous isoperimetric inequality for the\nhypercube, in the setting of MaxDLA, are shown for tournaments, orientations of\ngraphs with degree at most two, and transitive acyclic digraphs.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 17:45:11 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["DeVos", "Matt", ""], ["Nurse", "Kathryn", ""]]}, {"id": "1810.12322", "submitter": "Sebastian Wild", "authors": "Conrado Mart\\'inez, Markus Nebel, Sebastian Wild", "title": "Sesquickselect: One and a half pivots for cache-efficient selection", "comments": "appears in ANALCO 2019", "journal-ref": null, "doi": "10.1137/1.9781611975505.6", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of unmatched improvements in CPU performance, memory transfers have\nbecome a bottleneck of program execution. As discovered in recent years, this\nalso affects sorting in internal memory. Since partitioning around several\npivots reduces overall memory transfers, we have seen renewed interest in\nmultiway Quicksort. Here, we analyze in how far multiway partitioning helps in\nQuickselect.\n  We compute the expected number of comparisons and scanned elements\n(approximating memory transfers) for a generic class of (non-adaptive) multiway\nQuickselect and show that three or more pivots are not helpful, but two pivots\nare. Moreover, we consider \"adaptive\" variants which choose partitioning and\npivot-selection methods in each recursive step from a finite set of\nalternatives depending on the current (relative) sought rank. We show that\n\"Sesquickselect\", a new Quickselect variant that uses either one or two pivots,\nmakes better use of small samples w.r.t. memory transfers than other\nQuickselect variants.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 18:02:20 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Mart\u00ednez", "Conrado", ""], ["Nebel", "Markus", ""], ["Wild", "Sebastian", ""]]}, {"id": "1810.12388", "submitter": "Jiecao Chen", "authors": "Jiecao Chen, Qin Zhang", "title": "Distinct Sampling on Streaming Data with Near-Duplicates", "comments": "Accepted to PODS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study how to perform distinct sampling in the streaming\nmodel where data contain near-duplicates. The goal of distinct sampling is to\nreturn a distinct element uniformly at random from the universe of elements,\ngiven that all the near-duplicates are treated as the same element. We also\nextend the result to the sliding window cases in which we are only interested\nin the most recent items. We present algorithms with provable theoretical\nguarantees for datasets in the Euclidean space, and also verify their\neffectiveness via an extensive set of experiments.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 20:16:28 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Chen", "Jiecao", ""], ["Zhang", "Qin", ""]]}, {"id": "1810.12518", "submitter": "Ilias Zadik", "authors": "Christian Borgs, Jennifer Chayes, Adam Smith, Ilias Zadik", "title": "Private Algorithms Can Always Be Extended", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR cs.DS stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following fundamental question on $\\epsilon$-differential\nprivacy. Consider an arbitrary $\\epsilon$-differentially private algorithm\ndefined on a subset of the input space. Is it possible to extend it to an\n$\\epsilon'$-differentially private algorithm on the whole input space for some\n$\\epsilon'$ comparable with $\\epsilon$? In this note we answer affirmatively\nthis question for $\\epsilon'=2\\epsilon$. Our result applies to every input\nmetric space and space of possible outputs. This result originally appeared in\na recent paper by the authors [BCSZ18]. We present a self-contained version in\nthis note, in the hopes that it will be broadly useful.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 04:12:13 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 15:37:29 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Borgs", "Christian", ""], ["Chayes", "Jennifer", ""], ["Smith", "Adam", ""], ["Zadik", "Ilias", ""]]}, {"id": "1810.12861", "submitter": "Nived Rajaraman", "authors": "Nived Rajaraman, Rahul Vaze", "title": "Submodular Maximization Under A Matroid Constraint: Asking more from an\n  old friend, the Greedy Algorithm", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The classical problem of maximizing a submodular function under a matroid\nconstraint is considered. Defining a new measure for the increments made by the\ngreedy algorithm at each step, called the discriminant, improved approximation\nratio guarantees are derived for the greedy algorithm. At each step,\ndiscriminant measures the multiplicative gap in the incremental valuation\nbetween the item chosen by the greedy algorithm and the largest potential\nincremental valuation for eligible items not selected by it. The new guarantee\nsubsumes all the previous known results for the greedy algorithm, including the\ncurvature based ones, and the derived guarantees are shown to be tight via\nconstructing specific instances. More refined approximation guarantee is\nderived for a special case called the submodular welfare maximization/partition\nproblem that is also tight, for both the offline and the online case.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 17:02:12 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Rajaraman", "Nived", ""], ["Vaze", "Rahul", ""]]}, {"id": "1810.12980", "submitter": "Sitan Chen", "authors": "Sitan Chen, Michelle Delcourt, Ankur Moitra, Guillem Perarnau, Luke\n  Postle", "title": "Improved Bounds for Randomly Sampling Colorings via Linear Programming", "comments": "This is a merger of arxiv:1804.04025 and arxiv:1804.03156.\n  Preliminary version accepted to SODA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math-ph math.MP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known conjecture in computer science and statistical physics is that\nGlauber dynamics on the set of $k$-colorings of a graph $G$ on $n$ vertices\nwith maximum degree $\\Delta$ is rapidly mixing for $k\\ge\\Delta+2$. In FOCS\n1999, Vigoda showed that the flip dynamics (and therefore also Glauber\ndynamics) is rapidly mixing for any $k>\\frac{11}{6}\\Delta$. It turns out that\nthere is a natural barrier at $\\frac{11}{6}$, below which there is no one-step\ncoupling that is contractive with respect to the Hamming metric, even for the\nflip dynamics.\n  We use linear programming and duality arguments to fully characterize the\nobstructions to going beyond $\\frac{11}{6}$. These extremal configurations turn\nout to be quite brittle, and in this paper we use this to give two proofs that\nthe Glauber dynamics is rapidly mixing for any $k\\ge\\left(\\frac{11}{6} -\n\\epsilon_0\\right)\\Delta$ for some absolute constant $\\epsilon_0>0$. This is the\nfirst improvement to Vigoda's result that holds for general graphs. Our first\napproach analyzes a variable-length coupling in which these configurations\nbreak apart with high probability before the coupling terminates, and our other\napproach analyzes a one-step path coupling with a new metric that counts the\nextremal configurations. Additionally, our results extend to list coloring, a\nwidely studied generalization of coloring, where the previously best known\nresults required $k > 2 \\Delta$.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 19:51:29 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Chen", "Sitan", ""], ["Delcourt", "Michelle", ""], ["Moitra", "Ankur", ""], ["Perarnau", "Guillem", ""], ["Postle", "Luke", ""]]}, {"id": "1810.12982", "submitter": "Dekel Tsur", "authors": "Dekel Tsur", "title": "Weighted vertex cover on graphs with maximum degree 3", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a parameterized algorithm for weighted vertex cover on graphs with\nmaximum degree 3 whose time complexity is $O^*(1.402^t)$, where $t$ is the\nminimum size of a vertex cover of the input graph.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 19:53:03 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Tsur", "Dekel", ""]]}, {"id": "1810.13187", "submitter": "Anders Aamand", "authors": "Anders Aamand and Mikkel Thorup", "title": "Non-Empty Bins with Simple Tabulation Hashing", "comments": "To appear at SODA'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the hashing of a set $X\\subseteq U$ with $|X|=m$ using a simple\ntabulation hash function $h:U\\to [n]=\\{0,\\dots,n-1\\}$ and analyse the number of\nnon-empty bins, that is, the size of $h(X)$. We show that the expected size of\n$h(X)$ matches that with fully random hashing to within low-order terms. We\nalso provide concentration bounds. The number of non-empty bins is a\nfundamental measure in the balls and bins paradigm, and it is critical in\napplications such as Bloom filters and Filter hashing. For example, normally\nBloom filters are proportioned for a desired low false-positive probability\nassuming fully random hashing (see \\url{en.wikipedia.org/wiki/Bloom_filter}).\nOur results imply that if we implement the hashing with simple tabulation, we\nobtain the same low false-positive probability for any possible input.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 09:53:50 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Aamand", "Anders", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1810.13258", "submitter": "Luigi Carratino", "authors": "Alessandro Rudi, Daniele Calandriello, Luigi Carratino, Lorenzo\n  Rosasco", "title": "On Fast Leverage Score Sampling and Optimal Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leverage score sampling provides an appealing way to perform approximate\ncomputations for large matrices. Indeed, it allows to derive faithful\napproximations with a complexity adapted to the problem at hand. Yet,\nperforming leverage scores sampling is a challenge in its own right requiring\nfurther approximations. In this paper, we study the problem of leverage score\nsampling for positive definite matrices defined by a kernel. Our contribution\nis twofold. First we provide a novel algorithm for leverage score sampling and\nsecond, we exploit the proposed method in statistical learning by deriving a\nnovel solver for kernel ridge regression. Our main technical contribution is\nshowing that the proposed algorithms are currently the most efficient and\naccurate for these problems.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 12:54:56 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 11:08:26 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Rudi", "Alessandro", ""], ["Calandriello", "Daniele", ""], ["Carratino", "Luigi", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1810.13297", "submitter": "Marcel Radermacher", "authors": "Lukas Barth, Guido Br\\\"uckner, Paul Jungeblut, Marcel Radermacher", "title": "Multilevel Planarity", "comments": "Preliminary work appeared in the Proceedings of the 13th\n  International Conference and Workshops on Algorithms and Computation (WALCOM\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce and study the multilevel-planarity testing\nproblem, which is a generalization of upward planarity and level planarity. Let\n$G = (V, E)$ be a directed graph and let $\\ell: V \\to \\mathcal P(\\mathbb Z)$ be\na function that assigns a finite set of integers to each vertex. A\nmultilevel-planar drawing of $G$ is a planar drawing of $G$ such that the\n$y$-coordinate of each vertex $v \\in V$ is $y(v) \\in \\ell(v)$, and each edge is\ndrawn as a strictly $y$-monotone curve. We present linear-time algorithms for\ntesting multilevel planarity of embedded graphs with a single source and of\noriented cycles. Complementing these algorithmic results, we show that\nmultilevel-planarity testing is NP-complete even in very restricted cases.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 14:18:11 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Barth", "Lukas", ""], ["Br\u00fcckner", "Guido", ""], ["Jungeblut", "Paul", ""], ["Radermacher", "Marcel", ""]]}, {"id": "1810.13331", "submitter": "Mirza Galib Anwarul Husain Baig", "authors": "Mirza Galib Anwarul Husain Baig, Deepanjan Kesh, and Chirag Sodani", "title": "A Two Query Adaptive Bitprobe Scheme Storing Five Elements", "comments": "This paper is accepted for the proceeding of WALCOM 2019. It contains\n  some of the cases which could not be added in the original paper due to page\n  limit. Furthermore, we have added a counter example for a six element subset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are studying the adaptive bitprobe model to store an arbitrary subset S of\nsize at most five from a universe U of size m and answer the membership queries\nof the form \"Is x in S?\" in two bitprobes. In this paper, we present a data\nstructure for the aforementioned problem. Our data structure takes O(m^{10/11})\nspace. This result improves the non-explicit result by Garg and Radhakrishnan\n[2015] which takes O(m^{20/21}) space, and the explicit result by Garg [2016]\nwhich takes O(m^{18/19} ) space for the aforementioned set and query sizes.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 15:21:35 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 10:02:59 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 05:24:42 GMT"}, {"version": "v4", "created": "Wed, 23 Oct 2019 16:27:42 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Baig", "Mirza Galib Anwarul Husain", ""], ["Kesh", "Deepanjan", ""], ["Sodani", "Chirag", ""]]}, {"id": "1810.13351", "submitter": "Sepehr Assadi", "authors": "Arpit Agarwal, Sepehr Assadi, Sanjeev Khanna", "title": "Stochastic Submodular Cover with Limited Adaptivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the submodular cover problem, we are given a non-negative monotone\nsubmodular function $f$ over a ground set $E$ of items, and the goal is to\nchoose a smallest subset $S \\subseteq E$ such that $f(S) = Q$ where $Q = f(E)$.\nIn the stochastic version of the problem, we are given $m$ stochastic items\nwhich are different random variables that independently realize to some item in\n$E$, and the goal is to find a smallest set of stochastic items whose\nrealization $R$ satisfies $f(R) = Q$. The problem captures as a special case\nthe stochastic set cover problem and more generally, stochastic covering\ninteger programs.\n  We define an $r$-round adaptive algorithm to be an algorithm that chooses a\npermutation of all available items in each round $k \\in [r]$, and a threshold\n$\\tau_k$, and realizes items in the order specified by the permutation until\nthe function value is at least $\\tau_k$. The permutation for each round $k$ is\nchosen adaptively based on the realization in the previous rounds, but the\nordering inside each round remains fixed regardless of the realizations seen\ninside the round. Our main result is that for any integer $r$, there exists a\npoly-time $r$-round adaptive algorithm for stochastic submodular cover whose\nexpected cost is $\\tilde{O}(Q^{{1}/{r}})$ times the expected cost of a fully\nadaptive algorithm. Prior to our work, such a result was not known even for the\ncase of $r=1$ and when $f$ is the coverage function. On the other hand, we show\nthat for any $r$, there exist instances of the stochastic submodular cover\nproblem where no $r$-round adaptive algorithm can achieve better than\n$\\Omega(Q^{{1}/{r}})$ approximation to the expected cost of a fully adaptive\nalgorithm. Our lower bound result holds even for coverage function and for\nalgorithms with unbounded computational power.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 15:43:59 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Agarwal", "Arpit", ""], ["Assadi", "Sepehr", ""], ["Khanna", "Sanjeev", ""]]}]