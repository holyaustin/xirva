[{"id": "2007.00445", "submitter": "Priyank Deshpande", "authors": "Priyank Deshpande", "title": "Error Correcting Codes, finding polynomials of bounded degree agreeing\n  on a dense fraction of a set of points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Here we present some revised arguments to a randomized algorithm proposed by\nSudan to find the polynomials of bounded degree agreeing on a dense fraction of\na set of points in $\\mathbb{F}^{2}$ for some field $\\mathbb{F}$.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 21:40:11 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Deshpande", "Priyank", ""]]}, {"id": "2007.00533", "submitter": "Georgina Hall", "authors": "Georgina Hall and Laurent Massouli\\'e", "title": "Partial Recovery in the Graph Alignment Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.SI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the graph alignment problem, which is the problem\nof recovering, given two graphs, a one-to-one mapping between nodes that\nmaximizes edge overlap. This problem can be viewed as a noisy version of the\nwell-known graph isomorphism problem and appears in many applications,\nincluding social network deanonymization and cellular biology. Our focus here\nis on partial recovery, i.e., we look for a one-to-one mapping which is correct\non a fraction of the nodes of the graph rather than on all of them, and we\nassume that the two input graphs to the problem are correlated\nErd\\H{o}s-R\\'enyi graphs of parameters $(n,q,s)$. Our main contribution is then\nto give necessary and sufficient conditions on $(n,q,s)$ under which partial\nrecovery is possible with high probability as the number of nodes $n$ goes to\ninfinity. In particular, we show that it is possible to achieve partial\nrecovery in the $nqs=\\Theta(1)$ regime under certain additional assumptions. An\ninteresting byproduct of the analysis techniques we develop to obtain the\nsufficiency result in the partial recovery setting is a tighter analysis of the\nmaximum likelihood estimator for the graph alignment problem, which leads to\nimproved sufficient conditions for exact recovery.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 14:57:53 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 16:45:16 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 14:13:40 GMT"}, {"version": "v4", "created": "Wed, 28 Jul 2021 08:42:00 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Hall", "Georgina", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "2007.00657", "submitter": "Juanping Zhu", "authors": "Juanping Zhu, Qi Meng, Wei Chen, Yue Wang, Zhiming Ma", "title": "Constructing Basis Path Set by Eliminating Path Dependency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The way the basis path set works in neural network remains mysterious, and\nthe generalization of newly appeared G-SGD algorithm to more practical network\nis hindered. The Basis Path Set Searching problem is formulated from the\nperspective of graph theory, to find the basis path set in a regular\ncomplicated neural network. Our paper aims to discover the underlying cause of\npath dependency between two independent substructures. Algorithm DEAH is\ndesigned to solve the Basis Path Set Searching problem by eliminating such path\ndependency. The path subdivision chain is proposed to effectively eliminate the\npath dependency inside the chain and between chains. The theoretical proofs and\nanalysis of polynomial time complexity are presented. The paper therefore\nprovides one methodology to find the basis path set in a more general neural\nnetwork, which offers theoretical and algorithmic support for the application\nof G-SGD algorithm in more practical scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 10:22:22 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Zhu", "Juanping", ""], ["Meng", "Qi", ""], ["Chen", "Wei", ""], ["Wang", "Yue", ""], ["Ma", "Zhiming", ""]]}, {"id": "2007.01031", "submitter": "Caroline Brosse", "authors": "Caroline Brosse, Aur\\'elie Lagoutte, Vincent Limouzy, Arnaud Mary and\n  Lucas Pastor", "title": "Efficient enumeration of maximal split subgraphs and sub-cographs and\n  related classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in algorithms that take in input an\narbitrary graph $G$, and that enumerate in output all the (inclusion-wise)\nmaximal \"subgraphs\" of $G$ which fulfil a given property $\\Pi$. All over this\npaper, we study several different properties $\\Pi$, and the notion of subgraph\nunder consideration (induced or not) will vary from a result to another.\n  More precisely, we present efficient algorithms to list all maximal split\nsubgraphs, sub-cographs and some subclasses of cographs of a given input graph.\nAll the algorithms presented here run in polynomial delay, and moreover for\nsplit graphs it only requires polynomial space. In order to develop an\nalgorithm for maximal split (edge-)subgraphs, we establish a bijection between\nthe maximal split subgraphs and the maximal independent sets of an auxiliary\ngraph. For cographs and some subclasses , the algorithms rely on a framework\nrecently introduced by Conte & Uno called Proximity Search. Finally we consider\nthe extension problem, which consists in deciding if there exists a maximal\ninduced subgraph satisfying a property $\\Pi$ that contains a set of prescribed\nvertices and that avoids another set of vertices. We show that this problem is\nNP-complete for every \"interesting\" hereditary property $\\Pi$. We extend the\nhardness result to some specific edge version of the extension problem.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 11:34:01 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Brosse", "Caroline", ""], ["Lagoutte", "Aur\u00e9lie", ""], ["Limouzy", "Vincent", ""], ["Mary", "Arnaud", ""], ["Pastor", "Lucas", ""]]}, {"id": "2007.01116", "submitter": "Valentin Bakoev", "authors": "Valentin Bakoev", "title": "A Method for Fast Computing the Algebraic Degree of Boolean Functions", "comments": "arXiv admin note: text overlap with arXiv:1905.08649", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The algebraic degree of Boolean functions (or vectorial Boolean functions) is\nan important cryptographic parameter that should be computed by fast\nalgorithms. They work in two main ways: (1) by computing the algebraic normal\nform and then searching the monomial of the highest degree in it, or (2) by\nexamination the algebraic properties of the true table vector of a given\nfunction. We have already done four basic steps in the study of the first way,\nand the second one has been studied by other authors. Here we represent a\nmethod for fast computing (the fastest way we know) the algebraic degree of\nBoolean functions. It is a combination of the most efficient components of\nthese two ways and the corresponding algorithms. The theoretical time\ncomplexities of the method are derived in each of the cases when the Boolean\nfunction is represented in a byte-wise or in a bitwise manner. They are of the\nsame type $\\Theta(n.2^n)$ for a Boolean function of $n$ variables, but they\nhave big differences between the constants in $\\Theta$-notation. The\ntheoretical and experimental results shown here demonstrate the advantages of\nthe bitwise approach in computing the algebraic degree - they are dozens of\ntimes faster than the byte-wise approaches.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 06:09:08 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Bakoev", "Valentin", ""]]}, {"id": "2007.01118", "submitter": "V\\'aclav Rozho\\v{n}", "authors": "Christoph Grunau and V\\'aclav Rozho\\v{n}", "title": "Adapting $k$-means algorithms for outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how to adapt several simple and classical sampling-based\nalgorithms for the $k$-means problem to the setting with outliers.\n  Recently, Bhaskara et al. (NeurIPS 2019) showed how to adapt the classical\n$k$-means++ algorithm to the setting with outliers. However, their algorithm\nneeds to output $O(\\log (k) \\cdot z)$ outliers, where $z$ is the number of true\noutliers, to match the $O(\\log k)$-approximation guarantee of $k$-means++. In\nthis paper, we build on their ideas and show how to adapt several sequential\nand distributed $k$-means algorithms to the setting with outliers, but with\nsubstantially stronger theoretical guarantees: our algorithms output\n$(1+\\varepsilon)z$ outliers while achieving an $O(1 /\n\\varepsilon)$-approximation to the objective function. In the sequential world,\nwe achieve this by adapting a recent algorithm of Lattanzi and Sohler (ICML\n2019). In the distributed setting, we adapt a simple algorithm of Guha et al.\n(IEEE Trans. Know. and Data Engineering 2003) and the popular $k$-means$\\|$ of\nBahmani et al. (PVLDB 2012).\n  A theoretical application of our techniques is an algorithm with running time\n$\\tilde{O}(nk^2/z)$ that achieves an $O(1)$-approximation to the objective\nfunction while outputting $O(z)$ outliers, assuming $k \\ll z \\ll n$. This is\ncomplemented with a matching lower bound of $\\Omega(nk^2/z)$ for this problem\nin the oracle model.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:14:33 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Grunau", "Christoph", ""], ["Rozho\u0148", "V\u00e1clav", ""]]}, {"id": "2007.01199", "submitter": "Lukas Gianinazzi", "authors": "Lukas Gianinazzi, Torsten Hoefler", "title": "Parallel Planar Subgraph Isomorphism and Vertex Connectivity", "comments": "To appear in: Proceedings of the 32nd ACM Symposium on Parallelism in\n  Algorithms and Architectures (SPAA '20), July 15-17, 2020, Virtual Event,\n  USA. ACM, New York, NY, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first parallel fixed-parameter algorithm for subgraph\nisomorphism in planar graphs, bounded-genus graphs, and, more generally, all\nminor-closed graphs of locally bounded treewidth. Our randomized low depth\nalgorithm has a near-linear work dependency on the size of the target graph.\nExisting low depth algorithms do not guarantee that the work remains\nasymptotically the same for any constant-sized pattern. By using a connection\nto certain separating cycles, our subgraph isomorphism algorithm can decide the\nvertex connectivity of a planar graph (with high probability) in asymptotically\nnear-linear work and poly-logarithmic depth. Previously, no sub-quadratic work\nand poly-logarithmic depth bound was known in planar graphs (in particular for\ndistinguishing between four-connected and five-connected planar graphs).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 15:30:39 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Gianinazzi", "Lukas", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2007.01252", "submitter": "Leon Kellerhals", "authors": "Danny Hermelin, Leon Kellerhals, Rolf Niedermeier, and Rami Pugatch", "title": "Approximating Sparse Quadratic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a matrix $A \\in \\mathbb{R}^{n\\times n}$, we consider the problem of\nmaximizing $x^TAx$ subject to the constraint $x \\in \\{-1,1\\}^n$. This problem,\ncalled MaxQP by Charikar and Wirth [FOCS'04], generalizes MaxCut and has\nnatural applications in data clustering and in the study of disordered magnetic\nphases of matter. Charikar and Wirth showed that the problem admits an\n$\\Omega(1/\\lg n)$ approximation via semidefinite programming, and Alon,\nMakarychev, Makarychev, and Naor [STOC'05] showed that the same approach yields\nan $\\Omega(1)$ approximation when $A$ corresponds to a graph of bounded\nchromatic number. Both these results rely on solving the semidefinite\nrelaxation of MaxQP, whose currently best running time is\n$\\tilde{O}(n^{1.5}\\cdot \\min\\{N,n^{1.5}\\})$, where $N$ is the number of nonzero\nentries in $A$ and $\\tilde{O}$ ignores polylogarithmic factors.\n  In this sequel, we abandon the semidefinite approach and design purely\ncombinatorial approximation algorithms for special cases of MaxQP where $A$ is\nsparse (i.e., has $O(n)$ nonzero entries). Our algorithms are superior to the\nsemidefinite approach in terms of running time, yet are still competitive in\nterms of their approximation guarantees. More specifically, we show that:\n  - MaxQP admits a $(1/2\\Delta)$-approximation in $O(n \\lg n)$ time, where\n$\\Delta$ is the maximum degree of the corresponding graph.\n  - UnitMaxQP, where $A \\in \\{-1,0,1\\}^{n\\times n}$, admits a\n$(1/2d)$-approximation in $O(n)$ time when the corresponding graph is\n$d$-degenerate, and a $(1/3\\delta)$-approximation in $O(n^{1.5})$ time when the\ncorresponding graph has $\\delta n$ edges.\n  - MaxQP admits a $(1-\\varepsilon)$-approximation in $O(n)$ time when the\ncorresponding graph and each of its minors have bounded local treewidth.\n  - UnitMaxQP admits a $(1-\\varepsilon)$-approximation in $O(n^2)$ time when\nthe corresponding graph is $H$-minor free.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:02:04 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 10:16:30 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 16:41:30 GMT"}, {"version": "v4", "created": "Tue, 15 Dec 2020 11:11:09 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Hermelin", "Danny", ""], ["Kellerhals", "Leon", ""], ["Niedermeier", "Rolf", ""], ["Pugatch", "Rami", ""]]}, {"id": "2007.01280", "submitter": "Nikhil Kumar", "authors": "Nikhil Kumar", "title": "Multicommodity Flows in Planar Graphs with Demands on Faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of multicommodity flows in planar graphs. Seymour\nshowed that if the union of supply and demand graphs is planar, then the cut\ncondition is sufficient for routing demands. Okamura-Seymour showed that if all\ndemands are incident on one face, then again cut condition is sufficient for\nrouting demands. We consider a common generalization of these settings where\nthe end points of each demand are on the same face of the planar graph. We show\nthat if the source sink pairs on each face of the graph are such that sources\nand sinks appear contiguously on the cycle bounding the face, then the flow cut\ngap is at most 3. We come up with a notion of approximating demands on a face\nby convex combination of laminar demands to prove this result.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:37:01 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Kumar", "Nikhil", ""]]}, {"id": "2007.01394", "submitter": "Ainesh Bakshi", "authors": "Ainesh Bakshi and Adarsh Prasad", "title": "Robust Linear Regression: Optimal Rates in Polynomial Time", "comments": "Updated exposition of sum-of-squares background and preliminaries", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain robust and computationally efficient estimators for learning\nseveral linear models that achieve statistically optimal convergence rate under\nminimal distributional assumptions. Concretely, we assume our data is drawn\nfrom a $k$-hypercontractive distribution and an $\\epsilon$-fraction is\nadversarially corrupted. We then describe an estimator that converges to the\noptimal least-squares minimizer for the true distribution at a rate\nproportional to $\\epsilon^{2-2/k}$, when the noise is independent of the\ncovariates. We note that no such estimator was known prior to our work, even\nwith access to unbounded computation. The rate we achieve is\ninformation-theoretically optimal and thus we resolve the main open question in\nKlivans, Kothari and Meka [COLT'18].\n  Our key insight is to identify an analytic condition that serves as a\npolynomial relaxation of independence of random variables. In particular, we\nshow that when the moments of the noise and covariates are\nnegatively-correlated, we obtain the same rate as independent noise. Further,\nwhen the condition is not satisfied, we obtain a rate proportional to\n$\\epsilon^{2-4/k}$, and again match the information-theoretic lower bound. Our\ncentral technical contribution is to algorithmically exploit independence of\nrandom variables in the \"sum-of-squares\" framework by formulating it as the\naforementioned polynomial inequality.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 17:22:16 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 20:53:11 GMT"}, {"version": "v3", "created": "Sat, 14 Nov 2020 00:13:24 GMT"}, {"version": "v4", "created": "Fri, 4 Dec 2020 15:18:28 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Bakshi", "Ainesh", ""], ["Prasad", "Adarsh", ""]]}, {"id": "2007.01409", "submitter": "Shayan Oveis Gharan", "authors": "Anna R. Karlin and Nathan Klein and Shayan Oveis Gharan", "title": "A (Slightly) Improved Approximation Algorithm for Metric TSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For some $\\epsilon > 10^{-36}$ we give a $3/2-\\epsilon$ approximation\nalgorithm for metric TSP.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 22:07:03 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 01:53:34 GMT"}, {"version": "v3", "created": "Sun, 30 Aug 2020 20:24:01 GMT"}, {"version": "v4", "created": "Sat, 8 May 2021 16:16:15 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Karlin", "Anna R.", ""], ["Klein", "Nathan", ""], ["Gharan", "Shayan Oveis", ""]]}, {"id": "2007.01445", "submitter": "Haotian Jiang", "authors": "Haotian Jiang", "title": "Minimizing Convex Functions with Integral Minimizers", "comments": "This version of the paper simplifies and generalizes the results in\n  an earlier version which will appear in SODA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a separation oracle $\\mathsf{SO}$ for a convex function $f$ that has an\nintegral minimizer inside a box with radius $R$, we show how to find an exact\nminimizer of $f$ using at most (a) $O(n (n + \\log(R)))$ calls to $\\mathsf{SO}$\nand $\\mathsf{poly}(n, \\log(R))$ arithmetic operations, or (b) $O(n \\log(nR))$\ncalls to $\\mathsf{SO}$ and $\\exp(n) \\cdot \\mathsf{poly}(\\log(R))$ arithmetic\noperations. When the set of minimizers of $f$ has integral extreme points, our\nalgorithm outputs an integral minimizer of $f$. This improves upon the\npreviously best oracle complexity of $O(n^2 (n + \\log(R)))$ for polynomial time\nalgorithms obtained by [Gr\\\"otschel, Lov\\'asz and Schrijver, Prog. Comb. Opt.\n1984, Springer 1988] over thirty years ago.\n  For the Submodular Function Minimization problem, our result immediately\nimplies a strongly polynomial algorithm that makes at most $O(n^3)$ calls to an\nevaluation oracle, and an exponential time algorithm that makes at most $O(n^2\n\\log(n))$ calls to an evaluation oracle. These improve upon the previously best\n$O(n^3 \\log^2(n))$ oracle complexity for strongly polynomial algorithms given\nin [Lee, Sidford and Wong, FOCS 2015] and [Dadush, V\\'egh and Zambelli, SODA\n2018], and an exponential time algorithm with oracle complexity $O(n^3\n\\log(n))$ given in the former work.\n  Our result is achieved via a reduction to the Shortest Vector Problem in\nlattices. We show how an approximately shortest vector of certain lattice can\nbe used to effectively reduce the dimension of the problem. Our analysis of the\noracle complexity is based on a potential function that captures simultaneously\nthe size of the search set and the density of the lattice, which we analyze via\ntechnical tools from convex geometry.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 00:51:12 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 20:28:36 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 08:15:28 GMT"}, {"version": "v4", "created": "Mon, 23 Nov 2020 01:45:45 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Jiang", "Haotian", ""]]}, {"id": "2007.01533", "submitter": "Atsushi Miyauchi", "authors": "Francesco Bonchi, David Garc\\'ia-Soriano, Atsushi Miyauchi,\n  Charalampos E. Tsourakakis", "title": "Finding Densest $k$-Connected Subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense subgraph discovery is an important graph-mining primitive with a\nvariety of real-world applications. One of the most well-studied optimization\nproblems for dense subgraph discovery is the densest subgraph problem, where\ngiven an edge-weighted undirected graph $G=(V,E,w)$, we are asked to find\n$S\\subseteq V$ that maximizes the density $d(S)$, i.e., half the weighted\naverage degree of the induced subgraph $G[S]$. This problem can be solved\nexactly in polynomial time and well-approximately in almost linear time.\nHowever, a densest subgraph has a structural drawback, namely, the subgraph may\nnot be robust to vertex/edge failure. Indeed, a densest subgraph may not be\nwell-connected, which implies that the subgraph may be disconnected by removing\nonly a few vertices/edges within it. In this paper, we provide an algorithmic\nframework to find a dense subgraph that is well-connected in terms of\nvertex/edge connectivity. Specifically, we introduce the following problems:\ngiven a graph $G=(V,E,w)$ and a positive integer/real $k$, we are asked to find\n$S\\subseteq V$ that maximizes the density $d(S)$ under the constraint that\n$G[S]$ is $k$-vertex/edge-connected. For both problems, we propose\npolynomial-time (bicriteria and ordinary) approximation algorithms, using\nclassic Mader's theorem in graph theory and its extensions.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 07:50:43 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Bonchi", "Francesco", ""], ["Garc\u00eda-Soriano", "David", ""], ["Miyauchi", "Atsushi", ""], ["Tsourakakis", "Charalampos E.", ""]]}, {"id": "2007.01673", "submitter": "Cl\\'ement Dallard", "authors": "Valentin Bartier, Nicolas Bousquet, Cl\\'ement Dallard, Kyle Lomer,\n  Amer E. Mouawad", "title": "On girth and the parameterized complexity of token sliding and token\n  jumping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Token Jumping problem we are given a graph $G = (V,E)$ and two\nindependent sets $S$ and $T$ of $G$, each of size $k \\geq 1$. The goal is to\ndetermine whether there exists a sequence of $k$-sized independent sets in $G$,\n$\\langle S_0, S_1, \\ldots, S_\\ell \\rangle$, such that for every $i$, $|S_i| =\nk$, $S_i$ is an independent set, $S = S_0$, $S_\\ell = T$, and $|S_i \\Delta\nS_{i+1}| = 2$. In other words, if we view each independent set as a collection\nof tokens placed on a subset of the vertices of $G$, then the problem asks for\na sequence of independent sets which transforms $S$ to $T$ by individual token\njumps which maintain the independence of the sets. This problem is known to be\nPSPACE-complete on very restricted graph classes, e.g., planar bounded degree\ngraphs and graphs of bounded bandwidth. A closely related problem is the Token\nSliding problem, where instead of allowing a token to jump to any vertex of the\ngraph we instead require that a token slides along an edge of the graph. Token\nSliding is also known to be PSPACE-complete on the aforementioned graph\nclasses. We investigate the parameterized complexity of both problems on\nseveral graph classes, focusing on the effect of excluding certain cycles from\nthe input graph. In particular, we show that both Token Sliding and Token\nJumping are fixed-parameter tractable on $C_4$-free bipartite graphs when\nparameterized by $k$. For Token Jumping, we in fact show that the problem\nadmits a polynomial kernel on $\\{C_3,C_4\\}$-free graphs. In the case of Token\nSliding, we also show that the problem admits a polynomial kernel on bipartite\ngraphs of bounded degree. We believe both of these results to be of independent\ninterest. We complement these positive results by showing that, for any\nconstant $p \\geq 4$, both problems are W[1]-hard on $\\{C_4, \\dots, C_p\\}$-free\ngraphs and Token Sliding remains W[1]-hard even on bipartite graphs.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 13:28:30 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 12:36:18 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Bartier", "Valentin", ""], ["Bousquet", "Nicolas", ""], ["Dallard", "Cl\u00e9ment", ""], ["Lomer", "Kyle", ""], ["Mouawad", "Amer E.", ""]]}, {"id": "2007.01779", "submitter": "Stanislav Zivny", "authors": "Caterina Viola and Stanislav Zivny", "title": "The combined basic LP and affine IP relaxation for promise VCSPs on\n  infinite domains", "comments": "Full version of an MFCS'20 paper", "journal-ref": "ACM Transactions on Algorithms 17(3) Article No. 21 (2021)", "doi": "10.1145/3458041", "report-no": null, "categories": "cs.CC cs.DM cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex relaxations have been instrumental in solvability of constraint\nsatisfaction problems (CSPs), as well as in the three different generalisations\nof CSPs: valued CSPs, infinite-domain CSPs, and most recently promise CSPs. In\nthis work, we extend an existing tractability result to the three\ngeneralisations of CSPs combined: We give a sufficient condition for the\ncombined basic linear programming and affine integer programming relaxation for\nexact solvability of promise valued CSPs over infinite-domains. This extends a\nresult of Brakensiek and Guruswami [SODA'20] for promise (non-valued) CSPs (on\nfinite domains).\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:06:43 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 15:12:15 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Viola", "Caterina", ""], ["Zivny", "Stanislav", ""]]}, {"id": "2007.01811", "submitter": "Chris von Csefalvay", "authors": "Tamas Foldi, Chris von Csefalvay and Nicolas A. Perez", "title": "JAMPI: efficient matrix multiplication in Spark using Barrier Execution\n  Mode", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new barrier mode in Apache Spark allows embedding distributed deep\nlearning training as a Spark stage to simplify the distributed training\nworkflow. In Spark, a task in a stage does not depend on any other tasks in the\nsame stage, and hence it can be scheduled independently. However, several\nalgorithms require more sophisticated inter-task communications, similar to the\nMPI paradigm. By combining distributed message passing (using asynchronous\nnetwork IO), OpenJDK's new auto-vectorization and Spark's barrier execution\nmode, we can add non-map/reduce based algorithms, such as Cannon's distributed\nmatrix multiplication to Spark. We document an efficient distributed matrix\nmultiplication using Cannon's algorithm, which improves significantly on the\nperformance of the existing MLlib implementation. Used within a barrier task,\nthe algorithm described herein results in an up to 24 percent performance\nincrease on a 10,000x10,000 square matrix with a significantly lower memory\nfootprint. Applications of efficient matrix multiplication include, among\nothers, accelerating the training and implementation of deep convolutional\nneural network based workloads, and thus such efficient algorithms can play a\nground-breaking role in faster, more efficient execution of even the most\ncomplicated machine learning tasks.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 17:31:23 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Foldi", "Tamas", ""], ["von Csefalvay", "Chris", ""], ["Perez", "Nicolas A.", ""]]}, {"id": "2007.01897", "submitter": "R Jaberi", "authors": "Raed Jaberi", "title": "b-articulation points and b-bridges in strongly biconnected directed\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A directed graph $G=(V,E)$ is called strongly biconnected if $G$ is strongly\nconnected and the underlying graph of $G$ is biconnected. This class of\ndirected graphs was first introduced by Wu and Grumbach. Let $G=(V,E)$ be a\nstrongly biconnected directed graph. An edge $e\\in E$ is a b-bridge if the\nsubgraph $G\\setminus \\left\\lbrace e\\right\\rbrace =(V,E\\setminus \\left\\lbrace\ne\\right\\rbrace) $ is not strongly biconnected. A vertex $w\\in V$ is a\nb-articulation point if $G\\setminus \\left\\lbrace w\\right\\rbrace$ is not\nstrongly biconnected, where $G\\setminus \\left\\lbrace w\\right\\rbrace$ is the\nsubgraph obtained from $G$ by removing $w$. In this paper we study\nb-articulation points and b-bridges.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 18:21:48 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Jaberi", "Raed", ""]]}, {"id": "2007.01980", "submitter": "Yuan Zhou", "authors": "Yufei Ruan, Jiaqi Yang, Yuan Zhou", "title": "Linear Bandits with Limited Adaptivity and Learning Distributional\n  Optimal Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by practical needs such as large-scale learning, we study the\nimpact of adaptivity constraints to linear contextual bandits, a central\nproblem in online active learning. We consider two popular limited adaptivity\nmodels in literature: batch learning and rare policy switches. We show that,\nwhen the context vectors are adversarially chosen in $d$-dimensional linear\ncontextual bandits, the learner needs $O(d \\log d \\log T)$ policy switches to\nachieve the minimax-optimal regret, and this is optimal up to\n$\\mathrm{poly}(\\log d, \\log \\log T)$ factors; for stochastic context vectors,\neven in the more restricted batch learning model, only $O(\\log \\log T)$ batches\nare needed to achieve the optimal regret. Together with the known results in\nliterature, our results present a complete picture about the adaptivity\nconstraints in linear contextual bandits. Along the way, we propose the\ndistributional optimal design, a natural extension of the optimal experiment\ndesign, and provide a both statistically and computationally efficient learning\nalgorithm for the problem, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 01:34:22 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 04:17:42 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 13:01:03 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Ruan", "Yufei", ""], ["Yang", "Jiaqi", ""], ["Zhou", "Yuan", ""]]}, {"id": "2007.02377", "submitter": "Vincent Cohen-Addad", "authors": "Amir Abboud and Vincent Cohen-Addad and Philip N. Klein", "title": "New Hardness Results for Planar Graph Problems in P and an Algorithm for\n  Sparsest Cut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Sparsest Cut is a fundamental optimization problem that has been\nextensively studied. For planar inputs the problem is in $P$ and can be solved\nin $\\tilde{O}(n^3)$ time if all vertex weights are $1$. Despite a significant\namount of effort, the best algorithms date back to the early 90's and can only\nachieve $O(\\log n)$-approximation in $\\tilde{O}(n)$ time or a constant factor\napproximation in $\\tilde{O}(n^2)$ time [Rao, STOC92]. Our main result is an\n$\\Omega(n^{2-\\epsilon})$ lower bound for Sparsest Cut even in planar graphs\nwith unit vertex weights, under the $(min,+)$-Convolution conjecture, showing\nthat approximations are inevitable in the near-linear time regime. To\ncomplement the lower bound, we provide a constant factor approximation in\nnear-linear time, improving upon the 25-year old result of Rao in both time and\naccuracy.\n  Our lower bound accomplishes a repeatedly raised challenge by being the first\nfine-grained lower bound for a natural planar graph problem in P. Moreover, we\nprove near-quadratic lower bounds under SETH for variants of the closest pair\nproblem in planar graphs, and use them to show that the popular Average-Linkage\nprocedure for Hierarchical Clustering cannot be simulated in truly subquadratic\ntime.\n  We prove an $\\Omega(n/\\log{n})$ lower bound on the number of communication\nrounds required to compute the weighted diameter of a network in the CONGEST\nmodel, even when the underlying graph is planar and all nodes are $D=4$ hops\naway from each other. This is the first poly($n$) + $\\omega(D)$ lower bound in\nthe planar-distributed setting, and it complements the recent poly$(D,\n\\log{n})$ upper bounds of Li and Parter [STOC 2019] for (exact) unweighted\ndiameter and for ($1+\\epsilon$) approximate weighted diameter.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 16:26:27 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Abboud", "Amir", ""], ["Cohen-Addad", "Vincent", ""], ["Klein", "Philip N.", ""]]}, {"id": "2007.02392", "submitter": "Alkis Kalavasis", "authors": "Dimitris Fotakis, Alkis Kalavasis, Christos Tzamos", "title": "Efficient Parameter Estimation of Truncated Boolean Product\n  Distributions", "comments": "33 pages, 33rd Conference on Learning Theory (COLT 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the parameters of a Boolean product\ndistribution in $d$ dimensions, when the samples are truncated by a set $S\n\\subset \\{0, 1\\}^d$ accessible through a membership oracle. This is the first\ntime that the computational and statistical complexity of learning from\ntruncated samples is considered in a discrete setting.\n  We introduce a natural notion of fatness of the truncation set $S$, under\nwhich truncated samples reveal enough information about the true distribution.\nWe show that if the truncation set is sufficiently fat, samples from the true\ndistribution can be generated from truncated samples. A stunning consequence is\nthat virtually any statistical task (e.g., learning in total variation\ndistance, parameter estimation, uniformity or identity testing) that can be\nperformed efficiently for Boolean product distributions, can also be performed\nfrom truncated samples, with a small increase in sample complexity. We\ngeneralize our approach to ranking distributions over $d$ alternatives, where\nwe show how fatness implies efficient parameter estimation of Mallows models\nfrom truncated samples.\n  Exploring the limits of learning discrete models from truncated samples, we\nidentify three natural conditions that are necessary for efficient\nidentifiability: (i) the truncation set $S$ should be rich enough; (ii) $S$\nshould be accessible through membership queries; and (iii) the truncation by\n$S$ should leave enough randomness in all directions. By carefully adapting the\nStochastic Gradient Descent approach of (Daskalakis et al., FOCS 2018), we show\nthat these conditions are also sufficient for efficient learning of truncated\nBoolean product distributions.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 17:20:39 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Fotakis", "Dimitris", ""], ["Kalavasis", "Alkis", ""], ["Tzamos", "Christos", ""]]}, {"id": "2007.02660", "submitter": "Sebastian Berndt", "authors": "Max Bannach, Sebastian Berndt, Marten Maack, Matthias Mnich, Alexandra\n  Lassota, Malin Rau, Malte Skambath", "title": "Solving Packing Problems with Few Small Items Using Rainbow Matchings", "comments": "MFCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important area of combinatorial optimization is the study of packing and\ncovering problems, such as Bin Packing, Multiple Knapsack, and Bin Covering.\nThose problems have been studied extensively from the viewpoint of\napproximation algorithms, but their parameterized complexity has only been\ninvestigated barely. For problem instances containing no \"small\" items,\nclassical matching algorithms yield optimal solutions in polynomial time. In\nthis paper we approach them by their distance from triviality, measuring the\nproblem complexity by the number $k$ of small items.\n  Our main results are fixed-parameter algorithms for vector versions of Bin\nPacking, Multiple Knapsack, and Bin Covering parameterized by $k$. The\nalgorithms are randomized with one-sided error and run in time $4^{k} \\cdot k!\n\\cdot n^{O(1)}$. To achieve this, we introduce a colored matching problem to\nwhich we reduce all these packing problems. The colored matching problem is\nnatural in itself and we expect it to be useful for other applications. We also\npresent a deterministic fixed-parameter for Bin Packing with run time\n$(k!)^{2}\\cdot k \\cdot 2^{k}\\cdot n\\cdot \\log(n)$.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 11:38:08 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Bannach", "Max", ""], ["Berndt", "Sebastian", ""], ["Maack", "Marten", ""], ["Mnich", "Matthias", ""], ["Lassota", "Alexandra", ""], ["Rau", "Malin", ""], ["Skambath", "Malte", ""]]}, {"id": "2007.02738", "submitter": "Zhijie Zhang", "authors": "Wei Chen, Xiaoming Sun, Jialin Zhang, Zhijie Zhang", "title": "Optimization from Structured Samples for Coverage Functions", "comments": "To appear in ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the optimization from samples (OPS) model, which studies the\nproblem of optimizing objective functions directly from the sample data.\nPrevious results showed that we cannot obtain a constant approximation ratio\nfor the maximum coverage problem using polynomially many independent samples of\nthe form $\\{S_i, f(S_i)\\}_{i=1}^t$ (Balkanski et al., 2017), even if coverage\nfunctions are $(1 - \\epsilon)$-PMAC learnable using these samples (Badanidiyuru\net al., 2012), which means most of the function values can be approximately\nlearned very well with high probability. In this work, to circumvent the\nimpossibility result of OPS, we propose a stronger model called optimization\nfrom structured samples (OPSS) for coverage functions, where the data samples\nencode the structural information of the functions. We show that under three\ngeneral assumptions on the sample distributions, we can design efficient OPSS\nalgorithms that achieve a constant approximation for the maximum coverage\nproblem. We further prove a constant lower bound under these assumptions, which\nis tight when not considering computational efficiency. Moreover, we also show\nthat if we remove any one of the three assumptions, OPSS for the maximum\ncoverage problem has no constant approximation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:18:11 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Chen", "Wei", ""], ["Sun", "Xiaoming", ""], ["Zhang", "Jialin", ""], ["Zhang", "Zhijie", ""]]}, {"id": "2007.02817", "submitter": "Matthew Fahrbach", "authors": "Matthew Fahrbach, Gramoz Goranci, Richard Peng, Sushant Sachdeva, Chi\n  Wang", "title": "Faster Graph Embeddings via Coarsening", "comments": "18 pages, 2 figures, to appear in the Proceedings of the 37th\n  International Conference on Machine Learning (ICML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embeddings are a ubiquitous tool for machine learning tasks, such as\nnode classification and link prediction, on graph-structured data. However,\ncomputing the embeddings for large-scale graphs is prohibitively inefficient\neven if we are interested only in a small subset of relevant vertices. To\naddress this, we present an efficient graph coarsening approach, based on Schur\ncomplements, for computing the embedding of the relevant vertices. We prove\nthat these embeddings are preserved exactly by the Schur complement graph that\nis obtained via Gaussian elimination on the non-relevant vertices. As computing\nSchur complements is expensive, we give a nearly-linear time algorithm that\ngenerates a coarsened graph on the relevant vertices that provably matches the\nSchur complement in expectation in each iteration. Our experiments involving\nprediction tasks on graphs demonstrate that computing embeddings on the\ncoarsened graph, rather than the entire graph, leads to significant time\nsavings without sacrificing accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:22:25 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 03:42:19 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 13:49:46 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Fahrbach", "Matthew", ""], ["Goranci", "Gramoz", ""], ["Peng", "Richard", ""], ["Sachdeva", "Sushant", ""], ["Wang", "Chi", ""]]}, {"id": "2007.03039", "submitter": "Prantar Ghosh", "authors": "Amit Chakrabarti, Prantar Ghosh, Justin Thaler", "title": "Streaming Verification for Graph Problems: Optimal Tradeoffs and\n  Nonlinear Sketches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study graph computations in an enhanced data streaming setting, where a\nspace-bounded client reading the edge stream of a massive graph may delegate\nsome of its work to a cloud service. We seek algorithms that allow the client\nto verify a purported proof sent by the cloud service that the work done in the\ncloud is correct. A line of work starting with Chakrabarti et al. (ICALP 2009)\nhas provided such algorithms, which we call schemes, for several statistical\nand graph-theoretic problems, many of which exhibit a tradeoff between the\nlength of the proof and the space used by the streaming verifier.\n  This work designs new schemes for a number of basic graph\nproblems---including triangle counting, maximum matching, topological sorting,\nand single-source shortest paths---where past work had either failed to obtain\nsmooth tradeoffs between these two key complexity measures or only obtained\nsuboptimal tradeoffs. Our key innovation is having the verifier compute certain\nnonlinear sketches of the input stream, leading to either new or improved\ntradeoffs. In many cases, our schemes in fact provide optimal tradeoffs up to\nlogarithmic factors.\n  Specifically, for most graph problems that we study, it is known that the\nproduct of the verifier's space cost $v$ and the proof length $h$ must be at\nleast $\\Omega(n^2)$ for $n$-vertex graphs. However, matching upper bounds are\nonly known for a handful of settings of $h$ and $v$ on the curve $h \\cdot\nv=\\tilde{\\Theta}(n^2)$. For example, for counting triangles and maximum\nmatching, schemes with costs lying on this curve are only known for\n$(h=\\tilde{O}(n^2), v=\\tilde{O}(1))$, $(h=\\tilde{O}(n), v=\\tilde{O}(n))$, and\nthe trivial $(h=\\tilde{O}(1), v=\\tilde{O}(n^2))$. A major message of this work\nis that by exploiting nonlinear sketches, a significant ``portion'' of costs on\nthe tradeoff curve $h \\cdot v = n^2$ can be achieved.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 19:57:52 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Chakrabarti", "Amit", ""], ["Ghosh", "Prantar", ""], ["Thaler", "Justin", ""]]}, {"id": "2007.03040", "submitter": "Arun Ganesh", "authors": "Arun Ganesh, Aaron Sy", "title": "Near-Linear Time Edit Distance for Indel Channels", "comments": "Appears in WABI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following model for sampling pairs of strings: $s_1$ is a\nuniformly random bitstring of length $n$, and $s_2$ is the bitstring arrived at\nby applying substitutions, insertions, and deletions to each bit of $s_1$ with\nsome probability. We show that the edit distance between $s_1$ and $s_2$ can be\ncomputed in $O(n \\ln n)$ time with high probability, as long as each bit of\n$s_1$ has a mutation applied to it with probability at most a small constant.\nThe algorithm is simple and only uses the textbook dynamic programming\nalgorithm as a primitive, first computing an approximate alignment between the\ntwo strings, and then running the dynamic programming algorithm restricted to\nentries close to the approximate alignment. The analysis of our algorithm\nprovides theoretical justification for alignment heuristics used in practice\nsuch as BLAST, FASTA, and MAFFT, which also start by computing approximate\nalignments quickly and then find the best alignment near the approximate\nalignment. Our main technical contribution is a partitioning of alignments such\nthat the number of the subsets in the partition is not too large and every\nalignment in one subset is worse than an alignment considered by our algorithm\nwith high probability. Similar techniques may be of interest in the\naverage-case analysis of other problems commonly solved via dynamic\nprogramming.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 19:58:58 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Ganesh", "Arun", ""], ["Sy", "Aaron", ""]]}, {"id": "2007.03057", "submitter": "Kelin Luo", "authors": "Kelin Luo and Frits C. R. Spieksma", "title": "Approximation algorithms for car-sharing problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider several variants of a car-sharing problem. Given are a number of\nrequests each consisting of a pick-up location and a drop-off location, a\nnumber of cars, and nonnegative, symmetric travel times that satisfy the\ntriangle inequality. Each request needs to be served by a car, which means that\na car must first visit the pick-up location of the request, and then visit the\ndrop-off location of the request. Each car can serve two requests. One problem\nis to serve all requests with the minimum total travel time (called\n$\\CS_{sum}$), and the other problem is to serve all requests with the minimum\ntotal latency (called $\\CS_{lat}$). We also study the special case where the\npick-up and drop-off location of a request coincide. We propose two basic\nalgorithms, called the match and assign algorithm and the transportation\nalgorithm. We show that the best of the resulting two solutions is a $\n2$-approximation for $\\CS_{sum}$ (and a $7/5$-approximation for its special\ncase), and a $5/3 $-approximation for $\\CS_{lat}$ (and a $3/2$-approximation\nfor its special case); these ratios are better than the ratios of the\nindividual algorithms. Finally, we indicate how our algorithms can be applied\nto more general settings where each car can serve more than two requests, or\nwhere cars have distinct speeds.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 20:51:49 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Luo", "Kelin", ""], ["Spieksma", "Frits C. R.", ""]]}, {"id": "2007.03402", "submitter": "Yixin Shen", "authors": "Andris Ambainis, Kaspars Balodis, J\\=anis Iraids, Kamil Khadiev,\n  Vladislavs K\\c{l}evickis, Kri\\v{s}j\\=anis Pr\\=usis, Yixin Shen, Juris\n  Smotrovs, Jevg\\=enijs Vihrovs", "title": "Quantum Lower and Upper Bounds for 2D-Grid and Dyck Language", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.12638", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the quantum query complexity of two problems.\n  First, we consider the problem of determining if a sequence of parentheses is\na properly balanced one (a Dyck word), with a depth of at most $k$. We call\nthis the $Dyck_{k,n}$ problem. We prove a lower bound of $\\Omega(c^k\n\\sqrt{n})$, showing that the complexity of this problem increases exponentially\nin $k$. Here $n$ is the length of the word. When $k$ is a constant, this is\ninteresting as a representative example of star-free languages for which a\nsurprising $\\tilde{O}(\\sqrt{n})$ query quantum algorithm was recently\nconstructed by Aaronson et al. Their proof does not give rise to a general\nalgorithm. When $k$ is not a constant, $Dyck_{k,n}$ is not context-free. We\ngive an algorithm with $O\\left(\\sqrt{n}(\\log{n})^{0.5k}\\right)$ quantum queries\nfor $Dyck_{k,n}$ for all $k$. This is better than the trival upper bound $n$\nfor $k=o\\left(\\frac{\\log(n)}{\\log\\log n}\\right)$.\n  Second, we consider connectivity problems on grid graphs in 2 dimensions, if\nsome of the edges of the grid may be missing. By embedding the \"balanced\nparentheses\" problem into the grid, we show a lower bound of\n$\\Omega(n^{1.5-\\epsilon})$ for the directed 2D grid and\n$\\Omega(n^{2-\\epsilon})$ for the undirected 2D grid. The directed problem is\ninteresting as a black-box model for a class of classical dynamic programming\nstrategies including the one that is usually used for the well-known edit\ndistance problem. We also show a generalization of this result to more than 2\ndimensions.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 09:51:41 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 10:37:53 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Ambainis", "Andris", ""], ["Balodis", "Kaspars", ""], ["Iraids", "J\u0101nis", ""], ["Khadiev", "Kamil", ""], ["K\u013cevickis", "Vladislavs", ""], ["Pr\u016bsis", "Kri\u0161j\u0101nis", ""], ["Shen", "Yixin", ""], ["Smotrovs", "Juris", ""], ["Vihrovs", "Jevg\u0113nijs", ""]]}, {"id": "2007.03556", "submitter": "Fabio Henrique Viduani Martinez", "authors": "Diego P. Rubert and F\\'abio V. Martinez and Mar\\'ilia D. V. Braga", "title": "Natural family-free genomic distance", "comments": "Workshop on Algorithms in Bioinformatics (WABI) 2020, 24 pages, 10\n  figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical problem in comparative genomics is to compute the rearrangement\ndistance, that is the minimum number of large-scale rearrangements required to\ntransform a given genome into another given genome.\n  While the most traditional approaches in this area are family-based, i.e.,\nrequire the classification of DNA fragments into families, more recently an\nalternative family-free approach was proposed, and consists of studying the\nrearrangement distances without prior family assignment. On the one hand the\ncomputation of genomic distances in the family-free setting helps to match\noccurrences of duplicated genes and find homologies, but on the other hand this\ncomputation is NP-hard. In this paper, by letting structural rearrangements be\nrepresented by the generic double cut and join (DCJ) operation and also\nallowing insertions and deletions of DNA segments, we propose a new and more\ngeneral family-free genomic distance, providing an efficient ILP formulation to\nsolve it.\n  Our experiments show that the ILP produces accurate results and can handle\nnot only bacterial genomes, but also fungi and insects, or subsets of\nchromosomes of mammals and plants.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:33:47 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 19:52:06 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Rubert", "Diego P.", ""], ["Martinez", "F\u00e1bio V.", ""], ["Braga", "Mar\u00edlia D. V.", ""]]}, {"id": "2007.03589", "submitter": "Galia Rina Zimerman", "authors": "G. R. Zimerman (1), D. Svetlitsky (1), M. Zehavi (1) and M.\n  Ziv-Ukelson (1) ((1) Ben Gurion University of the Negev, Israel)", "title": "Approximate Search for Known Gene Clusters in New Genomes Using PQ-Trees", "comments": "45 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a new problem in comparative genomics, denoted PQ-Tree Search, that\ntakes as input a PQ-tree $T$ representing the known gene orders of a gene\ncluster of interest, a gene-to-gene substitution scoring function $h$, integer\nparameters $d_T$ and $d_S$, and a new genome $S$. The objective is to identify\nin $S$ approximate new instances of the gene cluster that could vary from the\nknown gene orders by genome rearrangements that are constrained by $T$, by gene\nsubstitutions that are governed by $h$, and by gene deletions and insertions\nthat are bounded from above by $d_T$ and $d_S$, respectively. We prove that the\nPQ-Tree Search problem is NP-hard and propose a parameterized algorithm that\nsolves the optimization variant of PQ-Tree Search in $O^*(2^{\\gamma})$ time,\nwhere $\\gamma$ is the maximum degree of a node in $T$ and $O^*$ is used to hide\nfactors polynomial in the input size. The algorithm is implemented as a search\ntool, denoted PQFinder, and applied to search for instances of chromosomal gene\nclusters in plasmids, within a dataset of 1,487 prokaryotic genomes. We report\non 29 chromosomal gene clusters that are rearranged in plasmids, where the\nrearrangements are guided by the corresponding PQ-tree. One of these results,\ncoding for a heavy metal efflux pump, is further analysed to exemplify how\nPQFinder can be harnessed to reveal interesting new structural variants of\nknown gene clusters. The code for the tool as well as all the data needed to\nreconstruct the results are publicly available on GitHub\n(github.com/GaliaZim/PQFinder).\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 16:16:32 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Zimerman", "G. R.", "", "Ben Gurion University of the Negev, Israel"], ["Svetlitsky", "D.", "", "Ben Gurion University of the Negev, Israel"], ["Zehavi", "M.", "", "Ben Gurion University of the Negev, Israel"], ["Ziv-Ukelson", "M.", "", "Ben Gurion University of the Negev, Israel"]]}, {"id": "2007.03633", "submitter": "Collin Burns", "authors": "Alexandr Andoni, Collin Burns, Yi Li, Sepideh Mahabadi, David P.\n  Woodruff", "title": "Streaming Complexity of SVMs", "comments": "APPROX 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the space complexity of solving the bias-regularized SVM problem in\nthe streaming model. This is a classic supervised learning problem that has\ndrawn lots of attention, including for developing fast algorithms for solving\nthe problem approximately. One of the most widely used algorithms for\napproximately optimizing the SVM objective is Stochastic Gradient Descent\n(SGD), which requires only $O(\\frac{1}{\\lambda\\epsilon})$ random samples, and\nwhich immediately yields a streaming algorithm that uses\n$O(\\frac{d}{\\lambda\\epsilon})$ space. For related problems, better streaming\nalgorithms are only known for smooth functions, unlike the SVM objective that\nwe focus on in this work. We initiate an investigation of the space complexity\nfor both finding an approximate optimum of this objective, and for the related\n``point estimation'' problem of sketching the data set to evaluate the function\nvalue $F_\\lambda$ on any query $(\\theta, b)$. We show that, for both problems,\nfor dimensions $d=1,2$, one can obtain streaming algorithms with space\npolynomially smaller than $\\frac{1}{\\lambda\\epsilon}$, which is the complexity\nof SGD for strongly convex functions like the bias-regularized SVM, and which\nis known to be tight in general, even for $d=1$. We also prove polynomial lower\nbounds for both point estimation and optimization. In particular, for point\nestimation we obtain a tight bound of $\\Theta(1/\\sqrt{\\epsilon})$ for $d=1$ and\na nearly tight lower bound of $\\widetilde{\\Omega}(d/{\\epsilon}^2)$ for $d =\n\\Omega( \\log(1/\\epsilon))$. Finally, for optimization, we prove a\n$\\Omega(1/\\sqrt{\\epsilon})$ lower bound for $d = \\Omega( \\log(1/\\epsilon))$,\nand show similar bounds when $d$ is constant.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:10:00 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Andoni", "Alexandr", ""], ["Burns", "Collin", ""], ["Li", "Yi", ""], ["Mahabadi", "Sepideh", ""], ["Woodruff", "David P.", ""]]}, {"id": "2007.03829", "submitter": "Mingyu Xiao", "authors": "Huairui Chu, Mingyu Xiao and Zhe Zhang", "title": "An Improved Upper Bound for SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the CNF satisfiability problem can be solved $O^*(1.2226^m)$\ntime, where $m$ is the number of clauses in the formula, improving the known\nupper bounds $O^*(1.234^m)$ given by Yamamoto 15 years ago and $O^*(1.239^m)$\ngiven by Hirsch 22 years ago. By using an amortized technique and careful case\nanalysis, we successfully avoid the bottlenecks in previous algorithms and get\nthe improvement.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 00:18:49 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Chu", "Huairui", ""], ["Xiao", "Mingyu", ""], ["Zhang", "Zhe", ""]]}, {"id": "2007.03859", "submitter": "Toshiki Saitoh", "authors": "Toshiki Saitoh, Ryo Yoshinaka, and Hans L. Bodlaender", "title": "Fixed-Treewidth-Efficient Algorithms for Edge-Deletion to Intersection\n  Graph Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph class $\\mathcal{C}$, the $\\mathcal{C}$-\\textsc{Edge-Deletion}\nproblem asks for a given graph $G$ to delete the minimum number of edges from\n$G$ in order to obtain a graph in $\\mathcal{C}$. We study the\n$\\mathcal{C}$-\\textsc{Edge-Deletion} problem for $\\mathcal{C}$ the permutation\ngraphs, interval graphs, and other related graph classes. It follows from\nCourcelle's Theorem that these problems are fixed parameter tractable when\nparameterized by treewidth. In this paper, we present concrete FPT algorithms\nfor these problems. By giving explicit algorithms and analyzing these in\ndetail, we obtain algorithms that are significantly faster than the algorithms\nobtained by using Courcelle's theorem.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 02:29:51 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Saitoh", "Toshiki", ""], ["Yoshinaka", "Ryo", ""], ["Bodlaender", "Hans L.", ""]]}, {"id": "2007.03909", "submitter": "Clara Meister", "authors": "Clara Meister, Tim Vieira, Ryan Cotterell", "title": "Best-First Beam Search", "comments": "TACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decoding for many NLP tasks requires an effective heuristic algorithm for\napproximating exact search since the problem of searching the full output space\nis often intractable, or impractical in many settings. The default algorithm\nfor this job is beam search -- a pruned version of breadth-first search. Quite\nsurprisingly, beam search often returns better results than exact inference due\nto beneficial search bias for NLP tasks. In this work, we show that the\nstandard implementation of beam search can be made up to 10x faster in\npractice. Our method assumes that the scoring function is monotonic in the\nsequence length, which allows us to safely prune hypotheses that cannot be in\nthe final set of hypotheses early on. We devise effective monotonic\napproximations to popular nonmonontic scoring functions, including length\nnormalization and mutual information decoding. Lastly, we propose a\nmemory-reduced variant of Best-First Beam Search, which has a similar\nbeneficial search bias in terms of downstream performance, but runs in a\nfraction of the time.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 05:56:01 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 09:56:49 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2021 09:31:46 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Meister", "Clara", ""], ["Vieira", "Tim", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2007.03927", "submitter": "Amir Zandieh", "authors": "David P. Woodruff, Amir Zandieh", "title": "Near Input Sparsity Time Kernel Embeddings via Adaptive Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To accelerate kernel methods, we propose a near input sparsity time algorithm\nfor sampling the high-dimensional feature space implicitly defined by a kernel\ntransformation. Our main contribution is an importance sampling method for\nsubsampling the feature space of a degree $q$ tensoring of data points in\nalmost input sparsity time, improving the recent oblivious sketching method of\n(Ahle et al., 2020) by a factor of $q^{5/2}/\\epsilon^2$. This leads to a\nsubspace embedding for the polynomial kernel, as well as the Gaussian kernel,\nwith a target dimension that is only linearly dependent on the statistical\ndimension of the kernel and in time which is only linearly dependent on the\nsparsity of the input dataset. We show how our subspace embedding bounds imply\nnew statistical guarantees for kernel ridge regression. Furthermore, we\nempirically show that in large-scale regression tasks, our algorithm\noutperforms state-of-the-art kernel approximation methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 07:19:17 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 06:56:30 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Woodruff", "David P.", ""], ["Zandieh", "Amir", ""]]}, {"id": "2007.03933", "submitter": "Evangelos Kosinas", "authors": "Loukas Georgiadis, Evangelos Kosinas", "title": "Linear-Time Algorithms for Computing Twinless Strong Articulation Points\n  and Related Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A directed graph $G=(V,E)$ is twinless strongly connected if it contains a\nstrongly connected spanning subgraph without any pair of antiparallel (or twin)\nedges. The twinless strongly connected components (TSCCs) of a directed graph\n$G$ are its maximal twinless strongly connected subgraphs. These concepts have\nseveral diverse applications, such as the design of telecommunication networks\nand the structural stability of buildings. A vertex $v \\in V$ is a twinless\nstrong articulation point of $G$ if the deletion of $v$ increases the number of\nTSCCs of $G$. Here, we present the first linear-time algorithm that finds all\nthe twinless strong articulation points of a directed graph. We show that the\ncomputation of twinless strong articulation points reduces to the following\nproblem in undirected graphs, which may be of independent interest: Given a\n$2$-vertex-connected (biconnected) undirected graph $H$, find all vertices $v$\nthat belong to a vertex-edge cut-pair, i.e., for which there exists an edge $e$\nsuch that $H \\setminus \\{v,e\\}$ is not connected. We develop a linear-time\nalgorithm that not only finds all such vertices $v$, but also computes the\nnumber of edges $e$ such that $H \\setminus \\{v,e\\}$ is not connected. This also\nimplies that for each twinless strong articulation point $v$ which is not a\nstrong articulation point in a strongly connected digraph $G$, we can compute\nthe number of TSCCs in $G \\setminus v$. We note that the problem of computing\nall vertices that belong to a vertex-edge cut-pair can be solved in linear-time\nby exploiting the structure of $3$-vertex-connected (triconnected) components\nof $H$, represented by an SPQR tree of $H$. Our approach, however, is\nconceptually simple, and thus likely to be more amenable to practical\nimplementations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 07:32:01 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 22:07:39 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Georgiadis", "Loukas", ""], ["Kosinas", "Evangelos", ""]]}, {"id": "2007.03946", "submitter": "Georg Anegg", "authors": "Georg Anegg, Haris Angelidakis, Adam Kurpisz, and Rico Zenklusen", "title": "A Technique for Obtaining True Approximations for $k$-Center with\n  Covering Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a recent surge of interest in incorporating fairness aspects\ninto classical clustering problems. Two recently introduced variants of the\n$k$-Center problem in this spirit are Colorful $k$-Center, introduced by\nBandyapadhyay, Inamdar, Pai, and Varadarajan, and lottery models, such as the\nFair Robust $k$-Center problem introduced by Harris, Pensyl, Srinivasan, and\nTrinh. To address fairness aspects, these models, compared to traditional\n$k$-Center, include additional covering constraints. Prior approximation\nresults for these models require to relax some of the normally hard\nconstraints, like the number of centers to be opened or the involved covering\nconstraints, and therefore, only obtain constant-factor pseudo-approximations.\nIn this paper, we introduce a new approach to deal with such covering\nconstraints that leads to (true) approximations, including a $4$-approximation\nfor Colorful $k$-Center with constantly many colors---settling an open question\nraised by Bandyapadhyay, Inamdar, Pai, and Varadarajan---and a\n$4$-approximation for Fair Robust $k$-Center, for which the existence of a\n(true) constant-factor approximation was also open. We complement our results\nby showing that if one allows an unbounded number of colors, then Colorful\n$k$-Center admits no approximation algorithm with finite approximation\nguarantee, assuming that $\\mathrm{P} \\neq \\mathrm{NP}$. Moreover, under the\nExponential Time Hypothesis, the problem is inapproximable if the number of\ncolors grows faster than logarithmic in the size of the ground set.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 08:11:23 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Anegg", "Georg", ""], ["Angelidakis", "Haris", ""], ["Kurpisz", "Adam", ""], ["Zenklusen", "Rico", ""]]}, {"id": "2007.03950", "submitter": "Polina Rozenshtein", "authors": "Polina Rozenshtein, Giulia Preti, Aristides Gionis, and Yannis\n  Velegrakis", "title": "Mining Dense Subgraphs with Similar Edges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When searching for interesting structures in graphs, it is often important to\ntake into account not only the graph connectivity, but also the metadata\navailable, such as node and edge labels, or temporal information. In this paper\nwe are interested in settings where such metadata is used to define a\nsimilarity between edges. We consider the problem of finding subgraphs that are\ndense and whose edges are similar to each other with respect to a given\nsimilarity function. Depending on the application, this function can be, for\nexample, the Jaccard similarity between the edge label sets, or the temporal\ncorrelation of the edge occurrences in a temporal graph. We formulate a\nLagrangian relaxation-based optimization problem to search for dense subgraphs\nwith high pairwise edge similarity. We design a novel algorithm to solve the\nproblem through parametric MinCut, and provide an efficient search scheme to\niterate through the values of the Lagrangian multipliers. Our study is\ncomplemented by an evaluation on real-world datasets, which demonstrates the\nusefulness and efficiency of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 08:15:31 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Rozenshtein", "Polina", ""], ["Preti", "Giulia", ""], ["Gionis", "Aristides", ""], ["Velegrakis", "Yannis", ""]]}, {"id": "2007.04008", "submitter": "\\v{S}imon Schierreich", "authors": "\\v{S}imon Schierreich and Ond\\v{r}ej Such\\'y", "title": "Waypoint Routing on Bounded Treewidth Graphs", "comments": null, "journal-ref": null, "doi": "10.1016/j.ipl.2021.106165", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the \\textsc{Waypoint Routing Problem} one is given an undirected\ncapacitated and weighted graph $G$, a source-destination pair $s,t\\in V(G)$ and\na set $W\\subseteq V(G)$, of \\emph{waypoints}. The task is to find a walk which\nstarts at the source vertex $s$, visits, in any order, all waypoints, ends at\nthe destination vertex $t$, respects edge capacities, that is, traverses each\nedge at most as many times as is its capacity, and minimizes the cost computed\nas the sum of costs of traversed edges with multiplicities. We study the\nproblem for graphs of bounded treewidth and present a new algorithm for the\nproblem working in $2^{O(\\mathrm{tw})}\\cdot n$ time, significantly improving\nupon the previously known algorithms. We also show that this running time is\noptimal for the problem under Exponential Time Hypothesis.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 10:24:13 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Schierreich", "\u0160imon", ""], ["Such\u00fd", "Ond\u0159ej", ""]]}, {"id": "2007.04059", "submitter": "Xinrui Jia", "authors": "Xinrui Jia, Kshiteej Sheth, Ola Svensson", "title": "Fair Colorful k-Center Clustering", "comments": "19 pages, 5 figures. A preliminary version of this work was presented\n  at the 21st Conference on Integer Programming and Combinatorial Optimization\n  (IPCO 2020)", "journal-ref": "In: Integer Programming and Combinatorial Optimization. IPCO 2020.\n  LNCS, vol 12125. pp 209-222", "doi": "10.1007/978-3-030-45771-6_17", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An instance of colorful k-center consists of points in a metric space that\nare colored red or blue, along with an integer k and a coverage requirement for\neach color. The goal is to find the smallest radius \\r{ho} such that there\nexist balls of radius \\r{ho} around k of the points that meet the coverage\nrequirements. The motivation behind this problem is twofold. First, from\nfairness considerations: each color/group should receive a similar service\nguarantee, and second, from the algorithmic challenges it poses: this problem\ncombines the difficulties of clustering along with the subset-sum problem. In\nparticular, we show that this combination results in strong integrality gap\nlower bounds for several natural linear programming relaxations. Our main\nresult is an efficient approximation algorithm that overcomes these\ndifficulties to achieve an approximation guarantee of 3, nearly matching the\ntight approximation guarantee of 2 for the classical k-center problem which\nthis problem generalizes.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 12:18:23 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Jia", "Xinrui", ""], ["Sheth", "Kshiteej", ""], ["Svensson", "Ola", ""]]}, {"id": "2007.04128", "submitter": "Max Pedersen", "authors": "Philip Bille and Inge Li G{\\o}rtz and Max Rish{\\o}j Pedersen and Eva\n  Rotenberg and Teresa Anna Steiner", "title": "String Indexing for Top-$k$ Close Consecutive Occurrences", "comments": "Fixed typos, minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic string indexing problem is to preprocess a string $S$ into a\ncompact data structure that supports efficient subsequent pattern matching\nqueries, that is, given a pattern string $P$, report all occurrences of $P$\nwithin $S$. In this paper, we study a basic and natural extension of string\nindexing called the string indexing for top-$k$ close consecutive occurrences\nproblem (SITCCO). Here, a consecutive occurrence is a pair $(i,j)$, $i < j$,\nsuch that $P$ occurs at positions $i$ and $j$ in $S$ and there is no occurrence\nof $P$ between $i$ and $j$, and their distance is defined as $j-i$. Given a\npattern $P$ and a parameter $k$, the goal is to report the top-$k$ consecutive\noccurrences of $P$ in $S$ of minimal distance. The challenge is to compactly\nrepresent $S$ while supporting queries in time close to length of $P$ and $k$.\nWe give two time-space trade-offs for the problem. Let $n$ be the length of\n$S$, $m$ the length of $P$, and $\\epsilon\\in(0,1]$. Our first result achieves\n$O(n\\log n)$ space and optimal query time of $O(m+k)$, and our second result\nachieves linear space and query time $O(m+k^{1+\\epsilon})$. Along the way, we\ndevelop several techniques of independent interest, including a new translation\nof the problem into a line segment intersection problem and a new recursive\nclustering technique for trees.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 13:55:10 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 09:18:53 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Bille", "Philip", ""], ["G\u00f8rtz", "Inge Li", ""], ["Pedersen", "Max Rish\u00f8j", ""], ["Rotenberg", "Eva", ""], ["Steiner", "Teresa Anna", ""]]}, {"id": "2007.04411", "submitter": "Suman Banerjee", "authors": "Suman Banerjee, Bithika Pal", "title": "An Efficient Updation Approach for Enumerating Maximal $(\\Delta,\n  \\gamma)$\\mbox{-}Cliques of a Temporal Network", "comments": "37 Pages, Submitted to a Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a temporal network $\\mathcal{G}(\\mathcal{V}, \\mathcal{E},\n\\mathcal{T})$, $(\\mathcal{X},[t_a,t_b])$ (where $\\mathcal{X} \\subseteq\n\\mathcal{V}(\\mathcal{G})$ and $[t_a,t_b] \\subseteq \\mathcal{T}$) is said to be\na $(\\Delta, \\gamma)$\\mbox{-}clique of $\\mathcal{G}$, if for every pair of\nvertices in $\\mathcal{X}$, there must exist at least $\\gamma$ links in each\n$\\Delta$ duration within the time interval $[t_a,t_b]$. Enumerating such\nmaximal cliques is an important problem in temporal network analysis, as it\nreveals contact pattern among the nodes of $\\mathcal{G}$. In this paper, we\nstudy the maximal $(\\Delta, \\gamma)$\\mbox{-}clique enumeration problem in\nonline setting; i.e.; the entire link set of the network is not known in\nadvance, and the links are coming as a batch in an iterative manner. Suppose,\nthe link set till time stamp $T_{1}$ (i.e., $\\mathcal{E}^{T_{1}}$), and its\ncorresponding $(\\Delta, \\gamma)$-clique set are known. In the next batch (till\ntime $T_{2}$), a new set of links (denoted as $\\mathcal{E}^{(T_1,T_2]}$) is\narrived.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 20:24:20 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Banerjee", "Suman", ""], ["Pal", "Bithika", ""]]}, {"id": "2007.04468", "submitter": "Guilherme de Castro Mendes Gomes", "authors": "Guilherme C. M. Gomes, Vinicius F. dos Santos, Murilo V. G. da Silva,\n  Jayme L. Szwarcfiter", "title": "FPT and kernelization algorithms for the k-in-a-tree problem", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The three-in-a-tree problem asks for an induced tree of the input graph\ncontaining three mandatory vertices. In 2006, Chudnovsky and Seymour\n[Combinatorica, 2010] presented the first polynomial time algorithm for this\nproblem, which has become a critical subroutine in many algorithms for\ndetecting induced subgraphs, such as beetles, pyramids, thetas, and even and\nodd-holes. In 2007, Derhy and Picouleau [Discrete Applied Mathematics, 2009]\nconsidered the natural generalization to $k$ mandatory vertices, proving that,\nwhen $k$ is part of the input, the problem is $\\mathsf{NP}$-complete, and ask\nwhat is the complexity of four-in-a-tree. Motivated by this question and the\nrelevance of the original problem, we study the parameterized complexity of\n$k$-in-a-tree. We begin by showing that the problem is $\\mathsf{W[1]}$-hard\nwhen jointly parameterized by the size of the solution and minimum clique cover\nand, under the Exponential Time Hypothesis, does not admit an $n^{o(k)}$ time\nalgorithm. Afterwards, we use Courcelle's Theorem to prove fixed-parameter\ntractability under cliquewidth, which prompts our investigation into which\nparameterizations admit single exponential algorithms; we show that such\nalgorithms exist for the unrelated parameterizations treewidth, distance to\ncluster, and distance to co-cluster. In terms of kernelization, we present a\nlinear kernel under feedback edge set, and show that no polynomial kernel\nexists under vertex cover nor distance to clique unless $\\mathsf{NP} \\subseteq\n\\mathsf{coNP}/\\mathsf{poly}$. Along with other remarks and previous work, our\ntractability and kernelization results cover many of the most commonly employed\nparameters in the graph parameter hierarchy.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 22:53:27 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Gomes", "Guilherme C. M.", ""], ["Santos", "Vinicius F. dos", ""], ["da Silva", "Murilo V. G.", ""], ["Szwarcfiter", "Jayme L.", ""]]}, {"id": "2007.04513", "submitter": "Tesshu Hanaka", "authors": "Gabriel L. Duarte, Hiroshi Eto, Tesshu Hanaka, Yasuaki Kobayashi,\n  Yusuke Kobayashi, Daniel Lokshtanov, Lehilton L. C. Pedrosa, Rafael C. S.\n  Schouery, and U\\'everton S. Souza", "title": "Computing the Largest Bond and the Maximum Connected Cut of a Graph", "comments": "This paper resulted from a merge of two papers submitted to arXiv\n  (arXiv:1908.03389 and arXiv:1910.01071). Both preliminary versions were\n  presented at the 14th International Symposium on Parameterized and Exact\n  Computation (IPEC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cut-set $\\partial(S)$ of a graph $G=(V,E)$ is the set of edges that have\none endpoint in $S\\subset V$ and the other endpoint in $V\\setminus S$, and\nwhenever $G[S]$ is connected, the cut $[S,V\\setminus S]$ of $G$ is called a\nconnected cut. A bond of a graph $G$ is an inclusion-wise minimal disconnecting\nset of $G$, i.e., bonds are cut-sets that determine cuts $[S,V\\setminus S]$ of\n$G$ such that $G[S]$ and $G[V\\setminus S]$ are both connected. Contrasting with\na large number of studies related to maximum cuts, there exist very few results\nregarding the largest bond of general graphs. In this paper, we aim to reduce\nthis gap on the complexity of computing the largest bond, and the maximum\nconnected cut of a graph. Although cuts and bonds are similar, we remark that\ncomputing the largest bond and the maximum connected cut of a graph tends to be\nharder than computing its maximum cut. We show that it does not exist a\nconstant-factor approximation algorithm to compute the largest bond, unless P =\nNP. Also, we show that {\\sc Largest Bond} and {\\sc Maximum Connected Cut} are\nNP-hard even for planar bipartite graphs, whereas \\textsc{Maximum Cut} is\ntrivial on bipartite graphs and polynomial-time solvable on planar graphs. In\naddition, we show that {\\sc Largest Bond} and {\\sc Maximum Connected Cut} are\nNP-hard on split graphs, and restricted to graphs of clique-width $w$ they can\nnot be solved in time $f(w)\\times n^{o(w)}$ unless the Exponential Time\nHypothesis fails, but they can be solved in time $f(w)\\times n^{O(w)}$.\nFinally, we show that both problems are fixed-parameter tractable when\nparameterized by the size of the solution, the treewidth, and the twin-cover\nnumber.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 02:25:25 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Duarte", "Gabriel L.", ""], ["Eto", "Hiroshi", ""], ["Hanaka", "Tesshu", ""], ["Kobayashi", "Yasuaki", ""], ["Kobayashi", "Yusuke", ""], ["Lokshtanov", "Daniel", ""], ["Pedrosa", "Lehilton L. C.", ""], ["Schouery", "Rafael C. S.", ""], ["Souza", "U\u00e9verton S.", ""]]}, {"id": "2007.04726", "submitter": "Sebastian Schmidt", "authors": "Massimo Cairo, Shahbaz Khan, Romeo Rizzi, Sebastian Schmidt and\n  Alexandru I. Tomescu", "title": "Safety in $s$-$t$ Paths, Trails and Walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a directed graph $G$ and a pair of nodes $s$ and $t$, an \\emph{$s$-$t$\nbridge} of $G$ is an edge whose removal breaks all $s$-$t$ paths of $G$ (and\nthus appears in all $s$-$t$ paths). Computing all $s$-$t$ bridges of $G$ is a\nbasic graph problem, solvable in linear time.\n  In this paper, we consider a natural generalisation of this problem, with the\nnotion of \"safety\" from bioinformatics. We say that a walk $W$ is \\emph{safe}\nwith respect to a set $\\mathcal{W}$ of $s$-$t$ walks, if $W$ is a subwalk of\nall walks in $\\mathcal{W}$. We start by considering the maximal safe walks when\n$\\mathcal{W}$ consists of: all $s$-$t$ paths, all $s$-$t$ trails, or all\n$s$-$t$ walks of $G$. We show that the first two problems are immediate\nlinear-time generalisations of finding all $s$-$t$ bridges, while the third\nproblem is more involved. In particular, we show that there exists a compact\nrepresentation computable in linear time, that allows outputting all maximal\nsafe walks in time linear in their length.\n  We further generalise these problems, by assuming that safety is defined only\nwith respect to a subset of \\emph{visible} edges. Here we prove a dichotomy\nbetween the $s$-$t$ paths and $s$-$t$ trails cases, and the $s$-$t$ walks case:\nthe former two are NP-hard, while the latter is solvable with the same\ncomplexity as when all edges are visible. We also show that the same complexity\nresults hold for the analogous generalisations of \\emph{$s$-$t$ articulation\npoints} (nodes appearing in all $s$-$t$ paths).\n  We thus obtain the best possible results for natural \"safety\"-generalisations\nof these two fundamental graph problems. Moreover, our algorithms are simple\nand do not employ any complex data structures, making them ideal for use in\npractice.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 11:53:19 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 10:09:57 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Cairo", "Massimo", ""], ["Khan", "Shahbaz", ""], ["Rizzi", "Romeo", ""], ["Schmidt", "Sebastian", ""], ["Tomescu", "Alexandru I.", ""]]}, {"id": "2007.04937", "submitter": "Zeev Nutov", "authors": "Moran Feldman, Zeev Nutov and Elad Shoham", "title": "Practical Budgeted Submodular Maximization", "comments": "31 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing a non-negative monotone submodular\nfunction subject to a knapsack constraint, which is also known as the Budgeted\nSubmodular Maximization (BSM) problem. Sviridenko (2004) showed that by\nguessing 3 appropriate elements of an optimal solution, and then executing a\ngreedy algorithm, one can obtain the optimal approximation ratio of $\\alpha\n=1-1/e\\approx 0.632$ for BSM. However, the need to guess (by enumeration) 3\nelements makes the algorithm of Sviridenko impractical as it leads to a time\ncomplexity of $O(n^5)$ (which can be slightly improved using the thresholding\ntechnique of Badanidiyuru & Vondrak (2014), but only to roughly $O(n^4)$). Our\nmain results in this paper show that fewer guesses suffice. Specifically, by\nmaking only 2 guesses, we get the same optimal approximation ratio of $\\alpha$\nwith an improved time complexity of roughly $O(n^3)$. Furthermore, by making\nonly a single guess, we get an almost as good approximation ratio of\n$0.6174>0.9767\\alpha$ in roughly $O(n^2)$ time.\n  Prior to our work, the only algorithms that were known to obtain an\napproximation ratio close to $\\alpha$ for BSM were the algorithm of Sviridenko\nand an algorithm of Ene & Nguyen (2019) that achieves\n$(\\alpha-\\epsilon)$-approximation. However, the algorithm of Ene & Nguyen\nrequires ${(1/\\epsilon)}^{O(1/\\epsilon^4)}n\\log^2 n$ time, and hence, is of\ntheoretical interest only as ${(1/\\epsilon)}^{O(1/\\epsilon^4)}$ is huge even\nfor moderate values of $\\epsilon$. In contrast, all the algorithms we analyze\nare simple and parallelizable, which makes them good candidates for practical\nuse.\n  Recently, Tang et al. (2020) studied a simple greedy algorithm that already\nhas a long research history, and proved that its approximation ratio is at\nleast 0.405. We improve over this result, and show that the approximation ratio\nof this algorithm is within the range [0.427, 0.462].\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:07:45 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 06:59:18 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Feldman", "Moran", ""], ["Nutov", "Zeev", ""], ["Shoham", "Elad", ""]]}, {"id": "2007.05014", "submitter": "Georgios Amanatidis", "authors": "Georgios Amanatidis, Federico Fusco, Philip Lazos, Stefano Leonardi,\n  Rebecca Reiffenh\\\"auser", "title": "Fast Adaptive Non-Monotone Submodular Maximization Subject to a Knapsack\n  Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained submodular maximization problems encompass a wide variety of\napplications, including personalized recommendation, team formation, and\nrevenue maximization via viral marketing. The massive instances occurring in\nmodern day applications can render existing algorithms prohibitively slow,\nwhile frequently, those instances are also inherently stochastic. Focusing on\nthese challenges, we revisit the classic problem of maximizing a (possibly\nnon-monotone) submodular function subject to a knapsack constraint. We present\na simple randomized greedy algorithm that achieves a $5.83$ approximation and\nruns in $O(n \\log n)$ time, i.e., at least a factor $n$ faster than other\nstate-of-the-art algorithms. The robustness of our approach allows us to\nfurther transfer it to a stochastic version of the problem. There, we obtain a\n$9$-approximation to the best adaptive policy, which is the first constant\napproximation for non-monotone objectives. Experimental evaluation of our\nalgorithms showcases their improved performance on real and synthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 18:15:01 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Amanatidis", "Georgios", ""], ["Fusco", "Federico", ""], ["Lazos", "Philip", ""], ["Leonardi", "Stefano", ""], ["Reiffenh\u00e4user", "Rebecca", ""]]}, {"id": "2007.05246", "submitter": "Ignasi Sau", "authors": "Lucas Keiler, Carlos Vinicius G. C. Lima, Ana Karolinna Maia, Rudini\n  Sampaio, Ignasi Sau", "title": "Target set selection with maximum activation time", "comments": "27 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A target set selection model is a graph $G$ with a threshold function\n$\\tau:V\\to \\mathbb{N}$ upper-bounded by the vertex degree. For a given model, a\nset $S_0\\subseteq V(G)$ is a target set if $V(G)$ can be partitioned into\nnon-empty subsets $S_0,S_1,\\dotsc,S_t$ such that, for $i \\in \\{1, \\ldots, t\\}$,\n$S_i$ contains exactly every vertex $v$ having at least $\\tau(v)$ neighbors in\n$S_0\\cup\\dots\\cup S_{i-1}$. We say that $t$ is the activation time\n$t_{\\tau}(S_0)$ of the target set $S_0$. The problem of, given such a model,\nfinding a target set of minimum size has been extensively studied in the\nliterature. In this article, we investigate its variant, which we call\nTSS-time, in which the goal is to find a target set $S_0$ that maximizes\n$t_{\\tau}(S_0)$. That is, given a graph $G$, a threshold function $\\tau$ in\n$G$, and an integer $k$, the objective of the TSS-time problem is to decide\nwhether $G$ contains a target set $S_0$ such that $t_{\\tau}(S_0)\\geq k$. Let\n$\\tau^* = \\max_{v \\in V(G)} \\tau(v)$. Our main result is the following\ndichotomy about the complexity of TSS-time when $G$ belongs to a minor-closed\ngraph class ${\\cal C}$: if ${\\cal C}$ has bounded local treewidth, the problem\nis FPT parameterized by $k$ and $\\tau^{\\star}$; otherwise, it is NP-complete\neven for fixed $k=4$ and $\\tau^{\\star}=2$. We also prove that, with $\\tau^*=2$,\nthe problem is NP-hard in bipartite graphs for fixed $k=5$, and from previous\nresults we observe that TSS-time is NP-hard in planar graphs and W[1]-hard\nparameterized by treewidth. Finally, we present a linear-time algorithm to find\na target set $S_0$ in a given tree maximizing $t_{\\tau}(S_0)$.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 08:46:03 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Keiler", "Lucas", ""], ["Lima", "Carlos Vinicius G. C.", ""], ["Maia", "Ana Karolinna", ""], ["Sampaio", "Rudini", ""], ["Sau", "Ignasi", ""]]}, {"id": "2007.05316", "submitter": "Keren Censor-Hillel", "authors": "Keren Censor-Hillel and Fran\\c{c}ois Le Gall and Dean Leitersdorf", "title": "On Distributed Listing of Cliques", "comments": "To appear in PODC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show an $\\tilde{O}(n^{p/(p+2)})$-round algorithm in the \\congest model for\n\\emph{listing} of $K_p$ (a clique with $p$ nodes), for all $p =4, p\\geq 6$. For\n$p = 5$, we show an $\\tilde{O}(n^{3/4})$-round algorithm.\n  For $p=4$ and $p=5$, our results improve upon the previous state-of-the-art\nof $O(n^{5/6+o(1)})$ and $O(n^{21/22+o(1)})$, respectively, by Eden et al.\n[DISC 2019]. For all $p\\geq 6$, ours is the first sub-linear round algorithm\nfor $K_p$ listing.\n  We leverage the recent expander decomposition algorithm of Chang et al. [SODA\n2019] to create clusters with a good mixing time. Three key novelties in our\nalgorithm are: (1) we carefully iterate our listing process with coupled values\nof min-degree within the clusters and arboricity outside the clusters, (2) all\nthe listing is done within the cluster, which necessitates new techniques for\nbringing into the cluster the information about \\emph{all} edges that can\npotentially form $K_p$ instances with the cluster edges, and (3) within each\ncluster we use a sparsity-aware listing algorithm, which is faster than a\ngeneral listing algorithm and which we can allow the cluster to use since we\nmake sure to sparsify the graph as the iterations proceed.\n  As a byproduct of our algorithm, we show an \\emph{optimal} sparsity-aware\nalgorithm for $K_p$ listing, which runs in $\\tilde{\\Theta}(1 + m/n^{1 + 2/p})$\nrounds in the \\clique model. Previously, Pandurangan et al. [SPAA 2018], Chang\net al. [SODA 2019], and Censor-Hillel et al. [TCS 2020] showed sparsity-aware\nalgorithms for the case of $p = 3$, yet ours is the first such sparsity aware\nalgorithm for $p \\geq 4$.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 11:27:32 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Gall", "Fran\u00e7ois Le", ""], ["Leitersdorf", "Dean", ""]]}, {"id": "2007.05453", "submitter": "Giuseppe Vietri", "authors": "Giuseppe Vietri, Grace Tian, Mark Bun, Thomas Steinke, Zhiwei Steven\n  Wu", "title": "New Oracle-Efficient Algorithms for Private Synthetic Data Release", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present three new algorithms for constructing differentially private\nsynthetic data---a sanitized version of a sensitive dataset that approximately\npreserves the answers to a large collection of statistical queries. All three\nalgorithms are \\emph{oracle-efficient} in the sense that they are\ncomputationally efficient when given access to an optimization oracle. Such an\noracle can be implemented using many existing (non-private) optimization tools\nsuch as sophisticated integer program solvers. While the accuracy of the\nsynthetic data is contingent on the oracle's optimization performance, the\nalgorithms satisfy differential privacy even in the worst case. For all three\nalgorithms, we provide theoretical guarantees for both accuracy and privacy.\nThrough empirical evaluation, we demonstrate that our methods scale well with\nboth the dimensionality of the data and the number of queries. Compared to the\nstate-of-the-art method High-Dimensional Matrix Mechanism \\cite{McKennaMHM18},\nour algorithms provide better accuracy in the large workload and high privacy\nregime (corresponding to low privacy loss $\\varepsilon$).\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 15:46:05 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Vietri", "Giuseppe", ""], ["Tian", "Grace", ""], ["Bun", "Mark", ""], ["Steinke", "Thomas", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "2007.05523", "submitter": "Rogers Epstein", "authors": "Rogers Epstein", "title": "Local Access to Sparse Connected Subgraphs Via Edge Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We contribute an approach to the problem of locally computing sparse\nconnected subgraphs of dense graphs. In this setting, given an edge in a\nconnected graph $G = (V, E)$, an algorithm locally decides its membership in a\nsparse connected subgraph $G^* = (V, E^*)$, where $E^* \\subseteq E$ and $|E^*|\n= o(|E|)$. Such an approach to subgraph construction is useful when dealing\nwith massive graphs, where reading in the graph's full network description is\nimpractical.\n  While most prior results in this area require assumptions on $G$ or that\n$|E'| \\le (1+\\epsilon)|V|$ for some $\\epsilon > 0$, we relax these assumptions.\nGiven a general graph and a parameter $T$, we provide membership queries to a\nsubgraph with $O(|V|T)$ edges using $\\widetilde{O}(|E|/T)$ probes. This is the\nfirst algorithm to work on general graphs and allow for a tradeoff between its\nprobe complexity and the number of edges in the resulting subgraph.\n  We achieve this result with ideas motivated from edge sparsification\ntechniques that were previously unused in this problem. We believe these\ntechniques will motivate new algorithms for this problem and related ones.\nAdditionally, we describe an efficient method to access any node's neighbor set\nin a sparsified version of $G$ where each edge is deleted with some i.i.d.\nprobability.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 17:57:33 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Epstein", "Rogers", ""]]}, {"id": "2007.05557", "submitter": "Yingyu Liang", "authors": "Yingyu Liang and Hui Yuan", "title": "Learning Entangled Single-Sample Gaussians in the Subset-of-Signals\n  Model", "comments": "Appear in COLT'2020. Updates: corrected comments on existing works;\n  added comparison to median estimator", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the setting of entangled single-sample distributions, the goal is to\nestimate some common parameter shared by a family of $n$ distributions, given\none single sample from each distribution. This paper studies mean estimation\nfor entangled single-sample Gaussians that have a common mean but different\nunknown variances. We propose the subset-of-signals model where an unknown\nsubset of $m$ variances are bounded by 1 while there are no assumptions on the\nother variances. In this model, we analyze a simple and natural method based on\niteratively averaging the truncated samples, and show that the method achieves\nerror $O \\left(\\frac{\\sqrt{n\\ln n}}{m}\\right)$ with high probability when\n$m=\\Omega(\\sqrt{n\\ln n})$, matching existing bounds for this range of $m$. We\nfurther prove lower bounds, showing that the error is\n$\\Omega\\left(\\left(\\frac{n}{m^4}\\right)^{1/2}\\right)$ when $m$ is between\n$\\Omega(\\ln n)$ and $O(n^{1/4})$, and the error is\n$\\Omega\\left(\\left(\\frac{n}{m^4}\\right)^{1/6}\\right)$ when $m$ is between\n$\\Omega(n^{1/4})$ and $O(n^{1 - \\epsilon})$ for an arbitrarily small\n$\\epsilon>0$, improving existing lower bounds and extending to a wider range of\n$m$.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 18:25:38 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liang", "Yingyu", ""], ["Yuan", "Hui", ""]]}, {"id": "2007.05634", "submitter": "Victor Reis", "authors": "Victor Reis, Thomas Rothvoss", "title": "Vector Balancing in Lebesgue Spaces", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tantalizing conjecture in discrete mathematics is the one of Koml\\'os,\nsuggesting that for any vectors $\\mathbf{a}_1,\\ldots,\\mathbf{a}_n \\in B_2^m$\nthere exist signs $x_1, \\dots, x_n \\in \\{ -1,1\\}$ so that $\\|\\sum_{i=1}^n\nx_i\\mathbf{a}_i\\|_\\infty \\le O(1)$. It is a natural extension to ask what\n$\\ell_q$-norm bound to expect for $\\mathbf{a}_1,\\ldots,\\mathbf{a}_n \\in B_p^m$.\nWe prove that, for $2 \\le p \\le q \\le \\infty$, such vectors admit fractional\ncolorings $x_1, \\dots, x_n \\in [-1,1]$ with a linear number of $\\pm 1$\ncoordinates so that $\\|\\sum_{i=1}^n x_i\\mathbf{a}_i\\|_q \\leq\nO(\\sqrt{\\min(p,\\log(2m/n))}) \\cdot n^{1/2-1/p+ 1/q}$, and that one can obtain a\nfull coloring at the expense of another factor of $\\frac{1}{1/2 - 1/p + 1/q}$.\nIn particular, for $p \\in (2,3]$ we can indeed find signs $\\mathbf{x} \\in \\{\n-1,1\\}^n$ with $\\|\\sum_{i=1}^n x_i\\mathbf{a}_i\\|_\\infty \\le O(n^{1/2-1/p} \\cdot\n\\frac{1}{p-2})$. Our result generalizes Spencer's theorem, for which $p = q =\n\\infty$, and is tight for $m = n$.\n  Additionally, we prove that for any fixed constant $\\delta>0$, in a centrally\nsymmetric body $K \\subseteq \\mathbb{R}^n$ with measure at least $e^{-\\delta n}$\none can find such a fractional coloring in polynomial time. Previously this was\nknown only for a small enough constant -- indeed in this regime classical\nnonconstructive arguments do not apply and partial colorings of the form\n$\\mathbf{x} \\in \\{ -1,0,1\\}^n$ do not necessarily exist.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 22:25:06 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Reis", "Victor", ""], ["Rothvoss", "Thomas", ""]]}, {"id": "2007.05637", "submitter": "Gautam Mahapatra", "authors": "Gautam Mahapatra, Priodyuti Pradhan, Ranjan Chattaraj and Soumya\n  Banerjee", "title": "Dynamic Graph Streaming Algorithm for Digital Contact Tracing", "comments": "13 Pages, 6 Figures with Supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital contact tracing of an infected person, testing the possible infection\nfor the contacted persons, and isolation play a crucial role in alleviating the\noutbreak. Here, we design a dynamic graph streaming algorithm that can trace\nthe contacts under the control of the Public Health Authorities (PHA). Our\nalgorithm receives proximity data from the mobile devices as contact data\nstreams and uses a sliding window model to construct a dynamic contact graph\nsketch. Prominently, we introduce the edge label of the contact graph as a\nbinary contact vector, which acts like a sliding window and holds the latest D\ndays (incubation period) of temporal social interactions. Notably, the\nalgorithm prepares the direct and indirect (multilevel) contact list from the\ncontact graph sketch for a given set of infected persons. Finally, the\nalgorithm also uses a disjoint set data structure to construct the infection\npathways for the trace list. The present study offers the design of algorithms\nwith underlying data structures for digital contact trace relevant to the\nproximity data produced by Bluetooth enabled mobile devices. Our analysis\nreveals that for COVID-19 close contact parameters, the storage space requires\nmaintaining the contact graph of ten million users; having 14 days of close\ncontact data in the PHA server takes 55 Gigabytes of memory and preparation of\nthe contact list for a given set of the infected person depends on the size of\nthe infected list. Our centralized digital contact tracing framework can also\nbe applicable for other relevant diseases parameterized by an incubation period\nand proximity duration of contacts.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 22:45:57 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 08:28:58 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 01:03:25 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Mahapatra", "Gautam", ""], ["Pradhan", "Priodyuti", ""], ["Chattaraj", "Ranjan", ""], ["Banerjee", "Soumya", ""]]}, {"id": "2007.05647", "submitter": "Wenshuo Guo", "authors": "Wenshuo Guo, Mihaela Curmei, Serena Wang, Benjamin Recht, Michael I.\n  Jordan", "title": "Finding Equilibrium in Multi-Agent Games with Payoff Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding equilibrium strategies in multi-agent games\nwith incomplete payoff information, where the payoff matrices are only known to\nthe players up to some bounded uncertainty sets. In such games, an ex-post\nequilibrium characterizes equilibrium strategies that are robust to the payoff\nuncertainty. When the game is one-shot, we show that in zero-sum polymatrix\ngames, an ex-post equilibrium can be computed efficiently using linear\nprogramming. We further extend the notion of ex-post equilibrium to stochastic\ngames, where the game is played repeatedly in a sequence of stages and the\ntransition dynamics are governed by an Markov decision process (MDP). We\nprovide sufficient condition for the existence of an ex-post Markov perfect\nequilibrium (MPE). We show that under bounded payoff uncertainty, the value of\nany two-player zero-sum stochastic game can be computed up to a tight value\ninterval using dynamic programming.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 23:38:53 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Guo", "Wenshuo", ""], ["Curmei", "Mihaela", ""], ["Wang", "Serena", ""], ["Recht", "Benjamin", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2007.05852", "submitter": "Arman Adibi", "authors": "Arman Adibi, Aryan Mokhtari, Hamed Hassani", "title": "Submodular Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a discrete variant of the meta-learning\nframework. Meta-learning aims at exploiting prior experience and data to\nimprove performance on future tasks. By now, there exist numerous formulations\nfor meta-learning in the continuous domain. Notably, the Model-Agnostic\nMeta-Learning (MAML) formulation views each task as a continuous optimization\nproblem and based on prior data learns a suitable initialization that can be\nadapted to new, unseen tasks after a few simple gradient updates. Motivated by\nthis terminology, we propose a novel meta-learning framework in the discrete\ndomain where each task is equivalent to maximizing a set function under a\ncardinality constraint. Our approach aims at using prior data, i.e., previously\nvisited tasks, to train a proper initial solution set that can be quickly\nadapted to a new task at a relatively low computational cost. This approach\nleads to (i) a personalized solution for each individual task, and (ii)\nsignificantly reduced computational cost at test time compared to the case\nwhere the solution is fully optimized once the new task is revealed. The\ntraining procedure is performed by solving a challenging discrete optimization\nproblem for which we present deterministic and randomized algorithms. In the\ncase where the tasks are monotone and submodular, we show strong theoretical\nguarantees for our proposed methods even though the training objective may not\nbe submodular. We also demonstrate the effectiveness of our framework on two\nreal-world problem instances where we observe that our methods lead to a\nsignificant reduction in computational complexity in solving the new tasks\nwhile incurring a small performance loss compared to when the tasks are fully\noptimized.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 21:02:48 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 00:39:23 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Adibi", "Arman", ""], ["Mokhtari", "Aryan", ""], ["Hassani", "Hamed", ""]]}, {"id": "2007.05870", "submitter": "Andrej (Andy) Brodnik", "authors": "Andrej Brodnik and Aleksander Malni\\v{c} and Rok Po\\v{z}ar", "title": "A subquadratic algorithm for the simultaneous conjugacy problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $d$-Simultaneous Conjugacy problem in the symmetric group $S_n$ asks\nwhether there exists a permutation $\\tau \\in S_n$ such that $b_j = \\tau^{-1}a_j\n\\tau$ holds for all $j = 1,2,\\ldots, d$, where $a_1, a_2,\\ldots , a_d$ and\n$b_1, b_2,\\ldots , b_d$ are given sequences of permutations in $S_n$. The time\ncomplexity of existing algorithms for solving the problem is $O(dn^2)$. We show\nthat for a given positive integer $d$ the $d$-Simultaneous Conjugacy problem in\n$S_n$ can be solved in $o(n^2)$ time.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 22:50:58 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Brodnik", "Andrej", ""], ["Malni\u010d", "Aleksander", ""], ["Po\u017ear", "Rok", ""]]}, {"id": "2007.05912", "submitter": "Daniel Kane", "authors": "Daniel M. Kane", "title": "Robust Learning of Mixtures of Gaussians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We resolve one of the major outstanding problems in robust statistics. In\nparticular, if $X$ is an evenly weighted mixture of two arbitrary\n$d$-dimensional Gaussians, we devise a polynomial time algorithm that given\naccess to samples from $X$ an $\\eps$-fraction of which have been adversarially\ncorrupted, learns $X$ to error $\\poly(\\eps)$ in total variation distance.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 05:15:50 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Kane", "Daniel M.", ""]]}, {"id": "2007.06060", "submitter": "Mathew Francis", "authors": "Mathew Francis and Rian Neogi and Venkatesh Raman", "title": "Recognizing $k$-Clique Extendible Orderings", "comments": "15 pages, 4 figures, an extended abstract of this paper will be\n  included in the proceedings of WG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is $k$-clique-extendible if there is an ordering of the vertices such\nthat whenever two $k$-sized overlapping cliques $A$ and $B$ have $k-1$ common\nvertices, and these common vertices appear between the two vertices $a,b\\in\n(A\\setminus B)\\cup (B\\setminus A)$ in the ordering, there is an edge between\n$a$ and $b$, implying that $A\\cup B$ is a $(k+1)$-sized clique. Such an\nordering is said to be a $k$-C-E ordering. These graphs arise in applications\nrelated to modelling preference relations. Recently, it has been shown that a\nmaximum sized clique in such a graph can be found in $n^{O(k)}$ time when the\nordering is given. When $k$ is $2$, such graphs are precisely the well-known\nclass of comparability graphs and when $k$ is $3$ they are called\ntriangle-extendible graphs. It has been shown that triangle-extendible graphs\nappear as induced subgraphs of visibility graphs of simple polygons, and the\ncomplexity of recognizing them has been mentioned as an open problem in the\nliterature.\n  While comparability graphs (i.e. $2$-C-E graphs) can be recognized in\npolynomial time, we show that recognizing $k$-C-E graphs is NP-hard for any\nfixed $k \\geq 3$ and co-NP-hard when $k$ is part of the input. While our\nNP-hardness reduction for $k \\geq 4$ is from the betweenness problem, for\n$k=3$, our reduction is an intricate one from the $3$-colouring problem. We\nalso show that the problems of determining whether a given ordering of the\nvertices of a graph is a $k$-C-E ordering, and that of finding an $\\ell$-sized\n(or maximum sized) clique in a $k$-C-E graph, given a $k$-C-E ordering, are\ncomplete for the parameterized complexity classes co-W[1] and W[1]\nrespectively, when parameterized by $k$. However we show that the former is\nfixed-parameter tractable when parameterized by the treewidth of the graph.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 18:26:19 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Francis", "Mathew", ""], ["Neogi", "Rian", ""], ["Raman", "Venkatesh", ""]]}, {"id": "2007.06098", "submitter": "Deeparnab Chakrabarty", "authors": "Sepehr Assadi, Deeparnab Chakrabarty, Sanjeev Khanna", "title": "Graph Connectivity and Single Element Recovery via Linear and OR Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding a spanning forest in an undirected,\n$n$-vertex multi-graph under two basic query models. One is the Linear query\nmodel which are linear measurements on the incidence vector induced by the\nedges; the other is the weaker OR query model which only reveals whether a\ngiven subset of plausible edges is empty or not. At the heart of our study lies\na fundamental problem which we call the {\\em single element recovery} problem:\ngiven a non-negative real vector $x$ in $N$ dimension, return a single element\n$x_j > 0$ from the support. Queries can be made in rounds, and our goals is to\nunderstand the trade-offs between the query complexity and the rounds of\nadaptivity needed to solve these problems, for both deterministic and\nrandomized algorithms. These questions have connections and ramifications to\nmultiple areas such as sketching, streaming, graph reconstruction, and\ncompressed sensing. Our main results are:\n  * For the single element recovery problem, it is easy to obtain a\ndeterministic, $r$-round algorithm which makes $(N^{1/r}-1)$-queries per-round.\nWe prove that this is tight: any $r$-round deterministic algorithm must make\n$\\geq (N^{1/r} - 1)$ linear queries in some round. In contrast, a $1$-round\n$O(\\log^2 N)$-query randomized algorithm which succeeds 99% of the time is\nknown to exist.\n  * We design a deterministic $O(r)$-round, $\\tilde{O}(n^{1+1/r})$-OR query\nalgorithm for graph connectivity. We complement this with an\n$\\tilde{\\Omega}(n^{1 + 1/r})$-lower bound for any $r$-round deterministic\nalgorithm in the OR-model.\n  * We design a randomized, $2$-round algorithm for the graph connectivity\nproblem which makes $\\tilde{O}(n)$-OR queries. In contrast, we prove that any\n$1$-round algorithm (possibly randomized) requires $\\tilde{\\Omega}(n^2)$-OR\nqueries.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 21:12:19 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 14:15:52 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Assadi", "Sepehr", ""], ["Chakrabarty", "Deeparnab", ""], ["Khanna", "Sanjeev", ""]]}, {"id": "2007.06105", "submitter": "Wojciech Janczewski", "authors": "Maciej Dul\\k{e}ba, Pawe{\\l} Gawrychowski, Wojciech Janczewski", "title": "Efficient Labeling for Reachability in Digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider labeling nodes of a directed graph for reachability queries. A\nreachability labeling scheme for such a graph assigns a binary string, called a\nlabel, to each node. Then, given the labels of nodes $u$ and $v$ and no other\ninformation about the underlying graph, it should be possible to determine\nwhether there exists a directed path from $u$ to $v$. By a simple information\ntheoretical argument and invoking the bound on the number of partial orders, in\nany scheme some labels need to consist of at least $n/4$ bits, where $n$ is the\nnumber of nodes. On the other hand, it is not hard to design a scheme with\nlabels consisting of $n/2+O(\\log n)$ bits. In the classical centralised\nsetting, Munro and Nicholson designed a data structure for reachability queries\nconsisting of $n^2/4+o(n^2)$ bits (which is optimal, up to the lower order\nterm). We extend their approach to obtain a scheme with labels consisting of\n$n/3+o(n)$ bits.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 21:50:13 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Dul\u0119ba", "Maciej", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Janczewski", "Wojciech", ""]]}, {"id": "2007.06110", "submitter": "Kevin Schewior", "authors": "Jos\\'e Correa, Paul D\\\"utting, Felix Fischer, Kevin Schewior, Bruno\n  Ziliotto", "title": "Unknown I.I.D. Prophets: Better Bounds, Streaming Algorithms, and a New\n  Impossibility", "comments": "Accepted to ITCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prophet inequality states, for some $\\alpha\\in[0,1]$, that the expected\nvalue achievable by a gambler who sequentially observes random variables\n$X_1,\\dots,X_n$ and selects one of them is at least an $\\alpha$ fraction of the\nmaximum value in the sequence. We obtain three distinct improvements for a\nsetting that was first studied by Correa et al. (EC, 2019) and is particularly\nrelevant to modern applications in algorithmic pricing. In this setting, the\nrandom variables are i.i.d. from an unknown distribution and the gambler has\naccess to an additional $\\beta n$ samples for some $\\beta\\geq 0$. We first give\nimproved lower bounds on $\\alpha$ for a wide range of values of $\\beta$;\nspecifically, $\\alpha\\geq(1+\\beta)/e$ when $\\beta\\leq 1/(e-1)$, which is tight,\nand $\\alpha\\geq 0.648$ when $\\beta=1$, which improves on a bound of around\n$0.635$ due to Correa et al. (SODA, 2020). Adding to their practical appeal,\nspecifically in the context of algorithmic pricing, we then show that the new\nbounds can be obtained even in a streaming model of computation and thus in\nsituations where the use of relevant data is complicated by the sheer amount of\ndata available. We finally establish that the upper bound of $1/e$ for the case\nwithout samples is robust to additional information about the distribution, and\napplies also to sequences of i.i.d. random variables whose distribution is\nitself drawn, according to a known distribution, from a finite set of known\ncandidate distributions. This implies a tight prophet inequality for\nexchangeable sequences of random variables, answering a question of Hill and\nKertz (Contemporary Mathematics, 1992), but leaves open the possibility of\nbetter guarantees when the number of candidate distributions is small, a\nsetting we believe is of strong interest to applications.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 22:08:06 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 00:33:07 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Correa", "Jos\u00e9", ""], ["D\u00fctting", "Paul", ""], ["Fischer", "Felix", ""], ["Schewior", "Kevin", ""], ["Ziliotto", "Bruno", ""]]}, {"id": "2007.06167", "submitter": "Daniel Roodt", "authors": "Daniel Roodt, Ulrich Speidel, Vimal Kumar and Ryan K. L. Ko", "title": "Local Editing in LZ-End Compressed Data", "comments": "12 pages, 1 Figure, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm for the modification of data compressed\nusing LZ-End, a derivate of LZ77, without prior decompression. The performance\nof the algorithm and the impact of the modifications on the compression ratio\nis evaluated. Finally, we discuss the importance of this work as a first step\ntowards local editing in Lempel-Ziv compressed data.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 03:20:18 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Roodt", "Daniel", ""], ["Speidel", "Ulrich", ""], ["Kumar", "Vimal", ""], ["Ko", "Ryan K. L.", ""]]}, {"id": "2007.06242", "submitter": "Siddharth Barman", "authors": "Siddharth Barman, Umang Bhaskar, Nisarg Shah", "title": "Optimal Bounds on the Price of Fairness for Indivisible Goods", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the allocation of resources to a set of agents, how do fairness guarantees\nimpact the social welfare? A quantitative measure of this impact is the price\nof fairness, which measures the worst-case loss of social welfare due to\nfairness constraints. While initially studied for divisible goods, recent work\non the price of fairness also studies the setting of indivisible goods.\n  In this paper, we resolve the price of two well-studied fairness notions for\nthe allocation of indivisible goods: envy-freeness up to one good (EF1), and\napproximate maximin share (MMS). For both EF1 and 1/2-MMS guarantees, we show,\nvia different techniques, that the price of fairness is $O(\\sqrt{n})$, where\n$n$ is the number of agents. From previous work, it follows that our bounds are\ntight. Our bounds are obtained via efficient algorithms. For 1/2-MMS, our bound\nholds for additive valuations, whereas for EF1, our bound holds for the more\ngeneral class of subadditive valuations. This resolves an open problem posed by\nBei et al. (2019).\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 08:49:53 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 18:55:09 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 06:38:32 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Barman", "Siddharth", ""], ["Bhaskar", "Umang", ""], ["Shah", "Nisarg", ""]]}, {"id": "2007.06360", "submitter": "Vishesh Jain", "authors": "Vishesh Jain, Ashwin Sah, Mehtaab Sawhney", "title": "Perfectly Sampling $k\\geq (8/3 +o(1))\\Delta$-Colorings in Graphs", "comments": "21 pages; comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized algorithm which takes as input an undirected graph\n$G$ on $n$ vertices with maximum degree $\\Delta$, and a number of colors $k\n\\geq (8/3 + o_{\\Delta}(1))\\Delta$, and returns -- in expected time\n$\\tilde{O}(n\\Delta^{2}\\log{k})$ -- a proper $k$-coloring of $G$ distributed\nperfectly uniformly on the set of all proper $k$-colorings of $G$. Notably, our\nsampler breaks the barrier at $k = 3\\Delta$ encountered in recent work of\nBhandari and Chakraborty [STOC 2020]. We also sketch how to modify our methods\nto relax the restriction on $k$ to $k \\geq (8/3 - \\epsilon_0)\\Delta$ for an\nabsolute constant $\\epsilon_0 > 0$.\n  As in the work of Bhandari and Chakraborty, and the pioneering work of Huber\n[STOC 1998], our sampler is based on Coupling from the Past [Propp&Wilson,\nRandom Struct. Algorithms, 1995] and the bounding chain method [Huber, STOC\n1998; H\\\"aggstr\\\"om&Nelander, Scand. J. Statist., 1999]. Our innovations\ninclude a novel bounding chain routine inspired by Jerrum's analysis of the\nGlauber dynamics [Random Struct. Algorithms, 1995], as well as a\npreconditioning routine for bounding chains which uses the algorithmic Lov\\'asz\nLocal Lemma [Moser&Tardos, J.ACM, 2010].\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 13:12:32 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Jain", "Vishesh", ""], ["Sah", "Ashwin", ""], ["Sawhney", "Mehtaab", ""]]}, {"id": "2007.06444", "submitter": "Jeannette Janssen", "authors": "Jeannette Janssen and Aaron Smith", "title": "Reconstruction of Line-Embeddings of Graphons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a random graph process with $n$ vertices corresponding to points\n$v_{i} \\sim {Unif}[0,1]$ embedded randomly in the interval, and where edges are\ninserted between $v_{i}, v_{j}$ independently with probability given by the\ngraphon $w(v_{i},v_{j}) \\in [0,1]$. Following Chuangpishit et al. (2015), we\ncall a graphon $w$ diagonally increasing if, for each $x$, $w(x,y)$ decreases\nas $y$ moves away from $x$. We call a permutation $\\sigma \\in S_{n}$ an\nordering of these vertices if $v_{\\sigma(i)} < v_{\\sigma(j)}$ for all $i < j$,\nand ask: how can we accurately estimate $\\sigma$ from an observed graph? We\npresent a randomized algorithm with output $\\hat{\\sigma}$ that, for a large\nclass of graphons, achieves error $\\max_{1 \\leq i \\leq n} | \\sigma(i) -\n\\hat{\\sigma}(i)| = O^{*}(\\sqrt{n})$ with high probability; we also show that\nthis is the best-possible convergence rate for a large class of algorithms and\nproof strategies. Under an additional assumption that is satisfied by some\npopular graphon models, we break this \"barrier\" at $\\sqrt{n}$ and obtain the\nvastly better rate $O^{*}(n^{\\epsilon})$ for any $\\epsilon > 0$. These improved\nseriation bounds can be combined with previous work to give more efficient and\naccurate algorithms for related tasks, including: estimating diagonally\nincreasing graphons, and testing whether a graphon is diagonally increasing.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 15:30:33 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Janssen", "Jeannette", ""], ["Smith", "Aaron", ""]]}, {"id": "2007.06455", "submitter": "Pat Morin", "authors": "Prosenjit Bose, Vida Dujmovi\\'c, Mehrnoosh Javarsineh, and Pat Morin", "title": "Asymptotically Optimal Vertex Ranking of Planar Graphs", "comments": "Small corrections. Added a section describing the dependence on \\ell", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A (vertex) $\\ell$-ranking is a labelling $\\varphi:V(G)\\to\\mathbb{N}$ of the\nvertices of a graph $G$ with integer colours so that for any path\n$u_0,\\ldots,u_p$ of length at most $\\ell$, $\\varphi(u_0)\\neq\\varphi(u_p)$ or\n$\\varphi(u_0)<\\max\\{\\varphi(u_0),\\ldots,\\varphi(u_p)\\}$. We show that, for any\nfixed integer $\\ell\\ge 2$, every $n$-vertex planar graph has an $\\ell$-ranking\nusing $O(\\log n/\\log\\log\\log n)$ colours and this is tight even when $\\ell=2$;\nfor infinitely many values of $n$, there are $n$-vertex planar graphs, for\nwhich any 2-ranking requires $\\Omega(\\log n/\\log\\log\\log n)$ colours. This\nresult also extends to bounded genus graphs.\n  In developing this proof we obtain optimal bounds on the number of colours\nneeded for $\\ell$-ranking graphs of treewidth $t$ and graphs of simple\ntreewidth $t$. These upper bounds are constructive and give $O(n\\log n)$-time\nalgorithms. Additional results that come from our techniques include new\nsublogarithmic upper bounds on the number of colours needed for $\\ell$-rankings\nof apex minor-free graphs and $k$-planar graphs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 15:40:57 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 11:48:04 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Bose", "Prosenjit", ""], ["Dujmovi\u0107", "Vida", ""], ["Javarsineh", "Mehrnoosh", ""], ["Morin", "Pat", ""]]}, {"id": "2007.06539", "submitter": "Marcin Bria\\'nski", "authors": "Jan Gwinner, Marcin Bria\\'nski, Wojciech Burkot, {\\L}ukasz\n  Czerwi\\'nski, Vladyslav Hlembotskyi", "title": "Benchmarking 16-element quantum search algorithms on superconducting\n  quantum processors", "comments": "12 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present experimental results on running 4-qubit unstructured search on IBM\nquantum processors. Our best attempt attained probability of success around\n24.5%. We try several algorithms and use the most recent developments in\nquantum search to reduce the number of entangling gates that are currently\nconsidered the main source of errors in quantum computations. Comparing\ntheoretical expectations of an algorithm performance with the actual data, we\nexplore the hardware limits, showing sharp, phase-transition-like degradation\nof performance on quantum processors. We conclude that it is extremely\nimportant to design hardware-aware algorithms and to include any other low\nlevel optimizations on NISQ devices.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 17:46:19 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 15:19:57 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 10:34:51 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Gwinner", "Jan", ""], ["Bria\u0144ski", "Marcin", ""], ["Burkot", "Wojciech", ""], ["Czerwi\u0144ski", "\u0141ukasz", ""], ["Hlembotskyi", "Vladyslav", ""]]}, {"id": "2007.06555", "submitter": "Pranjal Awasthi", "authors": "Pranjal Awasthi, Himanshu Jain, Ankit Singh Rawat, Aravindan\n  Vijayaraghavan", "title": "Adversarial robustness via robust low rank representations", "comments": "fixed a bug in the proof of Proposition B.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial robustness measures the susceptibility of a classifier to\nimperceptible perturbations made to the inputs at test time. In this work we\nhighlight the benefits of natural low rank representations that often exist for\nreal data such as images, for training neural networks with certified\nrobustness guarantees.\n  Our first contribution is for certified robustness to perturbations measured\nin $\\ell_2$ norm. We exploit low rank data representations to provide improved\nguarantees over state-of-the-art randomized smoothing-based approaches on\nstandard benchmark datasets such as CIFAR-10 and CIFAR-100.\n  Our second contribution is for the more challenging setting of certified\nrobustness to perturbations measured in $\\ell_\\infty$ norm. We demonstrate\nempirically that natural low rank representations have inherent robustness\nproperties, that can be leveraged to provide significantly better guarantees\nfor certified robustness to $\\ell_\\infty$ perturbations in those\nrepresentations. Our certificate of $\\ell_\\infty$ robustness relies on a\nnatural quantity involving the $\\infty \\to 2$ matrix operator norm associated\nwith the representation, to translate robustness guarantees from $\\ell_2$ to\n$\\ell_\\infty$ perturbations.\n  A key technical ingredient for our certification guarantees is a fast\nalgorithm with provable guarantees based on the multiplicative weights update\nmethod to provide upper bounds on the above matrix norm. Our algorithmic\nguarantees improve upon the state of the art for this problem, and may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 17:57:00 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 04:25:25 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Jain", "Himanshu", ""], ["Rawat", "Ankit Singh", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "2007.06604", "submitter": "Amihood Amir", "authors": "Amihood Amir and Itai Boneh", "title": "Update Query Time Trade-off for dynamic Suffix Arrays", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Suffix Array SA(S) of a string S[1 ... n] is an array containing all the\nsuffixes of S sorted by lexicographic order. The suffix array is one of the\nmost well known indexing data structures, and it functions as a key tool in\nmany string algorithms. In this paper, we present a data structure for\nmaintaining the Suffix Array of a dynamic string. For every $0 \\leq \\varepsilon\n\\leq 1$, our data structure reports SA[i] in $\\tilde{O}(n^{\\varepsilon})$ time\nand handles text modification in $\\tilde{O}(n^{1-\\varepsilon})$ time.\nAdditionally, our data structure enables the same query time for reporting\niSA[i], with iSA being the Inverse Suffix Array of S[1 ... n]. Our data\nstructure can be used to construct sub-linear dynamic variants of static\nstrings algorithms or data structures that are based on the Suffix Array and\nthe Inverse Suffix Array.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 18:11:19 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Amir", "Amihood", ""], ["Boneh", "Itai", ""]]}, {"id": "2007.06693", "submitter": "Richard Barr", "authors": "Richard S. Barr (Southern Methodist University) and Thomas McLoud", "title": "The Invisible Hand Heuristic for Origin-Destination Integer\n  Multicommodity Network Flows", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Origin-destination integer multicommodity flow problems differ from classic\nmulticommodity models in that each commodity has one source and one sink, and\neach commodity must be routed along a single path. A new invisible-hand\nheuristic that mimics economic markets' behavior is presented and tested on\nlarge-scale telecommunications networks, with solution times two orders of\nmagnitude faster than Cplex's LP relaxation, more dramatic MIP ratios, and\nsmall solution value differences.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 21:10:53 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Barr", "Richard S.", "", "Southern Methodist University"], ["McLoud", "Thomas", ""]]}, {"id": "2007.06744", "submitter": "David Woodruff", "authors": "Edith Cohen, Rasmus Pagh, David P. Woodruff", "title": "WOR and $p$'s: Sketches for $\\ell_p$-Sampling Without Replacement", "comments": "21 pages. Added references in related work in the introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted sampling is a fundamental tool in data analysis and machine learning\npipelines. Samples are used for efficient estimation of statistics or as sparse\nrepresentations of the data. When weight distributions are skewed, as is often\nthe case in practice, without-replacement (WOR) sampling is much more effective\nthan with-replacement (WR) sampling: it provides a broader representation and\nhigher accuracy for the same number of samples. We design novel composable\nsketches for WOR $\\ell_p$ sampling, weighted sampling of keys according to a\npower $p\\in[0,2]$ of their frequency (or for signed data, sum of updates). Our\nsketches have size that grows only linearly with the sample size. Our design is\nsimple and practical, despite intricate analysis, and based on off-the-shelf\nuse of widely implemented heavy hitters sketches such as CountSketch. Our\nmethod is the first to provide WOR sampling in the important regime of $p>1$\nand the first to handle signed updates for $p>0$.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 00:19:27 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 15:06:53 GMT"}, {"version": "v3", "created": "Sat, 15 Aug 2020 06:12:25 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Cohen", "Edith", ""], ["Pagh", "Rasmus", ""], ["Woodruff", "David P.", ""]]}, {"id": "2007.06754", "submitter": "Warut Suksompong", "authors": "Paul W. Goldberg, Alexandros Hollender, Ayumi Igarashi, Pasin\n  Manurangsi, Warut Suksompong", "title": "Consensus Halving for Sets of Items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus halving refers to the problem of dividing a resource into two parts\nso that every agent values both parts equally. Prior work has shown that when\nthe resource is represented by an interval, a consensus halving with at most\n$n$ cuts always exists, but is hard to compute even for agents with simple\nvaluation functions. In this paper, we study consensus halving in a natural\nsetting where the resource consists of a set of items without a linear\nordering. When agents have additive utilities, we present a polynomial-time\nalgorithm that computes a consensus halving with at most $n$ cuts, and show\nthat $n$ cuts are almost surely necessary when the agents' utilities are drawn\nfrom probabilistic distributions. On the other hand, we show that for a simple\nclass of monotonic utilities, the problem already becomes PPAD-hard.\nFurthermore, we compare and contrast consensus halving with the more general\nproblem of consensus $k$-splitting, where we wish to divide the resource into\n$k$ parts in possibly unequal ratios, and provide some consequences of our\nresults on the problem of computing small agreeable sets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 01:20:33 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Goldberg", "Paul W.", ""], ["Hollender", "Alexandros", ""], ["Igarashi", "Ayumi", ""], ["Manurangsi", "Pasin", ""], ["Suksompong", "Warut", ""]]}, {"id": "2007.06828", "submitter": "Dorit Hochbaum", "authors": "Dorit S. Hochbaum and Xu Rao", "title": "Network Flow Methods for the Minimum Covariates Imbalance Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of balancing covariates arises in observational studies where one\nis given a group of control samples and another group, disjoint from the\ncontrol group, of treatment samples. Each sample, in either group, has several\nobserved nominal covariates. The values, or categories, of each covariate\npartition the treatment and control samples to a number of subsets referred to\nas \\textit{levels} where the samples at every level share the same covariate\nvalue. We address here a problem of selecting a subset of the control group so\nas to balance, to the best extent possible, the sizes of the levels between the\ntreatment group and the selected subset of control group, the min-imbalance\nproblem.\n  It is proved here that the min-imbalance problem, on two covariates, is\nsolved efficiently with network flow techniques. We present an integer\nprogramming formulation of the problem where the constraint matrix is totally\nunimodular, implying that the linear programming relaxation to the problem has\nall basic solutions, and in particular the optimal solution, integral. This\ninteger programming formulation is linked to a minimum cost network flow\nproblem which is solvable in $O(n\\cdot (n' + n\\log n))$ steps, for $n$ the size\nof the treatment group and $n'$ the size of the control group. A more efficient\nalgorithm is further devised based on an alternative, maximum flow, formulation\nof the two-covariate min-imbalance problem, that runs in $O(n'^{3/2}\\log^2n)$\nsteps.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 05:30:45 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Hochbaum", "Dorit S.", ""], ["Rao", "Xu", ""]]}, {"id": "2007.06869", "submitter": "Karthik Abinav Sankararaman", "authors": "Karthik Abinav Sankararaman, Anand Louis, Navin Goyal", "title": "Robust Identifiability in Linear Structural Equation Models of Causal\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the problem of robust parameter estimation from\nobservational data in the context of linear structural equation models (LSEMs).\nLSEMs are a popular and well-studied class of models for inferring causality in\nthe natural and social sciences. One of the main problems related to LSEMs is\nto recover the model parameters from the observational data. Under various\nconditions on LSEMs and the model parameters the prior work provides efficient\nalgorithms to recover the parameters. However, these results are often about\ngeneric identifiability. In practice, generic identifiability is not sufficient\nand we need robust identifiability: small changes in the observational data\nshould not affect the parameters by a huge amount. Robust identifiability has\nreceived far less attention and remains poorly understood. Sankararaman et al.\n(2019) recently provided a set of sufficient conditions on parameters under\nwhich robust identifiability is feasible. However, a limitation of their work\nis that their results only apply to a small sub-class of LSEMs, called\n``bow-free paths.'' In this work, we significantly extend their work along\nmultiple dimensions. First, for a large and well-studied class of LSEMs, namely\n``bow free'' models, we provide a sufficient condition on model parameters\nunder which robust identifiability holds, thereby removing the restriction of\npaths required by prior work. We then show that this sufficient condition holds\nwith high probability which implies that for a large set of parameters robust\nidentifiability holds and that for such parameters, existing algorithms already\nachieve robust identifiability. Finally, we validate our results on both\nsimulated and real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 07:32:36 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Sankararaman", "Karthik Abinav", ""], ["Louis", "Anand", ""], ["Goyal", "Navin", ""]]}, {"id": "2007.06896", "submitter": "Gregory Gutin", "authors": "J. Bang-Jensen, E. Eiben, G. Gutin, M. Wahlstrom, A. Yeo", "title": "Component Order Connectivity in Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A directed graph $D$ is semicomplete if for every pair $x,y$ of vertices of\n$D,$ there is at least one arc between $x$ and $y.$ \\viol{Thus, a tournament is\na semicomplete digraph.} In the Directed Component Order Connectivity (DCOC)\nproblem, given a digraph $D=(V,A)$ and a pair of natural numbers $k$ and\n$\\ell$, we are to decide whether there is a subset $X$ of $V$ of size $k$ such\nthat the largest strong connectivity component in $D-X$ has at most $\\ell$\nvertices. Note that DCOC reduces to the Directed Feedback Vertex Set problem\nfor $\\ell=1.$ We study parametered complexity of DCOC for general and\nsemicomplete digraphs with the following parameters: $k, \\ell,\\ell+k$ and\n$n-\\ell$. In particular, we prove that DCOC with parameter $k$ on semicomplete\ndigraphs can be solved in time $O^*(2^{16k})$ but not in time $O^*(2^{o(k)})$\nunless the Exponential Time Hypothesis (ETH) fails. \\gutin{The upper bound\n$O^*(2^{16k})$ implies the upper bound $O^*(2^{16(n-\\ell)})$ for the parameter\n$n-\\ell.$ We complement the latter by showing that there is no algorithm of\ntime complexity $O^*(2^{o({n-\\ell})})$ unless ETH fails.} Finally, we improve\n\\viol{(in dependency on $\\ell$)} the upper bound of G{\\\"{o}}ke, Marx and Mnich\n(2019) for the time complexity of DCOC with parameter $\\ell+k$ on general\ndigraphs from $O^*(2^{O(k\\ell\\log (k\\ell))})$ to $O^*(2^{O(k\\log (k\\ell))}).$\nNote that Drange, Dregi and van 't Hof (2016) proved that even for the\nundirected version of DCOC on split graphs there is no algorithm of running\ntime $O^*(2^{o(k\\log \\ell)})$ unless ETH fails and it is a long-standing\nproblem to decide whether Directed Feedback Vertex Set admits an algorithm of\ntime complexity $O^*(2^{o(k\\log k)}).$\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 08:12:51 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 05:48:52 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Bang-Jensen", "J.", ""], ["Eiben", "E.", ""], ["Gutin", "G.", ""], ["Wahlstrom", "M.", ""], ["Yeo", "A.", ""]]}, {"id": "2007.06920", "submitter": "Tillmann Miltzow", "authors": "Simon Hengeveld and Tillmann Miltzow", "title": "A Practical Algorithm with Performance Guarantees for the Art~Gallery\n  Problem", "comments": "30 pages main body, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a closed simple polygon $P$, we say two points $p,q$ see each other if\nthe segment $pq$ is fully contained in $P$. The art gallery problem seeks a\nminimum size set $G\\subset P$ of guards that sees $P$ completely. The only\ncurrently correct algorithm to solve the art gallery problem exactly uses\nalgebraic methods and is attributed to Sharir. As the art gallery problem is\nER-complete, it seems unlikely to avoid algebraic methods, without additional\nassumptions. In this paper, we introduce the notion of vision stability. In\norder to describe vision stability consider an enhanced guard that can see\n``around the corner'' by an angle of $\\delta$ or a diminished guard whose\nvision is by an angle of $\\delta$ ``blocked'' by reflex vertices. A polygon $P$\nhas vision stability $\\delta$ if the optimal number of enhanced guards to guard\n$P$ is the same as the optimal number of diminished guards to guard $P$. We\nwill argue that most relevant polygons are vision stable. We describe a\none-shot vision stable algorithm that computes an optimal guard set for\nvisionstable polygons using polynomial time and solving one integer program. It\nguarantees to find the optimal solution for every vision stable polygon. We\nimplemented an iterative visionstable algorithm and show its practical\nperformance is slower, but comparable with other state of the art algorithms.\nOur iterative algorithm is inspired and follows closely the one-shot algorithm.\nIt delays several steps and only computes them when deemed necessary. Given a\nchord $c$ of a polygon, we denote by $n(c)$ the number of vertices visible from\n$c$. The chord-width of a polygon is the maximum $n(c)$ over all possible\nchords $c$. The set of vision stable polygons admits an FPT algorithm when\nparametrized by the chord-width. Furthermore, the one-shot algorithm runs in\nFPT time, when parameterized by the number of reflex vertices.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 09:09:22 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Hengeveld", "Simon", ""], ["Miltzow", "Tillmann", ""]]}, {"id": "2007.07025", "submitter": "Marcin Bienkowski", "authors": "Marcin Bienkowski, Bj\\\"orn Feldkord, Pawe{\\l} Schmidt", "title": "A Nearly Optimal Deterministic Online Algorithm for Non-Metric Facility\n  Location", "comments": "STACS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the online non-metric variant of the facility location problem, there is a\ngiven graph consisting of a set $F$ of facilities (each with a certain opening\ncost), a set $C$ of potential clients, and weighted connections between them.\nThe online part of the input is a sequence of clients from $C$, and in response\nto any requested client, an online algorithm may open an additional subset of\nfacilities and must connect the given client to an open facility.\n  We give an online, polynomial-time deterministic algorithm for this problem,\nwith a competitive ratio of $O(\\log |F| \\cdot (\\log |C| + \\log \\log |F|))$. The\nresult is optimal up to loglog factors. Our algorithm improves over the\n$O((\\log |C| + \\log |F|) \\cdot (\\log |C| + \\log \\log |F|))$-competitive\nconstruction that first reduces the facility location instance to a set cover\none and then later solves such instance using the deterministic algorithm by\nAlon et al. [TALG 2006]. This is an asymptotic improvement in a typical\nscenario where $|F| \\ll |C|$.\n  We achieve this by a more direct approach: we design an algorithm for a\nfractional relaxation of the non-metric facility location problem with\nclustered facilities. To handle the constraints of such non-covering LP, we\ncombine the dual fitting and multiplicative weight updates approach. By\nmaintaining certain additional monotonicity properties of the created\nfractional solution, we can handle the dependencies between facilities and\nconnections in a rounding routine.\n  Our result, combined with the algorithm by Naor et al. [FOCS 2011] yields the\nfirst deterministic algorithm for the online node-weighted Steiner tree\nproblem. The resulting competitive ratio is $O(\\log k \\cdot \\log^2 \\ell)$ on\ngraphs of $\\ell$ nodes and $k$ terminals.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 13:33:03 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 15:40:58 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Bienkowski", "Marcin", ""], ["Feldkord", "Bj\u00f6rn", ""], ["Schmidt", "Pawe\u0142", ""]]}, {"id": "2007.07040", "submitter": "Mathys Rennela", "authors": "Mathys Rennela, Alfons Laarman, Vedran Dunjko", "title": "Hybrid divide-and-conquer approach for tree search algorithms", "comments": "40 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we are entering the era of real-world small quantum computers, finding\napplications for these limited devices is a key challenge. In this vein, it was\nrecently shown that a hybrid classical-quantum method can help provide\npolynomial speed-ups to classical divide-and-conquer algorithms, even when only\ngiven access to a quantum computer much smaller than the problem itself. In\nthis work we study the hybrid divide-and-conquer method in the context of tree\nsearch algorithms, and extend it by including quantum backtracking, which\nallows better results than previous Grover-based methods. Further, we provide\ngeneral criteria for polynomial speed-ups in the tree search context, and\nprovide a number of examples where polynomial speed ups, using arbitrarily\nsmaller quantum computers, can still be obtained. This study possible speed-ups\nfor the well known algorithm of DPLL and prove threshold-free speed-ups for the\ntree search subroutines of the so-called PPSZ algorithm - which is the core of\nthe fastest exact Boolean satisfiability solver - for certain classes of\nformulas. We also provide a simple example where speed-ups can be obtained in\nan algorithm-independent fashion, under certain well-studied\ncomplexity-theoretical assumptions. Finally, we briefly discuss the fundamental\nlimitations of hybrid methods in providing speed-ups for larger problems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 13:57:12 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Rennela", "Mathys", ""], ["Laarman", "Alfons", ""], ["Dunjko", "Vedran", ""]]}, {"id": "2007.07049", "submitter": "Tongyang Li", "authors": "Daochen Wang, Xuchen You, Tongyang Li, Andrew M. Childs", "title": "Quantum exploration algorithms for multi-armed bandits", "comments": "18 pages, 1 figure. To appear in the Thirty-Fifth AAAI Conference on\n  Artificial Intelligence (AAAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the best arm of a multi-armed bandit is a central problem in\nbandit optimization. We study a quantum computational version of this problem\nwith coherent oracle access to states encoding the reward probabilities of each\narm as quantum amplitudes. Specifically, we show that we can find the best arm\nwith fixed confidence using\n$\\tilde{O}\\bigl(\\sqrt{\\sum_{i=2}^n\\Delta^{\\smash{-2}}_i}\\bigr)$ quantum\nqueries, where $\\Delta_{i}$ represents the difference between the mean reward\nof the best arm and the $i^\\text{th}$-best arm. This algorithm, based on\nvariable-time amplitude amplification and estimation, gives a quadratic speedup\ncompared to the best possible classical result. We also prove a matching\nquantum lower bound (up to poly-logarithmic factors).\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 14:15:20 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 18:31:01 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Wang", "Daochen", ""], ["You", "Xuchen", ""], ["Li", "Tongyang", ""], ["Childs", "Andrew M.", ""]]}, {"id": "2007.07161", "submitter": "Jiaxin Xie", "authors": "Ming-Jun Lai, Jiaxin Xie, Zhiqiang Xu", "title": "Graph Sparsification by Universal Greedy Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph sparsification is to approximate an arbitrary graph by a sparse graph\nand is useful in many applications, such as simplification of social networks,\nleast squares problems, numerical solution of symmetric positive definite\nlinear systems and etc. In this paper, inspired by the well-known sparse signal\nrecovery algorithm called orthogonal matching pursuit (OMP), we introduce a\ndeterministic, greedy edge selection algorithm called universal greedy approach\n(UGA) for graph sparsification. For a general spectral sparsification problem,\ne.g., positive subset selection problem from a set of $m$ vectors from\n$\\mathbb{R}^n$, we propose a nonnegative UGA algorithm which needs $O(mn^2+\nn^3/\\epsilon^2)$ time to find a\n$\\frac{1+\\epsilon/\\beta}{1-\\epsilon/\\beta}$-spectral sparsifier with positive\ncoefficients with sparsity $\\le\\lceil\\frac{n}{\\epsilon^2}\\rceil$, where $\\beta$\nis the ratio between the smallest length and largest length of the vectors. The\nconvergence of the nonnegative UGA algorithm will be established. For the graph\nsparsification problem, another UGA algorithm will be proposed which can output\na $\\frac{1+O(\\epsilon)}{1-O(\\epsilon)}$-spectral sparsifier with\n$\\lceil\\frac{n}{\\epsilon^2}\\rceil$ edges in $O(m+n^2/\\epsilon^2)$ time from a\ngraph with $m$ edges and $n$ vertices under some mild assumptions. This is a\nlinear time algorithm in terms of the number of edges that the community of\ngraph sparsification is looking for. The best result in the literature to the\nknowledge of the authors is the existence of a deterministic algorithm which is\nalmost linear, i.e. $O(m^{1+o(1)})$ for some\n$o(1)=O(\\frac{(\\log\\log(m))^{2/3}}{\\log^{1/3}(m)})$. Finally, extensive\nexperimental results, including applications to graph clustering and least\nsquares regression, show the effectiveness of proposed approaches.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 16:28:58 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 12:01:51 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Lai", "Ming-Jun", ""], ["Xie", "Jiaxin", ""], ["Xu", "Zhiqiang", ""]]}, {"id": "2007.07294", "submitter": "Max Bender", "authors": "Max Bender, Jacob Gilbert, Aditya Krishnan, Kirk Pruhs", "title": "Competitively Pricing Parking in a Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by demand-responsive parking pricing systems we consider\nposted-price algorithms for the online metrical matching problem and the online\nmetrical searching problem in a tree metric. Our main result is a poly-log\ncompetitive posted-price algorithm for online metrical searching.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 18:44:42 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 02:10:13 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Bender", "Max", ""], ["Gilbert", "Jacob", ""], ["Krishnan", "Aditya", ""], ["Pruhs", "Kirk", ""]]}, {"id": "2007.07384", "submitter": "Brian Brubach", "authors": "Brian Brubach, Darshan Chakrabarti, John P. Dickerson, Samir Khuller,\n  Aravind Srinivasan, Leonidas Tsepenekas", "title": "A Pairwise Fair and Community-preserving Approach to k-Center Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a foundational problem in machine learning with numerous\napplications. As machine learning increases in ubiquity as a backend for\nautomated systems, concerns about fairness arise. Much of the current\nliterature on fairness deals with discrimination against protected classes in\nsupervised learning (group fairness). We define a different notion of fair\nclustering wherein the probability that two points (or a community of points)\nbecome separated is bounded by an increasing function of their pairwise\ndistance (or community diameter). We capture the situation where data points\nrepresent people who gain some benefit from being clustered together.\nUnfairness arises when certain points are deterministically separated, either\narbitrarily or by someone who intends to harm them as in the case of\ngerrymandering election districts. In response, we formally define two new\ntypes of fairness in the clustering setting, pairwise fairness and community\npreservation. To explore the practicality of our fairness goals, we devise an\napproach for extending existing $k$-center algorithms to satisfy these fairness\nconstraints. Analysis of this approach proves that reasonable approximations\ncan be achieved while maintaining fairness. In experiments, we compare the\neffectiveness of our approach to classical $k$-center algorithms/heuristics and\nexplore the tradeoff between optimal clustering and fairness.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 22:32:27 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Brubach", "Brian", ""], ["Chakrabarti", "Darshan", ""], ["Dickerson", "John P.", ""], ["Khuller", "Samir", ""], ["Srinivasan", "Aravind", ""], ["Tsepenekas", "Leonidas", ""]]}, {"id": "2007.07405", "submitter": "Adalat Jabrayilov", "authors": "Adalat Jabrayilov", "title": "On the hop-constrained Steiner tree problems", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hop-constrained Steiner tree problem is a generalization of the classical\nSteiner tree problem and asks for a minimum cost subtree that spans some\nspecified nodes of a given graph, such that the number of edges between each\nnode of the tree and its root respects a given hop limit. This NP-hard problem\nhas many variants, often modeled as integer linear programs. Two of the models\nare so-called assignment and partial-ordering based models, which yield (up to\nour knowledge) the best two state-of-the-art formulations for the problem\nvariant Steiner tree problem with revenues, budgets, and hop constraints. We\nshow that the linear programming relaxation of the partial-ordering based model\nis stronger than that of the assignment model for the hop-constrained Steiner\ntree problem; this also remains true for a class of hop-constrained problems,\nincluding the hop-constrained minimum spanning tree problem, the Steiner tree\nproblem with revenues, budgets, and hop constraints.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 23:52:45 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 00:30:31 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 14:45:10 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Jabrayilov", "Adalat", ""]]}, {"id": "2007.07449", "submitter": "Nathaniel Harms", "authors": "Nathaniel Harms, Yuichi Yoshida", "title": "Downsampling for Testing and Learning in Product Distributions", "comments": "31 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the domain reduction problem of eliminating dependence on $n$ from\nthe complexity of property testing and learning algorithms on domain $[n]^d$,\nand the related problem of establishing testing and learning results for\nproduct distributions over $\\mathbb{R}^d$. Our method, which we call\ndownsampling, gives conceptually simple proofs for several results:\n  1. A 1-page proof of the recent $o(d)$-query monotonicity tester for the\nhypergrid (Black, Chakrabarty & Seshadhri, SODA 2020), and an improvement from\n$O(d^7)$ to $\\widetilde O(d^4)$ in the sample complexity of their\ndistribution-free monotonicity tester for product distributions over\n$\\mathbb{R}^d$;\n  2. An $\\exp(\\widetilde O(kd))$-time agnostic learning algorithm for functions\nof $k$ convex sets in product distributions;\n  3. A polynomial-time agnostic learning algorithm for functions of a constant\nnumber of halfspaces in product distributions;\n  4. A polynomial-time agnostic learning algorithm for constant-degree\npolynomial threshold functions in product distributions;\n  5. An $\\exp(\\widetilde O(k \\sqrt d))$-time agnostic learning algorithm for\n$k$-alternating functions in product distributions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 02:46:44 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Harms", "Nathaniel", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "2007.07488", "submitter": "Pramesh Kumar", "authors": "Pramesh Kumar and Alireza Khani", "title": "An algorithm for integrating peer-to-peer ridesharing and schedule-based\n  transit system for first mile/last mile access", "comments": "36 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to limited transit network coverage and infrequent service, suburban\ncommuters often face the transit first mile/last mile (FMLM) problem. To deal\nwith this, they either drive to a park-and-ride location to take transit, use\ncarpooling, or drive directly to their destination to avoid inconvenience.\nRidesharing, an emerging mode of transportation, can solve the transit first\nmile/last mile problem. In this setup, a driver can drive a ride-seeker to a\ntransit station, from where the rider can take transit to her respective\ndestination. The problem requires solving a ridesharing matching problem with\nthe routing of riders in a multimodal transportation network. We develop a\ntransit-based ridesharing matching algorithm to solve this problem. The method\nleverages the schedule-based transit shortest path to generate feasible matches\nand then solves a matching optimization program to find an optimal match\nbetween riders and drivers. The proposed method not only assigns an optimal\ndriver to the rider but also assigns an optimal transit stop and a transit\nvehicle trip departing from that stop for the rest of the rider's itinerary. We\nalso introduce the application of space-time prism (STP) (the geographical area\nwhich can be reached by a traveler given the time constraints) in the context\nof ridesharing to reduce the computational time by reducing the network search.\nAn algorithm to solve this problem dynamically using a rolling horizon approach\nis also presented. We use simulated data obtained from the activity-based\ntravel demand model of Twin Cities, MN to show that the transit-based\nridesharing can solve the FMLM problem and save a significant number of\nvehicle-hours spent in the system.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 05:16:51 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Kumar", "Pramesh", ""], ["Khani", "Alireza", ""]]}, {"id": "2007.07515", "submitter": "Yaxiong Liu", "authors": "Yaxiong Liu, Kohei Hatano, Eiji Takimoto", "title": "Improved algorithms for online load balancing", "comments": "16 pages; typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an online load balancing problem and its extensions in the\nframework of repeated games. On each round, the player chooses a distribution\n(task allocation) over $K$ servers, and then the environment reveals the load\nof each server, which determines the computation time of each server for\nprocessing the task assigned. After all rounds, the cost of the player is\nmeasured by some norm of the cumulative computation-time vector. The cost is\nthe makespan if the norm is $L_\\infty$-norm. The goal is to minimize the\nregret, i.e., minimizing the player's cost relative to the cost of the best\nfixed distribution in hindsight. We propose algorithms for general norms and\nprove their regret bounds. In particular, for $L_\\infty$-norm, our regret bound\nmatches the best known bound and the proposed algorithm runs in polynomial time\nper trial involving linear programming and second order programming, whereas no\npolynomial time algorithm was previously known to achieve the bound.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 07:28:06 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 00:43:22 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Liu", "Yaxiong", ""], ["Hatano", "Kohei", ""], ["Takimoto", "Eiji", ""]]}, {"id": "2007.07553", "submitter": "Frank Stephan", "authors": "Gordon Hoi, Sanjay Jain and Frank Stephan", "title": "A Faster Exact Algorithm to Count X3SAT Solutions", "comments": "Paper accepted at conference CP 2020 which will appear in Springer\n  LNCS proceedings. This is an extended version of that paper which contains\n  additional material. DOI will be added when known", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Exact Satisfiability problem, XSAT, is defined as the problem of finding\na satisfying assignment to a formula in CNF such that there is exactly one\nliteral in each clause assigned to be 1 and the other literals in the same\nclause are set to 0. If we restrict the length of each clause to be at most 3\nliterals, then it is known as the X3SAT problem. In this paper, we consider the\nproblem of counting the number of satisfying assignments to the X3SAT problem,\nwhich is also known as #X3SAT.\n  The current state of the art exact algorithm to solve #X3SAT is given by\nDahll\\\"of, Jonsson and Beigel and runs in $O(1.1487^n)$, where $n$ is the\nnumber of variables in the formula. In this paper, we propose an exact\nalgorithm for the #X3SAT problem that runs in $O(1.1120^n)$ with very few\nbranching cases to consider, by using a result from Monien and Preis to give us\na bisection width for graphs with at most degree 3.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 09:10:20 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Hoi", "Gordon", ""], ["Jain", "Sanjay", ""], ["Stephan", "Frank", ""]]}, {"id": "2007.07554", "submitter": "Mojtaba Abdolmaleki", "authors": "Mojtaba Abdolmaleki, Yafeng Yin, Neda Masoud", "title": "Minimum Weight Pairwise Distance Preservers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the Minimum Weight Pairwise Distance Preservers\n(MWPDP) problem. Consider a positively weighted undirected/directed connected\ngraph $G = (V, E, c)$ and a subset $P$ of pairs of vertices, also called demand\npairs. A subgraph $G'$ is a distance preserver with respect to $P$ if and only\nif every pair $(u, w) \\in P$ satisfies $dist_{G'} (u, w) = dist_{G}(u, w)$. In\nMWPDP problem, we aim to find the minimum-weight subgraph $G^*$ that is a\ndistance preserver with respect to $P$. Taking a shortest path between each\npair in $P$ gives us a trivial solution with the weight of at most\n$U=\\sum_{(u,v) \\in P} dist_{G} (u, w)$. Subsequently, we ask how much\nimprovement we can make upon $U$. In other words, we opt to find a distance\npreserver $G^*$ that maximizes $U-c(G^*)$. Denote this problem as Cost Sharing\nPairwise Distance Preservers (CSPDP), which has several applications in the\nplanning and operations of transportation systems.\n  The only known work that can provide a nontrivial solution for CSPDP is that\nof Chlamt\\'a\\v{c} et al. (SODA, 2017). This algorithm works for unweighted\ngraphs and guarantees a non-zero objective only if the optimal solution is\nextremely sparse with respect to the trivial solution. We address this issue by\nproposing an $O(|E|^{1/2+\\epsilon})$-approximation algorithm for CSPDP in\nweighted graphs that runs in $O((|P||E|)^{2.38} (1/\\epsilon))$ time. Moreover,\nwe prove CSPDP is at least as hard as $\\text{LABEL-COVER}_{\\max}$. This implies\nthat CSPDP cannot be approximated within $O(|E|^{1/6-\\epsilon})$ factor in\npolynomial time, unless there is an improvement in the notoriously difficult\n$\\text{LABEL-COVER}_{\\max}$.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 09:11:31 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Abdolmaleki", "Mojtaba", ""], ["Yin", "Yafeng", ""], ["Masoud", "Neda", ""]]}, {"id": "2007.07575", "submitter": "Manuel C\\'aceres", "authors": "Manuel C\\'aceres, Massimo Cairo, Brendan Mumey, Romeo Rizzi and\n  Alexandru I. Tomescu", "title": "A linear-time parameterized algorithm for computing the width of a DAG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The width $k$ of a directed acyclic graph (DAG) $G = (V, E)$ equals the\nlargest number of pairwise non-reachable vertices. Computing the width dates\nback to Dilworth's and Fulkerson's results in the 1950s, and is doable in\nquadratic time in the worst case. Since $k$ can be small in practical\napplications, research has also studied algorithms whose complexity is\nparameterized on $k$. Despite these efforts, it is still open whether there\nexists a linear-time $O(f(k)(|V| + |E|))$ parameterized algorithm computing the\nwidth. We answer this question affirmatively by presenting an $O(k^24^k|V| +\nk2^k|E|)$ time algorithm, based on a new notion of frontier antichains. As we\nprocess the vertices in a topological order, all frontier antichains can be\nmaintained with the help of several combinatorial properties, paying only\n$f(k)$ along the way. The fact that the width can be computed by a single\n$f(k)$-sweep of the DAG is a new surprising insight into this classical\nproblem. Our algorithm also allows deciding whether the DAG has width at most\n$w$ in time $O(f(\\min(w,k))(|V|+|E|))$.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 09:51:23 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 06:46:26 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 14:18:51 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["C\u00e1ceres", "Manuel", ""], ["Cairo", "Massimo", ""], ["Mumey", "Brendan", ""], ["Rizzi", "Romeo", ""], ["Tomescu", "Alexandru I.", ""]]}, {"id": "2007.07660", "submitter": "Carla Lintzmayer", "authors": "Cristina G. Fernandes and Carla N. Lintzmayer", "title": "Leafy Spanning Arborescences in DAGs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Broadcasting in a computer network is a method of transferring a message to\nall recipients simultaneously. It is common in this situation to use a tree\nwith many leaves to perform the broadcast, as internal nodes have to forward\nthe messages received, while leaves are only receptors. We consider the\nsubjacent problem of, given a directed graph~$D$, finding a spanning\narborescence of D, if one exists, with the maximum number of leaves. In this\npaper, we concentrate on the class of rooted directed acyclic graphs, for which\nthe problem is known to be MaxSNP-hard. A 2-approximation was previously known\nfor this problem on this class of directed graphs. We improve on this result,\npresenting a (3/2)-approximation. We also adapt a result for the undirected\ncase and derive an inapproximability result for the vertex-weighted version of\nMaximum Leaf Spanning Arborescence on rooted directed acyclic graphs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 12:35:46 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Fernandes", "Cristina G.", ""], ["Lintzmayer", "Carla N.", ""]]}, {"id": "2007.07718", "submitter": "Nicola Prezza", "authors": "Nicola Cotumaccio and Nicola Prezza", "title": "On Indexing and Compressing Finite Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An index for a finite automaton is a powerful data structure that supports\nlocating paths labeled with a query pattern, thus solving pattern matching on\nthe underlying regular language. In this paper, we solve the long-standing\nproblem of indexing arbitrary finite automata. Our solution consists in finding\na partial co-lexicographic order of the states and proving, as in the total\norder case, that states reached by a given string form one interval on the\npartial order, thus enabling indexing. We provide a lower bound stating that\nsuch an interval requires $O(p)$ words to be represented, $p$ being the order's\nwidth (i.e. the size of its largest antichain). Indeed, we show that $p$\ndetermines the complexity of several fundamental problems on finite automata:\n(i) Letting $\\sigma$ be the alphabet size, we provide an encoding for NFAs\nusing $\\lceil\\log \\sigma\\rceil + 2\\lceil\\log p\\rceil + 2$ bits per transition\nand a smaller encoding for DFAs using $\\lceil\\log \\sigma\\rceil + \\lceil\\log\np\\rceil + 2$ bits per transition. This is achieved by generalizing the\nBurrows-Wheeler transform to arbitrary automata. (ii) We show that indexed\npattern matching can be solved in $\\tilde O(m\\cdot p^2)$ query time on NFAs.\n(iii) We provide a polynomial-time algorithm to index DFAs, while matching the\noptimal value for $ p $. On the other hand, we prove that the problem is\nNP-hard on NFAs. (iv) We show that, in the worst case, the classic powerset\nconstruction algorithm for NFA determinization generates an equivalent DFA of\nsize $2^p(n-p+1)-1$, where $n$ is the number of NFA's states.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 14:45:14 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Cotumaccio", "Nicola", ""], ["Prezza", "Nicola", ""]]}, {"id": "2007.07720", "submitter": "Nathaniel Lahn", "authors": "Nathaniel Lahn, Sharath Raghvendra", "title": "An $\\tilde{O}(n^{5/4})$ Time $\\varepsilon$-Approximation Algorithm for\n  RMS Matching in a Plane", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2-Wasserstein distance (or RMS distance) is a useful measure of\nsimilarity between probability distributions that has exciting applications in\nmachine learning. For discrete distributions, the problem of computing this\ndistance can be expressed in terms of finding a minimum-cost perfect matching\non a complete bipartite graph given by two multisets of points $A,B \\subset\n\\mathbb{R}^2$, with $|A|=|B|=n$, where the ground distance between any two\npoints is the squared Euclidean distance between them. Although there is a\nnear-linear time relative $\\varepsilon$-approximation algorithm for the case\nwhere the ground distance is Euclidean (Sharathkumar and Agarwal, JACM 2020),\nall existing relative $\\varepsilon$-approximation algorithms for the RMS\ndistance take $\\Omega(n^{3/2})$ time. This is primarily because, unlike\nEuclidean distance, squared Euclidean distance is not a metric. In this paper,\nfor the RMS distance, we present a new $\\varepsilon$-approximation algorithm\nthat runs in $O(n^{5/4}\\mathrm{poly}\\{\\log n,1/\\varepsilon\\})$ time.\n  Our algorithm is inspired by a recent approach for finding a minimum-cost\nperfect matching in bipartite planar graphs (Asathulla et al., TALG 2020).\nTheir algorithm depends heavily on the existence of sub-linear sized vertex\nseparators as well as shortest path data structures that require planarity.\nSurprisingly, we are able to design a similar algorithm for a complete\ngeometric graph that is far from planar and does not have any vertex\nseparators. Central components of our algorithm include a quadtree-based\ndistance that approximates the squared Euclidean distance and a data structure\nthat supports both Hungarian search and augmentation in sub-linear time.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 14:47:25 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Lahn", "Nathaniel", ""], ["Raghvendra", "Sharath", ""]]}, {"id": "2007.07721", "submitter": "Viswanath Nagarajan", "authors": "Viswanath Nagarajan and Lily Wang", "title": "Online Generalized Network Design Under (Dis)Economies of Scale", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a general online network design problem where a sequence of N\nrequests arrive over time, each of which needs to use some subset of the\navailable resources E. The cost incurred by any resource e is some function\n$f_e$ of the total load $L_e$ on that resource. The objective is to minimize\nthe total cost $\\sum_{e\\in E} f_e(L_e)$. We focus on cost functions that\nexhibit (dis)economies of scale, that are of the form $f_e(x) = \\sigma_e +\n\\xi_e\\cdot x^{\\alpha_e}$ if $x>0$ (and zero if $x=0$), where the exponent\n$\\alpha_e\\ge 1$. Optimization problems under these functions have received\nsignificant recent attention due to applications in energy-efficient computing.\nOur main result is a deterministic online algorithm with tight competitive\nratio $\\Theta\\left(\\max_{e\\in E}\n\\left(\\frac{\\sigma_e}{\\xi_e}\\right)^{1/\\alpha_e}\\right)$ when $\\alpha_e$ is\nconstant for all $e\\in E$. This framework is applicable to a variety of network\ndesign problems in undirected and directed graphs, including multicommodity\nrouting, Steiner tree/forest connectivity and set-connectivity. In fact, our\nonline competitive ratio even matches the previous-best (offline) approximation\nratio for generalized network design.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 14:49:17 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Nagarajan", "Viswanath", ""], ["Wang", "Lily", ""]]}, {"id": "2007.07738", "submitter": "Ignasi Sau", "authors": "Victor Campos, Raul Lopes, Ana Karolinna Maia, Ignasi Sau", "title": "Adapting the Directed Grid Theorem into an FPT Algorithm", "comments": "31 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Grid Theorem of Robertson and Seymour [JCTB, 1986], is one of the most\nimportant tools in the field of structural graph theory, finding numerous\napplications in the design of algorithms for undirected graphs. An analogous\nversion of the Grid Theorem in digraphs was conjectured by Johnson et al.\n[JCTB, 2001], and proved by Kawarabayashi and Kreutzer [STOC, 2015]. Namely,\nthey showed that there is a function $f(k)$ such that every digraph of directed\ntree-width at least $f(k)$ contains a cylindrical grid of size $k$ as a\nbutterfly minor and stated that their proof can be turned into an XP algorithm,\nwith parameter $k$, that either constructs a decomposition of the appropriate\nwidth, or finds the claimed large cylindrical grid as a butterfly minor. In\nthis paper, we adapt some of the steps of the proof of Kawarabayashi and\nKreutzer to improve this XP algorithm into an FPT algorithm. Towards this, our\nmain technical contributions are two FPT algorithms with parameter $k$. The\nfirst one either produces an arboreal decomposition of width $3k-2$ or finds a\nhaven of order $k$ in a digraph $D$, improving on the original result for\narboreal decompositions by Johnson et al. The second algorithm finds a\nwell-linked set of order $k$ in a digraph $D$ of large directed tree-width. As\ntools to prove these results, we show how to solve a generalized version of the\nproblem of finding balanced separators for a given set of vertices $T$ in FPT\ntime with parameter $|T|$, a result that we consider to be of its own interest.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 15:10:59 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Campos", "Victor", ""], ["Lopes", "Raul", ""], ["Maia", "Ana Karolinna", ""], ["Sau", "Ignasi", ""]]}, {"id": "2007.07802", "submitter": "Vincent Pilaud", "authors": "Vincent Pilaud, Viviane Pons, Daniel Tamayo Jim\\'enez", "title": "Permutree sorting", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalizing stack sorting and $c$-sorting for permutations, we define the\npermutree sorting algorithm. Given two disjoint subsets $U$ and $D$ of $\\{2,\n\\dots, n-1\\}$, the $(U,D)$-permutree sorting tries to sort the permutation $\\pi\n\\in \\mathfrak{S}_n$ and fails if and only if there are $1 \\le i < j < k \\le n$\nsuch that $\\pi$ contains the subword $jki$ if $j \\in U$ and $kij$ if $j \\in D$.\nThis algorithm is seen as a way to explore an automaton which either rejects\nall reduced expressions of $\\pi$, or accepts those reduced expressions for\n$\\pi$ whose prefixes are all $(U,D)$-permutree sortable.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 16:23:31 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Pilaud", "Vincent", ""], ["Pons", "Viviane", ""], ["Jim\u00e9nez", "Daniel Tamayo", ""]]}, {"id": "2007.07862", "submitter": "Yang P. Liu", "authors": "Parinya Chalermsook, Syamantak Das, Bundit Laekhanukit, Yunbum Kook,\n  Yang P. Liu, Richard Peng, Mark Sellke, Daniel Vaz", "title": "Vertex Sparsification for Edge Connectivity", "comments": "Merged version of arXiv:1910.10359 and arXiv:1910.10665 with improved\n  bounds, 55 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph compression or sparsification is a basic information-theoretic and\ncomputational question. A major open problem in this research area is whether\n$(1+\\epsilon)$-approximate cut-preserving vertex sparsifiers with size close to\nthe number of terminals exist. As a step towards this goal, we study a\nthresholded version of the problem: for a given parameter $c$, find a smaller\ngraph, which we call connectivity-$c$ mimicking network, which preserves\nconnectivity among $k$ terminals exactly up to the value of $c$. We show that\nconnectivity-$c$ mimicking networks with $O(kc^4)$ edges exist and can be found\nin time $m(c\\log n)^{O(c)}$. We also give a separate algorithm that constructs\nsuch graphs with $k \\cdot O(c)^{2c}$ edges in time $mc^{O(c)}\\log^{O(1)}n$.\nThese results lead to the first data structures for answering fully dynamic\noffline $c$-edge-connectivity queries for $c \\ge 4$ in polylogarithmic time per\nquery, as well as more efficient algorithms for survivable network design on\nbounded treewidth graphs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 17:31:44 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Chalermsook", "Parinya", ""], ["Das", "Syamantak", ""], ["Laekhanukit", "Bundit", ""], ["Kook", "Yunbum", ""], ["Liu", "Yang P.", ""], ["Peng", "Richard", ""], ["Sellke", "Mark", ""], ["Vaz", "Daniel", ""]]}, {"id": "2007.07921", "submitter": "Ashwin Ganesan", "authors": "Ashwin Ganesan", "title": "Performance analysis of a distributed algorithm for admission control in\n  wireless networks under the $2$-hop interference model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general open problem in networking is: what are the fundamental limits to\nthe performance that is achievable with some given amount of resources? More\nspecifically, if each node in the network has information about only its\n$1$-hop neighborhood, then what are the limits to performance? This problem is\nconsidered for wireless networks where each communication link has a minimum\nbandwidth quality-of-service (QoS) requirement. Links in the same vicinity\ncontend for the shared wireless medium. The conflict graph captures which pairs\nof links interfere with each other and depends on the MAC protocol. In IEEE\n802.11 MAC protocol-based networks, when communication between nodes $i$ and\n$j$ takes place, the neighbors of both $i$ and $j$ remain silent. This model of\ninterference is called the $2$-hop interference model because the distance in\nthe network graph between any two links that can be simultaneously active is at\nleast $2$. In the admission control problem, the objective is to determine,\nusing only localized information, whether a given set of flow rates is\nfeasible.\n  In the present work, a distributed algorithm is proposed for this problem,\nwhere each node has information only about its $1$-hop neighborhood. The\nworst-case performance of the distributed algorithm, i.e. the largest factor by\nwhich the performance of this distributed algorithm is away from that of an\noptimal, centralized algorithm, is analyzed. Lower and upper bounds on the\nsuboptimality of the distributed algorithm are obtained, and both bounds are\nshown to be tight. The exact worst-case performance is obtained for some ring\ntopologies. While distance-$d$ distributed algorithms have been analyzed for\nthe $1$-hop interference model, an open problem in the literature is to extend\nthese results to the $K$-hop interference model, and the present work initiates\nthe generalization to the $K$-hop interference model.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 09:07:06 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Ganesan", "Ashwin", ""]]}, {"id": "2007.07975", "submitter": "L\\'aszl\\'o V\\'egh", "authors": "James B. Orlin and L\\'aszl\\'o A. V\\'egh", "title": "Directed Shortest Paths via Approximate Cost Balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an $O(nm)$ algorithm for all-pairs shortest paths computations in\na directed graph with $n$ nodes, $m$ arcs, and nonnegative integer arc costs.\nThis matches the complexity bound attained by Thorup (1999) for the all-pairs\nproblems in undirected graphs. Our main insight is that shortest paths problems\nwith approximately balanced directed cost functions can be solved similarly to\nthe undirected case. Our algorithm starts with an $O(m\\sqrt{n}\\log n)$\npreprocessing step that finds a 3-min-balanced reduced cost function. Using\nthese reduced costs, every shortest path query can be solved in $O(m)$ time\nusing an adaptation of Thorup's component hierarchy method. The balancing\nresult is of independent interest, and gives the best currently known\napproximate balancing algorithm for the problem.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 20:11:29 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Orlin", "James B.", ""], ["V\u00e9gh", "L\u00e1szl\u00f3 A.", ""]]}, {"id": "2007.07990", "submitter": "Thodoris Lykouris", "authors": "Shuchi Chawla, Nikhil Devanur, Thodoris Lykouris", "title": "Static pricing for multi-unit prophet inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a pricing problem where a seller has $k$ identical copies of a\nproduct, buyers arrive sequentially, and the seller prices the items aiming to\nmaximize social welfare. When $k=1$, this is the so called \\emph{prophet\ninequality} problem for which there is a simple pricing scheme achieving a\ncompetitive ratio of $1/2$. On the other end of the spectrum, as $k$ goes to\ninfinity, the asymptotic performance of both static and adaptive pricing is\nwell understood.\n  We provide a static pricing scheme for the small-supply regime: where $k$ is\nsmall but larger than $1$. Prior to our work, the best competitive ratio known\nfor this setting was the $1/2$ that follows from the single-unit prophet\ninequality. Our pricing scheme is easy to describe as well as practical -- it\nis anonymous, non-adaptive, and order-oblivious. We pick a single price that\nequalizes the expected fraction of items sold and the probability that the\nsupply does not sell out before all customers are served; this price is then\noffered to each customer while supply lasts. This pricing scheme achieves a\ncompetitive ratio that increases gradually with the supply and approaches to\n$1$ at the optimal rate. Astonishingly, for $k<20$, it even outperforms the\nstate-of-the-art adaptive pricing for the small-$k$ regime.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 20:57:29 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Chawla", "Shuchi", ""], ["Devanur", "Nikhil", ""], ["Lykouris", "Thodoris", ""]]}, {"id": "2007.07994", "submitter": "Kyle Fox", "authors": "Connor Colombe and Kyle Fox", "title": "Approximating the (Continuous) Fr\\'echet Distance", "comments": "To appear in SoCG 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the first strongly subquadratic time algorithm with\nsubexponential approximation ratio for approximately computing the Fr\\'echet\ndistance between two polygonal chains. Specifically, let $P$ and $Q$ be two\npolygonal chains with $n$ vertices in $d$-dimensional Euclidean space, and let\n$\\alpha \\in [\\sqrt{n}, n]$. Our algorithm deterministically finds an\n$O(\\alpha)$-approximate Fr\\'echet correspondence in time $O((n^3 / \\alpha^2)\n\\log n)$. In particular, we get an $O(n)$-approximation in near-linear $O(n\n\\log n)$ time, a vast improvement over the previously best know result, a\nlinear time $2^{O(n)}$-approximation. As part of our algorithm, we also\ndescribe how to turn any approximate decision procedure for the Fr\\'echet\ndistance into an approximate optimization algorithm whose approximation ratio\nis the same up to arbitrarily small constant factors. The transformation into\nan approximate optimization algorithm increases the running time of the\ndecision procedure by only an $O(\\log n)$ factor.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 21:12:06 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 03:24:13 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Colombe", "Connor", ""], ["Fox", "Kyle", ""]]}, {"id": "2007.08031", "submitter": "Wai Ming Tai", "authors": "Wai Ming Tai", "title": "Optimal Coreset for Gaussian Kernel Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a point set $P\\subset \\mathbb{R}^d$, a kernel density estimation for\nGaussian kernel is defined as $\\overline{\\mathcal{G}}_P(x) =\n\\frac{1}{\\left|P\\right|}\\sum_{p\\in P}e^{-\\left\\lVert x-p \\right\\rVert^2}$ for\nany $x\\in\\mathbb{R}^d$. We study how to construct a small subset $Q$ of $P$\nsuch that the kernel density estimation of $P$ can be approximated by the\nkernel density estimation of $Q$. This subset $Q$ is called coreset. The\nprimary technique in this work is to construct $\\pm 1$ coloring on the point\nset $P$ by the discrepancy theory and apply this coloring algorithm\nrecursively. Our result leverages Banaszczyk's Theorem. When $d>1$ is constant,\nour construction gives a coreset of size $O\\left(\\frac{1}{\\varepsilon}\\right)$\nas opposed to the best-known result of\n$O\\left(\\frac{1}{\\varepsilon}\\sqrt{\\log\\frac{1}{\\varepsilon}}\\right)$. It is\nthe first to give a breakthrough on the barrier of $\\sqrt{\\log}$ factor even\nwhen $d=2$.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 22:58:50 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 20:37:54 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Tai", "Wai Ming", ""]]}, {"id": "2007.08051", "submitter": "Dingyu Wang", "authors": "Seth Pettie and Dingyu Wang", "title": "Information Theoretic Limits of Cardinality Estimation: Fisher Meets\n  Shannon", "comments": "50 pages, 7 figures, submitted to STOC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the cardinality (number of distinct elements) of a large multiset\nis a classic problem in streaming and sketching, dating back to Flajolet and\nMartin's classic Probabilistic Counting (PCSA) algorithm from 1983. In this\npaper we study the intrinsic tradeoff between the space complexity of the\nsketch and its estimation error in the random oracle model. We define a new\nmeasure of efficiency for cardinality estimators called the Fisher-Shannon\n(Fish) number $\\mathcal{H}/\\mathcal{I}$. It captures the tension between the\nlimiting Shannon entropy ($\\mathcal{H}$) of the sketch and its normalized\nFisher information ($\\mathcal{I}$), which characterizes the variance of a\nstatistically efficient, asymptotically unbiased estimator.\n  Our results are as follows.\n  We prove that all base-$q$ variants of Flajolet and Martin's PCSA sketch have\nFish-number $H_0/I_0 \\approx 1.98016$ and that every base-$q$ variant of\n(Hyper)LogLog has Fish-number worse than $H_0/I_0$, but that they tend to\n$H_0/I_0$ in the limit as $q\\rightarrow \\infty$. Here $H_0,I_0$ are precisely\ndefined constants.\n  We describe a sketch called Fishmonger that is based on a smoothed,\nentropy-compressed variant of PCSA with a different estimator function. It is\nproved that with high probability, Fishmonger processes a multiset of $[U]$\nsuch that at all times, its space is $O(\\log^2\\log U) + (1+o(1))(H_0/I_0)b\n\\approx 1.98b$ bits and its standard error is $1/\\sqrt{b}$.\n  We give circumstantial evidence that $H_0/I_0$ is the optimum Fish-number of\nmergeable sketches for Cardinality Estimation. We define a class of\nlinearizable sketches and prove that no member of this class can beat\n$H_0/I_0$. The popular mergeable sketches are, in fact, also linearizable.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 00:43:40 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 03:17:32 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Pettie", "Seth", ""], ["Wang", "Dingyu", ""]]}, {"id": "2007.08058", "submitter": "Zongchen Chen", "authors": "Zongchen Chen, Andreas Galanis, Daniel \\v{S}tefankovi\\v{c}, Eric\n  Vigoda", "title": "Rapid Mixing for Colorings via Spectral Independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math-ph math.MP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectral independence approach of Anari et al. (2020) utilized recent\nresults on high-dimensional expanders of Alev and Lau (2020) and established\nrapid mixing of the Glauber dynamics for the hard-core model defined on\nweighted independent sets. We develop the spectral independence approach for\ncolorings, and obtain new algorithmic results for the corresponding\ncounting/sampling problems.\n  Let $\\alpha^*\\approx 1.763$ denote the solution to $\\exp(1/x)=x$ and let\n$\\alpha>\\alpha^*$. We prove that, for any triangle-free graph $G=(V,E)$ with\nmaximum degree $\\Delta$, for all $q\\geq\\alpha\\Delta+1$, the mixing time of the\nGlauber dynamics for $q$-colorings is polynomial in $n=|V|$, with the exponent\nof the polynomial independent of $\\Delta$ and $q$. In comparison, previous\napproximate counting results for colorings held for a similar range of $q$\n(asymptotically in $\\Delta$) but with larger girth requirement or with a\nrunning time where the polynomial exponent depended on $\\Delta$ and $q$\n(exponentially). One further feature of using the spectral independence\napproach to study colorings is that it avoids many of the technical\ncomplications in previous approaches caused by coupling arguments or by passing\nto the complex plane; the key improvement on the running time is based on\nrelatively simple combinatorial arguments which are then translated into\nspectral bounds.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 01:16:11 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Chen", "Zongchen", ""], ["Galanis", "Andreas", ""], ["\u0160tefankovi\u010d", "Daniel", ""], ["Vigoda", "Eric", ""]]}, {"id": "2007.08068", "submitter": "Zongchen Chen", "authors": "Antonio Blanca, Zongchen Chen, Daniel \\v{S}tefankovi\\v{c}, Eric Vigoda", "title": "The Swendsen-Wang Dynamics on Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DM cs.DS math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Swendsen-Wang algorithm is a sophisticated, widely-used Markov chain for\nsampling from the Gibbs distribution for the ferromagnetic Ising and Potts\nmodels. This chain has proved difficult to analyze, due in part to the global\nnature of its updates. We present optimal bounds on the convergence rate of the\nSwendsen-Wang algorithm for the complete $d$-ary tree. Our bounds extend to the\nnon-uniqueness region and apply to all boundary conditions.\n  We show that the spatial mixing conditions known as Variance Mixing and\nEntropy Mixing, introduced in the study of local Markov chains by Martinelli et\nal. (2003), imply $\\Omega(1)$ spectral gap and $O(\\log{n})$ mixing time,\nrespectively, for the Swendsen-Wang dynamics on the $d$-ary tree. We also show\nthat these bounds are asymptotically optimal. As a consequence, we establish\n$\\Theta(\\log{n})$ mixing for the Swendsen-Wang dynamics for all boundary\nconditions throughout the tree uniqueness region; in fact, our bounds hold\nbeyond the uniqueness threshold for the Ising model, and for the $q$-state\nPotts model when $q$ is small with respect to $d$. Our proofs feature a novel\nspectral view of the Variance Mixing condition inspired by several recent rapid\nmixing results on high-dimensional expanders and utilize recent work on block\nfactorization of entropy under spatial mixing conditions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 01:42:56 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 04:18:05 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Blanca", "Antonio", ""], ["Chen", "Zongchen", ""], ["\u0160tefankovi\u010d", "Daniel", ""], ["Vigoda", "Eric", ""]]}, {"id": "2007.08069", "submitter": "Bhaskar DasGupta", "authors": "Abolfazl Asudeh and Tanya Berger-Wolf and Bhaskar DasGupta and\n  Anastasios Sidiropoulos", "title": "Maximizing coverage while ensuring fairness: a tale of conflicting\n  objective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ensuring fairness in computational problems has emerged as a $key$ topic\nduring recent years, buoyed by considerations for equitable resource\ndistributions and social justice. It $is$ possible to incorporate fairness in\ncomputational problems from several perspectives, such as using optimization,\ngame-theoretic or machine learning frameworks. In this paper we address the\nproblem of incorporation of fairness from a $combinatorial$ $optimization$\nperspective. We formulate a combinatorial optimization framework, suitable for\nanalysis by researchers in approximation algorithms and related areas, that\nincorporates fairness in maximum coverage problems as an interplay between\n$two$ conflicting objectives. Fairness is imposed in coverage by using coloring\nconstraints that $minimizes$ the discrepancies between number of elements of\ndifferent colors covered by selected sets; this is in contrast to the usual\ndiscrepancy minimization problems studied extensively in the literature where\n(usually two) colors are $not$ given $a$ $priori$ but need to be selected to\nminimize the maximum color discrepancy of $each$ individual set. Our main\nresults are a set of randomized and deterministic approximation algorithms that\nattempts to $simultaneously$ approximate both fairness and coverage in this\nframework.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 01:45:02 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 19:21:22 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Asudeh", "Abolfazl", ""], ["Berger-Wolf", "Tanya", ""], ["DasGupta", "Bhaskar", ""], ["Sidiropoulos", "Anastasios", ""]]}, {"id": "2007.08075", "submitter": "Nate Veldt", "authors": "Austin R. Benson, Jon Kleinberg, Nate Veldt", "title": "Augmented Sparsifiers for Generalized Hypergraph Cuts with Applications\n  to Decomposable Submodular Function Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, hypergraph generalizations of many graph cut problems have\nbeen introduced and analyzed as a way to better explore and understand complex\nsystems and datasets characterized by multiway relationships. Recent work has\nmade use of a generalized hypergraph cut function which for a hypergraph\n$\\mathcal{H} = (V,E)$ can be defined by associating each hyperedge $e \\in E$\nwith a splitting function ${\\bf w}_e$, which assigns a penalty to each way of\nseparating the nodes of $e$. When each ${\\bf w}_e$ is a submodular\ncardinality-based splitting function, meaning that ${\\bf w}_e(S) = g(|S|)$ for\nsome concave function $g$, previous work has shown that a generalized\nhypergraph cut problem can be reduced to a directed graph cut problem on an\naugmented node set. However, existing reduction procedures often result in a\ndense graph, even when the hypergraph is sparse, which leads to slow runtimes\nfor algorithms that run on the reduced graph.\n  We introduce a new framework of sparsifying hypergraph-to-graph reductions,\nwhere a hypergraph cut defined by submodular cardinality-based splitting\nfunctions is $(1+\\varepsilon)$-approximated by a cut on a directed graph. Our\ntechniques are based on approximating concave functions using piecewise linear\ncurves. For $\\varepsilon > 0$ we need at most $O(\\varepsilon^{-1}|e| \\log |e|)$\nedges to reduce any hyperedge $e$, which leads to faster runtimes for\napproximating generalized hypergraph $s$-$t$ cut problems. For the machine\nlearning heuristic of a clique splitting function, our approach requires only\n$O(|e| \\varepsilon^{-1/2} \\log \\log \\frac{1}{\\varepsilon})$ edges. This\nsparsification leads to faster approximate min $s$-$t$ graph cut algorithms for\ncertain classes of co-occurrence graphs. Finally, we apply our sparsification\ntechniques to develop approximation algorithms for minimizing sums of\ncardinality-based submodular functions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 02:01:29 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 13:10:21 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Benson", "Austin R.", ""], ["Kleinberg", "Jon", ""], ["Veldt", "Nate", ""]]}, {"id": "2007.08091", "submitter": "Weiming Feng", "authors": "Weiming Feng, Heng Guo, Yitong Yin, Chihao Zhang", "title": "Rapid mixing from spectral independence beyond the Boolean domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the notion of spectral independence (introduced by Anari, Liu, and\nOveis Gharan [ALO20]) from the Boolean domain to general discrete domains. This\nproperty characterises distributions with limited correlations, and implies\nthat the corresponding Glauber dynamics is rapidly mixing.\n  As a concrete application, we show that Glauber dynamics for sampling proper\n$q$-colourings mixes in polynomial-time for the family of triangle-free graphs\nwith maximum degree $\\Delta$ provided $q\\ge (\\alpha^*+\\delta)\\Delta$ where\n$\\alpha^*\\approx 1.763$ is the unique solution to $\\alpha^*=\\exp(1/\\alpha^*)$\nand $\\delta>0$ is any constant. This is the first efficient algorithm for\nsampling proper $q$-colourings in this regime with possibly unbounded $\\Delta$.\nOur main tool of establishing spectral independence is the recursive coupling\nby Goldberg, Martin, and Paterson [GMP05].\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 03:29:11 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Feng", "Weiming", ""], ["Guo", "Heng", ""], ["Yin", "Yitong", ""], ["Zhang", "Chihao", ""]]}, {"id": "2007.08101", "submitter": "Spencer Gordon", "authors": "Spencer Gordon, Bijan Mazaheri, Leonard J. Schulman, Yuval Rabani", "title": "The Sparse Hausdorff Moment Problem, with Application to Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of identifying, from its first $m$ noisy moments, a\nprobability distribution on $[0,1]$ of support $k<\\infty$. This is equivalent\nto the problem of learning a distribution on $m$ observable binary random\nvariables $X_1,X_2,\\dots,X_m$ that are iid conditional on a hidden random\nvariable $U$ taking values in $\\{1,2,\\dots,k\\}$. Our focus is on accomplishing\nthis with $m=2k$, which is the minimum $m$ for which verifying that the source\nis a $k$-mixture is possible (even with exact statistics). This problem, so\nsimply stated, is quite useful: e.g., by a known reduction, any algorithm for\nit lifts to an algorithm for learning pure topic models.\n  We give an algorithm for identifying a $k$-mixture using samples of $m=2k$\niid binary random variables using a sample of size $\\left(1/w_{\\min}\\right)^2\n\\cdot\\left(1/\\zeta\\right)^{O(k)}$ and post-sampling runtime of only\n$O(k^{2+o(1)})$ arithmetic operations. Here $w_{\\min}$ is the minimum\nprobability of an outcome of $U$, and $\\zeta$ is the minimum separation between\nthe distinct success probabilities of the $X_i$s. Stated in terms of the moment\nproblem, it suffices to know the moments to additive accuracy\n$w_{\\min}\\cdot\\zeta^{O(k)}$. It is known that the sample complexity of any\nsolution to the identification problem must be at least exponential in $k$.\nPrevious results demonstrated either worse sample complexity and worse $O(k^c)$\nruntime for some $c$ substantially larger than $2$, or similar sample\ncomplexity and much worse $k^{O(k^2)}$ runtime.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 04:23:57 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 20:42:46 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 17:24:41 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Gordon", "Spencer", ""], ["Mazaheri", "Bijan", ""], ["Schulman", "Leonard J.", ""], ["Rabani", "Yuval", ""]]}, {"id": "2007.08110", "submitter": "Yue Gao", "authors": "Yue Gao, Or Sheffet", "title": "Private Approximations of a Convex Hull in Low Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first differentially private algorithms that estimate a variety\nof geometric features of points in the Euclidean space, such as diameter,\nwidth, volume of convex hull, min-bounding box, min-enclosing ball etc. Our\nwork relies heavily on the notion of \\emph{Tukey-depth}. Instead of\n(non-privately) approximating the convex-hull of the given set of points $P$,\nour algorithms approximate the geometric features of the $\\kappa$-Tukey region\ninduced by $P$ (all points of Tukey-depth $\\kappa$ or greater). Moreover, our\napproximations are all bi-criteria: for any geometric feature $\\mu$ our\n$(\\alpha,\\Delta)$-approximation is a value \"sandwiched\" between\n$(1-\\alpha)\\mu(D_P(\\kappa))$ and $(1+\\alpha)\\mu(D_P(\\kappa-\\Delta))$.\n  Our work is aimed at producing a \\emph{$(\\alpha,\\Delta)$-kernel of\n$D_P(\\kappa)$}, namely a set $\\mathcal{S}$ such that (after a shift) it holds\nthat $(1-\\alpha)D_P(\\kappa)\\subset \\mathsf{CH}(\\mathcal{S}) \\subset\n(1+\\alpha)D_P(\\kappa-\\Delta)$. We show that an analogous notion of a bi-critera\napproximation of a directional kernel, as originally proposed by Agarwal et\nal~[2004], \\emph{fails} to give a kernel, and so we result to subtler notions\nof approximations of projections that do yield a kernel. First, we give\ndifferentially private algorithms that find $(\\alpha,\\Delta)$-kernels for a\n\"fat\" Tukey-region. Then, based on a private approximation of the min-bounding\nbox, we find a transformation that does turn $D_P(\\kappa)$ into a \"fat\" region\n\\emph{but only if} its volume is proportional to the volume of\n$D_P(\\kappa-\\Delta)$. Lastly, we give a novel private algorithm that finds a\ndepth parameter $\\kappa$ for which the volume of $D_P(\\kappa)$ is comparable to\n$D_P(\\kappa-\\Delta)$. We hope this work leads to the further study of the\nintersection of differential privacy and computational geometry.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 04:49:50 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 22:13:59 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Gao", "Yue", ""], ["Sheffet", "Or", ""]]}, {"id": "2007.08123", "submitter": "David Eppstein", "authors": "David Eppstein", "title": "Dynamic Products of Ranks", "comments": "7 pages, 4 figures. To appear at the 32nd Canadian Conference on\n  Computational Geometry (CCCG 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a data structure that can maintain a dynamic set of points given\nby their Cartesian coordinates, and maintain the point whose product of ranks\nwithin the two coordinate orderings is minimum or maximum, in time\n$O(\\sqrt{n\\log n})$ per update.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 05:46:42 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Eppstein", "David", ""]]}, {"id": "2007.08137", "submitter": "Yeshwanth Cherapanamjeri", "authors": "Yeshwanth Cherapanamjeri, Efe Aras, Nilesh Tripuraneni, Michael I.\n  Jordan, Nicolas Flammarion, Peter L. Bartlett", "title": "Optimal Robust Linear Regression in Nearly Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of high-dimensional robust linear regression where a\nlearner is given access to $n$ samples from the generative model $Y = \\langle\nX,w^* \\rangle + \\epsilon$ (with $X \\in \\mathbb{R}^d$ and $\\epsilon$\nindependent), in which an $\\eta$ fraction of the samples have been\nadversarially corrupted. We propose estimators for this problem under two\nsettings: (i) $X$ is L4-L2 hypercontractive, $\\mathbb{E} [XX^\\top]$ has bounded\ncondition number and $\\epsilon$ has bounded variance and (ii) $X$ is\nsub-Gaussian with identity second moment and $\\epsilon$ is sub-Gaussian. In\nboth settings, our estimators: (a) Achieve optimal sample complexities and\nrecovery guarantees up to log factors and (b) Run in near linear time\n($\\tilde{O}(nd / \\eta^6)$). Prior to our work, polynomial time algorithms\nachieving near optimal sample complexities were only known in the setting where\n$X$ is Gaussian with identity covariance and $\\epsilon$ is Gaussian, and no\nlinear time estimators were known for robust linear regression in any setting.\nOur estimators and their analysis leverage recent developments in the\nconstruction of faster algorithms for robust mean estimation to improve\nruntimes, and refined concentration of measure arguments alongside Gaussian\nrounding techniques to improve statistical sample complexities.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 06:44:44 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Cherapanamjeri", "Yeshwanth", ""], ["Aras", "Efe", ""], ["Tripuraneni", "Nilesh", ""], ["Jordan", "Michael I.", ""], ["Flammarion", "Nicolas", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "2007.08179", "submitter": "Solon Pissis", "authors": "Takuya Mieno and Solon P. Pissis and Leen Stougie and Michelle\n  Sweering", "title": "String Sanitization Under Edit Distance: Improved and Generalized", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $W$ be a string of length $n$ over an alphabet $\\Sigma$, $k$ be a\npositive integer, and $\\mathcal{S}$ be a set of length-$k$ substrings of $W$.\nThe ETFS problem asks us to construct a string $X_{\\mathrm{ED}}$ such that: (i)\nno string of $\\mathcal{S}$ occurs in $X_{\\mathrm{ED}}$; (ii) the order of all\nother length-$k$ substrings over $\\Sigma$ is the same in $W$ and in\n$X_{\\mathrm{ED}}$; and (iii) $X_{\\mathrm{ED}}$ has minimal edit distance to\n$W$. When $W$ represents an individual's data and $\\mathcal{S}$ represents a\nset of confidential patterns, the ETFS problem asks for transforming $W$ to\npreserve its privacy and its utility [Bernardini et al., ECML PKDD 2019].\n  ETFS can be solved in $\\mathcal{O}(n^2k)$ time [Bernardini et al., CPM 2020].\nThe same paper shows that ETFS cannot be solved in $\\mathcal{O}(n^{2-\\delta})$\ntime, for any $\\delta>0$, unless the Strong Exponential Time Hypothesis (SETH)\nis false. Our main results can be summarized as follows: (i) an\n$\\mathcal{O}(n^2\\log^2k)$-time algorithm to solve ETFS; and (ii) an\n$\\mathcal{O}(n^2\\log^2n)$-time algorithm to solve AETFS, a generalization of\nETFS in which the elements of $\\mathcal{S}$ can have arbitrary lengths. Our\nalgorithms are thus optimal up to polylogarithmic factors, unless SETH fails.\nLet us also stress that our algorithms work under edit distance with arbitrary\nweights at no extra cost. As a bonus, we show how to modify some known\ntechniques, which speed up the standard edit distance computation, to be\napplied to our problems. Beyond string sanitization, our techniques may inspire\nsolutions to other problems related to regular expressions or context-free\ngrammars.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 08:38:14 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Mieno", "Takuya", ""], ["Pissis", "Solon P.", ""], ["Stougie", "Leen", ""], ["Sweering", "Michelle", ""]]}, {"id": "2007.08204", "submitter": "Karol W\\k{e}grzycki", "authors": "Jesper Nederlof, Jakub Pawlewicz, C\\'eline M. F. Swennenhuis, Karol\n  W\\k{e}grzycki", "title": "A Faster Exponential Time Algorithm for Bin Packing With a Constant\n  Number of Bins via Additive Combinatorics", "comments": "Presented at SODA 2021; 42 pages; 4 figures", "journal-ref": null, "doi": "10.1137/1.9781611976465.102", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Bin Packing problem one is given $n$ items with weights\n$w_1,\\ldots,w_n$ and $m$ bins with capacities $c_1,\\ldots,c_m$. The goal is to\nfind a partition of the items into sets $S_1,\\ldots,S_m$ such that $w(S_j) \\leq\nc_j$ for every bin $j$, where $w(X)$ denotes $\\sum_{i \\in X}w_i$.\n  Bj\\\"orklund, Husfeldt and Koivisto (SICOMP 2009) presented an\n$\\mathcal{O}^\\star(2^n)$ time algorithm for Bin Packing. In this paper, we show\nthat for every $m \\in \\mathbf{N}$ there exists a constant $\\sigma_m >0$ such\nthat an instance of Bin Packing with $m$ bins can be solved in\n$\\mathcal{O}(2^{(1-\\sigma_m)n})$ randomized time. Before our work, such\nimproved algorithms were not known even for $m$ equals $4$.\n  A key step in our approach is the following new result in Littlewood-Offord\ntheory on the additive combinatorics of subset sums: For every $\\delta >0$\nthere exists an $\\varepsilon >0$ such that if $|\\{ X\\subseteq \\{1,\\ldots,n \\} :\nw(X)=v \\}| \\geq 2^{(1-\\varepsilon)n}$ for some $v$ then $|\\{ w(X): X \\subseteq\n\\{1,\\ldots,n\\} \\}|\\leq 2^{\\delta n}$.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 09:30:09 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 08:53:22 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 08:55:12 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Nederlof", "Jesper", ""], ["Pawlewicz", "Jakub", ""], ["Swennenhuis", "C\u00e9line M. F.", ""], ["W\u0119grzycki", "Karol", ""]]}, {"id": "2007.08253", "submitter": "V\\'aclav Rozho\\v{n}", "authors": "Mohsen Ghaffari, Christoph Grunau, V\\'aclav Rozho\\v{n}", "title": "Improved Deterministic Network Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network decomposition is a central tool in distributed graph algorithms. We\npresent two improvements on the state of the art for network decomposition,\nwhich thus lead to improvements in the (deterministic and randomized)\ncomplexity of several well-studied graph problems.\n  - We provide a deterministic distributed network decomposition algorithm with\n$O(\\log^5 n)$ round complexity, using $O(\\log n)$-bit messages. This improves\non the $O(\\log^7 n)$-round algorithm of Rozho\\v{n} and Ghaffari [STOC'20],\nwhich used large messages, and their $O(\\log^8 n)$-round algorithm with $O(\\log\nn)$-bit messages. This directly leads to similar improvements for a wide range\nof deterministic and randomized distributed algorithms, whose solution relies\non network decomposition, including the general distributed derandomization of\nGhaffari, Kuhn, and Harris [FOCS'18].\n  - One drawback of the algorithm of Rozho\\v{n} and Ghaffari, in the\n$\\mathsf{CONGEST}$ model, was its dependence on the length of the identifiers.\nBecause of this, for instance, the algorithm could not be used in the\nshattering framework in the $\\mathsf{CONGEST}$ model. Thus, the state of the\nart randomized complexity of several problems in this model remained with an\nadditive $2^{O(\\sqrt{\\log\\log n})}$ term, which was a clear leftover of the\nolder network decomposition complexity [Panconesi and Srinivasan STOC'92]. We\npresent a modified version that remedies this, constructing a decomposition\nwhose quality does not depend on the identifiers, and thus improves the\nrandomized round complexity for various problems.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 10:57:29 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Grunau", "Christoph", ""], ["Rozho\u0148", "V\u00e1clav", ""]]}, {"id": "2007.08285", "submitter": "Troy Lee", "authors": "Troy Lee and Miklos Santha and Shengyu Zhang", "title": "Quantum algorithms for graph problems with cut queries", "comments": "Corrected an error in Lemma 1. This led to an extra log factor in the\n  complexity of the connectivity and spanning forest algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $G$ be an $n$-vertex graph with $m$ edges. When asked a subset $S$ of\nvertices, a cut query on $G$ returns the number of edges of $G$ that have\nexactly one endpoint in $S$. We show that there is a bounded-error quantum\nalgorithm that determines all connected components of $G$ after making\n$O(\\log(n)^6)$ many cut queries. In contrast, it follows from results in\ncommunication complexity that any randomized algorithm even just to decide\nwhether the graph is connected or not must make at least $\\Omega(n/\\log(n))$\nmany cut queries. We further show that with $O(\\log(n)^8)$ many cut queries a\nquantum algorithm can with high probability output a spanning forest for $G$.\n  En route to proving these results, we design quantum algorithms for learning\na graph using cut queries. We show that a quantum algorithm can learn a graph\nwith maximum degree $d$ after $O(d \\log(n)^2)$ many cut queries, and can learn\na general graph with $O(\\sqrt{m} \\log(n)^{3/2})$ many cut queries. These two\nupper bounds are tight up to the poly-logarithmic factors, and compare to\n$\\Omega(dn)$ and $\\Omega(m/\\log(n))$ lower bounds on the number of cut queries\nneeded by a randomized algorithm for the same problems, respectively.\n  The key ingredients in our results are the Bernstein-Vazirani algorithm,\napproximate counting with \"OR queries\", and learning sparse vectors from inner\nproducts as in compressed sensing.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 12:21:01 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 12:04:07 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Lee", "Troy", ""], ["Santha", "Miklos", ""], ["Zhang", "Shengyu", ""]]}, {"id": "2007.08357", "submitter": "Solon Pissis", "authors": "Giulia Bernardini and Gabriele Fici and Pawe{\\l} Gawrychowski and\n  Solon P. Pissis", "title": "Substring Complexity in Sublinear Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Shannon's entropy is a definitive lower bound for statistical compression.\nUnfortunately, no such clear measure exists for the compressibility of\nrepetitive strings. Thus, ad-hoc measures are employed to estimate the\nrepetitiveness of strings, e.g., the size $z$ of the Lempel-Ziv parse or the\nnumber $r$ of equal-letter runs of the Burrows-Wheeler transform. A more recent\none is the size $\\gamma$ of a smallest string attractor. Unfortunately, Kempa\nand Prezza [STOC 2018] showed that computing $\\gamma$ is NP-hard. Kociumaka et\nal. [LATIN 2020] considered a new measure that is based on the function $S_T$\ncounting the cardinalities of the sets of substrings of each length of $T$,\nalso known as the substring complexity. This new measure is defined as $\\delta=\n\\sup\\{S_T(k)/k, k\\geq 1\\}$ and lower bounds all the measures previously\nconsidered. In particular, $\\delta\\leq \\gamma$ always holds and $\\delta$ can be\ncomputed in $\\mathcal{O}(n)$ time using $\\Omega(n)$ working space. Kociumaka et\nal. showed that if $\\delta$ is given, one can construct an $\\mathcal{O}(\\delta\n\\log \\frac{n}{\\delta})$-sized representation of $T$ supporting efficient direct\naccess and efficient pattern matching queries on $T$. Given that for highly\ncompressible strings, $\\delta$ is significantly smaller than $n$, it is natural\nto pose the following question: Can we compute $\\delta$ efficiently using\nsublinear working space?\n  It is straightforward to show that any algorithm computing $\\delta$ using\n$\\mathcal{O}(b)$ space requires $\\Omega(n^{2-o(1)}/b)$ time through a reduction\nfrom the element distinctness problem [Yao, SIAM J. Comput. 1994]. We present\nthe following results: an $\\mathcal{O}(n^3/b^2)$-time and\n$\\mathcal{O}(b)$-space algorithm to compute $\\delta$, for any $b\\in[1,n]$; and\nan $\\tilde{\\mathcal{O}}(n^2/b)$-time and $\\mathcal{O}(b)$-space algorithm to\ncompute $\\delta$, for any $b\\in[n^{2/3},n]$.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 14:34:14 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Bernardini", "Giulia", ""], ["Fici", "Gabriele", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Pissis", "Solon P.", ""]]}, {"id": "2007.08401", "submitter": "Michael Dinitz", "authors": "Greg Bodwin, Michael Dinitz, Caleb Robelle", "title": "Optimal Vertex Fault-Tolerant Spanners in Polynomial Time", "comments": "Appears in SODA 2021. Corrects some references, answers reviewer\n  comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has pinned down the existentially optimal size bounds for vertex\nfault-tolerant spanners: for any positive integer $k$, every $n$-node graph has\na $(2k-1)$-spanner on $O(f^{1-1/k} n^{1+1/k})$ edges resilient to $f$ vertex\nfaults, and there are examples of input graphs on which this bound cannot be\nimproved. However, these proofs work by analyzing the output spanner of a\ncertain exponential-time greedy algorithm. In this work, we give the first\nalgorithm that produces vertex fault tolerant spanners of optimal size and\nwhich runs in polynomial time. Specifically, we give a randomized algorithm\nwhich takes $\\widetilde{O}\\left( f^{1-1/k} n^{2+1/k} + mf^2\\right)$ time. We\nalso derandomize our algorithm to give a deterministic algorithm with similar\nbounds. This reflects an exponential improvement in runtime over [Bodwin-Patel\nPODC '19], the only previously known algorithm for constructing optimal vertex\nfault-tolerant spanners.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 15:23:51 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 16:49:01 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 11:55:55 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Bodwin", "Greg", ""], ["Dinitz", "Michael", ""], ["Robelle", "Caleb", ""]]}, {"id": "2007.08415", "submitter": "Bertrand Simon", "authors": "Martin B\\\"ohm, Franziska Eberle, Nicole Megow, Lukas N\\\"olke, Jens\n  Schl\\\"oter, Bertrand Simon, Andreas Wiese", "title": "Fully Dynamic Algorithms for Knapsack Problems with Polylogarithmic\n  Update Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knapsack problems are among the most fundamental problems in optimization. In\nthe Multiple Knapsack problem, we are given multiple knapsacks with different\ncapacities and items with values and sizes. The task is to find a subset of\nitems of maximum total value that can be packed into the knapsacks without\nexceeding the capacities. We investigate this problem and special cases thereof\nin the context of dynamic algorithms and design data structures that\nefficiently maintain near-optimal knapsack solutions for dynamically changing\ninput. More precisely, we handle the arrival and departure of individual items\nor knapsacks during the execution of the algorithm with worst-case update time\npolylogarithmic in the number of items. As the optimal and any approximate\nsolution may change drastically, we only maintain implicit solutions and\nsupport certain queries in polylogarithmic time, such as the packing of an item\nand the solution value.\n  While dynamic algorithms are well-studied in the context of graph problems,\nthere is hardly any work on packing problems and generally much less on\nnon-graph problems. Given the theoretical interest in knapsack problems and\ntheir practical relevance, it is somewhat surprising that Knapsack has not been\naddressed before in the context of dynamic algorithms and our work bridges this\ngap.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 15:51:30 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 09:32:46 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["B\u00f6hm", "Martin", ""], ["Eberle", "Franziska", ""], ["Megow", "Nicole", ""], ["N\u00f6lke", "Lukas", ""], ["Schl\u00f6ter", "Jens", ""], ["Simon", "Bertrand", ""], ["Wiese", "Andreas", ""]]}, {"id": "2007.08575", "submitter": "Alexander Kozachinskiy", "authors": "Alexander Kozachinskiy", "title": "Polyhedral value iteration for discounted games and energy games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic algorithm, solving discounted games with $n$ nodes\nin $n^{O(1)}\\cdot (2 + \\sqrt{2})^n$-time. For bipartite discounted games our\nalgorithm runs in $n^{O(1)}\\cdot 2^n$-time. Prior to our work no deterministic\nalgorithm running in time $2^{o(n\\log n)}$ regardless of the discount factor\nwas known.\n  We call our approach polyhedral value iteration. We rely on a well-known fact\nthat the values of a discounted game can be found from the so-called optimality\nequations. In the algorithm we consider a polyhedron obtained by relaxing\noptimality equations. We iterate points on the border of this polyhedron by\nmoving each time along a carefully chosen shift as far as possible. This\ncontinues until the current point satisfies optimality equations.\n  Our approach is heavily inspired by a recent algorithm of Dorfman et al.\n(ICALP 2019) for energy games. For completeness, we present their algorithm in\nterms of polyhedral value iteration. Our exposition, unlike the original\nalgorithm, does not require edge weights to be integers and works for arbitrary\nreal weights.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:23:45 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 15:21:38 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kozachinskiy", "Alexander", ""]]}, {"id": "2007.08585", "submitter": "Yaowei Long", "authors": "Yaowei Long, Seth Pettie", "title": "Planar Distance Oracles with Better Time-Space Tradeoffs", "comments": "35 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent breakthrough, Charalampopoulos, Gawrychowski, Mozes, and Weimann\n(STOC 2019) showed that exact distance queries on planar graphs could be\nanswered in $n^{o(1)}$ time by a data structure occupying $n^{1+o(1)}$ space,\ni.e., up to $o(1)$ terms, optimal exponents in time (0) and space (1) can be\nachieved simultaneously. Their distance query algorithm is recursive: it makes\nsuccessive calls to a point-location algorithm for planar Voronoi diagrams,\nwhich involves many recursive distance queries. The depth of this recursion is\nnon-constant and the branching factor logarithmic, leading to $(\\log\nn)^{\\omega(1)} = n^{o(1)}$ query times.\n  In this paper we present a new way to do point-location in planar Voronoi\ndiagrams, which leads to a new exact distance oracle. At the two extremes of\nour space-time tradeoff curve we can achieve either\n  $n^{1+o(1)}$ space and $\\log^{2+o(1)}n$ query time, or\n  $n\\log^{2+o(1)}n$ space and $n^{o(1)}$ query time.\n  All previous oracles with $\\tilde{O}(1)$ query time occupy space\n$n^{1+\\Omega(1)}$, and all previous oracles with space $\\tilde{O}(n)$ answer\nqueries in $n^{\\Omega(1)}$ time.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:42:08 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Long", "Yaowei", ""], ["Pettie", "Seth", ""]]}, {"id": "2007.08643", "submitter": "Grigorios Koumoutsos", "authors": "Sujoy Bhore, Jean Cardinal, John Iacono, Grigorios Koumoutsos", "title": "Dynamic Geometric Independent Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present fully dynamic approximation algorithms for the Maximum Independent\nSet problem on several types of geometric objects: intervals on the real line,\narbitrary axis-aligned squares in the plane and axis-aligned $d$-dimensional\nhypercubes.\n  It is known that a maximum independent set of a collection of $n$ intervals\ncan be found in $O(n\\log n)$ time, while it is already \\textsf{NP}-hard for a\nset of unit squares. Moreover, the problem is inapproximable on many important\ngraph families, but admits a \\textsf{PTAS} for a set of arbitrary pseudo-disks.\nTherefore, a fundamental question in computational geometry is whether it is\npossible to maintain an approximate maximum independent set in a set of dynamic\ngeometric objects, in truly sublinear time per insertion or deletion. In this\nwork, we answer this question in the affirmative for intervals, squares and\nhypercubes.\n  First, we show that for intervals a $(1+\\varepsilon)$-approximate maximum\nindependent set can be maintained with logarithmic worst-case update time. This\nis achieved by maintaining a locally optimal solution using a constant number\nof constant-size exchanges per update.\n  We then show how our interval structure can be used to design a data\nstructure for maintaining an expected constant factor approximate maximum\nindependent set of axis-aligned squares in the plane, with polylogarithmic\namortized update time. Our approach generalizes to $d$-dimensional hypercubes,\nproviding a $O(4^d)$-approximation with polylogarithmic update time.\n  Those are the first approximation algorithms for any set of dynamic arbitrary\nsize geometric objects; previous results required bounded size ratios to obtain\npolylogarithmic update time. Furthermore, it is known that our results for\nsquares (and hypercubes) cannot be improved to a\n$(1+\\varepsilon)$-approximation with the same update time.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 21:13:40 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Bhore", "Sujoy", ""], ["Cardinal", "Jean", ""], ["Iacono", "John", ""], ["Koumoutsos", "Grigorios", ""]]}, {"id": "2007.08669", "submitter": "Grigorios Koumoutsos", "authors": "Dimitris Christou, Dimitris Fotakis, Grigorios Koumoutsos", "title": "Memoryless Algorithms for the Generalized $k$-server Problem on Uniform\n  Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the generalized $k$-server problem on uniform metrics. We study\nthe power of memoryless algorithms and show tight bounds of $\\Theta(k!)$ on\ntheir competitive ratio. In particular we show that the \\textit{Harmonic\nAlgorithm} achieves this competitive ratio and provide matching lower bounds.\nThis improves the $\\approx 2^{2^k}$ doubly-exponential bound of Chiplunkar and\nVishwanathan for the more general setting of uniform metrics with different\nweights.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 22:09:11 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Christou", "Dimitris", ""], ["Fotakis", "Dimitris", ""], ["Koumoutsos", "Grigorios", ""]]}, {"id": "2007.08761", "submitter": "Peter Gartland", "authors": "Peter Gartland and Daniel Lokshtanov", "title": "Dominated Minimal Separators are Tame (Nearly All Others are Feral)", "comments": "32 pages 5, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A class ${\\cal F}$ of graphs is called {\\em tame} if there exists a constant\n$k$ so that every graph in ${\\cal F}$ on $n$ vertices contains at most $O(n^k)$\nminimal separators, {\\em strongly-quasi-tame} if every graph in ${\\cal F}$ on\n$n$ vertices contains at most $O(n^{k \\log n})$ minimal separators, and {\\em\nferal} if there exists a constant $c > 1$ so that ${\\cal F}$ contains\n$n$-vertex graphs with at least $c^n$ minimal separators for arbitrarily large\n$n$. The classification of graph classes into tame or feral has numerous\nalgorithmic consequences, and has recently received considerable attention.\n  A key graph-theoretic object in the quest for such a classification is the\nnotion of a $k$-{\\em creature}. In a recent manuscript [Abrishami et al., Arxiv\n2020] conjecture that every hereditary class ${\\cal F}$ that excludes\n$k$-creatures for some fixed constant $k$ is tame. We give a counterexample to\nthis conjecture and prove the weaker result that a hereditary class ${\\cal F}$\nis strongly quasi-tame if it excludes $k$-creatures for some fixed constant $k$\nand additionally every minimal separator can be dominated by another fixed\nconstant $k'$ number of vertices. The tools developed also lead to a number of\nadditional results of independent interest.\n  {\\bf (i) We obtain a complete classification of all hereditary graph classes\ndefined by a finite set of forbidden induced subgraphs into strongly quasi-tame\nor feral. This generalizes Milani\\v{c} and Piva\\v{c} [WG'19]. {\\bf (ii)} We\nshow that hereditary class that excludes $k$-creatures and additionally\nexcludes all cycles of length at least $c$, for some constant $c$, are tame.\nThis generalizes the result of [Chudnovsky et al., Arxiv 2019]. {\\bf (iii)} We\nshow that every hereditary class that excludes $k$-creatures and additionally\nexcludes a complete graph on $c$ vertices for some fixed constant $c$ is tame.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 05:12:51 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Gartland", "Peter", ""], ["Lokshtanov", "Daniel", ""]]}, {"id": "2007.08787", "submitter": "Pedro Matias", "authors": "Ramtin Afshar, Amihood Amir, Michael T. Goodrich, Pedro Matias", "title": "Adaptive Exact Learning in a Mixed-Up World: Dealing with Periodicity,\n  Errors and Jumbled-Index Queries in String Reconstruction", "comments": "Full version of SPIRE 2020 conference proceedings paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the query complexity of exactly reconstructing a string from\nadaptive queries, such as substring, subsequence, and jumbled-index queries.\nSuch problems have applications, e.g., in computational biology. We provide a\nnumber of new and improved bounds for exact string reconstruction for settings\nwhere either the string or the queries are \"mixed-up\". For example, we show\nthat a periodic (i.e., \"mixed-up\") string, $S=p^kp'$, of smallest period $p$,\nwhere $|p'|<|p|$, can be reconstructed using $O(\\sigma|p|+\\lg n)$ substring\nqueries, where $\\sigma$ is the alphabet size, if $n=|S|$ is unknown. We also\nshow that we can reconstruct $S$ after having been corrupted by a small number\nof errors $d$, measured by Hamming distance. In this case, we give an algorithm\nthat uses $O(d\\sigma|p| + d|p|\\lg \\frac{n}{d+1})$ queries. In addition, we show\nthat a periodic string can be reconstructed using $2\\sigma\\lceil\\lg n\\rceil +\n2|p|\\lceil\\lg \\sigma\\rceil$ subsequence queries, and that general strings can\nbe reconstructed using $2\\sigma\\lceil\\lg n\\rceil + n\\lceil\\lg \\sigma\\rceil$\nsubsequence queries, without knowledge of $n$ in advance. This latter result\nimproves the previous best, decades-old result, by Skiena and Sundaram.\nFinally, we believe we are the first to study the exact-learning query\ncomplexity for string reconstruction using jumbled-index queries, which are a\n\"mixed-up\" typeA of query that have received much attention of late.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 07:23:15 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 18:36:21 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 19:54:35 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Afshar", "Ramtin", ""], ["Amir", "Amihood", ""], ["Goodrich", "Michael T.", ""], ["Matias", "Pedro", ""]]}, {"id": "2007.08811", "submitter": "Yota Otachi", "authors": "Yasuaki Kobayashi and Yota Otachi", "title": "Parameterized Complexity of Graph Burning", "comments": "10 pages, 2 figures, IPEC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Burning asks, given a graph $G = (V,E)$ and an integer $k$, whether\nthere exists $(b_{0},\\dots,b_{k-1}) \\in V^{k}$ such that every vertex in $G$\nhas distance at most $i$ from some $b_{i}$. This problem is known to be\nNP-complete even on connected caterpillars of maximum degree $3$. We study the\nparameterized complexity of this problem and answer all questions arose by Kare\nand Reddy [IWOCA 2019] about parameterized complexity of the problem. We show\nthat the problem is W[2]-complete parameterized by $k$ and that it does no\nadmit a polynomial kernel parameterized by vertex cover number unless\n$\\mathrm{NP} \\subseteq \\mathrm{coNP/poly}$. We also show that the problem is\nfixed-parameter tractable parameterized by clique-width plus the maximum\ndiameter among all connected components. This implies the fixed-parameter\ntractability parameterized by modular-width, by treedepth, and by distance to\ncographs. Although the parameterization by distance to split graphs cannot be\nhandled with the clique-width argument, we show that this is also tractable by\na reduction to a generalized problem with a smaller solution size.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 08:16:45 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 07:21:54 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Kobayashi", "Yasuaki", ""], ["Otachi", "Yota", ""]]}, {"id": "2007.08836", "submitter": "Lu Chen", "authors": "Lu Chen, Chengfei Liu, Rui Zhou, Jiajie Xu, Jianxin Li", "title": "Efficient Exact Algorithms for Maximum Balanced Biclique Search in\n  Bipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a bipartite graph, the maximum balanced biclique (\\textsf{MBB})\nproblem, discovering a mutually connected while equal-sized disjoint sets with\nthe maximum cardinality, plays a significant role for mining the bipartite\ngraph and has numerous applications. Despite the NP-hardness of the\n\\textsf{MBB} problem, in this paper, we show that an exact \\textsf{MBB} can be\ndiscovered extremely fast in bipartite graphs for real applications. We propose\ntwo exact algorithms dedicated for dense and sparse bipartite graphs\nrespectively. For dense bipartite graphs, an $\\mathcal{O}^{*}( 1.3803^{n})$\nalgorithm is proposed. This algorithm in fact can find an \\textsf{MBB} in near\npolynomial time for dense bipartite graphs that are common for applications\nsuch as VLSI design. This is because, using our proposed novel techniques, the\nsearch can fast converge to sufficiently dense bipartite graphs which we prove\nto be polynomially solvable. For large sparse bipartite graphs typical for\napplications such as biological data analysis, an $\\mathcal{O}^{*}(\n1.3803^{\\ddot{\\delta}})$ algorithm is proposed, where $\\ddot{\\delta}$ is only a\nfew hundreds for large sparse bipartite graphs with millions of vertices. The\nindispensible optimizations that lead to this time complexity are: we transform\na large sparse bipartite graph into a limited number of dense subgraphs with\nsize up to $\\ddot{\\delta}$ and then apply our proposed algorithm for dense\nbipartite graphs on each of the subgraphs. To further speed up this algorithm,\ntighter upper bounds, faster heuristics and effective reductions are proposed,\nallowing an \\textsf{MBB} to be discovered within a few seconds for bipartite\ngraphs with millions of vertices. Extensive experiments are conducted on\nsynthetic and real large bipartite graphs to demonstrate the efficiency and\neffectiveness of our proposed algorithms and techniques.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 09:07:46 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Chen", "Lu", ""], ["Liu", "Chengfei", ""], ["Zhou", "Rui", ""], ["Xu", "Jiajie", ""], ["Li", "Jianxin", ""]]}, {"id": "2007.08840", "submitter": "Alina Ene", "authors": "Alina Ene, Huy L. Nguyen, Adrian Vladu", "title": "Adaptive Gradient Methods for Constrained Convex Optimization and\n  Variational Inequalities", "comments": "Full version of AAAI-21 paper. The current version adds an\n  experimental evaluation and revises the exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide new adaptive first-order methods for constrained convex\noptimization. Our main algorithms AdaACSA and AdaAGD+ are accelerated methods,\nwhich are universal in the sense that they achieve nearly-optimal convergence\nrates for both smooth and non-smooth functions, even when they only have access\nto stochastic gradients. In addition, they do not require any prior knowledge\non how the objective function is parametrized, since they automatically adjust\ntheir per-coordinate learning rate. These can be seen as truly accelerated\nAdagrad methods for constrained optimization.\n  We complement them with a simpler algorithm AdaGrad+ which enjoys the same\nfeatures, and achieves the standard non-accelerated convergence rate. We also\npresent a set of new results involving adaptive methods for unconstrained\noptimization and monotone operators.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 09:10:21 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 14:27:04 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 19:47:21 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""], ["Vladu", "Adrian", ""]]}, {"id": "2007.08914", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Fabrizio Grandoni, Giuseppe F. Italiano, Aleksander {\\L}ukasiewicz,\n  Nikos Parotsidis, Przemys{\\l}aw Uzna\\'nski", "title": "All-Pairs LCA in DAGs: Breaking through the $O(n^{2.5})$ barrier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be an $n$-vertex directed acyclic graph (DAG). A lowest common\nancestor (LCA) of two vertices $u$ and $v$ is a common ancestor $w$ of $u$ and\n$v$ such that no descendant of $w$ has the same property. In this paper, we\nconsider the problem of computing an LCA, if any, for all pairs of vertices in\na DAG. The fastest known algorithms for this problem exploit fast matrix\nmultiplication subroutines and have running times ranging from $O(n^{2.687})$\n[Bender et al.~SODA'01] down to $O(n^{2.615})$ [Kowaluk and Lingas~ICALP'05]\nand $O(n^{2.569})$ [Czumaj et al.~TCS'07]. Somewhat surprisingly, all those\nbounds would still be $\\Omega(n^{2.5})$ even if matrix multiplication could be\nsolved optimally (i.e., $\\omega=2$). This appears to be an inherent barrier for\nall the currently known approaches, which raises the natural question on\nwhether one could break through the $O(n^{2.5})$ barrier for this problem.\n  In this paper, we answer this question affirmatively: in particular, we\npresent an $\\tilde O(n^{2.447})$ ($\\tilde O(n^{7/3})$ for $\\omega=2$) algorithm\nfor finding an LCA for all pairs of vertices in a DAG, which represents the\nfirst improvement on the running times for this problem in the last 13 years. A\nkey tool in our approach is a fast algorithm to partition the vertex set of the\ntransitive closure of $G$ into a collection of $O(\\ell)$ chains and $O(n/\\ell)$\nantichains, for a given parameter $\\ell$. As usual, a chain is a path while an\nantichain is an independent set. We then find, for all pairs of vertices, a\n\\emph{candidate} LCA among the chain and antichain vertices, separately. The\nfirst set is obtained via a reduction to min-max matrix multiplication. The\ncomputation of the second set can be reduced to Boolean matrix multiplication\nsimilarly to previous results on this problem. We finally combine the two\nsolutions together in a careful (non-obvious) manner.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 11:42:42 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 14:52:01 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Grandoni", "Fabrizio", ""], ["Italiano", "Giuseppe F.", ""], ["\u0141ukasiewicz", "Aleksander", ""], ["Parotsidis", "Nikos", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "2007.09018", "submitter": "Magnus Wahlstr\\\"om", "authors": "Eun Jung Kim, Stefan Kratsch, Marcin Pilipczuk, Magnus Wahlstr\\\"om", "title": "Solving hard cut problems via flow-augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new technique for designing FPT algorithms for graph cut\nproblems in undirected graphs, which we call flow augmentation. Our technique\nis applicable to problems that can be phrased as a search for an (edge)\n$(s,t)$-cut of cardinality at most $k$ in an undirected graph $G$ with\ndesignated terminals $s$ and $t$.\n  More precisely, we consider problems where an (unknown) solution is a set $Z\n\\subseteq E(G)$ of size at most $k$ such that (1) in $G-Z$, $s$ and $t$ are in\ndistinct connected components, (2) every edge of $Z$ connects two distinct\nconnected components of $G-Z$, and (3) if we define the set $Z_{s,t} \\subseteq\nZ$ as these edges $e \\in Z$ for which there exists an $(s,t)$-path $P_e$ with\n$E(P_e) \\cap Z = \\{e\\}$, then $Z_{s,t}$ separates $s$ from $t$. We prove that\nin this scenario one can in randomized time $k^{O(1)} (|V(G)|+|E(G)|)$ add a\nnumber of edges to the graph so that with $2^{-O(k \\log k)}$ probability no\nadded edge connects two components of $G-Z$ and $Z_{s,t}$ becomes a minimum cut\nbetween $s$ and $t$.\n  We apply our method to obtain a randomized FPT algorithm for a notorious\n\"hard nut\" graph cut problem we call Coupled Min-Cut. This problem emerges out\nof the study of FPT algorithms for Min CSP problems, and was unamenable to\nother techniques for parameterized algorithms in graph cut problems, such as\nRandomized Contractions, Treewidth Reduction or Shadow Removal.\n  To demonstrate the power of the approach, we consider more generally Min\nSAT($\\Gamma$), parameterized by the solution cost. We show that every problem\nMin SAT($\\Gamma$) is either (1) FPT, (2) W[1]-hard, or (3) able to express the\nsoft constraint $(u \\to v)$, and thereby also the min-cut problem in directed\ngraphs. All the W[1]-hard cases were known or immediate, and the main new\nresult is an FPT algorithm for a generalization of Coupled Min-Cut.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 14:16:11 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Kim", "Eun Jung", ""], ["Kratsch", "Stefan", ""], ["Pilipczuk", "Marcin", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "2007.09065", "submitter": "Cosimo Vinci", "authors": "Gianlorenzo D'Angelo, Debashmita Poddar, Cosimo Vinci", "title": "Improved Approximation Factor for Adaptive Influence Maximization via\n  Simple Greedy Strategies", "comments": "arXiv admin note: text overlap with arXiv:2006.15374", "journal-ref": "The 48th International Colloquium on Automata, Languages, and\n  Programming (ICALP 2021)", "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the adaptive influence maximization problem, we are given a social network\nand a budget $k$, and we iteratively select $k$ nodes, called seeds, in order\nto maximize the expected number of nodes that are reached by an influence\ncascade that they generate according to a stochastic model for influence\ndiffusion. Differently from the non-adaptive influence maximization problem,\nwhere all the seeds must be selected beforehand, here nodes are selected\nsequentially one by one, and the decision on the $i$th seed is based on the\nobserved cascade produced by the first $i-1$ seeds. We focus on the myopic\nfeedback model, in which we can only observe which neighbors of previously\nselected seeds have been influenced and on the independent cascade model, where\neach edge is associated with an independent probability of diffusing influence.\nPrevious works showed that the adaptivity gap is at most $4$, which implies\nthat the non-adaptive greedy algorithm guarantees an approximation factor of\n$\\frac{1}{4}\\left(1-\\frac{1}{e}\\right)$ for the adaptive problem. In this\npaper, we improve the bounds on both the adaptivity gap and on the\napproximation factor. We directly analyze the approximation factor of the\nnon-adaptive greedy algorithm, without passing through the adaptivity gap, and\nshow that it is at least $\\frac{1}{2}\\left(1-\\frac{1}{e}\\right)$. Therefore,\nthe adaptivity gap is at most $\\frac{2e}{e-1}\\approx 3.164$. To prove these\nbounds, we introduce a new approach to relate the greedy non-adaptive algorithm\nto the adaptive optimum. The new approach does not rely on multi-linear\nextensions or random walks on optimal decision trees, which are commonly used\ntechniques in the field. We believe that it is of independent interest and may\nbe used to analyze other adaptive optimization problems.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 11:46:15 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 14:01:27 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["D'Angelo", "Gianlorenzo", ""], ["Poddar", "Debashmita", ""], ["Vinci", "Cosimo", ""]]}, {"id": "2007.09075", "submitter": "Kuan Cheng", "authors": "Kuan Cheng, Venkatesan Guruswami, Bernhard Haeupler, Xin Li", "title": "Efficient Linear and Affine Codes for Correcting Insertions/Deletions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DM cs.DS math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies \\emph{linear} and \\emph{affine} error-correcting codes for\ncorrecting synchronization errors such as insertions and deletions. We call\nsuch codes linear/affine insdel codes.\n  Linear codes that can correct even a single deletion are limited to have\ninformation rate at most $1/2$ (achieved by the trivial 2-fold repetition\ncode). Previously, it was (erroneously) reported that more generally no\nnon-trivial linear codes correcting $k$ deletions exist, i.e., that the\n$(k+1)$-fold repetition codes and its rate of $1/(k+1)$ are basically optimal\nfor any $k$. We disprove this and show the existence of binary linear codes of\nlength $n$ and rate just below $1/2$ capable of correcting $\\Omega(n)$\ninsertions and deletions. This identifies rate $1/2$ as a sharp threshold for\nrecovery from deletions for linear codes, and reopens the quest for a better\nunderstanding of the capabilities of linear codes for correcting\ninsertions/deletions.\n  We prove novel outer bounds and existential inner bounds for the rate vs.\n(edit) distance trade-off of linear insdel codes. We complement our existential\nresults with an efficient synchronization-string-based transformation that\nconverts any asymptotically-good linear code for Hamming errors into an\nasymptotically-good linear code for insdel errors. Lastly, we show that the\n$\\frac{1}{2}$-rate limitation does not hold for affine codes by giving an\nexplicit affine code of rate $1-\\epsilon$ which can efficiently correct a\nconstant fraction of insdel errors.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 15:56:05 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 10:07:17 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 12:54:40 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Cheng", "Kuan", ""], ["Guruswami", "Venkatesan", ""], ["Haeupler", "Bernhard", ""], ["Li", "Xin", ""]]}, {"id": "2007.09116", "submitter": "Lars Rohwedder", "authors": "Etienne Bamas, Paritosh Garg, Lars Rohwedder", "title": "The Combinatorial Santa Claus Problem or: How to Find Good Matchings in\n  Non-Uniform Hypergraphs", "comments": "Superseded by arXiv:2011.06939", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider hypergraphs on vertices $P\\cup R$ where each hyperedge contains\nexactly one vertex in $P$. Our goal is to select a matching that covers all of\n$P$, but we allow each selected hyperedge to drop all but an\n$(1/\\alpha)$-fraction of its intersection with $R$ (thus relaxing the matching\nconstraint). Here $\\alpha$ is to be minimized. We dub this problem the\nCombinatorial Santa Claus problem, since we show in this paper that this\nproblem and the Santa Claus problem are almost equivalent in terms of their\napproximability.\n  The non-trivial observation that any uniform regular hypergraph admits a\nrelaxed matching for $\\alpha = O(1)$ was a major step in obtaining a constant\napproximation rate for a special case of the Santa Claus problem, which\nreceived great attention in literature. It is natural to ask if the uniformity\ncondition can be omitted. Our main result is that every (non-uniform) regular\nhypergraph admits a relaxed matching for $\\alpha = O(\\log\\log(|R|))$, when all\nhyperedges are sufficiently large (a condition that is necessary). In\nparticular, this implies an $O(\\log\\log(|R|))$-approximation algorithm for the\nCombinatorial Santa Claus problem with large hyperedges.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 17:01:13 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 09:10:18 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Bamas", "Etienne", ""], ["Garg", "Paritosh", ""], ["Rohwedder", "Lars", ""]]}, {"id": "2007.09172", "submitter": "Nikhil Bansal", "authors": "Nikhil Bansal, Jatin Batra, Majid Farhadi, Prasad Tetali", "title": "Improved Approximations for Min Sum Vertex Cover and Generalized Min Sum\n  Set Cover", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the generalized min sum set cover (GMSSC) problem, wherein given a\ncollection of hyperedges $E$ with arbitrary covering requirements $k_e$, the\ngoal is to find an ordering of the vertices to minimize the total cover time of\nthe hyperedges; a hyperedge $e$ is considered covered by the first time when\n$k_e$ many of its vertices appear in the ordering. We give a $4.642$\napproximation algorithm for GMSSC, coming close to the best possible bound of\n$4$, already for the classical special case (with all $k_e=1$) of min sum set\ncover (MSSC) studied by Feige, Lov\\'{a}sz and Tetali, and improving upon the\nprevious best known bound of $12.4$ due to Im, Sviridenko and van der Zwaan.\nOur algorithm is based on transforming the LP solution by a suitable kernel and\napplying randomized rounding. This also gives an LP-based $4$ approximation for\nMSSC. As part of the analysis of our algorithm, we also derive an inequality on\nthe lower tail of a sum of independent Bernoulli random variables, which might\nbe of independent interest and broader utility.\n  Another well-known special case is the min sum vertex cover (MSVC) problem,\nin which the input hypergraph is a graph and $k_e = 1$, for every edge. We give\na $16/9$ approximation for MSVC, and show a matching integrality gap for the\nnatural LP relaxation. This improves upon the previous best $1.999946$\napproximation of Barenholz, Feige and Peleg. (The claimed $1.79$ approximation\nresult of Iwata, Tetali and Tripathi for the MSVC turned out have an\nunfortunate, seemingly unfixable, mistake in it.) Finally, we revisit MSSC and\nconsider the $\\ell_p$ norm of cover-time of the hyperedges. Using a dual\nfitting argument, we show that the natural greedy algorithm achieves tight, up\nto NP-hardness, approximation guarantees of $(p+1)^{1+1/p}$, for all $p\\ge 1$.\nFor $p=1$, this gives yet another proof of the $4$ approximation for MSSC.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 18:16:16 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Bansal", "Nikhil", ""], ["Batra", "Jatin", ""], ["Farhadi", "Majid", ""], ["Tetali", "Prasad", ""]]}, {"id": "2007.09192", "submitter": "Maria Kosche", "authors": "Pamela Fleischmann (1), Maria Kosche (2), Tore Ko{\\ss} (2), Florin\n  Manea (2), Stefan Siemer (2) ((1) Kiel University, Computer Science\n  Department, Germany, (2) G\\\"ottingen University, Computer Science Department,\n  Germany)", "title": "The Edit Distance to $k$-Subsequence Universality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A word $u$ is a subsequence of another word $w$ if $u$ can be obtained from\n$w$ by deleting some of its letters. The word $w$ with alph$(w)=\\Sigma$ is\ncalled $k$-subsequence universal if the set of subsequences of length $k$ of\n$w$ contains all possible words of length $k$ over $\\Sigma$. We propose a\nseries of efficient algorithms computing the minimal number of edit operations\n(insertion, deletion, substitution) one needs to apply to a given word in order\nto reach the set of $k$-subsequence universal words.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 19:13:09 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Fleischmann", "Pamela", ""], ["Kosche", "Maria", ""], ["Ko\u00df", "Tore", ""], ["Manea", "Florin", ""], ["Siemer", "Stefan", ""]]}, {"id": "2007.09202", "submitter": "Gopinath Mishra", "authors": "Arijit Bishnu, Arijit Ghosh, Gopinath Mishra and Manaswi Paraashar", "title": "Query Complexity of Global Minimum Cut", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we resolve the query complexity of global minimum cut problem\nfor a graph by designing a randomized algorithm for approximating the size of\nminimum cut in a graph, where the graph can be accessed through local queries\nlike {\\sc Degree}, {\\sc Neighbor}, and {\\sc Adjacency} queries.\n  Given $\\epsilon \\in (0,1)$, the algorithm with high probability outputs an\nestimate $\\hat{t}$ satisfying the following $(1-\\epsilon) t \\leq \\hat{t} \\leq\n(1+\\epsilon) t$, where $m$ is the number of edges in the graph and $t$ is the\nsize of minimum cut in the graph. The expected number of local queries used by\nour algorithm is $\\min\\left\\{m+n,\\frac{m}{t}\\right\\}\\mbox{poly}\\left(\\log\nn,\\frac{1}{\\epsilon}\\right)$ where $n$ is the number of vertices in the graph.\nEden and Rosenbaum showed that $\\Omega(m/t)$ many local queries are required\nfor approximating the size of minimum cut in graphs. These two results together\nresolve the query complexity of the problem of estimating the size of minimum\ncut in graphs using local queries.\n  Building on the lower bound of Eden and Rosenbaum, we show that, for all $t\n\\in \\mathbb{N}$, $\\Omega(m)$ local queries are required to decide if the size\nof the minimum cut in the graph is $t$ or $t-2$. Also, we show that, for any $t\n\\in \\mathbb{N}$, $\\Omega(m)$ local queries are required to find all the minimum\ncut edges even if it is promised that the input graph has a minimum cut of size\n$t$. Both of our lower bound results are randomized, and hold even if we can\nmake {\\sc Random Edge} query apart from local queries.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 19:37:28 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 09:59:49 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Bishnu", "Arijit", ""], ["Ghosh", "Arijit", ""], ["Mishra", "Gopinath", ""], ["Paraashar", "Manaswi", ""]]}, {"id": "2007.09261", "submitter": "Vassilis Digalakis Jr.", "authors": "Dimitris Bertsimas and Vassilis Digalakis Jr", "title": "Frequency Estimation in Data Streams: Learning the Optimal Hashing\n  Scheme", "comments": "Submitted to IEEE Transactions on Knowledge and Data Engineering on\n  07/2020. Revised on 05/2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for the problem of frequency estimation in data\nstreams that is based on optimization and machine learning. Contrary to\nstate-of-the-art streaming frequency estimation algorithms, which heavily rely\non random hashing to maintain the frequency distribution of the data steam\nusing limited storage, the proposed approach exploits an observed stream prefix\nto near-optimally hash elements and compress the target frequency distribution.\nWe develop an exact mixed-integer linear optimization formulation, which\nenables us to compute optimal or near-optimal hashing schemes for elements seen\nin the observed stream prefix; then, we use machine learning to hash unseen\nelements. Further, we develop an efficient block coordinate descent algorithm,\nwhich, as we empirically show, produces high quality solutions, and, in a\nspecial case, we are able to solve the proposed formulation exactly in linear\ntime using dynamic programming. We empirically evaluate the proposed approach\nboth on synthetic datasets and on real-world search query data. We show that\nthe proposed approach outperforms existing approaches by one to two orders of\nmagnitude in terms of its average (per element) estimation error and by 45-90%\nin terms of its expected magnitude of estimation error.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 22:15:22 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 04:38:39 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Digalakis", "Vassilis", "Jr"]]}, {"id": "2007.09282", "submitter": "Bogdan Armaselu", "authors": "Bogdan Armaselu", "title": "An APX for the Maximum-Profit Routing Problem with Variable Supply", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the Maximum-Profit Routing Problem with Variable\nSupply (MPRP-VS). This is a more general version of the Maximum-Profit Public\nTransportation Route Planning Problem, or simply Maximum-Profit Routing Problem\n(MPRP), introduced in \\cite{Armaselu-PETRA}. In this new version, the quantity\n$q_i(t)$ supplied at site $i$ is linearly increasing in time $t$, as opposed to\n\\cite{Armaselu-PETRA}, where the quantity is constant in time. Our main result\nis a $5.5 \\log{T} (1 + \\epsilon) (1 + \\frac{1}{1 + \\sqrt{m}})^2$ approximation\nalgorithm, where $T$ is the latest time window and $m$ is the number of\nvehicles used. In addition, we improve upon the MPRP algorithm in\n\\cite{Armaselu-PETRA} under certain conditions.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 23:48:18 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Armaselu", "Bogdan", ""]]}, {"id": "2007.09333", "submitter": "Lars Rohwedder", "authors": "Moritz Buchem, Lars Rohwedder, Tjark Vredeveld, Andreas Wiese", "title": "Additive Approximation Schemes for Load Balancing Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the concept of additive approximation schemes and\napply it to load balancing problems. Additive approximation schemes aim to find\na solution with an absolute error in the objective of at most $\\epsilon h$ for\nsome suitable parameter $h$. In the case that the parameter $h$ provides a\nlower bound an additive approximation scheme implies a standard multiplicative\napproximation scheme and can be much stronger when $h \\ll$ OPT. On the other\nhand, when no PTAS exists (or is unlikely to exist), additive approximation\nschemes can provide a different notion for approximation.\n  We consider the problem of assigning jobs to identical machines with lower\nand upper bounds for the loads of the machines. This setting generalizes\nproblems like makespan minimization, the Santa Claus problem (on identical\nmachines), and the envy-minimizing Santa Claus problem. For the last problem,\nin which the objective is to minimize the difference between the maximum and\nminimum load, the optimal objective value may be zero and hence it is NP-hard\nto obtain any multiplicative approximation guarantee. For this class of\nproblems we present additive approximation schemes for $h = p_{\\max}$, the\nmaximum processing time of the jobs.\n  Our technical contribution is two-fold. First, we introduce a new relaxation\nbased on integrally assigning slots to machines and fractionally assigning jobs\nto the slots (the slot-MILP). We identify structural properties of\n(near-)optimal solutions of the slot-MILP, which allow us to solve it\nefficiently, assuming that there are $O(1)$ different lower and upper bounds on\nthe machine loads (which is the relevant setting for the three problems\nmentioned above). The second technical contribution is a local-search based\nalgorithm which rounds a solution to the slot-MILP introducing an additive\nerror on the target load intervals of at most $\\epsilon\\cdot p_{\\max}$.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 05:24:47 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Buchem", "Moritz", ""], ["Rohwedder", "Lars", ""], ["Vredeveld", "Tjark", ""], ["Wiese", "Andreas", ""]]}, {"id": "2007.09556", "submitter": "Noah Stephens-Davidowitz", "authors": "Divesh Aggarwal, Zeyong Li, Noah Stephens-Davidowitz", "title": "A $2^{n/2}$-Time Algorithm for $\\sqrt{n}$-SVP and $\\sqrt{n}$-Hermite\n  SVP, and an Improved Time-Approximation Tradeoff for (H)SVP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a $2^{n/2+o(n)}$-time algorithm that finds a (non-zero) vector in a\nlattice $\\mathcal{L} \\subset \\mathbb{R}^n$ with norm at most\n$\\tilde{O}(\\sqrt{n})\\cdot \\min\\{\\lambda_1(\\mathcal{L}),\n\\det(\\mathcal{L})^{1/n}\\}$, where $\\lambda_1(\\mathcal{L})$ is the length of a\nshortest non-zero lattice vector and $\\det(\\mathcal{L})$ is the lattice\ndeterminant. Minkowski showed that $\\lambda_1(\\mathcal{L}) \\leq \\sqrt{n}\n\\det(\\mathcal{L})^{1/n}$ and that there exist lattices with\n$\\lambda_1(\\mathcal{L}) \\geq \\Omega(\\sqrt{n}) \\cdot \\det(\\mathcal{L})^{1/n}$,\nso that our algorithm finds vectors that are as short as possible relative to\nthe determinant (up to a polylogarithmic factor).\n  The main technical contribution behind this result is new analysis of (a\nsimpler variant of) an algorithm from arXiv:1412.7994, which was only\npreviously known to solve less useful problems. To achieve this, we rely\ncrucially on the ``reverse Minkowski theorem'' (conjectured by Dadush\narXiv:1606.06913 and proven by arXiv:1611.05979), which can be thought of as a\npartial converse to the fact that $\\lambda_1(\\mathcal{L}) \\leq \\sqrt{n}\n\\det(\\mathcal{L})^{1/n}$.\n  Previously, the fastest known algorithm for finding such a vector was the\n$2^{.802n + o(n)}$-time algorithm due to [Liu, Wang, Xu, and Zheng, 2011],\nwhich actually found a non-zero lattice vector with length $O(1) \\cdot\n\\lambda_1(\\mathcal{L})$. Though we do not show how to find lattice vectors with\nthis length in time $2^{n/2+o(n)}$, we do show that our algorithm suffices for\nthe most important application of such algorithms: basis reduction. In\nparticular, we show a modified version of Gama and Nguyen's slide-reduction\nalgorithm [Gama and Nguyen, STOC 2008], which can be combined with the\nalgorithm above to improve the time-length tradeoff for shortest-vector\nalgorithms in nearly all regimes, including the regimes relevant to\ncryptography.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 02:08:23 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Aggarwal", "Divesh", ""], ["Li", "Zeyong", ""], ["Stephens-Davidowitz", "Noah", ""]]}, {"id": "2007.09599", "submitter": "Huck Bennett", "authors": "Huck Bennett, Anindya De, Rocco A. Servedio, Emmanouil-Vasileios\n  Vlatakis-Gkaragkounis", "title": "Reconstructing weighted voting schemes from partial information about\n  their power indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of recent works [Goldberg 2006; O'Donnell and Servedio 2011; De,\nDiakonikolas, and Servedio 2017; De, Diakonikolas, Feldman, and Servedio 2014]\nhave considered the problem of approximately reconstructing an unknown weighted\nvoting scheme given information about various sorts of ``power indices'' that\ncharacterize the level of control that individual voters have over the final\noutcome. In the language of theoretical computer science, this is the problem\nof approximating an unknown linear threshold function (LTF) over $\\{-1, 1\\}^n$\ngiven some numerical measure (such as the function's $n$ ``Chow parameters,''\na.k.a. its degree-1 Fourier coefficients, or the vector of its $n$ Shapley\nindices) of how much each of the $n$ individual input variables affects the\noutcome of the function.\n  In this paper we consider the problem of reconstructing an LTF given only\npartial information about its Chow parameters or Shapley indices; i.e. we are\ngiven only the Chow parameters or the Shapley indices corresponding to a subset\n$S \\subseteq [n]$ of the $n$ input variables. A natural goal in this partial\ninformation setting is to find an LTF whose Chow parameters or Shapley indices\ncorresponding to indices in $S$ accurately match the given Chow parameters or\nShapley indices of the unknown LTF. We refer to this as the Partial Inverse\nPower Index Problem.\n  Our main results are a polynomial time algorithm for the\n($\\varepsilon$-approximate) Chow Parameters Partial Inverse Power Index Problem\nand a quasi-polynomial time algorithm for the ($\\varepsilon$-approximate)\nShapley Indices Partial Inverse Power Index Problem.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 05:54:29 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 21:03:39 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Bennett", "Huck", ""], ["De", "Anindya", ""], ["Servedio", "Rocco A.", ""], ["Vlatakis-Gkaragkounis", "Emmanouil-Vasileios", ""]]}, {"id": "2007.09634", "submitter": "Yanhao Wang", "authors": "Yanhao Wang, Michael Mathioudakis, Yuchen Li, Kian-Lee Tan", "title": "GRMR: Generalized Regret-Minimizing Representatives", "comments": "23 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extracting a small subset of representative tuples from a large database is\nan important task in multi-criteria decision making. The regret-minimizing set\n(RMS) problem is recently proposed for representative discovery from databases.\nSpecifically, for a set of tuples (points) in $d$ dimensions, an RMS problem\nfinds the smallest subset such that, for any possible ranking function, the\nrelative difference in scores between the top-ranked point in the subset and\nthe top-ranked point in the entire database is within a parameter $\\varepsilon\n\\in (0,1)$. Although RMS and its variations have been extensively investigated\nin the literature, existing approaches only consider the class of nonnegative\n(monotonic) linear functions for ranking, which have limitations in modeling\nuser preferences and decision-making processes.\n  To address this issue, we define the generalized regret-minimizing\nrepresentative (GRMR) problem that extends RMS by taking into account all\nlinear functions including non-monotonic ones with negative weights. For\ntwo-dimensional databases, we propose an optimal algorithm for GRMR via a\ntransformation into the shortest cycle problem in a directed graph. Since GRMR\nis proven to be NP-hard even in three dimensions, we further develop a\npolynomial-time heuristic algorithm for GRMR on databases in arbitrary\ndimensions. Finally, we conduct extensive experiments on real and synthetic\ndatasets to confirm the efficiency, effectiveness, and scalability of our\nproposed algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 09:27:48 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wang", "Yanhao", ""], ["Mathioudakis", "Michael", ""], ["Li", "Yuchen", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "2007.09640", "submitter": "Amos Korman", "authors": "Amos Korman, Yuval Emek, Simon Collet, Aya Goldshtein, Yossi Yovel", "title": "Exploitation of Multiple Replenishing Resources with Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an optimization problem in which a (single) bat aims to exploit\nthe nectar in a set of $n$ cacti with the objective of maximizing the expected\ntotal amount of nectar it drinks. Each cactus $i \\in [n]$ is characterized by a\nparameter $r_{i} > 0$ that determines the rate in which nectar accumulates in\n$i$. In every round, the bat can visit one cactus and drink all the nectar\naccumulated there since its previous visit. Furthermore, competition with other\nbats, that may also visit some cacti and drink their nectar, is modeled by\nmeans of a stochastic process in which cactus $i$ is emptied in each round\n(independently) with probability $0 < s_i < 1$. Our attention is restricted to\npurely-stochastic strategies that are characterized by a probability vector\n$(p_1, \\ldots, p_n)$ determining the probability $p_i$ that the bat visits\ncactus $i$ in each round. We prove that for every $\\epsilon > 0$, there exists\na purely-stochastic strategy that approximates the optimal purely-stochastic\nstrategy to within a multiplicative factor of $1 + \\epsilon$, while exploiting\nonly a small core of cacti. Specifically, we show that it suffices to include\nat most $\\frac{2 (1 - \\sigma)}{\\epsilon \\cdot \\sigma}$ cacti in the core, where\n$\\sigma = \\min_{i \\in [n]} s_{i}$. We also show that this upper bound on core\nsize is asymptotically optimal as a core of a significantly smaller size cannot\nprovide a $(1 + \\epsilon)$-approximation of the optimal purely-stochastic\nstrategy. This means that when the competition is more intense (i.e., $\\sigma$\nis larger), a strategy based on exploiting smaller cores will be favorable.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 10:02:30 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Korman", "Amos", ""], ["Emek", "Yuval", ""], ["Collet", "Simon", ""], ["Goldshtein", "Aya", ""], ["Yovel", "Yossi", ""]]}, {"id": "2007.09768", "submitter": "Edin Husic", "authors": "Edin Husic and Tim Roughgarden", "title": "FPT Algorithms for Finding Dense Subgraphs in $c$-Closed Graphs", "comments": "29 pages, 15 pages main body, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense subgraph detection is a fundamental problem in network analysis for\nwhich few worst-case guarantees are known, motivating its study through the\nlens of fixed-parameter tractability. But for what parameter? Recent work has\nproposed parameterizing graphs by their degree of triadic closure, with a\n$c$-closed graph defined as one in which every vertex pair with at least $c$\ncommon neighbors are themselves connected by an edge. The special case of\nenumerating all maximal cliques (and hence computing a maximum clique) of a\n$c$-closed graph is known to be fixed-parameter tractable with respect to $c$\n(Fox et al., SICOMP 2020).\n  In network analysis, sufficiently dense subgraphs are typically as notable\nand meaningful as cliques. We investigate the fixed-parameter tractability\n(with respect to $c$) of optimization and enumeration in $c$-closed graphs, for\nseveral notions of dense subgraphs. We focus on graph families that are the\ncomplements of the most well-studied notions of sparse graphs, including graphs\nwith bounded degree, bounded treewidth, or bounded degeneracy, and provide\nfixed-parameter tractable enumeration and optimization algorithms for these\nfamilies. To go beyond the special case of maximal cliques, we use a new\ncombinatorial bound (generalizing the Moon-Moser theorem); new techniques for\nexploiting the $c$-closed condition; and more sophisticated enumeration\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 20:05:59 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 17:10:49 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2021 14:40:25 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Husic", "Edin", ""], ["Roughgarden", "Tim", ""]]}, {"id": "2007.09793", "submitter": "R Jaberi", "authors": "Raed Jaberi", "title": "$2$-blocks in strongly biconnected directed graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A directed graph $G=(V,E)$ is called strongly biconnected if $G$ is strongly\nconnected and the underlying graph of $G$ is biconnected. A strongly\nbiconnected component of a strongly connected graph $G=(V,E)$ is a maximal\nvertex subset $L\\subseteq V$ such that the induced subgraph on $L$ is strongly\nbiconnected. Let $G=(V,E)$ be a strongly biconnected directed graph. A\n$2$-edge-biconnected block in $G$ is a maximal vertex subset $U\\subseteq V$\nsuch that for any two distict vertices $v,w \\in U$ and for each edge $b\\in E$,\nthe vertices $v,w$ are in the same strongly biconnected components of\n$G\\setminus\\left\\lbrace b\\right\\rbrace $. A $2$-strong-biconnected block in $G$\nis a maximal vertex subset $U\\subseteq V$ of size at least $2$ such that for\nevery pair of distinct vertices $v,w\\in U$ and for every vertex $z\\in\nV\\setminus\\left\\lbrace v,w \\right\\rbrace $, the vertices $v$ and $w$ are in the\nsame strongly biconnected component of $G\\setminus \\left\\lbrace v,w\n\\right\\rbrace $. In this paper we study $2$-edge-biconnected blocks and\n$2$-strong biconnected blocks.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 21:59:52 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Jaberi", "Raed", ""]]}, {"id": "2007.09816", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang, Varsha Dani, Thomas P. Hayes, Seth Pettie", "title": "The Energy Complexity of BFS in Radio Networks", "comments": "To appear in PODC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a model of energy complexity in Radio Networks in which\ntransmitting or listening on the channel costs one unit of energy and\ncomputation is free. This simplified model captures key aspects of\nbattery-powered sensors: that battery life is most influenced by transceiver\nusage, and that at low transmission powers, the actual cost of transmitting and\nlistening are very similar.\n  The energy complexity of tasks in single-hop networks is well understood.\nRecent work of Chang et al. considered energy complexity in multi-hop networks\nand showed that $\\mathsf{Broadcast}$ admits an energy-efficient protocol, by\nwhich we mean each of the $n$ nodes in the network spends\n$O(\\text{polylog}(n))$ energy. This work left open the strange possibility that\nall natural problems in multi-hop networks might admit such an energy-efficient\nsolution.\n  In this paper we prove that the landscape of energy complexity is rich enough\nto support a multitude of problem complexities. Whereas $\\mathsf{Broadcast}$\ncan be solved by an energy-efficient protocol, exact computation of\n$\\mathsf{Diameter}$ cannot, requiring $\\Omega(n)$ energy. Our main result is\nthat $\\mathsf{Breadth First Search}$ has sub-polynomial energy complexity at\nmost $2^{O(\\sqrt{\\log n\\log\\log n})}=n^{o(1)}$; whether it admits an efficient\n$O(\\text{polylog}(n))$-energy protocol is an open problem.\n  Our main algorithm involves recursively solving a generalized BFS problem on\na cluster graph introduced by Miller, Peng, and Xu. In this application, we\nmake crucial use of a close relationship between distances in this cluster\ngraph, and distances in the original network. This relationship is new and may\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 23:26:59 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Dani", "Varsha", ""], ["Hayes", "Thomas P.", ""], ["Pettie", "Seth", ""]]}, {"id": "2007.09849", "submitter": "N.S Narayanaswamy", "authors": "S Anil Kumar and N S Narayanaswamy", "title": "A polynomial time 12-approximation algorithm for restricted Santa Claus\n  problem", "comments": "This paper first obtained the 12-approximation algorithm and the work\n  is under review. Subsequently, we thought we had our technique to obtain a\n  stronger result. A reader pointed out a bug in the whole technique. The bug\n  is now fixed and the original result is submitted to arxiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the restricted case of the problem and improve the\ncurrent best approximation ratio by presenting a polynomial time\n12-approximation algorithm using linear programming and semi-definite\nprogramming. Our algorithm starts by solving the configuration LP and uses the\noptimum value to get a 12-gap instance. This is then followed by the well-known\nclustering technique of Bansal and Sviridenko\\cite{bansal}. We then apply the\nanalysis of Asadpour \\textit{et al.} \\cite{AFS,AFS2} to show that the clustered\ninstance has an integer solution which is at least $\\frac{1}{6}$ times the best\npossible value, which was computed by solving the configuration LP. To find\nthis solution, we formulate a problem called the Extended Assignment Problem,\nand formulate it as an LP. We then, show that the associated polytope is\nintegral and gives us an fractional solution of value at least $\\frac{1}{6}$\ntimes the optimum. From this solution we find a solution to a new quadratic\nprogram that we introduce to select one machine from each cluster, and then we\nshow that the resulting instance has an Assignment LP fractional solution of\nvalue at least $\\frac{1}{6}$ times the optimum. We then use the well known\nrounding technique due to Bezakova and Dani \\cite{bezakova} on the 12-gap\ninstance to get our 12-approximate solution.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 02:40:40 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 11:11:28 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Kumar", "S Anil", ""], ["Narayanaswamy", "N S", ""]]}, {"id": "2007.09890", "submitter": "Ali Vakilian", "authors": "Simin Liu, Tianrui Liu, Ali Vakilian, Yulin Wan, David P. Woodruff", "title": "Learning the Positions in CountSketch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider sketching algorithms which first quickly compress data by\nmultiplication with a random sketch matrix, and then apply the sketch to\nquickly solve an optimization problem, e.g., low rank approximation. In the\nlearning-based sketching paradigm proposed by Indyk et al. [2019], the sketch\nmatrix is found by choosing a random sparse matrix, e.g., the CountSketch, and\nthen updating the values of the non-zero entries by running gradient descent on\na training data set. Despite the growing body of work on this paradigm, a\nnoticeable omission is that the locations of the non-zero entries of previous\nalgorithms were fixed, and only their values were learned. In this work we\npropose the first learning algorithm that also optimizes the locations of the\nnon-zero entries. We show this algorithm gives better accuracy for low rank\napproximation than previous work, and apply it to other problems such as\n$k$-means clustering for the first time. We show that our algorithm is provably\nbetter in the spiked covariance model and for Zipfian matrices. We also show\nthe importance of the sketch monotonicity property for combining learned\nsketches. Our empirical results show the importance of optimizing not only the\nvalues of the non-zero entries but also their positions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 05:06:29 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 01:43:01 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 07:30:46 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Simin", ""], ["Liu", "Tianrui", ""], ["Vakilian", "Ali", ""], ["Wan", "Yulin", ""], ["Woodruff", "David P.", ""]]}, {"id": "2007.09895", "submitter": "Shyam Narayanan", "authors": "Shyam Narayanan", "title": "On Distribution Testing in the Conditional Sampling Model", "comments": "53 pages. Added result on monotonicity testing. Abridged version to\n  appear in Symposium on Discrete Algorithms (SODA), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been significant work studying distribution testing under\nthe Conditional Sampling model. In this model, a query specifies a subset $S$\nof the domain, and the output received is a sample drawn from the distribution\nconditioned on being in $S$. In this paper, we improve query complexity bounds\nfor several classic distribution testing problems in this model.\n  First, we prove that tolerant uniformity testing in the conditional sampling\nmodel can be solved using $\\tilde{O}(\\varepsilon^{-2})$ queries, which is\noptimal and improves upon the $\\tilde{O}(\\varepsilon^{-20})$-query algorithm of\nCanonne et al. [CRS15]. This bound even holds under a restricted version of the\nconditional sampling model called the Pair Conditional Sampling model. Next, we\nprove that tolerant identity testing in the conditional sampling model can be\nsolved in $\\tilde{O}(\\varepsilon^{-4})$ queries, which is the first known bound\nindependent of the support size of the distribution for this problem. Next, we\nuse our algorithm for tolerant uniformity testing to get an\n$\\tilde{O}(\\varepsilon^{-4})$-query algorithm for monotonicity testing in the\nconditional sampling model, improving on the\n$\\tilde{O}(\\varepsilon^{-22})$-query algorithm of Canonne [Can15]. Finally, we\nstudy (non-tolerant) identity testing under the pair conditional sampling\nmodel, and provide a tight bound of $\\tilde{\\Theta}(\\sqrt{\\log N} \\cdot\n\\varepsilon^{-2})$ for the query complexity, where the domain of the\ndistribution has size $N$. This improves upon both the known upper and lower\nbounds in [CRS15].\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 05:51:35 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 04:23:50 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Narayanan", "Shyam", ""]]}, {"id": "2007.10095", "submitter": "Simona Rombo", "authors": "Mario Randazzo, Simona E. Rombo", "title": "A Big Data Approach for Sequences Indexing on the Cloud via Burrows\n  Wheeler Transform", "comments": "Accepted at HELPLINE@ECAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing sequence data is important in the context of Precision Medicine,\nwhere large amounts of ``omics'' data have to be daily collected and analyzed\nin order to categorize patients and identify the most effective therapies. Here\nwe propose an algorithm for the computation of Burrows Wheeler transform\nrelying on Big Data technologies, i.e., Apache Spark and Hadoop. Our approach\nis the first that distributes the index computation and not only the input\ndataset, allowing to fully benefit of the available cloud resources.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 13:39:57 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Randazzo", "Mario", ""], ["Rombo", "Simona E.", ""]]}, {"id": "2007.10137", "submitter": "Kirill Simonov", "authors": "Sayan Bandyapadhyay, Fedor V. Fomin, Kirill Simonov", "title": "On Coresets for Fair Clustering in Metric and Euclidean Spaces and Their\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fair clustering is a constrained variant of clustering where the goal is to\npartition a set of colored points, such that the fraction of points of any\ncolor in every cluster is more or less equal to the fraction of points of this\ncolor in the dataset. This variant was recently introduced by Chierichetti et\nal. [NeurIPS, 2017] in a seminal work and became widely popular in the\nclustering literature. In this paper, we propose a new construction of coresets\nfor fair clustering based on random sampling. The new construction allows us to\nobtain the first coreset for fair clustering in general metric spaces. For\nEuclidean spaces, we obtain the first coreset whose size does not depend\nexponentially on the dimension. Our coreset results solve open questions\nproposed by Schmidt et al. [WAOA, 2019] and Huang et al. [NeurIPS, 2019]. The\nnew coreset construction helps to design several new approximation and\nstreaming algorithms. In particular, we obtain the first true\nconstant-approximation algorithm for metric fair clustering, whose running time\nis fixed-parameter tractable (FPT). In the Euclidean case, we derive the first\n$(1+\\epsilon)$-approximation algorithm for fair clustering whose time\ncomplexity is near-linear and does not depend exponentially on the dimension of\nthe space. Besides, our coreset construction scheme is fairly general and gives\nrise to coresets for a wide range of constrained clustering problems. This\nleads to improved constant-approximations for these problems in general metrics\nand near-linear time $(1+\\epsilon)$-approximations in the Euclidean metric.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 14:15:23 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Bandyapadhyay", "Sayan", ""], ["Fomin", "Fedor V.", ""], ["Simonov", "Kirill", ""]]}, {"id": "2007.10237", "submitter": "Giosu\\'e Lo Bosco", "authors": "Domenico Amato, Giosu\\'e Lo Bosco, Raffaele Giancarlo", "title": "Learning from Data to Speed-up Sorted Table Search Procedures:\n  Methodology and Practical Guidelines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorted Table Search Procedures are the quintessential query-answering tool,\nwith widespread usage that now includes also Web Applications, e.g, Search\nEngines (Google Chrome) and ad Bidding Systems (AppNexus). Speeding them up, at\nvery little cost in space, is still a quite significant achievement. Here we\nstudy to what extend Machine Learning Techniques can contribute to obtain such\na speed-up via a systematic experimental comparison of known efficient\nimplementations of Sorted Table Search procedures, with different Data Layouts,\nand their Learned counterparts developed here. We characterize the scenarios in\nwhich those latter can be profitably used with respect to the former,\naccounting for both CPU and GPU computing. Our approach contributes also to the\nstudy of Learned Data Structures, a recent proposal to improve the time/space\nperformance of fundamental Data Structures, e.g., B-trees, Hash Tables, Bloom\nFilters. Indeed, we also formalize an Algorithmic Paradigm of Learned\nDichotomic Sorted Table Search procedures that naturally complements the\nLearned one proposed here and that characterizes most of the known Sorted Table\nSearch Procedures as having a \"learning phase\" that approximates Simple Linear\nRegression.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:26:54 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 16:10:43 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 11:56:44 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Amato", "Domenico", ""], ["Bosco", "Giosu\u00e9 Lo", ""], ["Giancarlo", "Raffaele", ""]]}, {"id": "2007.10253", "submitter": "Tongyang Li", "authors": "Chenyi Zhang, Jiaqi Leng, Tongyang Li", "title": "Quantum Algorithms for Escaping from Saddle Points", "comments": "54 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of quantum algorithms for escaping from saddle points\nwith provable guarantee. Given a function $f\\colon\\mathbb{R}^{n}\\to\\mathbb{R}$,\nour quantum algorithm outputs an $\\epsilon$-approximate second-order stationary\npoint using $\\tilde{O}(\\log^{2} n/\\epsilon^{1.75})$ queries to the quantum\nevaluation oracle (i.e., the zeroth-order oracle). Compared to the classical\nstate-of-the-art algorithm by Jin et al. with $\\tilde{O}(\\log^{6}\nn/\\epsilon^{1.75})$ queries to the gradient oracle (i.e., the first-order\noracle), our quantum algorithm is polynomially better in terms of $n$ and\nmatches its complexity in terms of $1/\\epsilon$. Our quantum algorithm is built\nupon two techniques: First, we replace the classical perturbations in gradient\ndescent methods by simulating quantum wave equations, which constitutes the\npolynomial speedup in $n$ for escaping from saddle points. Second, we show how\nto use a quantum gradient computation algorithm due to Jordan to replace the\nclassical gradient queries by quantum evaluation queries with the same\ncomplexity. Finally, we also perform numerical experiments that support our\nquantum speedup.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:42:53 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 16:16:28 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Zhang", "Chenyi", ""], ["Leng", "Jiaqi", ""], ["Li", "Tongyang", ""]]}, {"id": "2007.10254", "submitter": "Richard Peng", "authors": "Richard Peng, Santosh Vempala", "title": "Solving Sparse Linear Systems Faster than Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can linear systems be solved faster than matrix multiplication? While there\nhas been remarkable progress for the special cases of graph structured linear\nsystems, in the general setting, the bit complexity of solving an $n \\times n$\nlinear system $Ax=b$ is $\\tilde{O}(n^\\omega)$, where $\\omega < 2.372864$ is the\nmatrix multiplication exponent. Improving on this has been an open problem even\nfor sparse linear systems with poly$(n)$ condition number.\n  In this paper, we present an algorithm that solves linear systems in sparse\nmatrices asymptotically faster than matrix multiplication for any $\\omega > 2$.\nThis speedup holds for any input matrix $A$ with $o(n^{\\omega\n-1}/\\log(\\kappa(A)))$ non-zeros, where $\\kappa(A)$ is the condition number of\n$A$. For poly$(n)$-conditioned matrices with $\\tilde{O}(n)$ nonzeros, and the\ncurrent value of $\\omega$, the bit complexity of our algorithm to solve to\nwithin any $1/\\text{poly}(n)$ error is $O(n^{2.331645})$.\n  Our algorithm can be viewed as an efficient, randomized implementation of the\nblock Krylov method via recursive low displacement rank factorizations. It is\ninspired by the algorithm of [Eberly et al. ISSAC `06 `07] for inverting\nmatrices over finite fields. In our analysis of numerical stability, we develop\nmatrix anti-concentration techniques to bound the smallest eigenvalue and the\nsmallest gap in eigenvalues of semi-random matrices.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:44:18 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 07:03:08 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Peng", "Richard", ""], ["Vempala", "Santosh", ""]]}, {"id": "2007.10307", "submitter": "Arvind Mahankali", "authors": "Arvind V. Mahankali (1), David P. Woodruff (1) ((1) Carnegie Mellon\n  University)", "title": "Optimal $\\ell_1$ Column Subset Selection and a Fast PTAS for Low Rank\n  Approximation", "comments": "To appear in SODA 2021. Changes: (1) Fixed errors in hardness proof\n  for constrained $\\ell_1$ low rank approximation. (2) Simplified analysis of\n  column subset selection algorithm. (3) Improved runtime of\n  $\\text{poly}(k)$-approximation algorithm with output rank $k$ from\n  $2^{O(k\\log k)} + \\text{poly}(nd)$ to $\\text{poly}(nd)$. Results are\n  unchanged aside from (3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of entrywise $\\ell_1$ low rank approximation. We give\nthe first polynomial time column subset selection-based $\\ell_1$ low rank\napproximation algorithm sampling $\\tilde{O}(k)$ columns and achieving an\n$\\tilde{O}(k^{1/2})$-approximation for any $k$, improving upon the previous\nbest $\\tilde{O}(k)$-approximation and matching a prior lower bound for column\nsubset selection-based $\\ell_1$-low rank approximation which holds for any\n$\\text{poly}(k)$ number of columns. We extend our results to obtain tight upper\nand lower bounds for column subset selection-based $\\ell_p$ low rank\napproximation for any $1 < p < 2$, closing a long line of work on this problem.\n  We next give a $(1 + \\varepsilon)$-approximation algorithm for entrywise\n$\\ell_p$ low rank approximation, for $1 \\leq p < 2$, that is not a column\nsubset selection algorithm. First, we obtain an algorithm which, given a matrix\n$A \\in \\mathbb{R}^{n \\times d}$, returns a rank-$k$ matrix $\\hat{A}$ in\n$2^{\\text{poly}(k/\\varepsilon)} + \\text{poly}(nd)$ running time such that:\n$$\\|A - \\hat{A}\\|_p \\leq (1 + \\varepsilon) \\cdot OPT +\n\\frac{\\varepsilon}{\\text{poly}(k)}\\|A\\|_p$$ where $OPT = \\min_{A_k \\text{ rank\n}k} \\|A - A_k\\|_p$. Using this algorithm, in the same running time we give an\nalgorithm which obtains error at most $(1 + \\varepsilon) \\cdot OPT$ and outputs\na matrix of rank at most $3k$ -- these algorithms significantly improve upon\nall previous $(1 + \\varepsilon)$- and $O(1)$-approximation algorithms for the\n$\\ell_p$ low rank approximation problem, which required at least\n$n^{\\text{poly}(k/\\varepsilon)}$ or $n^{\\text{poly}(k)}$ running time, and\neither required strong bit complexity assumptions (our algorithms do not) or\nhad bicriteria rank $3k$. Finally, we show hardness results which nearly match\nour $2^{\\text{poly}(k)} + \\text{poly}(nd)$ running time and the above additive\nerror guarantee.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:50:30 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 07:22:43 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Mahankali", "Arvind V.", ""], ["Woodruff", "David P.", ""]]}, {"id": "2007.10470", "submitter": "Ariel Kulik", "authors": "Yaron Fairstein, Ariel Kulik, Hadas Shachnai", "title": "Modular and Submodular Optimization with Multiple Knapsack Constraints\n  via Fractional Grouping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multiple knapsack constraint over a set of items is defined by a set of\nbins of arbitrary capacities, and a weight for each of the items. An assignment\nfor the constraint is an allocation of subsets of items to the bins which\nadheres to bin capacities. In this paper we present a unified algorithm that\nyields efficient approximations for a wide class of submodular and modular\noptimization problems involving multiple knapsack constraints. One notable\nexample is a polynomial time approximation scheme for Multiple-Choice Multiple\nKnapsack, improving upon the best known ratio of $2$. Another example is\nNon-monotone Submodular Multiple Knapsack, for which we obtain a\n$(0.385-\\varepsilon)$-approximation, matching the best known ratio for a single\nknapsack constraint. The robustness of our algorithm is achieved by applying a\nnovel fractional variant of the classical linear grouping technique, which is\nof independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 20:56:13 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 09:06:20 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 08:41:42 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Fairstein", "Yaron", ""], ["Kulik", "Ariel", ""], ["Shachnai", "Hadas", ""]]}, {"id": "2007.10537", "submitter": "Henry Xia", "authors": "Guyslain Naves, Bruce Shepherd, Henry Xia", "title": "Maximum Weight Disjoint Paths in Outerplanar Graphs via Single-Tree Cut\n  Approximators", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since 1997 there has been a steady stream of advances for the maximum\ndisjoint paths problem. Achieving tractable results has usually required\nfocusing on relaxations such as: (i) to allow some bounded edge congestion in\nsolutions, (ii) to only consider the unit weight (cardinality) setting, (iii)\nto only require fractional routability of the selected demands (the\nall-or-nothing flow setting). For the general form (no congestion, general\nweights, integral routing) of edge-disjoint paths ({\\sc edp}) even the case of\nunit capacity trees which are stars generalizes the maximum matching problem\nfor which Edmonds provided an exact algorithm. For general capacitated trees,\nGarg, Vazirani, Yannakakis showed the problem is APX-Hard and Chekuri, Mydlarz,\nShepherd provided a $4$-approximation. This is essentially the only setting\nwhere a constant approximation is known for the general form of \\textsc{edp}.\nWe extend their result by giving a constant-factor approximation algorithm for\ngeneral-form \\textsc{edp} in outerplanar graphs. A key component for the\nalgorithm is to find a {\\em single-tree} $O(1)$ cut approximator for\nouterplanar graphs. Previously $O(1)$ cut approximators were only known via\ndistributions on trees and these were based implicitly on the results of Gupta,\nNewman, Rabinovich and Sinclair for distance tree embeddings combined with\nresults of Anderson and Feige.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 00:26:37 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2021 01:40:31 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Naves", "Guyslain", ""], ["Shepherd", "Bruce", ""], ["Xia", "Henry", ""]]}, {"id": "2007.10545", "submitter": "Sahil Singla", "authors": "Anupam Gupta, Ravishankar Krishnaswamy, Amit Kumar, and Sahil Singla", "title": "Online Carpooling using Expander Decompositions", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online carpooling problem: given $n$ vertices, a sequence of\nedges arrive over time. When an edge $e_t = (u_t, v_t)$ arrives at time step\n$t$, the algorithm must orient the edge either as $v_t \\rightarrow u_t$ or $u_t\n\\rightarrow v_t$, with the objective of minimizing the maximum discrepancy of\nany vertex, i.e., the absolute difference between its in-degree and out-degree.\nEdges correspond to pairs of persons wanting to ride together, and orienting\ndenotes designating the driver. The discrepancy objective then corresponds to\nevery person driving close to their fair share of rides they participate in.\n  In this paper, we design efficient algorithms which can maintain\npolylog$(n,T)$ maximum discrepancy (w.h.p) over any sequence of $T$ arrivals,\nwhen the arriving edges are sampled independently and uniformly from any given\ngraph $G$. This provides the first polylogarithmic bounds for the online\n(stochastic) carpooling problem. Prior to this work, the best known bounds were\n$O(\\sqrt{n \\log n})$-discrepancy for any adversarial sequence of arrivals, or\n$O(\\log\\!\\log n)$-discrepancy bounds for the stochastic arrivals when $G$ is\nthe complete graph.\n  The technical crux of our paper is in showing that the simple greedy\nalgorithm, which has provably good discrepancy bounds when the arriving edges\nare drawn uniformly at random from the complete graph, also has polylog\ndiscrepancy when $G$ is an expander graph. We then combine this with known\nexpander-decomposition results to design our overall algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 01:17:11 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 22:36:03 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Gupta", "Anupam", ""], ["Krishnaswamy", "Ravishankar", ""], ["Kumar", "Amit", ""], ["Singla", "Sahil", ""]]}, {"id": "2007.10592", "submitter": "Venkatesan Guruswami", "authors": "Venkatesan Guruswami and Johan H{\\aa}stad", "title": "Explicit two-deletion codes with redundancy matching the existential\n  bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DM cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an explicit construction of length-$n$ binary codes capable of\ncorrecting the deletion of two bits that have size $2^n/n^{4+o(1)}$. This\nmatches up to lower order terms the existential result, based on an inefficient\ngreedy choice of codewords, that guarantees such codes of size\n$\\Omega(2^n/n^4)$. Our construction is based on augmenting the classic\nVarshamov-Tenengolts construction of single deletion codes with additional\ncheck equations. We also give an explicit construction of binary codes of size\n$\\Omega(2^n/n^{3+o(1)})$ that can be list decoded from two deletions using\nlists of size two. Previously, even the existence of such codes was not clear.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 04:39:34 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["H\u00e5stad", "Johan", ""]]}, {"id": "2007.10622", "submitter": "Haotian Jiang", "authors": "Nikhil Bansal, Haotian Jiang, Raghu Meka, Sahil Singla, Makrand Sinha", "title": "Online Discrepancy Minimization for Stochastic Arrivals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the stochastic online vector balancing problem, vectors\n$v_1,v_2,\\ldots,v_T$ chosen independently from an arbitrary distribution in\n$\\mathbb{R}^n$ arrive one-by-one and must be immediately given a $\\pm$ sign.\nThe goal is to keep the norm of the discrepancy vector, i.e., the signed\nprefix-sum, as small as possible for a given target norm.\n  We consider some of the most well-known problems in discrepancy theory in the\nabove online stochastic setting, and give algorithms that match the known\noffline bounds up to $\\mathsf{polylog}(nT)$ factors. This substantially\ngeneralizes and improves upon the previous results of Bansal, Jiang, Singla,\nand Sinha (STOC' 20). In particular, for the Koml\\'{o}s problem where\n$\\|v_t\\|_2\\leq 1$ for each $t$, our algorithm achieves $\\tilde{O}(1)$\ndiscrepancy with high probability, improving upon the previous\n$\\tilde{O}(n^{3/2})$ bound. For Tusn\\'{a}dy's problem of minimizing the\ndiscrepancy of axis-aligned boxes, we obtain an $O(\\log^{d+4} T)$ bound for\narbitrary distribution over points. Previous techniques only worked for product\ndistributions and gave a weaker $O(\\log^{2d+1} T)$ bound. We also consider the\nBanaszczyk setting, where given a symmetric convex body $K$ with Gaussian\nmeasure at least $1/2$, our algorithm achieves $\\tilde{O}(1)$ discrepancy with\nrespect to the norm given by $K$ for input distributions with sub-exponential\ntails.\n  Our key idea is to introduce a potential that also enforces constraints on\nhow the discrepancy vector evolves, allowing us to maintain certain\nanti-concentration properties. For the Banaszczyk setting, we further enhance\nthis potential by combining it with ideas from generic chaining. Finally, we\nalso extend these results to the setting of online multi-color discrepancy.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 06:58:00 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Bansal", "Nikhil", ""], ["Jiang", "Haotian", ""], ["Meka", "Raghu", ""], ["Singla", "Sahil", ""], ["Sinha", "Makrand", ""]]}, {"id": "2007.10790", "submitter": "Or Zamir", "authors": "Or Zamir", "title": "Breaking the $2^n$ barrier for 5-coloring and 6-coloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coloring problem (i.e., computing the chromatic number of a graph) can be\nsolved in $O^*(2^n)$ time, as shown by Bj\\\"orklund, Husfeldt and Koivisto in\n2009. For $k=3,4$, better algorithms are known for the $k$-coloring problem.\n$3$-coloring can be solved in $O(1.33^n)$ time (Beigel and Eppstein, 2005) and\n$4$-coloring can be solved in $O(1.73^n)$ time (Fomin, Gaspers and Saurabh,\n2007). Surprisingly, for $k>4$ no improvements over the general $O^*(2^n)$ are\nknown. We show that both $5$-coloring and $6$-coloring can also be solved in\n$O\\left(\\left(2-\\varepsilon\\right)^n\\right)$ time for some $\\varepsilon>0$. As\na crucial step, we obtain an exponential improvement for computing the\nchromatic number of a very large family of graphs. In particular, for any\nconstants $\\Delta,\\alpha>0$, the chromatic number of graphs with at least\n$\\alpha\\cdot n$ vertices of degree at most $\\Delta$ can be computed in\n$O\\left(\\left(2-\\varepsilon\\right)^n\\right)$ time, for some $\\varepsilon =\n\\varepsilon_{\\Delta,\\alpha} > 0$. This statement generalizes previous results\nfor bounded-degree graphs (Bj\\\"orklund, Husfeldt, Kaski, and Koivisto, 2010)\nand graphs with bounded average degree (Golovnev, Kulikov and Mihajilin, 2016).\nWe generalize the aforementioned statement to List Coloring, for which no\nprevious improvements are known even for the case bounded-degree graphs.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 13:33:25 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 22:56:16 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Zamir", "Or", ""]]}, {"id": "2007.10805", "submitter": "Suneel Sarswat", "authors": "Suneel Sarswat and Abhishek Kr Singh", "title": "Formally Verified Trades in Financial Markets", "comments": "Aceepted in ICFEM 2020. arXiv admin note: substantial text overlap\n  with arXiv:1907.07885", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS cs.GT q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a formal framework for analyzing trades in financial markets.\nThese days, all big exchanges use computer algorithms to match buy and sell\nrequests and these algorithms must abide by certain regulatory guidelines. For\nexample, market regulators enforce that a matching produced by exchanges should\nbe fair, uniform and individual rational. To verify these properties of trades,\nwe first formally define these notions in a theorem prover and then develop\nmany important results about matching demand and supply. Finally, we use this\nframework to verify properties of two important classes of double sided auction\nmechanisms. All the definitions and results presented in this paper are\ncompletely formalized in the Coq proof assistant without adding any additional\naxioms to it.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 10:03:22 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Sarswat", "Suneel", ""], ["Singh", "Abhishek Kr", ""]]}, {"id": "2007.10824", "submitter": "David Harris", "authors": "David G. Harris, Vladimir Kolmogorov", "title": "Parameter estimation for Gibbs distributions", "comments": "This is a significantly extended version of a paper \"A Faster\n  Approximation Algorithm for the Gibbs Partition Function\" (arXiv:1608.04223),\n  which was published in COLT 2018. It covers many additional topics; most\n  importantly, algorithms to estimate counts and algorithm specialized for\n  integer-valued distributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider \\emph{Gibbs distributions}, which are families of probability\ndistributions over a discrete space $\\Omega$ with probability mass function of\nthe form $\\mu^\\Omega_\\beta(\\omega) \\propto e^{\\beta H(\\omega)}$ for $\\beta$ in\nan interval $[\\beta_{\\min}, \\beta_{\\max}]$ and $H( \\omega ) \\in \\{0 \\} \\cup [1,\nn]$. The \\emph{partition function} is the normalization factor\n$Z(\\beta)=\\sum_{\\omega \\in\\Omega}e^{\\beta H(\\omega)}$.\n  Two important parameters of these distributions are the log partition ratio\n$q = \\log \\tfrac{Z(\\beta_{\\max})}{Z(\\beta_{\\min})}$ and the counts $c_x =\n|H^{-1}(x)|$. These are correlated with system parameters in a number of\nphysical applications and sampling algorithms. Our first main result is to\nestimate the counts $c_x$ using roughly $\\tilde O( \\frac{q}{\\varepsilon^2})$\nsamples for general Gibbs distributions and $\\tilde O(\n\\frac{n^2}{\\varepsilon^2} )$ samples for integer-valued distributions (ignoring\nsome second-order terms and parameters), and we show this is optimal up to\nlogarithmic factors. We illustrate with improved algorithms for counting\nconnected subgraphs and perfect matchings in a graph.\n  We develop a key subroutine to estimate the partition function $Z$.\nSpecifically, it generates a data structure to estimate $Z(\\beta)$ for\n\\emph{all} values $\\beta$, without further samples. Constructing the data\nstructure requires $O(\\frac{q \\log n}{\\varepsilon^2})$ samples for general\nGibbs distributions and $O(\\frac{n^2 \\log n}{\\varepsilon^2} + n \\log q)$\nsamples for integer-valued distributions. This improves over a prior algorithm\nof Huber (2015) which computes a single point estimate $Z(\\beta_\\max)$ using\n$O( q \\log n( \\log q + \\log \\log n + \\varepsilon^{-2}))$ samples. We show\nmatching lower bounds, demonstrating that this complexity is optimal as a\nfunction of $n$ and $q$ up to logarithmic terms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 11:27:08 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 13:03:27 GMT"}, {"version": "v3", "created": "Sat, 30 Jan 2021 20:54:19 GMT"}, {"version": "v4", "created": "Mon, 17 May 2021 20:41:26 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Harris", "David G.", ""], ["Kolmogorov", "Vladimir", ""]]}, {"id": "2007.10829", "submitter": "Muhammad Ahmad Kaleem", "authors": "Ahmad Kaleem and Ahsan Kaleem", "title": "On Algorithms for Solving the Rubik's Cube", "comments": "arXiv admin note: text overlap with arXiv:1106.5736 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a novel algorithm and its three variations for\nsolving the Rubik's cube more efficiently. This algorithm can be used to solve\nthe complete $n \\times n \\times n$ cube in $O(\\frac{n^2}{\\log n})$ moves. This\nalgorithm can also be useful in certain cases for speedcubers. We will prove\nthat our algorithm always works and then perform a basic analysis on the\nalgorithm to determine its algorithmic complexity of $O(n^2)$. Finally, we\nfurther optimize this complexity to $O(\\frac{n^2}{\\log n})$.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 06:23:58 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 09:14:24 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Kaleem", "Ahmad", ""], ["Kaleem", "Ahsan", ""]]}, {"id": "2007.10898", "submitter": "Omrit Filtser", "authors": "Arnold Filtser and Omrit Filtser", "title": "Static and Streaming Data Structures for Fr\\'echet Distance Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a curve $P$ with points in $\\mathbb{R}^d$ in a streaming fashion, and\nparameters $\\varepsilon>0$ and $k$, we construct a distance oracle that uses\n$O(\\frac{1}{\\varepsilon})^{kd}\\log\\varepsilon^{-1}$ space, and given a query\ncurve $Q$ with $k$ points in $\\mathbb{R}^d$, returns in $\\tilde{O}(kd)$ time a\n$1+\\varepsilon$ approximation of the discrete Fr\\'echet distance between $Q$\nand $P$.\n  In addition, we construct simplifications in the streaming model, oracle for\ndistance queries to a sub-curve (in the static setting), and introduce the\nzoom-in problem. Our algorithms work in any dimension $d$, and therefore we\ngeneralize some useful tools and algorithms for curves under the discrete\nFr\\'echet distance to work efficiently in high dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 15:42:29 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Filtser", "Arnold", ""], ["Filtser", "Omrit", ""]]}, {"id": "2007.10976", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya, Cl\\'ement L. Canonne, Yuhan Liu, Ziteng Sun, and\n  Himanshu Tyagi", "title": "Interactive Inference under Information Constraints", "comments": "Adding a section on information losses; improving presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the role of interactivity in distributed statistical inference under\ninformation constraints, e.g., communication constraints and local differential\nprivacy. We focus on the tasks of goodness-of-fit testing and estimation of\ndiscrete distributions. From prior work, these tasks are well understood under\nnoninteractive protocols. Extending these approaches directly for interactive\nprotocols is difficult due to correlations that can build due to interactivity;\nin fact, gaps can be found in prior claims of tight bounds of distribution\nestimation using interactive protocols.\n  We propose a new approach to handle this correlation and establish a unified\nmethod to establish lower bounds for both tasks. As an application, we obtain\noptimal bounds for both estimation and testing under local differential privacy\nand communication constraints. We also provide an example of a natural testing\nproblem where interactivity helps.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:51:34 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 18:26:27 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 03:09:45 GMT"}, {"version": "v4", "created": "Mon, 25 Jan 2021 04:25:06 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Liu", "Yuhan", ""], ["Sun", "Ziteng", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "2007.11093", "submitter": "Scott Summers", "authors": "David Furcy, Scott M. Summers, Logan Withers", "title": "Improved lower and upper bounds on the tile complexity of uniquely\n  self-assembling a thin rectangle non-cooperatively in 3D", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a fundamental question regarding a benchmark class of shapes\nin one of the simplest, yet most widely utilized abstract models of algorithmic\ntile self-assembly. Specifically, we study the directed tile complexity of a $k\n\\times N$ thin rectangle in Winfree's abstract Tile Assembly Model, assuming\nthat cooperative binding cannot be enforced (temperature-1 self-assembly) and\nthat tiles are allowed to be placed at most one step into the third dimension\n(just-barely 3D). While the directed tile complexities of a square and a\nscaled-up version of any algorithmically specified shape at temperature 1 in\njust-barely 3D are both asymptotically the same as they are (respectively) at\ntemperature 2 in 2D, the bounds on the directed tile complexity of a thin\nrectangle at temperature 2 in 2D are not known to hold at temperature 1 in\njust-barely 3D. Motivated by this discrepancy, we establish new lower and upper\nbounds on the directed tile complexity of a thin rectangle at temperature 1 in\njust-barely 3D. We develop a new, more powerful type of Window Movie Lemma that\nlets us upper bound the number of \"sufficiently similar\" ways to assign glues\nto a set of fixed locations. Consequently, our lower bound,\n$\\Omega\\left(N^{\\frac{1}{k}}\\right)$, is an asymptotic improvement over the\nprevious best lower bound and is more aesthetically pleasing since it\neliminates the $k$ that used to divide $N^{\\frac{1}{k}}$. The proof of our\nupper bound is based on a just-barely 3D, temperature-1 counter, organized\naccording to \"digit regions\", which affords it roughly fifty percent more\ndigits for the same target rectangle compared to the previous best counter.\nThis increase in digit density results in an upper bound of\n$O\\left(N^{\\frac{1}{\\left\\lfloor\\frac{k}{2}\\right\\rfloor}}+\\log N\\right)$, that\nis an asymptotic improvement over the previous best upper bound and roughly the\nsquare of our lower bound.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 21:20:00 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Furcy", "David", ""], ["Summers", "Scott M.", ""], ["Withers", "Logan", ""]]}, {"id": "2007.11094", "submitter": "Yakov Nekrich", "authors": "Yakov Nekrich", "title": "New Data Structures for Orthogonal Range Reporting and Range Minima\n  Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present new data structures for two extensively studied\nvariants of the orthogonal range searching problem.\n  First, we describe a data structure that supports two-dimensional orthogonal\nrange minima queries in $O(n)$ space and $O(\\log^{\\varepsilon} n)$ time, where\n$n$ is the number of points in the data structure and $\\varepsilon$ is an\narbitrarily small positive constant. Previously known linear-space solutions\nfor this problem require $O(\\log^{1+\\varepsilon} n)$ (Chazelle, 1988) or\n$O(\\log n\\log \\log n)$ time (Farzan et al., 2012). A modification of our data\nstructure uses space $O(n\\log \\log n)$ and supports range minima queries in\ntime $O(\\log \\log n)$. Both results can be extended to support\nthree-dimensional five-sided reporting queries.\n  Next, we turn to the four-dimensional orthogonal range reporting problem and\npresent a data structure that answers queries in optimal $O(\\log n/\\log \\log n\n+ k)$ time, where $k$ is the number of points in the answer. This is the first\ndata structure that achieves the optimal query time for this problem.\n  Our results are obtained by exploiting the properties of three-dimensional\nshallow cuttings.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 21:22:25 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Nekrich", "Yakov", ""]]}, {"id": "2007.11389", "submitter": "Roland Vincze", "authors": "Krist\\'of B\\'erczi, Matthias Mnich, Roland Vincze", "title": "A 3/2-Approximation for the Metric Many-visits Path TSP", "comments": "arXiv admin note: text overlap with arXiv:1911.09890", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Many-visits Path TSP, we are given a set of $n$ cities along with\ntheir pairwise distances (or cost) $c(uv)$, and moreover each city $v$ comes\nwith an associated positive integer request $r(v)$.\n  The goal is to find a minimum-cost path, starting at city $s$ and ending at\ncity $t$, that visits each city $v$ exactly $r(v)$ times.\n  We present a $\\frac32$-approximation algorithm for the metric Many-visits\nPath TSP, that runs in time polynomial in $n$ and poly-logarithmic in the\nrequests $r(v)$.\n  Our algorithm can be seen as a far-reaching generalization of the\n$\\frac32$-approximation algorithm for Path TSP by Zenklusen (SODA 2019), which\nanswered a long-standing open problem by providing an efficient algorithm which\nmatches the approximation guarantee of Christofides' algorithm from 1976 for\nmetric TSP.\n  One of the key components of our approach is a polynomial-time algorithm to\ncompute a connected, degree bounded multigraph of minimum cost.\n  We tackle this problem by generalizing a fundamental result of Kir\\'aly, Lau\nand Singh (Combinatorica, 2012) on the Minimum Bounded Degree Matroid Basis\nproblem, and devise such an algorithm for general polymatroids, even allowing\nelement multiplicities.\n  Our result directly yields a $\\frac32$-approximation to the metric\nMany-visits TSP, as well as a $\\frac32$-approximation for the problem of\nscheduling classes of jobs with sequence-dependent setup times on a single\nmachine so as to minimize the makespan.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 13:58:20 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["B\u00e9rczi", "Krist\u00f3f", ""], ["Mnich", "Matthias", ""], ["Vincze", "Roland", ""]]}, {"id": "2007.11398", "submitter": "Peter Chini", "authors": "Peter Chini, Prakash Saivasan", "title": "A Framework for Consistency Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework that provides deterministic consistency algorithms for\ngiven memory models. Such an algorithm checks whether the executions of a\nshared-memory concurrent program are consistent under the axioms defined by a\nmodel. For memory models like SC and TSO, checking consistency is NP-complete.\nOur framework shows, that despite the hardness, fast deterministic consistency\nalgorithms can be obtained by employing tools from fine-grained complexity. The\nframework is based on a universal consistency problem which can be instantiated\nby different memory models. We construct an algorithm for the problem running\nin time O*(2^k), where k is the number of write accesses in the execution that\nis checked for consistency. Each instance of the framework then admits an\nO*(2^k)-time consistency algorithm. By applying the framework, we obtain\ncorresponding consistency algorithms for SC, TSO, PSO, and RMO. Moreover, we\nshow that the obtained algorithms for SC, TSO, and PSO are optimal in the\nfine-grained sense: there is no consistency algorithm for these running in time\n2^o(k) unless the exponential time hypothesis fails.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 08:18:01 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Chini", "Peter", ""], ["Saivasan", "Prakash", ""]]}, {"id": "2007.11402", "submitter": "Peter Gartland", "authors": "Peter Gartland, Daniel Lokshtanov, Marcin Pilipczuk, Michal Pilipczuk,\n  Pawel Rzazewski", "title": "Finding large induced sparse subgraphs in $C_{>t}$-free graphs in\n  quasipolynomial time", "comments": "49 pages, 2 figures. Major changes from first (preliminary) version\n  including changing title, adding co-authors, and significant addition to\n  content of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an integer $t$, a graph $G$ is called {\\em{$C_{>t}$-free}} if $G$ does\nnot contain any induced cycle on more than~$t$ vertices. We prove the following\nstatement: for every pair of integers $d$ and $t$ and a CMSO$_2$\nstatement~$\\phi$, there exists an algorithm that, given an $n$-vertex\n$C_{>t}$-free graph $G$ with weights on vertices, finds in time $n^{O(\\log^4\nn)}$ a maximum-weight vertex subset $S$ such that $G[S]$ has degeneracy at most\n$d$ and satisfies $\\phi$. The running time can be improved to $n^{O(\\log^2 n)}$\nassuming $G$ is $P_t$-free, that is, $G$ does not contain an induced path on\n$t$ vertices. This expands the recent results of the authors [to appear at FOCS\n2020 and SOSA 2021] on the {\\sc{Maximum Weight Independent Set}} problem on\n$P_t$-free graphs in two directions: by encompassing the more general setting\nof $C_{>t}$-free graphs, and by being applicable to a much wider variety of\nproblems, such as {\\sc{Maximum Weight Induced Forest}} or {\\sc{Maximum Weight\nInduced Planar Graph}}.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 05:36:20 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 02:57:04 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 07:54:10 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Gartland", "Peter", ""], ["Lokshtanov", "Daniel", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Michal", ""], ["Rzazewski", "Pawel", ""]]}, {"id": "2007.11410", "submitter": "Yang Zheng", "authors": "Yang Zheng and Giovanni Fantuzzi", "title": "Sum-of-squares chordal decomposition of polynomial matrix inequalities", "comments": "32 pages, 7 figures, 4 tables. Established sparsity-exploiting\n  versions of Reznick and Putinar--Vasilescu Positivstellens\\\"atze, and updated\n  the second numerical example. Code for our numerical experiments is available\n  here:\n  https://github.com/aeroimperial-optimization/sos-chordal-decomposition-pmi", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.SY eess.SY math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove decomposition theorems for sparse positive (semi)definite polynomial\nmatrices that can be viewed as sparsity-exploiting versions of the\nHilbert--Artin, Reznick, Putinar, and Putinar--Vasilescu Positivstellens\\\"atze.\nFirst, we establish that a polynomial matrix $P(x)$ with chordal sparsity is\npositive semidefinite for all $x\\in \\mathbb{R}^n$ if and only if there exists a\nsum-of-squares (SOS) polynomial $\\sigma(x)$ such that $\\sigma P$ is a sum of\nsparse SOS matrices. Second, we show that setting $\\sigma(x)=(x_1^2 + \\cdots +\nx_n^2)^\\nu$ for some integer $\\nu$ suffices if $P$ is homogeneous and positive\ndefinite globally. Third, we prove that if $P$ is positive definite on a\ncompact semialgebraic set $\\mathcal{K}=\\{x:g_1(x)\\geq 0,\\ldots,g_m(x)\\geq 0\\}$\nsatisfying the Archimedean condition, then $P(x) = S_0(x) + g_1(x)S_1(x) +\n\\cdots + g_m(x)S_m(x)$ for matrices $S_i(x)$ that are sums of sparse SOS\nmatrices. Finally, if $\\mathcal{K}$ is not compact or does not satisfy the\nArchimedean condition, we obtain a similar decomposition for $(x_1^2 + \\ldots +\nx_n^2)^\\nu P(x)$ with some integer $\\nu\\geq 0$ when $P$ and $g_1,\\ldots,g_m$\nare homogeneous of even degree. Using these results, we find sparse SOS\nrepresentation theorems for polynomials that are quadratic and correlatively\nsparse in a subset of variables, and we construct new convergent hierarchies of\nsparsity-exploiting SOS reformulations for convex optimization problems with\nlarge and sparse polynomial matrix inequalities. Numerical examples demonstrate\nthat these hierarchies can have a significantly lower computational complexity\nthan traditional ones.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 13:13:09 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 23:43:19 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Zheng", "Yang", ""], ["Fantuzzi", "Giovanni", ""]]}, {"id": "2007.11476", "submitter": "Sayan Bandyapadhyay", "authors": "Sayan Bandyapadhyay, Aniket Basu Roy", "title": "Approximate Covering with Lower and Upper Bounds via LP Rounding", "comments": "There is an error in the algorithm for LUC in Section 3. The proof of\n  Lemma 5 does not hold", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the lower- and upper-bounded covering (LUC) problem,\nwhere we are given a set $P$ of $n$ points, a collection $\\mathcal{B}$ of\nballs, and parameters $L$ and $U$. The goal is to find a minimum-sized subset\n$\\mathcal{B}'\\subseteq \\mathcal{B}$ and an assignment of the points in $P$ to\n$\\mathcal{B}'$, such that each point $p\\in P$ is assigned to a ball that\ncontains $p$ and for each ball $B_i\\in \\mathcal{B}'$, at least $L$ and at most\n$U$ points are assigned to $B_i$. We obtain an LP rounding based constant\napproximation for LUC by violating the lower and upper bound constraints by\nsmall constant factors and expanding the balls by again a small constant\nfactor. Similar results were known before for covering problems with only the\nupper bound constraint. We also show that with only the lower bound constraint,\nthe above result can be obtained without any lower bound violation.\n  Covering problems have close connections with facility location problems. We\nnote that the known constant-approximation for the corresponding lower- and\nupper-bounded facility location problem, violates the lower and upper bound\nconstraints by a constant factor.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 14:53:12 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 20:28:46 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Bandyapadhyay", "Sayan", ""], ["Roy", "Aniket Basu", ""]]}, {"id": "2007.11495", "submitter": "Hanlin Ren", "authors": "Hanlin Ren", "title": "Improved Distance Sensitivity Oracles with Subcubic Preprocessing Time", "comments": "To appear in ESA'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of building Distance Sensitivity Oracles (DSOs).\nGiven a directed graph $G=(V, E)$ with edge weights in $\\{1, 2, \\dots, M\\}$, we\nneed to preprocess it into a data structure, and answer the following queries:\ngiven vertices $u,v\\in V$ and a failed vertex or edge $f\\in (V\\cup E)$, output\nthe length of the shortest path from $u$ to $v$ that does not go through $f$.\nOur main result is a simple DSO with $\\tilde{O}(n^{2.7233}M)$ preprocessing\ntime and $O(1)$ query time. Moreover, if the input graph is undirected, the\npreprocessing time can be improved to $\\tilde{O}(n^{2.6865}M)$. The\npreprocessing algorithm is randomized with correct probability $\\ge 1-1/n^C$,\nfor a constant $C$ that can be made arbitrarily large. Previously, there is a\nDSO with $\\tilde{O}(n^{2.8729}M)$ preprocessing time and\n$\\operatorname{polylog}(n)$ query time [Chechik and Cohen, STOC'20].\n  At the core of our DSO is the following observation from [Bernstein and\nKarger, STOC'09]: if there is a DSO with preprocessing time $P$ and query time\n$Q$, then we can construct a DSO with preprocessing time $P+\\tilde{O}(n^2)\\cdot\nQ$ and query time $O(1)$. (Here $\\tilde{O}(\\cdot)$ hides\n$\\operatorname{polylog}(n)$ factors.)\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 15:34:08 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Ren", "Hanlin", ""]]}, {"id": "2007.11532", "submitter": "Sebastian Perez-Salazar", "authors": "Sebastian Perez-Salazar, Mohit Singh, Alejandro Toriello", "title": "Adaptive Bin Packing with Overflow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by bursty bandwidth allocation and by the allocation of virtual\nmachines to servers in the cloud, we consider the online problem of packing\nitems with random sizes into unit-capacity bins. Items arrive sequentially, but\nupon arrival an item's actual size is unknown; only its probabilistic\ninformation is available to the decision maker. Without knowing this size, the\ndecision maker must irrevocably pack the item into an available bin or place it\nin a new bin. Once packed in a bin, the decision maker observes the item's\nactual size, and overflowing the bin is a possibility. An overflow incurs a\nlarge penalty cost and the corresponding bin is unusable for the rest of the\nprocess. In practical terms, this overflow models delayed services, failure of\nservers, and/or loss of end-user goodwill. The objective is to minimize the\ntotal expected cost given by the sum of the number of opened bins and the\noverflow penalty cost. We present an online algorithm with expected cost at\nmost a constant factor times the cost incurred by the optimal packing policy\nwhen item sizes are drawn from an i.i.d. sequence of unknown length. We give a\nsimilar result when item size distributions are exponential with arbitrary\nrates. We also study the offline model, where distributions are known in\nadvance but must be packed sequentially. We construct a soft-capacity PTAS for\nthis problem, and show that the complexity of computing the optimal offline\ncost is $\\#\\mathbf{P}$-hard. Finally, we provide an empirical study of our\nonline algorithm's performance.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 16:43:24 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 17:00:46 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Perez-Salazar", "Sebastian", ""], ["Singh", "Mohit", ""], ["Toriello", "Alejandro", ""]]}, {"id": "2007.11559", "submitter": "Joseph Cheriyan", "authors": "J.Cheriyan, R.Cummings, J.Dippel, J.Zhu", "title": "An Improved Approximation Algorithm for the Matching Augmentation\n  Problem", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a $\\frac53$-approximation algorithm for the matching augmentation\nproblem (MAP): given a multi-graph with edges of cost either zero or one such\nthat the edges of cost zero form a matching, find a 2-edge connected spanning\nsubgraph (2-ECSS) of minimum cost.\n  A $\\frac74$-approximation algorithm for the same problem was presented\nrecently, see Cheriyan, et al., \"The matching augmentation problem: a\n$\\frac{7}{4}$-approximation algorithm,\" {\\em Math. Program.}, 182(1):315--354,\n2020; arXiv:1810.07816.\n  Our improvement is based on new algorithmic techniques, and some of these may\nlead to advances on related problems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 17:28:02 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 05:05:58 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Cheriyan", "J.", ""], ["Cummings", "R.", ""], ["Dippel", "J.", ""], ["Zhu", "J.", ""]]}, {"id": "2007.11643", "submitter": "Andrej Sajenko", "authors": "Frank Kammer and Andrej Sajenko", "title": "FPT-space Graph Kernelizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $n$ be the size of a parametrized problem and $k$ the parameter. We\npresent a full kernel for Path Contraction and Cluster Editing/Deletion as well\nas a kernel for Feedback Vertex Set whose sizes are all polynomial in $k$, that\nare computable in polynomial time, and use $O(\\rm{poly}(k) \\log n)$ bits. By\nfirst executing the new kernelizations and subsequently the best known\npolynomial-time kernelizations for the problem under consideration, we obtain\nthe best known kernels in polynomial time with $O(\\rm{poly}(k) \\log n)$ bits.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 19:39:05 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 11:09:09 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Kammer", "Frank", ""], ["Sajenko", "Andrej", ""]]}, {"id": "2007.11651", "submitter": "Tin Vu", "authors": "Tin Vu, Ahmed Eldawy", "title": "R*-Grove: Balanced Spatial Partitioning for Large-scale Datasets", "comments": "29 pages, to be published in Frontiers in Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of big spatial data urged the research community to develop\nseveral big spatial data systems. Regardless of their architecture, one of the\nfundamental requirements of all these systems is to spatially partition the\ndata efficiently across machines. The core challenges of big spatial\npartitioning are building high spatial quality partitions while simultaneously\ntaking advantages of distributed processing models by providing load balanced\npartitions. Previous works on big spatial partitioning are to reuse existing\nindex search trees as-is, e.g., the R-tree family, STR, Kd-tree, and Quad-tree,\nby building a temporary tree for a sample of the input and use its leaf nodes\nas partition boundaries. However, we show in this paper that none of those\ntechniques has addressed the mentioned challenges completely. This paper\nproposes a novel partitioning method, termed R*-Grove, which can partition very\nlarge spatial datasets into high quality partitions with excellent load balance\nand block utilization. This appealing property allows R*-Grove to outperform\nexisting techniques in spatial query processing. R*-Grove can be easily\nintegrated into any big data platforms such as Apache Spark or Apache Hadoop.\nOur experiments show that R*-Grove outperforms the existing partitioning\ntechniques for big spatial data systems. With all the proposed work publicly\navailable as open source, we envision that R*-Grove will be adopted by the\ncommunity to better serve big spatial data research.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 20:08:41 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Vu", "Tin", ""], ["Eldawy", "Ahmed", ""]]}, {"id": "2007.11773", "submitter": "Ragesh Jaiswal", "authors": "Dishant Goyal, Ragesh Jaiswal, Amit Kumar", "title": "FPT Approximation for Constrained Metric $k$-Median/Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Metric $k$-median problem over a metric space $(\\mathcal{X}, d)$ is\ndefined as follows: given a set $L \\subseteq \\mathcal{X}$ of facility locations\nand a set $C \\subseteq \\mathcal{X}$ of clients, open a set $F \\subseteq L$ of\n$k$ facilities such that the total service cost, defined as $\\Phi(F, C) \\equiv\n\\sum_{x \\in C} \\min_{f \\in F} d(x, f)$, is minimised. The metric $k$-means\nproblem is defined similarly using squared distances. In many applications\nthere are additional constraints that any solution needs to satisfy. This gives\nrise to different constrained versions of the problem such as $r$-gather,\nfault-tolerant, outlier $k$-means/$k$-median problem. Surprisingly, for many of\nthese constrained problems, no constant-approximation algorithm is known. We\ngive FPT algorithms with constant approximation guarantee for a range of\nconstrained $k$-median/means problems. For some of the constrained problems,\nours is the first constant factor approximation algorithm whereas for others,\nwe improve or match the approximation guarantee of previous works. We work\nwithin the unified framework of Ding and Xu that allows us to simultaneously\nobtain algorithms for a range of constrained problems. In particular, we obtain\na $(3+\\varepsilon)$-approximation and $(9+\\varepsilon)$-approximation for the\nconstrained versions of the $k$-median and $k$-means problem respectively in\nFPT time. In many practical settings of the $k$-median/means problem, one is\nallowed to open a facility at any client location, i.e., $C \\subseteq L$. For\nthis special case, our algorithm gives a $(2+\\varepsilon)$-approximation and\n$(4+\\varepsilon)$-approximation for the constrained versions of $k$-median and\n$k$-means problem respectively in FPT time. Since our algorithm is based on\nsimple sampling technique, it can also be converted to a constant-pass\nlog-space streaming algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 03:42:33 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Goyal", "Dishant", ""], ["Jaiswal", "Ragesh", ""], ["Kumar", "Amit", ""]]}, {"id": "2007.11868", "submitter": "Diodato Ferraioli", "authors": "Diodato Ferraioli and Paolo Penna and Carmine Ventre", "title": "Two-way Greedy: Algorithms for Imperfect Rationality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The realization that selfish interests need to be accounted for in the design\nof algorithms has produced many contributions in computer science under the\numbrella of algorithmic mechanism design. Novel algorithmic properties and\nparadigms have been identified and studied. Our work stems from the observation\nthat selfishness is different from rationality; agents will attempt to\nstrategize whenever they perceive it to be convenient according to their\nimperfect rationality. Recent work has focused on a particular notion of\nimperfect rationality, namely absence of contingent reasoning skills, and\ndefined obvious strategyproofness (OSP) as a way to deal with the selfishness\nof these agents. Essentially, this definition states that to care for the\nincentives of these agents, we need not only pay attention about the\nrelationship between input and output, but also about the way the algorithm is\nrun. However, it is not clear what algorithmic approaches must be used for OSP.\nIn this paper, we show that, for binary allocation problems, OSP is fully\ncaptured by a combination of two well-known algorithmic techniques: forward and\nreverse greedy. We call two-way greedy this algorithmic design paradigm. Our\nmain technical contribution establishes the connection between OSP and two-way\ngreedy. We build upon the recently introduced cycle monotonicity technique for\nOSP. By means of novel structural properties of cycles and queries of OSP\nmechanisms, we fully characterize these mechanisms in terms of extremal\nimplementations. These are protocols that ask each agent to consistently\nseparate one extreme of their domain at the current history from the rest.\nThrough the connection with the greedy paradigm, we are able to import a host\nof approximation bounds to OSP and strengthen the strategic properties of this\nfamily of algorithms. Finally, we begin exploring the power of two-way greedy\nfor set systems.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 09:10:44 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Ferraioli", "Diodato", ""], ["Penna", "Paolo", ""], ["Ventre", "Carmine", ""]]}, {"id": "2007.11997", "submitter": "Sangram Kishor Jena Mr", "authors": "Sangram K. Jena and Gautam K. Das", "title": "Total Domination in Unit Disk Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be an undirected graph. We call $D_t \\subseteq V$ as a total\ndominating set (TDS) of $G$ if each vertex $v \\in V$ has a dominator in $D$\nother than itself. Here we consider the TDS problem in unit disk graphs, where\nthe objective is to find a minimum cardinality total dominating set for an\ninput graph. We prove that the TDS problem is NP-hard in unit disk graphs.\nNext, we propose an 8-factor approximation algorithm for the problem. The\nrunning time of the proposed approximation algorithm is $O(n \\log k)$, where\n$n$ is the number of vertices of the input graph and $k$ is output size. We\nalso show that TDS problem admits a PTAS in unit disk graphs.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 13:11:19 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Jena", "Sangram K.", ""], ["Das", "Gautam K.", ""]]}, {"id": "2007.12077", "submitter": "Tomohiro Koana", "authors": "Tomohiro Koana, Andr\\'e Nichterlein", "title": "Detecting and Enumerating Small Induced Subgraphs in $c$-Closed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fox et al. [SIAM J. Comp. 2020] introduced a new parameter, called\n$c$-closure, for a parameterized study of clique enumeration problems. A graph\n$G$ is $c$-closed if every pair of vertices with at least $c$ common neighbors\nis adjacent. The $c$-closure of $G$ is the smallest $c$ such that $G$ is\n$c$-closed. We systematically explore the impact of $c$-closure on the\ncomputational complexity of detecting and enumerating small induced subgraphs.\nMore precisely, for each graph $H$ on three or four vertices, we investigate\nparameterized polynomial-time algorithms for detecting $H$ and for enumerating\nall occurrences of $H$ in a given $c$-closed graph.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 15:41:43 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Koana", "Tomohiro", ""], ["Nichterlein", "Andr\u00e9", ""]]}, {"id": "2007.12102", "submitter": "Marco Bressan", "authors": "Marco Bressan", "title": "Efficient and near-optimal algorithms for sampling connected subgraphs", "comments": "53 pages, accepted at STOC'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the graphlet sampling problem: given an integer $k \\ge 3$ and a\nsimple graph $G=(V,E)$, sample a connected induced $k$-node subgraph of $G$\n(also called $k$-graphlet) uniformly at random. This is a fundamental graph\nmining primitive, with applications in social network analysis and\nbioinformatics. In this work, we give the following results. (1) A near-tight\nbound for the classic $k$-graphlet random walk, as a function of the mixing\ntime of $G$. In particular, ignoring $k^{O(k)}$ factors, we show that the\nrandom walk mixes in time $\\tilde{\\Theta}(t(G) \\cdot \\rho(G)^{k-1})$, where\n$t(G)$ is the mixing time of $G$ and $\\rho(G)$ is the ratio between its maximum\nand minimum degree. (2) The first efficient algorithm for uniform graphlet\nsampling. The algorithm has a preprocessing phase that uses time ${O}(n k^2 \\ln\nk + m)$ and space $O(n)$, and a sampling phase that uses $k^{O(k)} O(\\log\n\\Delta)$ time per sample. (3) A near-optimal algorithm for $\\epsilon$-uniform\ngraphlet sampling. The preprocessing takes time $O\\big(\\frac{k^6}{\\epsilon}\\, n\n\\log n \\big)$ and space $O(n)$, and the sampling takes\n$k^{O(k)}O\\big((\\frac{1}{\\epsilon})^{10}\\log \\frac{1}{\\epsilon} \\big)$ expected\ntime per sample.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 16:08:36 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 13:08:41 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 18:07:58 GMT"}, {"version": "v4", "created": "Thu, 11 Feb 2021 18:49:49 GMT"}, {"version": "v5", "created": "Wed, 17 Feb 2021 07:55:49 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Bressan", "Marco", ""]]}, {"id": "2007.12120", "submitter": "Lukasz Kowalik", "authors": "{\\L}ukasz Kowalik, Konrad Majewski", "title": "The Asymmetric Travelling Salesman Problem in Sparse Digraphs", "comments": "A shorter version accepted to IPEC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymmetric Travelling Salesman Problem (ATSP) and its special case Directed\nHamiltonicity are among the most fundamental problems in computer science. The\ndynamic programming algorithm running in time $O^*(2^n)$ developed almost 60\nyears ago by Bellman, Held and Karp, is still the state of the art for both of\nthese problems.\n  In this work we focus on sparse digraphs. First, we recall known approaches\nfor Undirected Hamiltonicity and TSP in sparse graphs and we analyse their\nconsequences for Directed Hamiltonicity and ATSP in sparse digraphs, either by\nadapting the algorithm, or by using reductions. In this way, we get a number of\nrunning time upper bounds for a few classes of sparse digraphs, including\n$O^*(2^{n/3})$ for digraphs with both out- and indegree bounded by 2, and\n$O^*(3^{n/2})$ for digraphs with outdegree bounded by 3.\n  Our main results are focused on digraphs of bounded average outdegree $d$.\nThe baseline for ATSP here is a simple enumeration of cycle covers which can be\ndone in time bounded by $O^*(\\mu(d)^n)$ for a function\n$\\mu(d)\\le(\\lceil{d}\\rceil!)^{1/{\\lceil{d}\\rceil}}$. One can also observe that\nDirected Hamiltonicity can be solved in randomized time $O^*((2-2^{-d})^n)$ and\npolynomial space, by adapting a recent result of Bj\\\"{o}rklund [ISAAC 2018]\nstated originally for Undirected Hamiltonicity in sparse bipartite graphs.\n  We present two new deterministic algorithms for ATSP: the first running in\ntime $O(2^{0.441(d-1)n})$ and polynomial space, and the second in exponential\nspace with running time of $O^*(\\tau(d)^{n/2})$ for a function $\\tau(d)\\le d$.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 16:47:58 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 12:36:18 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Kowalik", "\u0141ukasz", ""], ["Majewski", "Konrad", ""]]}, {"id": "2007.12307", "submitter": "Alex Gavryushkin", "authors": "Lena Collienne and Alex Gavryushkin", "title": "Computing nearest neighbour interchange distances between ranked\n  phylogenetic trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular algorithms for searching the space of leaf-labelled trees are\nbased on tree rearrangement operations. Under any such operation, the problem\nis reduced to searching a graph where vertices are trees and (undirected) edges\nare given by pairs of trees connected by one rearrangement operation (sometimes\ncalled a move). Most popular are the classical nearest neighbour interchange,\nsubtree prune and regraft, and tree bisection and reconnection moves. The\nproblem of computing distances, however, is NP-hard in each of these graphs,\nmaking tree inference and comparison algorithms challenging to design in\npractice.\n  Although ranked phylogenetic trees are one of the central objects of interest\nin applications such as cancer research, immunology, and epidemiology, the\ncomputational complexity of the shortest path problem for these trees remained\nunsolved for decades. In this paper, we settle this problem for the ranked\nnearest neighbour interchange operation by establishing that the complexity\ndepends on the weight difference between the two types of tree rearrangements\n(rank moves and edge moves), and varies from quadratic, which is the lowest\npossible complexity for this problem, to NP-hard, which is the highest. In\nparticular, our result provides the first example of a phylogenetic tree\nrearrangement operation for which shortest paths, and hence the distance, can\nbe computed efficiently. Specifically, our algorithm scales to trees with\nthousands of leaves (and likely hundreds of thousands if implemented\nefficiently).\n  We also connect the problem of computing distances in our graph of ranked\ntrees with the well-known version of this problem on unranked trees by\nintroducing a parameter for the weight difference between move types. We\npropose to study a family of shortest path problems indexed by this parameter\nwith computational complexity varying from quadratic to NP-hard.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 00:50:45 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Collienne", "Lena", ""], ["Gavryushkin", "Alex", ""]]}, {"id": "2007.12323", "submitter": "Huacheng Yu", "authors": "Huacheng Yu", "title": "Tight Distributed Sketching Lower Bound for Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the distributed sketching complexity of connectivity.\nIn distributed graph sketching, an $n$-node graph $G$ is distributed to $n$\nplayers such that each player sees the neighborhood of one vertex. The players\nthen simultaneously send one message to the referee, who must compute some\nfunction of $G$ with high probability. For connectivity, the referee must\noutput whether $G$ is connected. The goal is to minimize the message lengths.\nSuch sketching schemes are equivalent to one-round protocols in the broadcast\ncongested clique model.\n  We prove that the expected average message length must be at least\n$\\Omega(\\log^3 n)$ bits, if the error probability is at most $1/4$. It matches\nthe upper bound obtained by the AGM sketch [AGM12], which even allows the\nreferee to output a spanning forest of $G$ with probability\n$1-1/\\mathrm{poly}\\, n$. Our lower bound strengthens the previous\n$\\Omega(\\log^3 n)$ lower bound for spanning forest computation [NY19]. Hence,\nit implies that connectivity, a decision problem, is as hard as its \"search\"\nversion in this model.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 02:41:18 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Yu", "Huacheng", ""]]}, {"id": "2007.12388", "submitter": "Weidong Li", "authors": "Weidong Li", "title": "Improved approximation schemes for early work scheduling on identical\n  parallel machines with common due date", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the early work scheduling problem on identical parallel machines in\norder to maximize the total early work, i.e., the parts of non-preemptive jobs\nexecuted before a common due date. By preprocessing and constructing an\nauxiliary instance which has several good properties, we propose an efficient\npolynomial time approximation scheme with running time $O(n)$, which improves\nthe result in [Gy\\\"{o}rgyi, P., Kis, T. (2020). A common approximation\nframework for early work, late work, and resource leveling problems. {\\it\nEuropean Journal of Operational Research}, 286(1), 129-137], and a fully\npolynomial time approximation scheme with running time $O(n)$ when the number\nof machines is a fixed number, which improves the result in [Chen, X., Liang,\nY., Sterna, M., Wang, W., B{\\l}a\\.{z}ewicz, J. (2020b). Fully polynomial time\napproximation scheme to maximize early work on parallel machines with common\ndue date. {\\it European Journal of Operational Research}, 284(1), 67-74], where\n$n$ is the number of jobs, and the hidden constant depends on the desired\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 07:24:41 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Li", "Weidong", ""]]}, {"id": "2007.12502", "submitter": "Philipp Zschoche", "authors": "Matthias Bentert, Andr\\'e Nichterlein, Malte Renken, and Philipp\n  Zschoche", "title": "Using a geometric lens to find k disjoint shortest paths", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2021.26", "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected $n$-vertex graph and $k$ pairs of terminal vertices\n$(s_1,t_1), \\ldots, (s_k,t_k)$, the $k$-Disjoint Shortest Paths\n($k$-DSP)-problem asks whether there are $k$ pairwise vertex-disjoint paths\n$P_1,\\ldots, P_k$ such that $P_i$ is a shortest $s_i$-$t_i$-path for each $i\n\\in [k]$. Recently, Lochet [arXiv 2019] provided an algorithm that solves\n$k$-DSP in $n^{O(k^{4^k})}$ time, answering a 20-year old question about the\ncomputational complexity of $k$-DSP for constant $k$. On the one hand, we\npresent an improved $O(k n^{12k \\cdot k! + k + 1})$-time algorithm based on a\nnovel geometric view on this problem. For the special case $k=2$, we show that\nthe running time can be further reduced to $O(n^2m)$ by small modifications of\nthe algorithm and a further refined analysis. On the other hand, we show that\n$k$-DSP is W[1]-hard with respect to $k$, showing that the dependency of the\ndegree of the polynomial running time on the parameter $k$ is presumably\nunavoidable.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 12:38:59 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Bentert", "Matthias", ""], ["Nichterlein", "Andr\u00e9", ""], ["Renken", "Malte", ""], ["Zschoche", "Philipp", ""]]}, {"id": "2007.12651", "submitter": "Marcos Villagra", "authors": "Javier T. Akagi and Eduardo A. Canale and Marcos Villagra", "title": "Tromino Tilings with Pegs via Flow Networks", "comments": "9 pages, 7 figures. Lemmas 4.1 and 4.2 were revised, but the the\n  final result is unaffected. To appear in Proceedings of LAGOS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tromino tiling problem is a packing puzzle where we are given a region of\nconnected lattice squares and we want to decide whether there exists a tiling\nof the region using trominoes with the shape of an L. In this work we study a\nslight variation of the tromino tiling problem where some positions of the\nregion have pegs and each tromino comes with a hole that can only be placed on\ntop of the pegs. We present a characterization of this tiling problem with pegs\nusing flow networks and show that (i) there exists a linear-time parsimonious\nreduction to the maximum-flow problem, and (ii) counting the number of such\ntilings can be done in linear-time. The proofs of both results contain\nalgorithms that can then be used to decide the tiling of a region with pegs in\n$O(n)$ time.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 17:06:06 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 13:28:11 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Akagi", "Javier T.", ""], ["Canale", "Eduardo A.", ""], ["Villagra", "Marcos", ""]]}, {"id": "2007.12652", "submitter": "Emir Demirovi\\'c", "authors": "Emir Demirovi\\'c, Anna Lukina, Emmanuel Hebrard, Jeffrey Chan, James\n  Bailey, Christopher Leckie, Kotagiri Ramamohanarao, Peter J. Stuckey", "title": "MurTree: Optimal Classification Trees via Dynamic Programming and Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision tree learning is a widely used approach in machine learning,\nfavoured in applications that require concise and interpretable models.\nHeuristic methods are traditionally used to quickly produce models with\nreasonably high accuracy. A commonly criticised point, however, is that the\nresulting trees may not necessarily be the best representation of the data in\nterms of accuracy and size. In recent years, this motivated the development of\noptimal classification tree algorithms that globally optimise the decision tree\nin contrast to heuristic methods that perform a sequence of locally optimal\ndecisions. We follow this line of work and provide a novel algorithm for\nlearning optimal classification trees based on dynamic programming and search.\nOur algorithm supports constraints on the depth of the tree and number of\nnodes. The success of our approach is attributed to a series of specialised\ntechniques that exploit properties unique to classification trees. Whereas\nalgorithms for optimal classification trees have traditionally been plagued by\nhigh runtimes and limited scalability, we show in a detailed experimental study\nthat our approach uses only a fraction of the time required by the\nstate-of-the-art and can handle datasets with tens of thousands of instances,\nproviding several orders of magnitude improvements and notably contributing\ntowards the practical realisation of optimal decision trees.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 17:06:55 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 16:46:36 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Demirovi\u0107", "Emir", ""], ["Lukina", "Anna", ""], ["Hebrard", "Emmanuel", ""], ["Chan", "Jeffrey", ""], ["Bailey", "James", ""], ["Leckie", "Christopher", ""], ["Ramamohanarao", "Kotagiri", ""], ["Stuckey", "Peter J.", ""]]}, {"id": "2007.12653", "submitter": "Mahsa Derakhshan", "authors": "Mahsa Derakhshan, David M. Pennock, Aleksandrs Slivkins", "title": "Beating Greedy For Approximating Reserve Prices in Multi-Unit VCG\n  Auctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding personalized reserve prices for unit-demand\nbuyers in multi-unit eager VCG auctions with correlated buyers. The input to\nthis problem is a dataset of submitted bids of $n$ buyers in a set of auctions.\nThe goal is to find a vector of reserve prices, one for each buyer, that\nmaximizes the total revenue across all auctions.\n  Roughgarden and Wang (2016) showed that this problem is APX-hard but admits a\ngreedy $\\frac{1}{2}$-approximation algorithm. Later, Derakhshan, Golrezai, and\nPaes Leme (2019) gave an LP-based algorithm achieving a $0.68$-approximation\nfor the (important) special case of the problem with a single-item, thereby\nbeating greedy. We show in this paper that the algorithm of Derakhshan et al.\nin fact does not beat greedy for the general multi-item problem. This raises\nthe question of whether or not the general problem admits a\nbetter-than-$\\frac{1}{2}$ approximation.\n  In this paper, we answer this question in the affirmative and provide a\npolynomial-time algorithm with a significantly better approximation-factor of\n$0.63$. Our solution is based on a novel linear programming formulation, for\nwhich we propose two different rounding schemes. We prove that the best of\nthese two and the no-reserve case (all-zero vector) is a $0.63$-approximation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 17:08:42 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Derakhshan", "Mahsa", ""], ["Pennock", "David M.", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "2007.12762", "submitter": "Tomasz Kociumaka", "authors": "Tomasz Kociumaka, Barna Saha", "title": "Sublinear-Time Algorithms for Computing & Embedding Gap Edit Distance", "comments": null, "journal-ref": "FOCS 2020", "doi": "10.1109/FOCS46700.2020.00112", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design new sublinear-time algorithms for solving the gap\nedit distance problem and for embedding edit distance to Hamming distance. For\nthe gap edit distance problem, we give an $\\tilde{O}(\\frac{n}{k}+k^2)$-time\ngreedy algorithm that distinguishes between length-$n$ input strings with edit\ndistance at most $k$ and those with edit distance exceeding $(3k+5)k$. This is\nan improvement and a simplification upon the result of Goldenberg, Krauthgamer,\nand Saha [FOCS 2019], where the $k$ vs $\\Theta(k^2)$ gap edit distance problem\nis solved in $\\tilde{O}(\\frac{n}{k}+k^3)$ time. We further generalize our\nresult to solve the $k$ vs $k'$ gap edit distance problem in time\n$\\tilde{O}(\\frac{nk}{k'}+k^2+ \\frac{k^2}{k'}\\sqrt{nk})$, strictly improving\nupon the previously known bound $\\tilde{O}(\\frac{nk}{k'}+k^3)$. Finally, we\nshow that if the input strings do not have long highly periodic substrings,\nthen already the $k$ vs $(1+\\epsilon)k$ gap edit distance problem can be solved\nin sublinear time. Specifically, if the strings contain no substring of length\n$\\ell$ with period at most $2k$, then the running time we achieve is\n$\\tilde{O}(\\frac{n}{\\epsilon^2 k}+k^2\\ell)$.\n  We further give the first sublinear-time probabilistic embedding of edit\ndistance to Hamming distance. For any parameter $p$, our\n$\\tilde{O}(\\frac{n}{p})$-time procedure yields an embedding with distortion\n$O(kp)$, where $k$ is the edit distance of the original strings. Specifically,\nthe Hamming distance of the resultant strings is between $\\frac{k-p+1}{p+1}$\nand $O(k^2)$ with good probability. This generalizes the linear-time embedding\nof Chakraborty, Goldenberg, and Kouck\\'y [STOC 2016], where the resultant\nHamming distance is between $\\frac k2$ and $O(k^2)$. Our algorithm is based on\na random walk over samples, which we believe will find other applications in\nsublinear-time algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 20:33:23 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 03:05:29 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kociumaka", "Tomasz", ""], ["Saha", "Barna", ""]]}, {"id": "2007.12815", "submitter": "Surbhi Goel", "authors": "Surbhi Goel, Adam Klivans, Frederic Koehler", "title": "From Boltzmann Machines to Neural Networks and Back Again", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are powerful tools for modeling high-dimensional data, but\nlearning graphical models in the presence of latent variables is well-known to\nbe difficult. In this work we give new results for learning Restricted\nBoltzmann Machines, probably the most well-studied class of latent variable\nmodels. Our results are based on new connections to learning two-layer neural\nnetworks under $\\ell_{\\infty}$ bounded input; for both problems, we give nearly\noptimal results under the conjectured hardness of sparse parity with noise.\nUsing the connection between RBMs and feedforward networks, we also initiate\nthe theoretical study of $supervised~RBMs$ [Hinton, 2012], a version of\nneural-network learning that couples distributional assumptions induced from\nthe underlying graphical model with the architecture of the unknown function\nclass. We then give an algorithm for learning a natural class of supervised\nRBMs with better runtime than what is possible for its related class of\nnetworks without distributional assumptions.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 00:42:50 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Goel", "Surbhi", ""], ["Klivans", "Adam", ""], ["Koehler", "Frederic", ""]]}, {"id": "2007.12823", "submitter": "Billy Jin", "authors": "Billy Jin, David P. Williamson", "title": "Improved Analysis of RANKING for Online Vertex-Weighted Bipartite\n  Matching", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the online vertex-weighted bipartite matching\nproblem in the random arrival model. We consider the generalization of the\nRANKING algorithm for this problem introduced by Huang, Tang, Wu, and Zhang\n(TALG 2019), who show that their algorithm has a competitive ratio of 0.6534.\nWe show that assumptions in their analysis can be weakened, allowing us to\nreplace their derivation of a crucial function $g$ on the unit square with a\nlinear program that computes the values of a best possible $g$ under these\nassumptions on a discretized unit square. We show that the discretization does\nnot incur much error, and show computationally that we can obtain a competitive\nratio of 0.6629. To compute the bound over our discretized unit square we use\nparallelization, and still needed two days of computing on a 64-core machine.\nFurthermore, by modifying our linear program somewhat, we can show\ncomputationally an upper bound on our approach of 0.6688; any further progress\nbeyond this bound will require either further weakening in the assumptions of\n$g$ or a stronger analysis than that of Huang et al.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 01:38:17 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Jin", "Billy", ""], ["Williamson", "David P.", ""]]}, {"id": "2007.13016", "submitter": "Farhad Shahrokhi", "authors": "Farhad Shahrokhi", "title": "Bounding the trace function of a hypergraph with applications", "comments": "Portion of the results were presented in 51st southeastern\n  international conference on combinatorics, graph theory and computing, March\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An upper bound on the trace function of a hypergraph $H$ is derived and its\napplications are demonstrated. For instance, a new upper bound for the VC\ndimension of $H$, or $vc(H)$, follows as a consequence and can be used to\ncompute $vc(H)$ in polynomial time provided that $H$ has bounded degeneracy.\nThis was not previously known. Particularly, when $H$ is a hypergraph arising\nfrom closed neighborhoods of a graph, this approach asymptotically improves the\ntime complexity of the previous result for computing $vc(H)$. Another\nconsequence is a general lower bound on the {\\it distinguishing transversal\nnumber } of $H$ that gives rise to applications in domination theory of graphs.\nTo effectively apply the methods developed here, one needs to have good\nestimations of degeneracy, and its variation or reduced degeneracy which is\nintroduced here.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 21:50:58 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Shahrokhi", "Farhad", ""]]}, {"id": "2007.13121", "submitter": "Sahil Singla", "authors": "Danny Segev and Sahil Singla", "title": "Efficient Approximation Schemes for Stochastic Probing and Prophet\n  Problems", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our main contribution is a general framework to design efficient polynomial\ntime approximation schemes (EPTAS) for fundamental classes of stochastic\ncombinatorial optimization problems. Given an error parameter $\\epsilon>0$,\nsuch algorithmic schemes attain a $(1+\\epsilon)$-approximation in only\n$t(\\epsilon)\\cdot poly(n)$ time, where $t(\\cdot)$ is some function that depends\nonly on $\\epsilon$. Technically speaking, our approach relies on presenting\ntailor-made reductions to a newly-introduced multi-dimensional extension of the\nSanta Claus problem [Bansal-Sviridenko, STOC'06]. Even though the\nsingle-dimensional problem is already known to be APX-Hard, we prove that an\nEPTAS can be designed under certain structural assumptions, which hold for our\napplications.\n  To demonstrate the versatility of our framework, we obtain an EPTAS for the\nadaptive ProbeMax problem as well as for its non-adaptive counterpart; in both\ncases, state-of-the-art approximability results have been inefficient\npolynomial time approximation schemes (PTAS) [Chen et al., NIPS'16; Fu et al.,\nICALP'18]. Turning our attention to selection-stopping settings, we further\nderive an EPTAS for the Free-Order Prophets problem [Agrawal et al., EC'20] and\nfor its cost-driven generalization, Pandora's Box with Commitment [Fu et al.,\nICALP'18]. These results improve on known PTASes for their adaptive variants,\nand constitute the first non-trivial approximations in the non-adaptive\nsetting.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 12:47:12 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Segev", "Danny", ""], ["Singla", "Sahil", ""]]}, {"id": "2007.13234", "submitter": "Tim Roughgarden", "authors": "Tim Roughgarden", "title": "Resource Augmentation", "comments": "Chapter 4 of the book Beyond the Worst-Case Analysis of Algorithms,\n  edited by Tim Roughgarden and published by Cambridge University Press (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter introduces resource augmentation, in which the performance of an\nalgorithm is compared to the best-possible solution that is handicapped by less\nresources. We consider three case studies: online paging, with cache size as\nthe resource; selfish routing, with capacity as the resource; and scheduling,\nwith processor speed as the resource. Resource augmentation bounds also imply\n\"loosely competitive\" bounds, which show that an algorithm's performance is\nnear-optimal for most resource levels.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 22:41:39 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Roughgarden", "Tim", ""]]}, {"id": "2007.13240", "submitter": "Tim Roughgarden", "authors": "Tim Roughgarden", "title": "Distributional Analysis", "comments": "Chapter 8 of the book Beyond the Worst-Case Analysis of Algorithms,\n  edited by Tim Roughgarden and published by Cambridge University Press (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributional or average-case analysis, the goal is to design an\nalgorithm with good-on-average performance with respect to a specific\nprobability distribution. Distributional analysis can be useful for the study\nof general-purpose algorithms on \"non-pathological\" inputs, and for the design\nof specialized algorithms in applications in which there is detailed\nunderstanding of the relevant input distribution. For some problems, however,\npure distributional analysis encourages \"overfitting\" an algorithmic solution\nto a particular distributional assumption and a more robust analysis framework\nis called for. This chapter presents numerous examples of the pros and cons of\ndistributional analysis, highlighting some of its greatest hits while also\nsetting the stage for the hybrids of worst- and average-case analysis studied\nin later chapters.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 23:16:26 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Roughgarden", "Tim", ""]]}, {"id": "2007.13241", "submitter": "Tim Roughgarden", "authors": "Tim Roughgarden", "title": "Beyond the Worst-Case Analysis of Algorithms (Introduction)", "comments": "Chapter 1 of the book Beyond the Worst-Case Analysis of Algorithms,\n  edited by Tim Roughgarden and published by Cambridge University Press (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary goals of the mathematical analysis of algorithms is to\nprovide guidance about which algorithm is the \"best\" for solving a given\ncomputational problem. Worst-case analysis summarizes the performance profile\nof an algorithm by its worst performance on any input of a given size,\nimplicitly advocating for the algorithm with the best-possible worst-case\nperformance. Strong worst-case guarantees are the holy grail of algorithm\ndesign, providing an application-agnostic certification of an algorithm's\nrobustly good performance. However, for many fundamental problems and\nperformance measures, such guarantees are impossible and a more nuanced\nanalysis approach is called for. This chapter surveys several alternatives to\nworst-case analysis that are discussed in detail later in the book.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 23:18:19 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Roughgarden", "Tim", ""]]}, {"id": "2007.13356", "submitter": "Oliver Serang", "authors": "Jake Pennington, Patrick Kreitzberg, Kyle Lucke, Oliver Serang", "title": "Optimal construction of a layer-ordered heap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The layer-ordered heap (LOH) is a simple, recently proposed data structure\nused in optimal selection on $X+Y$, thealgorithm with the best known runtime\nfor selection on $X_1+X_2+\\cdots+X_m$, and the fastest method in practice for\ncomputing the most abundant isotope peaks in a chemical compound. Here, we\nintroduce a few algorithms for constructing LOHs, analyze their complexity, and\ndemonstrate that one algorithm is optimal for building a LOH of any rank\n$\\alpha$. These results are shown to correspond with empirical experiments of\nruntimes when applying the LOH construction algorithms to a common task in\nmachine learning.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 08:17:57 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 23:56:39 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Pennington", "Jake", ""], ["Kreitzberg", "Patrick", ""], ["Lucke", "Kyle", ""], ["Serang", "Oliver", ""]]}, {"id": "2007.13466", "submitter": "Sofya Vorotnikova", "authors": "Sofya Vorotnikova", "title": "Improved 3-pass Algorithm for Counting 4-cycles in Arbitrary Order\n  Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of counting small subgraphs, and specifically cycles, in the\nstreaming model received a lot of attention over the past few years. In this\npaper, we consider arbitrary order insertion-only streams, improving over the\nstate-of-the-art result on counting 4-cycles. Our algorithm computes a\n$(1+\\epsilon)$-approximation by taking three passes over the stream and using\nspace $O(\\frac{m \\log n}{\\epsilon^2 T^{1/3}})$, where $m$ is the number of\nedges in the graph and $T$ is the number of 4-cycles.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 12:12:21 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Vorotnikova", "Sofya", ""]]}, {"id": "2007.13471", "submitter": "Tomasz Wale\\'n", "authors": "Maxime Crochemore, Costas Iliopoulos, Jakub Radoszewski, Wojciech\n  Rytter, Juliusz Straszy\\'nski, Tomasz Wale\\'n and Wiktor Zuba", "title": "Internal Quasiperiod Queries", "comments": "To appear in the SPIRE 2020 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internal pattern matching requires one to answer queries about factors of a\ngiven string. Many results are known on answering internal period queries,\nasking for the periods of a given factor. In this paper we investigate (for the\nfirst time) internal queries asking for covers (also known as quasiperiods) of\na given factor. We propose a data structure that answers such queries in\n$O(\\log n \\log \\log n)$ time for the shortest cover and in $O(\\log n (\\log \\log\nn)^2)$ time for a representation of all the covers, after $O(n \\log n)$ time\nand space preprocessing.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 12:18:52 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Crochemore", "Maxime", ""], ["Iliopoulos", "Costas", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Straszy\u0144ski", "Juliusz", ""], ["Wale\u0144", "Tomasz", ""], ["Zuba", "Wiktor", ""]]}, {"id": "2007.13594", "submitter": "Silvia Butti", "authors": "Silvia Butti, Victor Dalmau", "title": "The Complexity of the Distributed Constraint Satisfaction Problem", "comments": "Full version of a STACS'21 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the Distributed Constraint Satisfaction Problem\n(DCSP) on a synchronous, anonymous network from a theoretical standpoint. In\nthis setting, variables and constraints are controlled by agents which\ncommunicate with each other by sending messages through fixed communication\nchannels. Our results endorse the well-known fact from classical CSPs that the\ncomplexity of fixed-template computational problems depends on the template's\ninvariance under certain operations. Specifically, we show that DCSP($\\Gamma$)\nis polynomial-time tractable if and only if $\\Gamma$ is invariant under\nsymmetric polymorphisms of all arities. Otherwise, there are no algorithms that\nsolve DCSP($\\Gamma$) in finite time. We also show that the same condition holds\nfor the search variant of DCSP. Collaterally, our results unveil a feature of\nthe processes' neighbourhood in a distributed network, its iterated degree,\nwhich plays a major role in the analysis. We explore this notion establishing a\ntight connection with the basic linear programming relaxation of a CSP.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 14:23:26 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 13:16:37 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Butti", "Silvia", ""], ["Dalmau", "Victor", ""]]}, {"id": "2007.13615", "submitter": "Leo van Iersel", "authors": "Sander Borst, Leo van Iersel, Mark Jones and Steven Kelk", "title": "New FPT algorithms for finding the temporal hybridization number for\n  sets of phylogenetic trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding a temporal hybridization network for a set of\nphylogenetic trees that minimizes the number of reticulations. First, we\nintroduce an FPT algorithm for this problem on an arbitrary set of $m$ binary\ntrees with $n$ leaves each with a running time of $O(5^k\\cdot n\\cdot m)$, where\n$k$ is the minimum temporal hybridization number. We also present the concept\nof temporal distance, which is a measure for how close a tree-child network is\nto being temporal. Then we introduce an algorithm for computing a tree-child\nnetwork with temporal distance at most $d$ and at most $k$ reticulations in\n$O((8k)^d5^ k\\cdot n\\cdot m)$ time. Lastly, we introduce a $O(6^kk!\\cdot k\\cdot\nn^2)$ time algorithm for computing a minimum temporal hybridization network for\na set of two nonbinary trees. We also provide an implementation of all\nalgorithms and an experimental analysis on their performance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 14:55:59 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Borst", "Sander", ""], ["van Iersel", "Leo", ""], ["Jones", "Mark", ""], ["Kelk", "Steven", ""]]}, {"id": "2007.13660", "submitter": "Yuhan Liu", "authors": "Yuhan Liu, Ananda Theertha Suresh, Felix Yu, Sanjiv Kumar, Michael\n  Riley", "title": "Learning discrete distributions: user vs item-level privacy", "comments": "NeurIPS 2020, 38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the literature on differential privacy focuses on item-level privacy,\nwhere loosely speaking, the goal is to provide privacy per item or training\nexample. However, recently many practical applications such as federated\nlearning require preserving privacy for all items of a single user, which is\nmuch harder to achieve. Therefore understanding the theoretical limit of\nuser-level privacy becomes crucial.\n  We study the fundamental problem of learning discrete distributions over $k$\nsymbols with user-level differential privacy. If each user has $m$ samples, we\nshow that straightforward applications of Laplace or Gaussian mechanisms\nrequire the number of users to be $\\mathcal{O}(k/(m\\alpha^2) +\nk/\\epsilon\\alpha)$ to achieve an $\\ell_1$ distance of $\\alpha$ between the true\nand estimated distributions, with the privacy-induced penalty\n$k/\\epsilon\\alpha$ independent of the number of samples per user $m$. Moreover,\nwe show that any mechanism that only operates on the final aggregate counts\nshould require a user complexity of the same order. We then propose a mechanism\nsuch that the number of users scales as $\\tilde{\\mathcal{O}}(k/(m\\alpha^2) +\nk/\\sqrt{m}\\epsilon\\alpha)$ and hence the privacy penalty is\n$\\tilde{\\Theta}(\\sqrt{m})$ times smaller compared to the standard mechanisms in\ncertain settings of interest. We further show that the proposed mechanism is\nnearly-optimal under certain regimes.\n  We also propose general techniques for obtaining lower bounds on restricted\ndifferentially private estimators and a lower bound on the total variation\nbetween binomial distributions, both of which might be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 16:15:14 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 17:42:38 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 22:15:07 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Liu", "Yuhan", ""], ["Suresh", "Ananda Theertha", ""], ["Yu", "Felix", ""], ["Kumar", "Sanjiv", ""], ["Riley", "Michael", ""]]}, {"id": "2007.13694", "submitter": "Sepideh Aghamolaei", "authors": "Sepideh Aghamolaei", "title": "Symmetries: From Proofs To Algorithms And Back", "comments": "The definition of symmetry discussed here is too general, and the\n  examples in this paper has mislead people to believe this is a desirable or\n  useful property. A counter-example is SAT, where there is a symmetry between\n  each variable and its complement, and it is equivalent to the original\n  problem", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We call an objective function or algorithm symmetric with respect to an input\nif after swapping two parts of the input in any algorithm, the solution of the\nalgorithm and the output remain the same. More formally, for a permutation\n$\\pi$ of an indexed input, and another permutation $\\pi'$ of the same input,\nsuch that swapping two items converts $\\pi$ to $\\pi'$, $f(\\pi)=f(\\pi')$, where\n$f$ is the objective function.\n  After reviewing samples of the algorithms that exploit symmetry, we give\nseveral new ones, for finding lower-bounds, beating adversaries in online\nalgorithms, designing parallel algorithms and data summarization. We show how\nto use the symmetry between the sampled points to get a lower/upper bound on\nthe solution. This mostly depends on the equivalence class of the parts of the\ninput that when swapped, do not change the solution or its cost.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 17:15:36 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 10:06:48 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 08:20:35 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Aghamolaei", "Sepideh", ""]]}, {"id": "2007.14028", "submitter": "Yanhao Wang", "authors": "Jingjing Wang and Yanhao Wang and Wenjun Jiang and Yuchen Li and\n  Kian-Lee Tan", "title": "Efficient Sampling Algorithms for Approximate Temporal Motif Counting\n  (Extended Version)", "comments": "17 pages, 9 figures, to appear in CIKM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A great variety of complex systems ranging from user interactions in\ncommunication networks to transactions in financial markets can be modeled as\ntemporal graphs, which consist of a set of vertices and a series of timestamped\nand directed edges. Temporal motifs in temporal graphs are generalized from\nsubgraph patterns in static graphs which take into account edge orderings and\ndurations in addition to structures. Counting the number of occurrences of\ntemporal motifs is a fundamental problem for temporal network analysis.\nHowever, existing methods either cannot support temporal motifs or suffer from\nperformance issues. In this paper, we focus on approximate temporal motif\ncounting via random sampling. We first propose a generic edge sampling (ES)\nalgorithm for estimating the number of instances of any temporal motif.\nFurthermore, we devise an improved EWS algorithm that hybridizes edge sampling\nwith wedge sampling for counting temporal motifs with 3 vertices and 3 edges.\nWe provide comprehensive analyses of the theoretical bounds and complexities of\nour proposed algorithms. Finally, we conduct extensive experiments on several\nreal-world datasets, and the results show that our ES and EWS algorithms have\nhigher efficiency, better accuracy, and greater scalability than the\nstate-of-the-art sampling method for temporal motif counting.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 07:15:25 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Wang", "Jingjing", ""], ["Wang", "Yanhao", ""], ["Jiang", "Wenjun", ""], ["Li", "Yuchen", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "2007.14092", "submitter": "Petteri Kaski", "authors": "Andreas Bj\\\"orklund, Petteri Kaski", "title": "Counting Short Vector Pairs by Inner Product and Relations to the\n  Permanent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given as input two $n$-element sets $\\mathcal A,\\mathcal B\\subseteq\\{0,1\\}^d$\nwith $d=c\\log n\\leq(\\log n)^2/(\\log\\log n)^4$ and a target $t\\in\n\\{0,1,\\ldots,d\\}$, we show how to count the number of pairs $(x,y)\\in \\mathcal\nA\\times \\mathcal B$ with integer inner product $\\langle x,y \\rangle=t$\ndeterministically, in $n^2/2^{\\Omega\\bigl(\\!\\sqrt{\\log n\\log \\log n/(c\\log^2\nc)}\\bigr)}$ time. This demonstrates that one can solve this problem in\ndeterministic subquadratic time almost up to $\\log^2 n$ dimensions, nearly\nmatching the dimension bound of a subquadratic randomized detection algorithm\nof Alman and Williams [FOCS 2015]. We also show how to modify their randomized\nalgorithm to count the pairs w.h.p., to obtain a fast randomized algorithm. Our\ndeterministic algorithm builds on a novel technique of reconstructing a\nfunction from sum-aggregates by prime residues, which can be seen as an {\\em\nadditive} analog of the Chinese Remainder Theorem. As our second contribution,\nwe relate the fine-grained complexity of the task of counting of vector pairs\nby inner product to the task of computing a zero-one matrix permanent over the\nintegers.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 09:55:29 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""], ["Kaski", "Petteri", ""]]}, {"id": "2007.14142", "submitter": "Pratik Ghosal", "authors": "Pratik Ghosal and Syed Mohammad Meesum and Katarzyna Paluch", "title": "Rectangle Tiling Binary Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of rectangle tiling binary arrays is defined as follows. Given an\n$n \\times n$ array $A$ of zeros and ones and a natural number $p$, our task is\nto partition $A$ into at most $p$ rectangular tiles, so that the maximal weight\nof a tile is minimized. A tile is any rectangular subarray of $A$. The weight\nof a tile is the sum of elements that fall within it. We present a linear\n$(O(n^2))$ time $(\\frac{3}{2}+\\frac{p^2}{w(A)})$-approximation algorithm for\nthis problem, where $w(A)$ denotes the weight of the whole array $A$.\n  The algorithm employs the lower bound of $L=\\lceil \\frac{w(A)}{p} \\rceil$,\nwhich is the same lower bound on the optimum that was used in all algorithms\nfor rectangle tiling. We prove that a better approximation factor for the\nbinary \\RTILE cannot be achieved using the same lower bound $L$, because there\nexist arrays, whose every partition contains a tile of weight at least\n$(\\frac{3}{2}+\\frac{p^2}{w(A)})L$. The previously known approximation algorithm\nfor rectangle tiling binary arrays achieved the ratio of $2$. We also consider\nthe dual problem of rectangle tiling for binary arrays, where we are given an\nupper bound on the weight of the tiles, and we have to cover the array $A$ with\nthe minimum number of non-overlapping tiles. Both problems have natural\nextensions to $d$-dimensional versions, for which we provide analogous results.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 12:00:08 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Ghosal", "Pratik", ""], ["Meesum", "Syed Mohammad", ""], ["Paluch", "Katarzyna", ""]]}, {"id": "2007.14156", "submitter": "Nikhil Kumar", "authors": "Naveen Garg, Nikhil Kumar", "title": "Dual Half-integrality for Uncrossable Cut Cover and its Application to\n  Maximum Half-Integral Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an edge weighted graph and a forest $F$, the $\\textit{2-edge\nconnectivity augmentation problem}$ is to pick a minimum weighted set of edges,\n$E'$, such that every connected component of $E'\\cup F$ is 2-edge connected.\nWilliamson et al. gave a 2-approximation algorithm (WGMV) for this problem\nusing the primal-dual schema. We show that when edge weights are integral, the\nWGMV procedure can be modified to obtain a half-integral dual. The 2-edge\nconnectivity augmentation problem has an interesting connection to routing flow\nin graphs where the union of supply and demand is planar. The half-integrality\nof the dual leads to a tight 2-approximate max-half-integral-flow min-multicut\ntheorem.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 12:23:05 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Garg", "Naveen", ""], ["Kumar", "Nikhil", ""]]}, {"id": "2007.14161", "submitter": "\\'Edouard Bonnet", "authors": "\\'Edouard Bonnet, Colin Geniet, Eun Jung Kim, St\\'ephan Thomass\\'e,\n  R\\'emi Watrigant", "title": "Twin-width III: Max Independent Set, Min Dominating Set, and Coloring", "comments": "38 pages, 6 figures. This version contains more results, notably the\n  approximation for Min Dominating Set, and the title has been edited\n  accordingly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recently introduced the graph invariant twin-width, and showed that\nfirst-order model checking can be solved in time $f(d,k)n$ for $n$-vertex\ngraphs given with a witness that the twin-width is at most $d$, called\n$d$-contraction sequence or $d$-sequence, and formulas of size $k$ [Bonnet et\nal., FOCS '20]. The inevitable price to pay for such a general result is that\n$f$ is a tower of exponentials of height roughly $k$. In this paper, we show\nthat algorithms based on twin-width need not be impractical. We present\n$2^{O(k)}n$-time algorithms for $k$-Independent Set, $r$-Scattered Set,\n$k$-Clique, and $k$-Dominating Set when an $O(1)$-sequence is provided. We\nfurther show how to solve weighted $k$-Independent Set, Subgraph Isomorphism,\nand Induced Subgraph Isomorphism, in time $2^{O(k \\log k)}n$. These algorithms\nare based on a dynamic programming scheme following the sequence of\ncontractions forward. We then show a second algorithmic use of the contraction\nsequence, by starting at its end and rewinding it. As an example, we establish\nthat bounded twin-width classes are $\\chi$-bounded. This significantly extends\nthe $\\chi$-boundedness of bounded rank-width classes, and does so with a very\nconcise proof. The third algorithmic use of twin-width builds on the second\none. Playing the contraction sequence backward, we show that bounded twin-width\ngraphs can be edge-partitioned into a linear number of bicliques, such that\nboth sides of the bicliques are on consecutive vertices, in a fixed vertex\nordering. Given that biclique edge-partition, we show how to solve the\nunweighted Single-Source Shortest Paths and hence All-Pairs Shortest Paths in\nsublinear time $O(n \\log n)$ and time $O(n^2 \\log n)$, respectively. Finally we\nshow that Min Dominating Set and related problems have constant integrality\ngaps on bounded twin-width classes, thereby getting constant approximations on\nthese classes.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 12:36:03 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 12:32:34 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Geniet", "Colin", ""], ["Kim", "Eun Jung", ""], ["Thomass\u00e9", "St\u00e9phan", ""], ["Watrigant", "R\u00e9mi", ""]]}, {"id": "2007.14179", "submitter": "\\'Edouard Bonnet", "authors": "Benjamin Bergougnoux, \\'Edouard Bonnet, Nick Brettell, O-joung Kwon", "title": "Close relatives of Feedback Vertex Set without single-exponential\n  algorithms parameterized by treewidth", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cut & Count technique and the rank-based approach have lead to\nsingle-exponential FPT algorithms parameterized by treewidth, that is, running\nin time $2^{O(tw)}n^{O(1)}$, for Feedback Vertex Set and connected versions of\nthe classical graph problems (such as Vertex Cover and Dominating Set). We show\nthat Subset Feedback Vertex Set, Subset Odd Cycle Transversal, Restricted\nEdge-Subset Feedback Edge Set, Node Multiway Cut, and Multiway Cut are unlikely\nto have such running times. More precisely, we match algorithms running in time\n$2^{O(tw \\log tw)}n^{O(1)}$ with tight lower bounds under the Exponential-Time\nHypothesis (ETH), ruling out $2^{o(tw \\log tw)}n^{O(1)}$, where $n$ is the\nnumber of vertices and $tw$ is the treewidth of the input graph. Our algorithms\nextend to the weighted case, while our lower bounds also hold for the larger\nparameter pathwidth and do not require weights. We also show that, in contrast\nto Odd Cycle Transversal, there is no $2^{o(tw \\log tw)}n^{O(1)}$-time\nalgorithm for Even Cycle Transversal under the ETH.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 13:06:53 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Bergougnoux", "Benjamin", ""], ["Bonnet", "\u00c9douard", ""], ["Brettell", "Nick", ""], ["Kwon", "O-joung", ""]]}, {"id": "2007.14204", "submitter": "Arnold Filtser", "authors": "Arnold Filtser, Michael Kapralov, Navid Nouri", "title": "Graph Spanners by Sketching in Dynamic Streams and the Simultaneous\n  Communication Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph sketching is a powerful technique introduced by the seminal work of\nAhn, Guha and McGregor'12 on connectivity in dynamic graph streams that has\nenjoyed considerable attention in the literature since then, and has led to\nnear optimal dynamic streaming algorithms for many fundamental problems such as\nconnectivity, cut and spectral sparsifiers and matchings. Interestingly,\nhowever, the sketching and dynamic streaming complexity of approximating the\nshortest path metric of a graph is still far from well-understood. Besides a\ndirect $k$-pass implementation of classical spanner constructions (recently\nimproved to $\\lfloor\\frac k2\\rfloor+1$-passes by Fernandez, Woodruff and\nYasuda'20) the state of the art amounts to a $O(\\log k)$-pass algorithm of Ahn,\nGuha and McGregor'12, and a $2$-pass algorithm of Kapralov and Woodruff'14. In\nparticular, no single pass algorithm is known, and the optimal tradeoff between\nthe number of passes, stretch and space complexity is open.\n  In this paper we introduce several new graph sketching techniques for\napproximating the shortest path metric of the input graph. We give the first\n{\\em single pass} sketching algorithm for constructing graph spanners: we show\nhow to obtain a $\\widetilde{O}(n^{\\frac23})$-spanner using $\\widetilde{O}(n)$\nspace, and in general a $\\widetilde{O}(n^{\\frac23(1-\\alpha)})$-spanner using\n$\\widetilde{O}(n^{1+\\alpha})$ space for every $\\alpha\\in [0, 1]$, a tradeoff\nthat we think may be close optimal. We also give new spanner construction\nalgorithms for any number of passes, simultaneously improving upon all prior\nwork on this problem. Finally, we study the simultaneous communication model\nand propose the first protocols with low per player information.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 13:36:43 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 21:19:01 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Filtser", "Arnold", ""], ["Kapralov", "Michael", ""], ["Nouri", "Navid", ""]]}, {"id": "2007.14225", "submitter": "Mingyu Xiao", "authors": "Zhenyu Guo, Mingyu Xiao and Yi Zhou", "title": "The Complexity of the Partition Coloring Problem", "comments": "To appear in TAMC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a simple undirected graph $G=(V,E)$ and a partition of the vertex set\n$V$ into $p$ parts, the \\textsc{Partition Coloring Problem} asks if we can\nselect one vertex from each part of the partition such that the chromatic\nnumber of the subgraph induced on the $p$ selected vertices is bounded by $k$.\nPCP is a generalized problem of the classical \\textsc{Vertex Coloring Problem}\nand has applications in many areas, such as scheduling and encoding etc.\n  In this paper, we show the complexity status of the \\textsc{Partition\nColoring Problem} with three parameters: the number of colors, the number of\nparts of the partition, and the maximum size of each part of the partition.\n  Furthermore, we give a new exact algorithm for this problem.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 13:51:11 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Guo", "Zhenyu", ""], ["Xiao", "Mingyu", ""], ["Zhou", "Yi", ""]]}, {"id": "2007.14307", "submitter": "Yuval Gil", "authors": "Yuval Emek and Yuval Gil", "title": "Twenty-Two New Approximate Proof Labeling Schemes (Full Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduced by Korman, Kutten, and Peleg (Distributed Computing 2005), a\n\\emph{proof labeling scheme (PLS)} is a system dedicated to verifying that a\ngiven configuration graph satisfies a certain property.\n  It is composed of a centralized \\emph{prover}, whose role is to generate a\nproof for yes-instances in the form of an assignment of labels to the nodes,\nand a distributed \\emph{verifier}, whose role is to verify the validity of the\nproof by local means and accept it if and only if the property is satisfied.\n  To overcome lower bounds on the label size of PLSs for certain graph\nproperties, Censor-Hillel, Paz, and Perry (SIROCCO 2017) introduced the notion\nof an \\emph{approximate proof labeling scheme (APLS)} that allows the verifier\nto accept also some no-instances as long as they are not \"too far\" from\nsatisfying the property.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 15:20:34 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 06:35:50 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 12:56:09 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Emek", "Yuval", ""], ["Gil", "Yuval", ""]]}, {"id": "2007.14339", "submitter": "Ajinkya Ramdas Gaikwad", "authors": "Ajinkya Gaikwad, Soumen Maity, Shuvam Kant Tripathi", "title": "The Satisfactory Partition Problem", "comments": "1-15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Satisfactory Partition problem consists in deciding if the set of\nvertices of a given undirected graph can be partitioned into two nonempty parts\nsuch that each vertex has at least as many neighbours in its part as in the\nother part. This problem was introduced by Gerber and Kobler [European J. Oper.\nRes. 125 (2000) 283-291] and further studied by other authors, but its\nparameterized complexity remains open until now. It is known that the\nSatisfactory Partition problem, as well as a variant where the parts are\nrequired to be of the same cardinality, are NP-complete. We enhance our\nunderstanding of the problem from the viewpoint of parameterized complexity by\nshowing that (1) the problem is FPT when parameterized by the neighbourhood\ndiversity of the input graph, (2) it can be solved in $O(n^{8 {\\tt cw}})$ where\n${\\tt cw}$ is the clique-width,(3) a generalized version of the problem is\nW[1]-hard when parameterized by the treewidth.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 16:15:07 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Gaikwad", "Ajinkya", ""], ["Maity", "Soumen", ""], ["Tripathi", "Shuvam Kant", ""]]}, {"id": "2007.14368", "submitter": "Joshua Brakensiek", "authors": "Joshua Brakensiek and Moses Charikar and Aviad Rubinstein", "title": "A Simple Sublinear Algorithm for Gap Edit Distance", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the edit distance between two\n$n$-character strings. While exact computation in the worst case is believed to\nrequire near-quadratic time, previous work showed that in certain regimes it is\npossible to solve the following {\\em gap edit distance} problem in sub-linear\ntime: distinguish between inputs of distance $\\le k$ and $>k^2$. Our main\nresult is a very simple algorithm for this benchmark that runs in time $\\tilde\nO(n/\\sqrt{k})$, and in particular settles the open problem of obtaining a truly\nsublinear time for the entire range of relevant $k$.\n  Building on the same framework, we also obtain a $k$-vs-$k^2$ algorithm for\nthe one-sided preprocessing model with $\\tilde O(n)$ preprocessing time and\n$\\tilde O(n/k)$ query time (improving over a recent $\\tilde O(n/k+k^2)$-query\ntime algorithm for the same problem [GRS'20].\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 17:19:03 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Brakensiek", "Joshua", ""], ["Charikar", "Moses", ""], ["Rubinstein", "Aviad", ""]]}, {"id": "2007.14502", "submitter": "Viresh Patel", "authors": "Viresh Patel and Fabian Stroh", "title": "A polynomial-time algorithm to determine (almost) Hamiltonicity of dense\n  regular graphs", "comments": "35 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a polynomial-time algorithm for detecting very long cycles in dense\nregular graphs. Specifically, we show that, given $\\alpha \\in (0,1)$, there\nexists a $c=c(\\alpha)$ such that the following holds: there is a\npolynomial-time algorithm that, given a $D$-regular graph $G$ on $n$ vertices\nwith $D\\geq \\alpha n$, determines whether $G$ contains a cycle on at least $n -\nc$ vertices. The problem becomes NP-complete if we drop either the density or\nthe regularity condition. The algorithm combines tools from extremal graph\ntheory and spectral partitioning as well as some further algorithmic\ningredients.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 22:19:00 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Patel", "Viresh", ""], ["Stroh", "Fabian", ""]]}, {"id": "2007.14514", "submitter": "Daniel Paulusma", "authors": "Nick Brettell and Matthew Johnson and Daniel Paulusma", "title": "Computing Weighted Subset Transversals in $H$-Free Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the Odd Cycle Transversal problem, the task is to find a small set $S$ of\nvertices in a graph that intersects every cycle of odd length. The Subset Odd\nCycle Transversal problem requires S to intersect only those odd cycles that\ninclude a vertex of a distinguished vertex subset $T$. If we are given weights\nfor the vertices, we ask instead that $S$ has small weight: this is the problem\nWeighted Subset Odd Cycle Transversal. We prove an almost-complete complexity\ndichotomy for Weighted Subset Odd Cycle Transversal for graphs that do not\ncontain a graph $H$ as an induced subgraph. Our general approach can also be\nused for Weighted Subset Feedback Vertex Set, which enables us to generalize a\nrecent result of Papadopoulos and Tzimas.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 22:51:07 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 18:01:31 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Brettell", "Nick", ""], ["Johnson", "Matthew", ""], ["Paulusma", "Daniel", ""]]}, {"id": "2007.14539", "submitter": "Dhruv Rohatgi", "authors": "Constantinos Daskalakis, Dhruv Rohatgi, Manolis Zampetakis", "title": "Truncated Linear Regression in High Dimensions", "comments": "30 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As in standard linear regression, in truncated linear regression, we are\ngiven access to observations $(A_i, y_i)_i$ whose dependent variable equals\n$y_i= A_i^{\\rm T} \\cdot x^* + \\eta_i$, where $x^*$ is some fixed unknown vector\nof interest and $\\eta_i$ is independent noise; except we are only given an\nobservation if its dependent variable $y_i$ lies in some \"truncation set\" $S\n\\subset \\mathbb{R}$. The goal is to recover $x^*$ under some favorable\nconditions on the $A_i$'s and the noise distribution. We prove that there\nexists a computationally and statistically efficient method for recovering\n$k$-sparse $n$-dimensional vectors $x^*$ from $m$ truncated samples, which\nattains an optimal $\\ell_2$ reconstruction error of $O(\\sqrt{(k \\log n)/m})$.\nAs a corollary, our guarantees imply a computationally efficient and\ninformation-theoretically optimal algorithm for compressed sensing with\ntruncation, which may arise from measurement saturation effects. Our result\nfollows from a statistical and computational analysis of the Stochastic\nGradient Descent (SGD) algorithm for solving a natural adaptation of the LASSO\noptimization problem that accommodates truncation. This generalizes the works\nof both: (1) [Daskalakis et al. 2018], where no regularization is needed due to\nthe low-dimensionality of the data, and (2) [Wainright 2009], where the\nobjective function is simple due to the absence of truncation. In order to deal\nwith both truncation and high-dimensionality at the same time, we develop new\ntechniques that not only generalize the existing ones but we believe are of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 00:31:34 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Rohatgi", "Dhruv", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "2007.14569", "submitter": "Long Gong", "authors": "Long Gong, Ziheng Liu, Liang Liu, Jun Xu, Mitsunori Ogihara, Tong Yang", "title": "Space- and Computationally-Efficient Set Reconciliation via Parity\n  Bitmap Sketch (PBS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set reconciliation is a fundamental algorithmic problem that arises in many\nnetworking, system, and database applications. In this problem, two large sets\nA and B of objects (bitcoins, files, records, etc.) are stored respectively at\ntwo different network-connected hosts, which we name Alice and Bob\nrespectively. Alice and Bob communicate with each other to learn $A\\Delta B$,\nthe difference between A and B, and as a result the reconciled set $A\\bigcup\nB$.\n  Current set reconciliation schemes are based on either Invertible Bloom\nFilters (IBF) or Error-Correction Codes (ECC). The former has a low\ncomputational complexity of O(d), where d is the cardinality of $A\\Delta B$,\nbut has a high communication overhead that is several times larger than the\ntheoretical minimum. The latter has a low communication overhead close to the\ntheoretical minimum, but has a much higher computational complexity of\n$O(d^2)$. In this work, we propose Parity Bitmap Sketch (PBS), an ECC- based\nset reconciliation scheme that gets the better of both worlds: PBS has both a\nlow computational complexity of O(d) just like IBF-based solutions and a low\ncommunication overhead of roughly twice the theoretical minimum. A separate\ncontribution of this work is a novel rigorous analytical framework that can be\nused for the precise calculation of various performance metrics and for the\nnear-optimal parameter tuning of PBS.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 03:15:23 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 06:29:14 GMT"}, {"version": "v3", "created": "Sat, 15 Aug 2020 22:47:02 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Gong", "Long", ""], ["Liu", "Ziheng", ""], ["Liu", "Liang", ""], ["Xu", "Jun", ""], ["Ogihara", "Mitsunori", ""], ["Yang", "Tong", ""]]}, {"id": "2007.14775", "submitter": "Giorgio Barnabo' Mr.", "authors": "Giorgio Barnabo', Carlos Castillo, Michael Mathioudakis, Sergio Celis", "title": "Intersectional Affirmative Action Policies for Top-k Candidates\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of selecting the top-k candidates from a pool of\napplicants, where each candidate is associated with a score indicating his/her\naptitude. Depending on the specific scenario, such as job search or college\nadmissions, these scores may be the results of standardized tests or other\npredictors of future performance and utility. We consider a situation in which\nsome groups of candidates experience historical and present disadvantage that\nmakes their chances of being accepted much lower than other groups. In these\ncircumstances, we wish to apply an affirmative action policy to reduce\nacceptance rate disparities, while avoiding any large decrease in the aptitude\nof the candidates that are eventually selected. Our algorithmic design is\nmotivated by the frequently observed phenomenon that discrimination\ndisproportionately affects individuals who simultaneously belong to multiple\ndisadvantaged groups, defined along intersecting dimensions such as gender,\nrace, sexual orientation, socio-economic status, and disability. In short, our\nalgorithm's objective is to simultaneously: select candidates with high\nutility, and level up the representation of disadvantaged intersectional\nclasses. This naturally involves trade-offs and is computationally challenging\ndue to the the combinatorial explosion of potential subgroups as more\nattributes are considered. We propose two algorithms to solve this problem,\nanalyze them, and evaluate them experimentally using a dataset of university\napplication scores and admissions to bachelor degrees in an OECD country. Our\nconclusion is that it is possible to significantly reduce disparities in\nadmission rates affecting intersectional classes with a small loss in terms of\nselected candidate aptitude. To the best of our knowledge, we are the first to\nstudy fairness constraints with regards to intersectional classes in the\ncontext of top-k selection.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 12:27:18 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 16:21:37 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Barnabo'", "Giorgio", ""], ["Castillo", "Carlos", ""], ["Mathioudakis", "Michael", ""], ["Celis", "Sergio", ""]]}, {"id": "2007.14898", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang, Thatchaphol Saranurak", "title": "Deterministic Distributed Expander Decomposition and Routing with\n  Applications in Distributed Derandomization", "comments": "To appear in FOCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a recent exciting line of work in distributed graph algorithms in\nthe $\\mathsf{CONGEST}$ model that exploit expanders. All these algorithms so\nfar are based on two tools: expander decomposition and expander routing. An\n$(\\epsilon,\\phi)$-expander decomposition removes $\\epsilon$-fraction of the\nedges so that the remaining connected components have conductance at least\n$\\phi$, i.e., they are $\\phi$-expanders, and expander routing allows each\nvertex $v$ in a $\\phi$-expander to very quickly exchange $\\text{deg}(v)$\nmessages with any other vertices, not just its local neighbors.\n  In this paper, we give the first efficient deterministic distributed\nalgorithms for both tools. We show that an $(\\epsilon,\\phi)$-expander\ndecomposition can be deterministically computed in $\\text{poly}(\\epsilon^{-1})\nn^{o(1)}$ rounds for $\\phi = \\text{poly}(\\epsilon) n^{-o(1)}$, and that\nexpander routing can be performed deterministically in\n$\\text{poly}(\\phi^{-1})n^{o(1)}$ rounds. Both results match previous bounds of\nrandomized algorithms by [Chang and Saranurak, PODC 2019] and [Ghaffari, Kuhn,\nand Su, PODC 2017] up to subpolynomial factors.\n  Consequently, we derandomize existing distributed algorithms that exploit\nexpanders. We show that a minimum spanning tree on $n^{o(1)}$-expanders can be\nconstructed deterministically in $n^{o(1)}$ rounds, and triangle detection and\nenumeration on general graphs can be solved deterministically in $O(n^{0.58})$\nand $n^{2/3 + o(1)}$ rounds, respectively.\n  We also give the first polylogarithmic-round randomized algorithm for\nconstructing an $(\\epsilon,\\phi)$-expander decomposition in\n$\\text{poly}(\\epsilon^{-1}, \\log n)$ rounds for $\\phi = 1 /\n\\text{poly}(\\epsilon^{-1}, \\log n)$. The previous algorithm by [Chang and\nSaranurak, PODC 2019] needs $n^{\\Omega(1)}$ rounds for any $\\phi\\ge\n1/\\text{poly}\\log n$.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:15:06 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "2007.15102", "submitter": "Sergey Pupyrev", "authors": "Sergey Pupyrev", "title": "Book Embeddings of Graph Products", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $k$-stack layout (also called a $k$-page book embedding) of a graph\nconsists of a total order of the vertices, and a partition of the edges into\n$k$ sets of non-crossing edges with respect to the vertex order. The stack\nnumber (book thickness, page number) of a graph is the minimum $k$ such that it\nadmits a $k$-stack layout. A $k$-queue layout is defined similarly, except that\nno two edges in a single set may be nested.\n  It was recently proved that graphs of various non-minor-closed classes are\nsubgraphs of the strong product of a path and a graph with bounded treewidth.\nMotivated by this decomposition result, we explore stack layouts of graph\nproducts. We show that the stack number is bounded for the strong product of a\npath and (i) a graph of bounded pathwidth or (ii) a bipartite graph of bounded\ntreewidth and bounded degree. The results are obtained via a novel concept of\nsimultaneous stack-queue layouts, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 20:48:52 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Pupyrev", "Sergey", ""]]}, {"id": "2007.15125", "submitter": "Alexandros Hollender", "authors": "Argyrios Deligkas, Aris Filos-Ratsikas, Alexandros Hollender", "title": "Two's Company, Three's a Crowd: Consensus-Halving for a Constant Number\n  of Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the $\\varepsilon$-Consensus-Halving problem, in which a set of\nheterogeneous agents aim at dividing a continuous resource into two (not\nnecessarily contiguous) portions that all of them simultaneously consider to be\nof approximately the same value (up to $\\varepsilon$). This problem was\nrecently shown to be PPA-complete, for $n$ agents and $n$ cuts, even for very\nsimple valuation functions. In a quest to understand the root of the complexity\nof the problem, we consider the setting where there is only a constant number\nof agents, and we consider both the computational complexity and the query\ncomplexity of the problem.\n  For agents with monotone valuation functions, we show a dichotomy: for two\nagents the problem is polynomial-time solvable, whereas for three or more\nagents it becomes PPA-complete. Similarly, we show that for two monotone agents\nthe problem can be solved with polynomially-many queries, whereas for three or\nmore agents, we provide exponential query complexity lower bounds. These\nresults are enabled via an interesting connection to a monotone Borsuk-Ulam\nproblem, which may be of independent interest. For agents with general\nvaluations, we show that the problem is PPA-complete and admits exponential\nquery complexity lower bounds, even for two agents.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 21:46:01 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Deligkas", "Argyrios", ""], ["Filos-Ratsikas", "Aris", ""], ["Hollender", "Alexandros", ""]]}, {"id": "2007.15154", "submitter": "Jiajian Liang", "authors": "Qian-Ping Gu, Jiajian Leo Liang, Guochuan Zhang", "title": "Approximate Ridesharing of Personal Vehicles Problem", "comments": "39 pages, 6 figures", "journal-ref": "Theoretical Computer Science, Volume 871, 6 June 2021, Pages 30-50", "doi": "10.1016/j.tcs.2021.04.009", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ridesharing problem is that given a set of trips, each trip consists of\nan individual, a vehicle of the individual and some requirements, select a\nsubset of trips and use the vehicles of selected trips to deliver all\nindividuals to their destinations satisfying the requirements. Requirements of\ntrips are specified by parameters including source, destination, vehicle\ncapacity, preferred paths of a driver, detour distance and number of stops a\ndriver is willing to make, and time constraints. We analyze the relations\nbetween the time complexity and parameters for two optimization problems:\nminimizing the number of selected vehicles and minimizing total travel distance\nof the vehicles. We consider the following conditions: (1) all trips have the\nsame source or same destination, (2) no detour is allowed, (3) each participant\nhas one preferred path, (4) no limit on the number of stops, and (5) all trips\nhave the same departure and same arrival time. It is known that both\nminimization problems are NP-hard if one of Conditions (1), (2) and (3) is not\nsatisfied. We prove that both problems are NP-hard and further show that it is\nNP-hard to approximate both problems within a constant factor if Conditions (4)\nor (5) is not satisfied. We give $\\frac{K+2}{2}$-approximation algorithms for\nminimizing the number of selected vehicles when condition (4) is not satisfied,\nwhere $K$ is the largest capacity of all vehicles.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 00:01:22 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 21:44:40 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gu", "Qian-Ping", ""], ["Liang", "Jiajian Leo", ""], ["Zhang", "Guochuan", ""]]}, {"id": "2007.15192", "submitter": "Yatharth Dubey", "authors": "Santanu S. Dey, Yatharth Dubey, Marco Molinaro", "title": "Branch-and-Bound Solves Random Binary IPs in Polytime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Branch-and-bound is the workhorse of all state-of-the-art mixed integer\nlinear programming (MILP) solvers. These implementations of branch-and-bound\ntypically use variable branching, that is, the child nodes are obtained by\nfixing some variable to an integer value $v$ in one node and to $v + 1$ in the\nother node. Even though modern MILP solvers are able to solve very large-scale\ninstances efficiently, relatively little attention has been given to\nunderstanding why the underlying branch-and-bound algorithm performs so well.\nIn this paper our goal is to theoretically analyze the performance of the\nstandard variable branching based branch-and-bound algorithm. In order to avoid\nthe exponential worst-case lower bounds, we follow the common idea of\nconsidering random instances. More precisely, we consider random integer\nprograms where the entries of the coefficient matrix and the objective function\nare randomly sampled.\n  Our main result is that with good probability branch-and-bound with variable\nbranching explores only a polynomial number of nodes to solve these instances,\nfor a fixed number of constraints. To the best of our knowledge this is the\nfirst known such result for a standard version of branch-and-bound. We believe\nthat this result provides a compelling indication of why branch-and-bound with\nvariable branching works so well in practice.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 02:40:00 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 02:48:18 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 20:45:36 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Dey", "Santanu S.", ""], ["Dubey", "Yatharth", ""], ["Molinaro", "Marco", ""]]}, {"id": "2007.15203", "submitter": "Vijay  Menon", "authors": "Vijay Menon and Kate Larson", "title": "Algorithmic Stability in Fair Allocation of Indivisible Goods Among Two\n  Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many allocation problems in multiagent systems rely on agents specifying\ncardinal preferences. However, allocation mechanisms can be sensitive to small\nperturbations in cardinal preferences, thus causing agents who make ``small\" or\n``innocuous\" mistakes while reporting their preferences to experience a large\nchange in their utility for the final outcome. To address this, we introduce a\nnotion of algorithmic stability and study it in the context of fair and\nefficient allocations of indivisible goods among two agents. We show that it is\nimpossible to achieve exact stability along with even a weak notion of fairness\nand even approximate efficiency. As a result, we propose two relaxations to\nstability, namely, approximate-stability and weak-approximate-stability, and\nshow how existing algorithms in the fair division literature that guarantee\nfair and efficient outcomes perform poorly with respect to these relaxations.\nThis leads us to explore the possibility of designing new algorithms that are\nmore stable. Towards this end, we present a general characterization result for\npairwise maximin share allocations, and in turn use it to design an algorithm\nthat is approximately-stable and guarantees a pairwise maximin share and Pareto\noptimal allocation for two agents. Finally, we present a simple framework that\ncan be used to modify existing fair and efficient algorithms in order to ensure\nthat they also achieve weak-approximate-stability.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 03:09:02 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 15:58:27 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Menon", "Vijay", ""], ["Larson", "Kate", ""]]}, {"id": "2007.15219", "submitter": "Harsh Bhatia", "authors": "Harsh Bhatia, Duong Hoang, Garrett Morrison, Will Usher, Valerio\n  Pascucci, Peer-Timo Bremer, Peter Lindstrom", "title": "AMM: Adaptive Multilinear Meshes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Adaptive Multilinear Meshes (AMM), a new framework that\nsignificantly reduces the memory footprint compared to existing data\nstructures. AMM uses a hierarchy of cuboidal cells to create continuous,\npiecewise multilinear representation of uniformly sampled data. Furthermore,\nAMM can selectively relax or enforce constraints on conformity, continuity, and\ncoverage, creating a highly adaptive and flexible representation to support a\nwide range of use cases. AMM supports incremental updates in both spatial\nresolution and numerical precision establishing the first practical data\nstructure that can seamlessly explore the tradeoff between resolution and\nprecision. We use tensor products of linear B-spline wavelets to create an\nadaptive representation and illustrate the advantages of our framework. AMM\nprovides a simple interface for evaluating the function defined on the adaptive\nmesh, efficiently traversing the mesh, and manipulating the mesh, including\nincremental, partial updates. Our framework is easy to adopt for standard\nvisualization and analysis tasks. As an example, we provide a VTK interface,\nthrough efficient on-demand conversion, which can be used directly by\ncorresponding tools, such as VisIt, disseminating the advantages of faster\nprocessing and a smaller memory footprint to a wider audience. We demonstrate\nthe advantages of our approach for simplifying scalar-valued data for commonly\nused visualization and analysis tasks using incremental construction, according\nto mixed resolution and precision data streams.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 04:18:14 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Bhatia", "Harsh", ""], ["Hoang", "Duong", ""], ["Morrison", "Garrett", ""], ["Usher", "Will", ""], ["Pascucci", "Valerio", ""], ["Bremer", "Peer-Timo", ""], ["Lindstrom", "Peter", ""]]}, {"id": "2007.15220", "submitter": "Pasin Manurangsi", "authors": "Ilias Diakonikolas, Daniel M. Kane, Pasin Manurangsi", "title": "The Complexity of Adversarially Robust Proper Learning of Halfspaces\n  with Agnostic Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of adversarially robust proper learning\nof halfspaces in the distribution-independent agnostic PAC model, with a focus\non $L_p$ perturbations. We give a computationally efficient learning algorithm\nand a nearly matching computational hardness result for this problem. An\ninteresting implication of our findings is that the $L_{\\infty}$ perturbations\ncase is provably computationally harder than the case $2 \\leq p < \\infty$.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 04:18:51 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Manurangsi", "Pasin", ""]]}, {"id": "2007.15251", "submitter": "Yannic Maus", "authors": "Yannic Maus and Tigran Tonoyan", "title": "Local Conflict Coloring Revisited: Linial for Lists", "comments": "to appear at DISC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linial's famous color reduction algorithm reduces a given $m$-coloring of a\ngraph with maximum degree $\\Delta$ to a $O(\\Delta^2\\log m)$-coloring, in a\nsingle round in the LOCAL model. We show a similar result when nodes are\nrestricted to choose their color from a list of allowed colors: given an\n$m$-coloring in a directed graph of maximum outdegree $\\beta$, if every node\nhas a list of size $\\Omega(\\beta^2 (\\log \\beta+\\log\\log m + \\log \\log\n|\\mathcal{C}|))$ from a color space $\\mathcal{C}$ then they can select a color\nin two rounds in the LOCAL model. Moreover, the communication of a node\nessentially consists of sending its list to the neighbors. This is obtained as\npart of a framework that also contains Linial's color reduction (with an\nalternative proof) as a special case. Our result also leads to a defective list\ncoloring algorithm. As a corollary, we improve the state-of-the-art truly local\n$(deg+1)$-list coloring algorithm from Barenboim et al. [PODC'18] by slightly\nreducing the runtime to $O(\\sqrt{\\Delta\\log\\Delta})+\\log^* n$ and significantly\nreducing the message size (from huge to roughly $\\Delta$). Our techniques are\ninspired by the local conflict coloring framework of Fraigniaud et al.\n[FOCS'16].\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 06:18:23 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Maus", "Yannic", ""], ["Tonoyan", "Tigran", ""]]}, {"id": "2007.15306", "submitter": "Emilio Cruciani", "authors": "Emilio Cruciani, Hlafo Alfie Mimun, Matteo Quattropani, Sara Rizzo", "title": "Phase Transition of the k-Majority Dynamics in Biased Communication\n  Models", "comments": "Preliminary versions published in DISC 2020 (Brief Announcement) and\n  ICDCN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a graph where each of the $n$ nodes is either in state $\\mathcal{R}$\nor $\\mathcal{B}$. Herein, we analyze the synchronous $k$-Majority dynamics,\nwhere in each discrete-time round nodes simultaneously sample $k$ neighbors\nuniformly at random with replacement and adopt the majority state among the\nnodes in the sample (breaking ties uniformly at random).\n  Differently from previous work, we study the robustness of the $k$-Majority\nin maintaining a majority, that we consider $\\mathcal{R}$ w.l.o.g., when the\ndynamics is subject to two forms of adversarial noise, or bias, toward state\n$\\mathcal{B}$. We consider an external agent that wants to subvert the initial\nmajority and, in each round, either tries to alter the communication between\neach pair of nodes transmitting state $\\mathcal{B}$ (first form of bias), or\ntries to corrupt each node directly making it update to $\\mathcal{B}$ (second\nform of bias), with a probability of success $p$.\n  Our results show a phase transition in both forms of bias and on the same\ncritical value. By considering initial configurations in which each node has\nprobability $q \\in (\\frac{1}{2},1]$ of being in state $\\mathcal{R}$, we prove\nthat for every $k\\geq3$ there exists a critical value $p_{k,q}^*$ such that,\nwith high probability: if $p>p_{k,q}^*$, the external agent is able to subvert\nthe initial majority within a constant number of rounds; if $p<p_{k,q}^*$, the\nexternal agent needs at least a superpolynomial number of rounds to subvert the\ninitial majority.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 08:30:42 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 14:25:14 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Cruciani", "Emilio", ""], ["Mimun", "Hlafo Alfie", ""], ["Quattropani", "Matteo", ""], ["Rizzo", "Sara", ""]]}, {"id": "2007.15362", "submitter": "Simon D. Fink", "authors": "Thomas Bl\\\"asius, Simon D. Fink, Ignaz Rutter", "title": "Synchronized Planarity with Applications to Constrained Planarity\n  Problems", "comments": "to appear in Proceedings of ESA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the problem Synchronized Planarity. Roughly speaking, its input\nis a loop-free multi-graph together with synchronization constraints that,\ne.g., match pairs of vertices of equal degree by providing a bijection between\ntheir edges. Synchronized Planarity then asks whether the graph admits a\ncrossing-free embedding into the plane such that the orders of edges around\nsynchronized vertices are consistent. We show, on the one hand, that\nSynchronized Planarity can be solved in quadratic time, and, on the other hand,\nthat it serves as a powerful modeling language that lets us easily formulate\nseveral constrained planarity problems as instances of Synchronized Planarity.\nIn particular, this lets us solve Clustered Planarity in quadratic time, where\nthe most efficient previously known algorithm has an upper bound of $O(n^{8})$.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 10:26:55 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 07:49:08 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Fink", "Simon D.", ""], ["Rutter", "Ignaz", ""]]}, {"id": "2007.15589", "submitter": "Aravindan Vijayaraghavan", "authors": "Aravindan Vijayaraghavan", "title": "Efficient Tensor Decomposition", "comments": "Chapter 19 of the book \"Beyond the Worst-Case Analysis of\n  Algorithms\", edited by Tim Roughgarden and published by Cambridge University\n  Press (2020). We hope to occasionally update the survey here to include\n  discussions of new results and advances", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter studies the problem of decomposing a tensor into a sum of\nconstituent rank one tensors. While tensor decompositions are very useful in\ndesigning learning algorithms and data analysis, they are NP-hard in the\nworst-case. We will see how to design efficient algorithms with provable\nguarantees under mild assumptions, and using beyond worst-case frameworks like\nsmoothed analysis.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 16:53:38 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Vijayaraghavan", "Aravindan", ""]]}, {"id": "2007.15618", "submitter": "Ankit Pensia", "authors": "Ilias Diakonikolas, Daniel M. Kane, Ankit Pensia", "title": "Outlier Robust Mean Estimation with Subgaussian Rates via Stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of outlier robust high-dimensional mean estimation under\na finite covariance assumption, and more broadly under finite low-degree moment\nassumptions. We consider a standard stability condition from the recent robust\nstatistics literature and prove that, except with exponentially small failure\nprobability, there exists a large fraction of the inliers satisfying this\ncondition. As a corollary, it follows that a number of recently developed\nalgorithms for robust mean estimation, including iterative filtering and\nnon-convex gradient descent, give optimal error estimators with\n(near-)subgaussian rates. Previous analyses of these algorithms gave\nsignificantly suboptimal rates. As a corollary of our approach, we obtain the\nfirst computationally efficient algorithm with subgaussian rate for\noutlier-robust mean estimation in the strong contamination model under a finite\ncovariance assumption.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:33:03 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 15:58:25 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Pensia", "Ankit", ""]]}, {"id": "2007.15709", "submitter": "Ilan Cohen", "authors": "Nikhil Bansal, Ilan Reuven Cohen", "title": "An Asymptotic Lower Bound for Online Vector Bin Packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online vector bin packing problem where $n$ items specified\nby $d$-dimensional vectors must be packed in the fewest number of identical\n$d$-dimensional bins. Azar et al. (STOC'13) showed that for any online\nalgorithm $A$, there exist instances I, such that $A(I)$, the number of bins\nused by $A$ to pack $I$, is $\\Omega(d/\\log^2 d)$ times $OPT(I)$, the minimal\nnumber of bins to pack $I$. However in those instances, $OPT(I)$ was only\n$O(\\log d)$, which left open the possibility of improved algorithms with better\nasymptotic competitive ratio when $OPT(I) \\gg d$. We rule this out by showing\nthat for any arbitrary function $q(\\cdot)$ and any randomized online algorithm\n$A$, there exist instances $I$ such that $ E[A(I)] \\geq c\\cdot d/\\log^3d \\cdot\nOPT(I) + q(d)$, for some universal constant $c$.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 19:50:02 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 20:15:56 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Bansal", "Nikhil", ""], ["Cohen", "Ilan Reuven", ""]]}, {"id": "2007.15743", "submitter": "Tim Roughgarden", "authors": "Tim Roughgarden and C. Seshadhri", "title": "Distribution-Free Models of Social Networks", "comments": "Chapter 28 of the book Beyond the Worst-Case Analysis of Algorithms,\n  edited by Tim Roughgarden and published by Cambridge University Press (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure of large-scale social networks has predominantly been\narticulated using generative models, a form of average-case analysis. This\nchapter surveys recent proposals of more robust models of such networks. These\nmodels posit deterministic and empirically supported combinatorial structure\nrather than a specific probability distribution. We discuss the formal\ndefinitions of these models and how they relate to empirical observations in\nsocial networks, as well as the known structural and algorithmic results for\nthe corresponding graph classes.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 21:08:44 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Roughgarden", "Tim", ""], ["Seshadhri", "C.", ""]]}, {"id": "2007.15839", "submitter": "Fred Zhang", "authors": "Samuel B. Hopkins, Jerry Li, Fred Zhang", "title": "Robust and Heavy-Tailed Mean Estimation Made Simple, via Regret\n  Minimization", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the mean of a distribution in high\ndimensions when either the samples are adversarially corrupted or the\ndistribution is heavy-tailed. Recent developments in robust statistics have\nestablished efficient and (near) optimal procedures for both settings. However,\nthe algorithms developed on each side tend to be sophisticated and do not\ndirectly transfer to the other, with many of them having ad-hoc or complicated\nanalyses.\n  In this paper, we provide a meta-problem and a duality theorem that lead to a\nnew unified view on robust and heavy-tailed mean estimation in high dimensions.\nWe show that the meta-problem can be solved either by a variant of the Filter\nalgorithm from the recent literature on robust estimation or by the quantum\nentropy scoring scheme (QUE), due to Dong, Hopkins and Li (NeurIPS '19). By\nleveraging our duality theorem, these results translate into simple and\nefficient algorithms for both robust and heavy-tailed settings. Furthermore,\nthe QUE-based procedure has run-time that matches the fastest known algorithms\non both fronts.\n  Our analysis of Filter is through the classic regret bound of the\nmultiplicative weights update method. This connection allows us to avoid the\ntechnical complications in previous works and improve upon the run-time\nanalysis of a gradient-descent-based algorithm for robust mean estimation by\nCheng, Diakonikolas, Ge and Soltanolkotabi (ICML '20).\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 04:18:32 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 00:27:26 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Hopkins", "Samuel B.", ""], ["Li", "Jerry", ""], ["Zhang", "Fred", ""]]}, {"id": "2007.16111", "submitter": "Ahmed Soliman", "authors": "Subrata Saha, Ahmed Soliman and Sanguthevar Rajasekaran", "title": "MSPP: A Highly Efficient and Scalable Algorithm for Mining Similar Pairs\n  of Points", "comments": "15 pages and 6 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The closest pair of points problem or closest pair problem (CPP) is an\nimportant problem in computational geometry where we have to find a pair of\npoints from a set of points in metric space with the smallest distance between\nthem. This problem arises in a number of applications, such as but not limited\nto clustering, graph partitioning, image processing, patterns identification,\nand intrusion detection. For example, in air-traffic control, we must monitor\naircrafts that come too close together, since this may potentially indicate a\npossible collision. Numerous algorithms have been presented for solving the\nCPP. The algorithms that are employed in practice have a worst case quadratic\nrun time complexity. In this article we present an elegant approximation\nalgorithm for the CPP called MSPP: Mining Similar Pairs of Points. It is faster\nthan currently best known algorithms while maintaining a very good accuracy.\nThe proposed algorithm also detects a set of closely similar pairs of points in\nEuclidean and Pearson metric spaces and can be adapted in numerous real world\napplications, such as clustering, dimension reduction, constructing and\nanalyzing gene/transcript co-expression network, among others.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 14:49:37 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Saha", "Subrata", ""], ["Soliman", "Ahmed", ""], ["Rajasekaran", "Sanguthevar", ""]]}, {"id": "2007.16144", "submitter": "Arturo Merino", "authors": "Arturo Merino and Andreas Wiese", "title": "On the Two-Dimensional Knapsack Problem for Convex Polygons", "comments": "32 pages, 7 figures. A preliminary version appears in ICALP 2020", "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2020.84", "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the two-dimensional geometric knapsack problem for convex polygons.\nGiven a set of weighted convex polygons and a square knapsack, the goal is to\nselect the most profitable subset of the given polygons that fits\nnon-overlappingly into the knapsack. We allow to rotate the polygons by\narbitrary angles. We present a quasi-polynomial time $O(1)$-approximation\nalgorithm for the general case and a polynomial time $O(1)$-approximation\nalgorithm if all input polygons are triangles, both assuming polynomially\nbounded integral input data. Also, we give a quasi-polynomial time algorithm\nthat computes a solution of optimal weight under resource augmentation, i.e.,\nwe allow to increase the size of the knapsack by a factor of $1+\\delta$ for\nsome $\\delta>0$ but compare ourselves with the optimal solution for the\noriginal knapsack. To the best of our knowledge, these are the first results\nfor two-dimensional geometric knapsack in which the input objects are more\ngeneral than axis-parallel rectangles or circles and in which the input\npolygons can be rotated by arbitrary angles.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 15:47:29 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Merino", "Arturo", ""], ["Wiese", "Andreas", ""]]}, {"id": "2007.16192", "submitter": "Peter Ahrens", "authors": "Peter Ahrens", "title": "Contiguous Graph Partitioning For Optimal Total Or Bottleneck\n  Communication", "comments": "20 pages; added total partitioning algorithm, added total costs,\n  added experimental results, added lazy near-linear bisection algorithm,\n  simplified presentation. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph partitioning schedules parallel calculations like sparse matrix-vector\nmultiply (SpMV). We consider contiguous partitions, where the $m$ rows (or\ncolumns) of a sparse matrix with $N$ nonzeros are split into $K$ parts without\nreordering. We propose the first near-linear time algorithms for several graph\npartitioning problems in the contiguous regime.\n  Traditional objectives such as the simple edge cut, hyperedge cut, or\nhypergraph connectivity minimize the total cost of all parts under a balance\nconstraint. Our total partitioners use $O(Km + N)$ space. They run in\n$O((Km\\log(m) + N)\\log(N))$ time, a significant improvement over prior $O(K(m^2\n+ N))$ time algorithms due to Kernighan and Grandjean et. al.\n  Bottleneck partitioning minimizes the maximum cost of any part. We propose a\nnew bottleneck cost which reflects the sum of communication and computation on\neach part. Our bottleneck partitioners use linear space. The exact algorithm\nruns in linear time when $K^2$ is $O(N^C)$ for $C < 1$. Our $(1 +\n\\epsilon)$-approximate algorithm runs in linear time when\n$K\\log(c_{high}/(c_{low}\\epsilon))$ is $O(N^C)$ for $C < 1$, where $c_{high}$\nand $c_{low}$ are upper and lower bounds on the optimal cost. We also propose a\nsimpler $(1 + \\epsilon)$-approximate algorithm which runs in a factor of\n$\\log(c_{high}/(c_{low}\\epsilon))$ from linear time.\n  We empirically demonstrate that our algorithms efficiently produce\nhigh-quality contiguous partitions on a test suite of $42$ test matrices. When\n$K = 8$, our hypergraph connectivity partitioner achieved a speedup of\n$53\\times$ (mean $15.1\\times$) over prior algorithms. The mean runtime of our\nbottleneck partitioner was $5.15$ SpMVs.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 17:41:31 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 17:46:28 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2020 00:11:21 GMT"}, {"version": "v4", "created": "Mon, 21 Jun 2021 17:02:26 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ahrens", "Peter", ""]]}]