[{"id": "1409.0035", "submitter": "Thomas Pajor", "authors": "Edith Cohen, Daniel Delling, Thomas Pajor, Renato F. Werneck", "title": "Computing Classic Closeness Centrality, at Scale", "comments": "13 pages, 2 figures, appeared at the 2nd ACM Conference on Online\n  Social Networks (COSN'14)", "journal-ref": null, "doi": "10.1145/2660460.2660465", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Closeness centrality, first considered by Bavelas (1948), is an importance\nmeasure of a node in a network which is based on the distances from the node to\nall other nodes. The classic definition, proposed by Bavelas (1950), Beauchamp\n(1965), and Sabidussi (1966), is (the inverse of) the average distance to all\nother nodes.\n  We propose the first highly scalable (near linear-time processing and linear\nspace overhead) algorithm for estimating, within a small relative error, the\nclassic closeness centralities of all nodes in the graph. Our algorithm applies\nto undirected graphs, as well as for centrality computed with respect to\nround-trip distances in directed graphs.\n  For directed graphs, we also propose an efficient algorithm that approximates\ngeneralizations of classic closeness centrality to outbound and inbound\ncentralities. Although it does not provide worst-case theoretical approximation\nguarantees, it is designed to perform well on real networks.\n  We perform extensive experiments on large networks, demonstrating high\nscalability and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 21:16:24 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Cohen", "Edith", ""], ["Delling", "Daniel", ""], ["Pajor", "Thomas", ""], ["Werneck", "Renato F.", ""]]}, {"id": "1409.0088", "submitter": "Akira SaiToh", "authors": "Akira SaiToh", "title": "Quantum digital-to-analog conversion algorithm using decoherence", "comments": "22 pages, no figure, v2: minor revision, style changed, minor changes\n  of proofs in section 2, v3: revision with more detailed explanations, style\n  changed, supplementary material added, v4: minor revision, v5: typos\n  corrected, to appear in QIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of mapping digital data encoded on a quantum register\nto analog amplitudes in parallel. It is shown to be unlikely that a fully\nunitary polynomial-time quantum algorithm exists for this problem; NP becomes a\nsubset of BQP if it exists. In the practical point of view, we propose a\nnonunitary linear-time algorithm using quantum decoherence. It tacitly uses an\nexponentially large physical resource, which is typically a huge number of\nidentical molecules. Quantumness of correlation appearing in the process of the\nalgorithm is also discussed.\n", "versions": [{"version": "v1", "created": "Sat, 30 Aug 2014 06:16:08 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 15:58:00 GMT"}, {"version": "v3", "created": "Sun, 28 Dec 2014 14:51:47 GMT"}, {"version": "v4", "created": "Thu, 14 May 2015 08:14:33 GMT"}, {"version": "v5", "created": "Tue, 19 May 2015 05:03:12 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["SaiToh", "Akira", ""]]}, {"id": "1409.0098", "submitter": "Hung-Lung Wang", "authors": "Hung-Lung Wang", "title": "An optimal algorithm for the weighted backup 2-center problem on a tree", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are concerned with the weighted backup 2-center problem on\na tree. The backup 2-center problem is a kind of center facility location\nproblem, in which one is asked to deploy two facilities, with a given\nprobability to fail, in a network. Given that the two facilities do not fail\nsimultaneously, the goal is to find two locations, possibly on edges, that\nminimize the expected value of the maximum distance over all vertices to their\nclosest functioning facility. In the weighted setting, each vertex in the\nnetwork is associated with a nonnegative weight, and the distance from vertex\n$u$ to $v$ is weighted by the weight of $u$. With the strategy of\nprune-and-search, we propose a linear time algorithm, which is asymptotically\noptimal, to solve the weighted backup 2-center problem on a tree.\n", "versions": [{"version": "v1", "created": "Sat, 30 Aug 2014 10:16:48 GMT"}, {"version": "v2", "created": "Wed, 17 Sep 2014 04:03:20 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2015 03:49:42 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Wang", "Hung-Lung", ""]]}, {"id": "1409.0173", "submitter": "Sayan Bandyapadhyay", "authors": "Sayan Bandyapadhyay", "title": "A Variant of the Maximum Weight Independent Set Problem", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a natural extension of the Maximum Weight Independent Set Problem\n(MWIS), one of the most studied optimization problems in Graph algorithms. We\nare given a graph $G=(V,E)$, a weight function $w: V \\rightarrow \\mathbb{R^+}$,\na budget function $b: V \\rightarrow \\mathbb{Z^+}$, and a positive integer $B$.\nThe weight (resp. budget) of a subset of vertices is the sum of weights (resp.\nbudgets) of the vertices in the subset. A $k$-budgeted independent set in $G$\nis a subset of vertices, such that no pair of vertices in that subset are\nadjacent, and the budget of the subset is at most $k$. The goal is to find a\n$B$-budgeted independent set in $G$ such that its weight is maximum among all\nthe $B$-budgeted independent sets in $G$. We refer to this problem as MWBIS.\nBeing a generalization of MWIS, MWBIS also has several applications in\nScheduling, Wireless networks and so on. Due to the hardness results implied\nfrom MWIS, we study the MWBIS problem in several special classes of graphs. We\ndesign exact algorithms for trees, forests, cycle graphs, and interval graphs.\nIn unweighted case we design an approximation algorithm for $d+1$-claw free\ngraphs whose approximation ratio ($d$) is competitive with the approximation\nratio ($\\frac{d}{2}$) of MWIS (unweighted). Furthermore, we extend Baker's\ntechnique \\cite{Baker83} to get a PTAS for MWBIS in planar graphs.\n", "versions": [{"version": "v1", "created": "Sun, 31 Aug 2014 01:00:55 GMT"}, {"version": "v2", "created": "Sun, 28 Sep 2014 20:23:53 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Bandyapadhyay", "Sayan", ""]]}, {"id": "1409.0499", "submitter": "Martin Fink", "authors": "Maximilian Aulbach, Martin Fink, Julian Schuhmann, Alexander Wolff", "title": "Drawing Graphs within Restricted Area", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of selecting a maximum-weight subgraph of a given graph\nsuch that the subgraph can be drawn within a prescribed drawing area subject to\ngiven non-uniform vertex sizes. We develop and analyze heuristics both for the\ngeneral (undirected) case and for the use case of (directed) calculation graphs\nwhich are used to analyze the typical mistakes that high school students make\nwhen transforming mathematical expressions in the process of calculating, for\nexample, sums of fractions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 18:20:51 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Aulbach", "Maximilian", ""], ["Fink", "Martin", ""], ["Schuhmann", "Julian", ""], ["Wolff", "Alexander", ""]]}, {"id": "1409.0597", "submitter": "Joseph A. Simons", "authors": "Michael T. Goodrich and Joseph A. Simons", "title": "Data-Oblivious Graph Algorithms in Outsourced External Memory", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by privacy preservation for outsourced data, data-oblivious\nexternal memory is a computational framework where a client performs\ncomputations on data stored at a semi-trusted server in a way that does not\nreveal her data to the server. This approach facilitates collaboration and\nreliability over traditional frameworks, and it provides privacy protection,\neven though the server has full access to the data and he can monitor how it is\naccessed by the client. The challenge is that even if data is encrypted, the\nserver can learn information based on the client data access pattern; hence,\naccess patterns must also be obfuscated. We investigate privacy-preserving\nalgorithms for outsourced external memory that are based on the use of\ndata-oblivious algorithms, that is, algorithms where each possible sequence of\ndata accesses is independent of the data values. We give new efficient\ndata-oblivious algorithms in the outsourced external memory model for a number\nof fundamental graph problems. Our results include new data-oblivious\nexternal-memory methods for constructing minimum spanning trees, performing\nvarious traversals on rooted trees, answering least common ancestor queries on\ntrees, computing biconnected components, and forming open ear decompositions.\nNone of our algorithms make use of constant-time random oracles.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 03:20:46 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Goodrich", "Michael T.", ""], ["Simons", "Joseph A.", ""]]}, {"id": "1409.0607", "submitter": "Chidambaram Annamalai", "authors": "Chidambaram Annamalai, Christos Kalaitzis, Ola Svensson", "title": "Combinatorial Algorithm for Restricted Max-Min Fair Allocation", "comments": "28 pages, 3 figures; revised version with expanded algorithm\n  intuition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the basic allocation problem of assigning resources to players so as\nto maximize fairness. This is one of the few natural problems that enjoys the\nintriguing status of having a better estimation algorithm than approximation\nalgorithm. Indeed, a certain Configuration-LP can be used to estimate the value\nof the optimal allocation to within a factor of $4 + {\\epsilon}$. In contrast,\nhowever, the best known approximation algorithm for the problem has an\nunspecified large constant guarantee.\n  In this paper we significantly narrow this gap by giving a $13$-approximation\nalgorithm for the problem. Our approach develops a local search technique\nintroduced by Haxell [Hax95] for hypergraph matchings, and later used in this\ncontext by Asadpour, Feige, and Saberi [AFS12]. For our local search procedure\nto terminate in polynomial time, we introduce several new ideas such as lazy\nupdates and greedy players. Besides the improved approximation guarantee, the\nhighlight of our approach is that it is purely combinatorial and uses the\nConfiguration-LP only in the analysis.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 05:28:40 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 10:24:00 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Annamalai", "Chidambaram", ""], ["Kalaitzis", "Christos", ""], ["Svensson", "Ola", ""]]}, {"id": "1409.0706", "submitter": "Shiyan Zhong", "authors": "Shiyan Zhong", "title": "Efficient Scheme for Active Particle Selection in N-body Simulations", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient method for active particle selection, working with\nHermite Individual Time Steps (HITS) scheme in direct N-body simulation code\n$\\varphi$GRAPE. For a simulation with $N$ particles, this method can reduce the\ncomputation complexity of active particle selection, from $O(N\\cdot N_{step})$\nto $O(\\overline{N_{act}}\\cdot N_{step})$, where $\\overline{N_{act}}$ is the\naverage active particle number in every time step which is much smaller than\n$N$ and $N_{step}$ is the total time steps integrated during the simulation.\nThus can save a lot of time spent on active particle selection part, especially\nin the case of low $\\overline{N_{act}}$.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 13:46:06 GMT"}, {"version": "v2", "created": "Sat, 13 Sep 2014 07:02:14 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Zhong", "Shiyan", ""]]}, {"id": "1409.0973", "submitter": "Jin-Kao Hao", "authors": "Xiangjing Lai, Zhipeng Lu, Jin-Kao Hao, Fred Glover, Liping Xu", "title": "Path Relinking for Bandwidth Coloring Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Path Relinking algorithm is proposed for the Bandwidth Coloring problem and\nthe Bandwidth MultiColoring problem. It combines a population based relinking\nmethod and a tabu search based local search procedure. The proposed algorithm\nis assessed on two sets of 66 benchmark instances commonly used in the\nliterature. Computational results demonstrate that the proposed algorithm is\nhighly competitive in terms of both solution quality and efficiency compared to\nthe best performing algorithms in the literature. Specifically, it improves the\nprevious best known results for 15 out of 66 instances, while matching the\nprevious best known results for 47 cases. Some key elements of the proposed\nalgorithm are investigated.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 07:17:16 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Lai", "Xiangjing", ""], ["Lu", "Zhipeng", ""], ["Hao", "Jin-Kao", ""], ["Glover", "Fred", ""], ["Xu", "Liping", ""]]}, {"id": "1409.1002", "submitter": "Jacek Pierzchlewski", "authors": "Jacek Pierzchlewski, Thomas Arildsen", "title": "Generation and Analysis of Constrained Random Sampling Patterns", "comments": "29 pages, 12 figures, submitted to Circuits, Systems and Signal\n  Processing journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random sampling is a technique for signal acquisition which is gaining\npopularity in practical signal processing systems. Nowadays, event-driven\nanalog-to-digital converters make random sampling feasible in practical\napplications. A process of random sampling is defined by a sampling pattern,\nwhich indicates signal sampling points in time. Practical random sampling\npatterns are constrained by ADC characteristics and application requirements.\nIn this paper authors introduce statistical methods which evaluate random\nsampling pattern generators with emphasis on practical applications.\nFurthermore, the authors propose a new random pattern generator which copes\nwith strict practical limitations imposed on patterns, with possibly minimal\nloss in randomness of sampling. The proposed generator is compared with\nexisting sampling pattern generators using the introduced statistical methods.\nIt is shown that the proposed algorithm generates random sampling patterns\ndedicated for event-driven-ADCs better than existed sampling pattern\ngenerators. Finally, implementation issues of random sampling patterns are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 09:12:19 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2015 06:47:25 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Pierzchlewski", "Jacek", ""], ["Arildsen", "Thomas", ""]]}, {"id": "1409.1005", "submitter": "Alexander Setzer", "authors": "Alexander Setzer", "title": "The planar minimum linear arrangement problem is different from the\n  minimum linear arrangement problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various research papers, such as [2], one will find the claim that the\nminLA is optimally solvable on outerplanar graphs, with a reference to [1].\nHowever, the problem solved in that publication, which we refer to as the\nplanar minimum linear arrangement problem (planar minLA), is different from the\nminimum linear arrangement problem (minLA), as we show in this article.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 09:22:47 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Setzer", "Alexander", ""]]}, {"id": "1409.1073", "submitter": "Jun He", "authors": "Xinsheng Lai, Yuren Zhou, Jun He and Jun Zhang", "title": "Performance Analysis on Evolutionary Algorithms for the Minimum Label\n  Spanning Tree Problem", "comments": null, "journal-ref": null, "doi": "10.1109/TEVC.2013.2291790", "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some experimental investigations have shown that evolutionary algorithms\n(EAs) are efficient for the minimum label spanning tree (MLST) problem.\nHowever, we know little about that in theory. As one step towards this issue,\nwe theoretically analyze the performances of the (1+1) EA, a simple version of\nEAs, and a multi-objective evolutionary algorithm called GSEMO on the MLST\nproblem. We reveal that for the MLST$_{b}$ problem the (1+1) EA and GSEMO\nachieve a $\\frac{b+1}{2}$-approximation ratio in expected polynomial times of\n$n$ the number of nodes and $k$ the number of labels. We also show that GSEMO\nachieves a $(2ln(n))$-approximation ratio for the MLST problem in expected\npolynomial time of $n$ and $k$. At the same time, we show that the (1+1) EA and\nGSEMO outperform local search algorithms on three instances of the MLST\nproblem. We also construct an instance on which GSEMO outperforms the (1+1) EA.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 13:17:31 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Lai", "Xinsheng", ""], ["Zhou", "Yuren", ""], ["He", "Jun", ""], ["Zhang", "Jun", ""]]}, {"id": "1409.1399", "submitter": "Stanislav Zivny", "authors": "Justin Ward and Stanislav Zivny", "title": "Maximizing k-Submodular Functions and Beyond", "comments": "Full version of a SODA'14 paper, to appear in ACM Transactions on\n  Algorithms (TALG)", "journal-ref": "ACM Transactions on Algorithms 12(4) Article no. 47 (2016)", "doi": "10.1145/2850419", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the maximization problem in the value oracle model of functions\ndefined on $k$-tuples of sets that are submodular in every orthant and $r$-wise\nmonotone, where $k\\geq 2$ and $1\\leq r\\leq k$. We give an analysis of a\ndeterministic greedy algorithm that shows that any such function can be\napproximated to a factor of $1/(1+r)$. For $r=k$, we give an analysis of a\nrandomised greedy algorithm that shows that any such function can be\napproximated to a factor of $1/(1+\\sqrt{k/2})$.\n  In the case of $k=r=2$, the considered functions correspond precisely to\nbisubmodular functions, in which case we obtain an approximation guarantee of\n$1/2$. We show that, as in the case of submodular functions, this result is the\nbest possible in both the value query model, and under the assumption that\n$NP\\neq RP$.\n  Extending a result of Ando et al., we show that for any $k\\geq 3$\nsubmodularity in every orthant and pairwise monotonicity (i.e. $r=2$) precisely\ncharacterize $k$-submodular functions. Consequently, we obtain an approximation\nguarantee of $1/3$ (and thus independent of $k$) for the maximization problem\nof $k$-submodular functions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 11:06:04 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 11:00:33 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Ward", "Justin", ""], ["Zivny", "Stanislav", ""]]}, {"id": "1409.1416", "submitter": "Li Yang", "authors": "Hong-Wei Li and Li Yang", "title": "A quantum algorithm for approximating the influences of Boolean\n  functions and its applications", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the influences of variables on a Boolean function $f$ based on\nthe quantum Bernstein-Vazirani algorithm. A previous paper (Floess et al. in\nMath. Struct. in Comp. Science 23: 386, 2013) has proved that if a $n$-variable\nBoolean function $f(x_1,\\ldots,x_n)$ does not depend on an input variable\n$x_i$, using the Bernstein-Vazirani circuit to $f$ will always obtain an output\n$y$ that has a $0$ in the $i$th position. We generalize this result and show\nthat after one time running the algorithm, the probability of getting a 1 in\neach position $i$ is equal to the dependence degree of $f$ on the variable\n$x_i$, i.e. the influence of $x_i$ on $f$. On this foundation, we give an\napproximation algorithm to evaluate the influence of any variable on a Boolean\nfunction. Next, as an application, we use it to study the Boolean functions\nwith juntas, and construct probabilistic quantum algorithms to learn certain\nBoolean functions. Compared with the deterministic algorithms given by Floess\net al., our probabilistic algorithms are faster.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 12:02:09 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 09:39:11 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Li", "Hong-Wei", ""], ["Yang", "Li", ""]]}, {"id": "1409.1694", "submitter": "Emanuele Giaquinta", "authors": "Tomas Flouri and Emanuele Giaquinta and Kassian Kobert and Esko\n  Ukkonen", "title": "Longest common substrings with k mismatches", "comments": "Accepted version", "journal-ref": null, "doi": "10.1016/j.ipl.2015.03.006", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The longest common substring with $k$-mismatches problem is to find, given\ntwo strings $S_1$ and $S_2$, a longest substring $A_1$ of $S_1$ and $A_2$ of\n$S_2$ such that the Hamming distance between $A_1$ and $A_2$ is $\\le k$. We\nintroduce a practical $O(nm)$ time and $O(1)$ space solution for this problem,\nwhere $n$ and $m$ are the lengths of $S_1$ and $S_2$, respectively. This\nalgorithm can also be used to compute the matching statistics with\n$k$-mismatches of $S_1$ and $S_2$ in $O(nm)$ time and $O(m)$ space. Moreover,\nwe also present a theoretical solution for the $k = 1$ case which runs in $O(n\n\\log m)$ time, assuming $m\\le n$, and uses $O(m)$ space, improving over the\nexisting $O(nm)$ time and $O(m)$ space bound of Babenko and Starikovskaya.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 08:57:02 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 12:22:05 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Flouri", "Tomas", ""], ["Giaquinta", "Emanuele", ""], ["Kobert", "Kassian", ""], ["Ukkonen", "Esko", ""]]}, {"id": "1409.1722", "submitter": "Kim S. Larsen", "authors": "Marie G. Christ and Lene M. Favrholdt and Kim S. Larsen", "title": "Online Multi-Coloring with Advice", "comments": "IMADA-preprint-cs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online graph multi-coloring with advice.\nMulti-coloring is often used to model frequency allocation in cellular\nnetworks. We give several nearly tight upper and lower bounds for the most\nstandard topologies of cellular networks, paths and hexagonal graphs. For the\npath, negative results trivially carry over to bipartite graphs, and our\npositive results are also valid for bipartite graphs. The advice given\nrepresents information that is likely to be available, studying for instance\nthe data from earlier similar periods of time.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 10:12:01 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["Christ", "Marie G.", ""], ["Favrholdt", "Lene M.", ""], ["Larsen", "Kim S.", ""]]}, {"id": "1409.1749", "submitter": "Marek Piotr\\'ow", "authors": "Marek Piotr\\'ow", "title": "Faster Small-Constant-Periodic Merging Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1401.0396", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of merging two sorted sequences on a comparator\nnetwork that is used repeatedly, that is, if the output is not sorted, the\nnetwork is applied again using the output as input. The challenging task is to\nconstruct such networks of small depth (called a period in this context). In\nour previous paper entitled Faster 3-Periodic Merging Network we reduced twice\nthe time of merging on $3$-periodic networks, i.e. from $12\\log N$ to $6\\log\nN$, compared to the first construction given by Kuty{\\l}owski, Lory\\'s and\nOesterdikhoff. Note that merging on $2$-periodic networks require linear time.\nIn this paper we extend our construction, which is based on Canfield and\nWilliamson $(\\log N)$-periodic sorter, and the analysis from that paper to any\nperiod $p \\ge 4$. For $p\\ge 4$ our $p$-periodic network merges two sorted\nsequences of length $N/2$ in at most $\\frac{2p}{p-2}\\log N + p\\frac{p-8}{p-2}$\nrounds. The previous bound given by Kuty{\\l}owski at al. was\n$\\frac{2.25p}{p-2.42}\\log N$. That means, for example, that our $4$-periodic\nmerging networks work in time upper-bounded by $4\\log N$ and our $6$-periodic\nones in time upper-bounded by $3\\log N$ compared to the corresponding $5.67\\log\nN$ and $3.8\\log N$ previous bounds. Our construction is regular and follows the\nsame periodification schema, whereas some additional techniques were used\npreviously to tune the construction for $p \\ge 4$. Moreover, our networks are\nalso periodic sorters and tests on random permutations show that average\nsorting time is closed to $\\log^2 N$.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 11:48:09 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["Piotr\u00f3w", "Marek", ""]]}, {"id": "1409.2042", "submitter": "Arda Antikacioglu", "authors": "Arda Antikacioglu, R. Ravi, Srinath Srihdar", "title": "Recommendation Subgraphs for Web Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendations are central to the utility of many websites including\nYouTube, Quora as well as popular e-commerce stores. Such sites typically\ncontain a set of recommendations on every product page that enables visitors to\neasily navigate the website. Choosing an appropriate set of recommendations at\neach page is one of the key features of backend engines that have been deployed\nat several e-commerce sites.\n  Specifically at BloomReach, an engine consisting of several independent\ncomponents analyzes and optimizes its clients' websites. This paper focuses on\nthe structure optimizer component which improves the website navigation\nexperience that enables the discovery of novel content.\n  We begin by formalizing the concept of recommendations used for discovery. We\nformulate this as a natural graph optimization problem which in its simplest\ncase, reduces to a bipartite matching problem. In practice, solving these\nmatching problems requires superlinear time and is not scalable. Also,\nimplementing simple algorithms is critical in practice because they are\nsignificantly easier to maintain in production. This motivated us to analyze\nthree methods for solving the problem in increasing order of sophistication: a\nsampling algorithm, a greedy algorithm and a more involved partitioning based\nalgorithm.\n  We first theoretically analyze the performance of these three methods on\nrandom graph models characterizing when each method will yield a solution of\nsufficient quality and the parameter ranges when more sophistication is needed.\nWe complement this by providing an empirical analysis of these algorithms on\nsimulated and real-world production data. Our results confirm that it is not\nalways necessary to implement complicated algorithms in the real-world and that\nvery good practical results can be obtained by using heuristics that are backed\nby the confidence of concrete theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Sat, 6 Sep 2014 18:21:21 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Antikacioglu", "Arda", ""], ["Ravi", "R.", ""], ["Srihdar", "Srinath", ""]]}, {"id": "1409.2116", "submitter": "Sean Sedwards", "authors": "Pedro D'Argenio, Axel Legay, Sean Sedwards and Louis-Marie Traonouez", "title": "Smart Sampling for Lightweight Verification of Markov Decision Processes", "comments": "IEEE conference style, 11 pages, 5 algorithms, 11 figures, 1 table", "journal-ref": null, "doi": "10.1007/s10009-015-0383-0", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov decision processes (MDP) are useful to model optimisation problems in\nconcurrent systems. To verify MDPs with efficient Monte Carlo techniques\nrequires that their nondeterminism be resolved by a scheduler. Recent work has\nintroduced the elements of lightweight techniques to sample directly from\nscheduler space, but finding optimal schedulers by simple sampling may be\ninefficient. Here we describe \"smart\" sampling algorithms that can make\nsubstantial improvements in performance.\n", "versions": [{"version": "v1", "created": "Sun, 7 Sep 2014 13:12:06 GMT"}, {"version": "v2", "created": "Wed, 4 Feb 2015 17:34:20 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["D'Argenio", "Pedro", ""], ["Legay", "Axel", ""], ["Sedwards", "Sean", ""], ["Traonouez", "Louis-Marie", ""]]}, {"id": "1409.2138", "submitter": "Michael Kapralov", "authors": "Michael Kapralov, Sanjeev Khanna, Madhu Sudan", "title": "Streaming Lower Bounds for Approximating MAX-CUT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the value of max cut in a graph in the\nstreaming model of computation. At one extreme, there is a trivial\n$2$-approximation for this problem that uses only $O(\\log n)$ space, namely,\ncount the number of edges and output half of this value as the estimate for max\ncut value. On the other extreme, if one allows $\\tilde{O}(n)$ space, then a\nnear-optimal solution to the max cut value can be obtained by storing an\n$\\tilde{O}(n)$-size sparsifier that essentially preserves the max cut. An\nintriguing question is if poly-logarithmic space suffices to obtain a\nnon-trivial approximation to the max-cut value (that is, beating the factor\n$2$). It was recently shown that the problem of estimating the size of a\nmaximum matching in a graph admits a non-trivial approximation in\npoly-logarithmic space.\n  Our main result is that any streaming algorithm that breaks the\n$2$-approximation barrier requires $\\tilde{\\Omega}(\\sqrt{n})$ space even if the\nedges of the input graph are presented in random order. Our result is obtained\nby exhibiting a distribution over graphs which are either bipartite or\n$\\frac{1}{2}$-far from being bipartite, and establishing that\n$\\tilde{\\Omega}(\\sqrt{n})$ space is necessary to differentiate between these\ntwo cases. Thus as a direct corollary we obtain that $\\tilde{\\Omega}(\\sqrt{n})$\nspace is also necessary to test if a graph is bipartite or $\\frac{1}{2}$-far\nfrom being bipartite.\n  We also show that for any $\\epsilon > 0$, any streaming algorithm that\nobtains a $(1 + \\epsilon)$-approximation to the max cut value when edges arrive\nin adversarial order requires $n^{1 - O(\\epsilon)}$ space, implying that\n$\\Omega(n)$ space is necessary to obtain an arbitrarily good approximation to\nthe max cut value.\n", "versions": [{"version": "v1", "created": "Sun, 7 Sep 2014 16:48:25 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Kapralov", "Michael", ""], ["Khanna", "Sanjeev", ""], ["Sudan", "Madhu", ""]]}, {"id": "1409.2139", "submitter": "Huy Nguyen", "authors": "Moses Charikar, Monika Henzinger, Huy L. Nguyen", "title": "Online Bipartite Matching with Decomposable Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a weighted online bipartite matching problem: $G(V_1, V_2, E)$ is a\nweighted bipartite graph where $V_1$ is known beforehand and the vertices of\n$V_2$ arrive online. The goal is to match vertices of $V_2$ as they arrive to\nvertices in $V_1$, so as to maximize the sum of weights of edges in the\nmatching. If assignments to $V_1$ cannot be changed, no bounded competitive\nratio is achievable. We study the weighted online matching problem with {\\em\nfree disposal}, where vertices in $V_1$ can be assigned multiple times, but\nonly get credit for the maximum weight edge assigned to them over the course of\nthe algorithm. For this problem, the greedy algorithm is $0.5$-competitive and\ndetermining whether a better competitive ratio is achievable is a well known\nopen problem.\n  We identify an interesting special case where the edge weights are\ndecomposable as the product of two factors, one corresponding to each end point\nof the edge. This is analogous to the well studied related machines model in\nthe scheduling literature, although the objective functions are different. For\nthis case of decomposable edge weights, we design a 0.5664 competitive\nrandomized algorithm in complete bipartite graphs. We show that such instances\nwith decomposable weights are non-trivial by establishing upper bounds of 0.618\nfor deterministic and $0.8$ for randomized algorithms.\n  A tight competitive ratio of $1-1/e \\approx 0.632$ was known previously for\nboth the 0-1 case as well as the case where edge weights depend on the offline\nvertices only, but for these cases, reassignments cannot change the quality of\nthe solution. Beating 0.5 for weighted matching where reassignments are\nnecessary has been a significant challenge. We thus give the first online\nalgorithm with competitive ratio strictly better than 0.5 for a non-trivial\ncase of weighted matching with free disposal.\n", "versions": [{"version": "v1", "created": "Sun, 7 Sep 2014 17:03:07 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Charikar", "Moses", ""], ["Henzinger", "Monika", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "1409.2177", "submitter": "Kamalika Chaudhuri", "authors": "Kamalika Chaudhuri and Daniel Hsu and Shuang Song", "title": "The Large Margin Mechanism for Differentially Private Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic problem in the design of privacy-preserving algorithms is the private\nmaximization problem: the goal is to pick an item from a universe that\n(approximately) maximizes a data-dependent function, all under the constraint\nof differential privacy. This problem has been used as a sub-routine in many\nprivacy-preserving algorithms for statistics and machine-learning.\n  Previous algorithms for this problem are either range-dependent---i.e., their\nutility diminishes with the size of the universe---or only apply to very\nrestricted function classes. This work provides the first general-purpose,\nrange-independent algorithm for private maximization that guarantees\napproximate differential privacy. Its applicability is demonstrated on two\nfundamental tasks in data mining and machine learning.\n", "versions": [{"version": "v1", "created": "Sun, 7 Sep 2014 23:51:00 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Hsu", "Daniel", ""], ["Song", "Shuang", ""]]}, {"id": "1409.2291", "submitter": "Andreas Pavlogiannis", "authors": "Krishnendu Chatterjee, Andreas Pavlogiannis, Alexander K\\\"o{\\ss}ler,\n  Ulrich Schmid", "title": "A Framework for Automated Competitive Analysis of On-line Scheduling of\n  Firm-Deadline Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a flexible framework for the automated competitive analysis of\non-line scheduling algorithms for firm-deadline real-time tasks based on\nmulti-objective graphs: Given a taskset and an on-line scheduling algorithm\nspecified as a labeled transition system, along with some optional safety,\nliveness, and/or limit-average constraints for the adversary, we automatically\ncompute the competitive ratio of the algorithm w.r.t. a clairvoyant scheduler.\nWe demonstrate the flexibility and power of our approach by comparing the\ncompetitive ratio of several on-line algorithms, including $D^{over}$, that\nhave been proposed in the past, for various tasksets. Our experimental results\nreveal that none of these algorithms is universally optimal, in the sense that\nthere are tasksets where other schedulers provide better performance. Our\nframework is hence a very useful design tool for selecting optimal algorithms\nfor a given application.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 11:10:45 GMT"}, {"version": "v2", "created": "Tue, 9 Sep 2014 10:29:43 GMT"}, {"version": "v3", "created": "Sun, 14 Sep 2014 17:30:01 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Pavlogiannis", "Andreas", ""], ["K\u00f6\u00dfler", "Alexander", ""], ["Schmid", "Ulrich", ""]]}, {"id": "1409.2391", "submitter": "Robert Krauthgamer", "authors": "Dmitry Kogan and Robert Krauthgamer", "title": "Sketching Cuts in Graphs and Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketching and streaming algorithms are in the forefront of current research\ndirections for cut problems in graphs. In the streaming model, we show that\n$(1-\\epsilon)$-approximation for Max-Cut must use $n^{1-O(\\epsilon)}$ space;\nmoreover, beating $4/5$-approximation requires polynomial space. For the\nsketching model, we show that $r$-uniform hypergraphs admit a\n$(1+\\epsilon)$-cut-sparsifier (i.e., a weighted subhypergraph that\napproximately preserves all the cuts) with $O(\\epsilon^{-2} n (r+\\log n))$\nedges. We also make first steps towards sketching general CSPs (Constraint\nSatisfaction Problems).\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 15:10:13 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Kogan", "Dmitry", ""], ["Krauthgamer", "Robert", ""]]}, {"id": "1409.2398", "submitter": "Sebastian Ordyniak", "authors": "Sebastian Ordyniak and Alexandru Popa", "title": "A Parameterized Study of Maximum Generalized Pattern Matching Problems", "comments": "to appear in Proc. IPEC'14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized function matching (GFM) problem has been intensively studied\nstarting with [Ehrenfeucht and Rozenberg, 1979]. Given a pattern p and a text\nt, the goal is to find a mapping from the letters of p to non-empty substrings\nof t, such that applying the mapping to p results in t. Very recently, the\nproblem has been investigated within the framework of parameterized complexity\n[Fernau, Schmid, and Villanger, 2013].\n  In this paper we study the parameterized complexity of the optimization\nvariant of GFM (called Max-GFM), which has been introduced in [Amir and Nor,\n2007]. Here, one is allowed to replace some of the pattern letters with some\nspecial symbols \"?\", termed wildcards or don't cares, which can be mapped to an\narbitrary substring of the text. The goal is to minimize the number of\nwildcards used.\n  We give a complete classification of the parameterized complexity of Max-GFM\nand its variants under a wide range of parameterizations, such as, the number\nof occurrences of a letter in the text, the size of the text alphabet, the\nnumber of occurrences of a letter in the pattern, the size of the pattern\nalphabet, the maximum length of a string matched to any pattern letter, the\nnumber of wildcards and the maximum size of a string that a wildcard can be\nmapped to.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 15:34:32 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Ordyniak", "Sebastian", ""], ["Popa", "Alexandru", ""]]}, {"id": "1409.2433", "submitter": "Antonina Kolokolova", "authors": "Antonina Kolokolova, Renesa Nizamee", "title": "Approximating solution structure of the Weighted Sentence Alignment\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of approximating solution structure of the bijective\nweighted sentence alignment problem of DeNero and Klein (2008). In particular,\nwe consider the complexity of finding an alignment that has a significant\noverlap with an optimal alignment. We discuss ways of representing the solution\nfor the general weighted sentence alignment as well as phrases-to-words\nalignment problem, and show that computing a string which agrees with the\noptimal sentence partition on more than half (plus an arbitrarily small\npolynomial fraction) positions for the phrases-to-words alignment is NP-hard.\nFor the general weighted sentence alignment we obtain such bound from the\nagreement on a little over 2/3 of the bits. Additionally, we generalize the\nHamming distance approximation of a solution structure to approximating it with\nrespect to the edit distance metric, obtaining similar lower bounds.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 17:19:11 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Kolokolova", "Antonina", ""], ["Nizamee", "Renesa", ""]]}, {"id": "1409.2603", "submitter": "Zhanpeng Cheng", "authors": "Zhanpeng Cheng, David Eppstein", "title": "Linear-time Algorithms for Proportional Apportionment", "comments": "13 pages, to appear in ISAAC 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The apportionment problem deals with the fair distribution of a discrete set\nof $k$ indivisible resources (such as legislative seats) to $n$ entities (such\nas parties or geographic subdivisions). Highest averages methods are a\nfrequently used class of methods for solving this problem. We present an\n$O(n)$-time algorithm for performing apportionment under a large class of\nhighest averages methods. Our algorithm works for all highest averages methods\nused in practice.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 05:55:34 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Cheng", "Zhanpeng", ""], ["Eppstein", "David", ""]]}, {"id": "1409.2733", "submitter": "Jean-Florent Raymond", "authors": "Aistis Atminas, Marcin Kami\\'nski and Jean-Florent Raymond", "title": "Scattered packings of cycles", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem Scattered Cycles which, given a graph $G$ and two\npositive integers $r$ and $\\ell$, asks whether $G$ contains a collection of $r$\ncycles that are pairwise at distance at least $\\ell$. This problem generalizes\nthe problem Disjoint Cycles which corresponds to the case $\\ell = 1$. We prove\nthat when parameterized by $r$, $\\ell$, and the maximum degree $\\Delta$, the\nproblem Scattered Cycles admits a kernel on $24 \\ell^2 \\Delta^\\ell r \\log(8\n\\ell^2 \\Delta^\\ell r)$ vertices. We also provide a $(16 \\ell^2\n\\Delta^\\ell)$-kernel for the case $r=2$ and a $(148 \\Delta r \\log r)$-kernel\nfor the case $\\ell = 1$. Our proofs rely on two simple reduction rules and a\ncareful analysis.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 13:34:53 GMT"}, {"version": "v2", "created": "Thu, 11 Sep 2014 11:31:16 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2015 14:46:42 GMT"}, {"version": "v4", "created": "Tue, 26 Jul 2016 18:05:31 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Atminas", "Aistis", ""], ["Kami\u0144ski", "Marcin", ""], ["Raymond", "Jean-Florent", ""]]}, {"id": "1409.2913", "submitter": "Mohit Singh", "authors": "Ronen Eldan and Mohit Singh", "title": "Efficient Algorithms for Discrepancy Minimization in Convex Sets", "comments": "Preliminary version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A result of Spencer states that every collection of $n$ sets over a universe\nof size $n$ has a coloring of the ground set with $\\{-1,+1\\}$ of discrepancy\n$O(\\sqrt{n})$. A geometric generalization of this result was given by Gluskin\n(see also Giannopoulos) who showed that every symmetric convex body $K\\subseteq\nR^n$ with Gaussian measure at least $e^{-\\epsilon n}$, for a small\n$\\epsilon>0$, contains a point $y\\in K$ where a constant fraction of\ncoordinates of $y$ are in $\\{-1,1\\}$. This is often called a partial coloring\nresult. While both these results were inherently non-algorithmic, recently\nBansal (see also Lovett-Meka) gave a polynomial time algorithm for Spencer's\nsetting and Rothvo\\ss gave a randomized polynomial time algorithm obtaining the\nsame guarantee as the result of Gluskin and Giannopoulos.\n  This paper has several related results. First we prove another constructive\nversion of the result of Gluskin and Giannopoulos via an optimization of a\nlinear function. This implies a linear programming based algorithm for\ncombinatorial discrepancy obtaining the same result as Spencer. Our second\nresult gives a new approach to obtains partial colorings and shows that every\nconvex body $K\\subseteq R^n$, possibly non-symmetric, with Gaussian measure at\nleast $e^{-\\epsilon n}$, for a small $\\epsilon>0$, contains a point $y\\in K$\nwhere a constant fraction of coordinates of $y$ are in $\\{-1,1\\}$. Finally, we\ngive a simple proof that shows that for any $\\delta >0$ there exists a constant\n$c>0$ such that given a body $K$ with $\\gamma_n(K)\\geq \\delta$, a uniformly\nrandom $x$ from $\\{-1,1\\}^n$ is in $cK$ with constant probability. This gives\nan algorithmic version of a special case of the result of Banaszczyk.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 22:50:21 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Eldan", "Ronen", ""], ["Singh", "Mohit", ""]]}, {"id": "1409.2928", "submitter": "Anatoly Rodionov", "authors": "Anatoly Rodionov", "title": "Path algebra algorithm for finding longest increasing subsequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  New algorithm for finding longest increasing subsequence is discussed. This\nalgorithm is based on the ideas of idempotent mathematics and uses Max-Plus\nidempotent semiring. Problem of finding longest increasing sub- sequence is\nreformulated in a matrix form and solved with linear algebra.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 00:49:34 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Rodionov", "Anatoly", ""]]}, {"id": "1409.3081", "submitter": "Martin Gro{\\ss}", "authors": "Ashwin Arulselvan, Martin Gro{\\ss}, Martin Skutella", "title": "Graph Orientation and Flows Over Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flows over time are used to model many real-world logistic and routing\nproblems. The networks underlying such problems -- streets, tracks, etc. -- are\ninherently undirected and directions are only imposed on them to reduce the\ndanger of colliding vehicles and similar problems. Thus the question arises,\nwhat influence the orientation of the network has on the network flow over time\nproblem that is being solved on the oriented network. In the literature, this\nis also referred to as the contraflow or lane reversal problem.\n  We introduce and analyze the price of orientation: How much flow is lost in\nany orientation of the network if the time horizon remains fixed? We prove that\nthere is always an orientation where we can still send $\\frac{1}{3}$ of the\nflow and this bound is tight. For the special case of networks with a single\nsource or sink, this fraction is $\\frac12$ which is again tight. We present\nmore results of similar flavor and also show non-approximability results for\nfinding the best orientation for single and multicommodity maximum flows over\ntime.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 14:18:28 GMT"}, {"version": "v2", "created": "Fri, 12 Sep 2014 10:57:46 GMT"}, {"version": "v3", "created": "Thu, 2 Oct 2014 09:23:20 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Arulselvan", "Ashwin", ""], ["Gro\u00df", "Martin", ""], ["Skutella", "Martin", ""]]}, {"id": "1409.3192", "submitter": "Pawe{\\l} Pszona", "authors": "Michael T. Goodrich, Pawe{\\l} Pszona", "title": "Two-Phase Bicriterion Search for Finding Fast and Efficient Electric\n  Vehicle Routes", "comments": "11 pages, 5 tables, 10 figures. To appear at ACM SIGSPATIAL 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding an electric vehicle route that optimizes both driving\ntime and energy consumption can be modeled as a bicriterion path problem.\nUnfortunately, the problem of finding optimal bicriterion paths is NP-complete.\nThis paper studies such problems restricted to two-phase paths, which\ncorrespond to a common way people drive electric vehicles, where a driver uses\none driving style (say, minimizing driving time) at the beginning of a route\nand another driving style (say, minimizing energy consumption) at the end. We\nprovide efficient polynomial-time algorithms for finding optimal two-phase\npaths in bicriterion networks, and we empirically verify the effectiveness of\nthese algorithms for finding good electric vehicle driving routes in the road\nnetworks of various U.S. states. In addition, we show how to incorporate\ncharging stations into these algorithms, in spite of the computational\nchallenges introduced by the negative energy consumption of such network\nvertices.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 19:03:16 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Goodrich", "Michael T.", ""], ["Pszona", "Pawe\u0142", ""]]}, {"id": "1409.3225", "submitter": "Saurabh Aggarwal", "authors": "Saurabh Aggarwal, Joy Kuri", "title": "Strategies for Utility Maximization in Social Groups with Preferential\n  Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a \\emph{Social Group} of networked nodes, seeking a \"universe\" of\nsegments for maximization of their utility. Each node has a subset of the\nuniverse, and access to an expensive link for downloading data. Nodes can also\nacquire the universe by exchanging copies of segments among themselves, at low\ncost, using inter-node links. While exchanges over inter-node links ensure\nminimum or negligible cost, some nodes in the group try to exploit the system.\nWe term such nodes as `non-reciprocating nodes' and prohibit such behavior by\nproposing the \"Give-and-Take\" criterion, where exchange is allowed iff each\nparticipating node has segments unavailable with the other. Following this\ncriterion for inter-node links, each node wants to maximize its utility, which\ndepends on the node's segment set available with the node. Link activation\namong nodes requires mutual consent of participating nodes. Each node tries to\nfind a pairing partner by preferentially exploring nodes for link formation and\nunpaired nodes choose to download a segment using the expensive link with\nsegment aggressive probability. We present various linear complexity\ndecentralized algorithms based on \\emph{Stable Roommates Problem} that can be\nused by nodes (as per their behavioral nature) for choosing the best strategy\nbased on available information. Then, we present decentralized randomized\nalgorithm that performs close to optimal for large number of nodes. We define\n\\emph{Price of Choices} for benchmarking performance for social groups\n(consisting of non-aggressive nodes only). We evaluate performances of various\nalgorithms and characterize the behavioral regime that will yield best results\nfor node and social group, spending the minimal on expensive link. We consider\nsocial group consisting of non-aggressive nodes and benchmark performances of\nproposed algorithms with the optimal.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 20:00:07 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Aggarwal", "Saurabh", ""], ["Kuri", "Joy", ""]]}, {"id": "1409.3600", "submitter": "Adrian Dumitrescu", "authors": "Ke Chen and Adrian Dumitrescu", "title": "Selection Algorithms with Small Groups", "comments": "13 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the selection problem, namely that of computing the $i$th order\nstatistic of $n$ given elements, in particular the classic deterministic\nalgorithm by grouping and partition due to Blum, Floyd, Pratt, Rivest, and\nTarjan (1973). Whereas the original algorithm uses groups of odd size at least\n$5$ and runs in linear time, it has been perpetuated in the literature that\nusing smaller group sizes will force the worst-case running time to become\nsuperlinear, namely $\\Omega(n \\log{n})$. We first point out that the usual\narguments found in the literature justifying the superlinear worst-case running\ntime fall short of proving this claim. We further prove that it is possible to\nuse group size smaller than $5$ while maintaining the worst case linear running\ntime. To this end we introduce three simple variants of the classic algorithm,\nthe repeated step algorithm, the shifting target algorithm, and the hyperpair\nalgorithm, all running in linear time.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 21:18:21 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 03:18:57 GMT"}, {"version": "v3", "created": "Fri, 5 Apr 2019 19:08:10 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Chen", "Ke", ""], ["Dumitrescu", "Adrian", ""]]}, {"id": "1409.3700", "submitter": "Xingfu Li", "authors": "Xingfu Li, Daming Zhu", "title": "A 4/3-approximation algorithm for finding a spanning tree to maximize\n  its internal vertices", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on finding a spanning tree of a graph to maximize the\nnumber of its internal vertices. We present an approximation algorithm for this\nproblem which can achieve a performance ratio $\\frac{4}{3}$ on undirected\nsimple graphs. This improves upon the best known approximation algorithm with\nperformance ratio $\\frac{5}{3}$ before. Our algorithm benefits from a new\nobservation for bounding the number of internal vertices of a spanning tree,\nwhich reveals that a spanning tree of an undirected simple graph has less\ninternal vertices than the edges a maximum path-cycle cover of that graph has.\nWe can also give an example to show that the performance ratio $\\frac{4}{3}$ is\nactually tight for this algorithm. To decide how difficult it is for this\nproblem to be approximated, we show that finding a spanning tree of an\nundirected simple graph to maximize its internal vertices is Max-SNP-Hard.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 11:20:10 GMT"}], "update_date": "2014-09-15", "authors_parsed": [["Li", "Xingfu", ""], ["Zhu", "Daming", ""]]}, {"id": "1409.3742", "submitter": "Henning Fernau", "authors": "Faisal N. Abu-Khzam, Cristina Bazgan, Morgan Chopin and Henning Fernau", "title": "Data Reductions and Combinatorial Bounds for Improved Approximation\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernelization algorithms in the context of Parameterized Complexity are often\nbased on a combination of reduction rules and combinatorial insights. We will\nexpose in this paper a similar strategy for obtaining polynomial-time\napproximation algorithms. Our method features the use of\napproximation-preserving reductions, akin to the notion of parameterized\nreductions. We exemplify this method to obtain the currently best approximation\nalgorithms for \\textsc{Harmless Set}, \\textsc{Differential} and\n\\textsc{Multiple Nonblocker}, all of them can be considered in the context of\nsecuring networks or information propagation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 14:14:04 GMT"}], "update_date": "2014-09-15", "authors_parsed": [["Abu-Khzam", "Faisal N.", ""], ["Bazgan", "Cristina", ""], ["Chopin", "Morgan", ""], ["Fernau", "Henning", ""]]}, {"id": "1409.3905", "submitter": "Mark Rudelson", "authors": "Mark Rudelson, Alex Samorodnitsky, Ofer Zeitouni", "title": "Hafnians, perfect matchings and Gaussian matrices", "comments": "Published at http://dx.doi.org/10.1214/15-AOP1036 in the Annals of\n  Probability (http://www.imstat.org/aop/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Probability 2016, Vol. 44, No. 4, 2858-2888", "doi": "10.1214/15-AOP1036", "report-no": "IMS-AOP-AOP1036", "categories": "math.PR cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the behavior of the Barvinok estimator of the hafnian of even\ndimension, symmetric matrices with nonnegative entries. We introduce a\ncondition under which the Barvinok estimator achieves subexponential errors,\nand show that this condition is almost optimal. Using that hafnians count the\nnumber of perfect matchings in graphs, we conclude that Barvinok's estimator\ngives a polynomial-time algorithm for the approximate (up to subexponential\nerrors) evaluation of the number of perfect matchings.\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 03:13:06 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 12:55:45 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Rudelson", "Mark", ""], ["Samorodnitsky", "Alex", ""], ["Zeitouni", "Ofer", ""]]}, {"id": "1409.4081", "submitter": "Carlo Nicola", "authors": "Markus Knecht, Willi Meier, and Carlo U. Nicola", "title": "A space- and time-efficient Implementation of the Merkle Tree Traversal\n  Algorithm", "comments": "19 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for the Merkle tree traversal problem which combines\nthe efficient space-time trade-off from the fractal Merkle tree [3] and the\nspace efficiency from the improved log space-time Merkle trees traversal [8].\nWe give an exhaustive analysis of the space and time efficiency of our\nalgorithm in function of the parameters H (the height of the Merkle tree) and h\n(h = H L where L is the number of levels in the Merkle tree). We also analyze\nthe space impact when a continuous deterministic pseudo-random number generator\n(PRNG) is used to generate the leaves. We further program a low storage-space\nand a low time-overhead version of the algorithm in Java and measure its\nperformance with respect to the two different implementations cited above. Our\nimplementation uses the least space when a continuous PRNG is used for the leaf\ncalculation.\n", "versions": [{"version": "v1", "created": "Sun, 14 Sep 2014 17:41:33 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Knecht", "Markus", ""], ["Meier", "Willi", ""], ["Nicola", "Carlo U.", ""]]}, {"id": "1409.4092", "submitter": "Minati De", "authors": "Binay K. Bhattacharya, Minati De, Subhas C. Nandy and Sasanka Roy", "title": "Facility location problems in the constant work-space read-only memory\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facility location problems are captivating both from theoretical and\npractical point of view. In this paper, we study some fundamental facility\nlocation problems from the space-efficient perspective. Here the input is\nconsidered to be given in a read-only memory and only constant amount of\nwork-space is available during the computation. This {\\em constant-work-space\nmodel} is well-motivated for handling big-data as well as for computing in\nsmart portable devices with small amount of extra-space.\n  First, we propose a strategy to implement prune-and-search in this model. As\na warm up, we illustrate this technique for finding the Euclidean 1-center\nconstrained on a line for a set of points in $\\IR^2$. This method works even if\nthe input is given in a sequential access read-only memory. Using this we show\nhow to compute (i) the Euclidean 1-center of a set of points in $\\IR^2$, and\n(ii) the weighted 1-center and weighted 2-center of a tree network. The running\ntime of all these algorithms are $O(n~poly(\\log n))$. While the result of (i)\ngives a positive answer to an open question asked by Asano, Mulzer, Rote and\nWang in 2011, the technique used can be applied to other problems which admit\nsolutions by prune-and-search paradigm. For example, we can apply the technique\nto solve two and three dimensional linear programming in $O(n~poly(\\log n))$\ntime in this model. To the best of our knowledge, these are the first\nsub-quadratic time algorithms for all the above mentioned problems in the\nconstant-work-space model. We also present optimal linear time algorithms for\nfinding the centroid and weighted median of a tree in this model.\n", "versions": [{"version": "v1", "created": "Sun, 14 Sep 2014 19:16:07 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Bhattacharya", "Binay K.", ""], ["De", "Minati", ""], ["Nandy", "Subhas C.", ""], ["Roy", "Sasanka", ""]]}, {"id": "1409.4271", "submitter": "Xiangrong Zeng", "authors": "Xiangrong Zeng, and M\\'ario A. T. Figueiredo", "title": "The Ordered Weighted $\\ell_1$ Norm: Atomic Formulation, Projections, and\n  Algorithms", "comments": "13 pages, 17 figures. The latest version of this paper was submitted\n  to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ordered weighted $\\ell_1$ norm (OWL) was recently proposed, with two\ndifferent motivations: its good statistical properties as a sparsity promoting\nregularizer; the fact that it generalizes the so-called {\\it octagonal\nshrinkage and clustering algorithm for regression} (OSCAR), which has the\nability to cluster/group regression variables that are highly correlated. This\npaper contains several contributions to the study and application of OWL\nregularization: the derivation of the atomic formulation of the OWL norm; the\nderivation of the dual of the OWL norm, based on its atomic formulation; a new\nand simpler derivation of the proximity operator of the OWL norm; an efficient\nscheme to compute the Euclidean projection onto an OWL ball; the instantiation\nof the conditional gradient (CG, also known as Frank-Wolfe) algorithm for\nlinear regression problems under OWL regularization; the instantiation of\naccelerated projected gradient algorithms for the same class of problems.\nFinally, a set of experiments give evidence that accelerated projected gradient\nalgorithms are considerably faster than CG, for the class of problems\nconsidered.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 14:22:34 GMT"}, {"version": "v2", "created": "Tue, 16 Sep 2014 14:01:09 GMT"}, {"version": "v3", "created": "Mon, 22 Sep 2014 14:08:29 GMT"}, {"version": "v4", "created": "Tue, 10 Mar 2015 16:07:39 GMT"}, {"version": "v5", "created": "Fri, 10 Apr 2015 13:21:46 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Zeng", "Xiangrong", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1409.4276", "submitter": "Paul Vitanyi", "authors": "Rudi L. Cilibrasi (CWI, Amsterdam) and Paul M.B. Vitanyi (CWI and\n  University of Amsterdam)", "title": "A Fast Quartet Tree Heuristic for Hierarchical Clustering", "comments": "LaTeX, 40 pages, 11 figures; this paper has substantial overlap with\n  arXiv:cs/0606048 in cs.DS", "journal-ref": "Pattern Recognition, 44 (2011) 662-677", "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Quartet Tree Cost problem is to construct an optimal weight tree\nfrom the $3{n \\choose 4}$ weighted quartet topologies on $n$ objects, where\noptimality means that the summed weight of the embedded quartet topologies is\noptimal (so it can be the case that the optimal tree embeds all quartets as\nnonoptimal topologies). We present a Monte Carlo heuristic, based on randomized\nhill climbing, for approximating the optimal weight tree, given the quartet\ntopology weights. The method repeatedly transforms a dendrogram, with all\nobjects involved as leaves, achieving a monotonic approximation to the exact\nsingle globally optimal tree. The problem and the solution heuristic has been\nextensively used for general hierarchical clustering of nontree-like\n(non-phylogeny) data in various domains and across domains with heterogeneous\ndata. We also present a greatly improved heuristic, reducing the running time\nby a factor of order a thousand to ten thousand. All this is implemented and\navailable, as part of the CompLearn package. We compare performance and running\ntime of the original and improved versions with those of UPGMA, BioNJ, and NJ,\nas implemented in the SplitsTree package on genomic data for which the latter\nare optimized.\n  Keywords: Data and knowledge visualization, Pattern\nmatching--Clustering--Algorithms/Similarity measures, Hierarchical clustering,\nGlobal optimization, Quartet tree, Randomized hill-climbing,\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 15:55:25 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Cilibrasi", "Rudi L.", "", "CWI, Amsterdam"], ["Vitanyi", "Paul M. B.", "", "CWI and\n  University of Amsterdam"]]}, {"id": "1409.4299", "submitter": "Giordano Da Lozzo", "authors": "Giordano Da Lozzo, V\\'it Jel\\'inek, Jan Kratochv\\'il, Ignaz Rutter", "title": "Planar Embeddings with Small and Uniform Faces", "comments": "23 pages, 5 figures, extended version of 'Planar Embeddings with\n  Small and Uniform Faces' (The 25th International Symposium on Algorithms and\n  Computation, 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by finding planar embeddings that lead to drawings with favorable\naesthetics, we study the problems MINMAXFACE and UNIFORMFACES of embedding a\ngiven biconnected multi-graph such that the largest face is as small as\npossible and such that all faces have the same size, respectively.\n  We prove a complexity dichotomy for MINMAXFACE and show that deciding whether\nthe maximum is at most $k$ is polynomial-time solvable for $k \\leq 4$ and\nNP-complete for $k \\geq 5$. Further, we give a 6-approximation for minimizing\nthe maximum face in a planar embedding. For UNIFORMFACES, we show that the\nproblem is NP-complete for odd $k \\geq 7$ and even $k \\geq 10$. Moreover, we\ncharacterize the biconnected planar multi-graphs admitting 3- and 4-uniform\nembeddings (in a $k$-uniform embedding all faces have size $k$) and give an\nefficient algorithm for testing the existence of a 6-uniform embedding.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 15:42:07 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Da Lozzo", "Giordano", ""], ["Jel\u00ednek", "V\u00edt", ""], ["Kratochv\u00edl", "Jan", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1409.4828", "submitter": "Damien Woods", "authors": "Ho-Lin Chen, David Doty, Dhiraj Holden, Chris Thachuk, Damien Woods,\n  Chun-Tao Yang", "title": "Fast algorithmic self-assembly of simple shapes using random agitation", "comments": "Conference version at DNA20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the power of uncontrolled random molecular movement in the nubot\nmodel of self-assembly. The nubot model is an asynchronous nondeterministic\ncellular automaton augmented with rigid-body movement rules (push/pull,\ndeterministically and programmatically applied to specific monomers) and random\nagitations (nondeterministically applied to every monomer and direction with\nequal probability all of the time). Previous work on the nubot model showed how\nto build simple shapes such as lines and squares quickly---in expected time\nthat is merely logarithmic of their size. These results crucially make use of\nthe programmable rigid-body movement rule: the ability for a single monomer to\ncontrol the movement of a large objects quickly, and only at a time and place\nof the programmers' choosing. However, in engineered molecular systems,\nmolecular motion is largely uncontrolled and fundamentally random. This raises\nthe question of whether similar results can be achieved in a more restrictive,\nand perhaps easier to justify, model where uncontrolled random movements, or\nagitations, are happening throughout the self-assembly process and are the only\nform of rigid-body movement. We show that this is indeed the case: we give a\npolylogarithmic expected time construction for squares using agitation, and a\nsublinear expected time construction to build a line. Such results are\nimpossible in an agitation-free (and movement-free) setting and thus show the\nbenefits of exploiting uncontrolled random movement.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 23:14:27 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Chen", "Ho-Lin", ""], ["Doty", "David", ""], ["Holden", "Dhiraj", ""], ["Thachuk", "Chris", ""], ["Woods", "Damien", ""], ["Yang", "Chun-Tao", ""]]}, {"id": "1409.4843", "submitter": "Jiecao Chen", "authors": "Jiecao Chen, Qin Zhang", "title": "Improved Algorithms for Distributed Entropy Monitoring", "comments": "23 pages (include reference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data management systems often need to deal with massive, dynamic and\ninherently distributed data sources. We collect the data using a distributed\nnetwork, and at the same time try to maintain a global view of the data at a\ncentral coordinator using a minimal amount of communication. Such applications\nhave been captured by the distributed monitoring model which has attracted a\nlot of attention in recent years. In this paper we investigate the monitoring\nof the entropy functions, which are very useful in network monitoring\napplications such as detecting distributed denial-of-service attacks. Our\nresults improve the previous best results by Arackaparambil et al. [2]. Our\ntechnical contribution also includes implementing the celebrated AMS sampling\nmethod (by Alon et al. [1]) in the distributed monitoring model, which could be\nof independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 01:06:08 GMT"}, {"version": "v2", "created": "Mon, 12 Jan 2015 05:28:39 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2015 14:03:28 GMT"}, {"version": "v4", "created": "Wed, 3 Aug 2016 05:12:20 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Chen", "Jiecao", ""], ["Zhang", "Qin", ""]]}, {"id": "1409.4935", "submitter": "Fahad Panolan", "authors": "Prachi Goyal, Pranabendu Misra, Fahad Panolan, Geevarghese Philip,\n  Saket Saurabh", "title": "Finding Even Subgraphs Even Faster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems of the following kind have been the focus of much recent research in\nthe realm of parameterized complexity: Given an input graph (digraph) on $n$\nvertices and a positive integer parameter $k$, find if there exist $k$ edges\n(arcs) whose deletion results in a graph that satisfies some specified parity\nconstraints. In particular, when the objective is to obtain a connected graph\nin which all the vertices have even degrees---where the resulting graph is\n\\emph{Eulerian}---the problem is called Undirected Eulerian Edge Deletion. The\ncorresponding problem in digraphs where the resulting graph should be strongly\nconnected and every vertex should have the same in-degree as its out-degree is\ncalled Directed Eulerian Edge Deletion. Cygan et al. [\\emph{Algorithmica,\n2014}] showed that these problems are fixed parameter tractable (FPT), and gave\nalgorithms with the running time $2^{O(k \\log k)}n^{O(1)}$. They also asked, as\nan open problem, whether there exist FPT algorithms which solve these problems\nin time $2^{O(k)}n^{O(1)}$. In this paper we answer their question in the\naffirmative: using the technique of computing \\emph{representative families of\nco-graphic matroids} we design algorithms which solve these problems in time\n$2^{O(k)}n^{O(1)}$. The crucial insight we bring to these problems is to view\nthe solution as an independent set of a co-graphic matroid. We believe that\nthis view-point/approach will be useful in other problems where one of the\nconstraints that need to be satisfied is that of connectivity.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 10:12:48 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Goyal", "Prachi", ""], ["Misra", "Pranabendu", ""], ["Panolan", "Fahad", ""], ["Philip", "Geevarghese", ""], ["Saurabh", "Saket", ""]]}, {"id": "1409.4955", "submitter": "Alois Panholzer", "authors": "Hsien-Kuei Hwang, Alois Panholzer, Nicolas Rolin, Tsung-Hsi Tsai,\n  Wei-Mei Chen", "title": "Probabilistic analysis of the (1+1)-evolutionary algorithm", "comments": "53 pages with 8 figures and 4 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a detailed analysis of the cost used by the (1+1)-evolutionary\nalgorithm. The problem has been approached in the evolutionary algorithm\nliterature under various views, formulation and degree of rigor. Our asymptotic\napproximations for the mean and the variance represent the strongest of their\nkind. The approach we develop is also applicable to characterize the limit laws\nand is based on asymptotic resolution of the underlying recurrence. While most\napproximations have their simple formal nature, we elaborate on the delicate\nerror analysis required for rigorous justifications.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 11:47:25 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Hwang", "Hsien-Kuei", ""], ["Panholzer", "Alois", ""], ["Rolin", "Nicolas", ""], ["Tsai", "Tsung-Hsi", ""], ["Chen", "Wei-Mei", ""]]}, {"id": "1409.4977", "submitter": "Meghana Nasre Ms.", "authors": "Pratik Ghoshal, Meghana Nasre, Prajakta Nimbhorkar", "title": "Rank Maximal Matchings -- Structure and Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let G = (A U P, E) be a bipartite graph where A denotes a set of agents, P\ndenotes a set of posts and ranks on the edges denote preferences of the agents\nover posts. A matching M in G is rank-maximal if it matches the maximum number\nof applicants to their top-rank post, subject to this, the maximum number of\napplicants to their second rank post and so on.\n  In this paper, we develop a switching graph characterization of rank-maximal\nmatchings, which is a useful tool that encodes all rank-maximal matchings in an\ninstance. The characterization leads to simple and efficient algorithms for\nseveral interesting problems. In particular, we give an efficient algorithm to\ncompute the set of rank-maximal pairs in an instance. We show that the problem\nof counting the number of rank-maximal matchings is #P-Complete and also give\nan FPRAS for the problem. Finally, we consider the problem of deciding whether\na rank-maximal matching is popular among all the rank-maximal matchings in a\ngiven instance, and give an efficient algorithm for the problem.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 12:57:52 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Ghoshal", "Pratik", ""], ["Nasre", "Meghana", ""], ["Nimbhorkar", "Prajakta", ""]]}, {"id": "1409.4991", "submitter": "Alexander Setzer", "authors": "Martina Eikel, Christian Scheideler, Alexander Setzer", "title": "RoBuSt: A Crash-Failure-Resistant Distributed Storage System", "comments": "Revised full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present the first distributed storage system that is provably\nrobust against crash failures issued by an adaptive adversary, i.e., for each\nbatch of requests the adversary can decide based on the entire system state\nwhich servers will be unavailable for that batch of requests. Despite up to\n$\\gamma n^{1/\\log\\log n}$ crashed servers, with $\\gamma>0$ constant and $n$\ndenoting the number of servers, our system can correctly process any batch of\nlookup and write requests (with at most a polylogarithmic number of requests\nissued at each non-crashed server) in at most a polylogarithmic number of\ncommunication rounds, with at most polylogarithmic time and work at each server\nand only a logarithmic storage overhead.\n  Our system is based on previous work by Eikel and Scheideler (SPAA 2013), who\npresented IRIS, a distributed information system that is provably robust\nagainst the same kind of crash failures. However, IRIS is only able to serve\nlookup requests. Handling both lookup and write requests has turned out to\nrequire major changes in the design of IRIS.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 13:38:30 GMT"}, {"version": "v2", "created": "Thu, 18 Sep 2014 11:05:37 GMT"}, {"version": "v3", "created": "Fri, 6 Feb 2015 16:11:11 GMT"}, {"version": "v4", "created": "Mon, 23 Feb 2015 08:13:37 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Eikel", "Martina", ""], ["Scheideler", "Christian", ""], ["Setzer", "Alexander", ""]]}, {"id": "1409.5000", "submitter": "Vincenzo Fioriti dr", "authors": "Vincenzo Fioriti and Marta Chinnici", "title": "Identifying sparse and dense sub-graphs in large graphs with a fast\n  algorithm", "comments": null, "journal-ref": null, "doi": "10.1209/0295-5075/108/50006", "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the nodes of small sub-graphs with no a priori information is a\nhard problem. In this work, we want to find each node of a sparse sub-graph\nembedded in both dynamic and static background graphs, of larger average\ndegree. We show that exploiting the summability over several background\nrealizations of the Estrada-Benzi communicability and the Krylov approximation\nof the matrix exponential, it is possible to recover the sub-graph with a fast\nalgorithm with computational complexity O(N n). Relaxing the problem to\ncomplete sub-graphs, the same performance is obtained with a single background.\nThe worst case complexity for the single background is O(n log(n)).\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 13:56:19 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Fioriti", "Vincenzo", ""], ["Chinnici", "Marta", ""]]}, {"id": "1409.5166", "submitter": "Hu Qin", "authors": "Hu Qin, Zizhen Zhang, Yubin Xie, Andrew Lim", "title": "A Tabu Search Algorithm for the Multi-period Inspector Scheduling\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a multi-period inspector scheduling problem (MPISP),\nwhich is a new variant of the multi-trip vehicle routing problem with time\nwindows (VRPTW). In the MPISP, each inspector is scheduled to perform a route\nin a given multi-period planning horizon. At the end of each period, each\ninspector is not required to return to the depot but has to stay at one of the\nvertices for recuperation. If the remaining time of the current period is\ninsufficient for an inspector to travel from his/her current vertex $A$ to a\ncertain vertex B, he/she can choose either waiting at vertex A until the start\nof the next period or traveling to a vertex C that is closer to vertex B.\nTherefore, the shortest transit time between any vertex pair is affected by the\nlength of the period and the departure time. We first describe an approach of\ncomputing the shortest transit time between any pair of vertices with an\narbitrary departure time. To solve the MPISP, we then propose several local\nsearch operators adapted from classical operators for the VRPTW and integrate\nthem into a tabu search framework. In addition, we present a constrained\nknapsack model that is able to produce an upper bound for the problem. Finally,\nwe evaluate the effectiveness of our algorithm with extensive experiments based\non a set of test instances. Our computational results indicate that our\napproach generates high-quality solutions.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 23:29:46 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Qin", "Hu", ""], ["Zhang", "Zizhen", ""], ["Xie", "Yubin", ""], ["Lim", "Andrew", ""]]}, {"id": "1409.5214", "submitter": "Mathieu Hemery", "authors": "David Gamarnik, Mathieu Hemery, Samuel Hetterich", "title": "Local Algorithms for Graphs", "comments": "Chapter of \"Statistical Physics, Optimization, Inference, and\n  Message-Passing Algorithms\", Eds.: F. Krzakala, F. Ricci-Tersenghi, L.\n  Zdeborova, R. Zecchina, E. W. Tramel, L. F. Cugliandolo (Oxford University\n  Press, to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We are going to analyze local algorithms over sparse random graphs. These\nalgorithms are based on local information where local regards to a decision\nmade by the exploration of a small neighbourhood of a certain vertex plus a\nbelieve of the structure of the whole graph and maybe added some randomness.\nThis kind of algorithms can be a natural response to the given problem or an\nefficient approximation such as the Belief Propagation Algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 07:46:06 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Gamarnik", "David", ""], ["Hemery", "Mathieu", ""], ["Hetterich", "Samuel", ""]]}, {"id": "1409.5308", "submitter": "Mohammed El-Kebir", "authors": "Mohammed El-Kebir and Gunnar W. Klau", "title": "Solving the Maximum-Weight Connected Subgraph Problem to Optimality", "comments": "11th DIMACS implementation challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected node-weighted graph, the Maximum-Weight Connected\nSubgraph problem (MWCS) is to identify a subset of nodes of maximalsum of\nweights that induce a connected subgraph. MWCS is closely related to the\nwell-studied Prize Collecting Steiner Tree problem and has many applications in\ndifferent areas, including computational biology, network design and computer\nvision. The problem is NP-hard and even hard to approximate within a constant\nfactor. In this work we describe an algorithmic scheme for solving MWCS to\nprovable optimality, which is based on preprocessing rules, new results on\ndecomposing an instance into its biconnected and triconnected components and a\nbranch-and-cut approach combined with a primal heuristic. We demonstrate the\nperformance of our method on the benchmark instances of the 11th DIMACS\nimplementation challenge consisting of MWCS as well as transformed PCST\ninstances.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 14:15:45 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2014 22:13:05 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["El-Kebir", "Mohammed", ""], ["Klau", "Gunnar W.", ""]]}, {"id": "1409.5414", "submitter": "Jalaj Upadhyay", "authors": "Jalaj Upadhyay", "title": "Differentially Private Linear Algebra in the Streaming Model", "comments": "This paper is subsumed by https://arxiv.org/abs/1604.01429", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical linear algebra plays an important role in computer science. In this\npaper, we initiate the study of performing linear algebraic tasks while\npreserving privacy when the data is streamed online. Our main focus is the\nspace requirement of the privacy-preserving data-structures. We give the first\n{\\em sketch-based} algorithm for differential privacy. We give optimal, up to\nlogarithmic factor, space data-structures that can compute low rank\napproximation, linear regression, and matrix multiplication, while preserving\ndifferential privacy with better additive error bounds compared to the known\nresults. Notably, we match the best known space bound in the non-private\nsetting by Kane and Nelson (J. ACM, 61(1):4).\n  Our mechanism for differentially private low-rank approximation {\\em reuses}\nthe random Gaussian matrix in a specific way to provide a single-pass\nmechanism. We prove that the resulting distribution also preserve differential\nprivacy. This can be of independent interest. We do not make any assumptions,\nlike singular value separation or normalized row assumption, as made in the\nearlier works. The mechanisms for matrix multiplication and linear regression\ncan be seen as the private analogues of the known non-private algorithms. All\nour mechanisms, in the form presented, can also be computed in the distributed\nsetting.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 19:08:58 GMT"}, {"version": "v2", "created": "Thu, 9 Oct 2014 13:46:35 GMT"}, {"version": "v3", "created": "Wed, 5 Nov 2014 16:01:33 GMT"}, {"version": "v4", "created": "Sun, 19 Apr 2015 01:17:18 GMT"}, {"version": "v5", "created": "Wed, 25 Oct 2017 12:48:42 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Upadhyay", "Jalaj", ""]]}, {"id": "1409.5452", "submitter": "Michael Bannister", "authors": "Michael J. Bannister, William E. Devanny, Michael T. Goodrich, Joseph\n  A. Simons, Lowell Trott", "title": "Windows into Geometric Events: Data Structures for Time-Windowed\n  Querying of Temporal Point Sets", "comments": "CCCG 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study geometric data structures for sets of point-based temporal events,\nanswering time-windowed queries, i.e., given a contiguous time interval we\nanswer common geometric queries about the point events with time stamps in this\ninterval. The geometric queries we consider include queries based on the\nskyline, convex hull, and proximity relations of the point set. We provide\nspace efficient data structures which answer queries in polylogarithmic time.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 20:16:45 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Bannister", "Michael J.", ""], ["Devanny", "William E.", ""], ["Goodrich", "Michael T.", ""], ["Simons", "Joseph A.", ""], ["Trott", "Lowell", ""]]}, {"id": "1409.5512", "submitter": "Liangyue Li", "authors": "Liangyue Li, Hanghang Tong, Nan Cao, Kate Ehrlich, Yu-Ru Lin, Norbou\n  Buchler", "title": "Replacing the Irreplaceable: Fast Algorithms for Team Member\n  Recommendation", "comments": "Initially submitted to KDD 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of Team Member Replacement: given a team\nof people embedded in a social network working on the same task, find a good\ncandidate who can fit in the team after one team member becomes unavailable. We\nconjecture that a good team member replacement should have good skill matching\nas well as good structure matching. We formulate this problem using the concept\nof graph kernel. To tackle the computational challenges, we propose a family of\nfast algorithms by (a) designing effective pruning strategies, and (b)\nexploring the smoothness between the existing and the new team structures. We\nconduct extensive experimental evaluations on real world datasets to\ndemonstrate the effectiveness and efficiency. Our algorithms (a) perform\nsignificantly better than the alternative choices in terms of both precision\nand recall; and (b) scale sub-linearly.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 04:05:16 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Li", "Liangyue", ""], ["Tong", "Hanghang", ""], ["Cao", "Nan", ""], ["Ehrlich", "Kate", ""], ["Lin", "Yu-Ru", ""], ["Buchler", "Norbou", ""]]}, {"id": "1409.5561", "submitter": "Takuro Fukunaga", "authors": "Takuro Fukunaga", "title": "Approximating the generalized terminal backup problem via half-integral\n  multiflow relaxation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a network design problem called the generalized terminal backup\nproblem. Whereas earlier work investigated the edge-connectivity constraints\nonly, we consider both edge- and node-connectivity constraints for this\nproblem. A major contribution of this paper is the development of a strongly\npolynomial-time 4/3-approximation algorithm for the problem. Specifically, we\nshow that a linear programming relaxation of the problem is half-integral, and\nthat the half-integral optimal solution can be rounded to a 4/3-approximate\nsolution. We also prove that the linear programming relaxation of the problem\nwith the edge-connectivity constraints is equivalent to minimizing the cost of\nhalf-integral multiflows that satisfy flow demands given from terminals. This\nobservation presents a strongly polynomial-time algorithm for computing a\nminimum cost half-integral multiflow under flow demand constraints.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 09:14:28 GMT"}, {"version": "v2", "created": "Mon, 19 Jan 2015 01:14:11 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Fukunaga", "Takuro", ""]]}, {"id": "1409.5641", "submitter": "Dmitry Kosolobov", "authors": "Dmitry Kosolobov", "title": "Lempel-Ziv Factorization May Be Harder Than Computing All Runs", "comments": "12 pages, 3 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of computing the Lempel-Ziv factorization and the set of all\nruns (= maximal repetitions) is studied in the decision tree model of\ncomputation over ordered alphabet. It is known that both these problems can be\nsolved by RAM algorithms in $O(n\\log\\sigma)$ time, where $n$ is the length of\nthe input string and $\\sigma$ is the number of distinct letters in it. We prove\nan $\\Omega(n\\log\\sigma)$ lower bound on the number of comparisons required to\nconstruct the Lempel-Ziv factorization and thereby conclude that a popular\ntechnique of computation of runs using the Lempel-Ziv factorization cannot\nachieve an $o(n\\log\\sigma)$ time bound. In contrast with this, we exhibit an\n$O(n)$ decision tree algorithm finding all runs in a string. Therefore, in the\ndecision tree model the runs problem is easier than the Lempel-Ziv\nfactorization. Thus we support the conjecture that there is a linear RAM\nalgorithm finding all runs.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 13:11:13 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Kosolobov", "Dmitry", ""]]}, {"id": "1409.5757", "submitter": "Andrey Vladimirov", "authors": "Ryo Asai and Andrey Vladimirov", "title": "Intel Cilk Plus for Complex Parallel Algorithms: \"Enormous Fast Fourier\n  Transform\" (EFFT) Library", "comments": "17 pages. Submitted to Parallel Computing", "journal-ref": null, "doi": "10.1016/j.parco.2015.05.004", "report-no": null, "categories": "cs.MS cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate the methodology for parallelizing the\ncomputation of large one-dimensional discrete fast Fourier transforms (DFFTs)\non multi-core Intel Xeon processors. DFFTs based on the recursive Cooley-Tukey\nmethod have to control cache utilization, memory bandwidth and vector hardware\nusage, and at the same time scale across multiple threads or compute nodes. Our\nmethod builds on single-threaded Intel Math Kernel Library (MKL) implementation\nof DFFT, and uses the Intel Cilk Plus framework for thread parallelism. We\ndemonstrate the ability of Intel Cilk Plus to handle parallel recursion with\nnested loop-centric parallelism without tuning the code to the number of cores\nor cache metrics. The result of our work is a library called EFFT that performs\n1D DFTs of size 2^N for N>=21 faster than the corresponding Intel MKL parallel\nDFT implementation by up to 1.5x, and faster than FFTW by up to 2.5x. The code\nof EFFT is available for free download under the GPLv3 license. This work\nprovides a new efficient DFFT implementation, and at the same time demonstrates\nan educational example of how computer science problems with complex parallel\npatterns can be optimized for high performance using the Intel Cilk Plus\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 18:48:58 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Asai", "Ryo", ""], ["Vladimirov", "Andrey", ""]]}, {"id": "1409.5834", "submitter": "Tim Roughgarden", "authors": "Amir Globerson and Tim Roughgarden and David Sontag and Cafer Yildirim", "title": "Tight Error Bounds for Structured Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured prediction tasks in machine learning involve the simultaneous\nprediction of multiple labels. This is typically done by maximizing a score\nfunction on the space of labels, which decomposes as a sum of pairwise\nelements, each depending on two specific labels. Intuitively, the more pairwise\nterms are used, the better the expected accuracy. However, there is currently\nno theoretical account of this intuition. This paper takes a significant step\nin this direction.\n  We formulate the problem as classifying the vertices of a known graph\n$G=(V,E)$, where the vertices and edges of the graph are labelled and correlate\nsemi-randomly with the ground truth. We show that the prospects for achieving\nlow expected Hamming error depend on the structure of the graph $G$ in\ninteresting ways. For example, if $G$ is a very poor expander, like a path,\nthen large expected Hamming error is inevitable. Our main positive result shows\nthat, for a wide class of graphs including 2D grid graphs common in machine\nvision applications, there is a polynomial-time algorithm with small and\ninformation-theoretically near-optimal expected error. Our results provide a\nfirst step toward a theoretical justification for the empirical success of the\nefficient approximate inference algorithms that are used for structured\nprediction in models where exact inference is intractable.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 23:51:09 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Globerson", "Amir", ""], ["Roughgarden", "Tim", ""], ["Sontag", "David", ""], ["Yildirim", "Cafer", ""]]}, {"id": "1409.5900", "submitter": "Moran Feldman", "authors": "Moran Feldman", "title": "Maximizing Symmetric Submodular Functions", "comments": "31 pages, an extended abstract appeared in ESA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric submodular functions are an important family of submodular\nfunctions capturing many interesting cases including cut functions of graphs\nand hypergraphs. Maximization of such functions subject to various constraints\nreceives little attention by current research, unlike similar minimization\nproblems which have been widely studied. In this work, we identify a few\nsubmodular maximization problems for which one can get a better approximation\nfor symmetric objectives than the state of the art approximation for general\nsubmodular functions.\n  We first consider the problem of maximizing a non-negative symmetric\nsubmodular function $f\\colon 2^\\mathcal{N} \\to \\mathbb{R}^+$ subject to a\ndown-monotone solvable polytope $\\mathcal{P} \\subseteq [0, 1]^\\mathcal{N}$. For\nthis problem we describe an algorithm producing a fractional solution of value\nat least $0.432 \\cdot f(OPT)$, where $OPT$ is the optimal integral solution.\nOur second result considers the problem $\\max \\{f(S) : |S| = k\\}$ for a\nnon-negative symmetric submodular function $f\\colon 2^\\mathcal{N} \\to\n\\mathbb{R}^+$. For this problem, we give an approximation ratio that depends on\nthe value $k / |\\mathcal{N}|$ and is always at least $0.432$. Our method can\nalso be applied to non-negative non-symmetric submodular functions, in which\ncase it produces $1/e - o(1)$ approximation, improving over the best known\nresult for this problem. For unconstrained maximization of a non-negative\nsymmetric submodular function we describe a deterministic linear-time\n$1/2$-approximation algorithm. Finally, we give a $[1 - (1 - 1/k)^{k -\n1}]$-approximation algorithm for Submodular Welfare with $k$ players having\nidentical non-negative submodular utility functions, and show that this is the\nbest possible approximation ratio for the problem.\n", "versions": [{"version": "v1", "created": "Sat, 20 Sep 2014 15:48:35 GMT"}, {"version": "v2", "created": "Sun, 17 Apr 2016 08:44:17 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Feldman", "Moran", ""]]}, {"id": "1409.6011", "submitter": "Ben Cousins", "authors": "Ben Cousins and Santosh Vempala", "title": "Gaussian Cooling and O*(n^3) Algorithms for Volume and Gaussian Volume", "comments": "This paper is a combination of two previously published conference\n  papers: \"A Cubic Algorithm for Computing Gaussian Volume\" (SODA 2014,\n  arXiv:1306.5829) and \"Bypassing KLS: Gaussian Cooling and an $O^*(n^3)$\n  Volume Algorithm\" (STOC 2015). Additionally, this version has a major\n  simplification to the main proof in the latter conference paper. (Lemma 3.2\n  in this version) 36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an $O^*(n^3)$ randomized algorithm for estimating the volume of a\nwell-rounded convex body given by a membership oracle, improving on the\nprevious best complexity of $O^*(n^4)$. The new algorithmic ingredient is an\naccelerated cooling schedule where the rate of cooling increases with the\ntemperature. Previously, the known approach for potentially achieving this\nasymptotic complexity relied on a positive resolution of the KLS hyperplane\nconjecture, a central open problem in convex geometry.\n  We also obtain an $O^*(n^3)$ randomized algorithm for integrating a standard\nGaussian distribution over an arbitrary convex set containing the unit ball.\nBoth the volume and Gaussian volume algorithms use an improved algorithm for\nsampling a Gaussian distribution restricted to a convex body. In this latter\nsetting, as we show, the KLS conjecture holds and for a spherical Gaussian\ndistribution with variance $\\sigma^2$, the sampling complexity is\n$O^*(\\max\\{n^3, \\sigma^2n^2\\})$ for the first sample and $O^*(\\max\\{n^2,\n\\sigma^2n^2\\})$ for every subsequent sample.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 16:33:53 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 20:01:45 GMT"}, {"version": "v3", "created": "Mon, 5 Dec 2016 02:38:41 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Cousins", "Ben", ""], ["Vempala", "Santosh", ""]]}, {"id": "1409.6015", "submitter": "Marcelo Siqueira", "authors": "Suneeta Ramaswami and Marcelo Siqueira", "title": "A fast algorithm for computing irreducible triangulations of closed\n  surfaces in $E^d$", "comments": "52 pages, a shorter version of this Technical Report is about to be\n  submitted to Elsevier Journal Computational Geometry: Theory and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GT cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a fast algorithm for computing an irreducible triangulation\n$T^\\prime$ of an oriented, connected, boundaryless, and compact surface $S$ in\n$E^d$ from any given triangulation $T$ of $S$. If the genus $g$ of $S$ is\npositive, then our algorithm takes $O(g^2+gn)$ time to obtain $T^\\prime$, where\n$n$ is the number of triangles of $T$. Otherwise, $T^\\prime$ is obtained in\nlinear time in $n$. While the latter upper bound is optimal, the former upper\nbound improves upon the currently best known upper bound by a $(\\lg n / g)$\nfactor. In both cases, the memory space required by our algorithm is in\n${\\Theta}(n)$.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 17:02:25 GMT"}, {"version": "v2", "created": "Thu, 25 Sep 2014 11:21:42 GMT"}, {"version": "v3", "created": "Mon, 27 Oct 2014 04:40:50 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Ramaswami", "Suneeta", ""], ["Siqueira", "Marcelo", ""]]}, {"id": "1409.6199", "submitter": "Chandan K. Dubey", "authors": "Chandan Dubey and Thomas Holenstein", "title": "Computing the $p$-adic Canonical Quadratic Form in Polynomial Time", "comments": "arXiv admin note: text overlap with arXiv:1404.0281", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.NT math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $n$-ary integral quadratic form is a formal expression\n$Q(x_1,..,x_n)=\\sum_{1\\leq i,j\\leq n}a_{ij}x_ix_j$ in $n$-variables\n$x_1,...,x_n$, where $a_{ij}=a_{ji} \\in \\mathbb{Z}$. We present a randomized\npolynomial time algorithm that given a quadratic form $Q(x_1,...,x_n)$, a prime\n$p$, and a positive integer $k$ outputs a $\\mathtt{U} \\in\n\\text{GL}_n(\\mathbb{Z}/p^k\\mathbb{Z})$ such that $\\mathtt{U}$ transforms $Q$ to\nits $p$-adic canonical form.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 15:27:52 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Dubey", "Chandan", ""], ["Holenstein", "Thomas", ""]]}, {"id": "1409.6241", "submitter": "Christian Lorenz Staudt", "authors": "Elisabetta Bergamini and Henning Meyerhenke and Christian L. Staudt", "title": "Approximating Betweenness Centrality in Large Evolving Networks", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Betweenness centrality ranks the importance of nodes by their participation\nin all shortest paths of the network. Therefore computing exact betweenness\nvalues is impractical in large networks. For static networks, approximation\nbased on randomly sampled paths has been shown to be significantly faster in\npractice. However, for dynamic networks, no approximation algorithm for\nbetweenness centrality is known that improves on static recomputation. We\naddress this deficit by proposing two incremental approximation algorithms (for\nweighted and unweighted connected graphs) which provide a provable guarantee on\nthe absolute approximation error. Processing batches of edge insertions, our\nalgorithms yield significant speedups up to a factor of $10^4$ compared to\nrestarting the approximation. This is enabled by investing memory to store and\nefficiently update shortest paths. As a building block, we also propose an\nasymptotically faster algorithm for updating the SSSP problem in unweighted\ngraphs. Our experimental study shows that our algorithms are the first to make\nin-memory computation of a betweenness ranking practical for million-edge\nsemi-dynamic networks. Moreover, our results show that the accuracy is even\nbetter than the theoretical guarantees in terms of absolutes errors and the\nrank of nodes is well preserved, in particular for those with high betweenness.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 16:51:41 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Bergamini", "Elisabetta", ""], ["Meyerhenke", "Henning", ""], ["Staudt", "Christian L.", ""]]}, {"id": "1409.6277", "submitter": "Luigi Laura", "authors": "Loukas Georgiadis, Giuseppe F. Italiano, Luigi Laura, Nikos Parotsidis", "title": "2-Vertex Connectivity in Directed Graphs", "comments": "arXiv admin note: substantial text overlap with arXiv:1407.3041", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We complement our study of 2-connectivity in directed graphs, by considering\nthe computation of the following 2-vertex-connectivity relations: We say that\ntwo vertices v and w are 2-vertex-connected if there are two internally\nvertex-disjoint paths from v to w and two internally vertex-disjoint paths from\nw to v. We also say that v and w are vertex-resilient if the removal of any\nvertex different from v and w leaves v and w in the same strongly connected\ncomponent. We show how to compute the above relations in linear time so that we\ncan report in constant time if two vertices are 2-vertex-connected or if they\nare vertex-resilient. We also show how to compute in linear time a sparse\ncertificate for these relations, i.e., a subgraph of the input graph that has\nO(n) edges and maintains the same 2-vertex-connectivity and vertex-resilience\nrelations as the input graph, where n is the number of vertices.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 18:48:27 GMT"}, {"version": "v2", "created": "Thu, 19 Feb 2015 07:51:16 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Georgiadis", "Loukas", ""], ["Italiano", "Giuseppe F.", ""], ["Laura", "Luigi", ""], ["Parotsidis", "Nikos", ""]]}, {"id": "1409.6365", "submitter": "Edward Lee", "authors": "Konstantinos Georgiou, Andy Jiang, Edward Lee, Astrid A. Olave, Ian\n  Seong, Twesh Upadhyaya", "title": "Lift & Project Systems Performing on the Partial-Vertex-Cover Polytope", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study integrality gap (IG) lower bounds on strong LP and SDP relaxations\nderived by the Sherali-Adams (SA), Lovasz-Schrijver-SDP (LS+), and\nSherali-Adams-SDP (SA+) lift-and-project (L&P) systems for the\nt-Partial-Vertex-Cover (t-PVC) problem, a variation of the classic Vertex-Cover\nproblem in which only t edges need to be covered. t-PVC admits a\n2-approximation using various algorithmic techniques, all relying on a natural\nLP relaxation. Starting from this LP relaxation, our main results assert that\nfor every epsilon > 0, level-Theta(n) LPs or SDPs derived by all known L&P\nsystems that have been used for positive algorithmic results (but the Lasserre\nhierarchy) have IGs at least (1-epsilon)n/t, where n is the number of vertices\nof the input graph. Our lower bounds are nearly tight.\n  Our results show that restricted yet powerful models of computation derived\nby many L&P systems fail to witness c-approximate solutions to t-PVC for any\nconstant c, and for t = O(n). This is one of the very few known examples of an\nintractable combinatorial optimization problem for which LP-based algorithms\ninduce a constant approximation ratio, still lift-and-project LP and SDP\ntightenings of the same LP have unbounded IGs.\n  We also show that the SDP that has given the best algorithm known for t-PVC\nhas integrality gap n/t on instances that can be solved by the level-1 LP\nrelaxation derived by the LS system. This constitutes another rare phenomenon\nwhere (even in specific instances) a static LP outperforms an SDP that has been\nused for the best approximation guarantee for the problem at hand. Finally, one\nof our main contributions is that we make explicit of a new and simple\nmethodology of constructing solutions to LP relaxations that almost trivially\nsatisfy constraints derived by all SDP L&P systems known to be useful for\nalgorithmic positive results (except the La system).\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 22:39:31 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 16:05:46 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2020 18:53:53 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Georgiou", "Konstantinos", ""], ["Jiang", "Andy", ""], ["Lee", "Edward", ""], ["Olave", "Astrid A.", ""], ["Seong", "Ian", ""], ["Upadhyaya", "Twesh", ""]]}, {"id": "1409.6510", "submitter": "Eranda Cela", "authors": "Eranda Cela and Vladimir G. Deineko and Gerhard J. Woeginger", "title": "Linearizable special cases of the QAP", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider special cases of the quadratic assignment problem (QAP) that are\nlinearizable in the sense of Bookhold. We provide combinatorial\ncharacterizations of the linearizable instances of the weighted feedback arc\nset QAP, and of the linearizable instances of the traveling salesman QAP. As a\nby-product, this yields a new well-solvable special case of the weighted\nfeedback arc set problem.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 12:36:46 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["Cela", "Eranda", ""], ["Deineko", "Vladimir G.", ""], ["Woeginger", "Gerhard J.", ""]]}, {"id": "1409.6551", "submitter": "Joachim Spoerhase", "authors": "Markus Chimani and Joachim Spoerhase", "title": "Network Design Problems with Bounded Distances via Shallow-Light Steiner\n  Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a directed graph $G$ with non-correlated edge lengths and costs, the\n\\emph{network design problem with bounded distances} asks for a cost-minimal\nspanning subgraph subject to a length bound for all node pairs. We give a\nbi-criteria $(2+\\varepsilon,O(n^{0.5+\\varepsilon}))$-approximation for this\nproblem. This improves on the currently best known linear approximation bound,\nat the cost of violating the distance bound by a factor of at\nmost~$2+\\varepsilon$.\n  In the course of proving this result, the related problem of \\emph{directed\nshallow-light Steiner trees} arises as a subproblem. In the context of directed\ngraphs, approximations to this problem have been elusive. We present the first\nnon-trivial result by proposing a\n$(1+\\varepsilon,O(|R|^{\\varepsilon}))$-ap\\-proxi\\-ma\\-tion, where $R$ are the\nterminals.\n  Finally, we show how to apply our results to obtain an\n$(\\alpha+\\varepsilon,O(n^{0.5+\\varepsilon}))$-approximation for\n\\emph{light-weight directed $\\alpha$-spanners}. For this, no non-trivial\napproximation algorithm has been known before. All running times depends on $n$\nand $\\varepsilon$ and are polynomial in $n$ for any fixed $\\varepsilon>0$.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 14:20:18 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["Chimani", "Markus", ""], ["Spoerhase", "Joachim", ""]]}, {"id": "1409.6739", "submitter": "Shi Li", "authors": "Shi Li", "title": "On Uniform Capacitated $k$-Median Beyond the Natural LP Relaxation", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the uniform capacitated $k$-median problem. Obtaining\na constant approximation algorithm for this problem is a notorious open\nproblem; most previous works gave constant approximations by either violating\nthe capacity constraints or the cardinality constraint. Notably, all these\nalgorithms are based on the natural LP-relaxation for the problem. The\nLP-relaxation has unbounded integrality gap, even when we are allowed to\nviolate the capacity constraints or the cardinality constraint by a factor of\n$2-\\epsilon$.\n  Our result is an $\\exp(O(1/\\epsilon^2))$-approximation algorithm for the\nproblem that violates the cardinality constraint by a factor of $1+\\epsilon$.\nThis is already beyond the capability of the natural LP relaxation, as it has\nunbounded integrality gap even if we are allowed to open $(2-\\epsilon)k$\nfacilities. Indeed, our result is based on a novel LP for this problem.\n  The version as we described is the hard-capacitated version of the problem,\nas we can only open one facility at each location. This is as opposed to the\nsoft-capacitated version, in which we are allowed to open more than one\nfacilities at each location. We give a simple proof that in the uniform\ncapacitated case, the soft-capacitated version and the hard-capacitated version\nare actually equivalent, up to a small constant loss in the approximation\nratio.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 20:15:41 GMT"}, {"version": "v2", "created": "Fri, 17 Oct 2014 23:13:52 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Li", "Shi", ""]]}, {"id": "1409.6780", "submitter": "Jouni Sir\\'en", "authors": "Travis Gagie, Aleksi Hartikainen, Juha K\\\"arkk\\\"ainen, Gonzalo\n  Navarro, Simon J. Puglisi, Jouni Sir\\'en", "title": "Document Counting in Practice", "comments": "This is a slightly extended version of the paper that was presented\n  at DCC 2015. The implementations are available at\n  http://jltsiren.kapsi.fi/rlcsa and https://github.com/ahartik/succinct", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of counting the number of strings in a collection\nwhere a given pattern appears, which has applications in information retrieval\nand data mining. Existing solutions are in a theoretical stage. We implement\nthese solutions and develop some new variants, comparing them experimentally on\nvarious datasets. Our results not only show which are the best options for each\nsituation and help discard practically unappealing solutions, but also uncover\nsome unexpected compressibility properties of the best data structures. By\ntaking advantage of these properties, we can reduce the size of the structures\nby a factor of 5--400, depending on the dataset.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 00:27:17 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 10:57:53 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Gagie", "Travis", ""], ["Hartikainen", "Aleksi", ""], ["K\u00e4rkk\u00e4inen", "Juha", ""], ["Navarro", "Gonzalo", ""], ["Puglisi", "Simon J.", ""], ["Sir\u00e9n", "Jouni", ""]]}, {"id": "1409.6913", "submitter": "Chandan K. Dubey", "authors": "Chandan Dubey and Thomas Holenstein", "title": "Generating a Quadratic Forms from a Given Genus", "comments": "arXiv admin note: text overlap with arXiv:1409.6199", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.NT math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a non-empty genus in $n$ dimensions with determinant $d$, we give a\nrandomized algorithm that outputs a quadratic form from this genus. The time\ncomplexity of the algorithm is poly$(n,\\log d)$; assuming Generalized Riemann\nHypothesis (GRH).\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 11:52:06 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 09:49:26 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Dubey", "Chandan", ""], ["Holenstein", "Thomas", ""]]}, {"id": "1409.6952", "submitter": "Georgios Stamoulis", "authors": "Edouard Bonnet, Bruno Escoffier, Vangelis Paschos, Georgios Stamoulis", "title": "A 0.821-ratio purely combinatorial algorithm for maximum $k$-vertex\n  cover in bipartite graphs", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal in this paper is to propose a \\textit{combinatorial algorithm} that\nbeats the only such algorithm known previously, the greedy one. We study the\npolynomial approximation of the Maximum Vertex Cover Problem in bipartite\ngraphs by a purely combinatorial algorithm and present a computer assisted\nanalysis of it, that finds the worst case approximation guarantee that is\nbounded below by~0.821.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 13:30:08 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2015 13:42:08 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Bonnet", "Edouard", ""], ["Escoffier", "Bruno", ""], ["Paschos", "Vangelis", ""], ["Stamoulis", "Georgios", ""]]}, {"id": "1409.6967", "submitter": "Karthik Gurumoorthy", "authors": "Amit Dhurandhar and Karthik Gurumoorthy", "title": "Symmetric Submodular Clustering with Actionable Constraint", "comments": "This research work benefited from the support of the AIRBUS Group\n  Corporate Foundation Chair in Mathematics of Complex Systems established in\n  ICTS-TIFR. appears in Discrete Optimization in Machine Learning, A Neural\n  Information Processing Systems (NIPS) Workshop, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering with submodular functions has been of interest over the last few\nyears. Symmetric submodular functions are of particular interest as minimizing\nthem is significantly more efficient and they include many commonly used\nfunctions in practice viz. graph cuts, mutual information. In this paper, we\npropose a novel constraint to make clustering actionable which is motivated by\napplications across multiple domains, and pose the problem of performing\nsymmetric submodular clustering subject to this constraint. We see that\nobtaining a $k$ partition with approximation guarantees is a non-trivial task\nrequiring further work.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 14:20:40 GMT"}, {"version": "v2", "created": "Thu, 20 Nov 2014 07:47:33 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Dhurandhar", "Amit", ""], ["Gurumoorthy", "Karthik", ""]]}, {"id": "1409.7033", "submitter": "Zolt\\'an Kir\\'aly", "authors": "Zolt\\'an Kir\\'aly", "title": "Shortest Paths in Nearly Conservative Digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the following notion: a digraph $D=(V,A)$ with arc weights $c:\nA\\rightarrow \\R$ is called nearly conservative if every negative cycle consists\nof two arcs. Computing shortest paths in nearly conservative digraphs is\nNP-hard, and even deciding whether a digraph is nearly conservative is\ncoNP-complete.\n  We show that the \"All Pairs Shortest Path\" problem is fixed parameter\ntractable with various parameters for nearly conservative digraphs. The results\nalso apply for the special case of conservative mixed graphs.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 18:10:29 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Kir\u00e1ly", "Zolt\u00e1n", ""]]}, {"id": "1409.7217", "submitter": "Szymon Grabowski", "authors": "Szymon Grabowski", "title": "A note on the longest common substring with $k$-mismatches problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced longest common substring with $k$-mismatches\n($k$-LCF) problem is to find, given two sequences $S_1$ and $S_2$ of length $n$\neach, a longest substring $A_1$ of $S_1$ and $A_2$ of $S_2$ such that the\nHamming distance between $A_1$ and $A_2$ is at most $k$. So far, the only\nsubquadratic time result for this problem was known for $k =\n1$~\\cite{FGKU2014}. We first present two output-dependent algorithms solving\nthe $k$-LCF problem and show that for $k = O(\\log^{1-\\varepsilon} n)$, where\n$\\varepsilon > 0$, at least one of them works in subquadratic time, using\n$O(n)$ words of space. The choice of one of these two algorithms to be applied\nfor a given input can be done after linear time and space preprocessing.\nFinally we present a tabulation-based algorithm working, in its range of\napplicability, in $O(n^2\\log\\min(k+\\ell_0, \\sigma)/\\log n)$ time, where\n$\\ell_0$ is the length of the standard longest common substring.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 11:29:18 GMT"}, {"version": "v2", "created": "Tue, 14 Oct 2014 15:15:12 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Grabowski", "Szymon", ""]]}, {"id": "1409.7240", "submitter": "Jakub {\\L}\\k{a}cki", "authors": "Jakub {\\L}\\k{a}cki, Piotr Sankowski", "title": "Optimal decremental connectivity in planar graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show an algorithm for dynamic maintenance of connectivity information in\nan undirected planar graph subject to edge deletions. Our algorithm may answer\nconnectivity queries of the form `Are vertices $u$ and $v$ connected with a\npath?' in constant time. The queries can be intermixed with any sequence of\nedge deletions, and the algorithm handles all updates in $O(n)$ time. This\nresults improves over previously known $O(n \\log n)$ time algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 12:48:56 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["\u0141\u0105cki", "Jakub", ""], ["Sankowski", "Piotr", ""]]}, {"id": "1409.7261", "submitter": "Stefan Kratsch", "authors": "Gregory Gutin and Stefan Kratsch and Magnus Wahlstr\\\"om", "title": "Polynomial Kernels and User Reductions for the Workflow Satisfiability\n  Problem", "comments": "An extended abstract appears in the proceedings of IPEC 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Workflow Satisfiability Problem (WSP) is a problem of practical interest\nthat arises whenever tasks need to be performed by authorized users, subject to\nconstraints defined by business rules. We are required to decide whether there\nexists a plan -- an assignment of tasks to authorized users -- such that all\nconstraints are satisfied.\n  The WSP is, in fact, the conservative Constraint Satisfaction Problem (i.e.,\nfor each variable, here called task, we have a unary authorization constraint)\nand is, thus, NP-complete. It was observed by Wang and Li (2010) that the\nnumber k of tasks is often quite small and so can be used as a parameter, and\nseveral subsequent works have studied the parameterized complexity of WSP\nregarding parameter k.\n  We take a more detailed look at the kernelization complexity of WSP(\\Gamma)\nwhen \\Gamma\\ denotes a finite or infinite set of allowed constraints. Our main\nresult is a dichotomy for the case that all constraints in \\Gamma\\ are regular:\n(1) We are able to reduce the number n of users to n' <= k. This entails a\nkernelization to size poly(k) for finite \\Gamma, and, under mild technical\nconditions, to size poly(k+m) for infinite \\Gamma, where m denotes the number\nof constraints. (2) Already WSP(R) for some R \\in \\Gamma\\ allows no polynomial\nkernelization in k+m unless the polynomial hierarchy collapses.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 14:03:18 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Gutin", "Gregory", ""], ["Kratsch", "Stefan", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "1409.7289", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ognjen Arandjelovic, Ducson Pham, Svetha Venkatesh", "title": "Stream quantiles via maximal entropy histograms", "comments": "appears in International Conference on Neural Information Processing,\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating the running quantile of a data stream\nwhen the memory for storing observations is limited. We (i) highlight the\nlimitations of approaches previously described in the literature which make\nthem unsuitable for non-stationary streams, (ii) describe a novel principle for\nthe utilization of the available storage space, and (iii) introduce two novel\nalgorithms which exploit the proposed principle. Experiments on three large\nreal-world data sets demonstrate that the proposed methods vastly outperform\nthe existing alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 15:20:18 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Arandjelovic", "Ognjen", ""], ["Pham", "Ducson", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1409.7352", "submitter": "Zhengjun Cao", "authors": "Zhengjun Cao and Zhenfu Cao", "title": "On Shor's Factoring Algorithm with More Registers and the Problem to\n  Certify Quantum Computers", "comments": "12 pages. The extended abstract of this paper appeared in Proceeding\n  of 2nd International Symposium on Information Science and Engineering, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shor's factoring algorithm uses two quantum registers. By introducing more\nregisters we show that the measured numbers in these registers which are of the\nsame pre-measurement state, should be equal if the original Shor's complexity\nargument is sound. This contradicts the argument that the second register has\n$r$ possible measured values. There is an anonymous comment which argues that\nthe states in these registers are entangled. If so, the entanglement involving\nmany quantum registers can not be interpreted by the mechanism of EPR pairs and\nthe like. In view of this peculiar entanglement has not yet been mentioned and\ninvestigated, we think the claim that the Shor's algorithm runs in polynomial\ntime needs more physical verifications. We also discuss the problem to certify\nquantum computers.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 00:22:20 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Cao", "Zhengjun", ""], ["Cao", "Zhenfu", ""]]}, {"id": "1409.7458", "submitter": "Jiantao Jiao", "authors": "Jiantao Jiao, Kartik Venkat, Yanjun Han, Tsachy Weissman", "title": "Beyond Maximum Likelihood: from Theory to Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood is the most widely used statistical estimation technique.\nRecent work by the authors introduced a general methodology for the\nconstruction of estimators for functionals in parametric models, and\ndemonstrated improvements - both in theory and in practice - over the maximum\nlikelihood estimator (MLE), particularly in high dimensional scenarios\ninvolving parameter dimension comparable to or larger than the number of\nsamples. This approach to estimation, building on results from approximation\ntheory, is shown to yield minimax rate-optimal estimators for a wide class of\nfunctionals, implementable with modest computational requirements. In a\nnutshell, a message of this recent work is that, for a wide class of\nfunctionals, the performance of these essentially optimal estimators with $n$\nsamples is comparable to that of the MLE with $n \\ln n$ samples.\n  In the present paper, we highlight the applicability of the aforementioned\nmethodology to statistical problems beyond functional estimation, and show that\nit can yield substantial gains. For example, we demonstrate that for learning\ntree-structured graphical models, our approach achieves a significant reduction\nof the required data size compared with the classical Chow--Liu algorithm,\nwhich is an implementation of the MLE, to achieve the same accuracy. The key\nstep in improving the Chow--Liu algorithm is to replace the empirical mutual\ninformation with the estimator for mutual information proposed by the authors.\nFurther, applying the same replacement approach to classical Bayesian network\nclassification, the resulting classifiers uniformly outperform the previous\nclassifiers on 26 widely used datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 01:45:34 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Jiao", "Jiantao", ""], ["Venkat", "Kartik", ""], ["Han", "Yanjun", ""], ["Weissman", "Tsachy", ""]]}, {"id": "1409.7688", "submitter": "Pablo Romero Rodr\\'iguez", "authors": "Eduardo Canale, Pablo Romero, Gerardo Rubino", "title": "Irrelevant Components and Exact Computation of the Diameter Constrained\n  Reliability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be a simple graph with $|V|=n$ nodes and $|E|=m$ links, a\nsubset $K \\subseteq V$ of \\emph{terminals}, a vector $p=(p_1,...,p_m) \\in\n[0,1]^m$ and a positive integer $d$, called \\emph{diameter}. We assume nodes\nare perfect but links fail stochastically and independently, with probabilities\n$q_i=1-p_i$. The \\emph{diameter-constrained reliability} (DCR for short), is\nthe probability that the terminals of the resulting subgraph remain connected\nby paths composed by $d$ links, or less. This number is denoted by\n$R_{K,G}^{d}(p)$. The general computation of the parameter $R_{K,G}^{d}(p)$\nbelongs to the class of $\\mathcal{N}\\mathcal{P}$-Hard problems, since is\nsubsumes the complexity that a random graph is connected.\n  A discussion of the computational complexity for DCR-subproblems is provided\nin terms of the number of terminal nodes $k=|K|$ and diameter $d$. Either when\n$d=1$ or when $d=2$ and $k$ is fixed, the DCR is inside the class $\\mathcal{P}$\nof polynomial-time problems. The DCR turns $\\mathcal{N}\\mathcal{P}$-Hard even\nif $k \\geq 2$ and $d\\geq 3$ are fixed, or in an all-terminal scenario when\n$d=2$. The traditional approach is to design either exponential exact\nalgorithms or efficient solutions for particular graph classes.\n  The contributions of this paper are two-fold. First, a new recursive class of\ngraphs are shown to have efficient DCR computation. Second, we define a\nfactorization method in order to develop an exact DCR computation in general.\nThe approach is inspired in prior works related with the determination of\nirrelevant links and deletion-contraction formula.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 15:30:30 GMT"}, {"version": "v2", "created": "Wed, 1 Oct 2014 10:58:38 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Canale", "Eduardo", ""], ["Romero", "Pablo", ""], ["Rubino", "Gerardo", ""]]}, {"id": "1409.7764", "submitter": "Saad Quader", "authors": "Saad Quader", "title": "A (Somewhat Dated) Comparative Study of Betweenness Centrality\n  Algorithms on GPU", "comments": "This study was done as a class project on the HPC course CSE 5304\n  (Fall 2012) at the University of Connecticut, and hence it does not cover any\n  advances since January 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of computing the Betweenness Centrality (BC) is important in\nanalyzing graphs in many practical applications like social networks,\nbiological networks, transportation networks, electrical circuits, etc. Since\nthis problem is computation intensive, researchers have been developing\nalgorithms using high performance computing resources like supercomputers,\nclusters, and Graphics Processing Units (GPUs). Current GPU algorithms for\ncomputing BC employ Brandes' sequential algorithm with different trade-offs for\nthread scheduling, data structures, and atomic operations. In this paper, we\nstudy three GPU algorithms for computing BC of unweighted, directed, scale-free\nnetworks. We discuss and measure the trade-offs of their design choices about\nbalanced thread scheduling, atomic operations, synchronizations and latency\nhiding. Our program is written in NVIDIA CUDA C and was tested on an NVIDIA\nTesla M2050 GPU.\n", "versions": [{"version": "v1", "created": "Sat, 27 Sep 2014 04:57:30 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Quader", "Saad", ""]]}, {"id": "1409.7771", "submitter": "Chinmoy Dutta", "authors": "Chinmoy Dutta and Gopal Pandurangan and Rajmohan Rajaraman and Zhifeng\n  Sun and Emanuele Viola", "title": "Global Information Sharing under Network Dynamics", "comments": "arXiv admin note: substantial text overlap with arXiv:1112.0384", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to spread $k$ tokens of information to every node on an $n$-node\ndynamic network, the edges of which are changing at each round. This basic {\\em\ngossip problem} can be completed in $O(n + k)$ rounds in any static network,\nand determining its complexity in dynamic networks is central to understanding\nthe algorithmic limits and capabilities of various dynamic network models. Our\nfocus is on token-forwarding algorithms, which do not manipulate tokens in any\nway other than storing, copying and forwarding them.\n  We first consider the {\\em strongly adaptive} adversary model where in each\nround, each node first chooses a token to broadcast to all its neighbors\n(without knowing who they are), and then an adversary chooses an arbitrary\nconnected communication network for that round with the knowledge of the tokens\nchosen by each node. We show that $\\Omega(nk/\\log n + n)$ rounds are needed for\nany randomized (centralized or distributed) token-forwarding algorithm to\ndisseminate the $k$ tokens, thus resolving an open problem raised\nin~\\cite{kuhn+lo:dynamic}. The bound applies to a wide class of initial token\ndistributions, including those in which each token is held by exactly one node\nand {\\em well-mixed} ones in which each node has each token independently with\na constant probability.\n  We also show several upper bounds in varying models.\n", "versions": [{"version": "v1", "created": "Sat, 27 Sep 2014 06:36:19 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Dutta", "Chinmoy", ""], ["Pandurangan", "Gopal", ""], ["Rajaraman", "Rajmohan", ""], ["Sun", "Zhifeng", ""], ["Viola", "Emanuele", ""]]}, {"id": "1409.7852", "submitter": "Sivaram Ambikasaran", "authors": "Sivaram Ambikasaran", "title": "Generalized Rybicki Press algorithm", "comments": "13 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article discusses a more general and numerically stable Rybicki Press\nalgorithm, which enables inverting and computing determinants of covariance\nmatrices, whose elements are sums of exponentials. The algorithm is true in\nexact arithmetic and relies on introducing new variables and corresponding\nequations, thereby converting the matrix into a banded matrix of larger size.\nLinear complexity banded algorithms for solving linear systems and computing\ndeterminants on the larger matrix enable linear complexity algorithms for the\ninitial semi-separable matrix as well. Benchmarks provided illustrate the\nlinear scaling of the algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 27 Sep 2014 22:24:03 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 01:10:27 GMT"}, {"version": "v3", "created": "Wed, 8 Oct 2014 20:31:33 GMT"}, {"version": "v4", "created": "Sat, 2 May 2015 02:35:51 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Ambikasaran", "Sivaram", ""]]}, {"id": "1409.7938", "submitter": "Baharan Mirzasoleiman", "authors": "Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan\n  Vondrak, and Andreas Krause", "title": "Lazier Than Lazy Greedy", "comments": "In Proc. Conference on Artificial Intelligence (AAAI), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to maximize a monotone submodular function faster than the\nwidely used lazy greedy algorithm (also known as accelerated greedy), both in\ntheory and practice? In this paper, we develop the first linear-time algorithm\nfor maximizing a general monotone submodular function subject to a cardinality\nconstraint. We show that our randomized algorithm, STOCHASTIC-GREEDY, can\nachieve a $(1-1/e-\\varepsilon)$ approximation guarantee, in expectation, to the\noptimum solution in time linear in the size of the data and independent of the\ncardinality constraint. We empirically demonstrate the effectiveness of our\nalgorithm on submodular functions arising in data summarization, including\ntraining large-scale kernel methods, exemplar-based clustering, and sensor\nplacement. We observe that STOCHASTIC-GREEDY practically achieves the same\nutility value as lazy greedy but runs much faster. More surprisingly, we\nobserve that in many practical scenarios STOCHASTIC-GREEDY does not evaluate\nthe whole fraction of data points even once and still achieves\nindistinguishable results compared to lazy greedy.\n", "versions": [{"version": "v1", "created": "Sun, 28 Sep 2014 18:06:23 GMT"}, {"version": "v2", "created": "Wed, 26 Nov 2014 08:45:32 GMT"}, {"version": "v3", "created": "Fri, 28 Nov 2014 13:06:54 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Mirzasoleiman", "Baharan", ""], ["Badanidiyuru", "Ashwinkumar", ""], ["Karbasi", "Amin", ""], ["Vondrak", "Jan", ""], ["Krause", "Andreas", ""]]}, {"id": "1409.8063", "submitter": "Noah Stephens-Davidowitz", "authors": "Daniel Dadush, Oded Regev, Noah Stephens-Davidowitz", "title": "On the Closest Vector Problem with a Distance Guarantee", "comments": "An early version of the paper was titled \"On Bounded Distance\n  Decoding and the Closest Vector Problem with Preprocessing\". Conference on\n  Computational Complexity (2014)", "journal-ref": "CCC 2014", "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a substantially more efficient variant, both in terms of running\ntime and size of preprocessing advice, of the algorithm by Liu, Lyubashevsky,\nand Micciancio for solving CVPP (the preprocessing version of the Closest\nVector Problem, CVP) with a distance guarantee. For instance, for any $\\alpha <\n1/2$, our algorithm finds the (unique) closest lattice point for any target\npoint whose distance from the lattice is at most $\\alpha$ times the length of\nthe shortest nonzero lattice vector, requires as preprocessing advice only $N\n\\approx \\widetilde{O}(n \\exp(\\alpha^2 n /(1-2\\alpha)^2))$ vectors, and runs in\ntime $\\widetilde{O}(nN)$.\n  As our second main contribution, we present reductions showing that it\nsuffices to solve CVP, both in its plain and preprocessing versions, when the\ninput target point is within some bounded distance of the lattice. The\nreductions are based on ideas due to Kannan and a recent sparsification\ntechnique due to Dadush and Kun. Combining our reductions with the LLM\nalgorithm gives an approximation factor of $O(n/\\sqrt{\\log n})$ for search\nCVPP, improving on the previous best of $O(n^{1.5})$ due to Lagarias, Lenstra,\nand Schnorr. When combined with our improved algorithm we obtain, somewhat\nsurprisingly, that only O(n) vectors of preprocessing advice are sufficient to\nsolve CVPP with (the only slightly worse) approximation factor of O(n).\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 10:40:28 GMT"}, {"version": "v2", "created": "Fri, 26 Dec 2014 00:30:59 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Dadush", "Daniel", ""], ["Regev", "Oded", ""], ["Stephens-Davidowitz", "Noah", ""]]}, {"id": "1409.8135", "submitter": "Igor Stassiy", "authors": "Igor Stassiy", "title": "A note on the Minimum Norm Point algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a provably more efficient implementation of the Minimum Norm Point\nAlgorithm conceived by Fujishige than the one presented in \\cite{FUJI06}. The\nalgorithm solves the minimization problem for a class of functions known as\nsubmodular. Many important functions, such as minimum cut in the graph, have\nthe so called submodular property \\cite{FUJI82}. It is known that the problem\ncan also be efficiently solved in strongly polynomial time \\cite{IWAT01},\nhowever known theoretical bounds are far from being practical. We present an\nimproved implementation of the algorithm, for which unfortunately no worst case\nbounds are know, but which performs very well in practice. With the\nmodifications presented, the algorithm performs an order of magnitude faster\nfor certain submodular functions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 14:40:26 GMT"}, {"version": "v2", "created": "Wed, 1 Oct 2014 18:09:47 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Stassiy", "Igor", ""]]}, {"id": "1409.8318", "submitter": "Stephan Beyer", "authors": "Stephan Beyer, Markus Chimani", "title": "Strong Steiner Tree Approximations in Practice", "comments": "33 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this experimental study we consider Steiner tree approximations that\nguarantee a constant approximation of ratio smaller than $2$. The considered\ngreedy algorithms and approaches based on linear programming involve the\nincorporation of $k$-restricted full components for some $k \\geq 3$. For most\nof the algorithms, their strongest theoretical approximation bounds are only\nachieved for $k \\to \\infty$. However, the running time is also exponentially\ndependent on $k$, so only small $k$ are tractable in practice.\n  We investigate different implementation aspects and parameter choices that\nfinally allow us to construct algorithms (somewhat) feasible for practical use.\nWe compare the algorithms against each other, to an exact LP-based algorithm,\nand to fast and simple $2$-approximations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 20:41:36 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 17:56:15 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Beyer", "Stephan", ""], ["Chimani", "Markus", ""]]}, {"id": "1409.8389", "submitter": "Feodor Dragan F", "authors": "Feodor F. Dragan and Ekkehard K\\\"ohler and Arne Leitert", "title": "Line-distortion, Bandwidth and Path-length of a graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the minimum line-distortion and the minimum bandwidth problems\non unweighted graphs and their relations with the minimum length of a\nRobertson-Seymour's path-decomposition. The length of a path-decomposition of a\ngraph is the largest diameter of a bag in the decomposition. The path-length of\na graph is the minimum length over all its path-decompositions. In particular,\nwe show:\n  - if a graph $G$ can be embedded into the line with distortion $k$, then $G$\nadmits a Robertson-Seymour's path-decomposition with bags of diameter at most\n$k$ in $G$;\n  - for every class of graphs with path-length bounded by a constant, there\nexist an efficient constant-factor approximation algorithm for the minimum\nline-distortion problem and an efficient constant-factor approximation\nalgorithm for the minimum bandwidth problem;\n  - there is an efficient 2-approximation algorithm for computing the\npath-length of an arbitrary graph;\n  - AT-free graphs and some intersection families of graphs have path-length at\nmost 2;\n  - for AT-free graphs, there exist a linear time 8-approximation algorithm for\nthe minimum line-distortion problem and a linear time 4-approximation algorithm\nfor the minimum bandwidth problem.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 05:31:24 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Dragan", "Feodor F.", ""], ["K\u00f6hler", "Ekkehard", ""], ["Leitert", "Arne", ""]]}, {"id": "1409.8464", "submitter": "Friedrich Slivovsky", "authors": "Friedrich Slivovsky and Stefan Szeider", "title": "Model Counting for Formulas of Bounded Clique-Width", "comments": "Extended version of a paper published at ISAAC 2013", "journal-ref": "Proceedings of ISAAC 2013. Lecture Notes in Computer Science, vol.\n  8283, pp. 677-687, Springer, 2013", "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that #SAT is polynomial-time tractable for classes of CNF formulas\nwhose incidence graphs have bounded symmetric clique-width (or bounded\nclique-width, or bounded rank-width). This result strictly generalizes\npolynomial-time tractability results for classes of formulas with signed\nincidence graphs of bounded clique-width and classes of formulas with incidence\ngraphs of bounded modular treewidth, which were the most general results of\nthis kind known so far.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 10:27:07 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Slivovsky", "Friedrich", ""], ["Szeider", "Stefan", ""]]}, {"id": "1409.8524", "submitter": "Manuel Sorge", "authors": "Yann Disser, Stefan Kratsch, Manuel Sorge", "title": "The Minimum Feasible Tileset problem", "comments": "23 pages, 2 figures. An extended abstract of this article appeared at\n  the 12th Workshop on Approximation and Online Algorithms, Wroclaw, September\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study the Minimum Feasible Tileset problem: Given a set of\nsymbols and subsets of these symbols (scenarios), find a smallest possible\nnumber of pairs of symbols (tiles) such that each scenario can be formed by\nselecting at most one symbol from each tile. We show that this problem is\nAPX-hard and that it is NP-hard even if each scenario contains at most three\nsymbols. Our main result is a 4/3-approximation algorithm for the general case.\nIn addition, we show that the Minimum Feasible Tileset problem is\nfixed-parameter tractable both when parameterized with the number of scenarios\nand with the number of symbols.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 12:51:57 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 16:05:19 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Disser", "Yann", ""], ["Kratsch", "Stefan", ""], ["Sorge", "Manuel", ""]]}, {"id": "1409.8670", "submitter": "David Wajc", "authors": "Joseph (Seffi) Naor, David Wajc", "title": "Near-Optimum Online Ad Allocation for Targeted Advertising", "comments": null, "journal-ref": null, "doi": "10.1145/2764468.2764482", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by Internet targeted advertising, we address several ad allocation\nproblems. Prior work has established these problems admit no randomized online\nalgorithm better than $(1-\\frac{1}{e})$-competitive\n(\\cite{karp1990optimal,mehta2007adwords}), yet simple heuristics have been\nobserved to perform much better in practice. We explain this phenomenon by\nstudying a generalization of the bounded-degree inputs considered by Buchbinder\net al.~\\cite{buchbinder2007online}, graphs which we call $(k,d)-bounded$. In\nsuch graphs the maximal degree on the online side is at most $d$ and the\nminimal degree on the offline side is at least $k$. We prove that for such\ngraphs, these problems' natural greedy algorithms attain competitive ratio\n$1-\\frac{d-1}{k+d-1}$, tending to \\emph{one} as $d/k$ tends to zero. We prove\nthis bound is tight for these algorithms.\n  Next, we develop deterministic primal-dual algorithms for the above problems\nachieving competitive ratio $1-(1-\\frac{1}{d})^k>1-\\frac{1}{e^{k/d}}$, or\n\\emph{exponentially} better loss as a function of $k/d$, and strictly better\nthan $1-\\frac{1}{e}$ whenever $k\\geq d$. We complement our lower bounds with\nmatching upper bounds for the vertex-weighted problem. Finally, we use our\ndeterministic algorithms to prove by dual-fitting that simple randomized\nalgorithms achieve the same bounds in expectation. Our algorithms and analysis\ndiffer from previous ad allocation algorithms, which largely scale bids based\non the spent fraction of their bidder's budget, whereas we scale bids according\nto the number of times the bidder could have spent as much as her current bid.\nOur algorithms differ from previous online primal-dual algorithms, as they do\nnot maintain dual feasibility, but only primal-to-dual ratio, and only attain\ndual feasibility upon termination. We believe our techniques could find\napplications to other well-behaved online packing problems.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 19:49:29 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 18:41:49 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Joseph", "", "", "Seffi"], ["Naor", "", ""], ["Wajc", "David", ""]]}]