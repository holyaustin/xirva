[{"id": "1702.00142", "submitter": "Yitong Yin", "authors": "Weiming Feng, Yuxin Sun, Yitong Yin", "title": "What can be sampled locally?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The local computation of Linial [FOCS'87] and Naor and Stockmeyer [STOC'93]\nconcerns with the question of whether a locally definable distributed computing\nproblem can be solved locally: for a given local CSP whether a CSP solution can\nbe constructed by a distributed algorithm using local information. In this\npaper, we consider the problem of sampling a uniform CSP solution by\ndistributed algorithms, and ask whether a locally definable joint distribution\ncan be sampled from locally. More broadly, we consider sampling from Gibbs\ndistributions induced by weighted local CSPs in the LOCAL model.\n  We give two Markov chain based distributed algorithms which we believe to\nrepresent two fundamental approaches for sampling from Gibbs distributions via\ndistributed algorithms. The first algorithm generically parallelizes the\nsingle-site sequential Markov chain by iteratively updating a random\nindependent set of variables in parallel, and achieves an $O(\\Delta\\log n)$\ntime upper bound in the LOCAL model, where $\\Delta$ is the maximum degree, when\nthe Dobrushin's condition for the Gibbs distribution is satisfied. The second\nalgorithm is a novel parallel Markov chain which proposes to update all\nvariables simultaneously yet still guarantees to converge correctly with no\nbias. It surprisingly parallelizes an intrinsically sequential process:\nstabilizing to a joint distribution with massive local dependencies, and may\nachieve an optimal $O(\\log n)$ time upper bound independent of the maximum\ndegree $\\Delta$ under a stronger mixing condition.\n  We also show a strong $\\Omega(diam)$ lower bound for sampling independent set\nin graphs with maximum degree $\\Delta\\ge 6$. This lower bound holds even when\nevery node is aware of the graph. This gives a strong separation between\nsampling and constructing locally checkable labelings.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 06:10:47 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 06:57:39 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Feng", "Weiming", ""], ["Sun", "Yuxin", ""], ["Yin", "Yitong", ""]]}, {"id": "1702.00196", "submitter": "He Sun", "authors": "Jiecao Chen and He Sun and David P. Woodruff and Qin Zhang", "title": "Communication-Optimal Distributed Clustering", "comments": "A preliminary version of this paper appeared at the 30th Annual\n  Conference on Neural Information Processing Systems (NIPS), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering large datasets is a fundamental problem with a number of\napplications in machine learning. Data is often collected on different sites\nand clustering needs to be performed in a distributed manner with low\ncommunication. We would like the quality of the clustering in the distributed\nsetting to match that in the centralized setting for which all the data resides\non a single site. In this work, we study both graph and geometric clustering\nproblems in two distributed models: (1) a point-to-point model, and (2) a model\nwith a broadcast channel. We give protocols in both models which we show are\nnearly optimal by proving almost matching communication lower bounds. Our work\nhighlights the surprising power of a broadcast channel for clustering problems;\nroughly speaking, to spectrally cluster $n$ points or $n$ vertices in a graph\ndistributed across $s$ servers, for a worst-case partitioning the communication\ncomplexity in a point-to-point model is $n \\cdot s$, while in the broadcast\nmodel it is $n + s$. A similar phenomenon holds for the geometric setting as\nwell. We implement our algorithms and demonstrate this phenomenon on real life\ndatasets, showing that our algorithms are also very efficient in practice.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 10:30:32 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Chen", "Jiecao", ""], ["Sun", "He", ""], ["Woodruff", "David P.", ""], ["Zhang", "Qin", ""]]}, {"id": "1702.00353", "submitter": "Damien Woods", "authors": "Pierre-\\'Etienne Meunier and Damien Woods", "title": "The non-cooperative tile assembly model is not intrinsically universal\n  or capable of bounded Turing machine simulation", "comments": "Extended version of STOC 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of algorithmic self-assembly is concerned with the computational\nand expressive power of nanoscale self-assembling molecular systems. In the\nwell-studied cooperative, or temperature 2, abstract tile assembly model it is\nknown that there is a tile set to simulate any Turing machine and an\nintrinsically universal tile set that simulates the shapes and dynamics of any\ninstance of the model, up to spatial rescaling. It has been an open question as\nto whether the seemingly simpler noncooperative, or temperature 1, model is\ncapable of such behaviour. Here we show that this is not the case, by showing\nthat there is no tile set in the noncooperative model that is intrinsically\nuniversal, nor one capable of time-bounded Turing machine simulation within a\nbounded region of the plane.\n  Although the noncooperative model intuitively seems to lack the complexity\nand power of the cooperative model it was not obvious how to prove this. One\nreason is that there have been few tools to analyse the structure of\ncomplicated paths in the plane. This paper provides a number of such tools. A\nsecond reason is that almost every obvious and small generalisation to the\nmodel (e.g. allowing error, 3D, non-square tiles, signals/wires on tiles, tiles\nthat repel each other, parallel synchronous growth) endows it with great\ncomputational, and sometimes simulation, power. Our main results show that all\nof these generalisations provably increase computational and/or simulation\npower. Our results hold for both deterministic and nondeterministic\nnoncooperative systems. Our first main result stands in stark contrast with the\nfact that for both the cooperative tile assembly model, and for 3D\nnoncooperative tile assembly, there are respective intrinsically universal\ntilesets. Our second main result gives a new technique (reduction to\nsimulation) for proving negative results about computation in tile assembly.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 16:55:41 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 17:13:57 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Meunier", "Pierre-\u00c9tienne", ""], ["Woods", "Damien", ""]]}, {"id": "1702.00458", "submitter": "Qiuyi Zhang", "authors": "Rina Panigrahy, Sushant Sachdeva, Qiuyi Zhang", "title": "Convergence Results for Neural Networks via Electrodynamics", "comments": "in ITCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study whether a depth two neural network can learn another depth two\nnetwork using gradient descent. Assuming a linear output node, we show that the\nquestion of whether gradient descent converges to the target function is\nequivalent to the following question in electrodynamics: Given $k$ fixed\nprotons in $\\mathbb{R}^d,$ and $k$ electrons, each moving due to the attractive\nforce from the protons and repulsive force from the remaining electrons,\nwhether at equilibrium all the electrons will be matched up with the protons,\nup to a permutation. Under the standard electrical force, this follows from the\nclassic Earnshaw's theorem. In our setting, the force is determined by the\nactivation function and the input distribution. Building on this equivalence,\nwe prove the existence of an activation function such that gradient descent\nlearns at least one of the hidden nodes in the target network. Iterating, we\nshow that gradient descent can be used to learn the entire network one node at\na time.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 21:25:13 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 05:44:06 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 20:35:14 GMT"}, {"version": "v4", "created": "Tue, 21 Nov 2017 01:08:49 GMT"}, {"version": "v5", "created": "Tue, 4 Dec 2018 23:04:49 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Panigrahy", "Rina", ""], ["Sachdeva", "Sushant", ""], ["Zhang", "Qiuyi", ""]]}, {"id": "1702.00763", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu", "title": "Natasha: Faster Non-Convex Stochastic Optimization Via Strongly\n  Non-Convex Parameter", "comments": "V2-V5 corrected typos, polished writing, and added citations. (We\n  mis-stated the complexity of the prior work repeatSVRG in V1-V4, and have\n  fixed this mistake in V5.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a nonconvex function that is an average of $n$ smooth functions, we\ndesign stochastic first-order methods to find its approximate stationary\npoints. The convergence of our new methods depends on the smallest (negative)\neigenvalue $-\\sigma$ of the Hessian, a parameter that describes how nonconvex\nthe function is.\n  Our methods outperform known results for a range of parameter $\\sigma$, and\ncan be used to find approximate local minima. Our result implies an interesting\ndichotomy: there exists a threshold $\\sigma_0$ so that the currently fastest\nmethods for $\\sigma>\\sigma_0$ and for $\\sigma<\\sigma_0$ have different\nbehaviors: the former scales with $n^{2/3}$ and the latter scales with\n$n^{3/4}$.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 17:45:09 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 03:50:25 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 09:37:06 GMT"}, {"version": "v4", "created": "Sat, 16 Jun 2018 10:05:32 GMT"}, {"version": "v5", "created": "Thu, 27 Sep 2018 09:55:54 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""]]}, {"id": "1702.00787", "submitter": "Shrisha Rao", "authors": "Ananth Murthy, Chandan Yeshwanth and Shrisha Rao", "title": "Distributed Approximation Algorithms for the Multiple Knapsack Problem", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the distributed version of the Multiple Knapsack Problem (MKP),\nwhere $m$ items are to be distributed amongst $n$ processors, each with a\nknapsack. We propose different distributed approximation algorithms with a\ntradeoff between time and message complexities. The algorithms are based on the\ngreedy approach of assigning the best item to the knapsack with the largest\ncapacity. These algorithms obtain a solution with a bound of $\\frac{1}{n+1}$\ntimes the optimum solution, with either $\\mathcal{O}\\left(m\\log n\\right)$ time\nand $\\mathcal{O}\\left(m n\\right)$ messages, or $\\mathcal{O}\\left(m\\right)$ time\nand $\\mathcal{O}\\left(mn^{2}\\right)$ messages.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 09:55:43 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Murthy", "Ananth", ""], ["Yeshwanth", "Chandan", ""], ["Rao", "Shrisha", ""]]}, {"id": "1702.00817", "submitter": "Renato J Cintra", "authors": "F. M. Bayer, R. J. Cintra", "title": "DCT-like Transform for Image Compression Requires 14 Additions Only", "comments": "6 pages, 3 figures, 1 table", "journal-ref": "Electronics Letters, Volume 48, Issue 15, pp. 919-921 (2012)", "doi": "10.1049/el.2012.1148", "report-no": null, "categories": "cs.MM cs.DS stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A low-complexity 8-point orthogonal approximate DCT is introduced. The\nproposed transform requires no multiplications or bit-shift operations. The\nderived fast algorithm requires only 14 additions, less than any existing DCT\napproximation. Moreover, in several image compression scenarios, the proposed\ntransform could outperform the well-known signed DCT, as well as\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 20:08:42 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""]]}, {"id": "1702.00948", "submitter": "Javlon Isomurodov", "authors": "Javlon E. Isomurodov and Alexander A. Loboda and Alexey A.\n  Sergushichev", "title": "Ranking vertices for active module recovery problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting a connected subnetwork enriched in individually important vertices\nis an approach commonly used in many areas of bioinformatics, including\nanalysis of gene expression data, mutations, metabolomic profiles and others.\nIt can be formulated as a recovery of an active module from which an\nexperimental signal is generated. Commonly, methods for solving this problem\nresult in a single subnetwork that is considered to be a good candidate.\nHowever, it is usually useful to consider not one but multiple candidate\nmodules at different significance threshold levels. Therefore, in this paper we\nsuggest to consider a problem of finding a vertex ranking instead of finding a\nsingle module. We also propose two algorithms for solving this problem: one\nthat we consider to be optimal but computationally expensive for real-world\nnetworks and one that works close to the optimal in practice and is also able\nto work with big networks.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 09:55:07 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Isomurodov", "Javlon E.", ""], ["Loboda", "Alexander A.", ""], ["Sergushichev", "Alexey A.", ""]]}, {"id": "1702.01083", "submitter": "Fatemeh Rajabi-Alni", "authors": "Fatemeh Rajabi-Alni, Alireza Bagheri", "title": "An O(n^2) algorithm for Many-To-Many Matching of Points with Demands in\n  One Dimension", "comments": "14 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:1210.8123", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two point sets S and T, we study the many-to-many matching with demands\nproblem (MMD problem) which is a generalization of the many-to-many matching\nproblem (MM problem). In an MMD, each point of one set must be matched to a\ngiven number of the points of the other set (each point has a demand). In this\npaper we consider a special case of MMD problem, the one-dimensional MMD\n(OMMD), where the input point sets S and T lie on the line. That is, the cost\nof matching a pair of points is equal to the distance between the two points.\nwe present the first O(n^2)time algorithm for computing an OMMD between S and\nT, where |S| + |T| = n.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 17:20:21 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 11:28:10 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 15:03:59 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Rajabi-Alni", "Fatemeh", ""], ["Bagheri", "Alireza", ""]]}, {"id": "1702.01136", "submitter": "Gramoz Goranci", "authors": "Gramoz Goranci, Monika Henzinger, Pan Peng", "title": "Improved Guarantees for Vertex Sparsification in Planar Graphs", "comments": "Extended abstract appeared in proceedings of ESA 2017", "journal-ref": null, "doi": "10.4230/LIPIcs.ESA.2017.44", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Sparsification aims at compressing large graphs into smaller ones while\npreserving important characteristics of the input graph. In this work we study\nVertex Sparsifiers, i.e., sparsifiers whose goal is to reduce the number of\nvertices. We focus on the following notions:\n  (1) Given a digraph $G=(V,E)$ and terminal vertices $K \\subset V$ with $|K| =\nk$, a (vertex) reachability sparsifier of $G$ is a digraph $H=(V_H,E_H)$, $K\n\\subset V_H$ that preserves all reachability information among terminal pairs.\nIn this work we introduce the notion of reachability-preserving minors (RPMs) ,\ni.e., we require $H$ to be a minor of $G$. We show any directed graph $G$\nadmits a RPM $H$ of size $O(k^3)$, and if $G$ is planar, then the size of $H$\nimproves to $O(k^{2} \\log k)$. We complement our upper-bound by showing that\nthere exists an infinite family of grids such that any RPM must have\n$\\Omega(k^{2})$ vertices.\n  (2) Given a weighted undirected graph $G=(V,E)$ and terminal vertices $K$\nwith $|K|=k$, an exact (vertex) cut sparsifier of $G$ is a graph $H$ with $K\n\\subset V_H$ that preserves the value of minimum-cuts separating any\nbipartition of $K$. We show that planar graphs with all the $k$ terminals lying\non the same face admit exact cut sparsifiers of size $O(k^{2})$ that are also\nplanar. Our result extends to flow and distance sparsifiers. It improves the\nprevious best-known bound of $O(k^22^{2k})$ for cut and flow sparsifiers by an\nexponential factor, and matches an $\\Omega(k^2)$ lower-bound for this class of\ngraphs.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 19:35:00 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 15:46:50 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Goranci", "Gramoz", ""], ["Henzinger", "Monika", ""], ["Peng", "Pan", ""]]}, {"id": "1702.01280", "submitter": "Travis Gagie", "authors": "Lu\\'is Cunha, Simone Dantas, Travis Gagie, Roland Wittler, Luis Kowada\n  and Jens Stoye", "title": "Fast and Simple Jumbled Indexing for Binary RLE Strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important papers have appeared recently on the problem of indexing binary\nstrings for jumbled pattern matching, and further lowering the time bounds in\nterms of the input size would now be a breakthrough with broad implications. We\ncan still make progress on the problem, however, by considering other natural\nparameters. Badkobeh et al.\\ (IPL, 2013) and Amir et al.\\ (TCS, 2016) gave\nalgorithms that index a binary string in $O (n + \\rho^2 \\log \\rho)$ time, where\n$n$ is the length and $\\rho$ is the number of runs, and Giaquinta and Grabowski\n(IPL, 2013) gave one that runs in $O (n + \\rho^2)$ time. In this paper we\npropose a new and very simple algorithm that also runs in $O(n + \\rho^2)$ time\nand can be extended either so that the index returns the position of a match\n(if there is one), or so that the algorithm uses only $O (n)$ bits of space.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 12:15:06 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 12:24:33 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Cunha", "Lu\u00eds", ""], ["Dantas", "Simone", ""], ["Gagie", "Travis", ""], ["Wittler", "Roland", ""], ["Kowada", "Luis", ""], ["Stoye", "Jens", ""]]}, {"id": "1702.01284", "submitter": "Otmar Ertl", "authors": "Otmar Ertl", "title": "New cardinality estimation algorithms for HyperLogLog sketches", "comments": "Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new methods to estimate the cardinalities of data sets\nrecorded by HyperLogLog sketches. A theoretically motivated extension to the\noriginal estimator is presented that eliminates the bias for small and large\ncardinalities. Based on the maximum likelihood principle a second unbiased\nmethod is derived together with a robust and efficient numerical algorithm to\ncalculate the estimate. The maximum likelihood approach can also be applied to\nmore than a single HyperLogLog sketch. In particular, it is shown that it gives\nmore precise cardinality estimates for union, intersection, or relative\ncomplements of two sets that are both represented by HyperLogLog sketches\ncompared to the conventional technique using the inclusion-exclusion principle.\nAll the new methods are demonstrated and verified by extensive simulations.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 13:25:05 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 20:30:22 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Ertl", "Otmar", ""]]}, {"id": "1702.01286", "submitter": "Michael Kapralov", "authors": "Volkan Cevher and Michael Kapralov and Jonathan Scarlett and Amir\n  Zandieh", "title": "An Adaptive Sublinear-Time Block Sparse Fourier Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of approximately computing the $k$ dominant Fourier coefficients\nof a vector $X$ quickly, and using few samples in time domain, is known as the\nSparse Fourier Transform (sparse FFT) problem. A long line of work on the\nsparse FFT has resulted in algorithms with $O(k\\log n\\log (n/k))$ runtime\n[Hassanieh et al., STOC'12] and $O(k\\log n)$ sample complexity [Indyk et al.,\nFOCS'14]. These results are proved using non-adaptive algorithms, and the\nlatter $O(k\\log n)$ sample complexity result is essentially the best possible\nunder the sparsity assumption alone.\n  This paper revisits the sparse FFT problem with the added twist that the\nsparse coefficients approximately obey a $(k_0,k_1)$-block sparse model. In\nthis model, signal frequencies are clustered in $k_0$ intervals with width\n$k_1$ in Fourier space, where $k= k_0k_1$ is the total sparsity. Signals\narising in applications are often well approximated by this model with $k_0\\ll\nk$.\n  Our main result is the first sparse FFT algorithm for $(k_0, k_1)$-block\nsparse signals with the sample complexity of $O^*(k_0k_1 + k_0\\log(1+ k_0)\\log\nn)$ at constant signal-to-noise ratios, and sublinear runtime. A similar sample\ncomplexity was previously achieved in the works on model-based compressive\nsensing using random Gaussian measurements, but used $\\Omega(n)$ runtime. To\nthe best of our knowledge, our result is the first sublinear-time algorithm for\nmodel based compressed sensing, and the first sparse FFT result that goes below\nthe $O(k\\log n)$ sample complexity bound.\n  Our algorithm crucially uses {\\em adaptivity} to achieve the improved sample\ncomplexity bound, and we prove that adaptivity is in fact necessary if Fourier\nmeasurements are used: Any non-adaptive algorithm must use $\\Omega(k_0k_1\\log\n\\frac{n}{k_0k_1})$ samples for the $(k_0,k_1$)-block sparse model, ruling out\nimprovements over the vanilla sparsity assumption.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 13:31:47 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 10:53:46 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Cevher", "Volkan", ""], ["Kapralov", "Michael", ""], ["Scarlett", "Jonathan", ""], ["Zandieh", "Amir", ""]]}, {"id": "1702.01290", "submitter": "Bojana Kodric", "authors": "Martin Hoefer and Bojana Kodric", "title": "Combinatorial Secretary Problems with Ordinal Information", "comments": "Full version of ICALP 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The secretary problem is a classic model for online decision making.\nRecently, combinatorial extensions such as matroid or matching secretary\nproblems have become an important tool to study algorithmic problems in dynamic\nmarkets. Here the decision maker must know the numerical value of each arriving\nelement, which can be a demanding informational assumption. In this paper, we\ninitiate the study of combinatorial secretary problems with ordinal\ninformation, in which the decision maker only needs to be aware of a preference\norder consistent with the values of arrived elements. The goal is to design\nonline algorithms with small competitive ratios.\n  For a variety of combinatorial problems, such as bipartite matching, general\npacking LPs, and independent set with bounded local independence number, we\ndesign new algorithms that obtain constant competitive ratios.\n  For the matroid secretary problem, we observe that many existing algorithms\nfor special matroid structures maintain their competitive ratios even in the\nordinal model. In these cases, the restriction to ordinal information does not\nrepresent any additional obstacle. Moreover, we show that ordinal variants of\nthe submodular matroid secretary problems can be solved using algorithms for\nthe linear versions by extending [Feldman and Zenklusen, 2015]. In contrast, we\nprovide a lower bound of $\\Omega(\\sqrt{n}/(\\log n))$ for algorithms that are\noblivious to the matroid structure, where $n$ is the total number of elements.\nThis contrasts an upper bound of $O(\\log n)$ in the cardinal model, and it\nshows that the technique of thresholding is not sufficient for good algorithms\nin the ordinal model.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 14:09:46 GMT"}, {"version": "v2", "created": "Sat, 29 Apr 2017 18:58:25 GMT"}, {"version": "v3", "created": "Fri, 25 Aug 2017 13:53:28 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Hoefer", "Martin", ""], ["Kodric", "Bojana", ""]]}, {"id": "1702.01340", "submitter": "Nicola Prezza", "authors": "Alberto Policriti, Nicola Prezza", "title": "From LZ77 to the Run-Length Encoded Burrows-Wheeler Transform, and Back", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lempel-Ziv factorization (LZ77) and the Run-Length encoded\nBurrows-Wheeler Transform (RLBWT) are two important tools in text compression\nand indexing, being their sizes $z$ and $r$ closely related to the amount of\ntext self-repetitiveness. In this paper we consider the problem of converting\nthe two representations into each other within a working space proportional to\nthe input and the output. Let $n$ be the text length. We show that $RLBWT$ can\nbe converted to $LZ77$ in $\\mathcal{O}(n\\log r)$ time and $\\mathcal{O}(r)$\nwords of working space. Conversely, we provide an algorithm to convert $LZ77$\nto $RLBWT$ in $\\mathcal{O}\\big(n(\\log r + \\log z)\\big)$ time and\n$\\mathcal{O}(r+z)$ words of working space. Note that $r$ and $z$ can be\n\\emph{constant} if the text is highly repetitive, and our algorithms can\noperate with (up to) \\emph{exponentially} less space than naive solutions based\non full decompression.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 20:19:26 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Policriti", "Alberto", ""], ["Prezza", "Nicola", ""]]}, {"id": "1702.01446", "submitter": "Nirman Kumar", "authors": "Pankaj K. Agarwal and Nirman Kumar and Stavros Sintos and Subhash Suri", "title": "Efficient Algorithms for k-Regret Minimizing Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A regret minimizing set Q is a small size representation of a much larger\ndatabase P so that user queries executed on Q return answers whose scores are\nnot much worse than those on the full dataset. In particular, a k-regret\nminimizing set has the property that the regret ratio between the score of the\ntop-1 item in Q and the score of the top-k item in P is minimized, where the\nscore of an item is the inner product of the item's attributes with a user's\nweight (preference) vector. The problem is challenging because we want to find\na single representative set Q whose regret ratio is small with respect to all\npossible user weight vectors.\n  We show that k-regret minimization is NP-Complete for all dimensions d >= 3.\nThis settles an open problem from Chester et al. [VLDB 2014], and resolves the\ncomplexity status of the problem for all d: the problem is known to have\npolynomial-time solution for d <= 2. In addition, we propose two new\napproximation schemes for regret minimization, both with provable guarantees,\none based on coresets and another based on hitting sets. We also carry out\nextensive experimental evaluation, and show that our schemes compute\nregret-minimizing sets comparable in size to the greedy algorithm proposed in\n[VLDB 14] but our schemes are significantly faster and scalable to large data\nsets.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2017 19:30:44 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 01:46:20 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Agarwal", "Pankaj K.", ""], ["Kumar", "Nirman", ""], ["Sintos", "Stavros", ""], ["Suri", "Subhash", ""]]}, {"id": "1702.01586", "submitter": "Yanhao Wang", "authors": "Yanhao Wang and Qi Fan and Yuchen Li and Kian-Lee Tan", "title": "Real-Time Influence Maximization on Dynamic Social Streams", "comments": "An extended version of VLDB 2017 paper \"Real-Time Influence\n  Maximization on Dynamic Social Streams\", 14 pages", "journal-ref": "Proc. VLDB Endow., Vol. 10, No. 7, 2017, Pages 805-816", "doi": "10.14778/3067421.3067429", "report-no": null, "categories": "cs.SI cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Influence maximization (IM), which selects a set of $k$ users (called seeds)\nto maximize the influence spread over a social network, is a fundamental\nproblem in a wide range of applications such as viral marketing and network\nmonitoring. Existing IM solutions fail to consider the highly dynamic nature of\nsocial influence, which results in either poor seed qualities or long\nprocessing time when the network evolves. To address this problem, we define a\nnovel IM query named Stream Influence Maximization (SIM) on social streams.\nTechnically, SIM adopts the sliding window model and maintains a set of $k$\nseeds with the largest influence value over the most recent social actions.\nNext, we propose the Influential Checkpoints (IC) framework to facilitate\ncontinuous SIM query processing. The IC framework creates a checkpoint for each\nwindow slide and ensures an $\\varepsilon$-approximate solution. To improve its\nefficiency, we further devise a Sparse Influential Checkpoints (SIC) framework\nwhich selectively keeps $O(\\frac{\\log{N}}{\\beta})$ checkpoints for a sliding\nwindow of size $N$ and maintains an\n$\\frac{\\varepsilon(1-\\beta)}{2}$-approximate solution. Experimental results on\nboth real-world and synthetic datasets confirm the effectiveness and efficiency\nof our proposed frameworks against the state-of-the-art IM approaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 12:15:22 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Wang", "Yanhao", ""], ["Fan", "Qi", ""], ["Li", "Yuchen", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "1702.01677", "submitter": "Dennis Kraft", "authors": "Susanne Albers, Dennis Kraft", "title": "On the Value of Penalties in Time-Inconsistent Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People tend to behave inconsistently over time due to an inherent present\nbias. As this may impair performance, social and economic settings need to be\nadapted accordingly. Common tools to reduce the impact of time-inconsistent\nbehavior are penalties and prohibition. Such tools are called commitment\ndevices. In recent work Kleinberg and Oren connect the design of\nprohibition-based commitment devices to a combinatorial problem in which edges\nare removed from a task graph $G$ with $n$ nodes. However, this problem is\nNP-hard to approximate within a ratio less than $\\sqrt{n}/3$. To address this\nissue, we propose a penalty-based commitment device that does not delete edges\nbut raises their cost. The benefits of our approach are twofold. On the\nconceptual side, we show that penalties are up to $1/\\beta$ times more\nefficient than prohibition, where $\\beta \\in (0,1]$ parameterizes the present\nbias. On the computational side, we significantly improve approximability by\npresenting a $2$-approximation algorithm for allocating the penalties. To\ncomplement this result, we prove that optimal penalties are NP-hard to\napproximate within a ratio of $1.08192$.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 16:09:19 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Albers", "Susanne", ""], ["Kraft", "Dennis", ""]]}, {"id": "1702.01692", "submitter": "Christian Schulz", "authors": "Peter Sanders, Christian Schulz, Darren Strash and Robert Williger", "title": "Distributed Evolutionary k-way Node Separators", "comments": "arXiv admin note: text overlap with arXiv:1509.01190, arXiv:1110.0477", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing high quality node separators in large graphs is necessary for a\nvariety of applications, ranging from divide-and-conquer algorithms to VLSI\ndesign. In this work, we present a novel distributed evolutionary algorithm\ntackling the k-way node separator problem. A key component of our contribution\nincludes new k-way local search algorithms based on maximum flows. We combine\nour local search with a multilevel approach to compute an initial population\nfor our evolutionary algorithm, and further show how to modify the coarsening\nstage of our multilevel algorithm to create effective combine and mutation\noperations. Lastly, we combine these techniques with a scalable communication\nprotocol, producing a system that is able to compute high quality solutions in\na short amount of time. Our experiments against competing algorithms show that\nour advanced evolutionary algorithm computes the best result on 94% of the\nchosen benchmark instances.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 16:34:27 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Sanders", "Peter", ""], ["Schulz", "Christian", ""], ["Strash", "Darren", ""], ["Williger", "Robert", ""]]}, {"id": "1702.01703", "submitter": "Jens Quedenfeld", "authors": "Jens Quedenfeld and Sven Rahmann", "title": "Variant tolerant read mapping using min-hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNA read mapping is a ubiquitous task in bioinformatics, and many tools have\nbeen developed to solve the read mapping problem. However, there are two trends\nthat are changing the landscape of readmapping: First, new sequencing\ntechnologies provide very long reads with high error rates (up to 15%). Second,\nmany genetic variants in the population are known, so the reference genome is\nnot considered as a single string over ACGT, but as a complex object containing\nthese variants. Most existing read mappers do not handle these new\ncircumstances appropriately.\n  We introduce a new read mapper prototype called VATRAM that considers\nvariants. It is based on Min-Hashing of q-gram sets of reference genome\nwindows. Min-Hashing is one form of locality sensitive hashing. The variants\nare directly inserted into VATRAMs index which leads to a fast mapping process.\nOur results show that VATRAM achieves better precision and recall than\nstate-of-the-art read mappers like BWA under certain cirumstances. VATRAM is\nopen source and can be accessed at\nhttps://bitbucket.org/Quedenfeld/vatram-src/.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 16:52:05 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 10:41:23 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Quedenfeld", "Jens", ""], ["Rahmann", "Sven", ""]]}, {"id": "1702.01719", "submitter": "Therese Biedl", "authors": "Therese Biedl and Philippe Demontigny", "title": "A 2-Approximation for the Height of Maximal Outerplanar Graph Drawings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study planar drawings of maximal outerplanar graphs with\nthe objective of achieving small height. A recent paper gave an algorithm for\nsuch drawings that is within a factor of 4 of the optimum height. In this\npaper, we substantially improve the approximation factor to become 2. The main\ningredient is to define a new parameter of outerplanar graphs (the so-called\numbrella depth, obtained by recursively splitting the graph into graphs called\numbrellas). We argue that the height of any poly-line drawing must be at least\nthe umbrella depth, and then devise an algorithm that achieves height at most\ntwice the umbrella depth.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 17:38:26 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Biedl", "Therese", ""], ["Demontigny", "Philippe", ""]]}, {"id": "1702.01805", "submitter": "Renato J Cintra", "authors": "F. M. Bayer, R. J. Cintra, A. Edirisuriya, A. Madanayake", "title": "A Digital Hardware Fast Algorithm and FPGA-based Prototype for a Novel\n  16-point Approximate DCT for Image Compression Applications", "comments": "17 pages, 6 figures, 6 tables", "journal-ref": "Measurement Science and Technology, Volume 23, Number 11, 2012", "doi": "10.1088/0957-0233/23/11/114010", "report-no": null, "categories": "cs.MM cs.AR cs.DS cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrete cosine transform (DCT) is the key step in many image and video\ncoding standards. The 8-point DCT is an important special case, possessing\nseveral low-complexity approximations widely investigated. However, 16-point\nDCT transform has energy compaction advantages. In this sense, this paper\npresents a new 16-point DCT approximation with null multiplicative complexity.\nThe proposed transform matrix is orthogonal and contains only zeros and ones.\nThe proposed transform outperforms the well-know Walsh-Hadamard transform and\nthe current state-of-the-art 16-point approximation. A fast algorithm for the\nproposed transform is also introduced. This fast algorithm is experimentally\nvalidated using hardware implementations that are physically realized and\nverified on a 40 nm CMOS Xilinx Virtex-6 XC6VLX240T FPGA chip for a maximum\nclock rate of 342 MHz. Rapid prototypes on FPGA for 8-bit input word size shows\nsignificant improvement in compressed image quality by up to 1-2 dB at the cost\nof only eight adders compared to the state-of-art 16-point DCT approximation\nalgorithm in the literature [S. Bouguezel, M. O. Ahmad, and M. N. S. Swamy. A\nnovel transform for image compression. In {\\em Proceedings of the 53rd IEEE\nInternational Midwest Symposium on Circuits and Systems (MWSCAS)}, 2010].\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 22:00:34 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""], ["Edirisuriya", "A.", ""], ["Madanayake", "A.", ""]]}, {"id": "1702.01877", "submitter": "Guohui Lin", "authors": "Yao Xu, Yong Chen, Taibo Luo and Guohui Lin", "title": "A local search 2.917-approximation algorithm for duo-preservation string\n  mapping", "comments": "37 pages, 33 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the {\\em maximum duo-preservation string mapping} ({\\sc Max-Duo})\nproblem, which is the complement of the well studied {\\em minimum common string\npartition} ({\\sc MCSP}) problem. Both problems have applications in many fields\nincluding text compression and bioinformatics. Motivated by an earlier local\nsearch algorithm, we present an improved approximation and show that its\nperformance ratio is no greater than ${35}/{12} < 2.917$. This beats the\ncurrent best $3.25$-approximation for {\\sc Max-Duo}. The performance analysis\nof our algorithm is done through a complex yet interesting amortization. Two\nlower bounds on the locality gap of our algorithm are also provided.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 04:40:50 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Xu", "Yao", ""], ["Chen", "Yong", ""], ["Luo", "Taibo", ""], ["Lin", "Guohui", ""]]}, {"id": "1702.02133", "submitter": "Lalla Mouatadid", "authors": "Pierre Charbit, Michel Habib, Lalla Mouatadid, Reza Naserasr", "title": "A New Graph Parameter To Measure Linearity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When applying graph searching algorithms, such as LexBFS, to order the\nvertices of a graph, one must occasionally break ties. Different tie-breaking\nrules have shed light on different structural properties of graphs. A rule in\nparticular that's proven powerful is the $^+$ rule, which builds a sequence\n$\\sigma_1, \\sigma_2, \\ldots$ of vertex orderings, where each ordering\n$\\sigma_i$ is used to break ties for $\\sigma_{i+1}$. As the total number of\nvertex orderings of a finite graph is finite, this sequence must end up in a\ncycle of vertex orderings. The possible length of this is the main subject of\nthis work. Intuitively speaking, we prove for graphs with a known notion of\nlinearity (e.g., interval graphs with their interval representation on the real\nline), this cycle cannot be too big, no matter which vertex ordering we start\nwith.\n  More precisely, Dusart and Habib conjectured that for a cocomparability graph\nthe size of such a cycle will always be 2, independent of the starting order.\nStacho then asked whether for an arbitrary graph, the size of such a cycle can\nbe bounded by the asteroidal number of the graph. In this work while we answer\nthe latter question negatively, we provide support for the former conjecture by\nproving it for the subclass of cocomparability graphs with no induced dominos.\nThis subclass contains cographs, proper interval graphs, interval graphs and\ncobipartite graphs. We provide simpler independent proofs for each of these\ncases which lead to stronger results on these subclasses. Finally we prove that\nthe same holds for trees.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 18:46:02 GMT"}, {"version": "v2", "created": "Sun, 16 Jul 2017 03:47:09 GMT"}, {"version": "v3", "created": "Thu, 20 Jul 2017 01:47:10 GMT"}, {"version": "v4", "created": "Sat, 9 Jan 2021 23:22:32 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Charbit", "Pierre", ""], ["Habib", "Michel", ""], ["Mouatadid", "Lalla", ""], ["Naserasr", "Reza", ""]]}, {"id": "1702.02267", "submitter": "Quan Li", "authors": "David Gamarnik, Quan Li and Hongyi Zhang", "title": "Matrix Completion from $O(n)$ Samples in Linear Time", "comments": "45 pages, 1 figure. Short version accepted for presentation at\n  Conference on Learning Theory (COLT) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reconstructing a rank-$k$ $n \\times n$ matrix $M$\nfrom a sampling of its entries. Under a certain incoherence assumption on $M$\nand for the case when both the rank and the condition number of $M$ are\nbounded, it was shown in \\cite{CandesRecht2009, CandesTao2010, keshavan2010,\nRecht2011, Jain2012, Hardt2014} that $M$ can be recovered exactly or\napproximately (depending on some trade-off between accuracy and computational\ncomplexity) using $O(n \\, \\text{poly}(\\log n))$ samples in super-linear time\n$O(n^{a} \\, \\text{poly}(\\log n))$ for some constant $a \\geq 1$.\n  In this paper, we propose a new matrix completion algorithm using a novel\nsampling scheme based on a union of independent sparse random regular bipartite\ngraphs. We show that under the same conditions w.h.p. our algorithm recovers an\n$\\epsilon$-approximation of $M$ in terms of the Frobenius norm using $O(n\n\\log^2(1/\\epsilon))$ samples and in linear time $O(n \\log^2(1/\\epsilon))$. This\nprovides the best known bounds both on the sample complexity and computational\ncomplexity for reconstructing (approximately) an unknown low-rank matrix.\n  The novelty of our algorithm is two new steps of thresholding singular values\nand rescaling singular vectors in the application of the \"vanilla\" alternating\nminimization algorithm. The structure of sparse random regular graphs is used\nheavily for controlling the impact of these regularization steps.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 03:52:40 GMT"}, {"version": "v2", "created": "Sat, 3 Jun 2017 21:59:54 GMT"}, {"version": "v3", "created": "Tue, 6 Jun 2017 04:14:45 GMT"}, {"version": "v4", "created": "Tue, 22 Aug 2017 04:05:36 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Gamarnik", "David", ""], ["Li", "Quan", ""], ["Zhang", "Hongyi", ""]]}, {"id": "1702.02279", "submitter": "Ahmed El Alaoui", "authors": "Ahmed El Alaoui, Aaditya Ramdas, Florent Krzakala, Lenka Zdeborova,\n  Michael I. Jordan", "title": "Decoding from Pooled Data: Phase Transitions of Message Passing", "comments": null, "journal-ref": "IEEE Transactions on Information Theory (Volume: 65 , Issue: 1 ,\n  Jan. 2019)", "doi": "10.1109/TIT.2018.2855698", "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of decoding a discrete signal of categorical\nvariables from the observation of several histograms of pooled subsets of it.\nWe present an Approximate Message Passing (AMP) algorithm for recovering the\nsignal in the random dense setting where each observed histogram involves a\nrandom subset of entries of size proportional to n. We characterize the\nperformance of the algorithm in the asymptotic regime where the number of\nobservations $m$ tends to infinity proportionally to n, by deriving the\ncorresponding State Evolution (SE) equations and studying their dynamics. We\ninitiate the analysis of the multi-dimensional SE dynamics by proving their\nconvergence to a fixed point, along with some further properties of the\niterates. The analysis reveals sharp phase transition phenomena where the\nbehavior of AMP changes from exact recovery to weak correlation with the signal\nas m/n crosses a threshold. We derive formulae for the threshold in some\nspecial cases and show that they accurately match experimental behavior.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 04:28:05 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Alaoui", "Ahmed El", ""], ["Ramdas", "Aaditya", ""], ["Krzakala", "Florent", ""], ["Zdeborova", "Lenka", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1702.02321", "submitter": "Diptarama", "authors": "Diptarama and Takashi Katsura and Yuhei Otomo and Kazuyuki Narisawa\n  and Ayumi Shinohara", "title": "Position Heaps for Parameterized Strings", "comments": "14 pages, 4 figures, accepted to CPM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new indexing structure for parameterized strings, called\nparameterized position heap. Parameterized position heap is applicable for\nparameterized pattern matching problem, where the pattern matches a substring\nof the text if there exists a bijective mapping from the symbols of the pattern\nto the symbols of the substring. We propose an online construction algorithm of\nparameterized position heap of a text and show that our algorithm runs in\nlinear time with respect to the text size. We also show that by using\nparameterized position heap, we can find all occurrences of a pattern in the\ntext in linear time with respect to the product of the pattern size and the\nalphabet size.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 08:28:46 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 01:39:24 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Diptarama", "", ""], ["Katsura", "Takashi", ""], ["Otomo", "Yuhei", ""], ["Narisawa", "Kazuyuki", ""], ["Shinohara", "Ayumi", ""]]}, {"id": "1702.02405", "submitter": "Pawel Gawrychowski", "authors": "Bart{\\l}omiej Dudek, Pawe{\\l} Gawrychowski, Piotr Ostropolski-Nalewaja", "title": "A Family of Approximation Algorithms for the Maximum Duo-Preservation\n  String Mapping Problem", "comments": "to appear in CPM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Maximum Duo-Preservation String Mapping problem we are given two\nstrings and wish to map the letters of the former to the letters of the latter\nso as to maximise the number of duos. A duo is a pair of consecutive letters\nthat is mapped to a pair of consecutive letters in the same order. This is\ncomplementary to the well-studied Minimum Common String Partition problem,\nwhere the goal is to partition the former string into blocks that can be\npermuted and concatenated to obtain the latter string.\n  Maximum Duo-Preservation String Mapping is APX-hard. After a series of\nimprovements, Brubach [WABI 2016] showed a polynomial-time $3.25$-approximation\nalgorithm. Our main contribution is that for any $\\epsilon>0$ there exists a\npolynomial-time $(2+\\epsilon)$-approximation algorithm. Similarly to a previous\nsolution by Boria et al. [CPM 2016], our algorithm uses the local search\ntechnique. However, this is used only after a certain preliminary greedy\nprocedure, which gives us more structure and makes a more general local search\npossible. We complement this with a specialised version of the algorithm that\nachieves $2.67$-approximation in quadratic time.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 12:46:12 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 04:04:25 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Dudek", "Bart\u0142omiej", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Ostropolski-Nalewaja", "Piotr", ""]]}, {"id": "1702.02455", "submitter": "William Moses Jr.", "authors": "William K. Moses Jr. and Shailesh Vaya", "title": "Deterministic Protocols in the SINR Model without Knowledge of\n  Coordinates", "comments": "This is the author version of the paper which will appear in the\n  Journal of Computer and System Sciences. 36 pages, 1 table, 4 figures; v3\n  improves the presentation, style, and some technical matter of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much work has been developed for studying the classical broadcasting problem\nin the SINR (Signal-to-Interference-plus-Noise-Ratio) model for wireless device\ntransmission. The setting typically studied is when all radio nodes transmit a\nsignal of the same strength. This work studies the challenging problem of\ndevising a distributed algorithm for multi-broadcasting, assuming a subset of\nnodes are initially awake, for the SINR model when each device only has access\nto knowledge about the total number of nodes in the network $n$, the range from\nwhich each node's label is taken $\\lbrace 1,\\dots,N \\rbrace$, and the label of\nthe device itself. Specifically, we assume no knowledge of the physical\ncoordinates of devices and also no knowledge of the neighborhood of each node.\n  We present a deterministic protocol for this problem in $O(n \\lg N \\lg n)$\nrounds. There is no known polynomial time deterministic algorithm in literature\nfor this setting, and it remains the principle open problem in this domain. A\nlower bound of $\\Omega(n \\lg N)$ rounds is known for deterministic broadcasting\nwithout local knowledge.\n  In addition to the above result, we present algorithms to achieve\nmulti-broadcast in $O(n \\lg N)$ rounds and create a backbone in $O(n \\lg N)$\nrounds, assuming that all nodes are initially awake. For a given backbone,\nmessages can be exchanged between every pair of connected nodes in the backbone\nin $O(\\lg N)$ rounds and between any node and its designated contact node in\nthe backbone in $O(\\Delta \\lg N)$ rounds.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 14:56:25 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 20:20:24 GMT"}, {"version": "v3", "created": "Sat, 15 Aug 2020 11:14:16 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Moses", "William K.", "Jr."], ["Vaya", "Shailesh", ""]]}, {"id": "1702.02460", "submitter": "William Moses Jr.", "authors": "Dariusz R. Kowalski, William K. Moses Jr., Shailesh Vaya", "title": "Deterministic Backbone Creation in an SINR Network without Knowledge of\n  Location", "comments": "12 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given network, a backbone is an overlay network consisting of a\nconnected dominating set with additional accessibility properties. Once a\nbackbone is created for a network, it can be utilized for fast communication\namongst the nodes of the network.\n  The Signal-to-Interference-plus-Noise-Ratio (SINR) model has become the\nstandard for modeling communication among devices in wireless networks. For\nthis model, the community has pondered what the most realistic solutions for\ncommunication problems in wireless networks would look like. Such solutions\nwould have the characteristic that they would make the least number of\nassumptions about the availability of information about the participating\nnodes. Solving problems when nothing at all is known about the network and\nhaving nodes just start participating would be ideal. However, this is quite\nchallenging and most likely not feasible. The pragmatic approach is then to\nmake meaningful assumptions about the available information and present\nefficient solutions based on this information.\n  We present a solution for creation of backbone in the SINR model, when nodes\ndo not have access to their physical coordinates or the coordinates of other\nnodes in the network. This restriction models the deployment of nodes in\nvarious situations for sensing hurricanes, cyclones, and so on, where only\ninformation about nodes prior to their deployment may be known but not their\nactual locations post deployment. We assume that nodes have access to knowledge\nof their label, the labels of nodes within their neighborhood, the range from\nwhich labels are taken $[N]$ and the total number of participating nodes $n$.\nWe also assume that nodes wake up spontaneously. We present an efficient\ndeterministic protocol to create a backbone with a round complexity of\n$O(\\Delta \\lg^2 N)$.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 15:14:05 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Kowalski", "Dariusz R.", ""], ["Moses", "William K.", "Jr."], ["Vaya", "Shailesh", ""]]}, {"id": "1702.02547", "submitter": "David Harris", "authors": "David G. Harris", "title": "Oblivious resampling oracles and parallel algorithms for the Lopsided\n  Lovasz Local Lemma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lov\\'{a}sz Local Lemma (LLL) is a probabilistic tool which shows that, if\na collection of \"bad\" events $\\mathcal B$ in a probability space are not too\nlikely and not too interdependent, then there is a positive probability that no\nbad-events in $\\mathcal B$ occur. Moser & Tardos (2010) gave sequential and\nparallel algorithms which transformed most applications of the\nvariable-assignment LLL into efficient algorithms. A framework of Harvey &\nVondr\\'{a}k (2015) based on \"resampling oracles\" extended this to general\nsequential algorithms for other probability spaces satisfying the Lopsided\nLov\\'{a}sz Local Lemma (LLLL).\n  We describe a new structural property which holds for all known resampling\noracles, which we call \"obliviousness.\" Essentially, it means that the\ninteraction between two bad-events $B, B'$ depends only on the randomness used\nto resample $B$, and not the precise state within $B$ itself.\n  This property has two major consequences. First, combined with a framework of\nKolmogorov (2016), it is the key to achieving a unified parallel LLLL\nalgorithm, which is faster than previous, problem-specific algorithms of Harris\n(2016) for the variable-assignment LLLL algorithm and of Harris \\& Srinivasan\n(2014) for permutations. This gives the first RNC algorithms for rainbow\nperfect matchings and rainbow hamiltonian cycles of $K_n$.\n  Second, this property allows us to build LLLL probability spaces out of\nrelatively simple \"atomic\" events. This provides the first sequential\nresampling oracle for rainbow perfect matchings on the complete $s$-uniform\nhypergraph $K_n^{(s)}$, and the first commutative resampling oracle for\nhamiltonian cycles of $K_n$.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 18:10:56 GMT"}, {"version": "v10", "created": "Tue, 25 Jun 2019 01:51:03 GMT"}, {"version": "v11", "created": "Tue, 3 Dec 2019 22:37:24 GMT"}, {"version": "v12", "created": "Wed, 22 Jul 2020 19:27:10 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 20:52:49 GMT"}, {"version": "v3", "created": "Wed, 1 Mar 2017 23:11:41 GMT"}, {"version": "v4", "created": "Mon, 3 Apr 2017 21:34:59 GMT"}, {"version": "v5", "created": "Fri, 30 Jun 2017 22:51:57 GMT"}, {"version": "v6", "created": "Wed, 1 Nov 2017 22:36:37 GMT"}, {"version": "v7", "created": "Fri, 6 Apr 2018 23:08:15 GMT"}, {"version": "v8", "created": "Sun, 1 Jul 2018 21:42:15 GMT"}, {"version": "v9", "created": "Thu, 25 Oct 2018 15:39:15 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Harris", "David G.", ""]]}, {"id": "1702.02559", "submitter": "Sagar Kale", "authors": "Sagar Kale and Sumedh Tirodkar", "title": "Maximum Matching in Two, Three, and a Few More Passes Over Graph Streams", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the maximum matching problem in the semi-streaming model\nformalized by Feigenbaum, Kannan, McGregor, Suri, and Zhang that is inspired by\ngiant graphs of today. As our main result, we give a two-pass $(1/2 +\n1/16)$-approximation algorithm for triangle-free graphs and a two-pass $(1/2 +\n1/32)$-approximation algorithm for general graphs; these improve the\napproximation ratios of $1/2 + 1/52$ for bipartite graphs and $1/2 + 1/140$ for\ngeneral graphs by Konrad, Magniez, and Mathieu. In three passes, we achieve\napproximation ratios of $1/2 + 1/10$ for triangle-free graphs and $1/2 +\n1/19.753$ for general graphs. We also give a multi-pass algorithm where we\nbound the number of passes precisely---we give a $(2/3\n-\\varepsilon)$-approximation algorithm that uses $2/(3\\varepsilon)$ passes for\ntriangle-free graphs and $4/(3\\varepsilon)$ passes for general graphs. Our\nalgorithms are simple and combinatorial, use $O(n \\log n)$ space, and have\n$O(1)$ update time per edge.\n  For general graphs, our multi-pass algorithm improves the best known\ndeterministic algorithms in terms of the number of passes:\n  --Ahn and Guha give a $(2/3 - \\varepsilon)$-approximation algorithm that uses\n$O(\\log(1/\\varepsilon)/\\varepsilon^2)$ passes, whereas our $(2/3 -\n\\varepsilon)$-approximation algorithm uses $4/(3\\varepsilon)$ passes;\n  --they also give a $(1-\\varepsilon)$-approximation algorithm that uses\n$O(\\log n \\cdot \\mathrm{poly}(1/\\varepsilon))$ passes, where $n$ is the number\nof vertices of the input graph; although our algorithm is $(2/3 -\n\\varepsilon)$-approximation, our number of passes do not depend on $n$.\n  Earlier multi-pass algorithms either have a large constant inside big-$O$\nnotation for the number of passes or the constant cannot be determined due to\nthe involved analysis, so our multi-pass algorithm should use much fewer passes\nfor approximation ratios bounded slightly below $2/3$.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 18:48:02 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2017 16:22:01 GMT"}, {"version": "v3", "created": "Fri, 21 Apr 2017 15:19:17 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Kale", "Sagar", ""], ["Tirodkar", "Sumedh", ""]]}, {"id": "1702.02693", "submitter": "Pinyan Lu", "authors": "Jin-Yi Cai, Pinyan Lu, Mingji Xia", "title": "Dichotomy for Real Holant$^c$ Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Holant problems capture a class of Sum-of-Product computations such as\ncounting matchings. It is inspired by holographic algorithms and is equivalent\nto tensor networks, with counting CSP being a special case. A classification\nfor Holant problems is more difficult to prove, not only because it implies a\nclassification for counting CSP, but also due to the deeper reason that there\nexist more intricate polynomial time tractable problems in the broader\nframework.\n  We discover a new family of constraint functions $\\mathscr{L}$ which define\npolynomial time computable counting problems. These do not appear in counting\nCSP, and no newly discovered tractable constraints can be symmetric. It has a\ndelicate support structure related to error-correcting codes. Local holographic\ntransformations is fundamental in its tractability. We prove a complexity\ndichotomy theorem for all Holant problems defined by any real valued constraint\nfunction set on Boolean variables and contains two 0-1 pinning functions.\nPreviously, dichotomy for the same framework was only known for symmetric\nconstraint functions. he set $\\mathscr{L}$ supplies the last piece of\ntractability. We also prove a dichotomy for a variant of counting CSP as a\ntechnical component toward this Holant dichotomy.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 03:51:01 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Cai", "Jin-Yi", ""], ["Lu", "Pinyan", ""], ["Xia", "Mingji", ""]]}, {"id": "1702.02848", "submitter": "Patrice Ossona de Mendez", "authors": "Saeed Akhoondian Amiri, Patrice Ossona de Mendez, Roman Rabinovich,\n  Sebastian Siebertz", "title": "Distributed Domination on Graph Classes of Bounded Expansion", "comments": "presented at the 30th ACM Symposium on Parallelism in Algorithms and\n  Architectures (SPAA 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide a new constant factor approximation algorithm for the (connected)\ndistance-$r$ dominating set problem on graph classes of bounded expansion.\nClasses of bounded expansion include many familiar classes of sparse graphs\nsuch as planar graphs and graphs with excluded (topological) minors, and\nnotably, these classes form the most general subgraph closed classes of graphs\nfor which a sequential constant factor approximation algorithm for the\ndistance-$r$ dominating set problem is currently known. Our algorithm can be\nimplemented in the \\congestbc model of distributed computing and uses\n$\\mathcal{O}(r^2 \\log n)$ communication rounds.\n  Our techniques, which may be of independent interest, are based on a\ndistributed computation of sparse neighborhood covers of small radius on\nbounded expansion classes. We show how to compute an $r$-neighborhood cover of\nradius~$2r$ and overlap $f(r)$ on every class of bounded expansion in\n$\\mathcal{O}(r^2 \\log n)$ communication rounds for some function~$f$.% in the\n$\\mathcal{CONGEST}_{\\mathrm{BC}}$ model.\n  Finally, we show how to use the greater power of the $\\mathcal{LOCAL}$ model\nto turn any distance-$r$ dominating set into a constantly larger connected\ndistance-$r$ dominating set in $3r+1$ rounds on any class of bounded expansion.\nCombining this algorithm, e.g., with the constant factor approximation\nalgorithm for dominating sets on planar graphs of Lenzen et al.\\ gives a\nconstant factor approximation algorithm for connected dominating sets on planar\ngraphs in a constant number of rounds in the $\\mathcal{LOCAL}$ model, where the\napproximation ratio is only $6$ times larger than that of Lenzen et al.'s\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 14:32:14 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 20:54:35 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Amiri", "Saeed Akhoondian", ""], ["de Mendez", "Patrice Ossona", ""], ["Rabinovich", "Roman", ""], ["Siebertz", "Sebastian", ""]]}, {"id": "1702.02891", "submitter": "Ali \\c{C}ivril", "authors": "Ali \\c{C}ivril", "title": "Sparse Approximation by Semidefinite Programming", "comments": "The result is already implied by a trivial greedy algorithm. There is\n  no need for SDP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of sparse approximation and the closely related compressed\nsensing have received tremendous attention in the past decade. Primarily\nstudied from the viewpoint of applied harmonic analysis and signal processing,\nthere have been two dominant algorithmic approaches to this problem: Greedy\nmethods called the matching pursuit (MP) and the linear programming based\napproaches called the basis pursuit (BP). The aim of the current paper is to\nbring a fresh perspective to sparse approximation by treating it as a\ncombinatorial optimization problem and providing an algorithm based on the\npowerful optimization technique semidefinite programming (SDP). In particular,\nwe show that there is a randomized algorithm based on a semidefinite relaxation\nof the problem with performance guarantees depending on the coherence and the\nrestricted isometry constant of the dictionary used. We then show a\nderandomization of the algorithm based on the method of conditional\nprobabilities.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 16:51:34 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 13:34:29 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["\u00c7ivril", "Ali", ""]]}, {"id": "1702.02937", "submitter": "Nima Anari", "authors": "Nima Anari and Shayan Oveis Gharan", "title": "A Generalization of Permanent Inequalities and Applications in Counting\n  and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT math.CO math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A polynomial $p\\in\\mathbb{R}[z_1,\\dots,z_n]$ is real stable if it has no\nroots in the upper-half complex plane. Gurvits's permanent inequality gives a\nlower bound on the coefficient of the $z_1z_2\\dots z_n$ monomial of a real\nstable polynomial $p$ with nonnegative coefficients. This fundamental\ninequality has been used to attack several counting and optimization problems.\n  Here, we study a more general question: Given a stable multilinear polynomial\n$p$ with nonnegative coefficients and a set of monomials $S$, we show that if\nthe polynomial obtained by summing up all monomials in $S$ is real stable, then\nwe can lowerbound the sum of coefficients of monomials of $p$ that are in $S$.\nWe also prove generalizations of this theorem to (real stable) polynomials that\nare not multilinear. We use our theorem to give a new proof of Schrijver's\ninequality on the number of perfect matchings of a regular bipartite graph,\ngeneralize a recent result of Nikolov and Singh, and give deterministic\npolynomial time approximation algorithms for several counting problems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 18:46:54 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Anari", "Nima", ""], ["Gharan", "Shayan Oveis", ""]]}, {"id": "1702.02970", "submitter": "Jonathan Ullman", "authors": "Mitali Bafna and Jonathan Ullman", "title": "The Price of Selection in Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the differentially private top-$k$ selection problem, we are given a\ndataset $X \\in \\{\\pm 1\\}^{n \\times d}$, in which each row belongs to an\nindividual and each column corresponds to some binary attribute, and our goal\nis to find a set of $k \\ll d$ columns whose means are approximately as large as\npossible. Differential privacy requires that our choice of these $k$ columns\ndoes not depend too much on any on individual's dataset. This problem can be\nsolved using the well known exponential mechanism and composition properties of\ndifferential privacy. In the high-accuracy regime, where we require the error\nof the selection procedure to be to be smaller than the so-called sampling\nerror $\\alpha \\approx \\sqrt{\\ln(d)/n}$, this procedure succeeds given a dataset\nof size $n \\gtrsim k \\ln(d)$.\n  We prove a matching lower bound, showing that a dataset of size $n \\gtrsim k\n\\ln(d)$ is necessary for private top-$k$ selection in this high-accuracy\nregime. Our lower bound is the first to show that selecting the $k$ largest\ncolumns requires more data than simply estimating the value of those $k$\ncolumns, which can be done using a dataset of size just $n \\gtrsim k$.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 20:11:49 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Bafna", "Mitali", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1702.03106", "submitter": "Ching-Lueh Chang", "authors": "Ching-Lueh Chang", "title": "A Las Vegas approximation algorithm for metric $1$-median selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an $n$-point metric space, consider the problem of finding a point with\nthe minimum sum of distances to all points. We show that this problem has a\nrandomized algorithm that {\\em always} outputs a $(2+\\epsilon)$-approximate\nsolution in an expected $O(n/\\epsilon^2)$ time for each constant $\\epsilon>0$.\nInheriting Indyk's algorithm, our algorithm outputs a\n$(1+\\epsilon)$-approximate $1$-median in $O(n/\\epsilon^2)$ time with\nprobability $\\Omega(1)$.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 09:11:18 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 06:15:31 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Chang", "Ching-Lueh", ""]]}, {"id": "1702.03152", "submitter": "Fouad Chedid", "authors": "Fouad B. Chedid", "title": "A Variation of Levin Search for All Well-Defined Problems", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1973, L.A. Levin published an algorithm that solves any inversion problem\n$\\pi$ as quickly as the fastest algorithm $p^*$ computing a solution for $\\pi$\nin time bounded by $2^{l(p^*)}.t^*$, where $l(p^*)$ is the length of the binary\nencoding of $p^*$, and $t^*$ is the runtime of $p^*$ plus the time to verify\nits correctness. In 2002, M. Hutter published an algorithm that solves any\nwell-defined problem $\\pi$ as quickly as the fastest algorithm $p^*$ computing\na solution for $\\pi$ in time bounded by $5.t_{p}(x)+d_p.time_{t_{p}}(x)+c_p$,\nwhere $d_p=40.2^{l(p)+l(t_{p})}$ and $c_p=40.2^{l(f)+1}.O(l(f)^2)$, where\n$l(f)$ is the length of the binary encoding of a proof $f$ that produces a pair\n$(p,t_p)$, where $t_p(x)$ is a provable time bound on the runtime of the\nfastest program $p$ provably equivalent to $p^*$. In this paper, we rewrite\nLevin Search using the ideas of Hutter so that we have a new simple algorithm\nthat solves any well-defined problem $\\pi$ as quickly as the fastest algorithm\n$p^*$ computing a solution for $\\pi$ in time bounded by $O(l(f)^2).t_p(x)$.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 12:43:46 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Chedid", "Fouad B.", ""]]}, {"id": "1702.03154", "submitter": "Pierre Peterlongo", "authors": "Antoine Limasset and Guillaume Rizk and Rayan Chikhi and Pierre\n  Peterlongo", "title": "Fast and scalable minimal perfect hashing for massive key sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimal perfect hash functions provide space-efficient and collision-free\nhashing on static sets. Existing algorithms and implementations that build such\nfunctions have practical limitations on the number of input elements they can\nprocess, due to high construction time, RAM or external memory usage. We\nrevisit a simple algorithm and show that it is highly competitive with the\nstate of the art, especially in terms of construction time and memory usage. We\nprovide a parallel C++ implementation called BBhash. It is capable of creating\na minimal perfect hash function of $10^{10}$ elements in less than 7 minutes\nusing 8 threads and 5 GB of memory, and the resulting function uses 3.7\nbits/element. To the best of our knowledge, this is also the first\nimplementation that has been successfully tested on an input of cardinality\n$10^{12}$. Source code: https://github.com/rizkg/BBHash\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 12:51:54 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 13:13:02 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Limasset", "Antoine", ""], ["Rizk", "Guillaume", ""], ["Chikhi", "Rayan", ""], ["Peterlongo", "Pierre", ""]]}, {"id": "1702.03259", "submitter": "Vincent Cohen-Addad", "authors": "Vincent Cohen-Addad, S{\\o}ren Dahlgaard, and Christian Wulff-Nilsen", "title": "Fast and Compact Exact Distance Oracle for Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given a graph, a distance oracle is a data structure that answers\ndistance queries between pairs of vertices. We introduce an $O(n^{5/3})$-space\ndistance oracle which answers exact distance queries in $O(\\log n)$ time for\n$n$-vertex planar edge-weighted digraphs. All previous distance oracles for\nplanar graphs with truly subquadratic space i.e., space $O(n^{2 - \\epsilon})$\nfor some constant $\\epsilon > 0$) either required query time polynomial in $n$\nor could only answer approximate distance queries.\n  Furthermore, we show how to trade-off time and space: for any $S \\ge\nn^{3/2}$, we show how to obtain an $S$-space distance oracle that answers\nqueries in time $O((n^{5/2}/ S^{3/2}) \\log n)$. This is a polynomial\nimprovement over the previous planar distance oracles with $o(n^{1/4})$ query\ntime.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 17:27:35 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 08:23:30 GMT"}, {"version": "v3", "created": "Tue, 2 May 2017 08:06:18 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Dahlgaard", "S\u00f8ren", ""], ["Wulff-Nilsen", "Christian", ""]]}, {"id": "1702.03375", "submitter": "Xue Chen", "authors": "Xue Chen", "title": "Derandomized Balanced Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the maximum loads of explicit hash families in the\n$d$-choice schemes when allocating sequentially $n$ balls into $n$ bins. We\nconsider the \\emph{Uniform-Greedy} scheme, which provides $d$ independent bins\nfor each ball and places the ball into the bin with the least load, and its\nnon-uniform variant --- the \\emph{Always-Go-Left} scheme introduced by\nV\\\"ocking. We construct a hash family with $O(\\log n \\log \\log n)$ random bits\nbased on the previous work of Celis et al. and show the following results.\n  1. With high probability, this hash family has a maximum load of $\\frac{\\log\n\\log n}{\\log d} + O(1)$ in the \\emph{Uniform-Greedy} scheme.\n  2. With high probability, it has a maximum load of $\\frac{\\log \\log n}{d \\log\n\\phi_d} + O(1)$ in the \\emph{Always-Go-Left} scheme for a constant\n$\\phi_d>1.61$.\n  The maximum loads of our hash family match the maximum loads of a perfectly\nrandom hash function in the \\emph{Uniform-Greedy} and \\emph{Always-Go-Left}\nscheme separately, up to the low order term of constants. Previously, the best\nknown hash families matching the same maximum loads of a perfectly random hash\nfunction in $d$-choice schemes were $O(\\log n)$-wise independent functions,\nwhich needs $\\Theta(\\log^2 n)$ random bits.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 02:50:27 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 22:19:03 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 21:27:54 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Chen", "Xue", ""]]}, {"id": "1702.03536", "submitter": "Patryk Mikos", "authors": "Patryk Mikos", "title": "A new lower bound for the on-line coloring of intervals with bandwidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The on-line interval coloring and its variants are important combinatorial\nproblems with many applications in network multiplexing, resource allocation\nand job scheduling. In this paper we present a new lower bound of $4.1626$ for\nthe competitive ratio for the on-line coloring of intervals with bandwidth\nwhich improves the best known lower bound of $\\frac{24}{7}$. For the on-line\ncoloring of unit intervals with bandwidth we improve the lower bound of $1.831$\nto $2$.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 16:07:32 GMT"}, {"version": "v2", "created": "Sat, 15 Apr 2017 10:25:48 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Mikos", "Patryk", ""]]}, {"id": "1702.03605", "submitter": "Mingda Qiao", "authors": "Lijie Chen, Jian Li, Mingda Qiao", "title": "Nearly Instance Optimal Sample Complexity Bounds for Top-k Arm Selection", "comments": "Accepted by AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Best-$k$-Arm problem, we are given $n$ stochastic bandit arms, each\nassociated with an unknown reward distribution. We are required to identify the\n$k$ arms with the largest means by taking as few samples as possible. In this\npaper, we make progress towards a complete characterization of the\ninstance-wise sample complexity bounds for the Best-$k$-Arm problem. On the\nlower bound side, we obtain a novel complexity term to measure the sample\ncomplexity that every Best-$k$-Arm instance requires. This is derived by an\ninteresting and nontrivial reduction from the Best-$1$-Arm problem. We also\nprovide an elimination-based algorithm that matches the instance-wise lower\nbound within doubly-logarithmic factors. The sample complexity of our algorithm\nstrictly dominates the state-of-the-art for Best-$k$-Arm (module constant\nfactors).\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 01:31:46 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Chen", "Lijie", ""], ["Li", "Jian", ""], ["Qiao", "Mingda", ""]]}, {"id": "1702.03644", "submitter": "Yan Zheng", "authors": "Yan Zheng and Jeff M. Phillips", "title": "Coresets for Kernel Regression", "comments": "11 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel regression is an essential and ubiquitous tool for non-parametric data\nanalysis, particularly popular among time series and spatial data. However, the\ncentral operation which is performed many times, evaluating a kernel on the\ndata set, takes linear time. This is impractical for modern large data sets.\n  In this paper we describe coresets for kernel regression: compressed data\nsets which can be used as proxy for the original data and have provably bounded\nworst case error. The size of the coresets are independent of the raw number of\ndata points, rather they only depend on the error guarantee, and in some cases\nthe size of domain and amount of smoothing. We evaluate our methods on very\nlarge time series and spatial data, and demonstrate that they incur negligible\nerror, can be constructed extremely efficiently, and allow for great\ncomputational gains.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 06:17:16 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 14:34:08 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Zheng", "Yan", ""], ["Phillips", "Jeff M.", ""]]}, {"id": "1702.03657", "submitter": "Xavier Bellekens", "authors": "Xavier Bellekens and Amar Seeam and Christos Tachtatzis and Robert\n  Atkinson", "title": "Trie Compression for GPU Accelerated Multi-Pattern Matching", "comments": "4 pages, 6 figures. Accepted and Published in The Ninth International\n  Conferences on Pervasive Patterns and Applications PATTERNS 2017 (19 - 23/02,\n  2017 - Athens, Greece)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graphics Processing Units allow for running massively parallel applications\noffloading the CPU from computationally intensive resources, however GPUs have\na limited amount of memory. In this paper a trie compression algorithm for\nmassively parallel pattern matching is presented demonstrating 85% less space\nrequirements than the original highly efficient parallel failure-less\naho-corasick, whilst demonstrating over 22 Gbps throughput. The algorithm\npresented takes advantage of compressed row storage matrices as well as shared\nand texture memory on the GPU.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 07:18:13 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Bellekens", "Xavier", ""], ["Seeam", "Amar", ""], ["Tachtatzis", "Christos", ""], ["Atkinson", "Robert", ""]]}, {"id": "1702.03773", "submitter": "Mael Canu", "authors": "Ma\\\"el Canu (LFI), Marie-Jeanne Lesot (LFI), Adrien Revault d'Allonnes\n  (LIASD)", "title": "Overlapping Community Detection by Local Decentralised Vertex-centred\n  Process", "comments": "IEEE Computer Society. 2016 IEEE 16th International Conference on\n  Data Mining Workshops (ICDMW'16), Dec 2016, Barcelone, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the identification of overlapping communities, allowing\nnodes to simultaneously belong to several communities, in a decentralised way.\nTo that aim it proposes LOCNeSs, an algorithm specially designed to run in a\ndecentralised environment and to limit propagation, two essential\ncharacteristics to be applied in mobile networks. It is based on the\nexploitation of the preferential attachment mechanism in networks. Experimental\nresults show that LOCNeSs is stable and achieves good overlapping vertex\nidentification.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 13:41:58 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Canu", "Ma\u00ebl", "", "LFI"], ["Lesot", "Marie-Jeanne", "", "LFI"], ["d'Allonnes", "Adrien Revault", "", "LIASD"]]}, {"id": "1702.03959", "submitter": "Victor Verdugo", "authors": "Varun Kanade, Frederik Mallmann-Trenn, Victor Verdugo", "title": "How large is your graph?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the graph size, where one is given only\nlocal access to the graph. We formally define a query model in which one starts\nwith a \\emph{seed} node and is allowed to make queries about neighbours of\nnodes that have already been seen. In the case of undirected graphs, an\nestimator of Katzir et al. (2014) based on a sample from the stationary\ndistribution $\\pi$ uses $O\\left(\\frac{1}{\\|\\pi\\|_2} + \\text{davg}\\right)$\nqueries, we prove that this is tight. In addition, we establish this as a lower\nbound even when the algorithm is allowed to crawl the graph arbitrarily, the\nresults of Katzir et al. give an upper bound that is worse by a multiplicative\nfactor $t_\\text{mix} \\cdot \\log(n)$. The picture becomes significantly\ndifferent in the case of directed graphs. We show that without strong\nassumptions on the graph structure, the number of nodes cannot be predicted to\nwithin a constant multiplicative factor without using a number of queries that\nare at least linear in the number of nodes, in particular, rapid mixing and\nsmall diameter, properties that most real-world networks exhibit, do not\nsuffice. The question of interest is whether any algorithm can beat\nbreadth-first search. We introduce a new parameter, generalising the\nwell-studied conductance, such that if a suitable bound on it exists and is\nknown to the algorithm, the number of queries required is sublinear in the\nnumber of edges, we show that this is tight.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 19:35:15 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Kanade", "Varun", ""], ["Mallmann-Trenn", "Frederik", ""], ["Verdugo", "Victor", ""]]}, {"id": "1702.03989", "submitter": "Sivan Sabato", "authors": "Tom Hess and Sivan Sabato", "title": "Selecting with History", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a new selection problem, \\emph{Selecting with History}, which\nextends the secretary problem to a setting with historical information. We\npropose a strategy for this problem and calculate its success probability in\nthe limit of a large sequence.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 21:19:32 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 15:27:06 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 05:59:57 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Hess", "Tom", ""], ["Sabato", "Sivan", ""]]}, {"id": "1702.04088", "submitter": "Andreas Gunawan", "authors": "Andreas Gunawan", "title": "Solving Tree Containment Problem for Reticulation-visible Networks with\n  Optimal Running Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree containment problem is a fundamental problem in phylogenetic study, as\nit is used to verify a network model. It asks whether a given network contain a\nsubtree that resembles a binary tree. The problem is NP-complete in general,\neven in the class of binary network. Recently, it was proven to be solvable in\ncubic time, and later in quadratic time for the class of general reticulation\nvisible networks. In this paper, we further improve the time complexity into\nlinear time.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 06:15:28 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Gunawan", "Andreas", ""]]}, {"id": "1702.04164", "submitter": "Christian Schulz", "authors": "Christian Schulz and Jesper Larsson Tr\\\"aff and Konrad von Kirchbach", "title": "Better Process Mapping and Sparse Quadratic Assignment", "comments": "additional algorithms and experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Communication and topology aware process mapping is a powerful approach to\nreduce communication time in parallel applications with known communication\npatterns on large, distributed memory systems. We address the problem as a\nquadratic assignment problem (QAP), and present algorithms to construct initial\nmappings of processes to processors, and fast local search algorithms to\nfurther improve the mappings. By exploiting assumptions that typically hold for\napplications and modern supercomputer systems such as sparse communication\npatterns and hierarchically organized communication systems, we obtain\nsignificantly more powerful algorithms for these special QAPs. Our multilevel\nconstruction algorithms employ perfectly balanced graph partitioning techniques\nand exploit the given communication system hierarchy in significant ways. We\npresent improvements to a local search algorithm of Brandfass et al. (2013),\nand further decrease the running time by reducing the time needed to perform\nswaps in the assignment as well as by carefully constraining local search\nneighborhoods. We also investigate different algorithms to create the\ncommunication graph that is mapped onto the processor network. Experiments\nindicate that our algorithms not only dramatically speed up local search, but\ndue to the multilevel approach also find much better solutions in practice.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 11:40:21 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 11:19:56 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Schulz", "Christian", ""], ["Tr\u00e4ff", "Jesper Larsson", ""], ["von Kirchbach", "Konrad", ""]]}, {"id": "1702.04170", "submitter": "Tomas Balyo", "authors": "Tomas Balyo, Kai Fieger, Christian Schulz", "title": "Optimal Longest Paths by Dynamic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an optimal algorithm for solving the longest path problem in\nundirected weighted graphs. By using graph partitioning and dynamic\nprogramming, we obtain an algorithm that is significantly faster than other\nstate-of-the-art methods. This enables us to solve instances that have\npreviously been unsolved.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 12:05:58 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Balyo", "Tomas", ""], ["Fieger", "Kai", ""], ["Schulz", "Christian", ""]]}, {"id": "1702.04211", "submitter": "Rosario Scatamacchia", "authors": "Federico Della Croce, Ulrich Pferschy, Rosario Scatamacchia", "title": "Dynamic programming algorithms, efficient solution of the LP-relaxation\n  and approximation schemes for the Penalized Knapsack Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the 0-1 Penalized Knapsack Problem (PKP). Each item has a profit,\na weight and a penalty and the goal is to maximize the sum of the profits minus\nthe greatest penalty value of the items included in a solution. We propose an\nexact approach relying on a procedure which narrows the relevant range of\npenalties, on the identification of a core problem and on dynamic programming.\nThe proposed approach turns out to be very effective in solving hard instances\nof PKP and compares favorably both to commercial solver CPLEX 12.5 applied to\nthe ILP formulation of the problem and to the best available exact algorithm in\nthe literature. Then we present a general inapproximability result and\ninvestigate several relevant special cases which permit fully polynomial time\napproximation schemes (FPTASs).\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 14:00:01 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Della Croce", "Federico", ""], ["Pferschy", "Ulrich", ""], ["Scatamacchia", "Rosario", ""]]}, {"id": "1702.04304", "submitter": "Sven Helmer", "authors": "Paolo Bolzoni and Sven Helmer", "title": "Solving Orienteering with Category Constraints Using Prioritized Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an approach for solving rooted orienteering problems with category\nconstraints as found in tourist trip planning and logistics. It is based on\nexpanding partial solutions in a systematic way, prioritizing promising ones,\nwhich reduces the search space we have to traverse during the search. The\ncategory constraints help in reducing the space we have to explore even\nfurther. We implement an algorithm that computes the optimal solution and also\nillustrate how our approach can be turned into an approximation algorithm,\nyielding much faster run times and guaranteeing lower bounds on the quality of\nthe solution found. We demonstrate the effectiveness of our algorithms by\ncomparing them to the state-of-the-art approach and an optimal algorithm based\non dynamic programming, showing that our technique clearly outperforms these\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 17:29:03 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Bolzoni", "Paolo", ""], ["Helmer", "Sven", ""]]}, {"id": "1702.04307", "submitter": "Kent Quanrud", "authors": "Chandra Chekuri and Kent Quanrud", "title": "Approximating the Held-Karp Bound for Metric TSP in Nearly Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a nearly linear time randomized approximation scheme for the\nHeld-Karp bound [Held and Karp, 1970] for metric TSP. Formally, given an\nundirected edge-weighted graph $G$ on $m$ edges and $\\epsilon > 0$, the\nalgorithm outputs in $O(m \\log^4n /\\epsilon^2)$ time, with high probability, a\n$(1+\\epsilon)$-approximation to the Held-Karp bound on the metric TSP instance\ninduced by the shortest path metric on $G$. The algorithm can also be used to\noutput a corresponding solution to the Subtour Elimination LP. We substantially\nimprove upon the $O(m^2 \\log^2(m)/\\epsilon^2)$ running time achieved previously\nby Garg and Khandekar. The LP solution can be used to obtain a fast randomized\n$\\big(\\frac{3}{2} + \\epsilon\\big)$-approximation for metric TSP which improves\nupon the running time of previous implementations of Christofides' algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 17:33:47 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 04:38:45 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Chekuri", "Chandra", ""], ["Quanrud", "Kent", ""]]}, {"id": "1702.04322", "submitter": "Manuel Sorge", "authors": "Iyad Kanj, Christian Komusiewicz, Manuel Sorge, and Erik Jan van\n  Leeuwen", "title": "Parameterized Algorithms for Recognizing Monopolar and 2-Subcolorable\n  Graphs", "comments": "A preliminary version of this paper appears in the proceedings of\n  SWAT 2016. A journal version of this paper appears in Journal of Computer and\n  System Sciences, volume 92, 2018. This ArXiv paper additionally discusses\n  relations to the iterative localization technique (Heggernes et al.,\n  Information and Computation, 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph $G$ is a $(\\Pi_A,\\Pi_B)$-graph if $V(G)$ can be bipartitioned into\n$A$ and $B$ such that $G[A]$ satisfies property $\\Pi_A$ and $G[B]$ satisfies\nproperty $\\Pi_B$. The $(\\Pi_{A},\\Pi_{B})$-Recognition problem is to recognize\nwhether a given graph is a $(\\Pi_A,\\Pi_B)$-graph. There are many\n$(\\Pi_{A},\\Pi_{B})$-Recognition problems, including the recognition problems\nfor bipartite, split, and unipolar graphs. We present efficient algorithms for\nmany cases of $(\\Pi_A,\\Pi_B)$-Recognition based on a technique which we dub\ninductive recognition. In particular, we give fixed-parameter algorithms for\ntwo NP-hard $(\\Pi_{A},\\Pi_{B})$-Recognition problems, Monopolar Recognition and\n2-Subcoloring. We complement our algorithmic results with several hardness\nresults for $(\\Pi_{A},\\Pi_{B})$-Recognition.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 18:21:28 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 10:19:10 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Kanj", "Iyad", ""], ["Komusiewicz", "Christian", ""], ["Sorge", "Manuel", ""], ["van Leeuwen", "Erik Jan", ""]]}, {"id": "1702.04376", "submitter": "Moses Ganardi", "authors": "Moses Ganardi, Danny Hucke, Daniel K\\\"onig, Markus Lohrey,\n  Konstantinos Mamouras", "title": "Automata theory on sliding windows", "comments": "Extended version of a paper presented at STACS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper we analyzed the space complexity of streaming algorithms\nwhose goal is to decide membership of a sliding window to a fixed language. For\nthe class of regular languages we proved a space trichotomy theorem: for every\nregular language the optimal space bound is either constant, logarithmic or\nlinear. In this paper we continue this line of research: We present natural\ncharacterizations for the constant and logarithmic space classes and establish\ntight relationships to the concept of language growth. We also analyze the\nspace complexity with respect to automata size and prove almost matching lower\nand upper bounds. Finally, we consider the decision problem whether a language\ngiven by a DFA/NFA admits a sliding window algorithm using logarithmic/constant\nspace.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 20:17:00 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 11:54:48 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Ganardi", "Moses", ""], ["Hucke", "Danny", ""], ["K\u00f6nig", "Daniel", ""], ["Lohrey", "Markus", ""], ["Mamouras", "Konstantinos", ""]]}, {"id": "1702.04536", "submitter": "Ami Paz", "authors": "Ami Paz, Gregory Schwartzman", "title": "A $(2+\\epsilon)$-Approximation for Maximum Weight Matching in the\n  Semi-Streaming Model", "comments": null, "journal-ref": "SODA 2017: 2153-2161", "doi": "10.1137/1.9781611974782.140", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple deterministic single-pass $(2+\\epsilon)$-approximation\nalgorithm for the maximum weight matching problem in the semi-streaming model.\nThis improves upon the currently best known approximation ratio of\n$(4+\\epsilon)$.\n  Our algorithm uses $O(n\\log^2 n)$ bits of space for constant values of\n$\\epsilon$. It relies on a variation of the local-ratio theorem, which may be\nof use for other algorithms in the semi-streaming model as well.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 10:14:41 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 20:36:45 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Paz", "Ami", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "1702.04645", "submitter": "Benjamin Chi\\^em", "authors": "Benjamin Chi\\^em, Andine Havelange, Paul Van Dooren", "title": "A parallel implementation of the Synchronised Louvain method", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection in networks is a very actual and important field of\nresearch with applications in many areas. But, given that the amount of\nprocessed data increases more and more, existing algorithms need to be adapted\nfor very large graphs. The objective of this project was to parallelise the\nSynchronised Louvain Method, a community detection algorithm developed by\nArnaud Browet, in order to improve its performances in terms of computation\ntime and thus be able to faster detect communities in very large graphs. To\nreach this goal, we used the API OpenMP to parallelise the algorithm and then\ncarried out performance tests. We studied the computation time and speedup of\nthe parallelised algorithm and were able to bring out some qualitative trends.\nWe obtained a great speedup, compared with the theoretical prediction of Amdahl\nlaw. To conclude, using the parallel implementation of the algorithm of Browet\non large graphs seems to give good results, both in terms of computation time\nand speedup. Further tests should be carried out in order to obtain more\nquantitative results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 15:09:15 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Chi\u00eam", "Benjamin", ""], ["Havelange", "Andine", ""], ["Van Dooren", "Paul", ""]]}, {"id": "1702.04786", "submitter": "Bundit Laekhanukit", "authors": "Jittat Fakcharoenphol, Bundit Laekhanukit, Pattara Sukprasert", "title": "Finding All Useless Arcs in Directed Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a linear-time algorithm for simplifying flow networks on directed\nplanar graphs: Given a directed planar graph on $n$ vertices, a source vertex\n$s$ and a sink vertex $t$, our algorithm removes all the arcs that do not\nparticipate in any simple $s,t$-path in linear-time. The output graph produced\nby our algorithm satisfies the prerequisite needed by the $O(n\\log n)$-time\nalgorithm of Weihe [FOCS'94 \\& JCSS'97] for computing maximum $s,t$-flow in\ndirected planar graphs. Previously, Weihe's algorithm could not run in $O(n\\log\nn)$-time due to the absence of the preprocessing step; all the preceding\nalgorithms run in $\\tilde{\\Omega}(n^2)$-time [Misiolek-Chen, COCOON'05 \\&\nIPL'06; Biedl, Brejov{\\'{a}} and Vinar, MFCS'00]. Consequently, this provides\nan alternative $O(n\\log n)$-time algorithm for computing maximum $s,t$-flow in\ndirected planar graphs in addition to the known $O(n\\log n)$-time algorithms\n[Borradaile-Klein, SODA'06 \\& J.ACM'09; Erickson, SODA'10].\n  Our algorithm can be seen as a (truly) linear-time $s,t$-flow sparsifier for\ndirected planar graphs, which runs faster than any maximum $s,t$-flow algorithm\n(which can also be seen of as a sparsifier). The simplified structures of the\nresulting graph might be useful in future developments of maximum $s,t$-flow\nalgorithms in both directed and undirected planar graphs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 21:35:22 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2017 20:40:11 GMT"}, {"version": "v3", "created": "Thu, 2 Mar 2017 10:31:38 GMT"}, {"version": "v4", "created": "Tue, 8 May 2018 15:40:13 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Fakcharoenphol", "Jittat", ""], ["Laekhanukit", "Bundit", ""], ["Sukprasert", "Pattara", ""]]}, {"id": "1702.04866", "submitter": "Paul Gazzillo", "authors": "Thomas D. Dickerson, Paul Gazzillo, Maurice Herlihy, Eric Koskinen", "title": "Proust: A Design Space for Highly-Concurrent Transactional Data\n  Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most STM systems are poorly equipped to support libraries of concurrent data\nstructures. One reason is that they typically detect conflicts by tracking\ntransactions' read sets and write sets, an approach that often leads to false\nconflicts. A second is that existing data structures and libraries often need\nto be rewritten from scratch to support transactional conflict detection and\nrollback. This paper introduces Proust, a framework for the design and\nimplementation of transactional data structures. Proust is designed to maximize\nre-use of existing well-engineered by providing transactional \"wrappers\" to\nmake existing thread-safe concurrent data structures transactional. Proustian\nobjects are also integrated with an underling STM system, allowing them to take\nadvantage of well-engineered STM conflict detection mechanisms. Proust\ngeneralizes and unifies prior approaches such as boosting and predication.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 06:07:25 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 14:38:14 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Dickerson", "Thomas D.", ""], ["Gazzillo", "Paul", ""], ["Herlihy", "Maurice", ""], ["Koskinen", "Eric", ""]]}, {"id": "1702.04871", "submitter": "Seeun William Umboh", "authors": "Jiawei Qian, Seeun William Umboh, David P. Williamson", "title": "Online Constrained Forest and Prize-Collecting Network Design", "comments": "A preliminary version [20] appeared in ICALP 2011 but was later\n  discovered to be flawed. This paper presents a corrected version of the\n  algorithm and proofs, and a generalisation of the prize-collecting problem\n  considered in [20]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a very general type of online network design problem,\nand generalize two different previous algorithms, one for an online network\ndesign problem due to Berman and Coulston [4] and one for (offline) general\nnetwork design problems due to Goemans and Williamson [9]; we give an O(log\nk)-competitive algorithm, where k is the number of nodes that must be\nconnected. We also consider a further generalization of the problem that allows\nus to pay penalties in exchange for violating connectivity constraints; we give\nan online O(log k)-competitive algorithm for this case as well.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 06:37:42 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Qian", "Jiawei", ""], ["Umboh", "Seeun William", ""], ["Williamson", "David P.", ""]]}, {"id": "1702.05051", "submitter": "Ranko Lazic", "authors": "Marcin Jurdzinski and Ranko Lazic", "title": "Succinct progress measures for solving parity games", "comments": null, "journal-ref": null, "doi": "10.1109/LICS.2017.8005092", "report-no": null, "categories": "cs.DS cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent breakthrough paper by Calude et al. has given the first algorithm\nfor solving parity games in quasi-polynomial time, where previously the best\nalgorithms were mildly subexponential. We devise an alternative\nquasi-polynomial time algorithm based on progress measures, which allows us to\nreduce the space required from quasi-polynomial to nearly linear. Our key\ntechnical tools are a novel concept of ordered tree coding, and a succinct tree\ncoding result that we prove using bounded adaptive multi-counters, both of\nwhich are interesting in their own right.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 17:02:05 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 14:23:07 GMT"}, {"version": "v3", "created": "Wed, 4 Apr 2018 19:43:23 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Jurdzinski", "Marcin", ""], ["Lazic", "Ranko", ""]]}, {"id": "1702.05217", "submitter": "Junhua Wu", "authors": "Frank Neumann, Sergey Polyakovskiy, Martin Skutella, Leen Stougie,\n  Junhua Wu", "title": "A Fully Polynomial Time Approximation Scheme for Packing While Traveling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the interactions between different combinatorial optimisation\nproblems in real-world applications is a challenging task. Recently, the\ntraveling thief problem (TTP), as a combination of the classical traveling\nsalesperson problem and the knapsack problem, has been introduced to study\nthese interactions in a systematic way. We investigate the underlying\nnon-linear packing while traveling (PWT) problem of the TTP where items have to\nbe selected along a fixed route. We give an exact dynamic programming approach\nfor this problem and a fully polynomial time approximation scheme (FPTAS) when\nmaximising the benefit that can be gained over the baseline travel cost. Our\nexperimental investigations show that our new approaches outperform current\nstate-of-the-art approaches on a wide range of benchmark instances.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 04:06:45 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Neumann", "Frank", ""], ["Polyakovskiy", "Sergey", ""], ["Skutella", "Martin", ""], ["Stougie", "Leen", ""], ["Wu", "Junhua", ""]]}, {"id": "1702.05218", "submitter": "Hanrui Zhang", "authors": "Wei Chen, Hanrui Zhang", "title": "Complete Submodularity Characterization in the Comparative Independent\n  Cascade Model", "comments": "Theoret. Comput. Sci. (2018)", "journal-ref": null, "doi": "10.1016/j.tcs.2018.03.026", "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the propagation of comparative ideas or items in social networks. A\nfull characterization for submodularity in the comparative independent cascade\n(Com-IC) model of two-idea cascade is given, for competing ideas and\ncomplementary ideas respectively, with or without reconsideration. We further\nintroduce One-Shot model where agents show less patience toward ideas, and show\nthat in One-Shot model, only the strongest idea spreads with submodularity.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 04:11:08 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 12:53:12 GMT"}, {"version": "v3", "created": "Sat, 30 Sep 2017 19:07:58 GMT"}, {"version": "v4", "created": "Tue, 3 Oct 2017 13:23:20 GMT"}, {"version": "v5", "created": "Mon, 9 Apr 2018 00:37:33 GMT"}, {"version": "v6", "created": "Wed, 14 Nov 2018 16:05:52 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Chen", "Wei", ""], ["Zhang", "Hanrui", ""]]}, {"id": "1702.05284", "submitter": "Lorenzo Severini", "authors": "Elisabetta Bergamini, Pierluigi Crescenzi, Gianlorenzo D'Angelo,\n  Henning Meyerhenke, Lorenzo Severini, Yllka Velaj", "title": "Improving the betweenness centrality of a node by adding links", "comments": "Accepted to ACM Journal of Experimental Algorithmics (JEA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Betweenness is a well-known centrality measure that ranks the nodes according\nto their participation in the shortest paths of a network. In several\nscenarios, having a high betweenness can have a positive impact on the node\nitself. Hence, in this paper we consider the problem of determining how much a\nvertex can increase its centrality by creating a limited amount of new edges\nincident to it. In particular, we study the problem of maximizing the\nbetweenness score of a given node -- Maximum Betweenness Improvement (MBI) --\nand that of maximizing the ranking of a given node -- Maximum Ranking\nImprovement (MRI). We show that MBI cannot be approximated in polynomial-time\nwithin a factor $(1-\\frac{1}{2e})$ and that MRI does not admit any\npolynomial-time constant factor approximation algorithm, both unless $P=NP$. We\nthen propose a simple greedy approximation algorithm for MBI with an almost\ntight approximation ratio and we test its performance on several real-world\nnetworks. We experimentally show that our algorithm highly increases both the\nbetweenness score and the ranking of a given node ant that it outperforms\nseveral competitive baselines. To speed up the computation of our greedy\nalgorithm, we also propose a new dynamic algorithm for updating the betweenness\nof one node after an edge insertion, which might be of independent interest.\nUsing the dynamic algorithm, we are now able to compute an approximation of MBI\non networks with up to $10^5$ edges in most cases in a matter of seconds or a\nfew minutes.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 10:00:58 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 17:59:07 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Bergamini", "Elisabetta", ""], ["Crescenzi", "Pierluigi", ""], ["D'Angelo", "Gianlorenzo", ""], ["Meyerhenke", "Henning", ""], ["Severini", "Lorenzo", ""], ["Velaj", "Yllka", ""]]}, {"id": "1702.05358", "submitter": "\\'Eric Colin de Verdi\\`ere", "authors": "\\'Eric Colin de Verdi\\`ere", "title": "Computational topology of graphs on surfaces", "comments": "To appear in the Handbook of Discrete and Computational Geometry, 3rd\n  edition. Minor changes compared to previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS math.AT math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational topology is an area that revisits topological problems from an\nalgorithmic point of view, and develops topological tools for improved\nalgorithms. We survey results in computational topology that are concerned with\ngraphs drawn on surfaces. Typical questions include representing surfaces and\ngraphs embedded on them computationally, deciding whether a graph embeds on a\nsurface, solving computational problems related to homotopy, optimizing curves\nand graphs on surfaces, and solving standard graph algorithm problems more\nefficiently in the case of surface-embedded graphs.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 14:34:08 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 14:57:55 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["de Verdi\u00e8re", "\u00c9ric Colin", ""]]}, {"id": "1702.05413", "submitter": "Julian Arz", "authors": "Julian Arz and Peter Sanders and Johannes Stegmaier and Ralf Mikut", "title": "3D Cell Nuclei Segmentation with Balanced Graph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell nuclei segmentation is one of the most important tasks in the analysis\nof biomedical images. With ever-growing sizes and amounts of three-dimensional\nimages to be processed, there is a need for better and faster segmentation\nmethods. Graph-based image segmentation has seen a rise in popularity in recent\nyears, but is seen as very costly with regard to computational demand. We\npropose a new segmentation algorithm which overcomes these limitations. Our\nmethod uses recursive balanced graph partitioning to segment foreground\ncomponents of a fast and efficient binarization. We construct a model for the\ncell nuclei to guide the partitioning process. Our algorithm is compared to\nother state-of-the-art segmentation algorithms in an experimental evaluation on\ntwo sets of realistically simulated inputs. Our method is faster, has similar\nor better quality and an acceptable memory overhead.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 16:01:50 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Arz", "Julian", ""], ["Sanders", "Peter", ""], ["Stegmaier", "Johannes", ""], ["Mikut", "Ralf", ""]]}, {"id": "1702.05425", "submitter": "Jonathan A. Marshall", "authors": "Jonathan A. Marshall, Lawrence C. Rafsky", "title": "Exact clustering in linear time", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time complexity of data clustering has been viewed as fundamentally\nquadratic, slowing with the number of data items, as each item is compared for\nsimilarity to preceding items. Clustering of large data sets has been\ninfeasible without resorting to probabilistic methods or to capping the number\nof clusters. Here we introduce MIMOSA, a novel class of algorithms which\nachieve linear time computational complexity on clustering tasks. MIMOSA\nalgorithms mark and match partial-signature keys in a hash table to obtain\nexact, error-free cluster retrieval. Benchmark measurements, on clustering a\ndata set of 10,000,000 news articles by news topic, found that a MIMOSA\nimplementation finished more than four orders of magnitude faster than a\nstandard centroid implementation.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 16:44:21 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 18:04:49 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Marshall", "Jonathan A.", ""], ["Rafsky", "Lawrence C.", ""]]}, {"id": "1702.05446", "submitter": "Arda Antikacioglu", "authors": "Arda Antikacioglu, R Ravi", "title": "Network Flow Based Post Processing for Sales Diversity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is a broad and powerful framework for building\nrecommendation systems that has seen widespread adoption. Over the past decade,\nthe propensity of such systems for favoring popular products and thus creating\necho chambers have been observed. This has given rise to an active area of\nresearch that seeks to diversify recommendations generated by such algorithms.\n  We address the problem of increasing diversity in recommendation systems that\nare based on collaborative filtering that use past ratings to predicting a\nrating quality for potential recommendations. Following our earlier work, we\nformulate recommendation system design as a subgraph selection problem from a\ncandidate super-graph of potential recommendations where both diversity and\nrating quality are explicitly optimized: (1) On the modeling side, we define a\nnew flexible notion of diversity that allows a system designer to prescribe the\nnumber of recommendations each item should receive, and smoothly penalizes\ndeviations from this distribution. (2) On the algorithmic side, we show that\nminimum-cost network flow methods yield fast algorithms in theory and practice\nfor designing recommendation subgraphs that optimize this notion of diversity.\n(3) On the empirical side, we show the effectiveness of our new model and\nmethod to increase diversity while maintaining high rating quality in standard\nrating data sets from Netflix and MovieLens.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 17:18:58 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Antikacioglu", "Arda", ""], ["Ravi", "R", ""]]}, {"id": "1702.05456", "submitter": "Janne H. Korhonen", "authors": "Sebastian Brandt, Juho Hirvonen, Janne H. Korhonen, Tuomo\n  Lempi\\\"ainen, Patric R. J. \\\"Osterg{\\aa}rd, Christopher Purcell, Joel\n  Rybicki, Jukka Suomela, Przemys{\\l}aw Uzna\\'nski", "title": "LCL problems on grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LCLs or locally checkable labelling problems (e.g. maximal independent set,\nmaximal matching, and vertex colouring) in the LOCAL model of computation are\nvery well-understood in cycles (toroidal 1-dimensional grids): every problem\nhas a complexity of $O(1)$, $\\Theta(\\log^* n)$, or $\\Theta(n)$, and the design\nof optimal algorithms can be fully automated.\n  This work develops the complexity theory of LCL problems for toroidal\n2-dimensional grids. The complexity classes are the same as in the\n1-dimensional case: $O(1)$, $\\Theta(\\log^* n)$, and $\\Theta(n)$. However, given\nan LCL problem it is undecidable whether its complexity is $\\Theta(\\log^* n)$\nor $\\Theta(n)$ in 2-dimensional grids.\n  Nevertheless, if we correctly guess that the complexity of a problem is\n$\\Theta(\\log^* n)$, we can completely automate the design of optimal\nalgorithms. For any problem we can find an algorithm that is of a normal form\n$A' \\circ S_k$, where $A'$ is a finite function, $S_k$ is an algorithm for\nfinding a maximal independent set in $k$th power of the grid, and $k$ is a\nconstant.\n  Finally, partially with the help of automated design tools, we classify the\ncomplexity of several concrete LCL problems related to colourings and\norientations.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 17:48:41 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 11:01:44 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Brandt", "Sebastian", ""], ["Hirvonen", "Juho", ""], ["Korhonen", "Janne H.", ""], ["Lempi\u00e4inen", "Tuomo", ""], ["\u00d6sterg\u00e5rd", "Patric R. J.", ""], ["Purcell", "Christopher", ""], ["Rybicki", "Joel", ""], ["Suomela", "Jukka", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1702.05570", "submitter": "Ramin Javadi", "authors": "Ramin Javadi and Saleh Ashkboos", "title": "Multi-way sparsest cut problem on trees with a control on the number of\n  parts and outliers", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph, the sparsest cut problem asks for a subset of vertices whose\nedge expansion (the normalized cut given by the subset) is minimized. In this\npaper, we study a generalization of this problem seeking for $ k $ disjoint\nsubsets of vertices (clusters) whose all edge expansions are small and\nfurthermore, the number of vertices remained in the exterior of the subsets\n(outliers) is also small. We prove that although this problem is $ NP-$hard for\ntrees, it can be solved in polynomial time for all weighted trees, provided\nthat we restrict the search space to subsets which induce connected subgraphs.\nThe proposed algorithm is based on dynamic programming and runs in the worst\ncase in $ O(k^2 n^3) $, when $ n $ is the number of vertices and $ k $ is the\nnumber of clusters. It also runs in linear time when the number of clusters and\nthe number of outliers is bounded by a constant.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 04:49:13 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Javadi", "Ramin", ""], ["Ashkboos", "Saleh", ""]]}, {"id": "1702.05589", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Pierre Bourhis, Louis Jachiet, Stefan Mengel", "title": "A Circuit-Based Approach to Efficient Enumeration", "comments": "45 pages, 1 figure, 36 references. Accepted at ICALP'17. This paper\n  is the full version with appendices of the article in the ICALP proceedings.\n  The main text of this full version is the same as the ICALP proceedings\n  version, except some superficial changes (to fit the proceedings version to\n  12 pages, and to obey LIPIcs-specific formatting requirements)", "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2017.111", "report-no": null, "categories": "cs.DS cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of enumerating the satisfying valuations of a circuit\nwhile bounding the delay, i.e., the time needed to compute each successive\nvaluation. We focus on the class of structured d-DNNF circuits originally\nintroduced in knowledge compilation, a sub-area of artificial intelligence. We\npropose an algorithm for these circuits that enumerates valuations with linear\npreprocessing and delay linear in the Hamming weight of each valuation.\nMoreover, valuations of constant Hamming weight can be enumerated with linear\npreprocessing and constant delay.\n  Our results yield a framework for efficient enumeration that applies to all\nproblems whose solutions can be compiled to structured d-DNNFs. In particular,\nwe use it to recapture classical results in database theory, for factorized\ndatabase representations and for MSO evaluation. This gives an independent\nproof of constant-delay enumeration for MSO formulae with first-order free\nvariables on bounded-treewidth structures.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 09:46:32 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 09:03:57 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Amarilli", "Antoine", ""], ["Bourhis", "Pierre", ""], ["Jachiet", "Louis", ""], ["Mengel", "Stefan", ""]]}, {"id": "1702.05597", "submitter": "Shuai Ma", "authors": "Xuelian Lin, Shuai Ma, Han Zhang, Tianyu Wo, Jinpeng Huai", "title": "One-Pass Error Bounded Trajectory Simplification", "comments": "published at the 43rd International Conference on Very Large Data\n  Bases (VLDB), Munich, Germany, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, various sensors are collecting, storing and transmitting tremendous\ntrajectory data, and it is known that raw trajectory data seriously wastes the\nstorage, network band and computing resource. Line simplification (LS)\nalgorithms are an effective approach to attacking this issue by compressing\ndata points in a trajectory to a set of continuous line segments, and are\ncommonly used in practice. However, existing LS algorithms are not sufficient\nfor the needs of sensors in mobile devices. In this study, we first develop a\none-pass error bounded trajectory simplification algorithm (OPERB), which scans\neach data point in a trajectory once and only once. We then propose an\naggressive one-pass error bounded trajectory simplification algorithm\n(OPERB-A), which allows interpolating new data points into a trajectory under\ncertain conditions. Finally, we experimentally verify that our approaches\n(OPERB and OPERB-A) are both efficient and effective, using four real-life\ntrajectory datasets.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 10:47:20 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Lin", "Xuelian", ""], ["Ma", "Shuai", ""], ["Zhang", "Han", ""], ["Wo", "Tianyu", ""], ["Huai", "Jinpeng", ""]]}, {"id": "1702.05626", "submitter": "Yi Li", "authors": "Yi Li and David P. Woodruff", "title": "Embeddings of Schatten Norms with Applications to Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an $n \\times d$ matrix $A$, its Schatten-$p$ norm, $p \\geq 1$, is\ndefined as $\\|A\\|_p = \\left (\\sum_{i=1}^{\\textrm{rank}(A)}\\sigma_i(A)^p \\right\n)^{1/p}$, where $\\sigma_i(A)$ is the $i$-th largest singular value of $A$.\nThese norms have been studied in functional analysis in the context of\nnon-commutative $\\ell_p$-spaces, and recently in data stream and linear\nsketching models of computation. Basic questions on the relations between these\nnorms, such as their embeddability, are still open. Specifically, given a set\nof matrices $A^1, \\ldots, A^{\\operatorname{poly}(nd)} \\in \\mathbb{R}^{n \\times\nd}$, suppose we want to construct a linear map $L$ such that $L(A^i) \\in\n\\mathbb{R}^{n' \\times d'}$ for each $i$, where $n' \\leq n$ and $d' \\leq d$, and\nfurther, $\\|A^i\\|_p \\leq \\|L(A^i)\\|_q \\leq D_{p,q} \\|A^i\\|_p$ for a given\napproximation factor $D_{p,q}$ and real number $q \\geq 1$. Then how large do\n$n'$ and $d'$ need to be as a function of $D_{p,q}$?\n  We nearly resolve this question for every $p, q \\geq 1$, for the case where\n$L(A^i)$ can be expressed as $R \\cdot A^i \\cdot S$, where $R$ and $S$ are\narbitrary matrices that are allowed to depend on $A^1, \\ldots, A^t$, that is,\n$L(A^i)$ can be implemented by left and right matrix multiplication. Namely,\nfor every $p, q \\geq 1$, we provide nearly matching upper and lower bounds on\nthe size of $n'$ and $d'$ as a function of $D_{p,q}$. Importantly, our upper\nbounds are {\\it oblivious}, meaning that $R$ and $S$ do not depend on the\n$A^i$, while our lower bounds hold even if $R$ and $S$ depend on the $A^i$. As\nan application of our upper bounds, we answer a recent open question of Blasiok\net al. about space-approximation trade-offs for the Schatten $1$-norm, showing\nin a data stream it is possible to estimate the Schatten-$1$ norm up to a\nfactor of $D \\geq 1$ using $\\tilde{O}(\\min(n,d)^2/D^4)$ space.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 15:47:39 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Li", "Yi", ""], ["Woodruff", "David P.", ""]]}, {"id": "1702.05678", "submitter": "Cl\\'ement Canonne", "authors": "Clement Canonne and Tom Gur", "title": "An Adaptivity Hierarchy Theorem for Property Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptivity is known to play a crucial role in property testing. In\nparticular, there exist properties for which there is an exponential gap\nbetween the power of \\emph{adaptive} testing algorithms, wherein each query may\nbe determined by the answers received to prior queries, and their\n\\emph{non-adaptive} counterparts, in which all queries are independent of\nanswers obtained from previous queries.\n  In this work, we investigate the role of adaptivity in property testing at a\nfiner level. We first quantify the degree of adaptivity of a testing algorithm\nby considering the number of \"rounds of adaptivity\" it uses. More accurately,\nwe say that a tester is $k$-(round) adaptive if it makes queries in $k+1$\nrounds, where the queries in the $i$'th round may depend on the answers\nobtained in the previous $i-1$ rounds. Then, we ask the following question:\n  Does the power of testing algorithms smoothly grow with the number of rounds\nof adaptivity?\n  We provide a positive answer to the foregoing question by proving an\nadaptivity hierarchy theorem for property testing. Specifically, our main\nresult shows that for every $n\\in \\mathbb{N}$ and $0 \\le k \\le n^{0.99}$ there\nexists a property $\\mathcal{P}_{n,k}$ of functions for which (1) there exists a\n$k$-adaptive tester for $\\mathcal{P}_{n,k}$ with query complexity\n$\\tilde{O}(k)$, yet (2) any $(k-1)$-adaptive tester for $\\mathcal{P}_{n,k}$\nmust make $\\Omega(n)$ queries. In addition, we show that such a qualitative\nadaptivity hierarchy can be witnessed for testing natural properties of graphs.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 00:03:09 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Canonne", "Clement", ""], ["Gur", "Tom", ""]]}, {"id": "1702.05710", "submitter": "Pallavi Jain", "authors": "Pallavi Jain, Gur Saran, Kamal Srivastava", "title": "Polynomial Time Efficient Construction Heuristics for Vertex Separation\n  Minimization Problem", "comments": "The paper will appear in the proceedings of International Conference\n  on Current Trends in Graph Theory and Computation which will be published in\n  Electronic Notes on Discrete Mathematics (ENDM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vertex Separation Minimization Problem (VSMP) consists of finding a layout of\na graph G = (V,E) which minimizes the maximum vertex cut or separation of a\nlayout. It is an NP-complete problem in general for which metaheuristic\ntechniques can be applied to find near optimal solution. VSMP has applications\nin VLSI design, graph drawing and computer language compiler design. VSMP is\npolynomially solvable for grids, trees, permutation graphs and cographs.\nConstruction heuristics play a very important role in the metaheuristic\ntechniques as they are responsible for generating initial solutions which lead\nto fast convergence. In this paper, we have proposed three construction\nheuristics H1, H2 and H3 and performed experiments on Grids, Small graphs,\nTrees and Harwell Boeing graphs, totaling 248 instances of graphs. Experiments\nreveal that H1, H2 and H3 are able to achieve best results for 88.71%, 43.5%\nand 37.1% of the total instances respectively while the best construction\nheuristic in the literature achieves the best solution for 39.9% of the total\ninstances. We have also compared the results with the state-of-the-art\nmetaheuristic GVNS and observed that the proposed construction heuristics\nimproves the results for some of the input instances. It was found that GVNS\nobtained best results for 82.9% instances of all input instances and the\nheuristic H1 obtained best results for 82.3% of all input instances.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 07:19:52 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Jain", "Pallavi", ""], ["Saran", "Gur", ""], ["Srivastava", "Kamal", ""]]}, {"id": "1702.05760", "submitter": "Thijs Laarhoven", "authors": "Thijs Laarhoven", "title": "Hypercube LSH for approximate near neighbors", "comments": "18 pages, 4 figures", "journal-ref": "42nd International Symposium on Mathematical Foundations of\n  Computer Science (MFCS 2017), pp. 7:1-7:20, 2017", "doi": "10.4230/LIPIcs.MFCS.2017.7", "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A celebrated technique for finding near neighbors for the angular distance\ninvolves using a set of \\textit{random} hyperplanes to partition the space into\nhash regions [Charikar, STOC 2002]. Experiments later showed that using a set\nof \\textit{orthogonal} hyperplanes, thereby partitioning the space into the\nVoronoi regions induced by a hypercube, leads to even better results [Terasawa\nand Tanaka, WADS 2007]. However, no theoretical explanation for this\nimprovement was ever given, and it remained unclear how the resulting hypercube\nhash method scales in high dimensions.\n  In this work, we provide explicit asymptotics for the collision probabilities\nwhen using hypercubes to partition the space. For instance, two near-orthogonal\nvectors are expected to collide with probability $(\\frac{1}{\\pi})^{d + o(d)}$\nin dimension $d$, compared to $(\\frac{1}{2})^d$ when using random hyperplanes.\nVectors at angle $\\frac{\\pi}{3}$ collide with probability\n$(\\frac{\\sqrt{3}}{\\pi})^{d + o(d)}$, compared to $(\\frac{2}{3})^d$ for random\nhyperplanes, and near-parallel vectors collide with similar asymptotic\nprobabilities in both cases.\n  For $c$-approximate nearest neighbor searching, this translates to a decrease\nin the exponent $\\rho$ of locality-sensitive hashing (LSH) methods of a factor\nup to $\\log_2(\\pi) \\approx 1.652$ compared to hyperplane LSH. For $c = 2$, we\nobtain $\\rho \\approx 0.302 + o(1)$ for hypercube LSH, improving upon the $\\rho\n\\approx 0.377$ for hyperplane LSH. We further describe how to use hypercube LSH\nin practice, and we consider an example application in the area of lattice\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 15:48:11 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Laarhoven", "Thijs", ""]]}, {"id": "1702.05763", "submitter": "Maurice Chandoo", "authors": "Maurice Chandoo", "title": "Canonical Representations for Circular-Arc Graphs Using Flip Sets", "comments": "24 pages", "journal-ref": null, "doi": "10.1007/s00453-018-0410-0", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that computing canonical representations for circular-arc (CA) graphs\nreduces to computing certain subsets of vertices called flip sets. For a broad\nclass of CA graphs, which we call uniform, it suffices to compute a CA\nrepresentation to find such flip sets. As a consequence canonical\nrepresentations for uniform CA graphs can be obtained in polynomial-time. We\nthen investigate what kind of CA graphs pose a challenge to this approach. This\nleads us to introduce the notion of restricted CA matrices and show that the\ncanonical representation problem for CA graphs is logspace-reducible to that of\nrestricted CA matrices. As a byproduct, we obtain the result that CA graphs\nwithout induced 4-cycles can be canonized in logspace.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 16:09:28 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 18:55:59 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Chandoo", "Maurice", ""]]}, {"id": "1702.05805", "submitter": "Ohad Trabelsi", "authors": "Robert Krauthgamer and Ohad Trabelsi", "title": "Conditional Lower Bounds for All-Pairs Max-Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide evidence that computing the maximum flow value between every pair\nof nodes in a directed graph on $n$ nodes, $m$ edges,and capacities in the\nrange $[1..n]$, which we call the All-Pairs Max-Flow problem, cannot be solved\nin time that is significantly faster (i.e., by a polynomial factor) than\n$O(n^3)$ even for sparse graphs. Since a single maximum $st$-flow can be solved\nin time $\\tilde{O}(m\\sqrt{n})$ [Lee and Sidford, FOCS 2014], we conclude that\nthe all-pairs version might require time equivalent to $\\tilde\\Omega(n^{3/2})$\ncomputations of maximum $st$-flow,which strongly separates the directed case\nfrom the undirected one. Moreover, if maximum $st$-flow can be solved in time\n$\\tilde{O}(m)$,then the runtime of $\\tilde\\Omega(n^2)$ computations is needed.\nThe latter settles a conjecture of Lacki, Nussbaum, Sankowski, and Wulf-Nilsen\n[FOCS 2012] negatively.\n  Specifically, we show that in sparse graphs $G=(V,E,w)$, if one can compute\nthe maximum $st$-flow from every $s$ in an input set of sources $S\\subseteq V$\nto every $t$ in an input set of sinks $T\\subseteq V$ in time $O((|S| |T|\nm)^{1-\\epsilon})$,for some $|S|$, $|T|$, and a constant $\\epsilon>0$,then\nMAX-CNF-SAT with $n'$ variables and $m'$ clauses can be solved in time\n${m'}^{O(1)}2^{(1-\\delta)n'}$ for a constant $\\delta(\\epsilon)>0$,a problem for\nwhich not even $2^{n'}/poly(n')$ algorithms are known. Such runtime for\nMAX-CNF-SAT would in particular refute the Strong Exponential Time Hypothesis\n(SETH). Hence, we improve the lower bound of Abboud, Vassilevska-Williams, and\nYu [STOC 2015], who showed that for every fixed $\\epsilon>0$ and\n$|S|=|T|=O(\\sqrt{n})$, if the above problem can be solved in time\n$O(n^{3/2-\\epsilon})$, then some incomparable conjecture is false. Furthermore,\na larger lower bound than ours implies strictly super-linear time for maximum\n$st$-flow problem, which would be an amazing breakthrough.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 22:04:58 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 22:17:52 GMT"}, {"version": "v3", "created": "Sun, 4 Feb 2018 17:14:43 GMT"}, {"version": "v4", "created": "Sun, 6 May 2018 00:49:31 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Krauthgamer", "Robert", ""], ["Trabelsi", "Ohad", ""]]}, {"id": "1702.05860", "submitter": "Jerry Li", "authors": "Jerry Li", "title": "Robust Sparse Estimation Tasks in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we initiate the study of whether or not sparse estimation tasks\ncan be performed efficiently in high dimensions, in the robust setting where an\n$\\eps$-fraction of samples are corrupted adversarially. We study the natural\nrobust version of two classical sparse estimation problems, namely, sparse mean\nestimation and sparse PCA in the spiked covariance model. For both of these\nproblems, we provide the first efficient algorithms that provide non-trivial\nerror guarantees in the presence of noise, using only a number of samples which\nis similar to the number required for these problems without noise. In\nparticular, our sample complexities are sublinear in the ambient dimension $d$.\nOur work also suggests evidence for new computational-vs-statistical gaps for\nthese problems (similar to those for sparse PCA without noise) which only arise\nin the presence of noise.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 05:22:55 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 20:49:30 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Li", "Jerry", ""]]}, {"id": "1702.05888", "submitter": "Thalaiyasingam Ajanthan", "authors": "Thalaiyasingam Ajanthan, Richard Hartley and Mathieu Salzmann", "title": "Memory Efficient Max Flow for Multi-label Submodular MRFs", "comments": "15 Pages, 13 Figures and 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label submodular Markov Random Fields (MRFs) have been shown to be\nsolvable using max-flow based on an encoding of the labels proposed by\nIshikawa, in which each variable $X_i$ is represented by $\\ell$ nodes (where\n$\\ell$ is the number of labels) arranged in a column. However, this method in\ngeneral requires $2\\,\\ell^2$ edges for each pair of neighbouring variables.\nThis makes it inapplicable to realistic problems with many variables and\nlabels, due to excessive memory requirement. In this paper, we introduce a\nvariant of the max-flow algorithm that requires much less storage.\nConsequently, our algorithm makes it possible to optimally solve multi-label\nsubmodular problems involving large numbers of variables and labels on a\nstandard computer.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 08:09:09 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Ajanthan", "Thalaiyasingam", ""], ["Hartley", "Richard", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1702.05932", "submitter": "Stefan Kratsch", "authors": "Yann Disser and Stefan Kratsch", "title": "Robust and adaptive search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary search finds a given element in a sorted array with an optimal number\nof $\\log n$ queries. However, binary search fails even when the array is only\nslightly disordered or access to its elements is subject to errors. We study\nthe worst-case query complexity of search algorithms that are robust to\nimprecise queries and that adapt to perturbations of the order of the elements.\nWe give (almost) tight results for various parameters that quantify query\nerrors and that measure array disorder. In particular, we exhibit settings\nwhere query complexities of $\\log n + ck$, $(1+\\varepsilon)\\log n + ck$, and\n$\\sqrt{cnk}+o(nk)$ are best-possible for parameter value $k$, any\n$\\varepsilon>0$, and constant $c$.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 11:16:35 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Disser", "Yann", ""], ["Kratsch", "Stefan", ""]]}, {"id": "1702.05951", "submitter": "Havana Rika", "authors": "Robert Krauthgamer and Havana (Inbal) Rika", "title": "Refined Vertex Sparsifiers of Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following version of cut sparsification. Given a large\nedge-weighted network $G$ with $k$ terminal vertices, compress it into a\nsmaller network $H$ with the same terminals, such that every minimum terminal\ncut in $H$ approximates the corresponding one in $G$, up to a factor $q\\geq 1$\nthat is called the quality. (The case $q=1$ is known also as a mimicking\nnetwork). We provide new insights about the structure of minimum terminal cuts,\nleading to new results for cut sparsifiers of planar graphs. Our first\ncontribution identifies a subset of the minimum terminal cuts, which we call\nelementary, that generates all the others. Consequently, $H$ is a cut\nsparsifier if and only if it preserves all the elementary terminal cuts (up to\nthis factor $q$). This structural characterization lead to improved bounds on\nthe size of $H$. For example, it improve the bound of mimicking-network size\nfor planar graphs into a near-optimal one. Our second and main contribution is\nto refine the known bounds in terms of $\\gamma=\\gamma(G)$, which is defined as\nthe minimum number of faces that are incident to all the terminals in a planar\ngraph $G$. We prove that the number of elementary terminal cuts is\n$O((2k/\\gamma)^{2\\gamma})$ (compared to $O(2^k)$ terminal cuts), and\nfurthermore obtain a mimicking-network of size $O(\\gamma 2^{2\\gamma} k^4)$,\nwhich is near-optimal as a function of $\\gamma$. In the analysis we break the\nelementary terminal cuts into fragments, and count them carefully. Our third\ncontribution is a duality between cut sparsification and distance\nsparsification for certain planar graphs, when the sparsifier $H$ is required\nto be a minor of $G$. This duality connects problems that were previously\nstudied separately, implying new results, new proofs of known results, and\nequivalences between open gaps.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 12:42:07 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 12:37:48 GMT"}, {"version": "v3", "created": "Fri, 4 Oct 2019 19:01:22 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Krauthgamer", "Robert", "", "Inbal"], ["Havana", "", "", "Inbal"], ["Rika", "", ""]]}, {"id": "1702.06087", "submitter": "Nicolas Kourtellis Ph.D.", "authors": "Nicolas Kourtellis, Tharaka Alahakoon, Ramanuja Simha, Adriana\n  Iamnitchi, Rahul Tripathi", "title": "Identifying high betweenness centrality nodes in large social networks", "comments": "16 pages, 10 figures, 2 tables", "journal-ref": "Springer Journal on Social Network Analysis and Mining, Volume 3,\n  Issue 4, pp. 899-914 (December 2013)", "doi": "10.1007/s13278-012-0076-6", "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an alternative way to identify nodes with high\nbetweenness centrality. It introduces a new metric, k-path centrality, and a\nrandomized algorithm for estimating it, and shows empirically that nodes with\nhigh k-path centrality have high node betweenness centrality. The randomized\nalgorithm runs in time $O(\\kappa^{3}n^{2-2\\alpha}\\log n)$ and outputs, for each\nvertex v, an estimate of its k-path centrality up to additive error of $\\pm\nn^{1/2+ \\alpha}$ with probability $1-1/n^2$. Experimental evaluations on real\nand synthetic social networks show improved accuracy in detecting high\nbetweenness centrality nodes and significantly reduced execution time when\ncompared with existing randomized algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 18:06:42 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 10:44:45 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Kourtellis", "Nicolas", ""], ["Alahakoon", "Tharaka", ""], ["Simha", "Ramanuja", ""], ["Iamnitchi", "Adriana", ""], ["Tripathi", "Rahul", ""]]}, {"id": "1702.06095", "submitter": "O-Joung Kwon", "authors": "Benjamin Bergougnoux and Mamadou Moustapha Kant\\'e and O-joung Kwon", "title": "An optimal XP algorithm for Hamiltonian cycle on graphs of bounded\n  clique-width", "comments": "17 page, 3 figure, Full version of the accepted paper in WADS 2017,\n  corrected the characterization of red-blue Eulerian digraphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove that, given a clique-width $k$-expression of an\n$n$-vertex graph, \\textsc{Hamiltonian Cycle} can be solved in time\n$n^{\\mathcal{O}(k)}$. This improves the naive algorithm that runs in time\n$n^{\\mathcal{O}(k^2)}$ by Espelage et al. (WG 2001), and it also matches with\nthe lower bound result by Fomin et al. that, unless the Exponential Time\nHypothesis fails, there is no algorithm running in time $n^{o(k)}$ (SIAM. J.\nComputing 2014).\n  We present a technique of representative sets using two-edge colored\nmultigraphs on $k$ vertices. The essential idea is that, for a two-edge colored\nmultigraph, the existence of an Eulerian trail that uses edges with different\ncolors alternately can be determined by two information: the number of colored\nedges incident with each vertex, and the connectedness of the multigraph. With\nthis idea, we avoid the bottleneck of the naive algorithm, which stores all the\npossible multigraphs on $k$ vertices with at most $n$ edges.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 18:33:11 GMT"}, {"version": "v2", "created": "Sun, 12 Aug 2018 16:19:11 GMT"}, {"version": "v3", "created": "Sun, 9 Jun 2019 16:45:10 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Bergougnoux", "Benjamin", ""], ["Kant\u00e9", "Mamadou Moustapha", ""], ["Kwon", "O-joung", ""]]}, {"id": "1702.06099", "submitter": "Christian Konrad", "authors": "Christian Konrad and Tigran Tonoyan", "title": "Preemptive Online Partitioning of Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online algorithms process their inputs piece by piece, taking irrevocable\ndecisions for each data item. This model is too restrictive for most\npartitioning problems, since data that is yet to arrive may render it\nimpossible to extend partial partitionings to the entire data set reasonably\nwell.\n  In this work, we show that preemption might be a potential remedy. We\nconsider the problem of partitioning online sequences, where $p-1$ separators\nneed to be inserted into a sequence of integers that arrives online so as to\ncreate $p$ contiguous partitions of similar weight. While without preemption no\nalgorithm with non-trivial competitive ratio is possible, if preemption is\nallowed, i.e., inserted partition separators may be removed but not reinserted\nagain, then we show that constant competitive algorithms can be obtained. Our\ncontributions include:\n  We first give a simple deterministic $2$-competitive preemptive algorithm for\narbitrary $p$ and arbitrary sequences. Our main contribution is the design of a\nhighly non-trivial partitioning scheme, which, under some natural conditions\nand $p$ being a power of two, allows us to improve the competitiveness to\n$1.68$. We also show that the competitiveness of deterministic (randomized)\nalgorithms is at least $\\frac{4}{3}$ (resp. $\\frac{6}{5}$).\n  For $p=2$, the problem corresponds to the interesting special case of\npreemptively guessing the center of a weighted request sequence. While\ndeterministic algorithms fail here, we provide a randomized $1.345$-competitive\nalgorithm for all-ones sequences and prove that this is optimal. For weighted\nsequences, we give a $1.628$-competitive algorithm and a lower bound of $1.5$.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 18:36:25 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Konrad", "Christian", ""], ["Tonoyan", "Tigran", ""]]}, {"id": "1702.06110", "submitter": "Pavel Kolev", "authors": "Gorav Jindal, Pavel Kolev, Richard Peng and Saurabh Sawlani", "title": "Density Independent Algorithms for Sparsifying $k$-Step Random Walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give faster algorithms for producing sparse approximations of the\ntransition matrices of $k$-step random walks on undirected, weighted graphs.\nThese transition matrices also form graphs, and arise as intermediate objects\nin a variety of graph algorithms. Our improvements are based on a better\nunderstanding of processes that sample such walks, as well as tighter bounds on\nkey weights underlying these sampling processes. On a graph with $n$ vertices\nand $m$ edges, our algorithm produces a graph with about $n\\log{n}$ edges that\napproximates the $k$-step random walk graph in about $m + n \\log^4{n}$ time. In\norder to obtain this runtime bound, we also revisit \"density independent\"\nalgorithms for sparsifying graphs whose runtime overhead is expressed only in\nterms of the number of vertices.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 18:50:26 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Jindal", "Gorav", ""], ["Kolev", "Pavel", ""], ["Peng", "Richard", ""], ["Sawlani", "Saurabh", ""]]}, {"id": "1702.06121", "submitter": "Andreas Alpers", "authors": "Andreas Alpers and Peter Gritzmann", "title": "Reconstructing binary matrices under window constraints from their row\n  and column sums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper deals with the discrete inverse problem of reconstructing\nbinary matrices from their row and column sums under additional constraints on\nthe number and pattern of entries in specified minors. While the classical\nconsistency and reconstruction problems for two directions in discrete\ntomography can be solved in polynomial time, it turns out that these window\nconstraints cause various unexpected complexity jumps back and forth from\npolynomial-time solvability to $\\mathbb{N}\\mathbb{P}$-hardness.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 14:51:14 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Alpers", "Andreas", ""], ["Gritzmann", "Peter", ""]]}, {"id": "1702.06237", "submitter": "David Steurer", "authors": "Aaron Potechin, David Steurer", "title": "Exact tensor completion with sum-of-squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain the first polynomial-time algorithm for exact tensor completion\nthat improves over the bound implied by reduction to matrix completion. The\nalgorithm recovers an unknown 3-tensor with $r$ incoherent, orthogonal\ncomponents in $\\mathbb R^n$ from $r\\cdot \\tilde O(n^{1.5})$ randomly observed\nentries of the tensor. This bound improves over the previous best one of\n$r\\cdot \\tilde O(n^{2})$ by reduction to exact matrix completion. Our bound\nalso matches the best known results for the easier problem of approximate\ntensor completion (Barak & Moitra, 2015).\n  Our algorithm and analysis extends seminal results for exact matrix\ncompletion (Candes & Recht, 2009) to the tensor setting via the sum-of-squares\nmethod. The main technical challenge is to show that a small number of randomly\nchosen monomials are enough to construct a degree-3 polynomial with precisely\nplanted orthogonal global optima over the sphere and that this fact can be\ncertified within the sum-of-squares proof system.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 02:14:31 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 02:48:22 GMT"}, {"version": "v3", "created": "Fri, 23 Jun 2017 18:24:50 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Potechin", "Aaron", ""], ["Steurer", "David", ""]]}, {"id": "1702.06256", "submitter": "Guohui Lin", "authors": "Yao Xu, Yong Chen, Guohui Lin, Tian Liu, Taibo Luo and Peng Zhang", "title": "A $(1.4 + \\epsilon)$-approximation algorithm for the $2$-Max-Duo problem", "comments": "14 pages, 10 figures; an extended abstract appears in Proceedings of\n  the 28th International Symposium on Algorithms and Computation (ISAAC 2017).\n  LIPICS 92, Article No. 66, pp. 66:1--66:12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum duo-preservation string mapping (Max-Duo) problem is the\ncomplement of the well studied minimum common string partition (MCSP) problem,\nboth of which have applications in many fields including text compression and\nbioinformatics. $k$-Max-Duo is the restricted version of Max-Duo, where every\nletter of the alphabet occurs at most $k$ times in each of the strings, which\nis readily reduced into the well known maximum independent set (MIS) problem on\na graph of maximum degree $\\Delta \\le 6(k-1)$. In particular, $2$-Max-Duo can\nthen be approximated arbitrarily close to $1.8$ using the state-of-the-art\napproximation algorithm for the MIS problem. $2$-Max-Duo was proved APX-hard\nand very recently a $(1.6 + \\epsilon)$-approximation was claimed, for any\n$\\epsilon > 0$. In this paper, we present a vertex-degree reduction technique,\nbased on which, we show that $2$-Max-Duo can be approximated arbitrarily close\nto $1.4$.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 04:06:15 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 22:04:04 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Xu", "Yao", ""], ["Chen", "Yong", ""], ["Lin", "Guohui", ""], ["Liu", "Tian", ""], ["Luo", "Taibo", ""], ["Zhang", "Peng", ""]]}, {"id": "1702.06298", "submitter": "Md Saiful Islam", "authors": "Md. Saiful Islam, Wenny Rahayu, Chengfei Liu, Tarique Anwar and Bela\n  Stantic", "title": "Computing Influence of a Product through Uncertain Reverse Skyline", "comments": "12 pages, 3 tables, 12 figures, submitted to SSDBM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the influence of a product is crucially important for making\ninformed business decisions. This paper introduces a new type of skyline\nqueries, called uncertain reverse skyline, for measuring the influence of a\nprobabilistic product in uncertain data settings. More specifically, given a\ndataset of probabilistic products P and a set of customers C, an uncertain\nreverse skyline of a probabilistic product q retrieves all customers c in C\nwhich include q as one of their preferred products. We present efficient\npruning ideas and techniques for processing the uncertain reverse skyline query\nof a probabilistic product using R-Tree data index. We also present an\nefficient parallel approach to compute the uncertain reverse skyline and\ninfluence score of a probabilistic product. Our approach significantly\noutperforms the baseline approach derived from the existing literature. The\nefficiency of our approach is demonstrated by conducting extensive experiments\nwith both real and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 09:06:04 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Islam", "Md. Saiful", ""], ["Rahayu", "Wenny", ""], ["Liu", "Chengfei", ""], ["Anwar", "Tarique", ""], ["Stantic", "Bela", ""]]}, {"id": "1702.06364", "submitter": "Mathias Weller", "authors": "Mathias Weller", "title": "Linear-Time Tree Containment in Phylogenetic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the NP-hard Tree Containment problem that has important\napplications in phylogenetics. The problem asks if a given leaf-labeled network\ncontains a subdivision of a given leaf-labeled tree. We develop a fast\nalgorithm for the case that the input network is indeed a tree in which\nmultiple leaves might share a label. By combining this algorithm with a\ngeneralization of a previously known decomposition scheme, we improve the\nrunning time on reticulation visible networks and nearly stable networks to\nlinear time. While these are special classes of networks, they rank among the\nmost general of the previously considered classes.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 13:06:59 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Weller", "Mathias", ""]]}, {"id": "1702.06365", "submitter": "Florian Zwicke", "authors": "Florian Zwicke, Philipp Knechtges, Marek Behr, Stefanie Elgeti", "title": "Automatic implementation of material laws: Jacobian calculation in a\n  finite element code with TAPENADE", "comments": "17 pages, 9 figures", "journal-ref": "Computers & Mathematics with Applications, 72 (2016) 2808-2822", "doi": "10.1016/j.camwa.2016.10.010", "report-no": null, "categories": "cs.NA cs.DS physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an effort to increase the versatility of finite element codes, we explore\nthe possibility of automatically creating the Jacobian matrix necessary for the\ngradient-based solution of nonlinear systems of equations. Particularly, we aim\nto assess the feasibility of employing the automatic differentiation tool\nTAPENADE for this purpose on a large Fortran codebase that is the result of\nmany years of continuous development. As a starting point we will describe the\nspecial structure of finite element codes and the implications that this code\ndesign carries for an efficient calculation of the Jacobian matrix. We will\nalso propose a first approach towards improving the efficiency of such a\nmethod. Finally, we will present a functioning method for the automatic\nimplementation of the Jacobian calculation in a finite element software, but\nwill also point out important shortcomings that will have to be addressed in\nthe future.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 13:08:06 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Zwicke", "Florian", ""], ["Knechtges", "Philipp", ""], ["Behr", "Marek", ""], ["Elgeti", "Stefanie", ""]]}, {"id": "1702.06370", "submitter": "Christoph Berkholz", "authors": "Christoph Berkholz, Jens Keppeler, Nicole Schweikardt", "title": "Answering Conjunctive Queries under Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of enumerating and counting answers to $k$-ary\nconjunctive queries against relational databases that may be updated by\ninserting or deleting tuples. We exhibit a new notion of q-hierarchical\nconjunctive queries and show that these can be maintained efficiently in the\nfollowing sense. During a linear time preprocessing phase, we can build a data\nstructure that enables constant delay enumeration of the query results; and\nwhen the database is updated, we can update the data structure and restart the\nenumeration phase within constant time. For the special case of self-join free\nconjunctive queries we obtain a dichotomy: if a query is not q-hierarchical,\nthen query enumeration with sublinear$^\\ast$ delay and sublinear update time\n(and arbitrary preprocessing time) is impossible.\n  For answering Boolean conjunctive queries and for the more general problem of\ncounting the number of solutions of k-ary queries we obtain complete\ndichotomies: if the query's homomorphic core is q-hierarchical, then size of\nthe the query result can be computed in linear time and maintained with\nconstant update time. Otherwise, the size of the query result cannot be\nmaintained with sublinear update time. All our lower bounds rely on the\nOMv-conjecture, a conjecture on the hardness of online matrix-vector\nmultiplication that has recently emerged in the field of fine-grained\ncomplexity to characterise the hardness of dynamic problems. The lower bound\nfor the counting problem additionally relies on the orthogonal vectors\nconjecture, which in turn is implied by the strong exponential time hypothesis.\n  $^\\ast)$ By sublinear we mean $O(n^{1-\\varepsilon})$ for some\n$\\varepsilon>0$, where $n$ is the size of the active domain of the current\ndatabase.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 13:15:27 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Berkholz", "Christoph", ""], ["Keppeler", "Jens", ""], ["Schweikardt", "Nicole", ""]]}, {"id": "1702.06503", "submitter": "Till Fluschnik", "authors": "Till Fluschnik, Christian Komusiewicz, George B. Mertzios, Andr\\'e\n  Nichterlein, Rolf Niedermeier, and Nimrod Talmon", "title": "When can Graph Hyperbolicity be computed in Linear Time?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperbolicity measures, in terms of (distance) metrics, how close a given\ngraph is to being a tree. Due to its relevance in modeling real-world networks,\nhyperbolicity has seen intensive research over the last years. Unfortunately,\nthe best known algorithms for computing the hyperbolicity number of a graph\n(the smaller, the more tree-like) have running time $O(n^4)$, where $n$ is the\nnumber of graph vertices. Exploiting the framework of parameterized complexity\nanalysis, we explore possibilities for \"linear-time FPT\" algorithms to compute\nhyperbolicity. For instance, we show that hyperbolicity can be computed in time\n$O(2^{O(k)} + n +m)$ ($m$ being the number of graph edges) while at the same\ntime, unless the SETH fails, there is no $2^{o(k)}n^2$-time algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 18:13:40 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Fluschnik", "Till", ""], ["Komusiewicz", "Christian", ""], ["Mertzios", "George B.", ""], ["Nichterlein", "Andr\u00e9", ""], ["Niedermeier", "Rolf", ""], ["Talmon", "Nimrod", ""]]}, {"id": "1702.06548", "submitter": "Matthias Bentert", "authors": "Matthias Bentert, Till Fluschnik, Andr\\'e Nichterlein, Rolf\n  Niedermeier", "title": "Parameterized Aspects of Triangle Enumeration", "comments": "Appeared at FCT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of listing all triangles in an undirected graph is a fundamental\ngraph primitive with numerous applications. It is trivially solvable in time\ncubic in the number of vertices. It has seen a significant body of work\ncontributing to both theoretical aspects (e.g., lower and upper bounds on\nrunning time, adaption to new computational models) as well as practical\naspects (e.g. algorithms tuned for large graphs). Motivated by the fact that\nthe worst-case running time is cubic, we perform a systematic parameterized\ncomplexity study of triangle enumeration. We provide both positive results (new\nenumerative kernelizations, \"subcubic\" parameterized solving algorithms) as\nwell as negative results (presumable uselessness in terms of \"faster\"\nparameterized algorithms of certain parameters such as graph diameter). To this\nend, we introduce new and extend previous concepts.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 19:02:49 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 13:02:00 GMT"}, {"version": "v3", "created": "Tue, 19 Dec 2017 17:13:55 GMT"}, {"version": "v4", "created": "Fri, 21 Dec 2018 14:27:28 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Bentert", "Matthias", ""], ["Fluschnik", "Till", ""], ["Nichterlein", "Andr\u00e9", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "1702.06614", "submitter": "Ross M. McConnell", "authors": "Peter Hamburger and Ross M. McConnell and Attila P\\'or and Jeremy P.\n  Spinrad", "title": "Double Threshold Digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A semiorder is a model of preference relations where each element $x$ is\nassociated with a utility value $\\alpha(x)$, and there is a threshold $t$ such\nthat $y$ is preferred to $x$ iff $\\alpha(y) > \\alpha(x)+t$. These are motivated\nby the notion that there is some uncertainty in the utility values we assign an\nobject or that a subject may be unable to distinguish a preference between\nobjects whose values are close. However, they fail to model the well-known\nphenomenon that preferences are not always transitive. Also, if we are\nuncertain of the utility values, it is not logical that preference is\ndetermined absolutely by a comparison of them with an exact threshold. We\npropose a new model in which there are two thresholds, $t_1$ and $t_2$; if the\ndifference $\\alpha(y) - \\alpha(x)$ less than $t_1$, then $y$ is not preferred\nto $x$; if the difference is greater than $t_2$ then $y$ is preferred to $x$;\nif it is between $t_1$ and $t_2$, then then $y$ may or may not be preferred to\n$x$. We call such a relation a double-threshold semiorder, and the\ncorresponding directed graph $G = (V,E)$ a double threshold digraph. Every\ndirected acyclic graph is a double threshold graph; bounds on $t_2/t_1$ give a\nnested hierarchy of subclasses of the directed acyclic graphs. In this paper we\ncharacterize the subclasses in terms of forbidden subgraphs, and give\nalgorithms for finding an assignment of of utility values that explains the\nrelation in terms of a given $(t_1,t_2)$ or else produces a forbidden subgraph,\nand finding the minimum value $\\lambda$ of $t_2/t_1$ that is satisfiable for a\ngiven directed acyclic graph. We show that $\\lambda$ gives a measure of the\ncomplexity of a directed acyclic graph with respect to several optimization\nproblems that are NP-hard on arbitrary directed acyclic graphs.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 23:02:59 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 17:03:13 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Hamburger", "Peter", ""], ["McConnell", "Ross M.", ""], ["P\u00f3r", "Attila", ""], ["Spinrad", "Jeremy P.", ""]]}, {"id": "1702.06723", "submitter": "Hans Raj Tiwary", "authors": "David Avis and Hans Raj Tiwary", "title": "Compact linear programs for 2SAT", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For each integer $n$ we present an explicit formulation of a compact linear\nprogram, with $O(n^3)$ variables and constraints, which determines the\nsatisfiability of any 2SAT formula with $n$ boolean variables by a single\nlinear optimization. This contrasts with the fact that the natural polytope for\nthis problem, formed from the convex hull of all satisfiable formulas and their\nsatisfying assignments, has superpolynomial extension complexity. Our\nformulation is based on multicommodity flows. We also discuss connections of\nthese results to the stable matching problem.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 09:35:30 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 08:46:40 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Avis", "David", ""], ["Tiwary", "Hans Raj", ""]]}, {"id": "1702.06829", "submitter": "Raimi Rufai", "authors": "Raimi A. Rufai and Dana S. Richards", "title": "A Simple Convex Layers Algorithm", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of $n$ points $P$ in the plane, the first layer $L_1$ of $P$ is\nformed by the points that appear on $P$'s convex hull. In general, a point\nbelongs to layer $L_i$, if it lies on the convex hull of the set $P \\setminus\n\\bigcup_{j<i}\\{L_j\\}$. The \\emph{convex layers problem} is to compute the\nconvex layers $L_i$. Existing algorithms for this problem either do not achieve\nthe optimal $\\mathcal{O}\\left(n\\log n\\right)$ runtime and linear space, or are\noverly complex and difficult to apply in practice. We propose a new algorithm\nthat is both optimal and simple. The simplicity is achieved by independently\ncomputing four sets of monotone convex chains in $\\mathcal{O}\\left(n\\log\nn\\right)$ time and linear space. These are then merged in\n$\\mathcal{O}\\left(n\\log n\\right)$ time.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 15:03:08 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 14:38:10 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Rufai", "Raimi A.", ""], ["Richards", "Dana S.", ""]]}, {"id": "1702.06844", "submitter": "Martin Koutecky", "authors": "Jakub Gajarsk\\'y, Petr Hlin\\v{e}n\\'y, Martin Kouteck\\'y, Shmuel Onn", "title": "Parameterized Shifted Combinatorial Optimization", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcss.2018.06.002", "report-no": null, "categories": "cs.CC cs.DM cs.DS math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shifted combinatorial optimization is a new nonlinear optimization framework\nwhich is a broad extension of standard combinatorial optimization, involving\nthe choice of several feasible solutions at a time. This framework captures\nwell studied and diverse problems ranging from so-called vulnerability problems\nto sharing and partitioning problems. In particular, every standard\ncombinatorial optimization problem has its shifted counterpart, which is\ntypically much harder. Already with explicitly given input set the shifted\nproblem may be NP-hard. In this article we initiate a study of the\nparameterized complexity of this framework. First we show that shifting over an\nexplicitly given set with its cardinality as the parameter may be in XP, FPT or\nP, depending on the objective function. Second, we study the shifted problem\nover sets definable in MSO logic (which includes, e.g., the well known MSO\npartitioning problems). Our main results here are that shifted combinatorial\noptimization over MSO definable sets is in XP with respect to the MSO formula\nand the treewidth (or more generally clique-width) of the input graph, and is\nW[1]-hard even under further severe restrictions.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 15:29:15 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Gajarsk\u00fd", "Jakub", ""], ["Hlin\u011bn\u00fd", "Petr", ""], ["Kouteck\u00fd", "Martin", ""], ["Onn", "Shmuel", ""]]}, {"id": "1702.06969", "submitter": "Vedat Levi Alev", "authors": "Vedat Levi Alev, Lap Chi Lau", "title": "Approximating Unique Games Using Low Diameter Graph Decomposition", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design approximation algorithms for Unique Games when the constraint graph\nadmits good low diameter graph decomposition. For the ${\\sf Max2Lin}_k$ problem\nin $K_r$-minor free graphs, when there is an assignment satisfying\n$1-\\varepsilon$ fraction of constraints, we present an algorithm that produces\nan assignment satisfying $1-O(r\\varepsilon)$ fraction of constraints, with the\napproximation ratio independent of the alphabet size. A corollary is an\nimproved approximation algorithm for the ${\\sf MaxCut}$ problem for $K_r$-minor\nfree graphs. For general Unique Games in $K_r$-minor free graphs, we provide\nanother algorithm that produces an assignment satisfying $1-O(r\n\\sqrt{\\varepsilon})$ fraction of constraints.\n  Our approach is to round a linear programming relaxation to find a minimum\nsubset of edges that intersects all the inconsistent cycles. We show that it is\npossible to apply the low diameter graph decomposition technique on the\nconstraint graph directly, rather than to work on the label extended graph as\nin previous algorithms for Unique Games. The same approach applies when the\nconstraint graph is of genus $g$, and we get similar results with $r$ replaced\nby $\\log g$ in the ${\\sf Max2Lin}_k$ problem and by $\\sqrt{\\log g}$ in the\ngeneral problem. The former result generalizes the result of Gupta-Talwar for\nUnique Games in the ${\\sf Max2Lin}_k$ case, and the latter result generalizes\nthe result of Trevisan for general Unique Games.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 19:08:25 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 05:09:46 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 16:23:37 GMT"}, {"version": "v4", "created": "Fri, 17 Nov 2017 13:25:16 GMT"}, {"version": "v5", "created": "Wed, 29 Nov 2017 20:02:24 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Alev", "Vedat Levi", ""], ["Lau", "Lap Chi", ""]]}, {"id": "1702.07002", "submitter": "Johnathan Smith", "authors": "J. David Smith and My T. Thai", "title": "Deterministic & Adaptive Non-Submodular Maximization via the Primal\n  Curvature", "comments": "revised version -- removes incorrect sampling method", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While greedy algorithms have long been observed to perform well on a wide\nvariety of problems, up to now approximation ratios have only been known for\ntheir application to problems having submodular objective functions $f$. Since\nmany practical problems have non-submodular $f$, there is a critical need to\ndevise new techniques to bound the performance of greedy algorithms in the case\nof non-submodularity.\n  Our primary contribution is the introduction of a novel technique for\nestimating the approximation ratio of the greedy algorithm for maximization of\nmonotone non-decreasing functions based on the curvature of $f$ without relying\non the submodularity constraint. We show that this technique reduces to the\nclassical $(1 - 1/e)$ ratio for submodular functions. Furthermore, we develop\nan extension of this ratio to the adaptive greedy algorithm, which allows\napplications to non-submodular stochastic maximization problems. This notably\nextends support to applications modeling incomplete data with uncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 20:59:22 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 20:51:17 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Smith", "J. David", ""], ["Thai", "My T.", ""]]}, {"id": "1702.07032", "submitter": "Dimitris Paparas", "authors": "Xi Chen, George Matikas, Dimitris Paparas, Mihalis Yannakakis", "title": "On the Complexity of Simple and Optimal Deterministic Mechanisms for an\n  Additive Buyer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Revenue-Optimal Deterministic Mechanism Design problem for a\nsingle additive buyer is #P-hard, even when the distributions have support size\n2 for each item and, more importantly, even when the optimal solution is\nguaranteed to be of a very simple kind: the seller picks a price for each\nindividual item and a price for the grand bundle of all the items; the buyer\ncan purchase either the grand bundle at its given price or any subset of items\nat their total individual prices. The following problems are also #P-hard, as\nimmediate corollaries of the proof:\n  1. determining if individual item pricing is optimal for a given instance,\n  2. determining if grand bundle pricing is optimal, and\n  3. computing the optimal (deterministic) revenue.\n  On the positive side, we show that when the distributions are i.i.d. with\nsupport size 2, the optimal revenue obtainable by any mechanism, even a\nrandomized one, can be achieved by a simple solution of the above kind\n(individual item pricing with a discounted price for the grand bundle) and\nfurthermore, it can be computed in polynomial time. The problem can be solved\nin polynomial time too when the number of items is constant.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 22:43:45 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 04:40:19 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Chen", "Xi", ""], ["Matikas", "George", ""], ["Paparas", "Dimitris", ""], ["Yannakakis", "Mihalis", ""]]}, {"id": "1702.07134", "submitter": "Faez Ahmed", "authors": "Faez Ahmed, John P. Dickerson, Mark Fuge", "title": "Diverse Weighted Bipartite b-Matching", "comments": null, "journal-ref": null, "doi": "10.24963/ijcai.2017/6", "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite matching, where agents on one side of a market are matched to\nagents or items on the other, is a classical problem in computer science and\neconomics, with widespread application in healthcare, education, advertising,\nand general resource allocation. A practitioner's goal is typically to maximize\na matching market's economic efficiency, possibly subject to some fairness\nrequirements that promote equal access to resources. A natural balancing act\nexists between fairness and efficiency in matching markets, and has been the\nsubject of much research.\n  In this paper, we study a complementary goal---balancing diversity and\nefficiency---in a generalization of bipartite matching where agents on one side\nof the market can be matched to sets of agents on the other. Adapting a\nclassical definition of the diversity of a set, we propose a quadratic\nprogramming-based approach to solving a supermodular minimization problem that\nbalances diversity and total weight of the solution. We also provide a scalable\ngreedy algorithm with theoretical performance bounds. We then define the price\nof diversity, a measure of the efficiency loss due to enforcing diversity, and\ngive a worst-case theoretical bound. Finally, we demonstrate the efficacy of\nour methods on three real-world datasets, and show that the price of diversity\nis not bad in practice.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 08:35:45 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 16:33:21 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Ahmed", "Faez", ""], ["Dickerson", "John P.", ""], ["Fuge", "Mark", ""]]}, {"id": "1702.07172", "submitter": "Sebastian Schraink", "authors": "Susanne Albers, Sebastian Schraink", "title": "Tight Bounds for Online Coloring of Basic Graph Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We resolve a number of long-standing open problems in online graph coloring.\nMore specifically, we develop tight lower bounds on the performance of online\nalgorithms for fundamental graph classes. An important contribution is that our\nbounds also hold for randomized online algorithms, for which hardly any results\nwere known. Technically, we construct lower bounds for chordal graphs. The\nconstructions then allow us to derive results on the performance of randomized\nonline algorithms for the following further graph classes: trees, planar,\nbipartite, inductive, bounded-treewidth and disk graphs. It shows that the best\ncompetitive ratio of both deterministic and randomized online algorithms is\n$\\Theta(\\log n)$, where $n$ is the number of vertices of a graph. Furthermore,\nwe prove that this guarantee cannot be improved if an online algorithm has a\nlookahead of size $O(n/\\log n)$ or access to a reordering buffer of size\n$n^{1-\\epsilon}$, for any $0<\\epsilon\\leq 1$. A consequence of our results is\nthat, for all of the above mentioned graph classes except bipartite graphs, the\nnatural $\\textit{First Fit}$ coloring algorithm achieves an optimal\nperformance, up to constant factors, among deterministic and randomized online\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 10:57:56 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 12:14:32 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Albers", "Susanne", ""], ["Schraink", "Sebastian", ""]]}, {"id": "1702.07292", "submitter": "Lev Reyzin", "authors": "Yi Huang, Mano Vikash Janardhanan, Lev Reyzin", "title": "Network Construction with Ordered Constraints", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of constructing a network by observing\nordered connectivity constraints, which we define herein. These ordered\nconstraints are made to capture realistic properties of real-world problems\nthat are not reflected in previous, more general models. We give hardness of\napproximation results and nearly-matching upper bounds for the offline problem,\nand we study the online problem in both general graphs and restricted\nsub-classes. In the online problem, for general graphs, we give exponentially\nbetter upper bounds than exist for algorithms for general connectivity\nproblems. For the restricted classes of stars and paths we are able to find\nalgorithms with optimal competitive ratios, the latter of which involve\nanalysis using a potential function defined over pq-trees.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 16:59:25 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Huang", "Yi", ""], ["Janardhanan", "Mano Vikash", ""], ["Reyzin", "Lev", ""]]}, {"id": "1702.07403", "submitter": "Ran Gelles", "authors": "Keren Censor-Hillel, Ran Gelles, Bernhard Haeupler", "title": "Making Asynchronous Distributed Computations Robust to Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of making distributed computations robust to noise,\nin particular to worst-case (adversarial) corruptions of messages. We give a\ngeneral distributed interactive coding scheme which simulates any asynchronous\ndistributed protocol while tolerating an optimal corruption of a $\\Theta(1/n)$\nfraction of all messages while incurring a moderate blowup of $O(n\\log^2 n)$ in\nthe communication complexity.\n  Our result is the first fully distributed interactive coding scheme in which\nthe topology of the communication network is not known in advance. Prior work\nrequired either a coordinating node to be connected to all other nodes in the\nnetwork or assumed a synchronous network in which all nodes already know the\ncomplete topology of the network.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 21:49:52 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Gelles", "Ran", ""], ["Haeupler", "Bernhard", ""]]}, {"id": "1702.07435", "submitter": "Lunjia Hu", "authors": "Hu Ding, Lunjia Hu, Lingxiao Huang, Jian Li", "title": "Capacitated Center Problems with Two-Sided Bounds and Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the capacitated center problems have attracted a lot of\nresearch interest. Given a set of vertices $V$, we want to find a subset of\nvertices $S$, called centers, such that the maximum cluster radius is\nminimized. Moreover, each center in $S$ should satisfy some capacity\nconstraint, which could be an upper or lower bound on the number of vertices it\ncan serve. Capacitated $k$-center problems with one-sided bounds (upper or\nlower) have been well studied in previous work, and a constant factor\napproximation was obtained.\n  We are the first to study the capacitated center problem with both capacity\nlower and upper bounds (with or without outliers). We assume each vertex has a\nuniform lower bound and a non-uniform upper bound. For the case of opening\nexactly $k$ centers, we note that a generalization of a recent LP approach can\nachieve constant factor approximation algorithms for our problems. Our main\ncontribution is a simple combinatorial algorithm for the case where there is no\ncardinality constraint on the number of open centers. Our combinatorial\nalgorithm is simpler and achieves better constant approximation factor compared\nto the LP approach.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 01:28:11 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Ding", "Hu", ""], ["Hu", "Lunjia", ""], ["Huang", "Lingxiao", ""], ["Li", "Jian", ""]]}, {"id": "1702.07458", "submitter": "Hideo Bannai", "authors": "Yuka Tanimura, Takaaki Nishimoto, Hideo Bannai, Shunsuke Inenaga,\n  Masayuki Takeda", "title": "Small-space encoding LCE data structure with constant-time queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\emph{longest common extension} (\\emph{LCE}) problem is to preprocess a\ngiven string $w$ of length $n$ so that the length of the longest common prefix\nbetween suffixes of $w$ that start at any two given positions is answered\nquickly. In this paper, we present a data structure of $O(z \\tau^2 +\n\\frac{n}{\\tau})$ words of space which answers LCE queries in $O(1)$ time and\ncan be built in $O(n \\log \\sigma)$ time, where $1 \\leq \\tau \\leq \\sqrt{n}$ is a\nparameter, $z$ is the size of the Lempel-Ziv 77 factorization of $w$ and\n$\\sigma$ is the alphabet size. This is an \\emph{encoding} data structure, i.e.,\nit does not access the input string $w$ when answering queries and thus $w$ can\nbe deleted after preprocessing. On top of this main result, we obtain further\nresults using (variants of) our LCE data structure, which include the\nfollowing:\n  - For highly repetitive strings where the $z\\tau^2$ term is dominated by\n$\\frac{n}{\\tau}$, we obtain a \\emph{constant-time and sub-linear space} LCE\nquery data structure.\n  - Even when the input string is not well compressible via Lempel-Ziv 77\nfactorization, we still can obtain a \\emph{constant-time and sub-linear space}\nLCE data structure for suitable $\\tau$ and for $\\sigma \\leq 2^{o(\\log n)}$.\n  - The time-space trade-off lower bounds for the LCE problem by Bille et al.\n[J. Discrete Algorithms, 25:42-50, 2014] and by Kosolobov [CoRR,\nabs/1611.02891, 2016] can be \"surpassed\" in some cases with our LCE data\nstructure.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 03:57:57 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Tanimura", "Yuka", ""], ["Nishimoto", "Takaaki", ""], ["Bannai", "Hideo", ""], ["Inenaga", "Shunsuke", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1702.07577", "submitter": "Dominik K\\\"oppl", "authors": "Patrick Dinklage, Johannes Fischer, Dominik K\\\"oppl, Marvin L\\\"obel,\n  Kunihiko Sadakane", "title": "Compression with the tudocomp Framework", "comments": null, "journal-ref": "Proc. SEA, LIPIcs 75, pages 13:1-13:22, 2017", "doi": "10.4230/LIPIcs.SEA.2017.13", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework facilitating the implementation and comparison of text\ncompression algorithms. We evaluate its features by a case study on two novel\ncompression algorithms based on the Lempel-Ziv compression schemes that perform\nwell on highly repetitive texts.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 13:41:29 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 07:07:15 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Dinklage", "Patrick", ""], ["Fischer", "Johannes", ""], ["K\u00f6ppl", "Dominik", ""], ["L\u00f6bel", "Marvin", ""], ["Sadakane", "Kunihiko", ""]]}, {"id": "1702.07578", "submitter": "Florian Kurpicz", "authors": "Johannes Fischer, Florian Kurpicz, Marvin L\\\"obel", "title": "Simple, Fast and Lightweight Parallel Wavelet Tree Construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wavelet tree (Grossi et al. [SODA, 2003]) and wavelet matrix (Claude et\nal. [Inf. Syst., 47:15--32, 2015]) are compact indices for texts over an\nalphabet $[0,\\sigma)$ that support rank, select and access queries in $O(\\lg\n\\sigma)$ time. We first present new practical sequential and parallel\nalgorithms for wavelet tree construction. Their unifying characteristics is\nthat they construct the wavelet tree bottomup}, i.e., they compute the last\nlevel first. We also show that this bottom-up construction can easily be\nadapted to wavelet matrices. In practice, our best sequential algorithm is up\nto twice as fast as the currently fastest sequential wavelet tree construction\nalgorithm (Shun [DCC, 2015]), simultaneously saving a factor of 2 in space.\nThis scales up to 32 cores, where we are about equally fast as the currently\nfastest parallel wavelet tree construction algorithm (Labeit et al. [DCC,\n2016]), but still use only about 75 % of the space. An additional theoretical\nresult shows how to adapt any wavelet tree construction algorithm to the\nwavelet matrix in the same (asymptotic) time, using only little extra space.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 13:43:20 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 10:54:22 GMT"}, {"version": "v3", "created": "Fri, 10 Nov 2017 14:22:23 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Fischer", "Johannes", ""], ["Kurpicz", "Florian", ""], ["L\u00f6bel", "Marvin", ""]]}, {"id": "1702.07665", "submitter": "Daniel Graf", "authors": "Andreas B\\\"artschi, Daniel Graf, Paolo Penna", "title": "Truthful Mechanisms for Delivery with Mobile Agents", "comments": "17 pages. An extended abstract of this paper will be published at the\n  Workshop on Algorithmic Approaches for Transportation Modeling, Optimization,\n  and Systems 2017, ATMOS'17", "journal-ref": "17th Workshop on Algorithmic Approaches for Transportation\n  Modelling, Optimization, and Systems, ATMOS'17, 2:1 -2:17, 2017", "doi": "10.4230/OASIcs.ATMOS.2017.2", "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the game-theoretic task of selecting mobile agents to deliver\nmultiple items on a network. An instance is given by $m$ messages (physical\nobjects) which have to be transported between specified source-target pairs in\na weighted undirected graph, and $k$ mobile heterogeneous agents, each being\nable to transport one message at a time. Following a recent model by\n[B\\\"artschi et al. 2016], each agent $i$ consumes energy proportional to the\ndistance it travels in the graph, where the different rates of energy\nconsumption are given by weight factors $w_i$. We are interested in optimizing\nor approximating the total energy consumption over all selected agents.\n  Unlike previous research, we assume the weights to be private values known\nonly to the respective agents. We present three different mechanisms which\nselect, route and pay the agents in a truthful way that guarantees voluntary\nparticipation of the agents, while approximating the optimum energy consumption\nby a constant factor. To this end we analyze a previous structural result and\nan approximation algorithm given by [B\\\"artschi et al. 2017]. Finally, we show\nthat for some instances in the case of a single package, the sum of the\npayments can be bounded in terms of the optimum.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 17:09:22 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 07:13:53 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["B\u00e4rtschi", "Andreas", ""], ["Graf", "Daniel", ""], ["Penna", "Paolo", ""]]}, {"id": "1702.07669", "submitter": "Karol W\\k{e}grzycki", "authors": "Marek Cygan, Marcin Mucha, Karol W\\k{e}grzycki, Micha{\\l}\n  W{\\l}odarczyk", "title": "On problems equivalent to (min,+)-convolution", "comments": "Extended abstract published in the proceedings of ICALP 2017. Full\n  version published in TALG 2019", "journal-ref": "ACM Transactions on Algorithms 2019", "doi": "10.1145/3293465", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, significant progress has been made in explaining the\napparent hardness of improving upon the naive solutions for many fundamental\npolynomially solvable problems. This progress has come in the form of\nconditional lower bounds -- reductions from a problem assumed to be hard. The\nhard problems include 3SUM, All-Pairs Shortest Path, SAT, Orthogonal Vectors,\nand others.\n  In the $(\\min,+)$-convolution problem, the goal is to compute a sequence\n$(c[i])^{n-1}_{i=0}$, where $c[k] = $ $\\min_{i=0,\\ldots,k} $ $\\{a[i] $ $+$\n$b[k-i]\\}$, given sequences $(a[i])^{n-1}_{i=0}$ and $(b[i])_{i=0}^{n-1}$. This\ncan easily be done in $O(n^2)$ time, but no $O(n^{2-\\varepsilon})$ algorithm is\nknown for $\\varepsilon > 0$. In this paper, we undertake a systematic study of\nthe $(\\min,+)$-convolution problem as a hardness assumption.\n  First, we establish the equivalence of this problem to a group of other\nproblems, including variants of the classic knapsack problem and problems\nrelated to subadditive sequences. The $(\\min,+)$-convolution problem has been\nused as a building block in algorithms for many problems, notably problems in\nstringology. It has also appeared as an ad hoc hardness assumption. Second, we\ninvestigate some of these connections and provide new reductions and other\nresults. We also explain why replacing this assumption with the SETH might not\nbe possible for some problems.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 17:18:02 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 11:42:37 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Cygan", "Marek", ""], ["Mucha", "Marcin", ""], ["W\u0119grzycki", "Karol", ""], ["W\u0142odarczyk", "Micha\u0142", ""]]}, {"id": "1702.07696", "submitter": "Sandor P. Fekete", "authors": "S\\'andor P. Fekete and Jan-Marc Reinhardt and Christian Scheffer", "title": "An Efficient Data Structure for Dynamic Two-Dimensional Reconfiguration", "comments": "11 pages, 12 figures; full version of extended abstract that appeared\n  in ARCS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the presence of dynamic insertions and deletions into a partially\nreconfigurable FPGA, fragmentation is unavoidable. This poses the challenge of\ndeveloping efficient approaches to dynamic defragmentation and reallocation.\nOne key aspect is to develop efficient algorithms and data structures that\nexploit the two-dimensional geometry of a chip, instead of just one. We propose\na new method for this task, based on the fractal structure of a quadtree, which\nallows dynamic segmentation of the chip area, along with dynamically adjusting\nthe necessary communication infrastructure. We describe a number of algorithmic\naspects, and present different solutions. We also provide a number of basic\nsimulations that indicate that the theoretical worst-case bound may be\npessimistic.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 18:30:03 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Fekete", "S\u00e1ndor P.", ""], ["Reinhardt", "Jan-Marc", ""], ["Scheffer", "Christian", ""]]}, {"id": "1702.07709", "submitter": "Simon Du", "authors": "Simon S. Du, Sivaraman Balakrishnan, Aarti Singh", "title": "Computationally Efficient Robust Estimation of Sparse Functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many conventional statistical procedures are extremely sensitive to seemingly\nminor deviations from modeling assumptions. This problem is exacerbated in\nmodern high-dimensional settings, where the problem dimension can grow with and\npossibly exceed the sample size. We consider the problem of robust estimation\nof sparse functionals, and provide a computationally and statistically\nefficient algorithm in the high-dimensional setting. Our theory identifies a\nunified set of deterministic conditions under which our algorithm guarantees\naccurate recovery. By further establishing that these deterministic conditions\nhold with high-probability for a wide range of statistical models, our theory\napplies to many problems of considerable interest including sparse mean and\ncovariance estimation; sparse linear regression; and sparse generalized linear\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 18:59:08 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Du", "Simon S.", ""], ["Balakrishnan", "Sivaraman", ""], ["Singh", "Aarti", ""]]}, {"id": "1702.07815", "submitter": "Sergio Cabello", "authors": "Sergio Cabello", "title": "Subquadratic Algorithms for the Diameter and the Sum of Pairwise\n  Distances in Planar Graphs", "comments": "Preliminary version at SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to compute for $n$-vertex planar graphs in $O(n^{11/6}{\\rm\npolylog}(n))$ expected time the diameter and the sum of the pairwise distances.\nThe algorithms work for directed graphs with real weights and no negative\ncycles. In $O(n^{15/8}{\\rm polylog}(n))$ expected time we can also compute the\nnumber of pairs of vertices at distance smaller than a given threshold. These\nare the first algorithms for these problems using time $O(n^c)$ for some\nconstant $c<2$, even when restricted to undirected, unweighted planar graphs.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 01:24:03 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 07:56:53 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Cabello", "Sergio", ""]]}, {"id": "1702.07832", "submitter": "Jeremy Kepner", "authors": "Hayden Jananthan, Karia Dibert, Jeremy Kepner", "title": "Constructing Adjacency Arrays from Incidence Arrays", "comments": "8 pages, 5 figures, accepted to IEEE IPDPS 2017 Workshop on Graph\n  Algorithm Building Blocks", "journal-ref": null, "doi": "10.1109/IPDPSW.2017.71", "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph construction, a fundamental operation in a data processing pipeline, is\ntypically done by multiplying the incidence array representations of a graph,\n$\\mathbf{E}_\\mathrm{in}$ and $\\mathbf{E}_\\mathrm{out}$, to produce an adjacency\narray of the graph, $\\mathbf{A}$, that can be processed with a variety of\nalgorithms. This paper provides the mathematical criteria to determine if the\nproduct $\\mathbf{A} = \\mathbf{E}^{\\sf T}_\\mathrm{out}\\mathbf{E}_\\mathrm{in}$\nwill have the required structure of the adjacency array of the graph. The\nvalues in the resulting adjacency array are determined by the corresponding\naddition $\\oplus$ and multiplication $\\otimes$ operations used to perform the\narray multiplication. Illustrations of the various results possible from\ndifferent $\\oplus$ and $\\otimes$ operations are provided using a small\ncollection of popular music metadata.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 04:13:22 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Jananthan", "Hayden", ""], ["Dibert", "Karia", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1702.07961", "submitter": "Ben Karsin", "authors": "Henri Casanova, John Iacono, Ben Karsin, Nodari Sitchinava, Volker\n  Weichert", "title": "An Efficient Multiway Mergesort for GPU Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting is a primitive operation that is a building block for countless\nalgorithms. As such, it is important to design sorting algorithms that approach\npeak performance on a range of hardware architectures. Graphics Processing\nUnits (GPUs) are particularly attractive architectures as they provides massive\nparallelism and computing power. However, the intricacies of their compute and\nmemory hierarchies make designing GPU-efficient algorithms challenging. In this\nwork we present GPU Multiway Mergesort (MMS), a new GPU-efficient multiway\nmergesort algorithm. MMS employs a new partitioning technique that exposes the\nparallelism needed by modern GPU architectures. To the best of our knowledge,\nMMS is the first sorting algorithm for the GPU that is asymptotically optimal\nin terms of global memory accesses and that is completely free of shared memory\nbank conflicts.\n  We realize an initial implementation of MMS, evaluate its performance on\nthree modern GPU architectures, and compare it to competitive implementations\navailable in state-of-the-art GPU libraries. Despite these implementations\nbeing highly optimized, MMS compares favorably, achieving performance\nimprovements for most random inputs. Furthermore, unlike MMS, state-of-the-art\nalgorithms are susceptible to bank conflicts. We find that for certain inputs\nthat cause these algorithms to incur large numbers of bank conflicts, MMS can\nachieve up to a 37.6% speedup over its fastest competitor. Overall, even though\nits current implementation is not fully optimized, due to its efficient use of\nthe memory hierarchy, MMS outperforms the fastest comparison-based sorting\nimplementations available to date.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 00:51:38 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 02:03:30 GMT"}, {"version": "v3", "created": "Thu, 30 Mar 2017 08:42:37 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Casanova", "Henri", ""], ["Iacono", "John", ""], ["Karsin", "Ben", ""], ["Sitchinava", "Nodari", ""], ["Weichert", "Volker", ""]]}, {"id": "1702.08207", "submitter": "Adrian Kosowski", "authors": "Dariusz Dereniowski (Gdansk University of Technology), Adrian Kosowski\n  (GANG), Przemyslaw Uznanski (ETH Zurich), Mengchuan Zou (GANG)", "title": "Approximation Strategies for Generalized Binary Search in Weighted Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following generalization of the binary search problem. A\nsearch strategy is required to locate an unknown target node $t$ in a given\ntree $T$. Upon querying a node $v$ of the tree, the strategy receives as a\nreply an indication of the connected component of $T\\setminus\\{v\\}$ containing\nthe target $t$. The cost of querying each node is given by a known non-negative\nweight function, and the considered objective is to minimize the total query\ncost for a worst-case choice of the target. Designing an optimal strategy for a\nweighted tree search instance is known to be strongly NP-hard, in contrast to\nthe unweighted variant of the problem which can be solved optimally in linear\ntime. Here, we show that weighted tree search admits a quasi-polynomial time\napproximation scheme: for any $0 \\textless{} \\varepsilon \\textless{} 1$, there\nexists a $(1+\\varepsilon)$-approximation strategy with a computation time of\n$n^{O(\\log n / \\varepsilon^2)}$. Thus, the problem is not APX-hard, unless $NP\n\\subseteq DTIME(n^{O(\\log n)})$. By applying a generic reduction, we obtain as\na corollary that the studied problem admits a polynomial-time $O(\\sqrt{\\log\nn})$-approximation. This improves previous $\\hat O(\\log n)$-approximation\napproaches, where the $\\hat O$-notation disregards $O(\\mathrm{poly}\\log\\log\nn)$-factors.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 09:52:17 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Dereniowski", "Dariusz", "", "Gdansk University of Technology"], ["Kosowski", "Adrian", "", "GANG"], ["Uznanski", "Przemyslaw", "", "ETH Zurich"], ["Zou", "Mengchuan", "", "GANG"]]}, {"id": "1702.08247", "submitter": "Kasra Khosoussi", "authors": "Kasra Khosoussi", "title": "On the Expected Value of the Determinant of Random Sum of Rank-One\n  Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, yet useful result about the expected value of the\ndeterminant of random sum of rank-one matrices. Computing such expectations in\ngeneral may involve a sum over exponentially many terms. Nevertheless, we show\nthat an interesting and useful class of such expectations that arise in, e.g.,\nD-optimal estimation and random graphs can be computed efficiently via\ncomputing a single determinant.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 11:55:20 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 01:39:26 GMT"}, {"version": "v3", "created": "Sun, 22 Mar 2020 18:39:22 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Khosoussi", "Kasra", ""]]}, {"id": "1702.08248", "submitter": "Olivier Bachem", "authors": "Olivier Bachem, Mario Lucic, Andreas Krause", "title": "Scalable k-Means Clustering via Lightweight Coresets", "comments": "To appear in the 24th ACM SIGKDD International Conference on\n  Knowledge Discovery & Data Mining (KDD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coresets are compact representations of data sets such that models trained on\na coreset are provably competitive with models trained on the full data set. As\nsuch, they have been successfully used to scale up clustering models to massive\ndata sets. While existing approaches generally only allow for multiplicative\napproximation errors, we propose a novel notion of lightweight coresets that\nallows for both multiplicative and additive errors. We provide a single\nalgorithm to construct lightweight coresets for k-means clustering as well as\nsoft and hard Bregman clustering. The algorithm is substantially faster than\nexisting constructions, embarrassingly parallel, and the resulting coresets are\nsmaller. We further show that the proposed approach naturally generalizes to\nstatistical k-means clustering and that, compared to existing results, it can\nbe used to compute smaller summaries for empirical risk minimization. In\nextensive experiments, we demonstrate that the proposed algorithm outperforms\nexisting data summarization strategies in practice.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 12:03:01 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 21:49:52 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Bachem", "Olivier", ""], ["Lucic", "Mario", ""], ["Krause", "Andreas", ""]]}, {"id": "1702.08299", "submitter": "Christian Konrad", "authors": "Graham Cormode, Jacques Dark, Christian Konrad", "title": "Independent Set Size Approximation in Graph Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the size of independent sets in a graph\n$G$ defined by a stream of edges. Our approach relies on the Caro-Wei bound,\nwhich expresses the desired quantity in terms of a sum over nodes of the\nreciprocal of their degrees, denoted by $\\beta(G)$. Our results show that\n$\\beta(G)$ can be approximated accurately, based on a provided lower bound on\n$\\beta$. Stronger results are possible when the edges are promised to arrive\ngrouped by an incident node. In this setting, we obtain a value that is at most\na logarithmic factor below the true value of $\\beta$ and no more than the true\nindependent set size. To justify the form of this bound, we also show an\n$\\Omega(n/\\beta)$ lower bound on any algorithm that approximates $\\beta$ up to\na constant factor.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 14:25:36 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Cormode", "Graham", ""], ["Dark", "Jacques", ""], ["Konrad", "Christian", ""]]}, {"id": "1702.08415", "submitter": "He Sun", "authors": "Yin Tat Lee and He Sun", "title": "An SDP-Based Algorithm for Linear-Sized Spectral Sparsification", "comments": "To appear at STOC'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any undirected and weighted graph $G=(V,E,w)$ with $n$ vertices and $m$\nedges, we call a sparse subgraph $H$ of $G$, with proper reweighting of the\nedges, a $(1+\\varepsilon)$-spectral sparsifier if \\[\n(1-\\varepsilon)x^{\\intercal}L_Gx\\leq x^{\\intercal} L_{H} x\\leq (1+\\varepsilon)\nx^{\\intercal} L_Gx \\] holds for any $x\\in\\mathbb{R}^n$, where $L_G$ and $L_{H}$\nare the respective Laplacian matrices of $G$ and $H$. Noticing that $\\Omega(m)$\ntime is needed for any algorithm to construct a spectral sparsifier and a\nspectral sparsifier of $G$ requires $\\Omega(n)$ edges, a natural question is to\ninvestigate, for any constant $\\varepsilon$, if a $(1+\\varepsilon)$-spectral\nsparsifier of $G$ with $O(n)$ edges can be constructed in $\\tilde{O}(m)$ time,\nwhere the $\\tilde{O}$ notation suppresses polylogarithmic factors. All previous\nconstructions on spectral sparsification require either super-linear number of\nedges or $m^{1+\\Omega(1)}$ time.\n  In this work we answer this question affirmatively by presenting an algorithm\nthat, for any undirected graph $G$ and $\\varepsilon>0$, outputs a\n$(1+\\varepsilon)$-spectral sparsifier of $G$ with $O(n/\\varepsilon^2)$ edges in\n$\\tilde{O}(m/\\varepsilon^{O(1)})$ time. Our algorithm is based on three novel\ntechniques: (1) a new potential function which is much easier to compute yet\nhas similar guarantees as the potential functions used in previous references;\n(2) an efficient reduction from a two-sided spectral sparsifier to a one-sided\nspectral sparsifier; (3) constructing a one-sided spectral sparsifier by a\nsemi-definite program.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 18:23:28 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Lee", "Yin Tat", ""], ["Sun", "He", ""]]}, {"id": "1702.08443", "submitter": "Marek Suchenek", "authors": "Marek A. Suchenek", "title": "Elementary Yet Precise Worst-case Analysis of MergeSort, A short version\n  (SV)", "comments": "25 pages, 12 figures, three of which contain working Java methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper offers two elementary yet precise derivations of an exact formula\n  \\[ W(n) = \\sum_{i=1} ^{n} \\lceil \\lg i \\rceil = n \\lceil \\lg n \\rceil -\n2^{\\lceil \\lg n \\rceil} + 1 \\] for the maximum number $ W(n) $ of comparisons\nof keys performed by $ {\\tt MergeSort} $ on an $ n $-element array. The first\nof the two, due to its structural regularity, is well worth carefully studying\nin its own right.\n  Close smooth bounds on $ W(n) $ are derived. It seems interesting that $ W(n)\n$ is linear between the points $ n = 2^{\\lfloor \\lg n \\rfloor} $ and it\nlinearly interpolates its own lower bound $ n \\lg n - n + 1 $ between these\npoints.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 07:12:46 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Suchenek", "Marek A.", ""]]}, {"id": "1702.08474", "submitter": "Christian Coester", "authors": "Christian Coester, Elias Koutsoupias, Philip Lazos", "title": "The Infinite Server Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variant of the $k$-server problem, the infinite server problem, in\nwhich infinitely many servers reside initially at a particular point of the\nmetric space and serve a sequence of requests. In the framework of competitive\nanalysis, we show a surprisingly tight connection between this problem and the\n$(h,k)$-server problem, in which an online algorithm with $k$ servers competes\nagainst an offline algorithm with $h$ servers. Specifically, we show that the\ninfinite server problem has bounded competitive ratio if and only if the\n$(h,k)$-server problem has bounded competitive ratio for some $k=O(h)$. We give\na lower bound of $3.146$ for the competitive ratio of the infinite server\nproblem, which implies the same lower bound for the $(h,k)$-server problem even\nwhen $k/h \\to \\infty$ and holds also for the line metric; the previous known\nbounds were 2.4 for general metric spaces and 2 for the line. For weighted\ntrees and layered graphs we obtain upper bounds, although they depend on the\ndepth. Of particular interest is the infinite server problem on the line, which\nwe show to be equivalent to the seemingly easier case in which all requests are\nin a fixed bounded interval away from the original position of the servers.\nThis is a special case of a more general reduction from arbitrary metric spaces\nto bounded subspaces. Unfortunately, classical approaches (double coverage and\ngeneralizations, work function algorithm, balancing algorithms) fail even for\nthis special case.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 19:11:44 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Coester", "Christian", ""], ["Koutsoupias", "Elias", ""], ["Lazos", "Philip", ""]]}, {"id": "1702.08503", "submitter": "Amit Daniely", "authors": "Amit Daniely", "title": "SGD Learns the Conjugate Kernel Class of the Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the standard stochastic gradient decent (SGD) algorithm is\nguaranteed to learn, in polynomial time, a function that is competitive with\nthe best function in the conjugate kernel space of the network, as defined in\nDaniely, Frostig and Singer. The result holds for log-depth networks from a\nrich family of architectures. To the best of our knowledge, it is the first\npolynomial-time guarantee for the standard neural network learning algorithm\nfor networks of depth more that two.\n  As corollaries, it follows that for neural networks of any depth between $2$\nand $\\log(n)$, SGD is guaranteed to learn, in polynomial time, constant degree\npolynomials with polynomially bounded coefficients. Likewise, it follows that\nSGD on large enough networks can learn any continuous function (not in\npolynomial time), complementing classical expressivity results.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 20:05:43 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 18:32:42 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Daniely", "Amit", ""]]}, {"id": "1702.08545", "submitter": "Carlos Ochoa", "authors": "J\\'er\\'emy Barbay, Carlos Ochoa", "title": "Synergistic Computation of Planar Maxima and Convex Hull", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Refinements of the worst case complexity over instances of fixed input size\nconsider the input order or the input structure, but rarely both at the same\ntime. Barbay et al. [2016] described ``synergistic'' solutions on multisets,\nwhich take advantage of the input order and the input structure, such as to\nasymptotically outperform any comparable solution which takes advantage only of\none of those features. We consider the extension of their results to the\ncomputation of the \\textsc{Maxima Set} and the \\textsc{Convex Hull} of a set of\nplanar points. After revisiting and improving previous approaches taking\nadvantage only of the input order or of the input structure, we describe\nsynergistic solutions taking optimally advantage of various notions of the\ninput order and input structure in the plane. As intermediate results, we\ndescribe and analyze the first adaptive algorithms for \\textsc{Merging Maxima}\nand \\textsc{Merging Convex Hulls}.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 21:48:00 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Barbay", "J\u00e9r\u00e9my", ""], ["Ochoa", "Carlos", ""]]}, {"id": "1702.08734", "submitter": "Matthijs Douze", "authors": "Jeff Johnson and Matthijs Douze and Herv\\'e J\\'egou", "title": "Billion-scale similarity search with GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search finds application in specialized database systems handling\ncomplex data such as images or videos, which are typically represented by\nhigh-dimensional features and require specific indexing structures. This paper\ntackles the problem of better utilizing GPUs for this task. While GPUs excel at\ndata-parallel tasks, prior approaches are bottlenecked by algorithms that\nexpose less parallelism, such as k-min selection, or make poor use of the\nmemory hierarchy.\n  We propose a design for k-selection that operates at up to 55% of theoretical\npeak performance, enabling a nearest neighbor implementation that is 8.5x\nfaster than prior GPU state of the art. We apply it in different similarity\nsearch scenarios, by proposing optimized design for brute-force, approximate\nand compressed-domain search based on product quantization. In all these\nsetups, we outperform the state of the art by large margins. Our implementation\nenables the construction of a high accuracy k-NN graph on 95 million images\nfrom the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion\nvectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced\nour approach for the sake of comparison and reproducibility.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 10:42:31 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Johnson", "Jeff", ""], ["Douze", "Matthijs", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1702.08791", "submitter": "Matthew Staib", "authors": "Matthew Staib and Stefanie Jegelka", "title": "Robust Budget Allocation via Continuous Submodular Functions", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS cs.SI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimal allocation of resources for maximizing influence, spread of\ninformation or coverage, has gained attention in the past years, in particular\nin machine learning and data mining. But in applications, the parameters of the\nproblem are rarely known exactly, and using wrong parameters can lead to\nundesirable outcomes. We hence revisit a continuous version of the Budget\nAllocation or Bipartite Influence Maximization problem introduced by Alon et\nal. (2012) from a robust optimization perspective, where an adversary may\nchoose the least favorable parameters within a confidence set. The resulting\nproblem is a nonconvex-concave saddle point problem (or game). We show that\nthis nonconvex problem can be solved exactly by leveraging connections to\ncontinuous submodular functions, and by solving a constrained submodular\nminimization problem. Although constrained submodular minimization is hard in\ngeneral, here, we establish conditions under which such a problem can be solved\nto arbitrary precision $\\epsilon$.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 14:07:42 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 15:24:28 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Staib", "Matthew", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1702.08862", "submitter": "Palash Dey", "authors": "Palash Dey, Nimrod Talmon, and Otniel van Handel", "title": "Proportional Representation in Vote Streams", "comments": "Accepted as a full paper in AAMAS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.CC cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider elections where the voters come one at a time, in a streaming\nfashion, and devise space-efficient algorithms which identify an approximate\nwinning committee with respect to common multiwinner proportional\nrepresentation voting rules; specifically, we consider the Approval-based and\nthe Borda-based variants of both the Chamberlin-- ourant rule and the Monroe\nrule. We complement our algorithms with lower bounds. Somewhat surprisingly,\nour results imply that, using space which does not depend on the number of\nvoters it is possible to efficiently identify an approximate representative\ncommittee of fixed size over vote streams with huge number of voters.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 16:57:49 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Dey", "Palash", ""], ["Talmon", "Nimrod", ""], ["van Handel", "Otniel", ""]]}, {"id": "1702.08899", "submitter": "George Mertzios", "authors": "Argyrios Deligkas and George B. Mertzios and Paul G. Spirakis", "title": "Binary Search in Graphs Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical binary search in a path the aim is to detect an unknown\ntarget by asking as few queries as possible, where each query reveals the\ndirection to the target. This binary search algorithm has been recently\nextended by [Emamjomeh-Zadeh et al., STOC, 2016] to the problem of detecting a\ntarget in an arbitrary graph. Similarly to the classical case in the path, the\nalgorithm of Emamjomeh-Zadeh et al. maintains a candidates' set for the target,\nwhile each query asks an appropriately chosen vertex-- the \"median\"--which\nminimises a potential $\\Phi$ among the vertices of the candidates' set. In this\npaper we address three open questions posed by Emamjomeh-Zadeh et al., namely\n(a) detecting a target when the query response is a direction to an\napproximately shortest path to the target, (b) detecting a target when querying\na vertex that is an approximate median of the current candidates' set (instead\nof an exact one), and (c) detecting multiple targets, for which to the best of\nour knowledge no progress has been made so far. We resolve questions (a) and\n(b) by providing appropriate upper and lower bounds, as well as a new potential\n$\\Gamma$ that guarantees efficient target detection even by querying an\napproximate median each time. With respect to (c), we initiate a systematic\nstudy for detecting two targets in graphs and we identify sufficient conditions\non the queries that allow for strong (linear) lower bounds and strong\n(polylogarithmic) upper bounds for the number of queries. All of our positive\nresults can be derived using our new potential $\\Gamma$ that allows querying\napproximate medians.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 18:38:41 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 18:52:17 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Deligkas", "Argyrios", ""], ["Mertzios", "George B.", ""], ["Spirakis", "Paul G.", ""]]}, {"id": "1702.08903", "submitter": "Michael Lampis", "authors": "R\\'emy Belmonte, Michael Lampis, Valia Mitsou", "title": "Defective Coloring on Classes of Perfect Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Defective Coloring we are given a graph $G$ and two integers $\\chi_d$,\n$\\Delta^*$ and are asked if we can $\\chi_d$-color $G$ so that the maximum\ndegree induced by any color class is at most $\\Delta^*$. We show that this\nnatural generalization of Coloring is much harder on several basic graph\nclasses. In particular, we show that it is NP-hard on split graphs, even when\none of the two parameters $\\chi_d$, $\\Delta^*$ is set to the smallest possible\nfixed value that does not trivialize the problem ($\\chi_d = 2$ or $\\Delta^* =\n1$). Together with a simple treewidth-based DP algorithm this completely\ndetermines the complexity of the problem also on chordal graphs. We then\nconsider the case of cographs and show that, somewhat surprisingly, Defective\nColoring turns out to be one of the few natural problems which are NP-hard on\nthis class. We complement this negative result by showing that Defective\nColoring is in P for cographs if either $\\chi_d$ or $\\Delta^*$ is fixed; that\nit is in P for trivially perfect graphs; and that it admits a sub-exponential\ntime algorithm for cographs when both $\\chi_d$ and $\\Delta^*$ are unbounded.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 18:47:57 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 10:18:24 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 21:06:23 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Belmonte", "R\u00e9my", ""], ["Lampis", "Michael", ""], ["Mitsou", "Valia", ""]]}]