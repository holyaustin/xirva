[{"id": "2006.00216", "submitter": "Shunsuke Inenaga", "authors": "Takafumi Inoue, Shunsuke Inenaga, Hideo Bannai", "title": "Longest Square Subsequence Problem Revisited", "comments": "Accepted for SPIRE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The longest square subsequence (LSS) problem consists of computing a longest\nsubsequence of a given string $S$ that is a square, i.e., a longest subsequence\nof form $XX$ appearing in $S$. It is known that an LSS of a string $S$ of\nlength $n$ can be computed using $O(n^2)$ time [Kosowski 2004], or with\n(model-dependent) polylogarithmic speed-ups using $O(n^2 (\\log \\log n)^2 /\n\\log^2 n)$ time [Tiskin 2013]. We present the first algorithm for LSS whose\nrunning time depends on other parameters, i.e., we show that an LSS of $S$ can\nbe computed in $O(r \\min\\{n, M\\}\\log \\frac{n}{r} + n + M \\log n)$ time with\n$O(M)$ space, where $r$ is the length of an LSS of $S$ and $M$ is the number of\nmatching points on $S$.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 08:09:44 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 13:40:09 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Inoue", "Takafumi", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""]]}, {"id": "2006.00322", "submitter": "Francisco Castillo-Zunino", "authors": "Francisco Castillo-Zunino, Pinar Keskinocak", "title": "Bi-Criteria Multiple Knapsack Problem with Grouped Items", "comments": "34 pages, 26 figures (18 pages, 6 figures without considering\n  appendix and references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multiple knapsack problem with grouped items aims to maximize rewards by\nassigning groups of items among multiple knapsacks, considering knapsack\ncapacities. Either all items in a group are assigned or none at all. We propose\nalgorithms which guarantee that rewards are not less than the optimal solution,\nwith a bound on exceeded knapsack capacities. To obtain capacity-feasible\nsolutions, we propose a binary-search heuristic combined with these algorithms.\nWe test the performance of the algorithms and heuristics in an extensive set of\nexperiments on randomly generated instances and show they are efficient and\neffective, i.e., they run reasonably fast and generate good quality solutions.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 17:18:06 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Castillo-Zunino", "Francisco", ""], ["Keskinocak", "Pinar", ""]]}, {"id": "2006.00354", "submitter": "Andreas B\\\"artschi", "authors": "Andreas B\\\"artschi and Stephan Eidenbenz", "title": "Grover Mixers for QAOA: Shifting Complexity from Mixer Design to State\n  Preparation", "comments": null, "journal-ref": "IEEE International Conference on Quantum Computing and\n  Engineering, QCE'20, 72-82, 2020", "doi": "10.1109/QCE49297.2020.00020", "report-no": "LA-UR-20-23893", "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose GM-QAOA, a variation of the Quantum Alternating Operator Ansatz\n(QAOA) that uses Grover-like selective phase shift mixing operators. GM-QAOA\nworks on any NP optimization problem for which it is possible to efficiently\nprepare an equal superposition of all feasible solutions; it is designed to\nperform particularly well for constraint optimization problems, where not all\npossible variable assignments are feasible solutions. GM-QAOA has the following\nfeatures: (i) It is not susceptible to Hamiltonian Simulation error (such as\nTrotterization errors) as its operators can be implemented exactly using\nstandard gate sets and (ii) Solutions with the same objective value are always\nsampled with the same amplitude.\n  We illustrate the potential of GM-QAOA on several optimization problem\nclasses: for permutation-based optimization problems such as the Traveling\nSalesperson Problem, we present an efficient algorithm to prepare a\nsuperposition of all possible permutations of $n$ numbers, defined on $O(n^2)$\nqubits; for the hard constraint $k$-Vertex-Cover problem, and for an\napplication to Discrete Portfolio Rebalancing, we show that GM-QAOA outperforms\nexisting QAOA approaches.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 20:24:53 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 21:02:15 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["B\u00e4rtschi", "Andreas", ""], ["Eidenbenz", "Stephan", ""]]}, {"id": "2006.00376", "submitter": "Peter Manohar", "authors": "Peter Manohar and Jalani Williams", "title": "Lower Bounds for Caching with Delayed Hits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caches are a fundamental component of latency-sensitive computer systems.\nRecent work of [ASWB20] has initiated the study of delayed hits: a phenomenon\nin caches that occurs when the latency between the cache and backing store is\nmuch larger than the time between new requests. We present two results for the\ndelayed hits caching model.\n  (1) Competitive ratio lower bound. We prove that the competitive ratio of the\nalgorithm in [ASWB20], and more generally of any deterministic online algorithm\nfor delayed hits, is at least Omega(kZ), where k is the cache size and Z is the\ndelay parameter.\n  (2) Antimonotonicity of the delayed hits latency. Antimonotonicity is a\nnaturally desirable property of cache latency: having a cache hit instead of a\ncache miss should result in lower overall latency. We prove that the latency of\nthe delayed hits model is not antimonotone by exhibiting a scenario where\nhaving a cache hit instead of a miss results in an increase in overall latency.\nWe additionally present a modification of the delayed hits model that makes the\nlatency antimonotone.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 22:14:14 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Manohar", "Peter", ""], ["Williams", "Jalani", ""]]}, {"id": "2006.00386", "submitter": "Maximilian Janke", "authors": "Susanne Albers and Maximilian Janke", "title": "Scheduling in the Random-Order Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Makespan minimization on identical machines is a fundamental problem in\nonline scheduling. The goal is to assign a sequence of jobs to $m$ identical\nparallel machines so as to minimize the maximum completion time of any job.\nAlready in the 1960s, Graham showed that Greedy is $(2-1/m)$-competitive. The\nbest deterministic online algorithm currently known achieves a competitive\nratio of 1.9201. No deterministic online strategy can obtain a competitiveness\nsmaller than 1.88.\n  In this paper, we study online makespan minimization in the popular\nrandom-order model, where the jobs of a given input arrive as a random\npermutation. It is known that Greedy does not attain a competitive factor\nasymptotically smaller than 2 in this setting. We present the first improved\nperformance guarantees. Specifically, we develop a deterministic online\nalgorithm that achieves a competitive ratio of 1.8478. The result relies on a\nnew analysis approach. We identify a set of properties that a random\npermutation of the input jobs satisfies with high probability. Then we conduct\na worst-case analysis of our algorithm, for the respective class of\npermutations. The analysis implies that the stated competitiveness holds not\nonly in expectation but with high probability. Moreover, it provides\nmathematical evidence that job sequences leading to higher performance ratios\nare extremely rare, pathological inputs. We complement the results by lower\nbounds, for the random-order model. We show that no deterministic online\nalgorithm can achieve a competitive ratio smaller than 4/3. Moreover, no\ndeterministic online algorithm can attain a competitiveness smaller than 3/2\nwith high probability.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 23:23:27 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Albers", "Susanne", ""], ["Janke", "Maximilian", ""]]}, {"id": "2006.00571", "submitter": "Micha{\\l} Pilipczuk", "authors": "Jiehua Chen, Wojciech Czerwi\\'nski, Yann Disser, Andreas Emil\n  Feldmann, Danny Hermelin, Wojciech Nadara, Micha{\\l} Pilipczuk, Marcin\n  Pilipczuk, Manuel Sorge, Bart{\\l}omiej Wr\\'oblewski, Anna Zych-Pawlewicz", "title": "Efficient fully dynamic elimination forests with applications to\n  detecting long paths and cycles", "comments": "74 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data structure that in a dynamic graph of treedepth at most $d$,\nwhich is modified over time by edge insertions and deletions, maintains an\noptimum-height elimination forest. The data structure achieves worst-case\nupdate time $2^{{\\cal O}(d^2)}$, which matches the best known parameter\ndependency in the running time of a static fpt algorithm for computing the\ntreedepth of a graph. This improves a result of Dvo\\v{r}\\'ak et al. [ESA 2014],\nwho for the same problem achieved update time $f(d)$ for some non-elementary\n(i.e. tower-exponential) function $f$. As a by-product, we improve known upper\nbounds on the sizes of minimal obstructions for having treedepth $d$ from\ndoubly-exponential in $d$ to $d^{{\\cal O}(d)}$.\n  As applications, we design new fully dynamic parameterized data structures\nfor detecting long paths and cycles in general graphs. More precisely, for a\nfixed parameter $k$ and a dynamic graph $G$, modified over time by edge\ninsertions and deletions, our data structures maintain answers to the following\nqueries:\n  - Does $G$ contain a simple path on $k$ vertices?\n  - Does $G$ contain a simple cycle on at least $k$ vertices?\n  In the first case, the data structure achieves amortized update time\n$2^{{\\cal O}(k^2)}$. In the second case, the amortized update time is $2^{{\\cal\nO}(k^4)} + {\\cal O}(k \\log n)$. In both cases we assume access to a dictionary\non the edges of $G$.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 17:47:01 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 15:33:04 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Chen", "Jiehua", ""], ["Czerwi\u0144ski", "Wojciech", ""], ["Disser", "Yann", ""], ["Feldmann", "Andreas Emil", ""], ["Hermelin", "Danny", ""], ["Nadara", "Wojciech", ""], ["Pilipczuk", "Micha\u0142", ""], ["Pilipczuk", "Marcin", ""], ["Sorge", "Manuel", ""], ["Wr\u00f3blewski", "Bart\u0142omiej", ""], ["Zych-Pawlewicz", "Anna", ""]]}, {"id": "2006.00602", "submitter": "Xue Chen", "authors": "Pranjal Awasthi, Xue Chen, Aravindan Vijayaraghavan", "title": "Estimating Principal Components under Adversarial Perturbations", "comments": "It is to appear at COLT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness is a key requirement for widespread deployment of machine learning\nalgorithms, and has received much attention in both statistics and computer\nscience. We study a natural model of robustness for high-dimensional\nstatistical estimation problems that we call the adversarial perturbation\nmodel. An adversary can perturb every sample arbitrarily up to a specified\nmagnitude $\\delta$ measured in some $\\ell_q$ norm, say $\\ell_\\infty$. Our model\nis motivated by emerging paradigms such as low precision machine learning and\nadversarial training.\n  We study the classical problem of estimating the top-$r$ principal subspace\nof the Gaussian covariance matrix in high dimensions, under the adversarial\nperturbation model. We design a computationally efficient algorithm that given\ncorrupted data, recovers an estimate of the top-$r$ principal subspace with\nerror that depends on a robustness parameter $\\kappa$ that we identify. This\nparameter corresponds to the $q \\to 2$ operator norm of the projector onto the\nprincipal subspace, and generalizes well-studied analytic notions of sparsity.\nAdditionally, in the absence of corruptions, our algorithmic guarantees recover\nexisting bounds for problems such as sparse PCA and its higher rank analogs. We\nalso prove that the above dependence on the parameter $\\kappa$ is almost\noptimal asymptotically, not just in a minimax sense, but remarkably for every\ninstance of the problem. This instance-optimal guarantee shows that the $q \\to\n2$ operator norm of the subspace essentially characterizes the estimation error\nunder adversarial perturbations.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 20:27:19 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 03:31:04 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Chen", "Xue", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "2006.00605", "submitter": "Kamil Khadiev", "authors": "Kamil Khadiev and Maxim Yagafarov", "title": "A Fast Algorithm for Online k-servers Problem on Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online algorithms for the $k$-server problem on trees. There is a\n$k$-competitive algorithm for this problem, and it is the best competitive\nratio. M. Chrobak and L. Larmore provided it. At the same time, the existing\nimplementation has $O(n)$ time complexity, where $n$ is a number of nodes in a\ntree. We provide a new time-efficient implementation of the algorithm. It has\n$O(n)$ time complexity for preprocessing and $O\\left(k(\\log n)^2\\right)$ for\nprocessing a query.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 20:49:52 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 20:47:46 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Khadiev", "Kamil", ""], ["Yagafarov", "Maxim", ""]]}, {"id": "2006.00743", "submitter": "Li-Yang Tan", "authors": "Guy Blanc and Jane Lange and Li-Yang Tan", "title": "Provable guarantees for decision tree induction: the agnostic setting", "comments": "20 pages, to appear in ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give strengthened provable guarantees on the performance of widely\nemployed and empirically successful {\\sl top-down decision tree learning\nheuristics}. While prior works have focused on the realizable setting, we\nconsider the more realistic and challenging {\\sl agnostic} setting. We show\nthat for all monotone functions~$f$ and parameters $s\\in \\mathbb{N}$, these\nheuristics construct a decision tree of size $s^{\\tilde{O}((\\log\ns)/\\varepsilon^2)}$ that achieves error $\\le \\mathsf{opt}_s + \\varepsilon$,\nwhere $\\mathsf{opt}_s$ denotes the error of the optimal size-$s$ decision tree\nfor $f$. Previously, such a guarantee was not known to be achievable by any\nalgorithm, even one that is not based on top-down heuristics. We complement our\nalgorithmic guarantee with a near-matching $s^{\\tilde{\\Omega}(\\log s)}$ lower\nbound.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 06:44:07 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Blanc", "Guy", ""], ["Lange", "Jane", ""], ["Tan", "Li-Yang", ""]]}, {"id": "2006.00808", "submitter": "Paola Vocca Prof.", "authors": "Maurizio Talamo and Paola Vocca", "title": "Note to \"An efficient Data Structure for Lattice Operation\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note is to publicly answer to a paper recently accepted to SWAT 2020 [1]\nthat claims to have solved an error in our papers [3,2] by proposing a solution\nwith worst performances. In the following section we describe in detail\nsections 4.2 (Cluster collection) and 5 (Data Structure and Space Complexity)\nin [3] to show the implementation of the data structure.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 09:24:49 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 08:28:09 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 14:47:39 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Talamo", "Maurizio", ""], ["Vocca", "Paola", ""]]}, {"id": "2006.01026", "submitter": "Antonios Antoniadis", "authors": "Antonios Antoniadis, Themis Gouleakis, Pieter Kleer, Pavel Kolev", "title": "Secretary and Online Matching Problems with Machine Learned Advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical analysis of online algorithms, due to its worst-case nature,\ncan be quite pessimistic when the input instance at hand is far from\nworst-case. Often this is not an issue with machine learning approaches, which\nshine in exploiting patterns in past inputs in order to predict the future.\nHowever, such predictions, although usually accurate, can be arbitrarily poor.\nInspired by a recent line of work, we augment three well-known online settings\nwith machine learned predictions about the future, and develop algorithms that\ntake them into account. In particular, we study the following online selection\nproblems: (i) the classical secretary problem, (ii) online bipartite matching\nand (iii) the graphic matroid secretary problem. Our algorithms still come with\na worst-case performance guarantee in the case that predictions are subpar\nwhile obtaining an improved competitive ratio (over the best-known classical\nonline algorithm for each problem) when the predictions are sufficiently\naccurate. For each algorithm, we establish a trade-off between the competitive\nratios obtained in the two respective cases.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 15:44:29 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 10:15:30 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Antoniadis", "Antonios", ""], ["Gouleakis", "Themis", ""], ["Kleer", "Pieter", ""], ["Kolev", "Pavel", ""]]}, {"id": "2006.01225", "submitter": "Supratim Shit", "authors": "Rachit Chhaya, Jayesh Choudhari, Anirban Dasgupta, Supratim Shit", "title": "Streaming Coresets for Symmetric Tensor Factorization", "comments": "Accepted at ICML 2020. Included algorithm with improved update time\n  and fixed minor bugs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorizing tensors has recently become an important optimization module in a\nnumber of machine learning pipelines, especially in latent variable models. We\nshow how to do this efficiently in the streaming setting. Given a set of $n$\nvectors, each in $\\mathbb{R}^d$, we present algorithms to select a sublinear\nnumber of these vectors as coreset, while guaranteeing that the CP\ndecomposition of the $p$-moment tensor of the coreset approximates the\ncorresponding decomposition of the $p$-moment tensor computed from the full\ndata. We introduce two novel algorithmic techniques: online filtering and\nkernelization. Using these two, we present six algorithms that achieve\ndifferent tradeoffs of coreset size, update time and working space, beating or\nmatching various state of the art algorithms. In the case of matrices\n($2$-ordered tensor), our online row sampling algorithm guarantees $(1 \\pm\n\\epsilon)$ relative error spectral approximation. We show applications of our\nalgorithms in learning single topic modeling.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 19:55:34 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 17:49:46 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Chhaya", "Rachit", ""], ["Choudhari", "Jayesh", ""], ["Dasgupta", "Anirban", ""], ["Shit", "Supratim", ""]]}, {"id": "2006.01400", "submitter": "Kaito Fujii", "authors": "Kaito Fujii", "title": "Approximation Guarantees of Local Search Algorithms via Localizability\n  of Set Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new framework for providing approximation guarantees of\nlocal search algorithms. Local search is a basic algorithm design technique and\nis widely used for various combinatorial optimization problems. To analyze\nlocal search algorithms for set function maximization, we propose a new notion\ncalled localizability of set functions, which measures how effective local\nimprovement is. Moreover, we provide approximation guarantees of standard local\nsearch algorithms under various combinatorial constraints in terms of\nlocalizability. The main application of our framework is sparse optimization,\nfor which we show that restricted strong concavity and restricted smoothness of\nthe objective function imply localizability, and further develop accelerated\nversions of local search algorithms. We conduct experiments in sparse\nregression and structure learning of graphical models to confirm the practical\nefficiency of the proposed local search algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 05:37:52 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Fujii", "Kaito", ""]]}, {"id": "2006.01428", "submitter": "Sanjeev Saxena", "authors": "Sanjeev Saxena", "title": "Zone Theorem for Arrangements in three dimensions", "comments": null, "journal-ref": "Information Processing Letters Volume 172, December 2021, 106161", "doi": "10.1016/j.ipl.2021.106161", "report-no": null, "categories": "cs.CG cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, a simple description of zone theorem in three dimensions is\ngiven.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 07:15:40 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Saxena", "Sanjeev", ""]]}, {"id": "2006.01588", "submitter": "Johan Van Rooij PhD", "authors": "Johan M. M. van Rooij", "title": "Fast Algorithms for Join Operations on Tree Decompositions", "comments": "An earlier version appeared in \"Treewidth, Kernels, and Algorithms.\n  Essays Dedicated to Hans L. Bodlaender on the Occasion of His 60th Birthday\"\n  LNCS 12160", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treewidth is a measure of how tree-like a graph is. It has many important\nalgorithmic applications because many NP-hard problems on general graphs become\ntractable when restricted to graphs of bounded treewidth. Algorithms for\nproblems on graphs of bounded treewidth mostly are dynamic programming\nalgorithms using the structure of a tree decomposition of the graph. The\nbottleneck in the worst-case run time of these algorithms often is the\ncomputations for the so called join nodes in the associated nice tree\ndecomposition.\n  In this paper, we review two different approaches that have appeared in the\nliterature about computations for the join nodes: one using fast zeta and\nM\\\"obius transforms and one using fast Fourier transforms. We combine these\napproaches to obtain new, faster algorithms for a broad class of vertex subset\nproblems known as the [\\sigma,\\rho]-domination problems. Our main result is\nthat we show how to solve [\\sigma,\\rho]-domination problems in $O(s^{t+2} t n^2\n(t\\log(s)+\\log(n)))$ arithmetic operations. Here, t is the treewidth, s is the\n(fixed) number of states required to represent partial solutions of the\nspecific [\\sigma,\\rho]-domination problem, and n is the number of vertices in\nthe graph. This reduces the polynomial factors involved compared to the\npreviously best time bound (van Rooij, Bodlaender, Rossmanith, ESA 2009) of $O(\ns^{t+2} (st)^{2(s-2)} n^3 )$ arithmetic operations. In particular, this removes\nthe dependence of the degree of the polynomial on the fixed number of\nstates~$s$.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 13:27:01 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["van Rooij", "Johan M. M.", ""]]}, {"id": "2006.01825", "submitter": "Djamal Belazzougui", "authors": "Djamal Belazzougui and Gregory Kucherov", "title": "Efficient tree-structured categorical retrieval", "comments": "Full version of a paper accepted for presentation at the 31st Annual\n  Symposium on Combinatorial Pattern Matching (CPM 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a document retrieval problem in the new framework where $D$ text\ndocuments are organized in a {\\em category tree} with a pre-defined number $h$\nof categories. This situation occurs e.g. with taxomonic trees in biology or\nsubject classification systems for scientific literature. Given a string\npattern $p$ and a category (level in the category tree), we wish to efficiently\nretrieve the $t$ \\emph{categorical units} containing this pattern and belonging\nto the category. We propose several efficient solutions for this problem. One\nof them uses $n(\\log\\sigma(1+o(1))+\\log D+O(h)) + O(\\Delta)$ bits of space and\n$O(|p|+t)$ query time, where $n$ is the total length of the documents, $\\sigma$\nthe size of the alphabet used in the documents and $\\Delta$ is the total number\nof nodes in the category tree. Another solution uses\n$n(\\log\\sigma(1+o(1))+O(\\log D))+O(\\Delta)+O(D\\log n)$ bits of space and\n$O(|p|+t\\log D)$ query time. We finally propose other solutions which are more\nspace-efficient at the expense of a slight increase in query time.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 17:56:42 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Belazzougui", "Djamal", ""], ["Kucherov", "Gregory", ""]]}, {"id": "2006.01933", "submitter": "Danny Vainstein", "authors": "Noga Alon, Yossi Azar, Danny Vainstein", "title": "Hierarchical Clustering: a 0.585 Revenue Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Clustering trees have been widely accepted as a useful form of\nclustering data, resulting in a prevalence of adopting fields including\nphylogenetics, image analysis, bioinformatics and more. Recently, Dasgupta\n(STOC 16') initiated the analysis of these types of algorithms through the\nlenses of approximation. Later, the dual problem was considered by Moseley and\nWang (NIPS 17') dubbing it the Revenue goal function. In this problem, given a\nnonnegative weight $w_{ij}$ for each pair $i,j \\in [n]=\\{1,2, \\ldots ,n\\}$, the\nobjective is to find a tree $T$ whose set of leaves is $[n]$ that maximizes the\nfunction $\\sum_{i<j \\in [n]} w_{ij} (n -|T_{ij}|)$, where $|T_{ij}|$ is the\nnumber of leaves in the subtree rooted at the least common ancestor of $i$ and\n$j$.\n  In our work we consider the revenue goal function and prove the following\nresults. First, we prove the existence of a bisection (i.e., a tree of depth 2\nin which the root has two children, each being a parent of $n/2$ leaves) which\napproximates the general optimal tree solution up to a factor of $\\frac{1}{2}$\n(which is tight). Second, we apply this result in order to prove a\n$\\frac{2}{3}p$ approximation for the general revenue problem, where $p$ is\ndefined as the approximation ratio of the Max-Uncut Bisection problem. Since\n$p$ is known to be at least 0.8776 (Wu et al., 2015, Austrin et al., 2016), we\nget a 0.585 approximation algorithm for the revenue problem. This improves a\nsequence of earlier results which culminated in an 0.4246-approximation\nguarantee (Ahmadian et al., 2019).\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 20:37:57 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Alon", "Noga", ""], ["Azar", "Yossi", ""], ["Vainstein", "Danny", ""]]}, {"id": "2006.01944", "submitter": "Jason Huang", "authors": "Aditya Dhar, Jason Huang", "title": "Designing Differentially Private Estimators in High Dimensions", "comments": "9 pages, 3 figures, presented at the ICML 2020 Workshop on Economics\n  of Privacy and Data Labor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study differentially private mean estimation in a high-dimensional\nsetting. Existing differential privacy techniques applied to large dimensions\nlead to computationally intractable problems or estimators with excessive\nprivacy loss. Recent work in high-dimensional robust statistics has identified\ncomputationally tractable mean estimation algorithms with asymptotic\ndimension-independent error guarantees. We incorporate these results to develop\na strict bound on the global sensitivity of the robust mean estimator. This\nyields a computationally tractable algorithm for differentially private mean\nestimation in high dimensions with dimension-independent privacy loss. Finally,\nwe show on synthetic data that our algorithm significantly outperforms classic\ndifferential privacy methods, overcoming barriers to high-dimensional\ndifferential privacy.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 21:17:30 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 01:05:12 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 17:01:13 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Dhar", "Aditya", ""], ["Huang", "Jason", ""]]}, {"id": "2006.01975", "submitter": "Yu Cheng", "authors": "Ruoxu Cen, Yu Cheng, Debmalya Panigrahi, Kevin Sun", "title": "Sparsification of Directed Graphs via Cut Balance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of designing cut sparsifiers and\nsketches for directed graphs. To bypass known lower bounds, we allow the\nsparsifier/sketch to depend on the balance of the input graph, which smoothly\ninterpolates between undirected and directed graphs. We give nearly matching\nupper and lower bounds for both for-all (cf. Bencz\\'ur and Karger, STOC 1996)\nand for-each (Andoni et al., ITCS 2016) cut sparsifiers/sketches as a function\nof cut balance, defined the maximum ratio of the cut value in the two\ndirections of a directed graph (Ene et al., STOC 2016). We also show an\ninteresting application of digraph sparsification via cut balance by using it\nto give a very short proof of a celebrated maximum flow result of Karger and\nLevine (STOC 2002).\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 23:14:03 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 21:31:17 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Cen", "Ruoxu", ""], ["Cheng", "Yu", ""], ["Panigrahi", "Debmalya", ""], ["Sun", "Kevin", ""]]}, {"id": "2006.02134", "submitter": "Takuya Mieno", "authors": "Takuya Mieno, Kiichi Watanabe, Yuto Nakashima, Shunsuke Inenaga, Hideo\n  Bannai and Masayuki Takeda", "title": "Palindromic Trees for a Sliding Window and Its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The palindromic tree (a.k.a. eertree) for a string $S$ of length $n$ is a\ntree-like data structure that represents the set of all distinct palindromic\nsubstrings of $S$, using $O(n)$ space [Rubinchik and Shur, 2018]. It is known\nthat, when $S$ is over an alphabet of size $\\sigma$ and is given in an online\nmanner, then the palindromic tree of $S$ can be constructed in $O(n\\log\\sigma)$\ntime with $O(n)$ space. In this paper, we consider the sliding window version\nof the problem: For a sliding window of length at most $d$, we present two\nversions of an algorithm which maintains the palindromic tree of size $O(d)$\nfor every sliding window $S[i..j]$ over $S$, where $1 \\leq j-i+1 \\leq d$. The\nfirst version works in $O(n\\log\\sigma')$ time with $O(d)$ space where $\\sigma'\n\\leq d$ is the maximum number of distinct characters in the windows, and the\nsecond one works in $O(n + d\\sigma)$ time with $(d+2)\\sigma + O(d)$ space. We\nalso show how our algorithms can be applied to efficient computation of minimal\nunique palindromic substrings (MUPS) and minimal absent palindromic words\n(MAPW) for a sliding window.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 10:02:51 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 09:15:50 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Mieno", "Takuya", ""], ["Watanabe", "Kiichi", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "2006.02219", "submitter": "Jonas Ellert", "authors": "Jonas Ellert, Johannes Fischer, Nodari Sitchinava", "title": "LCP-Aware Parallel String Sorting", "comments": "Accepted at Euro-Par 2020 and to be published by Springer as part of\n  the conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When lexicographically sorting strings, it is not always necessary to inspect\nall symbols. For example, the lexicographical rank of \"europar\" amongst the\nstrings \"eureka\", \"eurasia\", and \"excells\" only depends on its so called\nrelevant prefix \"euro\". The distinguishing prefix size $D$ of a set of strings\nis the number of symbols that actually need to be inspected to establish the\nlexicographical ordering of all strings. Efficient string sorters should be\n$D$-aware, i.e. their complexity should depend on $D$ rather than on the total\nnumber $N$ of all symbols in all strings. While there are many $D$-aware\nsorters in the sequential setting, there appear to be no such results in the\nPRAM model. We propose a framework yielding a $D$-aware modification of any\nexisting PRAM string sorter. The derived algorithms are work-optimal with\nrespect to their original counterpart: If the original algorithm requires\n$O(w(N))$ work, the derived one requires $O(w(D))$ work. The execution time\nincreases only by a small factor that is logarithmic in the length of the\nlongest relevant prefix. Our framework universally works for deterministic and\nrandomized algorithms in all variations of the PRAM model, such that future\nimprovements in ($D$-unaware) parallel string sorting will directly result in\nimprovements in $D$-aware parallel string sorting.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 12:30:53 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Ellert", "Jonas", ""], ["Fischer", "Johannes", ""], ["Sitchinava", "Nodari", ""]]}, {"id": "2006.02249", "submitter": "Marc Hellmuth", "authors": "David Schaller, Manuela Gei{\\ss}, Peter F. Stadler and Marc Hellmuth", "title": "Complete Characterization of Incorrect Orthology Assignments in Best\n  Match Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-scale orthology assignments are usually based on reciprocal best\nmatches. In the absence of horizontal gene transfer (HGT), every pair of\northologs forms a reciprocal best match. Incorrect orthology assignments\ntherefore are always false positives in the reciprocal best match graph. We\nconsider duplication/loss scenarios and characterize unambiguous false-positive\n(u-fp) orthology assignments, that is, edges in the best match graphs (BMGs)\nthat cannot correspond to orthologs for any gene tree that explains the BMG.\nMoreover, we provide a polynomial-time algorithm to identify all u-fp orthology\nassignments in a BMG. Simulations show that at least $75\\%$ of all incorrect\northology assignments can be detected in this manner. All results rely only on\nthe structure of the BMGs and not on any a priori knowledge about underlying\ngene or species trees.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 13:06:57 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 12:48:15 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Schaller", "David", ""], ["Gei\u00df", "Manuela", ""], ["Stadler", "Peter F.", ""], ["Hellmuth", "Marc", ""]]}, {"id": "2006.02399", "submitter": "Nave Frost", "authors": "Nave Frost, Michal Moshkovitz, Cyrus Rashtchian", "title": "ExKMC: Expanding Explainable $k$-Means Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the popularity of explainable AI, there is limited work on effective\nmethods for unsupervised learning. We study algorithms for $k$-means\nclustering, focusing on a trade-off between explainability and accuracy.\nFollowing prior work, we use a small decision tree to partition a dataset into\n$k$ clusters. This enables us to explain each cluster assignment by a short\nsequence of single-feature thresholds. While larger trees produce more accurate\nclusterings, they also require more complex explanations. To allow flexibility,\nwe develop a new explainable $k$-means clustering algorithm, ExKMC, that takes\nan additional parameter $k' \\geq k$ and outputs a decision tree with $k'$\nleaves. We use a new surrogate cost to efficiently expand the tree and to label\nthe leaves with one of $k$ clusters. We prove that as $k'$ increases, the\nsurrogate cost is non-increasing, and hence, we trade explainability for\naccuracy. Empirically, we validate that ExKMC produces a low cost clustering,\noutperforming both standard decision tree methods and other algorithms for\nexplainable clustering. Implementation of ExKMC available at\nhttps://github.com/navefr/ExKMC.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 17:14:55 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 01:24:51 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Frost", "Nave", ""], ["Moshkovitz", "Michal", ""], ["Rashtchian", "Cyrus", ""]]}, {"id": "2006.02408", "submitter": "Panagiotis Charalampopoulos", "authors": "Panagiotis Charalampopoulos, Pawe{\\l} Gawrychowski, Karol Pokorski", "title": "Dynamic Longest Common Substring in Polylogarithmic Time", "comments": "Full version of a paper that is to appear in the ICALP 2020\n  proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The longest common substring problem consists in finding a longest string\nthat appears as a (contiguous) substring of two input strings. We consider the\ndynamic variant of this problem, in which we are to maintain two dynamic\nstrings $S$ and $T$, each of length at most $n$, that undergo substitutions of\nletters, in order to be able to return a longest common substring after each\nsubstitution. Recently, Amir et al. [ESA 2019] presented a solution for this\nproblem that needs only $\\tilde{\\mathcal{O}}(n^{2/3})$ time per update. This\nbrought the challenge of determining whether there exists a faster solution\nwith polylogarithmic update time, or (as is the case for other dynamic\nproblems), we should expect a polynomial (conditional) lower bound. We answer\nthis question by designing a significantly faster algorithm that processes each\nsubstitution in amortized $\\log^{\\mathcal{O}(1)} n$ time with high probability.\nOur solution relies on exploiting the local consistency of the parsing of a\ncollection of dynamic strings due to Gawrychowski et al. [SODA 2018], and on\nmaintaining two dynamic trees with labeled bicolored leaves, so that after each\nupdate we can report a pair of nodes, one from each tree, of maximum combined\nweight, which have at least one common leaf-descendant of each color. We\ncomplement this with a lower bound of $\\Omega(\\log n/ \\log\\log n)$ for the\nupdate time of any polynomial-size data structure that maintains the LCS of two\ndynamic strings, and the same lower bound for the update time of any data\nstructure of size $\\tilde{\\mathcal{O}}(n)$ that maintains the LCS of a static\nand a dynamic string. Both lower bounds hold even allowing amortization and\nrandomization.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 17:33:31 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Charalampopoulos", "Panagiotis", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Pokorski", "Karol", ""]]}, {"id": "2006.03134", "submitter": "Allen Liu", "authors": "Allen Liu and Ankur Moitra", "title": "Tensor Completion Made Practical", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor completion is a natural higher-order generalization of matrix\ncompletion where the goal is to recover a low-rank tensor from sparse\nobservations of its entries. Existing algorithms are either heuristic without\nprovable guarantees, based on solving large semidefinite programs which are\nimpractical to run, or make strong assumptions such as requiring the factors to\nbe nearly orthogonal. In this paper we introduce a new variant of alternating\nminimization, which in turn is inspired by understanding how the progress\nmeasures that guide convergence of alternating minimization in the matrix\nsetting need to be adapted to the tensor setting. We show strong provable\nguarantees, including showing that our algorithm converges linearly to the true\ntensors even when the factors are highly correlated and can be implemented in\nnearly linear time. Moreover our algorithm is also highly practical and we show\nthat we can complete third order tensors with a thousand dimensions from\nobserving a tiny fraction of its entries. In contrast, and somewhat\nsurprisingly, we show that the standard version of alternating minimization,\nwithout our new twist, can converge at a drastically slower rate in practice.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 21:09:44 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Liu", "Allen", ""], ["Moitra", "Ankur", ""]]}, {"id": "2006.03176", "submitter": "Kapil Vaidya", "authors": "Kapil Vaidya, Eric Knorr, Tim Kraska, Michael Mitzenmacher", "title": "Partitioned Learned Bloom Filter", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bloom filters are space-efficient probabilistic data structures that are used\nto test whether an element is a member of a set, and may return false\npositives. Recently, variations referred to as learned Bloom filters were\ndeveloped that can provide improved performance in terms of the rate of false\npositives, by using a learned model for the represented set. However, previous\nmethods for learned Bloom filters do not take full advantage of the learned\nmodel. Here we show how to frame the problem of optimal model utilization as an\noptimization problem, and using our framework derive algorithms that can\nachieve near-optimal performance in many cases. Experimental results from both\nsimulated and real-world datasets show significant performance improvements\nfrom our optimization approach over both the original learned Bloom filter\nconstructions and previously proposed heuristic improvements.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 00:05:32 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 15:15:17 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Vaidya", "Kapil", ""], ["Knorr", "Eric", ""], ["Kraska", "Tim", ""], ["Mitzenmacher", "Michael", ""]]}, {"id": "2006.03198", "submitter": "Xiaolong Wan", "authors": "Xiaolong Wan, Hongzhi Wang", "title": "Efficient Semi-External Depth-First Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing Depth-First Search (DFS) results, i.e. depth-first order or\nDFS-Tree, on the semi-external environment becomes a hot topic, because the\nscales of the graphs grow rapidly which can hardly be hold in the main memory,\nin the big data era. Existing semi-external DFS algorithms assume the main\nmemory could, at least, hold a spanning tree T of a graph G, and gradually\nrestructure T into a DFS-Tree, which is non-trivial. In this paper, we present\na comprehensive study of semi-external DFS problem, including the first\ntheoretical analysis of the main challenge of this problem, as far as we know.\nBesides, we introduce a new semi-external DFS algorithm with an efficient edge\npruning principle, named EP-DFS. Unlike the traditional algorithms, we not only\nfocus on addressing such complex problem efficiently with less I/Os, but also\nfocus on that with simpler CPU calculation (Implementation-friendly) and less\nrandom I/O access (key-to-efficiency). The former is based on our efficient\npruning principle; the latter is addressed by a lightweight index N+-index,\nwhich is a compressed storage for a subset of the edges for G. The extensive\nexperimental evaluation on both synthetic and real graphs confirms that our\nEP-DFS algorithm outperforms the existing techniques.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 01:55:22 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 01:12:06 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Wan", "Xiaolong", ""], ["Wang", "Hongzhi", ""]]}, {"id": "2006.03213", "submitter": "Dmitriy Kunisky", "authors": "Alberto Del Pia, Aida Khajavirad, Dmitriy Kunisky", "title": "Linear Programming and Community Detection", "comments": "35 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of community detection with two equal-sized communities is\nclosely related to the minimum graph bisection problem over certain random\ngraph models. In the stochastic block model distribution over networks with\ncommunity structure, a well-known semidefinite programming (SDP) relaxation of\nthe minimum bisection problem recovers the underlying communities whenever\npossible. Motivated by their superior scalability, we study the theoretical\nperformance of linear programming (LP) relaxations of the minimum bisection\nproblem for the same random models. We show that unlike the SDP relaxation that\nundergoes a phase transition in the logarithmic average-degree regime, the LP\nrelaxation exhibits a transition from recovery to non-recovery in the linear\naverage-degree regime. We show that in the logarithmic average-degree regime,\nthe LP relaxation fails in recovering the planted bisection with high\nprobability.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 03:01:21 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Del Pia", "Alberto", ""], ["Khajavirad", "Aida", ""], ["Kunisky", "Dmitriy", ""]]}, {"id": "2006.03363", "submitter": "Amjad Ibrahim", "authors": "Amjad Ibrahim and Alexander Pretschner", "title": "From Checking to Inference: Actual Causality Computations as\n  Optimization Problems", "comments": "ATVA 2020 The 18th International Symposium on Automated Technology\n  for Verification and Analysis", "journal-ref": null, "doi": "10.1007/978-3-030-59152-6_19", "report-no": null, "categories": "cs.AI cs.CY cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Actual causality is increasingly well understood. Recent formal approaches,\nproposed by Halpern and Pearl, have made this concept mature enough to be\namenable to automated reasoning. Actual causality is especially vital for\nbuilding accountable, explainable systems. Among other reasons, causality\nreasoning is computationally hard due to the requirements of counterfactuality\nand the minimality of causes. Previous approaches presented either inefficient\nor restricted, and domain-specific, solutions to the problem of automating\ncausality reasoning. In this paper, we present a novel approach to formulate\ndifferent notions of causal reasoning, over binary acyclic models, as\noptimization problems, based on quantifiable notions within counterfactual\ncomputations. We contribute and compare two compact, non-trivial, and sound\ninteger linear programming (ILP) and Maximum Satisfiability (MaxSAT) encodings\nto check causality. Given a candidate cause, both approaches identify what a\nminimal cause is. Also, we present an ILP encoding to infer causality without\nrequiring a candidate cause. We show that both notions are efficiently\nautomated. Using models with more than $8000$ variables, checking is computed\nin a matter of seconds, with MaxSAT outperforming ILP in many cases. In\ncontrast, inference is computed in a matter of minutes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 10:56:52 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 11:28:01 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Ibrahim", "Amjad", ""], ["Pretschner", "Alexander", ""]]}, {"id": "2006.03365", "submitter": "Rosario Scatamacchia", "authors": "David Pisinger, Rosario Scatamacchia", "title": "The Baggage Belt Assignment Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of assigning flights to baggage belts in the baggage\nreclaim area of an airport. The problem is originated by a real-life\napplication in Copenhagen airport. The objective is to construct a robust\nschedule taking passenger and airline preferences into account. We consider a\nnumber of business and fairness constraints, avoiding congestions, and ensuring\na good passenger flow. Robustness of the solutions is achieved by matching the\ndelivery time with the expected arrival time of passengers, and by adding\nbuffer time between two flights scheduled on the same belt. We denote this\nproblem as the Baggage Belt Assignment Problem (BBAP). We first derive a\ngeneral Integer Linear Programming (ILP) formulation for the problem. Then, we\npropose a Branch-and-Price (B&P) algorithm based on a reformulation of the ILP\nmodel tackled by Column Generation. Our approach relies on an effective dynamic\nprogramming algorithm for handling the pricing problems. We tested the proposed\nalgorithm on a set of real-life data from Copenhagen airport as well as on a\nset of instances inspired by the real data. Our B&P scheme outperforms a\ncommercial solver launched on the ILP formulation of the problem and is\neffective in delivering high quality solutions in limited computational times,\nmaking it possible its use in daily operations in medium-sized and large\nairports.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 10:59:59 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 07:07:42 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Pisinger", "David", ""], ["Scatamacchia", "Rosario", ""]]}, {"id": "2006.03372", "submitter": "Qiangqiang Dai", "authors": "Qiangqiang Dai, Rong-Hua Li, Lu Qin, Guoren Wang, Weihua Yang, Zhiwei\n  Zhang and Ye Yuan", "title": "Scaling Up Distanced-generalized Core Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Core decomposition is a fundamental operator in network analysis. In this\npaper, we study a problem of computing distance-generalized core decomposition\non a network. A distance-generalized core, also termed $(k, h)$-core, is a\nmaximal subgraph in which every vertex has at least $k$ other vertices at\ndistance no larger than $h$. The state-of-the-art algorithm for solving this\nproblem is based on a peeling technique which iteratively removes the vertex\n(denoted by $v$) from the graph that has the smallest $h$-degree. The\n$h$-degree of a vertex $v$ denotes the number of other vertices that are\nreachable from $v$ within $h$ hops. Such a peeling algorithm, however, needs to\nfrequently recompute the $h$-degrees of $v$'s neighbors after deleting $v$,\nwhich is typically very costly for a large $h$. To overcome this limitation, we\npropose an efficient peeling algorithm based on a novel $h$-degree updating\ntechnique. Instead of recomputing the $h$-degrees, our algorithm can\ndynamically maintain the $h$-degrees for all vertices via exploring a very\nsmall subgraph, after peeling a vertex. We show that such an $h$-degree\nupdating procedure can be efficiently implemented by an elegant bitmap\ntechnique. In addition, we also propose a sampling-based algorithm and a\nparallelization technique to further improve the efficiency. Finally, we\nconduct extensive experiments on 12 real-world graphs to evaluate our\nalgorithms. The results show that, when $h\\ge 3$, our exact and sampling-based\nalgorithms can achieve up to $10\\times$ and $100\\times$ speedup over the\nstate-of-the-art algorithm, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 11:19:17 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Dai", "Qiangqiang", ""], ["Li", "Rong-Hua", ""], ["Qin", "Lu", ""], ["Wang", "Guoren", ""], ["Yang", "Weihua", ""], ["Zhang", "Zhiwei", ""], ["Yuan", "Ye", ""]]}, {"id": "2006.03399", "submitter": "Morteza Davari", "authors": "Dirk Briskorn, Morteza Davari, Jannik Matuschke", "title": "Single-machine scheduling with an external resource", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the complexity of single-machine scheduling with an\nexternal resource, which is rented for a non-interrupted period. Jobs that need\nthis external resource are executed only when the external resource is\navailable. There is a cost associated with the scheduling of jobs and a cost\nassociated with the duration of the renting period of the external resource. We\nlook at four classes of problems with an external resource: a class of problems\nwhere the renting period is budgeted and the scheduling cost needs to be\nminimized, a class of problems where the scheduling cost is budgeted and the\nrenting period needs to be minimized, a class of two-objective problems where\nboth, the renting period and the scheduling cost, are to be minimized, and a\nclass of problems where a linear combination of the scheduling cost and the\nrenting period is minimized. We provide a thorough complexity analysis\n(NP-hardness proofs and (pseudo-)polynomial algorithms) for different members\nof these four classes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 12:24:22 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 15:21:43 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Briskorn", "Dirk", ""], ["Davari", "Morteza", ""], ["Matuschke", "Jannik", ""]]}, {"id": "2006.03440", "submitter": "Ahad N. Zehmakan", "authors": "Ahad N. Zehmakan", "title": "Spread of Influence in Graphs", "comments": "arXiv admin note: text overlap with arXiv:1901.05917", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a graph $G$ and an initial configuration where each node is black or\nwhite. Assume that in each round all nodes simultaneously update their color\nbased on a predefined rule. One can think of graph $G$ as a social network,\nwhere each black/white node represents an individual who holds a\npositive/negative opinion regarding a particular topic. In the $r$-threshold\n(resp. $\\alpha$-threshold) model, a node becomes black if at least $r$ of its\nneighbors (resp. $\\alpha$ fraction of its neighbors) are black, and white\notherwise. The $r$-monotone (resp. $\\alpha$-monotone) model is the same as the\n$r$-threshold (resp. $\\alpha$-threshold) model, except that a black node\nremains black forever.\n  What is the number of rounds that the process needs to stabilize? How many\nnodes must be black initially so that black color takes over or survives? Our\nmain goal in the present paper is to address these two questions\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 23:28:09 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Zehmakan", "Ahad N.", ""]]}, {"id": "2006.03460", "submitter": "Logan Smith", "authors": "Logan A. Smith and Illya V. Hicks", "title": "Optimal Sensor Placement in Power Grids: Power Domination, Set Covering,\n  and the Neighborhoods of Zero Forcing Forts", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To monitor electrical activity throughout the power grid and mitigate\noutages, sensors known as phasor measurement units can installed. Due to\nimplementation costs, it is desirable to minimize the number of sensors\ndeployed while ensuring that the grid can be effectively monitored. This\noptimization problem motivates the graph theoretic power dominating set\nproblem. In this paper, we propose a novel integer program for identifying\nminimum power dominating sets by formulating a set cover problem. This\nproblem's constraints correspond to neighborhoods of zero forcing forts; we\nstudy their structural properties and show they can be separated, allowing the\nproposed model to be solved via row generation. The proposed and existing\nmethods are compared in several computational experiments in which the proposed\nmethod consistently exhibits an order of magnitude improvement in runtime\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 17:54:41 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Smith", "Logan A.", ""], ["Hicks", "Illya V.", ""]]}, {"id": "2006.03666", "submitter": "Jason Schoeters", "authors": "Arnaud Casteigts, Mathieu Raffinot, Jason Schoeters", "title": "VectorTSP: A Traveling Salesperson Problem with Racetrack-like\n  acceleration constraints", "comments": "12 pages, 22 pages with bibliography and appendices, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a new version of the Euclidean TSP called VectorTSP (VTSP for short)\nwhere a mobile entity is allowed to move according to a set of physical\nconstraints inspired from the pen-and-pencil game Racetrack (also known as\nVector Racer ). In contrast to other versions of TSP accounting for physical\nconstraints, such as Dubins TSP, the spirit of this model is that (1) no speed\nlimitations apply, and (2) inertia depends on the current velocity. As such,\nthis model is closer to typical models considered in path planning problems,\nalthough applied here to the visit of n cities in a non-predetermined order. We\nmotivate and introduce the VectorTSP problem, discussing fundamental\ndifferences with previous versions of TSP. In particular, an optimal visit\norder for ETSP may not be optimal for VTSP. We show that VectorTSP is NP-hard,\nand in the other direction, that VectorTSP reduces to GroupTSP in polynomial\ntime (although with a significant blow-up in size). On the algorithmic side, we\nformulate the search for a solution as an interactive scheme between a\nhigh-level algorithm and a trajectory oracle, the former being responsible for\ncomputing the visit order and the latter for computing the cost (or the\ntrajectory) for a given visit order. We present algorithms for both, and we\ndemonstrate and quantify through experiments that this approach frequently\nfinds a better solution than the optimal trajectory realizing an optimal ETSP\ntour, which legitimates the problem itself and (we hope) motivates further\nalgorithmic developments.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 20:17:06 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 08:45:25 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Casteigts", "Arnaud", ""], ["Raffinot", "Mathieu", ""], ["Schoeters", "Jason", ""]]}, {"id": "2006.03684", "submitter": "Damien Desfontaines", "authors": "Damien Desfontaines, James Voss, Bryant Gipson, Chinmoy Mandayam", "title": "Differentially private partition selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many data analysis operations can be expressed as a GROUP BY query on an\nunbounded set of partitions, followed by a per-partition aggregation. To make\nsuch a query differentially private, adding noise to each aggregation is not\nenough: we also need to make sure that the set of partitions released is also\ndifferentially private.\n  This problem is not new, and it was recently formally introduced as\ndifferentially private set union. In this work, we continue this area of study,\nand focus on the common setting where each user is associated with a single\npartition. In this setting, we propose a simple, optimal differentially private\nmechanism that maximizes the number of released partitions. We discuss\nimplementation considerations, as well as the possible extension of this\napproach to the setting where each user contributes to a fixed, small number of\npartitions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 21:03:01 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 22:26:25 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Desfontaines", "Damien", ""], ["Voss", "James", ""], ["Gipson", "Bryant", ""], ["Mandayam", "Chinmoy", ""]]}, {"id": "2006.03746", "submitter": "Shreyas Pai", "authors": "Reuven Bar-Yehuda, Keren Censor-Hillel, Yannic Maus, Shreyas Pai,\n  Sriram V. Pemmaraju", "title": "Distributed Approximation on Power Graphs", "comments": "Appears in PODC 2020. 40 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate graph problems in the following setting: we are given a graph\n$G$ and we are required to solve a problem on $G^2$. While we focus mostly on\nexploring this theme in the distributed CONGEST model, we show new results and\nsurprising connections to the centralized model of computation. In the CONGEST\nmodel, it is natural to expect that problems on $G^2$ would be quite difficult\nto solve efficiently on $G$, due to congestion. However, we show that the\npicture is both more complicated and more interesting.\n  Specifically, we encounter two phenomena acting in opposing directions: (i)\nslowdown due to congestion and (ii) speedup due to structural properties of\n$G^2$.\n  We demonstrate these two phenomena via two fundamental graph problems,\nnamely, Minimum Vertex Cover (MVC) and Minimum Dominating Set (MDS). Among our\nmany contributions, the highlights are the following.\n  - In the CONGEST model, we show an $O(n/\\epsilon)$-round\n$(1+\\epsilon)$-approximation algorithm for MVC on $G^2$, while no\n$o(n^2)$-round algorithm is known for any better-than-2 approximation for MVC\non $G$.\n  - We show a centralized polynomial time $5/3$-approximation algorithm for MVC\non $G^2$, whereas a better-than-2 approximation is UGC-hard for $G$.\n  - In contrast, for MDS, in the CONGEST model, we show an\n$\\tilde{\\Omega}(n^2)$ lower bound for a constant approximation factor for MDS\non $G^2$, whereas an $\\Omega(n^2)$ lower bound for MDS on $G$ is known only for\nexact computation.\n  In addition to these highlighted results, we prove a number of other results\nin the distributed CONGEST model including an $\\tilde{\\Omega}(n^2)$ lower bound\nfor computing an exact solution to MVC on $G^2$, a conditional hardness result\nfor obtaining a $(1+\\epsilon)$-approximation to MVC on $G^2$, and an $O(\\log\n\\Delta)$-approximation to the MDS problem on $G^2$ in $\\mbox{poly}\\log n$\nrounds.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 01:14:30 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Bar-Yehuda", "Reuven", ""], ["Censor-Hillel", "Keren", ""], ["Maus", "Yannic", ""], ["Pai", "Shreyas", ""], ["Pemmaraju", "Sriram V.", ""]]}, {"id": "2006.03781", "submitter": "Jie Shen", "authors": "Jie Shen and Chicheng Zhang", "title": "Attribute-Efficient Learning of Halfspaces with Malicious Noise:\n  Near-Optimal Label Complexity and Noise Tolerance", "comments": "V1/V2 had a problematic argument on polynomial-time solvability of a\n  form of sparse principal component analysis. V3 fixed it by using a new\n  approach based on semidefinite programming. V4/V5 polishes the writing and is\n  accepted to ALT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is concerned with computationally efficient learning of\nhomogeneous sparse halfspaces in $\\mathbb{R}^d$ under noise. Though recent\nworks have established attribute-efficient learning algorithms under various\ntypes of label noise (e.g. bounded noise), it remains an open question when and\nhow $s$-sparse halfspaces can be efficiently learned under the challenging\nmalicious noise model, where an adversary may corrupt both the unlabeled\nexamples and the labels. We answer this question in the affirmative by\ndesigning a computationally efficient active learning algorithm with\nnear-optimal label complexity of $\\tilde{O}\\big({s \\log^4 \\frac d \\epsilon}\n\\big)$ and noise tolerance $\\eta = \\Omega(\\epsilon)$, where $\\epsilon \\in (0,\n1)$ is the target error rate, under the assumption that the distribution over\n(uncorrupted) unlabeled examples is isotropic log-concave. Our algorithm can be\nstraightforwardly tailored to the passive learning setting, and we show that\nthe sample complexity is $\\tilde{O}\\big({\\frac 1 \\epsilon s^2 \\log^5 d} \\big)$\nwhich also enjoys the attribute efficiency. Our main techniques include\nattribute-efficient paradigms for instance reweighting and for empirical risk\nminimization, and a new analysis of uniform concentration for unbounded data --\nall of them crucially take the structure of the underlying halfspace into\naccount.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 04:57:39 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 01:57:38 GMT"}, {"version": "v3", "created": "Tue, 11 Aug 2020 07:26:35 GMT"}, {"version": "v4", "created": "Wed, 23 Dec 2020 05:47:58 GMT"}, {"version": "v5", "created": "Tue, 2 Mar 2021 07:19:55 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Shen", "Jie", ""], ["Zhang", "Chicheng", ""]]}, {"id": "2006.03856", "submitter": "Mordechai Shalom", "authors": "Arman Boyac{\\i} and T{\\i}naz Ekim and Mordechai Shalom", "title": "On the Maximum Cardinality Cut Problem in Proper Interval Graphs and\n  Related Graph Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although it has been claimed in two different papers that the maximum\ncardinality cut problem is polynomial-time solvable for proper interval graphs,\nboth of them turned out to be erroneous. In this paper, we give FPT algorithms\nfor the maximum cardinality cut problem in classes of graphs containing proper\ninterval graphs and mixed unit interval graphs when parameterized by some new\nparameters that we introduce. These new parameters are related to a\ngeneralization of the so-called bubble representations of proper interval\ngraphs and mixed unit interval graphs and to clique-width decompositions.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 12:55:59 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Boyac\u0131", "Arman", ""], ["Ekim", "T\u0131naz", ""], ["Shalom", "Mordechai", ""]]}, {"id": "2006.04094", "submitter": "Pan Peng", "authors": "Pan Peng, Yuichi Yoshida", "title": "Average Sensitivity of Spectral Clustering", "comments": "KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is one of the most popular clustering methods for finding\nclusters in a graph, which has found many applications in data mining. However,\nthe input graph in those applications may have many missing edges due to error\nin measurement, withholding for a privacy reason, or arbitrariness in data\nconversion. To make reliable and efficient decisions based on spectral\nclustering, we assess the stability of spectral clustering against edge\nperturbations in the input graph using the notion of average sensitivity, which\nis the expected size of the symmetric difference of the output clusters before\nand after we randomly remove edges.\n  We first prove that the average sensitivity of spectral clustering is\nproportional to $\\lambda_2/\\lambda_3^2$, where $\\lambda_i$ is the $i$-th\nsmallest eigenvalue of the (normalized) Laplacian. We also prove an analogous\nbound for $k$-way spectral clustering, which partitions the graph into $k$\nclusters. Then, we empirically confirm our theoretical bounds by conducting\nexperiments on synthetic and real networks. Our results suggest that spectral\nclustering is stable against edge perturbations when there is a cluster\nstructure in the input graph.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 09:14:44 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Peng", "Pan", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "2006.04166", "submitter": "Rares-Darius Buhai", "authors": "Guy Bresler, Rares-Darius Buhai", "title": "Learning Restricted Boltzmann Machines with Sparse Latent Variables", "comments": "33 pages, to appear at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machines (RBMs) are a common family of undirected\ngraphical models with latent variables. An RBM is described by a bipartite\ngraph, with all observed variables in one layer and all latent variables in the\nother. We consider the task of learning an RBM given samples generated\naccording to it. The best algorithms for this task currently have time\ncomplexity $\\tilde{O}(n^2)$ for ferromagnetic RBMs (i.e., with attractive\npotentials) but $\\tilde{O}(n^d)$ for general RBMs, where $n$ is the number of\nobserved variables and $d$ is the maximum degree of a latent variable. Let the\nMRF neighborhood of an observed variable be its neighborhood in the Markov\nRandom Field of the marginal distribution of the observed variables. In this\npaper, we give an algorithm for learning general RBMs with time complexity\n$\\tilde{O}(n^{2^s+1})$, where $s$ is the maximum number of latent variables\nconnected to the MRF neighborhood of an observed variable. This is an\nimprovement when $s < \\log_2 (d-1)$, which corresponds to RBMs with sparse\nlatent variables. Furthermore, we give a version of this learning algorithm\nthat recovers a model with small prediction error and whose sample complexity\nis independent of the minimum potential in the Markov Random Field of the\nobserved variables. This is of interest because the sample complexity of\ncurrent algorithms scales with the inverse of the minimum potential, which\ncannot be controlled in terms of natural properties of the RBM.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 14:42:50 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 18:54:36 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Bresler", "Guy", ""], ["Buhai", "Rares-Darius", ""]]}, {"id": "2006.04324", "submitter": "Morgan Chopin", "authors": "Amal Benhamiche and Morgan Chopin", "title": "Toward Scalable Algorithms for the Unsplittable Shortest Path Routing\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the Delay Constrained Unsplittable Shortest Path\nRouting problem which arises in the field of traffic engineering for IP\nnetworks. This problem consists, given a directed graph and a set of\ncommodities, to compute a set of routing paths and the associated\nadministrative weights such that each commodity is routed along the unique\nshortest path between its origin and its destination, according to these\nweights. We present a compact MILP formulation for the problem, extending the\nwork in (A. Bley, 2010) along with some valid inequalities to strengthen its\nlinear relaxation. This formulation is used as the bulding block of an\niterative approach that we develop to tackle large scale instances. We further\npropose a dynamic programming algorithm based on a tree decomposition of the\ngraph. To the best of our knowledge, this is the first exact combinatorial\nalgorithm for the problem. Finally, we assess the efficiency of our approaches\nthrough a set of experiments on state-of-the-art instances.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 02:29:40 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Benhamiche", "Amal", ""], ["Chopin", "Morgan", ""]]}, {"id": "2006.04351", "submitter": "Yu Chen", "authors": "Yu Chen, Sampath Kannan, Sanjeev Khanna", "title": "Near-Perfect Recovery in the One-Dimensional Latent Space Model", "comments": "Web Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose a graph $G$ is stochastically created by uniformly sampling vertices\nalong a line segment and connecting each pair of vertices with a probability\nthat is a known decreasing function of their distance. We ask if it is possible\nto reconstruct the actual positions of the vertices in $G$ by only observing\nthe generated unlabeled graph. We study this question for two natural edge\nprobability functions -- one where the probability of an edge decays\nexponentially with the distance and another where this probability decays only\nlinearly. We initiate our study with the weaker goal of recovering only the\norder in which vertices appear on the line segment. For a segment of length $n$\nand a precision parameter $\\delta$, we show that for both exponential and\nlinear decay edge probability functions, there is an efficient algorithm that\ncorrectly recovers (up to reflection symmetry) the order of all vertices that\nare at least $\\delta$ apart, using only $\\tilde{O}(\\frac{n}{\\delta ^ 2})$\nsamples (vertices). Building on this result, we then show that $O(\\frac{n^2\n\\log n}{\\delta ^2})$ vertices (samples) are sufficient to additionally recover\nthe location of each vertex on the line to within a precision of $\\delta$. We\ncomplement this result with an $\\Omega (\\frac{n^{1.5}}{\\delta})$ lower bound on\nsamples needed for reconstructing positions (even by a computationally\nunbounded algorithm), showing that the task of recovering positions is\ninformation-theoretically harder than recovering the order. We give\nexperimental results showing that our algorithm recovers the positions of\nalmost all points with high accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 04:49:41 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Chen", "Yu", ""], ["Kannan", "Sampath", ""], ["Khanna", "Sanjeev", ""]]}, {"id": "2006.04411", "submitter": "Andreas Emil Feldmann", "authors": "Andreas Emil Feldmann, Karthik C. S., Euiwoong Lee, Pasin Manurangsi", "title": "A Survey on Approximation in Parameterized Complexity: Hardness and\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameterization and approximation are two popular ways of coping with\nNP-hard problems. More recently, the two have also been combined to derive many\ninteresting results. We survey developments in the area both from the\nalgorithmic and hardness perspectives, with emphasis on new techniques and\npotential future research directions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 08:34:47 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Feldmann", "Andreas Emil", ""], ["S.", "Karthik C.", ""], ["Lee", "Euiwoong", ""], ["Manurangsi", "Pasin", ""]]}, {"id": "2006.04554", "submitter": "Jayanth Jagalur Mohan", "authors": "Jayanth Jagalur-Mohan, Youssef Marzouk", "title": "Batch greedy maximization of non-submodular functions: Guarantees and\n  applications to experimental design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze batch greedy heuristics for cardinality constrained\nmaximization of non-submodular non-decreasing set functions. Our theoretical\nguarantees are characterized by the combination of submodularity and\nsupermodularity ratios. We argue how these parameters define tight modular\nbounds based on incremental gains, and provide a novel reinterpretation of the\nclassical greedy algorithm using the minorize-maximize (MM) principle. Based on\nthat analogy, we propose a new class of methods exploiting any plausible\nmodular bound. In the context of optimal experimental design for linear\nBayesian inverse problems, we bound the submodularity and supermodularity\nratios when the underlying objective is based on mutual information. We also\ndevelop novel modular bounds for the mutual information in this setting, and\ndescribe certain connections to polyhedral combinatorics. We discuss how\nalgorithms using these modular bounds relate to established statistical notions\nsuch as leverage scores and to more recent efforts such as volume sampling. We\ndemonstrate our theoretical findings on synthetic problems and on a real-world\nclimate monitoring example.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 18:58:06 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 02:14:56 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Jagalur-Mohan", "Jayanth", ""], ["Marzouk", "Youssef", ""]]}, {"id": "2006.04625", "submitter": "V\\'aclav Rozho\\v{n}", "authors": "Sebastian Brandt, Christoph Grunau, V\\'aclav Rozho\\v{n}", "title": "Generalizing the Sharp Threshold Phenomenon for the Distributed\n  Complexity of the Lov\\'asz Local Lemma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Brandt, Maus and Uitto [PODC'19] showed that, in a restricted\nsetting, the dependency of the complexity of the distributed Lov\\'asz Local\nLemma (LLL) on the chosen LLL criterion exhibits a sharp threshold phenomenon:\nThey proved that, under the LLL criterion $p2^d < 1$, if each random variable\naffects at most $3$ events, the deterministic complexity of the LLL in the\nLOCAL model is $O(d^2 + \\log^* n)$. In stark contrast, under the criterion\n$p2^d \\leq 1$, there is a randomized lower bound of $\\Omega(\\log \\log n)$ by\nBrandt et al. [STOC'16] and a deterministic lower bound of $\\Omega(\\log n)$ by\nChang, Kopelowitz and Pettie [FOCS'16]. Brandt, Maus and Uitto conjectured that\nthe same behavior holds for the unrestricted setting where each random variable\naffects arbitrarily many events.\n  We prove their conjecture, by providing an algorithm that solves the LLL in\ntime $O(d^2 + \\log^* n)$ under the LLL criterion $p2^d < 1$, which is tight in\nbounded-degree graphs due to an $\\Omega(\\log^* n)$ lower bound by Chung, Pettie\nand Su [PODC'14]. By the work of Brandt, Maus and Uitto, obtaining such an\nalgorithm can be reduced to proving that all members in a certain family of\nfunctions in arbitrarily high dimensions are convex on some specific domain.\nUnfortunately, an analytical description of these functions is known only for\ndimension at most $3$, which led to the aforementioned restriction of their\nresult. While obtaining those descriptions for functions of (substantially)\nhigher dimension seems out of the reach of current techniques, we show that\ntheir convexity can be inferred by combinatorial means.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 14:24:36 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Brandt", "Sebastian", ""], ["Grunau", "Christoph", ""], ["Rozho\u0148", "V\u00e1clav", ""]]}, {"id": "2006.04663", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr", "title": "Runtime Analysis of Evolutionary Algorithms via Symmetry Arguments", "comments": "Minor changes compared to the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use an elementary argument building on group actions to prove that the\nselection-free steady state genetic algorithm analyzed by Sutton and Witt\n(GECCO 2019) takes an expected number of $\\Omega(2^n / \\sqrt n)$ iterations to\nfind any particular target search point. This bound is valid for all population\nsizes $\\mu$. Our result improves over the previous lower bound of\n$\\Omega(\\exp(n^{\\delta/2}))$ valid for population sizes $\\mu = O(n^{1/2 -\n\\delta})$, $0 < \\delta < 1/2$.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 15:04:51 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 11:32:39 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 10:42:28 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Doerr", "Benjamin", ""]]}, {"id": "2006.04704", "submitter": "Jakub Tarnawski", "authors": "Silvio Lattanzi, Slobodan Mitrovi\\'c, Ashkan Norouzi-Fard, Jakub\n  Tarnawski, Morteza Zadimoghaddam", "title": "Fully Dynamic Algorithm for Constrained Submodular Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of maximizing a monotone submodular function under a cardinality\nconstraint is at the core of many machine learning and data mining\napplications, including data summarization, sparse regression and coverage\nproblems. We study this classic problem in the fully dynamic setting, where\nelements can be both inserted and removed. Our main result is a randomized\nalgorithm that maintains an efficient data structure with a poly-logarithmic\namortized update time and yields a $(1/2-\\epsilon)$-approximate solution. We\ncomplement our theoretical analysis with an empirical study of the performance\nof our algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 16:00:30 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Lattanzi", "Silvio", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Norouzi-Fard", "Ashkan", ""], ["Tarnawski", "Jakub", ""], ["Zadimoghaddam", "Morteza", ""]]}, {"id": "2006.04778", "submitter": "Vijay Keswani", "authors": "L. Elisa Celis and Lingxiao Huang and Vijay Keswani and Nisheeth K.\n  Vishnoi", "title": "Fair Classification with Noisy Protected Attributes: A Framework with\n  Provable Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an optimization framework for learning a fair classifier in the\npresence of noisy perturbations in the protected attributes. Compared to prior\nwork, our framework can be employed with a very general class of linear and\nlinear-fractional fairness constraints, can handle multiple, non-binary\nprotected attributes, and outputs a classifier that comes with provable\nguarantees on both accuracy and fairness. Empirically, we show that our\nframework can be used to attain either statistical rate or false positive rate\nfairness guarantees with a minimal loss in accuracy, even when the noise is\nlarge, in two real-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 17:52:48 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 14:18:18 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2021 17:21:58 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Celis", "L. Elisa", ""], ["Huang", "Lingxiao", ""], ["Keswani", "Vijay", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "2006.04787", "submitter": "Sitan Chen", "authors": "Sitan Chen, Frederic Koehler, Ankur Moitra, Morris Yau", "title": "Classification Under Misspecification: Halfspaces, Generalized Linear\n  Models, and Connections to Evolvability", "comments": "51 pages, comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit some classic problems on classification under\nmisspecification. In particular, we study the problem of learning halfspaces\nunder Massart noise with rate $\\eta$. In a recent work, Diakonikolas,\nGoulekakis, and Tzamos resolved a long-standing problem by giving the first\nefficient algorithm for learning to accuracy $\\eta + \\epsilon$ for any\n$\\epsilon > 0$. However, their algorithm outputs a complicated hypothesis,\nwhich partitions space into $\\text{poly}(d,1/\\epsilon)$ regions. Here we give a\nmuch simpler algorithm and in the process resolve a number of outstanding open\nquestions:\n  (1) We give the first proper learner for Massart halfspaces that achieves\n$\\eta + \\epsilon$. We also give improved bounds on the sample complexity\nachievable by polynomial time algorithms.\n  (2) Based on (1), we develop a blackbox knowledge distillation procedure to\nconvert an arbitrarily complex classifier to an equally good proper classifier.\n  (3) By leveraging a simple but overlooked connection to evolvability, we show\nany SQ algorithm requires super-polynomially many queries to achieve\n$\\mathsf{OPT} + \\epsilon$.\n  Moreover we study generalized linear models where $\\mathbb{E}[Y|\\mathbf{X}] =\n\\sigma(\\langle \\mathbf{w}^*, \\mathbf{X}\\rangle)$ for any odd, monotone, and\nLipschitz function $\\sigma$. This family includes the previously mentioned\nhalfspace models as a special case, but is much richer and includes other\nfundamental models like logistic regression. We introduce a challenging new\ncorruption model that generalizes Massart noise, and give a general algorithm\nfor learning in this setting. Our algorithms are based on a small set of core\nrecipes for learning to classify in the presence of misspecification.\n  Finally we study our algorithm for learning halfspaces under Massart noise\nempirically and find that it exhibits some appealing fairness properties.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 17:59:11 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Chen", "Sitan", ""], ["Koehler", "Frederic", ""], ["Moitra", "Ankur", ""], ["Yau", "Morris", ""]]}, {"id": "2006.04933", "submitter": "Neil Simonetti", "authors": "Robert D. Carr, Neil Simonetti", "title": "A New Integer Programming Formulation of the Graphical Traveling\n  Salesman Problem", "comments": "19 pages, only one figure from an external image", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Traveling Salesman Problem (TSP), a salesman wants to visit a set of\ncities and return home. There is a cost $c_{ij}$ of traveling from city $i$ to\ncity $j$, which is the same in either direction for the Symmetric TSP. The\nobjective is to visit each city exactly once, minimizing total travel costs. In\nthe Graphical TSP, a city may be visited more than once, which may be necessary\non a sparse graph. We present a new integer programming formulation for the\nGraphical TSP requiring only two classes of constraints that are either\npolynomial in number or polynomially separable, while addressing an open\nquestion proposed by Denis Naddef.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 20:58:23 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Carr", "Robert D.", ""], ["Simonetti", "Neil", ""]]}, {"id": "2006.04940", "submitter": "Andreas Vitalis", "authors": "Andreas Vitalis", "title": "An Improved and Parallel Version of a Scalable Algorithm for Analyzing\n  Time Series Data", "comments": "14 pages, 5 figures, 28 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, very large amounts of data are produced and stored in all branches of\nsociety including science. Mining these data meaningfully has become a\nconsiderable challenge and is of the broadest possible interest. The size, both\nin numbers of observations and dimensionality thereof, requires data mining\nalgorithms to possess time complexities with both variables that are linear or\nnearly linear. One such algorithm, see Comput. Phys. Commun. 184, 2446-2453\n(2013), arranges observations into a sequence called the progress index. The\nprogress index steps through distinct regions of high sampling density\nsequentially. By means of suitable annotations, it allows a compact\nrepresentation of the behavior of complex systems, which is encoded in the\noriginal data set. The only essential parameter is a notion of distance between\nobservations. Here, we present the shared memory parallelization of the key\nstep in constructing the progress index, which is the calculation of an\napproximation of the minimum spanning tree of the complete graph of\nobservations. We demonstrate that excellent parallel efficiencies are obtained\nfor up to 72 logical (CPU) cores. In addition, we introduce three conceptual\nadvances to the algorithm that improve its controllability and the\ninterpretability of the progress index itself.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 10:01:48 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Vitalis", "Andreas", ""]]}, {"id": "2006.04953", "submitter": "Binghui Peng", "authors": "Xi Chen and Binghui Peng", "title": "Hedging in games: Faster convergence of external and swap regrets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the setting where players run the Hedge algorithm or its\noptimistic variant to play an $n$-action game repeatedly for $T$ rounds.\n  1) For two-player games, we show that the regret of optimistic Hedge decays\nat $\\tilde{O}( 1/T ^{5/6} )$, improving the previous bound $O(1/T^{3/4})$ by\nSyrgkanis, Agarwal, Luo and Schapire (NIPS'15)\n  2) In contrast, we show that the convergence rate of vanilla Hedge is no\nbetter than $\\tilde{\\Omega}(1/ \\sqrt{T})$, addressing an open question posted\nin Syrgkanis, Agarwal, Luo and Schapire (NIPS'15).\n  For general m-player games, we show that the swap regret of each player\ndecays at rate $\\tilde{O}(m^{1/2} (n/T)^{3/4})$ when they combine optimistic\nHedge with the classical external-to-internal reduction of Blum and Mansour\n(JMLR'07). The algorithm can also be modified to achieve the same rate against\nitself and a rate of $\\tilde{O}(\\sqrt{n/T})$ against adversaries. Via standard\nconnections, our upper bounds also imply faster convergence to coarse\ncorrelated equilibria in two-player games and to correlated equilibria in\nmultiplayer games.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 21:18:27 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 01:18:43 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Chen", "Xi", ""], ["Peng", "Binghui", ""]]}, {"id": "2006.05028", "submitter": "Slobodan Mitrovi\\'c", "authors": "Piotr Indyk, Frederik Mallmann-Trenn, Slobodan Mitrovi\\'c, Ronitt\n  Rubinfeld", "title": "Online Page Migration with ML Advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online algorithms for the {\\em page migration problem} that use\npredictions, potentially imperfect, to improve their performance. The best\nknown online algorithms for this problem, due to Westbrook'94 and Bienkowski et\nal'17, have competitive ratios strictly bounded away from 1. In contrast, we\nshow that if the algorithm is given a prediction of the input sequence, then it\ncan achieve a competitive ratio that tends to $1$ as the prediction error rate\ntends to $0$. Specifically, the competitive ratio is equal to $1+O(q)$, where\n$q$ is the prediction error rate. We also design a ``fallback option'' that\nensures that the competitive ratio of the algorithm for {\\em any} input\nsequence is at most $O(1/q)$. Our result adds to the recent body of work that\nuses machine learning to improve the performance of ``classic'' algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 03:15:34 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Indyk", "Piotr", ""], ["Mallmann-Trenn", "Frederik", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "2006.05051", "submitter": "Thodoris Lykouris", "authors": "Kiant\\'e Brantley, Miroslav Dudik, Thodoris Lykouris, Sobhan\n  Miryoosefi, Max Simchowitz, Aleksandrs Slivkins, Wen Sun", "title": "Constrained episodic reinforcement learning in concave-convex and\n  knapsack settings", "comments": "The NeurIPS 2020 version of this paper includes a small bug, leading\n  to an incorrect dependence on H in Theorem 3.4. This version fixes it by\n  adjusting Eq. (9), Theorem 3.4 and the relevant proofs. Changes in the main\n  text are noted in red. Changes in the appendix are limited to Appendices B.1,\n  B.5, and B.6 and the statement of Lemma F.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for tabular episodic reinforcement learning with\nconstraints. We provide a modular analysis with strong theoretical guarantees\nfor settings with concave rewards and convex constraints, and for settings with\nhard constraints (knapsacks). Most of the previous work in constrained\nreinforcement learning is limited to linear constraints, and the remaining work\nfocuses on either the feasibility question or settings with a single episode.\nOur experiments demonstrate that the proposed algorithm significantly\noutperforms these approaches in existing constrained episodic environments.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 05:02:44 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 03:30:29 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Brantley", "Kiant\u00e9", ""], ["Dudik", "Miroslav", ""], ["Lykouris", "Thodoris", ""], ["Miryoosefi", "Sobhan", ""], ["Simchowitz", "Max", ""], ["Slivkins", "Aleksandrs", ""], ["Sun", "Wen", ""]]}, {"id": "2006.05104", "submitter": "Takaaki Nishimoto", "authors": "Takaaki Nishimoto and Yasuo Tabei", "title": "Optimal-Time Queries on BWT-runs Compressed Indexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing highly repetitive strings (i.e., strings with many repetitions) for\nfast queries has become a central research topic in string processing, because\nit has a wide variety of applications in bioinformatics and natural language\nprocessing. Although a substantial number of indexes for highly repetitive\nstrings have been proposed thus far, developing compressed indexes that support\nvarious queries remains a challenge. The run-length Burrows-Wheeler transform\n(RLBWT) is a lossless data compression by a reversible permutation of an input\nstring and run-length encoding, and it has received interest for indexing\nhighly repetitive strings. LF and $\\phi^{-1}$ are two key functions for\nbuilding indexes on RLBWT, and the best previous result computes LF and\n$\\phi^{-1}$ in $O(\\log \\log n)$ time with $O(r)$ words of space for the string\nlength $n$ and the number $r$ of runs in RLBWT. In this paper, we improve LF\nand $\\phi^{-1}$ so that they can be computed in a constant time with $O(r)$\nwords of space. Subsequently, we present OptBWTR (optimal-time queries on\nBWT-runs compressed indexes), the first string index that supports various\nqueries including locate, count, extract queries in optimal time and $O(r)$\nwords of space.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 08:21:39 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 09:44:44 GMT"}, {"version": "v3", "created": "Fri, 16 Apr 2021 04:34:44 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Nishimoto", "Takaaki", ""], ["Tabei", "Yasuo", ""]]}, {"id": "2006.05440", "submitter": "Rachit Chhaya", "authors": "Rachit Chhaya, Anirban Dasgupta, Supratim Shit", "title": "On Coresets For Regularized Regression", "comments": "Accepted at ICML 2020. Acknowledgements added. Minor errors fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effect of norm based regularization on the size of coresets for\nregression problems. Specifically, given a matrix $ \\mathbf{A} \\in\n{\\mathbb{R}}^{n \\times d}$ with $n\\gg d$ and a vector $\\mathbf{b} \\in\n\\mathbb{R} ^ n $ and $\\lambda > 0$, we analyze the size of coresets for\nregularized versions of regression of the form $\\|\\mathbf{Ax}-\\mathbf{b}\\|_p^r\n+ \\lambda\\|{\\mathbf{x}}\\|_q^s$ . Prior work has shown that for ridge regression\n(where $p,q,r,s=2$) we can obtain a coreset that is smaller than the coreset\nfor the unregularized counterpart i.e. least squares regression (Avron et al).\nWe show that when $r \\neq s$, no coreset for regularized regression can have\nsize smaller than the optimal coreset of the unregularized version. The well\nknown lasso problem falls under this category and hence does not allow a\ncoreset smaller than the one for least squares regression. We propose a\nmodified version of the lasso problem and obtain for it a coreset of size\nsmaller than the least square regression. We empirically show that the modified\nversion of lasso also induces sparsity in solution, similar to the original\nlasso. We also obtain smaller coresets for $\\ell_p$ regression with $\\ell_p$\nregularization. We extend our methods to multi response regularized regression.\nFinally, we empirically demonstrate the coreset performance for the modified\nlasso and the $\\ell_1$ regression with $\\ell_1$ regularization.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 18:25:04 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 08:03:35 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 08:43:29 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Chhaya", "Rachit", ""], ["Dasgupta", "Anirban", ""], ["Shit", "Supratim", ""]]}, {"id": "2006.05490", "submitter": "Yu Chen", "authors": "Yu Chen, Sampath Kannan, Sanjeev Khanna", "title": "Sublinear Algorithms and Lower Bounds for Metric TSP Cost Estimation", "comments": "ICALP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing sublinear time algorithms for estimating\nthe cost of a minimum metric traveling salesman (TSP) tour. Specifically, given\naccess to a $n \\times n$ distance matrix $D$ that specifies pairwise distances\nbetween $n$ points, the goal is to estimate the TSP cost by performing only\nsublinear (in the size of $D$) queries. For the closely related problem of\nestimating the weight of a metric minimum spanning tree (MST), it is known that\nfor any $\\varepsilon > 0$, there exists an $\\tilde{O}(n/\\varepsilon^{O(1)})$\ntime algorithm that returns a $(1 + \\varepsilon)$-approximate estimate of the\nMST cost. This result immediately implies an $\\tilde{O}(n/\\varepsilon^{O(1)})$\ntime algorithm to estimate the TSP cost to within a $(2 + \\varepsilon)$ factor\nfor any $\\varepsilon > 0$. However, no $o(n^2)$ time algorithms are known to\napproximate metric TSP to a factor that is strictly better than $2$. On the\nother hand, there were also no known barriers that rule out the existence of\n$(1 + \\varepsilon)$-approximate estimation algorithms for metric TSP with\n$\\tilde{O}(n)$ time for any fixed $\\varepsilon > 0$. In this paper, we make\nprogress on both algorithms and lower bounds for estimating metric TSP cost. We\nalso show that the problem of estimating metric TSP cost is closely connected\nto the problem of estimating the size of a maximum matching in a graph.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 20:08:09 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Chen", "Yu", ""], ["Kannan", "Sampath", ""], ["Khanna", "Sanjeev", ""]]}, {"id": "2006.05592", "submitter": "Cameron Musco", "authors": "Sudhanshu Chanpuriya, Cameron Musco, Konstantinos Sotiropoulos,\n  Charalampos E. Tsourakakis", "title": "Node Embeddings and Exact Low-Rank Representations of Complex Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-dimensional embeddings, from classical spectral embeddings to modern\nneural-net-inspired methods, are a cornerstone in the modeling and analysis of\ncomplex networks. Recent work by Seshadhri et al. (PNAS 2020) suggests that\nsuch embeddings cannot capture local structure arising in complex networks. In\nparticular, they show that any network generated from a natural low-dimensional\nmodel cannot be both sparse and have high triangle density (high clustering\ncoefficient), two hallmark properties of many real-world networks.\n  In this work we show that the results of Seshadhri et al. are intimately\nconnected to the model they use rather than the low-dimensional structure of\ncomplex networks. Specifically, we prove that a minor relaxation of their model\ncan generate sparse graphs with high triangle density. Surprisingly, we show\nthat this same model leads to exact low-dimensional factorizations of many\nreal-world networks. We give a simple algorithm based on logistic principal\ncomponent analysis (LPCA) that succeeds in finding such exact embeddings.\nFinally, we perform a large number of experiments that verify the ability of\nvery low-dimensional embeddings to capture local structure in real-world\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 01:09:03 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 05:38:21 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Chanpuriya", "Sudhanshu", ""], ["Musco", "Cameron", ""], ["Sotiropoulos", "Konstantinos", ""], ["Tsourakakis", "Charalampos E.", ""]]}, {"id": "2006.05660", "submitter": "Thomas Espitau", "authors": "Thomas Espitau, Paul Kirchner", "title": "The nearest-colattice algorithm", "comments": "19 pages, presented at the Algorithmic Number Theory Symposium (ANTS\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we exhibit a hierarchy of polynomial time algorithms solving\napproximate variants of the Closest Vector Problem (CVP). Our first\ncontribution is a heuristic algorithm achieving the same distance tradeoff as\nHSVP algorithms, namely $\\approx\n  \\beta^{\\frac{n}{2\\beta}}\\textrm{covol}(\\Lambda)^{\\frac{1}{n}}$ for a random\nlattice $\\Lambda$ of rank $n$. Compared to the so-called Kannan's embedding\ntechnique, our algorithm allows using precomputations and can be used for\nefficient batch CVP instances. This implies that some attacks on lattice-based\nsignatures lead to very cheap forgeries, after a precomputation. Our second\ncontribution is a proven reduction from approximating the closest vector with a\nfactor $\\approx n^{\\frac32}\\beta^{\\frac{3n}{2\\beta}}$ to the Shortest Vector\nProblem (SVP) in dimension $\\beta$.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 05:26:09 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 01:44:59 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Espitau", "Thomas", ""], ["Kirchner", "Paul", ""]]}, {"id": "2006.05685", "submitter": "Igor Shparlinski", "authors": "Marek Karpinski and Igor Shparlinski", "title": "Noisy polynomial interpolation modulo prime powers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the {\\it noisy polynomial interpolation problem\\/} of recovering\nan unknown $s$-sparse polynomial $f(X)$ over the ring $\\mathbb Z_{p^k}$ of\nresidues modulo $p^k$, where $p$ is a small prime and $k$ is a large integer\nparameter, from approximate values of the residues of $f(t) \\in \\mathbb\nZ_{p^k}$. Similar results are known for residues modulo a large prime $p$,\nhowever the case of prime power modulus $p^k$, with small $p$ and large $k$, is\nnew and requires different techniques. We give a deterministic polynomial time\nalgorithm, which for almost given more than a half bits of $f(t)$ for\nsufficiently many randomly chosen points $t \\in \\mathbb Z_{p^k}^*$, recovers\n$f(X)$.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 06:54:21 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 23:25:06 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Karpinski", "Marek", ""], ["Shparlinski", "Igor", ""]]}, {"id": "2006.05740", "submitter": "Martin Olsen", "authors": "Martin Olsen, Allan Gross", "title": "An Asymptotically Optimal Algorithm for Online Stacking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a storage area where arriving items are stored temporarily in\nbounded capacity stacks until their departure. We look into the problem of\ndeciding where to put an arriving item with the objective of minimizing the\nmaximum number of stacks used over time. The decision has to be made as soon as\nan item arrives, and we assume that we only have information on the departure\ntimes for the arriving item and the items currently at the storage area. We are\nonly allowed to put an item on top of another item if the item below departs at\na later time. We refer to this problem as online stacking. We assume that the\nstorage time intervals are picked i.i.d. from $[0, 1] \\times [0, 1]$ using an\nunknown distribution with a bounded probability density function. Under this\nmild condition, we present a simple polynomial time online algorithm and show\nthat the competitive ratio converges to $1$ in probability. The result holds if\nthe stack capacity is $o(\\sqrt{n})$, where $n$ is the number of items,\nincluding the realistic case where the capacity is a constant. Our experiments\nshow that our results also have practical relevance. To the best of our\nknowledge, we are the first to present an asymptotically optimal algorithm for\nonline stacking, which is an important problem with many real-world\napplications within computational logistics.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 09:15:32 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Olsen", "Martin", ""], ["Gross", "Allan", ""]]}, {"id": "2006.05828", "submitter": "Marcin Bria\\'nski", "authors": "Marcin Bria\\'nski, Jan Gwinner, Vladyslav Hlembotskyi, Witold\n  Jarnicki, Szymon Pli\\'s, Adam Szady", "title": "Introducing Structure to Expedite Quantum Search", "comments": "22 pages, 7 figures", "journal-ref": "Phys. Rev. A 103, 062425 (2021)", "doi": "10.1103/PhysRevA.103.062425", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel quantum algorithm for solving the unstructured search\nproblem with one marked element. Our algorithm allows generating quantum\ncircuits that use asymptotically fewer additional quantum gates than the famous\nGrover's algorithm and may be successfully executed on NISQ devices. We prove\nthat our algorithm is optimal in the total number of elementary gates up to a\nmultiplicative constant. As many NP-hard problems are not in fact unstructured,\nwe also describe the \\emph{partial uncompute} technique which exploits the\noracle structure and allows a significant reduction in the number of elementary\ngates required to find the solution. Combining these results allows us to use\nasymptotically smaller number of elementary gates than the Grover's algorithm\nin various applications, keeping the number of queries to the oracle\nessentially the same. We show how the results can be applied to solve hard\ncombinatorial problems, for example Unique k-SAT. Additionally, we show how to\nasymptotically reduce the number of elementary gates required to solve the\nunstructured search problem with multiple marked elements.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 13:29:47 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 14:45:43 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 21:44:21 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Bria\u0144ski", "Marcin", ""], ["Gwinner", "Jan", ""], ["Hlembotskyi", "Vladyslav", ""], ["Jarnicki", "Witold", ""], ["Pli\u015b", "Szymon", ""], ["Szady", "Adam", ""]]}, {"id": "2006.05850", "submitter": "Alessandro Epasto", "authors": "Michele Borassi, Alessandro Epasto, Silvio Lattanzi, Sergei\n  Vassilvitskii, Morteza Zadimoghaddam", "title": "Sliding Window Algorithms for k-Clustering Problems", "comments": "43 pages, 7 figures", "journal-ref": "In Proceedings of the 34th Conference on Neural Information\n  Processing Systems (NeurIPS 2020)", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sliding window model of computation captures scenarios in which data is\narriving continuously, but only the latest $w$ elements should be used for\nanalysis. The goal is to design algorithms that update the solution efficiently\nwith each arrival rather than recomputing it from scratch. In this work, we\nfocus on $k$-clustering problems such as $k$-means and $k$-median. In this\nsetting, we provide simple and practical algorithms that offer stronger\nperformance guarantees than previous results. Empirically, we show that our\nmethods store only a small fraction of the data, are orders of magnitude\nfaster, and find solutions with costs only slightly higher than those returned\nby algorithms with access to the full dataset.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 14:26:57 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 14:20:27 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Borassi", "Michele", ""], ["Epasto", "Alessandro", ""], ["Lattanzi", "Silvio", ""], ["Vassilvitskii", "Sergei", ""], ["Zadimoghaddam", "Morteza", ""]]}, {"id": "2006.05871", "submitter": "Massimiliano Rossi", "authors": "Dustin Cobas, Veli M\\\"akinen, Massimiliano Rossi", "title": "Tailoring r-index for metagenomics", "comments": "17 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic problem in metagenomics is to assign a sequenced read to the correct\nspecies in the reference collection. In typical applications in genomic\nepidemiology and viral metagenomics the reference collection consists of set of\nspecies with each species represented by its highly similar strains. It has\nbeen recently shown that accurate read assignment can be achieved with $k$-mer\nhashing-based pseudoalignment: A read is assigned to species A if each of its\n$k$-mer hits to reference collection is located only on strains of A. We study\nthe underlying primitives required in pseudoalignment and related tasks. We\npropose three space-efficient solutions building upon the document listing with\nfrequencies problem. All the solutions use an $r$-index (Gagie et al., SODA\n2018) as an underlying index structure for the text obtained as concatenation\nof the set of species, as well as for each species. Given $t$ species whose\nconcatenation length is $n$, and whose Burrows-Wheeler transform contains $r$\nruns, our first solution, based on a grammar-compressed document array with\nprecomputed queries at non terminal symbols, reports the frequencies for the\n${\\tt ndoc}$ distinct documents in which the pattern of length $m$ occurs in\n${\\cal O}(m + \\log(n){\\tt ndoc}) $ time. Our second solution is also based on a\ngrammar-compressed document array, but enhanced with bitvectors and reports the\nfrequencies in ${\\cal O}(m + ((t/w)\\log n + \\log(n/r)){\\tt ndoc})$ time, over a\nmachine with wordsize $w$. Our third solution, based on the interleaved LCP\narray, answers the same query in ${\\cal O}(m + \\log(n/r){\\tt ndoc})$. We\nimplemented our solutions and tested them on real-world and synthetic datasets.\nThe results show that all the solutions are fast on highly-repetitive data, and\nthe size overhead introduced by the indexes are comparable with the size of the\n$r$-index.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 14:55:34 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Cobas", "Dustin", ""], ["M\u00e4kinen", "Veli", ""], ["Rossi", "Massimiliano", ""]]}, {"id": "2006.05976", "submitter": "Ruoqi Shen", "authors": "Ruoqi Shen, Kevin Tian, Yin Tat Lee", "title": "Composite Logconcave Sampling with a Restricted Gaussian Oracle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NA math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider sampling from composite densities on $\\mathbb{R}^d$ of the form\n$d\\pi(x) \\propto \\exp(-f(x) - g(x))dx$ for well-conditioned $f$ and convex (but\npossibly non-smooth) $g$, a family generalizing restrictions to a convex set,\nthrough the abstraction of a restricted Gaussian oracle. For $f$ with condition\nnumber $\\kappa$, our algorithm runs in $O \\left(\\kappa^2 d \\log^2\\tfrac{\\kappa\nd}{\\epsilon}\\right)$ iterations, each querying a gradient of $f$ and a\nrestricted Gaussian oracle, to achieve total variation distance $\\epsilon$. The\nrestricted Gaussian oracle, which draws samples from a distribution whose\nnegative log-likelihood sums a quadratic and $g$, has been previously studied\nand is a natural extension of the proximal oracle used in composite\noptimization. Our algorithm is conceptually simple and obtains stronger\nprovable guarantees and greater generality than existing methods for composite\nsampling. We conduct experiments showing our algorithm vastly improves upon the\nhit-and-run algorithm for sampling the restriction of a (non-diagonal) Gaussian\nto the positive orthant.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 17:43:55 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Shen", "Ruoqi", ""], ["Tian", "Kevin", ""], ["Lee", "Yin Tat", ""]]}, {"id": "2006.06048", "submitter": "Nikolaos Kallimanis", "authors": "Nikolaos D. Kallimanis, Eleni Kanellou, Charidimos Kiosterakis", "title": "Efficient Partial Snapshot Implementations", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose the $\\lambda$-scanner snapshot, a variation of the\nsnapshot object, which supports any fixed amount of $0 < \\lambda \\leq n$\ndifferent $SCAN$ operations being active at any given time. Whenever $\\lambda$\nis equal to the number of processes $n$ in the system, the $\\lambda$-scanner\nobject implements a multi-scanner object, while in case that $\\lambda$ is equal\nto $1$, the $\\lambda$-scanner object implements a single-scanner object. We\npresent the $\\lambda-Snap$ snapshot object, a wait-free $\\lambda$-scanner\nsnapshot implementation that has a step complexity of $O(\\lambda)$ for $UPDATE$\noperations and $O(\\lambda m)$ for $SCAN$ operations. The space complexity of\n$\\lambda-Snap$ is $O(\\lambda m)$. $\\lambda-Snap$ provides a trade-off between\nthe step/space complexity and the maximum number of $SCAN$ operations that the\nsystem can afford to be active on any given point in time. The low space\ncomplexity that our implementations provide makes them more appealing in real\nsystem applications. Moreover, we provide a slightly modified version of the\n$\\lambda-Snap$ implementation, which is called partial $\\lambda-Snap$, that is\nable to support dynamic partial scan operations. In such an object, processes\ncan execute modified $SCAN$ operations called $PARTIAL\\_SCAN$ that could obtain\na part of the snapshot object avoiding to read the whole set of components.\n  In this work, we first provide a simple single-scanner version of\n$\\lambda-Snap$, which is called $1-Snap$. We provide $1-Snap$ just for\npresentation purposes, since it is simpler than $\\lambda-Snap$. The $UPDATE$ in\n$1-Snap$ has a step complexity of $O(1)$, while the $SCAN$ has a step\ncomplexity of $O(m)$. This implementation uses $O(m)$ $CAS$ registers.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 20:10:00 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Kallimanis", "Nikolaos D.", ""], ["Kanellou", "Eleni", ""], ["Kiosterakis", "Charidimos", ""]]}, {"id": "2006.06052", "submitter": "Denis Demidov", "authors": "Denis Demidov, Lin Mu, Bin Wang", "title": "Accelerating linear solvers for Stokes problems with C++ metaprogramming", "comments": null, "journal-ref": null, "doi": "10.1016/j.jocs.2020.101285", "report-no": null, "categories": "cs.MS cs.DC cs.DS physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient solution of large sparse saddle point systems is very important\nin computational fluid mechanics. The discontinuous Galerkin finite element\nmethods have become increasingly popular for incompressible flow problems but\ntheir application is limited due to high computational cost. We describe the\nC++ programming techniques that may help to accelerate linear solvers for such\nproblems. The approach is based on the policy-based design pattern and partial\ntemplate specialization, and is implemented in the open source AMGCL library.\nThe efficiency is demonstrated with the example of accelerating an iterative\nsolver of a discontinuous Galerkin finite element method for the Stokes\nproblem. The implementation allows selecting algorithmic components of the\nsolver by adjusting template parameters without any changes to the codebase. It\nis possible to switch the system matrix to use small statically sized blocks to\nstore the nonzero values, or use a mixed precision solution, which results in\nup to 4 times speedup, and reduces the memory footprint of the algorithm by\nabout 40\\%. We evaluate both monolithic and composite preconditioning\nstrategies for the 3 benchmark problems. The performance of the proposed\nsolution is compared with a multithreaded direct Pardiso solver and a parallel\niterative PETSc solver.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 20:20:05 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 15:39:23 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 06:02:43 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Demidov", "Denis", ""], ["Mu", "Lin", ""], ["Wang", "Bin", ""]]}, {"id": "2006.06067", "submitter": "Cl\\'ement Dallard", "authors": "Cl\\'ement Dallard, Martin Milani\\v{c}, Kenny \\v{S}torgel", "title": "Treewidth versus clique number. I. Graph classes with a forbidden\n  structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treewidth is an important graph invariant, relevant for both structural and\nalgorithmic reasons. A necessary condition for a graph class to have bounded\ntreewidth is the absence of large cliques. We study graph classes closed under\ntaking induced subgraphs in which this condition is also sufficient, which we\ncall $(tw,\\omega)$-bounded. Such graph classes are known to have useful\nalgorithmic applications related to variants of the clique and $k$-coloring\nproblems. We consider six well-known graph containment relations: the minor,\ntopological minor, subgraph, induced minor, induced topological minor, and\ninduced subgraph relations. For each of them, we give a complete\ncharacterization of the graphs $H$ for which the class of graphs excluding $H$\nis $(tw,\\omega)$-bounded.\n  Our results yield an infinite family of $\\chi$-bounded induced-minor-closed\ngraph classes and imply that the class of $1$-perfectly orientable graphs is\n$(tw,\\omega)$-bounded, leading to linear-time algorithms for $k$-coloring\n$1$-perfectly orientable graphs for every fixed $k$. This answers a question of\nBre\\v{s}ar, Hartinger, Kos, and Milani\\v{c} from 2018, and one of Beisegel,\nChudnovsky, Gurvich, Milani\\v{c}, and Servatius from 2019, respectively. We\nalso reveal some further algorithmic implications of $(tw,\\omega)$-boundedness\nrelated to list $k$-coloring and clique problems. In addition, we propose a\nquestion about the complexity of the Maximum Weight Independent Set problem in\n$(tw,\\omega)$-bounded graph classes and prove that the problem is\npolynomial-time solvable in every class of graphs excluding a fixed star as an\ninduced minor.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 21:05:43 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 08:18:32 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Dallard", "Cl\u00e9ment", ""], ["Milani\u010d", "Martin", ""], ["\u0160torgel", "Kenny", ""]]}, {"id": "2006.06171", "submitter": "Chi-Ning Chou", "authors": "Chi-Ning Chou, Juspreet Singh Sandhu, Mien Brabeeba Wang, Tiancheng Yu", "title": "A General Framework for Analyzing Stochastic Dynamics in Learning\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges in analyzing a learning algorithm is the circular\nentanglement between the objective value and the stochastic noise. This is also\nknown as the \"chicken and egg\" phenomenon. Traditionally, people tackle this\nissue with the special structure of the problem and hence the analysis is\ndifficult to generalize.\n  In this paper, we present a general framework for analyzing high-probability\nbounds for stochastic dynamics in learning algorithms. Our framework composes\nstandard techniques from probability theory to give a streamlined three-step\nrecipe with a general and flexible principle to tackle the \"chicken and egg\"\nproblem. We demonstrate the power and the flexibility of our framework by\ngiving unifying analysis for three very different learning problems with both\nthe last iterate and the strong uniform high probability convergence guarantee.\nThe problems are stochastic gradient descent for strongly convex functions,\nstreaming principal component analysis and linear bandit with stochastic\ngradient descent updates. We either improve or match the state-of-the-art\nbounds on all three dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 03:43:45 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 16:07:30 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Chou", "Chi-Ning", ""], ["Sandhu", "Juspreet Singh", ""], ["Wang", "Mien Brabeeba", ""], ["Yu", "Tiancheng", ""]]}, {"id": "2006.06380", "submitter": "Petar Veli\\v{c}kovi\\'c", "authors": "Petar Veli\\v{c}kovi\\'c, Lars Buesing, Matthew C. Overlan, Razvan\n  Pascanu, Oriol Vinyals, Charles Blundell", "title": "Pointer Graph Networks", "comments": "To appear at NeurIPS 2020 (Spotlight talk)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) are typically applied to static graphs that are\nassumed to be known upfront. This static input structure is often informed\npurely by insight of the machine learning practitioner, and might not be\noptimal for the actual task the GNN is solving. In absence of reliable domain\nexpertise, one might resort to inferring the latent graph structure, which is\noften difficult due to the vast search space of possible graphs. Here we\nintroduce Pointer Graph Networks (PGNs) which augment sets or graphs with\nadditional inferred edges for improved model generalisation ability. PGNs allow\neach node to dynamically point to another node, followed by message passing\nover these pointers. The sparsity of this adaptable graph structure makes\nlearning tractable while still being sufficiently expressive to simulate\ncomplex algorithms. Critically, the pointing mechanism is directly supervised\nto model long-term sequences of operations on classical data structures,\nincorporating useful structural inductive biases from theoretical computer\nscience. Qualitatively, we demonstrate that PGNs can learn parallelisable\nvariants of pointer-based data structures, namely disjoint set unions and\nlink/cut trees. PGNs generalise out-of-distribution to 5x larger test inputs on\ndynamic graph connectivity tasks, outperforming unrestricted GNNs and Deep\nSets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 12:52:31 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 20:00:59 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Veli\u010dkovi\u0107", "Petar", ""], ["Buesing", "Lars", ""], ["Overlan", "Matthew C.", ""], ["Pascanu", "Razvan", ""], ["Vinyals", "Oriol", ""], ["Blundell", "Charles", ""]]}, {"id": "2006.06467", "submitter": "Vasilis Kontonis", "authors": "Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis", "title": "Learning Halfspaces with Tsybakov Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the efficient PAC learnability of halfspaces in the presence of\nTsybakov noise. In the Tsybakov noise model, each label is independently\nflipped with some probability which is controlled by an adversary. This noise\nmodel significantly generalizes the Massart noise model, by allowing the\nflipping probabilities to be arbitrarily close to $1/2$ for a fraction of the\nsamples. Our main result is the first non-trivial PAC learning algorithm for\nthis problem under a broad family of structured distributions -- satisfying\ncertain concentration and (anti-)anti-concentration properties -- including\nlog-concave distributions. Specifically, we given an algorithm that achieves\nmisclassification error $\\epsilon$ with respect to the true halfspace, with\nquasi-polynomial runtime dependence in $1/\\epsilin$. The only previous upper\nbound for this problem -- even for the special case of log-concave\ndistributions -- was doubly exponential in $1/\\epsilon$ (and follows via the\nnaive reduction to agnostic learning). Our approach relies on a novel\ncomputationally efficient procedure to certify whether a candidate solution is\nnear-optimal, based on semi-definite programming. We use this certificate\nprocedure as a black-box and turn it into an efficient learning algorithm by\nsearching over the space of halfspaces via online convex optimization.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 14:25:02 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kontonis", "Vasilis", ""], ["Tzamos", "Christos", ""], ["Zarifis", "Nikos", ""]]}, {"id": "2006.06566", "submitter": "Jiarui Gan", "authors": "Georgios Birmpas, Jiarui Gan, Alexandros Hollender, Francisco J.\n  Marmolejo-Coss\\'io, Ninad Rajgopal, Alexandros A. Voudouris", "title": "Optimally Deceiving a Learning Leader in Stackelberg Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results in the ML community have revealed that learning algorithms\nused to compute the optimal strategy for the leader to commit to in a\nStackelberg game, are susceptible to manipulation by the follower. Such a\nlearning algorithm operates by querying the best responses or the payoffs of\nthe follower, who consequently can deceive the algorithm by responding as if\nhis payoffs were much different than what they actually are. For this strategic\nbehavior to be successful, the main challenge faced by the follower is to\npinpoint the payoffs that would make the learning algorithm compute a\ncommitment so that best responding to it maximizes the follower's utility,\naccording to his true payoffs. While this problem has been considered before,\nthe related literature only focused on the simplified scenario in which the\npayoff space is finite, thus leaving the general version of the problem\nunanswered. In this paper, we fill in this gap, by showing that it is always\npossible for the follower to compute (near-)optimal payoffs for various\nscenarios about the learning interaction between leader and follower.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 16:18:21 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Birmpas", "Georgios", ""], ["Gan", "Jiarui", ""], ["Hollender", "Alexandros", ""], ["Marmolejo-Coss\u00edo", "Francisco J.", ""], ["Rajgopal", "Ninad", ""], ["Voudouris", "Alexandros A.", ""]]}, {"id": "2006.06618", "submitter": "Gautam Kamath", "authors": "Sourav Biswas, Yihe Dong, Gautam Kamath, Jonathan Ullman", "title": "CoinPress: Practical Private Mean and Covariance Estimation", "comments": "Code is available at https://github.com/twistedcubic/coin-press", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present simple differentially private estimators for the mean and\ncovariance of multivariate sub-Gaussian data that are accurate at small sample\nsizes. We demonstrate the effectiveness of our algorithms both theoretically\nand empirically using synthetic and real-world datasets---showing that their\nasymptotic error rates match the state-of-the-art theoretical bounds, and that\nthey concretely outperform all previous methods. Specifically, previous\nestimators either have weak empirical accuracy at small sample sizes, perform\npoorly for multivariate data, or require the user to provide strong a priori\nestimates for the parameters.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:17:28 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Biswas", "Sourav", ""], ["Dong", "Yihe", ""], ["Kamath", "Gautam", ""], ["Ullman", "Jonathan", ""]]}, {"id": "2006.06771", "submitter": "Xing Hu", "authors": "Vassos Hadzilacos, Xing Hu, Sam Toueg", "title": "Randomized Consensus with Regular Registers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well-known randomized consensus algorithm by Aspnes and Herlihy for\nasynchronous shared-memory systems was proved to work, even against a strong\nadversary, under the assumption that the registers that it uses are atomic\nregisters. With atomic registers, every read or write operation is\ninstantaneous (and thus indivisible). As pointed out by Golab et al. (2011),\nhowever, a randomized algorithm that works with atomic registers does not\nnecessarily work if we replace the atomic registers that it uses with\nlinearizable implementations of registers.\n  This raises the following question: does the randomized consensus algorithm\nby Aspnes and Herlihy still work against a strong adversary if we replace its\natomic registers with linearizable registers? We show that the answer is\naffirmative, in fact, we show that even linearizable registers are not\nnecessary. More precisely, we prove that the algorithm by Aspnes and Herlihy\nworks against a strong adversary even if the algorithm uses only regular\nregisters.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 19:49:46 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Hadzilacos", "Vassos", ""], ["Hu", "Xing", ""], ["Toueg", "Sam", ""]]}, {"id": "2006.06850", "submitter": "Brendan Juba", "authors": "Mahdi Cheraghchi, Elena Grigorescu, Brendan Juba, Karl Wimmer, and\n  Ning Xie", "title": "List Learning with Attribute Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study the model of list learning with attribute noise.\nLearning with attribute noise was introduced by Shackelford and Volper (COLT\n1988) as a variant of PAC learning, in which the algorithm has access to noisy\nexamples and uncorrupted labels, and the goal is to recover an accurate\nhypothesis. Sloan (COLT 1988) and Goldman and Sloan (Algorithmica 1995)\ndiscovered information-theoretic limits to learning in this model, which have\nimpeded further progress. In this article we extend the model to that of list\nlearning, drawing inspiration from the list-decoding model in coding theory,\nand its recent variant studied in the context of learning. On the positive\nside, we show that sparse conjunctions can be efficiently list learned under\nsome assumptions on the underlying ground-truth distribution. On the negative\nside, our results show that even in the list-learning model, efficient learning\nof parities and majorities is not possible regardless of the representation\nused.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 21:55:15 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Cheraghchi", "Mahdi", ""], ["Grigorescu", "Elena", ""], ["Juba", "Brendan", ""], ["Wimmer", "Karl", ""], ["Xie", "Ning", ""]]}, {"id": "2006.06951", "submitter": "Fabrizio Frati", "authors": "Fabrizio Frati", "title": "Planar Rectilinear Drawings of Outerplanar Graphs in Linear Time", "comments": "Appears in the Proceedings of the 28th International Symposium on\n  Graph Drawing and Network Visualization (GD 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to test in linear time whether an outerplanar graph admits a\nplanar rectilinear drawing, both if the graph has a prescribed plane embedding\nthat the drawing has to respect and if it does not. Our algorithm returns a\nplanar rectilinear drawing if the graph admits one.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 05:33:09 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 20:57:06 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 13:40:26 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2020 13:18:49 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Frati", "Fabrizio", ""]]}, {"id": "2006.06980", "submitter": "Kevin Tian", "authors": "Arun Jambulapati, Jerry Li, Kevin Tian", "title": "Robust Sub-Gaussian Principal Component Analysis and Width-Independent\n  Schatten Packing", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop two methods for the following fundamental statistical task: given\nan $\\epsilon$-corrupted set of $n$ samples from a $d$-dimensional sub-Gaussian\ndistribution, return an approximate top eigenvector of the covariance matrix.\nOur first robust PCA algorithm runs in polynomial time, returns a $1 -\nO(\\epsilon\\log\\epsilon^{-1})$-approximate top eigenvector, and is based on a\nsimple iterative filtering approach. Our second, which attains a slightly worse\napproximation factor, runs in nearly-linear time and sample complexity under a\nmild spectral gap assumption. These are the first polynomial-time algorithms\nyielding non-trivial information about the covariance of a corrupted\nsub-Gaussian distribution without requiring additional algebraic structure of\nmoments. As a key technical tool, we develop the first width-independent\nsolvers for Schatten-$p$ norm packing semidefinite programs, giving a $(1 +\n\\epsilon)$-approximate solution in\n$O(p\\log(\\tfrac{nd}{\\epsilon})\\epsilon^{-1})$ input-sparsity time iterations\n(where $n$, $d$ are problem dimensions).\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 07:45:38 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Jambulapati", "Arun", ""], ["Li", "Jerry", ""], ["Tian", "Kevin", ""]]}, {"id": "2006.07013", "submitter": "Zhize Li", "authors": "Zhize Li, Peter Richt\\'arik", "title": "A Unified Analysis of Stochastic Gradient Methods for Nonconvex\n  Federated Optimization", "comments": "77 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the performance of a large family of SGD variants in\nthe smooth nonconvex regime. To this end, we propose a generic and flexible\nassumption capable of accurate modeling of the second moment of the stochastic\ngradient. Our assumption is satisfied by a large number of specific variants of\nSGD in the literature, including SGD with arbitrary sampling, SGD with\ncompressed gradients, and a wide variety of variance-reduced SGD methods such\nas SVRG and SAGA. We provide a single convergence analysis for all methods that\nsatisfy the proposed unified assumption, thereby offering a unified\nunderstanding of SGD variants in the nonconvex regime instead of relying on\ndedicated analyses of each variant. Moreover, our unified analysis is accurate\nenough to recover or improve upon the best-known convergence results of several\nclassical methods, and also gives new convergence results for many new methods\nwhich arise as special cases. In the more general distributed/federated\nnonconvex optimization setup, we propose two new general algorithmic frameworks\ndiffering in whether direct gradient compression (DC) or compression of\ngradient differences (DIANA) is used. We show that all methods captured by\nthese two frameworks also satisfy our unified assumption. Thus, our unified\nconvergence analysis also captures a large variety of distributed methods\nutilizing compressed communication. Finally, we also provide a unified analysis\nfor obtaining faster linear convergence rates in this nonconvex regime under\nthe PL condition.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 08:58:03 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Li", "Zhize", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "2006.07016", "submitter": "Alexandre P Francisco", "authors": "C\\'atia Vaz and Marta Nascimento and Jo\\~ao A. Carri\\c{c}o and Tatiana\n  Rocher and Alexandre P. Francisco", "title": "Distance-based phylogenetic inference from typing data: a unifying view", "comments": null, "journal-ref": null, "doi": "10.1093/bib/bbaa147", "report-no": null, "categories": "cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typing methods are widely used in the surveillance of infectious diseases,\noutbreaks investigation and studies of the natural history of an infection. And\ntheir use is becoming standard, in particular with the introduction of High\nThroughput Sequencing (HTS). On the other hand, the data being generated is\nmassive and many algorithms have been proposed for phylogenetic analysis of\ntyping data, addressing both correctness and scalability issues. Most of the\ndistance-based algorithms for inferring phylogenetic trees follow the\nclosest-pair joining scheme. This is one of the approaches used in hierarchical\nclustering. And although phylogenetic inference algorithms may seem rather\ndifferent, the main difference among them resides on how one defines cluster\nproximity and on which optimization criterion is used. Both cluster proximity\nand optimization criteria rely often on a model of evolution. In this work we\nreview, and we provide an unified view of these algorithms. This is an\nimportant step not only to better understand such algorithms, but also to\nidentify possible computational bottlenecks and improvements, important to deal\nwith large data sets.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 09:00:38 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Vaz", "C\u00e1tia", ""], ["Nascimento", "Marta", ""], ["Carri\u00e7o", "Jo\u00e3o A.", ""], ["Rocher", "Tatiana", ""], ["Francisco", "Alexandre P.", ""]]}, {"id": "2006.07050", "submitter": "Marcin Wrochna", "authors": "Marcin Wrochna", "title": "Sallow: a heuristic algorithm for treedepth decompositions", "comments": "A shorter version will appear in proceedings of IPEC 2020", "journal-ref": null, "doi": "10.5281/zenodo.3870565", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a heuristic algorithm for computing treedepth decompositions,\nsubmitted for the PACE 2020 challenge. It relies on a variety of greedy\nalgorithms computing elimination orderings, as well as a Divide & Conquer\napproach on balanced cuts obtained using a from-scratch reimplementation of the\n2016 FlowCutter algorithm by Hamann & Strasser [ACM JEA 2018].\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 10:04:06 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 11:26:49 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 09:03:55 GMT"}, {"version": "v4", "created": "Wed, 30 Sep 2020 11:25:05 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Wrochna", "Marcin", ""]]}, {"id": "2006.07086", "submitter": "Sahil Dhoked", "authors": "Sahil Dhoked and Neeraj Mittal", "title": "An Adaptive Approach to Recoverable Mutual Exlcusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual exclusion (ME) is one of the most commonly used techniques to handle\nconflicts in concurrent systems. Traditionally, mutual exclusion algorithms\nhave been designed under the assumption that a process does not fail while\nacquiring/releasing a lock or while executing its critical section. However,\nfailures do occur in real life, potentially leaving the lock in an inconsistent\nstate. This gives rise to the problem of \\emph{recoverable mutual exclusion\n(RME)} that involves designing a mutual exclusion algorithm that can tolerate\nfailures, while maintaining safety and liveness properties.\n  One of the important measures of performance of any ME algorithm, including\nan RME algorithm, is the number of \\emph{remote memory references (RMRs)} made\nby a process (for acquiring and releasing a lock as well as recovering the lock\nstructure after a failure). The best known RME algorithm solves the problem for\n$n$ processes in sub-logarithmic number of RMRs, given by\n$\\mathcal{O}(\\frac{\\log n}{\\log \\log n})$, irrespective of the number of\nfailures in the system.\n  In this work, we present a new algorithm for solving the RME problem whose\nRMR complexity gradually \\emph{adapts} to the number of failures that have\noccurred in the system \"recently\". In the absence of failures, our algorithm\ngenerates only $\\mathcal{O}(1)$ RMRs. Furthermore, its RMR complexity is given\nby $\\mathcal{O}(\\min\\{ \\sqrt{F}, \\frac{\\log n}{\\log \\log n} \\})$ where $F$ is\nthe total number of failures in the \"recent\" past. In addition to read and\nwrite instructions, our algorithm uses compare-and-swap (\\CAS{}) and\nfetch-and-store (\\FAS{}) hardware instructions, both of which are commonly\navailable in most modern processors.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 11:18:04 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 06:04:27 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Dhoked", "Sahil", ""], ["Mittal", "Neeraj", ""]]}, {"id": "2006.07302", "submitter": "Tuukka Korhonen", "authors": "Tuukka Korhonen", "title": "SMS in PACE 2020", "comments": "3 pages, 3 appendix pages, 0 figures. Submitted as a solver\n  description of a solver in Parameterized Algorithms and Computational\n  Experiments Challenge 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe SMS, our submission to the exact treedepth track of PACE 2020.\nSMS computes the treedepth of a graph by branching on the small minimal\nseparators of the graph.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 16:34:24 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Korhonen", "Tuukka", ""]]}, {"id": "2006.07340", "submitter": "Christopher Musco", "authors": "Tam\\'as Erd\\'elyi and Cameron Musco and Christopher Musco", "title": "Fourier Sparse Leverage Scores and Approximate Kernel Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove new explicit upper bounds on the leverage scores of Fourier sparse\nfunctions under both the Gaussian and Laplace measures. In particular, we study\n$s$-sparse functions of the form $f(x) = \\sum_{j=1}^s a_j e^{i \\lambda_j x}$\nfor coefficients $a_j \\in \\mathbb{C}$ and frequencies $\\lambda_j \\in\n\\mathbb{R}$. Bounding Fourier sparse leverage scores under various measures is\nof pure mathematical interest in approximation theory, and our work extends\nexisting results for the uniform measure [Erd17,CP19a]. Practically, our bounds\nare motivated by two important applications in machine learning:\n  1. Kernel Approximation. They yield a new random Fourier features algorithm\nfor approximating Gaussian and Cauchy (rational quadratic) kernel matrices. For\nlow-dimensional data, our method uses a near optimal number of features, and\nits runtime is polynomial in the $statistical\\ dimension$ of the approximated\nkernel matrix. It is the first \"oblivious sketching method\" with this property\nfor any kernel besides the polynomial kernel, resolving an open question of\n[AKM+17,AKK+20b].\n  2. Active Learning. They can be used as non-uniform sampling distributions\nfor robust active learning when data follows a Gaussian or Laplace\ndistribution. Using the framework of [AKM+19], we provide essentially optimal\nresults for bandlimited and multiband interpolation, and Gaussian process\nregression. These results generalize existing work that only applies to\nuniformly distributed data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 17:25:39 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 03:37:49 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 01:55:27 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Erd\u00e9lyi", "Tam\u00e1s", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""]]}, {"id": "2006.07486", "submitter": "Zihan Tan", "authors": "Julia Chuzhoy, Merav Parter, Zihan Tan", "title": "On Packing Low-Diameter Spanning Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge connectivity of a graph is one of the most fundamental graph-theoretic\nconcepts. The celebrated tree packing theorem of Tutte and Nash-Williams from\n1961 states that every $k$-edge connected graph $G$ contains a collection\n$\\cal{T}$ of $\\lfloor k/2 \\rfloor$ edge-disjoint spanning trees, that we refer\nto as a tree packing; the diameter of the tree packing $\\cal{T}$ is the largest\ndiameter of any tree in $\\cal{T}$. A desirable property of a tree packing, that\nis both sufficient and necessary for leveraging the high connectivity of a\ngraph in distributed communication, is that its diameter is low. Yet, despite\nextensive research in this area, it is still unclear how to compute a tree\npacking, whose diameter is sublinear in $|V(G)|$, in a low-diameter graph $G$,\nor alternatively how to show that such a packing does not exist.\n  In this paper we provide first non-trivial upper and lower bounds on the\ndiameter of tree packing. First, we show that, for every $k$-edge connected\n$n$-vertex graph $G$ of diameter $D$, there is a tree packing $\\cal{T}$ of size\n$\\Omega(k)$, diameter $O((101k\\log n)^D)$, that causes edge-congestion at most\n$2$. Second, we show that for every $k$-edge connected $n$-vertex graph $G$ of\ndiameter $D$, the diameter of $G[p]$ is $O(k^{D(D+1)/2})$ with high\nprobability, where $G[p]$ is obtained by sampling each edge of $G$\nindependently with probability $p=\\Theta(\\log n/k)$. This provides a packing of\n$\\Omega(k/\\log n)$ edge-disjoint trees of diameter at most $O(k^{(D(D+1)/2)})$\neach. We then prove that these two results are nearly tight. Lastly, we show\nthat if every pair of vertices in a graph has $k$ edge-disjoint paths of length\nat most $D$ connecting them, then there is a tree packing of size $k$, diameter\n$O(D\\log n)$, causing edge-congestion $O(\\log n)$. We also provide several\napplications of low-diameter tree packing in distributed computation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 21:54:03 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Chuzhoy", "Julia", ""], ["Parter", "Merav", ""], ["Tan", "Zihan", ""]]}, {"id": "2006.07588", "submitter": "Ali Pourmiri", "authors": "Catherine Greenhill, Bernard Mans, and Ali Pourmiri", "title": "Balanced Allocation on Dynamic Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The {balls-into-bins model} randomly allocates $n$ sequential balls into $n$\nbins, as follows: each ball selects a set $D$ of $d\\ge 2$ bins, independently\nand uniformly at random, then the ball is allocated to a least-loaded bin from\n$D$ (ties broken randomly). The \\emph{maximum load} is the maximum number of\nballs in any bin. {In 1999, Azar et al.}\\ showed that provided ties are broken\nrandomly, after $n$ balls have been placed the \\emph{maximum load}, is\n${\\log_d\\log n}+O(1)$, with high probability. We consider this popular paradigm\nin a dynamic environment where the bins are structured as a \\emph{dynamic\nhypergraph}. A dynamic hypergraph is a sequence of hypergraphs, say\n$\\mathcal{H}^{(t)}$, arriving over discrete times $t=1,2,\\ldots$, such that the\nvertex set of $\\mathcal{H}^{(t)}$'s is the set of $n$ bins, but (hyper)edges\nmay change over time. In our model, the $t$-th ball chooses an edge from\n$\\mathcal{H}^{(t)}$ uniformly at random, and then chooses a set $D$ of $d\\ge 2$\nrandom bins from the selected edge. The ball is allocated to a least-loaded bin\nfrom $D$, with ties broken randomly. We quantify the dynamicity of the model by\nintroducing the notion of \\emph{pair visibility}, which measures the number of\nrounds in which a pair of bins appears within a (hyper)edge. We prove that if,\nfor some $\\varepsilon>0$, a dynamic hypergraph has pair visibility at most\n$n^{1-\\varepsilon}$, and some mild additional conditions hold, then with high\nprobability the process has maximum load $O(\\log_d\\log n)$. Our proof is based\non a variation of the witness tree technique, which is of independent interest.\n  The model can also be seen as an adversarial model where an adversary decides\nthe structure of the possible sets of $d$ bins available to each ball.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 08:22:05 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Greenhill", "Catherine", ""], ["Mans", "Bernard", ""], ["Pourmiri", "Ali", ""]]}, {"id": "2006.07628", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Shay Solomon", "title": "When Algorithms for Maximal Independent Set and Maximal Matching Run in\n  Sublinear-Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximal independent set (MIS), maximal matching (MM), and\n$(\\Delta+1)$-coloring in graphs of maximum degree $\\Delta$ are among the most\nprominent algorithmic graph theory problems. They are all solvable by a simple\nlinear-time greedy algorithm and up until very recently this constituted the\nstate-of-the-art. In SODA 2019, Assadi, Chen, and Khanna gave a randomized\nalgorithm for $(\\Delta+1)$-coloring that runs in $\\widetilde{O}(n\\sqrt{n})$\ntime, which even for moderately dense graphs is sublinear in the input size.\nThe work of Assadi et al. however contained a spoiler for MIS and MM: neither\nproblems provably admits a sublinear-time algorithm in general graphs. In this\nwork, we dig deeper into the possibility of achieving sublinear-time algorithms\nfor MIS and MM.\n  The neighborhood independence number of a graph $G$, denoted by $\\beta(G)$,\nis the size of the largest independent set in the neighborhood of any vertex.\nWe identify $\\beta(G)$ as the ``right'' parameter to measure the runtime of MIS\nand MM algorithms: Although graphs of bounded neighborhood independence may be\nvery dense (clique is one example), we prove that carefully chosen variants of\ngreedy algorithms for MIS and MM run in $O(n\\beta(G))$ and\n$O(n\\log{n}\\cdot\\beta(G))$ time respectively on any $n$-vertex graph $G$. We\ncomplement this positive result by observing that a simple extension of the\nlower bound of Assadi et.al. implies that $\\Omega(n\\beta(G))$ time is also\nnecessary for any algorithm to either problem for all values of $\\beta(G)$ from\n$1$ to $\\Theta(n)$. We note that our algorithm for MIS is deterministic while\nfor MM we use randomization which we prove is unavoidable: any deterministic\nalgorithm for MM requires $\\Omega(n^2)$ time even for $\\beta(G) = 2$.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 12:12:31 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Assadi", "Sepehr", ""], ["Solomon", "Shay", ""]]}, {"id": "2006.07677", "submitter": "Prajnanaswaroopa S", "authors": "Prajnanaswaroopa S, Geetha J and Somasundaram K", "title": "Total Coloring for some classes of Cayley graphs", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The Total coloring conjecture states that any simple graph G with maximum\ndegree D can be totally colored with at most D+2 colors. In this paper, we have\nobtained the total chromatic number for some classes of Cayley graphs.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 16:37:14 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 21:29:12 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["S", "Prajnanaswaroopa", ""], ["J", "Geetha", ""], ["K", "Somasundaram", ""]]}, {"id": "2006.07832", "submitter": "Deval Patel", "authors": "Deval Patel, Arindam Khan, Anand Louis", "title": "Group Fairness for Knapsack Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the knapsack problem with group fairness constraints. The input of\nthe problem consists of a knapsack of bounded capacity and a set of items, each\nitem belongs to a particular category and has and associated weight and value.\nThe goal of this problem is to select a subset of items such that all\ncategories are fairly represented, the total weight of the selected items does\nnot exceed the capacity of the knapsack,and the total value is maximized. We\nstudy the fairness parameters such as the bounds on the total value of items\nfrom each category, the total weight of items from each category, and the total\nnumber of items from each category. We give approximation algorithms for these\nproblems. These fairness notions could also be extended to the min-knapsack\nproblem. The fair knapsack problems encompass various important problems, such\nas participatory budgeting, fair budget allocation, advertising.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 07:59:27 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 07:00:58 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2021 05:54:26 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Patel", "Deval", ""], ["Khan", "Arindam", ""], ["Louis", "Anand", ""]]}, {"id": "2006.07846", "submitter": "Omri Puny", "authors": "Omri Puny, Heli Ben-Hamu, Yaron Lipman", "title": "Global Attention Improves Graph Networks Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper advocates incorporating a Low-Rank Global Attention (LRGA) module,\na computation and memory efficient variant of the dot-product attention\n(Vaswani et al., 2017), to Graph Neural Networks (GNNs) for improving their\ngeneralization power. To theoretically quantify the generalization properties\ngranted by adding the LRGA module to GNNs, we focus on a specific family of\nexpressive GNNs and show that augmenting it with LRGA provides algorithmic\nalignment to a powerful graph isomorphism test, namely the 2-Folklore\nWeisfeiler-Lehman (2-FWL) algorithm. In more detail we: (i) consider the recent\nRandom Graph Neural Network (RGNN) (Sato et al., 2020) framework and prove that\nit is universal in probability; (ii) show that RGNN augmented with LRGA aligns\nwith 2-FWL update step via polynomial kernels; and (iii) bound the sample\ncomplexity of the kernel's feature map when learned with a randomly initialized\ntwo-layer MLP. From a practical point of view, augmenting existing GNN layers\nwith LRGA produces state of the art results in current GNN benchmarks. Lastly,\nwe observe that augmenting various GNN architectures with LRGA often closes the\nperformance gap between different models.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 09:01:57 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 10:30:15 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Puny", "Omri", ""], ["Ben-Hamu", "Heli", ""], ["Lipman", "Yaron", ""]]}, {"id": "2006.07919", "submitter": "Renos Karamanis", "authors": "Renos Karamanis, Eleftherios Anastasiadis, Marc Stettler, Panagiotis\n  Angeloudis", "title": "Vehicle Redistribution in Ride-Sourcing Markets using Convex Minimum\n  Cost Flows", "comments": "12 pages, 9 figures, in IEEE Transactions on Intelligent\n  Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2021.3089777", "report-no": null, "categories": "cs.DS cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ride-sourcing platforms often face imbalances in the demand and supply of\nrides across areas in their operating road-networks. As such, dynamic pricing\nmethods have been used to mediate these demand asymmetries through surge price\nmultipliers, thus incentivising higher driver participation in the market.\nHowever, the anticipated commercialisation of autonomous vehicles could\ntransform the current ride-sourcing platforms to fleet operators. The absence\nof human drivers fosters the need for empty vehicle management to address any\nvehicle supply deficiencies. Proactive redistribution using integer programming\nand demand predictive models have been proposed in research to address this\nproblem. A shortcoming of existing models, however, is that they ignore the\nmarket structure and underlying customer choice behaviour. As such, current\nmodels do not capture the real value of redistribution. To resolve this, we\nformulate the vehicle redistribution problem as a non-linear minimum cost flow\nproblem which accounts for the relationship of supply and demand of rides, by\nassuming a customer discrete choice model and a market structure. We\ndemonstrate that this model can have a convex domain, and we introduce an edge\nsplitting algorithm to solve a transformed convex minimum cost flow problem for\nvehicle redistribution. By testing our model using simulation, we show that our\nredistribution algorithm can decrease wait times by more than 50%, increase\nprofit up to 10% with less than 20% increase in vehicle mileage. Our findings\noutline that the value of redistribution is contingent on localised market\nstructure and customer behaviour.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 14:52:51 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 18:20:56 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Karamanis", "Renos", ""], ["Anastasiadis", "Eleftherios", ""], ["Stettler", "Marc", ""], ["Angeloudis", "Panagiotis", ""]]}, {"id": "2006.07998", "submitter": "Kevin O'Connor", "authors": "Kevin O'Connor, Kevin McGoff, Andrew B. Nobel", "title": "Optimal Transport for Stationary Markov Chains via Policy Iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the optimal transport problem for pairs of stationary finite-state\nMarkov chains, with an emphasis on the computation of optimal transition\ncouplings. Transition couplings are a constrained family of transport plans\nthat capture the dynamics of Markov chains. Solutions of the optimal transition\ncoupling (OTC) problem correspond to alignments of the two chains that minimize\nlong-term average cost. We establish a connection between the OTC problem and\nMarkov decision processes, and show that solutions of the OTC problem can be\nobtained via an adaptation of policy iteration. For settings with large state\nspaces, we develop a fast approximate algorithm based on an entropy-regularized\nversion of the OTC problem, and provide bounds on its per-iteration complexity.\nWe establish a stability result for both the regularized and unregularized\nalgorithms, from which a statistical consistency result follows as a corollary.\nWe validate our theoretical results empirically through a simulation study,\ndemonstrating that the approximate algorithm exhibits faster overall runtime\nwith low error. Finally, we extend the setting and application of our methods\nto hidden Markov models, and illustrate the potential use of the proposed\nalgorithms in practice with an application to computer-generated music.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 19:55:58 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 20:32:32 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 17:37:46 GMT"}, {"version": "v4", "created": "Tue, 11 May 2021 21:15:43 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["O'Connor", "Kevin", ""], ["McGoff", "Kevin", ""], ["Nobel", "Andrew B.", ""]]}, {"id": "2006.08012", "submitter": "Jason Altschuler", "authors": "Jason M. Altschuler, Enric Boix-Adsera", "title": "Wasserstein barycenters can be computed in polynomial time in fixed\n  dimension", "comments": "15 pages + refs, 5 figs. Improved exposition. Title has been updated\n  for clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing Wasserstein barycenters is a fundamental geometric problem with\nwidespread applications in machine learning, statistics, and computer graphics.\nHowever, it is unknown whether Wasserstein barycenters can be computed in\npolynomial time, either exactly or to high precision (i.e., with\n$\\textrm{polylog}(1/\\varepsilon)$ runtime dependence). This paper answers these\nquestions in the affirmative for any fixed dimension. Our approach is to solve\nan exponential-size linear programming formulation by efficiently implementing\nthe corresponding separation oracle using techniques from computational\ngeometry.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 20:24:27 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 22:15:32 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Altschuler", "Jason M.", ""], ["Boix-Adsera", "Enric", ""]]}, {"id": "2006.08302", "submitter": "Atsushi Miyauchi", "authors": "Yuuki Takai, Atsushi Miyauchi, Masahiro Ikeda, Yuichi Yoshida", "title": "Hypergraph Clustering Based on PageRank", "comments": "KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.SI math.AP math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hypergraph is a useful combinatorial object to model ternary or\nhigher-order relations among entities. Clustering hypergraphs is a fundamental\ntask in network analysis. In this study, we develop two clustering algorithms\nbased on personalized PageRank on hypergraphs. The first one is local in the\nsense that its goal is to find a tightly connected vertex set with a bounded\nvolume including a specified vertex. The second one is global in the sense that\nits goal is to find a tightly connected vertex set. For both algorithms, we\ndiscuss theoretical guarantees on the conductance of the output vertex set.\nAlso, we experimentally demonstrate that our clustering algorithms outperform\nexisting methods in terms of both the solution quality and running time. To the\nbest of our knowledge, ours are the first practical algorithms for hypergraphs\nwith theoretical guarantees on the conductance of the output set.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 11:50:18 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Takai", "Yuuki", ""], ["Miyauchi", "Atsushi", ""], ["Ikeda", "Masahiro", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "2006.08397", "submitter": "Yi Li", "authors": "Yi Li, Ruosong Wang, Lin Yang, Hanrui Zhang", "title": "Nearly Linear Row Sampling Algorithm for Quantile Regression", "comments": "Accepted to ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a row sampling algorithm for the quantile loss function with sample\ncomplexity nearly linear in the dimensionality of the data, improving upon the\nprevious best algorithm whose sampling complexity has at least cubic dependence\non the dimensionality. Based upon our row sampling algorithm, we give the\nfastest known algorithm for quantile regression and a graph sparsification\nalgorithm for balanced directed graphs. Our main technical contribution is to\nshow that Lewis weights sampling, which has been used in row sampling\nalgorithms for $\\ell_p$ norms, can also be applied in row sampling algorithms\nfor a variety of loss functions. We complement our theoretical results by\nexperiments to demonstrate the practicality of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 13:40:07 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Li", "Yi", ""], ["Wang", "Ruosong", ""], ["Yang", "Lin", ""], ["Zhang", "Hanrui", ""]]}, {"id": "2006.08420", "submitter": "Vasileios Nakos", "authors": "Mahdi Cheraghchi and Vasileios Nakos", "title": "Combinatorial Group Testing and Sparse Recovery Schemes with\n  Near-Optimal Decoding Time", "comments": "Abstract shortened to meet the arxiv requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the long-studied problem of combinatorial group testing, one is asked to\ndetect a set of $k$ defective items out of a population of size $n$, using $m\n\\ll n$ disjunctive measurements. In the non-adaptive setting, the most widely\nused combinatorial objects are disjunct and list-disjunct matrices, which\ndefine incidence matrices of test schemes. Disjunct matrices allow the\nidentification of the exact set of defectives, whereas list disjunct matrices\nidentify a small superset of the defectives. Apart from the combinatorial\nguarantees, it is often of key interest to equip measurement designs with\nefficient decoding algorithms. The most efficient decoders should run in\nsublinear time in $n$, and ideally near-linear in the number of measurements\n$m$.\n  In this work, we give several constructions with an optimal number of\nmeasurements and near-optimal decoding time for the most fundamental group\ntesting tasks, as well as for central tasks in the compressed sensing and heavy\nhitters literature. For many of those tasks, the previous measurement-optimal\nconstructions needed time either quadratic in the number of measurements or\nlinear in the universe size.\n  Most of our results are obtained via a clean and novel approach which avoids\nlist-recoverable codes or related complex techniques which were present in\nalmost every state-of-the-art work on efficiently decodable constructions of\nsuch objects.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 14:17:58 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 15:59:53 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Cheraghchi", "Mahdi", ""], ["Nakos", "Vasileios", ""]]}, {"id": "2006.08473", "submitter": "Xiaojin Zhang", "authors": "Xiaojin Zhang", "title": "Improved algorithm for permutation testing", "comments": "There were some mistakes for the proposal of a simpler algorithm for\n  testing monotone pattern", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of testing forbidden patterns. The patterns that are of\nsignificant interest include monotone pattern and $(1,3,2)$-pattern. For the\nproblem of testing monotone patterns, \\cite{newman2019testing} propose a\nnon-adaptive algorithm with query complexity $(\\log n)^{O(k^2)}$.\n\\cite{ben2019finding} then improve the query complexity of non-adaptive\nalgorithm to $\\Omega((\\log n)^{\\lfloor\\log k\\rfloor})$. Further,\n\\cite{ben2019optimal} propose an adaptive algorithm for testing monotone\npattern with optimal query complexity $O(\\log n)$. However, the adaptive\nalgorithm and the analysis are rather complicated. We provide a simple adaptive\nalgorithm with one-sided error for testing monotone permutation. We also\npresent an algorithm with improved query complexity for testing\n$(1,3,2)$-pattern.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 15:25:40 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 01:03:49 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 10:49:09 GMT"}, {"version": "v4", "created": "Mon, 13 Jul 2020 00:55:48 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Zhang", "Xiaojin", ""]]}, {"id": "2006.08668", "submitter": "Hendrik Molter", "authors": "Sebastian Bu{\\ss}, Hendrik Molter, Rolf Niedermeier, Maciej Rymar", "title": "Algorithmic Aspects of Temporal Betweenness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The betweenness centrality of a graph vertex measures how often this vertex\nis visited on shortest paths between other vertices of the graph. In the\nanalysis of many real-world graphs or networks, betweenness centrality of a\nvertex is used as an indicator for its relative importance in the network. In\nparticular, it is among the most popular tools in social network analysis. In\nrecent years, a growing number of real-world networks is modeled as temporal\ngraphs, where we have a fixed set of vertices and there is a finite discrete\nset of time steps and every edge might be present only at some time steps.\nWhile shortest paths are straightforward to define in static graphs, temporal\npaths can be considered \"optimal\" with respect to many different criteria,\nincluding length, arrival time, and overall travel time (shortest, foremost,\nand fastest paths). This leads to different concepts of temporal betweenness\ncentrality and we provide a systematic study of temporal betweenness variants\nbased on various concepts of optimal temporal paths. Computing the betweenness\ncentrality for vertices in a graph is closely related to counting the number of\noptimal paths between vertex pairs. We show that counting foremost and fastest\npaths is computationally intractable (#P-hard) and hence the computation of the\ncorresponding temporal betweenness values is intractable as well. For shortest\npaths and two selected special cases of foremost paths, we devise\npolynomial-time algorithms for temporal betweenness computation. Moreover, we\nalso explore the distinction between strict (ascending time labels) and\nnon-strict (non-descending time labels) time labels in temporal paths. In our\nexperiments with established real-world temporal networks, we demonstrate the\npractical effectiveness of our algorithms, compare the various betweenness\nconcepts, and derive recommendations on their practical use.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 18:17:30 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 13:30:03 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Bu\u00df", "Sebastian", ""], ["Molter", "Hendrik", ""], ["Niedermeier", "Rolf", ""], ["Rymar", "Maciej", ""]]}, {"id": "2006.08926", "submitter": "Ashish Dwivedi", "authors": "Ashish Dwivedi and Nitin Saxena", "title": "Computing Igusa's local zeta function of univariates in deterministic\n  polynomial-time", "comments": "15 pages, ANTS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.CC cs.DS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Igusa's local zeta function $Z_{f,p}(s)$ is the generating function that\ncounts the number of integral roots, $N_{k}(f)$, of $f(\\mathbf x) \\bmod p^k$,\nfor all $k$. It is a famous result, in analytic number theory, that $Z_{f,p}$\nis a rational function in $\\mathbb{Q}(p^s)$. We give an elementary proof of\nthis fact for a univariate polynomial $f$. Our proof is constructive as it\ngives a closed-form expression for the number of roots $N_{k}(f)$.\n  Our proof, when combined with the recent root-counting algorithm of (Dwivedi,\nMittal, Saxena, CCC, 2019), yields the first deterministic poly($|f|, \\log p$)\ntime algorithm to compute $Z_{f,p}(s)$. Previously, an algorithm was known only\nin the case when $f$ completely splits over $\\mathbb{Q}_p$; it required the\nrational roots to use the concept of generating function of a tree\n(Z\\'u\\~niga-Galindo, J.Int.Seq., 2003).\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 04:59:41 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Dwivedi", "Ashish", ""], ["Saxena", "Nitin", ""]]}, {"id": "2006.08949", "submitter": "Mahdi Hajiabadi", "authors": "Mahdi Hajiabadi, Jasbir Singh, Venkatesh Srinivasan, Alex Thomo", "title": "Utility-Based Graph Summarization: New and Improved", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge in graph mining is the ever-increasing size of\ndatasets. Graph summarization aims to find a compact representation resulting\nin faster algorithms and reduced storage needs. The flip side of graph\nsummarization is the loss of utility which diminishes its usability. The key\nquestions we address in this paper are: (1)How to summarize a graph without any\nloss of utility? (2)How to summarize a graph with some loss of utility but\nabove a user-specified threshold? (3)How to query graph summaries without graph\nreconstruction?} We also aim at making graph summarization available for the\nmasses by efficiently handling web-scale graphs using only a consumer-grade\nmachine. Previous works suffer from conceptual limitations and lack of\nscalability. In this work, we make three key contributions. First, we present a\nutility-driven graph summarization method, based on a clique and independent\nset decomposition, that produces significant compression with zero loss of\nutility. The compression provided is significantly better than state-of-the-art\nin lossless graph summarization, while the runtime is two orders of magnitude\nlower. Second, we present a highly scalable algorithm for the lossy case, which\nforegoes the expensive iterative process that hampers previous work. Our\nalgorithm achieves this by combining a memory reduction technique and a novel\nbinary-search approach. In contrast to the competition, we are able to handle\nweb-scale graphs in a single machine without a performance impediment as the\nutility threshold (and size of summary) decreases. Third, we show that our\ngraph summaries can be used as-is to answer several important classes of\nqueries, such as triangle enumeration, Pagerank, and shortest paths. This is in\ncontrast to other works that incrementally reconstruct the original graph for\nanswering queries, thus incurring additional time costs.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 06:50:36 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Hajiabadi", "Mahdi", ""], ["Singh", "Jasbir", ""], ["Srinivasan", "Venkatesh", ""], ["Thomo", "Alex", ""]]}, {"id": "2006.09085", "submitter": "Leonardo Pellegrina", "authors": "Leonardo Pellegrina, Cyrus Cousins, Fabio Vandin, Matteo Riondato", "title": "MCRapper: Monte-Carlo Rademacher Averages for Poset Families and\n  Approximate Pattern Mining", "comments": null, "journal-ref": null, "doi": "10.1145/3394486.3403267", "report-no": null, "categories": "cs.LG cs.DB cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MCRapper, an algorithm for efficient computation of Monte-Carlo\nEmpirical Rademacher Averages (MCERA) for families of functions exhibiting\nposet (e.g., lattice) structure, such as those that arise in many pattern\nmining tasks. The MCERA allows us to compute upper bounds to the maximum\ndeviation of sample means from their expectations, thus it can be used to find\nboth statistically-significant functions (i.e., patterns) when the available\ndata is seen as a sample from an unknown distribution, and approximations of\ncollections of high-expectation functions (e.g., frequent patterns) when the\navailable data is a small sample from a large dataset. This feature is a strong\nimprovement over previously proposed solutions that could only achieve one of\nthe two. MCRapper uses upper bounds to the discrepancy of the functions to\nefficiently explore and prune the search space, a technique borrowed from\npattern mining itself. To show the practical use of MCRapper, we employ it to\ndevelop an algorithm TFP-R for the task of True Frequent Pattern (TFP) mining.\nTFP-R gives guarantees on the probability of including any false positives\n(precision) and exhibits higher statistical power (recall) than existing\nmethods offering the same guarantees. We evaluate MCRapper and TFP-R and show\nthat they outperform the state-of-the-art for their respective tasks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 11:42:56 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Pellegrina", "Leonardo", ""], ["Cousins", "Cyrus", ""], ["Vandin", "Fabio", ""], ["Riondato", "Matteo", ""]]}, {"id": "2006.09123", "submitter": "Sergei Vassilvitskii", "authors": "Michael Mitzenmacher and Sergei Vassilvitskii", "title": "Algorithms with Predictions", "comments": "survey is to appear as a chapter in Beyond the Worst-Case Analysis of\n  Algorithms, a collection edited by Tim Roughgarden. We hope to occasionally\n  update the survey here, with new versions that include discussions of new\n  results and advances in the area of Algorithms with Predictions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce algorithms that use predictions from machine learning applied to\nthe input to circumvent worst-case analysis. We aim for algorithms that have\nnear optimal performance when these predictions are good, but recover the\nprediction-less worst case behavior when the predictions have large errors.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 13:13:28 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Mitzenmacher", "Michael", ""], ["Vassilvitskii", "Sergei", ""]]}, {"id": "2006.09167", "submitter": "Szil\\'ard P\\'all", "authors": "Szil\\'ard P\\'all, Artem Zhmurov, Paul Bauer, Mark Abraham, Magnus\n  Lundborg, Alan Gray, Berk Hess, Erik Lindahl", "title": "Heterogeneous Parallelization and Acceleration of Molecular Dynamics\n  Simulations in GROMACS", "comments": "The following article has been submitted to the Journal of Chemical\n  Physics", "journal-ref": null, "doi": "10.1063/5.0018516", "report-no": null, "categories": "physics.comp-ph cs.DC cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The introduction of accelerator devices such as graphics processing units\n(GPUs) has had profound impact on molecular dynamics simulations and has\nenabled order-of-magnitude performance advances using commodity hardware. To\nfully reap these benefits, it has been necessary to reformulate some of the\nmost fundamental algorithms, including the Verlet list, pair searching and\ncut-offs. Here, we present the heterogeneous parallelization and acceleration\ndesign of molecular dynamics implemented in the GROMACS codebase over the last\ndecade. The setup involves a general cluster-based approach to pair lists and\nnon-bonded pair interactions that utilizes both GPUs and CPU SIMD acceleration\nefficiently, including the ability to load-balance tasks between CPUs and GPUs.\nThe algorithm work efficiency is tuned for each type of hardware, and to use\naccelerators more efficiently we introduce dual pair lists with rolling pruning\nupdates. Combined with new direct GPU-GPU communication as well as GPU\nintegration, this enables excellent performance from single GPU simulations\nthrough strong scaling across multiple GPUs and efficient multi-node\nparallelization.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 14:19:26 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 22:54:40 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["P\u00e1ll", "Szil\u00e1rd", ""], ["Zhmurov", "Artem", ""], ["Bauer", "Paul", ""], ["Abraham", "Mark", ""], ["Lundborg", "Magnus", ""], ["Gray", "Alan", ""], ["Hess", "Berk", ""], ["Lindahl", "Erik", ""]]}, {"id": "2006.09221", "submitter": "Alexander Barvinok", "authors": "Alexander Barvinok", "title": "Testing systems of real quadratic equations for approximate solutions", "comments": "Corrected several typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider systems of equations $q_i(x)=0$, where $q_i: {\\Bbb R}^n\n\\longrightarrow {\\Bbb R}$, $i=1, \\ldots, m$, are quadratic forms. Our goal is\nto tell efficiently systems with many non-trivial solutions or near-solutions\n$x \\ne 0$ from systems that are far from having a solution. For that, we pick a\ndelta-shaped penalty function $F: {\\Bbb R} \\longrightarrow [0, 1]$ with\n$F(0)=1$ and $F(y) < 1$ for $y \\ne 0$ and compute the expectation of $F(q_1(x))\n\\cdots F(q_m(x))$ for a random $x$ sampled from the standard Gaussian measure\nin ${\\Bbb R}^n$. We choose $F(y)=y^{-2}\\sin^2 y$ and show that the expectation\ncan be approximated within relative error $0< \\epsilon < 1$ in quasi-polynomial\ntime $(m+n)^{O(\\ln (m+n)-\\ln \\epsilon)}$, provided each form $q_i$ depends on\nnot more than $r$ real variables, has common variables with at most $r-1$ other\nforms and satisfies $|q_i(x)| \\leq \\gamma \\|x\\|^2/r$, where $\\gamma >0$ is an\nabsolute constant. This allows us to distinguish between \"easily solvable\" and\n\"badly unsolvable\" systems in some non-trivial situations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 14:50:50 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 16:39:43 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Barvinok", "Alexander", ""]]}, {"id": "2006.09327", "submitter": "Wenxin Li", "authors": "Wenxin Li", "title": "Optimal Algorithm and Lower Bound for Submodular Maximization", "comments": "There is text overlap with arXiv:1804.08178, which is replaced by a\n  paper with only results related to knapsack type constraint and beyond. This\n  paper contains the results for cardinality constraint and lower bound", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present the first linear query complexity algorithm for\nmaximizing a monotone submodular function subject to a cardinality constraint,\nwhich achieves the approximation ratio of $(1-1/e-\\varepsilon)$ using\n$O(n/\\varepsilon)$ queries. To the best of our knowledge, this is the first\ndeterministic algorithm that achieves the almost optimal approximation and\noptimal query complexity simultaneously.\n  Query complexity lower bound of submodular maximization problems is also\nstudied in this paper. We show that there exists no (randomized)\n$(1/4+\\varepsilon)$-approximate algorithm using $o(n/\\log n)$ queries for\nunconstrained submodular maximization. Combining with existing results, we\npresent a complete characterization of the query complexity of unconstrained\nsubmodular maximization.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 17:06:45 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 14:40:26 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 07:37:08 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Li", "Wenxin", ""]]}, {"id": "2006.09352", "submitter": "Benjamin Coleman", "authors": "Benjamin Coleman and Anshumali Shrivastava", "title": "A One-Pass Private Sketch for Most Machine Learning Tasks", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy (DP) is a compelling privacy definition that explains\nthe privacy-utility tradeoff via formal, provable guarantees. Inspired by\nrecent progress toward general-purpose data release algorithms, we propose a\nprivate sketch, or small summary of the dataset, that supports a multitude of\nmachine learning tasks including regression, classification, density\nestimation, near-neighbor search, and more. Our sketch consists of randomized\ncontingency tables that are indexed with locality-sensitive hashing and\nconstructed with an efficient one-pass algorithm. We prove competitive error\nbounds for DP kernel density estimation. Existing methods for DP kernel density\nestimation scale poorly, often exponentially slower with an increase in\ndimensions. In contrast, our sketch can quickly run on large, high-dimensional\ndatasets in a single pass. Exhaustive experiments show that our generic sketch\ndelivers a similar privacy-utility tradeoff when compared to existing DP\nmethods at a fraction of the computation cost. We expect that our sketch will\nenable differential privacy in distributed, large-scale machine learning\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 17:47:48 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Coleman", "Benjamin", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "2006.09509", "submitter": "Kevin Sun", "authors": "Zhihao Jiang, Debmalya Panigrahi, Kevin Sun", "title": "Online Algorithms for Weighted Paging with Predictions", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we initiate the study of the weighted paging problem with\npredictions. This continues the recent line of work in online algorithms with\npredictions, particularly that of Lykouris and Vassilvitski (ICML 2018) and\nRohatgi (SODA 2020) on unweighted paging with predictions. We show that unlike\nunweighted paging, neither a fixed lookahead nor knowledge of the next request\nfor every page is sufficient information for an algorithm to overcome existing\nlower bounds in weighted paging. However, a combination of the two, which we\ncall the strong per request prediction (SPRP) model, suffices to give a\n2-competitive algorithm. We also explore the question of gracefully degrading\nalgorithms with increasing prediction error, and give both upper and lower\nbounds for a set of natural measures of prediction error.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 20:42:51 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Jiang", "Zhihao", ""], ["Panigrahi", "Debmalya", ""], ["Sun", "Kevin", ""]]}, {"id": "2006.09665", "submitter": "Debmalya Panigrahi", "authors": "Anupam Gupta and Amit Kumar and Debmalya Panigrahi", "title": "Caching with Time Windows and Delays", "comments": "A preliminary version of this paper was published in STOC 2020 under\n  the title \"Caching with Time Windows\". This version gives full proofs,\n  generalizes the result from time windows to more general delay functions, and\n  makes a small technical correction in the main LP from the STOC paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two generalizations of the classical weighted paging problem that\nincorporate the notion of delayed service of page requests. The first is the\n(weighted) Paging with Time Windows (PageTW) problem, which is like the\nclassical weighted paging problem except that each page request only needs to\nbe served before a given deadline. This problem arises in many practical\napplications of online caching, such as the \"deadline\" I/O scheduler in the\nLinux kernel and video-on-demand streaming. The second, and more general,\nproblem is the (weighted) Paging with Delay (PageD) problem, where the delay in\nserving a page request results in a penalty being assessed to the objective.\nThis problem generalizes the caching problem to allow delayed service, a line\nof work that has recently gained traction in online algorithms (e.g., Emek et\nal. STOC '16, Azar et al. STOC '17, Azar and Touitou FOCS '19).\n  We give $O(\\log k\\log n)$-competitive algorithms for both the PageTW and\nPageD problems on $n$ pages with a cache of size $k$. This significantly\nimproves on the previous best bounds of $O(k)$ for both problems (Azar et al.\nSTOC '17). We also consider the offline PageTW and PageD problems, for which we\ngive $O(1)$ approximation algorithms and prove APX-hardness. These are the\nfirst results for the offline problems; even NP-hardness was not known before\nour work. At the heart of our algorithms is a novel \"hitting-set\" LP relaxation\nof the PageTW problem that overcomes the $\\Omega(k)$ integrality gap of the\nnatural LP for the problem. To the best of our knowledge, this is the first\nexample of an LP-based algorithm for an online algorithm with delays/deadlines.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 05:32:25 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Gupta", "Anupam", ""], ["Kumar", "Amit", ""], ["Panigrahi", "Debmalya", ""]]}, {"id": "2006.09735", "submitter": "Arnab Bhattacharyya", "authors": "Arnab Bhattacharyya and Rathin Desai and Sai Ganesh Nagarajan and\n  Ioannis Panageas", "title": "Efficient Statistics for Sparse Graphical Models from Truncated Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study high-dimensional estimation from truncated samples.\nWe focus on two fundamental and classical problems: (i) inference of sparse\nGaussian graphical models and (ii) support recovery of sparse linear models.\n  (i) For Gaussian graphical models, suppose $d$-dimensional samples ${\\bf x}$\nare generated from a Gaussian $N(\\mu,\\Sigma)$ and observed only if they belong\nto a subset $S \\subseteq \\mathbb{R}^d$. We show that ${\\mu}$ and ${\\Sigma}$ can\nbe estimated with error $\\epsilon$ in the Frobenius norm, using\n$\\tilde{O}\\left(\\frac{\\textrm{nz}({\\Sigma}^{-1})}{\\epsilon^2}\\right)$ samples\nfrom a truncated $\\mathcal{N}({\\mu},{\\Sigma})$ and having access to a\nmembership oracle for $S$. The set $S$ is assumed to have non-trivial measure\nunder the unknown distribution but is otherwise arbitrary.\n  (ii) For sparse linear regression, suppose samples $({\\bf x},y)$ are\ngenerated where $y = {\\bf x}^\\top{{\\Omega}^*} + \\mathcal{N}(0,1)$ and $({\\bf\nx}, y)$ is seen only if $y$ belongs to a truncation set $S \\subseteq\n\\mathbb{R}$. We consider the case that ${\\Omega}^*$ is sparse with a support\nset of size $k$. Our main result is to establish precise conditions on the\nproblem dimension $d$, the support size $k$, the number of observations $n$,\nand properties of the samples and the truncation that are sufficient to recover\nthe support of ${\\Omega}^*$. Specifically, we show that under some mild\nassumptions, only $O(k^2 \\log d)$ samples are needed to estimate ${\\Omega}^*$\nin the $\\ell_\\infty$-norm up to a bounded error.\n  For both problems, our estimator minimizes the sum of the finite population\nnegative log-likelihood function and an $\\ell_1$-regularization term.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 09:21:00 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Desai", "Rathin", ""], ["Nagarajan", "Sai Ganesh", ""], ["Panageas", "Ioannis", ""]]}, {"id": "2006.09872", "submitter": "Christoph Hertrich", "authors": "Christoph Hertrich, Christian Wei{\\ss}, Heiner Ackermann, Sandy\n  Heydrich, Sven O. Krumke", "title": "Scheduling a Proportionate Flow Shop of Batching Machines", "comments": "Version 2: replace initial preprint with authors' accepted manuscript", "journal-ref": "Journal of Scheduling 23, 575-593 (2020)", "doi": "10.1007/s10951-020-00667-2", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a proportionate flow shop of batching machines with\nrelease dates and a fixed number $m \\geq 2$ of machines. The scheduling problem\nhas so far barely received any attention in the literature, but recently its\nimportance has increased significantly, due to applications in the industrial\nscaling of modern bio-medicine production processes. We show that for any fixed\nnumber of machines, the makespan and the sum of completion times can be\nminimized in polynomial time. Furthermore, we show that the obtained algorithm\ncan also be used to minimize the weighted total completion time, maximum\nlateness, total tardiness and (weighted) number of late jobs in polynomial time\nif all release dates are $0$. Previously, polynomial time algorithms have only\nbeen known for two machines.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 13:54:48 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 16:32:05 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hertrich", "Christoph", ""], ["Wei\u00df", "Christian", ""], ["Ackermann", "Heiner", ""], ["Heydrich", "Sandy", ""], ["Krumke", "Sven O.", ""]]}, {"id": "2006.09877", "submitter": "\\'Edouard Bonnet", "authors": "\\'Edouard Bonnet, Colin Geniet, Eun Jung Kim, St\\'ephan Thomass\\'e,\n  R\\'emi Watrigant", "title": "Twin-width II: small classes", "comments": "37 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS cs.LO math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The twin-width of a graph $G$ is the minimum integer $d$ such that $G$ has a\n$d$-contraction sequence, that is, a sequence of $|V(G)|-1$ iterated vertex\nidentifications for which the overall maximum number of red edges incident to a\nsingle vertex is at most $d$, where a red edge appears between two sets of\nidentified vertices if they are not homogeneous in $G$. We show that if a graph\nadmits a $d$-contraction sequence, then it also has a linear-arity tree of\n$f(d)$-contractions, for some function $f$. First this permits to show that\nevery bounded twin-width class is small, i.e., has at most $n!c^n$ graphs\nlabeled by $[n]$, for some constant $c$. This unifies and extends the same\nresult for bounded treewidth graphs [Beineke and Pippert, JCT '69], proper\nsubclasses of permutations graphs [Marcus and Tardos, JCTA '04], and proper\nminor-free classes [Norine et al., JCTB '06]. The second consequence is an\n$O(\\log n)$-adjacency labeling scheme for bounded twin-width graphs, confirming\nseveral cases of the implicit graph conjecture. We then explore the \"small\nconjecture\" that, conversely, every small hereditary class has bounded\ntwin-width. Inspired by sorting networks of logarithmic depth, we show that\n$\\log_{\\Theta(\\log \\log d)}n$-subdivisions of $K_n$ (a small class when $d$ is\nconstant) have twin-width at most $d$. We obtain a rather sharp converse with a\nsurprisingly direct proof: the $\\log_{d+1}n$-subdivision of $K_n$ has\ntwin-width at least $d$. Secondly graphs with bounded stack or queue number\n(also small classes) have bounded twin-width. Thirdly we show that cubic\nexpanders obtained by iterated random 2-lifts from $K_4$~[Bilu and Linial,\nCombinatorica '06] have bounded twin-width, too. We suggest a promising\nconnection between the small conjecture and group theory. Finally we define a\nrobust notion of sparse twin-width and discuss how it compares with other\nsparse classes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 13:57:09 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Geniet", "Colin", ""], ["Kim", "Eun Jung", ""], ["Thomass\u00e9", "St\u00e9phan", ""], ["Watrigant", "R\u00e9mi", ""]]}, {"id": "2006.09912", "submitter": "James Trimble", "authors": "James Trimble", "title": "PACE Solver Description: Bute-Plus: A Bottom-Up Exact Solver for\n  Treedepth", "comments": "4 pages, 1 appendix pages, 0 figures. A version of this tool\n  description paper without the appendix is published in the proceedings of\n  IPEC 2020:\n  https://drops.dagstuhl.de/opus/volltexte/2020/13337/pdf/LIPIcs-IPEC-2020-34.pdf\n  . Changes: this version expands the paper from a preliminary version", "journal-ref": "In 15th International Symposium on Parameterized and Exact\n  Computation (IPEC 2020). Schloss Dagstuhl-Leibniz-Zentrum f\\\"ur Informatik", "doi": "10.4230/LIPIcs.IPEC.2020.34", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note introduces Bute-Plus, an exact solver for the treedepth problem.\nThe core of the solver is a positive-instance driven dynamic program that\nconstructs an elimination tree of minimum depth in a bottom-up fashion. Three\nfeatures greatly improve the algorithm's run time. The first of these is a\nspecialised trie data structure. The second is a domination rule. The third is\na heuristic presolve step can quickly find a treedepth decomposition of optimal\ndepth for many instances.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 14:45:58 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 23:20:36 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Trimble", "James", ""]]}, {"id": "2006.10129", "submitter": "Abhishek Shetty", "authors": "Nika Haghtalab, Tim Roughgarden, Abhishek Shetty", "title": "Smoothed Analysis of Online and Differentially Private Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical and pervasive needs for robustness and privacy in algorithms have\ninspired the design of online adversarial and differentially private learning\nalgorithms. The primary quantity that characterizes learnability in these\nsettings is the Littlestone dimension of the class of hypotheses [Ben-David et\nal., 2009, Alon et al., 2019]. This characterization is often interpreted as an\nimpossibility result because classes such as linear thresholds and neural\nnetworks have infinite Littlestone dimension. In this paper, we apply the\nframework of smoothed analysis [Spielman and Teng, 2004], in which\nadversarially chosen inputs are perturbed slightly by nature. We show that\nfundamentally stronger regret and error guarantees are possible with smoothed\nadversaries than with worst-case adversaries. In particular, we obtain regret\nand privacy error bounds that depend only on the VC dimension and the\nbracketing number of a hypothesis class, and on the magnitudes of the\nperturbations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 20:00:17 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Haghtalab", "Nika", ""], ["Roughgarden", "Tim", ""], ["Shetty", "Abhishek", ""]]}, {"id": "2006.10221", "submitter": "Benjamin Moseley", "authors": "Sara Ahmadian, Alessandro Epasto, Marina Knittel, Ravi Kumar, Mohammad\n  Mahdian, Benjamin Moseley, Philip Pham, Sergei Vassilvitskii, Yuyan Wang", "title": "Fair Hierarchical Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning has become more prevalent, researchers have begun to\nrecognize the necessity of ensuring machine learning systems are fair.\nRecently, there has been an interest in defining a notion of fairness that\nmitigates over-representation in traditional clustering.\n  In this paper we extend this notion to hierarchical clustering, where the\ngoal is to recursively partition the data to optimize a specific objective. For\nvarious natural objectives, we obtain simple, efficient algorithms to find a\nprovably good fair hierarchical clustering. Empirically, we show that our\nalgorithms can find a fair hierarchical clustering, with only a negligible loss\nin the objective.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 01:05:11 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 02:59:47 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Ahmadian", "Sara", ""], ["Epasto", "Alessandro", ""], ["Knittel", "Marina", ""], ["Kumar", "Ravi", ""], ["Mahdian", "Mohammad", ""], ["Moseley", "Benjamin", ""], ["Pham", "Philip", ""], ["Vassilvitskii", "Sergei", ""], ["Wang", "Yuyan", ""]]}, {"id": "2006.10268", "submitter": "Jonathan Scarlett", "authors": "Eric Price and Jonathan Scarlett", "title": "A Fast Binary Splitting Approach to Non-Adaptive Group Testing", "comments": "Accepted to RANDOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of noiseless non-adaptive group\ntesting under the for-each recovery guarantee, also known as probabilistic\ngroup testing. In the case of $n$ items and $k$ defectives, we provide an\nalgorithm attaining high-probability recovery with $O(k \\log n)$ scaling in\nboth the number of tests and runtime, improving on the best known $O(k^2 \\log k\n\\cdot \\log n)$ runtime previously available for any algorithm that only uses\n$O(k \\log n)$ tests. Our algorithm bears resemblance to Hwang's adaptive\ngeneralized binary splitting algorithm (Hwang, 1972); we recursively work with\ngroups of items of geometrically vanishing sizes, while maintaining a list of\n\"possibly defective\" groups and circumventing the need for adaptivity. While\nthe most basic form of our algorithm requires $\\Omega(n)$ storage, we also\nprovide a low-storage variant based on hashing, with similar recovery\nguarantees.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 04:20:19 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Price", "Eric", ""], ["Scarlett", "Jonathan", ""]]}, {"id": "2006.10286", "submitter": "Igor V. Netay", "authors": "Igor V. Netay", "title": "Cyclic space-filling curves and their clustering property", "comments": "19 pages, 13 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce an algorithm of construction of cyclic\nspace-filling curves. One particular construction provides a family of\nspace-filling curves in all dimensions (H-curves). They are compared here with\nthe Hilbert curve in the sense of clustering properties, and it turns out that\nthe constructed curve is very close and sometimes a bit better than the Hilbert\ncurve. At the same time, its construction is more simple and evaluation is\nsignificantly faster.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 05:35:47 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Netay", "Igor V.", ""]]}, {"id": "2006.10302", "submitter": "Oren Salzman", "authors": "Oren Salzman", "title": "Approximate bi-criteria search by efficient representation of subsets of\n  the Pareto-optimal frontier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the bi-criteria shortest-path problem where we want to compute\nshortest paths on a graph that simultaneously balance two cost functions. While\nthis problem has numerous applications, there is usually no path minimizing\nboth cost functions simultaneously. Thus, we typically consider the set of\npaths where no path is strictly better then the others in both cost functions,\na set called the Pareto-optimal frontier. Unfortunately, the size of this set\nmay be exponential in the number of graph vertices and the general problem is\nNP-hard. While existing schemes to approximate this set exist, they may be\nslower than exact approaches when applied to relatively small instances and\nrunning them on graphs with even a moderate number of nodes is often\nimpractical. The crux of the problem lies in how to efficiently approximate the\nPareto-optimal frontier. Our key insight is that the Pareto-optimal frontier\ncan be approximated using pairs of paths. This simple observation allows us to\nrun a best-first-search while efficiently and effectively pruning away\nintermediate solutions in order to obtain an approximation of the Pareto\nfrontier for any given approximation factor. We compared our approach with an\nadaptation of BOA*, the state-of-the-art algorithm for computing exact\nsolutions to the bi-criteria shortest-path problem. Our experiments show that\nas the problem becomes harder, the speedup obtained becomes more pronounced.\nSpecifically, on large roadmaps, we obtain an average speedup of more than\n$\\times 8.5$ and a maximal speedup of over $\\times 148$.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 06:39:11 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 05:36:13 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Salzman", "Oren", ""]]}, {"id": "2006.10361", "submitter": "Adil Erzin I", "authors": "Adil Erzin, Stepan Nazarenko, Gregory Melidi, Roman Plotnikov", "title": "A 3/2--approximation for big two-bar charts packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Two-Bar Charts Packing Problem (2-BCPP), in which it is\nnecessary to pack two-bar charts (2-BCs) in a unit-height strip of minimum\nlength. The problem is a generalization of the Bin Packing Problem (BPP).\nEarlier, we proposed an $O(n^2)$-time algorithm that constructs the packing\nwhich length at most $2\\cdot OPT+1$, where $OPT$ is the minimum length of the\npacking of $n$ 2-BCs. In this paper, we propose an $O(n^4)$-time\n3/2-approximate algorithm when each BC has at least one bar greater than 1/2.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 08:41:56 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Erzin", "Adil", ""], ["Nazarenko", "Stepan", ""], ["Melidi", "Gregory", ""], ["Plotnikov", "Roman", ""]]}, {"id": "2006.10364", "submitter": "Prafullkumar Tale Mr", "authors": "Spoorthy Gunda, Pallavi Jain, Daniel Lokshtanov, Saket Saurabh and\n  Prafullkumar Tale", "title": "On the Parameterized Approximability of Contraction to Classes of\n  Chordal Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph operation that {\\em contracts edges} is one of the fundamental\noperations in the theory of graph minors. Parameterized Complexity of editing\nto a family of graphs by contracting $k$ edges has recently gained substantial\nscientific attention, and several new results have been obtained. Some\nimportant families of graphs, namely the subfamilies of chordal graphs, in the\ncontext of edge contractions, have proven to be significantly difficult than\none might expect. In this paper, we study the \\textsc{$\\cal F$-Contraction}\nproblem, where $\\cal F$ is a subfamily of chordal graphs, in the realm of\nparameterized approximation. Formally, given a graph $G$ and an integer $k$,\n\\textsc{ $\\cal F$-Contraction} asks whether there exists $X \\subseteq E(G)$\nsuch that $G/X \\in \\cal F$ and $|X| \\leq k$. Here, $G/X$ is the graph obtained\nfrom $G$ by contracting edges in $X$. We obtain the following results for the\n\\textsc{ $\\cal F$-Contraction} problem. $(1)$ We show that \\textsc{Clique\nContraction} admits a polynomial-size approximate kernelization scheme\n(\\textsf{PSAKS}). $(2)$ We give a $(2+\\epsilon)$-approximate polynomial kernel\nfor \\textsc{Split Contraction} (which also implies a factor\n$(2+\\epsilon)$-\\FPT-approximation algorithm for \\textsc{ Split Contraction}).\nFurthermore, we show that, assuming \\textsf{ Gap-ETH}, there is no\n$\\left(\\frac{5}{4}-\\delta \\right)$-\\FPT-approximation algorithm for\n\\textsc{Split Contraction}. Here, $\\epsilon, \\delta>0$ are fixed constants.\n$(3)$ \\textsc{Chordal Contraction} is known to be \\WTH. We complement this\nresult by observing that the existing \\textsf{W[2]-hardness} reduction can be\nadapted to show that, assuming \\FPT $\\neq$ \\textsf{W[1]}, there is no\n$F(k)$-\\FPT-approximation algorithm for \\textsc{Chordal Contraction}. Here,\n$F(k)$ is an arbitrary function depending on $k$ alone.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 08:50:40 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Gunda", "Spoorthy", ""], ["Jain", "Pallavi", ""], ["Lokshtanov", "Daniel", ""], ["Saurabh", "Saket", ""], ["Tale", "Prafullkumar", ""]]}, {"id": "2006.10456", "submitter": "Sepehr Assadi", "authors": "Noga Alon, Sepehr Assadi", "title": "Palette Sparsification Beyond $(\\Delta+1)$ Vertex Coloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent palette sparsification theorem of Assadi, Chen, and Khanna [SODA'19]\nstates that in every $n$-vertex graph $G$ with maximum degree $\\Delta$,\nsampling $O(\\log{n})$ colors per each vertex independently from $\\Delta+1$\ncolors almost certainly allows for proper coloring of $G$ from the sampled\ncolors. Besides being a combinatorial statement of its own independent\ninterest, this theorem was shown to have various applications to design of\nalgorithms for $(\\Delta+1)$ coloring in different models of computation on\nmassive graphs such as streaming or sublinear-time algorithms.\n  In this paper, we further study palette sparsification problems:\n  * We prove that for $(1+\\varepsilon) \\Delta$ coloring, sampling only\n$O_{\\varepsilon}(\\sqrt{\\log{n}})$ colors per vertex is sufficient and necessary\nto obtain a proper coloring from the sampled colors.\n  * A natural family of graphs with chromatic number much smaller than\n$(\\Delta+1)$ are triangle-free graphs which are $O(\\frac{\\Delta}{\\ln{\\Delta}})$\ncolorable. We prove that sampling $O(\\Delta^{\\gamma} + \\sqrt{\\log{n}})$ colors\nper vertex is sufficient and necessary to obtain a proper\n$O_{\\gamma}(\\frac{\\Delta}{\\ln{\\Delta}})$ coloring of triangle-free graphs.\n  * We show that sampling $O_{\\varepsilon}(\\log{n})$ colors per vertex is\nsufficient for proper coloring of any graph with high probability whenever each\nvertex is sampling from a list of $(1+\\varepsilon) \\cdot deg(v)$ arbitrary\ncolors, or even only $deg(v)+1$ colors when the lists are the sets\n$\\{1,\\ldots,deg(v)+1\\}$.\n  Similar to previous work, our new palette sparsification results naturally\nlead to a host of new and/or improved algorithms for vertex coloring in\ndifferent models including streaming and sublinear-time algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 12:08:18 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 01:35:00 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Alon", "Noga", ""], ["Assadi", "Sepehr", ""]]}, {"id": "2006.10689", "submitter": "Alexander Wein", "authors": "G\\'erard Ben Arous, Alexander S. Wein, Ilias Zadik", "title": "Free Energy Wells and Overlap Gap Property in Sparse PCA", "comments": "63 pages. Accepted for presentation at the Conference on Learning\n  Theory (COLT) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variant of the sparse PCA (principal component analysis) problem\nin the \"hard\" regime, where the inference task is possible yet no\npolynomial-time algorithm is known to exist. Prior work, based on the\nlow-degree likelihood ratio, has conjectured a precise expression for the best\npossible (sub-exponential) runtime throughout the hard regime. Following\ninstead a statistical physics inspired point of view, we show bounds on the\ndepth of free energy wells for various Gibbs measures naturally associated to\nthe problem. These free energy wells imply hitting time lower bounds that\ncorroborate the low-degree conjecture: we show that a class of natural MCMC\n(Markov chain Monte Carlo) methods (with worst-case initialization) cannot\nsolve sparse PCA with less than the conjectured runtime. These lower bounds\napply to a wide range of values for two tuning parameters: temperature and\nsparsity misparametrization. Finally, we prove that the Overlap Gap Property\n(OGP), a structural property that implies failure of certain local search\nalgorithms, holds in a significant part of the hard regime.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:18:02 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Arous", "G\u00e9rard Ben", ""], ["Wein", "Alexander S.", ""], ["Zadik", "Ilias", ""]]}, {"id": "2006.10692", "submitter": "Marcin Bienkowski", "authors": "Marcin Bienkowski, David Fuchssteiner, Jan Marcinkowski, Stefan Schmid", "title": "Online Dynamic B-Matching With Applications to Reconfigurable Datacenter\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper initiates the study of online algorithms for the maximum weight\n$b$-matching problem, a generalization of maximum weight matching where each\nnode has at most $b \\geq 1$ adjacent matching edges. The problem is motivated\nby emerging optical technologies which allow to enhance datacenter networks\nwith reconfigurable matchings, providing direct connectivity between frequently\ncommunicating racks. These additional links may improve network performance, by\nleveraging spatial and temporal structure in the workload. We show that the\nunderlying algorithmic problem features an intriguing connection to online\npaging (a.k.a. caching), but introduces a novel challenge. Our main\ncontribution is an online algorithm which is $O(b)$-competitive; we also prove\nthat this is asymptotically optimal. We complement our theoretical results with\nextensive trace-driven simulations, based on real-world datacenter workloads as\nwell as synthetic traffic traces.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:23:46 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 17:41:51 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Bienkowski", "Marcin", ""], ["Fuchssteiner", "David", ""], ["Marcinkowski", "Jan", ""], ["Schmid", "Stefan", ""]]}, {"id": "2006.10698", "submitter": "Tim Roughgarden", "authors": "Andrew Lewis-Pye and Tim Roughgarden", "title": "Resource Pools and the CAP Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain protocols differ in fundamental ways, including the mechanics of\nselecting users to produce blocks (e.g., proof-of-work vs. proof-of-stake) and\nthe method to establish consensus (e.g., longest chain rules vs. BFT-inspired\nprotocols). These fundamental differences have hindered \"apples-to-apples\"\ncomparisons between different categories of blockchain protocols and, in turn,\nthe development of theory to formally discuss their relative merits.\n  This paper presents a parsimonious abstraction sufficient for capturing and\ncomparing properties of many well-known permissionless blockchain protocols,\nsimultaneously capturing essential properties of both proof-of-work and\nproof-of-stake protocols, and of both longest-chain-type and BFT-type\nprotocols. Our framework blackboxes the precise mechanics of the user selection\nprocess, allowing us to isolate the properties of the selection process which\nare significant for protocol design.\n  We illustrate our framework's utility with two results. First, we prove an\nanalog of the CAP theorem from distributed computing for our framework in a\npartially synchronous setting. This theorem shows that a fundamental dichotomy\nholds between protocols (such as Bitcoin) that are adaptive, in the sense that\nthey can function given unpredictable levels of participation, and protocols\n(such as Algorand) that have certain finality properties. Second, we formalize\nthe idea that proof-of-work (PoW) protocols and non-PoW protocols can be\ndistinguished by the forms of permission that users are given to carry out\nupdates to the state.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:33:07 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Lewis-Pye", "Andrew", ""], ["Roughgarden", "Tim", ""]]}, {"id": "2006.10715", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane and Daniel Kongsgaard", "title": "List-Decodable Mean Estimation via Iterative Multi-Filtering", "comments": "Fixed typo in title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of {\\em list-decodable mean estimation} for bounded\ncovariance distributions. Specifically, we are given a set $T$ of points in\n$\\mathbb{R}^d$ with the promise that an unknown $\\alpha$-fraction of points in\n$T$, where $0< \\alpha < 1/2$, are drawn from an unknown mean and bounded\ncovariance distribution $D$, and no assumptions are made on the remaining\npoints. The goal is to output a small list of hypothesis vectors such that at\nleast one of them is close to the mean of $D$. We give the first practically\nviable estimator for this problem. In more detail, our algorithm is sample and\ncomputationally efficient, and achieves information-theoretically near-optimal\nerror. While the only prior algorithm for this setting inherently relied on the\nellipsoid method, our algorithm is iterative and only uses spectral techniques.\nOur main technical innovation is the design of a soft outlier removal procedure\nfor high-dimensional heavy-tailed datasets with a majority of outliers.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:47:37 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 18:34:16 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Kongsgaard", "Daniel", ""]]}, {"id": "2006.10804", "submitter": "Alan Frieze", "authors": "Alan Frieze", "title": "Karp's patching algorithm on dense digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following question. We are given a dense digraph $D$ with\nminimum in- and out-degree at least $\\alpha n$, where $\\alpha>1/2$ is a\nconstant. The edges of $D$ are given edge costs $C(e),e\\in E(D)$, where $C(e)$\nis an independent copy of the uniform $[0,1]$ random variable $U$. Let\n$C(i,j),i,j\\in[n]$ be the associated $n\\times n$ cost matrix where\n$C(i,j)=\\infty$ if $(i,j)\\notin E(D)$. We show that w.h.p. the patching\nalgorithm of Karp finds a tour for the asymmetric traveling salesperson problem\nthat is asymptotically equal to that of the associated assignment problem.\nKarp's algorithm runs in polynomial time.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 18:45:14 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Frieze", "Alan", ""]]}, {"id": "2006.10847", "submitter": "Sebastian Berndt", "authors": "Sebastian Berndt and Klaus Jansen and Kim-Manuel Klein", "title": "New Bounds for the Vertices of the Integer Hull", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vertices of the integer hull are the integral equivalent to the\nwell-studied basic feasible solutions of linear programs. In this paper we give\nnew bounds on the number of non-zero components -- their support -- of these\nvertices matching either the best known bounds or improving upon them. While\nthe best known bounds make use of deep techniques, we only use basic results\nfrom probability theory to make use of the concentration of measure effect. To\nshow the versatility of our techniques, we use our results to give the best\nknown bounds on the number of such vertices and an algorithm to enumerate them.\nWe also improve upon the known lower bounds to show that our results are nearly\noptimal. One of the main ingredients of our work is a generalization of the\nfamous Hoeffding bound to vector-valued random variables that might be of\ngeneral interest.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 20:47:45 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Berndt", "Sebastian", ""], ["Jansen", "Klaus", ""], ["Klein", "Kim-Manuel", ""]]}, {"id": "2006.10916", "submitter": "Seyed Abdulaziz Esmaeili", "authors": "Seyed A. Esmaeili, Brian Brubach, Leonidas Tsepenekas, John P.\n  Dickerson", "title": "Probabilistic Fair Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clustering problems, a central decision-maker is given a complete metric\ngraph over vertices and must provide a clustering of vertices that minimizes\nsome objective function. In fair clustering problems, vertices are endowed with\na color (e.g., membership in a group), and the features of a valid clustering\nmight also include the representation of colors in that clustering. Prior work\nin fair clustering assumes complete knowledge of group membership. In this\npaper, we generalize prior work by assuming imperfect knowledge of group\nmembership through probabilistic assignments. We present clustering algorithms\nin this more general setting with approximation ratio guarantees. We also\naddress the problem of \"metric membership\", where different groups have a\nnotion of order and distance. Experiments are conducted using our proposed\nalgorithms as well as baselines to validate our approach and also surface\nnuanced concerns when group membership is not known deterministically.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 01:34:21 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Esmaeili", "Seyed A.", ""], ["Brubach", "Brian", ""], ["Tsepenekas", "Leonidas", ""], ["Dickerson", "John P.", ""]]}, {"id": "2006.11009", "submitter": "Mohsen Abbasi", "authors": "Mohsen Abbasi, Aditya Bhaskara, Suresh Venkatasubramanian", "title": "Fair clustering via equitable group representations", "comments": "11 pages, 5 figures, ACM Conference on Fairness, Accountability, and\n  Transparency (ACM FAccT)", "journal-ref": null, "doi": "10.1145/3442188.3445913", "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What does it mean for a clustering to be fair? One popular approach seeks to\nensure that each cluster contains groups in (roughly) the same proportion in\nwhich they exist in the population. The normative principle at play is balance:\nany cluster might act as a representative of the data, and thus should reflect\nits diversity.\n  But clustering also captures a different form of representativeness. A core\nprinciple in most clustering problems is that a cluster center should be\nrepresentative of the cluster it represents, by being \"close\" to the points\nassociated with it. This is so that we can effectively replace the points by\ntheir cluster centers without significant loss in fidelity, and indeed is a\ncommon \"use case\" for clustering. For such a clustering to be fair, the centers\nshould \"represent\" different groups equally well. We call such a clustering a\ngroup-representative clustering.\n  In this paper, we study the structure and computation of group-representative\nclusterings. We show that this notion naturally parallels the development of\nfairness notions in classification, with direct analogs of ideas like\ndemographic parity and equal opportunity. We demonstrate how these notions are\ndistinct from and cannot be captured by balance-based notions of fairness. We\npresent approximation algorithms for group representative $k$-median clustering\nand couple this with an empirical evaluation on various real-world data sets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 08:16:01 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 23:38:12 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Abbasi", "Mohsen", ""], ["Bhaskara", "Aditya", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "2006.11148", "submitter": "Chen Avin", "authors": "Chen Avin and Chen Griner and Iosif Salem and Stefan Schmid", "title": "An Online Matching Model for Self-Adjusting ToR-to-ToR Networks", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a short note that formally presents the matching model for the\ntheoretical study of self-adjusting networks as initially proposed in [1].\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 14:20:13 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Avin", "Chen", ""], ["Griner", "Chen", ""], ["Salem", "Iosif", ""], ["Schmid", "Stefan", ""]]}, {"id": "2006.11155", "submitter": "Pawe{\\l} Rz\\k{a}\\.zewski", "authors": "Karolina Okrasa and Marta Piecyk and Pawe{\\l} Rz\\k{a}\\.zewski", "title": "Full complexity classification of the list homomorphism problem for\n  bounded-treewidth graphs", "comments": "The extended abstract of the paper was accepted to ESA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A homomorphism from a graph $G$ to a graph $H$ is an edge-preserving mapping\nfrom $V(G)$ to $V(H)$. Let $H$ be a fixed graph with possible loops. In the\nlist homomorphism problem, denoted by LHom($H$), we are given a graph $G$,\nwhose every vertex $v$ is assigned with a list $L(v)$ of vertices of $H$. We\nask whether there exists a homomorphism $h$ from $G$ to $H$, which respects\nlists $L$, i.e., for every $v \\in V(G)$ it holds that $h(v) \\in L(v)$.\n  The complexity dichotomy for LHom($H$) was proven by Feder, Hell, and Huang\n[JGT 2003]. We are interested in the complexity of the problem, parameterized\nby the treewidth of the input graph. This problem was investigated by Egri,\nMarx, and Rz\\k{a}\\.zewski [STACS 2018], who obtained tight complexity bounds\nfor the special case of reflexive graphs $H$.\n  In this paper we extend and generalize their results for \\emph{all} relevant\ngraphs $H$, i.e., those, for which the LHom{H} problem is NP-hard. For every\nsuch $H$ we find a constant $k = k(H)$, such that LHom($H$) on instances with\n$n$ vertices and treewidth $t$\n  * can be solved in time $k^{t} \\cdot n^{\\mathcal{O}(1)}$, provided that the\ninput graph is given along with a tree decomposition of width $t$,\n  * cannot be solved in time $(k-\\varepsilon)^{t} \\cdot n^{\\mathcal{O}(1)}$,\nfor any $\\varepsilon >0$, unless the SETH fails.\n  For some graphs $H$ the value of $k(H)$ is much smaller than the trivial\nupper bound, i.e., $|V(H)|$.\n  Obtaining matching upper and lower bounds shows that the set of algorithmic\ntools we have discovered cannot be extended in order to obtain faster\nalgorithms for LHom($H$) in bounded-treewidth graphs. Furthermore, neither the\nalgorithm, nor the proof of the lower bound, is very specific to treewidth. We\nbelieve that they can be used for other variants of LHom($H$), e.g. with\ndifferent parameterizations.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 14:32:14 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 09:57:04 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Okrasa", "Karolina", ""], ["Piecyk", "Marta", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""]]}, {"id": "2006.11182", "submitter": "Uthaipon Tantipongpipat", "authors": "Uthaipon Tantipongpipat", "title": "$\\lambda$-Regularized A-Optimal Design and its Approximation by\n  $\\lambda$-Regularized Proportional Volume Sampling", "comments": "The previous work to which this work extends can be found at\n  arXiv:1802.08318", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the $\\lambda$-regularized $A$-optimal design problem\nand introduce the $\\lambda$-regularized proportional volume sampling algorithm,\ngeneralized from [Nikolov, Singh, and Tantipongpipat, 2019], for this problem\nwith the approximation guarantee that extends upon the previous work. In this\nproblem, we are given vectors $v_1,\\ldots,v_n\\in\\mathbb{R}^d$ in $d$\ndimensions, a budget $k\\leq n$, and the regularizer parameter $\\lambda\\geq0$,\nand the goal is to find a subset $S\\subseteq [n]$ of size $k$ that minimizes\nthe trace of $\\left(\\sum_{i\\in S}v_iv_i^\\top + \\lambda I_d\\right)^{-1}$ where\n$I_d$ is the $d\\times d$ identity matrix. The problem is motivated from optimal\ndesign in ridge regression, where one tries to minimize the expected squared\nerror of the ridge regression predictor from the true coefficient in the\nunderlying linear model. We introduce $\\lambda$-regularized proportional volume\nsampling and give its polynomial-time implementation to solve this problem. We\nshow its $(1+\\frac{\\epsilon}{\\sqrt{1+\\lambda'}})$-approximation for\n$k=\\Omega\\left(\\frac d\\epsilon+\\frac{\\log 1/\\epsilon}{\\epsilon^2}\\right)$ where\n$\\lambda'$ is proportional to $\\lambda$, extending the previous bound in\n[Nikolov, Singh, and Tantipongpipat, 2019] to the case $\\lambda>0$ and\nobtaining asymptotic optimality as $\\lambda\\rightarrow \\infty$.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 15:17:57 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Tantipongpipat", "Uthaipon", ""]]}, {"id": "2006.11580", "submitter": "Will Perkins", "authors": "Tyler Helmuth, Matthew Jenssen, Will Perkins", "title": "Finite-size scaling, phase coexistence, and algorithms for the random\n  cluster model on random graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For $\\Delta \\ge 5$ and $q$ large as a function of $\\Delta$, we give a\ndetailed picture of the phase transition of the random cluster model on random\n$\\Delta$-regular graphs. In particular, we determine the limiting distribution\nof the weights of the ordered and disordered phases at criticality and prove\nexponential decay of correlations away from criticality.\n  Our techniques are based on using polymer models and the cluster expansion to\ncontrol deviations from the ordered and disordered ground states. These\ntechniques also yield efficient approximate counting and sampling algorithms\nfor the Potts and random cluster models on random $\\Delta$-regular graphs at\nall temperatures when $q$ is large. Our algorithms apply more generally to\n$\\Delta$-regular graphs satisfying a small set expansion condition.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 14:03:48 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Helmuth", "Tyler", ""], ["Jenssen", "Matthew", ""], ["Perkins", "Will", ""]]}, {"id": "2006.11589", "submitter": "Calvin Beideman", "authors": "Calvin Beideman, Karthekeyan Chandrasekaran and Chao Xu", "title": "Multicritera Cuts and Size-Constrained $k$-cuts in Hypergraphs", "comments": "Accepted to RANDOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address counting and optimization variants of multicriteria global min-cut\nand size-constrained min-$k$-cut in hypergraphs.\n  1. For an $r$-rank $n$-vertex hypergraph endowed with $t$ hyperedge-cost\nfunctions, we show that the number of multiobjective min-cuts is\n$O(r2^{tr}n^{3t-1})$. In particular, this shows that the number of parametric\nmin-cuts in constant rank hypergraphs for a constant number of criteria is\nstrongly polynomial, thus resolving an open question by Aissi, Mahjoub,\nMcCormick, and Queyranne (Math Programming, 2015). In addition, we give\nrandomized algorithms to enumerate all multiobjective min-cuts and all\npareto-optimal cuts in strongly polynomial-time.\n  2. We also address node-budgeted multiobjective min-cuts: For an $n$-vertex\nhypergraph endowed with $t$ vertex-weight functions, we show that the number of\nnode-budgeted multiobjective min-cuts is $O(r2^{r}n^{t+2})$, where $r$ is the\nrank of the hypergraph, and the number of node-budgeted $b$-multiobjective\nmin-cuts for a fixed budget-vector $b$ is $O(n^2)$.\n  3. We show that min-$k$-cut in hypergraphs subject to constant lower bounds\non part sizes is solvable in polynomial-time for constant $k$, thus resolving\nan open problem posed by Queyranne. Our technique also shows that the number of\noptimal solutions is polynomial.\n  All of our results build on the random contraction approach of Karger (SODA,\n1993). Our techniques illustrate the versatility of the random contraction\napproach to address counting and algorithmic problems concerning multiobjective\nmin-cuts and size-constrained $k$-cuts in hypergraphs.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 14:41:00 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Beideman", "Calvin", ""], ["Chandrasekaran", "Karthekeyan", ""], ["Xu", "Chao", ""]]}, {"id": "2006.11607", "submitter": "Marco Molinaro", "authors": "Thomas Kesselheim and Marco Molinaro", "title": "Knapsack Secretary with Bursty Adversary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random-order or secretary model is one of the most popular beyond-worst\ncase model for online algorithms. While it avoids the pessimism of the\ntraditional adversarial model, in practice we cannot expect the input to be\npresented in perfectly random order. This has motivated research on ``best of\nboth worlds'' (algorithms with good performance on both purely stochastic and\npurely adversarial inputs), or even better, on inputs that are a mix of both\nstochastic and adversarial parts. Unfortunately the latter seems much harder to\nachieve and very few results of this type are known.\n  Towards advancing our understanding of designing such robust algorithms, we\npropose a random-order model with bursts of adversarial time steps. The\nassumption of burstiness of unexpected patterns is reasonable in many contexts,\nsince changes (e.g. spike in a demand for a good) are often triggered by a\ncommon external event. We then consider the Knapsack Secretary problem in this\nmodel: there is a knapsack of size $k$ (e.g., available quantity of a good),\nand in each of the $n$ time steps an item comes with its value and size in\n$[0,1]$ and the algorithm needs to make an irrevocable decision whether to\naccept or reject the item.\n  We design an algorithm that gives an approximation of $1 -\n\\tilde{O}(\\Gamma/k)$ when the adversarial time steps can be covered by $\\Gamma\n\\ge \\sqrt{k}$ intervals of size $\\tilde{O}(\\frac{n}{k})$. In particular,\nsetting $\\Gamma = \\sqrt{k}$ gives a $(1 - O(\\frac{\\ln^2\nk}{\\sqrt{k}}))$-approximation that is resistant to up to a $\\frac{\\ln^2\nk}{\\sqrt{k}}$-fraction of the items being adversarial, which is almost optimal\neven in the absence of adversarial items. Also, setting $\\Gamma =\n\\tilde{\\Omega}(k)$ gives a constant approximation that is resistant to up to a\nconstant fraction of items being adversarial.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 16:24:22 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Kesselheim", "Thomas", ""], ["Molinaro", "Marco", ""]]}, {"id": "2006.11648", "submitter": "Binghui Peng", "authors": "Jan van den Brand, Binghui Peng, Zhao Song, Omri Weinstein", "title": "Training (Overparametrized) Neural Networks in Near-Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The slow convergence rate and pathological curvature issues of first-order\ngradient methods for training deep neural networks, initiated an ongoing effort\nfor developing faster $\\mathit{second}$-$\\mathit{order}$ optimization\nalgorithms beyond SGD, without compromising the generalization error. Despite\ntheir remarkable convergence rate ($\\mathit{independent}$ of the training batch\nsize $n$), second-order algorithms incur a daunting slowdown in the\n$\\mathit{cost}$ $\\mathit{per}$ $\\mathit{iteration}$ (inverting the Hessian\nmatrix of the loss function), which renders them impractical. Very recently,\nthis computational overhead was mitigated by the works of [ZMG19,CGH+19},\nyielding an $O(mn^2)$-time second-order algorithm for training two-layer\noverparametrized neural networks of polynomial width $m$.\n  We show how to speed up the algorithm of [CGH+19], achieving an\n$\\tilde{O}(mn)$-time backpropagation algorithm for training (mildly\noverparametrized) ReLU networks, which is near-linear in the dimension ($mn$)\nof the full gradient (Jacobian) matrix. The centerpiece of our algorithm is to\nreformulate the Gauss-Newton iteration as an $\\ell_2$-regression problem, and\nthen use a Fast-JL type dimension reduction to $\\mathit{precondition}$ the\nunderlying Gram matrix in time independent of $M$, allowing to find a\nsufficiently good approximate solution via $\\mathit{first}$-$\\mathit{order}$\nconjugate gradient. Our result provides a proof-of-concept that advanced\nmachinery from randomized linear algebra -- which led to recent breakthroughs\nin $\\mathit{convex}$ $\\mathit{optimization}$ (ERM, LPs, Regression) -- can be\ncarried over to the realm of deep learning as well.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 20:26:14 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 02:02:10 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Brand", "Jan van den", ""], ["Peng", "Binghui", ""], ["Song", "Zhao", ""], ["Weinstein", "Omri", ""]]}, {"id": "2006.11687", "submitter": "Travis Gagie", "authors": "Christina Boucher, Ond\\v{r}ej Cvacho, Travis Gagie, Jan Holub,\n  Giovanni Manzini, Gonzalo Navarro and Massimiliano Rossi", "title": "PFP Data Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prefix-free parsing (PFP) was introduced by Boucher et al. (2019) as a\npreprocessing step to ease the computation of Burrows-Wheeler Transforms (BWTs)\nof genomic databases. Given a string $S$, it produces a dictionary $D$ and a\nparse $P$ of overlapping phrases such that $\\mathrm{BWT} (S)$ can be computed\nfrom $D$ and $P$ in time and workspace bounded in terms of their combined size\n$|\\mathrm{PFP} (S)|$. In practice $D$ and $P$ are significantly smaller than\n$S$ and computing $\\mathrm{BWT} (S)$ from them is more efficient than computing\nit from $S$ directly, at least when $S$ consists of genomes from individuals of\nthe same species. In this paper, we consider $\\mathrm{PFP} (S)$ as a {\\em data\nstructure} and show how it can be augmented to support the following queries\nquickly, still in $O (|\\mathrm{PFP} (S)|)$ space: longest common extension\n(LCE), suffix array (SA), longest common prefix (LCP) and BWT. Lastly, we\nprovide experimental evidence that the PFP data structure can be efficiently\nconstructed for very large repetitive datasets: it takes one hour and 54 GB\npeak memory for $1000$ variants of human chromosome 19, initially occupying\nroughly 56 GB.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 01:29:47 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Boucher", "Christina", ""], ["Cvacho", "Ond\u0159ej", ""], ["Gagie", "Travis", ""], ["Holub", "Jan", ""], ["Manzini", "Giovanni", ""], ["Navarro", "Gonzalo", ""], ["Rossi", "Massimiliano", ""]]}, {"id": "2006.11726", "submitter": "Moran Feldman", "authors": "Moran Feldman and Amin Karbasi", "title": "Continuous Submodular Maximization: Beyond DR-Submodularity", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the first continuous optimization algorithms that\nachieve a constant factor approximation guarantee for the problem of monotone\ncontinuous submodular maximization subject to a linear constraint. We first\nprove that a simple variant of the vanilla coordinate ascent, called\nCoordinate-Ascent+, achieves a $(\\frac{e-1}{2e-1}-\\varepsilon)$-approximation\nguarantee while performing $O(n/\\varepsilon)$ iterations, where the\ncomputational complexity of each iteration is roughly\n$O(n/\\sqrt{\\varepsilon}+n\\log n)$ (here, $n$ denotes the dimension of the\noptimization problem). We then propose Coordinate-Ascent++, that achieves the\ntight $(1-1/e-\\varepsilon)$-approximation guarantee while performing the same\nnumber of iterations, but at a higher computational complexity of roughly\n$O(n^3/\\varepsilon^{2.5} + n^3 \\log n / \\varepsilon^2)$ per iteration. However,\nthe computation of each round of Coordinate-Ascent++ can be easily parallelized\nso that the computational cost per machine scales as\n$O(n/\\sqrt{\\varepsilon}+n\\log n)$.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 06:57:59 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Feldman", "Moran", ""], ["Karbasi", "Amin", ""]]}, {"id": "2006.11827", "submitter": "Ellen Vitercik", "authors": "Maria-Florina Balcan, Tuomas Sandholm, Ellen Vitercik", "title": "Refined bounds for algorithm configuration: The knife-edge of dual class\n  approximability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automating algorithm configuration is growing increasingly necessary as\nalgorithms come with more and more tunable parameters. It is common to tune\nparameters using machine learning, optimizing performance metrics such as\nruntime and solution quality. The training set consists of problem instances\nfrom the specific domain at hand. We investigate a fundamental question about\nthese techniques: how large should the training set be to ensure that a\nparameter's average empirical performance over the training set is close to its\nexpected, future performance? We answer this question for algorithm\nconfiguration problems that exhibit a widely-applicable structure: the\nalgorithm's performance as a function of its parameters can be approximated by\na \"simple\" function. We show that if this approximation holds under the\nL-infinity norm, we can provide strong sample complexity bounds. On the flip\nside, if the approximation holds only under the L-p norm for p smaller than\ninfinity, it is not possible to provide meaningful sample complexity bounds in\nthe worst case. We empirically evaluate our bounds in the context of integer\nprogramming, one of the most powerful tools in computer science. Via\nexperiments, we obtain sample complexity bounds that are up to 700 times\nsmaller than the previously best-known bounds.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 15:32:21 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 16:47:46 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Sandholm", "Tuomas", ""], ["Vitercik", "Ellen", ""]]}, {"id": "2006.11876", "submitter": "Hanzhi Wang", "authors": "Hanzhi Wang, Zhewei Wei, Junhao Gan, Sibo Wang, Zengfeng Huang", "title": "Personalized PageRank to a Target Node, Revisited", "comments": "ACM SIGKDD 2020", "journal-ref": null, "doi": "10.1145/3394486.3403108", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized PageRank (PPR) is a widely used node proximity measure in graph\nmining and network analysis. Given a source node $s$ and a target node $t$, the\nPPR value $\\pi(s,t)$ represents the probability that a random walk from $s$\nterminates at $t$, and thus indicates the bidirectional importance between $s$\nand $t$. The majority of the existing work focuses on the single-source\nqueries, which asks for the PPR value of a given source node $s$ and every node\n$t \\in V$. However, the single-source query only reflects the importance of\neach node $t$ with respect to $s$. In this paper, we consider the {\\em\nsingle-target PPR query}, which measures the opposite direction of importance\nfor PPR. Given a target node $t$, the single-target PPR query asks for the PPR\nvalue of every node $s\\in V$ to a given target node $t$. We propose RBS, a\nnovel algorithm that answers approximate single-target queries with optimal\ncomputational complexity. We show that RBS improves three concrete\napplications: heavy hitters PPR query, single-source SimRank computation, and\nscalable graph neural networks. We conduct experiments to demonstrate that RBS\noutperforms the state-of-the-art algorithms in terms of both efficiency and\nprecision on real-world benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 18:54:53 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 06:17:59 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Wang", "Hanzhi", ""], ["Wei", "Zhewei", ""], ["Gan", "Junhao", ""], ["Wang", "Sibo", ""], ["Huang", "Zengfeng", ""]]}, {"id": "2006.11947", "submitter": "Suman Bera", "authors": "Suman K. Bera, C. Seshadhri", "title": "How to Count Triangles, without Seeing the Whole Graph", "comments": "Accepted for publication in KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Triangle counting is a fundamental problem in the analysis of large graphs.\nThere is a rich body of work on this problem, in varying streaming and\ndistributed models, yet all these algorithms require reading the whole input\ngraph. In many scenarios, we do not have access to the whole graph, and can\nonly sample a small portion of the graph (typically through crawling). In such\na setting, how can we accurately estimate the triangle count of the graph?\n  We formally study triangle counting in the {\\em random walk} access model\nintroduced by Dasgupta et al (WWW '14) and Chierichetti et al (WWW '16). We\nhave access to an arbitrary seed vertex of the graph, and can only perform\nrandom walks. This model is restrictive in access and captures the challenges\nof collecting real-world graphs. Even sampling a uniform random vertex is a\nhard task in this model.\n  Despite these challenges, we design a provable and practical algorithm,\nTETRIS, for triangle counting in this model. TETRIS is the first provably\nsublinear algorithm (for most natural parameter settings) that approximates the\ntriangle count in the random walk model, for graphs with low mixing time. Our\nresult builds on recent advances in the theory of sublinear algorithms. The\nfinal sample built by TETRIS is a careful mix of random walks and degree-biased\nsampling of neighborhoods. Empirically, TETRIS accurately counts triangles on a\nvariety of large graphs, getting estimates within 5\\% relative error by looking\nat 3\\% of the number of edges.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 00:08:43 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Bera", "Suman K.", ""], ["Seshadhri", "C.", ""]]}, {"id": "2006.11978", "submitter": "Younan Gao", "authors": "Younan Gao and Meng He and Yakov Nekrich", "title": "Fast Preprocessing for Optimal Orthogonal Range Reporting and Range\n  Successor with Applications to Text Indexing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Under the word RAM model, we design three data structures that can be\nconstructed in $O(n\\sqrt{\\lg n})$ time over $n$ points in an $n \\times n$ grid.\nThe first data structure is an $O(n\\lg^{\\epsilon} n)$-word structure supporting\northogonal range reporting in $O(\\lg\\lg n+k)$ time, where $k$ denotes output\nsize and $\\epsilon$ is an arbitrarily small constant. The second is an\n$O(n\\lg\\lg n)$-word structure supporting orthogonal range successor in\n$O(\\lg\\lg n)$ time, while the third is an $O(n\\lg^{\\epsilon} n)$-word structure\nsupporting sorted range reporting in $O(\\lg\\lg n+k)$ time. The query times of\nthese data structures are optimal when the space costs must be within $O(n\\\npolylog\\ n)$ words. Their exact space bounds match those of the best known\nresults achieving the same query times, and the $O(n\\sqrt{\\lg n})$ construction\ntime beats the previous bounds on preprocessing. Previously, among 2d range\nsearch structures, only the orthogonal range counting structure of Chan and\nP\\v{a}tra\\c{s}cu (SODA 2010) and the linear space, $O(\\lg^{\\epsilon} n)$ query\ntime structure for orthogonal range successor by Belazzougui and Puglisi (SODA\n2016) can be built in the same $O(n\\sqrt{\\lg n})$ time. Hence our work is the\nfirst that achieve the same preprocessing time for optimal orthogonal range\nreporting and range successor. We also apply our results to improve the\nconstruction time of text indexes.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 02:54:20 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Gao", "Younan", ""], ["He", "Meng", ""], ["Nekrich", "Yakov", ""]]}, {"id": "2006.12011", "submitter": "Aravind Gollakota", "authors": "Surbhi Goel, Aravind Gollakota, Zhihan Jin, Sushrut Karmalkar, Adam\n  Klivans", "title": "Superpolynomial Lower Bounds for Learning One-Layer Neural Networks\n  using Gradient Descent", "comments": "25 pages, ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the first superpolynomial lower bounds for learning one-layer neural\nnetworks with respect to the Gaussian distribution using gradient descent. We\nshow that any classifier trained using gradient descent with respect to\nsquare-loss will fail to achieve small test error in polynomial time given\naccess to samples labeled by a one-layer neural network. For classification, we\ngive a stronger result, namely that any statistical query (SQ) algorithm\n(including gradient descent) will fail to achieve small test error in\npolynomial time. Prior work held only for gradient descent run with small batch\nsizes, required sharp activations, and applied to specific classes of queries.\nOur lower bounds hold for broad classes of activations including ReLU and\nsigmoid. The core of our result relies on a novel construction of a simple\nfamily of neural networks that are exactly orthogonal with respect to all\nspherically symmetric distributions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 05:15:06 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 21:40:56 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Goel", "Surbhi", ""], ["Gollakota", "Aravind", ""], ["Jin", "Zhihan", ""], ["Karmalkar", "Sushrut", ""], ["Klivans", "Adam", ""]]}, {"id": "2006.12363", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi and Nisheeth K. Vishnoi", "title": "Greedy Adversarial Equilibrium: An Efficient Alternative to\n  Nonconvex-Nonconcave Min-Max Optimization", "comments": "Full version of a paper appearing in ACM Symposium on Theory of\n  Computing (STOC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Min-max optimization of an objective function $f: \\mathbb{R}^d \\times\n\\mathbb{R}^d \\rightarrow \\mathbb{R}$ is an important model for robustness in an\nadversarial setting, with applications to many areas including optimization,\neconomics, and deep learning. In many applications $f$ may be\nnonconvex-nonconcave, and finding a global min-max point may be computationally\nintractable. There is a long line of work that seeks computationally tractable\nalgorithms for alternatives to the min-max optimization model. However, many of\nthe alternative models have solution points which are only guaranteed to exist\nunder strong assumptions on $f$, such as convexity, monotonicity, or special\nproperties of the starting point. We propose an optimization model, the\n$\\varepsilon$-greedy adversarial equilibrium, and show that it can serve as a\ncomputationally tractable alternative to the min-max optimization model.\nRoughly, we say that a point $(x^\\star, y^\\star)$ is an $\\varepsilon$-greedy\nadversarial equilibrium if $y^\\star$ is an $\\varepsilon$-approximate local\nmaximum for $f(x^\\star,\\cdot)$, and $x^\\star$ is an $\\varepsilon$-approximate\nlocal minimum for a \"greedy approximation\" to the function $\\max_z f(x, z)$\nwhich can be efficiently estimated using second-order optimization algorithms.\nWe prove the existence of such a point for any smooth function which is bounded\nand has Lipschitz Hessian. To prove existence, we introduce an algorithm that\nconverges from any starting point to an $\\varepsilon$-greedy adversarial\nequilibrium in a number of evaluations of the function $f$, the max-player's\ngradient $\\nabla_y f(x,y)$, and its Hessian $\\nabla^2_y f(x,y)$, that is\npolynomial in the dimension $d$, $1/\\varepsilon$, and the bounds on $f$ and its\nLipschitz constant.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 16:03:41 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 17:50:29 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2020 15:53:30 GMT"}, {"version": "v4", "created": "Tue, 4 May 2021 16:41:54 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Mangoubi", "Oren", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "2006.12367", "submitter": "Chara Podimata", "authors": "Chara Podimata, Aleksandrs Slivkins", "title": "Adaptive Discretization for Adversarial Lipschitz Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lipschitz bandits is a prominent version of multi-armed bandits that studies\nlarge, structured action spaces such as the [0,1] interval, where similar\nactions are guaranteed to have similar rewards. A central theme here is the\nadaptive discretization of the action space, which gradually \"zooms in\" on the\nmore promising regions thereof. The goal is to take advantage of \"nicer\"\nproblem instances, while retaining near-optimal worst-case performance. While\nthe stochastic version of the problem is well-understood, the general version\nwith adversarial rewards is not.\n  We provide the first algorithm for adaptive discretization in the adversarial\nversion, and derive instance-dependent regret bounds. In particular, we recover\nthe worst-case optimal regret bound for the adversarial version, and the\ninstance-dependent regret bound for the stochastic version. Further, an\napplication of our algorithm to dynamic pricing (where a seller repeatedly\nadjusts prices for a product) enjoys these regret bounds without any smoothness\nassumptions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 16:06:25 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 02:33:38 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Podimata", "Chara", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "2006.12376", "submitter": "Vijay Keswani", "authors": "Vijay Keswani, Oren Mangoubi, Sushant Sachdeva, Nisheeth K. Vishnoi", "title": "A Convergent and Dimension-Independent First-Order Algorithm for Min-Max\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the recent work of Mangoubi and Vishnoi (STOC 2021), we propose\na variant of the min-max optimization framework where the max-player is\nconstrained to update the maximization variable in a greedy manner until it\nreaches a *first-order* stationary point. We present an algorithm that provably\nconverges to an approximate local equilibrium for our framework from any\ninitialization and for nonconvex-nonconcave loss functions. Compared to the\nsecond-order algorithm of Mangoubi and Vishnoi, whose iteration bound is\npolynomial in the dimension, our algorithm is first-order and its iteration\nbound is independent of dimension. We empirically evaluate our algorithm on\nchallenging nonconvex-nonconcave test-functions and loss functions that arise\nin GAN training. Our algorithm converges on these test functions and, when used\nto train GANs on synthetic and real-world datasets, trains stably and avoids\nmode collapse.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 16:11:30 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 17:54:43 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 05:22:51 GMT"}, {"version": "v4", "created": "Mon, 15 Feb 2021 07:53:06 GMT"}, {"version": "v5", "created": "Fri, 4 Jun 2021 00:35:11 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Keswani", "Vijay", ""], ["Mangoubi", "Oren", ""], ["Sachdeva", "Sushant", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "2006.12454", "submitter": "Sayan Bandyapadhyay", "authors": "Sayan Bandyapadhyay", "title": "Improved Bounds for Metric Capacitated Covering Problems", "comments": "To appear at European Symposia on Algorithms 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Metric Capacitated Covering (MCC) problem, given a set of balls\n$\\mathcal{B}$ in a metric space $P$ with metric $d$ and a capacity parameter\n$U$, the goal is to find a minimum sized subset $\\mathcal{B}'\\subseteq\n\\mathcal{B}$ and an assignment of the points in $P$ to the balls in\n$\\mathcal{B}'$ such that each point is assigned to a ball that contains it and\neach ball is assigned with at most $U$ points. MCC achieves an $O(\\log\n|P|)$-approximation using a greedy algorithm. On the other hand, it is hard to\napproximate within a factor of $o(\\log |P|)$ even with $\\beta < 3$ factor\nexpansion of the balls. Bandyapadhyay~{et al.} [SoCG 2018, DCG 2019] showed\nthat one can obtain an $O(1)$-approximation for the problem with $6.47$ factor\nexpansion of the balls. An open question left by their work is to reduce the\ngap between the lower bound $3$ and the upper bound $6.47$. In this current\nwork, we show that it is possible to obtain an $O(1)$-approximation with only\n$4.24$ factor expansion of the balls. We also show a similar upper bound of $5$\nfor a more generalized version of MCC for which the best previously known bound\nwas $9$.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 17:36:52 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Bandyapadhyay", "Sayan", ""]]}, {"id": "2006.12476", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane and Vasilis Kontonis and Nikos\n  Zarifis", "title": "Algorithms and SQ Lower Bounds for PAC Learning One-Hidden-Layer ReLU\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of PAC learning one-hidden-layer ReLU networks with $k$\nhidden units on $\\mathbb{R}^d$ under Gaussian marginals in the presence of\nadditive label noise. For the case of positive coefficients, we give the first\npolynomial-time algorithm for this learning problem for $k$ up to\n$\\tilde{O}(\\sqrt{\\log d})$. Previously, no polynomial time algorithm was known,\neven for $k=3$. This answers an open question posed by~\\cite{Kliv17}.\nImportantly, our algorithm does not require any assumptions about the rank of\nthe weight matrix and its complexity is independent of its condition number. On\nthe negative side, for the more general task of PAC learning one-hidden-layer\nReLU networks with arbitrary real coefficients, we prove a Statistical Query\nlower bound of $d^{\\Omega(k)}$. Thus, we provide a separation between the two\nclasses in terms of efficient learnability. Our upper and lower bounds are\ngeneral, extending to broader families of activation functions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 17:53:54 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Kontonis", "Vasilis", ""], ["Zarifis", "Nikos", ""]]}, {"id": "2006.12561", "submitter": "Ahmad Biniaz", "authors": "Ahmad Biniaz", "title": "Better approximation algorithms for maximum weight internal spanning\n  trees in cubic graphs and claw-free graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a connected vertex-weighted graph $G$, the maximum weight internal\nspanning tree (MaxwIST) problem asks for a spanning tree of $G$ that maximizes\nthe total weight of internal nodes. This problem is NP-hard and APX-hard, with\nthe currently best known approximation factor $1/2$ (Chen et al., Algorithmica\n2019). For the case of claw-free graphs, Chen et al. present an involved\napproximation algorithm with approximation factor $7/12$. They asked whether it\nis possible to improve these ratios, in particular for claw-free graphs and\ncubic graphs.\n  We improve the approximation factors for the MaxwIST problem in cubic graphs\nand claw-free graphs. For cubic graphs we present an algorithm that computes a\nspanning tree whose total weight of internal vertices is at least\n$\\frac{3}{4}-\\frac{3}{n}$ times the total weight of all vertices, where $n$ is\nthe number of vertices of $G$. This ratio is almost tight for large values of\n$n$. For claw-free graphs of degree at least three, we present an algorithm\nthat computes a spanning tree whose total internal weight is at least\n$\\frac{3}{5}-\\frac{1}{n}$ times the total vertex weight. The degree constraint\nis necessary as this ratio may not be achievable if we allow vertices of degree\nless than three.\n  With the above ratios, we immediately obtain better approximation algorithms\nwith factors $\\frac{3}{4}-\\epsilon$ and $\\frac{3}{5}-\\epsilon$ for the MaxwIST\nproblem in cubic graphs and claw-free graphs of degree at least three, for any\n$\\epsilon>0$. In addition to improving the approximation factors, the new\nalgorithms are relatively short compared to that of Chen et al.. The new\nalgorithms are fairly simple, and employ a variant of the depth-first search\nalgorithm that selects a relatively-large-weight vertex in every branching\nstep. Moreover, the new algorithms take linear time while previous algorithms\nfor similar problem instances are super-linear.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 18:47:54 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Biniaz", "Ahmad", ""]]}, {"id": "2006.12589", "submitter": "Suman Bera", "authors": "Nihesh Anderson, Suman K. Bera, Syamantak Das, Yang Liu", "title": "Distributional Individual Fairness in Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we initiate the study of fair clustering that ensures\ndistributional similarity among similar individuals. In response to improving\nfairness in machine learning, recent papers have investigated fairness in\nclustering algorithms and have focused on the paradigm of statistical\nparity/group fairness. These efforts attempt to minimize bias against some\nprotected groups in the population. However, to the best of our knowledge, the\nalternative viewpoint of individual fairness, introduced by Dwork et al. (ITCS\n2012) in the context of classification, has not been considered for clustering\nso far. Similar to Dwork et al., we adopt the individual fairness notion which\nmandates that similar individuals should be treated similarly for clustering\nproblems. We use the notion of $f$-divergence as a measure of statistical\nsimilarity that significantly generalizes the ones used by Dwork et al. We\nintroduce a framework for assigning individuals, embedded in a metric space, to\nprobability distributions over a bounded number of cluster centers. The\nobjective is to ensure (a) low cost of clustering in expectation and (b)\nindividuals that are close to each other in a given fairness space are mapped\nto statistically similar distributions.\n  We provide an algorithm for clustering with $p$-norm objective ($k$-center,\n$k$-means are special cases) and individual fairness constraints with provable\napproximation guarantee. We extend this framework to include both group\nfairness and individual fairness inside the protected groups. Finally, we\nobserve conditions under which individual fairness implies group fairness. We\npresent extensive experimental evidence that justifies the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 20:02:09 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Anderson", "Nihesh", ""], ["Bera", "Suman K.", ""], ["Das", "Syamantak", ""], ["Liu", "Yang", ""]]}, {"id": "2006.12608", "submitter": "Francesco Silvestri", "authors": "Thomas D. Ahle and Francesco Silvestri", "title": "Similarity Search with Tensor Core Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor Core Units (TCUs) are hardware accelerators developed for deep neural\nnetworks, which efficiently support the multiplication of two dense\n$\\sqrt{m}\\times \\sqrt{m}$ matrices, where $m$ is a given hardware parameter. In\nthis paper, we show that TCUs can speed up similarity search problems as well.\nWe propose algorithms for the Johnson-Lindenstrauss dimensionality reduction\nand for similarity join that, by leveraging TCUs, achieve a $\\sqrt{m}$ speedup\nup with respect to traditional approaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 20:47:38 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Ahle", "Thomas D.", ""], ["Silvestri", "Francesco", ""]]}, {"id": "2006.12670", "submitter": "Huan Li", "authors": "Anindya De, Sanjeev Khanna, Huan Li, Hesam Nikpey", "title": "An Efficient PTAS for Stochastic Load Balancing with Poisson Jobs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first polynomial-time approximation scheme (PTAS) for the\nstochastic load balancing problem when the job sizes follow Poisson\ndistributions. This improves upon the 2-approximation algorithm due to Goel and\nIndyk (FOCS'99). Moreover, our approximation scheme is an efficient PTAS that\nhas a running time double exponential in $1/\\epsilon$ but nearly-linear in $n$,\nwhere $n$ is the number of jobs and $\\epsilon$ is the target error. Previously,\na PTAS (not efficient) was only known for jobs that obey exponential\ndistributions (Goel and Indyk, FOCS'99).\n  Our algorithm relies on several probabilistic ingredients including some\n(seemingly) new results on scaling and the so-called \"focusing effect\" of\nmaximum of Poisson random variables which might be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 00:06:42 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["De", "Anindya", ""], ["Khanna", "Sanjeev", ""], ["Li", "Huan", ""], ["Nikpey", "Hesam", ""]]}, {"id": "2006.12748", "submitter": "Samson Zhou", "authors": "Agniva Chowdhury, Petros Drineas, David P. Woodruff, Samson Zhou", "title": "Approximation Algorithms for Sparse Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Principal component analysis (PCA) is a widely used dimension reduction\ntechnique in machine learning and multivariate statistics. To improve the\ninterpretability of PCA, various approaches to obtain sparse principal\ndirection loadings have been proposed, which are termed Sparse Principal\nComponent Analysis (SPCA). In this paper, we present thresholding as a provably\naccurate, polynomial time, approximation algorithm for the SPCA problem,\nwithout imposing any restrictive assumptions on the input covariance matrix.\nOur first thresholding algorithm using the Singular Value Decomposition is\nconceptually simple; is faster than current state-of-the-art; and performs well\nin practice. On the negative side, our (novel) theoretical bounds do not\naccurately predict the strong practical performance of this approach. The\nsecond algorithm solves a well-known semidefinite programming relaxation and\nthen uses a novel, two step, deterministic thresholding scheme to compute a\nsparse principal vector. It works very well in practice and, remarkably, this\nsolid practical performance is accurately predicted by our theoretical bounds,\nwhich bridge the theory-practice gap better than current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 04:25:36 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 16:25:42 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Chowdhury", "Agniva", ""], ["Drineas", "Petros", ""], ["Woodruff", "David P.", ""], ["Zhou", "Samson", ""]]}, {"id": "2006.12772", "submitter": "Haoyu Zhao", "authors": "Wei Chen, Yihan Du, Longbo Huang, Haoyu Zhao", "title": "Combinatorial Pure Exploration of Dueling Bandit", "comments": "Accepted to ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study combinatorial pure exploration for dueling bandits\n(CPE-DB): we have multiple candidates for multiple positions as modeled by a\nbipartite graph, and in each round we sample a duel of two candidates on one\nposition and observe who wins in the duel, with the goal of finding the best\ncandidate-position matching with high probability after multiple rounds of\nsamples. CPE-DB is an adaptation of the original combinatorial pure exploration\nfor multi-armed bandit (CPE-MAB) problem to the dueling bandit setting.\n  We consider both the Borda winner and the Condorcet winner cases. For Borda\nwinner, we establish a reduction of the problem to the original CPE-MAB setting\nand design PAC and exact algorithms that achieve both the sample complexity\nsimilar to that in the CPE-MAB setting (which is nearly optimal for a subclass\nof problems) and polynomial running time per round.\n  For Condorcet winner, we first design a fully polynomial time approximation\nscheme (FPTAS) for the offline problem of finding the Condorcet winner with\nknown winning probabilities, and then use the FPTAS as an oracle to design a\nnovel pure exploration algorithm ${\\sf CAR}$-${\\sf Cond}$ with sample\ncomplexity analysis. ${\\sf CAR}$-${\\sf Cond}$ is the first algorithm with\npolynomial running time per round for identifying the Condorcet winner in\nCPE-DB.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 05:36:27 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Chen", "Wei", ""], ["Du", "Yihan", ""], ["Huang", "Longbo", ""], ["Zhao", "Haoyu", ""]]}, {"id": "2006.12881", "submitter": "Erich Schubert", "authors": "Andreas Lang and Erich Schubert", "title": "BETULA: Numerically Stable CF-Trees for BIRCH Clustering", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-60936-8_22", "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  BIRCH clustering is a widely known approach for clustering, that has\ninfluenced much subsequent research and commercial products. The key\ncontribution of BIRCH is the Clustering Feature tree (CF-Tree), which is a\ncompressed representation of the input data. As new data arrives, the tree is\neventually rebuilt to increase the compression. Afterward, the leaves of the\ntree are used for clustering. Because of the data compression, this method is\nvery scalable. The idea has been adopted for example for k-means, data stream,\nand density-based clustering.\n  Clustering features used by BIRCH are simple summary statistics that can\neasily be updated with new data: the number of points, the linear sums, and the\nsum of squared values. Unfortunately, how the sum of squares is then used in\nBIRCH is prone to catastrophic cancellation.\n  We introduce a replacement cluster feature that does not have this numeric\nproblem, that is not much more expensive to maintain, and which makes many\ncomputations simpler and hence more efficient. These cluster features can also\neasily be used in other work derived from BIRCH, such as algorithms for\nstreaming data. In the experiments, we demonstrate the numerical problem and\ncompare the performance of the original algorithm compared to the improved\ncluster features.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 10:20:36 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Lang", "Andreas", ""], ["Schubert", "Erich", ""]]}, {"id": "2006.12897", "submitter": "Andreas Emil Feldmann", "authors": "Andreas Emil Feldmann, David Saulpic", "title": "Polynomial Time Approximation Schemes for Clustering in Low Highway\n  Dimension Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study clustering problems such as k-Median, k-Means, and Facility Location\nin graphs of low highway dimension, which is a graph parameter modeling\ntransportation networks. It was previously shown that approximation schemes for\nthese problems exist, which either run in quasi-polynomial time (assuming\nconstant highway dimension) [Feldmann et al. SICOMP 2018] or run in FPT time\n(parameterized by the number of clusters $k$, the highway dimension, and the\napproximation factor) [Becker et al. ESA~2018, Braverman et al. 2020]. In this\npaper we show that a polynomial-time approximation scheme (PTAS) exists\n(assuming constant highway dimension). We also show that the considered\nproblems are NP-hard on graphs of highway dimension 1.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 11:07:39 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 09:41:43 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 14:20:57 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Feldmann", "Andreas Emil", ""], ["Saulpic", "David", ""]]}, {"id": "2006.12929", "submitter": "Gregory Gutin", "authors": "Xiaoyan Zhang, Donglei Du, Gregory Gutin, Qiaoxia Ming, Jian Sun", "title": "Approximation algorithms for general cluster routing problem", "comments": "In COCOON 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph routing problems have been investigated extensively in operations\nresearch, computer science and engineering due to their ubiquity and vast\napplications. In this paper, we study constant approximation algorithms for\nsome variations of the general cluster routing problem. In this problem, we are\ngiven an edge-weighted complete undirected graph $G=(V,E,c),$ whose vertex set\nis partitioned into clusters $C_{1},\\dots ,C_{k}.$ We are also given a subset\n$V'$ of $V$ and a subset $E'$ of $E.$ The weight function $c$ satisfies the\ntriangle inequality. The goal is to find a minimum cost walk $T$ that visits\neach vertex in $V'$ only once, traverses every edge in $E'$ at least once and\nfor every $i\\in [k]$ all vertices of $C_i$ are traversed consecutively.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 12:09:24 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Zhang", "Xiaoyan", ""], ["Du", "Donglei", ""], ["Gutin", "Gregory", ""], ["Ming", "Qiaoxia", ""], ["Sun", "Jian", ""]]}, {"id": "2006.12943", "submitter": "Hao Wu", "authors": "Hao Wu, Junhao Gan, Rui Zhang", "title": "Learning Based Distributed Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the great success of machine learning in the past decade, people\nhave been thinking about the possibility of improving the theoretical results\nby exploring data distribution. In this paper, we revisit a fundamental problem\ncalled Distributed Tracking (DT) under an assumption that the data follows a\ncertain (known or unknown) distribution, and propose a number data-dependent\nalgorithms with improved theoretical bounds. Informally, in the DT problem,\nthere is a coordinator and k players, where the coordinator holds a threshold N\nand each player has a counter. At each time stamp, at most one counter can be\nincreased by one. The job of the coordinator is to capture the exact moment\nwhen the sum of all these k counters reaches N. The goal is to minimise the\ncommunication cost. While our first type of algorithms assume the concrete data\ndistribution is known in advance, our second type of algorithms can learn the\ndistribution on the fly. Both of the algorithms achieve a communication cost\nbounded byO(k log log N) with high probability, improving the state-of-the-art\ndata-independent bound O(k log N/k). We further propose a number of\nimplementation optimisation heuristics to improve both efficiency and\nrobustness of the algorithms. Finally, we conduct extensive experiments on\nthree real datasets and four synthetic datasets. The experimental results show\nthat the communication cost of our algorithms is as least as 20% of that of the\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 12:38:01 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Wu", "Hao", ""], ["Gan", "Junhao", ""], ["Zhang", "Rui", ""]]}, {"id": "2006.13241", "submitter": "Ryan Killick", "authors": "Jurek Czyzowicz, Konstantinos Georgiou, Ryan Killick, Evangelos\n  Kranakis, Danny Krizanc, Lata Narayanan, Jaroslav Opatrny, Denis Pankratov", "title": "The Bike Sharing Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assume that $m \\geq 1$ autonomous mobile agents and $0 \\leq b \\leq m$\nsingle-agent transportation devices (called {\\em bikes}) are initially placed\nat the left endpoint $0$ of the unit interval $[0,1]$. The agents are identical\nin capability and can move at speed one. The bikes cannot move on their own,\nbut any agent riding bike $i$ can move at speed $v_i > 1$. An agent may ride at\nmost one bike at a time. The agents can cooperate by sharing the bikes; an\nagent can ride a bike for a time, then drop it to be used by another agent, and\npossibly switch to a different bike.\n  We study two problems. In the \\BS problem, we require all agents and bikes\nstarting at the left endpoint of the interval to reach the end of the interval\nas soon as possible. In the \\RBS problem, we aim to minimize the arrival time\nof the agents; the bikes can be used to increase the average speed of the\nagents, but are not required to reach the end of the interval.\n  Our main result is the construction of a polynomial time algorithm for the\n\\BS problem that creates an arrival-time optimal schedule for travellers and\nbikes to travel across the interval. For the \\RBS problem, we give an algorithm\nthat gives an optimal solution for the case when at most one of the bikes can\nbe abandoned.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 18:00:39 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Czyzowicz", "Jurek", ""], ["Georgiou", "Konstantinos", ""], ["Killick", "Ryan", ""], ["Kranakis", "Evangelos", ""], ["Krizanc", "Danny", ""], ["Narayanan", "Lata", ""], ["Opatrny", "Jaroslav", ""], ["Pankratov", "Denis", ""]]}, {"id": "2006.13312", "submitter": "Guanghao Ye", "authors": "Jerry Li, Guanghao Ye", "title": "Robust Gaussian Covariance Estimation in Nearly-Matrix Multiplication\n  Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust covariance estimation is the following, well-studied problem in high\ndimensional statistics: given $N$ samples from a $d$-dimensional Gaussian\n$\\mathcal{N}(\\boldsymbol{0}, \\Sigma)$, but where an $\\varepsilon$-fraction of\nthe samples have been arbitrarily corrupted, output $\\widehat{\\Sigma}$\nminimizing the total variation distance between $\\mathcal{N}(\\boldsymbol{0},\n\\Sigma)$ and $\\mathcal{N}(\\boldsymbol{0}, \\widehat{\\Sigma})$. This corresponds\nto learning $\\Sigma$ in a natural affine-invariant variant of the Frobenius\nnorm known as the \\emph{Mahalanobis norm}. Previous work of Cheng et al\ndemonstrated an algorithm that, given $N = \\Omega (d^2 / \\varepsilon^2)$\nsamples, achieved a near-optimal error of $O(\\varepsilon \\log 1 /\n\\varepsilon)$, and moreover, their algorithm ran in time $\\widetilde{O}(T(N, d)\n\\log \\kappa / \\mathrm{poly} (\\varepsilon))$, where $T(N, d)$ is the time it\ntakes to multiply a $d \\times N$ matrix by its transpose, and $\\kappa$ is the\ncondition number of $\\Sigma$. When $\\varepsilon$ is relatively small, their\npolynomial dependence on $1/\\varepsilon$ in the runtime is prohibitively large.\nIn this paper, we demonstrate a novel algorithm that achieves the same\nstatistical guarantees, but which runs in time $\\widetilde{O} (T(N, d) \\log\n\\kappa)$. In particular, our runtime has no dependence on $\\varepsilon$. When\n$\\Sigma$ is reasonably conditioned, our runtime matches that of the fastest\nalgorithm for covariance estimation without outliers, up to poly-logarithmic\nfactors, showing that we can get robustness essentially \"for free.\"\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 20:21:27 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Li", "Jerry", ""], ["Ye", "Guanghao", ""]]}, {"id": "2006.13412", "submitter": "Dezhou Shen", "authors": "Dezhou Shen", "title": "Lower Bounds on Rate of Convergence of Matrix Products in All Pairs\n  Shortest Path of Social Network", "comments": "9 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of social network applications, social network has\nbecome an important medium for people to interact. For the minimum distance\ncomputation of all pairs in networks, Alon N[4] proposed an algorithm with\nmatrix multiplication, combining with distance product association law and\nblock matrix multiplication, all pairs shortest path length algorithm on\nnetworks has time bound O((2n^3)/B logn). In practical applications,\nconsidering the scale-free characteristics of social networks and the precision\nlimitations of floating-point operations on computer hardware, I found that the\nshortest path algorithm has an improved time bound O((14n^3)/B). Based on the\nabove theory, I propose an all pairs shortest path algorithm that combines\nsparseness judgment and convergence judgment, leveraging the distance product\nalgorithm with matrix multiplication, distance product association law, block\nmatrix multiplication, scale-free characteristics of social networks, and\nlimitation of floating-point operations on hardware. Testing on a social\nnetwork dataset with 8508 actors, compared to Alon N algorithm, proposed\nalgorithm has a performance improvement of 39% to 36.2 times on CPU and GPU.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 01:14:47 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Shen", "Dezhou", ""]]}, {"id": "2006.13430", "submitter": "Mauro da Silva", "authors": "Mauro R. C. da Silva, Lehilton L. C. Pedrosa, and Rafael C. S.\n  Schouery", "title": "Approximation algorithms for the MAXSPACE advertisement problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the MAXSPACE problem, given a set of ads A, one wants to schedule a subset\nA' of A into K slots B_1, ..., B_K of size L. Each ad A_i in A has a size s_i\nand a frequency w_i. A schedule is feasible if the total size of ads in any\nslot is at most L, and each ad A_i in A' appears in exactly w_i slots. The goal\nis to find a feasible schedule that maximizes the sum of the space occupied by\nall slots. We introduce a generalization called MAXSPACE-R in which each ad A_i\nalso has a release date r_i >= 1, and may only appear in a slot B_j with j >=\nr_i. We also introduce a generalization of MAXSPACE-R called MAXSPACE-RD in\nwhich each ad A_i also has a deadline d_i <= K, and may only appear in a slot\nB_j with r_i <= j <= d_i. These parameters model situations where a subset of\nads corresponds to a commercial campaign with an announcement date that may\nexpire after some defined period. We present a 1/9-approximation algorithm for\nMAXSPACE-R and a polynomial-time approximation scheme for MAXSPACE-RD when K is\nbounded by a constant. This is the best factor one can expect, since MAXSPACE\nis NP-hard, even if K = 2.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 02:35:19 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 21:24:12 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["da Silva", "Mauro R. C.", ""], ["Pedrosa", "Lehilton L. C.", ""], ["Schouery", "Rafael C. S.", ""]]}, {"id": "2006.13483", "submitter": "Shweta Jain", "authors": "Shweta Jain, C. Seshadhri", "title": "Provably and Efficiently Approximating Near-cliques using the Tur\\'an\n  Shadow: PEANUTS", "comments": "The Web Conference, 2020 (WWW)", "journal-ref": null, "doi": "10.1145/3366423.3380264", "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clique and near-clique counts are important graph properties with\napplications in graph generation, graph modeling, graph analytics, community\ndetection among others. They are the archetypal examples of dense subgraphs.\nWhile there are several different definitions of near-cliques, most of them\nshare the attribute that they are cliques that are missing a small number of\nedges. Clique counting is itself considered a challenging problem. Counting\nnear-cliques is significantly harder more so since the search space for\nnear-cliques is orders of magnitude larger than that of cliques.\n  We give a formulation of a near-clique as a clique that is missing a constant\nnumber of edges. We exploit the fact that a near-clique contains a smaller\nclique, and use techniques for clique sampling to count near-cliques. This\nmethod allows us to count near-cliques with 1 or 2 missing edges, in graphs\nwith tens of millions of edges. To the best of our knowledge, there was no\nknown efficient method for this problem, and we obtain a 10x - 100x speedup\nover existing algorithms for counting near-cliques.\n  Our main technique is a space-efficient adaptation of the Tur\\'an Shadow\nsampling approach, recently introduced by Jain and Seshadhri (WWW 2017). This\napproach constructs a large recursion tree (called the Tur\\'an Shadow) that\nrepresents cliques in a graph. We design a novel algorithm that builds an\nestimator for near-cliques, using an online, compact construction of the\nTur\\'an Shadow.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 04:59:47 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Jain", "Shweta", ""], ["Seshadhri", "C.", ""]]}, {"id": "2006.13642", "submitter": "Yuko Kuroki", "authors": "Yuko Kuroki, Atsushi Miyauchi, Junya Honda, Masashi Sugiyama", "title": "Online Dense Subgraph Discovery via Blurred-Graph Feedback", "comments": "ICML2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense subgraph discovery aims to find a dense component in edge-weighted\ngraphs. This is a fundamental graph-mining task with a variety of applications\nand thus has received much attention recently. Although most existing methods\nassume that each individual edge weight is easily obtained, such an assumption\nis not necessarily valid in practice. In this paper, we introduce a novel\nlearning problem for dense subgraph discovery in which a learner queries edge\nsubsets rather than only single edges and observes a noisy sum of edge weights\nin a queried subset. For this problem, we first propose a polynomial-time\nalgorithm that obtains a nearly-optimal solution with high probability.\nMoreover, to deal with large-sized graphs, we design a more scalable algorithm\nwith a theoretical guarantee. Computational experiments using real-world graphs\ndemonstrate the effectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 11:37:33 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Kuroki", "Yuko", ""], ["Miyauchi", "Atsushi", ""], ["Honda", "Junya", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2006.13673", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Shay Golan, Tomasz Kociumaka, Tsvi Kopelowitz, Ely Porat,\n  Przemys{\\l}aw Uzna\\'nski", "title": "Improved Circular $k$-Mismatch Sketches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shift distance $\\mathsf{sh}(S_1,S_2)$ between two strings $S_1$ and $S_2$\nof the same length is defined as the minimum Hamming distance between $S_1$ and\nany rotation (cyclic shift) of $S_2$. We study the problem of sketching the\nshift distance, which is the following communication complexity problem:\nStrings $S_1$ and $S_2$ of length $n$ are given to two identical players\n(encoders), who independently compute sketches (summaries) $\\mathtt{sk}(S_1)$\nand $\\mathtt{sk}(S_2)$, respectively, so that upon receiving the two sketches,\na third player (decoder) is able to compute (or approximate)\n$\\mathsf{sh}(S_1,S_2)$ with high probability.\n  This paper primarily focuses on the more general $k$-mismatch version of the\nproblem, where the decoder is allowed to declare a failure if\n$\\mathsf{sh}(S_1,S_2)>k$, where $k$ is a parameter known to all parties. Andoni\net al. (STOC'13) introduced exact circular $k$-mismatch sketches of size\n$\\widetilde{O}(k+D(n))$, where $D(n)$ is the number of divisors of $n$. Andoni\net al. also showed that their sketch size is optimal in the class of linear\nhomomorphic sketches.\n  We circumvent this lower bound by designing a (non-linear) exact circular\n$k$-mismatch sketch of size $\\widetilde{O}(k)$; this size matches\ncommunication-complexity lower bounds. We also design $(1\\pm\n\\varepsilon)$-approximate circular $k$-mismatch sketch of size\n$\\widetilde{O}(\\min(\\varepsilon^{-2}\\sqrt{k}, \\varepsilon^{-1.5}\\sqrt{n}))$,\nwhich improves upon an $\\widetilde{O}(\\varepsilon^{-2}\\sqrt{n})$-size sketch of\nCrouch and McGregor (APPROX'11).\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 12:44:22 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Golan", "Shay", ""], ["Kociumaka", "Tomasz", ""], ["Kopelowitz", "Tsvi", ""], ["Porat", "Ely", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "2006.13679", "submitter": "Eugenio Angriman", "authors": "Eugenio Angriman, Maria Predari, Alexander van der Grinten, Henning\n  Meyerhenke", "title": "Approximation of the Diagonal of a Laplacian's Pseudoinverse for Complex\n  Network Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of massive graph data sets in numerous applications requires\nfast algorithms for extracting knowledge from these data. We are motivated here\nby three electrical measures for the analysis of large small-world graphs $G =\n(V, E)$ -- i.e., graphs with diameter in $O(\\log |V|)$, which are abundant in\ncomplex network analysis. From a computational point of view, the three\nmeasures have in common that their crucial component is the diagonal of the\ngraph Laplacian's pseudoinverse, $L^\\dagger$. Computing diag$(L^\\dagger)$\nexactly by pseudoinversion, however, is as expensive as dense matrix\nmultiplication -- and the standard tools in practice even require cubic time.\nMoreover, the pseudoinverse requires quadratic space -- hardly feasible for\nlarge graphs. Resorting to approximation by, e.g., using the\nJohnson-Lindenstrauss transform, requires the solution of $O(\\log |V| /\n\\epsilon^2)$ Laplacian linear systems to guarantee a relative error, which is\nstill very expensive for large inputs.\n  In this paper, we present a novel approximation algorithm that requires the\nsolution of only one Laplacian linear system. The remaining parts are purely\ncombinatorial -- mainly sampling uniform spanning trees, which we relate to\ndiag$(L^\\dagger)$ via effective resistances. For small-world networks, our\nalgorithm obtains a $\\pm \\epsilon$-approximation with high probability, in a\ntime that is nearly-linear in $|E|$ and quadratic in $1 / \\epsilon$. Another\npositive aspect of our algorithm is its parallel nature due to independent\nsampling. We thus provide two parallel implementations of our algorithm: one\nusing OpenMP, one MPI + OpenMP. In our experiments against the state of the\nart, our algorithm (i) yields more accurate results, (ii) is much faster and\nmore memory-efficient, and (iii) obtains good parallel speedups, in particular\nin the distributed setting.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 12:50:08 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 08:58:44 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Angriman", "Eugenio", ""], ["Predari", "Maria", ""], ["van der Grinten", "Alexander", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "2006.13684", "submitter": "Petr Golovach", "authors": "Fedor V. Fomin and Petr A. Golovach", "title": "Kernelization of Whitney Switches", "comments": "To appear at ESA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental theorem of Whitney from 1933 asserts that 2-connected graphs G\nand H are 2-isomorphic, or equivalently, their cycle matroids are isomorphic,\nif and only if G can be transformed into H by a series of operations called\nWhitney switches. In this paper we consider the quantitative question arising\nfrom Whitney's theorem: Given two 2-isomorphic graphs, can we transform one\ninto another by applying at most k Whitney switches? This problem is already\nNP-complete for cycles, and we investigate its parameterized complexity. We\nshow that the problem admits a kernel of size O(k), and thus, is\nfixed-parameter tractable when parameterized by k.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 12:52:22 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""]]}, {"id": "2006.13738", "submitter": "Francesco Gullo", "authors": "Ilaria Bordino, Francesco Gullo, Giacomo Legnaro", "title": "The Power of Connection: Leveraging Network Analysis to Advance\n  Receivable Financing", "comments": "IEEE TNSE 2020; Bordino and Gullo \"Network-based Receivable\n  Financing\" Proc. of ACM CIKM 2018 Conf", "journal-ref": null, "doi": "10.1145/3269206.3272017", "report-no": null, "categories": "cs.DS cs.CE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Receivable financing is the process whereby cash is advanced to firms against\nreceivables their customers have yet to pay: a receivable can be sold to a\nfunder, which immediately gives the firm cash in return for a small percentage\nof the receivable amount as a fee. Receivable financing has been traditionally\nhandled in a centralized way, where every request is processed by the funder\nindividually and independently of one another. In this work we propose a novel,\nnetwork-based approach to receivable financing, which enables customers of the\nsame funder to autonomously pay each other as much as possible, and gives\nbenefits to both the funder (reduced cash anticipation and exposure risk) and\nits customers (smaller fees and lightweight service establishment). Our main\ncontributions consist in providing a principled formulation of the\nnetwork-based receivable-settlement strategy, and showing how to achieve all\nalgorithmic challenges posed by the design of this strategy. We formulate\nnetwork-based receivable financing as a novel combinatorial-optimization\nproblem on a multigraph of receivables. We show that the problem is NP-hard,\nand devise an exact branch-and-bound algorithm, as well as algorithms to\nefficiently find effective approximate solutions. Our more efficient algorithms\nare based on cycle enumeration and selection, and exploit a theoretical\ncharacterization in terms of a knapsack problem, as well as a refining strategy\nthat properly adds paths between cycles. We also investigate the real-world\nissue of avoiding temporary violations of the problem constraints, and design\nmethods for handling it. An extensive experimental evaluation is performed on\nreal receivable data. Results attest the good performance of our methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 13:52:14 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Bordino", "Ilaria", ""], ["Gullo", "Francesco", ""], ["Legnaro", "Giacomo", ""]]}, {"id": "2006.13754", "submitter": "Mehrdad Ghadiri", "authors": "Mehrdad Ghadiri, Richard Santiago, Bruce Shepherd", "title": "A Parameterized Family of Meta-Submodular Functions", "comments": "arXiv admin note: text overlap with arXiv:1904.09216", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular function maximization has found a wealth of new applications in\nmachine learning models during the past years. The related supermodular\nmaximization models (submodular minimization) also offer an abundance of\napplications, but they appeared to be highly intractable even under simple\ncardinality constraints. Hence, while there are well-developed tools for\nmaximizing a submodular function subject to a matroid constraint, there is much\nless work on the corresponding supermodular maximization problems.\n  We give a broad parameterized family of monotone functions which includes\nsubmodular functions and a class of supermodular functions containing diversity\nfunctions. Functions in this parameterized family are called\n\\emph{$\\gamma$-meta-submodular}. We develop local search algorithms with\napproximation factors that depend only on the parameter $\\gamma$. We show that\nthe $\\gamma$-meta-submodular families include well-known classes of functions\nsuch as meta-submodular functions ($\\gamma=0$), metric diversity functions and\nproportionally submodular functions (both with $\\gamma=1$), diversity functions\nbased on negative-type distances or Jensen-Shannon divergence (both with\n$\\gamma=2$), and $\\sigma$-semi metric diversity functions ($\\gamma = \\sigma$).\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 16:45:12 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Ghadiri", "Mehrdad", ""], ["Santiago", "Richard", ""], ["Shepherd", "Bruce", ""]]}, {"id": "2006.13911", "submitter": "Frank Gurski", "authors": "Frank Gurski and Dominique Komander and Carolin Rehs", "title": "Acyclic coloring of special digraphs", "comments": "17 pages; 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An acyclic r-coloring of a directed graph G=(V,E) is a partition of the\nvertex set V into r acyclic sets. The dichromatic number of a directed graph G\nis the smallest r such that G allows an acyclic r-coloring. For symmetric\ndigraphs the dichromatic number equals the well-known chromatic number of the\nunderlying undirected graph. This allows us to carry over the W[1]-hardness and\nlower bounds for running times of the chromatic number problem parameterized by\nclique-width to the dichromatic number problem parameterized by directed\nclique-width. We introduce the first polynomial-time algorithm for the acyclic\ncoloring problem on digraphs of constant directed clique-width. From a\nparameterized point of view our algorithm shows that the Dichromatic Number\nproblem is in XP when parameterized by directed clique-width and extends the\nonly known structural parameterization by directed modular width for this\nproblem. Furthermore, we apply defineability within monadic second order logic\nin order to show that Dichromatic Number problem is in FPT when parameterized\nby the directed clique-width and r.\n  For directed co-graphs, which is a class of digraphs of directed clique-width\n2, and several generalizations we even show linear time solutions for computing\nthe dichromatic number. Furthermore, we conclude that directed co-graphs and\nthe considered generalizations lead to subclasses of perfect digraphs. For\ndirected cactus forests, which is a set of digraphs of directed tree-width 1,\nwe conclude an upper bound of 2 for the dichromatic number and we show that an\noptimal acyclic coloring can be computed in linear time.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 17:45:22 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 08:20:57 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 18:08:13 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Gurski", "Frank", ""], ["Komander", "Dominique", ""], ["Rehs", "Carolin", ""]]}, {"id": "2006.14009", "submitter": "Yang P. Liu", "authors": "Ryan Alweiss, Yang P. Liu, Mehtaab Sawhney", "title": "Discrepancy Minimization via a Self-Balancing Walk", "comments": "v2, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study discrepancy minimization for vectors in $\\mathbb{R}^n$ under various\nsettings. The main result is the analysis of a new simple random process in\nmultiple dimensions through a comparison argument. As corollaries, we obtain\nbounds which are tight up to logarithmic factors for several problems in online\nvector balancing posed by Bansal, Jiang, Singla, and Sinha (STOC 2020), as well\nas linear time algorithms for logarithmic bounds for the Koml\\'{o}s conjecture.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 19:25:47 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 17:10:15 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Alweiss", "Ryan", ""], ["Liu", "Yang P.", ""], ["Sawhney", "Mehtaab", ""]]}, {"id": "2006.14015", "submitter": "Hanlin Zhu", "authors": "Cyrus Rashtchian, David P. Woodruff and Hanlin Zhu", "title": "Vector-Matrix-Vector Queries for Solving Linear Algebra, Statistics, and\n  Graph Problems", "comments": "26 pages, to be published in RANDOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the general problem of learning about a matrix through\nvector-matrix-vector queries. These queries provide the value of\n$\\boldsymbol{u}^{\\mathrm{T}}\\boldsymbol{M}\\boldsymbol{v}$ over a fixed field\n$\\mathbb{F}$ for a specified pair of vectors $\\boldsymbol{u},\\boldsymbol{v} \\in\n\\mathbb{F}^n$. To motivate these queries, we observe that they generalize many\npreviously studied models, such as independent set queries, cut queries, and\nstandard graph queries. They also specialize the recently studied matrix-vector\nquery model. Our work is exploratory and broad, and we provide new upper and\nlower bounds for a wide variety of problems, spanning linear algebra,\nstatistics, and graphs. Many of our results are nearly tight, and we use\ndiverse techniques from linear algebra, randomized algorithms, and\ncommunication complexity.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 19:33:49 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Rashtchian", "Cyrus", ""], ["Woodruff", "David P.", ""], ["Zhu", "Hanlin", ""]]}, {"id": "2006.14029", "submitter": "Lu\\'is M. S. Russo", "authors": "Lu\\'is M. S. Russo, Alexandre P. Francisco", "title": "Small Longest Tandem Scattered Subsequences", "comments": "The work reported in this article was supported by national funds\n  through Funda\\c{c}\\~ao para a Ci\\^encia e Tecnologia (FCT) with reference\n  UIDB/50021/2020 and through project NGPHYLO PTDC/CCI-BIO/29676/2017. Funded\n  in part by European Union's Horizon 2020 research and innovation programme\n  under the Marie Sk{\\l}odowska-Curie Actions grant agreement No 690941", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying tandem scattered subsequences within a\nstring. Our algorithm identifies a longest subsequence which occurs twice\nwithout overlap in a string. This algorithm is based on the Hunt-Szymanski\nalgorithm, therefore its performance improves if the string is not self\nsimilar. This occurs naturally on strings over large alphabets. Our algorithm\nrelies on new results for data structures that support dynamic longest\nincreasing sub-sequences. In the process we also obtain improved algorithms for\nthe decremental string comparison problem.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 20:26:33 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Russo", "Lu\u00eds M. S.", ""], ["Francisco", "Alexandre P.", ""]]}, {"id": "2006.14093", "submitter": "Yiming Zhao", "authors": "Haitao Wang, Yiming Zhao", "title": "A Linear-Time Algorithm for Discrete Radius Optimally Augmenting Paths\n  in a Metric Space", "comments": "A preliminary version to appear in CCCG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $P$ be a path graph of $n$ vertices embedded in a metric space. We\nconsider the problem of adding a new edge to $P$ so that the radius of the\nresulting graph is minimized, where any center is constrained to be one of the\nvertices of $P$. Previously, the \"continuous\" version of the problem where a\ncenter may be a point in the interior of an edge of the graph was studied and a\nlinear-time algorithm was known. Our \"discrete\" version of the problem has not\nbeen studied before. We present a linear-time algorithm for the problem.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 22:57:20 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Wang", "Haitao", ""], ["Zhao", "Yiming", ""]]}, {"id": "2006.14309", "submitter": "Paul Ouvrard", "authors": "Nicolas Bousquet, Takehiro Ito, Yusuke Kobayashi, Haruka Mizuta, Paul\n  Ouvrard, Akira Suzuki, Kunihiro Wasa", "title": "Reconfiguration of Spanning Trees with Many or Few Leaves", "comments": "32 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a graph and $T_1,T_2$ be two spanning trees of $G$. We say that\n$T_1$ can be transformed into $T_2$ via an edge flip if there exist two edges\n$e \\in T_1$ and $f$ in $T_2$ such that $T_2= (T_1 \\setminus e) \\cup f$. Since\nspanning trees form a matroid, one can indeed transform a spanning tree into\nany other via a sequence of edge flips, as observed by Ito et al.\n  We investigate the problem of determining, given two spanning trees $T_1,T_2$\nwith an additional property $\\Pi$, if there exists an edge flip transformation\nfrom $T_1$ to $T_2$ keeping property $\\Pi$ all along.\n  First we show that determining if there exists a transformation from $T_1$ to\n$T_2$ such that all the trees of the sequence have at most $k$ (for any fixed\n$k \\ge 3$) leaves is PSPACE-complete.\n  We then prove that determining if there exists a transformation from $T_1$ to\n$T_2$ such that all the trees of the sequence have at least $k$ leaves (where\n$k$ is part of the input) is PSPACE-complete even restricted to split,\nbipartite or planar graphs. We complete this result by showing that the problem\nbecomes polynomial for cographs, interval graphs and when $k=n-2$.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 11:10:33 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Bousquet", "Nicolas", ""], ["Ito", "Takehiro", ""], ["Kobayashi", "Yusuke", ""], ["Mizuta", "Haruka", ""], ["Ouvrard", "Paul", ""], ["Suzuki", "Akira", ""], ["Wasa", "Kunihiro", ""]]}, {"id": "2006.14312", "submitter": "Richard Santiago", "authors": "Richard Santiago", "title": "New Approximations and Hardness Results for Submodular Partitioning\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following class of submodular k-multiway partitioning\nproblems: (Sub-$k$-MP) $\\min \\sum_{i=1}^k f(S_i): S_1 \\uplus S_2 \\uplus \\cdots\n\\uplus S_k = V \\mbox{ and } S_i \\neq \\emptyset \\mbox{ for all }i\\in [k]$. Here\n$f$ is a non-negative submodular function, and $\\uplus$ denotes the union of\ndisjoint sets. Hence the goal is to partition $V$ into $k$ non-empty sets\n$S_1,S_2,\\ldots,S_k$ such that $\\sum_{i=1}^k f(S_i)$ is minimized. These\nproblems were introduced by Zhao et al. partly motivated by applications to\nnetwork reliability analysis, VLSI design, hypergraph cut, and other\npartitioning problems.\n  In this work we revisit this class of problems and shed some light onto their\nhardness of approximation in the value oracle model. We provide new\nunconditional hardness results for Sub-$k$-MP in the special settings where the\nfunction $f$ is either monotone or symmetric. For symmetric functions we show\nthat given any $\\epsilon > 0$, any algorithm achieving a $(2 -\n\\epsilon)$-approximation requires exponentially many queries in the value\noracle model. For monotone objectives we show that given any $\\epsilon > 0$,\nany algorithm achieving a $(4/3 - \\epsilon)$-approximation requires\nexponentially many queries in the value oracle model.\n  We then extend Sub-$k$-MP to a larger class of partitioning problems, where\nthe functions $f_i(S_i)$ can be different, and there is a more general\npartitioning constraint $ S_1 \\uplus S_2 \\uplus \\cdots \\uplus S_k \\in\n\\mathcal{F}$ for some family $\\mathcal{F} \\subseteq 2^V$ of feasible sets. We\nprovide a black box reduction that allows us to leverage several existing\nresults from the literature; leading to new approximations for this class of\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 11:20:18 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 11:10:19 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 11:31:07 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Santiago", "Richard", ""]]}, {"id": "2006.14403", "submitter": "Shichuan Deng", "authors": "Shichuan Deng, Jian Li, Yuval Rabani", "title": "Approximation Algorithms for Clustering with Dynamic Points", "comments": "To be published in the Proceedings of the 28th Annual European\n  Symposium on Algorithms (ESA 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many classic clustering problems, we seek to sketch a massive data set of\n$n$ points in a metric space, by segmenting them into $k$ categories or\nclusters, each cluster represented concisely by a single point in the metric\nspace. Two notable examples are the $k$-center/$k$-supplier problem and the\n$k$-median problem. In practical applications of clustering, the data set may\nevolve over time, reflecting an evolution of the underlying clustering model.\nIn this paper, we initiate the study of a dynamic version of clustering\nproblems that aims to capture these considerations. In this version there are\n$T$ time steps, and in each time step $t\\in\\{1,2,\\dots,T\\}$, the set of clients\nneeded to be clustered may change, and we can move the $k$ facilities between\ntime steps. More specifically, we study two concrete problems in this\nframework: the Dynamic Ordered $k$-Median and the Dynamic $k$-Supplier problem.\nWe first consider the Dynamic Ordered $k$-Median problem, where the objective\nis to minimize the weighted sum of ordered distances over all time steps, plus\nthe total cost of moving the facilities between time steps. We present one\nconstant-factor approximation algorithm for $T=2$ and another approximation\nalgorithm for fixed $T \\geq 3$. Then we consider the Dynamic $k$-Supplier\nproblem, where the objective is to minimize the maximum distance from any\nclient to its facility, subject to the constraint that between time steps the\nmaximum distance moved by any facility is no more than a given threshold. When\nthe number of time steps $T$ is 2, we present a simple constant factor\napproximation algorithm and a bi-criteria constant factor approximation\nalgorithm for the outlier version, where some of the clients can be discarded.\nWe also show that it is NP-hard to approximate the problem with any factor for\n$T \\geq 3$.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 13:41:52 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 15:28:57 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Deng", "Shichuan", ""], ["Li", "Jian", ""], ["Rabani", "Yuval", ""]]}, {"id": "2006.14449", "submitter": "Bogdan Manghiuc", "authors": "Bogdan-Adrian Manghiuc, Pan Peng, He Sun", "title": "Augmenting the Algebraic Connectivity of Graphs", "comments": "57 pages, 1 figure, a preliminary version appeared in ESA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any undirected graph $G=(V,E)$ and a set $E_W$ of candidate edges with\n$E\\cap E_W=\\emptyset$, the $(k,\\gamma)$-spectral augmentability problem is to\nfind a set $F$ of $k$ edges from $E_W$ with appropriate weighting, such that\nthe algebraic connectivity of the resulting graph $H=(V,E\\cup F)$ is least\n$\\gamma$. Because of a tight connection between the algebraic connectivity and\nmany other graph parameters, including the graph's conductance and the mixing\ntime of random walks in a graph, maximising the resulting graph's algebraic\nconnectivity by adding a small number of edges has been studied over the past\n15 years.\n  In this work we present an approximate and efficient algorithm for the\n$(k,\\gamma)$-spectral augmentability problem, and our algorithm runs in\nalmost-linear time under a wide regime of parameters. Our main algorithm is\nbased on the following two novel techniques developed in the paper, which might\nhave applications beyond the $(k,\\gamma)$-spectral augmentability problem.\n  (1) We present a fast algorithm for solving a feasibility version of an SDP\nfor the algebraic connectivity maximisation problem from [GB06]. Our algorithm\nis based on the classic primal-dual framework for solving SDP, which in turn\nuses the multiplicative weight update algorithm. We present a novel approach of\nunifying SDP constraints of different matrix and vector variables and give a\ngood separation oracle accordingly.\n  (2) We present an efficient algorithm for the subgraph sparsification\nproblem, and for a wide range of parameters our algorithm runs in almost-linear\ntime, in contrast to the previously best known algorithm running in at least\n$\\Omega(n^2mk)$ time [KMST10]. Our analysis shows how the randomised BSS\nframework can be generalised in the setting of subgraph sparsification, and how\nthe potential functions can be applied to approximately keep track of different\nsubspaces.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 14:41:48 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Manghiuc", "Bogdan-Adrian", ""], ["Peng", "Pan", ""], ["Sun", "He", ""]]}, {"id": "2006.14552", "submitter": "Giulio Ermanno Pibiri", "authors": "Giulio Ermanno Pibiri and Rossano Venturini", "title": "Practical Trade-Offs for the Prefix-Sum Problem", "comments": "Accepted by \"Software: Practice and Experience\", 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an integer array A, the prefix-sum problem is to answer sum(i) queries\nthat return the sum of the elements in A[0..i], knowing that the integers in A\ncan be changed. It is a classic problem in data structure design with a wide\nrange of applications in computing from coding to databases. In this work, we\npropose and compare several and practical solutions to this problem, showing\nthat new trade-offs between the performance of queries and updates can be\nachieved on modern hardware.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 16:52:10 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 08:18:03 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 19:54:27 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Pibiri", "Giulio Ermanno", ""], ["Venturini", "Rossano", ""]]}, {"id": "2006.14571", "submitter": "Kyriakos Axiotis", "authors": "Kyriakos Axiotis and Maxim Sviridenko", "title": "Sparse Convex Optimization via Adaptively Regularized Hard Thresholding", "comments": "Accepted to ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of Sparse Convex Optimization is to optimize a convex function $f$\nunder a sparsity constraint $s\\leq s^*\\gamma$, where $s^*$ is the target number\nof non-zero entries in a feasible solution (sparsity) and $\\gamma\\geq 1$ is an\napproximation factor. There has been a lot of work to analyze the sparsity\nguarantees of various algorithms (LASSO, Orthogonal Matching Pursuit (OMP),\nIterative Hard Thresholding (IHT)) in terms of the Restricted Condition Number\n$\\kappa$. The best known algorithms guarantee to find an approximate solution\nof value $f(x^*)+\\epsilon$ with the sparsity bound of $\\gamma =\nO\\left(\\kappa\\min\\left\\{\\log \\frac{f(x^0)-f(x^*)}{\\epsilon},\n\\kappa\\right\\}\\right)$, where $x^*$ is the target solution. We present a new\nAdaptively Regularized Hard Thresholding (ARHT) algorithm that makes\nsignificant progress on this problem by bringing the bound down to\n$\\gamma=O(\\kappa)$, which has been shown to be tight for a general class of\nalgorithms including LASSO, OMP, and IHT. This is achieved without significant\nsacrifice in the runtime efficiency compared to the fastest known algorithms.\nWe also provide a new analysis of OMP with Replacement (OMPR) for general $f$,\nunder the condition $s > s^* \\frac{\\kappa^2}{4}$, which yields Compressed\nSensing bounds under the Restricted Isometry Property (RIP). When compared to\nother Compressed Sensing approaches, it has the advantage of providing a strong\ntradeoff between the RIP condition and the solution sparsity, while working for\nany general function $f$ that meets the RIP condition.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 17:16:21 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Axiotis", "Kyriakos", ""], ["Sviridenko", "Maxim", ""]]}, {"id": "2006.14652", "submitter": "Ojas Parekh", "authors": "Ojas Parekh, Cynthia A. Phillips, Conrad D. James, James B. Aimone", "title": "Constant-Depth and Subcubic-Size Threshold Circuits for Matrix\n  Multiplication", "comments": "Appears in the proceedings of the ACM Symposium on Parallelism in\n  Algorithms and Architectures (SPAA), 2018", "journal-ref": null, "doi": "10.1145/3210377.3210410", "report-no": null, "categories": "cs.DS cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean circuits of McCulloch-Pitts threshold gates are a classic model of\nneural computation studied heavily in the late 20th century as a model of\ngeneral computation. Recent advances in large-scale neural computing hardware\nhas made their practical implementation a near-term possibility. We describe a\ntheoretical approach for multiplying two $N$ by $N$ matrices that integrates\nthreshold gate logic with conventional fast matrix multiplication algorithms,\nthat perform $O(N^\\omega)$ arithmetic operations for a positive constant\n$\\omega < 3$. Our approach converts such a fast matrix multiplication algorithm\ninto a constant-depth threshold circuit with approximately $O(N^\\omega)$ gates.\nPrior to our work, it was not known whether the $\\Theta(N^3)$-gate barrier for\nmatrix multiplication was surmountable by constant-depth threshold circuits.\n  Dense matrix multiplication is a core operation in convolutional neural\nnetwork training. Performing this work on a neural architecture instead of\noff-loading it to a GPU may be an appealing option.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 18:28:10 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Parekh", "Ojas", ""], ["Phillips", "Cynthia A.", ""], ["James", "Conrad D.", ""], ["Aimone", "James B.", ""]]}, {"id": "2006.14733", "submitter": "Debajyoti Mondal", "authors": "Debajyoti Mondal, N. Parthiban, V. Kavitha, Indra Rajasingh", "title": "APX-Hardness and Approximation for the k-Burning Number Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an information diffusion process on a graph $G$ that starts with\n$k>0$ burnt vertices, and at each subsequent step, burns the neighbors of the\ncurrently burnt vertices, as well as $k$ other unburnt vertices. The\n\\emph{$k$-burning number} of $G$ is the minimum number of steps $b_k(G)$ such\nthat all the vertices can be burned within $b_k(G)$ steps. Note that the last\nstep may have smaller than $k$ unburnt vertices available, where all of them\nare burned. The $1$-burning number coincides with the well-known burning number\nproblem, which was proposed to model the spread of social contagion. The\ngeneralization to $k$-burning number allows us to examine different worst-case\ncontagion scenarios by varying the spread factor $k$.\n  In this paper we prove that computing $k$-burning number is APX-hard, for any\nfixed constant $k$. We then give an $O((n+m)\\log n)$-time 3-approximation\nalgorithm for computing $k$-burning number, for any $k\\ge 1$, where $n$ and $m$\nare the number of vertices and edges, respectively. Finally, we show that even\nif the burning sources are given as an input, computing a burning sequence\nitself is an NP-hard problem.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 23:41:13 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 05:07:11 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Mondal", "Debajyoti", ""], ["Parthiban", "N.", ""], ["Kavitha", "V.", ""], ["Rajasingh", "Indra", ""]]}, {"id": "2006.14828", "submitter": "Guus Regts", "authors": "Pjotr Buys, Andreas Galanis, Viresh Patel, Guus Regts", "title": "Lee-Yang zeros and the complexity of the ferromagnetic Ising model on\n  bounded-degree graphs", "comments": "40 pages, 1 figure. We have included a new result for the case $b\\in\n  [1-2/\\Delta,1)$. This essentially gives a complete picture of the complexity\n  of the problem. An extended abstract has been presented at SODA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of approximating the partition function\nof the ferromagnetic Ising model with the external field parameter $\\lambda$ on\nthe unit circle in the complex plane. Complex-valued parameters for the Ising\nmodel are relevant for quantum circuit computations and phase transitions in\nstatistical physics, but have also been key in the recent deterministic\napproximation scheme for all $|\\lambda|\\neq 1$ by Liu, Sinclair, and\nSrivastava. Here, we focus on the unresolved complexity picture on the unit\ncircle, and on the tantalising question of what happens around $\\lambda=1$,\nwhere on one hand the classical algorithm of Jerrum and Sinclair gives a\nrandomised approximation scheme on the real axis suggesting tractability, and\non the other hand the presence of Lee-Yang zeros alludes to computational\nhardness.\n  Our main result establishes a sharp computational transition at the point\n$\\lambda=1$, and more generally on the entire unit circle. For an integer\n$\\Delta\\geq 3$ and edge interaction parameter $b\\in (0,1)$ we show #P-hardness\nfor approximating the partition function on graphs of maximum degree $\\Delta$\non the arc of the unit circle where the Lee-Yang zeros are dense. This result\ncontrasts with known approximation algorithms when $|\\lambda|\\neq 1$ or when\n$\\lambda$ is in the complementary arc around $1$ of the unit circle. Our work\nthus gives a direct connection between the presence/absence of Lee-Yang zeros\nand the tractability of efficiently approximating the partition function on\nbounded-degree graphs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 07:09:04 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 13:34:10 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 12:57:14 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Buys", "Pjotr", ""], ["Galanis", "Andreas", ""], ["Patel", "Viresh", ""], ["Regts", "Guus", ""]]}, {"id": "2006.14972", "submitter": "Andr\\'e Nichterlein", "authors": "Aleksander Figiel, Anne-Sophie Himmel, Andr\\'e Nichterlein, Rolf\n  Niedermeier", "title": "On 2-Clubs in Graph-Based Data Clustering: Theory and Algorithm\n  Engineering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Editing a graph into a disjoint union of clusters is a standard optimization\ntask in graph-based data clustering. Here, complementing classic work where the\nclusters shall be cliques, we focus on clusters that shall be 2-clubs, that is,\nsubgraphs of diameter two. This naturally leads to the two NP-hard problems\n2-Club Cluster Editing (the allowed editing operations are edge insertion and\nedge deletion) and 2-Club Cluster Vertex Deletion (the allowed editing\noperations are vertex deletions). Answering an open question from the\nliterature, we show that 2-Club Cluster Editing is W[2]-hard with respect to\nthe number of edge modifications, thus contrasting the fixed-parameter\ntractability result for the classic Cluster Editing problem (considering\ncliques instead of 2-clubs). Then focusing on 2-Club Cluster Vertex Deletion,\nwhich is easily seen to be fixed-parameter tractable, we show that under\nstandard complexity-theoretic assumptions it does not have a polynomial-size\nproblem kernel when parameterized by the number of vertex deletions.\nNevertheless, we develop several effective data reduction and pruning rules,\nresulting in a competitive solver, clearly outperforming a standard CPLEX\nsolver in most instances of an established biological test data set.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 13:16:46 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Figiel", "Aleksander", ""], ["Himmel", "Anne-Sophie", ""], ["Nichterlein", "Andr\u00e9", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "2006.15024", "submitter": "Shahbaz Khan", "authors": "Massimo Cairo, Shahbaz Khan, Romeo Rizzi, Sebastian Schmidt, Alexandru\n  I. Tomescu and Elia Zirondelli", "title": "Computing all $s$-$t$ bridges and articulation points simplified", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a directed graph $G$ and a pair of nodes $s$ and $t$, an $s$-$t$ bridge\nof $G$ is an edge whose removal breaks all $s$-$t$ paths of $G$. Similarly, an\n$s$-$t$ articulation point of $G$ is a node whose removal breaks all $s$-$t$\npaths of $G$. Computing the sequence of all $s$-$t$ bridges of $G$ (as well as\nthe $s$-$t$ articulation points) is a basic graph problem, solvable in linear\ntime using the classical min-cut algorithm.\n  When dealing with cuts of unit size ($s$-$t$ bridges) this algorithm can be\nsimplified to a single graph traversal from $s$ to $t$ avoiding an arbitrary\n$s$-$t$ path, which is interrupted at the $s$-$t$ bridges. Further, the\ncorresponding proof is also simplified making it independent of the theory of\nnetwork flows.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 14:47:13 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Cairo", "Massimo", ""], ["Khan", "Shahbaz", ""], ["Rizzi", "Romeo", ""], ["Schmidt", "Sebastian", ""], ["Tomescu", "Alexandru I.", ""], ["Zirondelli", "Elia", ""]]}, {"id": "2006.15089", "submitter": "Rathish Das", "authors": "Esther M. Arkin, Rathish Das, Jie Gao, Mayank Goswami, Joseph S. B.\n  Mitchell, Valentin Polishchuk, Csaba D. Toth", "title": "Cutting Polygons into Small Pieces with Chords: Laser-Based Localization", "comments": "This paper will appear in ESA2020, Track A proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by indoor localization by tripwire lasers, we study the problem of\ncutting a polygon into small-size pieces, using the chords of the polygon.\nSeveral versions are considered, depending on the definition of the \"size\" of a\npiece. In particular, we consider the area, the diameter, and the radius of the\nlargest inscribed circle as a measure of the size of a piece. We also consider\ndifferent objectives, either minimizing the maximum size of a piece for a given\nnumber of chords, or minimizing the number of chords that achieve a given size\nthreshold for the pieces. We give hardness results for polygons with holes and\napproximation algorithms for multiple variants of the problem.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 16:38:09 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Arkin", "Esther M.", ""], ["Das", "Rathish", ""], ["Gao", "Jie", ""], ["Goswami", "Mayank", ""], ["Mitchell", "Joseph S. B.", ""], ["Polishchuk", "Valentin", ""], ["Toth", "Csaba D.", ""]]}, {"id": "2006.15166", "submitter": "Abishek Sankararaman", "authors": "Abishek Sankararaman, Soumya Basu, Karthik Abinav Sankararaman", "title": "Dominate or Delete: Decentralized Competing Bandits in Serial\n  Dictatorship", "comments": "AISTATS, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning in a two-sided matching market, with demand side agents\ncontinuously competing to be matched with supply side (arms), abstracts the\ncomplex interactions under partial information on matching platforms (e.g.\nUpWork, TaskRabbit). We study the decentralized serial dictatorship setting, a\ntwo-sided matching market where the demand side agents have unknown and\nheterogeneous valuation over the supply side (arms), while the arms have known\nuniform preference over the demand side (agents). We design the first\ndecentralized algorithm -- UCB with Decentralized Dominant-arm Deletion\n(UCB-D3), for the agents, that does not require any knowledge of reward gaps or\ntime horizon. UCB-D3 works in phases, where in each phase, agents delete\n\\emph{dominated arms} -- the arms preferred by higher ranked agents, and play\nonly from the non-dominated arms according to the UCB. At the end of the phase,\nagents broadcast in a decentralized fashion, their estimated preferred arms\nthrough {\\em pure exploitation}. We prove both, a new regret lower bound for\nthe decentralized serial dictatorship model, and that UCB-D3 is order optimal.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 18:44:06 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 20:12:13 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Sankararaman", "Abishek", ""], ["Basu", "Soumya", ""], ["Sankararaman", "Karthik Abinav", ""]]}, {"id": "2006.15254", "submitter": "Aman Khalid", "authors": "Aman Khalid", "title": "Optimizing Cuckoo Filter for high burst tolerance,low latency, and high\n  throughput", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an implementation of a cuckoo filter for membership\ntesting, optimized for distributed data stores operating in high workloads. In\nlarge databases, querying becomes inefficient using traditional search methods.\nTo achieve optimal performance it is necessary to use probabilistic data\nstructures to test the membership of a given key, at the cost of getting false\npositives while querying data. The widely used bloom filters can be used for\nthis, but they have limitations like no support for deletes. To improve upon\nthis we use a modified version of the cuckoo filter that gives better amortized\ntimes for search, with less false positives.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 01:58:36 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Khalid", "Aman", ""]]}, {"id": "2006.15259", "submitter": "Ramtin Afshar", "authors": "Ramtin Afshar, Michael T. Goodrich, Pedro Matias, Martha C. Osegueda", "title": "Reconstructing Biological and Digital Phylogenetic Trees in Parallel", "comments": null, "journal-ref": "Leibniz International Proceedings in Informatics (LIPICS) 173\n  (2020) 3:1-3:24", "doi": "10.4230/LIPIcs.ESA.2020.3", "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the parallel query complexity of reconstructing\nbiological and digital phylogenetic trees from simple queries involving their\nnodes. This is motivated from computational biology, data protection, and\ncomputer security settings, which can be abstracted in terms of two parties, a\nresponder, Alice, who must correctly answer queries of a given type regarding a\ndegree-d tree, T, and a querier, Bob, who issues batches of queries, with each\nquery in a batch being independent of the others, so as to eventually infer the\nstructure of T. We show that a querier can efficiently reconstruct an n-node\ndegree-d tree, T, with a logarithmic number of rounds and quasilinear number of\nqueries, with high probability, for various types of queries, including\nrelative-distance queries and path queries. Our results are all asymptotically\noptimal and improve the asymptotic (sequential) query complexity for one of the\nproblems we study. Moreover, through an experimental analysis using both\nreal-world and synthetic data, we provide empirical evidence that our\nalgorithms provide significant parallel speedups while also improving the total\nquery complexities for the problems we study.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 02:28:44 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 20:40:23 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2020 17:58:57 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Afshar", "Ramtin", ""], ["Goodrich", "Michael T.", ""], ["Matias", "Pedro", ""], ["Osegueda", "Martha C.", ""]]}, {"id": "2006.15381", "submitter": "Sangram Kishor Jena Mr", "authors": "Sangram K. Jena, Ramesh K. Jallu, Gautam K. Das and Subhas C. Nandy", "title": "The Generalized Independent and Dominating Set Problems on Unit Disk\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study a generalized version of the maximum independent\nset and minimum dominating set problems, namely, the maximum $d$-distance\nindependent set problem and the minimum $d$-distance dominating set problem on\nunit disk graphs for a positive integer $d>0$. We first show that the maximum\n$d$-distance independent set problem and the minimum $d$-distance dominating\nset problem belongs to NP-hard class. Next, we propose a simple polynomial-time\nconstant-factor approximation algorithms and PTAS for both the problems.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 15:18:44 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Jena", "Sangram K.", ""], ["Jallu", "Ramesh K.", ""], ["Das", "Gautam K.", ""], ["Nandy", "Subhas C.", ""]]}, {"id": "2006.15412", "submitter": "Rishabh Iyer", "authors": "Rishabh Iyer and Ninad Khargonkar and Jeff Bilmes and Himanshu Asnani", "title": "Submodular Combinatorial Information Measures with Applications in\n  Machine Learning", "comments": "To Appear in the 32nd International Conference on Algorithmic\n  Learning Theory, ALT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information-theoretic quantities like entropy and mutual information have\nfound numerous uses in machine learning. It is well known that there is a\nstrong connection between these entropic quantities and submodularity since\nentropy over a set of random variables is submodular. In this paper, we study\ncombinatorial information measures that generalize independence, (conditional)\nentropy, (conditional) mutual information, and total correlation defined over\nsets of (not necessarily random) variables. These measures strictly generalize\nthe corresponding entropic measures since they are all parameterized via\nsubmodular functions that themselves strictly generalize entropy. Critically,\nwe show that, unlike entropic mutual information in general, the submodular\nmutual information is actually submodular in one argument, holding the other\nfixed, for a large class of submodular functions whose third-order partial\nderivatives satisfy a non-negativity property. This turns out to include a\nnumber of practically useful cases such as the facility location and set-cover\nfunctions. We study specific instantiations of the submodular information\nmeasures on these, as well as the probabilistic coverage, graph-cut, and\nsaturated coverage functions, and see that they all have mathematically\nintuitive and practically useful expressions. Regarding applications, we\nconnect the maximization of submodular (conditional) mutual information to\nproblems such as mutual-information-based, query-based, and privacy-preserving\nsummarization -- and we connect optimizing the multi-set submodular mutual\ninformation to clustering and robust partitioning.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 17:45:32 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 03:43:56 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 22:26:01 GMT"}, {"version": "v4", "created": "Tue, 29 Dec 2020 07:14:34 GMT"}, {"version": "v5", "created": "Fri, 1 Jan 2021 02:52:27 GMT"}, {"version": "v6", "created": "Tue, 2 Mar 2021 19:58:48 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Iyer", "Rishabh", ""], ["Khargonkar", "Ninad", ""], ["Bilmes", "Jeff", ""], ["Asnani", "Himanshu", ""]]}, {"id": "2006.15463", "submitter": "Michael Mitzenmacher", "authors": "Michael Mitzenmacher", "title": "Queues with Small Advice", "comments": "16 pages, draft version, to be submitted, subject to cahnge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent work on scheduling with predicted job sizes, we consider\nthe performance of scheduling algorithms with minimal advice, namely a single\nbit. Besides demonstrating the power of very limited advice, such schemes are\nquite natural. In the prediction setting, one bit of advice can be used to\nmodel a simple prediction as to whether a job is \"large\" or \"small\"; that is,\nwhether a job is above or below a given threshold. Further, one-bit advice\nschemes can correspond to mechanisms that tell whether to put a job at the\nfront or the back for the queue, a limitation which may be useful in many\nimplementation settings. Finally, queues with a single bit of advice have a\nsimple enough state that they can be analyzed in the limiting mean-field\nanalysis framework for the power of two choices. Our work follows in the path\nof recent work by showing that even small amounts of even possibly inaccurate\ninformation can greatly improve scheduling performance.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 22:44:52 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Mitzenmacher", "Michael", ""]]}, {"id": "2006.15512", "submitter": "Jeffrey M. Dudek", "authors": "Jeffrey M. Dudek and Moshe Y. Vardi", "title": "Parallel Weighted Model Counting with Tensor Networks", "comments": "Published at MCW-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A promising new algebraic approach to weighted model counting makes use of\ntensor networks, following a reduction from weighted model counting to\ntensor-network contraction. Prior work has focused on analyzing the single-core\nperformance of this approach, and demonstrated that it is an effective addition\nto the current portfolio of weighted-model-counting algorithms.\n  In this work, we explore the impact of multi-core and GPU use on\ntensor-network contraction for weighted model counting. To leverage multiple\ncores, we implement a parallel portfolio of tree-decomposition solvers to find\nan order to contract tensors. To leverage a GPU, we use TensorFlow to perform\nthe contractions. We compare the resulting weighted model counter on 1914\nstandard weighted model counting benchmarks and show that it significantly\nimproves the virtual best solver.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 05:09:18 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 18:59:25 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Dudek", "Jeffrey M.", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "2006.15575", "submitter": "Philip Bille", "authors": "Philip Bille and Inge Li G{\\o}rtz", "title": "Random Access in Persistent Strings and Segment Selection", "comments": "Extended abstract at ISAAC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider compact representations of collections of similar strings that\nsupport random access queries. The collection of strings is given by a rooted\ntree where edges are labeled by an edit operation (inserting, deleting, or\nreplacing a character) and a node represents the string obtained by applying\nthe sequence of edit operations on the path from the root to the node. The goal\nis to compactly represent the entire collection while supporting fast random\naccess to any part of a string in the collection. This problem captures natural\nscenarios such as representing the past history of an edited document or\nrepresenting highly-repetitive collections. Given a tree with $n$ nodes, we\nshow how to represent the corresponding collection in $O(n)$ space and $O(\\log\nn/ \\log \\log n)$ query time. This improves the previous time-space trade-offs\nfor the problem. Additionally, we show a lower bound proving that the query\ntime is optimal for any solution using near-linear space.\n  To achieve our bounds for random access in persistent strings we show how to\nreduce the problem to the following natural geometric selection problem on line\nsegments. Consider a set of horizontal line segments in the plane. Given\nparameters $i$ and $j$, a segment selection query returns the $j$th smallest\nsegment (the segment with the $j$th smallest $y$-coordinate) among the segments\ncrossing the vertical line through $x$-coordinate $i$. The segment selection\nproblem is to preprocess a set of horizontal line segments into a compact data\nstructure that supports fast segment selection queries. We present a solution\nthat uses $O(n)$ space and support segment selection queries in $O(\\log n/ \\log\n\\log n)$ time, where $n$ is the number of segments. Furthermore, we prove that\nthat this query time is also optimal for any solution using near-linear space.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 11:25:40 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 12:00:50 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2021 16:30:58 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Bille", "Philip", ""], ["G\u00f8rtz", "Inge Li", ""]]}, {"id": "2006.15584", "submitter": "Eduard Eiben", "authors": "Eduard Eiben and William Lochet", "title": "A Polynomial Kernel for Line Graph Deletion", "comments": "To be published in the Proceedings of the 28th Annual European\n  Symposium on Algorithms (ESA 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The line graph of a graph $G$ is the graph $L(G)$ whose vertex set is the\nedge set of $G$ and there is an edge between $e,f\\in E(G)$ if $e$ and $f$ share\nan endpoint in $G$. A graph is called line graph if it is a line graph of some\ngraph. We study the Line-Graph-Edge Deletion problem, which asks whether we can\ndelete at most $k$ edges from the input graph $G$ such that the resulting graph\nis a line graph. More precisely, we give a polynomial kernel for\nLine-Graph-Edge Deletion with $\\mathcal{O}(k^{5})$ vertices. This answers an\nopen question posed by Falk H\\\"{u}ffner at Workshop on Kernels (WorKer) in\n2013.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 12:17:18 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Eiben", "Eduard", ""], ["Lochet", "William", ""]]}, {"id": "2006.15744", "submitter": "Akbar Rafiey", "authors": "Akbar Rafiey, Yuichi Yoshida", "title": "Fast and Private Submodular and $k$-Submodular Functions Maximization\n  with Matroid Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of maximizing nonnegative monotone submodular functions under a\ncertain constraint has been intensively studied in the last decade, and a wide\nrange of efficient approximation algorithms have been developed for this\nproblem. Many machine learning problems, including data summarization and\ninfluence maximization, can be naturally modeled as the problem of maximizing\nmonotone submodular functions. However, when such applications involve\nsensitive data about individuals, their privacy concerns should be addressed.\nIn this paper, we study the problem of maximizing monotone submodular functions\nsubject to matroid constraints in the framework of differential privacy. We\nprovide $(1-\\frac{1}{\\mathrm{e}})$-approximation algorithm which improves upon\nthe previous results in terms of approximation guarantee. This is done with an\nalmost cubic number of function evaluations in our algorithm.\n  Moreover, we study $k$-submodularity, a natural generalization of\nsubmodularity. We give the first $\\frac{1}{2}$-approximation algorithm that\npreserves differential privacy for maximizing monotone $k$-submodular functions\nsubject to matroid constraints. The approximation ratio is asymptotically tight\nand is obtained with an almost linear number of function evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 23:18:58 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Rafiey", "Akbar", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "2006.15747", "submitter": "Haris Aziz", "authors": "Haris Aziz and Ethan Brown", "title": "Random Assignment Under Bi-Valued Utilities: Analyzing\n  Hylland-Zeckhauser, Nash-Bargaining, and other Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hylland-Zeckhauser (HZ) rule is a well-known rule for random assignment\nof items. The complexity of the rule has received renewed interest recently\nwith Vazirani and Yannakakis (2020) proposing a strongly polynomial-time\nalgorithm for the rule under bi-valued utilities, and making several general\ninsights. We study the rule under the case of agents having bi-valued\nutilities. We point out several characterizations of the HZ rule, drawing\nclearer relations with several well-known rules in the literature. As a\nconsequence, we point out alternative strongly polynomial-time algorithms for\nthe HZ solution. We also give reductions from computing the HZ solution to\ncomputing well-known solutions based on leximin or Nash social welfare. An\ninteresting contrast is that the HZ rule is group-strategyproof whereas the\nunconstrained competitive equilibrium with equal incomes rule is not even\nstrategyproof. We clarify which results change when moving from 1-0 utilities\nto the more general bi-valued utilities. Finally, we prove that the closely\nrelated Nash bargaining solution violates envy-freeness and strategyproofness\neven under 1-0 utilities.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 23:48:31 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 23:28:34 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Aziz", "Haris", ""], ["Brown", "Ethan", ""]]}, {"id": "2006.15812", "submitter": "Aravind Gollakota", "authors": "Surbhi Goel, Aravind Gollakota, Adam Klivans", "title": "Statistical-Query Lower Bounds via Functional Gradients", "comments": "34 pages, NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first statistical-query lower bounds for agnostically learning\nany non-polynomial activation with respect to Gaussian marginals (e.g., ReLU,\nsigmoid, sign). For the specific problem of ReLU regression (equivalently,\nagnostically learning a ReLU), we show that any statistical-query algorithm\nwith tolerance $n^{-(1/\\epsilon)^b}$ must use at least $2^{n^c} \\epsilon$\nqueries for some constant $b, c > 0$, where $n$ is the dimension and $\\epsilon$\nis the accuracy parameter. Our results rule out general (as opposed to\ncorrelational) SQ learning algorithms, which is unusual for real-valued\nlearning problems. Our techniques involve a gradient boosting procedure for\n\"amplifying\" recent lower bounds due to Diakonikolas et al. (COLT 2020) and\nGoel et al. (ICML 2020) on the SQ dimension of functions computed by two-layer\nneural networks. The crucial new ingredient is the use of a nonstandard convex\nfunctional during the boosting procedure. This also yields a best-possible\nreduction between two commonly studied models of learning: agnostic learning\nand probabilistic concepts.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 05:15:32 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 21:10:48 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Goel", "Surbhi", ""], ["Gollakota", "Aravind", ""], ["Klivans", "Adam", ""]]}, {"id": "2006.15999", "submitter": "Panagiotis Charalampopoulos", "authors": "Panagiotis Charalampopoulos, Jakub Radoszewski, Wojciech Rytter,\n  Tomasz Wale\\'n, Wiktor Zuba", "title": "The Number of Repetitions in 2D-Strings", "comments": "To appear in the ESA 2020 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notions of periodicity and repetitions in strings, and hence these of\nruns and squares, naturally extend to two-dimensional strings. We consider two\ntypes of repetitions in 2D-strings: 2D-runs and quartics (quartics are a\n2D-version of squares in standard strings). Amir et al. introduced 2D-runs,\nshowed that there are $O(n^3)$ of them in an $n \\times n$ 2D-string and\npresented a simple construction giving a lower bound of $\\Omega(n^2)$ for their\nnumber (TCS 2020). We make a significant step towards closing the gap between\nthese bounds by showing that the number of 2D-runs in an $n \\times n$ 2D-string\nis $O(n^2 \\log^2 n)$. In particular, our bound implies that the $O(n^2\\log n +\n\\textsf{output})$ run-time of the algorithm of Amir et al. for computing\n2D-runs is also $O(n^2 \\log^2 n)$. We expect this result to allow for\nexploiting 2D-runs algorithmically in the area of 2D pattern matching.\n  A quartic is a 2D-string composed of $2 \\times 2$ identical blocks\n(2D-strings) that was introduced by Apostolico and Brimkov (TCS 2000), where by\nquartics they meant only primitively rooted quartics, i.e. built of a primitive\nblock. Here our notion of quartics is more general and analogous to that of\nsquares in 1D-strings. Apostolico and Brimkov showed that there are $O(n^2\n\\log^2 n)$ occurrences of primitively rooted quartics in an $n \\times n$\n2D-string and that this bound is attainable. Consequently the number of\ndistinct primitively rooted quartics is $O(n^2 \\log^2 n)$. Here, we prove that\nthe number of distinct general quartics is also $O(n^2 \\log^2 n)$. This extends\nthe rich combinatorial study of the number of distinct squares in a 1D-string,\nthat was initiated by Fraenkel and Simpson (J. Comb. Theory A 1998), to two\ndimensions.\n  Finally, we show some algorithmic applications of 2D-runs. (Abstract\nshortened due to arXiv requirements.)\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 12:42:39 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Charalampopoulos", "Panagiotis", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Wale\u0144", "Tomasz", ""], ["Zuba", "Wiktor", ""]]}, {"id": "2006.16137", "submitter": "Solon Pissis", "authors": "Panagiotis Charalampopoulos and Huiping Chen and Peter Christen and\n  Grigorios Loukides and Nadia Pisanti and Solon P. Pissis and Jakub\n  Radoszewski", "title": "Pattern Masking for Dictionary Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the Pattern Masking for Dictionary Matching (PMDM) problem, we are given a\ndictionary $\\mathcal{D}$ of $d$ strings, each of length $\\ell$, a query string\n$q$ of length $\\ell$, and a positive integer $z$, and we are asked to compute a\nsmallest set $K\\subseteq\\{1,\\ldots,\\ell\\}$, so that if $q[i]$, for all $i\\in\nK$, is replaced by a wildcard, then $q$ matches at least $z$ strings from\n$\\mathcal{D}$. The PMDM problem lies at the heart of two important applications\nfeatured in large-scale real-world systems: record linkage of databases that\ncontain sensitive information, and query term dropping. In both applications,\nsolving PMDM allows for providing data utility guarantees as opposed to\nexisting approaches.\n  We first show, through a reduction from the well-known $k$-Clique problem,\nthat a decision version of the PMDM problem is NP-complete, even for strings\nover a binary alphabet. We present a data structure for PMDM that answers\nqueries over $\\mathcal{D}$ in time\n$\\mathcal{O}(2^{\\ell/2}(2^{\\ell/2}+\\tau)\\ell)$ and requires space\n$\\mathcal{O}(2^{\\ell}d^2/\\tau^2+2^{\\ell/2}d)$, for any parameter\n$\\tau\\in[1,d]$. We also approach the problem from a more practical perspective.\nWe show an $\\mathcal{O}((d\\ell)^{k/3}+d\\ell)$-time and\n$\\mathcal{O}(d\\ell)$-space algorithm for PMDM if $k=|K|=\\mathcal{O}(1)$. We\ngeneralize our exact algorithm to mask multiple query strings simultaneously.\nWe complement our results by showing a two-way polynomial-time reduction\nbetween PMDM and the Minimum Union problem [Chlamt\\'{a}\\v{c} et al., SODA\n2017]. This gives a polynomial-time\n$\\mathcal{O}(d^{1/4+\\epsilon})$-approximation algorithm for PMDM, which is\ntight under plausible complexity conjectures.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 15:56:54 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Charalampopoulos", "Panagiotis", ""], ["Chen", "Huiping", ""], ["Christen", "Peter", ""], ["Loukides", "Grigorios", ""], ["Pisanti", "Nadia", ""], ["Pissis", "Solon P.", ""], ["Radoszewski", "Jakub", ""]]}, {"id": "2006.16200", "submitter": "Nikos Zarifis", "authors": "Ilias Diakonikolas, Daniel M. Kane, Nikos Zarifis", "title": "Near-Optimal SQ Lower Bounds for Agnostically Learning Halfspaces and\n  ReLUs under Gaussian Marginals", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problems of agnostically learning halfspaces and\nReLUs under Gaussian marginals. In the former problem, given labeled examples\n$(\\mathbf{x}, y)$ from an unknown distribution on $\\mathbb{R}^d \\times \\{ \\pm\n1\\}$, whose marginal distribution on $\\mathbf{x}$ is the standard Gaussian and\nthe labels $y$ can be arbitrary, the goal is to output a hypothesis with 0-1\nloss $\\mathrm{OPT}+\\epsilon$, where $\\mathrm{OPT}$ is the 0-1 loss of the\nbest-fitting halfspace. In the latter problem, given labeled examples\n$(\\mathbf{x}, y)$ from an unknown distribution on $\\mathbb{R}^d \\times\n\\mathbb{R}$, whose marginal distribution on $\\mathbf{x}$ is the standard\nGaussian and the labels $y$ can be arbitrary, the goal is to output a\nhypothesis with square loss $\\mathrm{OPT}+\\epsilon$, where $\\mathrm{OPT}$ is\nthe square loss of the best-fitting ReLU. We prove Statistical Query (SQ) lower\nbounds of $d^{\\mathrm{poly}(1/\\epsilon)}$ for both of these problems. Our SQ\nlower bounds provide strong evidence that current upper bounds for these tasks\nare essentially best possible.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 17:10:10 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Zarifis", "Nikos", ""]]}, {"id": "2006.16222", "submitter": "Kazuhiro Kurita", "authors": "Kazuhiro Kurita and Yasuaki Kobayashi", "title": "Efficient Enumerations for Minimal Multicuts and Multiway Cuts", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.MFCS.2020.60", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G = (V, E)$ be an undirected graph and let $B \\subseteq V \\times V$ be a\nset of terminal pairs. A node/edge multicut is a subset of vertices/edges of\n$G$ whose removal destroys all the paths between every terminal pair in $B$.\nThe problem of computing a {\\em minimum} node/edge multicut is NP-hard and\nextensively studied from several viewpoints. In this paper, we study the\nproblem of enumerating all {\\em minimal} node multicuts. We give an incremental\npolynomial delay enumeration algorithm for minimal node multicuts, which\nextends an enumeration algorithm due to Khachiyan et al. (Algorithmica, 2008)\nfor minimal edge multicuts. Important special cases of node/edge multicuts are\nnode/edge {\\em multiway cuts}, where the set of terminal pairs contains every\npair of vertices in some subset $T \\subseteq V$, that is, $B = T \\times T$. We\nimprove the running time bound for this special case: We devise a polynomial\ndelay and exponential space enumeration algorithm for minimal node multiway\ncuts and a polynomial delay and space enumeration algorithm for minimal edge\nmultiway cuts.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 17:43:49 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Kurita", "Kazuhiro", ""], ["Kobayashi", "Yasuaki", ""]]}, {"id": "2006.16297", "submitter": "Abraham Frandsen", "authors": "Abraham Frandsen, Rong Ge", "title": "Optimization Landscape of Tucker Decomposition", "comments": null, "journal-ref": null, "doi": "10.1007/s10107-020-01531-z", "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tucker decomposition is a popular technique for many data analysis and\nmachine learning applications. Finding a Tucker decomposition is a nonconvex\noptimization problem. As the scale of the problems increases, local search\nalgorithms such as stochastic gradient descent have become popular in practice.\nIn this paper, we characterize the optimization landscape of the Tucker\ndecomposition problem. In particular, we show that if the tensor has an exact\nTucker decomposition, for a standard nonconvex objective of Tucker\ndecomposition, all local minima are also globally optimal. We also give a local\nsearch algorithm that can find an approximate local (and global) optimal\nsolution in polynomial time.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 18:15:22 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Frandsen", "Abraham", ""], ["Ge", "Rong", ""]]}, {"id": "2006.16312", "submitter": "Xiaotian Hao", "authors": "Xiaotian Hao, Zhaoqing Peng, Yi Ma, Guan Wang, Junqi Jin, Jianye Hao,\n  Shan Chen, Rongquan Bai, Mingzhou Xie, Miao Xu, Zhenzhe Zheng, Chuan Yu, Han\n  Li, Jian Xu, Kun Gai", "title": "Dynamic Knapsack Optimization Towards Efficient Multi-Channel Sequential\n  Advertising", "comments": "accepted by ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR cs.SY eess.SY stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In E-commerce, advertising is essential for merchants to reach their target\nusers. The typical objective is to maximize the advertiser's cumulative revenue\nover a period of time under a budget constraint. In real applications, an\nadvertisement (ad) usually needs to be exposed to the same user multiple times\nuntil the user finally contributes revenue (e.g., places an order). However,\nexisting advertising systems mainly focus on the immediate revenue with single\nad exposures, ignoring the contribution of each exposure to the final\nconversion, thus usually falls into suboptimal solutions. In this paper, we\nformulate the sequential advertising strategy optimization as a dynamic\nknapsack problem. We propose a theoretically guaranteed bilevel optimization\nframework, which significantly reduces the solution space of the original\noptimization space while ensuring the solution quality. To improve the\nexploration efficiency of reinforcement learning, we also devise an effective\naction space reduction approach. Extensive offline and online experiments show\nthe superior performance of our approaches over state-of-the-art baselines in\nterms of cumulative revenue.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 18:50:35 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Hao", "Xiaotian", ""], ["Peng", "Zhaoqing", ""], ["Ma", "Yi", ""], ["Wang", "Guan", ""], ["Jin", "Junqi", ""], ["Hao", "Jianye", ""], ["Chen", "Shan", ""], ["Bai", "Rongquan", ""], ["Xie", "Mingzhou", ""], ["Xu", "Miao", ""], ["Zheng", "Zhenzhe", ""], ["Yu", "Chuan", ""], ["Li", "Han", ""], ["Xu", "Jian", ""], ["Gai", "Kun", ""]]}, {"id": "2006.16339", "submitter": "Yongli Zhu", "authors": "Yongli Zhu, Renchang Dai, Guangyi Liu", "title": "Parallel Betweenness Computation in Graph Database for Contingency\n  Selection", "comments": "This paper has been accepted by the 2020 IEEE PES General Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel betweenness computation algorithms are proposed and implemented in a\ngraph database for power system contingency selection. Principles of the graph\ndatabase and graph computing are investigated for both node and edge\nbetweenness computation. Experiments on the 118-bus system and a real power\nsystem show that speed-up can be achieved for both node and edge betweenness\ncomputation while the speeding effect on the latter is more remarkable due to\nthe data retrieving advantages of the graph database on the power network data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 19:45:51 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Zhu", "Yongli", ""], ["Dai", "Renchang", ""], ["Liu", "Guangyi", ""]]}, {"id": "2006.16406", "submitter": "Soumyabrata Pal", "authors": "Arya Mazumdar and Soumyabrata Pal", "title": "Recovery of Sparse Signals from a Mixture of Linear Samples", "comments": "International Conference on Machine Learning (ICML), 2020. (26 pages,\n  3 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of linear regressions is a popular learning theoretic model that is\nused widely to represent heterogeneous data. In the simplest form, this model\nassumes that the labels are generated from either of two different linear\nmodels and mixed together. Recent works of Yin et al. and Krishnamurthy et al.,\n2019, focus on an experimental design setting of model recovery for this\nproblem. It is assumed that the features can be designed and queried with to\nobtain their label. When queried, an oracle randomly selects one of the two\ndifferent sparse linear models and generates a label accordingly. How many such\noracle queries are needed to recover both of the models simultaneously? This\nquestion can also be thought of as a generalization of the well-known\ncompressed sensing problem (Cand\\`es and Tao, 2005, Donoho, 2006). In this\nwork, we address this query complexity problem and provide efficient algorithms\nthat improves on the previously best known results.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 21:52:40 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 16:38:36 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Mazumdar", "Arya", ""], ["Pal", "Soumyabrata", ""]]}, {"id": "2006.16573", "submitter": "Rameshwar Pratap", "authors": "Amit Deshpande and Rameshwar Pratap", "title": "Subspace approximation with outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subspace approximation problem with outliers, for given $n$ points in $d$\ndimensions $x_{1},\\ldots, x_{n} \\in R^{d}$, an integer $1 \\leq k \\leq d$, and\nan outlier parameter $0 \\leq \\alpha \\leq 1$, is to find a $k$-dimensional\nlinear subspace of $R^{d}$ that minimizes the sum of squared distances to its\nnearest $(1-\\alpha)n$ points. More generally, the $\\ell_{p}$ subspace\napproximation problem with outliers minimizes the sum of $p$-th powers of\ndistances instead of the sum of squared distances. Even the case of robust PCA\nis non-trivial, and previous work requires additional assumptions on the input.\nAny multiplicative approximation algorithm for the subspace approximation\nproblem with outliers must solve the robust subspace recovery problem, a\nspecial case in which the $(1-\\alpha)n$ inliers in the optimal solution are\npromised to lie exactly on a $k$-dimensional linear subspace. However, robust\nsubspace recovery is Small Set Expansion (SSE)-hard.\n  We show how to extend dimension reduction techniques and bi-criteria\napproximations based on sampling to the problem of subspace approximation with\noutliers. To get around the SSE-hardness of robust subspace recovery, we assume\nthat the squared distance error of the optimal $k$-dimensional subspace summed\nover the optimal $(1-\\alpha)n$ inliers is at least $\\delta$ times its\nsquared-error summed over all $n$ points, for some $0 < \\delta \\leq 1 -\n\\alpha$. With this assumption, we give an efficient algorithm to find a subset\nof $poly(k/\\epsilon) \\log(1/\\delta) \\log\\log(1/\\delta)$ points whose span\ncontains a $k$-dimensional subspace that gives a multiplicative\n$(1+\\epsilon)$-approximation to the optimal solution. The running time of our\nalgorithm is linear in $n$ and $d$. Interestingly, our results hold even when\nthe fraction of outliers $\\alpha$ is large, as long as the obvious condition $0\n< \\delta \\leq 1 - \\alpha$ is satisfied.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 07:22:33 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Deshpande", "Amit", ""], ["Pratap", "Rameshwar", ""]]}, {"id": "2006.16613", "submitter": "Noga Alon", "authors": "Noga Alon, Andrei Graur", "title": "Efficient Splitting of Measures and Necklaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide approximation algorithms for two problems, known as NECKLACE\nSPLITTING and $\\epsilon$-CONSENSUS SPLITTING. In the problem\n$\\epsilon$-CONSENSUS SPLITTING, there are $n$ non-atomic probability measures\non the interval $[0, 1]$ and $k$ agents. The goal is to divide the interval,\nvia at most $n (k-1)$ cuts, into pieces and distribute them to the $k$ agents\nin an approximately equitable way, so that the discrepancy between the shares\nof any two agents, according to each measure, is at most $2 \\epsilon / k$. It\nis known that this is possible even for $\\epsilon = 0$. NECKLACE SPLITTING is a\ndiscrete version of $\\epsilon$-CONSENSUS SPLITTING. For $k = 2$ and some\nabsolute positive constant $\\epsilon$, both of these problems are PPAD-hard.\n  We consider two types of approximation. The first provides every agent a\npositive amount of measure of each type under the constraint of making at most\n$n (k - 1)$ cuts. The second obtains an approximately equitable split with as\nfew cuts as possible. Apart from the offline model, we consider the online\nmodel as well, where the interval (or necklace) is presented as a stream, and\ndecisions about cutting and distributing must be made on the spot.\n  For the first type of approximation, we describe an efficient algorithm that\ngives every agent at least $\\frac{1}{nk}$ of each measure and works even\nonline. For the second type of approximation, we provide an efficient online\nalgorithm that makes $\\text{poly}(n, k, \\epsilon)$ cuts and an offline\nalgorithm making $O(nk \\log \\frac{k}{\\epsilon})$ cuts. We also establish lower\nbounds for the number of cuts required in the online model for both problems\neven for $k=2$ agents, showing that the number of cuts in our online algorithm\nis optimal up to a logarithmic factor.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 09:07:54 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Alon", "Noga", ""], ["Graur", "Andrei", ""]]}, {"id": "2006.16726", "submitter": "Paul Ouvrard", "authors": "Nicolas Bousquet, Alice Joffard, Paul Ouvrard", "title": "Linear transformations between dominating sets in the TAR-model", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$ and an integer $k$, a token addition and removal ({\\sf TAR}\nfor short) reconfiguration sequence between two dominating sets $D_{\\sf s}$ and\n$D_{\\sf t}$ of size at most $k$ is a sequence $S= \\langle D_0 = D_{\\sf s}, D_1\n\\ldots, D_\\ell = D_{\\sf t} \\rangle$ of dominating sets of $G$ such that any two\nconsecutive dominating sets differ by the addition or deletion of one vertex,\nand no dominating set has size bigger than $k$.\n  We first improve a result of Haas and Seyffarth, by showing that if\n$k=\\Gamma(G)+\\alpha(G)-1$ (where $\\Gamma(G)$ is the maximum size of a minimal\ndominating set and $\\alpha(G)$ the maximum size of an independent set), then\nthere exists a linear {\\sf TAR} reconfiguration sequence between any pair of\ndominating sets.\n  We then improve these results on several graph classes by showing that the\nsame holds for $K_{\\ell}$-minor free graph as long as $k \\ge \\Gamma(G)+O(\\ell\n\\sqrt{\\log \\ell})$ and for planar graphs whenever $k \\ge \\Gamma(G)+3$. Finally,\nwe show that if $k=\\Gamma(G)+tw(G)+1$, then there also exists a linear\ntransformation between any pair of dominating sets.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 12:31:46 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Bousquet", "Nicolas", ""], ["Joffard", "Alice", ""], ["Ouvrard", "Paul", ""]]}, {"id": "2006.16762", "submitter": "Christine Markarian Dr", "authors": "Christine Markarian, Abdul-Nasser Kassar, Manal Yunis", "title": "Online Multi-Facility Location", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facility Location problems ask to place facilities in a way that optimizes a\ngiven objective function so as to provide a service to all clients. These are\none of the most well-studied optimization problems spanning many research areas\nsuch as operations research, computer science, and management science.\nTraditionally, these problems are solved with the assumption that clients need\nto be served by one facility each. In many real-world scenarios, it is very\nlikely that clients need a robust service that requires more than one facility\nfor each client. In this paper, we capture this robustness by exploring a\ngeneralization of Facility Location problems, called Multi-Facility Location\nproblems, in the online setting. An additional parameter k, which represents\nthe number of facilities required to serve a client, is given. We propose the\nfirst online algorithms for the metric and non-metric variants of\nMulti-Facility Location and measure their performance with competitive\nanalysis, the standard to measure online algorithms, in the worst case, in\nwhich the cost of the online algorithm is compared to that of the optimal\noffline algorithm that knows the entire input sequence in advance.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 13:16:18 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Markarian", "Christine", ""], ["Kassar", "Abdul-Nasser", ""], ["Yunis", "Manal", ""]]}, {"id": "2006.16898", "submitter": "Nicole Wein", "authors": "Hsin-Hao Su, Nicole Wein", "title": "Lower Bounds for Dynamic Distributed Task Allocation", "comments": "To appear in ICALP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of distributed task allocation in multi-agent systems.\nSuppose there is a collection of agents, a collection of tasks, and a demand\nvector, which specifies the number of agents required to perform each task. The\ngoal of the agents is to cooperatively allocate themselves to the tasks to\nsatisfy the demand vector. We study the dynamic version of the problem where\nthe demand vector changes over time. Here, the goal is to minimize the\nswitching cost, which is the number of agents that change tasks in response to\na change in the demand vector. The switching cost is an important metric since\nchanging tasks may incur significant overhead.\n  We study a mathematical formalization of the above problem introduced by Su,\nSu, Dornhaus, and Lynch, which can be reformulated as a question of finding a\nlow distortion embedding from symmetric difference to Hamming distance. In this\nmodel it is trivial to prove that the switching cost is at least 2. We present\nthe first non-trivial lower bounds for the switching cost, by giving lower\nbounds of 3 and 4 for different ranges of the parameters.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 15:22:57 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Su", "Hsin-Hao", ""], ["Wein", "Nicole", ""]]}, {"id": "2006.16924", "submitter": "Yihui Quek", "authors": "Andr\\'as Gily\\'en, Seth Lloyd, Iman Marvian, Yihui Quek, Mark M. Wilde", "title": "Quantum algorithm for Petz recovery channels and pretty good\n  measurements", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS hep-th math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Petz recovery channel plays an important role in quantum information\nscience as an operation that approximately reverses the effect of a quantum\nchannel. The pretty good measurement is a special case of the Petz recovery\nchannel, and it allows for near-optimal state discrimination. A hurdle to the\nexperimental realization of these vaunted theoretical tools is the lack of a\nsystematic and efficient method to implement them. This paper sets out to\nrectify this lack: using the recently developed tools of quantum singular value\ntransformation and oblivious amplitude amplification, we provide a quantum\nalgorithm to implement the Petz recovery channel when given the ability to\nperform the channel that one wishes to reverse. Moreover, we prove that our\nquantum algorithm's usage of the channel implementation cannot be improved by\nmore than a quadratic factor. Our quantum algorithm also provides a procedure\nto perform pretty good measurements when given multiple copies of the states\nthat one is trying to distinguish.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 15:51:59 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Gily\u00e9n", "Andr\u00e1s", ""], ["Lloyd", "Seth", ""], ["Marvian", "Iman", ""], ["Quek", "Yihui", ""], ["Wilde", "Mark M.", ""]]}, {"id": "2006.16947", "submitter": "Daniele Calandriello", "authors": "Daniele Calandriello, Micha{\\l} Derezi\\'nski, Michal Valko", "title": "Sampling from a $k$-DPP without looking at all items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are a useful probabilistic model for\nselecting a small diverse subset out of a large collection of items, with\napplications in summarization, stochastic optimization, active learning and\nmore. Given a kernel function and a subset size $k$, our goal is to sample $k$\nout of $n$ items with probability proportional to the determinant of the kernel\nmatrix induced by the subset (a.k.a. $k$-DPP). Existing $k$-DPP sampling\nalgorithms require an expensive preprocessing step which involves multiple\npasses over all $n$ items, making it infeasible for large datasets. A na\\\"ive\nheuristic addressing this problem is to uniformly subsample a fraction of the\ndata and perform $k$-DPP sampling only on those items, however this method\noffers no guarantee that the produced sample will even approximately resemble\nthe target distribution over the original dataset. In this paper, we develop an\nalgorithm which adaptively builds a sufficiently large uniform sample of data\nthat is then used to efficiently generate a smaller set of $k$ items, while\nensuring that this set is drawn exactly from the target distribution defined on\nall $n$ items. We show empirically that our algorithm produces a $k$-DPP sample\nafter observing only a small fraction of all elements, leading to several\norders of magnitude faster performance compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 16:40:44 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Calandriello", "Daniele", ""], ["Derezi\u0144ski", "Micha\u0142", ""], ["Valko", "Michal", ""]]}]