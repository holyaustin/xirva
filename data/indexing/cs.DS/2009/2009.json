[{"id": "2009.00083", "submitter": "Jonas Lukasczyk", "authors": "Jonas Lukasczyk, Christoph Garth, Ross Maciejewski, and Julien Tierny", "title": "Localized Topological Simplification of Scalar Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a localized algorithm for the topological simplification\nof scalar data, an essential pre-processing step of topological data analysis\n(TDA). Given a scalar field f and a selection of extrema to preserve, the\nproposed localized topological simplification (LTS) derives a function g that\nis close to f and only exhibits the selected set of extrema. Specifically, sub-\nand superlevel set components associated with undesired extrema are first\nlocally flattened and then correctly embedded into the global scalar field,\nsuch that these regions are guaranteed -- from a combinatorial perspective --\nto no longer contain any undesired extrema. In contrast to previous global\napproaches, LTS only and independently processes regions of the domain that\nactually need to be simplified, which already results in a noticeable speedup.\nMoreover, due to the localized nature of the algorithm, LTS can utilize\nshared-memory parallelism to simplify regions simultaneously with a high\nparallel efficiency (70%). Hence, LTS significantly improves interactivity for\nthe exploration of simplification parameters and their effect on subsequent\ntopological analysis. For such exploration tasks, LTS brings the overall\nexecution time of a plethora of TDA pipelines from minutes down to seconds,\nwith an average observed speedup over state-of-the-art techniques of up to x36.\nFurthermore, in the special case where preserved extrema are selected based on\ntopological persistence, an adapted version of LTS partially computes the\npersistence diagram and simultaneously simplifies features below a predefined\npersistence threshold. The effectiveness of LTS, its parallel efficiency, and\nits resulting benefits for TDA are demonstrated on several simulated and\nacquired datasets from different application domains, including physics,\nchemistry, and biomedical imaging.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 19:57:40 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Lukasczyk", "Jonas", ""], ["Garth", "Christoph", ""], ["Maciejewski", "Ross", ""], ["Tierny", "Julien", ""]]}, {"id": "2009.00098", "submitter": "Balaram Behera", "authors": "Balaram Behera", "title": "Sorting an Array Using the Topological Sort of a Corresponding\n  Comparison Graph", "comments": "18 pages, 0 figures. Keywords: graph algorithms; topological sort;\n  sorting algorithms; comparison graphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quest for efficient sorting is ongoing, and we will explore a graph-based\nstable sorting strategy, in particular employing comparison graphs. We use the\ntopological sort to map the comparison graph to a linear domain, and we can\nmanipulate our graph such that the resulting topological sort is the sorted\narray. By taking advantage of the many relations between Hamiltonian paths and\ntopological sorts in comparison graphs, we design a Divide-and-Conquer\nalgorithm that runs in the optimal $O(n \\log n)$ time. In the process, we\nconstruct a new merge process for graphs with relevant invariant properties for\nour use. Furthermore, this method is more space-efficient than the famous {\\sc\nMergeSort} since we modify our fixed graph only.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 21:04:16 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Behera", "Balaram", ""]]}, {"id": "2009.00188", "submitter": "Philip Klein", "authors": "Vincent Cohen-Addad and Philip N. Klein and D\\'aniel Marx", "title": "On the computational tractability of a geographic clustering problem\n  arising in redistricting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Redistricting is the problem of dividing a state into a number $k$ of\nregions, called districts. Voters in each district elect a representative. The\nprimary criteria are: each district is connected, district populations are\nequal (or nearly equal), and districts are \"compact\". There are multiple\ncompeting definitions of compactness, usually minimizing some quantity.\n  One measure that has been recently promoted by Duchin and others is number of\ncut edges. In redistricting, one is given atomic regions out of which each\ndistrict must be built. The populations of the atomic regions are given.\nConsider the graph with one vertex per atomic region (with weight equal to the\nregion's population) and an edge between atomic regions that share a boundary.\nA districting plan is a partition of vertices into $k$ parts, each connnected,\nof nearly equal weight. The districts are considered compact to the extent that\nthe plan minimizes the number of edges crossing between different parts.\n  Consider two problems: find the most compact districting plan, and sample\ndistricting plans under a compactness constraint uniformly at random. Both\nproblems are NP-hard so we restrict the input graph to have branchwidth at most\n$w$. (A planar graph's branchwidth is bounded by its diameter.) If both $k$ and\n$w$ are bounded by constants, the problems are solvable in polynomial time.\nAssume vertices have weight~1. One would like algorithms whose running times\nare of the form $O(f(k,w) n^c)$ for some constant $c$ independent of $k$ and\n$w$, in which case the problems are said to be fixed-parameter tractable with\nrespect to $k$ and $w$). We show that, under a complexity-theoretic assumption,\nno such algorithms exist. However, we do give algorithms with running time\n$O(c^wn^{k+1})$. Thus if the diameter of the graph is moderately small and the\nnumber of districts is very small, our algorithm is useable.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 02:30:19 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Klein", "Philip N.", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "2009.00384", "submitter": "Kanav Gupta", "authors": "Kanav Gupta, Mithilesh Kumar and H{\\aa}vard Raddum", "title": "Obtuse Lattice Bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lattice reduction is an algorithm that transforms the given basis of the\nlattice to another lattice basis such that problems like finding a shortest\nvector and closest vector become easier to solve. We define a class of bases\ncalled obtuse bases and show that any lattice basis can be transformed to an\nobtuse basis. A shortest vector $\\mathbf{s}$ can be written as\n$\\mathbf{s}=v_1\\mathbf{b}_1+\\dots+v_n\\mathbf{b}_n$ where\n$\\mathbf{b}_1,\\dots,\\mathbf{b}_n$ are the input basis vectors and\n$v_1,\\dots,v_n$ are integers. When the input basis is obtuse, all these\nintegers can be chosen to be positive for a shortest vector. This property of\nobtuse bases makes the lattice enumeration algorithm for finding a shortest\nvector exponentially faster. We have implemented the algorithm for making bases\nobtuse, and tested it some small bases.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 12:30:14 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 10:07:56 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Gupta", "Kanav", ""], ["Kumar", "Mithilesh", ""], ["Raddum", "H\u00e5vard", ""]]}, {"id": "2009.00672", "submitter": "Ilia Rushkin", "authors": "Ilia Rushkin", "title": "Document Similarity from Vector Space Densities", "comments": "12 pages, 3 figures", "journal-ref": "In: Arai K., Kapoor S., Bhatia R. (eds) Intelligent Systems and\n  Applications. IntelliSys 2020. Advances in Intelligent Systems and Computing,\n  vol 1251. Springer, Cham", "doi": "10.1007/978-3-030-55187-2_14", "report-no": null, "categories": "cs.CL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computationally light method for estimating similarities between\ntext documents, which we call the density similarity (DS) method. The method is\nbased on a word embedding in a high-dimensional Euclidean space and on kernel\nregression, and takes into account semantic relations among words. We find that\nthe accuracy of this method is virtually the same as that of a state-of-the-art\nmethod, while the gain in speed is very substantial. Additionally, we introduce\ngeneralized versions of the top-k accuracy metric and of the Jaccard metric of\nagreement between similarity models.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 19:28:51 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Rushkin", "Ilia", ""]]}, {"id": "2009.00697", "submitter": "Georg Anegg", "authors": "Georg Anegg, Haris Angelidakis, Rico Zenklusen", "title": "Simpler and Stronger Approaches for Non-Uniform Hypergraph Matching and\n  the F\\\"uredi, Kahn, and Seymour Conjecture", "comments": "Reorganized the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known conjecture of F\\\"uredi, Kahn, and Seymour (1993) on non-uniform\nhypergraph matching states that for any hypergraph with edge weights $w$, there\nexists a matching $M$ such that the inequality $\\sum_{e\\in M} g(e) w(e) \\geq\n\\mathrm{OPT}_{\\mathrm{LP}}$ holds with $g(e)=|e|-1+\\frac{1}{|e|}$, where\n$\\mathrm{OPT}_{\\mathrm{LP}}$ denotes the optimal value of the canonical LP\nrelaxation.\n  While the conjecture remains open, the strongest result towards it was very\nrecently obtained by Brubach, Sankararaman, Srinivasan, and Xu\n(2020)---building on and strengthening prior work by Bansal, Gupta, Li, Mestre,\nNagarajan, and Rudra (2012)---showing that the aforementioned inequality holds\nwith $g(e)=|e|+O(|e|\\exp(-|e|))$.\n  Actually, their method works in a more general sampling setting, where, given\na point $x$ of the canonical LP relaxation, the task is to efficiently sample a\nmatching $M$ containing each edge $e$ with probability at least\n$\\frac{x(e)}{g(e)}$.\n  We present simpler and easy-to-analyze procedures leading to improved\nresults. More precisely, for any solution $x$ to the canonical LP, we introduce\na simple algorithm based on exponential clocks for Brubach et al.'s sampling\nsetting achieving $g(e)=|e|-(|e|-1)x(e)$.\n  Apart from the slight improvement in $g$, our technique may open up new ways\nto attack the original conjecture.\n  Moreover, we provide a short and arguably elegant analysis showing that a\nnatural greedy approach for the original setting of the conjecture shows the\ninequality for the same $g(e)=|e|-(|e|-1)x(e)$ even for the more general\nhypergraph $b$-matching problem.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 21:02:29 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 19:56:26 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Anegg", "Georg", ""], ["Angelidakis", "Haris", ""], ["Zenklusen", "Rico", ""]]}, {"id": "2009.00800", "submitter": "Roie Levin", "authors": "Anupam Gupta, Roie Levin", "title": "Fully-Dynamic Submodular Cover with Bounded Recourse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In submodular covering problems, we are given a monotone, nonnegative\nsubmodular function $f: 2^N \\rightarrow\\mathbb{R}_+$ and wish to find the\nmin-cost set $S\\subseteq N$ such that $f(S)=f(N)$. This captures SetCover when\n$f$ is a coverage function. We introduce a general framework for solving such\nproblems in a fully-dynamic setting where the function $f$ changes over time,\nand only a bounded number of updates to the solution (recourse) is allowed. For\nconcreteness, suppose a nonnegative monotone submodular function $g_t$ is added\nor removed from an active set $G^{(t)}$ at each time $t$. If\n$f^{(t)}=\\sum_{g\\in G^{(t)}} g$ is the sum of all active functions, we wish to\nmaintain a competitive solution to SubmodularCover for $f^{(t)}$ as this active\nset changes, and with low recourse.\n  We give an algorithm that maintains an $O(\\log(f_{max}/f_{min}))$-competitive\nsolution, where $f_{max}, f_{min}$ are the largest/smallest marginals of\n$f^{(t)}$. The algorithm guarantees a total recourse of $O(\\log(c_{max}/\nc_{min})\\cdot\\sum_{t\\leq T}g_t(N))$, where $c_{max},c_{min}$ are the\nlargest/smallest costs of elements in $N$. This competitive ratio is best\npossible even in the offline setting, and the recourse bound is optimal up to\nthe logarithmic factor. For monotone submodular functions that also have\npositive mixed third derivatives, we show an optimal recourse bound of\n$O(\\sum_{t\\leq T}g_t(N))$. This structured class includes set-coverage\nfunctions, so our algorithm matches the known $O(\\log n)$-competitiveness and\n$O(1)$ recourse guarantees for fully-dynamic SetCover. Our work simultaneously\nsimplifies and unifies previous results, as well as generalizes to a\nsignificantly larger class of covering problems. Our key technique is a new\npotential function inspired by Tsallis entropy. We also extensively use the\nidea of Mutual Coverage, which generalizes the classic notion of mutual\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 03:30:52 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Gupta", "Anupam", ""], ["Levin", "Roie", ""]]}, {"id": "2009.00808", "submitter": "Rudy Zhou", "authors": "Anupam Gupta, Benjamin Moseley, Rudy Zhou", "title": "Structural Iterative Rounding for Generalized $k$-Median Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers approximation algorithms for generalized $k$-median\nproblems. This class of problems can be informally described as $k$-median with\na constant number of extra constraints, and includes $k$-median with outliers,\nand knapsack median. Our first contribution is a pseudo-approximation algorithm\nfor generalized $k$-median that outputs a $6.387$-approximate solution, with a\nconstant number of fractional variables. The algorithm builds on the iterative\nrounding framework introduced by Krishnaswamy, Li, and Sandeep for $k$-median\nwith outliers. The main technical innovation is allowing richer constraint sets\nin the iterative rounding and taking advantage of the structure of the\nresulting extreme points.\n  Using our pseudo-approximation algorithm, we give improved approximation\nalgorithms for $k$-median with outliers and knapsack median. This involves\ncombining our pseudo-approximation with pre- and post-processing steps to round\na constant number of fractional variables at a small increase in cost. Our\nalgorithms achieve approximation ratios $6.994 + \\epsilon$ and $6.387 +\n\\epsilon$ for $k$-median with outliers and knapsack median, respectively. These\nimprove on the best-known approximation ratio $7.081 + \\epsilon$ for both\nproblems \\cite{DBLP:conf/stoc/KrishnaswamyLS18}.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 04:06:45 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Gupta", "Anupam", ""], ["Moseley", "Benjamin", ""], ["Zhou", "Rudy", ""]]}, {"id": "2009.00809", "submitter": "Jungho Ahn", "authors": "Jungho Ahn, Eun Jung Kim, and Euiwoong Lee", "title": "Towards constant-factor approximation for chordal / distance-hereditary\n  vertex deletion", "comments": "27 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a family of graphs $\\mathcal{F}$, Weighted $\\mathcal{F}$-Deletion is the\nproblem for which the input is a vertex weighted graph $G=(V,E)$ and the goal\nis to delete $S\\subseteq V$ with minimum weight such that $G\\setminus\nS\\in\\mathcal{F}$. Designing a constant-factor approximation algorithm for large\nsubclasses of perfect graphs has been an interesting research direction. Block\ngraphs, 3-leaf power graphs, and interval graphs are known to admit\nconstant-factor approximation algorithms, but the question is open for chordal\ngraphs and distance-hereditary graphs.\n  In this paper, we add one more class to this list by presenting a\nconstant-factor approximation algorithm when $F$ is the intersection of chordal\ngraphs and distance-hereditary graphs. They are known as ptolemaic graphs and\nform a superset of both block graphs and 3-leaf power graphs above. Our proof\npresents new properties and algorithmic results on inter-clique digraphs as\nwell as an approximation algorithm for a variant of Feedback Vertex Set that\nexploits this relationship (named Feedback Vertex Set with Precedence\nConstraints), each of which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 04:07:53 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Ahn", "Jungho", ""], ["Kim", "Eun Jung", ""], ["Lee", "Euiwoong", ""]]}, {"id": "2009.00836", "submitter": "Xian Wu", "authors": "Xian Wu and Moses Charikar", "title": "Nearest Neighbor Search for Hyperbolic Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding into hyperbolic space is emerging as an effective representation\ntechnique for datasets that exhibit hierarchical structure. This development\nmotivates the need for algorithms that are able to effectively extract\nknowledge and insights from datapoints embedded in negatively curved spaces. We\nfocus on the problem of nearest neighbor search, a fundamental problem in data\nanalysis. We present efficient algorithmic solutions that build upon\nestablished methods for nearest neighbor search in Euclidean space, allowing\nfor easy adoption and integration with existing systems. We prove theoretical\nguarantees for our techniques and our experiments demonstrate the effectiveness\nof our approach on real datasets over competing algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 06:21:52 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Wu", "Xian", ""], ["Charikar", "Moses", ""]]}, {"id": "2009.01012", "submitter": "Benjamin Niedermann", "authors": "Alina Nitzke, Benjamin Niedermann, Luciana Fenoglio-Marc, J\\\"urgen\n  Kusche, Jan-Henrik Haunert", "title": "Reconstructing the Dynamic Sea Surface from Tide Gauge Records Using\n  Optimal Data-Dependent Triangulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructions of sea level prior to the satellite altimeter era are usually\nderived from tide gauge records; however most algorithms for this assume that\nmodes of sea level variability are stationary which is not true over several\ndecades. Here we suggest a method that is based on optimized data-dependent\ntriangulations of the network of gauge stations. Data-dependent triangulations\nare triangulations of point sets that rely not only on 2D point positions but\nalso on additional data (e.g. elevation, anomalies). In this article, we show\nhow data-dependent triangulations with min-error criteria can be used to\nreconstruct 2D maps of the sea surface anomaly over a longer time period,\nassuming that height anomalies are continuously monitored at a sparse set of\nstations and, in addition, observations of a reference surface is provided over\na shorter time period. At the heart of our method is the idea to learn a\nmin-error triangulation based on the available reference data, and to use the\nlearned triangulation subsequently to compute piece-wise linear surface models\nfor epochs in which only observations from monitoring stations are given. We\ncombine our approach of min-error triangulation with $k$-order Delaunay\ntriangulation to stabilize the triangles geometrically. We show that this\napproach is advantageous for the reconstruction of the sea surface by combining\ntide gauge measurements with data of modern satellite altimetry. We show how to\nlearn a min-error triangulation and a min-error $k$-order Delaunay\ntriangulation using integer linear programming. We confront our reconstructions\nagainst the Delaunay triangulation. With real data for the North Sea we show\nthat the min-error triangulation outperforms the Delaunay method significantly\nfor reconstructions back in time up to 18 years, and the $k$-order Delaunay\nmin-error triangulation even up to 21 years for $k=2$.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 12:44:39 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 16:22:12 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Nitzke", "Alina", ""], ["Niedermann", "Benjamin", ""], ["Fenoglio-Marc", "Luciana", ""], ["Kusche", "J\u00fcrgen", ""], ["Haunert", "Jan-Henrik", ""]]}, {"id": "2009.01161", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Ran Raz", "title": "Near-Quadratic Lower Bounds for Two-Pass Graph Streaming Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that any two-pass graph streaming algorithm for the $s$-$t$\nreachability problem in $n$-vertex directed graphs requires near-quadratic\nspace of $n^{2-o(1)}$ bits. As a corollary, we also obtain near-quadratic space\nlower bounds for several other fundamental problems including maximum bipartite\nmatching and (approximate) shortest path in undirected graphs.\n  Our results collectively imply that a wide range of graph problems admit\nessentially no non-trivial streaming algorithm even when two passes over the\ninput is allowed. Prior to our work, such impossibility results were only known\nfor single-pass streaming algorithms, and the best two-pass lower bounds only\nruled out $o(n^{7/6})$ space algorithms, leaving open a large gap between\n(trivial) upper bounds and lower bounds.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 16:06:35 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Assadi", "Sepehr", ""], ["Raz", "Ran", ""]]}, {"id": "2009.01220", "submitter": "Anamay Chaturvedi", "authors": "Anamay Chaturvedi, Huy Nguyen, Eric Xu", "title": "Differentially private $k$-means clustering via exponential mechanism\n  and max cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new $(\\epsilon_p, \\delta_p)$-differentially private algorithm\nfor the $k$-means clustering problem. Given a dataset in Euclidean space, the\n$k$-means clustering problem requires one to find $k$ points in that space such\nthat the sum of squares of Euclidean distances between each data point and its\nclosest respective point among the $k$ returned is minimised. Although there\nexist privacy-preserving methods with good theoretical guarantees to solve this\nproblem [Balcan et al., 2017; Kaplan and Stemmer, 2018], in practice it is seen\nthat it is the additive error which dictates the practical performance of these\nmethods. By reducing the problem to a sequence of instances of maximum coverage\non a grid, we are able to derive a new method that achieves lower additive\nerror then previous works. For input datasets with cardinality $n$ and diameter\n$\\Delta$, our algorithm has an $O(\\Delta^2 (k \\log^2 n\n\\log(1/\\delta_p)/\\epsilon_p + k\\sqrt{d \\log(1/\\delta_p)}/\\epsilon_p))$ additive\nerror whilst maintaining constant multiplicative error. We conclude with some\nexperiments and find an improvement over previously implemented work for this\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 17:52:54 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Chaturvedi", "Anamay", ""], ["Nguyen", "Huy", ""], ["Xu", "Eric", ""]]}, {"id": "2009.01346", "submitter": "Shyam Narayanan", "authors": "Shyam Narayanan, Michael Ren", "title": "Circular Trace Reconstruction", "comments": "25 pages, 1 figure. To appear in Innovations in Theoretical Computer\n  Science (ITCS), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trace reconstruction is the problem of learning an unknown string $x$ from\nindependent traces of $x$, where traces are generated by independently deleting\neach bit of $x$ with some deletion probability $q$. In this paper, we initiate\nthe study of Circular trace reconstruction, where the unknown string $x$ is\ncircular and traces are now rotated by a random cyclic shift. Trace\nreconstruction is related to many computational biology problems studying DNA,\nwhich is a primary motivation for this problem as well, as many types of DNA\nare known to be circular.\n  Our main results are as follows. First, we prove that we can reconstruct\narbitrary circular strings of length $n$ using\n$\\exp\\big(\\tilde{O}(n^{1/3})\\big)$ traces for any constant deletion probability\n$q$, as long as $n$ is prime or the product of two primes. For $n$ of this\nform, this nearly matches what was the best known bound of\n$\\exp\\big(O(n^{1/3})\\big)$ for standard trace reconstruction when this paper\nwas initially released. We note, however, that Chase very recently improved the\nstandard trace reconstruction bound to $\\exp\\big(\\tilde{O}(n^{1/5})\\big)$.\nNext, we prove that we can reconstruct random circular strings with high\nprobability using $n^{O(1)}$ traces for any constant deletion probability $q$.\nFinally, we prove a lower bound of $\\tilde{\\Omega}(n^3)$ traces for arbitrary\ncircular strings, which is greater than the best known lower bound of\n$\\tilde{\\Omega}(n^{3/2})$ in standard trace reconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 21:09:55 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 06:22:50 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Narayanan", "Shyam", ""], ["Ren", "Michael", ""]]}, {"id": "2009.01353", "submitter": "Luca Versari", "authors": "Luca Versari, Iulia M. Comsa, Alessio Conte, Roberto Grossi", "title": "Zuckerli: A New Compressed Representation for Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zuckerli is a scalable compression system meant for large real-world graphs.\nGraphs are notoriously challenging structures to store efficiently due to their\nlinked nature, which makes it hard to separate them into smaller, compact\ncomponents. Therefore, effective compression is crucial when dealing with large\ngraphs, which can have billions of nodes and edges. Furthermore, a good\ncompression system should give the user fast and reasonably flexible access to\nparts of the compressed data without requiring full decompression, which may be\nunfeasible on their system. Zuckerli improves multiple aspects of WebGraph, the\ncurrent state-of-the-art in compressing real-world graphs, by using advanced\ncompression techniques and novel heuristic graph algorithms. It can produce\nboth a compressed representation for storage and one which allows fast direct\naccess to the adjacency lists of the compressed graph without decompressing the\nentire graph. We validate the effectiveness of Zuckerli on real-world graphs\nwith up to a billion nodes and 90 billion edges, conducting an extensive\nexperimental evaluation of both compression density and decompression\nperformance. We show that Zuckerli-compressed graphs are 10% to 29% smaller,\nand more than 20% in most cases, with a resource usage for decompression\ncomparable to that of WebGraph.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 21:29:30 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Versari", "Luca", ""], ["Comsa", "Iulia M.", ""], ["Conte", "Alessio", ""], ["Grossi", "Roberto", ""]]}, {"id": "2009.01446", "submitter": "Abu Reyan Ahmed", "authors": "Saad Al Muttakee, Abu Reyan Ahmed, Md. Saidur Rahman", "title": "New Results and Bounds on Online Facility Assignment Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an online facility assignment problem where a set of facilities $F =\n\\{ f_1, f_2, f_3, \\cdots, f_{|F|} \\}$ of equal capacity $l$ is situated on a\nmetric space and customers arrive one by one in an online manner on that space.\nWe assign a customer $c_i$ to a facility $f_j$ before a new customer $c_{i+1}$\narrives. The cost of this assignment is the distance between $c_i$ and $f_j$.\nThe objective of this problem is to minimize the sum of all assignment costs.\nRecently Ahmed et al. (TCS, 806, pp. 455-467, 2020) studied the problem where\nthe facilities are situated on a line and computed competitive ratio of\n\"Algorithm Greedy\" which assigns the customer to the nearest available\nfacility. They computed competitive ratio of algorithm named \"Algorithm\nOptimal-Fill\" which assigns the new customer considering optimal assignment of\nall previous customers. They also studied the problem where the facilities are\nsituated on a connected unweighted graph.\n  In this paper we first consider that $F$ is situated on the vertices of a\nconnected unweighted grid graph $G$ of size $r \\times c$ and customers arrive\none by one having positions on the vertices of $G$. We show that Algorithm\nGreedy has competitive ratio $r \\times c + r + c$ and Algorithm Optimal-Fill\nhas competitive ratio $O(r \\times c)$. We later show that the competitive ratio\nof Algorithm Optimal-Fill is $2|F|$ for any arbitrary graph. Our bound is tight\nand better than the previous result. We also consider the facilities are\ndistributed arbitrarily on a plane and provide an algorithm for the scenario.\nWe also provide an algorithm that has competitive ratio $(2n-1)$. Finally, we\nconsider a straight line metric space and show that no algorithm for the online\nfacility assignment problem has competitive ratio less than $9.001$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 04:33:53 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Muttakee", "Saad Al", ""], ["Ahmed", "Abu Reyan", ""], ["Rahman", "Md. Saidur", ""]]}, {"id": "2009.01488", "submitter": "Dennis Rohde", "authors": "Maike Buchin and Anne Driemel and Dennis Rohde", "title": "Approximating $(k,\\ell)$-Median Clustering for Polygonal Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2015, Driemel, Krivo\\v{s}ija and Sohler introduced the $(k,\\ell)$-median\nproblem for clustering polygonal curves under the Fr\\'echet distance. Given a\nset of input curves, the problem asks to find $k$ median curves of at most\n$\\ell$ vertices each that minimize the sum of Fr\\'echet distances over all\ninput curves to their closest median curve. A major shortcoming of their\nalgorithm is that the input curves are restricted to lie on the real line. In\nthis paper, we present a randomized bicriteria-approximation algorithm that\nworks for polygonal curves in $\\mathbb{R}^d$ and achieves approximation factor\n$(1+\\epsilon)$ with respect to the clustering costs. The algorithm has\nworst-case running-time linear in the number of curves, polynomial in the\nmaximum number of vertices per curve, i.e. their complexity, and exponential in\n$d$, $\\ell$, $\\epsilon$ and $\\delta$, i.e., the failure probability. We achieve\nthis result through a shortcutting lemma, which guarantees the existence of a\npolygonal curve with similar cost as an optimal median curve of complexity\n$\\ell$, but of complexity at most $2\\ell-2$, and whose vertices can be computed\nefficiently. We combine this lemma with the superset-sampling technique by\nKumar et al. to derive our clustering result. In doing so, we describe and\nanalyze a generalization of the algorithm by Ackermann et al., which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 07:07:03 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 15:40:08 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 11:13:34 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Buchin", "Maike", ""], ["Driemel", "Anne", ""], ["Rohde", "Dennis", ""]]}, {"id": "2009.01498", "submitter": "Kurt Mehlhorn", "authors": "Vincenzo Bonifaci and Enrico Facca and Frederic Folz and Andreas\n  Karrenbauer and Pavel Kolev and Kurt Mehlhorn and Giovanna Morigi and\n  Golnoosh Shahkarami and Quentin Vermande", "title": "Physarum Multi-Commodity Flow Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In wet-lab experiments, the slime mold Physarum polycephalum has demonstrated\nits ability to solve shortest path problems and to design efficient networks.\nFor the shortest path problem, a mathematical model for the evolution of the\nslime is available and it has been shown in computer experiments and through\nmathematical analysis that the dynamics solves the shortest path problem. In\nthis paper, we introduce a dynamics for the network design problem. We\nformulate network design as the problem of constructing a network that\nefficiently supports a multi-commodity flow problem. We investigate the\ndynamics in computer simulations and analytically. The simulations show that\nthe dynamics is able to construct efficient and elegant networks. In the\ntheoretical part we show that the dynamics minimizes an objective combining the\ncost of the network and the cost of routing the demands through the network. We\nalso give alternative characterization of the optimum solution.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 07:48:48 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 15:17:07 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 11:36:33 GMT"}, {"version": "v4", "created": "Wed, 10 Mar 2021 21:05:59 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Bonifaci", "Vincenzo", ""], ["Facca", "Enrico", ""], ["Folz", "Frederic", ""], ["Karrenbauer", "Andreas", ""], ["Kolev", "Pavel", ""], ["Mehlhorn", "Kurt", ""], ["Morigi", "Giovanna", ""], ["Shahkarami", "Golnoosh", ""], ["Vermande", "Quentin", ""]]}, {"id": "2009.01544", "submitter": "Avery Miller", "authors": "Avery Miller, Ullash Saha", "title": "Fast Byzantine Gathering with Visibility in Graphs", "comments": "Conference version appeared at ALGOSENSORS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the gathering task by a team of $m$ synchronous mobile robots in\na graph of $n$ nodes. Each robot has an identifier (ID) and runs its own\ndeterministic algorithm, i.e., there is no centralized coordinator. We consider\na particularly challenging scenario: there are $f$ Byzantine robots in the team\nthat can behave arbitrarily, and even have the ability to change their IDs to\nany value at any time. There is no way to distinguish these robots from\nnon-faulty robots, other than perhaps observing strange or unexpected\nbehaviour. The goal of the gathering task is to eventually have all non-faulty\nrobots located at the same node in the same round. It is known that no\nalgorithm can solve this task unless there at least $f+1$ non-faulty robots in\nthe team. In this paper, we design an algorithm that runs in polynomial time\nwith respect to $n$ and $m$ that matches this bound, i.e., it works in a team\nthat has exactly $f+1$ non-faulty robots. In our model, we have equipped the\nrobots with sensors that enable each robot to see the subgraph (including\nrobots) within some distance $H$ of its current node. We prove that the\ngathering task is solvable if this visibility range $H$ is at least the radius\nof the graph, and not solvable if $H$ is any fixed constant.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 09:39:07 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Miller", "Avery", ""], ["Saha", "Ullash", ""]]}, {"id": "2009.01765", "submitter": "Sarnath Ramnath", "authors": "Sarnath Ramnath and Venkata M.V. Gunturi", "title": "Optimal Load Balanced Demand Distribution under Overload Penalties", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Input to the Load Balanced Demand Distribution (LBDD) consists of the\nfollowing: (a) a set of service centers; (b) a set of demand nodes and; (c) a\ncost matrix containing the cost of assignment for each (demand node, service\ncenter) pair. In addition, each service center is also associated with a notion\nof capacity and a penalty which is incurred if it gets overloaded. Given the\ninput, the LBDD problem determines a mapping from the set of n demand vertices\nto the set of k service centers, n being much larger than k. The objective is\nto determine a mapping that minimizes the sum of the following two terms: (i)\nthe total cost between demand units and their allotted service centers and,\n(ii) total penalties incurred. The problem of LBDD has a variety of\napplications. An instance of the LBDD problem can be reduced to an instance of\nthe min-cost bi-partite matching problem. The best known algorithm for min-cost\nmatching in an unbalanced bipartite graph yields a complexity of O($n^3k$).\nThis paper proposes novel allotment subspace re-adjustment based approach which\nallows us to characterize the optimality of the mapping without invoking\nmatching or mincost flow. This approach yields an optimal solution with time\ncomplexity $O(nk^3 +nk^2 log n)$, and also allows us to efficiently maintain an\noptimal allotment under insertions and deletions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 16:13:47 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Ramnath", "Sarnath", ""], ["Gunturi", "Venkata M. V.", ""]]}, {"id": "2009.01802", "submitter": "Jan van den Brand", "authors": "Jan van den Brand, Yin-Tat Lee, Danupon Nanongkai, Richard Peng,\n  Thatchaphol Saranurak, Aaron Sidford, Zhao Song, Di Wang", "title": "Bipartite Matching in Nearly-linear Time on Moderately Dense Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an $\\tilde O(m+n^{1.5})$-time randomized algorithm for maximum\ncardinality bipartite matching and related problems (e.g. transshipment,\nnegative-weight shortest paths, and optimal transport) on $m$-edge, $n$-node\ngraphs. For maximum cardinality bipartite matching on moderately dense graphs,\ni.e. $m = \\Omega(n^{1.5})$, our algorithm runs in time nearly linear in the\ninput size and constitutes the first improvement over the classic\n$O(m\\sqrt{n})$-time [Dinic 1970; Hopcroft-Karp 1971; Karzanov 1973] and $\\tilde\nO(n^\\omega)$-time algorithms [Ibarra-Moran 1981] (where currently\n$\\omega\\approx 2.373$). On sparser graphs, i.e. when $m = n^{9/8 + \\delta}$ for\nany constant $\\delta>0$, our result improves upon the recent advances of [Madry\n2013] and [Liu-Sidford 2020b, 2020a] which achieve an $\\tilde O(m^{4/3+o(1)})$\nruntime.\n  We obtain these results by combining and advancing recent lines of research\nin interior point methods (IPMs) and dynamic graph algorithms. First, we\nsimplify and improve the IPM of [v.d.Brand-Lee-Sidford-Song 2020], providing a\ngeneral primal-dual IPM framework and new sampling-based techniques for\nhandling infeasibility induced by approximate linear system solvers. Second, we\nprovide a simple sublinear-time algorithm for detecting and sampling\nhigh-energy edges in electric flows on expanders and show that when combined\nwith recent advances in dynamic expander decompositions, this yields efficient\ndata structures for maintaining the iterates of both [v.d.Brand et al.] and our\nnew IPMs. Combining this general machinery yields a simpler $\\tilde O(n\n\\sqrt{m})$ time algorithm for matching based on the logarithmic barrier\nfunction, and our state-of-the-art $\\tilde O(m+n^{1.5})$ time algorithm for\nmatching based on the [Lee-Sidford 2014] barrier (as regularized in [v.d.Brand\net al.]).\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 17:02:24 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Brand", "Jan van den", ""], ["Lee", "Yin-Tat", ""], ["Nanongkai", "Danupon", ""], ["Peng", "Richard", ""], ["Saranurak", "Thatchaphol", ""], ["Sidford", "Aaron", ""], ["Song", "Zhao", ""], ["Wang", "Di", ""]]}, {"id": "2009.01928", "submitter": "Quintino Francesco Lotito", "authors": "Quintino Francesco Lotito and Alberto Montresor", "title": "Efficient Algorithms to Mine Maximal Span-Trusses From Temporal Graphs", "comments": "Published at the 16th International Workshop on Mining and Learning\n  with Graphs (MLG '20)", "journal-ref": "International Workshop on Mining and Learning with Graphs 2020\n  (MLG '20)", "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, there has been an increasing interest in temporal\ngraphs, pushed by a growing availability of temporally-annotated network data\ncoming from social, biological and financial networks. Despite the importance\nof analyzing complex temporal networks, there is a huge gap between the set of\ndefinitions, algorithms and tools available to study large static graphs and\nthe ones available for temporal graphs. An important task in temporal graph\nanalysis is mining dense structures, i.e., identifying high-density subgraphs\ntogether with the span in which this high density is observed. In this paper,\nwe introduce the concept of $(k, \\Delta)$-truss (span-truss) in temporal\ngraphs, a temporal generalization of the $k$-truss, in which $k$ captures the\ninformation about the density and $\\Delta$ captures the time span in which this\ndensity holds. We then propose novel and efficient algorithms to identify\nmaximal span-trusses, namely the ones not dominated by any other span-truss\nneither in the order $k$ nor in the interval $\\Delta$, and evaluate them on a\nnumber of public available datasets.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 21:14:57 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 12:50:27 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Lotito", "Quintino Francesco", ""], ["Montresor", "Alberto", ""]]}, {"id": "2009.01947", "submitter": "Alan Kuhnle", "authors": "Alan Kuhnle", "title": "Nearly Linear-Time, Parallelizable Algorithms for Non-Monotone\n  Submodular Maximization", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study parallelizable algorithms for maximization of a submodular function,\nnot necessarily monotone, with respect to a cardinality constraint $k$. We\nimprove the best approximation factor achieved by an algorithm that has optimal\nadaptivity and query complexity, up to logarithmic factors in the size $n$ of\nthe ground set, from $0.039 - \\epsilon$ to $0.193 - \\epsilon$. We provide two\nalgorithms; the first has approximation ratio $1/6 - \\epsilon$, adaptivity $O(\n\\log n )$, and query complexity $O( n \\log k )$, while the second has\napproximation ratio $0.193 - \\epsilon$, adaptivity $O( \\log^2 n )$, and query\ncomplexity $O(n \\log k)$. Heuristic versions of our algorithms are empirically\nvalidated to use a low number of adaptive rounds and total queries while\nobtaining solutions with high objective value in comparison with\nstate-of-the-art approximation algorithms, including continuous algorithms that\nuse the multilinear extension.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 22:43:55 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 15:38:58 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 18:27:28 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Kuhnle", "Alan", ""]]}, {"id": "2009.01986", "submitter": "Rikhav Shah", "authors": "Rikhav Shah and Sandeep Silwal", "title": "Smoothed analysis of the condition number under low-rank perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $M$ be an arbitrary $n$ by $n$ matrix of rank $n-k$. We study the\ncondition number of $M$ plus a \\emph{low-rank} perturbation $UV^T$ where $U, V$\nare $n$ by $k$ random Gaussian matrices. Under some necessary assumptions, it\nis shown that $M+UV^T$ is unlikely to have a large condition number. The main\nadvantages of this kind of perturbation over the well-studied dense Gaussian\nperturbation, where every entry is independently perturbed, is the $O(nk)$ cost\nto store $U,V$ and the $O(nk)$ increase in time complexity for performing the\nmatrix-vector multiplication $(M+UV^T)x$. This improves the $\\Omega(n^2)$ space\nand time complexity increase required by a dense perturbation, which is\nespecially burdensome if $M$ is originally sparse. Our results also extend to\nthe case where $U$ and $V$ have rank larger than $k$ and to symmetric and\ncomplex settings. We also give an application to linear systems solving and\nperform some numerical experiments. Lastly, barriers in applying low-rank noise\nto other problems studied in the smoothed analysis framework are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 02:46:59 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 21:04:45 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Shah", "Rikhav", ""], ["Silwal", "Sandeep", ""]]}, {"id": "2009.02207", "submitter": "Huy Nguyen", "authors": "Niklas Smedemark-Margulies and Paul Langton and Huy L. Nguyen", "title": "Fair and Useful Cohort Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As important decisions about the distribution of society's resources become\nincreasingly automated, it is essential to consider the measurement and\nenforcement of fairness in these decisions. In this work we build on the\nresults of Dwork and Ilvento ITCS'19, which laid the foundations for the study\nof fair algorithms under composition. In particular, we study the cohort\nselection problem, where we wish to use a fair classifier to select $k$\ncandidates from an arbitrarily ordered set of size $n>k$, while preserving\nindividual fairness and maximizing utility. We define a linear utility function\nto measure performance relative to the behavior of the original classifier. We\ndevelop a fair, utility-optimal $O(n)$-time cohort selection algorithm for the\noffline setting, and our primary result, a solution to the problem in the\nstreaming setting that keeps no more than $O(k)$ pending candidates at all\ntime.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 14:06:08 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Smedemark-Margulies", "Niklas", ""], ["Langton", "Paul", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "2009.02233", "submitter": "Jeffrey Uhlmann", "authors": "Haley Massa and Jeffrey Uhlmann", "title": "Access-Adaptive Priority Search Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that the priority search tree of McCreight, which was\noriginally developed to satisfy a class of spatial search queries on\n2-dimensional points, can be adapted to the problem of dynamically maintaining\na set of keys so that the query complexity adapts to the distribution of\nqueried keys. Presently, the best-known example of such a data structure is the\nsplay tree, which dynamically reconfigures itself during each query so that\nfrequently accessed keys move to the top of the tree and thus can be retrieved\nwith fewer queries than keys that are lower in the tree. However, while the\nsplay tree is conjectured to offer optimal adaptive amortized query complexity,\nit may require O(n) for individual queries. We show that an access-adaptive\npriority search tree (AAPST) can provide competitive adaptive query performance\nwhile ensuring O(log n) worst-case query performance, thus potentially making\nit more suitable for certain interactive (e.g.,online and real-time)\napplications for which the response time must be bounded.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 15:05:49 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Massa", "Haley", ""], ["Uhlmann", "Jeffrey", ""]]}, {"id": "2009.02388", "submitter": "Sebastian U. Stich", "authors": "Sebastian U. Stich", "title": "On Communication Compression for Distributed Optimization on\n  Heterogeneous Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy gradient compression, with either unbiased or biased compressors, has\nbecome a key tool to avoid the communication bottleneck in centrally\ncoordinated distributed training of machine learning models. We analyze the\nperformance of two standard and general types of methods: (i) distributed\nquantized SGD (D-QSGD) with arbitrary unbiased quantizers and (ii) distributed\nSGD with error-feedback and biased compressors (D-EF-SGD) in the heterogeneous\n(non-iid) data setting. Our results indicate that D-EF-SGD is much less\naffected than D-QSGD by non-iid data, but both methods can suffer a slowdown if\ndata-skewness is high. We further study two alternatives that are not (or much\nless) affected by heterogenous data distributions: first, a recently proposed\nmethod that is effective on strongly convex problems, and secondly, we point\nout a more general approach that is applicable to linear compressors only but\neffective in all considered scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 20:48:08 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 09:41:09 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Stich", "Sebastian U.", ""]]}, {"id": "2009.02553", "submitter": "Luo Luo", "authors": "Luo Luo, Cheng Chen, Guangzeng Xie, Haishan Ye", "title": "Revisiting Co-Occurring Directions: Sharper Analysis and Efficient\n  Algorithm for Sparse Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study the streaming model for approximate matrix multiplication (AMM). We\nare interested in the scenario that the algorithm can only take one pass over\nthe data with limited memory. The state-of-the-art deterministic sketching\nalgorithm for streaming AMM is the co-occurring directions (COD), which has\nmuch smaller approximation errors than randomized algorithms and outperforms\nother deterministic sketching methods empirically. In this paper, we provide a\ntighter error bound for COD whose leading term considers the potential\napproximate low-rank structure and the correlation of input matrices. We prove\nCOD is space optimal with respect to our improved error bound. We also propose\na variant of COD for sparse matrices with theoretical guarantees. The\nexperiments on real-world sparse datasets show that the proposed algorithm is\nmore efficient than baseline methods.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 15:35:59 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 06:55:55 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Luo", "Luo", ""], ["Chen", "Cheng", ""], ["Xie", "Guangzeng", ""], ["Ye", "Haishan", ""]]}, {"id": "2009.02584", "submitter": "Maximilian Probst Gutenberg", "authors": "Aaron Bernstein, Maximilian Probst Gutenberg, Thatchaphol Saranurak", "title": "Deterministic Decremental Reachability, SCC, and Shortest Paths via\n  Directed Expanders and Congestion Balancing", "comments": "Reuploaded with some generalizations of previous theorems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G = (V,E,w)$ be a weighted, digraph subject to a sequence of adversarial\nedge deletions. In the decremental single-source reachability problem (SSR), we\nare given a fixed source $s$ and the goal is to maintain a data structure that\ncan answer path-queries $s \\rightarrowtail v$ for any $v \\in V$. In the more\ngeneral single-source shortest paths (SSSP) problem the goal is to return an\napproximate shortest path to $v$, and in the SCC problem the goal is to\nmaintain strongly connected components of $G$ and to answer path queries within\neach component. All of these problems have been very actively studied over the\npast two decades, but all the fast algorithms are randomized and, more\nsignificantly, they can only answer path queries if they assume a weaker model:\nthey assume an oblivious adversary which is not adaptive and must fix the\nupdate sequence in advance. This assumption significantly limits the use of\nthese data structures, most notably preventing them from being used as\nsubroutines in static algorithms. All the above problems are notoriously\ndifficult in the adaptive setting. In fact, the state-of-the-art is still the\nEven and Shiloach tree, which dates back all the way to 1981 and achieves total\nupdate time $O(mn)$. We present the first algorithms to break through this\nbarrier:\n  1) deterministic decremental SSR/SCC with total update time $mn^{2/3 + o(1)}$\n  2) deterministic decremental SSSP with total update time $n^{2+2/3+o(1)}$.\n  To achieve these results, we develop two general techniques of broader\ninterest for working with dynamic graphs: 1) a generalization of expander-based\ntools to dynamic directed graphs, and 2) a technique that we call congestion\nbalancing and which provides a new method for maintaining flow under\nadversarial deletions. Using the second technique, we provide the first\nnear-optimal algorithm for decremental bipartite matching.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 19:11:13 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 07:19:44 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 14:18:34 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Bernstein", "Aaron", ""], ["Gutenberg", "Maximilian Probst", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "2009.02595", "submitter": "Xinyu Wu", "authors": "Ryan O'Donnell, Xinyu Wu", "title": "Explicit near-fully X-Ramanujan graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $p(Y_1, \\dots, Y_d, Z_1, \\dots, Z_e)$ be a self-adjoint noncommutative\npolynomial, with coefficients from $\\mathbb{C}^{r \\times r}$, in the\nindeterminates $Y_1, \\dots, Y_d$ (considered to be self-adjoint), the\nindeterminates $Z_1, \\dots, Z_e$, and their adjoints $Z_1^*, \\dots, Z_e^*$.\nSuppose $Y_1, \\dots, Y_d$ are replaced by independent random $n \\times n$\nmatching matrices, and $Z_1, \\dots, Z_e$ are replaced by independent random $n\n\\times n$ permutation matrices. Assuming for simplicity that $p$'s coefficients\nare $0$-$1$ matrices, the result can be thought of as a kind of random\n$rn$-vertex graph $G$. As $n \\to \\infty$, there will be a natural limiting\ninfinite graph $X$ that covers any finite outcome for $G$. A recent landmark\nresult of Bordenave and Collins shows that for any $\\varepsilon > 0$, with high\nprobability the spectrum of a random $G$ will be $\\varepsilon$-close in\nHausdorff distance to the spectrum of $X$ (once the suitably defined \"trivial\"\neigenvalues are excluded). We say that $G$ is \"$\\varepsilon$-near fully\n$X$-Ramanujan\". Our work has two contributions: First we study and clarify the\nclass of infinite graphs $X$ that can arise in this way. Second, we derandomize\nthe Bordenave-Collins result: for any $X$, we provide explicit, arbitrarily\nlarge graphs $G$ that are covered by $X$ and that have (nontrivial) spectrum at\nHausdorff distance at most $\\varepsilon$ from that of $X$. This significantly\ngeneralizes the recent work of Mohanty et al., which provided explicit\nnear-Ramanujan graphs for every degree $d$ (meaning $d$-regular graphs with all\nnontrivial eigenvalues bounded in magnitude by $2\\sqrt{d-1} + \\varepsilon$). As\nan application of our main technical theorem, we are also able to determine the\n\"eigenvalue relaxation value\" for a wide class of average-case degree-$2$\nconstraint satisfaction problems.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 20:36:16 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["O'Donnell", "Ryan", ""], ["Wu", "Xinyu", ""]]}, {"id": "2009.02618", "submitter": "Xin Hong", "authors": "Xin Hong, Xiangzhen Zhou, Sanjiang Li, Yuan Feng, Mingsheng Ying", "title": "A Tensor Network based Decision Diagram for Representation of Quantum\n  Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor networks have been successfully applied in simulation of quantum\nphysical systems for decades. Recently, they have also been employed in\nclassical simulation of quantum computing, in particular, random quantum\ncircuits. This paper proposes a decision-diagram style data structure, called\nTDD (Tensor Decision Diagram), for more principled and convenient applications\nof tensor networks. This new data structure provides a compact and canonical\nrepresentation for quantum circuits. By exploiting circuit partition, the TDD\nof a quantum circuit can be computed efficiently. Furthermore, we show that the\noperations of tensor networks essential in their applications (e.g., addition\nand contraction), can also be implemented efficiently in TDDs. A\nproof-of-concept implementation of TDDs is presented and its efficiency is\nevaluated on a set of benchmark quantum circuits. It is expected that TDDs will\nplay an important role in various design automation tasks related to quantum\ncircuits, including but not limited to equivalence checking, error detection,\nsynthesis, simulation, and verification.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 00:12:31 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Hong", "Xin", ""], ["Zhou", "Xiangzhen", ""], ["Li", "Sanjiang", ""], ["Feng", "Yuan", ""], ["Ying", "Mingsheng", ""]]}, {"id": "2009.02668", "submitter": "Jalaj Upadhyay", "authors": "Jalaj Upadhyay, Sarvagya Upadhyay", "title": "A Framework for Private Matrix Analysis", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study private matrix analysis in the sliding window model where only the\nlast $W$ updates to matrices are considered useful for analysis. We give first\nefficient $o(W)$ space differentially private algorithms for spectral\napproximation, principal component analysis, and linear regression. We also\ninitiate and show efficient differentially private algorithms for two important\nvariants of principal component analysis: sparse principal component analysis\nand non-negative principal component analysis. Prior to our work, no such\nresult was known for sparse and non-negative differentially private principal\ncomponent analysis even in the static data setting. These algorithms are\nobtained by identifying sufficient conditions on positive semidefinite matrices\nformed from streamed matrices. We also show a lower bound on space required to\ncompute low-rank approximation even if the algorithm gives multiplicative\napproximation and incurs additive error. This follows via reduction to a\ncertain communication complexity problem.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 08:01:59 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Upadhyay", "Jalaj", ""], ["Upadhyay", "Sarvagya", ""]]}, {"id": "2009.02702", "submitter": "Jens Schneider", "authors": "Khaled Al-Thelaya and Marco Agus and Jens Schneider", "title": "The Mixture Graph-A Data Structure for Compressing, Rendering, and\n  Querying Segmentation Histograms", "comments": "To appear in IEEE Transacations on Visualization and Computer\n  Graphics (IEEE Vis 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel data structure, called the Mixture Graph.\nThis data structure allows us to compress, render, and query segmentation\nhistograms. Such histograms arise when building a mipmap of a volume containing\nsegmentation IDs. Each voxel in the histogram mipmap contains a convex\ncombination (mixture) of segmentation IDs. Each mixture represents the\ndistribution of IDs in the respective voxel's children. Our method factorizes\nthese mixtures into a series of linear interpolations between exactly two\nsegmentation IDs. The result is represented as a directed acyclic graph (DAG)\nwhose nodes are topologically ordered. Pruning replicate nodes in the tree\nfollowed by compression allows us to store the resulting data structure\nefficiently. During rendering, transfer functions are propagated from sources\n(leafs) through the DAG to allow for efficient, pre-filtered rendering at\ninteractive frame rates. Assembly of histogram contributions across the\nfootprint of a given volume allows us to efficiently query partial histograms,\nachieving up to 178$\\times$ speed-up over na$\\mathrm{\\\"{i}}$ve parallelized\nrange queries. Additionally, we apply the Mixture Graph to compute correctly\npre-filtered volume lighting and to interactively explore segments based on\nshape, geometry, and orientation using multi-dimensional transfer functions.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 10:40:01 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Al-Thelaya", "Khaled", ""], ["Agus", "Marco", ""], ["Schneider", "Jens", ""]]}, {"id": "2009.02710", "submitter": "Niloufar Ahmadypour", "authors": "Niloufar Ahmadypour and Amin Gohari", "title": "Multi-Way Number Partitioning: an Information-Theoretic View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number partitioning problem is the problem of partitioning a given list\nof numbers into multiple subsets so that the sum of the numbers in each subset\nare as nearly equal as possible. We introduce two closely related notions of\nthe \"most informative\" and \"most compressible\" partitions. Most informative\npartitions satisfy a principle of optimality property. We also give an exact\nalgorithm (based on Huffman coding) with a running time of O(nlog(n)) in input\nsize n to find the most compressible partition.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 11:12:02 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Ahmadypour", "Niloufar", ""], ["Gohari", "Amin", ""]]}, {"id": "2009.02778", "submitter": "Karthik C. S.", "authors": "Karthik C. S. and Inbal Livni-Navon", "title": "On Hardness of Approximation of Parameterized Set Cover and Label Cover:\n  Threshold Graphs from Error Correcting Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $(k,h)$-SetCover problem, we are given a collection $\\mathcal{S}$ of\nsets over a universe $U$, and the goal is to distinguish between the case that\n$\\mathcal{S}$ contains $k$ sets which cover $U$, from the case that at least\n$h$ sets in $\\mathcal{S}$ are needed to cover $U$. Lin (ICALP'19) recently\nshowed a gap creating reduction from the $(k,k+1)$-SetCover problem on universe\nof size $O_k(\\log |\\mathcal{S}|)$ to the\n$\\left(k,\\sqrt[k]{\\frac{\\log|\\mathcal{S}|}{\\log\\log |\\mathcal{S}|}}\\cdot\nk\\right)$-SetCover problem on universe of size $|\\mathcal{S}|$. In this paper,\nwe prove a more scalable version of his result: given any error correcting code\n$C$ over alphabet $[q]$, rate $\\rho$, and relative distance $\\delta$, we use\n$C$ to create a reduction from the $(k,k+1)$-SetCover problem on universe $U$\nto the $\\left(k,\\sqrt[2k]{\\frac{2}{1-\\delta}}\\right)$-SetCover problem on\nuniverse of size $\\frac{\\log|\\mathcal{S}|}{\\rho}\\cdot|U|^{q^k}$.\n  Lin established his result by composing the input SetCover instance (that has\nno gap) with a special threshold graph constructed from extremal combinatorial\nobject called universal sets, resulting in a final SetCover instance with gap.\nOur reduction follows along the exact same lines, except that we generate the\nthreshold graphs specified by Lin simply using the basic properties of the\nerror correcting code $C$.\n  We use the same threshold graphs mentioned above to prove inapproximability\nresults, under W[1]$\\neq$FPT and ETH, for the $k$-MaxCover problem introduced\nby Chalermsook et al. (SICOMP'20). Our inapproximaiblity results match the\nbounds obtained by Karthik et al. (JACM'19), although their proof framework is\nvery different, and involves generalization of the distributed PCP framework.\nPrior to this work, it was not clear how to adopt the proof strategy of Lin to\nprove inapproximability results for $k$-MaxCover.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 17:13:48 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["S.", "Karthik C.", ""], ["Livni-Navon", "Inbal", ""]]}, {"id": "2009.03038", "submitter": "Raghuvansh Saxena", "authors": "Sepehr Assadi, Gillat Kol, Raghuvansh R. Saxena, Huacheng Yu", "title": "Multi-Pass Graph Streaming Lower Bounds for Cycle Counting, MAX-CUT,\n  Matching Size, and Other Problems", "comments": "Fixed a mistake in one of the technical lemmas, see Section 1.4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following gap cycle counting problem in the streaming model: The\nedges of a $2$-regular $n$-vertex graph $G$ are arriving one-by-one in a stream\nand we are promised that $G$ is a disjoint union of either $k$-cycles or\n$2k$-cycles for some small $k$; the goal is to distinguish between these two\ncases. Verbin and Yu [SODA 2011] introduced this problem and showed that any\nsingle-pass streaming algorithm solving it requires $n^{1-\\Omega(\\frac{1}{k})}$\nspace. This result and the technique behind it -- the Boolean Hidden\nHypermatching communication problem -- has since been used extensively for\nproving streaming lower bounds for various problems.\n  Despite its significance and broad range of applications, the lower bound\ntechnique of Verbin and Yu comes with a key weakness that is inherited by all\nsubsequent results: the Boolean Hidden Hypermatching problem is hard only if\nthere is exactly one round of communication and can be solved with logarithmic\ncommunication in two rounds. Therefore, all streaming lower bounds derived from\nthis problem only hold for single-pass algorithms.\n  We prove the first multi-pass lower bound for the gap cycle counting problem:\nAny $p$-pass streaming algorithm that can distinguish between disjoint union of\n$k$-cycles vs $2k$-cycles -- or even $k$-cycles vs one Hamiltonian cycle --\nrequires $n^{1-\\frac{1}{k^{\\Omega(1/p)}}}$ space. As a corollary of this\nresult, we can extend many of previous lower bounds to multi-pass algorithms.\nFor instance, we can now prove that any streaming algorithm that\n$(1+\\epsilon)$-approximates the value of MAX-CUT, maximum matching size, or\nrank of an $n$-by-$n$ matrix, requires either $n^{\\Omega(1)}$ space or\n$\\Omega(\\log{(\\frac{1}{\\epsilon})})$ passes. For all these problems, prior work\nleft open the possibility of even an $O(\\log{n})$ space algorithm in only two\npasses.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 12:01:38 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 15:34:00 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Assadi", "Sepehr", ""], ["Kol", "Gillat", ""], ["Saxena", "Raghuvansh R.", ""], ["Yu", "Huacheng", ""]]}, {"id": "2009.03052", "submitter": "Stefano Leucci", "authors": "Marco Bressan, Stefano Leucci, Alessandro Panconesi", "title": "Faster motif counting via succinct color coding and adaptive sampling", "comments": null, "journal-ref": "ACM Trans. Knowl. Discov. Data 15, 6, Article 96 (June 2021)", "doi": "10.1145/3447397", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of computing the distribution of induced connected\nsubgraphs, aka \\emph{graphlets} or \\emph{motifs}, in large graphs. The current\nstate-of-the-art algorithms estimate the motif counts via uniform sampling, by\nleveraging the color coding technique by Alon, Yuster and Zwick. In this work\nwe extend the applicability of this approach, by introducing a set of\nalgorithmic optimizations and techniques that reduce the running time and space\nusage of color coding and improve the accuracy of the counts. To this end, we\nfirst show how to optimize color coding to efficiently build a compact table of\na representative subsample of all graphlets in the input graph. For $8$-node\nmotifs, we can build such a table in one hour for a graph with $65$M nodes and\n$1.8$B edges, which is $2000$ times larger than the state of the art. We then\nintroduce a novel adaptive sampling scheme that breaks the \"additive error\nbarrier\" of uniform sampling, guaranteeing multiplicative approximations\ninstead of just additive ones. This allows us to count not only the most\nfrequent motifs, but also extremely rare ones. For instance, on one graph we\naccurately count nearly $10.000$ distinct $8$-node motifs whose relative\nfrequency is so small that uniform sampling would literally take centuries to\nfind them. Our results show that color coding is still the most promising\napproach to scalable motif counting.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 11:54:19 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 17:21:23 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Bressan", "Marco", ""], ["Leucci", "Stefano", ""], ["Panconesi", "Alessandro", ""]]}, {"id": "2009.03076", "submitter": "Stefan Zellmann", "authors": "Ingo Wald and Stefan Zellmann and Will Usher and Nate Morrical and\n  Ulrich Lang and Valerio Pascucci", "title": "Ray Tracing Structured AMR Data Using ExaBricks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured Adaptive Mesh Refinement (Structured AMR) enables simulations to\nadapt the domain resolution to save computation and storage, and has become one\nof the dominant data representations used by scientific simulations; however,\nefficiently rendering such data remains a challenge. We present an efficient\napproach for volume- and iso-surface ray tracing of Structured AMR data on\nGPU-equipped workstations, using a combination of two different data\nstructures. Together, these data structures allow a ray tracing based renderer\nto quickly determine which segments along the ray need to be integrated and at\nwhat frequency, while also providing quick access to all data values required\nfor a smooth sample reconstruction kernel. Our method makes use of the RTX ray\ntracing hardware for surface rendering, ray marching, space skipping, and\nadaptive sampling; and allows for interactive changes to the transfer function\nand implicit iso-surfacing thresholds. We demonstrate that our method achieves\nhigh performance with little memory overhead, enabling interactive high quality\nrendering of complex AMR data sets on individual GPU workstations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 13:03:40 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Wald", "Ingo", ""], ["Zellmann", "Stefan", ""], ["Usher", "Will", ""], ["Morrical", "Nate", ""], ["Lang", "Ulrich", ""], ["Pascucci", "Valerio", ""]]}, {"id": "2009.03078", "submitter": "Anna Arutyunova", "authors": "Anna Arutyunova, Melanie Schmidt", "title": "Achieving anonymity via weak lower bound constraints for k-median and\n  k-means", "comments": "Improved structuring of the main proofs and some minor changes\n  throughout the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study $k$-clustering problems with lower bounds, including $k$-median and\n$k$-means clustering with lower bounds. In addition to the point set $P$ and\nthe number of centers $k$, a $k$-clustering problem with (uniform) lower bounds\ngets a number $B$. The solution space is restricted to clusterings where every\ncluster has at least $B$ points. We demonstrate how to approximate $k$-median\nwith lower bounds via a reduction to facility location with lower bounds, for\nwhich $O(1)$-approximation algorithms are known.\n  Then we propose a new constrained clustering problem with lower bounds where\nwe allow points to be assigned multiple times (to different centers). This\nmeans that for every point, the clustering specifies a set of centers to which\nit is assigned. We call this clustering with weak lower bounds. We give an\n$8$-approximation for $k$-median clustering with weak lower bounds and an\n$O(1)$-approximation for $k$-means with weak lower bounds. We conclude by\nshowing that at a constant increase in the approximation factor, we can\nrestrict the number of assignments of every point to $2$ (or, if we allow\nfractional assignments, to $1+\\epsilon$). This also leads to the first\nbicritera approximation algorithm for $k$-means with (standard) lower bounds\nwhere bicriteria is interpreted in the sense that the lower bounds are violated\nby a constant factor. All algorithms in this paper run in time that is\npolynomial in $n$ and $k$ (and $d$ for the Euclidean variants considered).\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 13:08:19 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 13:47:10 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Arutyunova", "Anna", ""], ["Schmidt", "Melanie", ""]]}, {"id": "2009.03158", "submitter": "Yuya Sasaki", "authors": "Yuya Sasaki, Yasuhiro Fujiwara, Makoto Onizuka", "title": "Efficient Network Reliability Computation in Uncertain Graphs", "comments": null, "journal-ref": "EDBT2019", "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network reliability is an important metric to evaluate the connectivity among\ngiven vertices in uncertain graphs. Since the network reliability problem is\nknown as #P-complete, existing studies have used approximation techniques. In\nthis paper, we propose a new sampling-based approach that efficiently and\naccurately approximates network reliability. Our approach improves efficiency\nby reducing the number of samples based on stratified sampling. We\ntheoretically guarantee that our approach improves the accuracy of\napproximation by using lower and upper bounds of network reliability, even\nthough it reduces the number of samples. To efficiently compute the bounds, we\ndevelop an extended BDD, called S2BDD. During constructing the S2BDD, our\napproach employs dynamic programming for efficiently sampling possible graphs.\nOur experiment with real datasets demonstrates that our approach is up to 51.2\ntimes faster than the existing sampling-based approach with higher accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 07:08:29 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Sasaki", "Yuya", ""], ["Fujiwara", "Yasuhiro", ""], ["Onizuka", "Makoto", ""]]}, {"id": "2009.03242", "submitter": "Rolf Drechsler", "authors": "Rolf Drechsler", "title": "PolyAdd: Polynomial Formal Verification of Adder Circuits", "comments": "7 pages, 8 figures, published at 24th International Symposium on\n  Design and Diagnostics of Electronic Circuits and Systems (DDECS), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Only by formal verification approaches functional correctness can be ensured.\nWhile for many circuits fast verification is possible, in other cases the\napproaches fail. In general no efficient algorithms can be given, since the\nunderlying verification problem is NP-complete. In this paper we prove that for\ndifferent types of adder circuits polynomial verification can be ensured based\non BDDs. While it is known that the output functions for addition are\npolynomially bounded, we show in the following that the entire construction\nprocess can be carried out in polynomial time. This is shown for the simple\nRipple Carry Adder, but also for fast adders like the Conditional Sum Adder and\nthe Carry Look Ahead Adder. Properties about the adder function are proven and\nthe core principle of polynomial verification is described that can also be\nextended to other classes of functions and circuit realizations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 17:10:21 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 08:51:26 GMT"}, {"version": "v3", "created": "Sat, 3 Apr 2021 10:20:50 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Drechsler", "Rolf", ""]]}, {"id": "2009.03260", "submitter": "Tarun Kathuria", "authors": "Tarun Kathuria", "title": "A Potential Reduction Inspired Algorithm for Exact Max Flow in Almost\n  $\\widetilde{O}(m^{4/3})$ Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for computing $s$-$t$ maximum flows in directed\ngraphs in $\\widetilde{O}(m^{4/3+o(1)}U^{1/3})$ time. Our algorithm is inspired\nby potential reduction interior point methods for linear programming. Instead\nof using scaled gradient/Newton steps of a potential function, we take the step\nwhich maximizes the decrease in the potential value subject to advancing a\ncertain amount on the central path, which can be efficiently computed. This\nallows us to trace the central path with our progress depending only\n$\\ell_\\infty$ norm bounds on the congestion vector (as opposed to the $\\ell_4$\nnorm required by previous works) and runs in $O(\\sqrt{m})$ iterations. To\nimprove the number of iterations by establishing tighter bounds on the\n$\\ell_\\infty$ norm, we then consider the weighted central path framework of\nMadry \\cite{M13,M16,CMSV17} and Liu-Sidford \\cite{LS20}. Instead of changing\nweights to maximize energy, we consider finding weights which maximize the\nmaximum decrease in potential value. Finally, similar to finding weights which\nmaximize energy as done in \\cite{LS20} this problem can be solved by the\niterative refinement framework for smoothed $\\ell_2$-$\\ell_p$ norm flow\nproblems \\cite{KPSW19} completing our algorithm. We believe our potential\nreduction based viewpoint provides a versatile framework which may lead to\nfaster algorithms for max flow.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 17:31:24 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Kathuria", "Tarun", ""]]}, {"id": "2009.03352", "submitter": "Jin Cao", "authors": "Jin Cao and Dewei Zhong", "title": "A Fast Randomized Algorithm for Finding the Maximal Common Subsequences", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the common subsequences of $L$ multiple strings has many applications\nin the area of bioinformatics, computational linguistics, and information\nretrieval. A well-known result states that finding a Longest Common Subsequence\n(LCS) for $L$ strings is NP-hard, e.g., the computational complexity is\nexponential in $L$. In this paper, we develop a randomized algorithm, referred\nto as {\\em Random-MCS}, for finding a random instance of Maximal Common\nSubsequence ($MCS$) of multiple strings. A common subsequence is {\\em maximal}\nif inserting any character into the subsequence no longer yields a common\nsubsequence. A special case of MCS is LCS where the length is the longest. We\nshow the complexity of our algorithm is linear in $L$, and therefore is\nsuitable for large $L$. Furthermore, we study the occurrence probability for a\nsingle instance of MCS and demonstrate via both theoretical and experimental\nstudies that the longest subsequence from multiple runs of {\\em Random-MCS}\noften yields a solution to $LCS$.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 18:12:58 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Cao", "Jin", ""], ["Zhong", "Dewei", ""]]}, {"id": "2009.03358", "submitter": "Jin Cao", "authors": "Jin Cao and Yibo Zhao and Linjun Zhang and Jason Li", "title": "A Lightweight Algorithm to Uncover Deep Relationships in Data Tables", "comments": "9 pages, 4 figures, paper presented on AutoML 2019 (The Third\n  International Workshop on Automation in Machine Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data we collect today are in tabular form, with rows as records and\ncolumns as attributes associated with each record. Understanding the structural\nrelationship in tabular data can greatly facilitate the data science process.\nTraditionally, much of this relational information is stored in table schema\nand maintained by its creators, usually domain experts. In this paper, we\ndevelop automated methods to uncover deep relationships in a single data table\nwithout expert or domain knowledge. Our method can decompose a data table into\nlayers of smaller tables, revealing its deep structure. The key to our approach\nis a computationally lightweight forward addition algorithm that we developed\nto recursively extract the functional dependencies between table columns that\nare scalable to tables with many columns. With our solution, data scientists\nwill be provided with automatically generated, data-driven insights when\nexploring new data sets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 18:25:15 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Cao", "Jin", ""], ["Zhao", "Yibo", ""], ["Zhang", "Linjun", ""], ["Li", "Jason", ""]]}, {"id": "2009.03416", "submitter": "Alan Frieze", "authors": "Alan Frieze and Tomasz Tkocz", "title": "Probabilistic analysis of algorithms for cost constrained minimum\n  weighted combinatorial objects", "comments": "8 pages", "journal-ref": "Oper. Res. Lett. 49 (2021), no. 3, 400-404", "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider cost constrained versions of the minimum spanning tree problem\nand the assignment problem. We assume edge weights are independent copies of a\ncontinuous random variable $Z$ that satisfies $F(x)=\\Pr(Z\\leq x)\\approx\nx^\\alpha$ as $x\\to0$, where $\\alpha\\geq 1$. Also, there are $r=O(1)$ budget\nconstraints with edge costs chosen from the same distribution. We use\nLagrangean duality to construct polynomial time algorithms that produce\nasymptotically optimal solutions. For the spanning tree problem, we allow\n$r>1$, but for the assignment problem we can only analyse the case $r=1$.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 20:54:41 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Frieze", "Alan", ""], ["Tkocz", "Tomasz", ""]]}, {"id": "2009.03675", "submitter": "Giovanni Manzini", "authors": "Lavinia Egidi, Felipe A. Louza, Giovanni Manzini", "title": "Space efficient merging of de Bruijn graphs and Wheeler graphs", "comments": "24 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:1902.02889", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The merging of succinct data structures is a well established technique for\nthe space efficient construction of large succinct indexes. In the first part\nof the paper we propose a new algorithm for merging succinct representations of\nde Bruijn graphs. Our algorithm has the same asymptotic cost of the state of\nthe art algorithm for the same problem but it uses less than half of its\nworking space. A novel important feature of our algorithm, not found in any of\nthe existing tools, is that it can compute the Variable Order succinct\nrepresentation of the union graph within the same asymptotic time/space bounds.\nIn the second part of the paper we consider the more general problem of merging\nsuccinct representations of Wheeler graphs, a recently introduced graph family\nwhich includes as special cases de Bruijn graphs and many other known succinct\nindexes based on the BWT or one of its variants. We show that Wheeler graphs\nmerging is in general a much more difficult problem, and we provide a space\nefficient algorithm for the slightly simplified problem of determining whether\nthe union graph has an ordering that satisfies the Wheeler conditions.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 19:09:41 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 09:49:38 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Egidi", "Lavinia", ""], ["Louza", "Felipe A.", ""], ["Manzini", "Giovanni", ""]]}, {"id": "2009.03687", "submitter": "Yasuaki Kobayashi", "authors": "Tesshu Hanaka, Yasuaki Kobayashi, Kazuhiro Kurita, Yota Otachi", "title": "Finding Diverse Trees, Paths, and More", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical modeling is a standard approach to solve many real-world\nproblems and {\\em diversity} of solutions is an important issue, emerging in\napplying solutions obtained from mathematical models to real-world problems.\nMany studies have been devoted to finding diverse solutions. Baste et al.\n(Algorithms 2019, IJCAI 2020) recently initiated the study of computing diverse\nsolutions of combinatorial problems from the perspective of fixed-parameter\ntractability. They considered problems of finding $r$ solutions that maximize\nsome diversity measures (the minimum or sum of the pairwise Hamming distances\namong them) and gave some fixed-parameter tractable algorithms for the diverse\nversion of several well-known problems, such as {\\sc Vertex Cover}, {\\sc\nFeedback Vertex Set}, {\\sc $d$-Hitting Set}, and problems on bounded-treewidth\ngraphs. In this work, we investigate the (fixed-parameter) tractability of\nproblems of finding diverse spanning trees, paths, and several subgraphs. In\nparticular, we show that, given a graph $G$ and an integer $r$, the problem of\ncomputing $r$ spanning trees of $G$ maximizing the sum of the pairwise Hamming\ndistances among them can be solved in polynomial time. To the best of the\nauthors' knowledge, this is the first polynomial-time solvable case for finding\ndiverse solutions of unbounded size.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 12:32:05 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 03:33:51 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Hanaka", "Tesshu", ""], ["Kobayashi", "Yasuaki", ""], ["Kurita", "Kazuhiro", ""], ["Otachi", "Yota", ""]]}, {"id": "2009.04013", "submitter": "Wanrong Zhang", "authors": "Wanrong Zhang, Olga Ohrimenko, Rachel Cummings", "title": "Attribute Privacy: Framework and Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring the privacy of training data is a growing concern since many machine\nlearning models are trained on confidential and potentially sensitive data.\nMuch attention has been devoted to methods for protecting individual privacy\nduring analyses of large datasets. However in many settings, global properties\nof the dataset may also be sensitive (e.g., mortality rate in a hospital rather\nthan presence of a particular patient in the dataset). In this work, we depart\nfrom individual privacy to initiate the study of attribute privacy, where a\ndata owner is concerned about revealing sensitive properties of a whole dataset\nduring analysis. We propose definitions to capture \\emph{attribute privacy} in\ntwo relevant cases where global attributes may need to be protected: (1)\nproperties of a specific dataset and (2) parameters of the underlying\ndistribution from which dataset is sampled. We also provide two efficient\nmechanisms and one inefficient mechanism that satisfy attribute privacy for\nthese settings. We base our results on a novel use of the Pufferfish framework\nto account for correlations across attributes in the data, thus addressing \"the\nchallenging problem of developing Pufferfish instantiations and algorithms for\ngeneral aggregate secrets\" that was left open by \\cite{kifer2014pufferfish}.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 22:38:57 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 23:23:04 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Zhang", "Wanrong", ""], ["Ohrimenko", "Olga", ""], ["Cummings", "Rachel", ""]]}, {"id": "2009.04114", "submitter": "Yuhao Zhang", "authors": "Zhiyi Huang, Qiankun Zhang, Yuhao Zhang", "title": "Adwords in a Panorama", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three decades ago, Karp, Vazirani, and Vazirani (STOC 1990) defined the\nonline matching problem and gave an optimal $1-\\frac{1}{e} \\approx\n0.632$-competitive algorithm. Fifteen years later, Mehta, Saberi, Vazirani, and\nVazirani (FOCS 2005) introduced the first generalization called AdWords driven\nby online advertising and obtained the optimal $1-\\frac{1}{e}$ competitive\nratio in the special case of small bids. It has been open ever since whether\nthere is an algorithm for general bids better than the $0.5$-competitive greedy\nalgorithm. This paper presents a $0.5016$-competitive algorithm for AdWords,\nanswering this open question on the positive end. The algorithm builds on\nseveral ingredients, including a combination of the online primal dual\nframework and the configuration linear program of matching problems recently\nexplored by Huang and Zhang (STOC 2020), a novel formulation of AdWords which\nwe call the panorama view, and a generalization of the online correlated\nselection by Fahrbach, Huang, Tao, and Zadimorghaddam (FOCS 2020) which we call\nthe panoramic online correlated selection.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 05:57:36 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 06:59:34 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2020 13:03:13 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Huang", "Zhiyi", ""], ["Zhang", "Qiankun", ""], ["Zhang", "Yuhao", ""]]}, {"id": "2009.04556", "submitter": "Samson Zhou", "authors": "Yuichi Yoshida, Samson Zhou", "title": "Sensitivity Analysis of the Maximum Matching Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the sensitivity of algorithms for the maximum matching problem\nagainst edge and vertex modifications. Algorithms with low sensitivity are\ndesirable because they are robust to edge failure or attack. In this work, we\nshow a randomized $(1-\\epsilon)$-approximation algorithm with worst-case\nsensitivity $O_{\\epsilon}(1)$, which substantially improves upon the\n$(1-\\epsilon)$-approximation algorithm of Varma and Yoshida (arXiv 2020) that\nobtains average sensitivity $n^{O(1/(1+\\epsilon^2))}$ sensitivity algorithm,\nand show a deterministic $1/2$-approximation algorithm with sensitivity\n$\\exp(O(\\log^*n))$ for bounded-degree graphs. We show that any deterministic\nconstant-factor approximation algorithm must have sensitivity $\\Omega(\\log^*\nn)$. Our results imply that randomized algorithms are strictly more powerful\nthan deterministic ones in that the former can achieve sensitivity independent\nof $n$ whereas the latter cannot. We also show analogous results for vertex\nsensitivity, where we remove a vertex instead of an edge. As an application of\nour results, we give an algorithm for the online maximum matching with\n$O_{\\epsilon}(n)$ total replacements in the vertex-arrival model. By\ncomparison, Bernstein et al. (J. ACM 2019) gave an online algorithm that always\noutputs the maximum matching, but only for bipartite graphs and with $O(n\\log\nn)$ total replacements.\n  Finally, we introduce the notion of normalized weighted sensitivity, a\nnatural generalization of sensitivity that accounts for the weights of deleted\nedges. We show that if all edges in a graph have polynomially bounded weight,\nthen given a trade-off parameter $\\alpha>2$, there exists an algorithm that\noutputs a $\\frac{1}{4\\alpha}$-approximation to the maximum weighted matching in\n$O(m\\log_{\\alpha} n)$ time, with normalized weighted sensitivity $O(1)$. See\npaper for full abstract.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 20:27:34 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Yoshida", "Yuichi", ""], ["Zhou", "Samson", ""]]}, {"id": "2009.04567", "submitter": "Lars Jaffke", "authors": "Fedor V. Fomin, Petr A. Golovach, Lars Jaffke, Geevarghese Philip, and\n  Danil Sagunov", "title": "Diverse Pairs of Matchings", "comments": "To appear at ISAAC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of the Diverse Pair of (Maximum/ Perfect) Matchings\nproblems which given a graph $G$ and an integer $k$, ask whether $G$ has two\n(maximum/perfect) matchings whose symmetric difference is at least $k$. Diverse\nPair of Matchings (asking for two not necessarily maximum or perfect matchings)\nis NP-complete on general graphs if $k$ is part of the input, and we consider\ntwo restricted variants. First, we show that on bipartite graphs, the problem\nis polynomial-time solvable, and second we show that Diverse Pair of Maximum\nMatchings is FPT parameterized by $k$. We round off the work by showing that\nDiverse Pair of Matchings has a kernel on $\\mathcal{O}(k^2)$ vertices.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 20:58:59 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Jaffke", "Lars", ""], ["Philip", "Geevarghese", ""], ["Sagunov", "Danil", ""]]}, {"id": "2009.04636", "submitter": "Farhad Shahrokhi", "authors": "Jonathan S. Li, Rohan Potru, Farhad Shahrokhi", "title": "A performance study of some approximation algorithms for minimum\n  dominating set in a graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implement and test the performances of several approximation algorithms\nfor computing the minimum dominating set of a graph. These algorithms are the\nstandard greedy algorithm, the recent LP rounding algorithms and a hybrid\nalgorithm that we design by combining the greedy and LP rounding algorithms.\nAll algorithms perform better than anticipated in their theoretical analysis,\nand have small performance ratios, measured as the size of output divided by\nthe LP objective lower-bound. However, each may have advantages over the\nothers. For instance, LP rounding algorithm normally outperforms the other\nalgorithms on sparse real-world graphs. On a graph with 400,000+ vertices, LP\nrounding took less than 15 seconds of CPU time to generate a solution with\nperformance ratio 1.011, while the greedy and hybrid algorithms generated\nsolutions of performance ratio 1.12 in similar time. For synthetic graphs, the\nhybrid algorithm normally outperforms the others, whereas for hypercubes and\nk-Queens graphs, greedy outperforms the rest. Another advantage of the hybrid\nalgorithm is to solve very large problems where LP solvers crash, as\ndemonstrated on a real-world graph with 7.7 million+ vertices.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 02:17:07 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Li", "Jonathan S.", ""], ["Potru", "Rohan", ""], ["Shahrokhi", "Farhad", ""]]}, {"id": "2009.04742", "submitter": "Andrei Nikolaev", "authors": "Alexander V. Korostil and Andrei V. Nikolaev", "title": "Backtracking algorithms for constructing the Hamiltonian decomposition\n  of a 4-regular multigraph", "comments": "In Russian. Computational experiments are revised", "journal-ref": null, "doi": "10.18255/1818-1015-2021-1-6-21", "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Hamiltonian decomposition problem of partitioning a regular\nmultigraph into edge-disjoint Hamiltonian cycles. It is known that verifying\nvertex nonadjacency in the 1-skeleton of the symmetric and asymmetric traveling\nsalesperson polytopes is an NP-complete problem. On the other hand, a\nsufficient condition for two vertices to be nonadjacent can be formulated as a\ncombinatorial problem of finding a Hamiltonian decomposition of a 4-regular\nmultigraph. We present two backtracking algorithms for verifying vertex\nnonadjacency in the 1-skeleton of the traveling salesperson polytope and\nconstructing a Hamiltonian decomposition: an algorithm based on a simple path\nextension and an algorithm based on the chain edge fixing procedure. According\nto the results of computational experiments for undirected multigraphs, both\nbacktracking algorithms lost to the known general variable neighborhood search\nalgorithm. However, for directed multigraphs, the algorithm based on chain edge\nfixing showed comparable results with heuristics on instances with the existing\nsolution and better results on instances of the problem where the Hamiltonian\ndecomposition does not exist.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 09:13:57 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 11:13:29 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Korostil", "Alexander V.", ""], ["Nikolaev", "Andrei V.", ""]]}, {"id": "2009.04827", "submitter": "Liam Jordon", "authors": "Liam Jordon, Philippe Moser", "title": "A Normal Sequence Compressed by PPM$^*$ but not by Lempel-Ziv 78", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-67731-2_28", "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we compare the difference in performance of two of the\nPrediction by Partial Matching (PPM) family of compressors (PPM$^*$ and the\noriginal Bounded PPM algorithm) and the Lempel-Ziv 78 (LZ) algorithm. We\nconstruct an infinite binary sequence whose worst-case compression ratio for\nPPM$^*$ is $0$, while Bounded PPM's and LZ's best-case compression ratios are\nat least $1/2$ and $1$ respectively. This sequence is an enumeration of all\nbinary strings in order of length, i.e. all strings of length $1$ followed by\nall strings of length $2$ and so on. It is therefore normal, and is built using\nrepetitions of de Bruijn strings of increasing order\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 12:56:00 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Jordon", "Liam", ""], ["Moser", "Philippe", ""]]}, {"id": "2009.04942", "submitter": "L\\'aszl\\'o V\\'egh", "authors": "Daniel Dadush, Bento Natura, L\\'aszl\\'o A. V\\'egh", "title": "Revisiting Tardos's Framework for Linear Programming: Faster Exact\n  Solutions using Approximate Solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In breakthrough work, Tardos (Oper. Res. '86) gave a proximity based\nframework for solving linear programming (LP) in time depending only on the\nconstraint matrix in the bit complexity model. In Tardos's framework, one\nreduces solving the LP $\\min \\langle c,{x}\\rangle$, $Ax=b$, $x \\geq 0$, $A \\in\n\\mathbb{Z}^{m \\times n}$, to solving $O(nm)$ LPs in $A$ having small integer\ncoefficient objectives and right-hand sides using any exact LP algorithm. This\ngives rise to an LP algorithm in time poly$(n,m\\log\\Delta_A)$, where $\\Delta_A$\nis the largest subdeterminant of $A$. A significant extension to the real model\nof computation was given by Vavasis and Ye (Math. Prog. '96), giving a\nspecialized interior point method that runs in time poly$(n,m,\\log\\bar\\chi_A)$,\ndepending on Stewart's $\\bar{\\chi}_A$, a well-studied condition number.\n  In this work, we extend Tardos's original framework to obtain such a running\ntime dependence. In particular, we replace the exact LP solves with approximate\nones, enabling us to directly leverage the tremendous recent algorithmic\nprogress for approximate linear programming. More precisely, we show that the\nfundamental \"accuracy\" needed to exactly solve any LP in $A$ is inverse\npolynomial in $n$ and $\\log\\bar{\\chi}_A$. Plugging in the recent algorithm of\nvan den Brand (SODA '20), our method computes an optimal primal and dual\nsolution using ${O}(m n^{\\omega+1} \\log (n)\\log(\\bar{\\chi}_A+n))$ arithmetic\noperations, outperforming the specialized interior point method of Vavasis and\nYe and its recent improvement by Dadush et al (STOC '20).\n  At a technical level, our framework combines together approximate LP\nsolutions to compute exact ones, making use of constructive proximity theorems\n-- which bound the distance between solutions of \"nearby\" LPs -- to keep the\nrequired accuracy low.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 15:41:48 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Dadush", "Daniel", ""], ["Natura", "Bento", ""], ["V\u00e9gh", "L\u00e1szl\u00f3 A.", ""]]}, {"id": "2009.04979", "submitter": "Alan Kuhnle", "authors": "Alan Kuhnle", "title": "Quick Streaming Algorithms for Maximization of Monotone Submodular\n  Functions in Linear Time", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of monotone, submodular maximization over a ground\nset of size $n$ subject to cardinality constraint $k$. For this problem, we\nintroduce the first deterministic algorithms with linear time complexity; these\nalgorithms are streaming algorithms. Our single-pass algorithm obtains a\nconstant ratio in $\\lceil n / c \\rceil + c$ oracle queries, for any $c \\ge 1$.\nIn addition, we propose a deterministic, multi-pass streaming algorithm with a\nconstant number of passes that achieves nearly the optimal ratio with linear\nquery and time complexities. We prove a lower bound that implies no\nconstant-factor approximation exists using $o(n)$ queries, even if queries to\ninfeasible sets are allowed. An empirical analysis demonstrates that our\nalgorithms require fewer queries (often substantially less than $n$) yet still\nachieve better objective value than the current state-of-the-art algorithms,\nincluding single-pass, multi-pass, and non-streaming algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 16:35:54 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 15:50:49 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 17:49:18 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Kuhnle", "Alan", ""]]}, {"id": "2009.04992", "submitter": "Yu Chen", "authors": "Yu Chen, Sanjeev Khanna, Ansh Nagda", "title": "Near-linear Size Hypergraph Cut Sparsifiers", "comments": "FOCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cuts in graphs are a fundamental object of study, and play a central role in\nthe study of graph algorithms. The problem of sparsifying a graph while\napproximately preserving its cut structure has been extensively studied and has\nmany applications. In a seminal work, Bencz\\'ur and Karger (1996) showed that\ngiven any $n$-vertex undirected weighted graph $G$ and a parameter $\\varepsilon\n\\in (0,1)$, there is a near-linear time algorithm that outputs a weighted\nsubgraph $G'$ of $G$ of size $\\tilde{O}(n/\\varepsilon^2)$ such that the weight\nof every cut in $G$ is preserved to within a $(1 \\pm \\varepsilon)$-factor in\n$G'$. The graph $G'$ is referred to as a {\\em $(1 \\pm \\varepsilon)$-approximate\ncut sparsifier} of $G$.\n  A natural question is if such cut-preserving sparsifiers also exist for\nhypergraphs. Kogan and Krauthgamer (2015) initiated a study of this question\nand showed that given any weighted hypergraph $H$ where the cardinality of each\nhyperedge is bounded by $r$, there is a polynomial-time algorithm to find a $(1\n\\pm \\varepsilon)$-approximate cut sparsifier of $H$ of size\n$\\tilde{O}(\\frac{nr}{\\varepsilon^2})$. Since $r$ can be as large as $n$, in\ngeneral, this gives a hypergraph cut sparsifier of size\n$\\tilde{O}(n^2/\\varepsilon^2)$, which is a factor $n$ larger than the\nBencz\\'ur-Karger bound for graphs. It has been an open question whether or not\nBencz\\'ur-Karger bound is achievable on hypergraphs. In this work, we resolve\nthis question in the affirmative by giving a new polynomial-time algorithm for\ncreating hypergraph sparsifiers of size $\\tilde{O}(n/\\varepsilon^2)$.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 17:03:04 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Chen", "Yu", ""], ["Khanna", "Sanjeev", ""], ["Nagda", "Ansh", ""]]}, {"id": "2009.05039", "submitter": "Hung Le", "authors": "Vincent Cohen-Addad and Arnold Filtser and Philip N. Klein and Hung Le", "title": "On Light Spanners, Low-treewidth Embeddings and Efficient Traversing in\n  Minor-free Graphs", "comments": "65 pages, 6 figures. Abstract shorten due to limited characters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the structure of minor-free metrics, namely shortest path\nmetrics obtained over a weighted graph excluding a fixed minor, has been an\nimportant research direction since the fundamental work of Robertson and\nSeymour. A fundamental idea that helps both to understand the structural\nproperties of these metrics and lead to strong algorithmic results is to\nconstruct a \"small-complexity\" graph that approximately preserves distances\nbetween pairs of points of the metric. We show the two following structural\nresults for minor-free metrics:\n  1. Construction of a light subset spanner. Given a subset of vertices called\nterminals, and $\\epsilon$, in polynomial time we construct a subgraph that\npreserves all pairwise distances between terminals up to a multiplicative\n$1+\\epsilon$ factor, of total weight at most $O_{\\epsilon}(1)$ times the weight\nof the minimal Steiner tree spanning the terminals.\n  2. Construction of a stochastic metric embedding into low treewidth graphs\nwith expected additive distortion $\\epsilon D$. Namely, given a minor free\ngraph $G=(V,E,w)$ of diameter $D$, and parameter $\\epsilon$, we construct a\ndistribution $\\mathcal{D}$ over dominating metric embeddings into\ntreewidth-$O_{\\epsilon}(\\log n)$ graphs such that the additive distortion is at\nmost $\\epsilon D$.\n  One of our important technical contributions is a novel framework that allows\nus to reduce \\emph{both problems} to problems on simpler graphs of bounded\ndiameter. Our results have the following algorithmic consequences: (1) the\nfirst efficient approximation scheme for subset TSP in minor-free metrics; (2)\nthe first approximation scheme for vehicle routing with bounded capacity in\nminor-free metrics; (3) the first efficient approximation scheme for vehicle\nrouting with bounded capacity on bounded genus metrics.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 17:58:53 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Filtser", "Arnold", ""], ["Klein", "Philip N.", ""], ["Le", "Hung", ""]]}, {"id": "2009.05382", "submitter": "Felix Hommelsheim", "authors": "David Adjiashvili, Felix Hommelsheim, Moritz M\\\"uhlenthaler, Oliver\n  Schaudt", "title": "Fault-Tolerant Edge-Disjoint Paths -- Beyond Uniform Faults", "comments": "arXiv admin note: substantial text overlap with arXiv:1301.6299", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The overwhelming majority of survivable (fault-tolerant) network design\nmodels assume a uniform fault model. Such a model assumes that every subset of\nthe network resources (edges or vertices) of a given cardinality $k$ may fail.\nWhile this approach yields problems with clean combinatorial structure and good\nalgorithms, it often fails to capture the true nature of the scenario set\ncoming from applications. One natural refinement of the uniform model is\nobtained by partitioning the set of resources into vulnerable and safe\nresources. The scenario set contains every subset of at most $k$ faulty\nresources. This work studies the Fault-Tolerant Path (FTP) problem, the\ncounterpart of the Shortest Path problem in this fault model and the\nFault-Tolerant Flow problem (FTF), the counterpart of the $\\ell$-disjoint\nShortest $s$-$t$ Path problem. We present complexity results alongside exact\nand approximation algorithms for both models. We emphasize the vast increase in\nthe complexity of the problem with respect to the uniform analogue, the\nEdge-Disjoint Paths problem.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 17:41:24 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Adjiashvili", "David", ""], ["Hommelsheim", "Felix", ""], ["M\u00fchlenthaler", "Moritz", ""], ["Schaudt", "Oliver", ""]]}, {"id": "2009.05736", "submitter": "Adam Kasperski", "authors": "Romain Guillaume, Adam Kasperski, Pawel Zielinski", "title": "Robust production planning with budgeted cumulative demand uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with a problem of production planning, which is a version of\nthe capacitated single-item lot sizing problem with backordering under demand\nuncertainty, modeled by uncertain cumulative demands. The well-known interval\nbudgeted uncertainty representation is assumed. Two of its variants are\nconsidered. The first one is the discrete budgeted uncertainty, in which at\nmost a specified number of cumulative demands can deviate from their nominal\nvalues at the same time.The second variant is the continuous budgeted\nuncertainty, in which the sum of the deviations of cumulative demands from\ntheir nominal values, at the same time, is at most a bound on the total\ndeviation provided. For both cases, in order to choose a production plan that\nhedges against the cumulative demand uncertainty, the robust minmax criterion\nis used. Polynomial algorithms for evaluating the impact of uncertainty in the\ndemand on a given production plan in terms of its cost, called the adversarial\nproblem, and for finding robust production plans under the discrete budgeted\nuncertainty are constructed. Hence, in this case, the problems under\nconsideration are not much computationally harder than their deterministic\ncounterparts. For the continuous budgeted uncertainty, it is shown that the\nadversarial problem and the problem of computing a robust production plan along\nwith its worst-case cost are NP-hard. In the case, when uncertainty intervals\nare non-overlapping, they can be solved in pseudopolynomial time and admit\nfully polynomial timeapproximation schemes. In the general case, a\ndecomposition algorithm for finding a robust plan is proposed.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 06:55:21 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Guillaume", "Romain", ""], ["Kasperski", "Adam", ""], ["Zielinski", "Pawel", ""]]}, {"id": "2009.05776", "submitter": "Amitabh Trehan", "authors": "Walter Hussak and Amitabh Trehan", "title": "Terminating cases of flooding", "comments": "Submitted for journal publication. 26 pages. Related to\n  arXiv:1907.07078, https://doi.org/10.4230/LIPIcs.STACS.2020.17, and\n  https://doi.org/10.1145/3293611.3331586", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basic synchronous flooding proceeds in rounds. Given a finite undirected\n(network) graph $G$, a set of sources $I \\subseteq G$ initiate flooding in the\nfirst round by every node in $I$ sending the same message to all of its\nneighbours. In each subsequent round, nodes send the message to all of their\nneighbours from which they did not receive the message in the previous round.\nFlooding terminates when no node in $G$ sends a message in a round. The\nquestion of termination has not been settled - rather, non-termination is\nimplicitly assumed to be possible.\n  We show that flooding terminates on every finite graph. In the case of a\nsingle source $g_0$, flooding terminates in $e$ rounds if $G$ is bipartite and\n$j$ rounds with $e < j \\leq e+d+1$ otherwise, where $e$ and $d$ are the\neccentricity of $g_0$ and diameter of $G$ respectively. For\ncommunication/broadcast to all nodes, this is asymptotically time optimal and\nobviates the need for construction and maintenance of spanning structures. We\nextend to dynamic flooding initiated in multiple rounds with possibly multiple\nmessages. The cases where a node only sends a message to neighbours from which\nit did not receive {\\it any} message in the previous round, and where a node\nsends some highest ranked message to all neighbours from which it did not\nreceive {\\it that} message in the previous round, both terminate. All these\ncases also hold if the network graph loses edges over time. Non-terminating\ncases include asynchronous flooding, flooding where messages have fixed delays\nat edges, cases of multiple-message flooding and cases where the network graph\nacquires edges over time.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 12:05:48 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Hussak", "Walter", ""], ["Trehan", "Amitabh", ""]]}, {"id": "2009.05823", "submitter": "Shivika Narang", "authors": "Shivika Narang, Arpita Biswas, Y Narahari", "title": "On Achieving Fairness and Stability in Many-to-One Matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have seen a surge of work on fairness in social choice\nliterature. This paper initiates the study of finding a stable many-to-one\nmatching, under cardinal valuations, while satisfying fairness among the agents\non either side. Specifically, motivated by several real-world settings, we\nfocus on leximin optimal fairness and seek leximin optimality over many-to-one\nstable matchings. We first consider the special case of ranked valuations where\nall agents on each side have the same preference orders or rankings over the\nagents on the other side (but not necessarily the same valuations). For this\nspecial case, we provide a complete characterisation of the space of stable\nmatchings. This leads to FaSt, a novel and efficient algorithm to compute a\nleximin optimal stable matching under ranked isometric valuations (where, for\neach pair of agents, the valuation of one agent for the other is the same). The\nrunning time of FaSt is linear in the number of edges. Building upon FaSt, we\npresent an efficient algorithm, FaSt-Gen, that finds the leximin optimal stable\nmatching for ranked but otherwise unconstrained valuations. The running time of\nFaSt-Gen is quadratic in the number of edges. We next establish that, in the\nabsence of rankings, finding a leximin optimal stable matching is NP-Hard, even\nunder isometric valuations. In fact, when additivity and non-negativity are the\nonly assumptions on the valuations, we show that, unless P=NP, no efficient\npolynomial factor approximation is possible. When additivity is relaxed to\nsubmodularity, we find that not even an exponential approximation is possible.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 16:28:00 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 18:41:13 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 12:13:15 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Narang", "Shivika", ""], ["Biswas", "Arpita", ""], ["Narahari", "Y", ""]]}, {"id": "2009.05953", "submitter": "Shmuel Onn", "authors": "Shmuel Onn", "title": "Optimization over Young Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding a Young diagram minimizing the sum of\nevaluations of a given pair of functions on the parts of the associated pair of\nconjugate partitions. While there are exponentially many diagrams, we show it\nis polynomial time solvable.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 08:31:52 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Onn", "Shmuel", ""]]}, {"id": "2009.06043", "submitter": "Peter Davies", "authors": "Artur Czumaj, Peter Davies, Merav Parter", "title": "Simple, Deterministic, Constant-Round Coloring in the Congested Clique", "comments": "20 pages, appeared at PODC 2020", "journal-ref": null, "doi": "10.1145/3382734.3405751", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We settle the complexity of the $(\\Delta+1)$-coloring and $(\\Delta+1)$-list\ncoloring problems in the CONGESTED CLIQUE model by presenting a simple\ndeterministic algorithm for both problems running in a constant number of\nrounds. This matches the complexity of the recent breakthrough randomized\nconstant-round $(\\Delta+1)$-list coloring algorithm due to Chang et al.\n(PODC'19), and significantly improves upon the state-of-the-art $O(\\log\n\\Delta)$-round deterministic $(\\Delta+1)$-coloring bound of Parter (ICALP'18).\n  A remarkable property of our algorithm is its simplicity. Whereas the\nstate-of-the-art randomized algorithms for this problem are based on the quite\ninvolved local coloring algorithm of Chang et al. (STOC'18), our algorithm can\nbe described in just a few lines. At a high level, it applies a careful\nderandomization of a recursive procedure which partitions the nodes and their\nrespective palettes into separate bins. We show that after $O(1)$ recursion\nsteps, the remaining uncolored subgraph within each bin has linear size, and\nthus can be solved locally by collecting it to a single node. This algorithm\ncan also be implemented in the Massively Parallel Computation (MPC) model\nprovided that each machine has linear (in $n$, the number of nodes in the input\ngraph) space.\n  We also show an extension of our algorithm to the MPC regime in which\nmachines have sublinear space: we present the first deterministic\n$(\\Delta+1)$-list coloring algorithm designed for sublinear-space MPC, which\nruns in $O(\\log \\Delta + \\log\\log n)$ rounds.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 16:59:21 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Czumaj", "Artur", ""], ["Davies", "Peter", ""], ["Parter", "Merav", ""]]}, {"id": "2009.06063", "submitter": "Pranabendu Misra", "authors": "Pranabendu Misra", "title": "On Fault Tolerant Feedback Vertex Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of fault-tolerant data structures for various network design\nproblems is a prominent area of research in computer science. Likewise, the\nstudy of NP-Complete problems lies at the heart of computer science with\nnumerous results in algorithms and complexity. In this paper we raise the\nquestion of computing fault tolerant solutions to NP-Complete problems; that is\ncomputing a solution that can survive the \"failure\" of a few constituent\nelements. This notion has appeared in a variety of theoretical and practical\nsettings such as estimating network reliability, kernelization (aka instance\ncompression), approximation algorithms and so on. In this paper, we seek to\nhighlight these questions for further research.\n  As a concrete example, we study the fault-tolerant version of the classical\nFeedback Vertex Set (FVS) problem, that we call Fault Tolerant Feedback Vertex\nSet (FT-FVS). Recall that, in FVS the input is a graph $G$ and the objective is\nto compute a minimum subset of vertices $S$ such that $G-S$ is a forest. In\nFT-FVS, the objective is to compute a minimum subset $S$ of vertices such that\n$G - (S \\setminus \\{v\\})$ is a forest for any $v \\in V(G)$. Here the vertex $v$\ndenotes a single vertex fault. We show that this problem is NP-Complete, and\nthen present a constant factor approximation algorithm as well as an\nFPT-algorithm parameterized by the solution size. We believe that the question\nof computing fault tolerant solutions to various NP-Complete problems is an\ninteresting direction for future research.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 18:49:59 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Misra", "Pranabendu", ""]]}, {"id": "2009.06090", "submitter": "Ohad Trabelsi", "authors": "Amir Abboud, Robert Krauthgamer, Ohad Trabelsi", "title": "Cut-Equivalent Trees are Optimal for Min-Cut Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Min-Cut queries are fundamental: Preprocess an undirected edge-weighted\ngraph, to quickly report a minimum-weight cut that separates a query pair of\nnodes $s,t$. The best data structure known for this problem simply builds a\ncut-equivalent tree, discovered 60 years ago by Gomory and Hu, who also showed\nhow to construct it using $n-1$ minimum $st$-cut computations. Using\nstate-of-the-art algorithms for minimum $st$-cut (Lee and Sidford, FOCS 2014)\narXiv:1312.6713, one can construct the tree in time $\\tilde{O}(mn^{3/2})$,\nwhich is also the preprocessing time of the data structure. (Throughout, we\nfocus on polynomially-bounded edge weights, noting that faster algorithms are\nknown for small/unit edge weights.)\n  Our main result shows the following equivalence: Cut-equivalent trees can be\nconstructed in near-linear time if and only if there is a data structure for\nMin-Cut queries with near-linear preprocessing time and polylogarithmic\n(amortized) query time, and even if the queries are restricted to a fixed\nsource. That is, equivalent trees are an essentially optimal solution for\nMin-Cut queries. This equivalence holds even for every minor-closed family of\ngraphs, such as bounded-treewidth graphs, for which a two-decade old data\nstructure (Arikati et al., J.~Algorithms 1998) implies the first near-linear\ntime construction of cut-equivalent trees.\n  Moreover, unlike all previous techniques for constructing cut-equivalent\ntrees, ours is robust to relying on approximation algorithms. In particular,\nusing the almost-linear time algorithm for $(1+\\epsilon)$-approximate minimum\n$st$-cut (Kelner et al., SODA 2014), we can construct a\n$(1+\\epsilon)$-approximate flow-equivalent tree (which is a slightly weaker\nnotion) in time $n^{2+o(1)}$. This leads to the first\n$(1+\\epsilon)$-approximation for All-Pairs Max-Flow that runs in time\n$n^{2+o(1)}$, and matches the output size almost-optimally.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 21:29:23 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Abboud", "Amir", ""], ["Krauthgamer", "Robert", ""], ["Trabelsi", "Ohad", ""]]}, {"id": "2009.06106", "submitter": "Hengjie Zhang", "authors": "S. Cliff Liu, Zhao Song, Hengjie Zhang", "title": "Breaking the $n$-Pass Barrier: A Streaming Algorithm for Maximum Weight\n  Bipartite Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Given a weighted bipartite graph with $n$ vertices and $m$ edges, the\n\\emph{maximum weight bipartite matching} problem is to find a set of\nvertex-disjoint edges with the maximum weight. This classic problem has been\nextensively studied for over a century.\n  In this paper, we present a new streaming algorithm for the maximum weight\nbipartite matching problem that uses $\\widetilde{O}(n)$ space and\n$\\widetilde{O}(\\sqrt{m})$ passes, which breaks the $n$-pass barrier. All the\nprevious streaming algorithms either require $\\Omega(n \\log n)$ passes or only\nfind an approximate solution. Our streaming algorithm constructs a subgraph\nwith $n$ edges of the input graph in $\\widetilde{O}(\\sqrt{m})$ passes, such\nthat the subgraph admits the optimal matching with good probability.\n  Our method combines various ideas from different fields, most notably the\nconstruction of \\emph{space-efficient} interior point method (IPM), SDD system\nsolvers, the isolation lemma, and LP duality. To the best of our knowledge,\nthis is the first work that implements the SDD solvers and IPMs in the\nstreaming model in $\\widetilde{O}(n)$ spaces for graph matrices; previous IPM\nalgorithms only focus on optimizing the running time, regardless of the space\nusage.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 22:54:25 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 00:14:49 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Liu", "S. Cliff", ""], ["Song", "Zhao", ""], ["Zhang", "Hengjie", ""]]}, {"id": "2009.06107", "submitter": "Samuel Hopkins", "authors": "Matthew Brennan and Guy Bresler and Samuel B. Hopkins and Jerry Li and\n  Tselil Schramm", "title": "Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent", "comments": "Version 3 fixes typos and adds note on presentation at COLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers currently use a number of approaches to predict and substantiate\ninformation-computation gaps in high-dimensional statistical estimation\nproblems. A prominent approach is to characterize the limits of restricted\nmodels of computation, which on the one hand yields strong computational lower\nbounds for powerful classes of algorithms and on the other hand helps guide the\ndevelopment of efficient algorithms. In this paper, we study two of the most\npopular restricted computational models, the statistical query framework and\nlow-degree polynomials, in the context of high-dimensional hypothesis testing.\nOur main result is that under mild conditions on the testing problem, the two\nclasses of algorithms are essentially equivalent in power. As corollaries, we\nobtain new statistical query lower bounds for sparse PCA, tensor PCA and\nseveral variants of the planted clique problem.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 22:55:18 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 05:28:15 GMT"}, {"version": "v3", "created": "Sat, 26 Jun 2021 17:06:23 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Brennan", "Matthew", ""], ["Bresler", "Guy", ""], ["Hopkins", "Samuel B.", ""], ["Li", "Jerry", ""], ["Schramm", "Tselil", ""]]}, {"id": "2009.06512", "submitter": "Haider Al Kim", "authors": "Haider Al Kim, Sven Puchinger, Antonia Wachter-Zeh", "title": "Bounds and Code Constructions for Partially Defect Memory Cells", "comments": "6 pages, 3 theorems, code construction, sphere-packing-like bound, 2\n  figures, Gilbert-Varshamov-like bound, 4 figures, Seventeenth International\n  Workshop on Algebraic and Combinatorial Coding Theory Acct 2020, October\n  11-17, 2020, Bulgaria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers coding for so-called partially stuck memory cells. Such\nmemory cells can only store partial information as some of their levels cannot\nbe used due to, e.g., wear out. First, we present a new code construction for\nmasking such partially stuck cells while additionally correcting errors. This\nconstruction (for cells with $q >2$ levels) is achieved by generalizing an\nexisting masking-only construction in [1] (based on binary codes) to correct\nerrors as well. Compared to previous constructions in [2], our new construction\nachieves larger rates for many sets of parameters. Second, we derive a\nsphere-packing (any number of $u$ partially stuck cells) and a\nGilbert-Varshamov bound ($u<q$ partially stuck cells) for codes that can mask a\ncertain number of partially stuck cells and correct errors additionally. A\nnumerical comparison between the new bounds and our previous construction of\nPSMCs for the case $u<q$ in [2] shows that our construction lies above the\nGilbert-Varshamov-like bound for several code parameters.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 15:20:35 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 10:24:10 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 19:57:43 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Kim", "Haider Al", ""], ["Puchinger", "Sven", ""], ["Wachter-Zeh", "Antonia", ""]]}, {"id": "2009.06538", "submitter": "Jianyu Yang", "authors": "Jianyu Yang, Tianhao Wang, Ninghui Li, Xiang Cheng, Sen Su", "title": "Answering Multi-Dimensional Range Queries under Local Differential\n  Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of answering multi-dimensional range\nqueries under local differential privacy. There are three key technical\nchallenges: capturing the correlations among attributes, avoiding the curse of\ndimensionality, and dealing with the large domains of attributes. None of the\nexisting approaches satisfactorily deals with all three challenges. Overcoming\nthese three challenges, we first propose an approach called Two-Dimensional\nGrids (TDG). Its main idea is to carefully use binning to partition the\ntwo-dimensional (2-D) domains of all attribute pairs into 2-D grids that can\nanswer all 2-D range queries and then estimate the answer of a higher\ndimensional range query from the answers of the associated 2-D range queries.\nHowever, in order to reduce errors due to noises, coarse granularities are\nneeded for each attribute in 2-D grids, losing fine-grained distribution\ninformation for individual attributes. To correct this deficiency, we further\npropose Hybrid-Dimensional Grids (HDG), which also introduces 1-D grids to\ncapture finer-grained information on distribution of each individual attribute\nand combines information from 1-D and 2-D grids to answer range queries. To\nmake HDG consistently effective, we provide a guideline for properly choosing\ngranularities of grids based on an analysis of how different sources of errors\nare impacted by these choices. Extensive experiments conducted on real and\nsynthetic datasets show that HDG can give a significant improvement over the\nexisting approaches.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 16:08:53 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Yang", "Jianyu", ""], ["Wang", "Tianhao", ""], ["Li", "Ninghui", ""], ["Cheng", "Xiang", ""], ["Su", "Sen", ""]]}, {"id": "2009.06540", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Themis Gouleakis and Daniel M. Kane and John\n  Peebles and Eric Price", "title": "Optimal Testing of Discrete Distributions with High Probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of testing discrete distributions with a focus on the\nhigh probability regime. Specifically, given samples from one or more discrete\ndistributions, a property $\\mathcal{P}$, and parameters $0< \\epsilon, \\delta\n<1$, we want to distinguish {\\em with probability at least $1-\\delta$} whether\nthese distributions satisfy $\\mathcal{P}$ or are $\\epsilon$-far from\n$\\mathcal{P}$ in total variation distance. Most prior work in distribution\ntesting studied the constant confidence case (corresponding to $\\delta =\n\\Omega(1)$), and provided sample-optimal testers for a range of properties.\nWhile one can always boost the confidence probability of any such tester by\nblack-box amplification, this generic boosting method typically leads to\nsub-optimal sample bounds.\n  Here we study the following broad question: For a given property\n$\\mathcal{P}$, can we {\\em characterize} the sample complexity of testing\n$\\mathcal{P}$ as a function of all relevant problem parameters, including the\nerror probability $\\delta$? Prior to this work, uniformity testing was the only\nstatistical task whose sample complexity had been characterized in this\nsetting. As our main results, we provide the first algorithms for closeness and\nindependence testing that are sample-optimal, within constant factors, as a\nfunction of all relevant parameters. We also show matching\ninformation-theoretic lower bounds on the sample complexity of these problems.\nOur techniques naturally extend to give optimal testers for related problems.\nTo illustrate the generality of our methods, we give optimal algorithms for\ntesting collections of distributions and testing closeness with unequal sized\nsamples.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 16:09:17 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Gouleakis", "Themis", ""], ["Kane", "Daniel M.", ""], ["Peebles", "John", ""], ["Price", "Eric", ""]]}, {"id": "2009.06778", "submitter": "Lutz Oettershagen", "authors": "Lutz Oettershagen, Anne Driemel, Petra Mutzel", "title": "Spatio-Temporal Top-k Similarity Search for Trajectories in Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding the $k$ most similar trajectories to a given\nquery trajectory. Our work is inspired by the work of Grossi et al. [6] that\nconsiders trajectories as walks in a graph. Each visited vertex is accompanied\nby a time-interval. Grossi et al. define a similarity function that captures\ntemporal and spatial aspects. We improve this similarity function to derive a\nnew spatio-temporal distance function for which we can show that a specific\ntype of triangle inequality is satisfied. This distance function is the basis\nfor our index structures, which can be constructed efficiently, need only\nlinear memory, and can quickly answer queries for the top-$k$ most similar\ntrajectories. Our evaluation on real-world and synthetic data sets shows that\nour algorithms outperform the baselines with respect to indexing time by\nseveral orders of magnitude while achieving similar or better query time and\nquality of results.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 22:43:32 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 10:13:22 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Oettershagen", "Lutz", ""], ["Driemel", "Anne", ""], ["Mutzel", "Petra", ""]]}, {"id": "2009.06784", "submitter": "Cheng Mao", "authors": "Cheng Mao and Yihong Wu", "title": "Learning Mixtures of Permutations: Groups of Pairwise Comparisons and\n  Combinatorial Method of Moments", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications such as rank aggregation, mixture models for permutations are\nfrequently used when the population exhibits heterogeneity. In this work, we\nstudy the widely used Mallows mixture model. In the high-dimensional setting,\nwe propose a polynomial-time algorithm that learns a Mallows mixture of\npermutations on $n$ elements with the optimal sample complexity that is\nproportional to $\\log n$, improving upon previous results that scale\npolynomially with $n$. In the high-noise regime, we characterize the optimal\ndependency of the sample complexity on the noise parameter. Both objectives are\naccomplished by first studying demixing permutations under a noiseless query\nmodel using groups of pairwise comparisons, which can be viewed as moments of\nthe mixing distribution, and then extending these results to the noisy Mallows\nmodel by simulating the noiseless oracle.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 23:11:46 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Mao", "Cheng", ""], ["Wu", "Yihong", ""]]}, {"id": "2009.06921", "submitter": "Emir Demirovi\\'c", "authors": "Emir Demirovi\\'c, Peter J. Stuckey", "title": "Optimal Decision Trees for Nonlinear Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear metrics, such as the F1-score, Matthews correlation coefficient,\nand Fowlkes-Mallows index, are often used to evaluate the performance of\nmachine learning models, in particular, when facing imbalanced datasets that\ncontain more samples of one class than the other. Recent optimal decision tree\nalgorithms have shown remarkable progress in producing trees that are optimal\nwith respect to linear criteria, such as accuracy, but unfortunately nonlinear\nmetrics remain a challenge. To address this gap, we propose a novel algorithm\nbased on bi-objective optimisation, which treats misclassifications of each\nbinary class as a separate objective. We show that, for a large class of\nmetrics, the optimal tree lies on the Pareto frontier. Consequently, we obtain\nthe optimal tree by using our method to generate the set of all nondominated\ntrees. To the best of our knowledge, this is the first method to compute\nprovably optimal decision trees for nonlinear metrics. Our approach leads to a\ntrade-off when compared to optimising linear metrics: the resulting trees may\nbe more desirable according to the given nonlinear metric at the expense of\nhigher runtimes. Nevertheless, the experiments illustrate that runtimes are\nreasonable for majority of the tested datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 08:30:56 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Demirovi\u0107", "Emir", ""], ["Stuckey", "Peter J.", ""]]}, {"id": "2009.07248", "submitter": "Lingyi Zhang", "authors": "Yuri Faenza, Danny Segev, Lingyi Zhang", "title": "Approximation Algorithms for The Generalized Incremental Knapsack\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study a discrete multi-period extension of the classical\nknapsack problem, dubbed generalized incremental knapsack. In this setting, we\nare given a set of $n$ items, each associated with a non-negative weight, and\n$T$ time periods with non-decreasing capacities $W_1 \\leq \\dots \\leq W_T$. When\nitem $i$ is inserted at time $t$, we gain a profit of $p_{it}$; however, this\nitem remains in the knapsack for all subsequent periods. The goal is to decide\nif and when to insert each item, subject to the time-dependent capacity\nconstraints, with the objective of maximizing our total profit. Interestingly,\nthis setting subsumes as special cases a number of recently-studied incremental\nknapsack problems, all known to be strongly NP-hard.\n  Our first contribution comes in the form of a polynomial-time\n$(\\frac{1}{2}-\\epsilon)$-approximation for the generalized incremental knapsack\nproblem. This result is based on a reformulation as a single-machine sequencing\nproblem, which is addressed by blending dynamic programming techniques and the\nclassical Shmoys-Tardos algorithm for the generalized assignment problem.\nCombined with further enumeration-based self-reinforcing ideas and\nnewly-revealed structural properties of nearly-optimal solutions, we turn our\nbasic algorithm into a quasi-polynomial time approximation scheme (QPTAS).\nHence, under widely believed complexity assumptions, this finding rules out the\npossibility that generalized incremental knapsack is APX-hard.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 17:32:55 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Faenza", "Yuri", ""], ["Segev", "Danny", ""], ["Zhang", "Lingyi", ""]]}, {"id": "2009.07268", "submitter": "Ewin Tang", "authors": "Andr\\'as Gily\\'en and Zhao Song and Ewin Tang", "title": "An improved quantum-inspired algorithm for linear regression", "comments": "16 pages, bug fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a classical algorithm for linear regression analogous to the quantum\nmatrix inversion algorithm [Harrow, Hassidim, and Lloyd, Physical Review\nLetters'09] for low-rank matrices [Wossnig et al., Physical Review Letters'18],\nwhen the input matrix $A$ is stored in a data structure applicable for\nQRAM-based state preparation.\n  Namely, given an $A \\in \\mathbb{C}^{m\\times n}$ with minimum singular value\n$\\sigma$ and which supports certain efficient $\\ell_2$-norm importance sampling\nqueries, along with a $b \\in \\mathbb{C}^m$, we can output a description of an\n$x \\in \\mathbb{C}^n$ such that $\\|x - A^+b\\| \\leq \\varepsilon\\|A^+b\\|$ in\n$\\tilde{\\mathcal{O}}\\Big(\\frac{\\|A\\|_{\\mathrm{F}}^6\\|A\\|^2}{\\sigma^8\\varepsilon^4}\\Big)$\ntime, improving on previous \"quantum-inspired\" algorithms in this line of\nresearch by a factor of $\\frac{\\|A\\|^{14}}{\\sigma^{14}\\varepsilon^2}$ [Chia et\nal., STOC'20]. The algorithm is stochastic gradient descent, and the analysis\nbears similarities to those of optimization algorithms for regression in the\nusual setting [Gupta and Sidford, NeurIPS'18]. Unlike earlier works, this is a\npromising avenue that could lead to feasible implementations of classical\nregression in a quantum-inspired setting, for comparison against future quantum\ncomputers.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 17:58:25 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 10:09:10 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Gily\u00e9n", "Andr\u00e1s", ""], ["Song", "Zhao", ""], ["Tang", "Ewin", ""]]}, {"id": "2009.07269", "submitter": "Dmitriy Kunisky", "authors": "Dmitriy Kunisky", "title": "Positivity-preserving extensions of sum-of-squares pseudomoments over\n  the hypercube", "comments": "101 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for building higher-degree sum-of-squares lower\nbounds over the hypercube $\\mathbf{x} \\in \\{\\pm 1\\}^N$ from a given degree 2\nlower bound. Our method constructs pseudoexpectations that are positive\nsemidefinite by design, lightening some of the technical challenges common to\nother approaches to SOS lower bounds, such as pseudocalibration.\n  We give general \"incoherence\" conditions under which degree 2 pseudomoments\ncan be extended to higher degrees. As an application, we extend previous lower\nbounds for the Sherrington-Kirkpatrick Hamiltonian from degree 4 to degree 6.\n(This is subsumed, however, in the stronger results of the parallel work of\nGhosh et al.) This amounts to extending degree 2 pseudomoments given by a\nrandom low-rank projection matrix. As evidence in favor of our construction for\nhigher degrees, we also show that random high-rank projection matrices (an\neasier case) can be extended to degree $\\omega(1)$. We identify the main\nobstacle to achieving the same in the low-rank case, and conjecture that while\nour construction remains correct to leading order, it also requires a\nnext-order adjustment.\n  Our technical argument involves the interplay of two ideas of independent\ninterest. First, our pseudomoment matrix factorizes in terms of certain\nmultiharmonic polynomials. This observation guides our proof of positivity.\nSecond, our pseudomoment values are described graphically by sums over forests,\nwith coefficients given by the M\\\"{o}bius function of a partial ordering of\nthose forests. This connection guides our proof that the pseudomoments satisfy\nthe hypercube constraints. We trace the reason that our pseudomoments can\nsatisfy both the hypercube and positivity constraints simultaneously to a\ncombinatorial relationship between multiharmonic polynomials and this\nM\\\"{o}bius function.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 17:59:08 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Kunisky", "Dmitriy", ""]]}, {"id": "2009.07735", "submitter": "Abdurrahman Ya\\c{s}ar", "authors": "Abdurrahman Ya\\c{s}ar and Muhammed Fat\\.ih Balin and Xiaojing An and\n  Kaan Sancak and \\\"Umit V. \\c{C}ataly\\\"urek", "title": "On Symmetric Rectilinear Matrix Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even distribution of irregular workload to processing units is crucial for\nefficient parallelization in many applications. In this work, we are concerned\nwith a spatial partitioning called rectilinear partitioning (also known as\ngeneralized block distribution) of sparse matrices. More specifically, in this\nwork, we address the problem of symmetric rectilinear partitioning of a square\nmatrix. By symmetric, we mean the rows and columns of the matrix are\nidentically partitioned yielding a tiling where the diagonal tiles (blocks)\nwill be squares. We first show that the optimal solution to this problem is\nNP-hard, and we propose four heuristics to solve two different variants of this\nproblem. We present a thorough analysis of the computational complexities of\nthose proposed heuristics. To make the proposed techniques more applicable in\nreal life application scenarios, we further reduce their computational\ncomplexities by utilizing effective sparsification strategies together with an\nefficient sparse prefix-sum data structure. We experimentally show the proposed\nalgorithms are efficient and effective on more than six hundred test matrices.\nWith sparsification, our methods take less than 3 seconds in the Twitter graph\non a modern 24 core system and output a solution whose load imbalance is no\nworse than 1%.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 15:10:14 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Ya\u015far", "Abdurrahman", ""], ["Balin", "Muhammed Fatih", ""], ["An", "Xiaojing", ""], ["Sancak", "Kaan", ""], ["\u00c7ataly\u00fcrek", "\u00dcmit V.", ""]]}, {"id": "2009.07770", "submitter": "Polly Fahey", "authors": "Isolde Adler and Polly Fahey", "title": "Faster Property Testers in a Variation of the Bounded Degree Model", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Property testing algorithms are highly efficient algorithms, that come with\nprobabilistic accuracy guarantees. For a property P, the goal is to distinguish\ninputs that have P from those that are far from having P with high probability\ncorrectly, by querying only a small number of local parts of the input. In\nproperty testing on graphs, the distance is measured by the number of edge\nmodifications (additions or deletions), that are necessary to transform a graph\ninto one with property P. Much research has focussed on the query complexity of\nsuch algorithms, i. e. the number of queries the algorithm makes to the input,\nbut in view of applications, the running time of the algorithm is equally\nrelevant.\n  In (Adler, Harwath STACS 2018), a natural extension of the bounded degree\ngraph model of property testing to relational databases of bounded degree was\nintroduced, and it was shown that on databases of bounded degree and bounded\ntree-width, every property that is expressible in monadic second-order logic\nwith counting (CMSO) is testable with constant query complexity and sublinear\nrunning time. It remains open whether this can be improved to constant running\ntime.\n  In this paper we introduce a new model, which is based on the bounded degree\nmodel, but the distance measure allows both edge (tuple) modifications and\nvertex (element) modifications. Our main theorem shows that on databases of\nbounded degree and bounded tree-width, every property that is expressible in\nCMSO is testable with constant query complexity and constant running time in\nthe new model. We also show that every property that is testable in the\nclassical model is testable in our model with the same query complexity and\nrunning time, but the converse is not true.\n  We argue that our model is natural and our meta-theorem showing constant-time\nCMSO testability supports this.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 15:55:44 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Adler", "Isolde", ""], ["Fahey", "Polly", ""]]}, {"id": "2009.07785", "submitter": "Boro Sofranac", "authors": "Boro Sofranac, Ambros Gleixner, Sebastian Pokutta", "title": "Accelerating Domain Propagation: an Efficient GPU-Parallel Algorithm\n  over Sparse Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS cs.MS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast domain propagation of linear constraints has become a crucial component\nof today's best algorithms and solvers for mixed integer programming and\npseudo-boolean optimization to achieve peak solving performance. Irregularities\nin the form of dynamic algorithmic behaviour, dependency structures, and\nsparsity patterns in the input data make efficient implementations of domain\npropagation on GPUs and, more generally, on parallel architectures challenging.\nThis is one of the main reasons why domain propagation in state-of-the-art\nsolvers is single thread only. In this paper, we present a new algorithm for\ndomain propagation which (a) avoids these problems and allows for an efficient\nimplementation on GPUs, and is (b) capable of running propagation rounds\nentirely on the GPU, without any need for synchronization or communication with\nthe CPU. We present extensive computational results which demonstrate the\neffectiveness of our approach and show that ample speedups are possible on\npractically relevant problems: on state-of-the-art GPUs, our geometric mean\nspeed-up for reasonably-large instances is around 10x to 20x and can be as high\nas 180x on favorably-large instances.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 16:25:29 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 13:17:25 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 16:27:00 GMT"}, {"version": "v4", "created": "Fri, 9 Jul 2021 14:28:53 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Sofranac", "Boro", ""], ["Gleixner", "Ambros", ""], ["Pokutta", "Sebastian", ""]]}, {"id": "2009.07800", "submitter": "Giuseppe Di Molfetta Prof.", "authors": "Basile Herzog and Giuseppe Di Molfetta", "title": "Searching via nonlinear quantum walk on the 2D-grid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide numerical evidence that the nonlinear searching algorithm\nintroduced by Wong and Meyer \\cite{meyer2013nonlinear}, rephrased in terms of\nquantum walks with effective nonlinear phase, can be extended to the finite\n2-dimensional grid, keeping the same computational advantage \\BHg{with} respect\nto the classical algorithms. For this purpose, we have considered the free\nlattice Hamiltonian, with linear dispersion relation introduced by Childs and\nGe \\cite{Childs_2014}. The numerical simulations showed that the walker finds\nthe marked vertex in $O(N^{1/4} \\log^{3/4} N) $ steps, with probability\n$O(1/\\log N)$, for an overall complexity of $O(N^{1/4}\\log^{7/4}N)$. We also\nproved that there exists an optimal choice of the walker parameters to avoid\nthat the time measurement precision affects the complexity searching time of\nthe algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 16:48:41 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 09:20:33 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 16:36:50 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Herzog", "Basile", ""], ["Di Molfetta", "Giuseppe", ""]]}, {"id": "2009.07925", "submitter": "Meghna Lowalekar", "authors": "Meghna Lowalekar, Pradeep Varakantham, Patrick Jaillet", "title": "Competitive Ratios for Online Multi-capacity Ridesharing", "comments": "28 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-capacity ridesharing, multiple requests (e.g., customers, food\nitems, parcels) with different origin and destination pairs travel in one\nresource. In recent years, online multi-capacity ridesharing services (i.e.,\nwhere assignments are made online) like Uber-pool, foodpanda, and on-demand\nshuttles have become hugely popular in transportation, food delivery, logistics\nand other domains. This is because multi-capacity ridesharing services benefit\nall parties involved { the customers (due to lower costs), the drivers (due to\nhigher revenues) and the matching platforms (due to higher revenues per\nvehicle/resource). Most importantly these services can also help reduce carbon\nemissions (due to fewer vehicles on roads).\n  Online multi-capacity ridesharing is extremely challenging as the underlying\nmatching graph is no longer bipartite (as in the unit-capacity case) but a\ntripartite graph with resources (e.g., taxis, cars), requests and request\ngroups (combinations of requests that can travel together). The desired\nmatching between resources and request groups is constrained by the edges\nbetween requests and request groups in this tripartite graph (i.e., a request\ncan be part of at most one request group in the final assignment). While there\nhave been myopic heuristic approaches employed for solving the online\nmulti-capacity ridesharing problem, they do not provide any guarantees on the\nsolution quality. To that end, this paper presents the first approach with\nbounds on the competitive ratio for online multi-capacity ridesharing (when\nresources rejoin the system at their initial location/depot after serving a\ngroup of requests).\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 20:29:21 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Lowalekar", "Meghna", ""], ["Varakantham", "Pradeep", ""], ["Jaillet", "Patrick", ""]]}, {"id": "2009.08000", "submitter": "Albert Cheu", "authors": "Albert Cheu and Jonathan Ullman", "title": "The Limits of Pan Privacy and Shuffle Privacy for Learning and\n  Estimation", "comments": "Corrected a proof in the Appendix. Added a reference to parallel and\n  concurrent work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a recent wave of interest in intermediate trust models for\ndifferential privacy that eliminate the need for a fully trusted central data\ncollector, but overcome the limitations of local differential privacy. This\ninterest has led to the introduction of the shuffle model (Cheu et al.,\nEUROCRYPT 2019; Erlingsson et al., SODA 2019) and revisiting the pan-private\nmodel (Dwork et al., ITCS 2010). The message of this line of work is that, for\na variety of low-dimensional problems -- such as counts, means, and histograms\n-- these intermediate models offer nearly as much power as central differential\nprivacy. However, there has been considerably less success using these models\nfor high-dimensional learning and estimation problems. In this work, we show\nthat, for a variety of high-dimensional learning and estimation problems, both\nthe shuffle model and the pan-private model inherently incur an exponential\nprice in sample complexity relative to the central model. For example, we show\nthat, private agnostic learning of parity functions over $d$ bits requires\n$\\Omega(2^{d/2})$ samples in these models, and privately selecting the most\ncommon attribute from a set of $d$ choices requires $\\Omega(d^{1/2})$ samples,\nboth of which are exponential separations from the central model. Our work\ngives the first non-trivial lower bounds for these problems for both the\npan-private model and the general multi-message shuffle model.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 01:15:55 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 17:22:28 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 19:05:20 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Cheu", "Albert", ""], ["Ullman", "Jonathan", ""]]}, {"id": "2009.08032", "submitter": "Pravesh K Kothari", "authors": "Jackson Abascal, Venkatesan Guruswami, Pravesh K. Kothari", "title": "Strongly refuting all semi-random Boolean CSPs", "comments": "31 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an efficient algorithm to strongly refute \\emph{semi-random}\ninstances of all Boolean constraint satisfaction problems. The number of\nconstraints required by our algorithm matches (up to polylogarithmic factors)\nthe best-known bounds for efficient refutation of fully random instances. Our\nmain technical contribution is an algorithm to strongly refute semi-random\ninstances of the Boolean $k$-XOR problem on $n$ variables that have\n$\\widetilde{O}(n^{k/2})$ constraints. (In a semi-random $k$-XOR instance, the\nequations can be arbitrary and only the right-hand sides are random.)\n  One of our key insights is to identify a simple combinatorial property of\nrandom XOR instances that makes spectral refutation work. Our approach involves\ntaking an instance that does not satisfy this property (i.e., is \\emph{not}\npseudorandom) and reducing it to a partitioned collection of $2$-XOR instances.\nWe analyze these subinstances using a carefully chosen quadratic form as a\nproxy, which in turn is bounded via a combination of spectral methods and\nsemidefinite programming. The analysis of our spectral bounds relies only on an\noff-the-shelf matrix Bernstein inequality. Even for the purely random case,\nthis leads to a shorter proof compared to the ones in the literature that rely\non problem-specific trace-moment computations.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 03:01:39 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Abascal", "Jackson", ""], ["Guruswami", "Venkatesan", ""], ["Kothari", "Pravesh K.", ""]]}, {"id": "2009.08158", "submitter": "Diptapriyo Majumdar", "authors": "Carl Einarson, Gregory Gutin, Bart M. P. Jansen, Diptapriyo Majumdar,\n  Magnus Wahlstrom", "title": "p-Edge/Vertex-Connected Vertex Cover: Parameterized and Approximation\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study two natural generalizations of the Connected\nVertexCover (VC) problem: the $p$-Edge-Connected and $p$-Vertex-Connected VC\nproblem (where $p \\geq 2$ is a fixed integer). Like Connected VC, both new VC\nproblems are FPT, but do not admit a polynomial kernel unless $NP \\subseteq\ncoNP/poly$, which is highly unlikely. We prove however that both problems admit\ntime efficient polynomial sized approximate kernelization schemes. We obtain an\n$O(2^{O(pk)}n^{O(1)})$-time algorithm for the $p$-Edge-Connected VC and an\n$O(2^{O(k^2)}n^{O(1)})$-time algorithm for the $p$-Vertex-Connected VC.\nFinally, we describe a $2(p+1)$-approximation algorithm for the\n$p$-Edge-Connected VC. The proofs for the new VC problems require more\nsophisticated arguments than for Connected VC. In particular, for the\napproximation algorithm we use Gomory-Hu trees and for the approximate kernels\na result on small-size spanning $p$-vertex/edge-connected subgraph of a\n$p$-vertex/edge-connected graph obtained independently by Nishizeki and Poljak\n(1994) and Nagamochi and Ibaraki (1992).\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 09:06:48 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Einarson", "Carl", ""], ["Gutin", "Gregory", ""], ["Jansen", "Bart M. P.", ""], ["Majumdar", "Diptapriyo", ""], ["Wahlstrom", "Magnus", ""]]}, {"id": "2009.08172", "submitter": "Asaf Levin", "authors": "Dorit S. Hochbaum and Asaf Levin and Xu Rao", "title": "Algorithms and Complexity for Variants of Covariates Fine Balance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study here several variants of the covariates fine balance problem where\nwe generalize some of these problems and introduce a number of others. We\npresent here a comprehensive complexity study of the covariates problems\nproviding polynomial time algorithms, or a proof of NP-hardness. The polynomial\ntime algorithms described are mostly combinatorial and rely on network flow\ntechniques. In addition we present several fixed-parameter tractable results\nfor problems where the number of covariates and the number of levels of each\ncovariate are seen as a parameter.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 09:31:41 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Hochbaum", "Dorit S.", ""], ["Levin", "Asaf", ""], ["Rao", "Xu", ""]]}, {"id": "2009.08208", "submitter": "Victor Isaac Kolobov", "authors": "Keren Censor-Hillel, Victor I. Kolobov, Gregory Schwartzman", "title": "Finding Subgraphs in Highly Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the fundamental problem of finding subgraphs in\nhighly dynamic distributed networks - networks which allow an arbitrary number\nof links to be inserted / deleted per round. We show that the problems of\n$k$-clique membership listing (for any $k\\geq 3$), 4-cycle listing and 5-cycle\nlisting can be deterministically solved in $O(1)$-amortized round complexity,\neven with limited logarithmic-sized messages.\n  To achieve $k$-clique membership listing we introduce a very useful\ncombinatorial structure which we name the robust $2$-hop neighborhood. This is\na subset of the 2-hop neighborhood of a node, and we prove that it can be\nmaintained in highly dynamic networks in $O(1)$-amortized rounds. We also show\nthat maintaining the actual 2-hop neighborhood of a node requires near linear\namortized time, showing the necessity of our definition. For $4$-cycle and\n$5$-cycle listing, we need edges within hop distance 3, for which we similarly\ndefine the robust $3$-hop neighborhood and prove it can be maintained in highly\ndynamic networks in $O(1)$-amortized rounds.\n  We complement the above with several impossibility results. We show that\nmembership listing of any other graph on $k\\geq 3$ nodes except $k$-clique\nrequires an almost linear number of amortized communication rounds. We also\nshow that $k$-cycle listing for $k\\geq 6$ requires $\\Omega(\\sqrt{n} / \\log n)$\namortized rounds. This, combined with our upper bounds, paints a detailed\npicture of the complexity landscape for ultra fast graph finding algorithms in\nthis highly dynamic environment.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 11:02:09 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Kolobov", "Victor I.", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "2009.08216", "submitter": "Daniel Stilck Franca", "authors": "Fernando G.S.L. Brand\\~ao, Richard Kueng, Daniel Stilck Fran\\c{c}a", "title": "Fast and robust quantum state tomography from few basis measurements", "comments": "Corrected typos and added numerical examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum state tomography is a powerful, but resource-intensive, general\nsolution for numerous quantum information processing tasks. This motivates the\ndesign of robust tomography procedures that use relevant resources as sparingly\nas possible. Important cost factors include the number of state copies and\nmeasurement settings, as well as classical postprocessing time and memory. In\nthis work, we present and analyze an online tomography algorithm designed to\noptimize all the aforementioned resources at the cost of a worse dependence on\naccuracy. The protocol is the first to give provably optimal performance in\nterms of rank and dimension for state copies, measurement settings and memory.\nClassical runtime is also reduced substantially and numerical experiments\ndemonstrate a favorable comparison with other state-of-the-art techniques.\nFurther improvements are possible by executing the algorithm on a quantum\ncomputer, giving a quantum speedup for quantum state tomography.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 11:28:41 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 10:12:18 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Brand\u00e3o", "Fernando G. S. L.", ""], ["Kueng", "Richard", ""], ["Fran\u00e7a", "Daniel Stilck", ""]]}, {"id": "2009.08266", "submitter": "Christian Coester", "authors": "S\\'ebastien Bubeck, Niv Buchbinder, Christian Coester, Mark Sellke", "title": "Metrical Service Systems with Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a generalization of the fundamental online metrical service\nsystems (MSS) problem where the feasible region can be transformed between\nrequests. In this problem, which we call T-MSS, an algorithm maintains a point\nin a metric space and has to serve a sequence of requests. Each request is a\nmap (transformation) $f_t\\colon A_t\\to B_t$ between subsets $A_t$ and $B_t$ of\nthe metric space. To serve it, the algorithm has to go to a point $a_t\\in A_t$,\npaying the distance from its previous position. Then, the transformation is\napplied, modifying the algorithm's state to $f_t(a_t)$. Such transformations\ncan model, e.g., changes to the environment that are outside of an algorithm's\ncontrol, and we therefore do not charge any additional cost to the algorithm\nwhen the transformation is applied. The transformations also allow to model\nrequests occurring in the $k$-taxi problem.\n  We show that for $\\alpha$-Lipschitz transformations, the competitive ratio is\n$\\Theta(\\alpha)^{n-2}$ on $n$-point metrics. Here, the upper bound is achieved\nby a deterministic algorithm and the lower bound holds even for randomized\nalgorithms. For the $k$-taxi problem, we prove a competitive ratio of $\\tilde\nO((n\\log k)^2)$. For chasing convex bodies, we show that even with contracting\ntransformations no competitive algorithm exists.\n  The problem T-MSS has a striking connection to the following deep\nmathematical question: Given a finite metric space $M$, what is the required\ncardinality of an extension $\\hat M\\supseteq M$ where each partial isometry on\n$M$ extends to an automorphism? We give partial answers for special cases.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 13:09:13 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Buchbinder", "Niv", ""], ["Coester", "Christian", ""], ["Sellke", "Mark", ""]]}, {"id": "2009.08320", "submitter": "Sjoerd Dirksen", "authors": "Sjoerd Dirksen and Alexander Stollenwerk", "title": "Binarized Johnson-Lindenstrauss embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of encoding a set of vectors into a minimal number of\nbits while preserving information on their Euclidean geometry. We show that\nthis task can be accomplished by applying a Johnson-Lindenstrauss embedding and\nsubsequently binarizing each vector by comparing each entry of the vector to a\nuniformly random threshold. Using this simple construction we produce two\nencodings of a dataset such that one can query Euclidean information for a pair\nof points using a small number of bit operations up to a desired additive error\n- Euclidean distances in the first case and inner products and squared\nEuclidean distances in the second. In the latter case, each point is encoded in\nnear-linear time. The number of bits required for these encodings is quantified\nin terms of two natural complexity parameters of the dataset - its covering\nnumbers and localized Gaussian complexity - and shown to be near-optimal.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 14:12:40 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Dirksen", "Sjoerd", ""], ["Stollenwerk", "Alexander", ""]]}, {"id": "2009.08416", "submitter": "Yasamin Nazari", "authors": "Jakub {\\L}\\k{a}cki, Yasamin Nazari", "title": "Near-Optimal Decremental Hopsets with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a weighted undirected graph $G=(V,E,w)$, a hopset $H$ of hopbound\n$\\beta$ and stretch $(1+\\epsilon)$ is a set of edges such that for any pair of\nnodes $u, v \\in V$, there is a path in $G \\cup H$ of at most $\\beta$ hops,\nwhose length is within a $(1+\\epsilon)$ factor from the distance between $u$\nand $v$ in $G$.\n  We show the first efficient decremental algorithm for maintaining hopsets\nwith a polylogarithmic hopbound.\n  The update time of our algorithm matches the best known static algorithm up\nto polylogarithmic factors. All the previous decremental hopset constructions\nhad a superpolylogarithmic (but subpolynomial) hopbound of $2^{\\log^{\\Omega(1)}\nn}$ [Bernstein, FOCS'09; HKN, FOCS'14; Chechik, FOCS'18].\n  By applying our decremental hopset construction, we get improved or near\noptimal bounds for several distance problems.\n  Most importantly, we show how to decrementally maintain\n$(2k-1)(1+\\epsilon)$-approximate all-pairs shortest paths (for any constant $k\n\\geq 2)$, in $\\tilde{O}(n^{1/k})$ amortized update time and $O(k)$ query time.\n  This significantly improves (by a polynomial factor) over the update-time of\nthe best previously known decremental algorithm in the constant query time\nregime. Moreover, it improves over the result of [Chechik, FOCS'18] that has a\nquery time of $O(\\log \\log(nW))$, where $W$ is the aspect ratio, and the\namortized update time is\n$n^{1/k}\\cdot(\\frac{1}{\\epsilon})^{\\tilde{O}(\\sqrt{\\log n})}$. For sparse\ngraphs our construction nearly matches the best known static running time/\nquery time tradeoff.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 17:01:00 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 18:43:38 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 15:36:20 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["\u0141\u0105cki", "Jakub", ""], ["Nazari", "Yasamin", ""]]}, {"id": "2009.08447", "submitter": "Yujia Jin", "authors": "Yair Carmon, Yujia Jin, Aaron Sidford, Kevin Tian", "title": "Coordinate Methods for Matrix Games", "comments": "Accepted at FOCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop primal-dual coordinate methods for solving bilinear saddle-point\nproblems of the form $\\min_{x \\in \\mathcal{X}} \\max_{y\\in\\mathcal{Y}} y^\\top A\nx$ which contain linear programming, classification, and regression as special\ncases. Our methods push existing fully stochastic sublinear methods and\nvariance-reduced methods towards their limits in terms of per-iteration\ncomplexity and sample complexity. We obtain nearly-constant per-iteration\ncomplexity by designing efficient data structures leveraging Taylor\napproximations to the exponential and a binomial heap. We improve sample\ncomplexity via low-variance gradient estimators using dynamic sampling\ndistributions that depend on both the iterates and the magnitude of the matrix\nentries.\n  Our runtime bounds improve upon those of existing primal-dual methods by a\nfactor depending on sparsity measures of the $m$ by $n$ matrix $A$. For\nexample, when rows and columns have constant $\\ell_1/\\ell_2$ norm ratios, we\noffer improvements by a factor of $m+n$ in the fully stochastic setting and\n$\\sqrt{m+n}$ in the variance-reduced setting. We apply our methods to\ncomputational geometry problems, i.e. minimum enclosing ball, maximum inscribed\nball, and linear regression, and obtain improved complexity bounds. For linear\nregression with an elementwise nonnegative matrix, our guarantees improve on\nexact gradient methods by a factor of $\\sqrt{\\mathrm{nnz}(A)/(m+n)}$.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 17:55:03 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Carmon", "Yair", ""], ["Jin", "Yujia", ""], ["Sidford", "Aaron", ""], ["Tian", "Kevin", ""]]}, {"id": "2009.08479", "submitter": "Thatchaphol Saranurak", "authors": "Julia Chuzhoy, Thatchaphol Saranurak", "title": "Deterministic Algorithms for Decremental Shortest Paths via Layered Core\n  Decomposition", "comments": "Abstract is shorten", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the decremental single-source shortest paths (SSSP) problem, the input is\nan undirected graph $G=(V,E)$ with $n$ vertices and $m$ edges undergoing edge\ndeletions, together with a fixed source vertex $s\\in V$. The goal is to\nmaintain a data structure that supports shortest-path queries: given a vertex\n$v\\in V$, quickly return an (approximate) shortest path from $s$ to $v$. The\ndecremental all-pairs shortest paths (APSP) problem is defined similarly, but\nnow the shortest-path queries are allowed between any pair of vertices of $V$.\nBoth problems have been studied extensively since the 80's, and algorithms with\nnear-optimal total update time and query time have been discovered for them.\nUnfortunately, all these algorithms are randomized and, more importantly, they\nneed to assume an oblivious adversary.\n  Our first result is a deterministic algorithm for the decremental SSSP\nproblem on weighted graphs with $O(n^{2+o(1)})$ total update time, that\nsupports $(1+\\epsilon)$-approximate shortest-path queries, with query time\n$O(|P|\\cdot n^{o(1)})$, where $P$ is the returned path. This is the first\n$(1+\\epsilon)$-approximation algorithm against an adaptive adversary that\nsupports shortest-path queries in time below $O(n)$, that breaks the $O(mn)$\ntotal update time bound of the classical algorithm of Even and Shiloah from\n1981.\n  Our second result is a deterministic algorithm for the decremental APSP\nproblem on unweighted graphs that achieves total update time\n$O(n^{2.5+\\delta})$, for any constant $\\delta>0$, supports approximate distance\nqueries in $O(\\log\\log n)$ time; the algorithm achieves an\n$O(1)$-multiplicative and $n^{o(1)}$-additive approximation on the path length.\nAll previous algorithms for APSP either assume an oblivious adversary or have\nan $\\Omega(n^{3})$ total update time when $m=\\Omega(n^{2})$.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 18:01:44 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Chuzhoy", "Julia", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "2009.08588", "submitter": "Yota Otachi", "authors": "Masashi Kiyomi, Takashi Horiyama, Yota Otachi", "title": "Longest Common Subsequence in Sublinear Space", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first $\\mathrm{o}(n)$-space polynomial-time algorithm for\ncomputing the length of a longest common subsequence. Given two strings of\nlength $n$, the algorithm runs in $\\mathrm{O}(n^{3})$ time with\n$\\mathrm{O}\\left(\\frac{n \\log^{1.5} n}{2^{\\sqrt{\\log n}}}\\right)$ bits of\nspace.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 02:08:40 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Kiyomi", "Masashi", ""], ["Horiyama", "Takashi", ""], ["Otachi", "Yota", ""]]}, {"id": "2009.08721", "submitter": "Xiaoyu He", "authors": "Xiaoyu He, Jialin Zhang, Xiaoming Sun", "title": "Quantum Search with Prior Knowledge", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Search-base algorithms have widespread applications in different scenarios.\nGrover's quantum search algorithms and its generalization, amplitude\namplification, provide a quadratic speedup over classical search algorithms for\nunstructured search. We consider the problem of searching with prior knowledge.\nMore preciously, search for the solution among N items with a prior probability\ndistribution. This letter proposes a new generalization of Grover's search\nalgorithm which performs better than the standard Grover algorithm in average\nunder this setting. We prove that our new algorithm achieves the optimal\nexpected success probability of finding the solution if the number of queries\nis fixed.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 09:50:33 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["He", "Xiaoyu", ""], ["Zhang", "Jialin", ""], ["Sun", "Xiaoming", ""]]}, {"id": "2009.08830", "submitter": "Kazuhiro Kurita", "authors": "Yasuaki Kobayashi, Kazuhiro Kurita, Kunihiro Wasa", "title": "Efficient Constant-Factor Approximate Enumeration of Minimal Subsets for\n  Monotone Properties with Weight Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A property $\\Pi$ on a finite set $U$ is \\emph{monotone} if for every $X\n\\subseteq U$ satisfying $\\Pi$, every superset $Y \\subseteq U$ of $X$ also\nsatisfies $\\Pi$. Many combinatorial properties can be seen as monotone\nproperties. The problem of finding a minimum subset of $U$ satisfying $\\Pi$ is\na central problem in combinatorial optimization. Although many\napproximate/exact algorithms have been developed to solve this kind of problem\non numerous properties, a solution obtained by these algorithms is often\nunsuitable for real-world applications due to the difficulty of building\naccurate mathematical models on real-world problems. A promising approach to\novercome this difficulty is to \\emph{enumerate} multiple small solutions rather\nthan to \\emph{find} a single small solution. To this end, given a weight\nfunction $w: U \\to \\mathbb N$ and an integer $k$, we devise algorithms that\n\\emph{approximately} enumerate all minimal subsets of $U$ with weight at most\n$k$ satisfying $\\Pi$ for various monotone properties $\\Pi$, where \"approximate\nenumeration\" means that algorithms output all minimal subsets satisfying $\\Pi$\nwhose weight at most $k$ and may output some minimal subsets satisfying $\\Pi$\nwhose weight exceeds $k$ but is at most $ck$ for some constant $c \\ge 1$. These\nalgorithms allow us to efficiently enumerate minimal vertex covers, minimal\ndominating sets in bounded degree graphs, minimal feedback vertex sets, minimal\nhitting sets in bounded rank hypergraphs, etc., of weight at most $k$ with\nconstant approximation factors.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 13:21:15 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 06:20:08 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 03:24:13 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Kobayashi", "Yasuaki", ""], ["Kurita", "Kazuhiro", ""], ["Wasa", "Kunihiro", ""]]}, {"id": "2009.08844", "submitter": "Anna Hermann", "authors": "Ulrich Brenner and Anna Hermann", "title": "Delay Optimization of Combinational Logic by And-Or Path Restructuring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dynamic programming algorithm that constructs delay-optimized\ncircuits for alternating And-Or paths with prescribed input arrival times. Our\nalgorithm fulfills best-known approximation guarantees and empirically\noutperforms earlier methods by exploring a significantly larger portion of the\nsolution space. Our algorithm is the core of a new timing optimization\nframework that replaces critical paths of arbitrary length by logically\nequivalent realizations with less delay. Our framework allows revising early\ndecisions on the logical structure of the netlist in a late step of an\nindustrial physical design flow. Experiments demonstrate the effectiveness of\nour tool on 7nm real-world instances.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 13:58:25 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Brenner", "Ulrich", ""], ["Hermann", "Anna", ""]]}, {"id": "2009.08966", "submitter": "Amy B.Z. Zhang", "authors": "Amy B.Z. Zhang, Itai Gurvich", "title": "A Low-rank Approximation for MDPs via Moment Coupling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework to approximate a Markov Decision Process that stands\non two pillars: state aggregation -- as the algorithmic infrastructure; and\ncentral-limit-theorem-type approximations -- as the mathematical underpinning\nof optimality guarantees. The theory is grounded in recent work Braverman et al\n(2020} that relates the solution of the Bellman equation to that of a PDE\nwhere, in the spirit of the central limit theorem, the transition matrix is\nreduced to its local first and second moments. Solving the PDE is\n$\\textit{not}$ required by our method. Instead, we construct a \"sister\"\n(controlled) Markov chain whose two local transition moments are approximately\nidentical with those of the focal chain. Because of this $\\textit{moment\nmatching}$, the original chain and its \"sister\" are coupled through the PDE, a\ncoupling that facilitates optimality guarantees. Embedded into standard soft\naggregation algorithms, moment matching provided a disciplined mechanism to\ntune the aggregation and disaggregation probabilities. The computational gains\narise from the reduction of the effective state space from $N$ to\n$N^{\\frac{1}{2}+\\epsilon}$ is as one might intuitively expect from\napproximations grounded in the central limit theorem.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 17:53:17 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 19:53:36 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhang", "Amy B. Z.", ""], ["Gurvich", "Itai", ""]]}, {"id": "2009.09288", "submitter": "Mark Levin Sh.", "authors": "Mark Sh. Levin", "title": "On combinatorial optimization for dominating sets (literature survey,\n  new models)", "comments": "17 pages, figures 6, table 5", "journal-ref": null, "doi": "10.13140/RG.2.2.34919.68006", "report-no": null, "categories": "cs.DS cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper focuses on some versions of connected dominating set problems:\nbasic problems and multicriteria problems. A literature survey on basic problem\nformulations and solving approaches is presented. The basic connected\ndominating set problems are illustrated by simplifyed numerical examples. New\ninteger programming formulations of dominating set problems (with multiset\nestimates) are suggested.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 19:52:53 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Levin", "Mark Sh.", ""]]}, {"id": "2009.09442", "submitter": "Feng Zhang", "authors": "Feng Zhang, Jidong Zhai, Xipeng Shen, Dalin Wang, Zheng Chen, Onur\n  Mutlu, Wenguang Chen, Xiaoyong Du", "title": "TADOC: Text Analytics Directly on Compression", "comments": "25 pages, 18 figures, VLDB Journal (2020)", "journal-ref": null, "doi": "10.1007/s00778-020-00636-3", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides a comprehensive description of Text Analytics Directly\non Compression (TADOC), which enables direct document analytics on compressed\ntextual data. The article explains the concept of TADOC and the challenges to\nits effective realizations. Additionally, a series of guidelines and technical\nsolutions that effectively address those challenges, including the adoption of\na hierarchical compression method and a set of novel algorithms and data\nstructure designs, are presented. Experiments on six data analytics tasks of\nvarious complexities show that TADOC can save 90.8% storage space and 87.9%\nmemory usage, while halving data processing times.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 14:46:48 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhang", "Feng", ""], ["Zhai", "Jidong", ""], ["Shen", "Xipeng", ""], ["Wang", "Dalin", ""], ["Chen", "Zheng", ""], ["Mutlu", "Onur", ""], ["Chen", "Wenguang", ""], ["Du", "Xiaoyong", ""]]}, {"id": "2009.09480", "submitter": "Andrew Lewis-Pye", "authors": "Andrew Lewis-Pye and Tim Roughgarden", "title": "A General Framework for the Security Analysis of Blockchain Protocols", "comments": "This paper has been merged with arXiv:2101.07095", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain protocols differ in fundamental ways, including the mechanics of\nselecting users to produce blocks (e.g., proof-of-work vs. proof-of-stake) and\nthe method to establish consensus (e.g., longest chain rules vs. Byzantine\nfault-tolerant (BFT) inspired protocols). These fundamental differences have\nhindered \"apples-to-apples\" comparisons between different categories of\nblockchain protocols and, in turn, the development of theory to formally\ndiscuss their relative merits.\n  This paper presents a parsimonious abstraction sufficient for capturing and\ncomparing properties of many well-known permissionless blockchain protocols,\nsimultaneously capturing essential properties of both proof-of-work (PoW) and\nproof-of-stake (PoS) protocols, and of both longest-chain-type and BFT-type\nprotocols. Our framework blackboxes the precise mechanics of the user selection\nprocess, allowing us to isolate the properties of the selection process that\nare significant for protocol design.\n  We demonstrate the utility of our general framework with several concrete\nresults:\n  1. We prove a CAP-type impossibility theorem asserting that liveness with an\nunknown level of participation rules out security in a partially synchronous\nsetting.\n  2. Delving deeper into the partially synchronous setting, we prove that a\nnecessary and sufficient condition for security is the production of\n\"certificates,\" meaning stand-alone proofs of block confirmation.\n  3. Restricting to synchronous settings, we prove that typical protocols with\na known level of participation (including longest chain-type PoS protocols) can\nbe adapted to provide certificates, but those with an unknown level of\nparticipation cannot.\n  4. Finally, we use our framework to articulate a modular two-step approach to\nblockchain security analysis that effectively reduces the permissionless case\nto the permissioned case.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 17:30:22 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 02:22:16 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 12:53:18 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Lewis-Pye", "Andrew", ""], ["Roughgarden", "Tim", ""]]}, {"id": "2009.09604", "submitter": "Badih Ghazi", "authors": "Lijie Chen, Badih Ghazi, Ravi Kumar, Pasin Manurangsi", "title": "On Distributed Differential Privacy and Counting Distinct Elements", "comments": "68 pages, 4 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the setup where each of $n$ users holds an element from a discrete\nset, and the goal is to count the number of distinct elements across all users,\nunder the constraint of $(\\epsilon, \\delta)$-differentially privacy:\n  - In the non-interactive local setting, we prove that the additive error of\nany protocol is $\\Omega(n)$ for any constant $\\epsilon$ and for any $\\delta$\ninverse polynomial in $n$.\n  - In the single-message shuffle setting, we prove a lower bound of\n$\\Omega(n)$ on the error for any constant $\\epsilon$ and for some $\\delta$\ninverse quasi-polynomial in $n$. We do so by building on the moment-matching\nmethod from the literature on distribution estimation.\n  - In the multi-message shuffle setting, we give a protocol with at most one\nmessage per user in expectation and with an error of $\\tilde{O}(\\sqrt(n))$ for\nany constant $\\epsilon$ and for any $\\delta$ inverse polynomial in $n$. Our\nprotocol is also robustly shuffle private, and our error of $\\sqrt(n)$ matches\na known lower bound for such protocols.\n  Our proof technique relies on a new notion, that we call dominated protocols,\nand which can also be used to obtain the first non-trivial lower bounds against\nmulti-message shuffle protocols for the well-studied problems of selection and\nlearning parity.\n  Our first lower bound for estimating the number of distinct elements provides\nthe first $\\omega(\\sqrt(n))$ separation between global sensitivity and error in\nlocal differential privacy, thus answering an open question of Vadhan (2017).\nWe also provide a simple construction that gives $\\tilde{\\Omega}(n)$ separation\nbetween global sensitivity and error in two-party differential privacy, thereby\nanswering an open question of McGregor et al. (2011).\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 04:13:34 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Chen", "Lijie", ""], ["Ghazi", "Badih", ""], ["Kumar", "Ravi", ""], ["Manurangsi", "Pasin", ""]]}, {"id": "2009.09605", "submitter": "Oussama Hanguir", "authors": "Oussama Hanguir, Clifford Stein", "title": "Distributed Algorithms for Matching in Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $ $We study the $d$-Uniform Hypergraph Matching ($d$-UHM) problem: given an\n$n$-vertex hypergraph $G$ where every hyperedge is of size $d$, find a maximum\ncardinality set of disjoint hyperedges. For $d\\geq3$, the problem of finding\nthe maximum matching is NP-complete, and was one of Karp's 21\n$\\mathcal{NP}$-complete problems. In this paper we are interested in the\nproblem of finding matchings in hypergraphs in the massively parallel\ncomputation (MPC) model that is a common abstraction of MapReduce-style\ncomputation. In this model, we present the first three parallel algorithms for\n$d$-Uniform Hypergraph Matching, and we analyse them in terms of resources such\nas memory usage, rounds of communication needed, and approximation ratio. The\nhighlights include:\n  $\\bullet$ A $O(\\log n)$-round $d$-approximation algorithm that uses $O(nd)$\nspace per machine.\n  $\\bullet$ A $3$-round, $O(d^2)$-approximation algorithm that uses\n$\\tilde{O}(\\sqrt{nm})$ space per machine.\n  $\\bullet$ A $3$-round algorithm that computes a subgraph containing a\n$(d-1+\\frac{1}{d})^2$-approximation, using $\\tilde{O}(\\sqrt{nm})$ space per\nmachine for linear hypergraphs, and $\\tilde{O}(n\\sqrt{nm})$ in general.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 04:14:42 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Hanguir", "Oussama", ""], ["Stein", "Clifford", ""]]}, {"id": "2009.09645", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang", "title": "The Complexity Landscape of Distributed Locally Checkable Problems on\n  Trees", "comments": "To appear in DISC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research revealed the existence of gaps in the complexity landscape of\nlocally checkable labeling (LCL) problems in the LOCAL model of distributed\ncomputing. For example, the deterministic round complexity of any LCL problem\non bounded-degree graphs is either $O(\\log^\\ast n)$ or $\\Omega(\\log n)$ [Chang,\nKopelowitz, and Pettie, FOCS 2016]. The complexity landscape of LCL problems is\nnow quite well-understood, but a few questions remain open.\n  For bounded-degree trees, there is an LCL problem with round complexity\n$\\Theta(n^{1/k})$ for each positive integer $k$ [Chang and Pettie, FOCS 2017].\nIt is conjectured that no LCL problem has round complexity $o(n^{1/(k-1)})$ and\n$\\omega(n^{1/k})$ on bounded-degree trees. As of now, only the case of $k = 2$\nhas been proved [Balliu et al., DISC 2018].\n  In this paper, we show that for LCL problems on bounded-degree trees, there\nis indeed a gap between $\\Theta(n^{1/(k-1)})$ and $\\Theta(n^{1/k})$ for each $k\n\\geq 2$. Our proof is constructive in the sense that it offers a sequential\nalgorithm that decides which side of the gap a given LCL problem belongs to. We\nalso show that it is EXPTIME-hard to distinguish between $\\Theta(1)$-round and\n$\\Theta(n)$-round LCL problems on bounded-degree trees. This improves upon a\nprevious PSPACE-hardness result [Balliu et al., PODC 2019].\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 07:07:55 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Chang", "Yi-Jun", ""]]}, {"id": "2009.09646", "submitter": "Aleksandar Shurbevski", "authors": "Naveed Ahmed Azam, Jianshen Zhu, Yanming Sun, Yu Shi, Aleksandar\n  Shurbevski, Liang Zhao, Hiroshi Nagamochi, Tatsuya Akutsu", "title": "A Novel Method for Inference of Acyclic Chemical Compounds with Bounded\n  Branch-height Based on Artificial Neural Networks and Integer Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of chemical graphs is a major research topic in computational\nmolecular biology due to its potential applications to drug design. One\napproach is inverse quantitative structure activity/property relationship\n(inverse QSAR/QSPR) analysis, which is to infer chemical structures from given\nchemical activities/properties. Recently, a framework has been proposed for\ninverse QSAR/QSPR using artificial neural networks (ANN) and mixed integer\nlinear programming (MILP). This method consists of a prediction phase and an\ninverse prediction phase. In the first phase, a feature vector $f(G)$ of a\nchemical graph $G$ is introduced and a prediction function $\\psi$ on a chemical\nproperty $\\pi$ is constructed with an ANN. In the second phase, given a target\nvalue $y^*$ of property $\\pi$, a feature vector $x^*$ is inferred by solving an\nMILP formulated from the trained ANN so that $\\psi(x^*)$ is close to $y^*$ and\nthen a set of chemical structures $G^*$ such that $f(G^*)= x^*$ is enumerated\nby a graph search algorithm. The framework has been applied to the case of\nchemical compounds with cycle index up to 2. The computational results\nconducted on instances with $n$ non-hydrogen atoms show that a feature vector\n$x^*$ can be inferred for up to around $n=40$ whereas graphs $G^*$ can be\nenumerated for up to $n=15$. When applied to the case of chemical acyclic\ngraphs, the maximum computable diameter of $G^*$ was around up to around 8. We\nintroduce a new characterization of graph structure, \"branch-height,\" based on\nwhich an MILP formulation and a graph search algorithm are designed for\nchemical acyclic graphs. The results of computational experiments using\nproperties such as octanol/water partition coefficient, boiling point and heat\nof combustion suggest that the proposed method can infer chemical acyclic\ngraphs $G^*$ with $n=50$ and diameter 30.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 07:11:59 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Azam", "Naveed Ahmed", ""], ["Zhu", "Jianshen", ""], ["Sun", "Yanming", ""], ["Shi", "Yu", ""], ["Shurbevski", "Aleksandar", ""], ["Zhao", "Liang", ""], ["Nagamochi", "Hiroshi", ""], ["Akutsu", "Tatsuya", ""]]}, {"id": "2009.09678", "submitter": "Christopher Weyand", "authors": "Thomas Bl\\\"asius, Tobias Friedrich, Christopher Weyand", "title": "Efficiently Computing Maximum Flows in Scale-Free Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the maximum-flow/minimum-cut problem on scale-free networks, i.e.,\ngraphs whose degree distribution follows a power-law. We propose a simple\nalgorithm that capitalizes on the fact that often only a small fraction of such\na network is relevant for the flow. At its core, our algorithm augments\nDinitz's algorithm with a balanced bidirectional search. Our experiments on a\nscale-free random network model indicate sublinear run time. On scale-free\nreal-world networks, we outperform the commonly used highest-label Push-Relabel\nimplementation by up to two orders of magnitude. Compared to Dinitz's original\nalgorithm, our modifications reduce the search space, e.g., by a factor of 275\non an autonomous systems graph.\n  Beyond these good run times, our algorithm has an additional advantage\ncompared to Push-Relabel. The latter computes a preflow, which makes the\nextraction of a minimum cut potentially more difficult. This is relevant, for\nexample, for the computation of Gomory-Hu trees. On a social network with 70000\nnodes, our algorithm computes the Gomory-Hu tree in 3 seconds compared to 12\nminutes when using Push-Relabel.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 08:35:50 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Friedrich", "Tobias", ""], ["Weyand", "Christopher", ""]]}, {"id": "2009.09743", "submitter": "Vera Traub", "authors": "Vera Traub", "title": "Improving on Best-of-Many-Christofides for $T$-tours", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $T$-tour problem is a natural generalization of TSP and Path TSP. Given a\ngraph $G=(V,E)$, edge cost $c: E \\to \\mathbb{R}_{\\ge 0}$, and an even\ncardinality set $T\\subseteq V$, we want to compute a minimum-cost $T$-join\nconnecting all vertices of $G$ (and possibly containing parallel edges).\n  In this paper we give an $\\frac{11}{7}$-approximation for the $T$-tour\nproblem and show that the integrality ratio of the standard LP relaxation is at\nmost $\\frac{11}{7}$. Despite much progress for the special case Path TSP, for\ngeneral $T$-tours this is the first improvement on Seb\\H{o}'s analysis of the\nBest-of-Many-Christofides algorithm (Seb\\H{o} [2013]).\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 10:20:47 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Traub", "Vera", ""]]}, {"id": "2009.09955", "submitter": "Lan N. Nguyen", "authors": "Lan N. Nguyen and My T. Thai", "title": "Length-Bounded Paths Interdiction in Continuous Domain for Network\n  Performance Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying on networked systems, in which a communication between nodes is\nfunctional if their distance under a given metric is lower than a pre-defined\nthreshold, has received significant attention recently. In this work, we\npropose a metric to measure network resilience on guaranteeing the pre-defined\nperformance constraint. This metric is investigated under an optimization\nproblem, namely \\textbf{Length-bounded Paths Interdiction in Continuous Domain}\n(cLPI), which aims to identify a minimum set of nodes whose changes cause\nrouting paths between nodes become undesirable for the network service.\n  We show the problem is NP-hard and propose a framework by designing two\noracles, \\textit{Threshold Blocking} (TB) and \\textit{Critical Path Listing}\n(CPL), which communicate back and forth to construct a feasible solution to\ncLPI with theoretical bicriteria approximation guarantees. Based on this\nframework, we propose two solutions for each oracle. Each combination of one\nsolution to \\tb and one solution to \\cpl gives us a solution to cLPI. The\nbicriteria guarantee of our algorithms allows us to control the solutions's\ntrade-off between the returned size and the performance accuracy. New insights\ninto the advantages of each solution are further discussed via experimental\nanalysis.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 15:04:43 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Nguyen", "Lan N.", ""], ["Thai", "My T.", ""]]}, {"id": "2009.10045", "submitter": "Antonio Fari\\~na", "authors": "Nieves R. Brisaboa, Ana Cerdeira-Pena, Guillermo de Bernardo, Antonio\n  Fari\\~na, Gonzalo Navarro", "title": "Space/time-efficient RDF stores based on circular suffix sorting", "comments": "This work has been submitted to the IEEE TKDE for possible\n  publication. Copyright may be transferred without notice, after which this\n  version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, RDF has gained popularity as a format for the standardized\npublication and exchange of information in the Web of Data. In this paper we\nintroduce RDFCSA, a data structure that is able to self-index an RDF dataset in\nsmall space and supports efficient querying. RDFCSA regards the triples of the\nRDF store as short circular strings and applies suffix sorting on those\nstrings, so that triple-pattern queries reduce to prefix searching on the\nstring set. The RDF store is then represented compactly using a Compressed\nSuffix Array (CSA), a proved technology in text indexing that efficiently\nsupports prefix searches. Our experimental evaluation shows that RDFCSA is able\nto answer triple-pattern queries in a few microseconds per result while using\nless than 60% of the space required by the raw original data. We also support\njoin queries, which provide the basis for full SPARQL query support. Even\nthough smaller-space solutions exist, as well as faster ones, RDFCSA is shown\nto provide an excellent space/time tradeoff, with fast and consistent query\ntimes within much less space than alternatives that compete in time.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 17:36:38 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Cerdeira-Pena", "Ana", ""], ["de Bernardo", "Guillermo", ""], ["Fari\u00f1a", "Antonio", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "2009.10160", "submitter": "Zeev Nutov", "authors": "Zeev Nutov", "title": "On rooted $k$-connectivity problems in quasi-bipartite digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the directed Rooted Subset $k$-Edge-Connectivity problem: given a\nset $T \\subseteq V$ of terminals in a digraph $G=(V+r,E)$ with edge costs and\nan integer $k$, find a min-cost subgraph of $G$ that contains $k$ edge disjoint\n$rt$-paths for all $t \\in T$. The case when every edge of positive cost has\nhead in $T$ admits a polynomial time algorithm due to Frank, and the case when\nall positive cost edges are incident to $r$ is equivalent to the $k$-Multicover\nproblem. Recently, [Chan et al. APPROX20] obtained ratio $O(\\ln k \\ln |T|)$ for\nquasi-bipartite instances, when every edge in $G$ has an end in $T+r$. We give\na simple proof for the same ratio for a more general problem of covering an\narbitrary $T$-intersecting supermodular set function by a minimum cost edge\nset, and for the case when only every positive cost edge has an end in $T+r$.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 20:17:43 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Nutov", "Zeev", ""]]}, {"id": "2009.10217", "submitter": "Swati Padmanabhan", "authors": "Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, Zhao\n  Song", "title": "A Faster Interior Point Method for Semidefinite Programming", "comments": "FOCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semidefinite programs (SDPs) are a fundamental class of optimization problems\nwith important recent applications in approximation algorithms, quantum\ncomplexity, robust learning, algorithmic rounding, and adversarial deep\nlearning. This paper presents a faster interior point method to solve generic\nSDPs with variable size $n \\times n$ and $m$ constraints in time \\begin{align*}\n\\widetilde{O}(\\sqrt{n}( mn^2 + m^\\omega + n^\\omega) \\log(1 / \\epsilon) ),\n\\end{align*} where $\\omega$ is the exponent of matrix multiplication and\n$\\epsilon$ is the relative accuracy. In the predominant case of $m \\geq n$, our\nruntime outperforms that of the previous fastest SDP solver, which is based on\nthe cutting plane method of Jiang, Lee, Song, and Wong [JLSW20].\n  Our algorithm's runtime can be naturally interpreted as follows:\n$\\widetilde{O}(\\sqrt{n} \\log (1/\\epsilon))$ is the number of iterations needed\nfor our interior point method, $mn^2$ is the input size, and $m^\\omega +\nn^\\omega$ is the time to invert the Hessian and slack matrix in each iteration.\nThese constitute natural barriers to further improving the runtime of interior\npoint methods for solving generic SDPs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 23:28:54 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Jiang", "Haotian", ""], ["Kathuria", "Tarun", ""], ["Lee", "Yin Tat", ""], ["Padmanabhan", "Swati", ""], ["Song", "Zhao", ""]]}, {"id": "2009.10255", "submitter": "EPTCS", "authors": "Thomas Prokosch (Institute for Informatics, Ludwig-Maximilian\n  University of Munich, Germany)", "title": "A Low-Level Index for Distributed Logic Programming", "comments": "In Proceedings ICLP 2020, arXiv:2009.09158", "journal-ref": "EPTCS 325, 2020, pp. 303-312", "doi": "10.4204/EPTCS.325.40", "report-no": null, "categories": "cs.SC cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed logic programming language with support for meta-programming\nand stream processing offers a variety of interesting research problems, such\nas: How can a versatile and stable data structure for the indexing of a large\nnumber of expressions be implemented with simple low-level data structures? Can\nlow-level programming help to reduce the number of occur checks in Robinson's\nunification algorithm? This article gives the answers.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 00:52:15 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Prokosch", "Thomas", "", "Institute for Informatics, Ludwig-Maximilian\n  University of Munich, Germany"]]}, {"id": "2009.10408", "submitter": "Giuseppe Di Molfetta Prof.", "authors": "Mathieu Roget, Basile Herzog, and Giuseppe Di Molfetta", "title": "Quantum control using quantum memory", "comments": "We added two figures respect to the previous version and improved the\n  introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new quantum numerical scheme to control the dynamics of a\nquantum walker in a two dimensional space-time grid. More specifically, we show\nhow, introducing a quantum memory for each of the spatial grid, this result can\nbe achieved simply by acting on the initial state of the whole system, and\ntherefore can be exactly controlled once for all. As example we prove\nanalytically how to encode in the initial state any arbitrary walker's mean\ntrajectory and variance. This brings significantly closer the possibility of\nimplementing dynamically interesting physics models on medium term quantum\ndevices, and introduces a new direction in simulating aspects of quantum field\ntheories (QFTs), notably on curved manifold.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 09:18:19 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 13:47:36 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Roget", "Mathieu", ""], ["Herzog", "Basile", ""], ["Di Molfetta", "Giuseppe", ""]]}, {"id": "2009.10502", "submitter": "Hirotaka Ono", "authors": "Tesshu Hanaka, Kazuma Kawai, Hirotaka Ono", "title": "Computing $L(p,1)$-Labeling with Combined Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph, an $L(p,1)$-labeling of the graph is an assignment $f$ from\nthe vertex set to the set of nonnegative integers such that for any pair of\nvertices $(u,v),|f (u) - f (v)| \\ge p$ if $u$ and $v$ are adjacent, and $f(u)\n\\neq f(v)$ if $u$ and $v$ are at distance $2$. The $L(p,1)$-labeling problem is\nto minimize the span of $f$ (i.e.,$\\max_{u\\in V}(f(u)) - \\min_{u\\in\nV}(f(u))+1$). It is known to be NP-hard even for graphs of maximum degree $3$\nor graphs with tree-width 2, whereas it is fixed-parameter tractable with\nrespect to vertex cover number. Since vertex cover number is a kind of the\nstrongest parameter, there is a large gap between tractability and\nintractability from the viewpoint of parameterization. To fill up the gap, in\nthis paper, we propose new fixed-parameter algorithms for $L(p,1)$-Labeling by\nthe twin cover number plus the maximum clique size and by the tree-width plus\nthe maximum degree. These algorithms reduce the gap in terms of several\ncombinations of parameters.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 12:51:06 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 08:55:37 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 11:58:27 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Hanaka", "Tesshu", ""], ["Kawai", "Kazuma", ""], ["Ono", "Hirotaka", ""]]}, {"id": "2009.10677", "submitter": "Joshua Brakensiek", "authors": "Joshua Brakensiek, Neng Huang, Aaron Potechin, Uri Zwick", "title": "On the Mysteries of MAX NAE-SAT", "comments": "40 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MAX NAE-SAT is a natural optimization problem, closely related to its\nbetter-known relative MAX SAT. The approximability status of MAX NAE-SAT is\nalmost completely understood if all clauses have the same size $k$, for some\n$k\\ge 2$. We refer to this problem as MAX NAE-$\\{k\\}$-SAT. For $k=2$, it is\nessentially the celebrated MAX CUT problem. For $k=3$, it is related to the MAX\nCUT problem in graphs that can be fractionally covered by triangles. For $k\\ge\n4$, it is known that an approximation ratio of $1-\\frac{1}{2^{k-1}}$, obtained\nby choosing a random assignment, is optimal, assuming $P\\ne NP$. For every\n$k\\ge 2$, an approximation ratio of at least $\\frac{7}{8}$ can be obtained for\nMAX NAE-$\\{k\\}$-SAT. There was some hope, therefore, that there is also a\n$\\frac{7}{8}$-approximation algorithm for MAX NAE-SAT, where clauses of all\nsizes are allowed simultaneously.\n  Our main result is that there is no $\\frac{7}{8}$-approximation algorithm for\nMAX NAE-SAT, assuming the unique games conjecture (UGC). In fact, even for\nalmost satisfiable instances of MAX NAE-$\\{3,5\\}$-SAT (i.e., MAX NAE-SAT where\nall clauses have size $3$ or $5$), the best approximation ratio that can be\nachieved, assuming UGC, is at most $\\frac{3(\\sqrt{21}-4)}{2}\\approx 0.8739$.\nUsing calculus of variations, we extend the analysis of O'Donnell and Wu for\nMAX CUT to MAX NAE-$\\{3\\}$-SAT. We obtain an optimal algorithm, assuming UGC,\nfor MAX NAE-$\\{3\\}$-SAT, slightly improving on previous algorithms. The\napproximation ratio of the new algorithm is $\\approx 0.9089$.\n  We complement our theoretical results with some experimental results. We\ndescribe an approximation algorithm for almost satisfiable instances of MAX\nNAE-$\\{3,5\\}$-SAT with a conjectured approximation ratio of 0.8728, and an\napproximation algorithm for almost satisfiable instances of MAX NAE-SAT with a\nconjectured approximation ratio of 0.8698.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 16:55:28 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Brakensiek", "Joshua", ""], ["Huang", "Neng", ""], ["Potechin", "Aaron", ""], ["Zwick", "Uri", ""]]}, {"id": "2009.10709", "submitter": "Johannes Bausch", "authors": "Johannes Bausch", "title": "Fast Black-Box Quantum State Preparation", "comments": "25 pages, 5 figures, 2 tables; v2: amendments to the introduction and\n  appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum state preparation is an important ingredient for other higher-level\nquantum algorithms, such as Hamiltonian simulation, or for loading\ndistributions into a quantum device to be used e.g. in the context of\noptimization tasks such as machine learning. Starting with a generic \"black\nbox\" method devised by Grover in 2000, which employs amplitude amplification to\nload coefficients calculated by an oracle, there has been a long series of\nresults and improvements with various additional conditions on the amplitudes\nto be loaded, culminating in Sanders et al.'s work which avoids almost all\narithmetic during the preparation stage. In this work, we improve upon this\nroutine in two aspects: we reduce the required qubit overhead from $g$ to\n$\\log_2(g)$ in the bit precision $g$ (at a cost of slightly increasing the\ncount of non-Clifford operations), and show how various sets of coefficients\ncan be loaded significantly faster than in $O(\\sqrt N)$ rounds of amplitude\namplification, up to only $O(1)$ many. This exponential speedup translates\nbeyond the black box case.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 17:53:16 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 13:00:47 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Bausch", "Johannes", ""]]}, {"id": "2009.10761", "submitter": "David Harris", "authors": "David G. Harris, Hsin-Hao Su, Hoa T. Vu", "title": "On the Locality of Nash-Williams Forest Decomposition and Star-Forest\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G=(V,E)$ with arboricity $\\alpha$, we study the problem of\ndecomposing the edges of $G$ into $(1+\\epsilon)\\alpha$ disjoint forests in the\ndistributed LOCAL model. Barenboim and Elkin [PODC `08] gave a LOCAL algorithm\nthat computes a $(2+\\epsilon)\\alpha$-forest decomposition using $O(\\frac{\\log\nn}{\\epsilon})$ rounds. Ghaffari and Su [SODA `17] made further progress by\ncomputing a $(1+\\epsilon) \\alpha$-forest decomposition in $O(\\frac{\\log^3\nn}{\\epsilon^4})$ rounds when $\\epsilon \\alpha = \\Omega(\\sqrt{\\alpha \\log n})$,\ni.e. the limit of their algorithm is an $(\\alpha+ \\Omega(\\sqrt{\\alpha \\log\nn}))$-forest decomposition. This algorithm, based on a combinatorial\nconstruction of Alon, McDiarmid \\& Reed [Combinatorica `92], in fact provides a\ndecomposition of the graph into \\emph{star-forests}, i.e. each forest is a\ncollection of stars.\n  Our main result in this paper is to reduce the threshold of $\\epsilon \\alpha$\nin $(1+\\epsilon)\\alpha$-forest decomposition and star-forest decomposition.\nThis further answers the $10^{\\text{th}}$ open question from Barenboim and\nElkin's \"Distributed Graph Algorithms\" book. Moreover, it gives the first\n$(1+\\epsilon)\\alpha$-orientation algorithms with {\\it linear dependencies} on\n$\\epsilon^{-1}$.\n  At a high level, our results for forest-decomposition are based on a\ncombination of network decomposition, load balancing, and a new structural\nresult on local augmenting sequences. Our result for star-forest decomposition\nuses a more careful probabilistic analysis for the construction of Alon,\nMcDiarmid, \\& Reed; the bounds on star-arboricity here were not previously\nknown, even non-constructively.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 18:44:06 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 19:59:25 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 12:08:28 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Harris", "David G.", ""], ["Su", "Hsin-Hao", ""], ["Vu", "Hoa T.", ""]]}, {"id": "2009.10882", "submitter": "EPTCS", "authors": "Jan K\\v{r}et\\'insk\\'y, Emanuel Ramneantu, Alexander Slivinskiy,\n  Maximilian Weininger (Technical University of Munich)", "title": "Comparison of Algorithms for Simple Stochastic Games", "comments": "In Proceedings GandALF 2020, arXiv:2009.09360", "journal-ref": "EPTCS 326, 2020, pp. 131-148", "doi": "10.4204/EPTCS.326.9", "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple stochastic games are turn-based 2.5-player zero-sum graph games with a\nreachability objective. The problem is to compute the winning probability as\nwell as the optimal strategies of both players. In this paper, we compare the\nthree known classes of algorithms -- value iteration, strategy iteration and\nquadratic programming -- both theoretically and practically. Further, we\nsuggest several improvements for all algorithms, including the first approach\nbased on quadratic programming that avoids transforming the stochastic game to\na stopping one. Our extensive experiments show that these improvements can lead\nto significant speed-ups. We implemented all algorithms in PRISM-games 3.0,\nthereby providing the first implementation of quadratic programming for solving\nsimple stochastic games.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 01:26:10 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["K\u0159et\u00ednsk\u00fd", "Jan", "", "Technical University of Munich"], ["Ramneantu", "Emanuel", "", "Technical University of Munich"], ["Slivinskiy", "Alexander", "", "Technical University of Munich"], ["Weininger", "Maximilian", "", "Technical University of Munich"]]}, {"id": "2009.11040", "submitter": "Shogo Isoda", "authors": "S. Isoda, M.Hidaka, Y.Matsuda, H.Suwa, K.Yasumoto", "title": "Timeliness-aware On-site Planning Method for Tour Navigation", "comments": "16 pages, 8 figures, 7 tables,\n  Laboratory(http://ubi-lab.naist.jp/?lang=en)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been a growing interest in travel applications\nthat provide on-site personalized tourist spot recommendations. While generally\nhelpful, most available options offer choices based solely on static\ninformation on places of interest without consideration of such dynamic factors\nas weather, time of day, and congestion, and with a focus on helping the\ntourist decide what single spot to visit next. Such limitations may prevent\nvisitors from optimizing the use of their limited resources (i.e., time and\nmoney). Some existing studies allow users to calculate a semi-optimal tour\nvisiting multiple spots in advance, but their on-site use is difficult due to\nthe large computation time, no consideration of dynamic factors, etc. To deal\nwith this situation, we formulate a tour score approach with three components:\nstatic tourist information on the next spot to visit, dynamic tourist\ninformation on the next spot to visit, and an aggregate measure of satisfaction\nassociated with visiting the next spot and the set of subsequent spots to be\nvisited. Determining the tour route that produces the best overall tour score\nis an NP-hard problem for which we propose three algorithms on the greedy\nmethod. To validate the usefulness of the proposed approach, we applied the\nthree algorithms to 20 points of interest in Higashiyama, Kyoto, Japan, and\nconfirmed that the output solution was superior to the model route for Kyoto,\nwith computation times of the three algorithms of $1.9\\pm0.1$, $2.0\\pm0.1$, and\n$27.0\\pm1.8$ s.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 10:24:12 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Isoda", "S.", ""], ["Hidaka", "M.", ""], ["Matsuda", "Y.", ""], ["Suwa", "H.", ""], ["Yasumoto", "K.", ""]]}, {"id": "2009.11133", "submitter": "Marcos Villagra", "authors": "Sergio Mercado and Marcos Villagra", "title": "Bounds on the Spectral Sparsification of Symmetric and Off-Diagonal\n  Nonnegative Real Matrices", "comments": "9 pages", "journal-ref": "Discrete Mathematics, Algorithms and Applications 2021", "doi": "10.1142/S1793830921501093", "report-no": null, "categories": "cs.DS cs.DM math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We say that a square real matrix $M$ is \\emph{off-diagonal nonnegative} if\nand only if all entries outside its diagonal are nonnegative real numbers. In\nthis note we show that for any off-diagonal nonnegative symmetric matrix $M$,\nthere exists a nonnegative symmetric matrix $\\widehat{M}$ which is sparse and\nclose in spectrum to $M$.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 13:19:44 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Mercado", "Sergio", ""], ["Villagra", "Marcos", ""]]}, {"id": "2009.11178", "submitter": "Jakub Tetek", "authors": "Jakub T\\v{e}tek", "title": "Sampling an Edge Uniformly in Sublinear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area of sublinear algorithms have recently received a lot of attention.\nIn this setting, one has to choose specific access model for the input, as the\nalgorithm does not have time to pre-process or even to see the whole input. A\nfundamental question remained open on the relationship between the two common\nmodels for graphs -- with and without access to the \"random edge\" query --\nnamely whether it is possible to sample an edge uniformly at random in the\nmodel without access to the random edge queries.\n  In this paper, we answer this question positively. Specifically, we give an\nalgorithm solving this problem that runs in expected time $O(\\frac{n}{\\sqrt{m}}\n\\log n)$. This is only a logarithmic factor slower than the lower bound given\nin [5]. Our algorithm uses the algorithm from [7] which we analyze in a more\ncareful way, leading to better bounds in general graphs. We also show a way to\nsample edges $\\epsilon$-close to uniform in expected time $O(\\frac{n}{\\sqrt{m}}\n\\log \\frac{1}{\\epsilon})$, improving upon the best previously known algorithm.\n  We also note that sampling edges from a distribution sufficiently close to\nuniform is sufficient to be able to simulate sublinear algorithms that use the\nrandom edge queries while decreasing the success probability of the algorithm\nonly by $o(1)$. This allows for a much simpler algorithm that can be used to\nemulate random edge queries.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 14:42:04 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["T\u011btek", "Jakub", ""]]}, {"id": "2009.11338", "submitter": "Aditi Laddha", "authors": "Aditi Laddha, Santosh Vempala", "title": "Convergence of Gibbs Sampling: Coordinate Hit-and-Run Mixes Fast", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gibbs Sampler is a general method for sampling high-dimensional\ndistributions, dating back to Turchin, 1971. In each step of the Gibbs Sampler,\nwe pick a random coordinate and re-sample that coordinate from the distribution\ninduced by fixing all other coordinates. While it has become widely used over\nthe past half-century, guarantees of efficient convergence have been elusive.\nWe show that for a convex body $K$ in $\\mathbb{R}^{n}$ with diameter $D$, the\nmixing time of the Coordinate Hit-and-Run (CHAR) algorithm on $K$ is polynomial\nin $n$ and $D$. We also give a lower bound on the conductance of CHAR, showing\nthat it is strictly worse than hit-and-run or the ball walk in the worst case.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 19:02:05 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 15:16:26 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 18:32:57 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Laddha", "Aditi", ""], ["Vempala", "Santosh", ""]]}, {"id": "2009.11435", "submitter": "Xiangyu Gao", "authors": "Xiangyu Gao, Jianzhong Li, Dongjing Miao", "title": "Fully Dynamic Approximate Maximum Independent Set on Massive Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing a maximum independent set (MaxIS) is a fundamental NP-hard problem\nin graph theory, which has important applications in a wide range of areas such\nas social network analysis, graphical information systems and coding theory.\nSince the underlying graphs of numerous applications are always changing\ncontinuously, the problem of maintaining a MaxIS over dynamic graphs has\nreceived increasing attention in recent years. Due to the intractability of\nmaintaining an exact MaxIS, this paper studies the problem of maintaining an\napproximate MaxIS over fully dynamic graphs, where 4 graph update operations\nare allowed \\ie, adding or deleting a vertex or an edge. Based on swap\noperation, we present a novel framework for maintaining an approximate maximum\nindependent set which contains no $k$-swaps and make a deep analysis of\nperformance ratio achieved by it. We implement a dynamic $(\\frac{\\Delta}{2} +\n1)$-approximate algorithm and a more effective algorithm based on one-swap\nvertex and two-swap vertex set respectively and make a further analysis of\ntheir performance based on Power-Law Random graph model. Extensive experiments\nare conducted over real graphs to confirm the effectiveness and efficiency of\nthe proposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 01:06:47 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 02:06:11 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gao", "Xiangyu", ""], ["Li", "Jianzhong", ""], ["Miao", "Dongjing", ""]]}, {"id": "2009.11463", "submitter": "Xiao Hu", "authors": "Xiao Hu, Paraschos Koutris, Spyros Blanas", "title": "Algorithms for a Topology-aware Massively Parallel Computation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the prior work in massively parallel data processing assumes\nhomogeneity, i.e., every computing unit has the same computational capability,\nand can communicate with every other unit with the same latency and bandwidth.\nHowever, this strong assumption of a uniform topology rarely holds in practical\nsettings, where computing units are connected through complex networks. To\naddress this issue, Blanas et al. recently proposed a topology-aware massively\nparallel computation model that integrates the network structure and\nheterogeneity in the modeling cost. The network is modeled as a directed graph,\nwhere each edge is associated with a cost function that depends on the data\ntransferred between the two endpoints. The computation proceeds in synchronous\nrounds, and the cost of each round is measured as the maximum cost over all the\nedges in the network.\n  In this work, we take the first step into investigating three fundamental\ndata processing tasks in this topology-aware parallel model: set intersection,\ncartesian product, and sorting. We focus on network topologies that are tree\ntopologies, and present both lower bounds, as well as (asymptotically) matching\nupper bounds. The optimality of our algorithms is with respect to the initial\ndata distribution among the network nodes, instead of assuming worst-case\ndistribution as in previous results. Apart from the theoretical optimality of\nour results, our protocols are simple, use a constant number of rounds, and we\nbelieve can be implemented in practical settings as well.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 03:22:46 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Hu", "Xiao", ""], ["Koutris", "Paraschos", ""], ["Blanas", "Spyros", ""]]}, {"id": "2009.11552", "submitter": "Laxman Dhulipala", "authors": "Soheil Behnezhad, Laxman Dhulipala, Hossein Esfandiari, Jakub\n  {\\L}\\k{a}cki, Vahab Mirrokni, Warren Schudy", "title": "Parallel Graph Algorithms in Constant Adaptive Rounds: Theory meets\n  Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study fundamental graph problems such as graph connectivity, minimum\nspanning forest (MSF), and approximate maximum (weight) matching in a\ndistributed setting. In particular, we focus on the Adaptive Massively Parallel\nComputation (AMPC) model, which is a theoretical model that captures\nMapReduce-like computation augmented with a distributed hash table.\n  We show the first AMPC algorithms for all of the studied problems that run in\na constant number of rounds and use only $O(n^\\epsilon)$ space per machine,\nwhere $0 < \\epsilon < 1$. Our results improve both upon the previous results in\nthe AMPC model, as well as the best-known results in the MPC model, which is\nthe theoretical model underpinning many popular distributed computation\nframeworks, such as MapReduce, Hadoop, Beam, Pregel and Giraph.\n  Finally, we provide an empirical comparison of the algorithms in the MPC and\nAMPC models in a fault-tolerant distriubted computation environment. We\nempirically evaluate our algorithms on a set of large real-world graphs and\nshow that our AMPC algorithms can achieve improvements in both running time and\nround-complexity over optimized MPC baselines.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 08:47:33 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Behnezhad", "Soheil", ""], ["Dhulipala", "Laxman", ""], ["Esfandiari", "Hossein", ""], ["\u0141\u0105cki", "Jakub", ""], ["Mirrokni", "Vahab", ""], ["Schudy", "Warren", ""]]}, {"id": "2009.11559", "submitter": "Shunsuke Kanda", "authors": "Shunsuke Kanda and Yasuo Tabei", "title": "Dynamic Similarity Search on Integer Sketches", "comments": "Accepted by IEEE ICDM 2020 as a full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity-preserving hashing is a core technique for fast similarity\nsearches, and it randomly maps data points in a metric space to strings of\ndiscrete symbols (i.e., sketches) in the Hamming space. While traditional\nhashing techniques produce binary sketches, recent ones produce integer\nsketches for preserving various similarity measures. However, most similarity\nsearch methods are designed for binary sketches and inefficient for integer\nsketches. Moreover, most methods are either inapplicable or inefficient for\ndynamic datasets, although modern real-world datasets are updated over time. We\npropose dynamic filter trie (DyFT), a dynamic similarity search method for both\nbinary and integer sketches. An extensive experimental analysis using large\nreal-world datasets shows that DyFT performs superiorly with respect to\nscalability, time performance, and memory efficiency. For example, on a huge\ndataset of 216 million data points, DyFT performs a similarity search 6,000\ntimes faster than a state-of-the-art method while reducing to one-thirteenth in\nmemory.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 09:13:17 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Kanda", "Shunsuke", ""], ["Tabei", "Yasuo", ""]]}, {"id": "2009.11780", "submitter": "Andreas Bjorklund", "authors": "Andreas Bj\\\"orklund", "title": "An Asymptotically Fast Polynomial Space Algorithm for Hamiltonicity\n  Detection in Sparse Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a polynomial space Monte Carlo algorithm that given a directed\ngraph on $n$ vertices and average outdegree $\\delta$, detects if the graph has\na Hamiltonian cycle in $2^{n-\\Omega(\\frac{n}{\\delta})}$ time. This asymptotic\nscaling of the savings in the running time matches the fastest known\nexponential space algorithm by Bj\\\"orklund and Williams ICALP 2019. By\ncomparison, the previously best polynomial space algorithm by Kowalik and\nMajewski IPEC 2020 guarantees a $2^{n-\\Omega(\\frac{n}{2^\\delta})}$ time bound.\n  Our algorithm combines for the first time the idea of obtaining a fingerprint\nof the presence of a Hamiltonian cycle through an inclusion--exclusion\nsummation over the Laplacian of the graph from Bj\\\"orklund, Kaski, and Koutis\nICALP 2017, with the idea of sieving for the non-zero terms in an\ninclusion--exclusion summation by listing solutions to systems of linear\nequations over $\\mathbb{Z}_2$ from Bj\\\"orklund and Husfeldt FOCS 2013.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 16:12:52 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""]]}, {"id": "2009.11789", "submitter": "Paulo S\\'ergio Almeida", "authors": "Paulo S\\'ergio Almeida", "title": "A Case for Partitioned Bloom Filters", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a partitioned Bloom Filter the $m$ bit vector is split into $k$ disjoint\n$m/k$ sized parts, one per hash function. Contrary to hardware designs, where\nthey prevail, software implementations mostly adopt standard Bloom filters,\nconsidering partitioned filters slightly worse, due to the slightly larger\nfalse positive rate (FPR). In this paper, by performing an in-depth analysis,\nfirst we show that the FPR advantage of standard Bloom filters is smaller than\nthought; more importantly, by studying the per-element FPR, we show that\nstandard Bloom filters have weak spots in the domain: elements which will be\ntested as false positives much more frequently than expected. This is relevant\nin scenarios where an element is tested against many filters, e.g., in packet\nforwarding. Moreover, standard Bloom filters are prone to exhibit extremely\nweak spots if naive double hashing is used, something occurring in several,\neven mainstream, libraries. Partitioned Bloom filters exhibit a uniform\ndistribution of the FPR over the domain and are robust to the naive use of\ndouble hashing, having no weak spots. Finally, by surveying several usages\nother than testing set membership, we point out the many advantages of having\ndisjoint parts: they can be individually sampled, extracted, added or retired,\nleading to superior designs for, e.g., SIMD usage, size reduction, test of set\ndisjointness, or duplicate detection in streams. Partitioned Bloom filters are\nbetter, and should replace the standard form, both in general purpose libraries\nand as the base for novel designs.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 16:33:22 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Almeida", "Paulo S\u00e9rgio", ""]]}, {"id": "2009.11793", "submitter": "Prafullkumar Tale Mr", "authors": "Saket Saurabh and Prafullkumar Tale", "title": "On the Parameterized Complexity of \\textsc{Maximum Degree Contraction}\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the \\textsc{Maximum Degree Contraction} problem, input is a graph $G$ on\n$n$ vertices, and integers $k, d$, and the objective is to check whether $G$\ncan be transformed into a graph of maximum degree at most $d$, using at most\n$k$ edge contractions. A simple brute-force algorithm that checks all possible\nsets of edges for a solution runs in time $n^{\\mathcal{O}(k)}$. As our first\nresult, we prove that this algorithm is asymptotically optimal, upto constants\nin the exponents, under Exponential Time Hypothesis (\\ETH).\n  Belmonte, Golovach, van't Hof, and Paulusma studied the problem in the realm\nof Parameterized Complexity and proved, among other things, that it admits an\n\\FPT\\ algorithm running in time $(d + k)^{2k} \\cdot n^{\\mathcal{O}(1)} =\n2^{\\mathcal{O}(k \\log (k+d) )} \\cdot n^{\\mathcal{O}(1)}$, and remains \\NP-hard\nfor every constant $d \\ge 2$ (Acta Informatica $(2014)$). We present a\ndifferent \\FPT\\ algorithm that runs in time $2^{\\mathcal{O}(dk)} \\cdot\nn^{\\mathcal{O}(1)}$. In particular, our algorithm runs in time\n$2^{\\mathcal{O}(k)} \\cdot n^{\\mathcal{O}(1)}$, for every fixed $d$. In the same\narticle, the authors asked whether the problem admits a polynomial kernel, when\nparameterized by $k + d$. We answer this question in the negative and prove\nthat it does not admit a polynomial compression unless $\\NP \\subseteq\n\\coNP/poly$.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 16:36:28 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Saurabh", "Saket", ""], ["Tale", "Prafullkumar", ""]]}, {"id": "2009.11840", "submitter": "Martin Kouteck\\'y", "authors": "Martin Kouteck\\'y and Johannes Zink", "title": "Complexity of Scheduling Few Types of Jobs on Related and Unrelated\n  Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of scheduling jobs to machines while minimizing the total makespan,\nthe sum of weighted completion times, or a norm of the load vector, are among\nthe oldest and most fundamental tasks in combinatorial optimization. Since all\nof these problems are in general NP-hard, much attention has been given to the\nregime where there is only a small number $k$ of job types, but possibly the\nnumber of jobs $n$ is large; this is the few job types, high-multiplicity\nregime. Despite many positive results, the hardness boundary of this regime was\nnot understood until now.\n  We show that makespan minimization on uniformly related machines\n($Q|HM|C_{\\max}$) is NP-hard already with $6$ job types, and that the related\nCutting Stock problem is NP-hard already with $8$ item types. For the more\ngeneral unrelated machines model ($R|HM|C_{\\max}$), we show that if either the\nlargest job size $p_{\\max}$, or the number of jobs $n$ are polynomially bounded\nin the instance size $|I|$, there are algorithms with complexity\n$|I|^{\\textrm{poly}(k)}$. Our main result is that this is unlikely to be\nimproved, because $Q||C_{\\max}$ is W[1]-hard parameterized by $k$ already when\n$n$, $p_{\\max}$, and the numbers describing the speeds are polynomial in $|I|$;\nthe same holds for $R|HM|C_{\\max}$ (without speeds) when the job sizes matrix\nhas rank $2$. Our positive and negative results also extend to the objectives\n$\\ell_2$-norm minimization of the load vector and, partially, sum of weighted\ncompletion times $\\sum w_j C_j$.\n  Along the way, we answer affirmatively the question whether makespan\nminimization on identical machines ($P||C_{\\max}$) is fixed-parameter tractable\nparameterized by $k$, extending our understanding of this fundamental problem.\nTogether with our hardness results for $Q||C_{\\max}$ this implies that the\ncomplexity of $P|HM|C_{\\max}$ is the only remaining open case.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 17:38:31 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Kouteck\u00fd", "Martin", ""], ["Zink", "Johannes", ""]]}, {"id": "2009.11867", "submitter": "Samuel Dooley", "authors": "Samuel Dooley, John P. Dickerson", "title": "The Affiliate Matching Problem: On Labor Markets where Firms are Also\n  Interested in the Placement of Previous Workers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.AI cs.CY cs.DS cs.GT q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many labor markets, workers and firms are connected via affiliative\nrelationships. A management consulting firm wishes to both accept the best new\nworkers but also place its current affiliated workers at strong firms.\nSimilarly, a research university wishes to hire strong job market candidates\nwhile also placing its own candidates at strong peer universities. We model\nthis affiliate matching problem in a generalization of the classic stable\nmarriage setting by permitting firms to state preferences over not just which\nworkers to whom they are matched, but also to which firms their affiliated\nworkers are matched. Based on results from a human survey, we find that\nparticipants (acting as firms) give preference to their own affiliate workers\nin surprising ways that violate some assumptions of the classical stable\nmarriage problem. This motivates a nuanced discussion of how stability could be\ndefined in affiliate matching problems; we give an example of a marketplace\nwhich admits a stable match under one natural definition of stability, and does\nnot for that same marketplace under a different, but still natural, definition.\nWe conclude by setting a research agenda toward the creation of a centralized\nclearing mechanism in this general setting.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 01:27:47 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Dooley", "Samuel", ""], ["Dickerson", "John P.", ""]]}, {"id": "2009.12022", "submitter": "Matthew Tsao", "authors": "Marco Pavone, Amin Saberi, Maximilian Schiffer, Matthew Tsao", "title": "Online Hypergraph Matching with Delays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an online hypergraph matching problem with delays, motivated by\nridesharing applications. In this model, users enter a marketplace\nsequentially, and are willing to wait up to $d$ timesteps to be matched, after\nwhich they will leave the system in favor of an outside option. A platform can\nmatch groups of up to $k$ users together, indicating that they will share a\nride. Each group of users yields a match value depending on how compatible they\nare with one another. As an example, in ridesharing, $k$ is the capacity of the\nservice vehicles, and $d$ is the amount of time a user is willing to wait for a\ndriver to be matched to them.\n  We present results for both the utility maximization and cost minimization\nvariants of the problem. In the utility maximization setting, the optimal\ncompetitive ratio is $\\frac{1}{d}$ whenever $k \\geq 3$, and is achievable in\npolynomial-time for any fixed $k$. In the cost minimization variation, when $k\n= 2$, the optimal competitive ratio for deterministic algorithms is\n$\\frac{3}{2}$ and is achieved by a polynomial-time thresholding algorithm. When\n$k>2$, we show that a polynomial-time randomized batching algorithm is $(2 -\n\\frac{1}{d}) \\log k$-competitive, and it is NP-hard to achieve a competitive\nratio better than $\\log k - O (\\log \\log k)$.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 04:01:55 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Pavone", "Marco", ""], ["Saberi", "Amin", ""], ["Schiffer", "Maximilian", ""], ["Tsao", "Matthew", ""]]}, {"id": "2009.12080", "submitter": "Matthias Schymura", "authors": "Jana Cslovjecsek, Romanos Diogenes Malikiosis, M\\'arton Nasz\\'odi and\n  Matthias Schymura", "title": "Computing the covering radius of a polytope with an application to\n  lonely runners", "comments": "19 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with the computational problem of determining the covering\nradius of a rational polytope. This parameter is defined as the minimal\ndilation factor that is needed for the lattice translates of the\ncorrespondingly dilated polytope to cover the whole space. As our main result,\nwe describe a new algorithm for this problem, which is simpler, more efficient\nand easier to implement than the only prior algorithm of Kannan (1992).\nMotivated by a variant of the famous Lonely Runner Conjecture, we use its\ngeometric interpretation in terms of covering radii of zonotopes, and apply our\nalgorithm to prove the first open case of three runners with individual\nstarting points.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 08:08:43 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Cslovjecsek", "Jana", ""], ["Malikiosis", "Romanos Diogenes", ""], ["Nasz\u00f3di", "M\u00e1rton", ""], ["Schymura", "Matthias", ""]]}, {"id": "2009.12127", "submitter": "Sergey Dovgal", "authors": "\\'Elie de Panafieu, Sergey Dovgal, Dimbinaina Ralaivaosaona, Vonjy\n  Rasendrahasina, Stephan Wagner", "title": "The birth of the strong components", "comments": "62 pages, 12 figures, 6 tables. Supplementary computer algebra\n  computations available at https://gitlab.com/vit.north/strong-components-aux", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random directed graphs $D(n,p)$ undergo a phase transition around the point\n$p = 1/n$, and the width of the transition window has been known since the\nworks of Luczak and Seierstad. They have established that as $n \\to \\infty$\nwhen $p = (1 + \\mu n^{-1/3})/n$, the asymptotic probability that the strongly\nconnected components of a random directed graph are only cycles and single\nvertices decreases from 1 to 0 as $\\mu$ goes from $-\\infty$ to $\\infty$.\n  By using techniques from analytic combinatorics, we establish the exact\nlimiting value of this probability as a function of $\\mu$ and provide more\nproperties of the structure of a random digraph around, below and above its\ntransition point. We obtain the limiting probability that a random digraph is\nacyclic and the probability that it has one strongly connected complex\ncomponent with a given difference between the number of edges and vertices\n(called excess). Our result can be extended to the case of several complex\ncomponents with given excesses as well in the whole range of sparse digraphs.\n  Our study is based on a general symbolic method which can deal with a great\nvariety of possible digraph families, and a version of the saddle-point method\nwhich can be systematically applied to the complex contour integrals appearing\nfrom the symbolic method. While the technically easiest model is the model of\nrandom multidigraphs, in which multiple edges are allowed, and where edge\nmultiplicities are sampled independently according to a Poisson distribution\nwith a fixed parameter $p$, we also show how to systematically approach the\nfamily of simple digraphs, where multiple edges are forbidden, and where\n2-cycles are either allowed or not.\n  Our theoretical predictions are supported by numerical simulations, and we\nprovide tables of numerical values for the integrals of Airy functions that\nappear in this study.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 11:02:09 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["de Panafieu", "\u00c9lie", ""], ["Dovgal", "Sergey", ""], ["Ralaivaosaona", "Dimbinaina", ""], ["Rasendrahasina", "Vonjy", ""], ["Wagner", "Stephan", ""]]}, {"id": "2009.12184", "submitter": "Tesshu Hanaka", "authors": "Tesshu Hanaka, Yasuaki Kobayashi, Yusuke Kobayashi, Tsuyoshi Yagita", "title": "Finding a Maximum Minimal Separator: Graph Classes and Fixed-Parameter\n  Tractability", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding a maximum cardinality minimal separator of a\ngraph. This problem is known to be NP-hard even for bipartite graphs. In this\npaper, we strengthen this hardness by showing that for planar bipartite graphs,\nthe problem remains NP-hard. Moreover, for co-bipartite graphs and for line\ngraphs, the problem also remains NP-hard. On the positive side, we give an\nalgorithm deciding whether an input graph has a minimal separator of size at\nleast $k$ that runs in time $2^{O(k)}n^{O(1)}$. We further show that a\nsubexponential parameterized algorithm does not exist unless the Exponential\nTime Hypothesis (ETH) fails. Finally, we discuss a lower bound for polynomial\nkernelizations of this problem.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 12:40:03 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Hanaka", "Tesshu", ""], ["Kobayashi", "Yasuaki", ""], ["Kobayashi", "Yusuke", ""], ["Yagita", "Tsuyoshi", ""]]}, {"id": "2009.12291", "submitter": "Jeremie Leguay M.", "authors": "Yacine Al-Najjar, and Walid Ben-Ameur and Jeremie Leguay", "title": "On the Approximability of Robust Network Design", "comments": "Preprint accepted to Elsevier Theoretical Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the dynamic nature of traffic, we investigate the variant of robust\nnetwork design where we have to determine the capacity to reserve on each link\nso that each demand vector belonging to a polyhedral set can be routed. The\nobjective is either to minimize congestion or a linear cost. Routing is assumed\nto be fractional and dynamic (i.e., dependent on the current traffic vector).\nWe first prove that the robust network design problem with minimum congestion\ncannot be approximated within any constant factor. Then, using the ETH\nconjecture, we get a $\\Omega(\\frac{\\log n}{\\log \\log n})$ lower bound for the\napproximability of this problem. This implies that the well-known $O(\\log n)$\napproximation ratio established by R\\\"{a}cke in 2008 is tight. Using Lagrange\nrelaxation, we obtain a new proof of the $O(\\log n)$ approximation. An\nimportant consequence of the Lagrange-based reduction and our inapproximability\nresults is that the robust network design problem with linear reservation cost\ncannot be approximated within any constant ratio. This answers a long-standing\nopen question of Chekuri (2007). We also give another proof of the result of\nGoyal\\&al (2009) stating that the optimal linear cost under static routing can\nbe $\\Omega(\\log n)$ more expensive than the cost obtained under dynamic\nrouting. Finally, we show that even if only two given paths are allowed for\neach commodity, the robust network design problem with minimum congestion or\nlinear cost is hard to approximate within some constant.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 15:29:59 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 08:40:48 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 20:44:16 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Al-Najjar", "Yacine", ""], ["Ben-Ameur", "Walid", ""], ["Leguay", "Jeremie", ""]]}, {"id": "2009.12309", "submitter": "Guido Br\\\"uckner", "authors": "Guido Br\\\"uckner and Ignaz Rutter", "title": "An SPQR-Tree-Like Embedding Representation for Level Planarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An SPQR-tree is a data structure that efficiently represents all planar\nembeddings of a biconnected planar graph. It is a key tool in a number of\nconstrained planarity testing algorithms, which seek a planar embedding of a\ngraph subject to some given set of constraints.\n  We develop an SPQR-tree-like data structure that represents all level-planar\nembeddings of a biconnected level graph with a single source, called the\nLP-tree, and give a simple algorithm to compute it in linear time. Moreover, we\nshow that LP-trees can be used to adapt three constrained planarity algorithms\nto the level-planar case by using them as a drop-in replacement for SPQR-trees.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 16:03:53 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Br\u00fcckner", "Guido", ""], ["Rutter", "Ignaz", ""]]}, {"id": "2009.12413", "submitter": "Katherine St. John", "authors": "Nathan Davidov, Amanda Hernandez, Justin Jian, Patrick McKenna, K.A.\n  Medlin, Roadra Mojumder, Megan Owen, Andrew Quijano, Amanda Rodriguez,\n  Katherine St. John, Katherine Thai, Meliza Uraga", "title": "Maximum Covering Subtrees for Phylogenetic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-based phylogenetic networks, which may be roughly defined as\nleaf-labeled networks built by adding arcs only between the original tree\nedges, have elegant properties for modeling evolutionary histories. We answer\nan open question of Francis, Semple, and Steel about the complexity of\ndetermining how far a phylogenetic network is from being tree-based, including\nnon-binary phylogenetic networks. We show that finding a phylogenetic tree\ncovering the maximum number of nodes in a phylogenetic network can be be\ncomputed in polynomial time via an encoding into a minimum-cost maximum flow\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 19:47:36 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 16:59:04 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Davidov", "Nathan", ""], ["Hernandez", "Amanda", ""], ["Jian", "Justin", ""], ["McKenna", "Patrick", ""], ["Medlin", "K. A.", ""], ["Mojumder", "Roadra", ""], ["Owen", "Megan", ""], ["Quijano", "Andrew", ""], ["Rodriguez", "Amanda", ""], ["John", "Katherine St.", ""], ["Thai", "Katherine", ""], ["Uraga", "Meliza", ""]]}, {"id": "2009.12442", "submitter": "Karthekeyan Chandrasekaran", "authors": "Karthekeyan Chandrasekaran, Chandra Chekuri", "title": "Hypergraph $k$-cut for fixed $k$ in deterministic polynomial time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Hypergraph-$k$-cut problem. The input consists of a\nhypergraph $G=(V,E)$ with non-negative hyperedge-costs $c: E\\rightarrow R_+$\nand a positive integer $k$. The objective is to find a least-cost subset\n$F\\subseteq E$ such that the number of connected components in $G-F$ is at\nleast $k$. An alternative formulation of the objective is to find a partition\nof $V$ into $k$ non-empty sets $V_1,V_2,\\ldots,V_k$ so as to minimize the cost\nof the hyperedges that cross the partition. Graph-$k$-cut, the special case of\nHypergraph-$k$-cut obtained by restricting to graph inputs, has received\nconsiderable attention. Several different approaches lead to a polynomial-time\nalgorithm for Graph-$k$-cut when $k$ is fixed, starting with the work of\nGoldschmidt and Hochbaum (1988). In contrast, it is only recently that a\nrandomized polynomial time algorithm for Hypergraph-$k$-cut was developed\n(Chandrasekaran, Xu, Yu, 2018) via a subtle generalization of Karger's random\ncontraction approach for graphs. In this work, we develop the first\ndeterministic polynomial time algorithm for Hypergraph-$k$-cut for all fixed\n$k$. We describe two algorithms both of which are based on a divide and conquer\napproach. The first algorithm is simpler and runs in $n^{O(k^2)}$ time while\nthe second one runs in $n^{O(k)}$ time. Our proof relies on new structural\nresults that allow for efficient recovery of the parts of an optimum\n$k$-partition by solving minimum $(S,T)$-terminal cuts. Our techniques give new\ninsights even for Graph-$k$-cut.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 21:41:48 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Chandrasekaran", "Karthekeyan", ""], ["Chekuri", "Chandra", ""]]}, {"id": "2009.12457", "submitter": "Abdurrahman Ya\\c{s}ar", "authors": "Abdurrahman Ya\\c{s}ar and Sivasankaran Rajamanickam and Jonathan Berry\n  and \\\"Umit V. \\c{C}ataly\\\"urek", "title": "A Block-Based Triangle Counting Algorithm on Heterogeneous Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Triangle counting is a fundamental building block in graph algorithms. In\nthis paper, we propose a block-based triangle counting algorithm to reduce data\nmovement during both sequential and parallel execution. Our block-based\nformulation makes the algorithm naturally suitable for heterogeneous\narchitectures. The problem of partitioning the adjacency matrix of a graph is\nwell-studied. Our task decomposition goes one step further: it partitions the\nset of triangles in the graph. By streaming these small tasks to compute\nresources, we can solve problems that do not fit on a device. We demonstrate\nthe effectiveness of our approach by providing an implementation on a compute\nnode with multiple sockets, cores and GPUs. The current state-of-the-art in\ntriangle enumeration processes the Friendster graph in 2.1 seconds, not\nincluding data copy time between CPU and GPU. Using that metric, our approach\nis 20 percent faster. When copy times are included, our algorithm takes 3.2\nseconds. This is 5.6 times faster than the fastest published CPU-only time.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 22:20:30 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Ya\u015far", "Abdurrahman", ""], ["Rajamanickam", "Sivasankaran", ""], ["Berry", "Jonathan", ""], ["\u00c7ataly\u00fcrek", "\u00dcmit V.", ""]]}, {"id": "2009.12685", "submitter": "Chang Shu", "authors": "Luis Rademacher, Chang Shu", "title": "The smoothed complexity of Frank-Wolfe methods via conditioning of\n  random matrices and polytopes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frank-Wolfe methods are popular for optimization over a polytope. One of the\nreasons is because they do not need projection onto the polytope but only\nlinear optimization over it. To understand its complexity, Lacoste-Julien and\nJaggi introduced a condition number for polytopes and showed linear convergence\nfor several variations of the method. The actual running time can still be\nexponential in the worst case (when the condition number is exponential). We\nstudy the smoothed complexity of the condition number, namely the condition\nnumber of small random perturbations of the input polytope and show that it is\npolynomial for any simplex and exponential for general polytopes. Our results\nalso apply to other condition measures of polytopes that have been proposed for\nthe analysis of Frank-Wolfe methods: vertex-facet distance (Beck and Shtern)\nand facial distance (Pe\\~na and Rodr\\'iguez).\n  Our argument for polytopes is a refinement of an argument that we develop to\nstudy the conditioning of random matrices. The basic argument shows that for\n$c>1$ a $d$-by-$n$ random Gaussian matrix with $n \\geq cd$ has a $d$-by-$d$\nsubmatrix with minimum singular value that is exponentially small with high\nprobability. This has consequences on results about the robust uniqueness of\ntensor decompositions.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 20:47:20 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 02:38:18 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Rademacher", "Luis", ""], ["Shu", "Chang", ""]]}, {"id": "2009.12809", "submitter": "Giulio Ermanno Pibiri", "authors": "Giulio Ermanno Pibiri and Shunsuke Kanda", "title": "Rank/Select Queries over Mutable Bitmaps", "comments": "Accepted by Information Systems (INFOSYS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of answering rank/select queries over a bitmap is of utmost\nimportance for many succinct data structures. When the bitmap does not change,\nmany solutions exist in the theoretical and practical side. In this work we\nconsider the case where one is allowed to modify the bitmap via a flip(i)\noperation that toggles its i-th bit. By adapting and properly extending some\nresults concerning prefix-sum data structures, we present a practical solution\nto the problem, tailored for modern CPU instruction sets. Compared to the\nstate-of-the-art, our solution improves runtime with no space degradation.\nMoreover, it does not incur in a significant runtime penalty when compared to\nthe fastest immutable indexes, while providing even lower space overhead.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 10:03:01 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 14:33:42 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Pibiri", "Giulio Ermanno", ""], ["Kanda", "Shunsuke", ""]]}, {"id": "2009.12892", "submitter": "Manuel Sorge", "authors": "Thomas Bellitto, Shaohua Li, Karolina Okrasa, Marcin Pilipczuk, and\n  Manuel Sorge", "title": "The Complexity of Connectivity Problems in Forbidden-Transition Graphs\n  and Edge-Colored Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of forbidden-transition graphs allows for a robust generalization\nof walks in graphs. In a forbidden-transition graph, every pair of edges\nincident to a common vertex is permitted or forbidden; a walk is compatible if\nall pairs of consecutive edges on the walk are permitted. Forbidden-transition\ngraphs and related models have found applications in a variety of fields, such\nas routing in optical telecommunication networks, road networks, and\nbio-informatics.\n  We initiate the study of fundamental connectivity problems from the point of\nview of parameterized complexity, including an in-depth study of tractability\nwith regards to various graph-width parameters. Among several results, we prove\nthat finding a simple compatible path between given endpoints in a\nforbidden-transition graph is W[1]-hard when parameterized by the\nvertex-deletion distance to a linear forest (so it is also hard when\nparameterized by pathwidth or treewidth). On the other hand, we show an\nalgebraic trick that yields tractability when parameterized by treewidth of\nfinding a properly colored Hamiltonian cycle in an edge-colored graph; properly\ncolored walks in edge-colored graphs is one of the most studied special cases\nof compatible walks in forbidden-transition graphs.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 16:35:49 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Bellitto", "Thomas", ""], ["Li", "Shaohua", ""], ["Okrasa", "Karolina", ""], ["Pilipczuk", "Marcin", ""], ["Sorge", "Manuel", ""]]}, {"id": "2009.13090", "submitter": "Arash Rafiey", "authors": "Tomas Feder, Jeff Kinne, Ashwin Murali, Arash Rafiey", "title": "Digraph homomorphism problem and weak near unanimity polymorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding a homomorphism from an input digraph $G$\nto a fixed digraph $H$. We show that if $H$ admits a weak near unanimity\npolymorphism $\\phi$ then deciding whether $G$ admits a homomorphism to $H$\n(HOM($H$)) is polynomial-time solvable. This gives proof of the dichotomy\nconjecture (now dichotomy theorem) by Feder and Vardi. Our approach is\ncombinatorial, and it is simpler than the two algorithms found by Bulatov and\nZhuk. We have implemented our algorithm and show some experimental results. We\nuse our algorithm together with the recent result [38] for recognition of\nMaltsev polymorphisms and decide in polynomial time if a given relational\nstructure $\\mathcal{R}$ admits a weak near unanimity polymorphism.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 06:17:36 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 01:43:02 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Feder", "Tomas", ""], ["Kinne", "Jeff", ""], ["Murali", "Ashwin", ""], ["Rafiey", "Arash", ""]]}, {"id": "2009.13198", "submitter": "Tatsuya Akutsu", "authors": "Xiaoqing Cheng, Wai-Ki Ching, Sini Guo, Tatsuya Akutsu", "title": "Discrimination of attractors with noisy nodes in Boolean networks", "comments": null, "journal-ref": "Automatica 130 (2021) 109630", "doi": "10.1016/j.automatica.2021.109630", "report-no": null, "categories": "cs.DS cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observing the internal state of the whole system using a small number of\nsensor nodes is important in analysis of complex networks. Here, we study the\nproblem of determining the minimum number of sensor nodes to discriminate\nattractors under the assumption that each attractor has at most K noisy nodes.\nWe present exact and approximation algorithms for this minimization problem.\nThe effectiveness of the algorithms is also demonstrated by computational\nexperiments using both synthetic data and realistic biological data.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 10:26:20 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Cheng", "Xiaoqing", ""], ["Ching", "Wai-Ki", ""], ["Guo", "Sini", ""], ["Akutsu", "Tatsuya", ""]]}, {"id": "2009.13257", "submitter": "Zeev Nutov", "authors": "Zeev Nutov", "title": "Approximation algorithms for connectivity augmentation problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Connectivity Augmentation problems we are given a graph $H=(V,E_H)$ and an\nedge set $E$ on $V$, and seek a min-size edge set $J \\subseteq E$ such that $H\n\\cup J$ has larger edge/node connectivity than $H$. In the Edge-Connectivity\nAugmentation problem we need to increase the edge-connectivity by $1$. In the\nBlock-Tree Augmentation problem $H$ is connected and $H \\cup S$ should be\n$2$-connected. In Leaf-to-Leaf Connectivity Augmentation problems every edge in\n$E$ connects minimal deficient sets. For this version we give a simple\ncombinatorial approximation algorithm with ratio $5/3$, improving the previous\n$1.91$ approximation that applies for the general case. We also show by a\nsimple proof that if the Steiner Tree problem admits approximation ratio\n$\\alpha$ then the general version admits approximation ratio\n$1+\\ln(4-x)+\\epsilon$, where $x$ is the solution to the equation\n$1+\\ln(4-x)=\\alpha+(\\alpha-1)x$. For the currently best value of $\\alpha=\\ln\n4+\\epsilon$ this gives ratio $1.942$. This is slightly worse than the best\nratio $1.91$, but has the advantage of using Steiner Tree approximation as a\n\"black box\", giving ratio $< 1.9$ if ratio $\\alpha \\leq 1.35$ can be achieved.\n  In the Element Connectivity Augmentation problem we are given a graph\n$G=(V,E)$, $S \\subseteq V$, and connectivity requirements $\\{r(u,v):u,v \\in\nS\\}$. The goal is to find a min-size set $J$ of new edges on $S$ such that for\nall $u,v \\in S$ the graph $G \\cup J$ contains $r(u,v)$ $uv$-paths such that no\ntwo of them have an edge or a node in $V \\setminus S$ in common. The problem is\nNP-hard even when $\\max_{u,v \\in S} r(u,v)=2$. We obtain approximation ratio\n$3/2$, improving the previous ratio $7/4$.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 12:27:05 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 21:37:16 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Nutov", "Zeev", ""]]}, {"id": "2009.13316", "submitter": "Alexander Eckl", "authors": "Susanne Albers and Alexander Eckl", "title": "Explorable Uncertainty in Scheduling with Non-Uniform Testing Times", "comments": "34 pages total, 16 pages main text including references, 18 pages\n  appendix. Conference: Workshop on Approximation and Online Algorithms (WAOA)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of scheduling with testing in the framework of explorable\nuncertainty models environments where some preliminary action can influence the\nduration of a task. In the model, each job has an unknown processing time that\ncan be revealed by running a test. Alternatively, jobs may be run untested for\nthe duration of a given upper limit. Recently, D\\\"urr et al. [5] have studied\nthe setting where all testing times are of unit size and have given lower and\nupper bounds for the objectives of minimizing the sum of completion times and\nthe makespan on a single machine. In this paper, we extend the problem to\nnon-uniform testing times and present the first competitive algorithms. The\ngeneral setting is motivated for example by online user surveys for market\nprediction or querying centralized databases in distributed computing.\nIntroducing general testing times gives the problem a new flavor and requires\nupdated methods with new techniques in the analysis. We present constant\ncompetitive ratios for the objective of minimizing the sum of completion times\nin the deterministic case, both in the non-preemptive and preemptive setting.\nFor the preemptive setting, we additionally give a first lower bound. We also\npresent a randomized algorithm with improved competitive ratio. Furthermore, we\ngive tight competitive ratios for the objective of minimizing the makespan,\nboth in the deterministic and the randomized setting.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 13:39:26 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Albers", "Susanne", ""], ["Eckl", "Alexander", ""]]}, {"id": "2009.13317", "submitter": "Huy Nguyen", "authors": "Huy L. Nguyen", "title": "A note on differentially private clustering with large additive error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we describe a simple approach to obtain a differentially\nprivate algorithm for k-clustering with nearly the same multiplicative factor\nas any non-private counterpart at the cost of a large polynomial additive\nerror. The approach is the combination of a simple geometric observation\nindependent of privacy consideration and any existing private algorithm with a\nconstant approximation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 13:40:04 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Nguyen", "Huy L.", ""]]}, {"id": "2009.13409", "submitter": "Christian Konrad", "authors": "Lidiya Khalidah binti Khalil and Christian Konrad", "title": "Constructing Large Matchings via Query Access to a Maximal Matching\n  Oracle", "comments": "To appear at FSTTCS 2020, fixed some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-pass streaming algorithm for Maximum Matching have been studied since\nmore than 15 years and various algorithmic results are known today, including\n$2$-pass streaming algorithms that break the $1/2$-approximation barrier, and\n$(1-\\epsilon)$-approximation streaming algorithms that run in $O(\\text{poly}\n\\frac{1}{\\epsilon})$ passes in bipartite graphs and in $O(\n(\\frac{1}{\\epsilon})^{\\frac{1}{\\epsilon}})$ or $O(\\text{poly}\n(\\frac{1}{\\epsilon}) \\cdot \\log n)$ passes in general graphs, where $n$ is the\nnumber of vertices of the input graph. However, proving impossibility results\nfor such algorithms has so far been elusive, and, for example, even the\nexistence of $2$-pass small space streaming algorithms with approximation\nfactor $0.999$ has not yet been ruled out.\n  The key building block of all multi-pass streaming algorithms for Maximum\nMatching is the Greedy matching algorithm. Our aim is to understand the\nlimitations of this approach: How many passes are required if the algorithm\nsolely relies on the invocation of the Greedy algorithm?\n  In this paper, we initiate the study of lower bounds for restricted families\nof multi-pass streaming algorithms for Maximum Matching. We focus on the simple\nyet powerful class of algorithms that in each pass run Greedy on a\nvertex-induced subgraph of the input graph. In bipartite graphs, we show that\n$3$ passes are necessary and sufficient to improve on the trivial approximation\nfactor of $1/2$: We give a lower bound of $0.6$ on the approximation ratio of\nsuch algorithms, which is optimal. We further show that $\\Omega(\n\\frac{1}{\\epsilon})$ passes are required for computing a\n$(1-\\epsilon)$-approximation, even in bipartite graphs. Last, the considered\nclass of algorithms is not well-suited to general graphs: We show that\n$\\Omega(n)$ passes are required in order to improve on the trivial\napproximation factor of $1/2$.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 15:24:24 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 12:27:55 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Khalil", "Lidiya Khalidah binti", ""], ["Konrad", "Christian", ""]]}, {"id": "2009.13494", "submitter": "Marcin Pilipczuk", "authors": "Marcin Pilipczuk and Micha{\\l} Pilipczuk and Pawe{\\l} Rz\\k{a}\\.zewski", "title": "Quasi-polynomial-time algorithm for Independent Set in $P_t$-free graphs\n  via shrinking the space of induced paths", "comments": "Paper accepted to SOSA 2021. The results on $C_{>t}$-free graphs from\n  v1 were moved to arXiv:2007.11402", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent breakthrough work, Gartland and Lokshtanov [FOCS 2020] showed a\nquasi-polynomial-time algorithm for Maximum Weight Independent Set in\n$P_t$-free graphs, that is, graphs excluding a fixed path as an induced\nsubgraph. Their algorithm runs in time $n^{\\mathcal{O}(\\log^3 n)}$, where $t$\nis assumed to be a constant. Inspired by their ideas, we present an arguably\nsimpler algorithm with an improved running time bound of $n^{\\mathcal{O}(\\log^2\nn)}$. Our main insight is that a connected $P_t$-free graph always contains a\nvertex $w$ whose neighborhood intersects, for a constant fraction of pairs\n$\\{u,v\\} \\in \\binom{V(G)}{2}$, a constant fraction of induced $u-v$ paths.\nSince a $P_t$-free graph contains $\\mathcal{O}(n^{t-1})$ induced paths in\ntotal, branching on such a vertex and recursing independently on the connected\ncomponents leads to a quasi-polynomial running time bound. We also show that\nthe same approach can be used to obtain quasi-polynomial-time algorithms for\nrelated problems, including Maximum Weight Induced Matching and 3-Coloring.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 17:38:01 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 11:32:11 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""]]}, {"id": "2009.13510", "submitter": "Uri Stemmer", "authors": "Amos Beimel, Iftach Haitner, Kobbi Nissim, Uri Stemmer", "title": "On the Round Complexity of the Shuffle Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shuffle model of differential privacy was proposed as a viable model for\nperforming distributed differentially private computations. Informally, the\nmodel consists of an untrusted analyzer that receives messages sent by\nparticipating parties via a shuffle functionality, the latter potentially\ndisassociates messages from their senders. Prior work focused on one-round\ndifferentially private shuffle model protocols, demonstrating that\nfunctionalities such as addition and histograms can be performed in this model\nwith accuracy levels similar to that of the curator model of differential\nprivacy, where the computation is performed by a fully trusted party.\n  Focusing on the round complexity of the shuffle model, we ask in this work\nwhat can be computed in the shuffle model of differential privacy with two\nrounds. Ishai et al. [FOCS 2006] showed how to use one round of the shuffle to\nestablish secret keys between every two parties. Using this primitive to\nsimulate a general secure multi-party protocol increases its round complexity\nby one. We show how two parties can use one round of the shuffle to send secret\nmessages without having to first establish a secret key, hence retaining round\ncomplexity. Combining this primitive with the two-round semi-honest protocol of\nApplebaun et al. [TCC 2018], we obtain that every randomized functionality can\nbe computed in the shuffle model with an honest majority, in merely two rounds.\nThis includes any differentially private computation. We then move to examine\ndifferentially private computations in the shuffle model that (i) do not\nrequire the assumption of an honest majority, or (ii) do not admit one-round\nprotocols, even with an honest majority. For that, we introduce two\ncomputational tasks: the common-element problem and the nested-common-element\nproblem, for which we show separations between one-round and two-round\nprotocols.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 17:57:42 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Beimel", "Amos", ""], ["Haitner", "Iftach", ""], ["Nissim", "Kobbi", ""], ["Stemmer", "Uri", ""]]}, {"id": "2009.13512", "submitter": "Sitan Chen", "authors": "Sitan Chen, Adam R. Klivans, Raghu Meka", "title": "Learning Deep ReLU Networks Is Fixed-Parameter Tractable", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning an unknown ReLU network with respect to\nGaussian inputs and obtain the first nontrivial results for networks of depth\nmore than two. We give an algorithm whose running time is a fixed polynomial in\nthe ambient dimension and some (exponentially large) function of only the\nnetwork's parameters.\n  Our bounds depend on the number of hidden units, depth, spectral norm of the\nweight matrices, and Lipschitz constant of the overall network (we show that\nsome dependence on the Lipschitz constant is necessary). We also give a bound\nthat is doubly exponential in the size of the network but is independent of\nspectral norm. These results provably cannot be obtained using gradient-based\nmethods and give the first example of a class of efficiently learnable neural\nnetworks that gradient descent will fail to learn.\n  In contrast, prior work for learning networks of depth three or higher\nrequires exponential time in the ambient dimension, even when the above\nparameters are bounded by a constant. Additionally, all prior work for the\ndepth-two case requires well-conditioned weights and/or positive coefficients\nto obtain efficient run-times. Our algorithm does not require these\nassumptions.\n  Our main technical tool is a type of filtered PCA that can be used to\niteratively recover an approximate basis for the subspace spanned by the hidden\nunits in the first layer. Our analysis leverages new structural results on\nlattice polynomials from tropical geometry.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 17:58:43 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Chen", "Sitan", ""], ["Klivans", "Adam R.", ""], ["Meka", "Raghu", ""]]}, {"id": "2009.13689", "submitter": "Olga Ohrimenko", "authors": "Sajin Sasy and Olga Ohrimenko", "title": "Oblivious Sampling Algorithms for Private Data Analysis", "comments": "Appeared in NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study secure and privacy-preserving data analysis based on queries\nexecuted on samples from a dataset. Trusted execution environments (TEEs) can\nbe used to protect the content of the data during query computation, while\nsupporting differential-private (DP) queries in TEEs provides record privacy\nwhen query output is revealed. Support for sample-based queries is attractive\ndue to \\emph{privacy amplification} since not all dataset is used to answer a\nquery but only a small subset. However, extracting data samples with TEEs while\nproving strong DP guarantees is not trivial as secrecy of sample indices has to\nbe preserved. To this end, we design efficient secure variants of common\nsampling algorithms. Experimentally we show that accuracy of models trained\nwith shuffling and sampling is the same for differentially private models for\nMNIST and CIFAR-10, while sampling provides stronger privacy guarantees than\nshuffling.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 23:45:30 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Sasy", "Sajin", ""], ["Ohrimenko", "Olga", ""]]}, {"id": "2009.13701", "submitter": "Haosen Wen", "authors": "Haosen Wen, Wentao Cai, Mingzhe Du, Louis Jenkins, Benjamin Valpey,\n  Michael L. Scott", "title": "Montage: A General System for Buffered Durably Linearizable Data\n  Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent emergence of fast, dense, nonvolatile main memory suggests that\ncertain long-lived data might remain in its natural pointer-rich format across\nprogram runs and hardware reboots. Operations on such data must be instrumented\nwith explicit write-back and fence instructions to ensure consistency in the\nwake of a crash. Techniques to minimize the cost of this instrumentation are an\nactive topic of research.\n  We present what we believe to be the first general-purpose approach to\nbuilding buffered durably linearizable persistent data structures, and a\nsystem, Montage, to support that approach. Montage is built on top of the\nRalloc nonblocking persistent allocator. It employs a slow-ticking epoch clock,\nand ensures that no operation appears to span an epoch boundary. It also\narranges to persist only that data minimally required to reconstruct the\nstructure after a crash. If a crash occurs in epoch $e$, all work performed in\nepochs $e$ and $e-1$ is lost, but work from prior epochs is preserved.\n  We describe the implementation of Montage, argue its correctness, and report\nunprecedented throughput for persistent queues, sets/mappings, and general\ngraphs.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 00:30:33 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Wen", "Haosen", ""], ["Cai", "Wentao", ""], ["Du", "Mingzhe", ""], ["Jenkins", "Louis", ""], ["Valpey", "Benjamin", ""], ["Scott", "Michael L.", ""]]}, {"id": "2009.13768", "submitter": "Kanat Tangwongsan", "authors": "Kanat Tangwongsan, Martin Hirzel, Scott Schneider", "title": "In-Order Sliding-Window Aggregation in Worst-Case Constant Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliding-window aggregation is a widely-used approach for extracting insights\nfrom the most recent portion of a data stream. The aggregations of interest can\nusually be expressed as binary operators that are associative but not\nnecessarily commutative nor invertible. Non-invertible operators, however, are\ndifficult to support efficiently. In a 2017 conference paper, we introduced\nDABA, the first algorithm for sliding-window aggregation with worst-case\nconstant time. Before DABA, if a window had size $n$, the best published\nalgorithms would require $O(\\log n)$ aggregation steps per window\noperation---and while for strictly in-order streams, this bound could be\nimproved to $O(1)$ aggregation steps on average, it was not known how to\nachieve an $O(1)$ bound for the worst-case, which is critical for\nlatency-sensitive applications.\n  This article is an extended version of our 2017 paper. Besides describing\nDABA in more detail, this article introduces a new variant, DABA Lite, which\nachieves the same time bounds in less memory. Whereas DABA requires space for\nstoring $2n$ partial aggregates, DABA Lite only requires space for $n+2$\npartial aggregates. Our experiments on synthetic and real data support the\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 04:11:13 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Tangwongsan", "Kanat", ""], ["Hirzel", "Martin", ""], ["Schneider", "Scott", ""]]}, {"id": "2009.13949", "submitter": "Kishen N Gowda", "authors": "Kishen N. Gowda, Aditya Lonkar, Fahad Panolan, Vraj Patel, Saket\n  Saurabh", "title": "Improved FPT Algorithms for Deletion to Forest-like Structures", "comments": "ISAAC 2020, 36 pages. arXiv admin note: text overlap with\n  arXiv:1906.12298, arXiv:1103.0534 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Feedback Vertex Set problem is undoubtedly one of the most well-studied\nproblems in Parameterized Complexity. In this problem, given an undirected\ngraph $G$ and a non-negative integer $k$, the objective is to test whether\nthere exists a subset $S\\subseteq V(G)$ of size at most $k$ such that $G-S$ is\na forest. After a long line of improvement, recently, Li and Nederlof [SODA,\n2020] designed a randomized algorithm for the problem running in time\n$\\mathcal{O}^{\\star}(2.7^k)$. In the Parameterized Complexity literature,\nseveral problems around Feedback Vertex Set have been studied. Some of these\ninclude Independent Feedback Vertex Set (where the set $S$ should be an\nindependent set in $G$), Almost Forest Deletion and Pseudoforest Deletion. In\nPseudoforest Deletion, each connected component in $G-S$ has at most one cycle\nin it. However, in Almost Forest Deletion, the input is a graph $G$ and\nnon-negative integers $k,\\ell \\in \\mathbb{N}$, and the objective is to test\nwhether there exists a vertex subset $S$ of size at most $k$, such that $G-S$\nis $\\ell$ edges away from a forest. In this paper, using the methodology of Li\nand Nederlof [SODA, 2020], we obtain the current fastest algorithms for all\nthese problems. In particular we obtain following randomized algorithms.\n  1) Independent Feedback Vertex Set can be solved in time\n$\\mathcal{O}^{\\star}(2.7^k)$.\n  2) Pseudo Forest Deletion can be solved in time\n$\\mathcal{O}^{\\star}(2.85^k)$.\n  3) Almost Forest Deletion can be solved in $\\mathcal{O}^{\\star}(\\min\\{2.85^k\n\\cdot 8.54^\\ell,2.7^k \\cdot 36.61^\\ell,3^k \\cdot 1.78^\\ell\\})$.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 19:04:38 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Gowda", "Kishen N.", ""], ["Lonkar", "Aditya", ""], ["Panolan", "Fahad", ""], ["Patel", "Vraj", ""], ["Saurabh", "Saket", ""]]}, {"id": "2009.13998", "submitter": "Christopher Harshaw", "authors": "Moran Feldman, Christopher Harshaw, Amin Karbasi", "title": "How Do You Want Your Greedy: Simultaneous or Repeated?", "comments": "Included analysis of RepeatedGreedy and open source Julia package", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SimultaneousGreedys, a deterministic algorithm for constrained\nsubmodular maximization. At a high level, the algorithm maintains $\\ell$\nsolutions and greedily updates them in a simultaneous fashion.\nSimultaneousGreedys achieves the tightest known approximation guarantees for\nboth $k$-extendible systems and the more general $k$-systems, which are\n$(k+1)^2/k = k + \\mathcal{O}(1)$ and $(1 + \\sqrt{k+2})^2 = k +\n\\mathcal{O}(\\sqrt{k})$, respectively. This is in contrast to previous\nalgorithms, which are designed to provide tight approximation guarantees in one\nsetting, but not both. We also improve the analysis of RepeatedGreedy, showing\nthat it achieves an approximation ratio of $k + \\mathcal{O}(\\sqrt{k})$ for\n$k$-systems when allowed to run for $\\mathcal{O}(\\sqrt{k})$ iterations, an\nimprovement in both the runtime and approximation over previous analyses. We\ndemonstrate that both algorithms may be modified to run in nearly linear time\nwith an arbitrarily small loss in the approximation.\n  Both SimultaneousGreedys and RepeatedGreedy are flexible enough to\nincorporate the intersection of $m$ additional knapsack constraints, while\nretaining similar approximation guarantees: both algorithms yield an\napproximation guarantee of roughly $k + 2m + \\mathcal{O}(\\sqrt{k+m})$ for\n$k$-systems and SimultaneousGreedys enjoys an improved approximation guarantee\nof $k+2m + \\mathcal{O}(\\sqrt{m})$ for $k$-extendible systems. To complement our\nalgorithmic contributions, we provide a hardness result which states that no\nalgorithm making polynomially many oracle queries can achieve an approximation\nbetter than $k + 1/2 + \\varepsilon$. We also present SubmodularGreedy.jl, a\nJulia package which implements these algorithms and may be downloaded at\nhttps://github.com/crharshaw/SubmodularGreedy.jl . Finally, we test the\neffectiveness of these algorithms on real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 13:34:09 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 18:02:50 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Feldman", "Moran", ""], ["Harshaw", "Christopher", ""], ["Karbasi", "Amin", ""]]}, {"id": "2009.14004", "submitter": "Piyush Srivastava", "authors": "Hariharan Narayanan and Piyush Srivastava", "title": "On the mixing time of coordinate Hit-and-Run", "comments": "14 pages, fixed minor typos and added reference to independent work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain a polynomial upper bound on the mixing time $T_{CHR}(\\epsilon)$ of\nthe coordinate Hit-and-Run random walk on an $n-$dimensional convex body, where\n$T_{CHR}(\\epsilon)$ is the number of steps needed in order to reach within\n$\\epsilon$ of the uniform distribution with respect to the total variation\ndistance, starting from a warm start (i.e., a distribution which has a density\nwith respect to the uniform distribution on the convex body that is bounded\nabove by a constant). Our upper bound is polynomial in $n, R$ and\n$\\frac{1}{\\epsilon}$, where we assume that the convex body contains the unit\n$\\Vert\\cdot\\Vert_\\infty$-unit ball $B_\\infty$ and is contained in its\n$R$-dilation $R\\cdot B_\\infty$. Whether coordinate Hit-and-Run has a polynomial\nmixing time has been an open question.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 13:41:35 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 11:36:57 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Narayanan", "Hariharan", ""], ["Srivastava", "Piyush", ""]]}, {"id": "2009.14043", "submitter": "Henri Lotze", "authors": "Hans-Joachim Boeckenhauer, Elisabet Burjons, Juraj Hromkovic, Henri\n  Lotze, Peter Rossmanith", "title": "Online Simple Knapsack with Reservation Costs", "comments": "27 pages, 4 figures, second version with additional results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the Online Simple Knapsack Problem we are given a knapsack of unit size 1.\nItems of size smaller or equal to 1 are presented in an iterative way and an\nalgorithm has to decide whether to permanently include or reject each item into\nthe knapsack without any knowledge about the rest of the instance. The goal is\nthen to pack the knapsack as full as possible. In this work we introduce a\nthird option additional to those of packing and rejecting an item, namely that\nof reserving an item for the cost of a fixed fraction $\\alpha$ of its size. An\nalgorithm may pay this fraction in order to postpone its decision on whether to\ninclude or reject the item until after the last item of the instance was\npresented.\n  We find that adding the possibility of reservation makes the problem\nconstantly competitive with varying competitive ratios depending on the value\nof $\\alpha$. We give upper and lower bounds for the whole range of reservation\ncosts, with tight bounds for costs up to $1/6$ -- an area that is strictly\n2-competitive, for costs between $\\sqrt{2}-1$ and $1$ -- an area that is\nstrictly $(2+\\alpha)$-competitive up to $\\phi -1$, and strictly\n$1/(1-\\alpha)$-competitive above $\\phi-1$, where $\\phi$ is the golden ratio.\n  We find a counterintuitive characteristic of the problem: Intuitively, one\nmay expect that the possibility of rejecting items becomes more helpful for an\nonline algorithm with growing reservation costs. However, for higher\nreservation costs above $\\sqrt{2}-1$, an algorithm that is unable to reject any\nitems tightly matches the lower bound and is thus the best possible. On the\nother hand, for any positive reservation cost smaller than $1/6$, any algorithm\nthat is unable to reject items performs considerably worse than one that is\nable to reject.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 14:27:39 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 16:48:58 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Boeckenhauer", "Hans-Joachim", ""], ["Burjons", "Elisabet", ""], ["Hromkovic", "Juraj", ""], ["Lotze", "Henri", ""], ["Rossmanith", "Peter", ""]]}, {"id": "2009.14358", "submitter": "Erin Taylor", "authors": "Pankaj K. Agarwal and Hsien-Chih Chang and Kamesh Munagala and Erin\n  Taylor and Emo Welzl", "title": "Clustering under Perturbation Stability in Near-Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of center-based clustering in low-dimensional\nEuclidean spaces under the perturbation stability assumption. An instance is\n$\\alpha$-stable if the underlying optimal clustering continues to remain\noptimal even when all pairwise distances are arbitrarily perturbed by a factor\nof at most $\\alpha$. Our main contribution is in presenting efficient exact\nalgorithms for $\\alpha$-stable clustering instances whose running times depend\nnear-linearly on the size of the data set when $\\alpha \\ge 2 + \\sqrt{3}$. For\n$k$-center and $k$-means problems, our algorithms also achieve polynomial\ndependence on the number of clusters, $k$, when $\\alpha \\geq 2 + \\sqrt{3} +\n\\epsilon$ for any constant $\\epsilon > 0$ in any fixed dimension. For\n$k$-median, our algorithms have polynomial dependence on $k$ for $\\alpha > 5$\nin any fixed dimension; and for $\\alpha \\geq 2 + \\sqrt{3}$ in two dimensions.\nOur algorithms are simple, and only require applying techniques such as local\nsearch or dynamic programming to a suitably modified metric space, combined\nwith careful choice of data structures.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 00:47:15 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Agarwal", "Pankaj K.", ""], ["Chang", "Hsien-Chih", ""], ["Munagala", "Kamesh", ""], ["Taylor", "Erin", ""], ["Welzl", "Emo", ""]]}, {"id": "2009.14479", "submitter": "Adam Polak", "authors": "Andrea Lincoln, Adam Polak, Virginia Vassilevska Williams", "title": "Monochromatic Triangles, Intermediate Matrix Products, and Convolutions", "comments": "Presented at ITCS 2020. Abstract abridged to meet arXiv requirements", "journal-ref": null, "doi": "10.4230/LIPIcs.ITCS.2020.53", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most studied linear algebraic operation, matrix multiplication, has\nsurprisingly fast $O(n^\\omega)$ time algorithms for $\\omega<2.373$. On the\nother hand, the $(\\min,+)$ matrix product which is at the heart of many\nfundamental graph problems such as APSP, has received only minor improvements\nover its brute-force cubic running time and is widely conjectured to require\n$n^{3-o(1)}$ time. There is a plethora of matrix products and graph problems\nwhose complexity seems to lie in the middle of these two problems. For\ninstance, the Min-Max matrix product, the Minimum Witness matrix product, APSP\nin directed unweighted graphs and determining whether an edge-colored graph\ncontains a monochromatic triangle, can all be solved in $\\tilde\nO(n^{(3+\\omega)/2})$ time. A similar phenomenon occurs for convolution\nproblems, where analogous intermediate problems can be solved in $\\tilde\nO(n^{1.5})$ time.\n  Can one improve upon the running times for these intermediate problems, in\neither the matrix product or the convolution world? Or, alternatively, can one\nrelate these problems to each other and to other key problems in a meaningful\nway?\n  This paper makes progress on these questions by providing a network of\nfine-grained reductions. We show for instance that APSP in directed unweighted\ngraphs and Minimum Witness product can be reduced to both the Min-Max product\nand a variant of the monochromatic triangle problem. We also show that a\nnatural convolution variant of monochromatic triangle is fine-grained\nequivalent to the famous 3SUM problem. As this variant is solvable in\n$O(n^{1.5})$ time and 3SUM is in $O(n^2)$ time (and is conjectured to require\n$n^{2-o(1)}$ time), our result gives the first fine-grained equivalence between\nnatural problems of different running times.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 07:33:23 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Lincoln", "Andrea", ""], ["Polak", "Adam", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "2009.14590", "submitter": "Lorenzo De Stefani", "authors": "Lorenzo De Stefani", "title": "Communication-Optimal Parallel Standard and Karatsuba Integer\n  Multiplication in the Distributed Memory Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present COPSIM a parallel implementation of standard integer\nmultiplication for the distributed memory setting, and COPK a parallel\nimplementation of Karatsuba's fast integer multiplication algorithm for a\ndistributed memory setting. When using $\\mathcal{P}$ processors, each equipped\nwith a local non-shared memory, to compute the product of tho $n$-digits\ninteger numbers, under mild conditions, our algorithms achieve optimal speedup\nof the computational time. That is, $\\mathcal{O}\\left(n^2/\\mathcal{P}\\right)$\nfor COPSIM, and $\\mathcal{O}\\left(n^{\\log_2 3}/\\mathcal{P}\\right)$ for COPK.\nThe total amount of memory required across the processors is\n$\\mathcal{O}\\left(n\\right)$, that is, within a constant factor of the minimum\nspace required to store the input values. We rigorously analyze the\nInput/Output (I/O) cost of the proposed algorithms. We show that their\nbandwidth cost (i.e., the number of memory words sent or received by at least\none processors) matches asymptotically corresponding known I/O lower bounds,\nand their latency (i.e., the number of messages sent or received in the\nalgorithm's critical execution path) is asymptotically within a multiplicative\nfactor $\\mathcal{O}\\left(\\log^2_2 \\mathcal{P}\\right)$ of the corresponding\nknown I/O lower bounds. Hence, our algorithms are asymptotically optimal with\nrespect to the bandwidth cost and almost asymptotically optimal with respect to\nthe latency cost.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 12:10:54 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["De Stefani", "Lorenzo", ""]]}, {"id": "2009.14716", "submitter": "Shay Mozes", "authors": "Viktor Fredslund-Hansen, Shay Mozes, Christian Wulff-Nilsen", "title": "Truly Subquadratic Exact Distance Oracles with Constant Query Time for\n  Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected, unweighted planar graph $G$ with $n$ vertices, we\npresent a truly subquadratic size distance oracle for reporting exact\nshortest-path distances between any pair of vertices of $G$ in constant time.\nFor any $\\varepsilon > 0$, our distance oracle takes up\n$O(n^{5/3+\\varepsilon})$ space and is capable of answering shortest-path\ndistance queries exactly for any pair of vertices of $G$ in worst-case time\n$O(\\log (1/\\varepsilon))$. Previously no truly sub-quadratic size distance\noracles with constant query time for answering exact all-pairs shortest paths\ndistance queries existed.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 14:51:41 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Fredslund-Hansen", "Viktor", ""], ["Mozes", "Shay", ""], ["Wulff-Nilsen", "Christian", ""]]}, {"id": "2009.14729", "submitter": "Shaked Matar", "authors": "Elkin Michael, Matar Shaked", "title": "Deterministic PRAM Approximate Shortest Paths in Polylogarithmic Time\n  and Slightly Super-Linear Work", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study a $(1+\\epsilon)$-approximate single-source shortest paths\n(henceforth, $(1+\\epsilon)$-SSSP) in $n$-vertex undirected, weighted graphs in\nthe parallel (PRAM) model of computation. A randomized algorithm with\npolylogarithmic time and slightly super-linear work $\\tilde{O}(|E|\\cdot\nn^\\rho)$, for an arbitrarily small $\\rho>0$, was given by Cohen [Coh94] more\nthan $25$ years ago.\n  Exciting progress on this problem was achieved in recent years\n[ElkinN17,ElkinN19,Li19,AndoniSZ19], culminating in randomized polylogarithmic\ntime and $\\tilde{O}(|E|)$ work. However, the question of whether there exists a\ndeterministic counterpart of Cohen's algorithm remained wide open.\n  In the current paper we devise the first deterministic polylogarithmic-time\nalgorithm for this fundamental problem, with work $\\tilde{O}(|E|\\cdot n^\\rho)$,\nfor an arbitrarily small $\\rho>0$. This result is based on the first efficient\ndeterministic parallel algorithm for building hopsets, which we devise in this\npaper.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 15:09:54 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Michael", "Elkin", ""], ["Shaked", "Matar", ""]]}, {"id": "2009.14746", "submitter": "Eranda Cela", "authors": "Eranda Cela (1), Vladimir G. Deineko (2), Gerhard J. Woeginger (3)\n  ((1) Graz University of Technology, (2) Warwick business School, (3) RWTH\n  Aachen)", "title": "Travelling salesman paths on Demidenko matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the path version of the Travelling Salesman Problem (Path-TSP), a salesman\nis looking for the shortest Hamiltonian path through a set of n cities. The\nsalesman has to start his journey at a given city s, visit every city exactly\nonce, and finally end his trip at another given city t. In this paper we\nidentify a new polynomially solvable case of the Path-TSP where the distance\nmatrix of the cities is a so-called Demidenko matrix. We identify a number of\ncrucial combinatorial properties of the optimal solution, and we design a\ndynamic program with time complexity $O(n^6)$.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 15:35:04 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Cela", "Eranda", ""], ["Deineko", "Vladimir G.", ""], ["Woeginger", "Gerhard J.", ""]]}, {"id": "2009.14793", "submitter": "Edin Husic", "authors": "Jugal Garg, Edin Husic and Laszlo A. Vegh", "title": "Approximating Nash Social Welfare under Rado Valuations", "comments": "44 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DM cs.DS cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating maximum Nash social welfare (NSW)\nwhile allocating a set of indivisible items to $n$ agents. The NSW is a popular\nobjective that provides a balanced tradeoff between the often conflicting\nrequirements of fairness and efficiency, defined as the weighted geometric mean\nof agents' valuations. For the symmetric additive case of the problem, where\nagents have the same weight with additive valuations, the first constant-factor\napproximation algorithm was obtained in 2015. This led to a flurry of work\nobtaining constant-factor approximation algorithms for the symmetric case under\nmild generalizations of additive, and $O(n)$-approximation algorithms for more\ngeneral valuations and for the asymmetric case.\n  In this paper, we make significant progress towards both symmetric and\nasymmetric NSW problems. We present the first constant-factor approximation\nalgorithm for the symmetric case under Rado valuations. Rado valuations form a\ngeneral class of valuation functions that arise from maximum cost independent\nmatching problems, including as special cases assignment (OXS) valuations and\nweighted matroid rank functions. Furthermore, our approach also gives the first\nconstant-factor approximation algorithm for the asymmetric case under Rado\nvaluations, provided that the maximum ratio between the weights is bounded by a\nconstant.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 17:07:51 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Garg", "Jugal", ""], ["Husic", "Edin", ""], ["Vegh", "Laszlo A.", ""]]}]