[{"id": "1012.0006", "submitter": "Christian Schulz", "authors": "Peter Sanders and Christian Schulz", "title": "Engineering Multilevel Graph Partitioning Algorithms", "comments": "fixed a problem with the medium sized instances", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multi-level graph partitioning algorithm using novel local\nimprovement algorithms and global search strategies transferred from the\nmulti-grid community. Local improvement algorithms are based max-flow min-cut\ncomputations and more localized FM searches. By combining these techniques, we\nobtain an algorithm that is fast on the one hand and on the other hand is able\nto improve the best known partitioning results for many inputs. For example, in\nWalshaw's well known benchmark tables we achieve 317 improvements for the\ntables 1%, 3% and 5% imbalance. Moreover, in 118 additional cases we have been\nable to reproduce the best cut in this benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 30 Nov 2010 21:01:48 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2011 14:54:48 GMT"}, {"version": "v3", "created": "Mon, 4 Apr 2011 12:08:32 GMT"}], "update_date": "2011-04-05", "authors_parsed": [["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1012.0012", "submitter": "Erik Jan Van Leeuwen", "authors": "Danny Hermelin and Matthias Mnich and Erik Jan van Leeuwen and Gerhard\n  Woeginger", "title": "Domination When the Stars Are Out", "comments": "Revised some proofs compared to v2. Significantly expanded proofs and\n  several additional results compared to v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We algorithmize the recent structural characterization for claw-free graphs\nby Chudnovsky and Seymour. Building on this result, we show that Dominating Set\non claw-free graphs is (i) fixed-parameter tractable and (ii) even possesses a\npolynomial kernel. To complement these results, we establish that Dominating\nSet is not fixed-parameter tractable on the slightly larger class of graphs\nthat exclude K_{1,4} as an induced subgraph (K_{1,4}-free graphs). We show that\nour algorithmization can also be used to show that the related Connected\nDominating Set problem is fixed-parameter tractable on claw-free graphs. To\ncomplement that result, we show that Connected Dominating Set has no polynomial\nkernel on claw-free graphs and is not fixed-parameter tractable on K_{1,4}-free\ngraphs. Combined, our results provide a dichotomy for Dominating Set and\nConnected Dominating Set on K_{1,L}-free graphs and show that the problem is\nfixed-parameter tractable if and only if L <= 3.\n", "versions": [{"version": "v1", "created": "Tue, 30 Nov 2010 21:05:40 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 13:05:40 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2019 20:28:18 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Hermelin", "Danny", ""], ["Mnich", "Matthias", ""], ["van Leeuwen", "Erik Jan", ""], ["Woeginger", "Gerhard", ""]]}, {"id": "1012.0032", "submitter": "Zolt\\'an K\\'asa", "authors": "Antal Iv\\'anyi, Bal\\'azs Nov\\'ak", "title": "Testing of sequences by simulation", "comments": null, "journal-ref": "Acta Univ. Sapientiae Informatica, vol. 2 no. 2 (2010) 135-153", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\xi$ be a random integer vector, having uniform distribution\n\\[\\mathbf{P} \\{\\xi = (i_1,i_2,...,i_n) = 1/n^n \\} \\ \\hbox{for} \\ 1 \\leq\ni_1,i_2,...,i_n\\leq n.\\] A realization $(i_1,i_2,...,i_n)$ of $\\xi$ is called\n\\textit{good}, if its elements are different. We present algorithms\n\\textsc{Linear}, \\textsc{Backward}, \\textsc{Forward}, \\textsc{Tree},\n\\textsc{Garbage}, \\textsc{Bucket} which decide whether a given realization is\ngood. We analyse the number of comparisons and running time of these algorithms\nusing simulation gathering data on all possible inputs for small values of $n$\nand generating random inputs for large values of $n$.\n", "versions": [{"version": "v1", "created": "Tue, 30 Nov 2010 21:41:24 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Iv\u00e1nyi", "Antal", ""], ["Nov\u00e1k", "Bal\u00e1zs", ""]]}, {"id": "1012.0058", "submitter": "Zolt\\'an K\\'asa", "authors": "Zolt\\'an K\\'atai", "title": "Modelling dynamic programming problems by generalized d-graphs", "comments": null, "journal-ref": "Acta Univ. Sapientiae Informatica, vol. 2 no. 2 (2010) 210-230", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the concept of generalized d-graph (admitting\ncycles) as special dependency-graphs for modelling dynamic programming (DP)\nproblems. We describe the d-graph versions of three famous single-source\nshortest algorithms (The algorithm based on the topological order of the\nvertices, Dijkstra algorithm and Bellman-Ford algorithm), which can be viewed\nas general DP strategies in the case of three different class of optimization\nproblems. The new modelling method also makes possible to classify DP problems\nand the corresponding DP strategies in term of graph theory.\n", "versions": [{"version": "v1", "created": "Tue, 30 Nov 2010 22:59:46 GMT"}], "update_date": "2010-12-07", "authors_parsed": [["K\u00e1tai", "Zolt\u00e1n", ""]]}, {"id": "1012.0255", "submitter": "Julia Chuzhoy", "authors": "Julia Chuzhoy", "title": "An Algorithm for the Graph Crossing Number Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Minimum Crossing Number problem: given an $n$-vertex graph $G$,\nthe goal is to find a drawing of $G$ in the plane with minimum number of edge\ncrossings. This is one of the central problems in topological graph theory,\nthat has been studied extensively over the past three decades. The first\nnon-trivial efficient algorithm for the problem, due to Leighton and Rao,\nachieved an $O(n\\log^4n)$-approximation for bounded degree graphs. This\nalgorithm has since been improved by poly-logarithmic factors, with the best\ncurrent approximation ratio standing on $O(n \\poly(d) \\log^{3/2}n)$ for graphs\nwith maximum degree $d$. In contrast, only APX-hardness is known on the\nnegative side.\n  In this paper we present an efficient randomized algorithm to find a drawing\nof any $n$-vertex graph $G$ in the plane with $O(OPT^{10}\\cdot \\poly(d \\log\nn))$ crossings, where $OPT$ is the number of crossings in the optimal solution,\nand $d$ is the maximum vertex degree in $G$. This result implies an\n$\\tilde{O}(n^{9/10} \\poly(d))$-approximation for Minimum Crossing Number, thus\nbreaking the long-standing $\\tilde{O}(n)$-approximation barrier for\nbounded-degree graphs.\n", "versions": [{"version": "v1", "created": "Wed, 1 Dec 2010 17:45:29 GMT"}], "update_date": "2010-12-02", "authors_parsed": [["Chuzhoy", "Julia", ""]]}, {"id": "1012.0256", "submitter": "Pavlos Efraimidis", "authors": "Pavlos S. Efraimidis", "title": "Weighted Random Sampling over Data Streams", "comments": "Corrected minor typos. Infeasible items are now additionally called\n  \"overweight\" items (WRS-N-P). Enriched the Introduction (Section 1) with more\n  text and references to related work. Revised the description of sampling with\n  a bounded number of replacements (Section 4.2)", "journal-ref": null, "doi": null, "report-no": "Technical Report LPDP-2010-03", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a comprehensive treatment of weighted random\nsampling (WRS) over data streams. More precisely, we examine two natural\ninterpretations of the item weights, describe an existing algorithm for each\ncase ([2, 4]), discuss sampling with and without replacement and show\nadaptations of the algorithms for several WRS problems and evolving data\nstreams.\n", "versions": [{"version": "v1", "created": "Wed, 1 Dec 2010 17:48:48 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2015 09:06:35 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Efraimidis", "Pavlos S.", ""]]}, {"id": "1012.0259", "submitter": "Pavlos Efraimidis", "authors": "Pavlos S. Efraimidis", "title": "(\\alpha, \\beta) Fibonacci Search", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Report LPDP-2010-02", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knuth [12, Page 417] states that \"the (program of the) Fibonaccian search\ntechnique looks very mysterious at first glance\" and that \"it seems to work by\nmagic\". In this work, we show that there is even more magic in Fibonaccian (or\nelse Fibonacci) search. We present a generalized Fibonacci procedure that\nfollows perfectly the implicit optimal decision tree for search problems where\nthe cost of each comparison depends on its outcome.\n", "versions": [{"version": "v1", "created": "Wed, 1 Dec 2010 18:00:01 GMT"}], "update_date": "2010-12-02", "authors_parsed": [["Efraimidis", "Pavlos S.", ""]]}, {"id": "1012.0280", "submitter": "Szymon Grabowski", "authors": "Szymon Grabowski, Simone Faro, Emanuele Giaquinta", "title": "String Matching with Inversions and Translocations in Linear Average\n  Time (Most of the Time)", "comments": "9 pages. A slightly shorter version of this manuscript was submitted\n  to Information Processing Letters", "journal-ref": null, "doi": "10.1016/j.ipl.2011.02.015", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient algorithm for finding all approximate occurrences of\na given pattern $p$ of length $m$ in a text $t$ of length $n$ allowing for\ntranslocations of equal length adjacent factors and inversions of factors. The\nalgorithm is based on an efficient filtering method and has an\n$\\bigO(nm\\max(\\alpha, \\beta))$-time complexity in the worst case and\n$\\bigO(\\max(\\alpha, \\beta))$-space complexity, where $\\alpha$ and $\\beta$ are\nrespectively the maximum length of the factors involved in any translocation\nand inversion. Moreover we show that under the assumptions of equiprobability\nand independence of characters our algorithm has a $\\bigO(n)$ average time\ncomplexity, whenever $\\sigma = \\Omega(\\log m / \\log\\log^{1-\\epsilon} m)$, where\n$\\epsilon > 0$ and $\\sigma$ is the dimension of the alphabet. Experiments show\nthat the new proposed algorithm achieves very good results in practical cases.\n", "versions": [{"version": "v1", "created": "Wed, 1 Dec 2010 19:53:05 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Grabowski", "Szymon", ""], ["Faro", "Simone", ""], ["Giaquinta", "Emanuele", ""]]}, {"id": "1012.0557", "submitter": "Andrey Rumyantsev", "authors": "Andrey Rumyantsev", "title": "Infinite computable version of Lovasz Local Lemma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lov\\'asz Local Lemma (LLL) is a probabilistic tool that allows us to prove\nthe existence of combinatorial objects in the cases when standard probabilistic\nargument does not work (there are many partly independent conditions).\n  LLL can be also used to prove the consistency of an infinite set of\nconditions, using standard compactness argument (if an infinite set of\nconditions is inconsistent, then some finite part of it is inconsistent, too,\nwhich contradicts LLL). In this way we show that objects satisfying all the\nconditions do exist (though the probability of this event equals~$0$). However,\nif we are interested in finding a computable solution that satisfies all the\nconstraints, compactness arguments do not work anymore.\n  Moser and Tardos recently gave a nice constructive proof of LLL. Lance\nFortnow asked whether one can apply Moser--Tardos technique to prove the\nexistence of a computable solution. We show that this is indeed possible (under\nalmost the same conditions as used in the non-constructive version).\n", "versions": [{"version": "v1", "created": "Thu, 2 Dec 2010 20:11:02 GMT"}], "update_date": "2010-12-03", "authors_parsed": [["Rumyantsev", "Andrey", ""]]}, {"id": "1012.0674", "submitter": "Pierre Guillon", "authors": "Thomas Worsch, Hidenosuke Nishio", "title": "Real-Time Sorting of Binary Numbers on One-Dimensional CA", "comments": "Journ\\'ees Automates Cellulaires 2010, Turku : Finland (2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new fast (real time) sorter of binary numbers by one-dimensional cellular\nautomata is proposed. It sorts a list of n numbers represented by k-bits each\nin exactly nk steps. This is only one step more than a lower bound.\n", "versions": [{"version": "v1", "created": "Fri, 3 Dec 2010 09:33:18 GMT"}], "update_date": "2010-12-06", "authors_parsed": [["Worsch", "Thomas", ""], ["Nishio", "Hidenosuke", ""]]}, {"id": "1012.0956", "submitter": "Ioannis Paparrizos", "authors": "Ioannis Paparrizos", "title": "A tight bound on the worst-case number of comparisons for Floyd's heap\n  construction algorithm", "comments": "This (full) paper was accepted for publication in the 37th\n  International Conference on Current Trends in Theory and Practice of Computer\n  Science (SOFSEM2011), 22-28 January 2011, Novy Smokovec, Slovakia (9 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a tight bound on the worst-case number of comparisons for\nFloyd's well known heap construction algorithm, is derived. It is shown that at\nmost 2n-2{\\mu}(n)-{\\sigma}(n) comparisons are executed in the worst case, where\n{\\mu}(n) is the number of ones and {\\sigma}(n) is the number of zeros after the\nlast one in the binary representation of the number of keys n.\n", "versions": [{"version": "v1", "created": "Sun, 5 Dec 2010 00:01:09 GMT"}, {"version": "v2", "created": "Mon, 3 Jan 2011 12:49:09 GMT"}, {"version": "v3", "created": "Mon, 18 Apr 2011 11:25:44 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Paparrizos", "Ioannis", ""]]}, {"id": "1012.1129", "submitter": "Yann Ponty", "authors": "Dani\\`ele Gardy (PRISM), Yann Ponty (LIX, INRIA Saclay - Ile de\n  France)", "title": "Weighted random generation of context-free languages: Analysis of\n  collisions in random urn occupancy models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work analyzes the redundancy of sets of combinatorial objects\nproduced by a weighted random generation algorithm proposed by Denise et al.\nThis scheme associates weights to the terminals symbols of a weighted\ncontext-free grammar, extends this weight definition multiplicatively on words,\nand draws words of length $n$ with probability proportional their weight. We\ninvestigate the level of redundancy within a sample of $k$ word, the proportion\nof the total probability covered by $k$ words (coverage), the time (number of\ngenerations) of the first collision, and the time of the full collection. For\nthese four questions, we use an analytic urn analogy to derive asymptotic\nestimates and/or polynomially computable exact forms. We illustrate these tools\nby an analysis of an RNA secondary structure statistical sampling algorithm\nintroduced by Ding et al.\n", "versions": [{"version": "v1", "created": "Mon, 6 Dec 2010 11:09:44 GMT"}], "update_date": "2010-12-07", "authors_parsed": [["Gardy", "Dani\u00e8le", "", "PRISM"], ["Ponty", "Yann", "", "LIX, INRIA Saclay - Ile de\n  France"]]}, {"id": "1012.1163", "submitter": "Heiko R\\\"oglin", "authors": "Tobias Brunsch and Heiko Roeglin", "title": "Lower Bounds for the Smoothed Number of Pareto optimal Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2009, Roeglin and Teng showed that the smoothed number of Pareto optimal\nsolutions of linear multi-criteria optimization problems is polynomially\nbounded in the number $n$ of variables and the maximum density $\\phi$ of the\nsemi-random input model for any fixed number of objective functions. Their\nbound is, however, not very practical because the exponents grow exponentially\nin the number $d+1$ of objective functions. In a recent breakthrough, Moitra\nand O'Donnell improved this bound significantly to $O(n^{2d} \\phi^{d(d+1)/2})$.\n  An \"intriguing problem\", which Moitra and O'Donnell formulate in their paper,\nis how much further this bound can be improved. The previous lower bounds do\nnot exclude the possibility of a polynomial upper bound whose degree does not\ndepend on $d$. In this paper we resolve this question by constructing a class\nof instances with $\\Omega ((n \\phi)^{(d-\\log{d}) \\cdot (1-\\Theta{1/\\phi})})$\nPareto optimal solutions in expectation. For the bi-criteria case we present a\nhigher lower bound of $\\Omega (n^2 \\phi^{1 - \\Theta{1/\\phi}})$, which almost\nmatches the known upper bound of $O(n^2 \\phi)$.\n", "versions": [{"version": "v1", "created": "Mon, 6 Dec 2010 13:14:36 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Brunsch", "Tobias", ""], ["Roeglin", "Heiko", ""]]}, {"id": "1012.1219", "submitter": "Pierre Guillon", "authors": "Alex Borello (LIF)", "title": "A Simulation of Oblivious Multi-Head One-Way Finite Automata by\n  Real-Time Cellular Automata", "comments": "Journ\\'ees Automates Cellulaires 2010, Turku : Finland (2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CG cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the simulation of a simple, yet significantly\npowerful, sequential model by cellular automata. The simulated model is called\noblivious multi-head one-way finite automata and is characterized by having its\nheads moving only forward, on a trajectory that only depends on the length of\nthe input. While the original finite automaton works in linear time, its\ncorresponding cellular automaton performs the same task in real time, that is,\nexactly the length of the input. Although not truly a speed-up, the simulation\nmay be interesting and reminds us of the open question about the equivalence of\nlinear and real times on cellular automata.\n", "versions": [{"version": "v1", "created": "Fri, 3 Dec 2010 09:36:30 GMT"}], "update_date": "2010-12-07", "authors_parsed": [["Borello", "Alex", "", "LIF"]]}, {"id": "1012.1338", "submitter": "Simone Faro", "authors": "Domenico Cantone and Simone Faro", "title": "On Tuning the Bad-Character Rule: the Worst-Character Rule", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we present the worst-character rule, an efficient variation of\nthe bad-character heuristic for the exact string matching problem, firstly\nintroduced in the well-known Boyer-Moore algorithm. Our proposed rule selects a\nposition relative to the current shift which yields the largest average\nadvancement, according to the characters distribution in the text. Experimental\nresults show that the worst-character rule achieves very good results\nespecially in the case of long patterns or small alphabets in random texts and\nin the case of texts in natural languages.\n", "versions": [{"version": "v1", "created": "Mon, 6 Dec 2010 21:16:37 GMT"}], "update_date": "2010-12-08", "authors_parsed": [["Cantone", "Domenico", ""], ["Faro", "Simone", ""]]}, {"id": "1012.1529", "submitter": "Ferdinando Cicalese", "authors": "Ferdinando Cicalese, Martin Milanic and Ugo Vaccaro", "title": "On the approximability and exact algorithms for vector domination and\n  related problems in graphs", "comments": "In the version published in DAM, weaker lower bounds for vector\n  domination and total vector domination were stated. Being these problems\n  generalization of domination and total domination, the lower bounds of 0.2267\n  ln n and (1-epsilon) ln n clearly hold for both problems, unless P = NP or NP\n  \\subseteq DTIME(n^{O(log log n)}), respectively. The claims are corrected in\n  the present version", "journal-ref": "Discrete Applied Mathematics 2012 (online)", "doi": "10.1016/j.dam.2012.10.007", "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two graph optimization problems called vector domination and\ntotal vector domination. In vector domination one seeks a small subset S of\nvertices of a graph such that any vertex outside S has a prescribed number of\nneighbors in S. In total vector domination, the requirement is extended to all\nvertices of the graph. We prove that these problems (and several variants\nthereof) cannot be approximated to within a factor of clnn, where c is a\nsuitable constant and n is the number of the vertices, unless P = NP. We also\nshow that two natural greedy strategies have approximation factors ln D+O(1),\nwhere D is the maximum degree of the input graph. We also provide exact\npolynomial time algorithms for several classes of graphs. Our results extend,\nimprove, and unify several results previously known in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 15:11:42 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2013 23:14:52 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Cicalese", "Ferdinando", ""], ["Milanic", "Martin", ""], ["Vaccaro", "Ugo", ""]]}, {"id": "1012.1547", "submitter": "Martin Hoefer", "authors": "Martin Hoefer, Michal Penn, Maria Polukarov, Alexander Skopalik,\n  Berhold V\\\"ocking", "title": "Considerate Equilibrium", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the existence and computational complexity of coalitional\nstability concepts based on social networks. Our concepts represent a natural\nand rich combinatorial generalization of a recent approach termed partition\nequilibrium. We assume that players in a strategic game are embedded in a\nsocial network, and there are coordination constraints that restrict the\npotential coalitions that can jointly deviate in the game to the set of cliques\nin the social network. In addition, players act in a \"considerate\" fashion to\nignore potentially profitable (group) deviations if the change in their\nstrategy may cause a decrease of utility to their neighbors.\n  We study the properties of such considerate equilibria in application to the\nclass of resource selection games (RSG). Our main result proves existence of a\nconsiderate equilibrium in all symmetric RSG with strictly increasing delays,\nfor any social network among the players. The existence proof is constructive\nand yields an efficient algorithm. In fact, the computed considerate\nequilibrium is a Nash equilibrium for the standard RSG showing that there\nexists a state that is stable against selfish and considerate behavior\nsimultaneously. In addition, we show results on convergence of considerate\ndynamics.\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 16:44:20 GMT"}], "update_date": "2010-12-08", "authors_parsed": [["Hoefer", "Martin", ""], ["Penn", "Michal", ""], ["Polukarov", "Maria", ""], ["Skopalik", "Alexander", ""], ["V\u00f6cking", "Berhold", ""]]}, {"id": "1012.1577", "submitter": "Jelani Nelson", "authors": "Daniel M. Kane, Jelani Nelson", "title": "Sparser Johnson-Lindenstrauss Transforms", "comments": "v6: journal version, minor changes, added Remark 23; v5: modified\n  abstract, fixed typos, added open problem section; v4: simplified section 4\n  by giving 1 analysis that covers both constructions; v3: proof of Theorem 25\n  in v2 was written incorrectly, now fixed; v2: Added another construction\n  achieving same upper bound, and added proof of near-tight lower bound for DKS\n  scheme", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM cs.IT math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give two different and simple constructions for dimensionality reduction\nin $\\ell_2$ via linear mappings that are sparse: only an\n$O(\\varepsilon)$-fraction of entries in each column of our embedding matrices\nare non-zero to achieve distortion $1+\\varepsilon$ with high probability, while\nstill achieving the asymptotically optimal number of rows. These are the first\nconstructions to provide subconstant sparsity for all values of parameters,\nimproving upon previous works of Achlioptas (JCSS 2003) and Dasgupta, Kumar,\nand Sarl\\'{o}s (STOC 2010). Such distributions can be used to speed up\napplications where $\\ell_2$ dimensionality reduction is used.\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 18:48:12 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2011 17:24:00 GMT"}, {"version": "v3", "created": "Thu, 28 Apr 2011 12:22:53 GMT"}, {"version": "v4", "created": "Fri, 7 Dec 2012 21:35:40 GMT"}, {"version": "v5", "created": "Wed, 21 Aug 2013 19:01:03 GMT"}, {"version": "v6", "created": "Wed, 5 Feb 2014 22:39:26 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Kane", "Daniel M.", ""], ["Nelson", "Jelani", ""]]}, {"id": "1012.1824", "submitter": "Massimo Torquati", "authors": "Massimo Torquati", "title": "Single-Producer/Single-Consumer Queues on Shared Cache Multi-Core\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using efficient point-to-point communication channels is critical for\nimplementing fine grained parallel program on modern shared cache multi-core\narchitectures.\n  This report discusses in detail several implementations of wait-free\nSingle-Producer/Single-Consumer queue (SPSC), and presents a novel and\nefficient algorithm for the implementation of an unbounded wait-free SPSC queue\n(uSPSC). The correctness proof of the new algorithm, and several performance\nmeasurements based on simple synthetic benchmark and microbenchmark, are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Wed, 8 Dec 2010 18:50:29 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Torquati", "Massimo", ""]]}, {"id": "1012.1850", "submitter": "Marco Molinaro", "authors": "Inge Li Gortz and Marco Molinaro and Viswanath Nagarajan and R. Ravi", "title": "Capacitated Vehicle Routing with Non-Uniform Speeds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacitated vehicle routing problem (CVRP) involves distributing\n(identical) items from a depot to a set of demand locations, using a single\ncapacitated vehicle. We study a generalization of this problem to the setting\nof multiple vehicles having non-uniform speeds (that we call Heterogenous\nCVRP), and present a constant-factor approximation algorithm.\n  The technical heart of our result lies in achieving a constant approximation\nto the following TSP variant (called Heterogenous TSP). Given a metric denoting\ndistances between vertices, a depot r containing k vehicles with possibly\ndifferent speeds, the goal is to find a tour for each vehicle (starting and\nending at r), so that every vertex is covered in some tour and the maximum\ncompletion time is minimized. This problem is precisely Heterogenous CVRP when\nvehicles are uncapacitated.\n  The presence of non-uniform speeds introduces difficulties for employing\nstandard tour-splitting techniques. In order to get a better understanding of\nthis technique in our context, we appeal to ideas from the 2-approximation for\nscheduling in parallel machine of Lenstra et al.. This motivates the\nintroduction of a new approximate MST construction called Level-Prim, which is\nrelated to Light Approximate Shortest-path Trees. The last component of our\nalgorithm involves partitioning the Level-Prim tree and matching the resulting\nparts to vehicles. This decomposition is more subtle than usual since now we\nneed to enforce correlation between the size of the parts and their distances\nto the depot.\n", "versions": [{"version": "v1", "created": "Wed, 8 Dec 2010 20:55:06 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Gortz", "Inge Li", ""], ["Molinaro", "Marco", ""], ["Nagarajan", "Viswanath", ""], ["Ravi", "R.", ""]]}, {"id": "1012.1886", "submitter": "Martin Strauss", "authors": "Ely Porat and Martin J. Strauss", "title": "Sublinear Time, Measurement-Optimal, Sparse Recovery For All", "comments": "Corrected argument with minor change to results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approximate sparse recovery system in ell_1 norm formally consists of\nparameters N, k, epsilon an m-by-N measurement matrix, Phi, and a decoding\nalgorithm, D. Given a vector, x, where x_k denotes the optimal k-term\napproximation to x, the system approximates x by hat_x = D(Phi.x), which must\nsatisfy\n  ||hat_x - x||_1 <= (1+epsilon)||x - x_k||_1.\n  Among the goals in designing such systems are minimizing m and the runtime of\nD. We consider the \"forall\" model, in which a single matrix Phi is used for all\nsignals x.\n  All previous algorithms that use the optimal number m=O(k log(N/k)) of\nmeasurements require superlinear time Omega(N log(N/k)). In this paper, we give\nthe first algorithm for this problem that uses the optimum number of\nmeasurements (up to a constant factor) and runs in sublinear time o(N) when\nk=o(N), assuming access to a data structure requiring space and preprocessing\nO(N).\n", "versions": [{"version": "v1", "created": "Wed, 8 Dec 2010 22:39:52 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2011 13:08:41 GMT"}], "update_date": "2011-07-15", "authors_parsed": [["Porat", "Ely", ""], ["Strauss", "Martin J.", ""]]}, {"id": "1012.2202", "submitter": "Nicolaos Matsakis", "authors": "Nicolaos Matsakis", "title": "Transforming a random graph drawing into a Lombardi drawing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visualization of any graph plays important role in various aspects, such\nas graph drawing software. Complex systems (like large databases or networks)\nthat have a graph structure should be properly visualized in order to avoid\nobfuscation. One way to provide an aesthetic improvement to a graph\nvisualization is to apply a force-directed drawing algorithm to it. This\nmethod, that emerged in the 60's views graphs as spring systems that exert\nforces (repulsive or attractive) to the nodes.\n  A Lombardi drawing of a graph is a drawing where the edges are drawn as\ncircular arcs (straight edges are considered circular arcs of infinite radius)\nwith perfect angular resolution. This means, that consecutive edges around a\nvertex are equally spaced around it. In other words, each angle between the\ntangents of two consecutive edges is equal to $2\\pi/d$ where d is the degree of\nthat specific vertex. The requirement of using circular edges in graphs when we\nwant to provide perfect angular resolution is necessary, since even cycle\ngraphs cannot be drawn with straight edges when perfect angular resolution is\nneeded.\n  In this survey, we provide an algorithm that takes as input a random drawing\nof a graph and provides its Lombardi drawing, giving a proper visualization of\nthe graph.\n", "versions": [{"version": "v1", "created": "Fri, 10 Dec 2010 08:31:21 GMT"}, {"version": "v2", "created": "Mon, 13 Dec 2010 08:22:20 GMT"}], "update_date": "2010-12-14", "authors_parsed": [["Matsakis", "Nicolaos", ""]]}, {"id": "1012.2289", "submitter": "Nicolai H\\\"ahnle", "authors": "Friedrich Eisenbrand, Nicolai H\\\"ahnle, Martin Niemeier", "title": "Covering Cubes and the Closest Vector Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the currently fastest randomized (1+epsilon)-approximation\nalgorithm for the closest vector problem in the infinity norm. The running time\nof our method depends on the dimension n and the approximation guarantee\nepsilon by 2^O(n) (log 1/epsilon)^O(n)$ which improves upon the\n(2+1/epsilon)^O(n) running time of the previously best algorithm by Bl\\\"omer\nand Naewe.\n  Our algorithm is based on a solution of the following geometric covering\nproblem that is of interest of its own: Given epsilon in (0,1), how many\nellipsoids are necessary to cover the cube [-1+epsilon, 1-epsilon]^n such that\nall ellipsoids are contained in the standard unit cube [-1,1]^n? We provide an\nalmost optimal bound for the case where the ellipsoids are restricted to be\naxis-parallel.\n  We then apply our covering scheme to a variation of this covering problem\nwhere one wants to cover [-1+epsilon,1-epsilon]^n with parallelepipeds that, if\nscaled by two, are still contained in the unit cube. Thereby, we obtain a\nmethod to boost any 2-approximation algorithm for closest-vector in the\ninfinity norm to a (1+epsilon)-approximation algorithm that has the desired\nrunning time.\n", "versions": [{"version": "v1", "created": "Fri, 10 Dec 2010 15:20:22 GMT"}], "update_date": "2010-12-13", "authors_parsed": [["Eisenbrand", "Friedrich", ""], ["H\u00e4hnle", "Nicolai", ""], ["Niemeier", "Martin", ""]]}, {"id": "1012.2547", "submitter": "Simone Faro", "authors": "Simone Faro and Thierry Lecroq", "title": "The Exact String Matching Problem: a Comprehensive Experimental\n  Evaluation", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the online exact string matching problem which consists\nin finding all occurrences of a given pattern p in a text t. It is an\nextensively studied problem in computer science, mainly due to its direct\napplications to such diverse areas as text, image and signal processing, speech\nanalysis and recognition, data compression, information retrieval,\ncomputational biology and chemistry. Since 1970 more than 80 string matching\nalgorithms have been proposed, and more than 50% of them in the last ten years.\nIn this note we present a comprehensive list of all string matching algorithms\nand present experimental results in order to compare them from a practical\npoint of view. From our experimental evaluation it turns out that the\nperformance of the algorithms are quite different for different alphabet sizes\nand pattern length.\n", "versions": [{"version": "v1", "created": "Sun, 12 Dec 2010 14:47:00 GMT"}], "update_date": "2010-12-14", "authors_parsed": [["Faro", "Simone", ""], ["Lecroq", "Thierry", ""]]}, {"id": "1012.2573", "submitter": "Marek Karpinski", "authors": "Jean Cardinal, Marek Karpinski, Richard Schmied, Claus Viehmann", "title": "Approximating Vertex Cover in Dense Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimum vertex cover problem in hypergraphs in which every\nhyperedge has size k (also known as minimum hitting set problem, or minimum set\ncover with element frequency k). Simple algorithms exist that provide\nk-approximations, and this is believed to be the best possible approximation\nachievable in polynomial time. We show how to exploit density and regularity\nproperties of the input hypergraph to break this barrier. In particular, we\nprovide a randomized polynomial-time algorithm with approximation factor k/(1\n+(k-1)d/(k Delta)), where d and Delta are the average and maximum degree,\nrespectively, and Delta must be Omega(n^{k-1}/log n). The proposed algorithm\ngeneralizes the recursive sampling technique of Imamura and Iwama (SODA'05) for\nvertex cover in dense graphs. As a corollary, we obtain an approximation factor\nk/(2-1/k) for subdense regular hypergraphs, which is shown to be the best\npossible under the unique games conjecture.\n", "versions": [{"version": "v1", "created": "Sun, 12 Dec 2010 18:47:05 GMT"}], "update_date": "2010-12-14", "authors_parsed": [["Cardinal", "Jean", ""], ["Karpinski", "Marek", ""], ["Schmied", "Richard", ""], ["Viehmann", "Claus", ""]]}, {"id": "1012.2825", "submitter": "Yahav Nussbaum", "authors": "Yahav Nussbaum", "title": "Improved distance queries in planar graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several known data structures that answer distance queries between\ntwo arbitrary vertices in a planar graph. The tradeoff is among preprocessing\ntime, storage space and query time. In this paper we present three data\nstructures that answer such queries, each with its own advantage over previous\ndata structures. The first one improves the query time of data structures of\nlinear space. The second improves the preprocessing time of data structures\nwith a space bound of O(n^(4/3)) or higher while matching the best known query\ntime. The third data structure improves the query time for a similar range of\nspace bounds, at the expense of a longer preprocessing time. The techniques\nthat we use include modifying the parameters of planar graph decompositions,\ncombining the different advantages of existing data structures, and using the\nMonge property for finding minimum elements of matrices.\n", "versions": [{"version": "v1", "created": "Mon, 13 Dec 2010 18:32:51 GMT"}, {"version": "v2", "created": "Tue, 22 Feb 2011 17:56:07 GMT"}], "update_date": "2011-02-23", "authors_parsed": [["Nussbaum", "Yahav", ""]]}, {"id": "1012.3011", "submitter": "Edo Liberty", "authors": "Nir Ailon and Noa Avigdor-Elgrabli and Edo Liberty", "title": "An Improved Algorithm for Bipartite Correlation Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite Correlation clustering is the problem of generating a set of\ndisjoint bi-cliques on a set of nodes while minimizing the symmetric difference\nto a bipartite input graph. The number or size of the output clusters is not\nconstrained in any way. The best known approximation algorithm for this problem\ngives a factor of 11. This result and all previous ones involve solving large\nlinear or semi-definite programs which become prohibitive even for modestly\nsized tasks. In this paper we present an improved factor 4 approximation\nalgorithm to this problem using a simple combinatorial algorithm which does not\nrequire solving large convex programs. The analysis extends a method developed\nby Ailon, Charikar and Alantha in 2008, where a randomized pivoting algorithm\nwas analyzed for obtaining a 3-approximation algorithm for Correlation\nClustering, which is the same problem on graphs (not bipartite). The analysis\nfor Correlation Clustering there required defining events for structures\ncontaining 3 vertices and using the probability of these events to produce a\nfeasible solution to a dual of a certain natural LP bounding the optimal cost.\nIt is tempting here to use sets of 4 vertices, which are the smallest\nstructures for which contradictions arise for Bipartite Correlation Clustering.\nThis simple idea, however, appears to be evasive. We show that, by modifying\nthe LP, we can analyze algorithms which take into consideration subgraph\nstructures of unbounded size. We believe our techniques are interesting in\ntheir own right, and may be used for other problems as well.\n", "versions": [{"version": "v1", "created": "Tue, 14 Dec 2010 12:58:59 GMT"}], "update_date": "2010-12-15", "authors_parsed": [["Ailon", "Nir", ""], ["Avigdor-Elgrabli", "Noa", ""], ["Liberty", "Edo", ""]]}, {"id": "1012.3018", "submitter": "Paolo Liberatore", "authors": "Paolo Liberatore and Marco Schaerf", "title": "On the size of data structures used in symbolic model checking", "comments": null, "journal-ref": null, "doi": "10.1109/TC.2015.2512872", "report-no": null, "categories": "cs.AI cs.CC cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal Logic Model Checking is a verification method in which we describe a\nsystem, the model, and then we verify whether some properties, expressed in a\ntemporal logic formula, hold in the system. It has many industrial\napplications. In order to improve performance, some tools allow preprocessing\nof the model, verifying on-line a set of properties reusing the same compiled\nmodel; we prove that the complexity of the Model Checking problem, without any\npreprocessing or preprocessing the model or the formula in a polynomial data\nstructure, is the same. As a result preprocessing does not always exponentially\nimprove performance.\n  Symbolic Model Checking algorithms work by manipulating sets of states, and\nthese sets are often represented by BDDs. It has been observed that the size of\nBDDs may grow exponentially as the model and formula increase in size. As a\nside result, we formally prove that a superpolynomial increase of the size of\nthese BDDs is unavoidable in the worst case. While this exponential growth has\nbeen empirically observed, to the best of our knowledge it has never been\nproved so far in general terms. This result not only holds for all types of\nBDDs regardless of the variable ordering, but also for more powerful data\nstructures, such as BEDs, RBCs, MTBDDs, and ADDs.\n", "versions": [{"version": "v1", "created": "Tue, 14 Dec 2010 13:18:44 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Liberatore", "Paolo", ""], ["Schaerf", "Marco", ""]]}, {"id": "1012.3024", "submitter": "Sebastiano Vigna", "authors": "Paolo Boldi, Sebastiano Vigna", "title": "E = I + T: The internal extent formula for compacted tries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that in a binary tree the external path length minus the\ninternal path length is exactly 2n-2, where n is the number of external nodes.\nWe show that a generalization of the formula holds for compacted tries,\nreplacing the role of paths with the notion of extent, and the value 2n-2 with\nthe trie measure, an estimation of the number of bits that are necessary to\ndescribe the trie.\n", "versions": [{"version": "v1", "created": "Tue, 14 Dec 2010 13:25:13 GMT"}], "update_date": "2010-12-15", "authors_parsed": [["Boldi", "Paolo", ""], ["Vigna", "Sebastiano", ""]]}, {"id": "1012.3030", "submitter": "Nathan M. Dunfield", "authors": "Nathan M. Dunfield and Anil N. Hirani", "title": "The Least Spanning Area of a Knot and the Optimal Bounding Chain Problem", "comments": "9 pages, 5 figures. V2: Added Remark 5.7. V3: Many minor\n  improvements. To appear in SoCG 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.DG math.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two fundamental objects in knot theory are the minimal genus surface and the\nleast area surface bounded by a knot in a 3-dimensional manifold. When the knot\nis embedded in a general 3-manifold, the problems of finding these surfaces\nwere shown to be NP-complete and NP-hard respectively. However, there is\nevidence that the special case when the ambient manifold is R^3, or more\ngenerally when the second homology is trivial, should be considerably more\ntractable. Indeed, we show here that a natural discrete version of the least\narea surface can be found in polynomial time. The precise setting is that the\nknot is a 1-dimensional subcomplex of a triangulation of the ambient\n3-manifold. The main tool we use is a linear programming formulation of the\nOptimal Bounding Chain Problem (OBCP), where one is required to find the\nsmallest norm chain with a given boundary. While the decision variant of OBCP\nis NP-complete in general, we give conditions under which it can be solved in\npolynomial time. We then show that the least area surface can be constructed\nfrom the optimal bounding chain using a standard desingularization argument\nfrom 3-dimensional topology. We also prove that the related Optimal Homologous\nChain Problem is NP-complete for homology with integer coefficients,\ncomplementing the corresponding result of Chen and Freedman for mod 2 homology.\n", "versions": [{"version": "v1", "created": "Mon, 13 Dec 2010 16:18:16 GMT"}, {"version": "v2", "created": "Mon, 24 Jan 2011 02:35:52 GMT"}, {"version": "v3", "created": "Wed, 23 Mar 2011 17:53:33 GMT"}], "update_date": "2011-03-24", "authors_parsed": [["Dunfield", "Nathan M.", ""], ["Hirani", "Anil N.", ""]]}, {"id": "1012.3130", "submitter": "Ran Gelles", "authors": "Vladimir Braverman, Ran Gelles, Rafail Ostrovsky", "title": "How to Catch L_2-Heavy-Hitters on Sliding Windows", "comments": "v3: updated acknowledgment v2: minor changes; extended abstract to\n  appear in COCOON 2013", "journal-ref": null, "doi": "10.1016/j.tcs.2014.06.008", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding heavy-elements (heavy-hitters) in streaming data is one of the\ncentral, and well-understood tasks. Despite the importance of this problem,\nwhen considering the sliding windows model of streaming (where elements\neventually expire) the problem of finding L_2-heavy elements has remained\ncompletely open despite multiple papers and considerable success in finding\nL_1-heavy elements.\n  In this paper, we develop the first poly-logarithmic-memory algorithm for\nfinding L_2-heavy elements in sliding window model. Since L_2 heavy elements\nplay a central role for many fundamental streaming problems (such as frequency\nmoments), we believe our method would be extremely useful for many\nsliding-windows algorithms and applications. For example, our technique allows\nus not only to find L_2-heavy elements, but also heavy elements with respect to\nany L_p for 0<p<2 on sliding windows. Thus, our paper completely resolves the\nquestion of finding L_p-heavy elements for sliding windows with\npoly-logarithmic memory for all values of p since it is well known that for p>2\nthis task is impossible.\n  Our method may have other applications as well. We demonstrate a broader\napplicability of our novel yet simple method on two additional examples: we\nshow how to obtain a sliding window approximation of other properties such as\nthe similarity of two streams, or the fraction of elements that appear exactly\na specified number of times within the window (the rarity problem). In these\ntwo illustrative examples of our method, we replace the current expected memory\nbounds with worst case bounds.\n", "versions": [{"version": "v1", "created": "Tue, 14 Dec 2010 18:52:58 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2013 19:47:32 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2013 21:32:43 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Braverman", "Vladimir", ""], ["Gelles", "Ran", ""], ["Ostrovsky", "Rafail", ""]]}, {"id": "1012.3189", "submitter": "Jian Li", "authors": "Jian Li, Amol Deshpande", "title": "Maximizing Expected Utility for Stochastic Combinatorial Optimization\n  Problems", "comments": "31 pages, Preliminary version appears in the Proceeding of the 52nd\n  Annual IEEE Symposium on Foundations of Computer Science (FOCS 2011), This\n  version contains several new results ( results (2) and (3) in the abstract)", "journal-ref": null, "doi": "10.1109/FOCS.2011.33", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stochastic versions of a broad class of combinatorial problems\nwhere the weights of the elements in the input dataset are uncertain. The class\nof problems that we study includes shortest paths, minimum weight spanning\ntrees, and minimum weight matchings, and other combinatorial problems like\nknapsack. We observe that the expected value is inadequate in capturing\ndifferent types of {\\em risk-averse} or {\\em risk-prone} behaviors, and instead\nwe consider a more general objective which is to maximize the {\\em expected\nutility} of the solution for some given utility function, rather than the\nexpected weight (expected weight becomes a special case). Under the assumption\nthat there is a pseudopolynomial time algorithm for the {\\em exact} version of\nthe problem (This is true for the problems mentioned above), we can obtain the\nfollowing approximation results for several important classes of utility\nfunctions: (1) If the utility function $\\uti$ is continuous, upper-bounded by a\nconstant and $\\lim_{x\\rightarrow+\\infty}\\uti(x)=0$, we show that we can obtain\na polynomial time approximation algorithm with an {\\em additive error}\n$\\epsilon$ for any constant $\\epsilon>0$. (2) If the utility function $\\uti$ is\na concave increasing function, we can obtain a polynomial time approximation\nscheme (PTAS). (3) If the utility function $\\uti$ is increasing and has a\nbounded derivative, we can obtain a polynomial time approximation scheme. Our\nresults recover or generalize several prior results on stochastic shortest\npath, stochastic spanning tree, and stochastic knapsack. Our algorithm for\nutility maximization makes use of the separability of exponential utility and a\ntechnique to decompose a general utility function into exponential utility\nfunctions, which may be useful in other stochastic optimization problems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Dec 2010 22:34:32 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2011 01:10:47 GMT"}, {"version": "v3", "created": "Sun, 14 Aug 2011 02:53:22 GMT"}, {"version": "v4", "created": "Fri, 19 Aug 2011 10:32:48 GMT"}, {"version": "v5", "created": "Tue, 19 Mar 2013 09:11:30 GMT"}, {"version": "v6", "created": "Sat, 19 Dec 2015 10:13:56 GMT"}, {"version": "v7", "created": "Wed, 10 Aug 2016 09:02:50 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Li", "Jian", ""], ["Deshpande", "Amol", ""]]}, {"id": "1012.3311", "submitter": "Frederic Magniez", "authors": "Christian Konrad and Frederic Magniez", "title": "Validating XML Documents in the Streaming Model with External Memory", "comments": "Change title. Remove a statement on a lower bound (now Conjecture 2\n  in Annexe B) since the proof was incomplete", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of validating XML documents of size $N$ against general\nDTDs in the context of streaming algorithms. The starting point of this work is\na well-known space lower bound. There are XML documents and DTDs for which\n$p$-pass streaming algorithms require $\\Omega(N/p)$ space.\n  We show that when allowing access to external memory, there is a\ndeterministic streaming algorithm that solves this problem with memory space\n$O(\\log^2 N)$, a constant number of auxiliary read/write streams, and $O(\\log\nN)$ total number of passes on the XML document and auxiliary streams.\n  An important intermediate step of this algorithm is the computation of the\nFirst-Child-Next-Sibling (FCNS) encoding of the initial XML document in a\nstreaming fashion. We study this problem independently, and we also provide\nmemory efficient streaming algorithms for decoding an XML document given in its\nFCNS encoding.\n  Furthermore, validating XML documents encoding binary trees in the usual\nstreaming model without external memory can be done with sublinear memory.\nThere is a one-pass algorithm using $O(\\sqrt{N \\log N})$ space, and a\nbidirectional two-pass algorithm using $O(\\log^2 N)$ space performing this\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 15 Dec 2010 12:43:26 GMT"}, {"version": "v2", "created": "Sun, 19 Dec 2010 21:14:40 GMT"}, {"version": "v3", "created": "Thu, 18 Aug 2011 08:31:35 GMT"}], "update_date": "2011-08-19", "authors_parsed": [["Konrad", "Christian", ""], ["Magniez", "Frederic", ""]]}, {"id": "1012.3697", "submitter": "Daniel Kuntze", "authors": "Marcel R. Ackermann, Johannes Bl\\\"omer, Daniel Kuntze and Christian\n  Sohler", "title": "Analysis of Agglomerative Clustering", "comments": "A preliminary version of this article appeared in Proceedings of the\n  28th International Symposium on Theoretical Aspects of Computer Science\n  (STACS '11), March 2011, pp. 308-319. This article also appeared in\n  Algorithmica. The final publication is available at\n  http://link.springer.com/article/10.1007/s00453-012-9717-4", "journal-ref": "Ackermann, M. R., Bl\\\"omer, J., Kuntze, D., and Sohler, C. (2014).\n  Analysis of Agglomerative Clustering. Algorithmica, 69(1):184-215", "doi": "10.1007/s00453-012-9717-4", "report-no": null, "categories": "cs.DS cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diameter $k$-clustering problem is the problem of partitioning a finite\nsubset of $\\mathbb{R}^d$ into $k$ subsets called clusters such that the maximum\ndiameter of the clusters is minimized. One early clustering algorithm that\ncomputes a hierarchy of approximate solutions to this problem (for all values\nof $k$) is the agglomerative clustering algorithm with the complete linkage\nstrategy. For decades, this algorithm has been widely used by practitioners.\nHowever, it is not well studied theoretically. In this paper, we analyze the\nagglomerative complete linkage clustering algorithm. Assuming that the\ndimension $d$ is a constant, we show that for any $k$ the solution computed by\nthis algorithm is an $O(\\log k)$-approximation to the diameter $k$-clustering\nproblem. Our analysis does not only hold for the Euclidean distance but for any\nmetric that is based on a norm. Furthermore, we analyze the closely related\n$k$-center and discrete $k$-center problem. For the corresponding agglomerative\nalgorithms, we deduce an approximation factor of $O(\\log k)$ as well.\n", "versions": [{"version": "v1", "created": "Thu, 16 Dec 2010 17:46:07 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2012 09:56:24 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2012 12:55:49 GMT"}, {"version": "v4", "created": "Fri, 7 Mar 2014 19:37:47 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Ackermann", "Marcel R.", ""], ["Bl\u00f6mer", "Johannes", ""], ["Kuntze", "Daniel", ""], ["Sohler", "Christian", ""]]}, {"id": "1012.3763", "submitter": "Tamal Dey", "authors": "Dan Burghelea, Tamal K. Dey, and Du Dong", "title": "Defining and Computing Topological Persistence for 1-cocycles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of topological persistence, introduced recently in computational\ntopology, finds applications in studying a map in relation to the topology of\nits domain. Since its introduction, it has been extended and generalized in\nvarious directions. However, no attempt has been made so far to extend the\nconcept of topological persistence to a generalization of `maps' such as\ncocycles which are discrete analogs of closed differential forms, a well known\nconcept in differential geometry. We define a notion of topological persistence\nfor 1-cocycles in this paper and show how to compute its relevant numbers. It\nturns out that, instead of the standard persistence, one of its variants which\nwe call level persistence can be leveraged for this purpose. It is worth\nmentioning that 1-cocyles appear in practice such as in data ranking or in\ndiscrete vector fields.\n", "versions": [{"version": "v1", "created": "Thu, 16 Dec 2010 21:43:19 GMT"}, {"version": "v2", "created": "Fri, 27 May 2011 13:56:46 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Burghelea", "Dan", ""], ["Dey", "Tamal K.", ""], ["Dong", "Du", ""]]}, {"id": "1012.3932", "submitter": "Alexander Souza", "authors": "Antonios Antoniadis and Falk H\\\"uffner and Pascal Lenzner and Carsten\n  Moldenhauer and Alexander Souza", "title": "Balanced Interval Coloring", "comments": "Accepted at STACS 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the discrepancy problem of coloring $n$ intervals with $k$ colors\nsuch that at each point on the line, the maximal difference between the number\nof intervals of any two colors is minimal. Somewhat surprisingly, a coloring\nwith maximal difference at most one always exists. Furthermore, we give an\nalgorithm with running time $O(n \\log n + kn \\log k)$ for its construction.\nThis is in particular interesting because many known results for discrepancy\nproblems are non-constructive. This problem naturally models a load balancing\nscenario, where $n$ tasks with given start- and endtimes have to be distributed\namong $k$ servers. Our results imply that this can be done ideally balanced.\n  When generalizing to $d$-dimensional boxes (instead of intervals), a solution\nwith difference at most one is not always possible. We show that for any $d \\ge\n2$ and any $k \\ge 2$ it is NP-complete to decide if such a solution exists,\nwhich implies also NP-hardness of the respective minimization problem.\n  In an online scenario, where intervals arrive over time and the color has to\nbe decided upon arrival, the maximal difference in the size of color classes\ncan become arbitrarily high for any online algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 17 Dec 2010 17:26:31 GMT"}], "update_date": "2010-12-20", "authors_parsed": [["Antoniadis", "Antonios", ""], ["H\u00fcffner", "Falk", ""], ["Lenzner", "Pascal", ""], ["Moldenhauer", "Carsten", ""], ["Souza", "Alexander", ""]]}, {"id": "1012.4062", "submitter": "Arnab Bhattacharyya", "authors": "Arnab Bhattacharyya and Konstantin Makarychev", "title": "Improved Approximation for the Directed Spanner Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the size of the sparsest directed k-spanner of a graph can be\napproximated in polynomial time to within a factor of $\\tilde{O}(\\sqrt{n})$,\nfor all k >= 3. This improves the $\\tilde{O}(n^{2/3})$-approximation recently\nshown by Dinitz and Krauthgamer.\n", "versions": [{"version": "v1", "created": "Sat, 18 Dec 2010 06:09:40 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Makarychev", "Konstantin", ""]]}, {"id": "1012.4231", "submitter": "Michael Mahoney", "authors": "Michael W. Mahoney", "title": "Computation in Large-Scale Scientific and Internet Data Applications is\n  a Focus of MMDS 2010", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2010 Workshop on Algorithms for Modern Massive Data Sets (MMDS 2010) was\nheld at Stanford University, June 15--18. The goals of MMDS 2010 were (1) to\nexplore novel techniques for modeling and analyzing massive, high-dimensional,\nand nonlinearly-structured scientific and Internet data sets; and (2) to bring\ntogether computer scientists, statisticians, applied mathematicians, and data\nanalysis practitioners to promote cross-fertilization of ideas. MMDS 2010\nfollowed on the heels of two previous MMDS workshops. The first, MMDS 2006,\naddressed the complementary perspectives brought by the numerical linear\nalgebra and theoretical computer science communities to matrix algorithms in\nmodern informatics applications; and the second, MMDS 2008, explored more\ngenerally fundamental algorithmic and statistical challenges in modern\nlarge-scale data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 20 Dec 2010 03:07:53 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["Mahoney", "Michael W.", ""]]}, {"id": "1012.4263", "submitter": "Simon Gog", "authors": "Simon Gog and Enno Ohlebusch", "title": "Lightweight LCP-Array Construction in Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The suffix tree is a very important data structure in string processing, but\nit suffers from a huge space consumption. In large-scale applications,\ncompressed suffix trees (CSTs) are therefore used instead. A CST consists of\nthree (compressed) components: the suffix array, the LCP-array, and data\nstructures for simulating navigational operations on the suffix tree. The\nLCP-array stores the lengths of the longest common prefixes of\nlexicographically adjacent suffixes, and it can be computed in linear time. In\nthis paper, we present new LCP-array construction algorithms that are fast and\nvery space efficient. In practice, our algorithms outperform the currently best\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 20 Dec 2010 09:08:05 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["Gog", "Simon", ""], ["Ohlebusch", "Enno", ""]]}, {"id": "1012.4404", "submitter": "Walter Quattrociocchi", "authors": "Sara Brunetti, Elena Lodi, Walter Quattrociocchi", "title": "Multicolored Dynamos on Toroidal Meshes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting on a graph the presence of the minimum number of nodes (target set)\nthat will be able to \"activate\" a prescribed number of vertices in the graph is\ncalled the target set selection problem (TSS) proposed by Kempe, Kleinberg, and\nTardos. In TSS's settings, nodes have two possible states (active or\nnon-active) and the threshold triggering the activation of a node is given by\nthe number of its active neighbors. Dealing with fault tolerance in a majority\nbased system the two possible states are used to denote faulty or non-faulty\nnodes, and the threshold is given by the state of the majority of neighbors.\nHere, the major effort was in determining the distribution of initial faults\nleading the entire system to a faulty behavior. Such an activation pattern,\nalso known as dynamic monopoly (or shortly dynamo), was introduced by Peleg in\n1996. In this paper we extend the TSS problem's settings by representing nodes'\nstates with a \"multicolored\" set. The extended version of the problem can be\ndescribed as follows: let G be a simple connected graph where every node is\nassigned a color from a finite ordered set C = {1, . . ., k} of colors. At each\nlocal time step, each node can recolor itself, depending on the local\nconfigurations, with the color held by the majority of its neighbors. Given G,\nwe study the initial distributions of colors leading the system to a k\nmonochromatic configuration in toroidal meshes, focusing on the minimum number\nof initial k-colored nodes. We find upper and lower bounds to the size of a\ndynamo, and then special classes of dynamos, outlined by means of a new\napproach based on recoloring patterns, are characterized.\n", "versions": [{"version": "v1", "created": "Mon, 20 Dec 2010 17:02:51 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["Brunetti", "Sara", ""], ["Lodi", "Elena", ""], ["Quattrociocchi", "Walter", ""]]}, {"id": "1012.4560", "submitter": "Yann Ponty", "authors": "Yann Ponty (INRIA Saclay - Ile de France, LIX)", "title": "Non-redundant random generation from weighted context-free languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the non-redundant random generation of k words of length n from a\ncontext-free language. Additionally, we want to avoid a predefined set of\nwords. We study the limits of a rejection-based approach, whose time complexity\nis shown to grow exponentially in k in some cases. We propose an alternative\nrecursive algorithm, whose careful implementation allows for a non-redundant\ngeneration of k words of size n in O(kn log n) arithmetic operations after the\nprecomputation of O(n) numbers. The overall complexity is therefore dominated\nby the generation of k words, and the non-redundancy comes at a negligible\ncost.\n", "versions": [{"version": "v1", "created": "Tue, 21 Dec 2010 07:43:34 GMT"}], "update_date": "2010-12-22", "authors_parsed": [["Ponty", "Yann", "", "INRIA Saclay - Ile de France, LIX"]]}, {"id": "1012.4701", "submitter": "Bart M. P. Jansen", "authors": "Bart M. P. Jansen and Hans L. Bodlaender", "title": "Vertex Cover Kernelization Revisited: Upper and Lower Bounds for a\n  Refined Parameter", "comments": "Published in \"Theory of Computing Systems\" as an Open Access\n  publication", "journal-ref": "Theory Comput. Syst. 53(2): 263-299 (2013)", "doi": "10.1007/s00224-012-9393-4", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important result in the study of polynomial-time preprocessing shows that\nthere is an algorithm which given an instance (G,k) of Vertex Cover outputs an\nequivalent instance (G',k') in polynomial time with the guarantee that G' has\nat most 2k' vertices (and thus O((k')^2) edges) with k' <= k. Using the\nterminology of parameterized complexity we say that k-Vertex Cover has a kernel\nwith 2k vertices. There is complexity-theoretic evidence that both 2k vertices\nand Theta(k^2) edges are optimal for the kernel size. In this paper we consider\nthe Vertex Cover problem with a different parameter, the size fvs(G) of a\nminimum feedback vertex set for G. This refined parameter is structurally\nsmaller than the parameter k associated to the vertex covering number vc(G)\nsince fvs(G) <= vc(G) and the difference can be arbitrarily large. We give a\nkernel for Vertex Cover with a number of vertices that is cubic in fvs(G): an\ninstance (G,X,k) of Vertex Cover, where X is a feedback vertex set for G, can\nbe transformed in polynomial time into an equivalent instance (G',X',k') such\nthat |V(G')| <= 2k and |V(G')| <= O(|X'|^3). A similar result holds when the\nfeedback vertex set X is not given along with the input. In sharp contrast we\nshow that the Weighted Vertex Cover problem does not have a polynomial kernel\nwhen parameterized by the cardinality of a given vertex cover of the graph\nunless NP is in coNP/poly and the polynomial hierarchy collapses to the third\nlevel.\n", "versions": [{"version": "v1", "created": "Tue, 21 Dec 2010 15:47:46 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2013 15:41:10 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["Bodlaender", "Hans L.", ""]]}, {"id": "1012.4763", "submitter": "Katrina Ligett", "authors": "Moritz Hardt, Katrina Ligett, Frank McSherry", "title": "A simple and practical algorithm for differentially private data release", "comments": "rewritten, with much more extensive experimental validation than the\n  previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new theoretical results on differentially private data release\nuseful with respect to any target class of counting queries, coupled with\nexperimental results on a variety of real world data sets.\n  Specifically, we study a simple combination of the multiplicative weights\napproach of [Hardt and Rothblum, 2010] with the exponential mechanism of\n[McSherry and Talwar, 2007]. The multiplicative weights framework allows us to\nmaintain and improve a distribution approximating a given data set with respect\nto a set of counting queries. We use the exponential mechanism to select those\nqueries most incorrectly tracked by the current distribution. Combing the two,\nwe quickly approach a distribution that agrees with the data set on the given\nset of queries up to small error.\n  The resulting algorithm and its analysis is simple, but nevertheless improves\nupon previous work in terms of both error and running time. We also empirically\ndemonstrate the practicality of our approach on several data sets commonly used\nin the statistical community for contingency table release.\n", "versions": [{"version": "v1", "created": "Tue, 21 Dec 2010 18:41:19 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2012 13:43:57 GMT"}], "update_date": "2012-03-16", "authors_parsed": [["Hardt", "Moritz", ""], ["Ligett", "Katrina", ""], ["McSherry", "Frank", ""]]}, {"id": "1012.4767", "submitter": "Yahav Nussbaum", "authors": "Yahav Nussbaum", "title": "Multiple-source multiple-sink maximum flow in planar graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show an O(n^(3/2) log^2 n) time algorithm for finding a\nmaximum flow in a planar graph with multiple sources and multiple sinks. This\nis the fastest algorithm whose running time depends only on the number of\nvertices in the graph. For general (non-planar) graphs the multiple-source\nmultiple-sink version of the maximum flow problem is as difficult as the\nstandard single-source single-sink version. However, the standard reduction\ndoes not preserve the planarity of the graph, and it is not known how to\ngeneralize existing maximum flow algorithms for planar graphs to the\nmultiple-source multiple-sink maximum flow problem.\n", "versions": [{"version": "v1", "created": "Tue, 21 Dec 2010 19:17:43 GMT"}, {"version": "v2", "created": "Tue, 28 Dec 2010 17:55:51 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Nussbaum", "Yahav", ""]]}, {"id": "1012.4889", "submitter": "Mert Sa\\u{g}lam", "authors": "Hossein Jowhari, Mert Sa\\u{g}lam, G\\'abor Tardos", "title": "Tight Bounds for Lp Samplers, Finding Duplicates in Streams, and Related\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present near-optimal space bounds for Lp-samplers. Given a\nstream of updates (additions and subtraction) to the coordinates of an\nunderlying vector x \\in R^n, a perfect Lp sampler outputs the i-th coordinate\nwith probability |x_i|^p/||x||_p^p. In SODA 2010, Monemizadeh and Woodruff\nshowed polylog space upper bounds for approximate Lp-samplers and demonstrated\nvarious applications of them. Very recently, Andoni, Krauthgamer and Onak\nimproved the upper bounds and gave a O(\\epsilon^{-p} log^3 n) space \\epsilon\nrelative error and constant failure rate Lp-sampler for p \\in [1,2]. In this\nwork, we give another such algorithm requiring only O(\\epsilon^{-p} log^2 n)\nspace for p \\in (1,2). For p \\in (0,1), our space bound is O(\\epsilon^{-1}\nlog^2 n), while for the $p=1$ case we have an O(log(1/\\epsilon)\\epsilon^{-1}\nlog^2 n) space algorithm. We also give a O(log^2 n) bits zero relative error\nL0-sampler, improving the O(log^3 n) bits algorithm due to Frahling, Indyk and\nSohler.\n  As an application of our samplers, we give better upper bounds for the\nproblem of finding duplicates in data streams. In case the length of the stream\nis longer than the alphabet size, L1 sampling gives us an O(log^2 n) space\nalgorithm, thus improving the previous O(log^3 n) bound due to Gopalan and\nRadhakrishnan.\n  In the second part of our work, we prove an Omega(log^2 n) lower bound for\nsampling from 0, \\pm 1 vectors (in this special case, the parameter p is not\nrelevant for Lp sampling). This matches the space of our sampling algorithms\nfor constant \\epsilon > 0. We also prove tight space lower bounds for the\nfinding duplicates and heavy hitters problems. We obtain these lower bounds\nusing reductions from the communication complexity problem augmented indexing.\n", "versions": [{"version": "v1", "created": "Wed, 22 Dec 2010 06:55:58 GMT"}], "update_date": "2010-12-23", "authors_parsed": [["Jowhari", "Hossein", ""], ["Sa\u011flam", "Mert", ""], ["Tardos", "G\u00e1bor", ""]]}, {"id": "1012.4938", "submitter": "Loukas Georgiadis", "authors": "Loukas Georgiadis, Stavros D. Nikolopoulos, Leonidas Palios", "title": "Join-Reachability Problems in Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given collection G of directed graphs we define the join-reachability\ngraph of G, denoted by J(G), as the directed graph that, for any pair of\nvertices a and b, contains a path from a to b if and only if such a path exists\nin all graphs of G. Our goal is to compute an efficient representation of J(G).\nIn particular, we consider two versions of this problem. In the explicit\nversion we wish to construct the smallest join-reachability graph for G. In the\nimplicit version we wish to build an efficient data structure (in terms of\nspace and query time) such that we can report fast the set of vertices that\nreach a query vertex in all graphs of G. This problem is related to the\nwell-studied reachability problem and is motivated by emerging applications of\ngraph-structured databases and graph algorithms. We consider the construction\nof join-reachability structures for two graphs and develop techniques that can\nbe applied to both the explicit and the implicit problem. First we present\noptimal and near-optimal structures for paths and trees. Then, based on these\nresults, we provide efficient structures for planar graphs and general directed\ngraphs.\n", "versions": [{"version": "v1", "created": "Wed, 22 Dec 2010 11:01:40 GMT"}], "update_date": "2010-12-23", "authors_parsed": [["Georgiadis", "Loukas", ""], ["Nikolopoulos", "Stavros D.", ""], ["Palios", "Leonidas", ""]]}, {"id": "1012.4962", "submitter": "Viswanath Nagarajan", "authors": "Anupam Gupta and Viswanath Nagarajan and R. Ravi", "title": "Robust and MaxMin Optimization under Matroid and Knapsack Uncertainty\n  Sets", "comments": "17 pages. Preliminary version combining this paper and\n  http://arxiv.org/abs/0912.1045 appeared in ICALP 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following problem: given a set system (U,I) and an edge-weighted\ngraph G = (U, E) on the same universe U, find the set A in I such that the\nSteiner tree cost with terminals A is as large as possible: \"which set in I is\nthe most difficult to connect up?\" This is an example of a max-min problem:\nfind the set A in I such that the value of some minimization (covering) problem\nis as large as possible.\n  In this paper, we show that for certain covering problems which admit good\ndeterministic online algorithms, we can give good algorithms for max-min\noptimization when the set system I is given by a p-system or q-knapsacks or\nboth. This result is similar to results for constrained maximization of\nsubmodular functions. Although many natural covering problems are not even\napproximately submodular, we show that one can use properties of the online\nalgorithm as a surrogate for submodularity.\n  Moreover, we give stronger connections between max-min optimization and\ntwo-stage robust optimization, and hence give improved algorithms for robust\nversions of various covering problems, for cases where the uncertainty sets are\ngiven by p-systems and q-knapsacks.\n", "versions": [{"version": "v1", "created": "Wed, 22 Dec 2010 13:05:51 GMT"}, {"version": "v2", "created": "Thu, 24 Feb 2011 19:36:50 GMT"}], "update_date": "2011-02-25", "authors_parsed": [["Gupta", "Anupam", ""], ["Nagarajan", "Viswanath", ""], ["Ravi", "R.", ""]]}, {"id": "1012.5024", "submitter": "Sandor P. Fekete", "authors": "Sandor Fekete, Tom Kamphans, and Michael Stelzer", "title": "Shortest Paths with Pairwise-Distinct Edge Labels: Finding Biochemical\n  Pathways in Metabolic Networks", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A problem studied in Systems Biology is how to find shortest paths in\nmetabolic networks. Unfortunately, simple (i.e., graph theoretic) shortest\npaths do not properly reflect biochemical facts. An approach to overcome this\nissue is to use edge labels and search for paths with distinct labels.\n  In this paper, we show that such biologically feasible shortest paths are\nhard to compute. Moreover, we present solutions to find such paths in networks\nin reasonable time.\n", "versions": [{"version": "v1", "created": "Wed, 22 Dec 2010 16:16:11 GMT"}], "update_date": "2010-12-23", "authors_parsed": [["Fekete", "Sandor", ""], ["Kamphans", "Tom", ""], ["Stelzer", "Michael", ""]]}, {"id": "1012.5330", "submitter": "Tom Kamphans", "authors": "Sandor Fekete, Tom Kamphans, Nils Schweer, Christopher Tessars, Jan C.\n  van der Veen, Josef Angermeier, Dirk Koch, Juergen Teich", "title": "No-Break Dynamic Defragmentation of Reconfigurable Devices", "comments": "18 pages, 13 figures, an earlier version appeared in the proceedings\n  of the FPL 08", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for defragmenting the module layout of a\nreconfigurable device, enabled by a novel approach for dealing with\ncommunication needs between relocated modules and with inhomogeneities found in\ncommonly used FPGAs. Our method is based on dynamic relocation of module\npositions during runtime, with only very little reconfiguration overhead; the\nobjective is to maximize the length of contiguous free space that is available\nfor new modules. We describe a number of algorithmic aspects of good\ndefragmentation, and present an optimization method based on tabu search.\nExperimental results indicate that we can improve the quality of module layout\nby roughly 50 % over static layout. Among other benefits, this improvement\navoids unnecessary rejections of modules\n", "versions": [{"version": "v1", "created": "Fri, 24 Dec 2010 01:16:08 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2011 00:37:28 GMT"}], "update_date": "2011-11-14", "authors_parsed": [["Fekete", "Sandor", ""], ["Kamphans", "Tom", ""], ["Schweer", "Nils", ""], ["Tessars", "Christopher", ""], ["van der Veen", "Jan C.", ""], ["Angermeier", "Josef", ""], ["Koch", "Dirk", ""], ["Teich", "Juergen", ""]]}, {"id": "1012.5351", "submitter": "Tobias Friedrich", "authors": "Benjamin Doerr, Tobias Friedrich, Thomas Sauerwald", "title": "Quasirandom Rumor Spreading", "comments": "34 pages, to appear in ACM Transactions of Algorithms, parts of the\n  results appeared in SODA'08 and ICALP'09", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a quasirandom analogue of the classical push model for\ndisseminating information in networks (\"randomized rumor spreading\"). In the\nclassical model, in each round each informed vertex chooses a neighbor at\nrandom and informs it, if it was not informed before. It is known that this\nsimple protocol succeeds in spreading a rumor from one vertex to all others\nwithin O(log n) rounds on complete graphs, hypercubes, random regular graphs,\nErdos-Renyi random graph and Ramanujan graphs with probability 1-o(1). In the\nquasirandom model, we assume that each vertex has a (cyclic) list of its\nneighbors. Once informed, it starts at a random position on the list, but from\nthen on informs its neighbors in the order of the list. Surprisingly,\nirrespective of the orders of the lists, the above-mentioned bounds still hold.\nIn some cases, even better bounds than for the classical model can be shown.\n", "versions": [{"version": "v1", "created": "Fri, 24 Dec 2010 07:24:06 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2013 21:06:48 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Doerr", "Benjamin", ""], ["Friedrich", "Tobias", ""], ["Sauerwald", "Thomas", ""]]}, {"id": "1012.5357", "submitter": "Tobias Friedrich", "authors": "Benjamin Doerr and Tobias Friedrich and Marvin K\\\"unnemann and Thomas\n  Sauerwald", "title": "Quasirandom Rumor Spreading: An Experimental Analysis", "comments": "14 pages, appeared in ALENEX'09", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We empirically analyze two versions of the well-known \"randomized rumor\nspreading\" protocol to disseminate a piece of information in networks. In the\nclassical model, in each round each informed node informs a random neighbor. In\nthe recently proposed quasirandom variant, each node has a (cyclic) list of its\nneighbors. Once informed, it starts at a random position of the list, but from\nthen on informs its neighbors in the order of the list. While for sparse random\ngraphs a better performance of the quasirandom model could be proven, all other\nresults show that, independent of the structure of the lists, the same\nasymptotic performance guarantees hold as for the classical model. In this\nwork, we compare the two models experimentally. This not only shows that the\nquasirandom model generally is faster, but also that the runtime is more\nconcentrated around the mean. This is surprising given that much fewer random\nbits are used in the quasirandom process. These advantages are also observed in\na lossy communication model, where each transmission does not reach its target\nwith a certain probability, and in an asynchronous model, where nodes send at\nrandom times drawn from an exponential distribution. We also show that\ntypically the particular structure of the lists has little influence on the\nefficiency.\n", "versions": [{"version": "v1", "created": "Fri, 24 Dec 2010 07:54:02 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Doerr", "Benjamin", ""], ["Friedrich", "Tobias", ""], ["K\u00fcnnemann", "Marvin", ""], ["Sauerwald", "Thomas", ""]]}, {"id": "1012.5546", "submitter": "Amine Farhat Mr.", "authors": "Mohamed Salah Gouider and Amine Farhat", "title": "Mining Multi-Level Frequent Itemsets under Constraints", "comments": "20 pages", "journal-ref": "Internatinal Journal of Database Theory and Application, Vol. 3,\n  No. 4, PP. 15-35, December, 2010", "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining association rules is a task of data mining, which extracts knowledge\nin the form of significant implication relation of useful items (objects) from\na database. Mining multilevel association rules uses concept hierarchies, also\ncalled taxonomies and defined as relations of type 'is-a' between objects, to\nextract rules that items belong to different levels of abstraction. These rules\nare more useful, more refined and more interpretable by the user. Several\nalgorithms have been proposed in the literature to discover the multilevel\nassociation rules. In this article, we are interested in the problem of\ndiscovering multi-level frequent itemsets under constraints, involving the user\nin the research process. We proposed a technique for modeling and\ninterpretation of constraints in a context of use of concept hierarchies. Three\napproaches for discovering multi-level frequent itemsets under constraints were\nproposed and discussed: Basic approach, \"Test and Generate\" approach and\nPruning based Approach.\n", "versions": [{"version": "v1", "created": "Sun, 26 Dec 2010 22:23:00 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Gouider", "Mohamed Salah", ""], ["Farhat", "Amine", ""]]}, {"id": "1012.5870", "submitter": "Shay Mozes", "authors": "Shay Mozes", "title": "Multiple-Source Multiple-Sink Maximum Flow in Directed Planar Graphs in\n  $O(n^{1.5} \\log n)$ Time", "comments": "to be merged with 1) Yahav Nussbaum 1012.4767 2) Philip N. Klein and\n  Shay Mozes 1008.5332 3) Glencora Borradaile and Christian Wulff-Nilsen\n  1008.4966", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an $O(n^{1.5} \\log n)$ algorithm that, given a directed planar graph\nwith arc capacities, a set of source nodes and a set of sink nodes, finds a\nmaximum flow from the sources to the sinks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Dec 2010 04:53:17 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Mozes", "Shay", ""]]}, {"id": "1012.5911", "submitter": "Andrzej Lingas", "authors": "Andrzej Lingas and Cui Di", "title": "Near approximation of maximum weight matching through efficient weight\n  reduction", "comments": "A very preliminary version has been presented at SOFSEM Student\n  Forum, January 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let G be an edge-weighted hypergraph on n vertices, m edges of size \\le s,\nwhere the edges have real weights in an interval [1,W]. We show that if we can\napproximate a maximum weight matching in G within factor alpha in time T(n,m,W)\nthen we can find a matching of weight at least (alpha-epsilon) times the\nmaximum weight of a matching in G in time (epsilon^{-1})^{O(1)}max_{1\\le q \\le\nO(epsilon \\frac {log {\\frac n {epsilon}}} {log epsilon^{-1}})}\nmax_{m_1+...m_q=m}\nsum_1^qT(min{n,sm_j},m_{j},(epsilon^{-1})^{O(epsilon^{-1})}). In particular, if\nwe combine our result with the recent (1-\\epsilon)-approximation algorithm for\nmaximum weight matching in graphs due to Duan and Pettie whose time complexity\nhas a poly-logarithmic dependence on W then we obtain a\n(1-\\epsilon)-approximation algorithm for maximum weight matching in graphs\nrunning in time (epsilon^{-1})^{O(1)}(m+n).\n", "versions": [{"version": "v1", "created": "Wed, 29 Dec 2010 11:22:31 GMT"}, {"version": "v2", "created": "Tue, 11 Jan 2011 09:06:35 GMT"}], "update_date": "2011-01-12", "authors_parsed": [["Lingas", "Andrzej", ""], ["Di", "Cui", ""]]}, {"id": "1012.5995", "submitter": "Sahil Singla", "authors": "S. K. Gupta, Sahil Singla, Akash Khandelwal, Apurv Tiwari, Srilekha", "title": "Exhaustive Verification of Weak Reconstruction For Self Complementary\n  Graphs", "comments": "9 pages, 4 figures, Submitted to Special Issue of Cochin in Discrete\n  Mathematics", "journal-ref": "Presented as a poster in ICRTGC 2010", "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an exhaustive approach for verification of the weak\nreconstruction of Self Complementary Graphs up to 17 vertices. It describes the\ngeneral problem of the Reconstruction Conjecture, explaining the complexity\ninvolved in checking deck-isomorphism between two graphs. In order to improve\nthe computation time, various pruning techniques have been employed to reduce\nthe number of graph-isomorphism comparisons. These techniques offer great help\nin proceeding with a reconstructive approach. An analysis of the numbers\ninvolved is provided, along with the various limitations of this approach. A\nlist enumerating the number of SC graphs up till 101 vertices is also appended.\n", "versions": [{"version": "v1", "created": "Wed, 29 Dec 2010 18:12:51 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Gupta", "S. K.", ""], ["Singla", "Sahil", ""], ["Khandelwal", "Akash", ""], ["Tiwari", "Apurv", ""], ["Srilekha", "", ""]]}]