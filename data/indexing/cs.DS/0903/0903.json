[{"id": "0903.0034", "submitter": "Vladimir Braverman", "authors": "Vladimir Braverman, Rafail Ostrovsky", "title": "Measuring Independence of Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data stream model represents setting where approximating pairwise, or\n$k$-wise, independence with sublinear memory is of considerable importance. In\nthe streaming model the joint distribution is given by a stream of $k$-tuples,\nwith the goal of testing correlations among the components measured over the\nentire stream. In the streaming model, Indyk and McGregor (SODA 08) recently\ngave exciting new results for measuring pairwise independence. The Indyk and\nMcGregor methods provide $\\log{n}$-approximation under statistical distance\nbetween the joint and product distributions in the streaming model. Indyk and\nMcGregor leave, as their main open question, the problem of improving their\n$\\log n$-approximation for the statistical distance metric.\n  In this paper we solve the main open problem posed by of Indyk and McGregor\nfor the statistical distance for pairwise independence and extend this result\nto any constant $k$. In particular, we present an algorithm that computes an\n$(\\epsilon, \\delta)$-approximation of the statistical distance between the\njoint and product distributions defined by a stream of $k$-tuples. Our\nalgorithm requires $O(({1\\over \\epsilon}\\log({nm\\over \\delta}))^{(30+k)^k})$\nmemory and a single pass over the data stream.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2009 01:29:54 GMT"}], "update_date": "2009-03-03", "authors_parsed": [["Braverman", "Vladimir", ""], ["Ostrovsky", "Rafail", ""]]}, {"id": "0903.0116", "submitter": "Siddhartha Sen", "authors": "Bernhard Haeupler, Siddhartha Sen, and Robert E. Tarjan", "title": "Heaps Simplified", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The heap is a basic data structure used in a wide variety of applications,\nincluding shortest path and minimum spanning tree algorithms. In this paper we\nexplore the design space of comparison-based, amortized-efficient heap\nimplementations. From a consideration of dynamic single-elimination\ntournaments, we obtain the binomial queue, a classical heap implementation, in\na simple and natural way. We give four equivalent ways of representing heaps\narising from tournaments, and we obtain two new variants of binomial queues, a\none-tree version and a one-pass version. We extend the one-pass version to\nsupport key decrease operations, obtaining the {\\em rank-pairing heap}, or {\\em\nrp-heap}. Rank-pairing heaps combine the performance guarantees of Fibonacci\nheaps with simplicity approaching that of pairing heaps. Like pairing heaps,\nrank-pairing heaps consist of trees of arbitrary structure, but these trees are\ncombined by rank, not by list position, and rank changes, but not structural\nchanges, cascade during key decrease operations.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2009 00:19:10 GMT"}], "update_date": "2009-03-03", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Sen", "Siddhartha", ""], ["Tarjan", "Robert E.", ""]]}, {"id": "0903.0136", "submitter": "Daniel Cosmin Porumbel", "authors": "Daniel Cosmin Porumbel", "title": "A polynomial graph extension procedure for improving graph isomorphism\n  algorithms", "comments": "A typo mistake!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this short note a polynomial graph extension procedure that can\nbe used to improve any graph isomorphism algorithm. This construction\npropagates new constraints from the isomorphism constraints of the input graphs\n(denoted by $G(V,E)$ and $G'(V',E')$). Thus, information from the edge\nstructures of $G$ and $G'$ is \"hashed\" into the weighted edges of the extended\ngraphs. A bijective mapping is an isomorphism of the initial graphs if and only\nif it is an isomorphism of the extended graphs. As such, the construction\nenables the identification of pair of vertices $i\\in V$ and $i'\\in V'$ that can\nnot be mapped by any isomorphism $h^*:V \\to V'$ (e.g. if the extended edges of\n$i$ and $i'$ are different). A forbidding matrix $F$, that encodes all pairs of\nincompatible mappings $(i,i')$, is constructed in order to be used by a\ndifferent algorithm. Moreover, tests on numerous graph classes show that the\nmatrix $F$ might leave only one compatible element for each $i \\in V$.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2009 12:54:57 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2009 07:31:53 GMT"}], "update_date": "2009-03-03", "authors_parsed": [["Porumbel", "Daniel Cosmin", ""]]}, {"id": "0903.0173", "submitter": "Alexander Gutfraind", "authors": "Alexander Gutfraind and Aric Hagberg and Feng Pan", "title": "Optimal Interdiction of Unreactive Markovian Evaders", "comments": "Accepted at the Sixth International Conference on integration of AI\n  and OR Techniques in Constraint Programming for Combinatorial Optimization\n  Problems (CPAIOR 2009)", "journal-ref": "CPAIOR 2009", "doi": null, "report-no": "LA-UR-09-00560", "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interdiction problem arises in a variety of areas including military\nlogistics, infectious disease control, and counter-terrorism. In the typical\nformulation of network interdiction, the task of the interdictor is to find a\nset of edges in a weighted network such that the removal of those edges would\nmaximally increase the cost to an evader of traveling on a path through the\nnetwork.\n  Our work is motivated by cases in which the evader has incomplete information\nabout the network or lacks planning time or computational power, e.g. when\nauthorities set up roadblocks to catch bank robbers, the criminals do not know\nall the roadblock locations or the best path to use for their escape.\n  We introduce a model of network interdiction in which the motion of one or\nmore evaders is described by Markov processes and the evaders are assumed not\nto react to interdiction decisions. The interdiction objective is to find an\nedge set of size B, that maximizes the probability of capturing the evaders.\n  We prove that similar to the standard least-cost formulation for\ndeterministic motion this interdiction problem is also NP-hard. But unlike that\nproblem our interdiction problem is submodular and the optimal solution can be\napproximated within 1-1/e using a greedy algorithm. Additionally, we exploit\nsubmodularity through a priority evaluation strategy that eliminates the linear\ncomplexity scaling in the number of network edges and speeds up the solution by\norders of magnitude. Taken together the results bring closer the goal of\nfinding realistic solutions to the interdiction problem on global-scale\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2009 20:10:51 GMT"}], "update_date": "2009-03-03", "authors_parsed": [["Gutfraind", "Alexander", ""], ["Hagberg", "Aric", ""], ["Pan", "Feng", ""]]}, {"id": "0903.0197", "submitter": "Sean Cleary", "authors": "Sean Cleary and Katherine St. John", "title": "Rotation Distance is Fixed-Parameter Tractable", "comments": "9 pages, 3 figures", "journal-ref": "Inform. Process. Lett. 109 (2009), no. 16, 918-922", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotation distance between trees measures the number of simple operations it\ntakes to transform one tree into another. There are no known polynomial-time\nalgorithms for computing rotation distance. In the case of ordered rooted\ntrees, we show that the rotation distance between two ordered trees is\nfixed-parameter tractable, in the parameter, k, the rotation distance. The\nproof relies on the kernalization of the initial trees to trees with size\nbounded by 7k.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2009 01:36:50 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Cleary", "Sean", ""], ["John", "Katherine St.", ""]]}, {"id": "0903.0199", "submitter": "Sean Cleary", "authors": "Sean Cleary and Katherine St. John", "title": "A Linear-Time Approximation Algorithm for Rotation Distance", "comments": "5 pages, 1 figure", "journal-ref": "J. Graph Algorithms Appl. 14 (2010), no. 2, 385-390", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotation distance between rooted binary trees measures the number of simple\noperations it takes to transform one tree into another. There are no known\npolynomial-time algorithms for computing rotation distance. We give an\nefficient, linear-time approximation algorithm, which estimates the rotation\ndistance, within a provable factor of 2, between ordered rooted binary trees. .\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2009 01:40:14 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2009 19:32:46 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Cleary", "Sean", ""], ["John", "Katherine St.", ""]]}, {"id": "0903.0367", "submitter": "Yury Makarychev", "authors": "Konstantin Makarychev and Yury Makarychev", "title": "How to Play Unique Games on Expanders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we improve a recent result by Arora, Khot, Kolla, Steurer,\nTulsiani, and Vishnoi on solving the Unique Games problem on expanders.\n  Given a $(1-\\varepsilon)$-satisfiable instance of Unique Games with the\nconstraint graph $G$, our algorithm finds an assignment satisfying at least a\n$1- C \\varepsilon/h_G$ fraction of all constraints if $\\varepsilon < c\n\\lambda_G$ where $h_G$ is the edge expansion of $G$, $\\lambda_G$ is the second\nsmallest eigenvalue of the Laplacian of $G$, and $C$ and $c$ are some absolute\nconstants.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2009 20:40:53 GMT"}], "update_date": "2009-03-03", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""]]}, {"id": "0903.0391", "submitter": "Yuriy Arbitman", "authors": "Yuriy Arbitman, Moni Naor and Gil Segev", "title": "De-amortized Cuckoo Hashing: Provable Worst-Case Performance and\n  Experimental Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cuckoo hashing is a highly practical dynamic dictionary: it provides\namortized constant insertion time, worst case constant deletion time and lookup\ntime, and good memory utilization. However, with a noticeable probability\nduring the insertion of n elements some insertion requires \\Omega(log n) time.\nWhereas such an amortized guarantee may be suitable for some applications, in\nother applications (such as high-performance routing) this is highly\nundesirable.\n  Recently, Kirsch and Mitzenmacher (Allerton '07) proposed a de-amortization\nof cuckoo hashing using various queueing techniques that preserve its\nattractive properties. Kirsch and Mitzenmacher demonstrated a significant\nimprovement to the worst case performance of cuckoo hashing via experimental\nresults, but they left open the problem of constructing a scheme with provable\nproperties.\n  In this work we follow Kirsch and Mitzenmacher and present a de-amortization\nof cuckoo hashing that provably guarantees constant worst case operations.\nSpecifically, for any sequence of polynomially many operations, with\noverwhelming probability over the randomness of the initialization phase, each\noperation is performed in constant time. Our theoretical analysis and\nexperimental results indicate that the scheme is highly efficient, and provides\na practical alternative to the only other known dynamic dictionary with such\nworst case guarantees, due to Dietzfelbinger and Meyer auf der Heide (ICALP\n'90).\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2009 21:21:47 GMT"}], "update_date": "2009-03-04", "authors_parsed": [["Arbitman", "Yuriy", ""], ["Naor", "Moni", ""], ["Segev", "Gil", ""]]}, {"id": "0903.0422", "submitter": "Hirotaka Ono", "authors": "Kazuhisa Makino and Hirotaka Ono", "title": "Deductive Inference for the Interiors and Exteriors of Horn Theories", "comments": "20 pages, 1 figure, An extended abstract of this article was\n  presented in Proceedings of Algorithms and Computation, 19th International\n  Symposium (ISAAC 2008), Lecture Notes in Computer Science, Vol. 5369, pp.\n  390-401, Springer-Verlag Berlin Heidelberg, 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the deductive inference for the interiors and\nexteriors of Horn knowledge bases, where the interiors and exteriors were\nintroduced by Makino and Ibaraki to study stability properties of knowledge\nbases. We present a linear time algorithm for the deduction for the interiors\nand show that it is co-NP-complete for the deduction for the exteriors. Under\nmodel-based representation, we show that the deduction problem for interiors is\nNP-complete while the one for exteriors is co-NP-complete. As for Horn\nenvelopes of the exteriors, we show that it is linearly solvable under\nmodel-based representation, while it is co-NP-complete under formula-based\nrepresentation. We also discuss the polynomially solvable cases for all the\nintractable problems.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2009 01:58:52 GMT"}], "update_date": "2009-03-04", "authors_parsed": [["Makino", "Kazuhisa", ""], ["Ono", "Hirotaka", ""]]}, {"id": "0903.0445", "submitter": "Salah A. Aly", "authors": "Salah A. Aly, Zhenning Kong, Emina Soljanin", "title": "Raptor Codes Based Distributed Storage Algorithms for Wireless Sensor\n  Networks", "comments": "published in IEEE ISIT 2008", "journal-ref": null, "doi": "10.1109/ISIT.2008.4595350", "report-no": null, "categories": "cs.IT cs.DS cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a distributed storage problem in a large-scale wireless sensor\nnetwork with $n$ nodes among which $k$ acquire (sense) independent data. The\ngoal is to disseminate the acquired information throughout the network so that\neach of the $n$ sensors stores one possibly coded packet and the original $k$\ndata packets can be recovered later in a computationally simple way from any\n$(1+\\epsilon)k$ of nodes for some small $\\epsilon>0$. We propose two Raptor\ncodes based distributed storage algorithms for solving this problem. In the\nfirst algorithm, all the sensors have the knowledge of $n$ and $k$. In the\nsecond one, we assume that no sensor has such global information.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2009 06:04:30 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Aly", "Salah A.", ""], ["Kong", "Zhenning", ""], ["Soljanin", "Emina", ""]]}, {"id": "0903.0460", "submitter": "Toby Walsh", "authors": "Alan Frisch, Brahim Hnich, Zeynep Kiziltan, Ian Miguel, Toby Walsh", "title": "Filtering Algorithms for the Multiset Ordering Constraint", "comments": null, "journal-ref": "Artificial Intelligence, 173 (2), 299-328, 2009", "doi": "10.1016/j.artint.2008.11.001", "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint programming (CP) has been used with great success to tackle a wide\nvariety of constraint satisfaction problems which are computationally\nintractable in general. Global constraints are one of the important factors\nbehind the success of CP. In this paper, we study a new global constraint, the\nmultiset ordering constraint, which is shown to be useful in symmetry breaking\nand searching for leximin optimal solutions in CP. We propose efficient and\neffective filtering algorithms for propagating this global constraint. We show\nthat the algorithms are sound and complete and we discuss possible extensions.\nWe also consider alternative propagation methods based on existing constraints\nin CP toolkits. Our experimental results on a number of benchmark problems\ndemonstrate that propagating the multiset ordering constraint via a dedicated\nalgorithm can be very beneficial.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2009 08:04:42 GMT"}], "update_date": "2009-03-04", "authors_parsed": [["Frisch", "Alan", ""], ["Hnich", "Brahim", ""], ["Kiziltan", "Zeynep", ""], ["Miguel", "Ian", ""], ["Walsh", "Toby", ""]]}, {"id": "0903.0544", "submitter": "Robin Moser", "authors": "Robin A. Moser, G\\'abor Tardos", "title": "A constructive proof of the general Lovasz Local Lemma", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lovasz Local Lemma [EL75] is a powerful tool to non-constructively prove\nthe existence of combinatorial objects meeting a prescribed collection of\ncriteria. In his breakthrough paper [Bec91], Beck demonstrated that a\nconstructive variant can be given under certain more restrictive conditions.\nSimplifications of his procedure and relaxations of its restrictions were\nsubsequently exhibited in several publications [Alo91, MR98, CS00, Mos06,\nSri08, Mos08]. In [Mos09], a constructive proof was presented that works under\nnegligible restrictions, formulated in terms of the Bounded Occurrence\nSatisfiability problem. In the present paper, we reformulate and improve upon\nthese findings so as to directly apply to almost all known applications of the\ngeneral Local Lemma.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2009 14:42:01 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2009 21:10:07 GMT"}, {"version": "v3", "created": "Wed, 20 May 2009 23:30:04 GMT"}], "update_date": "2009-05-21", "authors_parsed": [["Moser", "Robin A.", ""], ["Tardos", "G\u00e1bor", ""]]}, {"id": "0903.0938", "submitter": "Gregory Gutin", "authors": "Nathann Cohen, Fedor V. Fomin, Gregory Gutin, Eun Jung Kim, Saket\n  Saurabh, Anders Yeo", "title": "Algorithm for Finding $k$-Vertex Out-trees and its Application to\n  $k$-Internal Out-branching Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An out-tree $T$ is an oriented tree with only one vertex of in-degree zero. A\nvertex $x$ of $T$ is internal if its out-degree is positive. We design\nrandomized and deterministic algorithms for deciding whether an input digraph\ncontains a given out-tree with $k$ vertices. The algorithms are of runtime\n$O^*(5.704^k)$ and $O^*(5.704^{k(1+o(1))})$, respectively. We apply the\ndeterministic algorithm to obtain a deterministic algorithm of runtime\n$O^*(c^k)$, where $c$ is a constant, for deciding whether an input digraph\ncontains a spanning out-tree with at least $k$ internal vertices. This answers\nin affirmative a question of Gutin, Razgon and Kim (Proc. AAIM'08).\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2009 16:15:16 GMT"}], "update_date": "2009-03-06", "authors_parsed": [["Cohen", "Nathann", ""], ["Fomin", "Fedor V.", ""], ["Gutin", "Gregory", ""], ["Kim", "Eun Jung", ""], ["Saurabh", "Saket", ""], ["Yeo", "Anders", ""]]}, {"id": "0903.1095", "submitter": "Jakub Mare\\v{c}ek", "authors": "Edmund K. Burke, Jakub Marecek, Andrew J. Parkes, Hana Rudova", "title": "Decomposition, Reformulation, and Diving in University Course\n  Timetabling", "comments": "45 pages, 7 figures. Improved typesetting of figures and tables", "journal-ref": "Computers and Operations Research (2010) 37(3), 582-597", "doi": "10.1016/j.cor.2009.02.023", "report-no": "NOTTCS-TR-2008-02", "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-life optimisation problems, there are multiple interacting\ncomponents in a solution. For example, different components might specify\nassignments to different kinds of resource. Often, each component is associated\nwith different sets of soft constraints, and so with different measures of soft\nconstraint violation. The goal is then to minimise a linear combination of such\nmeasures. This paper studies an approach to such problems, which can be thought\nof as multiphase exploitation of multiple objective-/value-restricted\nsubmodels. In this approach, only one computationally difficult component of a\nproblem and the associated subset of objectives is considered at first. This\nproduces partial solutions, which define interesting neighbourhoods in the\nsearch space of the complete problem. Often, it is possible to pick the initial\ncomponent so that variable aggregation can be performed at the first stage, and\nthe neighbourhoods to be explored next are guaranteed to contain feasible\nsolutions. Using integer programming, it is then easy to implement heuristics\nproducing solutions with bounds on their quality.\n  Our study is performed on a university course timetabling problem used in the\n2007 International Timetabling Competition, also known as the Udine Course\nTimetabling Problem. In the proposed heuristic, an objective-restricted\nneighbourhood generator produces assignments of periods to events, with\ndecreasing numbers of violations of two period-related soft constraints. Those\nare relaxed into assignments of events to days, which define neighbourhoods\nthat are easier to search with respect to all four soft constraints. Integer\nprogramming formulations for all subproblems are given and evaluated using ILOG\nCPLEX 11. The wider applicability of this approach is analysed and discussed.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2009 20:40:32 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2009 17:16:13 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Burke", "Edmund K.", ""], ["Marecek", "Jakub", ""], ["Parkes", "Andrew J.", ""], ["Rudova", "Hana", ""]]}, {"id": "0903.1407", "submitter": "Ricardo Corr\\^ea", "authors": "Manoel Campelo and Ricardo C. Correa", "title": "A Lagrangian Relaxation for the Maximum Stable Set Problem", "comments": "Submitted to International Transactions on Operations Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new integer programming formulation for the problem of finding a\nmaximum stable set of a graph based on representatives of stable sets. In\naddition, we investigate exact solutions provided by a Lagrangian decomposition\nof this formulation in which only one constraint is relaxed. Some computational\nexperiments were carried out with an effective multi-threaded implementation of\nour algorithm in a multi-core system, and their results are presented.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2009 12:08:33 GMT"}], "update_date": "2009-03-10", "authors_parsed": [["Campelo", "Manoel", ""], ["Correa", "Ricardo C.", ""]]}, {"id": "0903.2015", "submitter": "Kang Ning", "authors": "Kang Ning", "title": "Deposition and Extension Approach to Find Longest Common Subsequence for\n  Multiple Sequences", "comments": "25 pages, 6 figures. Ready to be submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding the longest common subsequence (LCS) for a set of\nsequences is a very interesting and challenging problem in computer science.\nThis problem is NP-complete, but because of its importance, many heuristic\nalgorithms have been proposed, such as Long Run algorithm and Expansion\nalgorithm.\n  However, the performance of many current heuristic algorithms deteriorates\nfast when the number of sequences and sequence length increase. In this paper,\nwe have proposed a post process heuristic algorithm for the LCS problem, the\nDeposition and Extension algorithm (DEA). This algorithm first generates common\nsubsequence by the process of sequences deposition, and then extends this\ncommon subsequence. The algorithm is proven to generate Common Subsequences\n(CSs) with guaranteed lengths. The experiments show that the results of DEA\nalgorithm are better than those of Long Run and Expansion algorithm, especially\non many long sequences. The algorithm also has superior efficiency both in time\nand space.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2009 17:18:38 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2009 20:22:02 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2009 23:25:56 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Ning", "Kang", ""]]}, {"id": "0903.2265", "submitter": "Rolf Harren", "authors": "Rolf Harren, Rob van Stee", "title": "An Absolute 2-Approximation Algorithm for Two-Dimensional Bin Packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of packing rectangles into bins that are unit\nsquares, where the goal is to minimize the number of bins used. All rectangles\nhave to be packed non-overlapping and orthogonal, i.e., axis-parallel. We\npresent an algorithm for this problem with an absolute worst-case ratio of 2,\nwhich is optimal provided P != NP.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2009 11:39:14 GMT"}], "update_date": "2009-03-16", "authors_parsed": [["Harren", "Rolf", ""], ["van Stee", "Rob", ""]]}, {"id": "0903.2310", "submitter": "Kang Ning", "authors": "Kang Ning, Hoong Kee Ng, Hon Wai Leong", "title": "Analysis of the Relationships among Longest Common Subsequences,\n  Shortest Common Supersequences and Patterns and its application on Pattern\n  Discovery in Biological Sequences", "comments": "Extended version of paper presented in IEEE BIBE 2006 submitted to\n  journal for review", "journal-ref": null, "doi": "10.1504/IJDMB.2011.045413", "report-no": null, "categories": "cs.DS cs.DM cs.IR cs.OH q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a set of mulitple sequences, their patterns,Longest Common Subsequences\n(LCS) and Shortest Common Supersequences (SCS) represent different aspects of\nthese sequences profile, and they can all be used for biological sequence\ncomparisons and analysis. Revealing the relationship between the patterns and\nLCS,SCS might provide us with a deeper view of the patterns of biological\nsequences, in turn leading to better understanding of them. However, There is\nno careful examinaton about the relationship between patterns, LCS and SCS. In\nthis paper, we have analyzed their relation, and given some lemmas. Based on\ntheir relations, a set of algorithms called the PALS (PAtterns by Lcs and Scs)\nalgorithms are propsoed to discover patterns in a set of biological sequences.\nThese algorithms first generate the results for LCS and SCS of sequences by\nheuristic, and consequently derive patterns from these results. Experiments\nshow that the PALS algorithms perform well (both in efficiency and in accuracy)\non a variety of sequences. The PALS approach also provides us with a solution\nfor transforming between the heuristic results of SCS and LCS.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2009 04:45:05 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Ning", "Kang", ""], ["Ng", "Hoong Kee", ""], ["Leong", "Hon Wai", ""]]}, {"id": "0903.2507", "submitter": "David Eppstein", "authors": "Sergio Cabello, David Eppstein, and Sandi Klavzar", "title": "The Fibonacci dimension of a graph", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": "IMFM Preprint 1084", "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fibonacci dimension fdim(G) of a graph G is introduced as the smallest\ninteger f such that G admits an isometric embedding into Gamma_f, the\nf-dimensional Fibonacci cube. We give bounds on the Fibonacci dimension of a\ngraph in terms of the isometric and lattice dimension, provide a combinatorial\ncharacterization of the Fibonacci dimension using properties of an associated\ngraph, and establish the Fibonacci dimension for certain families of graphs.\n  From the algorithmic point of view we prove that it is NP-complete to decide\nif fdim(G) equals to the isometric dimension of G, and that it is also NP-hard\nto approximate fdim(G) within (741/740)-epsilon. We also give a\n(3/2)-approximation algorithm for fdim(G) in the general case and a\n(1+epsilon)-approximation algorithm for simplex graphs.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2009 22:24:17 GMT"}], "update_date": "2009-03-17", "authors_parsed": [["Cabello", "Sergio", ""], ["Eppstein", "David", ""], ["Klavzar", "Sandi", ""]]}, {"id": "0903.2816", "submitter": "Daniel A. Spielman", "authors": "Daniel A Spielman, Jaeoh Woo", "title": "A Note on Preconditioning by Low-Stretch Spanning Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boman and Hendrickson observed that one can solve linear systems in Laplacian\nmatrices in time $\\bigO{m^{3/2 + o (1)} \\ln (1/\\epsilon)}$ by preconditioning\nwith the Laplacian of a low-stretch spanning tree. By examining the\ndistribution of eigenvalues of the preconditioned linear system, we prove that\nthe preconditioned conjugate gradient will actually solve the linear system in\ntime $\\softO{m^{4/3} \\ln (1/\\epsilon)}$.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2009 18:04:31 GMT"}], "update_date": "2009-03-17", "authors_parsed": [["Spielman", "Daniel A", ""], ["Woo", "Jaeoh", ""]]}, {"id": "0903.3106", "submitter": "Sebastien Tixeuil", "authors": "Toshimitsu Masuzawa, S\\'ebastien Tixeuil (LIP6)", "title": "Stabilizing Maximal Independent Set in Unidirectional Networks is Hard", "comments": null, "journal-ref": null, "doi": null, "report-no": "RR-6880", "categories": "cs.DS cs.CC cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed algorithm is self-stabilizing if after faults and attacks hit\nthe system and place it in some arbitrary global state, the system recovers\nfrom this catastrophic situation without external intervention in finite time.\nIn this paper, we consider the problem of constructing self-stabilizingly a\n\\emph{maximal independent set} in uniform unidirectional networks of arbitrary\nshape. On the negative side, we present evidence that in uniform networks,\n\\emph{deterministic} self-stabilization of this problem is \\emph{impossible}.\nAlso, the \\emph{silence} property (\\emph{i.e.} having communication fixed from\nsome point in every execution) is impossible to guarantee, either for\ndeterministic or for probabilistic variants of protocols. On the positive side,\nwe present a deterministic protocol for networks with arbitrary unidirectional\nnetworks with unique identifiers that exhibits polynomial space and time\ncomplexity in asynchronous scheduling. We complement the study with\nprobabilistic protocols for the uniform case: the first probabilistic protocol\nrequires infinite memory but copes with asynchronous scheduling, while the\nsecond probabilistic protocol has polynomial space complexity but can only\nhandle synchronous scheduling. Both probabilistic solutions have expected\npolynomial time complexity.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2009 08:42:02 GMT"}], "update_date": "2009-04-20", "authors_parsed": [["Masuzawa", "Toshimitsu", "", "LIP6"], ["Tixeuil", "S\u00e9bastien", "", "LIP6"]]}, {"id": "0903.3276", "submitter": "Vitaly Shmatikov", "authors": "Arvind Narayanan, Vitaly Shmatikov", "title": "De-anonymizing Social Networks", "comments": "Published in the 30th IEEE Symposium on Security and Privacy, 2009.\n  The definitive version is available at:\n  http://www.cs.utexas.edu/~shmat/shmat_oak09.pdf Frequently Asked Questions\n  are answered at: http://www.cs.utexas.edu/~shmat/socialnetworks-faq.html", "journal-ref": null, "doi": "10.1109/SP.2009.22", "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operators of online social networks are increasingly sharing potentially\nsensitive information about users and their relationships with advertisers,\napplication developers, and data-mining researchers. Privacy is typically\nprotected by anonymization, i.e., removing names, addresses, etc.\n  We present a framework for analyzing privacy and anonymity in social networks\nand develop a new re-identification algorithm targeting anonymized\nsocial-network graphs. To demonstrate its effectiveness on real-world networks,\nwe show that a third of the users who can be verified to have accounts on both\nTwitter, a popular microblogging service, and Flickr, an online photo-sharing\nsite, can be re-identified in the anonymous Twitter graph with only a 12% error\nrate.\n  Our de-anonymization algorithm is based purely on the network topology, does\nnot require creation of a large number of dummy \"sybil\" nodes, is robust to\nnoise and all existing defenses, and works even when the overlap between the\ntarget network and the adversary's auxiliary information is small.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2009 06:55:46 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Narayanan", "Arvind", ""], ["Shmatikov", "Vitaly", ""]]}, {"id": "0903.3461", "submitter": "Andreas Tielmann", "authors": "Carole Delporte-Gallet (LIAFA), Hugues Fauconnier (LIAFA), Andreas\n  Tielmann (LIAFA)", "title": "Fault-Tolerant Consensus in Unknown and Anonymous Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates under which conditions information can be reliably\nshared and consensus can be solved in unknown and anonymous message-passing\nnetworks that suffer from crash-failures. We provide algorithms to emulate\nregisters and solve consensus under different synchrony assumptions. For this,\nwe introduce a novel pseudo leader-election approach which allows a\nleader-based consensus implementation without breaking symmetry.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2009 07:50:46 GMT"}], "update_date": "2009-03-23", "authors_parsed": [["Delporte-Gallet", "Carole", "", "LIAFA"], ["Fauconnier", "Hugues", "", "LIAFA"], ["Tielmann", "Andreas", "", "LIAFA"]]}, {"id": "0903.3579", "submitter": "Peter Krusche", "authors": "Peter Krusche, Alexander Tiskin", "title": "String comparison by transposition networks", "comments": "Published in London Algorithmics 2008: Theory And Practice, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing string or sequence alignments is a classical method of comparing\nstrings and has applications in many areas of computing, such as signal\nprocessing and bioinformatics. Semi-local string alignment is a recent\ngeneralisation of this method, in which the alignment of a given string and all\nsubstrings of another string are computed simultaneously at no additional\nasymptotic cost. In this paper, we show that there is a close connection\nbetween semi-local string alignment and a certain class of traditional\ncomparison networks known as transposition networks. The transposition network\napproach can be used to represent different string comparison algorithms in a\nunified form, and in some cases provides generalisations or improvements on\nexisting algorithms. This approach allows us to obtain new algorithms for\nsparse semi-local string comparison and for comparison of highly similar and\nhighly dissimilar strings, as well as of run-length compressed strings. We\nconclude that the transposition network method is a very general and flexible\nway of understanding and improving different string comparison algorithms, as\nwell as their efficient implementation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2009 17:53:55 GMT"}], "update_date": "2009-03-23", "authors_parsed": [["Krusche", "Peter", ""], ["Tiskin", "Alexander", ""]]}, {"id": "0903.3622", "submitter": "Mugurel Ionut Andreica", "authors": "Mugurel Ionut Andreica, Sorin Briciu, Madalina Ecaterina Andreica", "title": "Algorithmic Solutions to Some Transportation Optimization Problems with\n  Applications in the Metallurgical Industry", "comments": null, "journal-ref": "Metalurgia International, vol. 14, special issue no. 5, pp. 46-53,\n  2009. (ISSN: 1582-2214) ; http://www.metalurgia.ro/metalurgia_int.html", "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address several constrained transportation optimization\nproblems (e.g. vehicle routing, shortest Hamiltonian path), for which we\npresent novel algorithmic solutions and extensions, considering several\noptimization objectives, like minimizing costs and resource usage. All the\nconsidered problems are motivated by practical situations arising, for\ninstance, in the mining and metallurgical industry or in data communication. We\nrestrict our attention to transportation networks with path, tree or geometric\nstructures, for which the developed polynomial-time algorithms are optimal or\nnearly optimal.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2009 22:21:30 GMT"}], "update_date": "2009-03-24", "authors_parsed": [["Andreica", "Mugurel Ionut", ""], ["Briciu", "Sorin", ""], ["Andreica", "Madalina Ecaterina", ""]]}, {"id": "0903.4130", "submitter": "Amr Elmasry", "authors": "Amr Elmasry", "title": "Pairing Heaps with Costless Meld", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving the structure and analysis in \\cite{elm0}, we give a variation of\nthe pairing heaps that has amortized zero cost per meld (compared to an $O(\\log\n\\log{n})$ in \\cite{elm0}) and the same amortized bounds for all other\noperations. More precisely, the new pairing heap requires: no cost per meld,\nO(1) per find-min and insert, $O(\\log{n})$ per delete-min, and $O(\\log\\log{n})$\nper decrease-key. These bounds are the best known for any self-adjusting heap,\nand match the lower bound proved by Fredman for a family of such heaps.\nMoreover, the changes we have done make our structure even simpler than that in\n\\cite{elm0}.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2009 16:49:57 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2009 15:46:42 GMT"}], "update_date": "2009-04-09", "authors_parsed": [["Elmasry", "Amr", ""]]}, {"id": "0903.4251", "submitter": "Artur Ferreira", "authors": "Artur Ferreira, Arlindo Oliveira, Mario Figueiredo", "title": "On the Use of Suffix Arrays for Memory-Efficient Lempel-Ziv Data\n  Compression", "comments": "10 pages, submited to IEEE - Data Compression Conference 2009", "journal-ref": null, "doi": "10.1109/DCC.2009.50", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much research has been devoted to optimizing algorithms of the Lempel-Ziv\n(LZ) 77 family, both in terms of speed and memory requirements. Binary search\ntrees and suffix trees (ST) are data structures that have been often used for\nthis purpose, as they allow fast searches at the expense of memory usage.\n  In recent years, there has been interest on suffix arrays (SA), due to their\nsimplicity and low memory requirements. One key issue is that an SA can solve\nthe sub-string problem almost as efficiently as an ST, using less memory. This\npaper proposes two new SA-based algorithms for LZ encoding, which require no\nmodifications on the decoder side. Experimental results on standard benchmarks\nshow that our algorithms, though not faster, use 3 to 5 times less memory than\nthe ST counterparts. Another important feature of our SA-based algorithms is\nthat the amount of memory is independent of the text to search, thus the memory\nthat has to be allocated can be defined a priori. These features of low and\npredictable memory requirements are of the utmost importance in several\nscenarios, such as embedded systems, where memory is at a premium and speed is\nnot critical. Finally, we point out that the new algorithms are general, in the\nsense that they are adequate for applications other than LZ compression, such\nas text retrieval and forward/backward sub-string search.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2009 19:25:24 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Ferreira", "Artur", ""], ["Oliveira", "Arlindo", ""], ["Figueiredo", "Mario", ""]]}, {"id": "0903.4510", "submitter": "Kunal Talwar", "authors": "Anupam Gupta, Katrina Ligett, Frank McSherry, Aaron Roth and Kunal\n  Talwar", "title": "Differentially Private Combinatorial Optimization", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following problem: given a metric space, some of whose points\nare \"clients\", open a set of at most $k$ facilities to minimize the average\ndistance from the clients to these facilities. This is just the well-studied\n$k$-median problem, for which many approximation algorithms and hardness\nresults are known. Note that the objective function encourages opening\nfacilities in areas where there are many clients, and given a solution, it is\noften possible to get a good idea of where the clients are located. However,\nthis poses the following quandary: what if the identity of the clients is\nsensitive information that we would like to keep private? Is it even possible\nto design good algorithms for this problem that preserve the privacy of the\nclients?\n  In this paper, we initiate a systematic study of algorithms for discrete\noptimization problems in the framework of differential privacy (which\nformalizes the idea of protecting the privacy of individual input elements). We\nshow that many such problems indeed have good approximation algorithms that\npreserve differential privacy; this is even in cases where it is impossible to\npreserve cryptographic definitions of privacy while computing any non-trivial\napproximation to even the_value_ of an optimal solution, let alone the entire\nsolution.\n  Apart from the $k$-median problem, we study the problems of vertex and set\ncover, min-cut, facility location, Steiner tree, and the recently introduced\nsubmodular maximization problem, \"Combinatorial Public Projects\" (CPP).\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2009 05:08:46 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2009 01:19:22 GMT"}], "update_date": "2009-11-11", "authors_parsed": [["Gupta", "Anupam", ""], ["Ligett", "Katrina", ""], ["McSherry", "Frank", ""], ["Roth", "Aaron", ""], ["Talwar", "Kunal", ""]]}, {"id": "0903.4521", "submitter": "Geevarghese Philip", "authors": "Geevarghese Philip, Venkatesh Raman, Somnath Sikdar", "title": "Solving Dominating Set in Larger Classes of Graphs: FPT Algorithms and\n  Polynomial Kernels", "comments": "12 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the k-Dominating Set problem is fixed parameter tractable (FPT)\nand has a polynomial kernel for any class of graphs that exclude K_{i,j} as a\nsubgraph, for any fixed i, j >= 1. This strictly includes every class of graphs\nfor which this problem has been previously shown to have FPT algorithms and/or\npolynomial kernels. In particular, our result implies that the problem\nrestricted to bounded- degenerate graphs has a polynomial kernel, solving an\nopen problem posed by Alon and Gutner.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2009 06:23:58 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2009 13:41:18 GMT"}, {"version": "v3", "created": "Fri, 15 May 2009 07:33:23 GMT"}], "update_date": "2009-05-15", "authors_parsed": [["Philip", "Geevarghese", ""], ["Raman", "Venkatesh", ""], ["Sikdar", "Somnath", ""]]}, {"id": "0903.4726", "submitter": "Travis Gagie", "authors": "Travis Gagie, Simon J. Puglisi, Andrew Turpin", "title": "Range Quantile Queries: Another Virtue of Wavelet Trees", "comments": "Added note about generalization to any constant number of dimensions.", "journal-ref": null, "doi": "10.1007/978-3-642-03784-9_1", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to use a balanced wavelet tree as a data structure that stores a\nlist of numbers and supports efficient {\\em range quantile queries}. A range\nquantile query takes a rank and the endpoints of a sublist and returns the\nnumber with that rank in that sublist. For example, if the rank is half the\nsublist's length, then the query returns the sublist's median. We also show how\nthese queries can be used to support space-efficient {\\em coloured range\nreporting} and {\\em document listing}.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2009 02:29:01 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2009 15:54:35 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2009 09:44:31 GMT"}, {"version": "v4", "created": "Wed, 20 May 2009 12:43:47 GMT"}, {"version": "v5", "created": "Thu, 21 May 2009 08:09:09 GMT"}, {"version": "v6", "created": "Wed, 7 Apr 2010 12:48:51 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Gagie", "Travis", ""], ["Puglisi", "Simon J.", ""], ["Turpin", "Andrew", ""]]}, {"id": "0903.4796", "submitter": "Martin Vatshelle", "authors": "B.-M. Bui-Xuan and J. A. Telle and M. Vatshelle (Department of\n  Informatics, University of Bergen, Norway)", "title": "Fast FPT algorithms for vertex subset and vertex partitioning problems\n  using neighborhood unions", "comments": "The new version has runtimes expressed by number of equivalence\n  classes, but no other changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the graph parameter boolean-width, related to the number of\ndifferent unions of neighborhoods across a cut of a graph. Boolean-width is\nsimilar to rank-width, which is related to the number of $GF[2]$-sums (1+1=0)\nof neighborhoods instead of the boolean-sums (1+1=1) used for boolean-width. We\ngive algorithms for a large class of NP-hard vertex subset and vertex\npartitioning problems that are FPT when parameterized by either boolean-width,\nrank-width or clique-width, with runtime single exponential in either parameter\nif given the pertinent optimal decomposition. To compare boolean-width versus\nrank-width or clique-width, we first show that for any graph, the square root\nof its boolean-width is never more than its rank-width. Next, we exhibit a\nclass of graphs, the Hsu-grids, for which we can solve NP-hard problems in\npolynomial time, if we use the right parameter. An $n \\times \\frac{n}{10}$\nHsu-grid on ${1/10}n^2$ vertices has boolean-width $\\Theta(\\log n)$ and\nrank-width $\\Theta(n)$. Moreover, any optimal rank-decomposition of such a\ngraph will have boolean-width $\\Theta(n)$, i.e. exponential in the optimal\nboolean-width. A main open problem is to approximate the boolean-width better\nthan what is given by the algorithm for rank-width [Hlin\\v{e}n\\'y and Oum,\n2008]\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2009 13:34:08 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2009 07:24:30 GMT"}, {"version": "v3", "created": "Wed, 9 Mar 2011 15:17:52 GMT"}], "update_date": "2011-03-10", "authors_parsed": [["Bui-Xuan", "B. -M.", "", "Department of\n  Informatics, University of Bergen, Norway"], ["Telle", "J. A.", "", "Department of\n  Informatics, University of Bergen, Norway"], ["Vatshelle", "M.", "", "Department of\n  Informatics, University of Bergen, Norway"]]}, {"id": "0903.4898", "submitter": "Predrag Jelenkovic", "authors": "Predrag R. Jelenkovic and Ana Radovanovic", "title": "Asymptotic Optimality of the Static Frequency Caching in the Presence of\n  Correlated Requests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the static caching algorithm that keeps the most\nfrequently requested documents in the cache is optimal in case when documents\nare of the same size and requests are independent and equally distributed.\nHowever, it is hard to develop explicit and provably optimal caching algorithms\nwhen requests are statistically correlated. In this paper, we show that keeping\nthe most frequently requested documents in the cache is still optimal for large\ncache sizes even if the requests are strongly correlated.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2009 20:05:17 GMT"}], "update_date": "2009-03-31", "authors_parsed": [["Jelenkovic", "Predrag R.", ""], ["Radovanovic", "Ana", ""]]}]