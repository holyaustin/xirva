[{"id": "0811.0254", "submitter": "Masud Hasan", "authors": "Muhammad Abdullah Adnan and Masud Hasan", "title": "Characterizing Graphs of Zonohedra", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classic theorem by Steinitz states that a graph G is realizable by a convex\npolyhedron if and only if G is 3-connected planar. Zonohedra are an important\nsubclass of convex polyhedra having the property that the faces of a zonohedron\nare parallelograms and are in parallel pairs. In this paper we give\ncharacterization of graphs of zonohedra. We also give a linear time algorithm\nto recognize such a graph. In our quest for finding the algorithm, we prove\nthat in a zonohedron P both the number of zones and the number of faces in each\nzone is O(square root{n}), where n is the number of vertices of P.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2008 10:19:10 GMT"}], "update_date": "2008-11-04", "authors_parsed": [["Adnan", "Muhammad Abdullah", ""], ["Hasan", "Masud", ""]]}, {"id": "0811.0811", "submitter": "Andreas Blass", "authors": "Andreas Blass (University of Michigan), Nachum Dershowitz (Tel Aviv\n  University), and Yuri Gurevich (Microsoft Research)", "title": "When are two algorithms the same?", "comments": null, "journal-ref": "Bulletin of Symbolic Logic, vol. 15, no. 2, pp. 145-168, 2009", "doi": null, "report-no": null, "categories": "cs.GL cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People usually regard algorithms as more abstract than the programs that\nimplement them. The natural way to formalize this idea is that algorithms are\nequivalence classes of programs with respect to a suitable equivalence\nrelation. We argue that no such equivalence relation exists.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2008 20:38:22 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Blass", "Andreas", "", "University of Michigan"], ["Dershowitz", "Nachum", "", "Tel Aviv\n  University"], ["Gurevich", "Yuri", "", "Microsoft Research"]]}, {"id": "0811.1083", "submitter": "George  Fletcher", "authors": "George H. L. Fletcher and Peter W. Beck", "title": "A role-free approach to indexing large RDF data sets in secondary memory\n  for efficient SPARQL evaluation", "comments": "12 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive RDF data sets are becoming commonplace. RDF data is typically\ngenerated in social semantic domains (such as personal information management)\nwherein a fixed schema is often not available a priori. We propose a simple\nThree-way Triple Tree (TripleT) secondary-memory indexing technique to\nfacilitate efficient SPARQL query evaluation on such data sets. The novelty of\nTripleT is that (1) the index is built over the atoms occurring in the data\nset, rather than at a coarser granularity, such as whole triples occurring in\nthe data set; and (2) the atoms are indexed regardless of the roles (i.e.,\nsubjects, predicates, or objects) they play in the triples of the data set. We\nshow through extensive empirical evaluation that TripleT exhibits multiple\norders of magnitude improvement over the state of the art on RDF indexing, in\nterms of both storage and query processing costs.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2008 05:08:41 GMT"}], "update_date": "2008-11-20", "authors_parsed": [["Fletcher", "George H. L.", ""], ["Beck", "Peter W.", ""]]}, {"id": "0811.1301", "submitter": "Amit Bhosle", "authors": "Amit M. Bhosle and Teofilo F. Gonzalez", "title": "Distributed Algorithms for Computing Alternate Paths Avoiding Failed\n  Nodes and Links", "comments": "8 pages, 2 columns, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent study characterizing failures in computer networks shows that\ntransient single element (node/link) failures are the dominant failures in\nlarge communication networks like the Internet. Thus, having the routing paths\nglobally recomputed on a failure does not pay off since the failed element\nrecovers fairly quickly, and the recomputed routing paths need to be discarded.\nIn this paper, we present the first distributed algorithm that computes the\nalternate paths required by some \"proactive recovery schemes\" for handling\ntransient failures. Our algorithm computes paths that avoid a failed node, and\nprovides an alternate path to a particular destination from an upstream\nneighbor of the failed node. With minor modifications, we can have the\nalgorithm compute alternate paths that avoid a failed link as well. To the best\nof our knowledge all previous algorithms proposed for computing alternate paths\nare centralized, and need complete information of the network graph as input to\nthe algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 9 Nov 2008 03:34:39 GMT"}], "update_date": "2008-11-11", "authors_parsed": [["Bhosle", "Amit M.", ""], ["Gonzalez", "Teofilo F.", ""]]}, {"id": "0811.1304", "submitter": "Phuong Ha", "authors": "Phuong Hoai Ha, Philippas Tsigas and Otto J. Anshus", "title": "NB-FEB: An Easy-to-Use and Scalable Universal Synchronization Primitive\n  for Parallel Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": "CS:2008-69", "categories": "cs.DC cs.AR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of universal synchronization primitives that\ncan support scalable thread synchronization for large-scale many-core\narchitectures. The universal synchronization primitives that have been deployed\nwidely in conventional architectures like CAS and LL/SC are expected to reach\ntheir scalability limits in the evolution to many-core architectures with\nthousands of cores. We introduce a non-blocking full/empty bit primitive, or\nNB-FEB for short, as a promising synchronization primitive for parallel\nprogramming on may-core architectures. We show that the NB-FEB primitive is\nuniversal, scalable, feasible and convenient to use. NB-FEB, together with\nregisters, can solve the consensus problem for an arbitrary number of processes\n(universality). NB-FEB is combinable, namely its memory requests to the same\nmemory location can be combined into only one memory request, which\nconsequently mitigates performance degradation due to synchronization \"hot\nspots\" (scalability). Since NB-FEB is a variant of the original full/empty bit\nthat always returns a value instead of waiting for a conditional flag, it is as\nfeasible as the original full/empty bit, which has been implemented in many\ncomputer systems (feasibility). The original full/empty bit is well-known as a\nspecial-purpose primitive for fast producer-consumer synchronization and has\nbeen used extensively in the specific domain of applications. In this paper, we\nshow that NB-FEB can be deployed easily as a general-purpose primitive. Using\nNB-FEB, we construct a non-blocking software transactional memory system called\nNBFEB-STM, which can be used to handle concurrent threads conveniently.\nNBFEB-STM is space efficient: the space complexity of each object updated by\n$N$ concurrent threads/transactions is $\\Theta(N)$, the optimal.\n", "versions": [{"version": "v1", "created": "Sun, 9 Nov 2008 00:41:07 GMT"}], "update_date": "2008-11-11", "authors_parsed": [["Ha", "Phuong Hoai", ""], ["Tsigas", "Philippas", ""], ["Anshus", "Otto J.", ""]]}, {"id": "0811.1305", "submitter": "Ryan Williams", "authors": "Ryan Williams", "title": "Applying Practice to Theory", "comments": "16 pages, 1 figure; ACM SIGACT News, December 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can complexity theory and algorithms benefit from practical advances in\ncomputing? We give a short overview of some prior work using practical\ncomputing to attack problems in computational complexity and algorithms,\ninformally describe how linear program solvers may be used to help prove new\nlower bounds for satisfiability, and suggest a research program for developing\nnew understanding in circuit complexity.\n", "versions": [{"version": "v1", "created": "Sun, 9 Nov 2008 00:49:41 GMT"}], "update_date": "2008-11-11", "authors_parsed": [["Williams", "Ryan", ""]]}, {"id": "0811.1335", "submitter": "Mugurel Ionut Andreica", "authors": "Mugurel Ionut Andreica", "title": "Algorithmic Techniques for Several Optimization Problems Regarding\n  Distributed Systems with Tree Topologies", "comments": "The 16th International Conference on Applied and Industrial\n  Mathematics, Oradea, Romania, 9-11 October, 2008. ROMAI Journal, vol. 4,\n  2008. (ISSN: 841-5512). In Press", "journal-ref": "ROMAI Journal, vol. 4, no. 1, pp. 1-25, 2008 (ISSN: 1841-5512) ;\n  http://www.romai.ro", "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the development of distributed systems progresses, more and more\nchallenges arise and the need for developing optimized systems and for\noptimizing existing systems from multiple perspectives becomes more stringent.\nIn this paper I present novel algorithmic techniques for solving several\noptimization problems regarding distributed systems with tree topologies. I\naddress topics like: reliability improvement, partitioning, coloring, content\ndelivery, optimal matchings, as well as some tree counting aspects. Some of the\npresented techniques are only of theoretical interest, while others can be used\nin practical settings.\n", "versions": [{"version": "v1", "created": "Sun, 9 Nov 2008 12:59:45 GMT"}], "update_date": "2009-03-21", "authors_parsed": [["Andreica", "Mugurel Ionut", ""]]}, {"id": "0811.1875", "submitter": "Daniel Raible", "authors": "Henning Fernau, Serge Gaspers, Daniel Raible", "title": "Exact Exponential Time Algorithms for Max Internal Spanning Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the NP-hard problem of finding a spanning tree with a maximum\nnumber of internal vertices. This problem is a generalization of the famous\n  Hamiltonian Path problem. Our dynamic-programming algorithms for general and\ndegree-bounded graphs have running times of the form O*(c^n) (c <= 3). The main\nresult, however, is a branching algorithm for graphs with maximum degree three.\nIt only needs polynomial space and has a running time of O*(1.8669^n) when\nanalyzed with respect to the number of vertices. We also show that its running\ntime is 2.1364^k n^O(1) when the goal is to find a spanning tree with at least\nk internal vertices. Both running time bounds are obtained via a Measure &\nConquer analysis, the latter one being a novel use of this kind of analyses for\nparameterized algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2008 12:09:08 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2009 07:32:23 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2009 06:57:33 GMT"}], "update_date": "2009-06-12", "authors_parsed": [["Fernau", "Henning", ""], ["Gaspers", "Serge", ""], ["Raible", "Daniel", ""]]}, {"id": "0811.2457", "submitter": "Ashish Goel", "authors": "Ashish Goel, Michael Kapralov, Sanjeev Khanna", "title": "Perfect Matchings via Uniform Sampling in Regular Bipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we further investigate the well-studied problem of finding a\nperfect matching in a regular bipartite graph. The first non-trivial algorithm,\nwith running time $O(mn)$, dates back to K\\\"{o}nig's work in 1916 (here $m=nd$\nis the number of edges in the graph, $2n$ is the number of vertices, and $d$ is\nthe degree of each node). The currently most efficient algorithm takes time\n$O(m)$, and is due to Cole, Ost, and Schirra. We improve this running time to\n$O(\\min\\{m, \\frac{n^{2.5}\\ln n}{d}\\})$; this minimum can never be larger than\n$O(n^{1.75}\\sqrt{\\ln n})$. We obtain this improvement by proving a uniform\nsampling theorem: if we sample each edge in a $d$-regular bipartite graph\nindependently with a probability $p = O(\\frac{n\\ln n}{d^2})$ then the resulting\ngraph has a perfect matching with high probability. The proof involves a\ndecomposition of the graph into pieces which are guaranteed to have many\nperfect matchings but do not have any small cuts. We then establish a\ncorrespondence between potential witnesses to non-existence of a matching\n(after sampling) in any piece and cuts of comparable size in that same piece.\nKarger's sampling theorem for preserving cuts in a graph can now be adapted to\nprove our uniform sampling theorem for preserving perfect matchings. Using the\n$O(m\\sqrt{n})$ algorithm (due to Hopcroft and Karp) for finding maximum\nmatchings in bipartite graphs on the sampled graph then yields the stated\nrunning time. We also provide an infinite family of instances to show that our\nuniform sampling result is tight up to poly-logarithmic factors (in fact, up to\n$\\ln^2 n$).\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2008 05:49:17 GMT"}], "update_date": "2008-11-18", "authors_parsed": [["Goel", "Ashish", ""], ["Kapralov", "Michael", ""], ["Khanna", "Sanjeev", ""]]}, {"id": "0811.2497", "submitter": "Haris Aziz", "authors": "Haris Aziz and Mike Paterson", "title": "Computing voting power in easy weighted voting games", "comments": "12 pages, Presented at the International Symposium on Combinatorial\n  Optimization 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted voting games are ubiquitous mathematical models which are used in\neconomics, political science, neuroscience, threshold logic, reliability theory\nand distributed systems. They model situations where agents with variable\nvoting weight vote in favour of or against a decision. A coalition of agents is\nwinning if and only if the sum of weights of the coalition exceeds or equals a\nspecified quota. The Banzhaf index is a measure of voting power of an agent in\na weighted voting game. It depends on the number of coalitions in which the\nagent is the difference in the coalition winning or losing. It is well known\nthat computing Banzhaf indices in a weighted voting game is NP-hard. We give a\ncomprehensive classification of weighted voting games which can be solved in\npolynomial time. Among other results, we provide a polynomial\n($O(k{(\\frac{n}{k})}^k)$) algorithm to compute the Banzhaf indices in weighted\nvoting games in which the number of weight values is bounded by $k$.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2008 14:55:51 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2010 22:27:38 GMT"}], "update_date": "2010-02-02", "authors_parsed": [["Aziz", "Haris", ""], ["Paterson", "Mike", ""]]}, {"id": "0811.2546", "submitter": "Andrei Bulatov", "authors": "Andrei A. Bulatov, Evgeny S. Skvortsov", "title": "Phase transition for Local Search on planted SAT", "comments": "20 pages, 3 figures, submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Local Search algorithm (or Hill Climbing, or Iterative Improvement) is\none of the simplest heuristics to solve the Satisfiability and\nMax-Satisfiability problems. It is a part of many satisfiability and\nmax-satisfiability solvers, where it is used to find a good starting point for\na more sophisticated heuristics, and to improve a candidate solution. In this\npaper we give an analysis of Local Search on random planted 3-CNF formulas. We\nshow that if there is k<7/6 such that the clause-to-variable ratio is less than\nk ln(n) (n is the number of variables in a CNF) then Local Search whp does not\nfind a satisfying assignment, and if there is k>7/6 such that the\nclause-to-variable ratio is greater than k ln(n)$ then the local search whp\nfinds a satisfying assignment. As a byproduct we also show that for any\nconstant r there is g such that Local Search applied to a random (not\nnecessarily planted) 3-CNF with clause-to-variable ratio r produces an\nassignment that satisfies at least gn clauses less than the maximal number of\nsatisfiable clauses.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2008 01:41:15 GMT"}], "update_date": "2008-11-18", "authors_parsed": [["Bulatov", "Andrei A.", ""], ["Skvortsov", "Evgeny S.", ""]]}, {"id": "0811.2572", "submitter": "Gwena\\\"el Joret", "authors": "Jean Cardinal, Samuel Fiorini, Gwena\\\"el Joret, Rapha\\\"el M. Jungers,\n  J. Ian Munro", "title": "An Efficient Algorithm for Partial Order Production", "comments": "Referees' comments incorporated", "journal-ref": "SIAM J. Comput. Volume 39, Issue 7, pp. 2927-2940 (2010)", "doi": "10.1137/090759860", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of partial order production: arrange the elements of\nan unknown totally ordered set T into a target partially ordered set S, by\ncomparing a minimum number of pairs in T. Special cases include sorting by\ncomparisons, selection, multiple selection, and heap construction.\n  We give an algorithm performing ITLB + o(ITLB) + O(n) comparisons in the\nworst case. Here, n denotes the size of the ground sets, and ITLB denotes a\nnatural information-theoretic lower bound on the number of comparisons needed\nto produce the target partial order.\n  Our approach is to replace the target partial order by a weak order (that is,\na partial order with a layered structure) extending it, without increasing the\ninformation theoretic lower bound too much. We then solve the problem by\napplying an efficient multiple selection algorithm. The overall complexity of\nour algorithm is polynomial. This answers a question of Yao (SIAM J. Comput.\n18, 1989).\n  We base our analysis on the entropy of the target partial order, a quantity\nthat can be efficiently computed and provides a good estimate of the\ninformation-theoretic lower bound.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2008 16:23:45 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2009 14:29:48 GMT"}], "update_date": "2010-05-06", "authors_parsed": [["Cardinal", "Jean", ""], ["Fiorini", "Samuel", ""], ["Joret", "Gwena\u00ebl", ""], ["Jungers", "Rapha\u00ebl M.", ""], ["Munro", "J. Ian", ""]]}, {"id": "0811.2853", "submitter": "Mohsen Bayati", "authors": "Mohsen Bayati, Andrea Montanari and Amin Saberi", "title": "Generating Random Networks Without Short Cycles", "comments": "36 pages, 1 figure, accepted to Operations Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random graph generation is an important tool for studying large complex\nnetworks. Despite abundance of random graph models, constructing models with\napplication-driven constraints is poorly understood. In order to advance\nstate-of-the-art in this area, we focus on random graphs without short cycles\nas a stylized family of graphs, and propose the RandGraph algorithm for\nrandomly generating them. For any constant k, when m=O(n^{1+1/[2k(k+3)]}),\nRandGraph generates an asymptotically uniform random graph with n vertices, m\nedges, and no cycle of length at most k using O(n^2m) operations. We also\ncharacterize the approximation error for finite values of n. To the best of our\nknowledge, this is the first polynomial-time algorithm for the problem.\nRandGraph works by sequentially adding $m$ edges to an empty graph with n\nvertices. Recently, such sequential algorithms have been successful for random\nsampling problems. Our main contributions to this line of research includes\nintroducing a new approach for sequentially approximating edge-specific\nprobabilities at each step of the algorithm, and providing a new method for\nanalyzing such algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2008 08:05:26 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 17:52:07 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Bayati", "Mohsen", ""], ["Montanari", "Andrea", ""], ["Saberi", "Amin", ""]]}, {"id": "0811.2904", "submitter": "Srinivasa Rao Satti", "authors": "Rasmus Pagh and S. Srinivasa Rao", "title": "Secondary Indexing in One Dimension: Beyond B-trees and Bitmap Indexes", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let S be a finite, ordered alphabet, and let x = x_1 x_2 ... x_n be a string\nover S. A \"secondary index\" for x answers alphabet range queries of the form:\nGiven a range [a_l,a_r] over S, return the set I_{[a_l;a_r]} = {i |x_i \\in\n[a_l; a_r]}. Secondary indexes are heavily used in relational databases and\nscientific data analysis. It is well-known that the obvious solution, storing a\ndictionary for the position set associated with each character, does not always\ngive optimal query time. In this paper we give the first theoretically optimal\ndata structure for the secondary indexing problem. In the I/O model, the amount\nof data read when answering a query is within a constant factor of the minimum\nspace needed to represent I_{[a_l;a_r]}, assuming that the size of internal\nmemory is (|S| log n)^{delta} blocks, for some constant delta > 0. The space\nusage of the data structure is O(n log |S|) bits in the worst case, and we\nfurther show how to bound the size of the data structure in terms of the 0-th\norder entropy of x. We show how to support updates achieving various time-space\ntrade-offs.\n  We also consider an approximate version of the basic secondary indexing\nproblem where a query reports a superset of I_{[a_l;a_r]} containing each\nelement not in I_{[a_l;a_r]} with probability at most epsilon, where epsilon >\n0 is the false positive probability. For this problem the amount of data that\nneeds to be read by the query algorithm is reduced to O(|I_{[a_l;a_r]}|\nlog(1/epsilon)) bits.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2008 13:31:05 GMT"}], "update_date": "2008-11-19", "authors_parsed": [["Pagh", "Rasmus", ""], ["Rao", "S. Srinivasa", ""]]}, {"id": "0811.3055", "submitter": "Ke Xu", "authors": "Liang Li and Tian Liu and Ke Xu", "title": "Exact phase transition of backtrack-free search with implications on the\n  power of greedy algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backtracking is a basic strategy to solve constraint satisfaction problems\n(CSPs). A satisfiable CSP instance is backtrack-free if a solution can be found\nwithout encountering any dead-end during a backtracking search, implying that\nthe instance is easy to solve. We prove an exact phase transition of\nbacktrack-free search in some random CSPs, namely in Model RB and in Model RD.\nThis is the first time an exact phase transition of backtrack-free search can\nbe identified on some random CSPs. Our technical results also have interesting\nimplications on the power of greedy algorithms, on the width of random\nhypergraphs and on the exact satisfiability threshold of random CSPs.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2008 06:33:39 GMT"}], "update_date": "2008-11-20", "authors_parsed": [["Li", "Liang", ""], ["Liu", "Tian", ""], ["Xu", "Ke", ""]]}, {"id": "0811.3062", "submitter": "Qin Zhang", "authors": "Zhewei Wei, Ke Yi, Qin Zhang", "title": "Dynamic External Hashing: The Limit of Buffering", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash tables are one of the most fundamental data structures in computer\nscience, in both theory and practice. They are especially useful in external\nmemory, where their query performance approaches the ideal cost of just one\ndisk access. Knuth gave an elegant analysis showing that with some simple\ncollision resolution strategies such as linear probing or chaining, the\nexpected average number of disk I/Os of a lookup is merely $1+1/2^{\\Omega(b)}$,\nwhere each I/O can read a disk block containing $b$ items. Inserting a new item\ninto the hash table also costs $1+1/2^{\\Omega(b)}$ I/Os, which is again almost\nthe best one can do if the hash table is entirely stored on disk. However, this\nassumption is unrealistic since any algorithm operating on an external hash\ntable must have some internal memory (at least $\\Omega(1)$ blocks) to work\nwith. The availability of a small internal memory buffer can dramatically\nreduce the amortized insertion cost to $o(1)$ I/Os for many external memory\ndata structures. In this paper we study the inherent query-insertion tradeoff\nof external hash tables in the presence of a memory buffer. In particular, we\nshow that for any constant $c>1$, if the query cost is targeted at\n$1+O(1/b^{c})$ I/Os, then it is not possible to support insertions in less than\n$1-O(1/b^{\\frac{c-1}{4}})$ I/Os amortized, which means that the memory buffer\nis essentially useless. While if the query cost is relaxed to $1+O(1/b^{c})$\nI/Os for any constant $c<1$, there is a simple dynamic hash table with $o(1)$\ninsertion cost. These results also answer the open question recently posed by\nJensen and Pagh.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2008 08:11:14 GMT"}], "update_date": "2008-11-20", "authors_parsed": [["Wei", "Zhewei", ""], ["Yi", "Ke", ""], ["Zhang", "Qin", ""]]}, {"id": "0811.3244", "submitter": "Warren Schudy", "authors": "Marek Karpinski, Warren Schudy", "title": "Linear Time Approximation Schemes for the Gale-Berlekamp Game and\n  Related Minimization Problems", "comments": "18 pages LaTeX, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a linear time approximation scheme for the Gale-Berlekamp Switching\nGame and generalize it to a wider class of dense fragile minimization problems\nincluding the Nearest Codeword Problem (NCP) and Unique Games Problem. Further\napplications include, among other things, finding a constrained form of matrix\nrigidity and maximum likelihood decoding of an error correcting code. As\nanother application of our method we give the first linear time approximation\nschemes for correlation clustering with a fixed number of clusters and its\nhierarchical generalization. Our results depend on a new technique for dealing\nwith small objective function values of optimization problems and could be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2008 01:07:49 GMT"}], "update_date": "2008-11-21", "authors_parsed": [["Karpinski", "Marek", ""], ["Schudy", "Warren", ""]]}, {"id": "0811.3247", "submitter": "Marino Pagan", "authors": "Bruno Codenotti, Stefano De Rossi, Marino Pagan", "title": "An experimental analysis of Lemke-Howson algorithm", "comments": "15 pages, 18 figures. The source code of our implementation can be\n  found at http://allievi.sssup.it/game/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an experimental investigation of the performance of the\nLemke-Howson algorithm, which is the most widely used algorithm for the\ncomputation of a Nash equilibrium for bimatrix games. Lemke-Howson algorithm is\nbased upon a simple pivoting strategy, which corresponds to following a path\nwhose endpoint is a Nash equilibrium. We analyze both the basic Lemke-Howson\nalgorithm and a heuristic modification of it, which we designed to cope with\nthe effects of a 'bad' initial choice of the pivot. Our experimental findings\nshow that, on uniformly random games, the heuristics achieves a linear running\ntime, while the basic Lemke-Howson algorithm runs in time roughly proportional\nto a polynomial of degree seven. To conduct the experiments, we have developed\nour own implementation of Lemke-Howson algorithm, which turns out to be\nsignificantly faster than state-of-the-art software. This allowed us to run the\nalgorithm on a much larger set of data, and on instances of much larger size,\ncompared with previous work.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2008 00:32:16 GMT"}], "update_date": "2008-11-21", "authors_parsed": [["Codenotti", "Bruno", ""], ["De Rossi", "Stefano", ""], ["Pagan", "Marino", ""]]}, {"id": "0811.3448", "submitter": "William Gilreath", "authors": "William F. Gilreath", "title": "Binar Sort: A Linear Generalized Sorting Algorithm", "comments": "PDF from Word, 25-pages, 2-figures, 4-diagrams, version 2.0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting is a common and ubiquitous activity for computers. It is not\nsurprising that there exist a plethora of sorting algorithms. For all the\nsorting algorithms, it is an accepted performance limit that sorting algorithms\nare linearithmic or O(N lg N). The linearithmic lower bound in performance\nstems from the fact that the sorting algorithms use the ordering property of\nthe data. The sorting algorithm uses comparison by the ordering property to\narrange the data elements from an initial permutation into a sorted\npermutation.\n  Linear O(N) sorting algorithms exist, but use a priori knowledge of the data\nto use a specific property of the data and thus have greater performance. In\ncontrast, the linearithmic sorting algorithms are generalized by using a\nuniversal property of data-comparison, but have a linearithmic performance\nlower bound. The trade-off in sorting algorithms is generality for performance\nby the chosen property used to sort the data elements.\n  A general-purpose, linear sorting algorithm in the context of the trade-off\nof performance for generality at first consideration seems implausible. But,\nthere is an implicit assumption that only the ordering property is universal.\nBut, as will be discussed and examined, it is not the only universal property\nfor data elements. The binar sort is a general-purpose sorting algorithm that\nuses this other universal property to sort linearly.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2008 01:38:09 GMT"}, {"version": "v2", "created": "Tue, 17 May 2011 04:19:05 GMT"}], "update_date": "2011-05-18", "authors_parsed": [["Gilreath", "William F.", ""]]}, {"id": "0811.3449", "submitter": "William Gilreath", "authors": "William F. Gilreath", "title": "Binar Shuffle Algorithm: Shuffling Bit by Bit", "comments": "27-pages, watermarked", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequently, randomly organized data is needed to avoid an anomalous operation\nof other algorithms and computational processes. An analogy is that a deck of\ncards is ordered within the pack, but before a game of poker or solitaire the\ndeck is shuffled to create a random permutation. Shuffling is used to assure\nthat an aggregate of data elements for a sequence S is randomly arranged, but\navoids an ordered or partially ordered permutation.\n  Shuffling is the process of arranging data elements into a random\npermutation. The sequence S as an aggregation of N data elements, there are N!\npossible permutations. For the large number of possible permutations, two of\nthe possible permutations are for a sorted or ordered placement of data\nelements--both an ascending and descending sorted permutation. Shuffling must\navoid inadvertently creating either an ascending or descending permutation.\n  Shuffling is frequently coupled to another algorithmic function --\npseudo-random number generation. The efficiency and quality of the shuffle is\ndirectly dependent upon the random number generation algorithm utilized. A more\neffective and efficient method of shuffling is to use parameterization to\nconfigure the shuffle, and to shuffle into sub-arrays by utilizing the encoding\nof the data elements. The binar shuffle algorithm uses the encoding of the data\nelements and parameterization to avoid any direct coupling to a random number\ngeneration algorithm, but still remain a linear O(N) shuffle algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2008 01:45:50 GMT"}], "update_date": "2008-11-24", "authors_parsed": [["Gilreath", "William F.", ""]]}, {"id": "0811.3490", "submitter": "Philip Bille", "authors": "Philip Bille", "title": "Faster Approximate String Matching for Short Patterns", "comments": "To appear in Theory of Computing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classical approximate string matching problem, that is, given\nstrings $P$ and $Q$ and an error threshold $k$, find all ending positions of\nsubstrings of $Q$ whose edit distance to $P$ is at most $k$. Let $P$ and $Q$\nhave lengths $m$ and $n$, respectively. On a standard unit-cost word RAM with\nword size $w \\geq \\log n$ we present an algorithm using time $$ O(nk \\cdot\n\\min(\\frac{\\log^2 m}{\\log n},\\frac{\\log^2 m\\log w}{w}) + n) $$ When $P$ is\nshort, namely, $m = 2^{o(\\sqrt{\\log n})}$ or $m = 2^{o(\\sqrt{w/\\log w})}$ this\nimproves the previously best known time bounds for the problem. The result is\nachieved using a novel implementation of the Landau-Vishkin algorithm based on\ntabulation and word-level parallelism.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2008 08:52:59 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2011 21:11:16 GMT"}], "update_date": "2011-03-21", "authors_parsed": [["Bille", "Philip", ""]]}, {"id": "0811.3602", "submitter": "Yakov Nekrich", "authors": "Travis Gagie, Marek Karpinski, Yakov Nekrich", "title": "Low-Memory Adaptive Prefix Coding", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the adaptive prefix coding problem in cases where the\nsize of the input alphabet is large. We present an online prefix coding\nalgorithm that uses $O(\\sigma^{1 / \\lambda + \\epsilon}) $ bits of space for any\nconstants $\\eps>0$, $\\lambda>1$, and encodes the string of symbols in $O(\\log\n\\log \\sigma)$ time per symbol \\emph{in the worst case}, where $\\sigma$ is the\nsize of the alphabet. The upper bound on the encoding length is $\\lambda n H\n(s) +(\\lambda \\ln 2 + 2 + \\epsilon) n + O (\\sigma^{1 / \\lambda} \\log^2 \\sigma)$\nbits.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2008 18:23:00 GMT"}], "update_date": "2008-11-24", "authors_parsed": [["Gagie", "Travis", ""], ["Karpinski", "Marek", ""], ["Nekrich", "Yakov", ""]]}, {"id": "0811.3648", "submitter": "Jelani Nelson", "authors": "Daniel M. Kane, Jelani Nelson, David P. Woodruff", "title": "Revisiting Norm Estimation in Data Streams", "comments": "added content; modified L_0 algorithm -- ParityLogEstimator in\n  version 1 contained an error, and the new algorithm uses slightly more space", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating the pth moment F_p (p nonnegative and real) in data\nstreams is as follows. There is a vector x which starts at 0, and many updates\nof the form x_i <-- x_i + v come sequentially in a stream. The algorithm also\nreceives an error parameter 0 < eps < 1. The goal is then to output an\napproximation with relative error at most eps to F_p = ||x||_p^p.\n  Previously, it was known that polylogarithmic space (in the vector length n)\nwas achievable if and only if p <= 2. We make several new contributions in this\nregime, including:\n  (*) An optimal space algorithm for 0 < p < 2, which, unlike previous\nalgorithms which had optimal dependence on 1/eps but sub-optimal dependence on\nn, does not rely on a generic pseudorandom generator.\n  (*) A near-optimal space algorithm for p = 0 with optimal update and query\ntime.\n  (*) A near-optimal space algorithm for the \"distinct elements\" problem (p = 0\nand all updates have v = 1) with optimal update and query time.\n  (*) Improved L_2 --> L_2 dimensionality reduction in a stream.\n  (*) New 1-pass lower bounds to show optimality and near-optimality of our\nalgorithms, as well as of some previous algorithms (the \"AMS sketch\" for p = 2,\nand the L_1-difference algorithm of Feigenbaum et al.).\n  As corollaries of our work, we also obtain a few separations in the\ncomplexity of moment estimation problems: F_0 in 1 pass vs. 2 passes, p = 0 vs.\np > 0, and F_0 with strictly positive updates vs. arbitrary updates.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2008 22:55:07 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2009 02:45:30 GMT"}], "update_date": "2009-04-09", "authors_parsed": [["Kane", "Daniel M.", ""], ["Nelson", "Jelani", ""], ["Woodruff", "David P.", ""]]}, {"id": "0811.3723", "submitter": "Mingyu Xiao", "authors": "Mingyu Xiao, Leizhen Cai and Andrew C. Yao", "title": "Tight Approximation Ratio of a General Greedy Splitting Algorithm for\n  the Minimum k-Way Cut Problem", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an edge-weighted connected undirected graph, the minimum $k$-way cut\nproblem is to find a subset of edges of minimum total weight whose removal\nseparates the graph into $k$ connected components. The problem is NP-hard when\n$k$ is part of the input and W[1]-hard when $k$ is taken as a parameter.\n  A simple algorithm for approximating a minimum $k$-way cut is to iteratively\nincrease the number of components of the graph by $h-1$, where $2 \\le h \\le k$,\nuntil the graph has $k$ components. The approximation ratio of this algorithm\nis known for $h \\le 3$ but is open for $h \\ge 4$.\n  In this paper, we consider a general algorithm that iteratively increases the\nnumber of components of the graph by $h_i-1$, where $h_1 \\le h_2 \\le ... \\le\nh_q$ and $\\sum_{i=1}^q (h_i-1) = k-1$. We prove that the approximation ratio of\nthis general algorithm is $2 - (\\sum_{i=1}^q {h_i \\choose 2})/{k \\choose 2}$,\nwhich is tight. Our result implies that the approximation ratio of the simple\nalgorithm is $2-h/k + O(h^2/k^2)$ in general and $2-h/k$ if $k-1$ is a multiple\nof $h-1$.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2008 03:47:50 GMT"}], "update_date": "2008-11-25", "authors_parsed": [["Xiao", "Mingyu", ""], ["Cai", "Leizhen", ""], ["Yao", "Andrew C.", ""]]}, {"id": "0811.3760", "submitter": "Sebastien Tixeuil", "authors": "St\\'ephane Devismes, Toshimitsu Masuzawa, S\\'ebastien Tixeuil (LIP6)", "title": "Communication Efficiency in Self-stabilizing Silent Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": "RR-6731", "categories": "cs.DS cs.CC cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-stabilization is a general paradigm to provide forward recovery\ncapabilities to distributed systems and networks. Intuitively, a protocol is\nself-stabilizing if it is able to recover without external intervention from\nany catastrophic transient failure. In this paper, our focus is to lower the\ncommunication complexity of self-stabilizing protocols \\emph{below} the need of\nchecking every neighbor forever. In more details, the contribution of the paper\nis threefold: (i) We provide new complexity measures for communication\nefficiency of self-stabilizing protocols, especially in the stabilized phase or\nwhen there are no faults, (ii) On the negative side, we show that for\nnon-trivial problems such as coloring, maximal matching, and maximal\nindependent set, it is impossible to get (deterministic or probabilistic)\nself-stabilizing solutions where every participant communicates with less than\nevery neighbor in the stabilized phase, and (iii) On the positive side, we\npresent protocols for coloring, maximal matching, and maximal independent set\nsuch that a fraction of the participants communicates with exactly one neighbor\nin the stabilized phase.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2008 17:29:25 GMT"}], "update_date": "2008-11-25", "authors_parsed": [["Devismes", "St\u00e9phane", "", "LIP6"], ["Masuzawa", "Toshimitsu", "", "LIP6"], ["Tixeuil", "S\u00e9bastien", "", "LIP6"]]}, {"id": "0811.3779", "submitter": "Reid Andersen", "authors": "Reid Andersen and Yuval Peres", "title": "Finding Sparse Cuts Locally Using Evolving Sets", "comments": "20 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A {\\em local graph partitioning algorithm} finds a set of vertices with small\nconductance (i.e. a sparse cut) by adaptively exploring part of a large graph\n$G$, starting from a specified vertex. For the algorithm to be local, its\ncomplexity must be bounded in terms of the size of the set that it outputs,\nwith at most a weak dependence on the number $n$ of vertices in $G$. Previous\nlocal partitioning algorithms find sparse cuts using random walks and\npersonalized PageRank. In this paper, we introduce a randomized local\npartitioning algorithm that finds a sparse cut by simulating the {\\em\nvolume-biased evolving set process}, which is a Markov chain on sets of\nvertices. We prove that for any set of vertices $A$ that has conductance at\nmost $\\phi$, for at least half of the starting vertices in $A$ our algorithm\nwill output (with probability at least half), a set of conductance\n$O(\\phi^{1/2} \\log^{1/2} n)$. We prove that for a given run of the algorithm,\nthe expected ratio between its computational complexity and the volume of the\nset that it outputs is $O(\\phi^{-1/2} polylog(n))$. In comparison, the best\nprevious local partitioning algorithm, due to Andersen, Chung, and Lang, has\nthe same approximation guarantee, but a larger ratio of $O(\\phi^{-1}\npolylog(n))$ between the complexity and output volume. Using our local\npartitioning algorithm as a subroutine, we construct a fast algorithm for\nfinding balanced cuts. Given a fixed value of $\\phi$, the resulting algorithm\nhas complexity $O((m+n\\phi^{-1/2}) polylog(n))$ and returns a cut with\nconductance $O(\\phi^{1/2} \\log^{1/2} n)$ and volume at least $v_{\\phi}/2$,\nwhere $v_{\\phi}$ is the largest volume of any set with conductance at most\n$\\phi$.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2008 22:39:38 GMT"}], "update_date": "2008-11-25", "authors_parsed": [["Andersen", "Reid", ""], ["Peres", "Yuval", ""]]}, {"id": "0811.4007", "submitter": "Krishnam Raju Jampani", "authors": "Krishnam Raju Jampani and Anna Lubiw", "title": "The Simultaneous Membership Problem for Chordal, Comparability and\n  Permutation graphs", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the 'simultaneous membership problem', defined for\nany graph class C characterized in terms of representations, e.g. any class of\nintersection graphs. Two graphs G_1 and G_2, sharing some vertices X (and the\ncorresponding induced edges), are said to be 'simultaneous members' of graph\nclass C, if there exist representations R_1 and R_2 of G_1 and G_2 that are\n\"consistent\" on X. Equivalently (for the classes C that we consider) there\nexist edges E' between G_1-X and G_2-X such that G_1 \\cup G_2 \\cup E' belongs\nto class C.\n  Simultaneous membership problems have application in any situation where it\nis desirable to consistently represent two related graphs, for example:\ninterval graphs capturing overlaps of DNA fragments of two similar organisms;\nor graphs connected in time, where one is an updated version of the other.\nSimultaneous membership problems are related to simultaneous planar embeddings,\ngraph sandwich problems and probe graph recognition problems.\n  In this paper we give efficient algorithms for the simultaneous membership\nproblem on chordal, comparability and permutation graphs. These results imply\nthat graph sandwich problems for the above classes are tractable for an\ninteresting special case: when the set of optional edges form a complete\nbipartite graph. Our results complement the recent polynomial time recognition\nalgorithms for probe chordal, comparability, and permutation graphs, where the\nset of optional edges form a clique.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2008 02:54:32 GMT"}], "update_date": "2008-11-26", "authors_parsed": [["Jampani", "Krishnam Raju", ""], ["Lubiw", "Anna", ""]]}, {"id": "0811.4186", "submitter": "Aleksandar Bradic M", "authors": "Aleksandar Bradic", "title": "Search Result Clustering via Randomized Partitioning of Query-Induced\n  Subgraphs", "comments": "16th Telecommunications Forum TELFOR 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an approach to search result clustering, using\npartitioning of underlying link graph. We define the notion of \"query-induced\nsubgraph\" and formulate the problem of search result clustering as a problem of\nefficient partitioning of given subgraph into topic-related clusters. Also, we\npropose a novel algorithm for approximative partitioning of such graph, which\nresults in cluster quality comparable to the one obtained by deterministic\nalgorithms, while operating in more efficient computation time, suitable for\npractical implementations. Finally, we present a practical clustering search\nengine developed as a part of this research and use it to get results about\nreal-world performance of proposed concepts.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2008 23:11:55 GMT"}], "update_date": "2008-11-27", "authors_parsed": [["Bradic", "Aleksandar", ""]]}, {"id": "0811.4346", "submitter": "Ke Yi", "authors": "Ke Yi", "title": "Dynamic Indexability: The Query-Update Tradeoff for One-Dimensional\n  Range Queries", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The B-tree is a fundamental secondary index structure that is widely used for\nanswering one-dimensional range reporting queries. Given a set of $N$ keys, a\nrange query can be answered in $O(\\log_B \\nm + \\frac{K}{B})$ I/Os, where $B$ is\nthe disk block size, $K$ the output size, and $M$ the size of the main memory\nbuffer. When keys are inserted or deleted, the B-tree is updated in $O(\\log_B\nN)$ I/Os, if we require the resulting changes to be committed to disk right\naway. Otherwise, the memory buffer can be used to buffer the recent updates,\nand changes can be written to disk in batches, which significantly lowers the\namortized update cost. A systematic way of batching up updates is to use the\nlogarithmic method, combined with fractional cascading, resulting in a dynamic\nB-tree that supports insertions in $O(\\frac{1}{B}\\log\\nm)$ I/Os and queries in\n$O(\\log\\nm + \\frac{K}{B})$ I/Os. Such bounds have also been matched by several\nknown dynamic B-tree variants in the database literature.\n  In this paper, we prove that for any dynamic one-dimensional range query\nindex structure with query cost $O(q+\\frac{K}{B})$ and amortized insertion cost\n$O(u/B)$, the tradeoff $q\\cdot \\log(u/q) = \\Omega(\\log B)$ must hold if\n$q=O(\\log B)$. For most reasonable values of the parameters, we have $\\nm =\nB^{O(1)}$, in which case our query-insertion tradeoff implies that the bounds\nmentioned above are already optimal. Our lower bounds hold in a dynamic version\nof the {\\em indexability model}, which is of independent interests.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2008 15:36:14 GMT"}], "update_date": "2008-11-27", "authors_parsed": [["Yi", "Ke", ""]]}, {"id": "0811.4376", "submitter": "Soubhik Chakraborty", "authors": "Suman Kumar Sourabh and Soubhik Chakraborty", "title": "How robust is quicksort average complexity?", "comments": "15 pages;12figures;2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper questions the robustness of average case time complexity of the\nfast and popular quicksort algorithm. Among the six standard probability\ndistributions examined in the paper, only continuous uniform, exponential and\nstandard normal are supporting it whereas the others are supporting the worst\ncase complexity measure. To the question -why are we getting the worst case\ncomplexity measure each time the average case measure is discredited? -- one\nlogical answer is average case complexity under the universal distribution\nequals worst case complexity. This answer, which is hard to challenge, however\ngives no idea as to which of the standard probability distributions come under\nthe umbrella of universality. The morale is that average case complexity\nmeasures, in cases where they are different from those in worst case, should be\ndeemed as robust provided only they get the support from at least the standard\nprobability distributions, both discrete and continuous. Regretfully, this is\nnot the case with quicksort.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2008 17:23:22 GMT"}], "update_date": "2016-11-27", "authors_parsed": [["Sourabh", "Suman Kumar", ""], ["Chakraborty", "Soubhik", ""]]}, {"id": "0811.4672", "submitter": "Kui Wu", "authors": "Emad Soroush, Kui Wu, Jian Pei", "title": "Fast and Quality-Guaranteed Data Streaming in Resource-Constrained\n  Sensor Networks", "comments": "Published in ACM MobiHoc 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many emerging applications, data streams are monitored in a network\nenvironment. Due to limited communication bandwidth and other resource\nconstraints, a critical and practical demand is to online compress data streams\ncontinuously with quality guarantee. Although many data compression and digital\nsignal processing methods have been developed to reduce data volume, their\nsuper-linear time and more-than-constant space complexity prevents them from\nbeing applied directly on data streams, particularly over resource-constrained\nsensor networks. In this paper, we tackle the problem of online quality\nguaranteed compression of data streams using fast linear approximation (i.e.,\nusing line segments to approximate a time series). Technically, we address two\nversions of the problem which explore quality guarantees in different forms. We\ndevelop online algorithms with linear time complexity and constant cost in\nspace. Our algorithms are optimal in the sense they generate the minimum number\nof segments that approximate a time series with the required quality guarantee.\nTo meet the resource constraints in sensor networks, we also develop a fast\nalgorithm which creates connecting segments with very simple computation. The\nlow cost nature of our methods leads to a unique edge on the applications of\nmassive and fast streaming environment, low bandwidth networks, and heavily\nconstrained nodes in computational power. We implement and evaluate our methods\nin the application of an acoustic wireless sensor network.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2008 20:59:55 GMT"}], "update_date": "2008-12-01", "authors_parsed": [["Soroush", "Emad", ""], ["Wu", "Kui", ""], ["Pei", "Jian", ""]]}, {"id": "0811.4713", "submitter": "Mamadou Moustapha Kant\\'e", "authors": "Bruno Courcelle (LaBRI, IUF), Cyril Gavoille (LaBRI, INRIA Futurs),\n  Mamadou Moustapha Kant\\'e (LaBRI)", "title": "Compact Labelings For Efficient First-Order Model-Checking", "comments": null, "journal-ref": "Journal of Combinatorial Optimisation 21(1):19-46(2011)", "doi": "10.1007/s10878-009-9260-7", "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider graph properties that can be checked from labels, i.e., bit\nsequences, of logarithmic length attached to vertices. We prove that there\nexists such a labeling for checking a first-order formula with free set\nvariables in the graphs of every class that is \\emph{nicely locally\ncwd-decomposable}. This notion generalizes that of a \\emph{nicely locally\ntree-decomposable} class. The graphs of such classes can be covered by graphs\nof bounded \\emph{clique-width} with limited overlaps. We also consider such\nlabelings for \\emph{bounded} first-order formulas on graph classes of\n\\emph{bounded expansion}. Some of these results are extended to counting\nqueries.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2008 13:29:15 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 11:29:27 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Courcelle", "Bruno", "", "LaBRI, IUF"], ["Gavoille", "Cyril", "", "LaBRI, INRIA Futurs"], ["Kant\u00e9", "Mamadou Moustapha", "", "LaBRI"]]}]