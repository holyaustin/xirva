[{"id": "0909.0095", "submitter": "Rakesh Mohanty", "authors": "Rakesh Mohanty and N. S. Narayanaswamy", "title": "Online Algorithms for Self-Organizing Sequential Search - A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main objective of this survey is to present the important theoretical and\nexperimental results contributed till date in the area of online algorithms for\nthe self organizing sequential search problem, also popularly known as the List\nUpdate Problem(LUP) in a chronological way. The survey includes competitiveness\nresults of deterministic and randomized online algorithms and complexity\nresults of optimal off line algorithms for the list update problem. We also\npresent the results associated with list update with look ahead, list update\nwith locality of reference and other variants of the list update problem. We\ninvestigate research issues, explore scope of future work associated with each\nissue so that future researchers can find it useful to work on.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2009 05:46:32 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2009 03:48:38 GMT"}], "update_date": "2009-09-02", "authors_parsed": [["Mohanty", "Rakesh", ""], ["Narayanaswamy", "N. S.", ""]]}, {"id": "0909.0097", "submitter": "R Doomun", "authors": "Kamanashis Biswas, S. A. M. Harun", "title": "Constraint Minimum Vertex Cover in K Partite Graph, Approximation\n  Algorithm and Complexity Analysis", "comments": "5 Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact factor 0.423,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 4, No. 1 & 2, August 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generally, a graph G, an independent set is a subset S of vertices in G such\nthat no two vertices in S are adjacent (connected by an edge) and a vertex\ncover is a subset S of vertices such that each edge of G has at least one of\nits endpoints in S. Again, the minimum vertex cover problem is to find a vertex\ncover with the smallest number of vertices. This study shows that the\nconstrained minimum vertex cover problem in k-partite graph (MIN CVCK) is\nNP-Complete which is an important property of k partite graph. Many\ncombinatorial problems on general graphs are NP-complete, but when restricted\nto k partite graph with at most k vertices then many of these problems can be\nsolved in polynomial time. This paper also illustrates an approximation\nalgorithm for MIN CVCK and analyzes its complexity. In future work section, we\nspecified a number of dimensions which may be interesting for the researchers\nsuch as developing algorithm for maximum matching and polynomial algorithm for\nconstructing k-partite graph from general graph.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2009 06:10:44 GMT"}], "update_date": "2009-09-02", "authors_parsed": [["Biswas", "Kamanashis", ""], ["Harun", "S. A. M.", ""]]}, {"id": "0909.0814", "submitter": "Otfried Cheong", "authors": "Binay Bhattacharya, Arijit Bishnu, Otfried Cheong, Sandip Das, Arindam\n  Karmakar and Jack Snoeyink", "title": "Computation of Spatial Skyline Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a method of finding skyline or non-dominated sites in a set $P$ of\n$n$ point sites with respect to a set $S$ of $m$ points. A site $p \\in P$ is\nnon-dominated if and only if for each $q \\in P \\setminus \\{p\\}$, there exists\nat least one point $s \\in S$ that is closer to $p$ than to $q$. We reduce this\nproblem of determining non-dominated sites to the problem of finding sites that\nhave non-empty cells in an additively weighted Voronoi diagram under a convex\ndistance function. The weights of said Voronoi diagram are derived from the\ncoordinates of the sites of $P$, while the convex distance function is derived\nfrom $S$. In the two-dimensional plane, this reduction gives an $O((n + m) \\log\n(n + m))$-time algorithm to find the non-dominated points.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2009 05:31:53 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 12:31:38 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Bhattacharya", "Binay", ""], ["Bishnu", "Arijit", ""], ["Cheong", "Otfried", ""], ["Das", "Sandip", ""], ["Karmakar", "Arindam", ""], ["Snoeyink", "Jack", ""]]}, {"id": "0909.0941", "submitter": "Nicholas Harvey", "authors": "Michel X. Goemans, Nicholas J. A. Harvey, Kamal Jain, Mohit Singh", "title": "A Randomized Rounding Algorithm for the Asymmetric Traveling Salesman\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for the asymmetric traveling salesman problem on\ninstances which satisfy the triangle inequality. Like several existing\nalgorithms, it achieves approximation ratio O(log n). Unlike previous\nalgorithms, it uses randomized rounding.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2009 19:49:22 GMT"}], "update_date": "2009-09-07", "authors_parsed": [["Goemans", "Michel X.", ""], ["Harvey", "Nicholas J. A.", ""], ["Jain", "Kamal", ""], ["Singh", "Mohit", ""]]}, {"id": "0909.1030", "submitter": "Amir Ben-Amram", "authors": "Amir M. Ben-Amram", "title": "The Euler Path to Static Level-Ancestors", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that a rooted tree T is given for preprocessing. The Level-Ancestor\nProblem is to answer quickly queries of the following form. Given a vertex v\nand an integer i > 0, find the i-th vertex on the path from the root to v.\nAlgorithms that achieve a linear time bound for preprocessing and a constant\ntime bound for a query have been published by Dietz (1991), Alstrup and Holm\n(2000), and Bender and Farach (2002). The first two algorithms address dynamic\nversions of the problem; the last addresses the static version only and is the\nsimplest so far. The purpose of this note is to expose another simple\nalgorithm, derived from a complicated PRAM algorithm by Berkman and Vishkin\n(1990,1994). We further show some easy extensions of its functionality, adding\nqueries for descendants and level successors as well as ancestors, extensions\nfor which the formerly known algorithms are less suitable.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2009 14:15:29 GMT"}], "update_date": "2009-09-08", "authors_parsed": [["Ben-Amram", "Amir M.", ""]]}, {"id": "0909.1037", "submitter": "Michael Goodrich", "authors": "Michael T. Goodrich", "title": "Randomized Shellsort: A Simple Oblivious Sorting Algorithm", "comments": "Version of paper accepted to 2010 ACM-SIAM Symposium on Discrete\n  Algorithms (SODA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe randomized Shellsort--a simple, randomized,\ndata-oblivious version of the Shellsort algorithm that always runs in O(n log\nn) time and, as we show, succeeds in sorting any given input permutation with\nvery high probability. Thus, randomized Shellsort is simultaneously simple,\ntime-optimal, and data-oblivious. Taken together, these properties imply\napplications in the design of new efficient privacy-preserving computations\nbased on the secure multi-party computation (SMC) paradigm. In addition, by a\ntrivial conversion of this Monte Carlo algorithm to its Las Vegas equivalent,\none gets the first version of Shellsort with a running time that is provably\nO(n log n) with very high probability.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2009 15:40:42 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2009 00:37:15 GMT"}, {"version": "v3", "created": "Wed, 4 Aug 2010 18:16:47 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Goodrich", "Michael T.", ""]]}, {"id": "0909.1051", "submitter": "Gleb Oshanin", "authors": "G. Kabatiansky (1,2), G.Oshanin (2,3) ((1) Dobrushin Mathematical\n  Laboratory, Institute of Information Transmission Problems, Moscow, Russia,\n  (2) Laboratory J.-V. Poncelet, Independent University of Moscow, Russia, (3)\n  LPTMC, University Pierre & Marie Curie, Paris, France)", "title": "Finding passwords by random walks: How long does it take?", "comments": "To appear in J. Phys. A, special issue on \"Random Search Problem:\n  Trends and Perspectives\", eds.: MEG da Luz, E Raposo, GM Viswanathan and A\n  Grosberg", "journal-ref": "J. Phys. A 42 No 43, 434016 (2009)", "doi": "10.1088/1751-8113/42/43/434016", "report-no": null, "categories": "cs.CR cond-mat.other cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare an efficiency of a deterministic \"lawnmower\" and random search\nstrategies for finding a prescribed sequence of letters (a password) of length\nM in which all letters are taken from the same Q-ary alphabet. We show that at\nbest a random search takes two times longer than a \"lawnmower\" search.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2009 19:25:35 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Kabatiansky", "G.", ""], ["Oshanin", "G.", ""]]}, {"id": "0909.1062", "submitter": "Ankan Saha", "authors": "Ankan Saha (1), S.V.N. Vishwanathan (2), Xinhua Zhang (3) ((1)\n  University of Chicago, (2) Purdue University, (3) University of Alberta)", "title": "New Approximation Algorithms for Minimum Enclosing Convex Shapes", "comments": "18 Pages Accepted in SODA 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $n$ points in a $d$ dimensional Euclidean space, the Minimum Enclosing\nBall (MEB) problem is to find the ball with the smallest radius which contains\nall $n$ points. We give a $O(nd\\Qcal/\\sqrt{\\epsilon})$ approximation algorithm\nfor producing an enclosing ball whose radius is at most $\\epsilon$ away from\nthe optimum (where $\\Qcal$ is an upper bound on the norm of the points). This\nimproves existing results using \\emph{coresets}, which yield a $O(nd/\\epsilon)$\ngreedy algorithm. Finding the Minimum Enclosing Convex Polytope (MECP) is a\nrelated problem wherein a convex polytope of a fixed shape is given and the aim\nis to find the smallest magnification of the polytope which encloses the given\npoints. For this problem we present a $O(mnd\\Qcal/\\epsilon)$ approximation\nalgorithm, where $m$ is the number of faces of the polytope. Our algorithms\nborrow heavily from convex duality and recently developed techniques in\nnon-smooth optimization, and are in contrast with existing methods which rely\non geometric arguments. In particular, we specialize the excessive gap\nframework of \\citet{Nesterov05a} to obtain our results.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2009 23:24:32 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2009 16:01:53 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2009 14:30:54 GMT"}, {"version": "v4", "created": "Wed, 15 Sep 2010 00:54:09 GMT"}], "update_date": "2010-09-16", "authors_parsed": [["Saha", "Ankan", ""], ["Vishwanathan", "S. V. N.", ""], ["Zhang", "Xinhua", ""]]}, {"id": "0909.1334", "submitter": "Ankan Saha", "authors": "Ankan Saha (1), Xinhua Zhang (2), S.V.N. Vishwanathan (3) ((1)\n  University of Chicago, (2) Australian National University, NICTA, (3) Purdue\n  University)", "title": "Lower Bounds for BMRM and Faster Rates for Training SVMs", "comments": "21 pages, 49 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized risk minimization with the binary hinge loss and its variants\nlies at the heart of many machine learning problems. Bundle methods for\nregularized risk minimization (BMRM) and the closely related SVMStruct are\nconsidered the best general purpose solvers to tackle this problem. It was\nrecently shown that BMRM requires $O(1/\\epsilon)$ iterations to converge to an\n$\\epsilon$ accurate solution. In the first part of the paper we use the\nHadamard matrix to construct a regularized risk minimization problem and show\nthat these rates cannot be improved. We then show how one can exploit the\nstructure of the objective function to devise an algorithm for the binary hinge\nloss which converges to an $\\epsilon$ accurate solution in\n$O(1/\\sqrt{\\epsilon})$ iterations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2009 20:58:47 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2009 22:00:12 GMT"}], "update_date": "2009-09-09", "authors_parsed": [["Saha", "Ankan", ""], ["Zhang", "Xinhua", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "0909.1372", "submitter": "R Doomun", "authors": "Mohammed M. Kadhum, and Suhaidi Hassan", "title": "The Uniformization Process of the Fast Congestion Notification (FN)", "comments": "5 Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS 2009, ISSN 1947 5500,Impact Factor 0.423,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 4, No. 1 & 2, August 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast Congestion Notification (FN) is one of the proactive queue management\nmechanisms that practices congestion avoidance to help avoid the beginning of\ncongestion by marking or dropping packets before the routers queue gets full;\nand exercises congestion control, when congestion avoidance fails, by\nincreasing the rate of packet marking or dropping. Technically, FN avoids the\nqueue overflows by controlling the instantaneous queue size below the optimal\nqueue size, and control congestion by keeping the average arrival rate close to\nthe outgoing link capacity. Upon arrival of each packet, FN uses the\ninstantaneous queue size and the average arrival rate to calculate the packet\nmarking or dropping probability. FN marks or drops packets at fairly regular\nintervals to avoid long intermarking intervals and clustered packet marks or\ndrops. Too many marked or dropped packets close together can cause global\nsynchronization, and also too long packet intermarking times between marked or\ndropped packets can cause large queue sizes and congestion. This paper shows\nhow FN controls the queue size, avoids congestion, and reduces global\nsynchronization by uniformizing marked or dropped packet intervals.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2009 02:14:31 GMT"}], "update_date": "2009-09-09", "authors_parsed": [["Kadhum", "Mohammed M.", ""], ["Hassan", "Suhaidi", ""]]}, {"id": "0909.1374", "submitter": "R Doomun", "authors": "Jean Pierre Jung, Ibrahima Sakho", "title": "On The Optimality Of All To All Broadcast In k ary n dimensional Tori", "comments": "8 Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS 2009, ISSN 1947 5500,Impact Factor 0.423,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 4, No. 1 & 2, August 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All to all broadcast is a collective communication in a network with the\nconstraint that every node must send to each other certain piece of its data.\nThis paper addresses the problem of optimal all port all to all broadcast in\nmultidimensional tori. The optimality criteria considered are the minimum\nexchange steps, no duplicated data in the sense that only new data are conveyed\nto receivers and the balance of the communication links load. It is proved that\nunder these constraints, an optimal broadcast is not feasible in any\nmultidimensional torus. Then, the tori which are capable of optimal broadcasts\nare characterized.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2009 02:43:24 GMT"}], "update_date": "2009-09-09", "authors_parsed": [["Jung", "Jean Pierre", ""], ["Sakho", "Ibrahima", ""]]}, {"id": "0909.1552", "submitter": "Adrian Dumitrescu", "authors": "Adrian Dumitrescu and J\\'anos Pach", "title": "Minimum clique partition in unit disk graphs", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum clique partition (MCP) problem is that of partitioning the vertex\nset of a given graph into a minimum number of cliques. Given $n$ points in the\nplane, the corresponding unit disk graph (UDG) has these points as vertices,\nand edges connecting points at distance at most~1. MCP in unit disk graphs is\nknown to be NP-hard and several constant factor approximations are known,\nincluding a recent PTAS. We present two improved approximation algorithms for\nminimum clique partition in unit disk graphs:\n  (I) A polynomial time approximation scheme (PTAS) running in time\n$n^{O(1/\\eps^2)}$. This improves on a previous PTAS with $n^{O(1/\\eps^4)}$\nrunning time \\cite{PS09}.\n  (II) A randomized quadratic-time algorithm with approximation ratio 2.16.\nThis improves on a ratio 3 algorithm with $O(n^2)$ running time \\cite{CFFP04}.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2009 20:47:19 GMT"}], "update_date": "2009-09-10", "authors_parsed": [["Dumitrescu", "Adrian", ""], ["Pach", "J\u00e1nos", ""]]}, {"id": "0909.1758", "submitter": "Nicolas Bruno", "authors": "Nicolas Bruno (Microsoft)", "title": "Teaching an Old Elephant New Tricks", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In recent years, column stores (or C-stores for short) have emerged as a\nnovel approach to deal with read-mostly data warehousing applications.\nExperimental evidence suggests that, for certain types of queries, the new\nfeatures of C-stores result in orders of magnitude improvement over traditional\nrelational engines. At the same time, some C-store proponents argue that\nC-stores are fundamentally different from traditional engines, and therefore\ntheir benefits cannot be incorporated into a relational engine short of a\ncomplete rewrite. In this paper we challenge this claim and show that many of\nthe benefits of C-stores can indeed be simulated in traditional engines with no\nchanges whatsoever. We then identify some limitations of our ?pure-simulation?\napproach for the case of more complex queries. Finally, we predict that\ntraditional relational engines will eventually leverage most of the benefits of\nC-stores natively, as is currently happening in other domains such as XML data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:00:21 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Bruno", "Nicolas", "", "Microsoft"]]}, {"id": "0909.1866", "submitter": "David Eppstein", "authors": "David Eppstein", "title": "Optimally fast incremental Manhattan plane embedding and planar tight\n  span construction", "comments": "39 pages, 15 figures", "journal-ref": "Journal of Computational Geometry 2(1):144-182, 2011", "doi": "10.20382/jocg.v2i1a8", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a data structure, a rectangular complex, that can be used to\nrepresent hyperconvex metric spaces that have the same topology (although not\nnecessarily the same distance function) as subsets of the plane. We show how to\nuse this data structure to construct the tight span of a metric space given as\nan n x n distance matrix, when the tight span is homeomorphic to a subset of\nthe plane, in time O(n^2), and to add a single point to a planar tight span in\ntime O(n). As an application of this construction, we show how to test whether\na given finite metric space embeds isometrically into the Manhattan plane in\ntime O(n^2), and add a single point to the space and re-test whether it has\nsuch an embedding in time O(n).\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2009 03:26:48 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2011 00:30:42 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Eppstein", "David", ""]]}, {"id": "0909.1870", "submitter": "David Eppstein", "authors": "David Eppstein", "title": "Paired approximation problems and incompatible inapproximabilities", "comments": "13 pages, 3 figures. To appear at 21st ACM-SIAM Symp. Discrete\n  Algorithms (SODA 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers pairs of optimization problems that are defined from a\nsingle input and for which it is desired to find a good approximation to either\none of the problems. In many instances, it is possible to efficiently find an\napproximation of this type that is better than known inapproximability lower\nbounds for either of the two individual optimization problems forming the pair.\nIn particular, we find either a $(1+\\epsilon)$-approximation to $(1,2)$-TSP or\na $1/\\epsilon$-approximation to maximum independent set, from a given graph, in\nlinear time. We show a similar paired approximation result for finding either a\ncoloring or a long path. However, no such tradeoff exists in some other cases:\nfor set cover and hitting set problems defined from a single set family, and\nfor clique and independent set problems on the same graph, it is not possible\nto find an approximation when both problems are combined that is better than\nthe best approximation for either problem on its own.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2009 05:23:43 GMT"}], "update_date": "2009-09-11", "authors_parsed": [["Eppstein", "David", ""]]}, {"id": "0909.2000", "submitter": "Peter Krusche", "authors": "Peter Krusche, Alexander Tiskin", "title": "Computing alignment plots efficiently", "comments": "Presented at ParCo 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dot plots are a standard method for local comparison of biological sequences.\nIn a dot plot, a substring to substring distance is computed for all pairs of\nfixed-size windows in the input strings. Commonly, the Hamming distance is used\nsince it can be computed in linear time. However, the Hamming distance is a\nrather crude measure of string similarity, and using an alignment-based edit\ndistance can greatly improve the sensitivity of the dot plot method. In this\npaper, we show how to compute alignment plots of the latter type efficiently.\nGiven two strings of length m and n and a window size w, this problem consists\nin computing the edit distance between all pairs of substrings of length w, one\nfrom each input string. The problem can be solved by repeated application of\nthe standard dynamic programming algorithm in time O(mnw^2). This paper gives\nan improved data-parallel algorithm, running in time $O(mnw/\\gamma/p)$ using\nvector operations that work on $\\gamma$ values in parallel and $p$ processors.\nWe show experimental results from an implementation of this algorithm, which\nuses Intel's MMX/SSE instructions for vector parallelism and MPI for\ncoarse-grained parallelism.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2009 17:40:15 GMT"}], "update_date": "2009-09-11", "authors_parsed": [["Krusche", "Peter", ""], ["Tiskin", "Alexander", ""]]}, {"id": "0909.2005", "submitter": "Ofer Zeitouni", "authors": "Uriel Feige, Ofer Zeitouni", "title": "Deterministic approximation for the cover time of trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic algorithm that given a tree T with n vertices, a\nstarting vertex v and a slackness parameter epsilon > 0, estimates within an\nadditive error of epsilon the cover and return time, namely, the expected time\nit takes a simple random walk that starts at v to visit all vertices of T and\nreturn to v. The running time of our algorithm is polynomial in n/epsilon, and\nhence remains polynomial in n also for epsilon = 1/n^{O(1)}. We also show how\nthe algorithm can be extended to estimate the expected cover (without return)\ntime on trees.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2009 17:59:36 GMT"}], "update_date": "2009-09-11", "authors_parsed": [["Feige", "Uriel", ""], ["Zeitouni", "Ofer", ""]]}, {"id": "0909.2030", "submitter": "Gregory Valiant", "authors": "Gregory Valiant and Paul Valiant", "title": "Size Bounds for Conjunctive Queries with General Functional Dependencies", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the work of Gottlob, Lee, and Valiant (PODS 2009)[GLV],\nand considers worst-case bounds for the size of the result Q(D) of a\nconjunctive query Q to a database D given an arbitrary set of functional\ndependencies. The bounds in [GLV] are based on a \"coloring\" of the query\nvariables. In order to extend the previous bounds to the setting of arbitrary\nfunctional dependencies, we leverage tools from information theory to formalize\nthe original intuition that each color used represents some possible entropy of\nthat variable, and bound the maximum possible size increase via a linear\nprogram that seeks to maximize how much more entropy is in the result of the\nquery than the input. This new view allows us to precisely characterize the\nentropy structure of worst-case instances for conjunctive queries with simple\nfunctional dependencies (keys), providing new insights into the results of\n[GLV]. We extend these results to the case of general functional dependencies,\nproviding upper and lower bounds on the worst-case size increase. We identify\nthe fundamental connection between the gap in these bounds and a central open\nquestion in information theory.\n  Finally, we show that, while both the upper and lower bounds are given by\nexponentially large linear programs, one can distinguish in polynomial time\nwhether the result of a query with an arbitrary set of functional dependencies\ncan be any larger than the input database.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2009 20:28:20 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2009 08:00:21 GMT"}], "update_date": "2009-12-12", "authors_parsed": [["Valiant", "Gregory", ""], ["Valiant", "Paul", ""]]}, {"id": "0909.2194", "submitter": "Dominique Tschopp", "authors": "Dominique Tschopp, Suhas Diggavi", "title": "Approximate Nearest Neighbor Search through Comparisons", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of finding the nearest neighbor (or one of\nthe R-nearest neighbors) of a query object q in a database of n objects. In\ncontrast with most existing approaches, we can only access the ``hidden'' space\nin which the objects live through a similarity oracle. The oracle, given two\nreference objects and a query object, returns the reference object closest to\nthe query object. The oracle attempts to model the behavior of human users,\ncapable of making statements about similarity, but not of assigning meaningful\nnumerical values to distances between objects.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2009 15:32:03 GMT"}], "update_date": "2009-09-14", "authors_parsed": [["Tschopp", "Dominique", ""], ["Diggavi", "Suhas", ""]]}, {"id": "0909.2331", "submitter": "Jerome Kelleher", "authors": "Jerome Kelleher and Barry O'Sullivan", "title": "Generating All Partitions: A Comparison Of Two Encodings", "comments": "40 pages, 2 figures. Derived from J. Kelleher's Ph.D thesis, Encoding\n  Partitions as Ascending Compositions, University College Cork, 2006. V2:\n  corrected typos in RuleDesc algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integer partitions may be encoded as either ascending or descending\ncompositions for the purposes of systematic generation. Many algorithms exist\nto generate all descending compositions, yet none have previously been\npublished to generate all ascending compositions. We develop three new\nalgorithms to generate all ascending compositions and compare these with\ndescending composition generators from the literature. We analyse the new\nalgorithms and provide new and more precise analyses for the descending\ncomposition generators. In each case, the ascending composition generation\nalgorithm is substantially more efficient than its descending composition\ncounterpart. We develop a new formula for the partition function p(n) as part\nof our analysis of the lexicographic succession rule for ascending\ncompositions.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2009 13:29:56 GMT"}, {"version": "v2", "created": "Fri, 2 May 2014 09:02:35 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Kelleher", "Jerome", ""], ["O'Sullivan", "Barry", ""]]}, {"id": "0909.2504", "submitter": "Dominique Tschopp", "authors": "Dominique Tschopp, Suhas Diggavi, Matthias Grossglauser", "title": "Hierarchical Routing over Dynamic Wireless Networks", "comments": "29 pages, 19 figures, a shorter version was published in the\n  proceedings of the 2008 ACM Sigmetrics conference", "journal-ref": null, "doi": null, "report-no": "LICOS-REPORT-2007-005", "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless network topologies change over time and maintaining routes requires\nfrequent updates. Updates are costly in terms of consuming throughput available\nfor data transmission, which is precious in wireless networks. In this paper,\nwe ask whether there exist low-overhead schemes that produce low-stretch\nroutes. This is studied by using the underlying geometric properties of the\nconnectivity graph in wireless networks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2009 09:57:50 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2009 08:06:15 GMT"}], "update_date": "2009-09-16", "authors_parsed": [["Tschopp", "Dominique", ""], ["Diggavi", "Suhas", ""], ["Grossglauser", "Matthias", ""]]}, {"id": "0909.2547", "submitter": "Maxim Kolosovskiy", "authors": "Maxim Kolosovskiy (Altai State Technical University, Russia)", "title": "Simple implementation of deletion from open-address hash table", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deletion from open-address hash table is not so easy as deletion from chained\nhash table, because in open-address table we can't simply mark a slot\ncontaining deleted key as empty. Search for keys may become incorrect. The\nclassical method to implement deletion is to mark slots in hash table by three\nvalues: \"free\", \"busy\", \"deleted\". That method is easy to implement, but there\nare some disadvantages. In this article we consider alternative method of\ndeletion keys, where we avoid using the mark \"deleted\". The article contains\nthe implementation of the method in Java.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2009 13:36:58 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Kolosovskiy", "Maxim", "", "Altai State Technical University, Russia"]]}, {"id": "0909.2704", "submitter": "Aaron Sterling", "authors": "Aaron Sterling", "title": "Memory Consistency Conditions for Self-Assembly Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perhaps the two most significant theoretical questions about the programming\nof self-assembling agents are: (1) necessary and sufficient conditions to\nproduce a unique terminal assembly, and (2) error correction. We address both\nquestions, by reducing two well-studied models of tile assembly to models of\ndistributed shared memory (DSM), in order to obtain results from the memory\nconsistency systems induced by tile assembly systems when simulated in the DSM\nsetting. The Abstract Tile Assembly Model (aTAM) can be simulated by a DSM\nsystem that obeys causal consistency, and the locally deterministic tile\nassembly systems in the aTAM correspond exactly to the concurrent-write free\nprograms that simulate tile assembly in such a model. Thus, the detection of\nthe failure of local determinism (which had formerly been an open problem)\nreduces to the detection of data races in simulating programs. Further, the\nKinetic Tile Assembly Model can be simulated by a DSM system that obeys GWO, a\nmemory consistency condition defined by Steinke and Nutt. (To our knowledge,\nthis is the first natural example of a DSM system that obeys GWO, but no\nstronger consistency condition.) We combine these results with the observation\nthat self-assembly algorithms are local algorithms, and there exists a fast\nconversion of deterministic local algorithms into deterministic\nself-stabilizing algorithms. This provides an \"immediate\" generalization of a\ntheorem by Soloveichik et al. about the existence of tile assembly systems that\nsimultaneously perform two forms of self-stabilization: proofreading and\nself-healing. Our reductions and proof techniques can be extended to the\nprogramming of self-assembling agents in a variety of media, not just DNA\ntiles, and not just two-dimensional surfaces.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2009 19:45:22 GMT"}], "update_date": "2009-09-16", "authors_parsed": [["Sterling", "Aaron", ""]]}, {"id": "0909.2733", "submitter": "Pierre Fraigniaud", "authors": "Pierre Fraigniaud and Amos Korman", "title": "An Optimal Labeling Scheme for Ancestry Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ancestry labeling scheme assigns labels (bit strings) to the nodes of\nrooted trees such that ancestry queries between any two nodes in a tree can be\nanswered merely by looking at their corresponding labels. The quality of an\nancestry labeling scheme is measured by its label size, that is the maximal\nnumber of bits in a label of a tree node.\n  In addition to its theoretical appeal, the design of efficient ancestry\nlabeling schemes is motivated by applications in web search engines. For this\npurpose, even small improvements in the label size are important. In fact, the\nliterature about this topic is interested in the exact label size rather than\njust its order of magnitude. As a result, following the proposal of a simple\ninterval-based ancestry scheme with label size $2\\log_2 n$ bits (Kannan et al.,\nSTOC '88), a considerable amount of work was devoted to improve the bound on\nthe size of a label. The current state of the art upper bound is $\\log_2 n +\nO(\\sqrt{\\log n})$ bits (Abiteboul et al., SODA '02) which is still far from the\nknown $\\log_2 n + \\Omega(\\log\\log n)$ bits lower bound (Alstrup et al., SODA\n'03).\n  In this paper we close the gap between the known lower and upper bounds, by\nconstructing an ancestry labeling scheme with label size $\\log_2 n + O(\\log\\log\nn)$ bits. In addition to the optimal label size, our scheme assigns the labels\nin linear time and can support any ancestry query in constant time.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2009 13:31:54 GMT"}], "update_date": "2009-09-16", "authors_parsed": [["Fraigniaud", "Pierre", ""], ["Korman", "Amos", ""]]}, {"id": "0909.2787", "submitter": "Tamara Mchedlidze David", "authors": "Tamara Mchedlidze, Antonios Symvonis", "title": "Crossing-Free Acyclic Hamiltonian Path Completion for Planar st-Digraphs", "comments": "Accepted to ISAAC2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of existence of a crossing-free acyclic\nhamiltonian path completion (for short, HP-completion) set for embedded upward\nplanar digraphs. In the context of book embeddings, this question becomes:\ngiven an embedded upward planar digraph $G$, determine whether there exists an\nupward 2-page book embedding of $G$ preserving the given planar embedding.\n  Given an embedded $st$-digraph $G$ which has a crossing-free HP-completion\nset, we show that there always exists a crossing-free HP-completion set with at\nmost two edges per face of $G$. For an embedded $N$-free upward planar digraph\n$G$, we show that there always exists a crossing-free acyclic HP-completion set\nfor $G$ which, moreover, can be computed in linear time. For a width-$k$\nembedded planar $st$-digraph $G$, we show that we can be efficiently test\nwhether $G$ admits a crossing-free acyclic HP-completion set.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2009 12:12:06 GMT"}], "update_date": "2009-09-16", "authors_parsed": [["Mchedlidze", "Tamara", ""], ["Symvonis", "Antonios", ""]]}, {"id": "0909.2814", "submitter": "Ren\\'e van Bevern", "authors": "Ren\\'e van Bevern", "title": "Graph-based data clustering: a quadratic-vertex problem kernel for\n  s-Plex Cluster Vertex Deletion", "comments": "41 pages, 17 figures", "journal-ref": "Algorithmica, 62(3-4):930-950, 2012", "doi": "10.1007/s00453-011-9492-7", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the s-Plex Cluster Vertex Deletion problem. Like the Cluster\nVertex Deletion problem, it is NP-hard and motivated by graph-based data\nclustering. While the task in Cluster Vertex Deletion is to delete vertices\nfrom a graph so that its connected components become cliques, the task in\ns-Plex Cluster Vertex Deletion is to delete vertices from a graph so that its\nconnected components become s-plexes. An s-plex is a graph in which every\nvertex is nonadjacent to at most s-1 other vertices; a clique is an 1-plex. In\ncontrast to Cluster Vertex Deletion, s-Plex Cluster Vertex Deletion allows to\nbalance the number of vertex deletions against the sizes and the density of the\nresulting clusters, which are s-plexes instead of cliques. The focus of this\nwork is the development of provably efficient and effective data reduction\nrules for s-Plex Cluster Vertex Deletion. In terms of fixed-parameter\nalgorithmics, these yield a so-called problem kernel. A similar problem, s-Plex\nEditing, where the task is the insertion or the deletion of edges so that the\nconnected components of a graph become s-plexes, has also been studied in terms\nof fixed-parameter algorithmics. Using the number of allowed graph\nmodifications as parameter, we expect typical parameter values for s-Plex\nCluster Vertex Deletion to be significantly lower than for s-Plex Editing,\nbecause one vertex deletion can lead to a high number of edge deletions. This\nholds out the prospect for faster fixed-parameter algorithms for s-Plex Cluster\nVertex Deletion.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2009 17:27:00 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["van Bevern", "Ren\u00e9", ""]]}, {"id": "0909.2849", "submitter": "Shayan Oveis Gharan", "authors": "Shayan Oveis Gharan, Amin Saberi", "title": "The Asymmetric Traveling Salesman Problem on Graphs with Bounded Genus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a constant factor approximation algorithm for the asymmetric\ntraveling salesman problem when the support graph of the solution of the\nHeld-Karp linear programming relaxation has bounded orientable genus.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2009 18:39:03 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2009 17:46:49 GMT"}, {"version": "v3", "created": "Sat, 7 Nov 2009 17:11:54 GMT"}, {"version": "v4", "created": "Fri, 14 Jan 2011 20:49:58 GMT"}], "update_date": "2011-01-17", "authors_parsed": [["Gharan", "Shayan Oveis", ""], ["Saberi", "Amin", ""]]}, {"id": "0909.3122", "submitter": "Gwendal Simon", "authors": "Jacob Chakareski, Pascal Frossard, Herv\\'e Kerivin, Jimmy Leblet and\n  Gwendal Simon", "title": "A note on the data-driven capacity of P2P networks", "comments": "10 pages, technical report assisting a submission", "journal-ref": null, "doi": null, "report-no": "EPFL-LTS-2009-008", "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two capacity problems in P2P networks. In the first one, the\nnodes have an infinite amount of data to send and the goal is to optimally\nallocate their uplink bandwidths such that the demands of every peer in terms\nof receiving data rate are met. We solve this problem through a mapping from a\nnode-weighted graph featuring two labels per node to a max flow problem on an\nedge-weighted bipartite graph. In the second problem under consideration, the\nresource allocation is driven by the availability of the data resource that the\npeers are interested in sharing. That is a node cannot allocate its uplink\nresources unless it has data to transmit first. The problem of uplink bandwidth\nallocation is then equivalent to constructing a set of directed trees in the\noverlay such that the number of nodes receiving the data is maximized while the\nuplink capacities of the peers are not exceeded. We show that the problem is\nNP-complete, and provide a linear programming decomposition decoupling it into\na master problem and multiple slave subproblems that can be resolved in\npolynomial time. We also design a heuristic algorithm in order to compute a\nsuboptimal solution in a reasonable time. This algorithm requires only a local\nknowledge from nodes, so it should support distributed implementations.\n  We analyze both problems through a series of simulation experiments featuring\ndifferent network sizes and network densities. On large networks, we compare\nour heuristic and its variants with a genetic algorithm and show that our\nheuristic computes the better resource allocation. On smaller networks, we\ncontrast these performances to that of the exact algorithm and show that\nresource allocation fulfilling a large part of the peer can be found, even for\nhard configuration where no resources are in excess.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2009 22:44:47 GMT"}], "update_date": "2009-09-18", "authors_parsed": [["Chakareski", "Jacob", ""], ["Frossard", "Pascal", ""], ["Kerivin", "Herv\u00e9", ""], ["Leblet", "Jimmy", ""], ["Simon", "Gwendal", ""]]}, {"id": "0909.3137", "submitter": "Beno\\^it Hudson", "authors": "Beno\\^it Hudson", "title": "Succinct Representation of Well-Spaced Point Clouds", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of n points in low dimensions takes Theta(n w) bits to store on a w-bit\nmachine. Surface reconstruction and mesh refinement impose a requirement on the\ndistribution of the points they process. I show how to use this assumption to\nlossily compress a set of n input points into a representation that takes only\nO(n) bits, independent of the word size. The loss can keep inter-point\ndistances to within 10% relative error while still achieving a factor of three\nspace savings. The representation allows standard quadtree operations, along\nwith computing the restricted Voronoi cell of a point, in time O(w^2 + log n),\nwhich can be improved to time O(log n) if w is in Theta(log n). Thus one can\nuse this compressed representation to perform mesh refinement or surface\nreconstruction in O(n) bits with only a logarithmic slowdown.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2009 01:39:59 GMT"}], "update_date": "2009-09-18", "authors_parsed": [["Hudson", "Beno\u00eet", ""]]}, {"id": "0909.3180", "submitter": "Geevarghese Philip", "authors": "Neeldhara Misra, Geevarghese Philip, Venkatesh Raman, Saket Saurabh\n  and Somnath Sikdar", "title": "FPT Algorithms for Connected Feedback Vertex Set", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the recently introduced Connected Feedback Vertex Set (CFVS) problem\nfrom the view-point of parameterized algorithms. CFVS is the connected variant\nof the classical Feedback Vertex Set problem and is defined as follows: given a\ngraph G=(V,E) and an integer k, decide whether there exists a subset F of V, of\nsize at most k, such that G[V F] is a forest and G[F] is connected. We show\nthat Connected Feedback Vertex Set can be solved in time $O(2^{O(k)}n^{O(1)})$\non general graphs and in time $O(2^{O(\\sqrt{k}\\log k)}n^{O(1)})$ on graphs\nexcluding a fixed graph H as a minor. Our result on general undirected graphs\nuses as subroutine, a parameterized algorithm for Group Steiner Tree, a well\nstudied variant of Steiner Tree. We find the algorithm for Group Steiner Tree\nof independent interest and believe that it will be useful for obtaining\nparameterized algorithms for other connectivity problems.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2009 10:37:31 GMT"}], "update_date": "2009-09-18", "authors_parsed": [["Misra", "Neeldhara", ""], ["Philip", "Geevarghese", ""], ["Raman", "Venkatesh", ""], ["Saurabh", "Saket", ""], ["Sikdar", "Somnath", ""]]}, {"id": "0909.3221", "submitter": "Gwena\\\"el Joret", "authors": "Jean Cardinal, Erik D. Demaine, Samuel Fiorini, Gwena\\\"el Joret, Ilan\n  Newman, Oren Weimann", "title": "The Stackelberg Minimum Spanning Tree Game on Planar and\n  Bounded-Treewidth Graphs", "comments": "v2: Referees' comments incorporated, section on bounded-treewidth\n  graphs expanded", "journal-ref": "Journal of Combinatorial Optimization, 25/1:19--46, 2013", "doi": "10.1007/s10878-011-9414-2", "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Stackelberg Minimum Spanning Tree Game is a two-level combinatorial\npricing problem played on a graph representing a network. Its edges are colored\neither red or blue, and the red edges have a given fixed cost, representing the\ncompetitor's prices. The first player chooses an assignment of prices to the\nblue edges, and the second player then buys the cheapest spanning tree, using\nany combination of red and blue edges. The goal of the first player is to\nmaximize the total price of purchased blue edges.\n  We study this problem in the cases of planar and bounded-treewidth graphs. We\nshow that the problem is NP-hard on planar graphs but can be solved in\npolynomial time on graphs of bounded treewidth.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2009 13:52:46 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2011 04:25:08 GMT"}], "update_date": "2013-05-24", "authors_parsed": [["Cardinal", "Jean", ""], ["Demaine", "Erik D.", ""], ["Fiorini", "Samuel", ""], ["Joret", "Gwena\u00ebl", ""], ["Newman", "Ilan", ""], ["Weimann", "Oren", ""]]}, {"id": "0909.3346", "submitter": "Michael Kapralov", "authors": "Ashish Goel, Michael Kapralov and Sanjeev Khanna", "title": "Perfect Matchings in O(n \\log n) Time in Regular Bipartite Graphs", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the well-studied problem of finding a perfect\nmatching in a d-regular bipartite graph on 2n nodes with m=nd edges. The\nbest-known algorithm for general bipartite graphs (due to Hopcroft and Karp)\ntakes time O(m\\sqrt{n}). In regular bipartite graphs, however, a matching is\nknown to be computable in O(m) time (due to Cole, Ost and Schirra). In a recent\nline of work by Goel, Kapralov and Khanna the O(m) time algorithm was improved\nfirst to \\tilde O(min{m, n^{2.5}/d}) and then to \\tilde O(min{m, n^2/d}). It\nwas also shown that the latter algorithm is optimal up to polylogarithmic\nfactors among all algorithms that use non-adaptive uniform sampling to reduce\nthe size of the graph as a first step.\n  In this paper, we give a randomized algorithm that finds a perfect matching\nin a d-regular graph and runs in O(n\\log n) time (both in expectation and with\nhigh probability). The algorithm performs an appropriately truncated random\nwalk on a modified graph to successively find augmenting paths. Our algorithm\nmay be viewed as using adaptive uniform sampling, and is thus able to bypass\nthe limitations of (non-adaptive) uniform sampling established in earlier work.\nWe also show that randomization is crucial for obtaining o(nd) time algorithms\nby establishing an \\Omega(nd) lower bound for any deterministic algorithm. Our\ntechniques also give an algorithm that successively finds a matching in the\nsupport of a doubly stochastic matrix in expected time O(n\\log^2 n) time, with\nO(m) pre-processing time; this gives a simple O(m+mn\\log^2 n) time algorithm\nfor finding the Birkhoff-von Neumann decomposition of a doubly stochastic\nmatrix.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2009 02:11:43 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2010 01:44:43 GMT"}, {"version": "v3", "created": "Fri, 12 Nov 2010 06:45:01 GMT"}], "update_date": "2010-11-15", "authors_parsed": [["Goel", "Ashish", ""], ["Kapralov", "Michael", ""], ["Khanna", "Sanjeev", ""]]}, {"id": "0909.3533", "submitter": "Yavuz Oruc A.", "authors": "A. Yavuz Oruc and Abdullah Atmaca", "title": "On Ordinal Covering of Proposals Using Balanced Incomplete Block Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A frequently encountered problem in peer review systems is to facilitate\npairwise comparisons of a given set of proposals by as few as referees as\npossible. In [8], it was shown that, if each referee is assigned to review k\nproposals then ceil{n(n-1)/k(k-1)} referees are necessary and ceil{n(2n-k)/k^2}\nreferees are sufficient to cover all n(n-1)/2 pairs of n proposals. While the\nupper bound remains within a factor of 2 of the lower bound, it becomes\nrelatively large for small values of k and the ratio of the upper bound to the\nlower bound is not less than 3/2 when 2 <= k <= n/2. In this paper, we show\nthat, if sqrt(n) <= k <= n/2 then the upper and lower bounds can be made closer\nin that their ratio never exceeds 3/2. This is accomplished by a new method\nthat assigns proposals to referees using a particular family of balanced\nincomplete block designs. Specifically, the new method uses ceil{n(n+k)/k^2}\nreferees when n/k is a prime power, n divides k^2, and sqrt(n) <= k <= n/2.\nComparing this new upper bound to the one given in [8] shows that the new upper\nbound approaches the lower bound as k tends to sqrt(n) whereas the upper bound\nin [8] approaches the lower bound as k tends to n. Therefore, the new method\ngiven here when combined together with the one in [8] provides an assignment\nwhose upper bound referee complexity always remains within a factor of 3/2 of\nthe lower bound when sqrt(n) <= k <= n, thereby improving upon the assignment\ndescribed in [8]. Furthermore, the new method provides a minimal covering,\ni.e., it uses the minimum number of referees possible when k = sqrt(n) and k is\na prime power.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2009 21:01:14 GMT"}], "update_date": "2009-09-22", "authors_parsed": [["Oruc", "A. Yavuz", ""], ["Atmaca", "Abdullah", ""]]}, {"id": "0909.3637", "submitter": "Fei Li", "authors": "Fei Li", "title": "Packet Scheduling in a Size-Bounded Buffer", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider algorithms to schedule packets with values and deadlines in a\nsize-bounded buffer. At any time, the buffer can store at most B packets.\nPackets arrive over time. Each packet has a non-negative value and an integer\ndeadline. In each time step, at most one packet can be sent. Packets can be\ndropped at any time before they are sent. The objective is to maximize the\ntotal value gained by delivering packets no later than their respective\ndeadlines. This model generalizes the well-studied bounded-delay model (Hajek.\nCISS 2001. Kesselman et al. STOC 2001). We first provide an optimal offline\nalgorithm for this model. Then we present an alternative proof of the\n2-competitive deterministic online algorithm (Fung. arXiv July 2009). We also\nprove that the lower bound of competitive ratio of a family of (deterministic\nand randomized) algorithms is 2 - 1 / B.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2009 15:20:17 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2010 00:40:03 GMT"}], "update_date": "2010-02-01", "authors_parsed": [["Li", "Fei", ""]]}, {"id": "0909.3696", "submitter": "Victor Chen", "authors": "Victor Chen, Elena Grigorescu, Ronald de Wolf", "title": "Efficient and Error-Correcting Data Structures for Membership and\n  Polynomial Evaluation", "comments": "An abridged version of this paper appears in STACS 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct efficient data structures that are resilient against a constant\nfraction of adversarial noise. Our model requires that the decoder answers most\nqueries correctly with high probability and for the remaining queries, the\ndecoder with high probability either answers correctly or declares \"don't\nknow.\" Furthermore, if there is no noise on the data structure, it answers all\nqueries correctly with high probability. Our model is the common generalization\nof a model proposed recently by de Wolf and the notion of \"relaxed locally\ndecodable codes\" developed in the PCP literature.\n  We measure the efficiency of a data structure in terms of its length,\nmeasured by the number of bits in its representation, and query-answering time,\nmeasured by the number of bit-probes to the (possibly corrupted)\nrepresentation. In this work, we study two data structure problems: membership\nand polynomial evaluation. We show that these two problems have constructions\nthat are simultaneously efficient and error-correcting.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2009 07:16:22 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2010 16:06:04 GMT"}], "update_date": "2010-01-27", "authors_parsed": [["Chen", "Victor", ""], ["Grigorescu", "Elena", ""], ["de Wolf", "Ronald", ""]]}, {"id": "0909.3877", "submitter": "James Nastos", "authors": "James Nastos, Yong Gao", "title": "A note on the hardness of graph diameter augmentation problems", "comments": "No figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph has \\emph{diameter} D if every pair of vertices are connected by a\npath of at most D edges. The Diameter-D Augmentation problem asks how to add\nthe a number of edges to a graph in order to make the resulting graph have\ndiameter D. It was previously known that this problem is NP-hard \\cite{GJ},\neven in the D=2 case. In this note, we give a simpler reduction to arrive at\nthis fact and show that this problem is W[2]-hard.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2009 23:26:00 GMT"}], "update_date": "2009-09-23", "authors_parsed": [["Nastos", "James", ""], ["Gao", "Yong", ""]]}, {"id": "0909.4011", "submitter": "Micha{\\l} Adamaszek", "authors": "Anna Adamaszek, Michal Adamaszek", "title": "Large-girth roots of graphs", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recognizing graph powers and computing roots of\ngraphs. We provide a polynomial time recognition algorithm for r-th powers of\ngraphs of girth at least 2r+3, thus improving a bound conjectured by Farzad et\nal. (STACS 2009). Our algorithm also finds all r-th roots of a given graph that\nhave girth at least 2r+3 and no degree one vertices, which is a step towards a\nrecent conjecture of Levenshtein that such root should be unique. On the\nnegative side, we prove that recognition becomes an NP-complete problem when\nthe bound on girth is about twice smaller. Similar results have so far only\nbeen attempted for r=2,3.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2009 14:46:55 GMT"}], "update_date": "2009-09-23", "authors_parsed": [["Adamaszek", "Anna", ""], ["Adamaszek", "Michal", ""]]}, {"id": "0909.4021", "submitter": "Marek Cygan", "authors": "Marek Cygan, Marcin Pilipczuk and Jakub Onufry Wojtaszczyk", "title": "Beyond O*(2^n) in domination-type problems", "comments": "Submitted to STACS 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide algorithms faster than O*(2^n) for several\nNP-complete domination-type problems. More precisely, we provide: an algorithm\nfor CAPACITATED DOMINATING SET that solves it in O(1.89^n), a branch-and-reduce\nalgorithm solving LARGEST IRREDUNDANT SET in O(1.9657^n) time and a simple\niterative-DFS algorithm for SMALLEST INCLUSION-MAXIMAL IRREDUNDANT SET that\nsolves it in O(1.999956^n) time.\n  We also provide an exponential approximation scheme for CAPACITATED\nDOMINATING SET. All algorithms require polynomial space. Despite the fact that\nthe discussed problems are quite similar to the DOMINATING SET problem, we are\nnot aware of any published algorithms solving these problems faster than the\nobvious O*(2^n) solution prior to this paper.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2009 17:06:28 GMT"}], "update_date": "2009-09-23", "authors_parsed": [["Cygan", "Marek", ""], ["Pilipczuk", "Marcin", ""], ["Wojtaszczyk", "Jakub Onufry", ""]]}, {"id": "0909.4224", "submitter": "Henning Fernau", "authors": "Ljiljana Brankovic, Henning Fernau, Joachim Kneis, Dieter Kratsch\n  Alexander Langer Mathieu Liedloff Daniel Raible Peter Rossmanith", "title": "Breaking the 2^n-Barrier for Irredundance: A Parameterized Route to\n  Solving Exact Puzzles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lower and the upper irredundance numbers of a graph $G$, denoted $ir(G)$\nand $IR(G)$ respectively, are conceptually linked to domination and\nindependence numbers and have numerous relations to other graph parameters. It\nis a long-standing open question whether determining these numbers for a graph\n$G$ on $n$ vertices admits exact algorithms running in time less than the\ntrivial $\\Omega(2^n)$ enumeration barrier. We solve these open problems by\ndevising parameterized algorithms for the dual of the natural parameterizations\nof the problems with running times faster than $O^*(4^{k})$. For example, we\npresent an algorithm running in time $O^*(3.069^{k})$ for determining whether\n$IR(G)$ is at least $n-k$. Although the corresponding problem has been known to\nbe in FPT by kernelization techniques, this paper offers the first\nparameterized algorithms with an exponential dependency on the parameter in the\nrunning time. Additionally, our work also appears to be the first example of a\nparameterized approach leading to a solution to a problem in exponential time\nalgorithmics where the natural interpretation as an exact exponential-time\nalgorithm fails.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2009 15:01:24 GMT"}], "update_date": "2009-09-24", "authors_parsed": [["Brankovic", "Ljiljana", ""], ["Fernau", "Henning", ""], ["Kneis", "Joachim", ""], ["Rossmanith", "Dieter Kratsch Alexander Langer Mathieu Liedloff Daniel Raible Peter", ""]]}, {"id": "0909.4275", "submitter": "Ilya Safro", "authors": "Jie Chen and Ilya Safro", "title": "A Measure of the Connection Strengths between Graph Vertices with\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple iterative strategy for measuring the connection strength\nbetween a pair of vertices in a graph. The method is attractive in that it has\na linear complexity and can be easily parallelized. Based on an analysis of the\nconvergence property, we propose a mutually reinforcing model to explain the\nintuition behind the strategy. The practical effectiveness of this measure is\ndemonstrated through several combinatorial optimization problems on graphs and\nhypergraphs.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2009 18:31:32 GMT"}], "update_date": "2009-09-24", "authors_parsed": [["Chen", "Jie", ""], ["Safro", "Ilya", ""]]}, {"id": "0909.4341", "submitter": "Travis Gagie", "authors": "Paolo Ferragina, Travis Gagie and Giovanni Manzini", "title": "Lightweight Data Indexing and Compression in External Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe algorithms for computing the BWT and for building\n(compressed) indexes in external memory. The innovative feature of our\nalgorithms is that they are lightweight in the sense that, for an input of size\n$n$, they use only ${n}$ bits of disk working space while all previous\napproaches use $\\Th{n \\log n}$ bits of disk working space. Moreover, our\nalgorithms access disk data only via sequential scans, thus they take full\nadvantage of modern disk features that make sequential disk accesses much\nfaster than random accesses.\n  We also present a scan-based algorithm for inverting the BWT that uses\n$\\Th{n}$ bits of working space, and a lightweight {\\em internal-memory}\nalgorithm for computing the BWT which is the fastest in the literature when the\navailable working space is $\\os{n}$ bits.\n  Finally, we prove {\\em lower} bounds on the complexity of computing and\ninverting the BWT via sequential scans in terms of the classic product:\ninternal-memory space $\\times$ number of passes over the disk data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2009 18:23:20 GMT"}], "update_date": "2009-09-25", "authors_parsed": [["Ferragina", "Paolo", ""], ["Gagie", "Travis", ""], ["Manzini", "Giovanni", ""]]}, {"id": "0909.4348", "submitter": "Jan Vondrak", "authors": "Chandra Chekuri, Jan Vondrak, Rico Zenklusen", "title": "Dependent Randomized Rounding for Matroid Polytopes and Applications", "comments": "Rico Zenklusen joined as an author; paper substantially expanded\n  compared to previous version; note a slight change in the title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by several applications, we consider the problem of randomly\nrounding a fractional solution in a matroid (base) polytope to an integral one.\nWe consider the pipage rounding technique and also present a new technique,\nrandomized swap rounding. Our main technical results are concentration bounds\nfor functions of random variables arising from these rounding techniques. We\nprove Chernoff-type concentration bounds for linear functions of random\nvariables arising from both techniques, and also a lower-tail exponential bound\nfor monotone submodular functions of variables arising from randomized swap\nrounding.\n  The following are examples of our applications: (1) We give a\n(1-1/e-epsilon)-approximation algorithm for the problem of maximizing a\nmonotone submodular function subject to 1 matroid and k linear constraints, for\nany constant k and epsilon>0. (2) We present a result on minimax packing\nproblems that involve a matroid base constraint. We give an O(log m / log log\nm)-approximation for the general problem Min {lambda: x \\in {0,1}^N, x \\in\nB(M), Ax <= lambda b}, where m is the number of packing constraints. (3) We\ngeneralize the continuous greedy algorithm to problems involving multiple\nsubmodular functions, and use it to find a (1-1/e-epsilon)-approximate pareto\nset for the problem of maximizing a constant number of monotone submodular\nfunctions subject to a matroid constraint. An example is the Submodular Welfare\nProblem where we are looking for an approximate pareto set with respect to\nindividual players' utilities.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2009 17:03:10 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2009 23:26:04 GMT"}], "update_date": "2009-11-07", "authors_parsed": [["Chekuri", "Chandra", ""], ["Vondrak", "Jan", ""], ["Zenklusen", "Rico", ""]]}, {"id": "0909.4369", "submitter": "Bernard Mans", "authors": "Paola Flocchini, Bernard Mans and Nicola Santoro", "title": "Exploration of Periodically Varying Graphs", "comments": "22 pages. Shorter paper (10 pages) accepted at ISAAC 2009. ISAAC'09,\n  The 20th International Symposium on Algorithms and Computation, December\n  16-18, 2008, Hawaii, USA. Exploration of Periodically Varying Graphs, P.\n  Flocchini, B. Mans and N. Santoro, Springer-LNCS vol. 5878, accepted Aug.\n  2009, Y. Dong, D.-Z. Du, and O. Ibarra (Eds), to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computability and complexity of the exploration problem in a\nclass of highly dynamic graphs: periodically varying (PV) graphs, where the\nedges exist only at some (unknown) times defined by the periodic movements of\ncarriers. These graphs naturally model highly dynamic infrastructure-less\nnetworks such as public transports with fixed timetables, low earth orbiting\n(LEO) satellite systems, security guards' tours, etc. We establish necessary\nconditions for the problem to be solved. We also derive lower bounds on the\namount of time required in general, as well as for the PV graphs defined by\nrestricted classes of carriers movements: simple routes, and circular routes.\nWe then prove that the limitations on computability and complexity we have\nestablished are indeed tight. In fact we prove that all necessary conditions\nare also sufficient and all lower bounds on costs are tight. We do so\nconstructively presenting two worst case optimal solution algorithms, one for\nanonymous systems, and one for those with distinct nodes ids. An added benefit\nis that the algorithms are rather simple.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2009 06:01:25 GMT"}], "update_date": "2009-09-25", "authors_parsed": [["Flocchini", "Paola", ""], ["Mans", "Bernard", ""], ["Santoro", "Nicola", ""]]}, {"id": "0909.4686", "submitter": "Haralampos Tsaknakis", "authors": "Haralampos Tsaknakis and Paul G. Spirakis", "title": "A Graph Spectral Approach for Computing Approximate Nash Equilibria", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new methodology for computing approximate Nash equilibria for\ntwo-person non-cooperative games based upon certain extensions and\nspecializations of an existing optimization approach previously used for the\nderivation of fixed approximations for this problem. In particular, the general\ntwo-person problem is reduced to an indefinite quadratic programming problem of\nspecial structure involving the $n \\times n$ adjacency matrix of an induced\nsimple graph specified by the input data of the game, where $n$ is the number\nof players' strategies. Using this methodology and exploiting certain\nproperties of the positive part of the spectrum of the induced graph, we show\nthat for any $\\varepsilon > 0$ there is an algorithm to compute an\n$\\varepsilon$-approximate Nash equilibrium in time $n^{\\xi(m)/\\varepsilon}$,\nwhere, $\\xi (m) = \\sum_{i=1}^m \\lambda_i / n$ and $\\lambda_1, \\lambda_2, >...,\n\\lambda_m$ are the positive eigenvalues of the adjacency matrix of the graph.\nFor classes of games for which $\\xi (m)$ is a constant, there is a PTAS. Based\non the best upper bound derived for $\\xi(m)$ so far, the worst case complexity\nof the method is bounded by the subexponential $n^{\\sqrt{m}/\\varepsilon}$.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2009 12:58:59 GMT"}], "update_date": "2009-09-28", "authors_parsed": [["Tsaknakis", "Haralampos", ""], ["Spirakis", "Paul G.", ""]]}, {"id": "0909.4692", "submitter": "Frederic Dorn Harald", "authors": "Frederic Dorn", "title": "Planar Subgraph Isomorphism Revisited", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Subgraph Isomorphism is defined as follows: Given a pattern H\nand a host graph G on n vertices, does G contain a subgraph that is isomorphic\nto H? Eppstein [SODA 95, J'GAA 99] gives the first linear time algorithm for\nsubgraph isomorphism for a fixed-size pattern, say of order k, and arbitrary\nplanar host graph, improving upon the O(n^\\sqrt{k})-time algorithm when using\nthe ``Color-coding'' technique of Alon et al [J'ACM 95]. Eppstein's algorithm\nruns in time k^O(k) n, that is, the dependency on k is superexponential. We\nsolve an open problem posed in Eppstein's paper and improve the running time to\n2^O(k) n, that is, single exponential in k while keeping the term in n linear.\nNext to deciding subgraph isomorphism, we can construct a solution and\nenumerate all solutions in the same asymptotic running time. We may list w\nsubgraphs with an additive term O(w k) in the running time of our algorithm. We\nintroduce the technique of \"embedded dynamic programming\" on a suitably\nstructured graph decomposition, which exploits the topology of the underlying\nembeddings of the subgraph pattern (rather than of the host graph). To achieve\nour results, we give an upper bound on the number of partial solutions in each\ndynamic programming step as a function of pattern size--as it turns out, for\nthe planar subgraph isomorphism problem, that function is single exponential in\nthe number of vertices in the pattern.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2009 13:15:31 GMT"}], "update_date": "2009-09-28", "authors_parsed": [["Dorn", "Frederic", ""]]}, {"id": "0909.4808", "submitter": "Christina Fragouli", "authors": "Javad Ebrahimi and Christina Fragouli", "title": "Combinatiorial Algorithms for Wireless Information Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-standing open question in information theory is to characterize the\nunicast capacity of a wireless relay network. The difficulty arises due to the\ncomplex signal interactions induced in the network, since the wireless channel\ninherently broadcasts the signals and there is interference among\ntransmissions. Recently, Avestimehr, Diggavi and Tse proposed a linear\ndeterministic model that takes into account the shared nature of wireless\nchannels, focusing on the signal interactions rather than the background noise.\nThey generalized the min-cut max-flow theorem for graphs to networks of\ndeterministic channels and proved that the capacity can be achieved using\ninformation theoretical tools. They showed that the value of the minimum cut is\nin this case the minimum rank of all the adjacency matrices describing\nsource-destination cuts.\n  In this paper, we develop a polynomial time algorithm that discovers the\nrelay encoding strategy to achieve the min-cut value in linear deterministic\n(wireless) networks, for the case of a unicast connection. Our algorithm\ncrucially uses a notion of linear independence between channels to calculate\nthe capacity in polynomial time. Moreover, we can achieve the capacity by using\nvery simple one-symbol processing at the intermediate nodes, thereby\nconstructively yielding finite length strategies that achieve the unicast\ncapacity of the linear deterministic (wireless) relay network.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2009 21:33:35 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Ebrahimi", "Javad", ""], ["Fragouli", "Christina", ""]]}, {"id": "0909.4889", "submitter": "R Doomun", "authors": "Farah Jemili, Montaceur Zaghdoud, Mohamed Ben Ahmed", "title": "Hybrid Intrusion Detection and Prediction multiAgent System HIDPAS", "comments": "10 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "Farah Jemili, Montaceur Zaghdoud, Mohamed Ben Ahmed, International\n  Journal of Computer Science and Information Security, IJCSIS, Vol. 5, No. 1,\n  pp. 62-71, September 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.CR cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an intrusion detection and prediction system based on\nuncertain and imprecise inference networks and its implementation. Giving a\nhistoric of sessions, it is about proposing a method of supervised learning\ndoubled of a classifier permitting to extract the necessary knowledge in order\nto identify the presence or not of an intrusion in a session and in the\npositive case to recognize its type and to predict the possible intrusions that\nwill follow it. The proposed system takes into account the uncertainty and\nimprecision that can affect the statistical data of the historic. The\nsystematic utilization of an unique probability distribution to represent this\ntype of knowledge supposes a too rich subjective information and risk to be in\npart arbitrary. One of the first objectives of this work was therefore to\npermit the consistency between the manner of which we represent information and\ninformation which we really dispose.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2009 19:17:37 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Jemili", "Farah", ""], ["Zaghdoud", "Montaceur", ""], ["Ahmed", "Mohamed Ben", ""]]}, {"id": "0909.4893", "submitter": "Hagai Cohen", "authors": "Hagai Cohen and Ely Porat", "title": "Range Non-Overlapping Indexing", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the non-overlapping indexing problem: Given a text T, preprocess it\nso that you can answer queries of the form: given a pattern P, report the\nmaximal set of non-overlapping occurrences of P in T. A generalization of this\nproblem is the range non-overlapping indexing where in addition we are given\ntwo indexes i,j to report the maximal set of non-overlapping occurrences\nbetween these two indexes. We suggest new solutions for these problems. For the\nnon-overlapping problem our solution uses O(n) space with query time of O(m +\nocc_{NO}). For the range non-overlapping problem we propose a solution with\nO(n\\log^\\epsilon n) space for some 0<\\epsilon<1 and O(m + \\log\\log n +\nocc_{ij,NO}) query time.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2009 20:24:19 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2010 19:40:45 GMT"}], "update_date": "2010-01-12", "authors_parsed": [["Cohen", "Hagai", ""], ["Porat", "Ely", ""]]}, {"id": "0909.4969", "submitter": "Charalampos Tsourakakis", "authors": "Charalampos E. Tsourakakis", "title": "MACH: Fast Randomized Tensor Decompositions", "comments": "15 pages, 4 Tables, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensors naturally model many real world processes which generate multi-aspect\ndata. Such processes appear in many different research disciplines, e.g,\nchemometrics, computer vision, psychometrics and neuroimaging analysis. Tensor\ndecompositions such as the Tucker decomposition are used to analyze\nmulti-aspect data and extract latent factors, which capture the multilinear\ndata structure. Such decompositions are powerful mining tools, for extracting\npatterns from large data volumes. However, most frequently used algorithms for\nsuch decompositions involve the computationally expensive Singular Value\nDecomposition.\n  In this paper we propose MACH, a new sampling algorithm to compute such\ndecompositions. Our method is of significant practical value for tensor\nstreams, such as environmental monitoring systems, IP traffic matrices over\ntime, where large amounts of data are accumulated and the analysis is\ncomputationally intensive but also in \"post-mortem\" data analysis cases where\nthe tensor does not fit in the available memory. We provide the theoretical\nanalysis of our proposed method, and verify its efficacy in monitoring system\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2009 22:36:17 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Tsourakakis", "Charalampos E.", ""]]}, {"id": "0909.5032", "submitter": "EPTCS", "authors": "Alain Bretto (University of Caen), Yannick Silvestre (University of\n  Caen), Thierry Vall\\'ee (University of Caen)", "title": "Cartesian product of hypergraphs: properties and algorithms", "comments": null, "journal-ref": "EPTCS 4, 2009, pp. 22-28", "doi": "10.4204/EPTCS.4.3", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cartesian products of graphs have been studied extensively since the 1960s.\nThey make it possible to decrease the algorithmic complexity of problems by\nusing the factorization of the product. Hypergraphs were introduced as a\ngeneralization of graphs and the definition of Cartesian products extends\nnaturally to them. In this paper, we give new properties and algorithms\nconcerning coloring aspects of Cartesian products of hypergraphs. We also\nextend a classical prime factorization algorithm initially designed for graphs\nto connected conformal hypergraphs using 2-sections of hypergraphs.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2009 08:24:08 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Bretto", "Alain", "", "University of Caen"], ["Silvestre", "Yannick", "", "University of\n  Caen"], ["Vall\u00e9e", "Thierry", "", "University of Caen"]]}, {"id": "0909.5146", "submitter": "Hagai Cohen", "authors": "Hagai Cohen and Ely Porat", "title": "Fast Set Intersection and Two Patterns Matching", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new problem, the fast set intersection problem,\nwhich is to preprocess a collection of sets in order to efficiently report the\nintersection of any two sets in the collection. In addition we suggest new\nsolutions for the two-dimensional substring indexing problem and the document\nlisting problem for two patterns by reduction to the fast set intersection\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2009 17:13:56 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2010 19:45:57 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2010 17:07:48 GMT"}], "update_date": "2010-03-12", "authors_parsed": [["Cohen", "Hagai", ""], ["Porat", "Ely", ""]]}, {"id": "0909.5278", "submitter": "Yngve Villanger", "authors": "Fedor V. Fomin and Yngve Villanger", "title": "Finding Induced Subgraphs via Minimal Triangulations", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Potential maximal cliques and minimal separators are combinatorial objects\nwhich were introduced and studied in the realm of minimal triangulations\nproblems including Minimum Fill-in and Treewidth. We discover unexpected\napplications of these notions to the field of moderate exponential algorithms.\nIn particular, we show that given an n-vertex graph G together with its set of\npotential maximal cliques Pi_G, and an integer t, it is possible in time |Pi_G|\n* n^(O(t)) to find a maximum induced subgraph of treewidth t in G; and for a\ngiven graph F of treewidth t, to decide if G contains an induced subgraph\nisomorphic to F. Combined with an improved algorithm enumerating all potential\nmaximal cliques in time O(1.734601^n), this yields that both problems are\nsolvable in time 1.734601^n * n^(O(t)).\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2009 07:13:39 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2009 09:55:02 GMT"}], "update_date": "2009-12-22", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Villanger", "Yngve", ""]]}, {"id": "0909.5313", "submitter": "Srikanth Srinivasan", "authors": "Vikraman Arvind and Srikanth Srinivasan", "title": "The Remote Point Problem, Small Bias Space, and Expanding Generator Sets", "comments": "accepted to STACS 2010, conference version, 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Using $\\epsilon$-bias spaces over $F_2$, we show that the Remote Point\nProblem (RPP), introduced by Alon et al [APY09], has an $NC^2$ algorithm\n(achieving the same parameters as [APY09]). We study a generalization of the\nRemote Point Problem to groups: we replace $F^n$ by $G^n$ for an arbitrary\nfixed group $G$. When $G$ is Abelian, we give an $NC^2$ algorithm for RPP,\nagain using $\\epsilon$-bias spaces. For nonabelian $G$, we give a deterministic\npolynomial-time algorithm for RPP. We also show the connection to construction\nof expanding generator sets for the group $G^n$. All our algorithms for the RPP\nachieve essentially the same parameters as [APY09].\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2009 11:21:20 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2010 09:39:36 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2010 08:31:44 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Arvind", "Vikraman", ""], ["Srinivasan", "Srikanth", ""]]}, {"id": "0909.5649", "submitter": "Vitaly Osipov", "authors": "Nikolaj Leischner, Vitaly Osipov, Peter Sanders", "title": "GPU sample sort", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the design of a sample sort algorithm for manycore\nGPUs. Despite being one of the most efficient comparison-based sorting\nalgorithms for distributed memory architectures its performance on GPUs was\npreviously unknown. For uniformly distributed keys our sample sort is at least\n25% and on average 68% faster than the best comparison-based sorting algorithm,\nGPU Thrust merge sort, and on average more than 2 times faster than GPU\nquicksort. Moreover, for 64-bit integer keys it is at least 63% and on average\n2 times faster than the highly optimized GPU Thrust radix sort that directly\nmanipulates the binary representation of keys. Our implementation is robust to\ndifferent distributions and entropy levels of keys and scales almost linearly\nwith the input size. These results indicate that multi-way techniques in\ngeneral and sample sort in particular achieve substantially better performance\nthan two-way merge sort and quicksort.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2009 15:58:53 GMT"}], "update_date": "2009-10-01", "authors_parsed": [["Leischner", "Nikolaj", ""], ["Osipov", "Vitaly", ""], ["Sanders", "Peter", ""]]}, {"id": "0909.5653", "submitter": "John Fearnley", "authors": "John Fearnley, Marcin Jurdzi\\'nski, Rahul Savani", "title": "Linear Complementarity Algorithms for Infinite Games", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-11266-9_32", "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of two pivoting algorithms, due to Lemke and Cottle and\nDantzig, is studied on linear complementarity problems (LCPs) that arise from\ninfinite games, such as parity, average-reward, and discounted games. The\nalgorithms have not been previously studied in the context of infinite games,\nand they offer alternatives to the classical strategy-improvement algorithms.\nThe two algorithms are described purely in terms of discounted games, thus\nbypassing the reduction from the games to LCPs, and hence facilitating a better\nunderstanding of the algorithms when applied to games. A family of parity games\nis given, on which both algorithms run in exponential time, indicating that in\nthe worst case they perform no better for parity, average-reward, or discounted\ngames than they do for general P-matrix LCPs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2009 16:14:51 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Fearnley", "John", ""], ["Jurdzi\u0144ski", "Marcin", ""], ["Savani", "Rahul", ""]]}]