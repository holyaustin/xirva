[{"id": "1610.00082", "submitter": "Kamyar Khodamoradi", "authors": "Kamyar Khodamoradi, Ramesh Krishnamurti, Arash Rafiey, Georgios\n  Stamoulis", "title": "PTAS for Ordered Instances of Resource Allocation Problems with\n  Restrictions on Inclusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of allocating a set $I$ of $m$ indivisible resources\n(items) to a set $P$ of $n$ customers (players) competing for the resources.\nEach resource $j \\in I$ has a same value $v_j > 0$ for a subset of customers\ninterested in $j$, and zero value for the remaining customers. The utility\nreceived by each customer is the sum of the values of the resources allocated\nto her. The goal is to find a feasible allocation of the resources to the\ninterested customers such that for the Max-Min allocation problem (Min-Max\nallocation problem) the minimum of the utilities (maximum of the utilities)\nreceived by the customers is maximized (minimized). The Max-Min allocation\nproblem is also known as the \\textit{Fair Allocation problem}, or the\n\\textit{Santa Claus problem}. The Min-Max allocation problem is the problem of\nScheduling on Unrelated Parallel Machines, and is also known as the $R \\, | \\,\n| C_{\\max}$ problem.\n  In this paper, we are interested in instances of the problem that admit a\nPolynomial Time Approximation Scheme (PTAS). We show that an ordering property\non the resources and the customers is important and paves the way for a PTAS.\nFor the Max-Min allocation problem, we start with instances of the problem that\ncan be viewed as a \\textit{convex bipartite graph}; a bipartite graph for which\nthere exists an ordering of the resources such that each customer is interested\nin (has a positive evaluation for) a set of \\textit{consecutive} resources. We\ndemonstrate a PTAS for the inclusion-free cases. This class of instances is\nequivalent to the class of bipartite permutation graphs. For the Min-Max\nallocation problem, we also obtain a PTAS for inclusion-free instances. These\ninstances are not only of theoretical interest but also have practical\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 03:58:58 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 04:17:06 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Khodamoradi", "Kamyar", ""], ["Krishnamurti", "Ramesh", ""], ["Rafiey", "Arash", ""], ["Stamoulis", "Georgios", ""]]}, {"id": "1610.00130", "submitter": "Travis Gagie", "authors": "Leo Ferres, Jos\\'e Fuentes, Travis Gagie, Meng He and Gonzalo Navarro", "title": "Fast and Compact Planar Embeddings", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. Conference version\n  presented at WADS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many representations of planar graphs, but few are as elegant as\nTur\\'an's (1984): it is simple and practical, uses only 4 bits per edge, can\nhandle self-loops and multi-edges, and can store any specified embedding. Its\nmain disadvantage has been that \"it does not allow efficient searching\"\n(Jacobson, 1989). In this paper we show how to add a sublinear number of bits\nto Tur\\'an's representation such that it supports fast navigation while\nretaining simplicity. As a consequence of the inherited simplicity, we offer\nthe first efficient parallel construction of a compact encoding of a planar\ngraph embedding. Our experimental results show that the resulting\nrepresentation uses about 6 bits per edge in practice, supports basic\nnavigation operations within a few microseconds, and can be built sequentially\nat a rate below 1 microsecond per edge, featuring a linear speedup with a\nparallel efficiency around 50\\% for large datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 12:57:47 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 02:37:46 GMT"}, {"version": "v3", "created": "Sun, 19 Feb 2017 11:35:04 GMT"}, {"version": "v4", "created": "Tue, 2 May 2017 11:40:27 GMT"}, {"version": "v5", "created": "Thu, 31 Aug 2017 15:18:13 GMT"}, {"version": "v6", "created": "Thu, 21 Sep 2017 21:12:40 GMT"}, {"version": "v7", "created": "Sun, 18 Feb 2018 23:47:33 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Ferres", "Leo", ""], ["Fuentes", "Jos\u00e9", ""], ["Gagie", "Travis", ""], ["He", "Meng", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1610.00209", "submitter": "Nikhil Srivastava", "authors": "Prasad Raghavendra, Nick Ryder, Nikhil Srivastava", "title": "Real Stability Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a strongly polynomial time algorithm which determines whether or not\na bivariate polynomial is real stable. As a corollary, this implies an\nalgorithm for testing whether a given linear transformation on univariate\npolynomials preserves real-rootedness. The proof exploits properties of\nhyperbolic polynomials to reduce real stability testing to testing\nnonnegativity of a finite number of polynomials on an interval.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 00:15:53 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Raghavendra", "Prasad", ""], ["Ryder", "Nick", ""], ["Srivastava", "Nikhil", ""]]}, {"id": "1610.00300", "submitter": "Sayan Bandyapadhyay", "authors": "Sayan Bandyapadhyay, Aritra Banik", "title": "Polynomial Time Algorithms for Bichromatic Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider a collection of geometric problems involving\npoints colored by two colors (red and blue), referred to as bichromatic\nproblems. The motivation behind studying these problems is two fold; (i) these\nproblems appear naturally and frequently in the fields like Machine learning,\nData mining, and so on, and (ii) we are interested in extending the algorithms\nand techniques for single point set (monochromatic) problems to bichromatic\ncase. For all the problems considered in this paper, we design low polynomial\ntime exact algorithms. These algorithms are based on novel techniques which\nmight be of independent interest.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 16:12:07 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Bandyapadhyay", "Sayan", ""], ["Banik", "Aritra", ""]]}, {"id": "1610.00353", "submitter": "Moustapha Diaby", "authors": "M. Diaby, M.H. Karwan, and L. Sun", "title": "Exact extended formulation of the linear assignment problem (LAP)\n  polytope for solving the traveling salesman and quadratic assignment problems", "comments": "36 pages; 9 figures; 2 tables. Revision of the model in \"A\n  Small-Order-Polynomial-Sized Linear Program for Solving the Traveling\n  Salesman Problem.\" Model and proofs are more straightforward and much\n  simpler. In this version (#7): Step 2.b of the proof of integrality has been\n  elaborated on, while the whole proof has been further streamlined", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an O(n^6 ) linear programming model for the traveling salesman\n(TSP) and quadratic assignment (QAP) problems. The basic model is developed\nwithin the framework of the TSP. It does not involve the city-to-city\nvariables-based, traditional TSP polytope referred to in the literature as \"the\nTSP polytope.\" We do not model explicit Hamiltonian cycles of the cities.\nInstead, we use a time-dependent abstraction of TSP tours and develop a direct\nextended formulation of the linear assignment problem (LAP) polytope. The model\nis exact in the sense that it has integral extreme points which are in\none-to-one correspondence with TSP tours. It can be solved optimally using any\nlinear programming (LP) solver, hence offering a new (incidental) proof of the\nequality of the computational complexity classes \"P\" and \"NP.\" The extensions\nof the model to the time-dependent traveling salesman problem (TDTSP) as well\nas the quadratic assignment problem (QAP) are straightforward. The reasons for\nthe non-applicability of existing negative extended formulations results for\n\"the TSP polytope\" to the model in this paper as well as our software\nimplementation and the computational experimentation we conducted are briefly\ndiscussed.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 21:31:19 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 06:40:18 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 23:20:01 GMT"}, {"version": "v4", "created": "Mon, 11 Feb 2019 21:49:08 GMT"}, {"version": "v5", "created": "Sat, 6 Apr 2019 00:02:30 GMT"}, {"version": "v6", "created": "Mon, 29 Apr 2019 15:10:49 GMT"}, {"version": "v7", "created": "Sat, 11 May 2019 14:28:08 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Diaby", "M.", ""], ["Karwan", "M. H.", ""], ["Sun", "L.", ""]]}, {"id": "1610.00505", "submitter": "Manuel Lafond", "authors": "Manuel Lafond, C\\'eline Scornavacca", "title": "On the Weighted Quartet Consensus problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In phylogenetics, the consensus problem consists in summarizing a set of\nphylogenetic trees that all classify the same set of species into a single\ntree. Several definitions of consensus exist in the literature; in this paper\nwe focus on the Weighted Quartet Consensus problem, a problem with unknown\ncomplexity status so far. Here we prove that the Weighted Quartet Consensus\nproblem is NP-hard and we give a 1/2-factor approximation for this problem.\nDuring the process, we propose a derandomization procedure of a previously\nknown randomized 1/3-factor approximation. We also investigate the\nfixed-parameter tractability of this problem.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 11:51:21 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 18:21:42 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Lafond", "Manuel", ""], ["Scornavacca", "C\u00e9line", ""]]}, {"id": "1610.00574", "submitter": "Sepehr Eghbali", "authors": "Sepehr Eghbali and Ladan Tahvildari", "title": "Fast Cosine Similarity Search in Binary Space with Angular Multi-index\n  Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large dataset of binary codes and a binary query point, we address\nhow to efficiently find $K$ codes in the dataset that yield the largest cosine\nsimilarities to the query. The straightforward answer to this problem is to\ncompare the query with all items in the dataset, but this is practical only for\nsmall datasets. One potential solution to enhance the search time and achieve\nsublinear cost is to use a hash table populated with binary codes of the\ndataset and then look up the nearby buckets to the query to retrieve the\nnearest neighbors. However, if codes are compared in terms of cosine similarity\nrather than the Hamming distance, then the main issue is that the order of\nbuckets to probe is not evident. To examine this issue, we first elaborate on\nthe connection between the Hamming distance and the cosine similarity. Doing\nthis allows us to systematically find the probing sequence in the hash table.\nHowever, solving the nearest neighbor search with a single table is only\npractical for short binary codes. To address this issue, we propose the angular\nmulti-index hashing search algorithm which relies on building multiple hash\ntables on binary code substrings. The proposed search algorithm solves the\nexact angular $K$ nearest neighbor problem in a time that is often orders of\nmagnitude faster than the linear scan baseline and even approximation methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 23:16:37 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 12:55:36 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Eghbali", "Sepehr", ""], ["Tahvildari", "Ladan", ""]]}, {"id": "1610.00575", "submitter": "Murilo De Lima", "authors": "Murilo S. de Lima, M\\'ario C. San Felice, Orlando Lee", "title": "Facility Leasing with Penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the facility leasing problem with penalties. We\npresent a primal-dual algorithm which is a 3-approximation, based on the\nalgorithm by Nagarajan and Williamson for the facility leasing problem and on\nthe algorithm by Charikar et al. for the facility location problem with\npenalties.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 14:49:10 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["de Lima", "Murilo S.", ""], ["Felice", "M\u00e1rio C. San", ""], ["Lee", "Orlando", ""]]}, {"id": "1610.00581", "submitter": "Chris Cade", "authors": "Chris Cade, Ashley Montanaro, and Aleksandrs Belovs", "title": "Time and Space Efficient Quantum Algorithms for Detecting Cycles and\n  Testing Bipartiteness", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study space and time efficient quantum algorithms for two graph problems\n-- deciding whether an $n$-vertex graph is a forest, and whether it is\nbipartite. Via a reduction to the s-t connectivity problem, we describe quantum\nalgorithms for deciding both properties in $\\tilde{O}(n^{3/2})$ time and using\n$O(\\log n)$ classical and quantum bits of storage in the adjacency matrix\nmodel. We then present quantum algorithms for deciding the two properties in\nthe adjacency array model, which run in time $\\tilde{O}(n\\sqrt{d_m})$ and also\nrequire $O(\\log n)$ space, where $d_m$ is the maximum degree of any vertex in\nthe input graph.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 15:10:05 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Cade", "Chris", ""], ["Montanaro", "Ashley", ""], ["Belovs", "Aleksandrs", ""]]}, {"id": "1610.01004", "submitter": "Brijesh Dongol", "authors": "Alasdair Armstrong, Brijesh Dongol, Simon Doherty", "title": "Reducing Opacity to Linearizability: A Sound and Complete Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactional memory is a mechanism that manages thread synchronisation on\nbehalf of a programmer so that blocks of code execute with an illusion of\natomicity. The main safety criterion for transactional memory is opacity, which\ndefines conditions for serialising concurrent transactions.\n  Proving opacity is complicated because it allows concurrent transactions to\nobserve distinct memory states, while TM implementations are typically based on\none single shared store. This paper presents a sound and complete method, based\non coarse-grained abstraction, for reducing proofs of opacity to the relatively\nsimpler correctness condition: linearizability. We use our methods to verify\nTML and NORec from the literature and show our techniques extend to relaxed\nmemory models by showing that both are opaque under TSO without requiring\nadditional fences. Our methods also elucidate TM designs at higher level of\nabstraction; as an application, we develop a variation of NORec with fast-path\nreads transactions. All our proofs have been mechanised, either in the Isabelle\ntheorem prover or the PAT model checker.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 14:07:52 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Armstrong", "Alasdair", ""], ["Dongol", "Brijesh", ""], ["Doherty", "Simon", ""]]}, {"id": "1610.01058", "submitter": "Rishi Saket", "authors": "Alina Ene, Viswanath Nagarajan, Rishi Saket", "title": "Approximation Algorithms for Stochastic k-TSP", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the stochastic $k$-TSP problem where rewards at vertices are\nrandom and the objective is to minimize the expected length of a tour that\ncollects reward $k$. We present an adaptive $O(\\log k)$-approximation\nalgorithm, and a non-adaptive $O(\\log^2k)$-approximation algorithm. We also\nshow that the adaptivity gap of this problem is between $e$ and $O(\\log^2k)$.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 15:57:25 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Ene", "Alina", ""], ["Nagarajan", "Viswanath", ""], ["Saket", "Rishi", ""]]}, {"id": "1610.01132", "submitter": "Tengyu Ma", "authors": "Elad Hazan, Tengyu Ma", "title": "A Non-generative Framework and Convex Relaxations for Unsupervised\n  Learning", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a novel formal theoretical framework for unsupervised learning with\ntwo distinctive characteristics. First, it does not assume any generative model\nand based on a worst-case performance metric. Second, it is comparative, namely\nperformance is measured with respect to a given hypothesis class. This allows\nto avoid known computational hardness results and improper algorithms based on\nconvex relaxations. We show how several families of unsupervised learning\nmodels, which were previously only analyzed under probabilistic assumptions and\nare otherwise provably intractable, can be efficiently learned in our framework\nby convex optimization.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 19:22:44 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 00:30:02 GMT"}, {"version": "v3", "created": "Tue, 27 Dec 2016 20:59:01 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Hazan", "Elad", ""], ["Ma", "Tengyu", ""]]}, {"id": "1610.01150", "submitter": "Michel Broniatowski", "authors": "Assia Boumahdaf (LSTA), Michel Broniatowski (LSTA)", "title": "A recursive algorithm for a pipeline maintenance scheduling problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of preventive maintenance (PM) scheduling\nof pipelines subject to external corrosion defects. The preventive maintenance\nstrategy involves an inspection step at some epoch, together with a repair\nschedule. This paper proposes to determine the repair schedule as well as an\ninspection time minimizing the maintenance cost. This problem is formulated as\na binary integer non-linear programming model and we approach it under a\ndecision support framework. We derive a polynomial-time algorithm that computes\nthe optimum PM schedule and suggests different PM strategies in order to assist\npractitioners in making decision.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 13:16:52 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Boumahdaf", "Assia", "", "LSTA"], ["Broniatowski", "Michel", "", "LSTA"]]}, {"id": "1610.01175", "submitter": "Guangwu Xu", "authors": "Guangwu Xu, Bao Li", "title": "On the Algorithmic Significance and Analysis of the Method of DaYan\n  Deriving One", "comments": "9 Pages, in Chinese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modulo inverse is an important arithmetic operation. Many famous algorithms\nin public key cryptography require to compute modulo inverse. It is argued that\nthe method of DaYan deriving one of Jiushao Qin provides the most concise and\ntransparent way of computing modulo inverse. Based on the rule of taking the\nleast positive remainder in division, this paper presents a more precise\nalgorithmic description of the method of DaYan deriving one to reflect Qin's\noriginal idea. Our form of the algorithm is straightforward and different from\nthe ones in the literature. Some additional information can be revealed easily\nfrom the process of DaYan deriving one, e.g., the invariance property of the\npermanent of the state, natural connection to continued fractions. Comparison\nof Qin'a algorithm and the modern form of the Extended Euclidean algorithm is\nalso given. Since DaYan deriving one is the key technical ingredient of Jiushao\nQin's DaYan aggregation method (aka the Chinese Remainder Theorem), we include\nsome explanation to the latter as well.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 20:04:12 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 19:50:59 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Xu", "Guangwu", ""], ["Li", "Bao", ""]]}, {"id": "1610.01753", "submitter": "Yann Disser", "authors": "Yann Disser and Frank Mousset and Andreas Noever and Nemanja\n  \\v{S}kori\\'c and Angelika Steger", "title": "A general lower bound for collaborative tree exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider collaborative graph exploration with a set of $k$ agents. All\nagents start at a common vertex of an initially unknown graph and need to\ncollectively visit all other vertices. We assume agents are deterministic,\nvertices are distinguishable, moves are simultaneous, and we allow agents to\ncommunicate globally. For this setting, we give the first non-trivial lower\nbounds that bridge the gap between small ($k \\leq \\sqrt n$) and large ($k \\geq\nn$) teams of agents. Remarkably, our bounds tightly connect to existing results\nin both domains.\n  First, we significantly extend a lower bound of $\\Omega(\\log k / \\log\\log k)$\nby Dynia et al. on the competitive ratio of a collaborative tree exploration\nstrategy to the range $k \\leq n \\log^c n$ for any $c \\in \\mathbb{N}$. Second,\nwe provide a tight lower bound on the number of agents needed for any\ncompetitive exploration algorithm. In particular, we show that any\ncollaborative tree exploration algorithm with $k = Dn^{1+o(1)}$ agents has a\ncompetitive ratio of $\\omega(1)$, while Dereniowski et al. gave an algorithm\nwith $k = Dn^{1+\\varepsilon}$ agents and competitive ratio $O(1)$, for any\n$\\varepsilon > 0$ and with $D$ denoting the diameter of the graph. Lastly, we\nshow that, for any exploration algorithm using $k = n$ agents, there exist\ntrees of arbitrarily large height $D$ that require $\\Omega(D^2)$ rounds, and we\nprovide a simple algorithm that matches this bound for all trees.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 07:07:59 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Disser", "Yann", ""], ["Mousset", "Frank", ""], ["Noever", "Andreas", ""], ["\u0160kori\u0107", "Nemanja", ""], ["Steger", "Angelika", ""]]}, {"id": "1610.01861", "submitter": "Pascal Lenzner", "authors": "Tobias Friedrich and Sven Ihde and Christoph Ke{\\ss}ler and Pascal\n  Lenzner and Stefan Neubert and David Schumann", "title": "Efficient Best-Response Computation for Strategic Network Formation\n  under Attack", "comments": "A brief announcement of the paper will appear at SPAA'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DM cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by real world examples, e.g. the Internet, researchers have\nintroduced an abundance of strategic games to study natural phenomena in\nnetworks. Unfortunately, almost all of these games have the conceptual drawback\nof being computationally intractable, i.e. computing a best response strategy\nor checking if an equilibrium is reached is NP-hard. Thus, a main challenge in\nthe field is to find tractable realistic network formation models.\n  We address this challenge by investigating a very recently introduced model\nby Goyal et al. [WINE'16] which focuses on robust networks in the presence of a\nstrong adversary who attacks (and kills) nodes in the network and lets this\nattack spread virus-like to neighboring nodes and their neighbors. Our main\nresult is to establish that this natural model is one of the few exceptions\nwhich are both realistic and computationally tractable. In particular, we\nanswer an open question of Goyal et al. by providing an efficient algorithm for\ncomputing a best response strategy, which implies that deciding whether the\ngame has reached a Nash equilibrium can be done efficiently as well. Our\nalgorithm essentially solves the problem of computing a minimal connection to a\nnetwork which maximizes the reachability while hedging against severe attacks\non the network infrastructure and may thus be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 13:23:08 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 13:11:16 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Friedrich", "Tobias", ""], ["Ihde", "Sven", ""], ["Ke\u00dfler", "Christoph", ""], ["Lenzner", "Pascal", ""], ["Neubert", "Stefan", ""], ["Schumann", "David", ""]]}, {"id": "1610.01865", "submitter": "David Tian", "authors": "Wenhong Tian", "title": "On Equivalent Color Transform and Four Coloring Theorem", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we apply an equivalent color transform (ECT) for a minimal\n$k$-coloring of any graph $G$. It contracts each color class of the graph to a\nsingle vertex and produces a complete graph $K_k$ for $G$ by removing redundant\nedges between any two vertices. Based on ECT, a simple proof for four color\ntheorem for planar graph is then proposed.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 13:28:05 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Tian", "Wenhong", ""]]}, {"id": "1610.01876", "submitter": "Vladimir Deineko Dr", "authors": "Vladimir Deineko", "title": "A framework for small but rich vehicle routing problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a 2-vehicle routing problem which can be viewed as\na building block for the varieties of the vehicle routing problems (VRPs). To\napproach this problem, we suggest a framework based on the Held and Karp\ndynamic programming algorithm for the classical travelling salesman problem. An\nalgorithm based on this framework shows an exceptionally good performance on\npublished test data. Our approach can be easily extended to a variety of\nconstraints/attributes in the VRP, hence the wording \"small but rich\" in the\ntitle of our paper.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 11:59:50 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Deineko", "Vladimir", ""]]}, {"id": "1610.01959", "submitter": "Panos P. Markopoulos", "authors": "Panos P. Markopoulos, Sandipan Kundu, Shubham Chamadia, Dimitris A.\n  Pados", "title": "Efficient L1-Norm Principal-Component Analysis via Bit Flipping", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2708023", "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was shown recently that the $K$ L1-norm principal components (L1-PCs) of a\nreal-valued data matrix $\\mathbf X \\in \\mathbb R^{D \\times N}$ ($N$ data\nsamples of $D$ dimensions) can be exactly calculated with cost\n$\\mathcal{O}(2^{NK})$ or, when advantageous, $\\mathcal{O}(N^{dK - K + 1})$\nwhere $d=\\mathrm{rank}(\\mathbf X)$, $K<d$ [1],[2]. In applications where\n$\\mathbf X$ is large (e.g., \"big\" data of large $N$ and/or \"heavy\" data of\nlarge $d$), these costs are prohibitive. In this work, we present a novel\nsuboptimal algorithm for the calculation of the $K < d$ L1-PCs of $\\mathbf X$\nof cost $\\mathcal O(ND \\mathrm{min} \\{ N,D\\} + N^2(K^4 + dK^2) + dNK^3)$, which\nis comparable to that of standard (L2-norm) PC analysis. Our theoretical and\nexperimental studies show that the proposed algorithm calculates the exact\noptimal L1-PCs with high frequency and achieves higher value in the L1-PC\noptimization metric than any known alternative algorithm of comparable\ncomputational cost. The superiority of the calculated L1-PCs over standard\nL2-PCs (singular vectors) in characterizing potentially faulty\ndata/measurements is demonstrated with experiments on data dimensionality\nreduction and disease diagnosis from genomic data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 17:20:16 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Markopoulos", "Panos P.", ""], ["Kundu", "Sandipan", ""], ["Chamadia", "Shubham", ""], ["Pados", "Dimitris A.", ""]]}, {"id": "1610.01980", "submitter": "Tengyu Ma", "authors": "Tengyu Ma, Jonathan Shi, David Steurer", "title": "Polynomial-time Tensor Decompositions with Sum-of-Squares", "comments": "to appear in FOCS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new algorithms based on the sum-of-squares method for tensor\ndecomposition. Our results improve the best known running times from\nquasi-polynomial to polynomial for several problems, including decomposing\nrandom overcomplete 3-tensors and learning overcomplete dictionaries with\nconstant relative sparsity. We also give the first robust analysis for\ndecomposing overcomplete 4-tensors in the smoothed analysis model. A key\ningredient of our analysis is to establish small spectral gaps in moment\nmatrices derived from solutions to sum-of-squares relaxations. To enable this\nanalysis we augment sum-of-squares relaxations with spectral analogs of maximum\nentropy constraints.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 18:18:54 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Ma", "Tengyu", ""], ["Shi", "Jonathan", ""], ["Steurer", "David", ""]]}, {"id": "1610.02056", "submitter": "Shi Li", "authors": "Shi Li", "title": "Constant Approximation Algorithm for Non-Uniform Capacitated Multi-Item\n  Lot-Sizing via Strong Covering Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the non-uniform capacitated multi-item lot-sizing (\\lotsizing)\nproblem. In this problem, there is a set of demands over a planning horizon of\n$T$ time periods and all demands must be satisfied on time. We can place an\norder at the beginning of each period $s$, incurring an ordering cost $K_s$.\nThe total quantity of all products ordered at time $s$ can not exceed a given\ncapacity $C_s$. On the other hand, carrying inventory from time to time incurs\ninventory holding cost. The goal of the problem is to find a feasible solution\nthat minimizes the sum of ordering and holding costs.\n  Levi et al.\\ (Levi, Lodi and Sviridenko, Mathmatics of Operations Research\n33(2), 2008) gave a 2-approximation for the problem when the capacities $C_s$\nare the same. In this paper, we extend their result to the case of non-uniform\ncapacities. That is, we give a constant approximation algorithm for the\ncapacitated multi-item lot-sizing problem with general capacities.\n  The constant approximation is achieved by adding an exponentially large set\nof new covering inequalities to the natural facility-location type linear\nprogramming relaxation for the problem. Along the way of our algorithm, we\nreduce the \\lotsizing problem to two generalizations of the classic knapsack\ncovering problem. We give LP-based constant approximation algorithms for both\ngeneralizations, via the iterative rounding technique.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 20:18:45 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Li", "Shi", ""]]}, {"id": "1610.02066", "submitter": "John Michael Goddard Kallaugher", "authors": "John Kallaugher and Eric Price", "title": "A Hybrid Sampling Scheme for Triangle Counting", "comments": "48 pages. Fixed an error in the statement of the hubs lower bound", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the number of triangles in a graph stream.\nNo streaming algorithm can get sublinear space on all graphs, so methods in\nthis area bound the space in terms of parameters of the input graph such as the\nmaximum number of triangles sharing a single edge. We give a sampling algorithm\nthat is additionally parameterized by the maximum number of triangles sharing a\nsingle vertex. Our bound matches the best known turnstile results in all\ngraphs, and gets better performance on simple graphs like $G(n, p)$ or a set of\nindependent triangles.\n  We complement the upper bound with a lower bound showing that no sampling\nalgorithm can do better on those graphs by more than a log factor. In\nparticular, any insertion stream algorithm must use $\\sqrt{T}$ space when all\nthe triangles share a common vertex, and any sampling algorithm must take\n$T^\\frac{1}{3}$ samples when all the triangles are independent. We add another\nlower bound, also matching our algorithm's performance, which applies to all\ngraph classes. This lower bound covers \"triangle-dependent\" sampling\nalgorithms, a subclass that includes our algorithm and all previous sampling\nalgorithms for the problem.\n  Finally, we show how to generalize our algorithm to count arbitrary subgraphs\nof constant size.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 21:00:01 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 20:27:40 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 23:42:36 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Kallaugher", "John", ""], ["Price", "Eric", ""]]}, {"id": "1610.02072", "submitter": "G\\'abor Braun", "authors": "G\\'abor Braun and Sebastian Pokutta", "title": "An efficient high-probability algorithm for Linear Bandits", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the linear bandit problem, we extend the analysis of algorithm CombEXP\nfrom [R. Combes, M. S. Talebi Mazraeh Shahi, A. Proutiere, and M. Lelarge.\nCombinatorial bandits revisited. In C. Cortes, N. D. Lawrence, D. D. Lee, M.\nSugiyama, and R. Garnett, editors, Advances in Neural Information Processing\nSystems 28, pages 2116--2124. Curran Associates, Inc., 2015. URL\nhttp://papers.nips.cc/paper/5831-combinatorial-bandits-revisited.pdf] to the\nhigh-probability case against adaptive adversaries, allowing actions to come\nfrom an arbitrary polytope. We prove a high-probability regret of\n\\(O(T^{2/3})\\) for time horizon \\(T\\). While this bound is weaker than the\noptimal \\(O(\\sqrt{T})\\) bound achieved by GeometricHedge in [P. L. Bartlett, V.\nDani, T. Hayes, S. Kakade, A. Rakhlin, and A. Tewari. High-probability regret\nbounds for bandit online linear optimization. In 21th Annual Conference on\nLearning Theory (COLT 2008), July 2008.\nhttp://eprints.qut.edu.au/45706/1/30-Bartlett.pdf], CombEXP is computationally\nefficient, requiring only an efficient linear optimization oracle over the\nconvex hull of the actions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 21:14:16 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 16:09:41 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Braun", "G\u00e1bor", ""], ["Pokutta", "Sebastian", ""]]}, {"id": "1610.02132", "submitter": "Jerry Li", "authors": "Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, Milan Vojnovic", "title": "QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel implementations of stochastic gradient descent (SGD) have received\nsignificant research attention, thanks to excellent scalability properties of\nthis algorithm, and to its efficiency in the context of training deep neural\nnetworks. A fundamental barrier for parallelizing large-scale SGD is the fact\nthat the cost of communicating the gradient updates between nodes can be very\nlarge. Consequently, lossy compression heuristics have been proposed, by which\nnodes only communicate quantized gradients. Although effective in practice,\nthese heuristics do not always provably converge, and it is not clear whether\nthey are optimal.\n  In this paper, we propose Quantized SGD (QSGD), a family of compression\nschemes which allow the compression of gradient updates at each node, while\nguaranteeing convergence under standard assumptions. QSGD allows the user to\ntrade off compression and convergence time: it can communicate a sublinear\nnumber of bits per iteration in the model dimension, and can achieve\nasymptotically optimal communication cost. We complement our theoretical\nresults with empirical data, showing that QSGD can significantly reduce\ncommunication cost, while being competitive with standard uncompressed\ntechniques on a variety of real tasks.\n  In particular, experiments show that gradient quantization applied to\ntraining of deep neural networks for image classification and automated speech\nrecognition can lead to significant reductions in communication cost, and\nend-to-end training time. For instance, on 16 GPUs, we are able to train a\nResNet-152 network on ImageNet 1.8x faster to full accuracy. Of note, we show\nthat there exist generic parameter settings under which all known network\narchitectures preserve or slightly improve their full accuracy when using\nquantization.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 03:44:34 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 22:25:06 GMT"}, {"version": "v3", "created": "Thu, 25 May 2017 08:05:19 GMT"}, {"version": "v4", "created": "Wed, 6 Dec 2017 18:28:32 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Alistarh", "Dan", ""], ["Grubic", "Demjan", ""], ["Li", "Jerry", ""], ["Tomioka", "Ryota", ""], ["Vojnovic", "Milan", ""]]}, {"id": "1610.02336", "submitter": "Ramin Yarinezhad", "authors": "Ramin Yarinezhad, Seyed Naser Hashemi", "title": "Approximation Algorithms for Multi-Multiway Cut and Multicut Problems on\n  Directed Graphs", "comments": "This version is completely correct and does not have any problem. All\n  mistakes are corrected in this version. AUT Journal of Mathematics and\n  Computing (2020)", "journal-ref": null, "doi": "10.22060/AJMC.2018.15109.1014", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present two approximation algorithms for the directed\nmulti-multiway cut and directed multicut problems. The so called region growing\nparadigm \\cite{1} is modified and used for these two cut problems on directed\ngraphs. By using this paradigm, we give for each problem an approximation\nalgorithm such that both algorithms have the approximate factor $O(k)$ the same\nas the previous works done on these problems. However, the previous works need\nto solve $k$ linear programming, whereas our algorithms require only one linear\nprogramming. Therefore, our algorithms improve the running time of the previous\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 16:09:43 GMT"}, {"version": "v2", "created": "Sat, 3 Feb 2018 17:48:59 GMT"}, {"version": "v3", "created": "Wed, 7 Feb 2018 20:06:07 GMT"}, {"version": "v4", "created": "Thu, 28 Mar 2019 17:21:43 GMT"}, {"version": "v5", "created": "Sat, 30 Mar 2019 12:36:40 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Yarinezhad", "Ramin", ""], ["Hashemi", "Seyed Naser", ""]]}, {"id": "1610.02361", "submitter": "Jan Hackfeld", "authors": "Andreas B\\\"artschi and J\\'er\\'emie Chalopin and Shantanu Das and Yann\n  Disser and Daniel Graf and Jan Hackfeld and Paolo Penna", "title": "Energy-efficient Delivery by Heterogeneous Mobile Agents", "comments": null, "journal-ref": "34th International Symposium on Theoretical Aspects of Computer\n  Science, STACS'17, 10:1-14, 2017", "doi": "10.4230/LIPIcs.STACS.2017.10", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of delivering $m$ messages between specified\nsource-target pairs in a weighted undirected graph, by $k$ mobile agents\ninitially located at distinct nodes of the graph. Each agent consumes energy\nproportional to the distance it travels in the graph and we are interested in\noptimizing the total energy consumption for the team of agents. Unlike previous\nrelated work, we consider heterogeneous agents with different rates of energy\nconsumption (weights~$w_i$). To solve the delivery problem, agents face three\nmajor challenges: \\emph{Collaboration} (how to work together on each message),\n\\emph{Planning} (which route to take) and \\emph{Coordination} (how to assign\nagents to messages).\n  We first show that the delivery problem can be 2-approximated \\emph{without}\ncollaborating and that this is best possible, i.e., we show that the\n\\emph{benefit of collaboration} is 2 in general. We also show that the benefit\nof collaboration for a single message is~$1/\\ln 2 \\approx 1.44$. Planning turns\nout to be \\NP-hard to approximate even for a single agent, but can be\n2-approximated in polynomial time if agents have unit capacities and do not\ncollaborate. We further show that coordination is \\NP-hard even for agents with\nunit capacity, but can be efficiently solved exactly if they have uniform\nweights. Finally, we give a polynomial-time\n$(4\\max\\tfrac{w_i}{w_j})$-approximation for message delivery with unit\ncapacities.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 18:05:36 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["B\u00e4rtschi", "Andreas", ""], ["Chalopin", "J\u00e9r\u00e9mie", ""], ["Das", "Shantanu", ""], ["Disser", "Yann", ""], ["Graf", "Daniel", ""], ["Hackfeld", "Jan", ""], ["Penna", "Paolo", ""]]}, {"id": "1610.02420", "submitter": "David Harris", "authors": "David G. Harris", "title": "Lopsidependency in the Moser-Tardos framework: Beyond the Lopsided\n  Lovasz Local Lemma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lopsided Lov\\'{a}sz Local Lemma (LLLL) is a powerful probabilistic\nprinciple which has been used in a variety of combinatorial constructions.\nWhile originally a general statement about probability spaces, it has recently\nbeen transformed into a variety of polynomial-time algorithms. The resampling\nalgorithm of Moser & Tardos (2010) is the most well-known example of this. A\nvariety of criteria have been shown for the LLLL; the strongest possible\ncriterion was shown by Shearer, and other criteria which are easier to use\ncomputationally have been shown by Bissacot et al (2011), Pegden (2014),\nKolipaka & Szegedy (2011), and Kolipaka, Szegedy, Xu (2012).\n  We show a new criterion for the Moser-Tardos algorithm to converge. This\ncriterion is stronger than the LLLL criterion; this is possible because it does\nnot apply in the same generality as the original LLLL; yet, it is strong enough\nto cover many applications of the LLLL in combinatorics. We show a variety of\nnew bounds and algorithms. A noteworthy application is for $k$-SAT, with\nbounded occurrences of variables. As shown in Gebauer, Sz\\'{a}bo, and Tardos\n(2011), a $k$-SAT instance in which every variable appears $L \\leq\n\\frac{2^{k+1}}{e (k+1)}$ times, is satisfiable. Although this bound is\nasymptotically tight (in $k$), we improve it to $L \\leq \\frac{2^{k+1} (1 -\n1/k)^k}{k-1} - \\frac{2}{k}$ which can be significantly stronger when $k$ is\nsmall.\n  We introduce a new parallel algorithm for the LLLL. While Moser & Tardos\ndescribed a simple parallel algorithm for the Lov\\'{a}sz Local Lemma, and\ndescribed a simple sequential algorithm for a form of the Lopsided Lemma, they\nwere not able to combine the two. Our new algorithm applies in nearly all\nsettings in which the sequential algorithm works --- this includes settings\ncovered by our new stronger LLLL criterion.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 20:43:04 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 12:45:12 GMT"}, {"version": "v3", "created": "Tue, 1 Nov 2016 13:59:23 GMT"}, {"version": "v4", "created": "Thu, 1 Dec 2016 15:38:55 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Harris", "David G.", ""]]}, {"id": "1610.02575", "submitter": "Martin Zand Martin S. Zand", "authors": "Martin S. Zand, Melissa Trayhan, Samir A. Farooq, Christopher Fucile,\n  Grourab Ghoshal, Robert J. White, Caroline M. Quill, Alexander Rosenberg,\n  Hugo Serrano, Hassan Chafi and Timothy Boudreau", "title": "Properties of Healthcare Teaming Networks as a Function of Network\n  Construction Algorithms", "comments": "With links to comprehensive, high resolution figures and networks via\n  figshare.com", "journal-ref": null, "doi": "10.1371/journal.pone.0175876", "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network models of healthcare systems can be used to examine how providers\ncollaborate, communicate, refer patients to each other. Most healthcare service\nnetwork models have been constructed from patient claims data, using billing\nclaims to link patients with providers. The data sets can be quite large,\nmaking standard methods for network construction computationally challenging\nand thus requiring the use of alternate construction algorithms. While these\nalternate methods have seen increasing use in generating healthcare networks,\nthere is little to no literature comparing the differences in the structural\nproperties of the generated networks. To address this issue, we compared the\nproperties of healthcare networks constructed using different algorithms and\nthe 2013 Medicare Part B outpatient claims data. Three different algorithms\nwere compared: binning, sliding frame, and trace-route. Unipartite networks\nlinking either providers or healthcare organizations by shared patients were\nbuilt using each method. We found that each algorithm produced networks with\nsubstantially different topological properties. Provider networks adhered to a\npower law, and organization networks to a power law with exponential cutoff.\nCensoring networks to exclude edges with less than 11 shared patients, a common\nde-identification practice for healthcare network data, markedly reduced edge\nnumbers and greatly altered measures of vertex prominence such as the\nbetweenness centrality. We identified patterns in the distance patients travel\nbetween network providers, and most strikingly between providers in the\nNortheast United States and Florida. We conclude that the choice of network\nconstruction algorithm is critical for healthcare network analysis, and discuss\nthe implications for selecting the algorithm best suited to the type of\nanalysis to be performed.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 20:16:01 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Zand", "Martin S.", ""], ["Trayhan", "Melissa", ""], ["Farooq", "Samir A.", ""], ["Fucile", "Christopher", ""], ["Ghoshal", "Grourab", ""], ["White", "Robert J.", ""], ["Quill", "Caroline M.", ""], ["Rosenberg", "Alexander", ""], ["Serrano", "Hugo", ""], ["Chafi", "Hassan", ""], ["Boudreau", "Timothy", ""]]}, {"id": "1610.02704", "submitter": "Pravesh K Kothari", "authors": "Pravesh K. Kothari, Raghu Meka and Prasad Raghavendra", "title": "Approximating Rectangles by Juntas and Weakly-Exponential Lower Bounds\n  for LP Relaxations of CSPs", "comments": "Fixed bug in the statement of Theorem 1.7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for constraint satisfaction problems (CSPs), sub-exponential\nsize linear programming relaxations are as powerful as $n^{\\Omega(1)}$-rounds\nof the Sherali-Adams linear programming hierarchy. As a corollary, we obtain\nsub-exponential size lower bounds for linear programming relaxations that beat\nrandom guessing for many CSPs such as MAX-CUT and MAX-3SAT. This is a\nnearly-exponential improvement over previous results, previously, it was only\nknown that linear programs of size $n^{o(\\log n)}$ cannot beat random guessing\nfor any CSP (Chan-Lee-Raghavendra-Steurer 2013).\n  Our bounds are obtained by exploiting and extending the recent progress in\ncommunication complexity for \"lifting\" query lower bounds to communication\nproblems. The main ingredient in our results is a new structural result on\n\"high-entropy rectangles\" that may of independent interest in communication\ncomplexity.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 18:11:58 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 06:09:56 GMT"}, {"version": "v3", "created": "Sun, 20 Nov 2016 00:03:59 GMT"}, {"version": "v4", "created": "Sat, 30 Dec 2017 20:02:39 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Kothari", "Pravesh K.", ""], ["Meka", "Raghu", ""], ["Raghavendra", "Prasad", ""]]}, {"id": "1610.02820", "submitter": "May Szedl\\'ak", "authors": "Komei Fukuda, May Szedlak", "title": "Redundancies in Linear Systems with two Variables per Inequality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of detecting and removing redundant constraints is fundamental in\noptimization. We focus on the case of linear programs (LPs), given by $d$\nvariables with $n$ inequality constraints. A constraint is called\n\\emph{redundant}, if after its removal, the LP still has the same feasible\nregion. The currently fastest method to detect all redundancies is due to\nClarkson: it solves $n$ linear programs, but each of them has at most $s$\nconstraints, where $s$ is the number of nonredundant constraints.\n  In this paper, we study the special case where every constraint has at most\ntwo variables with nonzero coefficients. This family, denoted by $LI(2)$, has\nsome nice properties. Namely, as shown by Aspvall and Shiloach, given a\nvariable $x_i$ and a value $\\lambda$, we can test in time $O(nd)$ whether there\nis a feasible solution with $x_i = \\lambda$. Hochbaum and Naor present an\n$O(d^2 n \\log n)$ algorithm for solving the feasibility problem in $LI(2)$.\nTheir technique makes use of the Fourier-Motzkin elimination method and the\nearlier mentioned result by Aspvall and Shiloach.\n  We present a strongly polynomial algorithm that solves redundancy detection\nin time $O(n d^2 s \\log s)$. It uses a modification of Clarkson's algorithm,\ntogether with a revised version of Hochbaum and Naor's technique. Finally we\nshow that dimensionality testing can be done with the same running time as\nsolving feasibility.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 09:30:23 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Fukuda", "Komei", ""], ["Szedlak", "May", ""]]}, {"id": "1610.02841", "submitter": "Fabrizio Frati", "authors": "Fabrizio Frati, Maurizio Patrignani, Vincenzo Roselli", "title": "LR-Drawings of Ordered Rooted Binary Trees and Near-Linear Area Drawings\n  of Outerplanar Graphs", "comments": "A preliminary version appears at SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a family of algorithms, introduced by Chan [SODA 1999]\nand called LR-algorithms, for drawing ordered rooted binary trees. In\nparticular, we are interested in constructing LR-drawings (that are drawings\nobtained via LR-algorithms) with small width. Chan showed three different\nLR-algorithms that achieve, for an ordered rooted binary tree with $n$ nodes,\nwidth $O(n^{0.695})$, width $O(n^{0.5})$, and width $O(n^{0.48})$.\n  We prove that, for every $n$-node ordered rooted binary tree, an LR-drawing\nwith minimum width can be constructed in $O(n^{1.48})$ time. Further, we show\nan infinite family of $n$-node ordered rooted binary trees requiring\n$\\Omega(n^{0.418})$ width in any LR-drawing; no lower bound better than\n$\\Omega(\\log n)$ was previously known. Finally, we present the results of an\nexperimental evaluation that allowed us to determine the minimum width of all\nthe ordered rooted binary trees with up to $451$ nodes.\n  Our interest in LR-drawings is mainly motivated by a result of Di Battista\nand Frati [Algorithmica 2009], who proved that $n$-vertex outerplanar graphs\nhave outerplanar straight-line drawings in $O(n^{1.48})$ area by means of a\ndrawing algorithm which resembles an LR-algorithm.\n  We deepen the connection between LR-drawings and outerplanar straight-line\ndrawings by proving that, if $n$-node ordered rooted binary trees have\nLR-drawings with $f(n)$ width, for any function $f(n)$, then $n$-vertex\nouterplanar graphs have outerplanar straight-line drawings in $O(f(n))$ area.\n  Finally, we exploit a structural decomposition for ordered rooted binary\ntrees introduced by Chan in order to prove that every $n$-vertex outerplanar\ngraph has an outerplanar straight-line drawing in $O(n\\cdot 2^{\\sqrt{2 \\log_2\nn}} \\sqrt{\\log n})$ area.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 10:41:08 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 06:44:27 GMT"}, {"version": "v3", "created": "Sat, 22 Oct 2016 07:36:47 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Frati", "Fabrizio", ""], ["Patrignani", "Maurizio", ""], ["Roselli", "Vincenzo", ""]]}, {"id": "1610.02865", "submitter": "Travis Gagie", "authors": "Travis Gagie, Giovanni Manzini and Rossano Venturini", "title": "An Encoding for Order-Preserving Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoding data structures store enough information to answer the queries they\nare meant to support but not enough to recover their underlying datasets. In\nthis paper we give the first encoding data structure for the challenging\nproblem of order-preserving pattern matching. This problem was introduced only\na few years ago but has already attracted significant attention because of its\napplications in data analysis. Two strings are said to be an order-preserving\nmatch if the {\\em relative order} of their characters is the same: e.g., $4, 1,\n3, 2$ and $10, 3, 7, 5$ are an order-preserving match. We show how, given a\nstring $S [1..n]$ over an arbitrary alphabet and a constant $c \\geq 1$, we can\nbuild an $O (n \\log \\log n)$-bit encoding such that later, given a pattern $P\n[1..m]$ with $m \\leq \\lg^c n$, we can return the number of order-preserving\noccurrences of $P$ in $S$ in $O (m)$ time. Within the same time bound we can\nalso return the starting position of some order-preserving match for $P$ in $S$\n(if such a match exists). We prove that our space bound is within a constant\nfactor of optimal; our query time is optimal if $\\log \\sigma = \\Omega(\\log n)$.\nOur space bound contrasts with the $\\Omega (n \\log n)$ bits needed in the worst\ncase to store $S$ itself, an index for order-preserving pattern matching with\nno restrictions on the pattern length, or an index for standard pattern\nmatching even with restrictions on the pattern length. Moreover, we can build\nour encoding knowing only how each character compares to $O (\\lg^c n)$\nneighbouring characters.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 11:47:05 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 21:51:33 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Gagie", "Travis", ""], ["Manzini", "Giovanni", ""], ["Venturini", "Rossano", ""]]}, {"id": "1610.02953", "submitter": "Michael Fredman", "authors": "Michael L. Fredman", "title": "Comments on Dumitrescu's \"A Selectable Sloppy Heap\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dumitrescu [arXiv:1607.07673] describes a data structure referred to as a\nSelectable Sloppy Heap. We present a simplified approach, and also point out\naspects of Dumitrescu's exposition that require scrutiny.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 15:13:10 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Fredman", "Michael L.", ""]]}, {"id": "1610.02997", "submitter": "Joan Boyar", "authors": "Joan Boyar, Leah Epstein, Lene M. Favrholdt, Kim S. Larsen, Asaf Levin", "title": "Batch Coloring of Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In graph coloring problems, the goal is to assign a positive integer color to\neach vertex of an input graph such that adjacent vertices do not receive the\nsame color assignment. For classic graph coloring, the goal is to minimize the\nmaximum color used, and for the sum coloring problem, the goal is to minimize\nthe sum of colors assigned to all input vertices. In the offline variant, the\nentire graph is presented at once, and in online problems, one vertex is\npresented for coloring at each time, and the only information is the identity\nof its neighbors among previously known vertices. In batched graph coloring,\nvertices are presented in k batches, for a fixed integer k > 1, such that the\nvertices of a batch are presented as a set, and must be colored before the\nvertices of the next batch are presented. This last model is an intermediate\nmodel, which bridges between the two extreme scenarios of the online and\noffline models. We provide several results, including a general result for sum\ncoloring and results for the classic graph coloring problem on restricted graph\nclasses: We show tight bounds for any graph class containing trees as a\nsubclass (e.g., forests, bipartite graphs, planar graphs, and perfect graphs),\nand a surprising result for interval graphs and k = 2, where the value of the\n(strict and asymptotic) competitive ratio depends on whether the graph is\npresented with its interval representation or not.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 17:00:39 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Boyar", "Joan", ""], ["Epstein", "Leah", ""], ["Favrholdt", "Lene M.", ""], ["Larsen", "Kim S.", ""], ["Levin", "Asaf", ""]]}, {"id": "1610.03002", "submitter": "Chi-Kin Chau", "authors": "Areg Karapetyan, Majid Khonji, Chi-Kin Chau, Khaled Elbassioni and H.\n  H. Zeineldin", "title": "Efficient Algorithm for Scalable Event-based Demand Response Management\n  in Microgrids", "comments": "To appear in IEEE Transactions on Smart Grid", "journal-ref": "IEEE Transactions on Smart Grid ( Volume: 9, Issue: 4, July 2018 )\n  Pages: 2714 - 2725", "doi": "10.1109/TSG.2016.2616945", "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demand response management has become one of the key enabling technologies\nfor smart grids. Motivated by the increasing demand response incentives offered\nby service operators, more customers are subscribing to various demand response\nprograms. However, with growing customer participation, the problem of\ndetermining the optimal loads to be curtailed in a microgrid during\ncontingencies within a feasible time frame becomes computationally hard. This\npaper proposes an efficient approximation algorithm for event-based demand\nresponse management in microgrids. In event-based management, it is important\nto curtail loads as fast as possible to maintain the stability of a microgrid\nduring the islanded mode in a scalable manner. A simple greedy approach is\npresented that can rapidly determine a close-to-optimal load curtailment scheme\nto maximize the aggregate customer utility in milliseconds for a large number\nof customers. This paper further derives a novel theoretical guarantee of the\ngap between the proposed efficient algorithm and the optimal solution (that may\nbe computationally hard to obtain). The performance of algorithm is\ncorroborated extensively by simulations with up to thousands of customers. For\nthe sake of practicality, the proposed event-based demand response management\nalgorithm is applied to a feeder from the Canadian benchmark distribution\nsystem. The simulation results demonstrate that the proposed approach\nefficiently optimizes microgrid operation during islanded mode while\nmaintaining appropriate voltage levels and network constrains.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 17:28:08 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Karapetyan", "Areg", ""], ["Khonji", "Majid", ""], ["Chau", "Chi-Kin", ""], ["Elbassioni", "Khaled", ""], ["Zeineldin", "H. H.", ""]]}, {"id": "1610.03007", "submitter": "Timo Bingmann", "authors": "Timo Bingmann, Simon Gog, and Florian Kurpicz", "title": "Scalable Construction of Text Indexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The suffix array is the key to efficient solutions for myriads of string\nprocessing problems in different applications domains, like data compression,\ndata mining, or Bioinformatics. With the rapid growth of available data, suffix\narray construction algorithms had to be adapted to advanced computational\nmodels such as external memory and distributed computing. In this article, we\npresent five suffix array construction algorithms utilizing the new algorithmic\nbig data batch processing framework Thrill, which allows us to process input\nsizes in orders of magnitude that have not been considered before.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 17:40:26 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Bingmann", "Timo", ""], ["Gog", "Simon", ""], ["Kurpicz", "Florian", ""]]}, {"id": "1610.03125", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Pawe{\\l} Gawrychowski, Oleg Merkurev, Arseny M. Shur, Przemys{\\l}aw\n  Uzna\\'nski", "title": "Tight Tradeoffs for Real-Time Approximation of Longest Palindromes in\n  Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider computing a longest palindrome in the streaming model, where the\nsymbols arrive one-by-one and we do not have random access to the input. While\ncomputing the answer exactly using sublinear space is not possible in such a\nsetting, one can still hope for a good approximation guarantee. Our\ncontribution is twofold. First, we provide lower bounds on the space\nrequirements for randomized approximation algorithms processing inputs of\nlength $n$. We rule out Las Vegas algorithms, as they cannot achieve sublinear\nspace complexity. For Monte Carlo algorithms, we prove a lower bounds of\n$\\Omega( M \\log\\min\\{|\\Sigma|,M\\})$ bits of memory; here $M=n/E$ for\napproximating the answer with additive error $E$, and $M= \\frac{\\log n}{\\log\n(1+\\varepsilon)}$ for approximating the answer with multiplicative error $(1 +\n\\varepsilon)$. Second, we design three real-time algorithms for this problem.\nOur Monte Carlo approximation algorithms for both additive and multiplicative\nversions of the problem use $O(M)$ words of memory. Thus the obtained lower\nbounds are asymptotically tight up to a logarithmic factor. The third algorithm\nis deterministic and finds a longest palindrome exactly if it is short. This\nalgorithm can be run in parallel with a Monte Carlo algorithm to obtain better\nresults in practice. Overall, both the time and space complexity of finding a\nlongest palindrome in a stream are essentially settled.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 23:19:10 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Merkurev", "Oleg", ""], ["Shur", "Arseny M.", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1610.03266", "submitter": "Qian Li", "authors": "Qian Li, Xiaoming Sun, and Jialin Zhang", "title": "On the Optimality of Tape Merge of Two Lists with Similar Size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of merging sorted lists in the least number of pairwise\ncomparisons has been solved completely only for a few special cases. Graham and\nKarp \\cite{taocp} independently discovered that the tape merge algorithm is\noptimal in the worst case when the two lists have the same size. In the seminal\npapers, Stockmeyer and Yao\\cite{yao}, Murphy and Paull\\cite{3k3}, and\nChristen\\cite{christen1978optimality} independently showed when the lists to be\nmerged are of size $m$ and $n$ satisfying $m\\leq\nn\\leq\\lfloor\\frac{3}{2}m\\rfloor+1$, the tape merge algorithm is optimal in the\nworst case. This paper extends this result by showing that the tape merge\nalgorithm is optimal in the worst case whenever the size of one list is no\nlarger than 1.52 times the size of the other. The main tool we used to prove\nlower bounds is Knuth's adversary methods \\cite{taocp}. In addition, we show\nthat the lower bound cannot be improved to 1.8 via Knuth's adversary methods.\nWe also develop a new inequality about Knuth's adversary methods, which might\nbe interesting in its own right. Moreover, we design a simple procedure to\nachieve constant improvement of the upper bounds for $2m-2\\leq n\\leq 3m $.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 10:21:28 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Li", "Qian", ""], ["Sun", "Xiaoming", ""], ["Zhang", "Jialin", ""]]}, {"id": "1610.03298", "submitter": "Robert Ganian", "authors": "Robert Ganian and M. S. Ramanujan and Stefan Szeider", "title": "Combining Treewidth and Backdoors for CSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that CSP is fixed-parameter tractable when parameterized by the\ntreewidth of a backdoor into any tractable CSP problem over a finite constraint\nlanguage. This result combines the two prominent approaches for achieving\ntractability for CSP: (i) by structural restrictions on the interaction between\nthe variables and the constraints and (ii) by language restrictions on the\nrelations that can be used inside the constraints. Apart from defining the\nnotion of backdoor-treewidth and showing how backdoors of small treewidth can\nbe used to efficiently solve CSP, our main technical contribution is a\nfixed-parameter algorithm that finds a backdoor of small treewidth.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 12:11:53 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Ganian", "Robert", ""], ["Ramanujan", "M. S.", ""], ["Szeider", "Stefan", ""]]}, {"id": "1610.03317", "submitter": "Hsiang-Fu Yu", "authors": "Hsiang-Fu Yu, Cho-Jui Hsieh, Qi Lei, and Inderjit S. Dhillon", "title": "A Greedy Approach for Budgeted Maximum Inner Product Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum Inner Product Search (MIPS) is an important task in many machine\nlearning applications such as the prediction phase of a low-rank matrix\nfactorization model for a recommender system. There have been some works on how\nto perform MIPS in sub-linear time recently. However, most of them do not have\nthe flexibility to control the trade-off between search efficient and search\nquality. In this paper, we study the MIPS problem with a computational budget.\nBy carefully studying the problem structure of MIPS, we develop a novel\nGreedy-MIPS algorithm, which can handle budgeted MIPS by design. While simple\nand intuitive, Greedy-MIPS yields surprisingly superior performance compared to\nstate-of-the-art approaches. As a specific example, on a candidate set\ncontaining half a million vectors of dimension 200, Greedy-MIPS runs 200x\nfaster than the naive approach while yielding search results with the top-5\nprecision greater than 75\\%.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 13:10:48 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Yu", "Hsiang-Fu", ""], ["Hsieh", "Cho-Jui", ""], ["Lei", "Qi", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1610.03332", "submitter": "Florian Kurpicz", "authors": "Johannes Fischer, Florian Kurpicz, Peter Sanders", "title": "Engineering a Distributed Full-Text Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a distributed full-text index for big data applications in a\ndistributed environment. Our index can answer different types of pattern\nmatching queries (existential, counting and enumeration). We perform\nexperiments on inputs up to 100 GiB using up to 512 processors, and compare our\nindex with the distributed suffix array by Arroyuelo et al. [Parall. Comput.\n40(9): 471--495, 2014]. The result is that our index answers counting queries\nup to 5.5 times faster than the distributed suffix array, while using about the\nsame space. We also provide a succinct variant of our index that uses only one\nthird of the memory compared with our non-succinct variant, at the expense of\nonly 20% slower query times.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 13:46:54 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 21:39:22 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Fischer", "Johannes", ""], ["Kurpicz", "Florian", ""], ["Sanders", "Peter", ""]]}, {"id": "1610.03337", "submitter": "Travis Gagie", "authors": "Amihood Amir, Alberto Apostolico, Travis Gagie and Gad M. Landau", "title": "String Cadences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We say a string has a cadence if a certain character is repeated at regular\nintervals, possibly with intervening occurrences of that character. We call the\ncadence anchored if the first interval must be the same length as the others.\nWe give a sub-quadratic algorithm for determining whether a string has any\ncadence consisting of at least three occurrences of a character, and a nearly\nlinear algorithm for finding all anchored cadences.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 13:51:00 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Amir", "Amihood", ""], ["Apostolico", "Alberto", ""], ["Gagie", "Travis", ""], ["Landau", "Gad M.", ""]]}, {"id": "1610.03383", "submitter": "David Harris", "authors": "David G. Harris", "title": "Deterministic parallel algorithms for fooling polylogarithmic juntas and\n  the Lovasz Local Lemma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many randomized algorithms can be derandomized efficiently using either the\nmethod of conditional expectations or probability spaces with low (almost-)\nindependence. A series of papers, beginning with Luby (1993) and continuing\nwith Berger & Rompel (1991) and Chari et al. (2000), showed that these\ntechniques can be combined to give deterministic parallel algorithms for\ncombinatorial optimization problems involving sums of $w$-juntas. We improve\nthese algorithms through derandomized variable partitioning and a new code\nconstruction for fooling Fourier characters over $GF(2)$. This reduces the\nprocessor complexity to essentially independent of $w$ while the running time\nis reduced from exponential in $w$ to linear in $w$.\n  As a key subroutine, we give a new algorithm to generate a probability space\nwhich can fool a given set of neighborhoods. Schulman (1992) gave an NC\nalgorithm to do so for neighborhoods of size $w \\leq O(\\log n)$. Our new\nalgorithm is NC1, with essentially optimal time and processor complexity, when\n$w = O(\\log n)$; it remains NC up to $w = \\text{polylog}(n)$. This answers an\nopen problem of Schulman.\n  One major application of these algorithms is an NC algorithm for the\nLov\\'{a}sz Local Lemma. Previous NC algorithms, including the seminal algorithm\nof Moser & Tardos (2010) and the work of Chandrasekaran et. al (2013), required\nthat (essentially) the bad-events could span only $O(\\log n)$ variables; we\nrelax this to $\\text{polylog}(n)$ variables. We use this for an $\\text{NC}^2$\nalgorithm for defective vertex coloring, which works for arbitrary degree\ngraphs.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 15:11:31 GMT"}, {"version": "v10", "created": "Mon, 20 Aug 2018 19:07:59 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 12:13:12 GMT"}, {"version": "v3", "created": "Thu, 20 Oct 2016 15:59:48 GMT"}, {"version": "v4", "created": "Thu, 15 Jun 2017 20:55:31 GMT"}, {"version": "v5", "created": "Thu, 22 Jun 2017 21:47:45 GMT"}, {"version": "v6", "created": "Mon, 29 Jan 2018 15:42:55 GMT"}, {"version": "v7", "created": "Mon, 21 May 2018 20:40:04 GMT"}, {"version": "v8", "created": "Wed, 30 May 2018 20:13:52 GMT"}, {"version": "v9", "created": "Sat, 11 Aug 2018 14:28:58 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Harris", "David G.", ""]]}, {"id": "1610.03412", "submitter": "Jan Schiemann", "authors": "Lasse Kliemann, Jan Schiemann, Anand Srivastav", "title": "Finding Euler Tours in the StrSort Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a first algorithm for finding Euler tours in undirected graphs in\nthe StrSort model. This model is a relaxation of the semi streaming model. The\ngraph is given as a stream of its edges and can only be read sequentially, but\nwhile doing a pass over the stream we are allowed to write out another stream\nwhich will be the input for the next pass. In addition, items in the stream are\nsorted between passes. This model was introduced by Aggarwal et al. in 2004.\nHere we apply this model to the problem of finding an Euler tour in a graph (or\nto decide that the graph does not admit an Euler tour). The algorithm works in\ntwo steps. In the first step, a single pass is conducted while a linear (in the\nnumber of vertices $n$) amount of RAM is required. In the second step,\n$\\mathcal O(\\log(n))$ passes are conducted while only $\\mathcal O(\\log(n))$ RAM\nis required.\n  We use an alteration of the algorithm of Atallah and Vishkin from 1984 for\nfinding Euler tours in parallel. It finds a partition of edge-disjoint circuits\nand arranges them in a tree expressing their connectivity. Then the circuits\nare merged according to this tree. In order to minimize the needed amount of\nRAM, we evade the need to store the entire tree and use techniques suggested by\nAggarwal et al. to exchange information concerning the merging of circuits.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 16:16:05 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Kliemann", "Lasse", ""], ["Schiemann", "Jan", ""], ["Srivastav", "Anand", ""]]}, {"id": "1610.03421", "submitter": "Dominik K\\\"oppl", "authors": "Hideo Bannai and Shunsuke Inenaga and Dominik K\\\"oppl", "title": "Computing All Distinct Squares in Linear Time for Integer Alphabets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a string on an integer alphabet, we present an algorithm that computes\nthe set of all distinct squares belonging to this string in time linear to the\nstring length. As an application, we show how to compute the tree topology of\nthe minimal augmented suffix tree in linear time. Asides from that, we\nelaborate an algorithm computing the longest previous table in a succinct\nrepresentation using compressed working space.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 16:51:46 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 14:31:15 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Bannai", "Hideo", ""], ["Inenaga", "Shunsuke", ""], ["K\u00f6ppl", "Dominik", ""]]}, {"id": "1610.03474", "submitter": "Brandon Fain", "authors": "Brandon Fain, Ashish Goel, Kamesh Munagala", "title": "The Core of the Participatory Budgeting Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CY cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In participatory budgeting, communities collectively decide on the allocation\nof public tax dollars for local public projects. In this work, we consider the\nquestion of fairly aggregating the preferences of community members to\ndetermine an allocation of funds to projects. This problem is different from\nstandard fair resource allocation because of public goods: The allocated goods\nbenefit all users simultaneously. Fairness is crucial in participatory decision\nmaking, since generating equitable outcomes is an important goal of these\nprocesses. We argue that the classic game theoretic notion of core captures\nfairness in the setting. To compute the core, we first develop a novel\ncharacterization of a public goods market equilibrium called the Lindahl\nequilibrium, which is always a core solution. We then provide the first (to our\nknowledge) polynomial time algorithm for computing such an equilibrium for a\nbroad set of utility functions; our algorithm also generalizes (in a\nnon-trivial way) the well-known concept of proportional fairness. We use our\ntheoretical insights to perform experiments on real participatory budgeting\nvoting data. We empirically show that the core can be efficiently computed for\nutility functions that naturally model our practical setting, and examine the\nrelation of the core with the familiar welfare objective. Finally, we address\nconcerns of incentives and mechanism design by developing a randomized\napproximately dominant-strategy truthful mechanism building on the exponential\nmechanism from differential privacy.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 19:37:05 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 13:30:38 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Fain", "Brandon", ""], ["Goel", "Ashish", ""], ["Munagala", "Kamesh", ""]]}, {"id": "1610.03477", "submitter": "Carlos Barron-Romero Prof.", "authors": "Carlos Barr\\'on-Romero", "title": "Complexity and Stop Conditions for NP as General Assignment Problems,\n  the Travel Salesman Problem in $\\mathbb{R}^2$, Knight Tour Problem and\n  Boolean Satisfiability Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents stop conditions for solving General Assignment Problems\n(GAP), in particular for Travel Salesman Problem in an Euclidian 2D space the\nwell known condition Jordan's simple curve and opposite condition for the\nKnight Tour Problem. The Jordan's simple curve condition means that a optimal\ntrajectory must be simple curve, i.e., without crossing but for Knight Tour\nProblem we use the contrary, the feasible trajectory must have crossing in all\ncities of the tour. The paper presents the algorithms, examples and some\nresults come from Concorde's Home page. Several problem are studied to depict\ntheir properties. A classical decision problem SAT is studied in detail.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 03:55:53 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Barr\u00f3n-Romero", "Carlos", ""]]}, {"id": "1610.03564", "submitter": "Rad Niazadeh", "authors": "Rad Niazadeh, Jason Hartline, Nicole Immorlica, Mohammad Reza Khani,\n  Brendan Lucier", "title": "Fast Core Pricing for Rich Advertising Auctions", "comments": "50 pages, 22 figures; forthcoming in Operations Research (2020);\n  conference version presented at The nineteenth ACM conference on Economics\n  and Computation (EC'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard ad auction formats do not immediately extend to settings where\nmultiple size configurations and layouts are available to advertisers. In these\nsettings, the sale of web advertising space increasingly resembles a\ncombinatorial auction with complementarities, where truthful auctions such as\nthe Vickrey-Clarke-Groves (VCG) can yield unacceptably low revenue. We\ntherefore study core selecting auctions, which boost revenue by setting\npayments so that no group of agents, including the auctioneer, can jointly\nimprove their utilities by switching to a different outcome. Our main result is\na combinatorial algorithm that finds an approximate bidder optimal core point\nwith almost linear number of calls to the welfare maximization oracle. Our\nalgorithm is faster than previously-proposed heuristics in the literature and\nhas theoretical guarantees. We conclude that core pricing is implementable even\nfor very time sensitive practical use cases such as realtime auctions for\nonline advertising and can yield more revenue. We justify this claim\nexperimentally using the Microsoft Bing Ad Auction data, through which we show\nour core pricing algorithm generates almost 26% more revenue than VCG on\naverage, about 9% more revenue than other core pricing rules known in the\nliterature, and almost matches the revenue of the standard Generalized Second\nPrice (GSP) auction.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 23:48:00 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 14:19:05 GMT"}, {"version": "v3", "created": "Thu, 8 Mar 2018 01:33:13 GMT"}, {"version": "v4", "created": "Sun, 8 Nov 2020 01:17:50 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Niazadeh", "Rad", ""], ["Hartline", "Jason", ""], ["Immorlica", "Nicole", ""], ["Khani", "Mohammad Reza", ""], ["Lucier", "Brendan", ""]]}, {"id": "1610.03721", "submitter": "Gennaro Cordasco PhD", "authors": "Gennaro Cordasco and Luisa Gargano and Marco Mecchia and Adele A.\n  Rescigno and Ugo Vaccaro", "title": "Discovering Small Target Sets in Social Networks: A Fast and Effective\n  Algorithm", "comments": "A preliminary version of this paper was presented at the 9th Annual\n  International Conference on Combinatorial Optimization and Applications\n  (COCOA'15), December 18-20, 2015, Houston, Texas, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI math.CO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a network represented by a graph $G=(V,E)$, we consider a dynamical\nprocess of influence diffusion in $G$ that evolves as follows: Initially only\nthe nodes of a given $S\\subseteq V$ are influenced; subsequently, at each\nround, the set of influenced nodes is augmented by all the nodes in the network\nthat have a sufficiently large number of already influenced neighbors. The\nquestion is to determine a small subset of nodes $S$ (\\emph{a target set}) that\ncan influence the whole network. This is a widely studied problem that\nabstracts many phenomena in the social, economic, biological, and physical\nsciences. It is known that the above optimization problem is hard to\napproximate within a factor of $2^{\\log^{1-\\epsilon}|V|}$, for any $\\epsilon\n>0$. In this paper, we present a fast and surprisingly simple algorithm that\nexhibits the following features: 1) when applied to trees, cycles, or complete\ngraphs, it always produces an optimal solution (i.e, a minimum size target\nset); 2) when applied to arbitrary networks, it always produces a solution of\ncardinality which improves on the previously known upper bound; 3) when applied\nto real-life networks, it always produces solutions that substantially\noutperform the ones obtained by previously published algorithms (for which no\nproof of optimality or performance guarantee is known in any class of graphs).\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 14:18:04 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Cordasco", "Gennaro", ""], ["Gargano", "Luisa", ""], ["Mecchia", "Marco", ""], ["Rescigno", "Adele A.", ""], ["Vaccaro", "Ugo", ""]]}, {"id": "1610.03774", "submitter": "Rahul Kidambi", "authors": "Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli,\n  Aaron Sidford", "title": "Parallelizing Stochastic Gradient Descent for Least Squares Regression:\n  mini-batching, averaging, and model misspecification", "comments": "39 pages. Published in the Journal of Machine Learning Research\n  (JMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work characterizes the benefits of averaging schemes widely used in\nconjunction with stochastic gradient descent (SGD). In particular, this work\nprovides a sharp analysis of: (1) mini-batching, a method of averaging many\nsamples of a stochastic gradient to both reduce the variance of the stochastic\ngradient estimate and for parallelizing SGD and (2) tail-averaging, a method\ninvolving averaging the final few iterates of SGD to decrease the variance in\nSGD's final iterate. This work presents non-asymptotic excess risk bounds for\nthese schemes for the stochastic approximation problem of least squares\nregression.\n  Furthermore, this work establishes a precise problem-dependent extent to\nwhich mini-batch SGD yields provable near-linear parallelization speedups over\nSGD with batch size one. This allows for understanding learning rate versus\nbatch size tradeoffs for the final iterate of an SGD method. These results are\nthen utilized in providing a highly parallelizable SGD method that obtains the\nminimax risk with nearly the same number of serial updates as batch gradient\ndescent, improving significantly over existing SGD methods. A non-asymptotic\nanalysis of communication efficient parallelization schemes such as\nmodel-averaging/parameter mixing methods is then provided.\n  Finally, this work sheds light on some fundamental differences in SGD's\nbehavior when dealing with agnostic noise in the (non-realizable) least squares\nregression problem. In particular, the work shows that the stepsizes that\nensure minimax risk for the agnostic case must be a function of the noise\nproperties.\n  This paper builds on the operator view of analyzing SGD methods, introduced\nby Defossez and Bach (2015), followed by developing a novel analysis in\nbounding these operators to characterize the excess risk. These techniques are\nof broader interest in analyzing computational aspects of stochastic\napproximation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 16:30:11 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 17:09:30 GMT"}, {"version": "v3", "created": "Sun, 1 Apr 2018 16:20:07 GMT"}, {"version": "v4", "created": "Tue, 31 Jul 2018 17:50:00 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Jain", "Prateek", ""], ["Kakade", "Sham M.", ""], ["Kidambi", "Rahul", ""], ["Netrapalli", "Praneeth", ""], ["Sidford", "Aaron", ""]]}, {"id": "1610.03788", "submitter": "Constantinos Tsirogiannis", "authors": "Frank Staals and Constantinos Tsirogiannis", "title": "Computing the Expected Value and Variance of Geometric Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let P be a set of points in R^d, and let M be a function that maps any subset\nof P to a positive real number. We examine the problem of computing the exact\nmean and variance of M when a subset of points in P is selected according to a\nwell-defined random distribution. We consider two distributions; in the first\ndistribution (which we call the Bernoulli distribution), each point p in P is\nincluded in the random subset independently, with probability pi(p). In the\nsecond distribution (the fixed-size distribution), a subset of exactly s points\nis selected uniformly at random among all possible subsets of s points in P.\n  This problem is a crucial part of modern ecological analyses; each point in P\nrepresents a species in d-dimensional trait space, and the goal is to compute\nthe statistics of a geometric measure on this trait space, when subsets of\nspecies are selected under random processes.\n  We present efficient exact algorithms for computing the mean and variance of\nseveral geometric measures when point sets are selected under one of the\ndescribed random distributions. More specifically, we provide algorithms for\nthe following measures: the bounding box volume, the convex hull volume, the\nmean pairwise distance (MPD), the squared Euclidean distance from the centroid,\nand the diameter of the minimum enclosing disk. We also describe an efficient\n(1-e)-approximation algorithm for computing the mean and variance of the mean\npairwise distance. We implemented three of our algorithms and we show that our\nimplementations can provide major speedups compared to the existing imprecise\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 17:00:21 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Staals", "Frank", ""], ["Tsirogiannis", "Constantinos", ""]]}, {"id": "1610.03999", "submitter": "Florent Foucaud", "authors": "Laurent Beaudou, Florent Foucaud, Reza Naserasr", "title": "Homomorphism bounds and edge-colourings of $K_4$-minor-free graphs", "comments": "27 pages, 10 figures", "journal-ref": "Journal of Combinatorial Theory, Series B 124:128-164 (2017)", "doi": "10.1016/j.jctb.2017.01.001", "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a necessary and sufficient condition for a graph of odd-girth\n$2k+1$ to bound the class of $K_4$-minor-free graphs of odd-girth (at least)\n$2k+1$, that is, to admit a homomorphism from any such $K_4$-minor-free graph.\nThis yields a polynomial-time algorithm to recognize such bounds. Using this\ncondition, we first prove that every $K_4$-minor free graph of odd-girth $2k+1$\nadmits a homomorphism to the projective hypercube of dimension $2k$. This\nsupports a conjecture of the third author which generalizes the four-color\ntheorem and relates to several outstanding conjectures such as Seymour's\nconjecture on edge-colorings of planar graphs. Strengthening this result, we\nshow that the Kneser graph $K(2k+1,k)$ satisfies the conditions, thus implying\nthat every $K_4$-minor free graph of odd-girth $2k+1$ has fractional chromatic\nnumber exactly $2+\\frac{1}{k}$. Knowing that a smallest bound of odd-girth\n$2k+1$ must have at least ${k+2 \\choose 2}$ vertices, we build nearly optimal\nbounds of order $4k^2$. Furthermore, we conjecture that the suprema of the\nfractional and circular chromatic numbers for $K_4$-minor-free graphs of\nodd-girth $2k+1$ are achieved by a same bound of odd-girth $2k+1$. If true,\nthis improves, in the homomorphism order, earlier tight results on the circular\nchromatic number of $K_4$-minor-free graphs. We support our conjecture by\nproving it for the first few cases. Finally, as an application of our work, and\nafter noting that Seymour provided a formula for calculating the edge-chromatic\nnumber of $K_4$-minor-free multigraphs, we show that stronger results can be\nobtained in the case of $K_4$-minor-free regular multigraphs.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 09:52:58 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 15:05:59 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Beaudou", "Laurent", ""], ["Foucaud", "Florent", ""], ["Naserasr", "Reza", ""]]}, {"id": "1610.04010", "submitter": "Keerti Choudhary", "authors": "Surender Baswana, Keerti Choudhary, Liam Roditty", "title": "An efficient strongly connected components algorithm in the fault\n  tolerant model", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of maintaining the strongly connected\ncomponents of a graph in the presence of failures. In particular, we show that\ngiven a directed graph $G=(V,E)$ with $n=|V|$ and $m=|E|$, and an integer value\n$k\\geq 1$, there is an algorithm that computes in $O(2^{k}n\\log^2 n)$ time for\nany set $F$ of size at most $k$ the strongly connected components of the graph\n$G\\setminus F$. The running time of our algorithm is almost optimal since the\ntime for outputting the SCCs of $G\\setminus F$ is at least $\\Omega(n)$. The\nalgorithm uses a data structure that is computed in a preprocessing phase in\npolynomial time and is of size $O(2^{k} n^2)$.\n  Our result is obtained using a new observation on the relation between\nstrongly connected components (SCCs) and reachability. More specifically, one\nof the main building blocks in our result is a restricted variant of the\nproblem in which we only compute strongly connected components that intersect a\ncertain path. Restricting our attention to a path allows us to implicitly\ncompute reachability between the path vertices and the rest of the graph in\ntime that depends logarithmically rather than linearly in the size of the path.\nThis new observation alone, however, is not enough, since we need to find an\nefficient way to represent the strongly connected components using paths. For\nthis purpose we use a mixture of old and classical techniques such as the heavy\npath decomposition of Sleator and Tarjan and the classical Depth-First-Search\nalgorithm. Although, these are by now standard techniques, we are not aware of\nany usage of them in the context of dynamic maintenance of SCCs. Therefore, we\nexpect that our new insights and mixture of new and old techniques will be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 10:20:51 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 07:19:04 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Baswana", "Surender", ""], ["Choudhary", "Keerti", ""], ["Roditty", "Liam", ""]]}, {"id": "1610.04012", "submitter": "Dorit Hochbaum", "authors": "Dorit S. Hochbaum", "title": "Min cost flow on unit capacity networks and convex cost K-flow are as\n  easy as the assignment problem with All-Min-Cuts algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore here surprising links between the time-cost-tradeoff problem and\nthe minimum cost flow problem that lead to faster, strongly polynomial,\nalgorithms for both problems. One of the main results is a new algorithm for\nthe unit capacity min cost flow that culminates decades of efforts to match the\ncomplexity of the fastest strongly polynomial algorithm known for the\nassignment problem.\n  The time cost tradeoff (TCT) problem in project management is to expedite the\ndurations of activities, subject to precedence constraints, in order to achieve\na target project completion time at minimum expediting costs, or, to maximize\nthe net benefit from a reward associated with project completion time\nreduction. We devise here the all-min-cuts procedure, which for a given maximum\nflow, is capable of generating all minimum cuts of equal value very efficiently\nin average $O(m+n \\log n)$ time.\n  The all-min-cuts procedure implies faster algorithms for TCT problems: For a\nproject network on $n$ nodes and $m$ arcs, with $n'$ arcs of finite uniform\nexpediting costs, the run time is $O((n+n')(m+n\\log n))$; for projects with\nreward of $O(K)$ per unit reduction in the project completion time the run time\nis $O((n+K)(m+n\\log n))$.\n  Using the primal-dual relationship between TCT and the minimum cost flow\nproblem (MCF) we generate faster strongly polynomial algorithms for various\ncases of minimum cost flow: For the unit capacity MCF the complexity of our\nalgorithm is $O(n(m+n\\log n))$, which is faster than any other strongly\npolynomial known to date; For a minimum convex (or linear) cost $K$-flow\nproblem our algorithm runs in $O((n+K)(m+n\\log n))$. This complexity of the\nalgorithm for any min cost $O(n)$-flow matches the best known complexity for\nthe assignment problem, $O(n(m+n\\log n))$, yet with a significantly different\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 10:28:13 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Hochbaum", "Dorit S.", ""]]}, {"id": "1610.04055", "submitter": "Leslie Ann Goldberg", "authors": "Andreas Galanis, Leslie Ann Goldberg, and Kuan Yang", "title": "Approximating partition functions of bounded-degree Boolean counting\n  Constraint Satisfaction Problems", "comments": "To appear in JCSS. This version: minor corrections to typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of approximate counting Constraint Satisfaction\nProblems (#CSPs) in a bounded degree setting. Specifically, given a Boolean\nconstraint language $\\Gamma$ and a degree bound $\\Delta$, we study the\ncomplexity of #CSP$_\\Delta(\\Gamma)$, which is the problem of counting\nsatisfying assignments to CSP instances with constraints from $\\Gamma$ and\nwhose variables can appear at most $\\Delta$ times. Our main result shows that:\n(i) if every function in $\\Gamma$ is affine, then #CSP$_\\Delta(\\Gamma)$ is in\nFP for all $\\Delta$, (ii) otherwise, if every function in $\\Gamma$ is in a\nclass called IM$_2$, then for all sufficiently large $\\Delta$,\n#CSP$_\\Delta(\\Gamma)$ is equivalent under approximation-preserving (AP)\nreductions to the counting problem #BIS (the problem of counting independent\nsets in bipartite graphs) (iii) otherwise, for all sufficiently large $\\Delta$,\nit is NP-hard to approximate the number of satisfying assignments of an\ninstance of #CSP$_\\Delta(\\Gamma)$, even within an exponential factor. Our\nresult extends previous results, which apply only in the so-called\n\"conservative\" case.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 12:46:33 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 11:18:27 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 08:20:01 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Galanis", "Andreas", ""], ["Goldberg", "Leslie Ann", ""], ["Yang", "Kuan", ""]]}, {"id": "1610.04069", "submitter": "Shreyas Sekar", "authors": "Elliot Anshelevich and Shreyas Sekar", "title": "Truthful Mechanisms for Matching and Clustering in an Ordinal World", "comments": "To appear in the Proceedings of WINE 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study truthful mechanisms for matching and related problems in a partial\ninformation setting, where the agents' true utilities are hidden, and the\nalgorithm only has access to ordinal preference information. Our model is\nmotivated by the fact that in many settings, agents cannot express the\nnumerical values of their utility for different outcomes, but are still able to\nrank the outcomes in their order of preference. Specifically, we study problems\nwhere the ground truth exists in the form of a weighted graph of agent\nutilities, but the algorithm can only elicit the agents' private information in\nthe form of a preference ordering for each agent induced by the underlying\nweights. Against this backdrop, we design truthful algorithms to approximate\nthe true optimum solution with respect to the hidden weights. Our techniques\nyield universally truthful algorithms for a number of graph problems: a\n1.76-approximation algorithm for Max-Weight Matching, 2-approximation algorithm\nfor Max k-matching, a 6-approximation algorithm for Densest k-subgraph, and a\n2-approximation algorithm for Max Traveling Salesman as long as the hidden\nweights constitute a metric. We also provide improved approximation algorithms\nfor such problems when the agents are not able to lie about their preferences.\nOur results are the first non-trivial truthful approximation algorithms for\nthese problems, and indicate that in many situations, we can design robust\nalgorithms even when the agents may lie and only provide ordinal information\ninstead of precise utilities.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 13:26:35 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 22:56:45 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Anshelevich", "Elliot", ""], ["Sekar", "Shreyas", ""]]}, {"id": "1610.04317", "submitter": "Ankur Moitra", "authors": "Ankur Moitra", "title": "Approximate Counting, the Lovasz Local Lemma and Inference in Graphical\n  Models", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new approach for approximately counting in\nbounded degree systems with higher-order constraints. Our main result is an\nalgorithm to approximately count the number of solutions to a CNF formula\n$\\Phi$ when the width is logarithmic in the maximum degree. This closes an\nexponential gap between the known upper and lower bounds.\n  Moreover our algorithm extends straightforwardly to approximate sampling,\nwhich shows that under Lov\\'asz Local Lemma-like conditions it is not only\npossible to find a satisfying assignment, it is also possible to generate one\napproximately uniformly at random from the set of all satisfying assignments.\nOur approach is a significant departure from earlier techniques in approximate\ncounting, and is based on a framework to bootstrap an oracle for computing\nmarginal probabilities on individual variables. Finally, we give an application\nof our results to show that it is algorithmically possible to sample from the\nposterior distribution in an interesting class of graphical models.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 03:44:12 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 01:30:09 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Moitra", "Ankur", ""]]}, {"id": "1610.04430", "submitter": "Malin Rau", "authors": "Klaus Jansen, Malin Rau", "title": "Improved approximation for two dimensional strip packing with polynomial\n  bounded width", "comments": "Research was supported in part by German Research Foundation (DFG)\n  project JA 612 /14-2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the well-known two-dimensional strip packing problem. Given is a set\nof rectangular axis-parallel items and a strip of width $W$ with infinite\nheight. The objective is to find a packing of these items into the strip, which\nminimizes the packing height. Lately, it has been shown that the lower bound of\n$3/2$ of the absolute approximation ratio can be beaten when we allow a\npseudo-polynomial running-time of type $(n W)^{f(1/\\varepsilon)}$. If $W$ is\npolynomially bounded by the number of items, this is a polynomial running-time.\nWe present a pseudo-polynomial algorithm with approximation ratio $4/3\n+\\varepsilon$ and running time $(n\nW)^{1/\\varepsilon^{\\mathcal{O}(2^{1/\\varepsilon})}}$.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 12:29:31 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 09:52:23 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Jansen", "Klaus", ""], ["Rau", "Malin", ""]]}, {"id": "1610.04583", "submitter": "Alexander Wein", "authors": "Amelia Perry, Alexander S. Wein, Afonso S. Bandeira, Ankur Moitra", "title": "Message-passing algorithms for synchronization problems over compact\n  groups", "comments": "35 pages, 11 figures", "journal-ref": null, "doi": "10.1002/cpa.21750", "report-no": null, "categories": "cs.IT cs.CV cs.DS math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various alignment problems arising in cryo-electron microscopy, community\ndetection, time synchronization, computer vision, and other fields fall into a\ncommon framework of synchronization problems over compact groups such as Z/L,\nU(1), or SO(3). The goal of such problems is to estimate an unknown vector of\ngroup elements given noisy relative observations. We present an efficient\niterative algorithm to solve a large class of these problems, allowing for any\ncompact group, with measurements on multiple 'frequency channels' (Fourier\nmodes, or more generally, irreducible representations of the group). Our\nalgorithm is a highly efficient iterative method following the blueprint of\napproximate message passing (AMP), which has recently arisen as a central\ntechnique for inference problems such as structured low-rank estimation and\ncompressed sensing. We augment the standard ideas of AMP with ideas from\nrepresentation theory so that the algorithm can work with distributions over\ncompact groups. Using standard but non-rigorous methods from statistical\nphysics we analyze the behavior of our algorithm on a Gaussian noise model,\nidentifying phases where the problem is easy, (computationally) hard, and\n(statistically) impossible. In particular, such evidence predicts that our\nalgorithm is information-theoretically optimal in many cases, and that the\nremaining cases show evidence of statistical-to-computational gaps.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 19:05:32 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Perry", "Amelia", ""], ["Wein", "Alexander S.", ""], ["Bandeira", "Afonso S.", ""], ["Moitra", "Ankur", ""]]}, {"id": "1610.04597", "submitter": "Lucas Carvalho Cordeiro", "authors": "William C. da Rosa, Iury V. de Bessa, Lucas C. Cordeiro", "title": "Application of Global Route-Planning Algorithms with Geodesy", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global Route-Planning Algorithms (GRPA) are required to compute paths between\nseveral points located on Earth's surface. A geodesic algorithm is employed as\nan auxiliary tool, increasing the precision of distance calculations. This work\npresents a novel simulator for GRPA, which compares and evaluates three GRPAs\nimplemented to solve the shortest path problem for points located at different\ncities: A*, LPA*, and D*Lite. The performance of each algorithm is investigated\nwith a set of experiments, which are executed to check the answers provided by\nthe algorithms and to compare their execution time. It is shown that GRPAs\nimplementations with consistent heuristics lead to optimal paths. The\nnoticeable differences among those algorithms are related to the time execution\nafter successive executions.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 19:39:10 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["da Rosa", "William C.", ""], ["de Bessa", "Iury V.", ""], ["Cordeiro", "Lucas C.", ""]]}, {"id": "1610.04711", "submitter": "Mithilesh Kumar", "authors": "Mithilesh Kumar, Daniel Lokshtanov", "title": "A $2\\ell k$ Kernel for $\\ell$-Component Order Connectivity", "comments": "14 pages, 1 figure, accepted in IPEC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $\\ell$-Component Order Connectivity problem ($\\ell \\in \\mathbb{N}$),\nwe are given a graph $G$ on $n$ vertices, $m$ edges and a non-negative integer\n$k$ and asks whether there exists a set of vertices $S\\subseteq V(G)$ such that\n$|S|\\leq k$ and the size of the largest connected component in $G-S$ is at most\n$\\ell$. In this paper, we give a kernel for $\\ell$-Component Order Connectivity\nwith at most $2\\ell k$ vertices that takes $n^{\\mathcal{O}(\\ell)}$ time for\nevery constant $\\ell$. On the way to obtaining our kernel, we prove a\ngeneralization of the $q$-Expansion Lemma to weighted graphs. This\ngeneralization may be of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 10:17:35 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Kumar", "Mithilesh", ""], ["Lokshtanov", "Daniel", ""]]}, {"id": "1610.04712", "submitter": "Karl Bringmann", "authors": "Karl Bringmann", "title": "A Near-Linear Pseudopolynomial Time Algorithm for Subset Sum", "comments": "accepted at SODA'17, 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $Z$ of $n$ positive integers and a target value $t$, the Subset\nSum problem asks whether any subset of $Z$ sums to $t$. A textbook\npseudopolynomial time algorithm by Bellman from 1957 solves Subset Sum in time\n$O(nt)$. This has been improved to $O(n \\max Z)$ by Pisinger [J. Algorithms'99]\nand recently to $\\tilde O(\\sqrt{n} t)$ by Koiliaris and Xu [SODA'17].\n  Here we present a simple randomized algorithm running in time $\\tilde\nO(n+t)$. This improves upon a classic algorithm and is likely to be\nnear-optimal, since it matches conditional lower bounds from Set Cover and\nk-Clique.\n  We then use our new algorithm and additional tricks to improve the best known\npolynomial space solution from time $\\tilde O(n^3 t)$ and space $\\tilde O(n^2)$\nto time $\\tilde O(nt)$ and space $\\tilde O(n \\log t)$, assuming the Extended\nRiemann Hypothesis. Unconditionally, we obtain time $\\tilde O(n\nt^{1+\\varepsilon})$ and space $\\tilde O(n t^\\varepsilon)$ for any constant\n$\\varepsilon > 0$.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 10:20:45 GMT"}, {"version": "v2", "created": "Sun, 8 Jan 2017 14:24:00 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Bringmann", "Karl", ""]]}, {"id": "1610.04785", "submitter": "George Rabanca", "authors": "Amotz Bar-Noy and George Rabanca", "title": "Tight Approximation Bounds for the Seminar Assignment Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seminar assignment problem is a variant of the generalized assignment\nproblem in which items have unit size and the amount of space allowed in each\nbin is restricted to an arbitrary set of values. The problem has been shown to\nbe NP-complete and to not admit a PTAS. However, the only constant factor\napproximation algorithm known to date is randomized and it is not guaranteed to\nalways produce a feasible solution.\n  In this paper we show that a natural greedy algorithm outputs a solution with\nvalue within a factor of $(1 - e^{-1})$ of the optimal, and that unless\n$NP\\subseteq DTIME(n^{\\log\\log n})$, this is the best approximation guarantee\nachievable by any polynomial time algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 20:56:20 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Bar-Noy", "Amotz", ""], ["Rabanca", "George", ""]]}, {"id": "1610.04807", "submitter": "Fan Wei", "authors": "Omer Angel and S\\'ebastien Bubeck and Yuval Peres and Fan Wei", "title": "Local max-cut in smoothed polynomial time", "comments": "added some remarks and corollaries; to appear in STOC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1988, Johnson, Papadimitriou and Yannakakis wrote that \"Practically all\nthe empirical evidence would lead us to conclude that finding locally optimal\nsolutions is much easier than solving NP-hard problems\". Since then the\nempirical evidence has continued to amass, but formal proofs of this phenomenon\nhave remained elusive. A canonical (and indeed complete) example is the local\nmax-cut problem, for which no polynomial time method is known. In a\nbreakthrough paper, Etscheid and R\\\"oglin proved that the smoothed complexity\nof local max-cut is quasi-polynomial, i.e., if arbitrary bounded weights are\nrandomly perturbed, a local maximum can be found in $n^{O(\\log n)}$ steps. In\nthis paper we prove smoothed polynomial complexity for local max-cut, thus\nconfirming that finding local optima for max-cut is much easier than solving\nit.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 01:32:48 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 17:07:53 GMT"}, {"version": "v3", "created": "Mon, 10 Apr 2017 18:42:34 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Angel", "Omer", ""], ["Bubeck", "S\u00e9bastien", ""], ["Peres", "Yuval", ""], ["Wei", "Fan", ""]]}, {"id": "1610.04838", "submitter": "Gennaro Cordasco PhD", "authors": "Gennaro Cordasco and Luisa Gargano and Adele Anna Rescigno", "title": "On Finding Small Sets that Influence Large Networks", "comments": "This paper will appear in Social Network Analysis and Mining (SNAM)\n  Journal. A preliminary version of this paper was presented at the 1st\n  International Workshop on Dynamics in Networks (DyNo 2015) in conjunction\n  with the 2016 IEEE/ACM International Conference ASONAM, Paris, France, August\n  25-28, 2015", "journal-ref": "Soc. Netw. Anal. Min. (2016) 6:94", "doi": "10.1007/s13278-016-0408-z", "report-no": null, "categories": "cs.DS cs.SI math.CO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of selecting a minimum size subset of nodes in a\nnetwork, that allows to activate all the nodes of the network. We present a\nfast and simple algorithm that, in real-life networks, produces solutions that\noutperform the ones obtained by using the best algorithms in the literature. We\nalso investigate the theoretical performances of our algorithm and give proofs\nof optimality for some classes of graphs. From an experimental perspective,\nexperiments also show that the performance of the algorithms correlates with\nthe modularity of the analyzed network. Moreover, the more the influence among\ncommunities is hard to propagate, the less the performances of the algorithms\ndiffer. On the other hand, when the network allows some propagation of\ninfluence between different communities, the gap between the solutions returned\nby the proposed algorithm and by the previous algorithms in the literature\nincreases.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 10:08:50 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Cordasco", "Gennaro", ""], ["Gargano", "Luisa", ""], ["Rescigno", "Adele Anna", ""]]}, {"id": "1610.04915", "submitter": "Marcin Bienkowski", "authors": "Marcin Bienkowski, Martin B\\\"ohm, {\\L}ukasz Je\\.z, Pawe{\\l}\n  Lasko\\'s-Grabowski, Jan Marcinkowski, Ji\\v{r}\\'i Sgall, Aleksandra Spyra,\n  Pavel Vesel\\'y", "title": "Logarithmic price of buffer downscaling on line metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the reordering buffer problem on a line consisting of n\nequidistant points. We show that, for any constant delta, an (offline)\nalgorithm that has a buffer (1-delta) k performs worse by a factor of Omega(log\nn) than an offline algorithm with buffer k. In particular, this demonstrates\nthat the O(log n)-competitive online algorithm MovingPartition by Gamzu and\nSegev (ACM Trans. on Algorithms, 6(1), 2009) is essentially optimal against any\noffline algorithm with a slightly larger buffer.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 21:02:57 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 00:40:59 GMT"}, {"version": "v3", "created": "Sat, 22 Jul 2017 09:02:00 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Bienkowski", "Marcin", ""], ["B\u00f6hm", "Martin", ""], ["Je\u017c", "\u0141ukasz", ""], ["Lasko\u015b-Grabowski", "Pawe\u0142", ""], ["Marcinkowski", "Jan", ""], ["Sgall", "Ji\u0159\u00ed", ""], ["Spyra", "Aleksandra", ""], ["Vesel\u00fd", "Pavel", ""]]}, {"id": "1610.04935", "submitter": "Richard Taylor Dr", "authors": "Richard Taylor", "title": "Approximations of the Densest k-Subhypergraph and Set Union Knapsack\n  problems", "comments": "There are two figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any given $\\epsilon>0$ we provide an algorithm for the Densest\n$k$-Subhypergraph Problem with an approximation ratio of at most\n$O(n^{\\theta_m+2\\epsilon})$ for\n$\\theta_m=\\frac{1}{2}m-\\frac{1}{2}-\\frac{1}{2m}$ and run time at most\n$O(n^{m-2+1/\\epsilon})$, where the hyperedges have at most $m$ vertices. We use\nthis result to give an algorithm for the Set Union Knapsack Problem with an\napproximation ratio of at most $O(n^{\\alpha_m+\\epsilon})$ for\n$\\alpha_m=\\frac{2}{3}[m-1-\\frac{2m-2}{m^2+m-1}]$ and run time at most\n$O(n^{5(m-2)+9/\\epsilon})$, where the subsets have at most $m$ elements. The\nauthor is not aware of any previous results on the approximation of either of\nthese two problems.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 00:28:12 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Taylor", "Richard", ""]]}, {"id": "1610.05045", "submitter": "Luisa Cutillo", "authors": "Annamaria Carissimo and Luisa Cutillo and Italia Defeis", "title": "Validation of community robustness", "comments": "arXiv admin note: text overlap with arXiv:0908.1062,\n  arXiv:cond-mat/0610077 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large amount of work on community detection and its applications leaves\nunaddressed one important question: the statistical validation of the results.\nIn this paper we present a methodology able to clearly detect if the community\nstructure found by some algorithms is statistically significant or is a result\nof chance, merely due to edge positions in the network. Given a community\ndetection method and a network of interest, our proposal examines the stability\nof the partition recovered against random perturbations of the original graph\nstructure. To address this issue, we specify a perturbation strategy and a null\nmodel to build a set of procedures based on a special measure of clustering\ndistance, namely Variation of Information, using tools set up for functional\ndata analysis. The procedures determine whether the obtained clustering departs\nsignificantly from the null model. This strongly supports the robustness\nagainst perturbation of the algorithm used to identify the community structure.\nWe show the results obtained with the proposed technique on simulated and real\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 11:16:18 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Carissimo", "Annamaria", ""], ["Cutillo", "Luisa", ""], ["Defeis", "Italia", ""]]}, {"id": "1610.05068", "submitter": "Manuel Lafond", "authors": "Manuel Lafond, C\\'edric Chauve, Nadia El-Mabrouk, A\\\"ida Ouangraoua", "title": "Gene Tree Construction and Correction using SuperTree and Reconciliation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The supertree problem asking for a tree displaying a set of consistent input\ntrees has been largely considered for the reconstruction of species trees.\nHere, we rather explore this framework for the sake of reconstructing a gene\ntree from a set of input gene trees on partial data. In this perspective, the\nphylogenetic tree for the species containing the genes of interest can be used\nto choose among the many possible compatible \"supergenetrees\", the most natural\ncriteria being to minimize a reconciliation cost. We develop a variety of\nalgorithmic solutions for the construction and correction of gene trees using\nthe supertree framework. A dynamic programming supertree algorithm for\nconstructing or correcting gene trees, exponential in the number of input\ntrees, is first developed for the less constrained version of the problem. It\nis then adapted to gene trees with nodes labeled as duplication or speciation,\nthe additional constraint being to preserve the orthology and paralogy\nrelations between genes. Then, a quadratic time algorithm is developed for\nefficiently correcting an initial gene tree while preserving a set of \"trusted\"\nsubtrees, as well as the relative phylogenetic distance between them, in both\ncases of labeled or unlabeled input trees. By applying these algorithms to the\nset of Ensembl gene trees, we show that this new correction framework is\nparticularly useful to correct weaklysupported duplication nodes. The C++\nsource code for the algorithms and simulations described in the paper are\navailable at https://github.com/UdeM-LBIT/SuGeT.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 12:13:01 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 23:21:43 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Lafond", "Manuel", ""], ["Chauve", "C\u00e9dric", ""], ["El-Mabrouk", "Nadia", ""], ["Ouangraoua", "A\u00efda", ""]]}, {"id": "1610.05115", "submitter": "Timothy Highley Jr.", "authors": "Timothy Highley, Hoang Le", "title": "Tropical Vertex-Disjoint Cycles of a Vertex-Colored Digraph: Barter\n  Exchange with Multiple Items Per Agent", "comments": "Published in Discrete Mathematics and Theoretical Computer Science", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 20 no.\n  2, Analysis of Algorithms (July 31, 2018) dmtcs:4690", "doi": "10.23638/DMTCS-20-2-1", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a barter exchange market, agents bring items and seek to exchange their\nitems with one another. Agents may agree to a k-way exchange involving a cycle\nof k agents. A barter exchange market can be represented by a digraph where the\nvertices represent items and the edges out of a vertex indicate the items that\nan agent is willing to accept in exchange for that item. It is known that the\nproblem of finding a set of vertex-disjoint cycles with the maximum total\nnumber of vertices (MAX-SIZE-EXCHANGE) can be solved in polynomial time. We\nconsider a barter exchange where each agent may bring multiple items, and items\nof the same agent are represented by vertices with the same color. A set of\ncycles is said to be tropical if for every color there is a cycle that contains\na vertex of that color. We show that the problem of determining whether there\nexists a tropical set of vertex-disjoint cycles in a digraph\n(TROPICAL-EXCHANGE) is NP-complete and APX-hard. This is equivalent to\ndetermining whether it is possible to arrange an exchange of items among agents\nsuch that every agent trades away at least one item. TROPICAL-MAX-SIZE-EXCHANGE\nis a similar problem, where the goal is to find a set of vertex-disjoint cycles\nthat contains the maximum number of vertices and also contains all of the\ncolors in the graph. We show that this problem is likewise NP-complete and\nAPX-hard. For the restricted case where there are at most two vertices of each\ncolor (corresponding to a restriction that each agent may bring at most two\nitems), both problems remain NP-hard but are in APX. Finally, we consider\nMAX-SIZE-TROPICAL-EXCHANGE, where the set of cycles must primarily include as\nmany colors as possible and secondarily include as many vertices as possible.\nWe show that this problem is NP-hard.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 01:10:21 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 18:25:43 GMT"}, {"version": "v3", "created": "Thu, 3 Aug 2017 20:16:11 GMT"}, {"version": "v4", "created": "Fri, 6 Apr 2018 21:54:41 GMT"}, {"version": "v5", "created": "Wed, 18 Jul 2018 03:36:20 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Highley", "Timothy", ""], ["Le", "Hoang", ""]]}, {"id": "1610.05120", "submitter": "G\\'abor Braun", "authors": "G\\'abor Braun, Sebastian Pokutta, Daniel Zink", "title": "Lazifying Conditional Gradient Algorithms", "comments": "25 pages and 31 pages of computational results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional gradient algorithms (also often called Frank-Wolfe algorithms)\nare popular due to their simplicity of only requiring a linear optimization\noracle and more recently they also gained significant traction for online\nlearning. While simple in principle, in many cases the actual implementation of\nthe linear optimization oracle is costly. We show a general method to lazify\nvarious conditional gradient algorithms, which in actual computations leads to\nseveral orders of magnitude of speedup in wall-clock time. This is achieved by\nusing a faster separation oracle instead of a linear optimization oracle,\nrelying only on few linear optimization oracle calls.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 14:01:25 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 16:53:44 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 17:43:43 GMT"}, {"version": "v4", "created": "Wed, 5 Sep 2018 15:24:14 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Braun", "G\u00e1bor", ""], ["Pokutta", "Sebastian", ""], ["Zink", "Daniel", ""]]}, {"id": "1610.05141", "submitter": "Lorenz H\\\"ubschle-Schneider", "authors": "Peter Sanders, Sebastian Lamm, Lorenz H\\\"ubschle-Schneider, Emanuel\n  Schrade and Carsten Dachsbacher", "title": "Efficient Random Sampling -- Parallel, Vectorized, Cache-Efficient, and\n  Online", "comments": null, "journal-ref": "ACM Transactions on Mathematical Software (TOMS), Volume 44, Issue\n  3 (April 2018), pages 29:1-29:14", "doi": "10.1145/3157734", "report-no": null, "categories": "cs.DS cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sampling $n$ numbers from the range\n$\\{1,\\ldots,N\\}$ without replacement on modern architectures. The main result\nis a simple divide-and-conquer scheme that makes sequential algorithms more\ncache efficient and leads to a parallel algorithm running in expected time\n$\\mathcal{O}(n/p+\\log p)$ on $p$ processors, i.e., scales to massively parallel\nmachines even for moderate values of $n$. The amount of communication between\nthe processors is very small (at most $\\mathcal{O}(\\log p)$) and independent of\nthe sample size. We also discuss modifications needed for load balancing,\nonline sampling, sampling with replacement, Bernoulli sampling, and\nvectorization on SIMD units or GPUs.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 14:38:02 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 15:27:42 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Sanders", "Peter", ""], ["Lamm", "Sebastian", ""], ["H\u00fcbschle-Schneider", "Lorenz", ""], ["Schrade", "Emanuel", ""], ["Dachsbacher", "Carsten", ""]]}, {"id": "1610.05155", "submitter": "Ashish Chiplunkar", "authors": "Yossi Azar, Ashish Chiplunkar, Haim Kaplan", "title": "Polylogarithmic Bounds on the Competitiveness of Min-cost (Bipartite)\n  Perfect Matching with Delays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online Min-cost Perfect Matching with Delays\n(MPMD) recently introduced by Emek et al, (STOC 2016). This problem is defined\non an underlying $n$-point metric space. An adversary presents real-time\nrequests online at points of the metric space, and the algorithm is required to\nmatch them, possibly after keeping them waiting for some time. The cost\nincurred is the sum of the distances between matched pairs of points (the\nconnection cost), and the sum of the waiting times of the requests (the delay\ncost). We present an algorithm with a competitive ratio of $O(\\log n)$, which\nimproves the upper bound of $O(\\log^2n+\\log\\Delta)$ of Emek et al, by removing\nthe dependence on $\\Delta$, the aspect ratio of the metric space (which can be\nunbounded as a function of $n$). The core of our algorithm is a deterministic\nalgorithm for MPMD on metrics induced by edge-weighted trees of height $h$,\nwhose cost is guaranteed to be at most $O(1)$ times the connection cost plus\n$O(h)$ times the delay cost of every feasible solution. The reduction from MPMD\non arbitrary metrics to MPMD on trees is achieved using the result on embedding\n$n$-point metric spaces into distributions over weighted hierarchically\nseparated trees of height $O(\\log n)$, with distortion $O(\\log n)$. We also\nprove a lower bound of $\\Omega(\\sqrt{\\log n})$ on the competitive ratio of any\nrandomized algorithm. This is the first lower bound which increases with $n$,\nand is attained on the metric of $n$ equally spaced points on a line.\n  The problem of Min-cost Bipartite Perfect Matching with Delays (MBPMD) is the\nsame as MPMD except that every request is either positive or negative, and\nrequests can be matched only if they have opposite polarity. We prove an upper\nbound of $O(\\log n)$ and a lower bound of $\\Omega(\\log^{1/3}n)$ on the\ncompetitive ratio of MBPMD with a more involved analysis.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 15:05:32 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Azar", "Yossi", ""], ["Chiplunkar", "Ashish", ""], ["Kaplan", "Haim", ""]]}, {"id": "1610.05646", "submitter": "Anisur Molla Rahaman", "authors": "Anisur Rahaman Molla and Gopal Pandurangan", "title": "Distributed Computation of Mixing Time", "comments": "To appear in the Proceedings of ICDCN 2017, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixing time of a graph is an important metric, which is not only useful\nin analyzing connectivity and expansion properties of the network, but also\nserves as a key parameter in designing efficient algorithms. We present an\nefficient distributed algorithm for computing the mixing time of undirected\ngraphs. Our algorithm estimates the mixing time $\\tau_s$ (with respect to a\nsource node $s$) of any $n$-node undirected graph in $O(\\tau_s \\log n)$ rounds.\nOur algorithm is based on random walks and require very little memory and use\nlightweight local computations, and work in the CONGEST model. Hence our\nalgorithm is scalable under bandwidth constraints and can be an helpful\nbuilding block in the design of topologically aware networks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 14:40:12 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Molla", "Anisur Rahaman", ""], ["Pandurangan", "Gopal", ""]]}, {"id": "1610.05656", "submitter": "Sumit Kumar Jha", "authors": "Sumit Kumar Jha", "title": "Asymptotic expansions for moments of number of comparisons used by the\n  randomized quick sort algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We calculate asymptotic expansions for the moments of number of comparisons\nused by the randomized quick sort algorithm using the singularity analysis of\ncertain generating functions.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 14:56:19 GMT"}, {"version": "v2", "created": "Sat, 22 Oct 2016 16:09:19 GMT"}, {"version": "v3", "created": "Wed, 4 Jan 2017 18:32:33 GMT"}, {"version": "v4", "created": "Sat, 18 Mar 2017 21:35:14 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Jha", "Sumit Kumar", ""]]}, {"id": "1610.05710", "submitter": "Babak Hosseini", "authors": "Babak Hosseini, Barbara Hammer", "title": "Feasibility Based Large Margin Nearest Neighbor Metric Learning", "comments": "This is the preprint of the conference paper published in ESANN2018", "journal-ref": "ESANN 2018 proceedings", "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large margin nearest neighbor (LMNN) is a metric learner which optimizes the\nperformance of the popular $k$NN classifier. However, its resulting metric\nrelies on pre-selected target neighbors. In this paper, we address the\nfeasibility of LMNN's optimization constraints regarding these target points,\nand introduce a mathematical measure to evaluate the size of the feasible\nregion of the optimization problem. We enhance the optimization framework of\nLMNN by a weighting scheme which prefers data triplets which yield a larger\nfeasible region. This increases the chances to obtain a good metric as the\nsolution of LMNN's problem. We evaluate the performance of the resulting\nfeasibility-based LMNN algorithm using synthetic and real datasets. The\nempirical results show an improved accuracy for different types of datasets in\ncomparison to regular LMNN.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 17:06:26 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 16:56:04 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Hosseini", "Babak", ""], ["Hammer", "Barbara", ""]]}, {"id": "1610.05725", "submitter": "Anatoly Plotnikov", "authors": "Anatoly D. Plotnikov", "title": "Polynomial-time algorithm for determining the graph isomorphism (v.2)", "comments": "13 pages, 11 figures", "journal-ref": "American Journal of Information Science and Computer Engineering,\n  Vol. 3, No. 6, 2017, pp. 71-76", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the methodology of positioning graph vertices relative to each\nother to solve the problem of determining isomorphism of two undirected graphs.\nBased on the position of the vertex in one of the graphs, it is determined the\ncorresponding vertex in the other graph.\n  For the selected vertex of the undirected graph, we define the neighborhoods\nof the vertices. Next, we construct the auxiliary directed graph, spawned by\nthe selected vertex. The vertices of the digraph are positioned by special\ncharacteristics --- vectors, which locate each vertex of the digraph relative\nthe found neighborhoods.\n  This enabled to develop the algorithm for determining graph isomorphism, the\nruning time of which is equal to $O(n^4)$.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 20:06:12 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 21:03:20 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Plotnikov", "Anatoly D.", ""]]}, {"id": "1610.05773", "submitter": "Manuel Gomez Rodriguez", "authors": "Ali Zarezade and Utkarsh Upadhyay and Hamid Rabiee and Manuel Gomez\n  Rodriguez", "title": "RedQueen: An Online Algorithm for Smart Broadcasting in Social Networks", "comments": "To appear at the 10th ACM International Conference on Web Search and\n  Data Mining (WSDM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users in social networks whose posts stay at the top of their followers'{}\nfeeds the longest time are more likely to be noticed. Can we design an online\nalgorithm to help them decide when to post to stay at the top? In this paper,\nwe address this question as a novel optimal control problem for jump stochastic\ndifferential equations. For a wide variety of feed dynamics, we show that the\noptimal broadcasting intensity for any user is surprisingly simple -- it is\ngiven by the position of her most recent post on each of her follower's feeds.\nAs a consequence, we are able to develop a simple and highly efficient online\nalgorithm, RedQueen, to sample the optimal times for the user to post.\nExperiments on both synthetic and real data gathered from Twitter show that our\nalgorithm is able to consistently make a user's posts more visible over time,\nis robust to volume changes on her followers' feeds, and significantly\noutperforms the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 20:00:05 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Zarezade", "Ali", ""], ["Upadhyay", "Utkarsh", ""], ["Rabiee", "Hamid", ""], ["Rodriguez", "Manuel Gomez", ""]]}, {"id": "1610.05897", "submitter": "Muhammad Anis Uddin Nasir", "authors": "Muhammad Anis Uddin Nasir, Aristides Gionis, Gianmarco De Francisci\n  Morales, Sarunas Girdzijauskas", "title": "Fully Dynamic Algorithm for Top-$k$ Densest Subgraphs", "comments": "10 pages, 8 figures, accepted at CIKM 2017", "journal-ref": null, "doi": "10.1145/3132847.3132966", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large graph, the densest-subgraph problem asks to find a subgraph\nwith maximum average degree. When considering the top-$k$ version of this\nproblem, a na\\\"ive solution is to iteratively find the densest subgraph and\nremove it in each iteration. However, such a solution is impractical due to\nhigh processing cost. The problem is further complicated when dealing with\ndynamic graphs, since adding or removing an edge requires re-running the\nalgorithm. In this paper, we study the top-$k$ densest-subgraph problem in the\nsliding-window model and propose an efficient fully-dynamic algorithm. The\ninput of our algorithm consists of an edge stream, and the goal is to find the\nnode-disjoint subgraphs that maximize the sum of their densities. In contrast\nto existing state-of-the-art solutions that require iterating over the entire\ngraph upon any update, our algorithm profits from the observation that updates\nonly affect a limited region of the graph. Therefore, the top-$k$ densest\nsubgraphs are maintained by only applying local updates. We provide a\ntheoretical analysis of the proposed algorithm and show empirically that the\nalgorithm often generates denser subgraphs than state-of-the-art competitors.\nExperiments show an improvement in efficiency of up to five orders of magnitude\ncompared to state-of-the-art solutions.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 07:44:46 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 08:52:12 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Nasir", "Muhammad Anis Uddin", ""], ["Gionis", "Aristides", ""], ["Morales", "Gianmarco De Francisci", ""], ["Girdzijauskas", "Sarunas", ""]]}, {"id": "1610.05961", "submitter": "Mahdi Jafari Siavoshani", "authors": "Ali Pourmiri, Mahdi Jafari Siavoshani, Seyed Pooya Shariatpanahi", "title": "Proximity-Aware Balanced Allocations in Cache Networks", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider load balancing in a network of caching servers delivering\ncontents to end users. Randomized load balancing via the so-called power of two\nchoices is a well-known approach in parallel and distributed systems that\nreduces network imbalance. In this paper, we propose a randomized load\nbalancing scheme which simultaneously considers cache size limitation and\nproximity in the server redirection process.\n  Since the memory limitation and the proximity constraint cause correlation in\nthe server selection process, we may not benefit from the power of two choices\nin general. However, we prove that in certain regimes, in terms of memory\nlimitation and proximity constraint, our scheme results in the maximum load of\norder $\\Theta(\\log\\log n)$ (here $n$ is the number of servers and requests),\nand at the same time, leads to a low communication cost. This is an exponential\nimprovement in the maximum load compared to the scheme which assigns each\nrequest to the nearest available replica. Finally, we investigate our scheme\nperformance by extensive simulations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 11:16:08 GMT"}, {"version": "v2", "created": "Sun, 23 Oct 2016 15:29:47 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Pourmiri", "Ali", ""], ["Siavoshani", "Mahdi Jafari", ""], ["Shariatpanahi", "Seyed Pooya", ""]]}, {"id": "1610.05994", "submitter": "Jos\\'e Fuentes", "authors": "Jos\\'e Fuentes-Sep\\'ulveda and Erick Elejalde and Leo Ferres and Diego\n  Seco", "title": "Parallel Construction of Wavelet Trees on Multicore Architectures", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "Knowl Inf Syst (2016)", "doi": "10.1007/s10115-016-1000-6", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wavelet tree has become a very useful data structure to efficiently\nrepresent and query large volumes of data in many different domains, from\nbioinformatics to geographic information systems. One problem with wavelet\ntrees is their construction time. In this paper, we introduce two algorithms\nthat reduce the time complexity of a wavelet tree's construction by taking\nadvantage of nowadays ubiquitous multicore machines.\n  Our first algorithm constructs all the levels of the wavelet in parallel in\n$O(n)$ time and $O(n\\lg\\sigma + \\sigma\\lg n)$ bits of working space, where $n$\nis the size of the input sequence and $\\sigma$ is the size of the alphabet. Our\nsecond algorithm constructs the wavelet tree in a domain-decomposition fashion,\nusing our first algorithm in each segment, reaching $O(\\lg n)$ time and\n$O(n\\lg\\sigma + p\\sigma\\lg n/\\lg\\sigma)$ bits of extra space, where $p$ is the\nnumber of available cores. Both algorithms are practical and report good\nspeedup for large real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 12:57:48 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Fuentes-Sep\u00falveda", "Jos\u00e9", ""], ["Elejalde", "Erick", ""], ["Ferres", "Leo", ""], ["Seco", "Diego", ""]]}, {"id": "1610.06008", "submitter": "Giannis Nikolentzos", "authors": "G. Nikolentzos, P. Meladianos, Y. Stavrakas, M. Vazirgiannis", "title": "K-clique-graphs for Dense Subgraph Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding dense subgraphs in a graph is a fundamental graph mining task, with\napplications in several fields. Algorithms for identifying dense subgraphs are\nused in biology, in finance, in spam detection, etc. Standard formulations of\nthis problem such as the problem of finding the maximum clique of a graph are\nhard to solve. However, some tractable formulations of the problem have also\nbeen proposed, focusing mainly on optimizing some density function, such as the\ndegree density and the triangle density. However, maximization of degree\ndensity usually leads to large subgraphs with small density. In this paper, we\nintroduce the k-clique-graph densest subgraph problem, k >= 3, a novel\nformulation for the discovery of dense subgraphs. Given an input graph, its\nk-clique-graph is a new graph created from the input graph where each vertex of\nthe new graph corresponds to a k-clique of the input graph and two vertices are\nconnected with an edge if they share a common (k-1)-clique. We define a simple\ndensity function, the k-clique-graph density, which gives compact and at the\nsame time dense subgraphs, and we project its resulting subgraphs back to the\ninput graph. In this paper we focus on the triangle-graph densest subgraph\nproblem obtained for k=3. To optimize the proposed function, we present an\nefficient greedy approximation algorithm that scales well to larger graphs. We\nevaluate the proposed algorithm on real datasets and compare it with other\nalgorithms in terms of the size and the density of the extracted subgraphs. The\nresults verify the ability of the proposed algorithm in finding high-quality\nsubgraphs in terms of size and density. Finally, we apply the proposed method\nto the important problem of keyword extraction from textual documents.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 13:28:14 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 07:25:28 GMT"}, {"version": "v3", "created": "Sat, 7 Jul 2018 17:31:19 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Nikolentzos", "G.", ""], ["Meladianos", "P.", ""], ["Stavrakas", "Y.", ""], ["Vazirgiannis", "M.", ""]]}, {"id": "1610.06023", "submitter": "Jarek Duda dr", "authors": "Jarek Duda", "title": "Practical estimation of rotation distance and induced partial order for\n  binary trees", "comments": "5 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree rotations (left and right) are basic local deformations allowing to\ntransform between two unlabeled binary trees of the same size. Hence, there is\na natural problem of practically finding such transformation path with low\nnumber of rotations, the optimal minimal number is called the rotation\ndistance. Such distance could be used for instance to quantify similarity\nbetween two trees for various machine learning problems, for example to compare\nhierarchical clusterings or arbitrarily chosen spanning trees of two graphs,\nlike in SMILES notation popular for describing chemical molecules.\n  There will be presented inexpensive practical greedy algorithm for finding a\nshort rotation path, optimality of which has still to be determined. It uses\nintroduced partial order for binary trees of the same size: $t_1 \\leq t_2$ iff\n$t_2$ can be obtained from $t_1$ by a sequence of only right rotations.\nIntuitively, the shortest rotation path should go through the least upper bound\nor the greatest lower bound for this partial order. The algorithm finds a path\nthrough candidates for both points in representation of binary tree as stack\ngraph: describing evolution of content of stack while processing a formula\ndescribed by a given binary tree. The article is accompanied with Mathematica\nimplementation of all used procedures (Appendix).\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 14:17:15 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Duda", "Jarek", ""]]}, {"id": "1610.06060", "submitter": "Magnus Wahlstr\\\"om", "authors": "Euiwoong Lee and Magnus Wahlstr\\\"om", "title": "LP-branching algorithms based on biased graphs", "comments": "New version with new coauthor (Euiwoong Lee) and approximation\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a combinatorial condition for the existence of efficient, LP-based\nFPT algorithms for a broad class of graph-theoretical optimisation problems.\nOur condition is based on the notion of biased graphs known from matroid\ntheory. Specifically, we show that given a biased graph $\\Psi=(G,\\mathcal{B})$,\nwhere $\\mathcal{B}$ is a class of balanced cycles in $G$, the problem of\nfinding a set $X$ of at most $k$ vertices in $G$ which intersects every\nunbalanced cycle in $G$ admits an FPT algorithm using an LP-branching approach,\nsimilar to those previously seen for VCSP problems (Wahlstr\\\"om, SODA 2014).\nThis framework captures many of the problems previously solved via the VCSP\napproach to LP-branching, as well as new generalisations, such as Group\nFeedback Vertex Set for infinite groups (e.g., for graphs whose edges are\nlabelled by matrices). A major advantage compared to previous work is that it\nis immediate to check the applicability of the result for a given problem,\nwhereas testing applicability of the VCSP approach for a specific VCSP requires\ndetermining the existence of an embedding language with certain algebraically\ndefined properties, which is not known to be decidable in general.\nAdditionally, we study the approximation question, and show that every problem\nof this category admits an $O(\\log \\text{OPT})$-approximation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 15:26:11 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 07:49:47 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Lee", "Euiwoong", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "1610.06131", "submitter": "Ignasi Sau", "authors": "Valentin Garnero, Christophe Paul, Ignasi Sau, Dimitrios M. Thilikos", "title": "Explicit linear kernels for packing problems", "comments": "43 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last years, several algorithmic meta-theorems have appeared\n(Bodlaender et al. [FOCS 2009], Fomin et al. [SODA 2010], Kim et al. [ICALP\n2013]) guaranteeing the existence of linear kernels on sparse graphs for\nproblems satisfying some generic conditions. The drawback of such general\nresults is that it is usually not clear how to derive from them constructive\nkernels with reasonably low explicit constants. To fill this gap, we recently\npresented [STACS 2014] a framework to obtain explicit linear kernels for some\nfamilies of problems whose solutions can be certified by a subset of vertices.\nIn this article we enhance our framework to deal with packing problems, that\nis, problems whose solutions can be certified by collections of subgraphs of\nthe input graph satisfying certain properties. ${\\mathcal F}$-Packing is a\ntypical example: for a family ${\\mathcal F}$ of connected graphs that we assume\nto contain at least one planar graph, the task is to decide whether a graph $G$\ncontains $k$ vertex-disjoint subgraphs such that each of them contains a graph\nin ${\\mathcal F}$ as a minor. We provide explicit linear kernels on sparse\ngraphs for the following two orthogonal generalizations of ${\\mathcal\nF}$-Packing: for an integer $\\ell \\geq 1$, one aims at finding either\nminor-models that are pairwise at distance at least $\\ell$ in $G$\n($\\ell$-${\\mathcal F}$-Packing), or such that each vertex in $G$ belongs to at\nmost $\\ell$ minors-models (${\\mathcal F}$-Packing with $\\ell$-Membership).\nFinally, we also provide linear kernels for the versions of these problems\nwhere one wants to pack subgraphs instead of minors.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 18:05:05 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Garnero", "Valentin", ""], ["Paul", "Christophe", ""], ["Sau", "Ignasi", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1610.06199", "submitter": "Hoa Vu", "authors": "Andrew McGregor, Hoa T. Vu", "title": "Better Streaming Algorithms for the Maximum Coverage Problem", "comments": "- A preliminary version appeared in ICDT 2017 - Fix typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classic NP-Hard problem of finding the maximum $k$-set coverage\nin the data stream model: given a set system of $m$ sets that are subsets of a\nuniverse $\\{1,\\ldots,n \\}$, find the $k$ sets that cover the most number of\ndistinct elements. The problem can be approximated up to a factor $1-1/e$ in\npolynomial time. In the streaming-set model, the sets and their elements are\nrevealed online. The main goal of our work is to design algorithms, with\napproximation guarantees as close as possible to $1-1/e$, that use sublinear\nspace $o(mn)$. Our main results are:\n  Two $(1-1/e-\\epsilon)$ approximation algorithms: One uses $O(\\epsilon^{-1})$\npasses and $\\tilde{O}(\\epsilon^{-2} k)$ space whereas the other uses only a\nsingle pass but $\\tilde{O}(\\epsilon^{-2} m)$ space.\n  We show that any approximation factor better than $(1-(1-1/k)^k)$ in constant\npasses requires $\\Omega(m)$ space for constant $k$ even if the algorithm is\nallowed unbounded processing time. We also demonstrate a single-pass,\n$(1-\\epsilon)$ approximation algorithm using $\\tilde{O}(\\epsilon^{-2} m \\cdot\n\\min(k,\\epsilon^{-1}))$ space.\n  We also study the maximum $k$-vertex coverage problem in the dynamic graph\nstream model. In this model, the stream consists of edge insertions and\ndeletions of a graph on $N$ vertices. The goal is to find $k$ vertices that\ncover the most number of distinct edges.\n  We show that any constant approximation in constant passes requires\n$\\Omega(N)$ space for constant $k$ whereas $\\tilde{O}(\\epsilon^{-2}N)$ space is\nsufficient for a $(1-\\epsilon)$ approximation and arbitrary $k$ in a single\npass.\n  For regular graphs, we show that $\\tilde{O}(\\epsilon^{-3}k)$ space is\nsufficient for a $(1-\\epsilon)$ approximation in a single pass. We generalize\nthis to a $(\\kappa-\\epsilon)$ approximation when the ratio between the minimum\nand maximum degree is bounded below by $\\kappa$.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 20:17:08 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 20:17:01 GMT"}, {"version": "v3", "created": "Mon, 23 Jan 2017 20:06:26 GMT"}, {"version": "v4", "created": "Thu, 26 Oct 2017 18:47:32 GMT"}, {"version": "v5", "created": "Wed, 17 Jan 2018 10:53:01 GMT"}, {"version": "v6", "created": "Wed, 9 May 2018 18:17:38 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["McGregor", "Andrew", ""], ["Vu", "Hoa T.", ""]]}, {"id": "1610.06515", "submitter": "Samuel Haney", "authors": "Rupert Freeman and Samuel Haney and Debmalya Panigrahi", "title": "On the Price of Stability of Undirected Multicast Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multicast network design games, a set of agents choose paths from their\nsource locations to a common sink with the goal of minimizing their individual\ncosts, where the cost of an edge is divided equally among the agents using it.\nSince the work of Anshelevich et al. (FOCS 2004) that introduced network design\ngames, the main open problem in this field has been the price of stability\n(PoS) of multicast games. For the special case of broadcast games (every vertex\nis a terminal, i.e., has an agent), a series of works has culminated in a\nconstant upper bound on the PoS (Bilo` et al., FOCS 2013). However, no\nsignificantly sub-logarithmic bound is known for multicast games. In this\npaper, we make progress toward resolving this question by showing a constant\nupper bound on the PoS of multicast games for quasi-bipartite graphs. These are\ngraphs where all edges are between two terminals (as in broadcast games) or\nbetween a terminal and a nonterminal, but there is no edge between\nnonterminals. This represents a natural class of intermediate generality\nbetween broadcast and multicast games. In addition to the result itself, our\ntechniques overcome some of the fundamental difficulties of analyzing the PoS\nof general multicast games, and are a promising step toward resolving this\nmajor open problem.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 17:40:16 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Freeman", "Rupert", ""], ["Haney", "Samuel", ""], ["Panigrahi", "Debmalya", ""]]}, {"id": "1610.06539", "submitter": "Nina Chiarelli", "authors": "Nina Chiarelli and Martin Milani\\v{c}", "title": "Linear separation of connected dominating sets in graphs", "comments": "32 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A connected dominating set in a graph is a dominating set of vertices that\ninduces a connected subgraph. Following analogous studies in the literature\nrelated to independent sets, dominating sets, and total dominating sets, we\nstudy in this paper the class of graphs in which the connected dominating sets\ncan be separated from the other vertex subsets by a linear weight function.\nMore precisely, we say that a graph is connected-domishold if it admits\nnon-negative real weights associated to its vertices such that a set of\nvertices is a connected dominating set if and only if the sum of the\ncorresponding weights exceeds a certain threshold. We characterize the graphs\nin this non-hereditary class in terms of a property of the set of minimal\ncutsets of the graph. We give several characterizations for the hereditary\ncase, that is, when each connected induced subgraph is required to be\nconnected-domishold. The characterization by forbidden induced subgraphs\nimplies that the class properly generalizes two well known classes of chordal\ngraphs, the block graphs and the trivially perfect graphs. Finally, we study\ncertain algorithmic aspects of connected-domishold graphs. Building on\nconnections with minimal cutsets and properties of the derived hypergraphs and\nBoolean functions, we show that our approach leads to new polynomially solvable\ncases of the weighted connected dominating set problem.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 19:00:46 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 15:25:24 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Chiarelli", "Nina", ""], ["Milani\u010d", "Martin", ""]]}, {"id": "1610.06603", "submitter": "Wei Hu", "authors": "Wei Chen, Wei Hu, Fu Li, Jian Li, Yu Liu, Pinyan Lu", "title": "Combinatorial Multi-Armed Bandit with General Reward Functions", "comments": "Published in Neural Information Processing Systems (NIPS) 2016. New\n  in this version: a minor bug fix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the stochastic combinatorial multi-armed bandit\n(CMAB) framework that allows a general nonlinear reward function, whose\nexpected value may not depend only on the means of the input random variables\nbut possibly on the entire distributions of these variables. Our framework\nenables a much larger class of reward functions such as the $\\max()$ function\nand nonlinear utility functions. Existing techniques relying on accurate\nestimations of the means of random variables, such as the upper confidence\nbound (UCB) technique, do not work directly on these functions. We propose a\nnew algorithm called stochastically dominant confidence bound (SDCB), which\nestimates the distributions of underlying random variables and their\nstochastically dominant confidence bounds. We prove that SDCB can achieve\n$O(\\log{T})$ distribution-dependent regret and $\\tilde{O}(\\sqrt{T})$\ndistribution-independent regret, where $T$ is the time horizon. We apply our\nresults to the $K$-MAX problem and expected utility maximization problems. In\nparticular, for $K$-MAX, we provide the first polynomial-time approximation\nscheme (PTAS) for its offline problem, and give the first $\\tilde{O}(\\sqrt T)$\nbound on the $(1-\\epsilon)$-approximation regret of its online problem, for any\n$\\epsilon>0$.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 20:54:41 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 18:18:05 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2017 04:49:22 GMT"}, {"version": "v4", "created": "Fri, 20 Jul 2018 17:38:35 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Chen", "Wei", ""], ["Hu", "Wei", ""], ["Li", "Fu", ""], ["Li", "Jian", ""], ["Liu", "Yu", ""], ["Lu", "Pinyan", ""]]}, {"id": "1610.06656", "submitter": "Shanshan Wu", "authors": "Shanshan Wu, Srinadh Bhojanapalli, Sujay Sanghavi, Alexandros G.\n  Dimakis", "title": "Single Pass PCA of Matrix Products", "comments": "24 pages, 4 figures, NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new algorithm for computing a low rank\napproximation of the product $A^TB$ by taking only a single pass of the two\nmatrices $A$ and $B$. The straightforward way to do this is to (a) first sketch\n$A$ and $B$ individually, and then (b) find the top components using PCA on the\nsketch. Our algorithm in contrast retains additional summary information about\n$A,B$ (e.g. row and column norms etc.) and uses this additional information to\nobtain an improved approximation from the sketches. Our main analytical result\nestablishes a comparable spectral norm guarantee to existing two-pass methods;\nin addition we also provide results from an Apache Spark implementation that\nshows better computational and statistical performance on real-world and\nsynthetic evaluation datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 02:45:46 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 13:58:24 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Wu", "Shanshan", ""], ["Bhojanapalli", "Srinadh", ""], ["Sanghavi", "Sujay", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1610.06672", "submitter": "William Schwartz", "authors": "William K. Schwartz", "title": "The Broadcaster-Repacking Problem", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Federal Communications Commission's (FCC's) ongoing Incentive Auction\nwill, if successful, transfer billions of dollars of radio spectrum from\ntelevision broadcasters to mobile-network operators. Hundreds of broadcasters\nmay go off the air. Most of those remaining on the air, including hundreds of\nCanadian broadcasters not bidding, will have to move to new channels to\ncontinue broadcasting. The auction can only end if all these broadcasters will\nfit into the spectrum remaining for television. Whether a given set of\nbroadcasters fits is the broadcaster-repacking problem. The FCC must calculate\nits solutions thousands of times per round of bidding. Speed is essential.\n  By reducing the broadcaster-repacking problem to the maximum independent set\nproblem, we show that the former is $\\mathcal{NP}$-complete. This reduction\nalso allows us to expand on sparsity-exploiting heuristics in the literature,\nwhich have made the FCC's repacking-problem instances tractable. We conclude by\nrelating the heuristics to satisfiability and integer programming reductions.\nThese provide a basis for implementing algorithms in off-the-shelf software to\nsolve the broadcaster-repacking problem.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 04:52:38 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Schwartz", "William K.", ""]]}, {"id": "1610.06681", "submitter": "Khaled Elbassioni", "authors": "Endre Boros, Khaled Elbassioni, Vladimir Gurvich, Kazuhisa Makino", "title": "A Convex Programming-based Algorithm for Mean Payoff Stochastic Games\n  with Perfect Information", "comments": "arXiv admin note: text overlap with arXiv:1508.03431", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two-person zero-sum stochastic mean payoff games with perfect\ninformation, or BWR-games, given by a digraph $G = (V, E)$, with local rewards\n$r: E \\to \\ZZ$, and three types of positions: black $V_B$, white $V_W$, and\nrandom $V_R$ forming a partition of $V$. It is a long-standing open question\nwhether a polynomial time algorithm for BWR-games exists, even when $|V_R|=0$.\nIn fact, a pseudo-polynomial algorithm for BWR-games would already imply their\npolynomial solvability. In this short note, we show that BWR-games can be\nsolved via convex programming in pseudo-polynomial time if the number of random\npositions is a constant.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 06:51:51 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Boros", "Endre", ""], ["Elbassioni", "Khaled", ""], ["Gurvich", "Vladimir", ""], ["Makino", "Kazuhisa", ""]]}, {"id": "1610.06701", "submitter": "Haotian Jiang", "authors": "Haotian Jiang", "title": "Two Stage Optimization with Recourse and Revocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-stage optimization with recourse model is an important and widely used\nmodel, which has been studied extensively these years. In this article, we will\nlook at a new variant of it, called the two-stage optimization with recourse\nand revocation model. This new model differs from the traditional one in that\none is allowed to revoke some of his earlier decisions and withdraw part of the\nearlier costs, which is not unlikely in many real applications, and is\ntherefore considered to be more realistic under many situations. We will adopt\nseveral approaches to study this model. In fact, we will develop an LP rounding\nscheme for some cover problems and show that they can be solved using this\nscheme and an adaptation of the rounding approach for the deterministic\ncounterpart, provided the polynomial scenario assumption. Stochastic\nuncapacitated facility location problem will also be studied to show that the\napproximation algorithm that worked for the two-stage with recourse model\nworked for this model as well. In addition, we will use other methods to study\nthe model.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 08:46:45 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Jiang", "Haotian", ""]]}, {"id": "1610.06759", "submitter": "Leonid Barenboim", "authors": "Leonid Barenboim and Michael Elkin and Tzalik Maimon", "title": "Deterministic Distributed (Delta + o(\\Delta))-Edge-Coloring, and\n  Vertex-Coloring of Graphs with Bounded Diversity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider coloring problems in the distributed message-passing setting. The\npreviously-known deterministic algorithms for edge-coloring employed at least\n(2Delta - 1) colors, even though any graph admits an edge-coloring with Delta +\n1 colors [V64]. Moreover, the previously-known deterministic algorithms that\nemployed at most O(Delta) colors required superlogarithmic time\n[B15,BE10,BE11,FHK15]. In the current paper we devise deterministic\nedge-coloring algorithms that employ only Delta + o(Delta) colors, for a very\nwide family of graphs. Specifically, as long as the arboricity is a =\nO(Delta^{1 - \\epsilon}), for a constant epsilon > 0, our algorithm computes\nsuch a coloring within {polylogarithmic} deterministic time. We also devise\nsignificantly improved deterministic edge-coloring algorithms for {general\ngraphs} for a very wide range of parameters. Specifically, for any value $\\chi$\nin the range [4Delta, 2^{o(log Delta)} \\cdot Delta], our \\chi-edge-coloring\nalgorithm has smaller running time than the best previously-known\n\\chi-edge-coloring algorithms. Our algorithms are actually much more general,\nsince edge-coloring is equivalent to {vertex-coloring of line graphs.} Our\nmethod is applicable to vertex-coloring of the family of graphs with {bounded\ndiversity} that contains line graphs, line graphs of hypergraphs, and many\nother graphs.\n  Our results are obtained using a novel technique that connects vertices or\nedges in a certain way that reduces clique size. The resulting structures,\nwhich we call {connectors}, can be colored more efficiently than the original\ngraph. Moreover, the color classes constitute simpler subgraphs that can be\ncolored even more efficiently using appropriate connectors. Hence, we recurse\nuntil we obtain sufficiently simple structures that are colored directly. We\nintroduce several types of connectors that are useful for various scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 12:28:32 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Barenboim", "Leonid", ""], ["Elkin", "Michael", ""], ["Maimon", "Tzalik", ""]]}, {"id": "1610.06934", "submitter": "David Burstein", "authors": "David Burstein and Leigh Metcalf", "title": "The K Shortest Paths Problem with Application to Routing", "comments": "37 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the computational complexity of finding almost shortest simple paths,\nwe propose that identifying a larger collection of (nonbacktracking) paths is\nmore efficient than finding almost shortest simple paths on positively weighted\nreal-world networks. First, we present an easy to implement $O(m\\log m+kL)$\nsolution for finding all (nonbacktracking) paths with bounded length $D$\nbetween two arbitrary nodes on a positively weighted graph, where $L$ is an\nupperbound for the number of nodes in any of the $k$ outputted paths.\nSubsequently, we illustrate that for undirected Chung-Lu random graphs, the\nratio between the number of nonbacktracking and simple paths asymptotically\napproaches $1$ with high probability for a wide range of parameters. We then\nconsider an application to the almost shortest paths algorithm to measure path\ndiversity for internet routing in a snapshot of the Autonomous System graph\nsubject to an edge deletion process.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 20:02:53 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 17:25:07 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 17:25:15 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Burstein", "David", ""], ["Metcalf", "Leigh", ""]]}, {"id": "1610.07018", "submitter": "Hsiang-Hsuan Liu", "authors": "Wing-Kai Hon, Ton Kloks, Fu-Hong Liu, Hsiang-Hsuan Liu, Tao-Ming Wang,\n  and Yue-Li Wang", "title": "P_3-Games on Chordal Bipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let G=(V,E) be a connected graph. A set U subseteq V is convex if G[U] is\nconnected and all vertices of V\\U have at most one neighbor in U. Let sigma(W)\ndenote the unique smallest convex set that contains W subseteq V. Two players\nplay the following game. Consider a convex set U and call it the `playground.'\nInitially, U = emptyset. When U=V, the player to move loses the game.\nOtherwise, that player chooses a vertex x in V\\U which is at distance at most\ntwo from U. The effect of the move is that the playground U changes into\nsigma(U cup {x}) and the opponent is presented with this new playground.\n  A graph is chordal bipartite if it is bipartite and has no induced cycle of\nlength more than four. In this paper we show that, when G is chordal bipartite,\nthere is a polynomial-time algorithm that computes the Grundy number of the\nP_3-game played on G. This implies that there is an efficient algorithm to\ndecide whether the first player has a winning strategy.\n", "versions": [{"version": "v1", "created": "Sat, 22 Oct 2016 08:58:07 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Hon", "Wing-Kai", ""], ["Kloks", "Ton", ""], ["Liu", "Fu-Hong", ""], ["Liu", "Hsiang-Hsuan", ""], ["Wang", "Tao-Ming", ""], ["Wang", "Yue-Li", ""]]}, {"id": "1610.07100", "submitter": "Matthew Hastings", "authors": "M. B. Hastings", "title": "Local Maxima and Improved Exact Algorithm for MAX-2-SAT", "comments": "17 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a MAX-2-SAT instance, we define a local maximum to be an assignment\nsuch that changing any single variable reduces the number of satisfied clauses.\nWe consider the question of the number of local maxima that an instance of\nMAX-2-SAT can have. We give upper bounds in both the sparse and nonsparse case,\nwhere the sparse case means that there is a bound $d$ on the average number of\nclauses involving any given variable. The bounds in the nonsparse case are\ntight up to polylogarithmic factors, while in the sparse case the bounds are\ntight up to a multiplicative factor in $d$ for large $d$. Additionally, we\ngeneralize to the question of assignments which are maxima up to changing $k>\n1$ variables simultaneously; in this case, we give explicit constructions with\nlarge (in a sense explained below) numbers of such maxima in the sparse case.\nThe basic idea of the upper bound proof is to consider a random assignment to\nsome subset of the variables and determine the probability that some fraction\nof the remaining variables can be fixed without considering interactions\nbetween them. The bounded results hold in the case of weighted MAX-2-SAT as\nwell. Using this technique and combining with ideas from Ref. 6, we find an\nalgorithm for weighted MAX-2-SAT which is faster for large $d$ than previous\nalgorithms which use polynomial space; this algorithm does require an\nadditional bounds on maximum weights and degree.\n", "versions": [{"version": "v1", "created": "Sat, 22 Oct 2016 22:48:43 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Hastings", "M. B.", ""]]}, {"id": "1610.07204", "submitter": "Fritz B\\\"okler", "authors": "Fritz B\\\"okler and Matthias Ehrgott and Christopher Morris and Petra\n  Mutzel", "title": "Output-sensitive Complexity of Multiobjective Combinatorial Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study output-sensitive algorithms and complexity for multiobjective\ncombinatorial optimization problems. In this computational complexity\nframework, an algorithm for a general enumeration problem is regarded efficient\nif it is output-sensitive, i.e., its running time is bounded by a polynomial in\nthe input and the output size. We provide both practical examples of MOCO\nproblems for which such an efficient algorithm exists as well as problems for\nwhich no efficient algorithm exists under mild complexity theoretic\nassumptions.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 17:36:47 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["B\u00f6kler", "Fritz", ""], ["Ehrgott", "Matthias", ""], ["Morris", "Christopher", ""], ["Mutzel", "Petra", ""]]}, {"id": "1610.07229", "submitter": "O-Joung Kwon", "authors": "Eun Jung Kim and O-joung Kwon", "title": "A Polynomial Kernel for Distance-Hereditary Vertex Deletion", "comments": "37 pages, 6 figures; improved previous kernel size to O(k^{30}\n  polylogk)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is distance-hereditary if for any pair of vertices, their distance in\nevery connected induced subgraph containing both vertices is the same as their\ndistance in the original graph. The Distance-Hereditary Vertex Deletion problem\nasks, given a graph $G$ on $n$ vertices and an integer $k$, whether there is a\nset $S$ of at most $k$ vertices in $G$ such that $G-S$ is distance-hereditary.\nThis problem is important due to its connection to the graph parameter\nrank-width that distance-hereditary graphs are exactly graphs of rank-width at\nmost $1$. Eiben, Ganian, and Kwon (MFCS' 16) proved that Distance-Hereditary\nVertex Deletion can be solved in time $2^{\\mathcal{O}(k)}n^{\\mathcal{O}(1)}$,\nand asked whether it admits a polynomial kernelization. We show that this\nproblem admits a polynomial kernel, answering this question positively. For\nthis, we use a similar idea for obtaining an approximate solution for Chordal\nVertex Deletion due to Jansen and Pilipczuk (SODA' 17) to obtain an approximate\nsolution with $\\mathcal{O}(k^3\\log n)$ vertices when the problem is a\nYES-instance, and we exploit the structure of split decompositions of\ndistance-hereditary graphs to reduce the total size.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 20:30:28 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 08:28:18 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2017 21:37:09 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Kim", "Eun Jung", ""], ["Kwon", "O-joung", ""]]}, {"id": "1610.07294", "submitter": "arXiv Admin", "authors": "Akbar Moazzam, Babak Dalvand", "title": "Molecular solutions for the Maximum K-colourable Sub graph Problem in\n  Adleman-Lipton model", "comments": "This article has been withdrawn by arXiv administrators due to\n  excessive unattributed and verbatim text overlap from external sources", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adleman showed that deoxyribonucleic acid DNA strands could be employed\ntowards calculating solutions to an instance of the Hamiltonian path problem .\nLipton also demonstrated that Adleman techniques could be used to solve the\nSatisfiability problem. In this paper, we use Adleman Lipton model for\ndeveloping a DNA algorithm to solve Maximum k-colourable Sub graph problem. In\nspite of the NP-hardness of Maximum k-colourable Sub graph problem our DNA\nprocedures is done in a polynomial time.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 06:42:31 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 21:27:26 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Moazzam", "Akbar", ""], ["Dalvand", "Babak", ""]]}, {"id": "1610.07384", "submitter": "Antonin Novak", "authors": "Antonin Novak, Premysl Sucha, Zdenek Hanzalek", "title": "On Solving Non-preemptive Mixed-criticality Match-up Scheduling Problem\n  with Two and Three Criticality Levels", "comments": "16 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study an NP-hard problem of a single machine scheduling\nminimizing the makespan, where the mixed-critical tasks with an uncertain\nprocessing time are scheduled. We show the derivation of F-shaped tasks from\nthe probability distribution function of the processing time, then we study the\nstructure of problems with two and three criticality levels for which we\npropose efficient exact algorithms and we present computational experiments for\ninstances with up to 200 tasks. Moreover, we show that the considered problem\nis approximable within a constant multiplicative factor.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 12:25:27 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Novak", "Antonin", ""], ["Sucha", "Premysl", ""], ["Hanzalek", "Zdenek", ""]]}, {"id": "1610.07434", "submitter": "Haotian Jiang", "authors": "Haotian Jiang", "title": "An Attempt to Design a Better Algorithm for the Uncapacitated Facility\n  Location Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The uncapacitated facility location has always been an important problem due\nto its connection to operational research and infrastructure planning. Byrka\nobtained an algorithm that is parametrized by $\\gamma$ and proved that it is\noptimal when $\\gamma>1.6774$. He also proved that the algorithm achieved an\napproximation ratio of 1.50. A later work by Shi Li achieved an approximation\nfactor of 1.488. In this research, we studied these algorithms and several\nrelated works. Although we didn't improve upon the algorithm of Shi Li, our\nwork did provide some insight into the problem. We also reframed the problem as\na vector game, which provided a framework to design balanced algorithms for\nthis problem.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 14:42:26 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Jiang", "Haotian", ""]]}, {"id": "1610.07530", "submitter": "Du\\v{s}an Knop", "authors": "Pavel Dvo\\v{r}\\'ak and Du\\v{s}an Knop and Tom\\'a\\v{s} Toufar", "title": "Target Set Selection in Dense Graph Classes", "comments": "37 pages, 11 figures, short version appeared in Proc. of ISAAC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the Target Set Selection problem from a parameterized\ncomplexity perspective. Here for a given graph and a threshold for each vertex,\nthe task is to find a set of vertices (called a target set) which activates the\nwhole graph during the following iterative process. A vertex outside the active\nset becomes active if the number of so far activated vertices in its\nneighborhood is at least its threshold.\n  We give two parameterized algorithms for a special case where each vertex has\nthe threshold set to the half of its neighbors (the so-called Majority Target\nSet Selection problem) for parameterizations by the neighborhood diversity and\nthe twin cover number of the input graph.\n  We complement these results from the negative side. We give a hardness proof\nfor the Majority Target Set Selection problem when parameterized by (a\nrestriction of) the modular-width - a natural generalization of both previous\nstructural parameters. We also show the Target Set Selection problem\nparameterized by the neighborhood diversity or by the twin cover number is\nW[1]-hard when there is no restriction on the thresholds.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 18:31:55 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 13:45:13 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Dvo\u0159\u00e1k", "Pavel", ""], ["Knop", "Du\u0161an", ""], ["Toufar", "Tom\u00e1\u0161", ""]]}, {"id": "1610.07639", "submitter": "Marco Molinaro", "authors": "Marco Molinaro", "title": "Online and Random-order Load Balancing Simultaneously", "comments": "Accepted to SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online load balancing under lp-norms: sequential\njobs need to be assigned to one of the machines and the goal is to minimize the\nlp-norm of the machine loads. This generalizes the classical problem of\nscheduling for makespan minimization (case l_infty) and has been thoroughly\nstudied. However, despite the recent push for beyond worst-case analyses, no\nsuch results are known for this problem. In this paper we provide algorithms\nwith simultaneous guarantees for the worst-case model as well as for the\nrandom-order (i.e. secretary) model, where an arbitrary set of jobs comes in\nrandom order. First, we show that the greedy algorithm (with restart), known to\nhave optimal O(p) worst-case guarantee, also has a (typically) improved\nrandom-order guarantee. However, the behavior of this algorithm in the\nrandom-order model degrades with p. We then propose algorithm SIMULTANEOUSLB\nthat has simultaneously optimal guarantees (within constants) in both\nworst-case and random-order models. In particular, the random-order guarantee\nof SIMULTANEOUSLB improves as p increases.\n  One of the main components is a new algorithm with improved regret for Online\nLinear Optimization (OLO) over the non-negative vectors in the lq ball.\nInterestingly, this OLO algorithm is also used to prove a purely probabilistic\ninequality that controls the correlations arising in the random-order model, a\ncommon source of difficulty for the analysis. Another important component used\nin both SIMULTANEOUSLB and our OLO algorithm is a smoothing of the lp-norm that\nmay be of independent interest. This smoothness property allows us to see\nalgorithm SIMULTANEOUSLB as essentially a greedy one in the worst-case model\nand as a primal-dual one in the random-order model, which is instrumental for\nits simultaneous guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 20:37:06 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 14:14:52 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Molinaro", "Marco", ""]]}, {"id": "1610.07718", "submitter": "Jiecao Chen", "authors": "Jiecao Chen and Qin Zhang", "title": "Bias-Aware Sketches", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear sketching algorithms have been widely used for processing large-scale\ndistributed and streaming datasets. Their popularity is largely due to the fact\nthat linear sketches can be naturally composed in the distributed model and be\nefficiently updated in the streaming model. The errors of linear sketches are\ntypically expressed in terms of the sum of coordinates of the input vector\nexcluding those largest ones, or, the mass on the tail of the vector. Thus, the\nprecondition for these algorithms to perform well is that the mass on the tail\nis small, which is, however, not always the case -- in many real-world datasets\nthe coordinates of the input vector have a {\\em bias}, which will generate a\nlarge mass on the tail.\n  In this paper we propose linear sketches that are {\\em bias-aware}. We\nrigorously prove that they achieve strictly better error guarantees than the\ncorresponding existing sketches, and demonstrate their practicality and\nsuperiority via an extensive experimental evaluation on both real and synthetic\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 03:51:39 GMT"}, {"version": "v2", "created": "Sun, 26 Mar 2017 19:17:54 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Chen", "Jiecao", ""], ["Zhang", "Qin", ""]]}, {"id": "1610.07729", "submitter": "Hiroki Oshima", "authors": "Hiroki Oshima", "title": "Derandomization for k-submodular maximization", "comments": "9 pages ; added more detail to Introduction and Conclusion, added\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodularity is one of the most important property of combinatorial\noptimization, and $k$-submodularity is a generalization of submodularity.\nMaximization of $k$-submodular function is NP-hard, and approximation\nalgorithms are studied. For monotone $k$-submodular function, [Iwata, Tanigawa,\nand Yoshida 2016] gave $k/(2k-1)$-approximation algorithm. In this paper, we\ngive a deterministic algorithm by derandomizing that algorithm. Derandomization\nscheme is from [Buchbinder and Feldman 2016]. Our algorithm is\n$k/(2k-1)$-approximation and polynomial-time algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 04:32:23 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 05:53:19 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Oshima", "Hiroki", ""]]}, {"id": "1610.07766", "submitter": "Marcin Pilipczuk", "authors": "Anna Adamaszek and Tomasz Kociumaka and Marcin Pilipczuk and Micha{\\l}\n  Pilipczuk", "title": "Hardness of approximation for strip packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strip packing is a classical packing problem, where the goal is to pack a set\nof rectangular objects into a strip of a given width, while minimizing the\ntotal height of the packing. The problem has multiple applications, e.g. in\nscheduling and stock-cutting, and has been studied extensively.\n  When the dimensions of objects are allowed to be exponential in the total\ninput size, it is known that the problem cannot be approximated within a factor\nbetter than $3/2$, unless $\\mathrm{P}=\\mathrm{NP}$. However, there was no\ncorresponding lower bound for polynomially bounded input data. In fact,\nNadiradze and Wiese [SODA 2016] have recently proposed a $(1.4 + \\epsilon)$\napproximation algorithm for this variant, thus showing that strip packing with\npolynomially bounded data can be approximated better than when exponentially\nlarge values in the input data are allowed. Their result has subsequently been\nimproved to a $(4/3 + \\epsilon)$ approximation by two independent research\ngroups [FSTTCS 2016, arXiv:1610.04430]. This raises a question whether strip\npacking with polynomially bounded input data admits a quasi-polynomial time\napproximation scheme, as is the case for related two-dimensional packing\nproblems like maximum independent set of rectangles or two-dimensional\nknapsack.\n  In this paper we answer this question in negative by proving that it is\nNP-hard to approximate strip packing within a factor better than $12/11$, even\nwhen admitting only polynomially bounded input data. In particular, this shows\nthat the strip packing problem admits no quasi-polynomial time approximation\nscheme, unless $\\mathrm{NP} \\subseteq \\mathrm{DTIME}(2^{\\mathrm{polylog}(n)})$.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 07:44:21 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Adamaszek", "Anna", ""], ["Kociumaka", "Tomasz", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1610.07770", "submitter": "T-H. Hubert Chan", "authors": "T-H. Hubert Chan and Zhiyi Huang and Shaofeng H.-C. Jiang and Ning\n  Kang and Zhihao Gavin Tang", "title": "Online Submodular Maximization with Free Disposal: Randomization Beats\n  0.25 for Partition Matroids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the online submodular maximization problem with free disposal under\na matroid constraint. Elements from some ground set arrive one by one in\nrounds, and the algorithm maintains a feasible set that is independent in the\nunderlying matroid. In each round when a new element arrives, the algorithm may\naccept the new element into its feasible set and possibly remove elements from\nit, provided that the resulting set is still independent. The goal is to\nmaximize the value of the final feasible set under some monotone submodular\nfunction, to which the algorithm has oracle access.\n  For $k$-uniform matroids, we give a deterministic algorithm with competitive\nratio at least $0.2959$, and the ratio approaches $\\frac{1}{\\alpha_\\infty}\n\\approx 0.3178$ as $k$ approaches infinity, improving the previous best ratio\nof $0.25$ by Chakrabarti and Kale (IPCO 2014), Buchbinder et al. (SODA 2015)\nand Chekuri et al. (ICALP 2015). We also show that our algorithm is optimal\namong a class of deterministic monotone algorithms that accept a new arriving\nelement only if the objective is strictly increased.\n  Further, we prove that no deterministic monotone algorithm can be strictly\nbetter than $0.25$-competitive even for partition matroids, the most modest\ngeneralization of $k$-uniform matroids, matching the competitive ratio by\nChakrabarti and Kale (IPCO 2014) and Chekuri et al. (ICALP 2015).\nInterestingly, we show that randomized algorithms are strictly more powerful by\ngiving a (non-monotone) randomized algorithm for partition matroids with ratio\n$\\frac{1}{\\alpha_\\infty} \\approx 0.3178$.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 07:53:29 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Chan", "T-H. Hubert", ""], ["Huang", "Zhiyi", ""], ["Jiang", "Shaofeng H. -C.", ""], ["Kang", "Ning", ""], ["Tang", "Zhihao Gavin", ""]]}, {"id": "1610.07778", "submitter": "Marcin Pilipczuk", "authors": "D\\'aniel Marx and Marcin Pilipczuk", "title": "Subexponential parameterized algorithms for graphs of polynomial growth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for a number of parameterized problems for which only $2^{O(k)}\nn^{O(1)}$ time algorithms are known on general graphs, subexponential\nparameterized algorithms with running time $2^{O(k^{1-\\frac{1}{1+\\delta}}\n\\log^2 k)} n^{O(1)}$ are possible for graphs of polynomial growth with growth\nrate (degree) $\\delta$, that is, if we assume that every ball of radius $r$\ncontains only $O(r^\\delta)$ vertices. The algorithms use the technique of\nlow-treewidth pattern covering, introduced by Fomin et al. [FOCS 2016] for\nplanar graphs; here we show how this strategy can be made to work for graphs\nwith polynomial growth.\n  Formally, we prove that, given a graph $G$ of polynomial growth with growth\nrate $\\delta$ and an integer $k$, one can in randomized polynomial time find a\nsubset $A \\subseteq V(G)$ such that on one hand the treewidth of $G[A]$ is\n$O(k^{1-\\frac{1}{1+\\delta}} \\log k)$, and on the other hand for every set $X\n\\subseteq V(G)$ of size at most $k$, the probability that $X \\subseteq A$ is\n$2^{-O(k^{1-\\frac{1}{1+\\delta}} \\log^2 k)}$. Together with standard dynamic\nprogramming techniques on graphs of bounded treewidth, this statement gives\nsubexponential parameterized algorithms for a number of subgraph search\nproblems, such as Long Path or Steiner Tree, in graphs of polynomial growth.\n  We complement the algorithm with an almost tight lower bound for Long Path:\nunless the Exponential Time Hypothesis fails, no parameterized algorithm with\nrunning time $2^{k^{1-\\frac{1}{\\delta}-\\varepsilon}}n^{O(1)}$ is possible for\nany $\\varepsilon > 0$ and an integer $\\delta \\geq 3$.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 08:19:11 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Marx", "D\u00e1niel", ""], ["Pilipczuk", "Marcin", ""]]}, {"id": "1610.08096", "submitter": "Hossein Esfandiari", "authors": "Mohammadhossein Bateni, Hossein Esfandiari, Vahab Mirrokni", "title": "Almost Optimal Streaming Algorithms for Coverage Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum coverage and minimum set cover problems --collectively called\ncoverage problems-- have been studied extensively in streaming models. However,\nprevious research not only achieve sub-optimal approximation factors and space\ncomplexities, but also study a restricted set arrival model which makes an\nexplicit or implicit assumption on oracle access to the sets, ignoring the\ncomplexity of reading and storing the whole set at once. In this paper, we\naddress the above shortcomings, and present algorithms with improved\napproximation factor and improved space complexity, and prove that our results\nare almost tight. Moreover, unlike most of previous work, our results hold on a\nmore general edge arrival model. More specifically, we present (almost) optimal\napproximation algorithms for maximum coverage and minimum set cover problems in\nthe streaming model with an (almost) optimal space complexity of\n$\\tilde{O}(n)$, i.e., the space is {\\em independent of the size of the sets or\nthe size of the ground set of elements}. These results not only improve over\nthe best known algorithms for the set arrival model, but also are the first\nsuch algorithms for the more powerful {\\em edge arrival} model. In order to\nachieve the above results, we introduce a new general sketching technique for\ncoverage functions: This sketching scheme can be applied to convert an\n$\\alpha$-approximation algorithm for a coverage problem to a\n$(1-\\eps)\\alpha$-approximation algorithm for the same problem in streaming, or\nRAM models. We show the significance of our sketching technique by ruling out\nthe possibility of solving coverage problems via accessing (as a black box) a\n$(1 \\pm \\eps)$-approximate oracle (e.g., a sketch function) that estimates the\ncoverage function on any subfamily of the sets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 21:12:03 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 04:54:11 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Bateni", "Mohammadhossein", ""], ["Esfandiari", "Hossein", ""], ["Mirrokni", "Vahab", ""]]}, {"id": "1610.08111", "submitter": "Ritu Kundu", "authors": "Costas Iliopoulos and Ritu Kundu and Solon Pissis", "title": "Efficient Pattern Matching in Elastic-Degenerate Strings", "comments": "11 pages (without references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend the notion of gapped strings to elastic-degenerate\nstrings. An elastic-degenerate string can been seen as an ordered collection of\nk > 1 seeds (substrings/subpatterns) interleaved by elastic-degenerate symbols\nsuch that each elastic-degenerate symbol corresponds to a set of two or more\nvariable length strings. Here, we present an algorithm for solving the pattern\nmatching problem with (solid) pattern and elastic-degenerate text, running in\nO(N+{\\alpha}{\\gamma}nm) time; where m is the length of the given pattern; n and\nN are the length and total size of the given elastic-degenerate text,\nrespectively; {\\alpha} and {\\gamma} are small constants, respectively\nrepresenting the maximum number of strings in any elastic-degenerate symbol of\nthe text and the largest number of elastic-degenerate symbols spanned by any\noccurrence of the pattern in the text. The space used by the algorithm is\nlinear in the size of the input for a constant number of elastic-degenerate\nsymbols in the text; {\\alpha} and {\\gamma} are so small in real applications\nthat the algorithm is expected to work very efficiently in practice.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 22:38:08 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Iliopoulos", "Costas", ""], ["Kundu", "Ritu", ""], ["Pissis", "Solon", ""]]}, {"id": "1610.08154", "submitter": "Jessica Chang", "authors": "Jessica Chang, Samir Khuller, Koyel Mukherjee", "title": "LP Rounding and Combinatorial Algorithms for Minimizing Active and Busy\n  Time", "comments": "31 pages, originally appeared in SPAA 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider fundamental scheduling problems motivated by energy issues. In\nthis framework, we are given a set of jobs, each with a release time, deadline\nand required processing length. The jobs need to be scheduled on a machine so\nthat at most g jobs are active at any given time. The duration for which a\nmachine is active (i.e., \"on\") is referred to as its active time. The goal is\nto find a feasible schedule for all jobs, minimizing the total active time.\nWhen preemption is allowed at integer time points, we show that a minimal\nfeasible schedule already yields a 3-approximation (and this bound is tight)\nand we further improve this to a 2-approximation via LP rounding techniques.\nOur second contribution is for the non-preemptive version of this problem.\nHowever, since even asking if a feasible schedule on one machine exists is\nNP-hard, we allow for an unbounded number of virtual machines, each having\ncapacity of g. This problem is known as the busy time problem in the literature\nand a 4-approximation is known for this problem. We develop a new combinatorial\nalgorithm that gives a 3-approximation. Furthermore, we consider the preemptive\nbusy time problem, giving a simple and exact greedy algorithm when unbounded\nparallelism is allowed, i.e., g is unbounded. For arbitrary g, this yields an\nalgorithm that is 2-approximate.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 03:03:57 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Chang", "Jessica", ""], ["Khuller", "Samir", ""], ["Mukherjee", "Koyel", ""]]}, {"id": "1610.08305", "submitter": "Zhize Li", "authors": "Zhize Li, Jian Li, Hongwei Huo", "title": "Optimal In-Place Suffix Sorting", "comments": "36 pages. A disclaimer: https://suffixsorting.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The suffix array is a fundamental data structure for many applications that\ninvolve string searching and data compression. Designing time/space-efficient\nsuffix array construction algorithms has attracted significant attention and\nconsiderable advances have been made for the past 20 years. We obtain the\n\\emph{first} in-place suffix array construction algorithms that are optimal\nboth in time and space for (read-only) integer alphabets. Concretely, we make\nthe following contributions:\n  1. For integer alphabets, we obtain the first suffix sorting algorithm which\ntakes linear time and uses only $O(1)$ workspace (the workspace is the total\nspace needed beyond the input string and the output suffix array). The input\nstring may be modified during the execution of the algorithm, but should be\nrestored upon termination of the algorithm.\n  2. We strengthen the first result by providing the first in-place linear time\nalgorithm for read-only integer alphabets with $|\\Sigma|=O(n)$ (i.e., the input\nstring cannot be modified). This algorithm settles the open problem posed by\nFranceschini and Muthukrishnan in ICALP 2007. The open problem asked to design\nin-place algorithms in $o(n\\log n)$ time and ultimately, in $O(n)$ time for\n(read-only) integer alphabets with $|\\Sigma| \\leq n$. Our result is in fact\nslightly stronger since we allow $|\\Sigma|=O(n)$.\n  3. Besides, for the read-only general alphabets (i.e., only comparisons are\nallowed), we present an optimal in-place $O(n\\log n)$ time suffix sorting\nalgorithm, recovering the result obtained by Franceschini and Muthukrishnan\nwhich was an open problem posed by Manzini and Ferragina in ESA 2002.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 12:45:59 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 17:36:44 GMT"}, {"version": "v3", "created": "Sat, 18 Feb 2017 10:17:53 GMT"}, {"version": "v4", "created": "Fri, 10 Nov 2017 15:34:13 GMT"}, {"version": "v5", "created": "Mon, 28 May 2018 02:42:47 GMT"}, {"version": "v6", "created": "Fri, 9 Nov 2018 17:11:08 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Li", "Zhize", ""], ["Li", "Jian", ""], ["Huo", "Hongwei", ""]]}, {"id": "1610.08309", "submitter": "Edita Pelantova", "authors": "Christiane Frougny, Marta Pavelka, Edita Pelantova, Milena Svobodova", "title": "On-line algorithms for multiplication and division in real and complex\n  numeration systems", "comments": "Extended version of contribution on 23rd IEEE Symposium on Computer\n  Arithmetic ARITH23", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, Vol. 21 no. 3\n  , Discrete Algorithms (June 20, 2019) dmtcs:5569", "doi": "10.23638/DMTCS-21-3-14", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A positional numeration system is given by a base and by a set of digits. The\nbase is a real or complex number $\\beta$ such that $|\\beta|>1$, and the digit\nset $A$ is a finite set of digits including $0$. Thus a number can be seen as a\nfinite or infinite string of digits. An on-line algorithm processes the input\npiece-by-piece in a serial fashion. On-line arithmetic, introduced by Trivedi\nand Ercegovac, is a mode of computation where operands and results flow through\narithmetic units in a digit serial manner, starting with the most significant\ndigit.\n  In this paper, we first formulate a generalized version of the on-line\nalgorithms for multiplication and division of Trivedi and Ercegovac for the\ncases that $\\beta$ is any real or complex number, and digits are real or\ncomplex. We then define the so-called OL Property, and show that if $(\\beta,\nA)$ has the OL Property, then on-line multiplication and division are feasible\nby the Trivedi-Ercegovac algorithms. For a real base $\\beta$ and a digit set\n$A$ of contiguous integers, the system $(\\beta, A)$ has the OL Property if $\\#\nA > |\\beta|$. For a complex base $\\beta$ and symmetric digit set $A$ of\ncontiguous integers, the system $(\\beta, A)$ has the OL Property if $\\# A >\n\\beta\\overline{\\beta} + |\\beta + \\overline{\\beta}|$. Provided that addition and\nsubtraction are realizable in parallel in the system $(\\beta, A)$ and that\npreprocessing of the denominator is possible, our on-line algorithms for\nmultiplication and division have linear time complexity. Three examples are\npresented in detail: base $\\beta=\\frac{3+\\sqrt{5}}{2}$ with digits\n$A=\\{-1,0,1\\}$; base $\\beta=2i$ with digits $A = \\{-2,-1, 0,1,2\\}$; and base\n$\\beta = -\\frac{3}{2} + i \\frac{\\sqrt{3}}{2} = -1 + \\omega$, where $\\omega =\n\\exp{\\frac{2i\\pi}{3}}$, with digits $A = \\{0, \\pm 1, \\pm \\omega, \\pm \\omega^2\n\\}$.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 13:05:12 GMT"}, {"version": "v2", "created": "Sun, 18 Feb 2018 11:04:16 GMT"}, {"version": "v3", "created": "Fri, 25 Jan 2019 10:07:38 GMT"}, {"version": "v4", "created": "Mon, 20 May 2019 09:12:15 GMT"}, {"version": "v5", "created": "Tue, 11 Jun 2019 16:16:23 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Frougny", "Christiane", ""], ["Pavelka", "Marta", ""], ["Pelantova", "Edita", ""], ["Svobodova", "Milena", ""]]}, {"id": "1610.08407", "submitter": "Palash Dey", "authors": "Palash Dey and Neeldhara Misra", "title": "On the Exact Amount of Missing Information that makes Finding Possible\n  Winners Hard", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider election scenarios with incomplete information, a situation that\narises often in practice. There are several models of incomplete information\nand accordingly, different notions of outcomes of such elections. In one\nwell-studied model of incompleteness, the votes are given by partial orders\nover the candidates. In this context we can frame the problem of finding a\npossible winner, which involves determining whether a given candidate wins in\nat least one completion of a given set of partial votes for a specific voting\nrule.\n  The possible winner problem is well-known to be NP-complete in general, and\nit is in fact known to be NP-complete for several voting rules where the number\nof undetermined pairs in every vote is bounded only by some constant. In this\npaper, we address the question of determining precisely the smallest number of\nundetermined pairs for which the possible winner problem remains NP-complete.\nIn particular, we find the exact values of $t$ for which the possible winner\nproblem transitions to being NP-complete from being in P, where $t$ is the\nmaximum number of undetermined pairs in every vote. We demonstrate tight\nresults for a broad subclass of scoring rules which includes all the commonly\nused scoring rules (such as plurality, veto, Borda, $k$-approval, and so on),\nCopeland$^\\alpha$ for every $\\alpha\\in[0,1]$, maximin, and Bucklin voting\nrules. A somewhat surprising aspect of our results is that for many of these\nrules, the possible winner problem turns out to be hard even if every vote has\nat most one undetermined pair of candidates.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 16:37:17 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Dey", "Palash", ""], ["Misra", "Neeldhara", ""]]}, {"id": "1610.08461", "submitter": "Gang Hu", "authors": "Gang Hu", "title": "A Proposed Algorithm for Minimum Vertex Cover Problem and its Testing", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents an algorithm for minimum vertex cover problem, which is an\nNP-Complete problem. The algorithm computes a minimum vertex cover of each\ninput simple graph. Tested by the attached MATLAB programs, Stage 1 of the\nalgorithm is applicable to, i.e., yields a proved minimum vertex cover for,\nabout 99.99% of the tested 610,000 graphs of order 16 and 99.67% of the tested\n1,200 graphs of order 32, and Stage 2 of the algorithm is applicable to all of\nthe above tested graphs. All of the tested graphs are randomly generated graphs\nof random \"edge density\" or in other words, random probability of each edge. It\nis proved that Stage 1 and Stage 2 of the algorithm run in $O(n^{5+logn})$ and\n$O(n^{3(5+logn)/2})$ time respectively, where $n$ is the order of input graph.\nBecause there is no theoretical proof yet that Stage 2 is applicable to all\ngraphs, further stages of the algorithm are proposed, which are in a general\nform that is consistent with Stages 1 and 2.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 14:36:20 GMT"}], "update_date": "2016-10-30", "authors_parsed": [["Hu", "Gang", ""]]}, {"id": "1610.08739", "submitter": "Andre Droschinsky", "authors": "Andre Droschinsky and Nils Kriege and Petra Mutzel", "title": "Finding Largest Common Substructures of Molecules in Quadratic Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the common structural features of two molecules is a fundamental task\nin cheminformatics. Most drugs are small molecules, which can naturally be\ninterpreted as graphs. Hence, the task is formalized as maximum common subgraph\nproblem. Albeit the vast majority of molecules yields outerplanar graphs this\nproblem remains NP-hard.\n  We consider a variation of the problem of high practical relevance, where the\nrings of molecules must not be broken, i.e., the block and bridge structure of\nthe input graphs must be retained by the common subgraph. We present an\nalgorithm for finding a maximum common connected induced subgraph of two given\nouterplanar graphs subject to this constraint. Our approach runs in time\n$\\mathcal{O}(\\Delta n^2)$ in outerplanar graphs on $n$ vertices with maximum\ndegree $\\Delta$. This leads to a quadratic time complexity in molecular graphs,\nwhich have bounded degree. The experimental comparison on synthetic and\nreal-world datasets shows that our approach is highly efficient in practice and\noutperforms comparable state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 12:16:01 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Droschinsky", "Andre", ""], ["Kriege", "Nils", ""], ["Mutzel", "Petra", ""]]}, {"id": "1610.08809", "submitter": "Safa Jammali", "authors": "Safa Jammali, Esaie Kuitche, Ayoub Rachati, Fran\\c{c}ois B\\'elanger,\n  Michelle Scott, A\\\"ida Ouangraoua", "title": "Aligning coding sequences with frameshift extension penalties", "comments": "24 pages, 4 figures", "journal-ref": "Algorithms for Molecular Biology, 2017, vol. 12, no 1, p. 10", "doi": "10.1186/s13015-017-0101-4", "report-no": null, "categories": "cs.DS q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frameshift translation is an important phenomenon that contributes to the\nappearance of novel Coding DNA Sequences (CDS) and functions in gene evolution,\nby allowing alternative amino acid translations of genes coding regions.\nFrameshift translations can be identified by aligning two CDS, from a same gene\nor from homologous genes, while accounting for their codon structure. Two main\nclasses of algorithms have been proposed to solve the problem of aligning CDS,\neither by amino acid sequence alignment back-translation, or by simultaneously\naccounting for the nucleotide and amino acid levels. The former does not allow\nto account for frameshift translations and up to now, the latter exclusively\naccounts for frameshift translation initiation, not accounting for the length\nof the translation disruption caused by a frameshift.\n  Here, we introduce a new scoring scheme with an algorithm for the pairwise\nalignment of CDS accounting for frameshift translation initiation and length,\nwhile simultaneously accounting for nucleotide and amino acid sequences. We\ncompare the method to other CDS alignment methods based on an application to\nthe comparison of pairs of CDS from homologous \\emph{human}, \\emph{mouse} and\n\\emph{cow} genes of ten mammalian gene families from the Ensembl-Compara\ndatabase. The results show that our method is particularly robust to parameter\nchanges as compared to existing methods. It also appears to be a good\ncompromise, performing well both in the presence and absence of frameshift\ntranslations between the CDS. An implementation of the method is available at\nhttps://github.com/UdeS-CoBIUS/FsePSA.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 14:43:48 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 17:04:29 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Jammali", "Safa", ""], ["Kuitche", "Esaie", ""], ["Rachati", "Ayoub", ""], ["B\u00e9langer", "Fran\u00e7ois", ""], ["Scott", "Michelle", ""], ["Ouangraoua", "A\u00efda", ""]]}, {"id": "1610.08820", "submitter": "Abdolahad Noori Zehmakan", "authors": "Abdolahad Noori Zehmakan", "title": "Bin Packing Problem: A Linear Constant-Space 3/2-Approximation Algorithm", "comments": "19 pages, 11 figures, International Journal on Computational Science\n  & Applications (IJCSA) Vol.5,No.6, December 2015. arXiv admin note: text\n  overlap with arXiv:1509.06048", "journal-ref": null, "doi": "10.5121/ijcsa.2015.5601", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the Bin Packing Problem (BPP) is one of the main NP-hard problems, a\nlot of approximation algorithms have been suggested for it. It has been proven\nthat the best algorithm for BPP has the approximation ratio of 3/2 and the time\norder of O(n), unless P=NP. In the current paper, a linear 3/2-approximation\nalgorithm is presented. The suggested algorithm not only has the best possible\ntheoretical factors, approximation ratio, space order, and time order, but also\noutperforms the other approximation algorithms according to the experimental\nresults, therefore, we are able to draw the conclusion that the algorithms is\nthe best approximation algorithm which has been presented for the problem until\nnow. Key words: Approximation Algorithm, Bin Packing Problem, Approximation\nRatio, NP-hard.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2016 09:49:58 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Zehmakan", "Abdolahad Noori", ""]]}, {"id": "1610.08906", "submitter": "Francisco Javier Marmolejo-Coss\\'io", "authors": "Paul W. Goldberg, Francisco J. Marmolejo-Coss\\'io, Zhiwei Steven Wu", "title": "Logarithmic Query Complexity for Approximate Nash Computation in Large\n  Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of equilibrium computation for \"large\" $n$-player\ngames. Large games have a Lipschitz-type property that no single player's\nutility is greatly affected by any other individual player's actions. In this\npaper, we mostly focus on the case where any change of strategy by a player\ncauses other players' payoffs to change by at most $\\frac{1}{n}$. We study\nalgorithms having query access to the game's payoff function, aiming to find\n$\\epsilon$-Nash equilibria. We seek algorithms that obtain $\\epsilon$ as small\nas possible, in time polynomial in $n$.\n  Our main result is a randomised algorithm that achieves $\\epsilon$\napproaching $\\frac{1}{8}$ for 2-strategy games in a {\\em completely uncoupled}\nsetting, where each player observes her own payoff to a query, and adjusts her\nbehaviour independently of other players' payoffs/actions. $O(\\log n)$\nrounds/queries are required. We also show how to obtain a slight improvement\nover $\\frac{1}{8}$, by introducing a small amount of communication between the\nplayers.\n  Finally, we give extension of our results to large games with more than two\nstrategies per player, and alternative largeness parameters.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 17:55:48 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Goldberg", "Paul W.", ""], ["Marmolejo-Coss\u00edo", "Francisco J.", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1610.08910", "submitter": "Tong Zhang", "authors": "Tong Zhang", "title": "Perfect Memory Context Trees in time series modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Stochastic Context Tree (SCOT) is a useful tool for studying infinite\nrandom sequences generated by an m-Markov Chain (m-MC). It captures the\nphenomenon that the probability distribution of the next state sometimes\ndepends on less than m of the preceding states. This allows compressing the\ninformation needed to describe an m-MC. The SCOT construction has been earlier\nused under various names: VLMC, VOMC, PST, CTW. In this paper we study the\npossibility of reducing the m-MC to a 1-MC on the leaves of the SCOT. Such\ncontext trees are called perfect-memory. We give various combinatorial\ncharacterizations of perfect-memory context trees and an efficient algorithm to\nfind the minimal perfect-memory extension of a SCOT.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 19:29:17 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Zhang", "Tong", ""]]}, {"id": "1610.09058", "submitter": "Riley Murray", "authors": "Riley Murray and Samir Khuller and Megan Chao", "title": "Scheduling Distributed Clusters of Parallel Machines: Primal-Dual and\n  LP-based Approximation Algorithms [Full Version]", "comments": "A shorter version of this paper (one that omitted several proofs)\n  appeared in the proceedings of the 2016 European Symposium on Algorithms", "journal-ref": "Leibniz International Proceedings in Informatics (LIPIcs), Volume\n  58, 2016, pages 68:1--68:17", "doi": "10.4230/LIPIcs.ESA.2016.68", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Map-Reduce computing framework rose to prominence with datasets of such\nsize that dozens of machines on a single cluster were needed for individual\njobs. As datasets approach the exabyte scale, a single job may need distributed\nprocessing not only on multiple machines, but on multiple clusters. We consider\na scheduling problem to minimize weighted average completion time of N jobs on\nM distributed clusters of parallel machines. In keeping with the scale of the\nproblems motivating this work, we assume that (1) each job is divided into M\n\"subjobs\" and (2) distinct subjobs of a given job may be processed\nconcurrently.\n  When each cluster is a single machine, this is the NP-Hard concurrent open\nshop problem. A clear limitation of such a model is that a serial processing\nassumption sidesteps the issue of how different tasks of a given subjob might\nbe processed in parallel. Our algorithms explicitly model clusters as pools of\nresources and effectively overcome this issue.\n  Under a variety of parameter settings, we develop two constant factor\napproximation algorithms for this problem. The first algorithm uses an LP\nrelaxation tailored to this problem from prior work. This LP-based algorithm\nprovides strong performance guarantees. Our second algorithm exploits a\nsurprisingly simple mapping to the special case of one machine per cluster.\nThis mapping-based algorithm is combinatorial and extremely fast. These are the\nfirst constant factor approximations for this problem.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 02:14:25 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Murray", "Riley", ""], ["Khuller", "Samir", ""], ["Chao", "Megan", ""]]}, {"id": "1610.09132", "submitter": "Martin Olsen", "authors": "Martin Olsen", "title": "Towards Asymptotically Optimal One-to-One PDP Algorithms for Capacity 2+\n  Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the one-to-one Pickup and Delivery Problem (PDP) in Euclidean\nSpace with arbitrary dimension $d$ where $n$ transportation requests are picked\ni.i.d. with a separate origin-destination pair for each object to be moved.\nFirst, we consider the problem from the customer perspective where the\nobjective is to compute a plan for transporting the objects such that the\nEuclidean distance traveled by the vehicles when carrying objects is minimized.\nWe develop a polynomial time asymptotically optimal algorithm for vehicles with\ncapacity $o(\\sqrt[2d]{n})$ for this case. This result also holds imposing LIFO\nconstraints for loading and unloading objects. Secondly, we extend our\nalgorithm to the classical single-vehicle PDP where the objective is to\nminimize the total distance traveled by the vehicle and present results\nindicating that the extended algorithm is asymptotically optimal for a fixed\nvehicle capacity if the origins and destinations are picked i.i.d. using the\nsame distribution.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 09:18:06 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Olsen", "Martin", ""]]}, {"id": "1610.09146", "submitter": "Satya Pramod Jammy", "authors": "Satya P. Jammy, Christian T. Jacobs, Neil D. Sandham", "title": "Performance evaluation of explicit finite difference algorithms with\n  varying amounts of computational and memory intensity", "comments": "Author accepted version. Accepted for publication in Journal of\n  Computational Science on 27 October 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.MS physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future architectures designed to deliver exascale performance motivate the\nneed for novel algorithmic changes in order to fully exploit their\ncapabilities. In this paper, the performance of several numerical algorithms,\ncharacterised by varying degrees of memory and computational intensity, are\nevaluated in the context of finite difference methods for fluid dynamics\nproblems. It is shown that, by storing some of the evaluated derivatives as\nsingle thread- or process-local variables in memory, or recomputing the\nderivatives on-the-fly, a speed-up of ~2 can be obtained compared to\ntraditional algorithms that store all derivatives in global arrays.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 09:45:31 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Jammy", "Satya P.", ""], ["Jacobs", "Christian T.", ""], ["Sandham", "Neil D.", ""]]}, {"id": "1610.09223", "submitter": "Paolo Penna", "authors": "Barbara Geissmann and Paolo Penna", "title": "Sort well with energy-constrained comparisons", "comments": null, "journal-ref": "Phys. Rev. E 97, 052108 (2018)", "doi": "10.1103/PhysRevE.97.052108", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study very simple sorting algorithms based on a probabilistic comparator\nmodel. In our model, errors in comparing two elements are due to (1) the energy\nor effort put in the comparison and (2) the difference between the compared\nelements. Such algorithms keep comparing pairs of randomly chosen elements, and\nthey correspond to Markovian processes. The study of these Markov chains\nreveals an interesting phenomenon. Namely, in several cases, the algorithm\nwhich repeatedly compares only adjacent elements is better than the one making\narbitrary comparisons: on the long-run, the former algorithm produces sequences\nthat are \"better sorted\". The analysis of the underlying Markov chain poses new\ninteresting questions as the latter algorithm yields a non-reversible chain and\ntherefore its stationary distribution seems difficult to calculate explicitly.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 14:03:11 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Geissmann", "Barbara", ""], ["Penna", "Paolo", ""]]}, {"id": "1610.09320", "submitter": "Daniele Gorla", "authors": "Pietro Cenciarelli, Daniele Gorla, Ivano Salvo", "title": "A Polynomial-time Algorithm for Detecting the Possibility of Braess\n  Paradox in Directed Graphs", "comments": "arXiv admin note: text overlap with arXiv:1603.01983", "journal-ref": null, "doi": "10.1007/s00453-018-0486-6", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A directed multigraph is said vulnerable if it can generate Braess paradox in\nTraffic Networks. In this paper, we give a graph-theoretic characterisation of\nvulnerable directed multigraphs; analogous results appeared in the literature\nonly for undirected multigraphs and for a specific family of directed\nmultigraphs. The proof of our characterisation also provides an algorithm that\nchecks if a multigraph is vulnerable in O(|V| |E|^2); this is the first\npolynomial time algorithm that checks vulnerability for general directed\nmultigraphs. The resulting algorithm also contributes to another well known\nproblem, i.e. the directed subgraph homeomorphism problem without node mapping,\nby providing another pattern graph for which a polynomial time algorithm\nexists.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 17:21:39 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 16:05:11 GMT"}, {"version": "v3", "created": "Thu, 15 Dec 2016 23:02:54 GMT"}, {"version": "v4", "created": "Tue, 10 Oct 2017 16:04:40 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Cenciarelli", "Pietro", ""], ["Gorla", "Daniele", ""], ["Salvo", "Ivano", ""]]}, {"id": "1610.09411", "submitter": "C. Seshadhri", "authors": "Ali Pinar and C. Seshadhri and V. Vishal", "title": "ESCAPE: Efficiently Counting All 5-Vertex Subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting the frequency of small subgraphs is a fundamental technique in\nnetwork analysis across various domains, most notably in bioinformatics and\nsocial networks. The special case of triangle counting has received much\nattention. Getting results for 4-vertex or 5-vertex patterns is highly\nchallenging, and there are few practical results known that can scale to\nmassive sizes.\n  We introduce an algorithmic framework that can be adopted to count any small\npattern in a graph and apply this framework to compute exact counts for\n\\emph{all} 5-vertex subgraphs. Our framework is built on cutting a pattern into\nsmaller ones, and using counts of smaller patterns to get larger counts.\nFurthermore, we exploit degree orientations of the graph to reduce runtimes\neven further. These methods avoid the combinatorial explosion that typical\nsubgraph counting algorithms face. We prove that it suffices to enumerate only\nfour specific subgraphs (three of them have less than 5 vertices) to exactly\ncount all 5-vertex patterns.\n  We perform extensive empirical experiments on a variety of real-world graphs.\nWe are able to compute counts of graphs with tens of millions of edges in\nminutes on a commodity machine. To the best of our knowledge, this is the first\npractical algorithm for $5$-vertex pattern counting that runs at this scale. A\nstepping stone to our main algorithm is a fast method for counting all\n$4$-vertex patterns. This algorithm is typically ten times faster than the\nstate of the art $4$-vertex counters.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 21:49:44 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Pinar", "Ali", ""], ["Seshadhri", "C.", ""], ["Vishal", "V.", ""]]}, {"id": "1610.09486", "submitter": "Gennaro Cordasco PhD", "authors": "Gennaro Cordasco, Luisa Gargano, Adele Anna Rescigno and Ugo Vaccaro", "title": "Evangelism in Social Networks: Algorithms and Complexity", "comments": "An extended abstract of this paper was presented at the 27th\n  International Workshop on Combinatorial Algorithms (IWOCA 2016), Helsinki,\n  Finland, August 17-19, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI math.CO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a population of interconnected individuals that, with respect to\na piece of information, at each time instant can be subdivided into three\n(time-dependent) categories: agnostics, influenced, and evangelists. A\ndynamical process of information diffusion evolves among the individuals of the\npopulation according to the following rules. Initially, all individuals are\nagnostic. Then, a set of people is chosen from the outside and convinced to\nstart evangelizing, i.e., to start spreading the information. When a number of\nevangelists, greater than a given threshold, communicate with a node v, the\nnode v becomes influenced, whereas, as soon as the individual v is contacted by\na sufficiently much larger number of evangelists, it is itself converted into\nan evangelist and consequently it starts spreading the information. The\nquestion is: How to choose a bounded cardinality initial set of evangelists so\nas to maximize the final number of influenced individuals? We prove that the\nproblem is hard to solve, even in an approximate sense. On the positive side,\nwe present exact polynomial time algorithms for trees and complete graphs. For\ngeneral graphs, we derive exact parameterized algorithms. We also investigate\nthe problem when the objective is to select a minimum number of evangelists\ncapable of influencing the whole network. Our motivations to study these\nproblems come from the areas of Viral Marketing and the analysis of\nquantitative models of spreading of influence in social networks.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 11:00:10 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Cordasco", "Gennaro", ""], ["Gargano", "Luisa", ""], ["Rescigno", "Adele Anna", ""], ["Vaccaro", "Ugo", ""]]}, {"id": "1610.09623", "submitter": "Genevieve Simonet", "authors": "Anne Berry and Genevi\\`eve Simonet", "title": "Computing a clique tree with algorithm MLS (Maximal Label Search)", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithm MLS (Maximal Label Search) is a graph search algorithm which\ngeneralizes algorithms MCS, LexBFS, LexDFS and MNS. On a chordal graph, MLS\ncomputes a peo (perfect elimination ordering) of the graph. We show how\nalgorithm MLS can be modified to compute a pmo (perfect moplex ordering) as\nwell as a clique tree and the minimal separators of a chordal graph. We give a\nnecessary and sufficient condition on the labeling structure for the beginning\nof a new clique in the clique tree to be detected by a condition on labels. MLS\nis also used to compute a clique tree of the complement graph, and new cliques\nin the complement graph can be detected by a condition on labels for any\nlabeling structure. A linear time algorithm computing a pmo and the generators\nof the maximal cliques and minimal separators w.r.t. this pmo of the complement\ngraph is provided. On a non-chordal graph, algorithm MLSM is used to compute an\natom tree of the clique minimal separator decomposition of any graph.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 09:30:42 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Berry", "Anne", ""], ["Simonet", "Genevi\u00e8ve", ""]]}, {"id": "1610.09679", "submitter": "Carlo Comin", "authors": "Carlo Comin, Romeo Rizzi", "title": "Linear-Time Safe-Alternating DFS and SCCs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An alternating graph is a directed graph whose vertex set is partitioned into\ntwo classes, existential and universal. This forms the basic arena for a\nplethora of infinite duration two-player games where Player~$\\square$\nand~$\\ocircle$ alternate in a turn-based sliding of a pebble along the arcs\nthey control. We study alternating strongly-connectedness as a generalization\nof strongly-connectedness in directed graphs, aiming at providing a linear time\ndecomposition and a sound structural graph characterization. For this a refined\nnotion of alternating reachability is introduced: Player~$\\square$ attempts to\nreach vertices without leaving a prescribed subset of the vertices, while\nPlayer~$\\ocircle$ works against. This is named \\emph{safe alternating\nreachability}. It is shown that every arena uniquely decomposes into safe\nalternating strongly-connected components where Player~$\\square$ can visit each\nvertex within a given component infinitely often, without having to ever leave\nout the component itself. Our main result is a linear time algorithm for\ncomputing this alternating graph decomposition. Both the underlying graph\nstructures and the algorithm generalize the classical decomposition of a\ndirected graph into strongly-connected components. The algorithm builds on a\nlinear time generalization of the depth-first search on alternation, taking\ninspiration from Tarjan 1972 machinery. Our theory has direct applications in\nsolving well-known infinite duration pebble games faster. Dinneen and\nKhoussainov showed in 1999 that deciding a given Update Game costs $O(mn)$\ntime, where $n$ is the number of vertices and $m$ is that of arcs. We solve the\ntask in $\\Theta(m+n)$ linear~time. The complexity of Explicit\nMcNaughton-M\\\"uller Games also improves from cubic to quadratic.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 17:22:00 GMT"}, {"version": "v2", "created": "Sat, 19 Nov 2016 20:23:11 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 13:07:27 GMT"}, {"version": "v4", "created": "Mon, 28 Nov 2016 11:01:58 GMT"}, {"version": "v5", "created": "Sun, 9 Jun 2019 10:18:10 GMT"}, {"version": "v6", "created": "Fri, 5 Jun 2020 21:19:02 GMT"}, {"version": "v7", "created": "Wed, 10 Jun 2020 16:46:03 GMT"}, {"version": "v8", "created": "Sat, 5 Dec 2020 16:40:48 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Comin", "Carlo", ""], ["Rizzi", "Romeo", ""]]}, {"id": "1610.09732", "submitter": "Kuitche Kamela Esaie", "authors": "Esaie Kuitche, Manuel Lafond and A\\\"ida Ouangraoua", "title": "Reconstructing protein and gene phylogenies by extending the framework\n  of reconciliation", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.PE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The architecture of eukaryotic coding genes allows the production of several\ndifferent protein isoforms by genes. Current gene phylogeny reconstruction\nmethods make use of a single protein product per gene, ignoring information on\nalternative protein isoforms. These methods often lead to inaccurate gene tree\nreconstructions that require to be corrected before being used in phylogenetic\ntree reconciliation analyses or gene products phylogeny reconstructions. Here,\nwe propose a new approach for the reconstruction of accurate gene trees and\nprotein trees accounting for the production of alternative protein isoforms by\nthe genes of a gene family. We extend the concept of reconciliation to protein\ntrees, and we define a new reconciliation problem called MinDRGT that consists\nin finding a gene tree that minimizes a double reconciliation cost with a given\nprotein tree and a given species tree. We define a second problem called\nMinDRPGT that consists in finding a protein tree and a gene tree minimizing a\ndouble reconciliation cost, given a species tree and a set of protein subtrees.\nWe provide algorithmic exact and heuristic solutions for some versions of the\nproblems, and we present the results of an application to the correction of\ngene trees from the Ensembl database. An implementation of the heuristic method\nis available at https://github.com/UdeS-CoBIUS/Protein2GeneTree.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 23:52:21 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 14:38:54 GMT"}, {"version": "v3", "created": "Mon, 3 Jul 2017 18:28:28 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Kuitche", "Esaie", ""], ["Lafond", "Manuel", ""], ["Ouangraoua", "A\u00efda", ""]]}, {"id": "1610.09800", "submitter": "Deeparnab Chakrabarty", "authors": "Deeparnab Chakrabarty and Yin Tat Lee and Aaron Sidford and Sam\n  Chiu-wai Wong", "title": "Subquadratic Submodular Function Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular function minimization (SFM) is a fundamental discrete optimization\nproblem which generalizes many well known problems, has applications in various\nfields, and can be solved in polynomial time. Owing to applications in computer\nvision and machine learning, fast SFM algorithms are highly desirable. The\ncurrent fastest algorithms [Lee, Sidford, Wong, FOCS 2015] run in $O(n^{2}\\log\nnM\\cdot\\textrm{EO} +n^{3}\\log^{O(1)}nM)$ time and $O(n^{3}\\log^{2}n\\cdot\n\\textrm{EO} +n^{4}\\log^{O(1)}n$) time respectively, where $M$ is the largest\nabsolute value of the function (assuming the range is integers) and\n$\\textrm{EO}$ is the time taken to evaluate the function on any set. Although\nthe best known lower bound on the query complexity is only $\\Omega(n)$, the\ncurrent shortest non-deterministic proof certifying the optimum value of a\nfunction requires $\\Theta(n^{2})$ function evaluations.\n  The main contribution of this paper are subquadratic SFM algorithms. For\ninteger-valued submodular functions, we give an SFM algorithm which runs in\n$O(nM^{3}\\log n\\cdot\\textrm{EO})$ time giving the first nearly linear time\nalgorithm in any known regime. For real-valued submodular functions with range\nin $[-1,1]$, we give an algorithm which in\n$\\tilde{O}(n^{5/3}\\cdot\\textrm{EO}/\\varepsilon^{2})$ time returns an\n$\\varepsilon$-additive approximate solution. At the heart of it, our algorithms\nare projected stochastic subgradient descent methods on the Lovasz extension of\nsubmodular functions where we crucially exploit submodularity and data\nstructures to obtain fast, i.e. sublinear time subgradient updates. . The\nlatter is crucial for beating the $n^{2}$ bound as we show that algorithms\nwhich access only subgradients of the Lovasz extension, and these include the\ntheoretically best algorithms mentioned above, must make $\\Omega(n)$\nsubgradient calls (even for functions whose range is $\\{-1,0,1\\}$).\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 06:13:41 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Lee", "Yin Tat", ""], ["Sidford", "Aaron", ""], ["Wong", "Sam Chiu-wai", ""]]}, {"id": "1610.09806", "submitter": "Andrew Conway", "authors": "Andrew R. Conway", "title": "The design of efficient algorithms for enumeration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms have been developed for enumerating various combinatorial\nobjects in time exponentially less than the number of objects. Two common\nclasses of algorithms are dynamic programming and the transfer matrix method.\nThis paper covers the design and implementation of such algorithms.\n  A host of general techniques for improving efficiency are described. Three\nquite different example problems are used for examples: 1324 pattern avoiding\npermutations, three-dimensional polycubes, and two-dimensional directed\nanimals.\n  For those new to the field, this paper is designed to be an introduction to\nmany of the tricks for producing efficient enumeration algorithms. For those\nmore experienced, it will hopefully help them understand the interrelationship\nand implications of a variety of techniques, many or most of which will be\nfamiliar. The author certainly found his understanding improved as a result of\nwriting this paper.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 07:09:34 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 11:17:22 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Conway", "Andrew R.", ""]]}, {"id": "1610.09984", "submitter": "Alessandro Epasto", "authors": "Alessandro Epasto, Silvio Lattanzi, Sergei Vassilvitskii, Morteza\n  Zadimoghaddam", "title": "Submodular Optimization over Sliding Windows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximizing submodular functions under cardinality constraints lies at the\ncore of numerous data mining and machine learning applications, including data\ndiversification, data summarization, and coverage problems. In this work, we\nstudy this question in the context of data streams, where elements arrive one\nat a time, and we want to design low-memory and fast update-time algorithms\nthat maintain a good solution. Specifically, we focus on the sliding window\nmodel, where we are asked to maintain a solution that considers only the last\n$W$ items.\n  In this context, we provide the first non-trivial algorithm that maintains a\nprovable approximation of the optimum using space sublinear in the size of the\nwindow. In particular we give a $\\frac{1}{3} - \\epsilon$ approximation\nalgorithm that uses space polylogarithmic in the spread of the values of the\nelements, $\\Phi$, and linear in the solution size $k$ for any constant\n$\\epsilon > 0$ . At the same time, processing each element only requires a\npolylogarithmic number of evaluations of the function itself. When a better\napproximation is desired, we show a different algorithm that, at the cost of\nusing more memory, provides a $\\frac{1}{2} - \\epsilon$ approximation and allows\na tunable trade-off between average update time and space. This algorithm\nmatches the best known approximation guarantees for submodular optimization in\ninsertion-only streams, a less general formulation of the problem.\n  We demonstrate the efficacy of the algorithms on a number of real world\ndatasets, showing that their practical performance far exceeds the theoretical\nbounds. The algorithms preserve high quality solutions in streams with millions\nof items, while storing a negligible fraction of them.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 15:48:24 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Epasto", "Alessandro", ""], ["Lattanzi", "Silvio", ""], ["Vassilvitskii", "Sergei", ""], ["Zadimoghaddam", "Morteza", ""]]}]