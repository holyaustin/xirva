[{"id": "1209.0056", "submitter": "Brendan Juba", "authors": "Brendan Juba", "title": "Learning implicitly in reasoning in PAC-Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of answering queries about formulas of propositional\nlogic based on background knowledge partially represented explicitly as other\nformulas, and partially represented as partially obscured examples\nindependently drawn from a fixed probability distribution, where the queries\nare answered with respect to a weaker semantics than usual -- PAC-Semantics,\nintroduced by Valiant (2000) -- that is defined using the distribution of\nexamples. We describe a fairly general, efficient reduction to limited versions\nof the decision problem for a proof system (e.g., bounded space treelike\nresolution, bounded degree polynomial calculus, etc.) from corresponding\nversions of the reasoning problem where some of the background knowledge is not\nexplicitly given as formulas, only learnable from the examples. Crucially, we\ndo not generate an explicit representation of the knowledge extracted from the\nexamples, and so the \"learning\" of the background knowledge is only done\nimplicitly. As a consequence, this approach can utilize formulas as background\nknowledge that are not perfectly valid over the distribution---essentially the\nanalogue of agnostic learning here.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2012 05:13:00 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Juba", "Brendan", ""]]}, {"id": "1209.0375", "submitter": "Zdenek Dvorak", "authors": "Zdenek Dvorak and Vojtech Tuma", "title": "A dynamic data structure for counting subgraphs in sparse graphs", "comments": "27 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dynamic data structure representing a graph G, which allows\naddition and removal of edges from G and can determine the number of\nappearances of a graph of a bounded size as an induced subgraph of G. The\nqueries are answered in constant time. When the data structure is used to\nrepresent graphs from a class with bounded expansion (which includes planar\ngraphs and more generally all proper classes closed on topological minors, as\nwell as many other natural classes of graphs with bounded average degree), the\namortized time complexity of updates is polylogarithmic.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2012 15:00:41 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2013 07:51:02 GMT"}], "update_date": "2013-01-04", "authors_parsed": [["Dvorak", "Zdenek", ""], ["Tuma", "Vojtech", ""]]}, {"id": "1209.0533", "submitter": "David Harvey", "authors": "David Harvey", "title": "A subquadratic algorithm for computing the n-th Bernoulli number", "comments": "few minor changes, to appear in Mathematics of Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new algorithm that computes the n-th Bernoulli number in n^(4/3\n+ o(1)) bit operations. This improves on previous algorithms that had\ncomplexity n^(2 + o(1)).\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 06:19:35 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2012 05:58:10 GMT"}, {"version": "v3", "created": "Wed, 1 May 2013 06:04:07 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Harvey", "David", ""]]}, {"id": "1209.0572", "submitter": "A. Emre Cetin", "authors": "A. Emre Cetin", "title": "In-place associative integer sorting", "comments": "25 pages. arXiv admin note: substantial text overlap with\n  arXiv:1209.3668, arXiv:1210.1771, arXiv:1209.1942, arXiv:1209.4714", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A novel integer value-sorting technique is proposed replacing bucket sort,\ndistribution counting sort and address calculation sort family of algorithms.\nIt requires only constant amount of additional memory. The technique is\ninspired from one of the ordinal theories of \"serial order in behavior\" and\nexplained by the analogy with the three main stages in the formation and\nretrieval of memory in cognitive neuroscience namely (i) practicing, (ii)\nstoring and (iii) retrieval.\n  Although not suitable for integer rank-sorting where the problem is to put an\narray of elements into ascending or descending order by their numeric keys,\neach of which is an integer, the technique seems to be efficient and applicable\nto rank-sorting, as well as other problems such as hashing, searching, element\ndistinction, succinct data structures, gaining space, etc.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 09:10:28 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2012 14:14:41 GMT"}, {"version": "v3", "created": "Thu, 1 Nov 2012 14:28:30 GMT"}], "update_date": "2012-11-02", "authors_parsed": [["Cetin", "A. Emre", ""]]}, {"id": "1209.0598", "submitter": "Patrizio Angelini", "authors": "Patrizio Angelini, Marco Di Bartolomeo, Giuseppe Di Battista", "title": "Implementing a Partitioned 2-page Book Embedding Testing Algorithm", "comments": "21 pages, 11 figures, published at 20th International Symposyum on\n  Graph Drawing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a book embedding the vertices of a graph are placed on the \"spine\" of a\n\"book\" and the edges are assigned to \"pages\" so that edges on the same page do\nnot cross. In the Partitioned 2-page Book Embedding problem egdes are\npartitioned into two sets E_1 and E_2, the pages are two, the edges of E_1 are\nassigned to page 1, and the edges of E_2 are assigned to page 2. The problem\nconsists of checking if an ordering of the vertices exists along the spine so\nthat the edges of each page do not cross. Hong and Nagamochi give an\ninteresting and complex linear time algorithm for tackling Partitioned 2-page\nBook Embedding based on SPQR-trees. We show an efficient implementation of this\nalgorithm and show its effectiveness by performing a number of experimental\ntests. Because of the relationships between Partitioned 2-page Book Embedding\nand clustered planarity we yield as a side effect an implementation of a\nclustered planarity testing where the graph has exactly two clusters.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 10:45:47 GMT"}], "update_date": "2012-09-05", "authors_parsed": [["Angelini", "Patrizio", ""], ["Di Bartolomeo", "Marco", ""], ["Di Battista", "Giuseppe", ""]]}, {"id": "1209.0700", "submitter": "Jens M. Schmidt", "authors": "Jens M. Schmidt", "title": "A Simple Test on 2-Vertex- and 2-Edge-Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing a graph on 2-vertex- and 2-edge-connectivity are two fundamental\nalgorithmic graph problems. For both problems, different linear-time algorithms\nwith simple implementations are known. Here, an even simpler linear-time\nalgorithm is presented that computes a structure from which both the 2-vertex-\nand 2-edge-connectivity of a graph can be easily \"read off\". The algorithm\ncomputes all bridges and cut vertices of the input graph in the same time.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 16:46:34 GMT"}], "update_date": "2012-09-05", "authors_parsed": [["Schmidt", "Jens M.", ""]]}, {"id": "1209.0730", "submitter": "Hongchao Zhou", "authors": "Hongchao Zhou and Jehoshua Bruck", "title": "Streaming Algorithms for Optimal Generation of Random Bits", "comments": "2 columns, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating random bits from a source of biased coins (the biased is unknown)\nis a classical question that was originally studied by von Neumann. There are a\nnumber of known algorithms that have asymptotically optimal information\nefficiency, namely, the expected number of generated random bits per input bit\nis asymptotically close to the entropy of the source. However, only the\noriginal von Neumann algorithm has a `streaming property' - it operates on a\nsingle input bit at a time and it generates random bits when possible, alas, it\ndoes not have an optimal information efficiency.\n  The main contribution of this paper is an algorithm that generates random bit\nstreams from biased coins, uses bounded space and runs in expected linear time.\nAs the size of the allotted space increases, the algorithm approaches the\ninformation-theoretic upper bound on efficiency. In addition, we discuss how to\nextend this algorithm to generate random bit streams from m-sided dice or\ncorrelated sources such as Markov chains.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 18:51:22 GMT"}], "update_date": "2012-09-05", "authors_parsed": [["Zhou", "Hongchao", ""], ["Bruck", "Jehoshua", ""]]}, {"id": "1209.0756", "submitter": "Olga Ohrimenko", "authors": "Michael T. Goodrich and Olga Ohrimenko and Roberto Tamassia", "title": "Data-Oblivious Graph Drawing Model and Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study graph drawing in a cloud-computing context where data is stored\nexternally and processed using a small local working storage. We show that a\nnumber of classic graph drawing algorithms can be efficiently implemented in\nsuch a framework where the client can maintain privacy while constructing a\ndrawing of her graph.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 19:51:16 GMT"}], "update_date": "2012-09-05", "authors_parsed": [["Goodrich", "Michael T.", ""], ["Ohrimenko", "Olga", ""], ["Tamassia", "Roberto", ""]]}, {"id": "1209.0871", "submitter": "Adria Alcala Mena", "authors": "Adria Alcala Mena, Francesc Rossello", "title": "Ternary graph isomorphism in polynomial time, after Luks", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph isomorphism problem has a long history in mathematics and computer\nscience, with applications in computational chemistry and biology, and it is\nbelieved to be neither solvable in polynomial time nor NP-complete. E. Luks\nproposed in 1982 the best algorithm so far for the solution of this problem,\nwhich moreover runs in polynomial time if an upper bound for the degrees of the\nnodes in the graphs is taken as a constant. Unfortunately, Luks' algorithm is\npurely theoretical, very difficult to use in practice, and, in particular, we\nhave not been able to find any implementation of it in the literature. The main\ngoal of this paper is to present an efficient implementation of this algorithm\nfor ternary graphs in the SAGE system, as well as an adaptation to fully\nresolved rooted phylogenetic networks on a given set of taxa.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 06:41:26 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Mena", "Adria Alcala", ""], ["Rossello", "Francesc", ""]]}, {"id": "1209.1040", "submitter": "Adria Alcala Mena", "authors": "Adria Alcala Mena", "title": "Trivalent Graph isomorphism in polynomial time", "comments": "48 pages. It is a Master Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It's important to design polynomial time algorithms to test if two graphs are\nisomorphic at least for some special classes of graphs.\n  An approach to this was presented by Eugene M. Luks(1981) in the work\n\\textit{Isomorphism of Graphs of Bounded Valence Can Be Tested in Polynomial\nTime}. Unfortunately, it was a theoretical algorithm and was very difficult to\nput into practice. On the other hand, there is no known implementation of the\nalgorithm, although Galil, Hoffman and Luks(1983) shows an improvement of this\nalgorithm running in $O(n^3 \\log n)$.\n  The two main goals of this master thesis are to explain more carefully the\nalgorithm of Luks(1981), including a detailed study of the complexity and, then\nto provide an efficient implementation in SAGE system. It is divided into four\nchapters plus an appendix.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 16:54:02 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Mena", "Adria Alcala", ""]]}, {"id": "1209.1045", "submitter": "I Made Agus Dwi Suarjaya", "authors": "I. Made Agus Dwi Suarjaya", "title": "A New Algorithm for Data Compression Optimization", "comments": "4 pages", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications (IJACSA), Volume 3 Issue 8, 2012, 14-17", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People tend to store a lot of files inside theirs storage. When the storage\nnears it limit, they then try to reduce those files size to minimum by using\ndata compression software. In this paper we propose a new algorithm for data\ncompression, called j-bit encoding (JBE). This algorithm will manipulates each\nbit of data inside file to minimize the size without losing any data after\ndecoding which is classified to lossless compression. This basic algorithm is\nintended to be combining with other data compression algorithms to optimize the\ncompression ratio. The performance of this algorithm is measured by comparing\ncombination of different data compression algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 17:05:21 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Suarjaya", "I. Made Agus Dwi", ""]]}, {"id": "1209.1082", "submitter": "Lukasz Kowalik", "authors": "Andreas Bjorklund, Petteri Kaski, Lukasz Kowalik", "title": "Constrained Multilinear Detection and Generalized Graph Motifs", "comments": "Journal submission, 18 pages. The conference version (see\n  http://drops.dagstuhl.de/opus/volltexte/2013/3919/pdf/7.pdf) was presented at\n  STACS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new algebraic sieving technique to detect constrained\nmultilinear monomials in multivariate polynomial generating functions given by\nan evaluation oracle. As applications of the technique, we show an\n$O^*(2^k)$-time polynomial space algorithm for the $k$-sized Graph Motif\nproblem. We also introduce a new optimization variant of the problem, called\nClosest Graph Motif and solve it within the same time bound. The Closest Graph\nMotif problem encompasses several previously studied optimization variants,\nlike Maximum Graph Motif, Min-Substitute Graph Motif, and Min-Add Graph Motif.\nFinally, we provide a piece of evidence that our result might be essentially\ntight: the existence of an $O^*((2-\\epsilon)^k)$-time algorithm for the Graph\nMotif problem implies an $O((2-\\epsilon')^n)$-time algorithm for Set Cover.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 19:45:18 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2012 14:02:51 GMT"}, {"version": "v3", "created": "Tue, 14 May 2013 19:05:04 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Bjorklund", "Andreas", ""], ["Kaski", "Petteri", ""], ["Kowalik", "Lukasz", ""]]}, {"id": "1209.1645", "submitter": "Igor Sergeev", "authors": "Igor Sergeev", "title": "On additive complexity of a sequence of matrices", "comments": "7 pages, in English; 8 pages, in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show new upper and lower bounds for the complexity of implementation of a\nsequence of Boolean matrices proposed by Kaski et al. (arXiv:1208.0554) with\nadditive circuits.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2012 20:17:30 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Sergeev", "Igor", ""]]}, {"id": "1209.1721", "submitter": "Grigory Litvinov", "authors": "G.L. Litvinov", "title": "Idempotent and tropical mathematics. Complexity of algorithms and\n  interval analysis", "comments": "26 pages, submitted to the journal \"Computers and Mathematics with\n  Applications\". arXiv admin note: substantial text overlap with\n  arXiv:1001.4247, arXiv:1005.1247", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A very brief introduction to tropical and idempotent mathematics is\npresented. Tropical mathematics can be treated as a result of a dequantization\nof the traditional mathematics as the Planck constant tends to zero taking\nimaginary values. In the framework of idempotent mathematics usually\nconstructions and algorithms are more simple with respect to their traditional\nanalogs. We especially examine algorithms of tropical/idempotent mathematics\ngenerated by a collection of basic semiring (or semifield) operations and other\n\"good\" operations. Every algorithm of this type has an interval version. The\ncomplexity of this interval version coincides with the complexity of the\ninitial algorithm. The interval version of an algorithm of this type gives\nexact interval estimates for the corresponding output data. Algorithms of\nlinear algebra over idempotent and semirings are examined. In this case, basic\nalgorithms are polynomial as well as their interval versions. This situation is\nvery different from the traditional linear algebra, where basic algorithms are\npolynomial but the corresponding interval versions are NP-hard and interval\nestimates are not exact.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2012 14:30:53 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Litvinov", "G. L.", ""]]}, {"id": "1209.1942", "submitter": "A. Emre Cetin", "authors": "A. Emre Cetin", "title": "Sorting distinct integer keys using in-place associative sort", "comments": "20 pages. arXiv admin note: substantial text overlap with\n  arXiv:1209.0572", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In-place associative integer sorting technique was proposed for integer lists\nwhich requires only constant amount of additional memory replacing bucket sort,\ndistribution counting sort and address calculation sort family of algorithms.\nThe technique was explained by the analogy with the three main stages in the\nformation and retrieval of memory in cognitive neuroscience which are (i)\npracticing, (ii) storing and (iii) retrieval.\n  In this study, the technique is specialized with two variants one for\nread-only integer keys and the other for modifiable integers. Hence, a novel\nalgorithm is obtained that does not require additional memory other than a\nconstant amount and sorts faster than all no matter how large is the list\nprovided that m = O (n logn) where m is the range and n is the number of keys\n(or integers).\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 11:04:02 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2012 14:38:00 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Cetin", "A. Emre", ""]]}, {"id": "1209.1977", "submitter": "Sebastian B\\\"ocker", "authors": "Sebastian B\\\"ocker", "title": "Ten times eighteen", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following simple game: We are given a table with ten slots\nindexed one to ten. In each of the ten rounds of the game, three dice are\nrolled and the numbers are added. We then put this number into any free slot.\nFor each slot, we multiply the slot index with the number in this slot, and add\nup the products. The goal of the game is to maximize this score. In more\ndetail, we play the game many times, and try to maximize the sum of scores or,\nequivalently, the expected score. We present a strategy to optimally play this\ngame with respect to the expected score. We then modify our strategy so that we\nneed only polynomial time and space. Finally, we show that knowing all ten\nrolls in advance, results in a relatively small increase in score. Although the\ngame has a random component and requires a non-trivial strategy to be solved\noptimally, this strategy needs only polynomial time and space.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 13:13:56 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["B\u00f6cker", "Sebastian", ""]]}, {"id": "1209.2184", "submitter": "Benjamin Lipshitz", "authors": "Grey Ballard, James Demmel, Olga Holtz, Benjamin Lipshitz, Oded\n  Schwartz", "title": "Graph Expansion Analysis for Communication Costs of Fast Rectangular\n  Matrix Multiplication", "comments": null, "journal-ref": "Design and Analysis of Algorithms Volume 7659, 2012, pp 13-36", "doi": "10.1007/978-3-642-34862-4_2", "report-no": null, "categories": "cs.DS cs.CC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph expansion analysis of computational DAGs is useful for obtaining\ncommunication cost lower bounds where previous methods, such as geometric\nembedding, are not applicable. This has recently been demonstrated for\nStrassen's and Strassen-like fast square matrix multiplication algorithms. Here\nwe extend the expansion analysis approach to fast algorithms for rectangular\nmatrix multiplication, obtaining a new class of communication cost lower\nbounds. These apply, for example to the algorithms of Bini et al. (1979) and\nthe algorithms of Hopcroft and Kerr (1971). Some of our bounds are proved to be\noptimal.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 00:26:21 GMT"}], "update_date": "2012-11-30", "authors_parsed": [["Ballard", "Grey", ""], ["Demmel", "James", ""], ["Holtz", "Olga", ""], ["Lipshitz", "Benjamin", ""], ["Schwartz", "Oded", ""]]}, {"id": "1209.2185", "submitter": "Anastasios Zouzias", "authors": "Haim Avron, Christos Boutsidis, Sivan Toledo and Anastasios Zouzias", "title": "Efficient Dimensionality Reduction for Canonical Correlation Analysis", "comments": "22 pages. 4 figures. To appear in ICML 2013: The 30th International\n  Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast algorithm for approximate Canonical Correlation Analysis\n(CCA). Given a pair of tall-and-thin matrices, the proposed algorithm first\nemploys a randomized dimensionality reduction transform to reduce the size of\nthe input matrices, and then applies any CCA algorithm to the new pair of\nmatrices. The algorithm computes an approximate CCA to the original pair of\nmatrices with provable guarantees, while requiring asymptotically less\noperations than the state-of-the-art exact algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 00:32:56 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2012 20:08:41 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2013 16:07:11 GMT"}, {"version": "v4", "created": "Wed, 1 May 2013 22:29:05 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Avron", "Haim", ""], ["Boutsidis", "Christos", ""], ["Toledo", "Sivan", ""], ["Zouzias", "Anastasios", ""]]}, {"id": "1209.2503", "submitter": "Lajos Pongr\\'acz", "authors": "Lajos L. Pongr\\'acz", "title": "A greedy approximation algorithm for the longest path problem in\n  undirected graphs", "comments": "6 pages, 3 figures Withdrawn due to an error in search subroutine,\n  2nd figure and time complexity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In graph theory, the longest path problem is the problem of finding a simple\npath of maximum length in a given graph. For some small classes of graphs, the\nproblem can be solved in polynomial time [2, 4], but it remains NP-hard on\ngeneral graphs, since it includes the Hamiltonian path problem as a special\ncase [3]. Motivated by finding a simple, quick algorithm for finding long paths\nin large graphs, in this paper we show a greedy algorithm with a time\ncomplexity of O(n^2 (n+m)), where n is the number of the vertices and m is the\nnumber of edges.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 06:32:02 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2012 22:43:46 GMT"}, {"version": "v3", "created": "Wed, 10 Sep 2014 21:31:30 GMT"}], "update_date": "2014-09-15", "authors_parsed": [["Pongr\u00e1cz", "Lajos L.", ""]]}, {"id": "1209.2759", "submitter": "Adel Javanmard", "authors": "Adel Javanmard, Maya Haridasan and Li Zhang", "title": "Multi-track Map Matching", "comments": "11 pages, 8 figures, short version appears in 20th International\n  Conference on Advances in Geographic Information Systems (ACM SIGSPATIAL GIS\n  2012). Extended Abstract in Proceedings of the 10th international conference\n  on Mobile systems, applications, and services (MobiSys 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study algorithms for matching user tracks, consisting of time-ordered\nlocation points, to paths in the road network. Previous work has focused on the\nscenario where the location data is linearly ordered and consists of fairly\ndense and regular samples. In this work, we consider the \\emph{multi-track map\nmatching}, where the location data comes from different trips on the same\nroute, each with very sparse samples. This captures the realistic scenario\nwhere users repeatedly travel on regular routes and samples are sparsely\ncollected, either due to energy consumption constraints or because samples are\nonly collected when the user actively uses a service. In the multi-track\nproblem, the total set of combined locations is only partially ordered, rather\nthan globally ordered as required by previous map-matching algorithms. We\npropose two methods, the iterative projection scheme and the graph Laplacian\nscheme, to solve the multi-track problem by using a single-track map-matching\nsubroutine. We also propose a boosting technique which may be applied to either\napproach to improve the accuracy of the estimated paths. In addition, in order\nto deal with variable sampling rates in single-track map matching, we propose a\nmethod based on a particular regularized cost function that can be adapted for\ndifferent sampling rates and measurement errors. We evaluate the effectiveness\nof our techniques for reconstructing tracks under several different\nconfigurations of sampling error and sampling rate.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 01:44:12 GMT"}], "update_date": "2012-09-14", "authors_parsed": [["Javanmard", "Adel", ""], ["Haridasan", "Maya", ""], ["Zhang", "Li", ""]]}, {"id": "1209.2848", "submitter": "Peter Kling", "authors": "Peter Kling, Andreas Cord-Landwehr, Frederik Mallmann-Trenn", "title": "Slow Down & Sleep for Profit in Online Deadline Scheduling", "comments": "An extended abstract of this paper has been accepted for publication\n  in the proceedings of the 1st Mediterranean Conference on Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and study a new model for energy-aware and profit-oriented\nscheduling on a single processor. The processor features dynamic speed scaling\nas well as suspension to a sleep mode. Jobs arrive over time, are preemptable,\nand have different sizes, values, and deadlines. On the arrival of a new job,\nthe scheduler may either accept or reject the job. Accepted jobs need a certain\nenergy investment to be finished in time, while rejected jobs cause costs equal\nto their values. Here, power consumption at speed $s$ is given by\n$P(s)=s^{\\alpha}+\\beta$ and the energy investment is power integrated over\ntime. Additionally, the scheduler may decide to suspend the processor to a\nsleep mode in which no energy is consumed, though awaking entails fixed\ntransition costs $\\gamma$. The objective is to minimize the total value of\nrejected jobs plus the total energy.\n  Our model combines aspects from advanced energy conservation techniques\n(namely speed scaling and sleep states) and profit-oriented scheduling models.\nWe show that \\emph{rejection-oblivious} schedulers (whose rejection decisions\nare not based on former decisions) have -- in contrast to the model without\nsleep states -- an unbounded competitive ratio w.r.t\\text{.} the processor\nparameters $\\alpha$ and $\\beta$. It turns out that the worst-case performance\nof such schedulers depends linearly on the jobs' value densities (the ratio\nbetween a job's value and its work). We give an algorithm whose competitiveness\nnearly matches this lower bound. If the maximum value density is not too large,\nthe competitiveness becomes $\\alpha^{\\alpha}+2e\\alpha$. Also, we show that it\nsuffices to restrict the value density of low-value jobs only. Using a\ntechnique from \\cite{Chan:2010} we transfer our results to processors with a\nfixed maximum speed.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 10:36:40 GMT"}], "update_date": "2012-09-14", "authors_parsed": [["Kling", "Peter", ""], ["Cord-Landwehr", "Andreas", ""], ["Mallmann-Trenn", "Frederik", ""]]}, {"id": "1209.2946", "submitter": "Frederic Rodriguez", "authors": "Fr\\'ed\\'eric Rodriguez (SPCMIB)", "title": "Technical Report: CSVM Ecosystem", "comments": "31 pages including 2p of Annex", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CSVM format is derived from CSV format and allows the storage of tabular\nlike data with a limited but extensible amount of metadata. This approach could\nhelp computer scientists because all information needed to uses subsequently\nthe data is included in the CSVM file and is particularly well suited for\nhandling RAW data in a lot of scientific fields and to be used as a canonical\nformat. The use of CSVM has shown that it greatly facilitates: the data\nmanagement independently of using databases; the data exchange; the integration\nof RAW data in dataflows or calculation pipes; the search for best practices in\nRAW data management. The efficiency of this format is closely related to its\nplasticity: a generic frame is given for all kind of data and the CSVM parsers\ndon't make any interpretation of data types. This task is done by the\napplication layer, so it is possible to use same format and same parser codes\nfor a lot of purposes. In this document some implementation of CSVM format for\nten years and in different laboratories are presented. Some programming\nexamples are also shown: a Python toolkit for using the format, manipulating\nand querying is available. A first specification of this format (CSVM-1) is now\ndefined, as well as some derivatives such as CSVM dictionaries used for data\ninterchange. CSVM is an Open Format and could be used as a support for Open\nData and long term conservation of RAW or unpublished data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 15:44:41 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Rodriguez", "Fr\u00e9d\u00e9ric", "", "SPCMIB"]]}, {"id": "1209.3050", "submitter": "Samuel King Opoku", "authors": "Samuel King Opoku (Kumasi Polytechnic)", "title": "Parallel Sorting System for Objects", "comments": "8 pages, 13 figures", "journal-ref": "Cyber Journals: Multidisciplinary Journals in Science and\n  Technology, Journal of Selected Areas in Software Engineering (JSSE), Vol. 2,\n  No. 12, pages 1-8, 2011", "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional sorting algorithms make use of such data structures as array,\nfile and list which define access methods of the items to be sorted. Such\ntraditional methods as exchange sort, divide and conquer sort, selection sort\nand insertion sort require supervisory control program. The supervisory control\nprogram has access to the items and is responsible for arranging them in the\nproper order. This paper presents a different sorting algorithm that does not\nrequire supervisory control program. The objects sort themselves and they are\nable to terminate when sorting is completed. The algorithm also employs\nparallel processing mechanisms to increase its efficiency and effectiveness.\nThe paper makes a review of the traditional sorting methods, identifying their\npros and cons and proposes a different design based on conceptual combination\nof these algorithms. Algorithms designed were implemented and tested in Java\ndesktop application\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 22:01:07 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["Opoku", "Samuel King", "", "Kumasi Polytechnic"]]}, {"id": "1209.3314", "submitter": "George Teodoro", "authors": "George Teodoro, Tony Pan, Tahsin Kurc, Jun Kong, Lee Cooper, Joel\n  Saltz", "title": "Efficient Irregular Wavefront Propagation Algorithms on Hybrid CPU-GPU\n  Machines", "comments": "37 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of efficient execution of a computation\npattern, referred to here as the irregular wavefront propagation pattern\n(IWPP), on hybrid systems with multiple CPUs and GPUs. The IWPP is common in\nseveral image processing operations. In the IWPP, data elements in the\nwavefront propagate waves to their neighboring elements on a grid if a\npropagation condition is satisfied. Elements receiving the propagated waves\nbecome part of the wavefront. This pattern results in irregular data accesses\nand computations. We develop and evaluate strategies for efficient computation\nand propagation of wavefronts using a multi-level queue structure. This queue\nstructure improves the utilization of fast memories in a GPU and reduces\nsynchronization overheads. We also develop a tile-based parallelization\nstrategy to support execution on multiple CPUs and GPUs. We evaluate our\napproaches on a state-of-the-art GPU accelerated machine (equipped with 3 GPUs\nand 2 multicore CPUs) using the IWPP implementations of two widely used image\nprocessing operations: morphological reconstruction and euclidean distance\ntransform. Our results show significant performance improvements on GPUs. The\nuse of multiple CPUs and GPUs cooperatively attains speedups of 50x and 85x\nwith respect to single core CPU executions for morphological reconstruction and\neuclidean distance transform, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2012 20:17:23 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Teodoro", "George", ""], ["Pan", "Tony", ""], ["Kurc", "Tahsin", ""], ["Kong", "Jun", ""], ["Cooper", "Lee", ""], ["Saltz", "Joel", ""]]}, {"id": "1209.3352", "submitter": "Shipra Agrawal", "authors": "Shipra Agrawal, Navin Goyal", "title": "Thompson Sampling for Contextual Bandits with Linear Payoffs", "comments": "Improvements from previous version: (1) dependence on d improved from\n  d^2 to d^{3/2} (2) Simpler and more modular proof techniques (3) bounds in\n  terms of log(N) added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson Sampling is one of the oldest heuristics for multi-armed bandit\nproblems. It is a randomized algorithm based on Bayesian ideas, and has\nrecently generated significant interest after several studies demonstrated it\nto have better empirical performance compared to the state-of-the-art methods.\nHowever, many questions regarding its theoretical performance remained open. In\nthis paper, we design and analyze a generalization of Thompson Sampling\nalgorithm for the stochastic contextual multi-armed bandit problem with linear\npayoff functions, when the contexts are provided by an adaptive adversary. This\nis among the most important and widely studied versions of the contextual\nbandits problem. We provide the first theoretical guarantees for the contextual\nversion of Thompson Sampling. We prove a high probability regret bound of\n$\\tilde{O}(d^{3/2}\\sqrt{T})$ (or $\\tilde{O}(d\\sqrt{T \\log(N)})$), which is the\nbest regret bound achieved by any computationally efficient algorithm available\nfor this problem in the current literature, and is within a factor of\n$\\sqrt{d}$ (or $\\sqrt{\\log(N)}$) of the information-theoretic lower bound for\nthis problem.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2012 03:27:11 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2013 18:35:56 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2014 07:00:54 GMT"}, {"version": "v4", "created": "Mon, 3 Feb 2014 07:09:03 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Agrawal", "Shipra", ""], ["Goyal", "Navin", ""]]}, {"id": "1209.3353", "submitter": "Shipra Agrawal", "authors": "Shipra Agrawal, Navin Goyal", "title": "Further Optimal Regret Bounds for Thompson Sampling", "comments": "arXiv admin note: substantial text overlap with arXiv:1111.1797", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson Sampling is one of the oldest heuristics for multi-armed bandit\nproblems. It is a randomized algorithm based on Bayesian ideas, and has\nrecently generated significant interest after several studies demonstrated it\nto have better empirical performance compared to the state of the art methods.\nIn this paper, we provide a novel regret analysis for Thompson Sampling that\nsimultaneously proves both the optimal problem-dependent bound of\n$(1+\\epsilon)\\sum_i \\frac{\\ln T}{\\Delta_i}+O(\\frac{N}{\\epsilon^2})$ and the\nfirst near-optimal problem-independent bound of $O(\\sqrt{NT\\ln T})$ on the\nexpected regret of this algorithm. Our near-optimal problem-independent bound\nsolves a COLT 2012 open problem of Chapelle and Li. The optimal\nproblem-dependent regret bound for this problem was first proven recently by\nKaufmann et al. [ALT 2012]. Our novel martingale-based analysis techniques are\nconceptually simple, easily extend to distributions other than the Beta\ndistribution, and also extend to the more general contextual bandits setting\n[Manuscript, Agrawal and Goyal, 2012].\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2012 03:41:18 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Agrawal", "Shipra", ""], ["Goyal", "Navin", ""]]}, {"id": "1209.3387", "submitter": "Sumit Kumar", "authors": "Garimella Rama Murthy (International Institute of Information\n  Technology)", "title": "Graphs: Associated Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research paper, weighted / unweighted, directed / undirected graphs\nare associated with interesting Discrete Time Markov Chains (DTMCs) as well as\nContinuous Time Markov Chains (CTMCs). The equilibrium / transient behaviour of\nsuch Markov chains is studied. Also entropy dynamics (Shannon entropy) of\ncertain structured Markov chains is investigated. Finally certain structured\ngraphs and the associated Markov chains are studied.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2012 10:23:26 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Murthy", "Garimella Rama", "", "International Institute of Information\n  Technology"]]}, {"id": "1209.3436", "submitter": "David Harvey", "authors": "Edgar Costa, Robert Gerbicz, David Harvey", "title": "A search for Wilson primes", "comments": "24 pages, simplified notation and space analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Wilson prime is a prime p such that (p-1)! = -1 mod p^2. We report on a\nsearch for Wilson primes up to 2 * 10^13, and describe several new algorithms\nthat were used in the search. In particular we give the first known algorithm\nthat computes (p-1)! mod p^2 in average polynomial time per prime.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2012 21:13:36 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2012 19:29:21 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2012 00:29:09 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Costa", "Edgar", ""], ["Gerbicz", "Robert", ""], ["Harvey", "David", ""]]}, {"id": "1209.3523", "submitter": "Andr\\'as Seb\\H{o}", "authors": "Andr\\'as Seb\\\"o", "title": "Eight-Fifth Approximation for TSP Paths", "comments": "15 pages, corrected typos in citations, minor changes", "journal-ref": "International Conference on Integer Programming and Combinatorial\n  Optimization IPCO 2013, Lecture Notes in Computer Science, vol 7801.\n  Springer, Berlin, Heidelberg, pp 362-374", "doi": "10.1007/978-3-642-36694-9_31", "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the approximation ratio 8/5 for the metric $\\{s,t\\}$-path-TSP\nproblem, and more generally for shortest connected $T$-joins.\n  The algorithm that achieves this ratio is the simple \"Best of Many\" version\nof Christofides' algorithm (1976), suggested by An, Kleinberg and Shmoys\n(2012), which consists in determining the best Christofides $\\{s,t\\}$-tour out\nof those constructed from a family $\\Fscr_{>0}$ of trees having a convex\ncombination dominated by an optimal solution $x^*$ of the fractional\nrelaxation. They give the approximation guarantee $\\frac{\\sqrt{5}+1}{2}$ for\nsuch an $\\{s,t\\}$-tour, which is the first improvement after the 5/3 guarantee\nof Hoogeveen's Christofides type algorithm (1991). Cheriyan, Friggstad and Gao\n(2012) extended this result to a 13/8-approximation of shortest connected\n$T$-joins, for $|T|\\ge 4$.\n  The ratio 8/5 is proved by simplifying and improving the approach of An,\nKleinberg and Shmoys that consists in completing $x^*/2$ in order to dominate\nthe cost of \"parity correction\" for spanning trees. We partition the edge-set\nof each spanning tree in $\\Fscr_{>0}$ into an $\\{s,t\\}$-path (or more\ngenerally, into a $T$-join) and its complement, which induces a decomposition\nof $x^*$. This decomposition can be refined and then efficiently used to\ncomplete $x^*/2$ without using linear programming or particular properties of\n$T$, but by adding to each cut deficient for $x^*/2$ an individually tailored\nexplicitly given vector, inherent in $x^*$.\n  A simple example shows that the Best of Many Christofides algorithm may not\nfind a shorter $\\{s,t\\}$-tour than 3/2 times the incidentally common optima of\nthe problem and of its fractional relaxation.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2012 21:47:51 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2012 19:55:33 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2012 21:31:28 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Seb\u00f6", "Andr\u00e1s", ""]]}, {"id": "1209.3668", "submitter": "A. Emre Cetin", "authors": "A. Emre Cetin", "title": "Improved in-place associative integer sorting", "comments": "16 pages. arXiv admin note: substantial text overlap with\n  arXiv:1209.0572, arXiv:1209.1942", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A novel integer sorting technique was proposed replacing bucket sort,\ndistribution counting sort and address calculation sort family of algorithms\nwhich requires only constant amount of additional memory. The technique was\ninspired from one of the ordinal theories of \"serial order in behavior\" and\nexplained by the analogy with the three main stages in the formation and\nretrieval of memory in cognitive neuroscience namely (i) practicing, (ii)\nstoring and (iii) retrieval.\n  In this study, the technique is improved both theoretically and practically\nand an algorithm is obtained which is faster than the former making it more\ncompetitive. With the improved version, n integers S[0...n-1] each in the range\n[0, n-1] are sorted exactly in O(n) time while the complexity of the former\ntechnique was the recursion T(n) = T(n/2) + O(n) yielding T(n) = O(n).\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2012 14:44:42 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Cetin", "A. Emre", ""]]}, {"id": "1209.3694", "submitter": "Yifei Ma", "authors": "Yifei Ma, Roman Garnett, Jeff Schneider", "title": "Submodularity in Batch Active Learning and Survey Problems on Gaussian\n  Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world datasets can be represented in the form of a graph whose edge\nweights designate similarities between instances. A discrete Gaussian random\nfield (GRF) model is a finite-dimensional Gaussian process (GP) whose prior\ncovariance is the inverse of a graph Laplacian. Minimizing the trace of the\npredictive covariance Sigma (V-optimality) on GRFs has proven successful in\nbatch active learning classification problems with budget constraints. However,\nits worst-case bound has been missing. We show that the V-optimality on GRFs as\na function of the batch query set is submodular and hence its greedy selection\nalgorithm guarantees an (1-1/e) approximation ratio. Moreover, GRF models have\nthe absence-of-suppressor (AofS) condition. For active survey problems, we\npropose a similar survey criterion which minimizes 1'(Sigma)1. In practice,\nV-optimality criterion performs better than GPs with mutual information gain\ncriteria and allows nonuniform costs for different nodes.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2012 15:43:11 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Ma", "Yifei", ""], ["Garnett", "Roman", ""], ["Schneider", "Jeff", ""]]}, {"id": "1209.3849", "submitter": "Arie Matsliah", "authors": "Harry Buhrman, David Garcia-Soriano, Arie Matsliah, and Ronald de Wolf", "title": "The non-adaptive query complexity of testing k-parities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove tight bounds of Theta(k log k) queries for non-adaptively testing\nwhether a function f:{0,1}^n -> {0,1} is a k-parity or far from any k-parity.\nThe lower bound combines a recent method of Blais, Brody and Matulef [BBM11] to\nget lower bounds for testing from communication complexity with an Omega(k \\log\nk) lower bound for the one-way communication complexity of k-disjointness.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 05:45:57 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2013 16:56:17 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Buhrman", "Harry", ""], ["Garcia-Soriano", "David", ""], ["Matsliah", "Arie", ""], ["de Wolf", "Ronald", ""]]}, {"id": "1209.3868", "submitter": "Peter Kling", "authors": "Peter Kling and Peter Pietrzyk", "title": "Profitable Scheduling on Multiple Speed-Scalable Processors", "comments": "Extended abstract submitted to STACS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new online algorithm for profit-oriented scheduling on multiple\nspeed-scalable processors. Moreover, we provide a tight analysis of the\nalgorithm's competitiveness. Our results generalize and improve upon work by\n\\textcite{Chan:2010}, which considers a single speed-scalable processor. Using\nsignificantly different techniques, we can not only extend their model to\nmultiprocessors but also prove an enhanced and tight competitive ratio for our\nalgorithm.\n  In our scheduling problem, jobs arrive over time and are preemptable. They\nhave different workloads, values, and deadlines. The scheduler may decide not\nto finish a job but instead to suffer a loss equaling the job's value. However,\nto process a job's workload until its deadline the scheduler must invest a\ncertain amount of energy. The cost of a schedule is the sum of lost values and\ninvested energy. In order to finish a job the scheduler has to determine which\nprocessors to use and set their speeds accordingly. A processor's energy\nconsumption is power $\\Power{s}$ integrated over time, where\n$\\Power{s}=s^{\\alpha}$ is the power consumption when running at speed $s$.\nSince we consider the online variant of the problem, the scheduler has no\nknowledge about future jobs. This problem was introduced by\n\\textcite{Chan:2010} for the case of a single processor. They presented an\nonline algorithm which is $\\alpha^{\\alpha}+2e\\alpha$-competitive. We provide an\nonline algorithm for the case of multiple processors with an improved\ncompetitive ratio of $\\alpha^{\\alpha}$.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 08:17:21 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Kling", "Peter", ""], ["Pietrzyk", "Peter", ""]]}, {"id": "1209.3925", "submitter": "Laurent Najman", "authors": "Pierre Soille (IPSC), Laurent Najman (LIGM)", "title": "On morphological hierarchical representations for image processing and\n  spatial data clustering", "comments": null, "journal-ref": "Workshop on APPLICATIONS OF DISCRETE GEOMETRY AND MATHEMATICAL\n  MORPHOLOGY, Istanbul : Turkey (2010)", "doi": "10.1007/978-3-642-32313-3_4", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical data representations in the context of classi cation and data\nclustering were put forward during the fties. Recently, hierarchical image\nrepresentations have gained renewed interest for segmentation purposes. In this\npaper, we briefly survey fundamental results on hierarchical clustering and\nthen detail recent paradigms developed for the hierarchical representation of\nimages in the framework of mathematical morphology: constrained connectivity\nand ultrametric watersheds. Constrained connectivity can be viewed as a way to\nconstrain an initial hierarchy in such a way that a set of desired constraints\nare satis ed. The framework of ultrametric watersheds provides a generic scheme\nfor computing any hierarchical connected clustering, in particular when such a\nhierarchy is constrained. The suitability of this framework for solving\npractical problems is illustrated with applications in remote sensing.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 12:33:33 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Soille", "Pierre", "", "IPSC"], ["Najman", "Laurent", "", "LIGM"]]}, {"id": "1209.3995", "submitter": "Joerg Fliege", "authors": "Joerg Fliege", "title": "A Randomized Parallel Algorithm with Run Time $O(n^2)$ for Solving an $n\n  \\times n$ System of Linear Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, following suggestions by Tao, we extend the randomized\nalgorithm for linear equations over prime fields by Raghavendra to a randomized\nalgorithm for linear equations over the reals. We also show that the algorithm\ncan be parallelized to solve a system of linear equations $A x = b$ with a\nregular $n \\times n$ matrix $A$ in time $O(n^2)$, with probability one. Note\nthat we do not assume that $A$ is symmetric.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 15:42:46 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Fliege", "Joerg", ""]]}, {"id": "1209.4206", "submitter": "Barun  Biswas", "authors": "Barun Biswas, Krishnendu Basuli, Saptarshi Naskar, Saomya Chakraborti\n  and Samar Sen Sarma", "title": "A combinatorial algorithm to generate all spanning trees of a weighted\n  graph in order of increasing cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The most popular algorithms for generation of minimal spanning tree are\nKruskal and Prim algorithm. Many algorithms have been proposed for generation\nof all spanning tree. This paper deals with generation of all possible spanning\ntrees in increasing cost of a weighted graph. This approach uses one matrix\ncalled Difference Weighted Circuit Matrix; it is little bit modification of\nFCM.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2012 10:40:52 GMT"}], "update_date": "2012-09-20", "authors_parsed": [["Biswas", "Barun", ""], ["Basuli", "Krishnendu", ""], ["Naskar", "Saptarshi", ""], ["Chakraborti", "Saomya", ""], ["Sarma", "Samar Sen", ""]]}, {"id": "1209.4214", "submitter": "Armin Wei{\\ss}", "authors": "Volker Diekert, Armin Weiss", "title": "QuickHeapsort: Modifications and improved analysis", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-38536-0_3", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new analysis for QuickHeapsort splitting it into the analysis of\nthe partition-phases and the analysis of the heap-phases. This enables us to\nconsider samples of non-constant size for the pivot selection and leads to\nbetter theoretical bounds for the algorithm. Furthermore we introduce some\nmodifications of QuickHeapsort, both in-place and using n extra bits. We show\nthat on every input the expected number of comparisons is n lg n - 0.03n + o(n)\n(in-place) respectively n lg n -0.997 n+ o (n). Both estimates improve the\npreviously known best results. (It is conjectured in Wegener93 that the\nin-place algorithm Bottom-Up-Heapsort uses at most n lg n + 0.4 n on average\nand for Weak-Heapsort which uses n extra-bits the average number of comparisons\nis at most n lg n -0.42n in EdelkampS02.) Moreover, our non-in-place variant\ncan even compete with index based Heapsort variants (e.g. Rank-Heapsort in\nWangW07) and Relaxed-Weak-Heapsort (n lg n -0.9 n+ o (n) comparisons in the\nworst case) for which no O(n)-bound on the number of extra bits is known.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2012 11:54:17 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2013 10:43:03 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Diekert", "Volker", ""], ["Weiss", "Armin", ""]]}, {"id": "1209.4227", "submitter": "Sergey Bereg", "authors": "Sergey Bereg, Alexander E. Holroyd, Lev Nachmanson, Sergey Pupyrev", "title": "Edge Routing with Ordered Bundles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge bundling reduces the visual clutter in a drawing of a graph by uniting\nthe edges into bundles. We propose a method of edge bundling drawing each edge\nof a bundle separately as in metro-maps and call our method ordered bundles. To\nproduce aesthetically looking edge routes it minimizes a cost function on the\nedges. The cost function depends on the ink, required to draw the edges, the\nedge lengths, widths and separations. The cost also penalizes for too many\nedges passing through narrow channels by using the constrained Delaunay\ntriangulation. The method avoids unnecessary edge-node and edge-edge crossings.\nTo draw edges with the minimal number of crossings and separately within the\nsame bundle we develop an efficient algorithm solving a variant of the\nmetro-line crossing minimization problem. In general, the method creates clear\nand smooth edge routes giving an overview of the global graph structure, while\nstill drawing each edge separately and thus enabling local analysis.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2012 12:50:15 GMT"}], "update_date": "2012-09-20", "authors_parsed": [["Bereg", "Sergey", ""], ["Holroyd", "Alexander E.", ""], ["Nachmanson", "Lev", ""], ["Pupyrev", "Sergey", ""]]}, {"id": "1209.4403", "submitter": "Wolfgang Mulzer", "authors": "Kevin Buchin, Maike Buchin, Wouter Meulemans, Wolfgang Mulzer", "title": "Four Soviets Walk the Dog-Improved Bounds for Computing the Fr\\'echet\n  Distance", "comments": "34 pages, 15 figures. A preliminary version appeared in SODA 2014", "journal-ref": "Discrete and Computational Geometry (DCG), 58(1), July 2017, pp.\n  180-216", "doi": "10.1007/s00454-017-9878-7", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two polygonal curves in the plane, there are many ways to define a\nnotion of similarity between them. One popular measure is the Fr\\'echet\ndistance. Since it was proposed by Alt and Godau in 1992, many variants and\nextensions have been studied. Nonetheless, even more than 20 years later, the\noriginal $O(n^2 \\log n)$ algorithm by Alt and Godau for computing the Fr\\'echet\ndistance remains the state of the art (here, $n$ denotes the number of edges on\neach curve). This has led Helmut Alt to conjecture that the associated decision\nproblem is 3SUM-hard.\n  In recent work, Agarwal et al. show how to break the quadratic barrier for\nthe discrete version of the Fr\\'echet distance, where one considers sequences\nof points instead of polygonal curves. Building on their work, we give a\nrandomized algorithm to compute the Fr\\'echet distance between two polygonal\ncurves in time $O(n^2 \\sqrt{\\log n}(\\log\\log n)^{3/2})$ on a pointer machine\nand in time $O(n^2(\\log\\log n)^2)$ on a word RAM. Furthermore, we show that\nthere exists an algebraic decision tree for the decision problem of depth\n$O(n^{2-\\varepsilon})$, for some $\\varepsilon > 0$. We believe that this\nreveals an intriguing new aspect of this well-studied problem. Finally, we show\nhow to obtain the first subquadratic algorithm for computing the weak Fr\\'echet\ndistance on a word RAM.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 01:06:39 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2012 23:08:32 GMT"}, {"version": "v3", "created": "Mon, 13 Feb 2017 15:13:08 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Buchin", "Kevin", ""], ["Buchin", "Maike", ""], ["Meulemans", "Wouter", ""], ["Mulzer", "Wolfgang", ""]]}, {"id": "1209.4463", "submitter": "Oren Salzman", "authors": "Doron Shaharabani, Oren Salzman, Pankaj K. Agarwal and Dan Halperin", "title": "Sparsification of Motion-Planning Roadmaps by Edge Contraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Roadmap Sparsification by Edge Contraction (RSEC), a simple and\neffective algorithm for reducing the size of a motion-planning roadmap. The\nalgorithm exhibits minimal effect on the quality of paths that can be extracted\nfrom the new roadmap. The primitive operation used by RSEC is edge contraction\n- the contraction of a roadmap edge to a single vertex and the connection of\nthe new vertex to the neighboring vertices of the contracted edge. For certain\nscenarios, we compress more than 98% of the edges and vertices at the cost of\ndegradation of average shortest path length by at most 2%.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 09:07:47 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["Shaharabani", "Doron", ""], ["Salzman", "Oren", ""], ["Agarwal", "Pankaj K.", ""], ["Halperin", "Dan", ""]]}, {"id": "1209.4493", "submitter": "Oliver Melchert", "authors": "O. Melchert", "title": "Minimum weight spanning trees of weighted scale free networks", "comments": "10 pages, 5 figures, Lecture notes related to the summer school\n  \"Modern Computational Science 2012 - Optimization\", the example programs can\n  be downloaded at http://www.mcs.uni-oldenburg.de/58424.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this lecture we will consider the minimum weight spanning tree (MST)\nproblem, i.e., one of the simplest and most vital combinatorial optimization\nproblems. We will discuss a particular greedy algorithm that allows to compute\na MST for undirected weighted graphs, namely Kruskal's algorithm, and we will\nstudy the structure of MSTs obtained for weighted scale free random graphs.\nThis is meant to clarify whether the structure of MSTs is sensitive to\ncorrelations between edge weights and topology of the underlying scale free\ngraphs.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 11:09:13 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["Melchert", "O.", ""]]}, {"id": "1209.4508", "submitter": "Konstantin Kutzkov", "authors": "Konstantin Kutzkov", "title": "Deterministic algorithms for skewed matrix products", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Pagh presented a randomized approximation algorithm for the\nmultiplication of real-valued matrices building upon work for detecting the\nmost frequent items in data streams. We continue this line of research and\npresent new {\\em deterministic} matrix multiplication algorithms.\n  Motivated by applications in data mining, we first consider the case of\nreal-valued, nonnegative $n$-by-$n$ input matrices $A$ and $B$, and show how to\nobtain a deterministic approximation of the weights of individual entries, as\nwell as the entrywise $p$-norm, of the product $AB$. The algorithm is simple,\nspace efficient and runs in one pass over the input matrices. For a user\ndefined $b \\in (0, n^2)$ the algorithm runs in time $O(nb +\nn\\cdot\\text{Sort}(n))$ and space $O(n + b)$ and returns an approximation of the\nentries of $AB$ within an additive factor of $\\|AB\\|_{E1}/b$, where $\\|C\\|_{E1}\n= \\sum_{i, j} |C_{ij}|$ is the entrywise 1-norm of a matrix $C$ and\n$\\text{Sort}(n)$ is the time required to sort $n$ real numbers in linear space.\nBuilding upon a result by Berinde et al. we show that for skewed matrix\nproducts (a common situation in many real-life applications) the algorithm is\nmore efficient and achieves better approximation guarantees than previously\nknown randomized algorithms.\n  When the input matrices are not restricted to nonnegative entries, we present\na new deterministic group testing algorithm detecting nonzero entries in the\nmatrix product with large absolute value. The algorithm is clearly outperformed\nby randomized matrix multiplication algorithms, but as a byproduct we obtain\nthe first $O(n^{2 + \\varepsilon})$-time deterministic algorithm for matrix\nproducts with $O(\\sqrt{n})$ nonzero entries.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 12:27:15 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["Kutzkov", "Konstantin", ""]]}, {"id": "1209.4554", "submitter": "Erez Buchnik", "authors": "Erez M. Buchnik", "title": "Bouma2 - A Quasi-Stateless, Tunable Multiple String-Match Algorithm", "comments": "33 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bouma2 algorithm attempts to challenge the prevalent \"stateful\" exact\nstring-match paradigms by suggesting a \"quasi-stateless\" approach. We claim\nthat using state-machines to solve the multiple exact string-match problem\nintroduces a hidden artificial constraint, namely the Consume-Order Dependency,\nwhich results in unnecessary overhead. Bouma2 is not restricted in this sense;\nwe postulate that this allows memory-efficiency and improved performance versus\nits state-machine equivalents. The heart of the Bouma2 preprocessing problem is\nformulated as a weighted Integer Linear Programming problem, that can be tuned\nfor memory footprint and performance optimization. Specifically, this allows\nBouma2 to be input-sensitive, as tuning can be based on input characteristics.\nEvaluating Bouma2 against the Aho-Corasick variant of the popular Snort\nIntrusion Prevention System, we demonstrate double the throughput while using\nabout 10% of the memory.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 14:59:48 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["Buchnik", "Erez M.", ""]]}, {"id": "1209.4560", "submitter": "Patrick Prosser", "authors": "Ciaran McCreesh and Patrick Prosser", "title": "Distributing an Exact Algorithm for Maximum Clique: maximising the\n  costup", "comments": "13 pages, 2 Algorithms, 2 figures, 2 tables", "journal-ref": "Algorithms 2012, 5(4), 545-587", "doi": "10.3390/a5040545", "report-no": "TR-2012-334", "categories": "cs.DS cs.DC cs.DM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take an existing implementation of an algorithm for the maximum clique\nproblem and modify it so that we can distribute it over an ad-hoc cluster of\nmachines. Our goal was to achieve a significant speedup in performance with\nminimal development effort, i.e. a maximum costup. We present a simple\nmodification to a state-of-the-art exact algorithm for maximum clique that\nallows us to distribute it across many machines. An empirical study over large\nhard benchmarks shows that speedups of an order of magnitude are routine for 25\nor more machines.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 15:18:54 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["McCreesh", "Ciaran", ""], ["Prosser", "Patrick", ""]]}, {"id": "1209.4605", "submitter": "Marcin Kik", "authors": "Marcin Kik, Maciej G\\c{e}bala, Miros{\\l}aw Kuty{\\l}owski", "title": "One-side Energy costs of the RBO receiver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $n = 2^k$ be the length of the broadcast cycle of the RBO broadcast\nscheduling protocol (see [arXiv:1108.5095] and [arXiv:1201.3318]). Let $lb$ and\n$ub$ be the variables of the RBO receiver as defined in [ arXiv:1201.3318 ]. We\nshow that the number of changes of $lb$ (the \"left-side energy\") is not greater\nthan $k + 1$. We also show that the number of changes of $rb$ (the \"right-side\nenergy\") is not greater than $k + 2$. Thus the \"extra energy\" (defined in\n[arXiv:1201.3318]) is bounded by $2 k + 3$. This updates the previous bound\nfrom [arXiv:1201.3318], which was $4 k + 2$.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 18:46:46 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["Kik", "Marcin", ""], ["G\u0229bala", "Maciej", ""], ["Kuty\u0142owski", "Miros\u0142aw", ""]]}, {"id": "1209.4623", "submitter": "Timothy Yusun", "authors": "Tamon Stephen and Timothy Yusun", "title": "Counting inequivalent monotone Boolean functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monotone Boolean functions (MBFs) are Boolean functions $f: {0,1}^n\n\\rightarrow {0,1}$ satisfying the monotonicity condition $x \\leq y \\Rightarrow\nf(x) \\leq f(y)$ for any $x,y \\in {0,1}^n$. The number of MBFs in n variables is\nknown as the $n$th Dedekind number. It is a longstanding computational\nchallenge to determine these numbers exactly - these values are only known for\n$n$ at most 8. Two monotone Boolean functions are inequivalent if one can be\nobtained from the other by renaming the variables. The number of inequivalent\nMBFs in $n$ variables was known only for up to $n = 6$. In this paper we\npropose a strategy to count inequivalent MBF's by breaking the calculation into\nparts based on the profiles of these functions. As a result we are able to\ncompute the number of inequivalent MBFs in 7 variables. The number obtained is\n490013148.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 19:18:53 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["Stephen", "Tamon", ""], ["Yusun", "Timothy", ""]]}, {"id": "1209.4714", "submitter": "A. Emre Cetin", "authors": "A. Emre Cetin", "title": "Sorting distinct integers using improved in-place associative sort", "comments": "16 pages. arXiv admin note: substantial text overlap with\n  arXiv:1209.3668, arXiv:1209.1942, arXiv:1209.0572", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In-place associative integer sorting technique was proposed for integer lists\nwhich requires only constant amount of additional memory replacing bucket sort,\ndistribution counting sort and address calculation sort family of algorithms.\nAfterwards, the technique was further improved and an in-place sorting\nalgorithm is proposed where n integers S[0...n-1] each in the range [0, n-1]\nare sorted exactly in O(n) time while the complexity of the former technique\nwas the recursion T(n) = T(n/2) + O(n) yielding T(n) = O(n).\n  The technique was specialized with two variants one for read-only distinct\ninteger keys and the other for modifiable distinct integers, as well. Assuming\nw is the fixed word length, the variant for modifiable distinct integers was\ncapable of sorting n distinct integers S[0...n-1] each in the range [0, m-1] in\nexactly O(n) time if m < (w-logn)n. Otherwise, it sort in O(n + m/(w-logn))\ntime for the worst, O(m/(w-logn)) time for the average (uniformly distributed\nkeys) and O(n) time for the best case using only O(1) extra space.\n  In this study, the variant for modifiable distinct integers is improved and\nan algorithm is obtained that sorts n distinct integers S[0...n-1] each in the\nrange [0, m-1] in exactly O(n) time if m < (w-1)n. Otherwise, it sort in O(n +\nm/(w-1)) time for the worst, O(m/(w-1)) time for the average (uniformly\ndistributed keys) and O(n) time for the best case using only O(1) extra space.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2012 06:07:14 GMT"}], "update_date": "2012-09-24", "authors_parsed": [["Cetin", "A. Emre", ""]]}, {"id": "1209.4751", "submitter": "Priyanka Chatterjee", "authors": "Priyanka Chatterjee, Nikhil Agarwal", "title": "Energy Aware, Scalable, K-Hop Based Cluster Formation In MANET", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of Mobile Ad-hoc Network remains attractive due to the desire to\nachieve better performance and scalability. MANETs are distributed systems\nconsisting of mobile hosts that are connected by multi-hop wireless links. Such\nsystems are self organized and facilitate communication in the network without\nany centralized administration. MANETs exhibit battery power constraint and\nsuffer scalability issues therefore cluster formation is expensive. This is due\nto the large number of messages passed during the process of cluster formation.\nClustering has evolved as an imperative research domain that enhances system\nperformance such as throughput and delay in Mobile Ad hoc Networks (MANETs) in\nthe presence of both mobility and a large number of mobile terminals.In this\nthesis, we present a clustering scheme that minimizes message overhead and\ncongestion for cluster formation and maintenance. The algorithm is devised to\nbe independent of the MANET Routing algorithm. Depending upon the context, the\nclustering algorithm may be implemented in the routing or in higher layers. The\ndynamic formation of clusters helps reduce data packet overhead, node\ncomplexity and power consumption, The simulation has been performed in ns-2.\nThe simulation shows that the number of clusters formed is in proportion with\nthe number of nodes in MANET.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2012 08:49:57 GMT"}], "update_date": "2012-09-24", "authors_parsed": [["Chatterjee", "Priyanka", ""], ["Agarwal", "Nikhil", ""]]}, {"id": "1209.4761", "submitter": "Airat Urakov", "authors": "Airat Urakov, Timofey Timeryaev", "title": "Algorithms of Fast Search of Center, Radius and Diameter on Weighted\n  Graphs", "comments": "16 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two problems in the search of metric characteristics on weighted undirected\ngraphs with non-negative edge weights are being considered. The first problem:\na weighted undirected graph with non-negative edge weight is given. The radius,\ndiameter and at least one center and one pair of peripheral vertices of the\ngraph are to be found. In the second problem we have additionally calculated\nthe distances matrix. For the problems being considered, we proposed fast\nsearch algorithms which use only small fraction of graph's vertices for the\nsearch of the metric characteristics. The proposed algorithms have been\ncompared to other popular methods of solving problems considered on various\ninputs.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2012 09:39:38 GMT"}], "update_date": "2012-09-24", "authors_parsed": [["Urakov", "Airat", ""], ["Timeryaev", "Timofey", ""]]}, {"id": "1209.4771", "submitter": "Piotr Beling", "authors": "Stanis{\\l}aw Goldstein, Piotr Beling", "title": "Counting common substrings effectively", "comments": "6 pages, in Polish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents effective (dynamic) algorithm for solving a problem of\ncounting the number of substrings of given string which are also substrings of\nsecond string. Presented algorithm can be used for example for quick\ncalculation of strings similarity measure using generalized $n$-gram method\n(Niewiadomski measure), which are shown. Correctness and complexity analyses\nare included.\n  -----\n  W artykule przedstawiono efektywny (dynamiczny) algorytm wyznaczaj\\k{a}cy\nmiar\\k{e} podobie\\'nstwa wyraz\\'ow za pomoc\\k{a} uog\\'olnionej metody\n$n$-gram\\'ow (miary Niewiadomskiego). Uzasadniono tak\\.ze poprawno\\'s\\'c\ndzia{\\l}ania algorytmu i oszacowano jego z{\\l}o\\.zono\\'s\\'c obliczeniow\\k{a}.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2012 10:10:05 GMT"}], "update_date": "2012-09-24", "authors_parsed": [["Goldstein", "Stanis\u0142aw", ""], ["Beling", "Piotr", ""]]}, {"id": "1209.4971", "submitter": "Frederic Magniez", "authors": "Nathana\\\"el Fran\\c{c}ois and Frederic Magniez", "title": "Streaming Complexity of Checking Priority Queues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is in the line of designing efficient checkers for testing the\nreliability of some massive data structures. Given a sequential access to the\ninsert/extract operations on such a structure, one would like to decide, a\nposteriori only, if it corresponds to the evolution of a reliable structure. In\na context of massive data, one would like to minimize both the amount of\nreliable memory of the checker and the number of passes on the sequence of\noperations. Chu, Kannan and McGregor initiated the study of checking priority\nqueues in this setting. They showed that use of timestamps allows to check a\npriority queue with a single pass and memory space O(N^(1/2)), up to a\npolylogarithmic factor. Later, Chakrabarti, Cormode, Kondapally and McGregor\nremoved the use of timestamps, and proved that more passes do not help. We show\nthat, even in the presence of timestamps, more passes do not help, solving a\npreviously open problem. On the other hand, we show that a second pass, but in\nreverse direction, shrinks the memory space to O((log N)^2), extending a\nphenomenon the first time observed by Magniez, Mathieu and Nayak for checking\nwell-parenthesized expressions.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2012 08:37:17 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Fran\u00e7ois", "Nathana\u00ebl", ""], ["Magniez", "Frederic", ""]]}, {"id": "1209.5045", "submitter": "Pan Peng", "authors": "Angsheng Li and Pan Peng", "title": "Detecting and Characterizing Small Dense Bipartite-like Subgraphs by the\n  Bipartiteness Ratio Measure", "comments": "17 pages; ISAAC 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding and characterizing subgraphs with small\n\\textit{bipartiteness ratio}. We give a bicriteria approximation algorithm\n\\verb|SwpDB| such that if there exists a subset $S$ of volume at most $k$ and\nbipartiteness ratio $\\theta$, then for any $0<\\epsilon<1/2$, it finds a set\n$S'$ of volume at most $2k^{1+\\epsilon}$ and bipartiteness ratio at most\n$4\\sqrt{\\theta/\\epsilon}$. By combining a truncation operation, we give a local\nalgorithm \\verb|LocDB|, which has asymptotically the same approximation\nguarantee as the algorithm \\verb|SwpDB| on both the volume and bipartiteness\nratio of the output set, and runs in time\n$O(\\epsilon^2\\theta^{-2}k^{1+\\epsilon}\\ln^3k)$, independent of the size of the\ngraph. Finally, we give a spectral characterization of the small dense\nbipartite-like subgraphs by using the $k$th \\textit{largest} eigenvalue of the\nLaplacian of the graph.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2012 09:49:32 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2013 11:43:10 GMT"}], "update_date": "2013-09-19", "authors_parsed": [["Li", "Angsheng", ""], ["Peng", "Pan", ""]]}, {"id": "1209.5052", "submitter": "Pan Peng", "authors": "Angsheng Li and Pan Peng", "title": "Testing Small Set Expansion in General Graphs", "comments": "23 pages; STACS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of testing small set expansion for general graphs. A\ngraph $G$ is a $(k,\\phi)$-expander if every subset of volume at most $k$ has\nconductance at least $\\phi$. Small set expansion has recently received\nsignificant attention due to its close connection to the unique games\nconjecture, the local graph partitioning algorithms and locally testable codes.\n  We give testers with two-sided error and one-sided error in the adjacency\nlist model that allows degree and neighbor queries to the oracle of the input\ngraph. The testers take as input an $n$-vertex graph $G$, a volume bound $k$,\nan expansion bound $\\phi$ and a distance parameter $\\varepsilon>0$. For the\ntwo-sided error tester, with probability at least $2/3$, it accepts the graph\nif it is a $(k,\\phi)$-expander and rejects the graph if it is $\\varepsilon$-far\nfrom any $(k^*,\\phi^*)$-expander, where $k^*=\\Theta(k\\varepsilon)$ and\n$\\phi^*=\\Theta(\\frac{\\phi^4}{\\min\\{\\log(4m/k),\\log n\\}\\cdot(\\ln k)})$. The\nquery complexity and running time of the tester are\n$\\widetilde{O}(\\sqrt{m}\\phi^{-4}\\varepsilon^{-2})$, where $m$ is the number of\nedges of the graph. For the one-sided error tester, it accepts every\n$(k,\\phi)$-expander, and with probability at least $2/3$, rejects every graph\nthat is $\\varepsilon$-far from $(k^*,\\phi^*)$-expander, where\n$k^*=O(k^{1-\\xi})$ and $\\phi^*=O(\\xi\\phi^2)$ for any $0<\\xi<1$. The query\ncomplexity and running time of this tester are\n$\\widetilde{O}(\\sqrt{\\frac{n}{\\varepsilon^3}}+\\frac{k}{\\varepsilon \\phi^4})$.\nWe also give a two-sided error tester with smaller gap between $\\phi^*$ and\n$\\phi$ in the rotation map model that allows (neighbor, index) queries and\ndegree queries.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2012 11:18:33 GMT"}, {"version": "v2", "created": "Tue, 22 Apr 2014 21:00:00 GMT"}, {"version": "v3", "created": "Mon, 5 Jan 2015 08:00:10 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Li", "Angsheng", ""], ["Peng", "Pan", ""]]}, {"id": "1209.5360", "submitter": "Michael Mitzenmacher", "authors": "Michael Mitzenmacher", "title": "Balanced Allocations and Double Hashing", "comments": "Further updated, small improvements/typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Double hashing has recently found more common usage in schemes that use\nmultiple hash functions. In double hashing, for an item $x$, one generates two\nhash values $f(x)$ and $g(x)$, and then uses combinations $(f(x) +k g(x)) \\bmod\nn$ for $k=0,1,2,...$ to generate multiple hash values from the initial two. We\nfirst perform an empirical study showing that, surprisingly, the performance\ndifference between double hashing and fully random hashing appears negligible\nin the standard balanced allocation paradigm, where each item is placed in the\nleast loaded of $d$ choices, as well as several related variants. We then\nprovide theoretical results that explain the behavior of double hashing in this\ncontext.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 18:37:40 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2013 00:47:22 GMT"}, {"version": "v3", "created": "Thu, 11 Jul 2013 18:45:05 GMT"}, {"version": "v4", "created": "Wed, 29 Jan 2014 18:36:15 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Mitzenmacher", "Michael", ""]]}, {"id": "1209.5441", "submitter": "Sebastiano Vigna", "authors": "Djamal Belazzougui, Paolo Boldi, Sebastiano Vigna", "title": "Predecessor search with distance-sensitive query time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A predecessor (successor) search finds the largest element $x^-$ smaller than\nthe input string $x$ (the smallest element $x^+$ larger than or equal to $x$,\nrespectively) out of a given set $S$; in this paper, we consider the static\ncase (i.e., $S$ is fixed and does not change over time) and assume that the $n$\nelements of $S$ are available for inspection. We present a number of algorithms\nthat, with a small additional index (usually of O(n log w) bits, where $w$ is\nthe string length), can answer predecessor/successor queries quickly and with\ntime bounds that depend on different kinds of distance, improving significantly\nseveral results that appeared in the recent literature. Intuitively, our first\nresult has a running time that depends on the distance between $x$ and $x^\\pm$:\nit is especially efficient when the input $x$ is either very close to or very\nfar from $x^-$ or $x^+$; our second result depends on some global notion of\ndistance in the set $S$, and is fast when the elements of $S$ are more or less\nequally spaced in the universe; finally, for our third result we rely on a\nfinger (i.e., an element of $S$) to improve upon the first one; its running\ntime depends on the distance between the input and the finger.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 21:57:39 GMT"}], "update_date": "2012-09-26", "authors_parsed": [["Belazzougui", "Djamal", ""], ["Boldi", "Paolo", ""], ["Vigna", "Sebastiano", ""]]}, {"id": "1209.5491", "submitter": "Martin Roetteler", "authors": "Brittanney Amento, Martin Roetteler, Rainer Steinwandt", "title": "Quantum binary field inversion: improved circuit depth via choice of\n  basis representation", "comments": "17 pages, 7 figures, 2 tables", "journal-ref": "Quantum Information & Computation 13(1-2): 116-134 (2013)", "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite fields of the form GF(2^m) play an important role in coding theory and\ncryptography. We show that the choice of how to represent the elements of these\nfields can have a significant impact on the resource requirements for quantum\narithmetic. In particular, we show how the use of Gaussian normal basis\nrepresentations and of `ghost-bit basis' representations can be used to\nimplement inverters with a quantum circuit of depth O(m log(m)). To the best of\nour knowledge, this is the first construction with subquadratic depth reported\nin the literature. Our quantum circuit for the computation of multiplicative\ninverses is based on the Itoh-Tsujii algorithm which exploits that in normal\nbasis representation squaring corresponds to a permutation of the coefficients.\nWe give resource estimates for the resulting quantum circuit for inversion over\nbinary fields GF(2^m) based on an elementary gate set that is useful for\nfault-tolerant implementation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 04:50:34 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Amento", "Brittanney", ""], ["Roetteler", "Martin", ""], ["Steinwandt", "Rainer", ""]]}, {"id": "1209.5566", "submitter": "Neta Barkay", "authors": "Neta Barkay, Ely Porat, Bar Shalem", "title": "Feasible Sampling of Non-strict Turnstile Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first feasible method for sampling a dynamic data stream with\ndeletions, where the sample consists of pairs $(k,C_k)$ of a value $k$ and its\nexact total count $C_k$. Our algorithms are for both Strict Turnstile data\nstreams and the most general Non-strict Turnstile data streams, where each\nelement may have a negative total count. Our method improves by an order of\nmagnitude the known processing time of each element in the stream, which is\nextremely crucial for data stream applications. For example, for a sample of\nsize $O(\\epsilon^{-2} \\log{(1/\\delta)})$ in Non-strict streams, our solution\nrequires $O((\\log\\log(1/\\epsilon))^2 + (\\log\\log(1/\\delta)) ^ 2)$ operations\nper stream element, whereas the best previous solution requires\n$O(\\epsilon^{-2} \\log^2(1/\\delta))$ evaluations of a fully independent hash\nfunction per element. Here $1-\\delta$ is the success probability and $\\epsilon$\nis the additive approximation error.\n  We achieve this improvement by constructing a single data structure from\nwhich multiple elements can be extracted with very high success probability.\nThe sample we generate is useful for calculating both forward and inverse\ndistribution statistics, within an additive error, with provable guarantees on\nthe success probability. Furthermore, our algorithms can run on distributed\nsystems and extract statistics on the union or difference between data streams.\nThey can be used to calculate the Jaccard similarity coefficient as well.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 10:29:54 GMT"}], "update_date": "2012-09-26", "authors_parsed": [["Barkay", "Neta", ""], ["Porat", "Ely", ""], ["Shalem", "Bar", ""]]}, {"id": "1209.5608", "submitter": "Christian Wulff-Nilsen", "authors": "Christian Wulff-Nilsen", "title": "Faster Deterministic Fully-Dynamic Graph Connectivity", "comments": "To appear at SODA 2013. 19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new deterministic bounds for fully-dynamic graph connectivity. Our\ndata structure supports updates (edge insertions/deletions) in\n$O(\\log^2n/\\log\\log n)$ amortized time and connectivity queries in $O(\\log\nn/\\log\\log n)$ worst-case time, where $n$ is the number of vertices of the\ngraph. This improves the deterministic data structures of Holm, de Lichtenberg,\nand Thorup (STOC 1998, J.ACM 2001) and Thorup (STOC 2000) which both have\n$O(\\log^2n)$ amortized update time and $O(\\log n/\\log\\log n)$ worst-case query\ntime. Our model of computation is the same as that of Thorup, i.e., a pointer\nmachine with standard $AC^0$ instructions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 13:42:56 GMT"}], "update_date": "2012-09-26", "authors_parsed": [["Wulff-Nilsen", "Christian", ""]]}, {"id": "1209.5615", "submitter": "Robert Rettinger", "authors": "Robert Rettinger (FernUni Hagen)", "title": "On computable approximations of Landau's constant", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 8, Issue 4 (November\n  20, 2012) lmcs:1189", "doi": "10.2168/LMCS-8(4:15)2012", "report-no": null, "categories": "cs.NA cs.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm which computes the Landau constant up to any given\nprecision.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 14:05:20 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2012 14:18:04 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Rettinger", "Robert", "", "FernUni Hagen"]]}, {"id": "1209.5647", "submitter": "Hien Tran Dang", "authors": "Tran Dang Hien, Do Van Tuan, Pham Van At", "title": "Additive Update Algorithm for Nonnegative Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) is an emerging technique with a wide\nspectrum of potential applications in data analysis. Mathematically, NMF can be\nformulated as a minimization problem with nonnegative constraints. This problem\nis currently attracting much attention from researchers for theoretical reasons\nand for potential applications. Currently, the most popular approach to solve\nNMF is the multiplicative update algorithm proposed by D.D. Lee and H.S. Seung.\nIn this paper, we propose an additive update algorithm, that has faster\ncomputational speed than the algorithm of D.D. Lee and H.S. Seung.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 15:36:32 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2012 15:05:09 GMT"}, {"version": "v3", "created": "Tue, 25 Dec 2012 02:40:04 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Hien", "Tran Dang", ""], ["Van Tuan", "Do", ""], ["Van At", "Pham", ""]]}, {"id": "1209.5765", "submitter": "Kevin Mote", "authors": "Kevin Mote", "title": "Fast Point-Feature Label Placement for Dynamic Visualizations (2007)", "comments": null, "journal-ref": "Information Visualization (2007) 6, 249-260", "doi": "10.1057/PALGRAVE.IVS.9500163", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a fast approach to automatic point label de-confliction\non interactive maps. The general Map Labeling problem is NP-hard and has been\nthe subject of much study for decades. Computerized maps have introduced\ninteractive zooming and panning, which has intensified the problem. Providing\ndynamic labels for such maps typically requires a time-consuming pre-processing\nphase. In the realm of visual analytics, however, the labeling of interactive\nmaps is further complicated by the use of massive datasets laid out in\narbitrary configurations, thus rendering reliance on a pre-processing phase\nuntenable. This paper offers a method for labeling point-features on dynamic\nmaps in real time without pre-processing. The algorithm presented is efficient,\nscalable, and exceptionally fast; it can label interactive charts and diagrams\nat speeds of multiple frames per second on maps with tens of thousands of\nnodes. To accomplish this, the algorithm employs a novel geometric\nde-confliction approach, the 'trellis strategy,' along with a unique label\ncandidate cost analysis to determine the 'least expensive' label configuration.\nThe speed and scalability of this approach make it well-suited for visual\nanalytic applications.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 20:58:42 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Mote", "Kevin", ""]]}, {"id": "1209.5766", "submitter": "Kevin Mote", "authors": "Kevin Mote", "title": "Fast Point-Feature Label Placement for Dynamic Visualizations (Thesis)", "comments": "Master's Thesis, Washington State University", "journal-ref": null, "doi": "10.1057/PALGRAVE.IVS.9500163", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a fast approach to automatic point label de-confliction\non interactive maps. The general Map Labeling problem is NP-hard and has been\nthe subject of much study for decades. Computerized maps have introduced\ninteractive zooming and panning, which has intensified the problem. Providing\ndynamic labels for such maps typically requires a time-consuming pre-processing\nphase. In the realm of visual analytics, however, the labeling of interactive\nmaps is further complicated by the use of massive datasets laid out in\narbitrary configurations, thus rendering reliance on a pre-processing phase\nuntenable. This paper offers a method for labeling point-features on dynamic\nmaps in real time without pre-processing. The algorithm presented is efficient,\nscalable, and exceptionally fast; it can label interactive charts and diagrams\nat speeds of multiple frames per second on maps with tens of thousands of\nnodes. To accomplish this, the algorithm employs a novel geometric\nde-confliction approach, the 'trellis strategy,' along with a unique label\ncandidate cost analysis to determine the \"least expensive\" label configuration.\nThe speed and scalability of this approach make it well-suited for visual\nanalytic applications.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 20:59:51 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Mote", "Kevin", ""]]}, {"id": "1209.5791", "submitter": "Michael Bannister", "authors": "Michael J. Bannister, Christopher DuBois, David Eppstein, Padhraic\n  Smyth", "title": "Windows into Relational Events: Data Structures for Contiguous\n  Subsequences of Edges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of analyzing social network data sets in which the\nedges of the network have timestamps, and we wish to analyze the subgraphs\nformed from edges in contiguous subintervals of these timestamps. We provide\ndata structures for these problems that use near-linear preprocessing time,\nlinear space, and sublogarithmic query time to handle queries that ask for the\nnumber of connected components, number of components that contain cycles,\nnumber of vertices whose degree equals or is at most some predetermined value,\nnumber of vertices that can be reached from a starting set of vertices by\ntime-increasing paths, and related queries.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 23:25:03 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Bannister", "Michael J.", ""], ["DuBois", "Christopher", ""], ["Eppstein", "David", ""], ["Smyth", "Padhraic", ""]]}, {"id": "1209.5818", "submitter": "Bharath Pattabiraman", "authors": "Bharath Pattabiraman, Md. Mostofa Ali Patwary, Assefaw H. Gebremedhin,\n  Wei-keng Liao, and Alok Choudhary", "title": "Fast Algorithms for the Maximum Clique Problem on Massive Sparse Graphs", "comments": "15 pages (including 2-page appendix), 5 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum clique problem is a well known NP-Hard problem with applications\nin data mining, network analysis, informatics, and many other areas. Although\nthere exist several algorithms with acceptable runtimes for certain classes of\ngraphs, many of them are infeasible for massive graphs. We present a new exact\nalgorithm that employs novel pruning techniques to very quickly find maximum\ncliques in large sparse graphs. Extensive experiments on several types of\nsynthetic and real-world graphs show that our new algorithm is up to several\norders of magnitude faster than existing algorithms for most instances. We also\npresent a heuristic variant that runs orders of magnitude faster than the exact\nalgorithm, while providing optimal or near-optimal solutions.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 02:23:31 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2012 19:48:59 GMT"}, {"version": "v3", "created": "Sat, 10 Nov 2012 01:21:42 GMT"}, {"version": "v4", "created": "Wed, 14 Nov 2012 17:31:36 GMT"}], "update_date": "2012-11-15", "authors_parsed": [["Pattabiraman", "Bharath", ""], ["Patwary", "Md. Mostofa Ali", ""], ["Gebremedhin", "Assefaw H.", ""], ["Liao", "Wei-keng", ""], ["Choudhary", "Alok", ""]]}, {"id": "1209.5821", "submitter": "Ioannis Koutis", "authors": "Ioannis Koutis, Alex Levin, Richard Peng", "title": "Faster spectral sparsification and numerical algorithms for SDD matrices", "comments": "This work subsumes the results reported in our STACS 2012 paper\n  \"Improved spectral sparsification and numerical algorithms for SDD matrices\".\n  The first two algorithms are identical but the fastest O(mloglog n) time\n  algorithm applies now for graphs of average degree log^5 n and more,\n  improving upon the average degree n^c, c>0 of our previous work. Version 2\n  fixes a few typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study algorithms for spectral graph sparsification. The input is a graph\n$G$ with $n$ vertices and $m$ edges, and the output is a sparse graph\n$\\tilde{G}$ that approximates $G$ in an algebraic sense. Concretely, for all\nvectors $x$ and any $\\epsilon>0$, $\\tilde{G}$ satisfies $$ (1-\\epsilon) x^T L_G\nx \\leq x^T L_{\\tilde{G}} x \\leq (1+\\epsilon) x^T L_G x, $$ where $L_G$ and\n$L_{\\tilde{G}}$ are the Laplacians of $G$ and $\\tilde{G}$ respectively. We show\nthat the fastest known algorithm for computing a sparsifier with $O(n\\log\nn/\\epsilon^2)$ edges can actually run in $\\tilde{O}(m\\log^2 n)$ time, an\n$O(\\log n)$ factor faster than before. We also present faster sparsification\nalgorithms for slightly dense graphs. Specifically, we give an algorithm that\nruns in $\\tilde{O}(m\\log n)$ time and generates a sparsifier with\n$\\tilde{O}(n\\log^3{n}/\\epsilon^2)$ edges. This implies that a sparsifier with\n$O(n\\log n/\\epsilon^2)$ edges can be computed in $\\tilde{O}(m\\log n)$ time for\ngraphs with more than $O(n\\log^4 n)$ edges. We also give an $\\tilde{O}(m)$ time\nalgorithm for graphs with more than $n\\log^5 n (\\log \\log n)^3$ edges of\npolynomially bounded weights, and an $O(m)$ algorithm for unweighted graphs\nwith more than $n\\log^8 n (\\log \\log n)^3 $ edges and $n\\log^{10} n (\\log \\log\nn)^5$ edges in the weighted case. The improved sparsification algorithms are\nemployed to accelerate linear system solvers and algorithms for computing\nfundamental eigenvectors of slightly dense SDD matrices.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 03:15:16 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2013 17:20:14 GMT"}, {"version": "v3", "created": "Sat, 16 Nov 2013 15:08:23 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Koutis", "Ioannis", ""], ["Levin", "Alex", ""], ["Peng", "Richard", ""]]}, {"id": "1209.5828", "submitter": "Jian Li", "authors": "Lingxiao Huang, Jian Li", "title": "Approximating the Expected Values for Combinatorial Optimization\n  Problems over Stochastic Points", "comments": "30 pages. This version has several new results, a new title, and\n  completely subsumes the previous versions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the stochastic geometry model where the location of each node is\na random point in a given metric space, or the existence of each node is\nuncertain. We study the problems of computing the expected lengths of several\ncombinatorial or geometric optimization problems over stochastic points,\nincluding closest pair, minimum spanning tree, $k$-clustering, minimum perfect\nmatching, and minimum cycle cover. We also consider the problem of estimating\nthe probability that the length of closest pair, or the diameter, is at most,\nor at least, a given threshold. Most of the above problems are known to be\n$\\sharpP$-hard. We obtain FPRAS (Fully Polynomial Randomized Approximation\nScheme) for most of them in both the existential and locational uncertainty\nmodels. Our result for stochastic minimum spanning trees in the locational\nuncertain model improves upon the previously known constant factor\napproximation algorithm. Our results for other problems are the first known to\nthe best of our knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 04:13:05 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2012 00:38:37 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2012 07:02:10 GMT"}, {"version": "v4", "created": "Tue, 17 Feb 2015 14:24:39 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Huang", "Lingxiao", ""], ["Li", "Jian", ""]]}, {"id": "1209.5969", "submitter": "Mark Newman", "authors": "Maria A. Riolo and M. E. J. Newman", "title": "First-principles multiway spectral partitioning of graphs", "comments": "12 pages, 9 figures", "journal-ref": "Journal of Complex Networks 2, 121-140 (2014)", "doi": "10.1093/comnet/cnt021", "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimum-cut partitioning of a graph into more than two parts\nusing spectral methods. While there exist well-established spectral algorithms\nfor this problem that give good results, they have traditionally not been well\nmotivated. Rather than being derived from first principles by minimizing graph\ncuts, they are typically presented without direct derivation and then proved\nafter the fact to work. In this paper, we take a contrasting approach in which\nwe start with a matrix formulation of the minimum cut problem and then show,\nvia a relaxed optimization, how it can be mapped onto a spectral embedding\ndefined by the leading eigenvectors of the graph Laplacian. The end result is\nan algorithm that is similar in spirit to, but different in detail from,\nprevious spectral partitioning approaches. In tests of the algorithm we find\nthat it outperforms previous approaches on certain particularly difficult\npartitioning problems.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 15:24:14 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2013 15:59:10 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Riolo", "Maria A.", ""], ["Newman", "M. E. J.", ""]]}, {"id": "1209.6007", "submitter": "Ahmet Erdem Sariy\\\"uce", "authors": "Ahmet Erdem Sar{\\i}y\\\"uce, Erik Saule, Kamer Kaya, \\\"Umit V.\n  \\c{C}ataly\\\"urek", "title": "Shattering and Compressing Networks for Centrality Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Who is more important in a network? Who controls the flow between the nodes\nor whose contribution is significant for connections? Centrality metrics play\nan important role while answering these questions. The betweenness metric is\nuseful for network analysis and implemented in various tools. Since it is one\nof the most computationally expensive kernels in graph mining, several\ntechniques have been proposed for fast computation of betweenness centrality.\nIn this work, we propose and investigate techniques which compress a network\nand shatter it into pieces so that the rest of the computation can be handled\nindependently for each piece. Although we designed and tuned the shattering\nprocess for betweenness, it can be adapted for other centrality metrics in a\nstraightforward manner. Experimental results show that the proposed techniques\ncan be a great arsenal to reduce the centrality computation time for various\ntypes of networks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 17:09:16 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Sar\u0131y\u00fcce", "Ahmet Erdem", ""], ["Saule", "Erik", ""], ["Kaya", "Kamer", ""], ["\u00c7ataly\u00fcrek", "\u00dcmit V.", ""]]}, {"id": "1209.6129", "submitter": "Deepak Garg Dr", "authors": "Deepak Garg, S C Saxena, L M Bhardwaj", "title": "A New Middle Path Approach For Alignements In Blast", "comments": null, "journal-ref": "Journal of Biological Systems, Vol. 14, No. 4 , pp. 567-581 ISSN\n  0218-3390 2006", "doi": null, "report-no": null, "categories": "cs.DS cs.CE q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper deals with a new middle path approach developed for reducing\nalignment calculations in BLAST algorithm. This is a new step which is\nintroduced in BLAST algorithm in between the ungapped and gapped alignments.\nThis step of middle path approach between the ungapped and gapped alignments\nreduces the number of sequences going for gapped alignment. This results in the\nimprovement in speed for alignment up to 30 percent.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 05:47:32 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Garg", "Deepak", ""], ["Saxena", "S C", ""], ["Bhardwaj", "L M", ""]]}, {"id": "1209.6158", "submitter": "Carola Doerr", "authors": "Benjamin Doerr, Carola Doerr, Shay Moran, Shlomo Moran", "title": "Simple and Optimal Randomized Fault-Tolerant Rumor Spreading", "comments": "This is the author-generated version of a paper which is to appear in\n  Distributed Computing, Springer, DOI: 10.1007/s00446-014-0238-z It is\n  available online from\n  http://link.springer.com/article/10.1007/s00446-014-0238-z This version\n  contains some new results (Section 6)", "journal-ref": "Distributed Computing, Springer, 2015", "doi": "10.1007/s00446-014-0238-z", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the classic problem of spreading a piece of information in a group\nof $n$ fully connected processors. By suitably adding a small dose of\nrandomness to the protocol of Gasienic and Pelc (1996), we derive for the first\ntime protocols that (i) use a linear number of messages, (ii) are correct even\nwhen an arbitrary number of adversarially chosen processors does not\nparticipate in the process, and (iii) with high probability have the\nasymptotically optimal runtime of $O(\\log n)$ when at least an arbitrarily\nsmall constant fraction of the processors are working. In addition, our\nprotocols do not require that the system is synchronized nor that all\nprocessors are simultaneously woken up at time zero, they are fully based on\npush-operations, and they do not need an a priori estimate on the number of\nfailed nodes.\n  Our protocols thus overcome the typical disadvantages of the two known\napproaches, algorithms based on random gossip (typically needing a large number\nof messages due to their unorganized nature) and algorithms based on fair\nworkload splitting (which are either not {time-efficient} or require intricate\npreprocessing steps plus synchronization).\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 08:25:23 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2012 16:30:53 GMT"}, {"version": "v3", "created": "Wed, 13 Feb 2013 12:25:42 GMT"}, {"version": "v4", "created": "Tue, 17 Sep 2013 10:03:56 GMT"}, {"version": "v5", "created": "Mon, 5 Jan 2015 09:38:43 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Doerr", "Benjamin", ""], ["Doerr", "Carola", ""], ["Moran", "Shay", ""], ["Moran", "Shlomo", ""]]}, {"id": "1209.6204", "submitter": "Mikhail Kharinov Vyacheslavovich", "authors": "M. Kharinov", "title": "Reclassification formula that provides to surpass K-means method", "comments": "10 pages, 2 figures, 13 formulas", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a formula for the reclassification of multidimensional\ndata points (columns of real numbers, \"objects\", \"vectors\", etc.). This formula\ndescribes the change in the total squared error caused by reclassification of\ndata points from one cluster into another and prompts the way to calculate the\nsequence of optimal partitions, which are characterized by a minimum value of\nthe total squared error E (weighted sum of within-class variance,\nwithin-cluster sum of squares WCSS etc.), i.e. the sum of squared distances\nfrom each data point to its cluster center. At that source data points are\ntreated with repetitions allowed, and resulting clusters from different\npartitions, in general case, overlap each other. The final partitions are\ncharacterized by \"equilibrium\" stability with respect to the reclassification\nof the data points, where the term \"stability\" means that any prescribed\nreclassification of data points does not increase the total squared error E. It\nis important that conventional K-means method, in general case, provides\ngeneration of instable partitions with overstated values of the total squared\nerror E. The proposed method, based on the formula of reclassification, is more\nefficient than K-means method owing to converting of any partition into stable\none, as well as involving into the process of reclassification of certain sets\nof data points, in contrast to the classification of individual data points\naccording to K-means method.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 12:24:05 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Kharinov", "M.", ""]]}, {"id": "1209.6348", "submitter": "Martin Roetteler", "authors": "Brittanney Amento, Rainer Steinwandt, Martin Roetteler", "title": "Efficient quantum circuits for binary elliptic curve arithmetic:\n  reducing T-gate complexity", "comments": "14 pages", "journal-ref": "Quantum Information & Computation 13(7-8): 631-644 (2013)", "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elliptic curves over finite fields GF(2^n) play a prominent role in modern\ncryptography. Published quantum algorithms dealing with such curves build on a\nshort Weierstrass form in combination with affine or projective coordinates. In\nthis paper we show that changing the curve representation allows a substantial\nreduction in the number of T-gates needed to implement the curve arithmetic. As\na tool, we present a quantum circuit for computing multiplicative inverses in\nGF(2^n) in depth O(n log n) using a polynomial basis representation, which may\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 19:54:28 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Amento", "Brittanney", ""], ["Steinwandt", "Rainer", ""], ["Roetteler", "Martin", ""]]}, {"id": "1209.6396", "submitter": "Jeff M Phillips", "authors": "Jeff M. Phillips", "title": "Chernoff-Hoeffding Inequality and Applications", "comments": "Expository document hopefully at the level of an advanced undergrad\n  or beginning graduate student. The update corrects a missing bound on a\n  parameter in one form of the main theorem", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with modern big data sets, a very common theme is reducing the\nset through a random process. These generally work by making \"many simple\nestimates\" of the full data set, and then judging them as a whole. Perhaps\nmagically, these \"many simple estimates\" can provide a very accurate and small\nrepresentation of the large data set. The key tool in showing how many of these\nsimple estimates are needed for a fixed accuracy trade-off is the\nChernoff-Hoeffding inequality[Che52,Hoe63]. This document provides a simple\nform of this bound, and two examples of its use.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 23:41:52 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2013 05:25:19 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Phillips", "Jeff M.", ""]]}, {"id": "1209.6449", "submitter": "Simone Faro", "authors": "Simone Faro and M. Oguzhan K\\\"ulekci", "title": "Fast Packed String Matching for Short Patterns", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for all occurrences of a pattern in a text is a fundamental problem\nin computer science with applications in many other fields, like natural\nlanguage processing, information retrieval and computational biology. In the\nlast two decades a general trend has appeared trying to exploit the power of\nthe word RAM model to speed-up the performances of classical string matching\nalgorithms. In this model an algorithm operates on words of length w, grouping\nblocks of characters, and arithmetic and logic operations on the words take one\nunit of time. In this paper we use specialized word-size packed string matching\ninstructions, based on the Intel streaming SIMD extensions (SSE) technology, to\ndesign very fast string matching algorithms in the case of short patterns. From\nour experimental results it turns out that, despite their quadratic worst case\ntime complexity, the new presented algorithms become the clear winners on the\naverage for short patterns, when compared against the most effective algorithms\nknown in literature.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 08:28:43 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Faro", "Simone", ""], ["K\u00fclekci", "M. Oguzhan", ""]]}, {"id": "1209.6481", "submitter": "Giorgio Lucarelli", "authors": "Evripidis Bampis, Giorgio Lucarelli, Ioannis Nemparis", "title": "Improved Approximation Algorithms for the Non-preemptive Speed-scaling\n  Problem", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are given a set of jobs, each one specified by its release date, its\ndeadline and its processing volume (work), and a single (or a set of)\nspeed-scalable processor(s). We adopt the standard model in speed-scaling in\nwhich if a processor runs at speed s then the energy consumption is s^{\\alpha}\nper time unit, where \\alpha>1. Our goal is to find a schedule respecting the\nrelease dates and the deadlines of the jobs so that the total energy\nconsumption is minimized. While most previous works have studied the preemptive\ncase of the problem, where a job may be interrupted and resumed later, we focus\non the non-preemptive case where once a job starts its execution, it has to\ncontinue until its completion without any interruption. We propose improved\napproximation algorithms for particular instances of the multiprocessor\nnon-preemptive speed-scaling problem.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 11:13:59 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2012 16:21:36 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Bampis", "Evripidis", ""], ["Lucarelli", "Giorgio", ""], ["Nemparis", "Ioannis", ""]]}, {"id": "1209.6486", "submitter": "Deepak Garg Dr", "authors": "Megha Tyagi and Deepak Garg", "title": "Comparative Analysis of dynamic graph techniques and data strucutre", "comments": null, "journal-ref": "International Journal of Computer Applications 45(5):41-46, 2012", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamically changing graphs are used in many applications of graph\nalgorithms. The scope of these graphs are in graphics, communication networks\nand in VLSI designs where graphs are subjected to change, such as addition and\ndeletion of edges and vertices. There is a rich body of the algorithms and data\nstructures used for dynamic graphs. The paper overview the techniques and data\nstructures used in various dynamic algorithms. The effort is tried to find out\nthe comparison in these techniques namely the hierarchical decomposition of\ngraphs and highlighting the ingenuity used in designing these algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 11:38:47 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Tyagi", "Megha", ""], ["Garg", "Deepak", ""]]}, {"id": "1209.6495", "submitter": "Deepak Garg Dr", "authors": "Parth Patel, Deepak Garg", "title": "Comparison of Advance Tree Data Structures", "comments": null, "journal-ref": "International Journal of Computer Applications (2012) Vol. 41 No.2\n  pp. 11-21", "doi": "10.5120/5512-7504", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Btree and Rtree are two basic index structures; many different variants of\nthem are proposed after them. Different variants are used in specific\napplication for the performance optimization. In this paper different variants\nof Btree and Rtree are discussed and compared. Index structures are different\nin terms of structure, query support, data type support and application. Index\nstructures are discussed first. Btree and its variants are discussed and them\nRtree and its variants are discussed. Some structures example is also shown for\nthe more clear idea. Then comparison is made between all structure with respect\nto complexity, query type support, data type support and application.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 11:54:01 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Patel", "Parth", ""], ["Garg", "Deepak", ""]]}, {"id": "1209.6506", "submitter": "Torsten Ueckerdt", "authors": "Stephen Kobourov and Torsten Ueckerdt and Kevin Verbeek", "title": "Combinatorial and Geometric Properties of Planar Laman Graphs", "comments": "17 pages, 11 figures, SODA 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Laman graphs naturally arise in structural mechanics and rigidity theory.\nSpecifically, they characterize minimally rigid planar bar-and-joint systems\nwhich are frequently needed in robotics, as well as in molecular chemistry and\npolymer physics. We introduce three new combinatorial structures for planar\nLaman graphs: angular structures, angle labelings, and edge labelings. The\nlatter two structures are related to Schnyder realizers for maximally planar\ngraphs. We prove that planar Laman graphs are exactly the class of graphs that\nhave an angular structure that is a tree, called angular tree, and that every\nangular tree has a corresponding angle labeling and edge labeling.\n  Using a combination of these powerful combinatorial structures, we show that\nevery planar Laman graph has an L-contact representation, that is, planar Laman\ngraphs are contact graphs of axis-aligned L-shapes. Moreover, we show that\nplanar Laman graphs and their subgraphs are the only graphs that can be\nrepresented this way.\n  We present efficient algorithms that compute, for every planar Laman graph G,\nan angular tree, angle labeling, edge labeling, and finally an L-contact\nrepresentation of G. The overall running time is O(n^2), where n is the number\nof vertices of G, and the L-contact representation is realized on the n x n\ngrid.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 12:58:54 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Kobourov", "Stephen", ""], ["Ueckerdt", "Torsten", ""], ["Verbeek", "Kevin", ""]]}, {"id": "1209.6528", "submitter": "Gregory Gutin", "authors": "Robert Crowston, Gregory Gutin, Mark Jones, Gabriele Muciaccia, Anders\n  Yeo", "title": "Parameterizations of Test Cover with Bounded Test Sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the {\\sc Test Cover} problem we are given a hypergraph $H=(V,\n\\mathcal{E})$ with $|V|=n, |\\mathcal{E}|=m$, and we assume that $\\mathcal{E}$\nis a test cover, i.e. for every pair of vertices $x_i, x_j$, there exists an\nedge $e \\in \\mathcal{E}$ such that $|{x_i,x_j}\\cap e|=1$. The objective is to\nfind a minimum subset of $\\mathcal{E}$ which is a test cover. The problem is\nused for identification across many areas, and is NP-complete. From a\nparameterized complexity standpoint, many natural parameterizations of {\\sc\nTest Cover} are either $W[1]$-complete or have no polynomial kernel unless\n$coNP\\subseteq NP/poly$, and thus are unlikely to be solveable efficiently.\n  However, in practice the size of the edges is often bounded. In this paper we\nstudy the parameterized complexity of {\\sc Test-$r$-Cover}, the restriction of\n{\\sc Test Cover} in which each edge contains at most $r \\ge 2$ vertices. In\ncontrast to the unbounded case, we show that the following below-bound\nparameterizations of {\\sc Test-$r$-Cover} are fixed-parameter tractable with a\npolynomial kernel: (1) Decide whether there exists a test cover of size $n-k$,\nand (2) decide whether there exists a test cover of size $m-k$, where $k$ is\nthe parameter. In addition, we prove a new lower bound $\\lceil\n\\frac{2(n-1)}{r+1} \\rceil$ on the minimum size of a test cover when the size of\neach edge is bounded by $r$. {\\sc Test-$r$-Cover} parameterized above this\nbound is unlikely to be fixed-parameter tractable; in fact, we show that it is\npara-NP-complete, as it is NP-hard to decide whether an instance of {\\sc\nTest-$r$-Cover} has a test cover of size exactly $\\frac{2(n-1)}{r+1}$.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 14:13:14 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2013 09:55:54 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Crowston", "Robert", ""], ["Gutin", "Gregory", ""], ["Jones", "Mark", ""], ["Muciaccia", "Gabriele", ""], ["Yeo", "Anders", ""]]}, {"id": "1209.6540", "submitter": "Gabor Sarkozy", "authors": "G\\'abor N. S\\'ark\\\"ozy, Fei Song, Endre Szemer\\'edi, Shubhendu Trivedi", "title": "A Practical Regularity Partitioning Algorithm and its Applications in\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new clustering technique called Regularity\nClustering. This new technique is based on the practical variants of the two\nconstructive versions of the Regularity Lemma, a very useful tool in graph\ntheory. The lemma claims that every graph can be partitioned into pseudo-random\ngraphs. While the Regularity Lemma has become very important in proving\ntheoretical results, it has no direct practical applications so far. An\nimportant reason for this lack of practical applications is that the graph\nunder consideration has to be astronomically large. This requirement makes its\napplication restrictive in practice where graphs typically are much smaller. In\nthis paper we propose modifications of the constructive versions of the\nRegularity Lemma that work for smaller graphs as well. We call this the\nPractical Regularity partitioning algorithm. The partition obtained by this is\nused to build the reduced graph which can be viewed as a compressed\nrepresentation of the original graph. Then we apply a pairwise clustering\nmethod such as spectral clustering on this reduced graph to get a clustering of\nthe original graph that we call Regularity Clustering. We present results of\nusing Regularity Clustering on a number of benchmark datasets and compare them\nwith standard clustering techniques, such as $k$-means and spectral clustering.\nThese empirical results are very encouraging. Thus in this paper we report an\nattempt to harness the power of the Regularity Lemma for real-world\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 15:01:22 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["S\u00e1rk\u00f6zy", "G\u00e1bor N.", ""], ["Song", "Fei", ""], ["Szemer\u00e9di", "Endre", ""], ["Trivedi", "Shubhendu", ""]]}]