[{"id": "1512.00077", "submitter": "Gabriele Farina", "authors": "Massimo Cairo, Gabriele Farina, Romeo Rizzi", "title": "Decoding Hidden Markov Models Faster Than Viterbi Via Online\n  Matrix-Vector (max, +)-Multiplication", "comments": "AAAI 2016, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel algorithm for the maximum a posteriori\ndecoding (MAPD) of time-homogeneous Hidden Markov Models (HMM), improving the\nworst-case running time of the classical Viterbi algorithm by a logarithmic\nfactor. In our approach, we interpret the Viterbi algorithm as a repeated\ncomputation of matrix-vector $(\\max, +)$-multiplications. On time-homogeneous\nHMMs, this computation is online: a matrix, known in advance, has to be\nmultiplied with several vectors revealed one at a time. Our main contribution\nis an algorithm solving this version of matrix-vector $(\\max,+)$-multiplication\nin subquadratic time, by performing a polynomial preprocessing of the matrix.\nEmploying this fast multiplication algorithm, we solve the MAPD problem in\n$O(mn^2/ \\log n)$ time for any time-homogeneous HMM of size $n$ and observation\nsequence of length $m$, with an extra polynomial preprocessing cost negligible\nfor $m > n$. To the best of our knowledge, this is the first algorithm for the\nMAPD problem requiring subquadratic time per observation, under the only\nassumption -- usually verified in practice -- that the transition probability\nmatrix does not change with time.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 22:38:07 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2015 10:40:51 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Cairo", "Massimo", ""], ["Farina", "Gabriele", ""], ["Rizzi", "Romeo", ""]]}, {"id": "1512.00101", "submitter": "Miao Yu", "authors": "Miao Yu, Shuhan Shen, Zhanyi Hu", "title": "Dynamic Parallel and Distributed Graph Cuts", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2609819", "report-no": null, "categories": "cs.DS cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Graph-cuts are widely used in computer vision. In order to speed up the\noptimization process and improve the scalability for large graphs, Strandmark\nand Kahl introduced a splitting method to split a graph into multiple subgraphs\nfor parallel computation in both shared and distributed memory models. However,\nthis parallel algorithm (parallel BK-algorithm) does not have a polynomial\nbound on the number of iterations and is found non-convergent in some cases due\nto the possible multiple optimal solutions of its sub-problems.\n  To remedy this non-convergence problem, in this work we first introduce a\nmerging method capable of merging any number of those adjacent sub-graphs which\ncould hardly reach an agreement on their overlapped region in the parallel BK\nalgorithm. Based on the pseudo-boolean representations of graph cuts,our\nmerging method is shown able to effectively reuse all the computed flows in\nthese sub-graphs. Through both the splitting and merging, we further propose a\ndynamic parallel and distributed graph-cuts algorithm with guaranteed\nconvergence to the globally optimal solutions within a predefined number of\niterations. In essence, this work provides a general framework to allow more\nsophisticated splitting and merging strategies to be employed to further boost\nperformance. Our dynamic parallel algorithm is validated with extensive\nexperimental results.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 00:19:50 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 00:31:20 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Yu", "Miao", ""], ["Shen", "Shuhan", ""], ["Hu", "Zhanyi", ""]]}, {"id": "1512.00153", "submitter": "Rahul Vaze", "authors": "Ajil Jalal, Rahul Vaze, Umang Bhaskar", "title": "Online Budgeted Repeated Matching", "comments": "Constant competitive ratio for adwords problem without the 'small'\n  edge weight restriction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic combinatorial online resource allocation problem is considered, where\nmultiple servers have individual capacity constraints, and at each time slot, a\nset of jobs arrives, that have potentially different weights to different\nservers. At each time slot, a one-to-one matching has to be found between jobs\nand servers, subject to individual capacity constraints, in an online manner.\nThe objective is to maximize the aggregate weight of jobs allotted to servers,\nsummed across time slots and servers, subject to individual capacity\nconstraints. This problem generalizes the well known adwords problem, and is\nalso relevant for various other modern applications. A simple greedy algorithm\nis shown to be 3-competitive, whenever the weight of any edge is at most half\nof the corresponding server capacity. Moreover, a randomized version of the\ngreedy algorithm is shown to be 6-competitive for the unrestricted edge weights\ncase. For parallel servers with small-weight jobs, we show that a\nload-balancing algorithm is near-optimal.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 06:16:54 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Jalal", "Ajil", ""], ["Vaze", "Rahul", ""], ["Bhaskar", "Umang", ""]]}, {"id": "1512.00333", "submitter": "Till Fluschnik", "authors": "Till Fluschnik, Danny Hermelin, Andr\\'e Nichterlein, and Rolf\n  Niedermeier", "title": "Fractals for Kernelization Lower Bounds", "comments": "An extended abstract appeared in Proc. of the 43rd International\n  Colloquium on Automata, Languages, and Programming (ICALP 2016). A full\n  version will appear in SIAM Journal on Discrete Mathematics (SIDMA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The composition technique is a popular method for excluding polynomial-size\nproblem kernels for NP-hard parameterized problems. We present a new technique\nexploiting triangle-based fractal structures for extending the range of\napplicability of compositions. Our technique makes it possible to prove new\nno-polynomial-kernel results for a number of problems dealing with\nlength-bounded cuts. In particular, answering an open question of Golovach and\nThilikos [Discrete Optim. 2011], we show that, unless NP $\\subseteq$ coNP /\npoly, the NP-hard Length-Bounded Edge-Cut (LBEC) problem (delete at most $k$\nedges such that the resulting graph has no $s$-$t$ path of length shorter than\n$\\ell$) parameterized by the combination of $k$ and $\\ell$ has no\npolynomial-size problem kernel. Our framework applies to planar as well as\ndirected variants of the basic problems and also applies to both edge and\nvertex deletion problems. Along the way, we show that LBEC remains NP-hard on\nplanar graphs, a result which we believe is interesting in its own right.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 16:55:40 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 08:17:22 GMT"}, {"version": "v3", "created": "Fri, 22 Dec 2017 19:09:31 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Fluschnik", "Till", ""], ["Hermelin", "Danny", ""], ["Nichterlein", "Andr\u00e9", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "1512.00378", "submitter": "Bojian Xu", "authors": "Wing-Kai Hon and Sharma V. Thankachan and Bojian Xu", "title": "An In-place Framework for Exact and Approximate Shortest Unique\n  Substring Queries", "comments": "15 pages. A preliminary version of this paper appears in Proceedings\n  of the 26th International Symposium on Algorithms and Computation (ISAAC),\n  Nagoya, Japan, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the exact shortest unique substring (SUS) finding problem, and\npropose its approximate version where mismatches are allowed, due to its\napplications in subfields such as computational biology. We design a generic\nin-place framework that fits to solve both the exact and approximate\n$k$-mismatch SUS finding, using the minimum $2n$ memory words plus $n$ bytes\nspace, where $n$ is the input string size. By using the in-place framework, we\ncan find the exact and approximate $k$-mismatch SUS for every string position\nusing a total of $O(n)$ and $O(n^2)$ time, respectively, regardless of the\nvalue of $k$. Our framework does not involve any compressed or succinct data\nstructures and thus is practical and easy to implement.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 18:34:23 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Hon", "Wing-Kai", ""], ["Thankachan", "Sharma V.", ""], ["Xu", "Bojian", ""]]}, {"id": "1512.00442", "submitter": "Ke Li", "authors": "Ke Li, Jitendra Malik", "title": "Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing", "comments": "13 pages, 6 figures; International Conference on Machine Learning\n  (ICML), 2016. This version corrects a typo in the pseudocode", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for retrieving k-nearest neighbours suffer from the curse of\ndimensionality. We argue this is caused in part by inherent deficiencies of\nspace partitioning, which is the underlying strategy used by most existing\nmethods. We devise a new strategy that avoids partitioning the vector space and\npresent a novel randomized algorithm that runs in time linear in dimensionality\nof the space and sub-linear in the intrinsic dimensionality and the size of the\ndataset and takes space constant in dimensionality of the space and linear in\nthe size of the dataset. The proposed algorithm allows fine-grained control\nover accuracy and speed on a per-query basis, automatically adapts to\nvariations in data density, supports dynamic updates to the dataset and is\neasy-to-implement. We show appealing theoretical properties and demonstrate\nempirically that the proposed algorithm outperforms locality-sensitivity\nhashing (LSH) in terms of approximation quality, speed and space efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 20:53:16 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 18:47:10 GMT"}, {"version": "v3", "created": "Thu, 6 Apr 2017 06:51:49 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1512.00444", "submitter": "Rex Fernando", "authors": "Eric Bach and Rex Fernando", "title": "Infinitely Many Carmichael Numbers for a Modified Miller-Rabin Prime\n  Test", "comments": "17 pages (21 with appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a variant of the Miller-Rabin primality test, which is in between\nMiller-Rabin and Fermat in terms of strength. We show that this test has\ninfinitely many \"Carmichael\" numbers. We show that the test can also be thought\nof as a variant of the Solovay-Strassen test. We explore the growth of the\ntest's \"Carmichael\" numbers, giving some empirical results and a discussion of\none particularly strong pattern which appears in the results.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 20:54:59 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 07:48:00 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Bach", "Eric", ""], ["Fernando", "Rex", ""]]}, {"id": "1512.00481", "submitter": "L\\'aszl\\'o Kozma", "authors": "Karl Bringmann, L\\'aszl\\'o Kozma, Shay Moran, and N.S. Narayanaswamy", "title": "Hitting Set for hypergraphs of low VC-dimension", "comments": "to be presented at ESA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the Hitting Set problem in set systems\n(hypergraphs) that avoid certain sub-structures. In particular, we characterize\nthe classical and parameterized complexity of the problem when the\nVapnik-Chervonenkis dimension (VC-dimension) of the input is small.\nVC-dimension is a natural measure of complexity of set systems. Several\ntractable instances of Hitting Set with a geometric or graph-theoretical flavor\nare known to have low VC-dimension. In set systems of bounded VC-dimension,\nHitting Set is known to admit efficient and almost optimal approximation\nalgorithms (Br\\\"onnimann and Goodrich, 1995; Even, Rawitz, and Shahar, 2005;\nAgarwal and Pan, 2014).\n  In contrast to these approximation-results, a low VC-dimension does not\nnecessarily imply tractability in the parameterized sense. In fact, we show\nthat Hitting Set is W[1]-hard already on inputs with VC-dimension 2, even if\nthe VC-dimension of the dual set system is also 2. Thus, Hitting Set is very\nunlikely to be fixed-parameter tractable even in this arguably simple case.\nThis answers an open question raised by King in 2010. For set systems whose\n(primal or dual) VC-dimension is 1, we show that Hitting Set is solvable in\npolynomial time.\n  To bridge the gap in complexity between the classes of inputs with\nVC-dimension 1 and 2, we use a measure that is more fine-grained than\nVC-dimension. In terms of this measure, we identify a sharp threshold where the\ncomplexity of Hitting Set transitions from polynomial-time-solvable to NP-hard.\nThe tractable class that lies just under the threshold is a generalization of\nEdge Cover, and thus extends the domain of polynomial-time tractability of\nHitting Set.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 21:07:51 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 11:42:47 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Bringmann", "Karl", ""], ["Kozma", "L\u00e1szl\u00f3", ""], ["Moran", "Shay", ""], ["Narayanaswamy", "N. S.", ""]]}, {"id": "1512.00501", "submitter": "Dai Bui", "authors": "Dai Nguyen Bui", "title": "CacheDiff: Fast Random Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sampling method called, CacheDiff, that has both time and space\ncomplexity of O(k) to randomly select k items from a pool of N items, in which\nN is known.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 22:15:01 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Bui", "Dai Nguyen", ""]]}, {"id": "1512.00519", "submitter": "Bryan Knowles", "authors": "Bryan A. Knowles and Mustafa Atici", "title": "Proposed Approximate Dynamic Programming for Pathfinding under Visible\n  Uncertainty", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuing our preleminary work \\cite{knowles14}, we define the\nsafest-with-sight pathfinding problems and explore its solution using\ntechniques borrowed from measure-theoretic probability theory. We find a simple\nrecursive definition for the probability that an ideal pathfinder will select\nan edge in a given scenario of an uncertain network where edges have\nprobabilities of failure and vertices provide \"vision\" of edges via\nlines-of-sight. We propose an approximate solution based on our theoretical\nfindings that would borrow techniques from approximate dynamic programming.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 17:29:43 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Knowles", "Bryan A.", ""], ["Atici", "Mustafa", ""]]}, {"id": "1512.00717", "submitter": "Stanislav Pyatykh", "authors": "Stanislav Pyatykh and J\\\"urgen Hesser", "title": "MMSE Estimation for Poisson Noise Removal in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poisson noise suppression is an important preprocessing step in several\napplications, such as medical imaging, microscopy, and astronomical imaging. In\nthis work, we propose a novel patch-wise Poisson noise removal strategy, in\nwhich the MMSE estimator is utilized in order to produce the denoising result\nfor each image patch. Fast and accurate computation of the MMSE estimator is\ncarried out using k-d tree search followed by search in the K-nearest neighbor\ngraph. Our experiments show that the proposed method is the preferable choice\nfor low signal-to-noise ratios.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 14:49:12 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Pyatykh", "Stanislav", ""], ["Hesser", "J\u00fcrgen", ""]]}, {"id": "1512.00859", "submitter": "Salvatore Mandr\\`a", "authors": "Salvatore Mandr\\`a, Gian Giacomo Guerreschi, Al\\'an Aspuru-Guzik", "title": "Faster than Classical Quantum Algorithm for dense Formulas of Exact\n  Satisfiability and Occupation Problems", "comments": "Added a new section \"Application to the Hamiltonian cycle problem\".\n  The paper has been published in New Journal of Physics", "journal-ref": "New Journal of Physics 18(7), 073003, 2016", "doi": "10.1088/1367-2630/18/7/073003", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an exact quantum algorithm for solving the Exact Satisfiability\n(XSAT) problem, which belongs to the important NP-complete complexity class.\nThe algorithm is based on an intuitive approach that can be divided into two\nparts: First, the identification and efficient characterization of a restricted\nsubspace that contains all the valid assignments of the XSAT; Second, a quantum\nsearch in such restricted subspace. The quantum algorithm can be used either to\nfind a valid assignment (or to certify that no solution exists) or to count the\ntotal number of valid assignments. The query complexities for the worst-case\nare respectively bounded by $O(\\sqrt{2^{n-M^{\\prime}}})$ and\n$O(2^{n-M^{\\prime}})$, where $n$ is the number of variables and $M^{\\prime}$\nthe number of linearly independent clauses. Remarkably, the proposed quantum\nalgorithm results to be faster than any known exact classical algorithm to\nsolve dense formulas of XSAT. As a concrete application, we provide the\nworst-case complexity for the Hamiltonian cycle problem obtained after mapping\nit to a suitable XSAT. Specifically, we show that the time complexity for the\nproposed quantum algorithm is bounded by $O(2^{n/4})$ for 3-regular undirected\ngraphs, where $n$ is the number of nodes. The same worst-case complexity holds\nfor $(3,3)$-regular bipartite graphs (the current best classical algorithm has\na (worst-case) running time bounded by $O(2^{31n/96})$). Finally, when compared\nto heuristic techniques for XSAT, the proposed quantum algorithm is faster than\nthe classical WalkSAT and Adiabatic Quantum Optimization for random instances\nwith a density of constraints close to the satisfiability threshold, the regime\nin which instances are typically the hardest to solve. The proposed quantum\nalgorithm can be also extended to the generalized version of the XSAT known as\nOccupation problem.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 21:00:10 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2016 17:53:38 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Mandr\u00e0", "Salvatore", ""], ["Guerreschi", "Gian Giacomo", ""], ["Aspuru-Guzik", "Al\u00e1n", ""]]}, {"id": "1512.01085", "submitter": "Carl Barton", "authors": "Carl Barton, Chang Liu and Solon P. Pissis", "title": "Fast Average-Case Pattern Matching on Weighted Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A weighted string over an alphabet of size $\\sigma$ is a string in which a\nset of letters may occur at each position with respective occurrence\nprobabilities. Weighted strings, also known as position weight matrices or\nuncertain sequences, naturally arise in many contexts. In this article, we\nstudy the problem of weighted string matching with a special focus on\naverage-case analysis. Given a weighted pattern string $x$ of length $m$, a\ntext string $y$ of length $n>m$, and a cumulative weight threshold $1/z$,\ndefined as the minimal probability of occurrence of factors in a weighted\nstring, we present an algorithm requiring average-case search time $o(n)$ for\npattern matching for weight ratio $\\frac{z}{m} < \\min\\{\\frac{1}{\\log\nz},\\frac{\\log \\sigma}{\\log z (\\log m + \\log \\log \\sigma)}\\}$. For a pattern\nstring $x$ of length $m$, a weighted text string $y$ of length $n>m$, and a\ncumulative weight threshold $1/z$, we present an algorithm requiring\naverage-case search time $o(\\sigma n)$ for the same weight ratio. The\nimportance of these results lies on the fact that these algorithms work in\naverage-case sublinear search time in the size of the text, and in linear\npreprocessing time and space in the size of the pattern, for these ratios.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 14:17:13 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 12:34:42 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Barton", "Carl", ""], ["Liu", "Chang", ""], ["Pissis", "Solon P.", ""]]}, {"id": "1512.01256", "submitter": "Gaurav Sinha", "authors": "Gaurav Sinha", "title": "Reconstruction of depth-3, top fan-in two circuits over characteristic\n  zero fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of arithmetic circuits has been heavily studied in the past\nfew years and has connections to proving lower bounds and deterministic\nidentity testing. In this paper we present a polynomial time randomized\nalgorithm for reconstructing $\\Sigma\\Pi\\Sigma(2)$ circuits over $\\mathbb{F}$\n($char(\\mathbb{F})=0$), i.e. depth$-3$ circuits with fan-in $2$ at the top\naddition gate and having coefficients from a field of characteristic $0$. The\nalgorithm needs only a blackbox query access to the polynomial $f \\in\n\\mathbb{F}[x_1,\\ldots, x_n]$ of degree $d$, computable by a\n$\\Sigma\\Pi\\Sigma(2)$ circuit $C$. In addition, we assume that \"simple rank\" of\nthis polynomial (essential number of variables after removing gcd of the two\nmultiplication gates) is bigger than a constant. Our algorithm runs in time\n$poly(n, d)$ and returns an equivalent $\\Sigma\\Pi\\Sigma(2)$ circuit(with high\nprobability). The problem of reconstructing $\\Sigma\\Pi\\Sigma(2)$ circuits over\nfinite fields was first proposed by Shpilka in [24]. The generalization to\n$\\Sigma\\Pi\\Sigma(k)$ circuits, $k = O(1)$ (over finite fields) was addressed by\nKarnin and Shpilka in [15]. The techniques in these previous involve iterating\nover all objects of certain kinds over the ambient field and thus running time\ndepends on size of the field $\\mathbb{F}$. Their reconstruction algorithm uses\nlower bounds on the lengths of Linear Locally Decodable Codes with $2$ queries.\nIn our settings, such ideas immediately pose a problem and we need new ideas to\nhandle the case of the characteristic $0$ field $\\mathbb{F}$. Our main\ntechniques are based on the use of Quantitative Syslvester Gallai Theorems from\nthe work of Barak et.al. [3] to find a small collection of subspaces to project\nonto. The heart of our paper lies in subtle applications of Quantitative\nSylvester Gallai theorems to prove why projections w.r.t. these subspaces can\nbe glued.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 21:32:16 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 22:15:57 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Sinha", "Gaurav", ""]]}, {"id": "1512.01293", "submitter": "Huacheng Yu", "authors": "Huacheng Yu", "title": "Cell-probe Lower Bounds for Dynamic Problems via a New Communication\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a new communication model to prove a data structure\nlower bound for the dynamic interval union problem. The problem is to maintain\na multiset of intervals $\\mathcal{I}$ over $[0, n]$ with integer coordinates,\nsupporting the following operations:\n  - insert(a, b): add an interval $[a, b]$ to $\\mathcal{I}$, provided that $a$\nand $b$ are integers in $[0, n]$;\n  - delete(a, b): delete a (previously inserted) interval $[a, b]$ from\n$\\mathcal{I}$;\n  - query(): return the total length of the union of all intervals in\n$\\mathcal{I}$.\n  It is related to the two-dimensional case of Klee's measure problem. We prove\nthat there is a distribution over sequences of operations with $O(n)$\ninsertions and deletions, and $O(n^{0.01})$ queries, for which any data\nstructure with any constant error probability requires $\\Omega(n\\log n)$ time\nin expectation. Interestingly, we use the sparse set disjointness protocol of\nH\\aa{}stad and Wigderson [ToC'07] to speed up a reduction from a new kind of\nnondeterministic communication games, for which we prove lower bounds.\n  For applications, we prove lower bounds for several dynamic graph problems by\nreducing them from dynamic interval union.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 00:42:59 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Yu", "Huacheng", ""]]}, {"id": "1512.01621", "submitter": "Serge Gaspers", "authors": "Fedor V. Fomin, Serge Gaspers, Daniel Lokshtanov, Saket Saurabh", "title": "Exact Algorithms via Monotone Local Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new general approach for designing exact exponential-time\nalgorithms for subset problems. In a subset problem the input implicitly\ndescribes a family of sets over a universe of size n and the task is to\ndetermine whether the family contains at least one set. Our approach is based\non \"monotone local search\", where the goal is to extend a partial solution to a\nsolution by adding as few elements as possible. More formally, in the extension\nproblem we are also given as input a subset X of the universe and an integer k.\nThe task is to determine whether one can add at most k elements to X to obtain\na set in the (implicitly defined) family. Our main result is that an O*(c^k)\ntime algorithm for the extension problem immediately yields a randomized\nalgorithm for finding a solution of any size with running time O*((2-1/c)^n).\n  In many cases, the extension problem can be reduced to simply finding a\nsolution of size at most k. Furthermore, efficient algorithms for finding small\nsolutions have been extensively studied in the field of parameterized\nalgorithms. Directly applying these algorithms, our theorem yields in one\nstroke significant improvements over the best known exponential-time algorithms\nfor several well-studied problems, including d-Hitting Set, Feedback Vertex\nSet, Node Unique Label Cover, and Weighted d-SAT. Our results demonstrate an\ninteresting and very concrete connection between parameterized algorithms and\nexact exponential-time algorithms.\n  We also show how to derandomize our algorithms at the cost of a\nsubexponential multiplicative factor in the running time. Our derandomization\nis based on an efficient construction of a new pseudo-random object that might\nbe of independent interest. Finally, we extend our methods to establish new\ncombinatorial upper bounds and develop enumeration algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 03:58:49 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Gaspers", "Serge", ""], ["Lokshtanov", "Daniel", ""], ["Saurabh", "Saket", ""]]}, {"id": "1512.01713", "submitter": "Rahul Saladi", "authors": "Saladi Rahul", "title": "Approximate Range Counting Revisited", "comments": "A preliminary version of this paper has been accepted at SoCG 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study range-searching for colored objects, where one has to count\n(approximately) the number of colors present in a query range. The problems\nstudied mostly involve orthogonal range-searching in two and three dimensions,\nand the dual setting of rectangle stabbing by points. We present optimal and\nnear-optimal solutions for these problems. Most of the results are obtained via\nreductions to the approximate uncolored version, and improved data-structures\nfor them. An additional contribution of this work is the introduction of nested\nshallow cuttings.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 23:43:40 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 22:22:18 GMT"}, {"version": "v3", "created": "Mon, 20 Mar 2017 19:54:01 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Rahul", "Saladi", ""]]}, {"id": "1512.01727", "submitter": "Daniel Shea", "authors": "Daniel Shea", "title": "Solving the Subset Sum Problem with Heap-Ordered Subset Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of algorithmic analysis, one of the more well-known exercises is\nthe subset sum problem. That is, given a set of integers, determine whether one\nor more integers in the set can sum to a target value. Aside from the\nbrute-force approach of verifying all combinations of integers, several\nsolutions have been found, ranging from clever uses of various data structures\nto computationally-efficient approximation solutions. In this paper, a unique\napproach is discussed which builds upon the existing min-heap solution for\npositive integers, introducing a tree-based data structure influenced by the\nbinomial heap. Termed the subset tree, this data structure solves the subset\nsum problem for all integers in time $O(N^3k\\log k)$, where $N$ is the length\nof the set and $k$ is the index of the list of subsets that is being searched.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 02:54:38 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 12:25:48 GMT"}, {"version": "v3", "created": "Fri, 6 May 2016 18:25:34 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Shea", "Daniel", ""]]}, {"id": "1512.01748", "submitter": "Ying Zhang", "authors": "Ying Zhang", "title": "Restricted Low-Rank Approximation via ADMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The matrix low-rank approximation problem with additional convex constraints\ncan find many applications and has been extensively studied before. However,\nthis problem is shown to be nonconvex and NP-hard; most of the existing\nsolutions are heuristic and application-dependent. In this paper, we show that,\nother than tons of application in current literature, this problem can be used\nto recover a feasible solution for SDP relaxation. By some sophisticated\ntricks, it can be equivalently posed in an appropriate form for the Alternating\nDirection Method of Multipliers (ADMM) to solve. The two updates of ADMM\ninclude the basic matrix low-rank approximation and projection onto a convex\nset. Different from the general non-convex problems, the sub-problems in each\nstep of ADMM can be solved exactly and efficiently in spite of their\nnon-convexity. Moreover, the algorithm will converge exponentially under proper\nconditions. The simulation results confirm its superiority over existing\nsolutions. We believe that the results in this paper provide a useful tool for\nthis important problem and will help to extend the application of ADMM to the\nnon-convex regime.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 06:12:15 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Zhang", "Ying", ""]]}, {"id": "1512.01775", "submitter": "Lee-Ad Gottlieb", "authors": "Yair Bartal, Lee-Ad Gottlieb", "title": "Approximate nearest neighbor search for $\\ell_p$-spaces ($2 < p <\n  \\infty$) via embeddings", "comments": "arXiv admin note: substantial text overlap with arXiv:1408.1789", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the problem of approximate nearest neighbor search has been\nwell-studied for Euclidean space and $\\ell_1$, few non-trivial algorithms are\nknown for $\\ell_p$ when ($2 < p < \\infty$). In this paper, we revisit this\nfundamental problem and present approximate nearest-neighbor search algorithms\nwhich give the first non-trivial approximation factor guarantees in this\nsetting.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 11:02:39 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Bartal", "Yair", ""], ["Gottlieb", "Lee-Ad", ""]]}, {"id": "1512.01781", "submitter": "Rico Zenklusen", "authors": "Mohit Singh and Rico Zenklusen", "title": "k-Trails: Recognition, Complexity, and Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of degree-constrained spanning hierarchies, also called k-trails,\nwas recently introduced in the context of network routing problems. They\ndescribe graphs that are homomorphic images of connected graphs of degree at\nmost k. First results highlight several interesting advantages of k-trails\ncompared to previous routing approaches. However, so far, only little is known\nregarding computational aspects of k-trails.\n  In this work we aim to fill this gap by presenting how k-trails can be\nanalyzed using techniques from algorithmic matroid theory. Exploiting this\nconnection, we resolve several open questions about k-trails. In particular, we\nshow that one can recognize efficiently whether a graph is a k-trail.\nFurthermore, we show that deciding whether a graph contains a k-trail is\nNP-complete; however, every graph that contains a k-trail is a (k+1)-trail.\nMoreover, further leveraging the connection to matroids, we consider the\nproblem of finding a minimum weight k-trail contained in a graph G. We show\nthat one can efficiently find a (2k-1)-trail contained in G whose weight is no\nmore than the cheapest k-trail contained in G, even when allowing negative\nweights.\n  The above results settle several open questions raised by Molnar, Newman, and\nSebo.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 12:09:52 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Singh", "Mohit", ""], ["Zenklusen", "Rico", ""]]}, {"id": "1512.01829", "submitter": "Marcin Pilipczuk", "authors": "Alina Ene and Matthias Mnich and Marcin Pilipczuk and Andrej Risteski", "title": "On Routing Disjoint Paths in Bounded Treewidth Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of routing on disjoint paths in bounded treewidth graphs\nwith both edge and node capacities. The input consists of a capacitated graph\n$G$ and a collection of $k$ source-destination pairs $\\mathcal{M} = \\{(s_1,\nt_1), \\dots, (s_k, t_k)\\}$. The goal is to maximize the number of pairs that\ncan be routed subject to the capacities in the graph. A routing of a subset\n$\\mathcal{M}'$ of the pairs is a collection $\\mathcal{P}$ of paths such that,\nfor each pair $(s_i, t_i) \\in \\mathcal{M}'$, there is a path in $\\mathcal{P}$\nconnecting $s_i$ to $t_i$. In the Maximum Edge Disjoint Paths (MaxEDP) problem,\nthe graph $G$ has capacities $\\mathrm{cap}(e)$ on the edges and a routing\n$\\mathcal{P}$ is feasible if each edge $e$ is in at most $\\mathrm{cap}(e)$ of\nthe paths of $\\mathcal{P}$. The Maximum Node Disjoint Paths (MaxNDP) problem is\nthe node-capacitated counterpart of MaxEDP.\n  In this paper we obtain an $O(r^3)$ approximation for MaxEDP on graphs of\ntreewidth at most $r$ and a matching approximation for MaxNDP on graphs of\npathwidth at most $r$. Our results build on and significantly improve the work\nby Chekuri et al. [ICALP 2013] who obtained an $O(r \\cdot 3^r)$ approximation\nfor MaxEDP.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 20:35:37 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Ene", "Alina", ""], ["Mnich", "Matthias", ""], ["Pilipczuk", "Marcin", ""], ["Risteski", "Andrej", ""]]}, {"id": "1512.01841", "submitter": "Evgeny Nikulchev", "authors": "Evgeny Nikulchev, Evgeniy Pluzhnik, Dmitry Biryukov, Oleg Lukyanchikov", "title": "Designing Applications in a Hybrid Cloud", "comments": "8 pages", "journal-ref": "Contemporary Engineering Sciences 8 (2015) 963-970", "doi": "10.12988/ces.2015.57214", "report-no": null, "categories": "cs.DC cs.DS cs.SE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Designing applications for hybrid cloud has many features, including dynamic\nvirtualization management and route switching. This makes it impossible to\nevaluate the query and hence the optimal distribution of data. In this paper,\nwe formulate the main challenges of designing and simulation, offer\ninstallation for processing.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 21:53:30 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Nikulchev", "Evgeny", ""], ["Pluzhnik", "Evgeniy", ""], ["Biryukov", "Dmitry", ""], ["Lukyanchikov", "Oleg", ""]]}, {"id": "1512.01876", "submitter": "Kyle Fox", "authors": "Pankaj K. Agarwal, Kyle Fox, Jiangwei Pan and Rex Ying", "title": "Approximating Dynamic Time Warping and Edit Distance for a Pair of Point\n  Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first subquadratic-time approximation schemes for dynamic time\nwarping (DTW) and edit distance (ED) of several natural families of point\nsequences in $\\mathbb{R}^d$, for any fixed $d \\ge 1$. In particular, our\nalgorithms compute $(1+\\varepsilon)$-approximations of DTW and ED in time\nnear-linear for point sequences drawn from k-packed or k-bounded curves, and\nsubquadratic for backbone sequences. Roughly speaking, a curve is\n$\\kappa$-packed if the length of its intersection with any ball of radius $r$\nis at most $\\kappa \\cdot r$, and a curve is $\\kappa$-bounded if the sub-curve\nbetween two curve points does not go too far from the two points compared to\nthe distance between the two points. In backbone sequences, consecutive points\nare spaced at approximately equal distances apart, and no two points lie very\nclose together. Recent results suggest that a subquadratic algorithm for DTW or\nED is unlikely for an arbitrary pair of point sequences even for $d=1$. Our\nalgorithms work by constructing a small set of rectangular regions that cover\nthe entries of the dynamic programming table commonly used for these distance\nmeasures. The weights of entries inside each rectangle are roughly the same, so\nwe are able to use efficient procedures to approximately compute the cheapest\npaths through these rectangles.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 01:54:19 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 21:40:51 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Agarwal", "Pankaj K.", ""], ["Fox", "Kyle", ""], ["Pan", "Jiangwei", ""], ["Ying", "Rex", ""]]}, {"id": "1512.01892", "submitter": "Rasmus J Kyng", "authors": "Rasmus Kyng, Yin Tat Lee, Richard Peng, Sushant Sachdeva, Daniel A.\n  Spielman", "title": "Sparsified Cholesky and Multigrid Solvers for Connection Laplacians", "comments": "This article supersedes arXiv:1506.08204", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the sparsified Cholesky and sparsified multigrid algorithms for\nsolving systems of linear equations. These algorithms accelerate Gaussian\nelimination by sparsifying the nonzero matrix entries created by the\nelimination process. We use these new algorithms to derive the first nearly\nlinear time algorithms for solving systems of equations in connection\nLaplacians, a generalization of Laplacian matrices that arise in many problems\nin image and signal processing. We also prove that every connection Laplacian\nhas a linear sized approximate inverse. This is an LU factorization with a\nlinear number of nonzero entries that is a strong approximation of the original\nmatrix. Using such a factorization one can solve systems of equations in a\nconnection Laplacian in linear time. Such a factorization was unknown even for\nordinary graph Laplacians.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 03:13:54 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Kyng", "Rasmus", ""], ["Lee", "Yin Tat", ""], ["Peng", "Richard", ""], ["Sachdeva", "Sushant", ""], ["Spielman", "Daniel A.", ""]]}, {"id": "1512.01996", "submitter": "Aleksander Cis{\\l}ak", "authors": "Szymon Grabowski, Aleksander Cis{\\l}ak", "title": "A bloated FM-index reducing the number of cache misses during the search", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FM-index is a well-known compressed full-text index, based on the\nBurrows-Wheeler transform (BWT). During a pattern search, the BWT sequence is\naccessed at \"random\" locations, which is cache-unfriendly. In this paper, we\nare interested in speeding up the FM-index by working on $q$-grams rather than\nindividual characters, at the cost of using more space. The first presented\nvariant is related to an inverted index on $q$-grams, yet the occurrence lists\nin our solution are in the sorted suffix order rather than text order in a\ntraditional inverted index. This variant obtains $O(m/|CL| + \\log n \\log m)$\ncache misses in the worst case, where $n$ and $m$ are the text and pattern\nlengths, respectively, and $|CL|$ is the CPU cache line size, in symbols\n(typically 64 in modern hardware). This index is often several times faster\nthan the fastest known FM-indexes (especially for long patterns), yet the space\nrequirements are enormous, $O(n\\log^2 n)$ bits in theory and about $80n$-$95n$\nbytes in practice. For this reason, we dub our approach FM-bloated. The second\npresented variant requires $O(n\\log n)$ bits of space.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 11:51:11 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Grabowski", "Szymon", ""], ["Cis\u0142ak", "Aleksander", ""]]}, {"id": "1512.02068", "submitter": "Shay Mozes", "authors": "Shay Mozes, Cyril Nikolaev, Yahav Nussbaum, Oren Weimann", "title": "Minimum Cut of Directed Planar Graphs in O(nloglogn) Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an $O(n \\log \\log n)$ time algorithm for computing the minimum cut\n(or equivalently, the shortest cycle) of a weighted directed planar graph. This\nimproves the previous fastest $O(n\\log^3 n)$ solution. Interestingly, while in\nundirected planar graphs both min-cut and min $st$-cut have $O(n \\log \\log n)$\nsolutions, in directed planar graphs our result makes min-cut faster than min\n$st$-cut, which currently requires $O(n \\log n)$.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 14:37:39 GMT"}, {"version": "v2", "created": "Sat, 12 Nov 2016 19:36:51 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Mozes", "Shay", ""], ["Nikolaev", "Cyril", ""], ["Nussbaum", "Yahav", ""], ["Weimann", "Oren", ""]]}, {"id": "1512.02157", "submitter": "Udit Agarwal", "authors": "Udit Agarwal and Vijaya Ramachandran", "title": "Finding $k$ Simple Shortest Paths and Cycles", "comments": "The current version includes new results for undirected graphs. In\n  Section 4, the notion of an (m,n) reduction is generalized to an f(m,n)\n  reduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding multiple simple shortest paths in a weighted directed\ngraph $G=(V,E)$ has many applications, and is considerably more difficult than\nthe corresponding problem when cycles are allowed in the paths. Even for a\nsingle source-sink pair, it is known that two simple shortest paths cannot be\nfound in time polynomially smaller than $n^3$ (where $n=|V|$) unless the\nAll-Pairs Shortest Paths problem can be solved in a similar time bound. The\nlatter is a well-known open problem in algorithm design. We consider the\nall-pairs version of the problem, and we give a new algorithm to find $k$\nsimple shortest paths for all pairs of vertices. For $k=2$, our algorithm runs\nin $O(mn + n^2 \\log n)$ time (where $m=|E|$), which is almost the same bound as\nfor the single pair case, and for $k=3$ we improve earlier bounds. Our approach\nis based on forming suitable path extensions to find simple shortest paths;\nthis method is different from the `detour finding' technique used in most of\nthe prior work on simple shortest paths, replacement paths, and distance\nsensitivity oracles.\n  Enumerating simple cycles is a well-studied classical problem. We present new\nalgorithms for generating simple cycles and simple paths in $G$ in\nnon-decreasing order of their weights; the algorithm for generating simple\npaths is much faster, and uses another variant of path extensions. We also give\nhardness results for sparse graphs, relative to the complexity of computing a\nminimum weight cycle in a graph, for several variants of problems related to\nfinding $k$ simple paths and cycles.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 18:33:17 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 03:46:59 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Agarwal", "Udit", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1512.02179", "submitter": "Kevin P. Costello", "authors": "Marek Chrobak, Kevin P. Costello", "title": "Faster Information Gathering in Ad-Hoc Radio Tree Networks", "comments": "Full version; extended abstract to appear in LATIN '16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study information gathering in ad-hoc radio networks. Initially, each node\nof the network has a piece of information called a rumor, and the overall\nobjective is to gather all these rumors in the designated target node. The\nad-hoc property refers to the fact that the topology of the network is unknown\nwhen the computation starts. Aggregation of rumors is not allowed, which means\nthat each node may transmit at most one rumor in one step.\n  We focus on networks with tree topologies, that is we assume that the network\nis a tree with all edges directed towards the root, but, being ad-hoc, its\nactual topology is not known. We provide two deterministic algorithms for this\nproblem. For the model that does not assume any collision detection nor\nacknowledgement mechanisms, we give an $O(n\\log\\log n)$-time algorithm,\nimproving the previous upper bound of $O(n\\log n)$. We also show that this\nrunning time can be further reduced to $O(n)$ if the model allows for\nacknowledgements of successful transmissions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 19:21:59 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Chrobak", "Marek", ""], ["Costello", "Kevin P.", ""]]}, {"id": "1512.02254", "submitter": "Viswanath Nagarajan", "authors": "Nikhil Bansal and Viswanath Nagarajan", "title": "Approximation-Friendly Discrepancy Rounding", "comments": "21 pages, 1 figure. The final version of this paper is due to be\n  published in the collection of papers \"A Journey through Discrete\n  Mathematics. A Tribute to Jiri Matousek\" edited by Martin Loebl, Jaroslav\n  Nesetril and Robin Thomas, due to be published by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rounding linear programs using techniques from discrepancy is a recent\napproach that has been very successful in certain settings. However this method\nalso has some limitations when compared to approaches such as randomized and\niterative rounding. We provide an extension of the discrepancy-based rounding\nalgorithm due to Lovett-Meka that (i) combines the advantages of both\nrandomized and iterated rounding, (ii) makes it applicable to settings with\nmore general combinatorial structure such as matroids. As applications of this\napproach, we obtain new results for various classical problems such as linear\nsystem rounding, degree-bounded matroid basis and low congestion routing.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 21:40:37 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 14:19:33 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Bansal", "Nikhil", ""], ["Nagarajan", "Viswanath", ""]]}, {"id": "1512.02337", "submitter": "David Steurer", "authors": "Samuel B. Hopkins, Tselil Schramm, Jonathan Shi, David Steurer", "title": "Fast spectral algorithms from sum-of-squares proofs: tensor\n  decomposition and planted sparse vectors", "comments": "62 pages, title changed, to appear at STOC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two problems that arise in machine learning applications: the\nproblem of recovering a planted sparse vector in a random linear subspace and\nthe problem of decomposing a random low-rank overcomplete 3-tensor. For both\nproblems, the best known guarantees are based on the sum-of-squares method. We\ndevelop new algorithms inspired by analyses of the sum-of-squares method. Our\nalgorithms achieve the same or similar guarantees as sum-of-squares for these\nproblems but the running time is significantly faster.\n  For the planted sparse vector problem, we give an algorithm with running time\nnearly linear in the input size that approximately recovers a planted sparse\nvector with up to constant relative sparsity in a random subspace of $\\mathbb\nR^n$ of dimension up to $\\tilde \\Omega(\\sqrt n)$. These recovery guarantees\nmatch the best known ones of Barak, Kelner, and Steurer (STOC 2014) up to\nlogarithmic factors.\n  For tensor decomposition, we give an algorithm with running time close to\nlinear in the input size (with exponent $\\approx 1.086$) that approximately\nrecovers a component of a random 3-tensor over $\\mathbb R^n$ of rank up to\n$\\tilde \\Omega(n^{4/3})$. The best previous algorithm for this problem due to\nGe and Ma (RANDOM 2015) works up to rank $\\tilde \\Omega(n^{3/2})$ but requires\nquasipolynomial time.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 05:49:07 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 18:01:12 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Hopkins", "Samuel B.", ""], ["Schramm", "Tselil", ""], ["Shi", "Jonathan", ""], ["Steurer", "David", ""]]}, {"id": "1512.02505", "submitter": "Michael Bekos", "authors": "Patrizio Angelini, Michael A. Bekos, Michael Kaufmann and Vincenzo\n  Roselli", "title": "Vertex-Coloring with Star-Defects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defective coloring is a variant of traditional vertex-coloring, according to\nwhich adjacent vertices are allowed to have the same color, as long as the\nmonochromatic components induced by the corresponding edges have a certain\nstructure. Due to its important applications, as for example in the\nbipartisation of graphs, this type of coloring has been extensively studied,\nmainly with respect to the size, degree, and acyclicity of the monochromatic\ncomponents.\n  In this paper we focus on defective colorings in which the monochromatic\ncomponents are acyclic and have small diameter, namely, they form stars. For\nouterplanar graphs, we give a linear-time algorithm to decide if such a\ndefective coloring exists with two colors and, in the positive case, to\nconstruct one. Also, we prove that an outerpath (i.e., an outerplanar graph\nwhose weak-dual is a path) always admits such a two-coloring. Finally, we\npresent NP-completeness results for non-planar and planar graphs of bounded\ndegree for the cases of two and three colors.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 15:21:20 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 12:58:40 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Angelini", "Patrizio", ""], ["Bekos", "Michael A.", ""], ["Kaufmann", "Michael", ""], ["Roselli", "Vincenzo", ""]]}, {"id": "1512.02510", "submitter": "Eva-Maria Hols", "authors": "Eva-Maria C. Hols and Stefan Kratsch", "title": "A randomized polynomial kernel for Subset Feedback Vertex Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Subset Feedback Vertex Set problem generalizes the classical Feedback\nVertex Set problem and asks, for a given undirected graph $G=(V,E)$, a set $S\n\\subseteq V$, and an integer $k$, whether there exists a set $X$ of at most $k$\nvertices such that no cycle in $G-X$ contains a vertex of $S$. It was\nindependently shown by Cygan et al. (ICALP '11, SIDMA '13) and Kawarabayashi\nand Kobayashi (JCTB '12) that Subset Feedback Vertex Set is fixed-parameter\ntractable for parameter $k$. Cygan et al. asked whether the problem also admits\na polynomial kernelization.\n  We answer the question of Cygan et al. positively by giving a randomized\npolynomial kernelization for the equivalent version where $S$ is a set of\nedges. In a first step we show that Edge Subset Feedback Vertex Set has a\nrandomized polynomial kernel parameterized by $|S|+k$ with $O(|S|^2k)$\nvertices. For this we use the matroid-based tools of Kratsch and Wahlstr\\\"om\n(FOCS '12) that for example were used to obtain a polynomial kernel for\n$s$-Multiway Cut. Next we present a preprocessing that reduces the given\ninstance $(G,S,k)$ to an equivalent instance $(G',S',k')$ where the size of\n$S'$ is bounded by $O(k^4)$. These two results lead to a polynomial kernel for\nSubset Feedback Vertex Set with $O(k^9)$ vertices.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 15:32:45 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Hols", "Eva-Maria C.", ""], ["Kratsch", "Stefan", ""]]}, {"id": "1512.02831", "submitter": "Fabian Gieseke", "authors": "Fabian Gieseke and Cosmin Eugen Oancea and Ashish Mahabal and\n  Christian Igel and Tom Heskes", "title": "Bigger Buffer k-d Trees on Multi-Many-Core Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A buffer k-d tree is a k-d tree variant for massively-parallel nearest\nneighbor search. While providing valuable speed-ups on modern many-core devices\nin case both a large number of reference and query points are given, buffer k-d\ntrees are limited by the amount of points that can fit on a single device. In\nthis work, we show how to modify the original data structure and the associated\nworkflow to make the overall approach capable of dealing with massive data\nsets. We further provide a simple yet efficient way of using multiple devices\ngiven in a single workstation. The applicability of the modified framework is\ndemonstrated in the context of astronomy, a field that is faced with huge\namounts of data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 12:28:12 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Gieseke", "Fabian", ""], ["Oancea", "Cosmin Eugen", ""], ["Mahabal", "Ashish", ""], ["Igel", "Christian", ""], ["Heskes", "Tom", ""]]}, {"id": "1512.02963", "submitter": "L\\'aszl\\'o Kozma", "authors": "L\\'aszl\\'o Kozma, Tobias M\\\"omke", "title": "Maximum Scatter TSP in Doubling Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding a tour of $n$ points in which every edge is\nlong. More precisely, we wish to find a tour that visits every point exactly\nonce, maximizing the length of the shortest edge in the tour. The problem is\nknown as Maximum Scatter TSP, and was introduced by Arkin et al. (SODA 1997),\nmotivated by applications in manufacturing and medical imaging. Arkin et al.\ngave a $0.5$-approximation for the metric version of the problem and showed\nthat this is the best possible ratio achievable in polynomial time (assuming $P\n\\neq NP$). Arkin et al. raised the question of whether a better approximation\nratio can be obtained in the Euclidean plane.\n  We answer this question in the affirmative in a more general setting, by\ngiving a $(1-\\epsilon)$-approximation algorithm for $d$-dimensional doubling\nmetrics, with running time $\\tilde{O}\\big(n^3 + 2^{O(K \\log K)}\\big)$, where $K\n\\leq \\left( \\frac{13}{\\epsilon} \\right)^d$. As a corollary we obtain (i) an\nefficient polynomial-time approximation scheme (EPTAS) for all constant\ndimensions $d$, (ii) a polynomial-time approximation scheme (PTAS) for\ndimension $d = \\log\\log{n}/c$, for a sufficiently large constant $c$, and (iii)\na PTAS for constant $d$ and $\\epsilon = \\Omega(1/\\log\\log{n})$. Furthermore, we\nshow the dependence on $d$ in our approximation scheme to be essentially\noptimal, unless Satisfiability can be solved in subexponential time.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 17:38:07 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 15:51:38 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Kozma", "L\u00e1szl\u00f3", ""], ["M\u00f6mke", "Tobias", ""]]}, {"id": "1512.02985", "submitter": "Sayan Bandyapadhyay", "authors": "Sayan Bandyapadhyay, Kasturi Varadarajan", "title": "On Variants of k-means Clustering", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\textit{Clustering problems} often arise in the fields like data mining,\nmachine learning etc. to group a collection of objects into similar groups with\nrespect to a similarity (or dissimilarity) measure. Among the clustering\nproblems, specifically \\textit{$k$-means} clustering has got much attention\nfrom the researchers. Despite the fact that $k$-means is a very well studied\nproblem its status in the plane is still an open problem. In particular, it is\nunknown whether it admits a PTAS in the plane. The best known approximation\nbound in polynomial time is $9+\\eps$.\n  In this paper, we consider the following variant of $k$-means. Given a set\n$C$ of points in $\\mathcal{R}^d$ and a real $f > 0$, find a finite set $F$ of\npoints in $\\mathcal{R}^d$ that minimizes the quantity $f*|F|+\\sum_{p\\in C}\n\\min_{q \\in F} {||p-q||}^2$. For any fixed dimension $d$, we design a local\nsearch PTAS for this problem. We also give a \"bi-criterion\" local search\nalgorithm for $k$-means which uses $(1+\\eps)k$ centers and yields a solution\nwhose cost is at most $(1+\\eps)$ times the cost of an optimal $k$-means\nsolution. The algorithm runs in polynomial time for any fixed dimension.\n  The contribution of this paper is two fold. On the one hand, we are being\nable to handle the square of distances in an elegant manner, which yields near\noptimal approximation bound. This leads us towards a better understanding of\nthe $k$-means problem. On the other hand, our analysis of local search might\nalso be useful for other geometric problems. This is important considering that\nvery little is known about the local search method for geometric approximation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 18:37:49 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Bandyapadhyay", "Sayan", ""], ["Varadarajan", "Kasturi", ""]]}, {"id": "1512.03022", "submitter": "Robert Elsaesser", "authors": "Chen Avin and Robert Els\\\"asser", "title": "Breaking the log n barrier on rumor spreading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $O(\\log n)$ rounds has been a well known upper bound for rumor spreading\nusing push&pull in the random phone call model (i.e., uniform gossip in the\ncomplete graph). A matching lower bound of $\\Omega(\\log n)$ is also known for\nthis special case. Under the assumption of this model and with a natural\naddition that nodes can call a partner once they learn its address (e.g., its\nIP address) we present a new distributed, address-oblivious and robust\nalgorithm that uses push&pull with pointer jumping to spread a rumor to all\nnodes in only $O(\\sqrt{\\log n})$ rounds, w.h.p. This algorithm can also cope\nwith $F= O(n/2^{\\sqrt{\\log n}})$ node failures, in which case all but $O(F)$\nnodes become informed within $O(\\sqrt{\\log n})$ rounds, w.h.p.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 16:56:19 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Avin", "Chen", ""], ["Els\u00e4sser", "Robert", ""]]}, {"id": "1512.03220", "submitter": "Riccardo Dondi", "authors": "Stefano Beretta, Mauro Castelli, Riccardo Dondi", "title": "Parameterized Tractability of the Maximum-Duo Preservation String\n  Mapping Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the parameterized complexity of the Maximum-Duo\nPreservation String Mapping Problem, the complementary of the Minimum Common\nString Partition Problem. We show that this problem is fixed-parameter\ntractable when parameterized by the number k of conserved duos, by first giving\na parameterized algorithm based on the color-coding technique and then\npresenting a reduction to a kernel of size O(k^6 ).\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 11:31:53 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Beretta", "Stefano", ""], ["Castelli", "Mauro", ""], ["Dondi", "Riccardo", ""]]}, {"id": "1512.03246", "submitter": "Clemens R\\\"osner", "authors": "Matthias Mnich, Heiko R\\\"oglin, Clemens R\\\"osner", "title": "New Deterministic Algorithms for Solving Parity Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study parity games in which one of the two players controls only a small\nnumber $k$ of nodes and the other player controls the $n-k$ other nodes of the\ngame. Our main result is a fixed-parameter algorithm that solves bipartite\nparity games in time $k^{O(\\sqrt{k})}\\cdot O(n^3)$, and general parity games in\ntime $(p+k)^{O(\\sqrt{k})} \\cdot O(pnm)$, where $p$ is the number of distinct\npriorities and $m$ is the number of edges. For all games with $k = o(n)$ this\nimproves the previously fastest algorithm by Jurdzi{\\'n}ski, Paterson, and\nZwick (SICOMP 2008). We also obtain novel kernelization results and an improved\ndeterministic algorithm for graphs with small average degree.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 13:21:30 GMT"}], "update_date": "2015-12-12", "authors_parsed": [["Mnich", "Matthias", ""], ["R\u00f6glin", "Heiko", ""], ["R\u00f6sner", "Clemens", ""]]}, {"id": "1512.03306", "submitter": "Amos Korman", "authors": "Amos Korman (GANG, LIAFA), Jean-S\\'ebastien Sereni (MASCOTTE), Laurent\n  Viennot (GANG, LIAFA, LINCS)", "title": "Toward more localized local algorithms: removing assumptions concerning\n  global knowledge", "comments": null, "journal-ref": "Distributed Computing, Springer Verlag, 2013, 26 (5-6)", "doi": "10.1007/s00446-012-0174-8", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous sophisticated local algorithm were suggested in the literature for\nvarious fundamental problems. Notable examples are the MIS and\n$(\\Delta+1)$-coloring algorithms by Barenboim and Elkin [6], by Kuhn [22], and\nby Panconesi and Srinivasan [34], as well as the $O(\\Delta 2)$-coloring\nalgorithm by Linial [28]. Unfortunately, most known local algorithms\n(including, in particular, the aforementioned algorithms) are non-uniform, that\nis, local algorithms generally use good estimations of one or more global\nparameters of the network, e.g., the maximum degree $\\Delta$ or the number of\nnodes n. This paper provides a method for transforming a non-uniform local\nalgorithm into a uniform one. Furthermore , the resulting algorithm enjoys the\nsame asymp-totic running time as the original non-uniform algorithm. Our method\napplies to a wide family of both deterministic and randomized algorithms.\nSpecifically, it applies to almost all state of the art non-uniform algorithms\nfor MIS and Maximal Matching, as well as to many results concerning the\ncoloring problem. (In particular, it applies to all aforementioned algorithms.)\nTo obtain our transformations we introduce a new distributed tool called\npruning algorithms, which we believe may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 16:14:59 GMT"}], "update_date": "2015-12-12", "authors_parsed": [["Korman", "Amos", "", "GANG, LIAFA"], ["Sereni", "Jean-S\u00e9bastien", "", "MASCOTTE"], ["Viennot", "Laurent", "", "GANG, LIAFA, LINCS"]]}, {"id": "1512.03501", "submitter": "Marian-Andrei Rizoiu", "authors": "Marian-Andrei Rizoiu, Julien Velcin, St\\'ephane Bonnevay, St\\'ephane\n  Lallich", "title": "ClusPath: A Temporal-driven Clustering to Infer Typical Evolution Paths", "comments": null, "journal-ref": null, "doi": "10.1007/s10618-015-0445-7", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ClusPath, a novel algorithm for detecting general evolution\ntendencies in a population of entities. We show how abstract notions, such as\nthe Swedish socio-economical model (in a political dataset) or the companies\nfiscal optimization (in an economical dataset) can be inferred from low-level\ndescriptive features. Such high-level regularities in the evolution of entities\nare detected by combining spatial and temporal features into a spatio-temporal\ndissimilarity measure and using semi-supervised clustering techniques. The\nrelations between the evolution phases are modeled using a graph structure,\ninferred simultaneously with the partition, by using a \"slow changing world\"\nassumption. The idea is to ensure a smooth passage for entities along their\nevolution paths, which catches the long-term trends in the dataset.\nAdditionally, we also provide a method, based on an evolutionary algorithm, to\ntune the parameters of ClusPath to new, unseen datasets. This method assesses\nthe fitness of a solution using four opposed quality measures and proposes a\nbalanced compromise.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 01:32:20 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Rizoiu", "Marian-Andrei", ""], ["Velcin", "Julien", ""], ["Bonnevay", "St\u00e9phane", ""], ["Lallich", "St\u00e9phane", ""]]}, {"id": "1512.03512", "submitter": "Srikrishnan Divakaran", "authors": "Srikrishnan Divakaran", "title": "A Fast Heuristic for Exact String Matching", "comments": "arXiv admin note: substantial text overlap with arXiv:1509.09228", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a pattern string $P$ of length $n$ consisting of $\\delta$ distinct\ncharacters and a query string $T$ of length $m$, where the characters of $P$\nand $T$ are drawn from an alphabet $\\Sigma$ of size $\\Delta$, the {\\em exact\nstring matching} problem consists of finding all occurrences of $P$ in $T$. For\nthis problem, we present a randomized heuristic that in $O(n\\delta)$ time\npreprocesses $P$ to identify $sparse(P)$, a rarely occurring substring of $P$,\nand then use it to find all occurrences of $P$ in $T$ efficiently. This\nheuristic has an expected search time of $O( \\frac{m}{min(|sparse(P)|,\n\\Delta)})$, where $|sparse(P)|$ is at least $\\delta$. We also show that for a\npattern string $P$ whose characters are chosen uniformly at random from an\nalphabet of size $\\Delta$, $E[|sparse(P)|]$ is $\\Omega(\\Delta log\n(\\frac{2\\Delta}{2\\Delta-\\delta}))$.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 03:39:45 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Divakaran", "Srikrishnan", ""]]}, {"id": "1512.03531", "submitter": "G\\'abor Ivanyos", "authors": "G\\'abor Ivanyos, Youming Qiao, K. V. Subrahmanyam", "title": "Constructive noncommutative rank computation is in deterministic\n  polynomial time", "comments": "20 pages, accepted version (in Computational Complexity)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.AC math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend our techniques developed in our earlier paper appeared in\nComputational Complexity, 2017 (preprint: arXiv:1508.00690) to obtain a\ndeterministic polynomial time algorithm for computing the non-commutative rank\ntogether with certificates of linear spaces of matrices over sufficiently large\nbase fields.\n  The key new idea is a reduction procedure that keeps the blow-up parameter\nsmall, and there are two methods to implement this idea: the first one is a\ngreedy argument that removes certain rows and columns, and the second one is an\nefficient algorithmic version of a result of Derksen and Makam. Both methods\nrely crucially on the regularity lemma in our aforementioned paper, and in this\nmanuscript we also improve that lemma by removing a coprime condition there.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 05:47:56 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2015 03:37:18 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 23:05:31 GMT"}, {"version": "v4", "created": "Fri, 17 Jun 2016 09:32:14 GMT"}, {"version": "v5", "created": "Thu, 1 Feb 2018 14:07:03 GMT"}, {"version": "v6", "created": "Mon, 5 Feb 2018 17:00:56 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Ivanyos", "G\u00e1bor", ""], ["Qiao", "Youming", ""], ["Subrahmanyam", "K. V.", ""]]}, {"id": "1512.03532", "submitter": "Matthew Roughan", "authors": "Eric Parsonage and Matthew Roughan", "title": "Fast Generation of Spatially Embedded Random Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially Embedded Random Networks such as the Waxman random graph have been\nused in a variety of settings for synthesizing networks. However, little\nthought has been put into fast generation of these networks. Existing\ntechniques are $O(n^2)$ where $n$ is the number of nodes in the graph. In this\npaper we present an $O(n + e)$ algorithm, where $e$ is the number of edges.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 05:53:00 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Parsonage", "Eric", ""], ["Roughan", "Matthew", ""]]}, {"id": "1512.03543", "submitter": "Yu Cheng", "authors": "Umang Bhaskar, Yu Cheng, Young Kun Ko and Chaitanya Swamy", "title": "Hardness Results for Signaling in Bayesian Zero-Sum and Network Routing\n  Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the optimization problem faced by a perfectly informed principal in\na Bayesian game, who reveals information to the players about the state of\nnature to obtain a desirable equilibrium. This signaling problem is the natural\ndesign question motivated by uncertainty in games and has attracted much recent\nattention. We present new hardness results for signaling problems in (a)\nBayesian two-player zero-sum games, and (b) Bayesian network routing games.\n  For Bayesian zero-sum games, when the principal seeks to maximize the\nequilibrium utility of a player, we show that it is NP-hard to obtain an\nadditive FPTAS. Our hardness proof exploits duality and the equivalence of\nseparation and optimization in a novel way. Further, we rule out an additive\nPTAS assuming planted clique hardness, which states that no polynomial time\nalgorithm can recover a planted clique from an Erd\\H{o}s-R\\'enyi random graph.\nComplementing these, we obtain a PTAS for a structured class of zero-sum games\n(where obtaining an FPTAS is still NP-hard) when the payoff matrices obey a\nLipschitz condition. Previous results ruled out an FPTAS assuming\nplanted-clique hardness, and a PTAS only for implicit games with\nquasi-polynomial-size strategy sets.\n  For Bayesian network routing games, wherein the principal seeks to minimize\nthe average latency of the Nash flow, we show that it is NP-hard to obtain a\n(multiplicative) $(4/3 - \\epsilon)$-approximation, even for linear latency\nfunctions. This is the optimal inapproximability result for linear latencies,\nsince we show that full revelation achieves a $(4/3)$-approximation for linear\nlatencies.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 07:46:04 GMT"}, {"version": "v2", "created": "Sun, 30 Oct 2016 05:40:40 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Bhaskar", "Umang", ""], ["Cheng", "Yu", ""], ["Ko", "Young Kun", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "1512.03547", "submitter": "Laszlo Babai", "authors": "L\\'aszl\\'o Babai", "title": "Graph Isomorphism in Quasipolynomial Time", "comments": "89 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO math.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that the Graph Isomorphism (GI) problem and the related problems of\nString Isomorphism (under group action) (SI) and Coset Intersection (CI) can be\nsolved in quasipolynomial ($\\exp((\\log n)^{O(1)})$) time. The best previous\nbound for GI was $\\exp(O(\\sqrt{n\\log n}))$, where $n$ is the number of vertices\n(Luks, 1983); for the other two problems, the bound was similar,\n$\\exp(\\tilde{O}(\\sqrt{n}))$, where $n$ is the size of the permutation domain\n(Babai, 1983).\n  The algorithm builds on Luks's SI framework and attacks the barrier\nconfigurations for Luks's algorithm by group theoretic \"local certificates\" and\ncombinatorial canonical partitioning techniques. We show that in a well-defined\nsense, Johnson graphs are the only obstructions to effective canonical\npartitioning.\n  Luks's barrier situation is characterized by a homomorphism {\\phi} that maps\na given permutation group $G$ onto $S_k$ or $A_k$, the symmetric or alternating\ngroup of degree $k$, where $k$ is not too small. We say that an element $x$ in\nthe permutation domain on which $G$ acts is affected by {\\phi} if the\n{\\phi}-image of the stabilizer of $x$ does not contain $A_k$. The\naffected/unaffected dichotomy underlies the core \"local certificates\" routine\nand is the central divide-and-conquer tool of the algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 08:04:26 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 08:55:28 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Babai", "L\u00e1szl\u00f3", ""]]}, {"id": "1512.03697", "submitter": "Sean Skwerer", "authors": "Sean Skwerer and Heping Zhang", "title": "Computing Affine Combinations, Distances, and Correlations for Recursive\n  Partition Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive partitioning is the core of several statistical methods including\nCART, random forest, and boosted trees. Despite the popularity of tree based\nmethods, to date, there did not exist methods for combining multiple trees into\na single tree, or methods for systematically quantifying the discrepancy\nbetween two trees. Taking advantage of the recursive structure in trees we\nformulated fast algorithms for computing affine combinations, distances and\ncorrelations in a vector subspace of recursive partition functions.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 16:42:38 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 16:22:43 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 16:17:38 GMT"}, {"version": "v4", "created": "Thu, 17 Mar 2016 18:51:09 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Skwerer", "Sean", ""], ["Zhang", "Heping", ""]]}, {"id": "1512.04010", "submitter": "Mohammad Khabbaz", "authors": "Mohammad Khabbaz", "title": "Finding HeavyPaths in Weighted Graphs and a Case-Study on Community\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A heavy path in a weighted graph represents a notion of connectivity and\nordering that goes beyond two nodes. The heaviest path of length l in the\ngraph, simply means a sequence of nodes with edges between them, such that the\nsum of edge weights is maximum among all paths of length l. It is trivial to\nstate the heaviest edge in the graph is the heaviest path of length 1, that\nrepresents a heavy connection between (any) two existing nodes. This can be\ngeneralized in many different ways for more than two nodes, one of which is\nfinding the heavy weight paths in the graph. In an influence network, this\nrepresents a highway for spreading information from a node to one of its\nindirect neighbors at distance l. Moreover, a heavy path implies an ordering of\nnodes. For instance, we can discover which ordering of songs (tourist spots) on\na playlist (travel itinerary) is more pleasant to a user or a group of users\nwho enjoy all songs (tourist spots) on the playlist (itinerary). This can also\nserve as a hard optimization problem, maximizing different types of quantities\nof a path such as score, flow, probability or surprise, defined as edge weight.\nTherefore, if one can solve the Heavy Path Problem (HPP) efficiently, they can\nas well use HPP for modeling and reduce other complex problems to it.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 06:26:57 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Khabbaz", "Mohammad", ""]]}, {"id": "1512.04047", "submitter": "Ren\\'e van Bevern", "authors": "Ren\\'e van Bevern, Vincent Froese, Christian Komusiewicz", "title": "Parameterizing edge modification problems above lower bounds", "comments": "Version accepted to Theory of Computing Systems, CSR'16 special issue", "journal-ref": "Theory of Computing Systems 62(3):739-770, 2018", "doi": "10.1007/s00224-016-9746-5", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the parameterized complexity of a variant of the $F$-free Editing\nproblem: Given a graph $G$ and a natural number $k$, is it possible to modify\nat most $k$ edges in $G$ so that the resulting graph contains no induced\nsubgraph isomorphic to $F$? In our variant, the input additionally contains a\nvertex-disjoint packing $\\mathcal{H}$ of induced subgraphs of $G$, which\nprovides a lower bound $h(\\mathcal{H})$ on the number of edge modifications\nrequired to transform $G$ into an $F$-free graph. While earlier works used the\nnumber $k$ as parameter or structural parameters of the input graph $G$, we\nconsider instead the parameter $\\ell:=k-h(\\mathcal{H})$, that is, the number of\nedge modifications above the lower bound $h(\\mathcal{H})$. We develop a\nframework of generic data reduction rules to show fixed-parameter tractability\nwith respect to $\\ell$ for $K_3$-Free Editing, Feedback Arc Set in Tournaments,\nand Cluster Editing when the packing $\\mathcal{H}$ contains subgraphs with\nbounded solution size. For $K_3$-Free Editing, we also prove NP-hardness in\ncase of edge-disjoint packings of $K_3$s and $\\ell=0$, while for $K_q$-Free\nEditing and $q\\ge 6$, NP-hardness for $\\ell=0$ even holds for vertex-disjoint\npackings of $K_q$s. In addition, we provide NP-hardness results for $F$-free\nVertex Deletion, were the aim is to delete a minimum number of vertices to make\nthe input graph $F$-free.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 12:44:10 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2016 03:03:02 GMT"}, {"version": "v3", "created": "Fri, 23 Dec 2016 03:27:04 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["van Bevern", "Ren\u00e9", ""], ["Froese", "Vincent", ""], ["Komusiewicz", "Christian", ""]]}, {"id": "1512.04138", "submitter": "Noah Stephens-Davidowitz", "authors": "Noah Stephens-Davidowitz", "title": "Search-to-Decision Reductions for Lattice Problems with Approximation\n  Factors (Slightly) Greater Than One", "comments": "Updated to acknowledge additional prior work", "journal-ref": "APPROX 2016", "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show the first dimension-preserving search-to-decision reductions for\napproximate SVP and CVP. In particular, for any $\\gamma \\leq 1 + O(\\log n/n)$,\nwe obtain an efficient dimension-preserving reduction from $\\gamma^{O(n/\\log\nn)}$-SVP to $\\gamma$-GapSVP and an efficient dimension-preserving reduction\nfrom $\\gamma^{O(n)}$-CVP to $\\gamma$-GapCVP. These results generalize the known\nequivalences of the search and decision versions of these problems in the exact\ncase when $\\gamma = 1$. For SVP, we actually obtain something slightly stronger\nthan a search-to-decision reduction---we reduce $\\gamma^{O(n/\\log n)}$-SVP to\n$\\gamma$-unique SVP, a potentially easier problem than $\\gamma$-GapSVP.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 23:41:21 GMT"}, {"version": "v2", "created": "Sun, 13 Mar 2016 21:54:07 GMT"}, {"version": "v3", "created": "Wed, 20 Apr 2016 02:35:23 GMT"}, {"version": "v4", "created": "Sun, 3 Jul 2016 19:51:23 GMT"}, {"version": "v5", "created": "Mon, 24 Apr 2017 00:58:56 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Stephens-Davidowitz", "Noah", ""]]}, {"id": "1512.04170", "submitter": "Rakesh Venkat", "authors": "Amit Deshpande, Prahladh Harsha, Rakesh Venkat", "title": "Embedding approximately low-dimensional $\\ell_2^2$ metrics into $\\ell_1$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goemans showed that any $n$ points $x_1, \\dotsc x_n$ in $d$-dimensions\nsatisfying $\\ell_2^2$ triangle inequalities can be embedded into $\\ell_{1}$,\nwith worst-case distortion at most $\\sqrt{d}$. We extend this to the case when\nthe points are approximately low-dimensional, albeit with average distortion\nguarantees. More precisely, we give an $\\ell_{2}^{2}$-to-$\\ell_{1}$ embedding\nwith average distortion at most the stable rank, $\\mathrm{sr}(M)$, of the\nmatrix $M$ consisting of columns $\\{x_i-x_j\\}_{i<j}$. Average distortion\nembedding suffices for applications such as the Sparsest Cut problem. Our\nembedding gives an approximation algorithm for the \\sparsestcut problem on low\nthreshold-rank graphs, where earlier work was inspired by Lasserre SDP\nhierarchy, and improves on a previous result of the first and third author\n[Deshpande and Venkat, In Proc. 17th APPROX, 2014]. Our ideas give a new\nperspective on $\\ell_{2}^{2}$ metric, an alternate proof of Goemans' theorem,\nand a simpler proof for average distortion $\\sqrt{d}$. Furthermore, while the\nseminal result of Arora, Rao and Vazirani giving a $O(\\sqrt{\\log n})$ guarantee\nfor Uniform Sparsest Cut can be seen to imply Goemans' theorem with average\ndistortion, our work opens up the possibility of proving such a result directly\nvia a Goemans'-like theorem.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 04:54:08 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Deshpande", "Amit", ""], ["Harsha", "Prahladh", ""], ["Venkat", "Rakesh", ""]]}, {"id": "1512.04188", "submitter": "Rakesh Venkat", "authors": "Jaikumar Radhakrishnan, Saswata Shannigrahi, Rakesh Venkat", "title": "Hypergraph Two-Coloring in the Streaming Model", "comments": "Changes in the introduction and section on randomized algorithms to\n  make the exposition clearer. Main technical results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider space-efficient algorithms for two-coloring $n$-uniform\nhypergraphs $H=(V,E)$ in the streaming model, when the hyperedges arrive one at\na time. It is known that any such hypergraph with at most $0.7\n\\sqrt{\\frac{n}{\\ln n}} 2^n$ hyperedges has a two-coloring [Radhakrishnan &\nSrinivasan, RSA, 2000], which can be found deterministically in polynomial\ntime, if allowed full access to the input.\n  1. Let $s^D(v, q, n)$ be the minimum space used by a deterministic one-pass\nstreaming algorithm that on receiving an $n$-uniform hypergraph $H$ on $v$\nvertices and $q$ hyperedges produces a proper two-coloring of $H$. We show that\n$s^D(n^2, q, n) = \\Omega(q/n)$ when $q \\leq 0.7 \\sqrt{\\frac{n}{\\ln n}} 2^n$,\nand $s^D(n^2, q, n) = \\Omega(\\sqrt{\\frac{1}{n\\ln n}} 2^n)$ otherwise.\n  2. Let $s^R(v, q,n)$ be the minimum space used by a randomized one-pass\nstreaming algorithm that on receiving an $n$-uniform hypergraph $H$ on $v$\nvertices and $q$ hyperedges with high probability produces a proper\ntwo-coloring of $H$ (or declares failure). We show that $s^R(v,\n\\frac{1}{10}\\sqrt{\\frac{n}{\\ln n}} 2^n, n) = O(v \\log v)$ by giving an\nefficient randomized streaming algorithm.\n  The above results are inspired by the study of the number $q(n)$, the minimum\npossible number of hyperedges in a $n$-uniform hypergraph that is not\ntwo-colorable. It is known that $q(n) = \\Omega(\\sqrt{\\frac{n}{\\ln n}})$\n[Radhakrishnan-Srinivasan] and $ q(n)= O(n^2 2^n)$ [Erdos, 1963]. Our first\nresult shows that no space-efficient deterministic streaming algorithm can\nmatch the performance of the offline algorithm of Radhakrishnan and Srinivasan;\nthe second result shows that there is, however, a space-efficient randomized\nstreaming algorithm for the task.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 06:22:04 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 20:18:38 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Radhakrishnan", "Jaikumar", ""], ["Shannigrahi", "Saswata", ""], ["Venkat", "Rakesh", ""]]}, {"id": "1512.04200", "submitter": "Sudeshna Kolay", "authors": "Sudeshna Kolay and Fahad Panolan and Venkatesh Raman and Saket Saurabh", "title": "Parameterized Algorithms on Perfect Graphs for deletion to\n  $(r,\\ell)$-graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For fixed integers $r,\\ell \\geq 0$, a graph $G$ is called an {\\em\n$(r,\\ell)$-graph} if the vertex set $V(G)$ can be partitioned into $r$\nindependent sets and $\\ell$ cliques. The class of $(r, \\ell)$ graphs\ngeneralizes $r$-colourable graphs (when $\\ell =0)$ and hence not surprisingly,\ndetermining whether a given graph is an $(r, \\ell)$-graph is \\NP-hard even when\n$r \\geq 3$ or $\\ell \\geq 3$ in general graphs.\n  When $r$ and $\\ell$ are part of the input, then the recognition problem is\nNP-hard even if the input graph is a perfect graph (where the {\\sc Chromatic\nNumber} problem is solvable in polynomial time). It is also known to be\nfixed-parameter tractable (FPT) on perfect graphs when parameterized by $r$ and\n$\\ell$. I.e. there is an $f(r+\\ell) \\cdot n^{\\Oh(1)}$ algorithm on perfect\ngraphs on $n$ vertices where $f$ is some (exponential) function of $r$ and\n$\\ell$.\n  In this paper, we consider the parameterized complexity of the following\nproblem, which we call {\\sc Vertex Partization}. Given a perfect graph $G$ and\npositive integers $r,\\ell,k$ decide whether there exists a set $S\\subseteq\nV(G)$ of size at most $k$ such that the deletion of $S$ from $G$ results in an\n$(r,\\ell)$-graph. We obtain the following results: \\begin{enumerate} \\item {\\sc\nVertex Partization} on perfect graphs is FPT when parameterized by $k+r+\\ell$.\n\\item The problem does not admit any polynomial sized kernel when parameterized\nby $k+r+\\ell$. In other words, in polynomial time, the input graph can not be\ncompressed to an equivalent instance of size polynomial in $k+r+\\ell$. In fact,\nour result holds even when $k=0$.\n  \\item When $r,\\ell$ are universal constants, then {\\sc Vertex Partization} on\nperfect graphs, parameterized by $k$, has a polynomial sized kernel.\n\\end{enumerate}\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 07:05:04 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Kolay", "Sudeshna", ""], ["Panolan", "Fahad", ""], ["Raman", "Venkatesh", ""], ["Saurabh", "Saket", ""]]}, {"id": "1512.04303", "submitter": "Marcio Oshiro", "authors": "Cristina G. Fernandes and Marcio T. I. Oshiro", "title": "Kinetic Clustering of Points on the Line", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of clustering a set of points moving on the line consists of the\nfollowing: given positive integers n and k, the initial position and the\nvelocity of n points, find an optimal k-clustering of the points. We consider\ntwo classical quality measures for the clustering: minimizing the sum of the\nclusters diameters and minimizing the maximum diameter of a cluster. For the\nformer, we present polynomial-time algorithms under some assumptions and, for\nthe latter, a (2.71 + epsilon)-approximation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 13:28:47 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Fernandes", "Cristina G.", ""], ["Oshiro", "Marcio T. I.", ""]]}, {"id": "1512.04433", "submitter": "Samet Oymak", "authors": "Samet Oymak, Ben Recht", "title": "Near-Optimal Bounds for Binary Embeddings of Arbitrary Sets", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study embedding a subset $K$ of the unit sphere to the Hamming cube\n$\\{-1,+1\\}^m$. We characterize the tradeoff between distortion and sample\ncomplexity $m$ in terms of the Gaussian width $\\omega(K)$ of the set. For\nsubspaces and several structured sets we show that Gaussian maps provide the\noptimal tradeoff $m\\sim \\delta^{-2}\\omega^2(K)$, in particular for $\\delta$\ndistortion one needs $m\\approx\\delta^{-2}{d}$ where $d$ is the subspace\ndimension. For general sets, we provide sharp characterizations which reduces\nto $m\\approx{\\delta^{-4}}{\\omega^2(K)}$ after simplification. We provide\nimproved results for local embedding of points that are in close proximity of\neach other which is related to locality sensitive hashing. We also discuss\nfaster binary embedding where one takes advantage of an initial sketching\nprocedure based on Fast Johnson-Lindenstauss Transform. Finally, we list\nseveral numerical observations and discuss open problems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 17:57:41 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Oymak", "Samet", ""], ["Recht", "Ben", ""]]}, {"id": "1512.04515", "submitter": "Tsvi Kopelowitz", "authors": "Tsvi Kopelowitz and Ely Porat", "title": "Breaking the Variance: Approximating the Hamming Distance in $\\tilde\n  O(1/\\epsilon)$ Time Per Alignment", "comments": "Appeared in FOCS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The algorithmic tasks of computing the Hamming distance between a given\npattern of length $m$ and each location in a text of length $n$ is one of the\nmost fundamental algorithmic tasks in string algorithms. Unfortunately, there\nis evidence that for a text $T$ of size $n$ and a pattern $P$ of size $m$, one\ncannot compute the exact Hamming distance for all locations in $T$ in time\nwhich is less than $\\tilde O(n\\sqrt m)$. However, Karloff~\\cite{karloff} showed\nthat if one is willing to suffer a $1\\pm\\epsilon$ approximation, then it is\npossible to solve the problem with high probability, in $\\tilde O(\\frac n\n{\\epsilon^2})$ time.\n  Due to related lower bounds for computing the Hamming distance of two strings\nin the one-way communication complexity model, it is strongly believed that\nobtaining an algorithm for solving the approximation version cannot be done\nmuch faster as a function of $\\frac 1 \\epsilon$. We show here that this belief\nis false by introducing a new $\\tilde O(\\frac{n}{\\epsilon})$ time algorithm\nthat succeeds with high probability.\n  The main idea behind our algorithm, which is common in sparse recovery\nproblems, is to reduce the variance of a specific randomized experiment by\n(approximately) separating heavy hitters from non-heavy hitters. However, while\nknown sparse recovery techniques work very well on vectors, they do not seem to\napply here, where we are dealing with mismatches between pairs of characters.\nWe introduce two main algorithmic ingredients. The first is a new sparse\nrecovery method that applies for pair inputs (such as in our setting). The\nsecond is a new construction of hash/projection functions, which allows to\ncount the number of projections that induce mismatches between two characters\nexponentially faster than brute force. We expect that these algorithmic\ntechniques will be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 20:53:15 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Kopelowitz", "Tsvi", ""], ["Porat", "Ely", ""]]}, {"id": "1512.04633", "submitter": "Peter Lofgren", "authors": "Peter Lofgren", "title": "Efficient Algorithms for Personalized PageRank", "comments": "PhD Thesis (Stanford Computer Science). Based on joint work with Sid\n  Banerjee and Ashish Goel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new, more efficient algorithms for estimating random walk scores\nsuch as Personalized PageRank from a given source node to one or several target\nnodes. These scores are useful for personalized search and recommendations on\nnetworks including social networks, user-item networks, and the web. Past work\nhas proposed using Monte Carlo or using linear algebra to estimate scores from\na single source to every target, making them inefficient for a single pair. Our\ncontribution is a new bidirectional algorithm which combines linear algebra and\nMonte Carlo to achieve significant speed improvements. On a diverse set of six\ngraphs, our algorithm is 70x faster than past state-of-the-art algorithms. We\nalso present theoretical analysis: while past algorithms require $\\Omega(n)$\ntime to estimate a random walk score of typical size $\\frac{1}{n}$ on an\n$n$-node graph to a given constant accuracy, our algorithm requires only\n$O(\\sqrt{m})$ expected time for an average target, where $m$ is the number of\nedges, and is provably accurate.\n  In addition to our core bidirectional estimator for personalized PageRank, we\npresent an alternative algorithm for undirected graphs, a generalization to\narbitrary walk lengths and Markov Chains, an algorithm for personalized search\nranking, and an algorithm for sampling random paths from a given source to a\ngiven set of targets. We expect our bidirectional methods can be extended in\nother ways and will be useful subroutines in other graph analysis problems.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 02:54:26 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Lofgren", "Peter", ""]]}, {"id": "1512.04719", "submitter": "Carsten Fischer", "authors": "Carsten Fischer and Heiko R\\\"oglin", "title": "Probabilistic Analysis of the Dual Next-Fit Algorithm for Bin Covering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the bin covering problem, the goal is to fill as many bins as possible up\nto a certain minimal level with a given set of items of different sizes. Online\nvariants, in which the items arrive one after another and have to be packed\nimmediately on their arrival without knowledge about the future items, have\nbeen studied extensively in the literature. We study the simplest possible\nonline algorithm Dual Next-Fit, which packs all arriving items into the same\nbin until it is filled and then proceeds with the next bin in the same manner.\nThe competitive ratio of this and any other reasonable online algorithm is\n$1/2$.\n  We study Dual Next-Fit in a probabilistic setting where the item sizes are\nchosen i.i.d.\\ according to a discrete distribution and we prove that, for\nevery distribution, its expected competitive ratio is at least $1/2+\\epsilon$\nfor a constant $\\epsilon>0$ independent of the distribution. We also prove an\nupper bound of $2/3$ and better lower bounds for certain restricted classes of\ndistributions. Finally, we prove that the expected competitive ratio equals,\nfor a large class of distributions, the random-order ratio, which is the\nexpected competitive ratio when adversarially chosen items arrive in uniformly\nrandom order.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 10:50:54 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Fischer", "Carsten", ""], ["R\u00f6glin", "Heiko", ""]]}, {"id": "1512.04848", "submitter": "Travis Dick", "authors": "Travis Dick, Mu Li, Venkata Krishna Pillutla, Colin White, Maria\n  Florina Balcan, Alex Smola", "title": "Data Driven Resource Allocation for Distributed Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed machine learning, data is dispatched to multiple machines for\nprocessing. Motivated by the fact that similar data points often belong to the\nsame or similar classes, and more generally, classification rules of high\naccuracy tend to be \"locally simple but globally complex\" (Vapnik & Bottou\n1993), we propose data dependent dispatching that takes advantage of such\nstructure. We present an in-depth analysis of this model, providing new\nalgorithms with provable worst-case guarantees, analysis proving existing\nscalable heuristics perform well in natural non worst-case conditions, and\ntechniques for extending a dispatching rule from a small sample to the entire\ndistribution. We overcome novel technical challenges to satisfy important\nconditions for accurate distributed learning, including fault tolerance and\nbalancedness. We empirically compare our approach with baselines based on\nrandom partitioning, balanced partition trees, and locality sensitive hashing,\nshowing that we achieve significantly higher accuracy on both synthetic and\nreal world image and advertising datasets. We also demonstrate that our\ntechnique strongly scales with the available computing power.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 16:41:42 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 20:45:52 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Dick", "Travis", ""], ["Li", "Mu", ""], ["Pillutla", "Venkata Krishna", ""], ["White", "Colin", ""], ["Balcan", "Maria Florina", ""], ["Smola", "Alex", ""]]}, {"id": "1512.04866", "submitter": "Michael Bekos", "authors": "Michael A. Bekos, Michael Kaufmann and Robert Krug", "title": "On the Total Number of Bends for Planar Octilinear Drawings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An octilinear drawing of a planar graph is one in which each edge is drawn as\na sequence of horizontal, vertical and diagonal at 45 degrees line-segments.\nFor such drawings to be readable, special care is needed in order to keep the\nnumber of bends small. As the problem of finding planar octilinear drawings of\nminimum number of bends is NP-hard, in this paper we focus on upper and lower\nbounds. From a recent result of Keszegh et al. on the slope number of planar\ngraphs, we can derive an upper bound of 4n-10 bends for 8-planar graphs with n\nvertices. We considerably improve this general bound and corresponding previous\nones for triconnected 4-, 5- and 6-planar graphs. We also derive non-trivial\nlower bounds for these three classes of graphs by a technique inspired by the\nnetwork flow formulation of Tamassia.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 17:23:50 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Bekos", "Michael A.", ""], ["Kaufmann", "Michael", ""], ["Krug", "Robert", ""]]}, {"id": "1512.05028", "submitter": "Djamal Belazzougui", "authors": "Djamal Belazzougui", "title": "Optimal Las Vegas reduction from one-way set reconciliation to error\n  correction", "comments": "14 pages. Under submission to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we have two players $A$ and $C$, where player $A$ has a string\n$s[0..u-1]$ and player $C$ has a string $t[0..u-1]$ and none of the two players\nknows the other's string. Assume that $s$ and $t$ are both over an integer\nalphabet $[\\sigma]$, where the first string contains $n$ non-zero entries. We\nwould wish to answer to the following basic question. Assuming that $s$ and $t$\ndiffer in at most $k$ positions, how many bits does player $A$ need to send to\nplayer $C$ so that he can recover $s$ with certainty? Further, how much time\ndoes player $A$ need to spend to compute the sent bits and how much time does\nplayer $C$ need to recover the string $s$?\n  This problem has a certain number of applications, for example in databases,\nwhere each of the two parties possesses a set of $n$ key-value pairs, where\nkeys are from the universe $[u]$ and values are from $[\\sigma]$ and usually\n$n\\ll u$. In this paper, we show a time and message-size optimal Las Vegas\nreduction from this problem to the problem of systematic error correction of\n$k$ errors for strings of length $\\Theta(n)$ over an alphabet of size\n$2^{\\Theta(\\log\\sigma+\\log (u/n))}$. The additional running time incurred by\nthe reduction is linear randomized for player $A$ and linear deterministic for\nplayer $B$, but the correction works with certainty. When using the popular\nReed-Solomon codes, the reduction gives a protocol that transmits $O(k(\\log\nu+\\log\\sigma))$ bits and runs in time $O(n\\cdot\\mathrm{polylog}(n)(\\log\nu+\\log\\sigma))$ for all values of $k$. The time is randomized for player $A$\n(encoding time) and deterministic for player $C$ (decoding time). The space is\noptimal whenever $k\\leq (u\\sigma)^{1-\\Omega(1)}$.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 02:14:51 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Belazzougui", "Djamal", ""]]}, {"id": "1512.05059", "submitter": "Mina Ghashami", "authors": "Mina Ghashami, Daniel Perry, Jeff M. Phillips", "title": "Streaming Kernel Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel principal component analysis (KPCA) provides a concise set of basis\nvectors which capture non-linear structures within large data sets, and is a\ncentral tool in data analysis and learning. To allow for non-linear relations,\ntypically a full $n \\times n$ kernel matrix is constructed over $n$ data\npoints, but this requires too much space and time for large values of $n$.\nTechniques such as the Nystr\\\"om method and random feature maps can help\ntowards this goal, but they do not explicitly maintain the basis vectors in a\nstream and take more space than desired. We propose a new approach for\nstreaming KPCA which maintains a small set of basis elements in a stream,\nrequiring space only logarithmic in $n$, and also improves the dependence on\nthe error parameter. Our technique combines together random feature maps with\nrecent advances in matrix sketching, it has guaranteed spectral norm error\nbounds with respect to the original kernel matrix, and it compares favorably in\npractice to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 06:12:55 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Ghashami", "Mina", ""], ["Perry", "Daniel", ""], ["Phillips", "Jeff M.", ""]]}, {"id": "1512.05152", "submitter": "Salman Parsa", "authors": "Salman Parsa", "title": "Small Model $2$-Complexes in $4$-space and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider computational complexity of problems related to the fundamental\ngroup and the first homology group of (embeddable) $2$-complexes. We show, as\nan extension of an earlier work, that computing first homology of $2$-complexes\nis equivalent in computational complexity to matrix diagonalization. That is,\nthe usual procedures for computing homology cannot be improved other than by\nmatrix methods. This is true even if the complex is in the euclidean $4$-space.\nFor this purpose, we use $2$-complexes built in a standard way from group\npresentations, called model $2$-complexes. Model complexes have fundamental\ngroup isomorphic with the group defined by the presentation. We show that there\nare model complexes of size in the order of the bit-complexity of the\npresentation that can be realized linearly in $4$-space. We further derive some\napplications of this result regarding embeddability problems in the euclidean\n$4$-space.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 12:48:59 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 11:37:16 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Parsa", "Salman", ""]]}, {"id": "1512.05207", "submitter": "R\\\"udiger Ehlers", "authors": "Ruediger Ehlers", "title": "Computing the Complete Pareto Front", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an efficient algorithm to enumerate all elements of a Pareto front in\na multi-objective optimization problem in which the space of values is finite\nfor all objectives. Our algorithm uses a feasibility check for a search space\nelement as an oracle and minimizes the number of oracle calls that are\nnecessary to identify the Pareto front of the problem. Given a $k$-dimensional\nsearch space in which each dimension has $n$ elements, it needs $p \\cdot (k\n\\cdot \\lceil \\log_2 n \\rceil + 1) + \\psi(p)$ oracle calls, where $p$ is the\nsize of the Pareto front and $\\psi(p)$ is the number of greatest elements of\nthe part of the search space that is not dominated by the Pareto front\nelements. We show that this number of oracle calls is essentially optimal as\napproximately $p \\cdot k \\cdot \\log_2 n$ oracle calls are needed to identify\nthe Pareto front elements in sparse Pareto sets and $\\psi(p)$ calls are needed\nto show that no element is missing in the set of Pareto front elements found.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 15:23:16 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Ehlers", "Ruediger", ""]]}, {"id": "1512.05223", "submitter": "Ignasi Sau", "authors": "Luerbio Faria, Sulamita Klein, Ignasi Sau, Rubens Sucupira", "title": "Improved kernels for Signed Max Cut parameterized above lower bound on\n  (r,l)-graphs", "comments": "20 pages, 6 figures", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, Vol. 19 no.\n  1, Discrete Algorithms (June 7, 2017) dmtcs:3682", "doi": "10.23638/DMTCS-19-1-14", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph $G$ is signed if each edge is assigned $+$ or $-$. A signed graph is\nbalanced if there is a bipartition of its vertex set such that an edge has sign\n$-$ if and only if its endpoints are in different parts. The Edwards-Erd\\\"os\nbound states that every graph with $n$ vertices and $m$ edges has a balanced\nsubgraph with at least $\\frac{m}{2}+\\frac{n-1}{4}$ edges. In the Signed Max Cut\nAbove Tight Lower Bound (Signed Max Cut ATLB) problem, given a signed graph $G$\nand a parameter $k$, the question is whether $G$ has a balanced subgraph with\nat least $\\frac{m}{2}+\\frac{n-1}{4}+\\frac{k}{4}$ edges. This problem\ngeneralizes Max Cut Above Tight Lower Bound, for which a kernel with $O(k^5)$\nvertices was given by Crowston et al. [ICALP 2012, Algorithmica 2015]. Crowston\net al. [TCS 2013] improved this result by providing a kernel with $O(k^3)$\nvertices for the more general Signed Max Cut ATLB problem. In this article we\nare interested in improving the size of the kernels for Signed Max Cut ATLB on\nrestricted graph classes for which the problem remains hard. For two integers\n$r,\\ell \\geq 0$, a graph $G$ is an $(r,\\ell)$-graph if $V(G)$ can be\npartitioned into $r$ independent sets and $\\ell$ cliques. Building on the\ntechniques of Crowston et al. [TCS 2013], we provide a kernel with $O(k^2)$\nvertices on $(r,\\ell)$-graphs for any fixed $r,\\ell \\geq 0$, and a simple\nlinear kernel on subclasses of split graphs for which we prove that the problem\nis still NP-hard.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 16:00:56 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 18:30:57 GMT"}, {"version": "v3", "created": "Sun, 5 Mar 2017 21:58:07 GMT"}, {"version": "v4", "created": "Mon, 22 May 2017 14:59:51 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Faria", "Luerbio", ""], ["Klein", "Sulamita", ""], ["Sau", "Ignasi", ""], ["Sucupira", "Rubens", ""]]}, {"id": "1512.05279", "submitter": "Omer Gold", "authors": "Omer Gold and Micha Sharir", "title": "Improved Bounds for 3SUM, $k$-SUM, and Linear Degeneracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of $n$ real numbers, the 3SUM problem is to decide whether there\nare three of them that sum to zero. Until a recent breakthrough by Gr{\\o}nlund\nand Pettie [FOCS'14], a simple $\\Theta(n^2)$-time deterministic algorithm for\nthis problem was conjectured to be optimal. Over the years many algorithmic\nproblems have been shown to be reducible from the 3SUM problem or its variants,\nincluding the more generalized forms of the problem, such as $k$-SUM and\n$k$-variate linear degeneracy testing ($k$-LDT). The conjectured hardness of\nthese problems have become extremely popular for basing conditional lower\nbounds for numerous algorithmic problems in P.\n  In this paper, we show that the randomized $4$-linear decision tree\ncomplexity of 3SUM is $O(n^{3/2})$, and that the randomized $(2k-2)$-linear\ndecision tree complexity of $k$-SUM and $k$-LDT is $O(n^{k/2})$, for any odd\n$k\\ge 3$. These bounds improve (albeit randomized) the corresponding\n$O(n^{3/2}\\sqrt{\\log n})$ and $O(n^{k/2}\\sqrt{\\log n})$ decision tree bounds\nobtained by Gr{\\o}nlund and Pettie. Our technique includes a specialized\nrandomized variant of fractional cascading data structure. Additionally, we\ngive another deterministic algorithm for 3SUM that runs in $O(n^2 \\log\\log n /\n\\log n )$ time. The latter bound matches a recent independent bound by Freund\n[Algorithmica 2017], but our algorithm is somewhat simpler, due to a better use\nof word-RAM model.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 18:55:23 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 00:12:44 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Gold", "Omer", ""], ["Sharir", "Micha", ""]]}, {"id": "1512.05411", "submitter": "Reut Levi", "authors": "Mika G\\\"o\\\"os, Juho Hirvonen, Reut Levi, Moti Medina, Jukka Suomela", "title": "Non-Local Probes Do Not Help with Graph Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work bridges the gap between distributed and centralised models of\ncomputing in the context of sublinear-time graph algorithms. A priori, typical\ncentralised models of computing (e.g., parallel decision trees or centralised\nlocal algorithms) seem to be much more powerful than distributed\nmessage-passing algorithms: centralised algorithms can directly probe any part\nof the input, while in distributed algorithms nodes can only communicate with\ntheir immediate neighbours. We show that for a large class of graph problems,\nthis extra freedom does not help centralised algorithms at all: for example,\nefficient stateless deterministic centralised local algorithms can be simulated\nwith efficient distributed message-passing algorithms. In particular, this\nenables us to transfer existing lower bound results from distributed algorithms\nto centralised local algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 23:31:25 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["G\u00f6\u00f6s", "Mika", ""], ["Hirvonen", "Juho", ""], ["Levi", "Reut", ""], ["Medina", "Moti", ""], ["Suomela", "Jukka", ""]]}, {"id": "1512.05448", "submitter": "Yangyang Xu", "authors": "Danilo Elias Oliveira, Henry Wolkowicz, Yangyang Xu", "title": "ADMM for the SDP relaxation of the QAP", "comments": "12 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semidefinite programming (SDP) relaxation has proven to be extremely\nstrong for many hard discrete optimization problems. This is in particular true\nfor the quadratic assignment problem (QAP), arguably one of the hardest NP-hard\ndiscrete optimization problems. There are several difficulties that arise in\nefficiently solving the SDP relaxation, e.g.,~increased dimension; inefficiency\nof the current primal-dual interior point solvers in terms of both time and\naccuracy; and difficulty and high expense in adding cutting plane constraints.\n  We propose using the alternating direction method of multipliers (ADMM) to\nsolve the SDP relaxation. This first order approach allows for inexpensive\niterations, a method of cheaply obtaining low rank solutions, as well a trivial\nway of adding cutting plane inequalities. When compared to current approaches\nand current best available bounds we obtain remarkable robustness, efficiency\nand improved bounds.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 02:57:27 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Oliveira", "Danilo Elias", ""], ["Wolkowicz", "Henry", ""], ["Xu", "Yangyang", ""]]}, {"id": "1512.05656", "submitter": "Benjamin Albrecht", "authors": "Benjamin Albrecht", "title": "Fast computation of all maximum acyclic agreement forests for two rooted\n  binary phylogenetic trees", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary scenarios displaying reticulation events are often represented\nby rooted phylogenetic networks. Due to biological reasons, those events occur\nvery rarely, and, thus, networks containing a minimum number of such events,\nso-called minimum hybridization networks, are of particular interest for\nresearch. Moreover, to study reticulate evolution, biologist need not only a\nsubset but all of those networks. To achieve this goal, the less complex\nconcept of rooted phylogenetic trees can be used as building block. Here, as a\nfirst important step, the trees are disjoint into common parts, so-called\nmaximum acyclic agreement forests, which can then be turned into minimum\nhybridization networks by applying further network building algorithms. In this\npaper, we present two modifications of the first non-naive algorithm --- called\nallMAAFs --- computing all maximum acyclic agreement forests for two rooted\nbinary phylogenetic trees on the same set of taxa. By a simulation study, we\nindicate that through these modifications the algorithm is on average 8 times\nfaster than the original algorithm making this algorithm accessible to larger\ninput trees and, thus, to a wider range of biological problems.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 16:21:47 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Albrecht", "Benjamin", ""]]}, {"id": "1512.05703", "submitter": "Benjamin Albrecht", "authors": "Benjamin Albrecht", "title": "Computing a Relevant Set of Nonbinary Maximum Acyclic Agreement Forests", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist several methods dealing with the reconstruction of rooted\nphylogenetic networks explaining different evolutionary histories given by\nrooted binary phylogenetic trees. In practice, however, due to insufficient\ninformation of the underlying data, phylogenetic trees are in general not\ncompletely resolved and, thus, those methods can often not be applied to\nbiological data. In this work, we make a first important step to approach this\ngoal by presenting the first algorithm --- called allMulMAAFs --- that enables\nthe computation of all relevant nonbinary maximum acyclic agreement forests for\ntwo rooted (nonbinary) phylogenetic trees on the same set of taxa. Notice that\nour algorithm is part of the freely available software Hybroscale computing\nminimum hybridization networks for a set of rooted (nonbinary) phylogenetic\ntrees on an overlapping set of taxa.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 18:10:20 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Albrecht", "Benjamin", ""]]}, {"id": "1512.05876", "submitter": "Yasuaki Kobayashi", "authors": "Yasuaki Kobayashi, Hisao Tamaki", "title": "A faster fixed parameter algorithm for two-layer crossing minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm that decides whether the bipartite crossing number of a\ngiven graph is at most $k$. The running time of the algorithm is upper bounded\nby $2^{O(k)} + n^{O(1)}$, where $n$ is the number of vertices of the input\ngraph, which improves the previously known algorithm due to Kobayashi et al.\n(TCS 2014) that runs in $2^{O(k \\log k)} + n^{O(1)}$ time. This result is based\non a combinatorial upper bound on the number of two-layer drawings of a\nconnected bipartite graph with a bounded crossing number.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 09:09:41 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Kobayashi", "Yasuaki", ""], ["Tamaki", "Hisao", ""]]}, {"id": "1512.05947", "submitter": "Kathrin Bujna", "authors": "Johannes Bl\\\"omer, Sascha Brauer, and Kathrin Bujna", "title": "Complexity and Approximation of the Fuzzy K-Means Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fuzzy $K$-means problem is a generalization of the classical $K$-means\nproblem to soft clusterings, i.e. clusterings where each points belongs to each\ncluster to some degree. Although popular in practice, prior to this work the\nfuzzy $K$-means problem has not been studied from a complexity theoretic or\nalgorithmic perspective. We show that optimal solutions for fuzzy $K$-means\ncannot, in general, be expressed by radicals over the input points.\nSurprisingly, this already holds for very simple inputs in one-dimensional\nspace. Hence, one cannot expect to compute optimal solutions exactly. We give\nthe first $(1+\\epsilon)$-approximation algorithms for the fuzzy $K$-means\nproblem. First, we present a deterministic approximation algorithm whose\nruntime is polynomial in $N$ and linear in the dimension $D$ of the input set,\ngiven that $K$ is constant, i.e. a polynomial time approximation algorithm\ngiven a fixed $K$. We achieve this result by showing that for each soft\nclustering there exists a hard clustering with comparable properties. Second,\nby using techniques known from coreset constructions for the $K$-means problem,\nwe develop a deterministic approximation algorithm that runs in time almost\nlinear in $N$ but exponential in the dimension $D$. We complement these results\nwith a randomized algorithm which imposes some natural restrictions on the\ninput set and whose runtime is comparable to some of the most efficient\napproximation algorithms for $K$-means, i.e. linear in the number of points and\nthe dimension, but exponential in the number of clusters.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 13:35:59 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Bl\u00f6mer", "Johannes", ""], ["Brauer", "Sascha", ""], ["Bujna", "Kathrin", ""]]}, {"id": "1512.05974", "submitter": "Omar Darwish", "authors": "Omar Darwish and Kurt Mehlhorn", "title": "Improved Balanced Flow Computation Using Parametric Flow", "comments": "Article will appear in Information Processing Letters", "journal-ref": null, "doi": "10.1016/j.ipl.2016.04.008", "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for computing balanced flows in equality networks\narising in market equilibrium computations. The current best time bound for\ncomputing balanced flows in such networks requires $O(n)$ maxflow computations,\nwhere $n$ is the number of nodes in the network [Devanur et al. 2008]. Our\nalgorithm requires only a single parametric flow computation. The best\nalgorithm for computing parametric flows [Gallo et al. 1989] is only by a\nlogarithmic factor slower than the best algorithms for computing maxflows.\nHence, the running time of the algorithms in [Devanur et al. 2008] and [Duan\nand Mehlhorn 2015] for computing market equilibria in linear Fisher and\nArrow-Debreu markets improve by almost a factor of $n$.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 14:41:01 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 11:34:17 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Darwish", "Omar", ""], ["Mehlhorn", "Kurt", ""]]}, {"id": "1512.05996", "submitter": "Christian Kudahl", "authors": "Dennis Komm, Rastislav Kr\\'alovi\\v{c}, Richard Kr\\'alovi\\v{c}, and\n  Christian Kudahl", "title": "Advice Complexity of the Online Induced Subgraph Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several well-studied graph problems aim to select a largest (or smallest)\ninduced subgraph with a given property of the input graph. Examples of such\nproblems include maximum independent set, maximum planar graph, and many\nothers. We consider these problems, where the vertices are presented online.\nWith each vertex, the online algorithm must decide whether to include it into\nthe constructed subgraph, based only on the subgraph induced by the vertices\npresented so far. We study the properties that are common to all these problems\nby investigating the generalized problem: for a hereditary property \\pty, find\nsome maximal induced subgraph having \\pty. We study this problem from the point\nof view of advice complexity. Using a result from Boyar et al. [STACS 2015], we\ngive a tight trade-off relationship stating that for inputs of length n roughly\nn/c bits of advice are both needed and sufficient to obtain a solution with\ncompetitive ratio c, regardless of the choice of \\pty, for any c (possibly a\nfunction of n). Surprisingly, a similar result cannot be obtained for the\nsymmetric problem: for a given cohereditary property \\pty, find a minimum\nsubgraph having \\pty. We show that the advice complexity of this problem varies\nsignificantly with the choice of \\pty.\n  We also consider preemptive online model, where the decision of the algorithm\nis not completely irreversible. In particular, the algorithm may discard some\nvertices previously assigned to the constructed set, but discarded vertices\ncannot be reinserted into the set again. We show that, for the maximum induced\nsubgraph problem, preemption cannot help much, giving a lower bound of\n$\\Omega(n/(c^2\\log c))$ bits of advice needed to obtain competitive ratio $c$,\nwhere $c$ is any increasing function bounded by \\sqrt{n/log n}. We also give a\nlinear lower bound for c close to 1.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 15:41:34 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Komm", "Dennis", ""], ["Kr\u00e1lovi\u010d", "Rastislav", ""], ["Kr\u00e1lovi\u010d", "Richard", ""], ["Kudahl", "Christian", ""]]}, {"id": "1512.06073", "submitter": "Keno Merckx", "authors": "Jean Cardinal, Jean-Paul Doignon, Keno Merckx", "title": "On the shelling antimatroids of split graphs", "comments": null, "journal-ref": "Discrete Mathematics & Theoretical Computer Science, Vol. 19 no.\n  1, Combinatorics (March 24, 2017) dmtcs:3201", "doi": "10.23638/DMTCS-19-1-7", "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chordal graph shelling antimatroids have received little attention with\nregard to their combinatorial properties and related optimization problems, as\ncompared to the case of poset shelling antimatroids. Here we consider a special\ncase of these antimatroids, namely the split graph shelling antimatroids. We\nshow that the feasible sets of such an antimatroid relate to some poset\nshelling antimatroids constructed from the graph. We discuss a few\napplications, obtaining in particular a simple polynomial-time algorithm to\nfind a maximum weight feasible set. We also provide a simple description of the\ncircuits and the free sets.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 18:54:14 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 09:44:50 GMT"}, {"version": "v3", "created": "Thu, 16 Mar 2017 15:01:25 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Cardinal", "Jean", ""], ["Doignon", "Jean-Paul", ""], ["Merckx", "Keno", ""]]}, {"id": "1512.06143", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, Sanjeev Khanna, Yang Li, Val Tannen", "title": "Algorithms for Provisioning Queries and Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Provisioning is a technique for avoiding repeated expensive computations in\nwhat-if analysis. Given a query, an analyst formulates $k$ hypotheticals, each\nretaining some of the tuples of a database instance, possibly overlapping, and\nshe wishes to answer the query under scenarios, where a scenario is defined by\na subset of the hypotheticals that are \"turned on\". We say that a query admits\ncompact provisioning if given any database instance and any $k$ hypotheticals,\none can create a poly-size (in $k$) sketch that can then be used to answer the\nquery under any of the $2^{k}$ possible scenarios without accessing the\noriginal instance.\n  In this paper, we focus on provisioning complex queries that combine\nrelational algebra (the logical component), grouping, and statistics/analytics\n(the numerical component). We first show that queries that compute quantiles or\nlinear regression (as well as simpler queries that compute count and\nsum/average of positive values) can be compactly provisioned to provide\n(multiplicative) approximate answers to an arbitrary precision. In contrast,\nexact provisioning for each of these statistics requires the sketch size to be\nexponential in $k$. We then establish that for any complex query whose logical\ncomponent is a positive relational algebra query, as long as the numerical\ncomponent can be compactly provisioned, the complex query itself can be\ncompactly provisioned. On the other hand, introducing negation or recursion in\nthe logical component again requires the sketch size to be exponential in $k$.\nWhile our positive results use algorithms that do not access the original\ninstance after a scenario is known, we prove our lower bounds even for the case\nwhen, knowing the scenario, limited access to the instance is allowed.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 21:29:58 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Assadi", "Sepehr", ""], ["Khanna", "Sanjeev", ""], ["Li", "Yang", ""], ["Tannen", "Val", ""]]}, {"id": "1512.06238", "submitter": "Eric Balkanski", "authors": "Eric Balkanski, Aviad Rubinstein, Yaron Singer", "title": "The Limitations of Optimization from Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the following question: can we optimize objective\nfunctions from the training data we use to learn them? We formalize this\nquestion through a novel framework we call optimization from samples (OPS). In\nOPS, we are given sampled values of a function drawn from some distribution and\nthe objective is to optimize the function under some constraint.\n  While there are interesting classes of functions that can be optimized from\nsamples, our main result is an impossibility. We show that there are classes of\nfunctions which are statistically learnable and optimizable, but for which no\nreasonable approximation for optimization from samples is achievable. In\nparticular, our main result shows that there is no constant factor\napproximation for maximizing coverage functions under a cardinality constraint\nusing polynomially-many samples drawn from any distribution.\n  We also show tight approximation guarantees for maximization under a\ncardinality constraint of several interesting classes of functions including\nunit-demand, additive, and general monotone submodular functions, as well as a\nconstant factor approximation for monotone submodular functions with bounded\ncurvature.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 12:35:36 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 21:25:06 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2016 21:41:09 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Balkanski", "Eric", ""], ["Rubinstein", "Aviad", ""], ["Singer", "Yaron", ""]]}, {"id": "1512.06271", "submitter": "Sahil Singla", "authors": "Guru Guruganesh, Sahil Singla", "title": "Online Matroid Intersection: Beating Half for Random Arrival", "comments": "39 pages, 3 figures, 1 notation table, Part of this appeared in IPCO\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For two matroids $\\mathcal{M}_1$ and $\\mathcal{M}_2$ defined on the same\nground set $E$, the online matroid intersection problem is to design an\nalgorithm that constructs a large common independent set in an online fashion.\nThe algorithm is presented with the ground set elements one-by-one in a\nuniformly random order. At each step, the algorithm must irrevocably decide\nwhether to pick the element, while always maintaining a common independent set.\nWhile the natural greedy algorithm---pick an element whenever possible---is\nhalf competitive, nothing better was previously known; even for the special\ncase of online bipartite matching in the edge arrival model. We present the\nfirst randomized online algorithm that has a $\\frac12 + \\delta$ competitive\nratio in expectation, where $\\delta >0$ is a constant. The expectation is over\nthe random order and the coin tosses of the algorithm. As a corollary, we also\nobtain the first linear time algorithm that beats half competitiveness for\noffline matroid intersection.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 17:09:41 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 02:48:24 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 03:37:28 GMT"}, {"version": "v4", "created": "Mon, 19 Feb 2018 18:24:36 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Guruganesh", "Guru", ""], ["Singla", "Sahil", ""]]}, {"id": "1512.06283", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, Mark Jones, Bin Sheng, Magnus Wahlstr\\\"om, Anders Yeo", "title": "Chinese Postman Problem on Edge-Colored Multigraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that the Chinese postman problem on undirected and directed\ngraphs is polynomial-time solvable. We extend this result to edge-colored\nmultigraphs. Our result is in sharp contrast to the Chinese postman problem on\nmixed graphs, i.e., graphs with directed and undirected edges, for which the\nproblem is NP-hard.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 20:17:33 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 12:23:35 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Gutin", "Gregory", ""], ["Jones", "Mark", ""], ["Sheng", "Bin", ""], ["Wahlstr\u00f6m", "Magnus", ""], ["Yeo", "Anders", ""]]}, {"id": "1512.06353", "submitter": "Iftah Gamzu", "authors": "Iftah Gamzu and Danny Segev", "title": "A Polynomial-Time Approximation Scheme for The Airplane Refueling\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the airplane refueling problem which was introduced by the\nphysicists Gamow and Stern in their classical book Puzzle-Math (1958). Sticking\nto the original story behind this problem, suppose we have to deliver a bomb in\nsome distant point of the globe, the distance being much greater than the range\nof any individual airplane at our disposal. Therefore, the only feasible option\nto carry out this mission is to better utilize our fleet via mid-air refueling.\nStarting with several airplanes that can refuel one another, and gradually drop\nout of the flight until the single plane carrying the bomb reaches the target,\nhow would you plan the refueling policy?\n  The main contribution of Gamow and Stern was to provide a complete\ncharacterization of the optimal refueling policy for the special case of\nidentical airplanes. In spite of their elegant and easy-to-analyze solution,\nthe computational complexity of the general airplane refueling problem, with\narbitrary tank volumes and consumption rates, has remained widely open ever\nsince, as recently pointed out by Woeginger (Open Problems in Scheduling,\nDagstuhl 2010, page 24). To our knowledge, other than a logarithmic\napproximation, which can be attributed to folklore, it is not entirely obvious\neven if constant-factor performance guarantees are within reach.\n  In this paper, we propose a polynomial-time approximation scheme for the\nairplane refueling problem in its utmost generality. Our approach builds on a\nnovel combination of ideas related to parametric pruning, efficient guessing\ntricks, reductions to well-structured instances of generalized assignment, and\nadditional insight into how LP-rounding algorithms in this context actually\nwork. We complement this result by presenting a fast and easy-to-implement\nalgorithm that approximates the optimal refueling policy to within a constant\nfactor.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 10:58:08 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Gamzu", "Iftah", ""], ["Segev", "Danny", ""]]}, {"id": "1512.06372", "submitter": "Gennaro Cordasco PhD", "authors": "Gennaro Cordasco, Luisa Gargano, Adele A. Rescigno, Ugo Vaccaro", "title": "Optimizing Spread of Influence in Weighted Social Networks via Partial\n  Incentives", "comments": "An extended abstract of a preliminary version of this paper appeared\n  in: Proceedings of 22nd International Colloquium on Structural Information\n  and Communication Complexity (SIROCCO 2015), Lectures Notes in Computer\n  Science vol. 9439, C. Scheideler (Ed.), pp. 119-134, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI math.CO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A widely studied process of influence diffusion in social networks posits\nthat the dynamics of influence diffusion evolves as follows: Given a graph\n$G=(V,E)$, representing the network, initially \\emph{only} the members of a\ngiven $S\\subseteq V$ are influenced; subsequently, at each round, the set of\ninfluenced nodes is augmented by all the nodes in the network that have a\nsufficiently large number of already influenced neighbors. The general problem\nis to find a small initial set of nodes that influences the whole network. In\nthis paper we extend the previously described basic model in the following\nways: firstly, we assume that there are non negative values $c(v)$ associated\nto each node $v\\in V$, measuring how much it costs to initially influence node\n$v$, and the algorithmic problem is to find a set of nodes of \\emph{minimum\ntotal cost} that influences the whole network; successively, we study the\nconsequences of giving \\emph{incentives} to member of the networks, and we\nquantify how this affects (i.e., reduces) the total costs of starting process\nthat influences the whole network. For the two above problems we provide both\nhardness and algorithmic results. We also experimentally validate our\nalgorithms via extensive simulations on real life networks.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 13:25:53 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Cordasco", "Gennaro", ""], ["Gargano", "Luisa", ""], ["Rescigno", "Adele A.", ""], ["Vaccaro", "Ugo", ""]]}, {"id": "1512.06389", "submitter": "Russell Brown", "authors": "Russell A. Brown", "title": "Building a Balanced k-d Tree with MapReduce", "comments": "7 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original description of the k-d tree recognized that rebalancing\ntechniques, such as are used to build an AVL tree or a red-black tree, are not\napplicable to a k-d tree. Hence, in order to build a balanced k-d tree, it is\nnecessary to obtain all of the data prior to building the tree then to build\nthe tree via recursive subdivision of the data. One algorithm for building a\nbalanced k-d tree finds the median of the data for each recursive subdivision\nof the data and builds the tree in O(n log n) time. A new algorithm builds a\nbalanced k-d tree by presorting the data in each of k dimensions prior to\nbuilding the tree, then preserves the order of the k presorts during recursive\nsubdivision of the data and builds the tree in O(kn log n) time. This new\nalgorithm is amenable to execution via MapReduce and permits building and\nsearching a k-d tree that is represented as a distributed graph.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 15:57:00 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 15:49:19 GMT"}, {"version": "v3", "created": "Mon, 28 Dec 2015 14:13:30 GMT"}, {"version": "v4", "created": "Fri, 1 Jan 2016 20:37:03 GMT"}, {"version": "v5", "created": "Thu, 14 Jan 2016 16:07:06 GMT"}, {"version": "v6", "created": "Sat, 16 Jan 2016 01:18:14 GMT"}, {"version": "v7", "created": "Sat, 31 Oct 2020 00:36:43 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Brown", "Russell A.", ""]]}, {"id": "1512.06427", "submitter": "Mark Levin", "authors": "Mark Sh. Levin", "title": "Towards Integrated Glance To Restructuring in Combinatorial Optimization", "comments": "31 pages, 34 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper focuses on a new class of combinatorial problems which consists in\nrestructuring of solutions (as sets/structures) in combinatorial optimization.\nTwo main features of the restructuring process are examined: (i) a cost of the\nrestructuring, (ii) a closeness to a goal solution. Three types of the\nrestructuring problems are under study: (a) one-stage structuring, (b)\nmulti-stage structuring, and (c) structuring over changed element set.\nOne-criterion and multicriteria problem formulations can be considered. The\nrestructuring problems correspond to redesign (improvement, upgrade) of modular\nsystems or solutions. The restructuring approach is described and illustrated\n(problem statements, solving schemes, examples) for the following combinatorial\noptimization problems: knapsack problem, multiple choice problem, assignment\nproblem, spanning tree problems, clustering problem, multicriteria ranking\n(sorting) problem, morphological clique problem. Numerical examples illustrate\nthe restructuring problems and solving schemes.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 20:27:23 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Levin", "Mark Sh.", ""]]}, {"id": "1512.06488", "submitter": "David Eppstein", "authors": "David Eppstein and Daniel S. Hirschberg", "title": "From Discrepancy to Majority", "comments": "15 pages, 3 figures. Extended version of a paper to appear at LATIN\n  2016", "journal-ref": "Algorithmica 80 (4): 1278-1297, 2018", "doi": "10.1007/s00453-017-0303-7", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to select an item with the majority color from $n$ two-colored\nitems, given access to the items only through an oracle that returns the\ndiscrepancy of subsets of $k$ items. We use $n/\\lfloor\\tfrac{k}{2}\\rfloor+O(k)$\nqueries, improving a previous method by De Marco and Kranakis that used\n$n-k+k^2/2$ queries. We also prove a lower bound of $n/(k-1)-O(n^{1/3})$ on the\nnumber of queries needed, improving a lower bound of $\\lfloor n/k\\rfloor$ by De\nMarco and Kranakis.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 04:22:22 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Eppstein", "David", ""], ["Hirschberg", "Daniel S.", ""]]}, {"id": "1512.06532", "submitter": "Mohamed Lamine Lamali", "authors": "Mohamed Lamine Lamali, H\\'elia Pouyllau, Dominique Barth (PRISM)", "title": "Path computation in multi-layer multi-domain networks: A language\n  theoretic approach", "comments": "Journal on Computer Communications, 2013", "journal-ref": null, "doi": "10.1016/j.comcom.2012.11.009", "report-no": null, "categories": "cs.DS cs.FL cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-layer networks are networks in which several protocols may coexist at\ndifferent layers. The Pseudo-Wire architecture provides encapsulation and\nde-capsulation functions of protocols over Packet-Switched Networks. In a\nmulti-domain context, computing a path to support end-to-end services requires\nthe consideration of encapsulation and decapsulation capabilities. It appears\nthat graph models are not expressive enough to tackle this problem. In this\npaper, we propose a new model of heterogeneous networks using Automata Theory.\nA network is modeled as a Push-Down Automaton (PDA) which is able to capture\nthe encapsulation and decapsulation capabilities, the PDA stack corresponding\nto the stack of encapsulated protocols. We provide polynomial algorithms that\ncompute the shortest path either in hops or in the number of encapsulations and\ndecapsulations along the inter-domain path, the latter reducing manual\nconfigurations and possible loops in the path.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 08:55:41 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Lamali", "Mohamed Lamine", "", "PRISM"], ["Pouyllau", "H\u00e9lia", "", "PRISM"], ["Barth", "Dominique", "", "PRISM"]]}, {"id": "1512.06649", "submitter": "Nicolas Catusse", "authors": "Hadrien Cambazard, Nicolas Catusse", "title": "Fixed-Parameter Algorithms for Rectilinear Steiner tree and Rectilinear\n  Traveling Salesman Problem in the plane", "comments": "24 pages, 13 figures, 6 tables", "journal-ref": "European Journal of Operational Research, Volume 270, Issue 2,\n  2018, Pages 419-429, ISSN 0377-2217", "doi": "10.1016/j.ejor.2018.03.042", "report-no": null, "categories": "cs.DS cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $P$ of $n$ points with their pairwise distances, the traveling\nsalesman problem (TSP) asks for a shortest tour that visits each point exactly\nonce. A TSP instance is rectilinear when the points lie in the plane and the\ndistance considered between two points is the $l_1$ distance. In this paper, a\nfixed-parameter algorithm for the Rectilinear TSP is presented and relies on\ntechniques for solving TSP on bounded-treewidth graphs. It proves that the\nproblem can be solved in $O\\left(nh7^h\\right)$ where $h \\leq n$ denotes the\nnumber of horizontal lines containing the points of $P$. The same technique can\nbe directly applied to the problem of finding a shortest rectilinear Steiner\ntree that interconnects the points of $P$ providing a $O\\left(nh5^h\\right)$\ntime complexity. Both bounds improve over the best time bounds known for these\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 14:38:29 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 20:21:28 GMT"}, {"version": "v3", "created": "Thu, 23 Feb 2017 21:54:39 GMT"}, {"version": "v4", "created": "Tue, 23 Jul 2019 07:14:46 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Cambazard", "Hadrien", ""], ["Catusse", "Nicolas", ""]]}, {"id": "1512.06678", "submitter": "Aur\\'elien Ooms", "authors": "Jean Cardinal and John Iacono and Aur\\'elien Ooms", "title": "Solving $k$-SUM using few linear queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-SUM problem is given $n$ input real numbers to determine whether any\n$k$ of them sum to zero. The problem is of tremendous importance in the\nemerging field of complexity theory within $P$, and it is in particular open\nwhether it admits an algorithm of complexity $O(n^c)$ with $c<\\lceil\n\\frac{k}{2} \\rceil$. Inspired by an algorithm due to Meiser (1993), we show\nthat there exist linear decision trees and algebraic computation trees of depth\n$O(n^3\\log^3 n)$ solving $k$-SUM. Furthermore, we show that there exists a\nrandomized algorithm that runs in $\\tilde{O}(n^{\\lceil \\frac{k}{2} \\rceil+8})$\ntime, and performs $O(n^3\\log^3 n)$ linear queries on the input. Thus, we show\nthat it is possible to have an algorithm with a runtime almost identical (up to\nthe $+8$) to the best known algorithm but for the first time also with the\nnumber of queries on the input a polynomial that is independent of $k$. The\n$O(n^3\\log^3 n)$ bound on the number of linear queries is also a tighter bound\nthan any known algorithm solving $k$-SUM, even allowing unlimited total time\noutside of the queries. By simultaneously achieving few queries to the input\nwithout significantly sacrificing runtime vis-\\`{a}-vis known algorithms, we\ndeepen the understanding of this canonical problem which is a cornerstone of\ncomplexity-within-$P$.\n  We also consider a range of tradeoffs between the number of terms involved in\nthe queries and the depth of the decision tree. In particular, we prove that\nthere exist $o(n)$-linear decision trees of depth $o(n^4)$.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 16:07:35 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 10:57:33 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Cardinal", "Jean", ""], ["Iacono", "John", ""], ["Ooms", "Aur\u00e9lien", ""]]}, {"id": "1512.06736", "submitter": "Hadas Shachnai", "authors": "Ariel Kulik, Hadas Shachnai and Gal Tamir", "title": "On Lagrangian Relaxation and Reoptimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a general result demonstrating the power of Lagrangian relaxation in\nsolving constrained maximization problems with arbitrary objective functions.\nThis yields a unified approach for solving a wide class of {\\em subset\nselection} problems with linear constraints. Given a problem in this class and\nsome small $\\eps \\in (0,1)$, we show that if there exists an $r$-approximation\nalgorithm for the Lagrangian relaxation of the problem, for some $r \\in (0,1)$,\nthen our technique achieves a ratio of $\\frac{r}{r+1} -\\! \\eps$ to the optimal,\nand this ratio is tight.\n  The number of calls to the $r$-approximation algorithm, used by our\nalgorithms, is {\\em linear} in the input size and in $\\log (1 / \\eps)$ for\ninputs with cardinality constraint, and polynomial in the input size and in\n$\\log (1 / \\eps)$ for inputs with arbitrary linear constraint. Using the\ntechnique we obtain (re)approximation algorithms for natural (reoptimization)\nvariants of classic subset selection problems, including real-time scheduling,\nthe {\\em maximum generalized assignment problem (GAP)} and maximum weight\nindependent set.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 17:58:38 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Kulik", "Ariel", ""], ["Shachnai", "Hadas", ""], ["Tamir", "Gal", ""]]}, {"id": "1512.07019", "submitter": "R\\'emi Watrigant", "authors": "Jason Crampton, Gregory Gutin, Daniel Karapetyan, R\\'emi Watrigant", "title": "The Bi-Objective Workflow Satisfiability Problem and Workflow Resiliency", "comments": "to appear in the Journal of Computer Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computerized workflow management system may enforce a security policy,\nspecified in terms of authorized actions and constraints, thereby restricting\nwhich users can perform particular steps in a workflow. The existence of a\nsecurity policy may mean it is impossible to find a valid plan (an assignment\nof steps to authorized users such that all constraints are satisfied). Work in\nthe literature focuses on the workflow satisfiability problem, a\n\\emph{decision} problem that outputs a valid plan if the instance is\nsatisfiable (and a negative result otherwise).\n  In this paper, we introduce the \\textsc{Bi-Objective Workflow Satisfiability\nProblem} (\\BOWSP), which enables us to solve \\emph{optimization} problems\nrelated to workflows and security policies. In particular, we are able to\ncompute a \"least bad\" plan when some components of the security policy may be\nviolated. In general, \\BOWSP is intractable from both the classical and\nparameterized complexity point of view. We prove there exists an\nfixed-parameter tractable (FPT) algorithm to compute a Pareto front for \\BOWSP\nif we restrict our attention to user-independent constraints.\n  We also present a second algorithm to compute a Pareto front which uses mixed\ninteger programming (MIP). We compare the performance of both our algorithms on\nsynthetic instances, and show that the FPT algorithm outperforms the MIP-based\none by several orders of magnitude on most of the instances.\n  Finally, we study the important question of workflow resiliency and prove new\nresults establishing that known decision problems are fixed-parameter tractable\nwhen restricted to user-independent constraints. We then propose a new way of\nmodeling the availability of users and demonstrate that many questions related\nto resiliency in the context of this new model may be reduced to instances of\n\\BOWSP.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 10:27:15 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2016 12:14:45 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2016 16:42:54 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Crampton", "Jason", ""], ["Gutin", "Gregory", ""], ["Karapetyan", "Daniel", ""], ["Watrigant", "R\u00e9mi", ""]]}, {"id": "1512.07160", "submitter": "Haitao Wang", "authors": "Sang Won Bae, Matias Korman, Joseph S.B. Mitchell, Yoshio Okamoto,\n  Valentin Polishchuk, and Haitao Wang", "title": "Computing the $L_1$ Geodesic Diameter and Center of a Polygonal Domain", "comments": "Made a few cosmetic changes over the previous version; this version\n  to appear in Discrete & Computational Geometry", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a polygonal domain with $h$ holes and a total of $n$ vertices, we present\nalgorithms that compute the $L_1$ geodesic diameter in $O(n^2+h^4)$ time and\nthe $L_1$ geodesic center in $O((n^4+n^2 h^4)\\alpha(n))$ time, respectively,\nwhere $\\alpha(\\cdot)$ denotes the inverse Ackermann function. No algorithms\nwere known for these problems before. For the Euclidean counterpart, the best\nalgorithms compute the geodesic diameter in $O(n^{7.73})$ or $O(n^7(h+\\log n))$\ntime, and compute the geodesic center in $O(n^{11}\\log n)$ time. Therefore, our\nalgorithms are significantly faster than the algorithms for the Euclidean\nproblems. Our algorithms are based on several interesting observations on $L_1$\nshortest paths in polygonal domains.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 17:09:20 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 22:29:48 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Bae", "Sang Won", ""], ["Korman", "Matias", ""], ["Mitchell", "Joseph S. B.", ""], ["Okamoto", "Yoshio", ""], ["Polishchuk", "Valentin", ""], ["Wang", "Haitao", ""]]}, {"id": "1512.07263", "submitter": "Tom Portegys", "authors": "Tom Portegys", "title": "General Graph Identification By Hashing", "comments": "First made public: February 2008 Code available at:\n  https://sourceforge.net/projects/graph-hashing/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for identifying graphs using MD5 hashing is presented. This allows\nfast graph equality comparisons and can also be used to facilitate graph\nisomorphism testing. The graphs can be labeled or unlabeled. The method\nidentifies vertices by hashing the graph configuration in their neighborhoods.\nWith each vertex hashed, the entire graph can be identified by hashing the\nvertex hashes.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 21:11:43 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Portegys", "Tom", ""]]}, {"id": "1512.07437", "submitter": "Adam Gudy\\'s", "authors": "Adam Gudys and Sebastian Deorowicz", "title": "QuickProbs 2: towards rapid construction of high-quality alignments of\n  large protein families", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": "10.1038/srep41553", "report-no": null, "categories": "q-bio.QM cs.CE cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing size of sequence databases caused by the development of high\nthroughput sequencing, poses multiple alignment algorithms to face one of the\ngreatest challenges yet. As we show, well-established techniques employed for\nincreasing alignment quality, i.e., refinement and consistency, are ineffective\nwhen large protein families are of interest. We present QuickProbs 2, an\nalgorithm for multiple sequence alignment. Based on probabilistic models,\nequipped with novel column-oriented refinement and selective consistency, it\noffers outstanding accuracy. When analysing hundreds of sequences, QuickProbs 2\nis significantly better than Clustal Omega, the previous leader for processing\nnumerous protein families. In the case of smaller sets, for which\nconsistency-based methods are the best performing, QuickProbs 2 is also\nsuperior to the competitors. Due to computational scalability of selective\nconsistency and utilisation of massively parallel architectures, presented\nalgorithm is comparable to Clustal Omega in terms of execution time, and orders\nof magnitude faster than full consistency approaches, like MSAProbs or PicXAA.\nAll these make QuickProbs 2 a useful tool for aligning families ranging from\nfew, to hundreds of proteins. QuickProbs 2 is available at\nhttps://github.com/refresh-bio/QuickProbs.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 11:29:18 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 18:02:10 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Gudys", "Adam", ""], ["Deorowicz", "Sebastian", ""]]}, {"id": "1512.07449", "submitter": "Martin Romauch", "authors": "Martin Romauch, Thibaut Vidal, Richard F. Hartl", "title": "The lateral transhipment problem with a-priori routes, and a lot sizing\n  application", "comments": "Working Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose exact solution approaches for a lateral transhipment problem\nwhich, given a pre-specified sequence of customers, seeks an optimal inventory\nredistribution plan considering travel costs and profits dependent on inventory\nlevels. Trip-duration and vehicle-capacity constraints are also imposed. The\nsame problem arises in some lot sizing applications, in the presence of setup\ncosts and equipment re-qualifications.\n  We introduce a pure dynamic programming approach and a branch-and-bound\nframework that combines dynamic programming with Lagrangian relaxation.\nComputational experiments are conducted to determine the most suitable solution\napproach for different instances, depending on their size, vehicle capacities\nand duration constraints. The branch-and-bound approach, in particular, solves\nproblems with up to 50 delivery locations in less than ten seconds on a modern\ncomputer.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 12:28:10 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Romauch", "Martin", ""], ["Vidal", "Thibaut", ""], ["Hartl", "Richard F.", ""]]}, {"id": "1512.07459", "submitter": "Steven Kelk", "authors": "Steven Kelk, Mareike Fischer, Vincent Moulton, Taoyang Wu", "title": "Reduction rules for the maximum parsimony distance on phylogenetic trees", "comments": "Added material on graph minors, MSOL and treewidth", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In phylogenetics, distances are often used to measure the incongruence\nbetween a pair of phylogenetic trees that are reconstructed by different\nmethods or using different regions of genome. Motivated by the maximum\nparsimony principle in tree inference, we recently introduced the maximum\nparsimony (MP) distance, which enjoys various attractive properties due to its\nconnection with several other well-known tree distances, such as TBR and SPR.\nHere we show that computing the MP distance between two trees, a NP-hard\nproblem in general, is fixed parameter tractable in terms of the TBR distance\nbetween the tree pair. Our approach is based on two reduction rules--the chain\nreduction and the subtree reduction--that are widely used in computing TBR and\nSPR distances. More precisely, we show that reducing chains to length 4 (but\nnot shorter) preserves the MP distance. In addition, we describe a\ngeneralization of the subtree reduction which allows the pendant subtrees to be\nrooted in different places, and show that this still preserves the MP distance.\nOn a slightly different note we also show that Monadic Second Order Logic\n(MSOL), posited over an auxiliary graph structure known as the display graph\n(obtained by merging the two trees at their leaves), can be used to obtain an\nalternative proof that computation of MP distance is fixed parameter tractable\nin terms of TBR-distance. We conclude with an extended discussion in which we\nfocus on similarities and differences between MP distance and TBR distance and\npresent a number of open problems. One particularly intriguing question,\nemerging from the MSOL formulation, is whether two trees with bounded MP\ndistance induce display graphs of bounded treewidth.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 13:07:16 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2016 09:22:27 GMT"}, {"version": "v3", "created": "Thu, 7 Jul 2016 09:28:18 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Kelk", "Steven", ""], ["Fischer", "Mareike", ""], ["Moulton", "Vincent", ""], ["Wu", "Taoyang", ""]]}, {"id": "1512.07494", "submitter": "Sebastien Bougleux", "authors": "S\\'ebastien Bougleux, Luc Brun, Vincenzo Carletti, Pasquale Foggia,\n  Benoit Ga\\\"uz\\`ere (LITIS), Mario Vento", "title": "A Quadratic Assignment Formulation of the Graph Edit Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing efficiently a robust measure of similarity or dissimilarity between\ngraphs is a major challenge in Pattern Recognition. The Graph Edit Distance\n(GED) is a flexible measure of dissimilarity between graphs which arises in\nerror-tolerant graph matching. It is defined from an optimal sequence of edit\noperations (edit path) transforming one graph into an other. Unfortunately, the\nexact computation of this measure is NP-hard. In the last decade, several\napproaches have been proposed to approximate the GED in polynomial time, mainly\nby solving linear programming problems. Among them, the bipartite GED has\nreceived much attention. It is deduced from a linear sum assignment of the\nnodes of the two graphs, which can be efficiently computed by Hungarian-type\nalgorithms. However, edit operations on nodes and edges are not handled\nsimultaneously, which limits the accuracy of the approximation. To overcome\nthis limitation, we propose to extend the linear assignment model to a\nquadratic one, for directed or undirected graphs having labelized nodes and\nedges. This is realized through the definition of a family of edit paths\ninduced by assignments between nodes. We formally show that the GED, restricted\nto the paths in this family, is equivalent to a quadratic assignment problem.\nSince this problem is NP-hard, we propose to compute an approximate solution by\nan adaptation of the Integer Projected Fixed Point method. Experiments show\nthat the proposed approach is generally able to reach a more accurate\napproximation of the optimal GED than the bipartite GED, with a computational\ncost that is still affordable for graphs of non trivial sizes.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 14:33:47 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Bougleux", "S\u00e9bastien", "", "LITIS"], ["Brun", "Luc", "", "LITIS"], ["Carletti", "Vincenzo", "", "LITIS"], ["Foggia", "Pasquale", "", "LITIS"], ["Ga\u00fcz\u00e8re", "Benoit", "", "LITIS"], ["Vento", "Mario", ""]]}, {"id": "1512.07550", "submitter": "Srinivasan Arunachalam", "authors": "Srinivasan Arunachalam (CWI) and Ronald de Wolf (CWI and University of\n  Amsterdam)", "title": "Optimizing the Number of Gates in Quantum Search", "comments": "11 pages LaTeX. Version 2: small improvements in the proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $ $In its usual form, Grover's quantum search algorithm uses $O(\\sqrt{N})$\nqueries and $O(\\sqrt{N} \\log N)$ other elementary gates to find a solution in\nan $N$-bit database. Grover in 2002 showed how to reduce the number of other\ngates to $O(\\sqrt{N}\\log\\log N)$ for the special case where the database has a\nunique solution, without significantly increasing the number of queries. We\nshow how to reduce this further to $O(\\sqrt{N}\\log^{(r)} N)$ gates for any\nconstant $r$, and sufficiently large $N$. This means that, on average, the\ngates between two queries barely touch more than a constant number of the $\\log\nN$ qubits on which the algorithm acts. For a very large $N$ that is a power of\n2, we can choose $r$ such that the algorithm uses essentially the minimal\nnumber $\\frac{\\pi}{4}\\sqrt{N}$ of queries, and only\n$O(\\sqrt{N}\\log(\\log^{\\star} N))$ other gates.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 17:23:26 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 08:53:26 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Arunachalam", "Srinivasan", "", "CWI"], ["de Wolf", "Ronald", "", "CWI and University of\n  Amsterdam"]]}, {"id": "1512.07771", "submitter": "Bart Kamphorst", "authors": "Nikhil Bansal, Bart Kamphorst, Bert Zwart", "title": "Achievable Performance of Blind Policies in Heavy Traffic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a GI/GI/1 queue, we show that the average sojourn time under the (blind)\nRandomized Multilevel Feedback algorithm is no worse than that under the\nShortest Remaining Processing Time algorithm times a logarithmic function of\nthe system load. Moreover, it is verified that this bound is tight in heavy\ntraffic, up to a constant multiplicative factor. We obtain this result by\ncombining techniques from two disparate areas: competitive analysis and applied\nprobability.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 09:52:19 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 09:33:23 GMT"}, {"version": "v3", "created": "Fri, 3 Feb 2017 08:33:24 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Bansal", "Nikhil", ""], ["Kamphorst", "Bart", ""], ["Zwart", "Bert", ""]]}, {"id": "1512.07806", "submitter": "Zhi-Hong Deng", "authors": "Zhi-Hong Deng", "title": "Mining Top-K Co-Occurrence Items", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent itemset mining has emerged as a fundamental problem in data mining\nand plays an important role in many data mining tasks, such as association\nanalysis, classification, etc. In the framework of frequent itemset mining, the\nresults are itemsets that are frequent in the whole database. However, in some\napplications, such recommendation systems and social networks, people are more\ninterested in finding out the items that occur with some user-specified\nitemsets (query itemsets) most frequently in a database. In this paper, we\naddress the problem by proposing a new mining task named top-k co-occurrence\nitem mining, where k is the desired number of items to be found. Four baseline\nalgorithms are presented first. Then, we introduce a special data structure\nnamed Pi-Tree (Prefix itemset Tree) to maintain the information of itemsets.\nBased on Pi-Tree, we propose two algorithms, namely PT (Pi-Tree-based\nalgorithm) and PT-TA (Pi-Tree-based algorithm with TA pruning), for mining\ntop-k co-occurrence items by incorporating several novel strategies for pruning\nthe search space to achieve high efficiency. The performance of PT and PT-TA\nwas evaluated against the four proposed baseline algorithms on both synthetic\nand real databases. Extensive experiments show that PT not only outperforms\nother algorithms substantially in terms execution time but also has excellent\nscalability.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 12:49:46 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Deng", "Zhi-Hong", ""]]}, {"id": "1512.08147", "submitter": "Sebastian Krinninger", "authors": "Monika Henzinger, Sebastian Krinninger, Danupon Nanongkai", "title": "Sublinear-Time Maintenance of Breadth-First Spanning Trees in Partially\n  Dynamic Networks", "comments": "This article appears in ACM Transactions on Algorithms 13.4 (2017),\n  pp 51:1-51:24, doi:10.1145/3146550. A preliminary version of this paper was\n  presented at the 40th International Colloquium on Automata, Languages and\n  Programming (ICALP 2013)", "journal-ref": "ACM Transactions on Algorithms 13(4): 51:1-51:24 (2017)", "doi": "10.1145/3146550", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of maintaining a breadth-first spanning tree (BFS tree)\nin partially dynamic distributed networks modeling a sequence of either\nfailures or additions of communication links (but not both). We present\ndeterministic $(1+\\epsilon)$-approximation algorithms whose amortized time\n(over some number of link changes) is sublinear in $D$, the maximum diameter of\nthe network.\n  Our technique also leads to a deterministic $(1+\\epsilon)$-approximate\nincremental algorithm for single-source shortest paths (SSSP) in the sequential\n(usual RAM) model. Prior to our work, the state of the art was the classic\nexact algorithm of Even and Shiloach [JACM 1981] that is optimal under some\nassumptions [Roditty and Zwick ESA 2004, Henzinger et al. STOC 2015]. Our\nresult is the first to show that, in the incremental setting, this bound can be\nbeaten in certain cases if some approximation is allowed.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2015 21:54:27 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 12:21:33 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Henzinger", "Monika", ""], ["Krinninger", "Sebastian", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1512.08148", "submitter": "Sebastian Forster", "authors": "Monika Henzinger, Sebastian Krinninger, Danupon Nanongkai", "title": "Decremental Single-Source Shortest Paths on Undirected Graphs in\n  Near-Linear Total Update Time", "comments": "Accepted to Journal of the ACM. A preliminary version of this paper\n  was presented at the 55th IEEE Symposium on Foundations of Computer Science\n  (FOCS 2014). Abstract shortened to respect the arXiv limit of 1920 characters", "journal-ref": null, "doi": "10.1109/FOCS.2014.24", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the decremental single-source shortest paths (SSSP) problem we want to\nmaintain the distances between a given source node $s$ and every other node in\nan $n$-node $m$-edge graph $G$ undergoing edge deletions. While its static\ncounterpart can be solved in near-linear time, this decremental problem is much\nmore challenging even in the undirected unweighted case. In this case, the\nclassic $O(mn)$ total update time of Even and Shiloach [JACM 1981] has been the\nfastest known algorithm for three decades. At the cost of a\n$(1+\\epsilon)$-approximation factor, the running time was recently improved to\n$n^{2+o(1)}$ by Bernstein and Roditty [SODA 2011]. In this paper, we bring the\nrunning time down to near-linear: We give a $(1+\\epsilon)$-approximation\nalgorithm with $m^{1+o(1)}$ expected total update time, thus obtaining\nnear-linear time. Moreover, we obtain $m^{1+o(1)} \\log W$ time for the weighted\ncase, where the edge weights are integers from $1$ to $W$. The only prior work\non weighted graphs in $o(m n)$ time is the $m n^{0.9 + o(1)}$-time algorithm by\nHenzinger et al. [STOC 2014, ICALP 2015] which works for directed graphs with\nquasi-polynomial edge weights. The expected running time bound of our algorithm\nholds against an oblivious adversary.\n  In contrast to the previous results which rely on maintaining a sparse\nemulator, our algorithm relies on maintaining a so-called sparse $(h,\n\\epsilon)$-hop set introduced by Cohen [JACM 2000] in the PRAM literature. An\n$(h, \\epsilon)$-hop set of a graph $G=(V, E)$ is a set $F$ of weighted edges\nsuch that the distance between any pair of nodes in $G$ can be\n$(1+\\epsilon)$-approximated by their $h$-hop distance (given by a path\ncontaining at most $h$ edges) on $G'=(V, E\\cup F)$. Our algorithm can maintain\nan $(n^{o(1)}, \\epsilon)$-hop set of near-linear size in near-linear time under\nedge deletions.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2015 21:54:45 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 13:21:57 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Henzinger", "Monika", ""], ["Krinninger", "Sebastian", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1512.08555", "submitter": "Jonathan S Turner", "authors": "Jonathan Turner", "title": "Maximium Priority Matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": "WUCSE-2015-06", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be an undirected graph with $n$ vertices and $m$ edges, in\nwhich each vertex $u$ is assigned an integer priority in $[1,n]$, with 1 being\nthe \"highest\" priority. Let $M$ be a matching of $G$. We define the priority\nscore of $M$ to be an $n$-ary integer in which the $i$-th most-significant\ndigit is the number of vertices with priority $i$ that are incident to an edge\nin $M$. We describe a variation of the augmenting path method (Edmonds'\nalgorithm) that finds a matching with maximum priority score in $O(mn)$ time.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 23:15:12 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Turner", "Jonathan", ""]]}, {"id": "1512.08602", "submitter": "Adrian Vladu", "authors": "Vahab Mirrokni, Renato Paes Leme, Adrian Vladu, Sam Chiu-wai Wong", "title": "Tight Bounds for Approximate Carath\\'eodory and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a deterministic nearly-linear time algorithm for approximating any\npoint inside a convex polytope with a sparse convex combination of the\npolytope's vertices. Our result provides a constructive proof for the\nApproximate Carath\\'{e}odory Problem, which states that any point inside a\npolytope contained in the $\\ell_p$ ball of radius $D$ can be approximated to\nwithin $\\epsilon$ in $\\ell_p$ norm by a convex combination of only $O\\left(D^2\np/\\epsilon^2\\right)$ vertices of the polytope for $p \\geq 2$. We also show that\nthis bound is tight, using an argument based on anti-concentration for the\nbinomial distribution.\n  Along the way of establishing the upper bound, we develop a technique for\nminimizing norms over convex sets with complicated geometry; this is achieved\nby running Mirror Descent on a dual convex function obtained via Sion's\nTheorem.\n  As simple extensions of our method, we then provide new algorithms for\nsubmodular function minimization and SVM training. For submodular function\nminimization we obtain a simplification and (provable) speed-up over Wolfe's\nalgorithm, the method commonly found to be the fastest in practice. For SVM\ntraining, we obtain $O(1/\\epsilon^2)$ convergence for arbitrary kernels; each\niteration only requires matrix-vector operations involving the kernel matrix,\nso we overcome the obstacle of having to explicitly store the kernel or compute\nits Cholesky factorization.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 05:06:23 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Mirrokni", "Vahab", ""], ["Leme", "Renato Paes", ""], ["Vladu", "Adrian", ""], ["Wong", "Sam Chiu-wai", ""]]}, {"id": "1512.08831", "submitter": "Sergey Polyakovskiy", "authors": "Sergey Polyakovskiy and Frank Neumann", "title": "The Packing While Traveling Problem", "comments": "arXiv admin note: text overlap with arXiv:1411.5768", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Packing While Traveling problem as a new non-linear\nknapsack problem. Given are a set of cities that have a set of items of\ndistinct profits and weights and a vehicle that may collect the items when\nvisiting all the cities in a fixed order. Each selected item contributes its\nprofit, but produces a transportation cost relative to its weight. The problem\nasks to find a subset of the items such that the total gain is maximized. We\ninvestigate constrained and unconstrained versions of the problem and show that\nboth are NP-hard. We propose a pre-processing scheme that decreases the size of\ninstances making them easier for computation. We provide lower and upper bounds\nbased on mixed-integer programming (MIP) adopting the ideas of piecewise linear\napproximation. Furthermore, we introduce two exact approaches: one is based on\nMIP employing linearization technique, and another is a branch-infer-and-bound\n(BIB) hybrid approach that compounds the upper bound procedure with a\nconstraint programming model strengthened with customized constraints. Our\nexperimental results show the effectiveness of our exact and approximate\nsolutions in terms of solution quality and computational time.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 02:40:39 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 12:26:34 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Polyakovskiy", "Sergey", ""], ["Neumann", "Frank", ""]]}, {"id": "1512.08995", "submitter": "Jonathan S Turner", "authors": "Jonathan Turner", "title": "The Edge Group Coloring Problem with Applications to Multicast Switching", "comments": null, "journal-ref": null, "doi": null, "report-no": "WUCSE-2015-02", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a natural generalization of the classical edge coloring\nproblem in graphs that provides a useful abstraction for two well-known\nproblems in multicast switching. We show that the problem is NP-hard and\nevaluate the performance of several approximation algorithms, both analytically\nand experimentally. We find that for random $\\chi$-colorable graphs, the number\nof colors used by the best algorithms falls within a small constant factor of\n$\\chi$, where the constant factor is mainly a function of the ratio of the\nnumber of outputs to inputs. When this ratio is less than 10, the best\nalgorithms produces solutions that use fewer than $2\\chi$ colors. In addition,\none of the algorithms studied finds high quality approximate solutions for any\ngraph with high probability, where the probability of a low quality solution is\na function only of the random choices made by the algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 16:26:26 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Turner", "Jonathan", ""]]}, {"id": "1512.09002", "submitter": "Jonathan S Turner", "authors": "Jonathan Turner", "title": "The Bounded Edge Coloring Problem and Offline Crossbar Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": "WUCSE-2015-07", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a variant of the classical edge coloring problem in\ngraphs that can be applied to an offline scheduling problem for crossbar\nswitches. We show that the problem is NP-complete, develop three lower bounds\nbounds on the optimal solution value and evaluate the performance of several\napproximation algorithms, both analytically and experimentally. We show how to\napproximate an optimal solution with a worst-case performance ratio of $3/2$\nand our experimental results demonstrate that the best algorithms produce\nresults that very closely track a lower bound.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 16:36:17 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Turner", "Jonathan", ""]]}, {"id": "1512.09090", "submitter": "Julian Dibbelt", "authors": "Moritz Baum, Valentin Buchhold, Julian Dibbelt, Dorothea Wagner", "title": "Fast Computation of Isochrones in Road Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing isochrones in road networks, where the\nobjective is to identify the region that is reachable from a given source\nwithin a certain amount of time. While there is a wide range of practical\napplications for this problem (e.g., reachability analyses, geomarketing,\nvisualizing the cruising range of a vehicle), there has been little research on\nfast computation of isochrones on large, realistic inputs. In this work, we\nformalize the notion of isochrones in road networks and present a basic\napproach for the resulting problem based on Dijkstra's algorithm. Moreover, we\nconsider several speedup techniques that are based on previous approaches for\none-to-many shortest path computation (or similar scenarios). In contrast to\nsuch related problems, the set of targets is not part of the input when\ncomputing isochrones. We extend known Multilevel Dijkstra techniques (such as\nCRP) to the isochrone scenario, adapting a previous technique called isoGRASP\nto our problem setting (thereby, enabling faster queries). Moreover, we\nintroduce a family of algorithms based on (single-level) graph partitions,\nfollowing different strategies to exploit the efficient access patterns of\nPHAST, a well-known approach towards one-to-all queries. Our experimental study\nreveals that all speedup techniques allow fast isochrone computation on input\ngraphs at continental scale, while providing different tradeoffs between\npreprocessing effort, space consumption, and query performance. Finally, we\ndemonstrate that all techniques scale well when run in parallel, decreasing\nquery times to a few milliseconds (orders of magnitude faster than the basic\napproach) and enabling even interactive applications.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 20:09:44 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Baum", "Moritz", ""], ["Buchhold", "Valentin", ""], ["Dibbelt", "Julian", ""], ["Wagner", "Dorothea", ""]]}, {"id": "1512.09103", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Zheng Qu, Peter Richt\\'arik, Yang Yuan", "title": "Even Faster Accelerated Coordinate Descent Using Non-Uniform Sampling", "comments": "same result, but polished writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated coordinate descent is widely used in optimization due to its\ncheap per-iteration cost and scalability to large-scale problems. Up to a\nprimal-dual transformation, it is also the same as accelerated stochastic\ngradient descent that is one of the central methods used in machine learning.\n  In this paper, we improve the best known running time of accelerated\ncoordinate descent by a factor up to $\\sqrt{n}$. Our improvement is based on a\nclean, novel non-uniform sampling that selects each coordinate with a\nprobability proportional to the square root of its smoothness parameter. Our\nproof technique also deviates from the classical estimation sequence technique\nused in prior work. Our speed-up applies to important problems such as\nempirical risk minimization and solving linear systems, both in theory and in\npractice.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 20:30:07 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 20:54:11 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 19:09:29 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Qu", "Zheng", ""], ["Richt\u00e1rik", "Peter", ""], ["Yuan", "Yang", ""]]}, {"id": "1512.09132", "submitter": "Julian Dibbelt", "authors": "Moritz Baum, Julian Dibbelt, Thomas Pajor, Dorothea Wagner", "title": "Dynamic Time-Dependent Route Planning in Road Networks with User\n  Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been tremendous progress in algorithmic methods for computing\ndriving directions on road networks. Most of that work focuses on\ntime-independent route planning, where it is assumed that the cost on each arc\nis constant per query. In practice, the current traffic situation significantly\ninfluences the travel time on large parts of the road network, and it changes\nover the day. One can distinguish between traffic congestion that can be\npredicted using historical traffic data, and congestion due to unpredictable\nevents, e.g., accidents. In this work, we study the \\emph{dynamic and\ntime-dependent} route planning problem, which takes both prediction (based on\nhistorical data) and live traffic into account. To this end, we propose a\npractical algorithm that, while robust to user preferences, is able to\nintegrate global changes of the time-dependent metric~(e.g., due to traffic\nupdates or user restrictions) faster than previous approaches, while allowing\nsubsequent queries that enable interactive applications.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 20:58:03 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Baum", "Moritz", ""], ["Dibbelt", "Julian", ""], ["Pajor", "Thomas", ""], ["Wagner", "Dorothea", ""]]}, {"id": "1512.09170", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman, Cristobal Guzman, Santosh Vempala", "title": "Statistical Query Algorithms for Mean Vector Estimation and Stochastic\n  Convex Optimization", "comments": "Substantial revision. To appear in SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic convex optimization, where the objective is the expectation of a\nrandom convex function, is an important and widely used method with numerous\napplications in machine learning, statistics, operations research and other\nareas. We study the complexity of stochastic convex optimization given only\nstatistical query (SQ) access to the objective function. We show that\nwell-known and popular first-order iterative methods can be implemented using\nonly statistical queries. For many cases of interest we derive nearly matching\nupper and lower bounds on the estimation (sample) complexity including linear\noptimization in the most general setting. We then present several consequences\nfor machine learning, differential privacy and proving concrete lower bounds on\nthe power of convex optimization based methods.\n  The key ingredient of our work is SQ algorithms and lower bounds for\nestimating the mean vector of a distribution over vectors supported on a convex\nbody in $\\mathbb{R}^d$. This natural problem has not been previously studied\nand we show that our solutions can be used to get substantially improved SQ\nversions of Perceptron and other online algorithms for learning halfspaces.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 22:29:23 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 01:49:05 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Feldman", "Vitaly", ""], ["Guzman", "Cristobal", ""], ["Vempala", "Santosh", ""]]}, {"id": "1512.09227", "submitter": "Zemin Zhang", "authors": "Zemin Zhang and Shuchin Aeron", "title": "Denoising and Completion of 3D Data via Multidimensional Dictionary\n  Learning", "comments": "9 pages, submitted to Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new dictionary learning algorithm for multidimensional data\nis proposed. Unlike most conventional dictionary learning methods which are\nderived for dealing with vectors or matrices, our algorithm, named KTSVD,\nlearns a multidimensional dictionary directly via a novel algebraic approach\nfor tensor factorization as proposed in [3, 12, 13]. Using this approach one\ncan define a tensor-SVD and we propose to extend K-SVD algorithm used for 1-D\ndata to a K-TSVD algorithm for handling 2-D and 3-D data. Our algorithm, based\non the idea of sparse coding (using group-sparsity over multidimensional\ncoefficient vectors), alternates between estimating a compact representation\nand dictionary learning. We analyze our KTSVD algorithm and demonstrate its\nresult on video completion and multispectral image denoising.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 06:37:54 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Zhang", "Zemin", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1512.09236", "submitter": "Katarzyna Paluch", "authors": "Szymon Dudycz and Jan Marcinkowski and Katarzyna Paluch and Bartosz\n  Rybicki", "title": "A $4/5$ - Approximation Algorithm for the Maximum Traveling Salesman\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the maximum traveling salesman problem (Max TSP) we are given a complete\nundirected graph with nonnegative weights on the edges and we wish to compute a\ntraveling salesman tour of maximum weight. We present a fast combinatorial\n$\\frac 45$ - approximation algorithm for Max TSP. The previous best\napproximation for this problem was $\\frac 79$. The new algorithm is based on a\nnovel technique of eliminating difficult subgraphs via half-edges, a new method\nof edge coloring and a technique of exchanging edges. A half-edge of edge\n$e=(u,v)$ is informally speaking \"a half of $e$ containing either $u$ or $v$\".\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 07:34:37 GMT"}, {"version": "v2", "created": "Sun, 20 Mar 2016 22:10:19 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Dudycz", "Szymon", ""], ["Marcinkowski", "Jan", ""], ["Paluch", "Katarzyna", ""], ["Rybicki", "Bartosz", ""]]}, {"id": "1512.09349", "submitter": "Jonathan S Turner", "authors": "Jonathan Turner", "title": "Faster Maximium Priority Matchings in Bipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": "WUCSE-2015-08", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A maximum priority matching is a matching in an undirected graph that\nmaximizes a priority score defined with respect to given vertex priorities. An\nearlier paper showed how to find maximum priority matchings in unweighted\ngraphs. This paper describes an algorithm for bipartite graphs that is faster\nwhen the number of distinct priority classes is limited. For graphs with $k$\ndistinct priority classes it runs in $O(kmn^{1/2})$ time, where $n$ is the\nnumber of vertices in the graph and $m$ is the number of edges.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 19:24:42 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Turner", "Jonathan", ""]]}, {"id": "1512.09358", "submitter": "Wouter Kuijper", "authors": "Wouter Kuijper", "title": "Geometric Memory Management", "comments": "This is a draft version of the report, future updates are planned", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we discuss the concepts of geometric memory align- ment,\ngeometric memory allocation and geometric memory mapping. We introduce block\ntrees as an efficient data structure for representing geo- metrically aligned\nblock allocation states. We introduce niche maps as an efficient means to find\nthe right place to allocate a chunk of a given size whilst maintaining good\npacking and avoiding fragmentation. We intro- duce ledging as a process to deal\nwith chunk sizes that are not a power of two. We discuss using block trees for\nmemory mapping in the context of virtual memory management. We show how ledging\ncan also be applied at the level of virtual memory in order to create fixed\nsize virtual mem- ory spaces. Finally we discuss implementation of both\ngeometric memory allocation as well as geometric memory mapping in hardware.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 20:11:46 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 14:57:13 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2016 15:51:11 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Kuijper", "Wouter", ""]]}]