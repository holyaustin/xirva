[{"id": "1203.0120", "submitter": "Olivia Saierli", "authors": "Mita Pal, Soubhik Chakraborty and N.C. Mahanti", "title": "How does the Shift-insertion sort behave when the sorting elements\n  follow a Normal distribution?", "comments": "6 pages", "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series VIII/2 (2010), 93-98", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper examines the behavior of Shift-insertion sort (insertion\nsort with shifting) for normal distribution inputs and is in continuation of\nour earlier work on this new algorithm for discrete distribution inputs,\nnamely, negative binomial. Shift insertion sort is found more sensitive for\nmain effects but not for all interaction effects compared to conventional\ninsertion sort.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 08:55:06 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Pal", "Mita", ""], ["Chakraborty", "Soubhik", ""], ["Mahanti", "N. C.", ""]]}, {"id": "1203.0224", "submitter": "Michael Dinitz", "authors": "Michael Dinitz and Guy Kortsarz and Ran Raz", "title": "Label Cover instances with large girth and the hardness of approximating\n  basic k-spanner", "comments": "16 pages, revised to add a reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the well-known Label Cover problem under the additional requirement\nthat problem instances have large girth. We show that if the girth is some $k$,\nthe problem is roughly $2^{\\log^{1-\\epsilon} n/k}$ hard to approximate for all\nconstant $\\epsilon > 0$. A similar theorem was claimed by Elkin and Peleg\n[ICALP 2000], but their proof was later found to have a fundamental error. We\nuse the new proof to show inapproximability for the basic $k$-spanner problem,\nwhich is both the simplest problem in graph spanners and one of the few for\nwhich super-logarithmic hardness was not known. Assuming $NP \\not\\subseteq\nBPTIME(2^{polylog(n)})$, we show that for every $k \\geq 3$ and every constant\n$\\epsilon > 0$ it is hard to approximate the basic $k$-spanner problem within a\nfactor better than $2^{(\\log^{1-\\epsilon} n) / k}$ (for large enough $n$). A\nsimilar hardness for basic $k$-spanner was claimed by Elkin and Peleg [ICALP\n2000], but the error in their analysis of Label Cover made this proof fail as\nwell. Thus for the problem of Label Cover with large girth we give the first\nnon-trivial lower bound. For the basic $k$-spanner problem we improve the\nprevious best lower bound of $\\Omega(\\log n)/k$ by Kortsarz [Algorithmica\n1998]. Our main technique is subsampling the edges of 2-query PCPs, which\nallows us to reduce the degree of a PCP to be essentially equal to the\nsoundness desired. This turns out to be enough to essentially guarantee large\ngirth.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 15:58:01 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2012 13:24:26 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Dinitz", "Michael", ""], ["Kortsarz", "Guy", ""], ["Raz", "Ran", ""]]}, {"id": "1203.0259", "submitter": "Boris Alexeev", "authors": "Boris Alexeev and M. Brian Jacokes", "title": "A rearrangement step with potential uses in priority queues", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link-based data structures, such as linked lists and binary search trees,\nhave many well-known rearrangement steps allowing for efficient implementations\nof insertion, deletion, and other operations. We describe a rearrangement\nprimitive designed for link-based, heap-ordered priority queues in the\ncomparison model, such as those similar to Fibonacci heaps or binomial heaps.\n  In its most basic form, the primitive rearranges a collection of heap-ordered\nperfect binary trees. Doing so offers a data structure control on the number of\ntrees involved in such a collection, in particular keeping this number\nlogarithmic in the number of elements. The rearrangement step is free from an\namortized complexity standpoint (using an appropriate potential function).\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 18:21:14 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Alexeev", "Boris", ""], ["Jacokes", "M. Brian", ""]]}, {"id": "1203.0289", "submitter": "Mahnush Movahedi", "authors": "Varsha Dani, Valerie King, Mahnush Movahedi, Jared Saia, Mahdi Zamani", "title": "Secure Multi-Party Computation in Large Networks", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe scalable protocols for solving the secure multi-party computation\n(MPC) problem among a large number of parties. We consider both the synchronous\nand the asynchronous communication models. In the synchronous setting, our\nprotocol is secure against a static malicious adversary corrupting less than a\n$1/3$ fraction of the parties. In the asynchronous setting, we allow the\nadversary to corrupt less than a $1/8$ fraction of parties. For any\ndeterministic function that can be computed by an arithmetic circuit with $m$\ngates, both of our protocols require each party to send a number of field\nelements and perform an amount of computation that is $\\tilde{O}(m/n + \\sqrt\nn)$. We also show that our protocols provide perfect and universally-composable\nsecurity.\n  To achieve our asynchronous MPC result, we define the \\emph{threshold\ncounting problem} and present a distributed protocol to solve it in the\nasynchronous setting. This protocol is load balanced, with computation,\ncommunication and latency complexity of $O(\\log{n})$, and can also be used for\ndesigning other load-balanced applications in the asynchronous communication\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 20:44:41 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 07:26:56 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2015 01:30:01 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Dani", "Varsha", ""], ["King", "Valerie", ""], ["Movahedi", "Mahnush", ""], ["Saia", "Jared", ""], ["Zamani", "Mahdi", ""]]}, {"id": "1203.0353", "submitter": "Aaron Roth", "authors": "Aaron Roth and Grant Schoenebeck", "title": "Conducting Truthful Surveys, Cheaply", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of conducting a survey with the goal of obtaining an\nunbiased estimator of some population statistic when individuals have unknown\ncosts (drawn from a known prior) for participating in the survey. Individuals\nmust be compensated for their participation and are strategic agents, and so\nthe payment scheme must incentivize truthful behavior. We derive optimal\ntruthful mechanisms for this problem for the two goals of minimizing the\nvariance of the estimator given a fixed budget, and minimizing the expected\ncost of the survey given a fixed variance goal.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 02:58:04 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Roth", "Aaron", ""], ["Schoenebeck", "Grant", ""]]}, {"id": "1203.0536", "submitter": "Olga Goussevskaia", "authors": "Olga Goussevskaia, Magn\\'us M. Halld\\'orsson and Roger Wattenhofer", "title": "Algorithms for Wireless Capacity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address two basic questions in wireless communication:\nFirst, how long does it take to schedule an arbitrary set of communication\nrequests? Second, given a set of communication requests, how many of them can\nbe scheduled concurrently? Our results are derived in an interference model\nwith geometric path loss and consist of efficient algorithms that find a\nconstant approximation for the second problem and a logarithmic approximation\nfor the first problem. In addition, we analyze some important properties of the\ninterference model and show that it is robust to various factors that can\ninfluence the signal attenuation. More specifically, we prove that as long as\nsuch influences on the signal attenuation are constant, they affect the\ncapacity only by a constant factor.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 17:57:20 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Goussevskaia", "Olga", ""], ["Halld\u00f3rsson", "Magn\u00fas M.", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1203.0543", "submitter": "Michael Kallitsis", "authors": "Michalis Kallitsis, Stilian Stoev, George Michailidis", "title": "Efficient Approximation Algorithms for Optimal Large-scale Network\n  Monitoring", "comments": "Paper withdrawn since the official journal paper is now available.\n  arXiv admin note: substantial text overlap with arXiv:1108.3048", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing amount of applications that generate vast amount of data in short\ntime scales render the problem of partial monitoring, coupled with prediction,\na rather fundamental one. We study the aforementioned canonical problem under\nthe context of large-scale monitoring of communication networks. We consider\nthe problem of selecting the \"best\" subset of links so as to optimally predict\nthe quantity of interest at the remaining ones. This is a well know NP-hard\nproblem, and algorithms seeking the exact solution are prohibitively expensive.\nWe present a number of approximation algorithms that: 1) their computational\ncomplexity gains a significant improvement over existing greedy algorithms; 2)\nexploit the geometry of principal component analysis, which also helps us\nestablish theoretical bounds on the prediction error; 3) are amenable for\nrandomized implementation and execution in parallel or distributed fashion, a\nprocess that often yields the exact solution. The new algorithms are\ndemonstrated and evaluated using real-world network data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 18:38:59 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2012 18:06:10 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2013 20:12:22 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Kallitsis", "Michalis", ""], ["Stoev", "Stilian", ""], ["Michailidis", "George", ""]]}, {"id": "1203.0594", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman", "title": "Learning DNF Expressions from Fourier Spectrum", "comments": "Appears in Conference on Learning Theory (COLT) 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its introduction by Valiant in 1984, PAC learning of DNF expressions\nremains one of the central problems in learning theory. We consider this\nproblem in the setting where the underlying distribution is uniform, or more\ngenerally, a product distribution. Kalai, Samorodnitsky and Teng (2009) showed\nthat in this setting a DNF expression can be efficiently approximated from its\n\"heavy\" low-degree Fourier coefficients alone. This is in contrast to previous\napproaches where boosting was used and thus Fourier coefficients of the target\nfunction modified by various distributions were needed. This property is\ncrucial for learning of DNF expressions over smoothed product distributions, a\nlearning model introduced by Kalai et al. (2009) and inspired by the seminal\nsmoothed analysis model of Spielman and Teng (2001).\n  We introduce a new approach to learning (or approximating) a polynomial\nthreshold functions which is based on creating a function with range [-1,1]\nthat approximately agrees with the unknown function on low-degree Fourier\ncoefficients. We then describe conditions under which this is sufficient for\nlearning polynomial threshold functions. Our approach yields a new, simple\nalgorithm for approximating any polynomial-size DNF expression from its \"heavy\"\nlow-degree Fourier coefficients alone. Our algorithm greatly simplifies the\nproof of learnability of DNF expressions over smoothed product distributions.\nWe also describe an application of our algorithm to learning monotone DNF\nexpressions over product distributions. Building on the work of Servedio\n(2001), we give an algorithm that runs in time $\\poly((s \\cdot\n\\log{(s/\\eps)})^{\\log{(s/\\eps)}}, n)$, where $s$ is the size of the target DNF\nexpression and $\\eps$ is the accuracy. This improves on $\\poly((s \\cdot\n\\log{(ns/\\eps)})^{\\log{(s/\\eps)} \\cdot \\log{(1/\\eps)}}, n)$ bound of Servedio\n(2001).\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2012 00:43:08 GMT"}, {"version": "v2", "created": "Fri, 4 May 2012 03:47:56 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2013 05:14:46 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Feldman", "Vitaly", ""]]}, {"id": "1203.0786", "submitter": "Michael Mahoney", "authors": "Michael W. Mahoney", "title": "Approximate Computation and Implicit Regularization for Very Large-scale\n  Data Analysis", "comments": "To appear in the Proceedings of the 2012 ACM Symposium on Principles\n  of Database Systems (PODS 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database theory and database practice are typically the domain of computer\nscientists who adopt what may be termed an algorithmic perspective on their\ndata. This perspective is very different than the more statistical perspective\nadopted by statisticians, scientific computers, machine learners, and other who\nwork on what may be broadly termed statistical data analysis. In this article,\nI will address fundamental aspects of this algorithmic-statistical disconnect,\nwith an eye to bridging the gap between these two very different approaches. A\nconcept that lies at the heart of this disconnect is that of statistical\nregularization, a notion that has to do with how robust is the output of an\nalgorithm to the noise properties of the input data. Although it is nearly\ncompletely absent from computer science, which historically has taken the input\ndata as given and modeled algorithms discretely, regularization in one form or\nanother is central to nearly every application domain that applies algorithms\nto noisy data. By using several case studies, I will illustrate, both\ntheoretically and empirically, the nonobvious fact that approximate\ncomputation, in and of itself, can implicitly lead to statistical\nregularization. This and other recent work suggests that, by exploiting in a\nmore principled way the statistical properties implicit in worst-case\nalgorithms, one can in many cases satisfy the bicriteria of having algorithms\nthat are scalable to very large-scale databases and that also have good\ninferential or predictive properties.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2012 23:29:06 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Mahoney", "Michael W.", ""]]}, {"id": "1203.0833", "submitter": "Daniel Lokshtanov", "authors": "Daniel Lokshtanov and N. S. Narayanaswamy and Venkatesh Raman and M.\n  S. Ramanujan and Saket Saurabh", "title": "Faster Parameterized Algorithms using Linear Programming", "comments": "A preliminary version of this paper appears in the proceedings of\n  STACS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the parameterized complexity of Vertex Cover parameterized by\nthe difference between the size of the optimal solution and the value of the\nlinear programming (LP) relaxation of the problem. By carefully analyzing the\nchange in the LP value in the branching steps, we argue that combining\npreviously known preprocessing rules with the most straightforward branching\nalgorithm yields an $O^*((2.618)^k)$ algorithm for the problem. Here $k$ is the\nexcess of the vertex cover size over the LP optimum, and we write $O^*(f(k))$\nfor a time complexity of the form $O(f(k)n^{O(1)})$, where $f (k)$ grows\nexponentially with $k$. We proceed to show that a more sophisticated branching\nalgorithm achieves a runtime of $O^*(2.3146^k)$.\n  Following this, using known and new reductions, we give $O^*(2.3146^k)$\nalgorithms for the parameterized versions of Above Guarantee Vertex Cover, Odd\nCycle Transversal, Split Vertex Deletion and Almost 2-SAT, and an\n$O^*(1.5214^k)$ algorithm for Ko\\\"nig Vertex Deletion, Vertex Cover Param by\nOCT and Vertex Cover Param by KVD. These algorithms significantly improve the\nbest known bounds for these problems. The most notable improvement is the new\nbound for Odd Cycle Transversal - this is the first algorithm which beats the\ndependence on $k$ of the seminal $O^*(3^k)$ algorithm of Reed, Smith and Vetta.\nFinally, using our algorithm, we obtain a kernel for the standard\nparameterization of Vertex Cover with at most $2k - c \\log k$ vertices. Our\nkernel is simpler than previously known kernels achieving the same size bound.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 09:00:23 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2012 20:45:02 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Narayanaswamy", "N. S.", ""], ["Raman", "Venkatesh", ""], ["Ramanujan", "M. S.", ""], ["Saurabh", "Saket", ""]]}, {"id": "1203.1017", "submitter": "Dimitrios Diochnos", "authors": "Dimitrios I. Diochnos, Ioannis Z. Emiris, Elias P. Tsigaridas", "title": "On the asymptotic and practical complexity of solving bivariate systems\n  over the reals", "comments": "17 pages, 4 algorithms, 1 table, and 1 figure with 2 sub-figures", "journal-ref": "J. Symb. Comput. 44(7): 818-835 (2009)", "doi": "10.1016/j.jsc.2008.04.009", "report-no": null, "categories": "cs.SC cs.DS cs.MS cs.NA math.AG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with exact real solving of well-constrained,\nbivariate polynomial systems. The main problem is to isolate all common real\nroots in rational rectangles, and to determine their intersection\nmultiplicities. We present three algorithms and analyze their asymptotic bit\ncomplexity, obtaining a bound of $\\sOB(N^{14})$ for the purely projection-based\nmethod, and $\\sOB(N^{12})$ for two subresultant-based methods: this notation\nignores polylogarithmic factors, where $N$ bounds the degree and the bitsize of\nthe polynomials. The previous record bound was $\\sOB(N^{14})$.\n  Our main tool is signed subresultant sequences. We exploit recent advances on\nthe complexity of univariate root isolation, and extend them to sign evaluation\nof bivariate polynomials over two algebraic numbers, and real root counting for\npolynomials over an extension field. Our algorithms apply to the problem of\nsimultaneous inequalities; they also compute the topology of real plane\nalgebraic curves in $\\sOB(N^{12})$, whereas the previous bound was\n$\\sOB(N^{14})$.\n  All algorithms have been implemented in MAPLE, in conjunction with numeric\nfiltering. We compare them against FGB/RS, system solvers from SYNAPS, and\nMAPLE libraries INSULATE and TOP, which compute curve topology. Our software is\namong the most robust, and its runtimes are comparable, or within a small\nconstant factor, with respect to the C/C++ libraries.\n  Key words: real solving, polynomial systems, complexity, MAPLE software\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 19:39:05 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Diochnos", "Dimitrios I.", ""], ["Emiris", "Ioannis Z.", ""], ["Tsigaridas", "Elias P.", ""]]}, {"id": "1203.1080", "submitter": "Wei Yu", "authors": "Shiteng Chen, Elad Verbin, Wei Yu", "title": "Data Structure Lower Bounds on Random Access to Grammar-Compressed\n  Strings", "comments": "submitted to ICALP 2012, with strengthened results included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the problem of building a static data structure\nthat represents a string s using space close to its compressed size, and allows\nfast access to individual characters of s. This type of structures was\ninvestigated by the recent paper of Bille et al. Let n be the size of a\ncontext-free grammar that derives a unique string s of length L. (Note that L\nmight be exponential in n.) Bille et al. showed a data structure that uses\nspace O(n) and allows to query for the i-th character of s using running time\nO(log L). Their data structure works on a word RAM with a word size of logL\nbits. Here we prove that for such data structures, if the space is poly(n),\nthen the query time must be at least (log L)^{1-\\epsilon}/log S where S is the\nspace used, for any constant eps>0. As a function of n, our lower bound is\n\\Omega(n^{1/2-\\epsilon}). Our proof holds in the cell-probe model with a word\nsize of log L bits, so in particular it holds in the word RAM model. We show\nthat no lower bound significantly better than n^{1/2-\\epsilon} can be achieved\nin the cell-probe model, since there is a data structure in the cell-probe\nmodel that uses O(n) space and achieves O(\\sqrt{n log n}) query time. The \"bad\"\nsetting of parameters occurs roughly when L=2^{\\sqrt{n}}. We also prove a lower\nbound for the case of not-as-compressible strings, where, say,\nL=n^{1+\\epsilon}. For this case, we prove that if the space is n polylog(n),\nthen the query time must be at least \\Omega(log n/loglog n).\n  The proof works by reduction to communication complexity, namely to the LSD\nproblem, recently employed by Patrascu and others. We prove lower bounds also\nfor the case of LZ-compression and Burrows-Wheeler (BWT) compression. All of\nour lower bounds hold even when the strings are over an alphabet of size 2 and\nhold even for randomized data structures with 2-sided error.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2012 00:52:14 GMT"}, {"version": "v2", "created": "Thu, 3 May 2012 15:45:08 GMT"}], "update_date": "2012-05-04", "authors_parsed": [["Chen", "Shiteng", ""], ["Verbin", "Elad", ""], ["Yu", "Wei", ""]]}, {"id": "1203.1226", "submitter": "Thomas Kesselheim", "authors": "Thomas Kesselheim", "title": "Dynamic Packet Scheduling in Wireless Networks", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider protocols that serve communication requests arising over time in\na wireless network that is subject to interference. Unlike previous approaches,\nwe take the geometry of the network and power control into account, both\nallowing to increase the network's performance significantly. We introduce a\nstochastic and an adversarial model to bound the packet injection. Although\ntaken as the primary motivation, this approach is not only suitable for models\nbased on the signal-to-interference-plus-noise ratio (SINR). It also covers\nvirtually all other common interference models, for example the multiple-access\nchannel, the radio-network model, the protocol model, and distance-2 matching.\nPacket-routing networks allowing each edge or each node to transmit or receive\none packet at a time can be modeled as well.\n  Starting from algorithms for the respective scheduling problem with static\ntransmission requests, we build distributed stable protocols. This is more\ninvolved than in previous, similar approaches because the algorithms we\nconsider do not necessarily scale linearly when scaling the input instance. We\ncan guarantee a throughput that is as large as the one of the original static\nalgorithm. In particular, for SINR models the competitive ratios of the\nprotocol in comparison to optimal ones in the respective model are between\nconstant and O(log^2 m) for a network of size m.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2012 15:23:41 GMT"}], "update_date": "2012-03-07", "authors_parsed": [["Kesselheim", "Thomas", ""]]}, {"id": "1203.1250", "submitter": "Florentina Pintea", "authors": "Olusegun Folorunso, Olufunke R. Vincent, Oluwatimilehin Salako", "title": "An Exploratory Study of Critical Factors Affecting the Efficiency of\n  Sorting Techniques (Shell, Heap and Treap)", "comments": "10 pages", "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series VIII / 1 (2010), 163-172", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency of sorting techniques has a significant impact on the overall\nefficiency of a program. The efficiency of Shell, Heap and Treap sorting\ntechniques in terms of both running time and memory usage was studied,\nexperiments conducted and results subjected to factor analysis by SPSS. The\nstudy revealed the main factor affecting these sorting techniques was time\ntaken to sort.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2012 21:09:42 GMT"}], "update_date": "2012-03-07", "authors_parsed": [["Folorunso", "Olusegun", ""], ["Vincent", "Olufunke R.", ""], ["Salako", "Oluwatimilehin", ""]]}, {"id": "1203.1692", "submitter": "Nicolas Bock", "authors": "Nicolas Bock, Matt Challacombe", "title": "An Optimized Sparse Approximate Matrix Multiply for Matrices with Decay", "comments": null, "journal-ref": null, "doi": null, "report-no": "LA-UR 11-06091", "categories": "cs.NA cs.DS cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an optimized single-precision implementation of the Sparse\nApproximate Matrix Multiply (\\SpAMM{}) [M. Challacombe and N. Bock, arXiv {\\bf\n1011.3534} (2010)], a fast algorithm for matrix-matrix multiplication for\nmatrices with decay that achieves an $\\mathcal{O} (n \\log n)$ computational\ncomplexity with respect to matrix dimension $n$. We find that the max norm of\nthe error achieved with a \\SpAMM{} tolerance below $2 \\times 10^{-8}$ is lower\nthan that of the single-precision {\\tt SGEMM} for dense quantum chemical\nmatrices, while outperforming {\\tt SGEMM} with a cross-over already for small\nmatrices ($n \\sim 1000$). Relative to naive implementations of \\SpAMM{} using\nIntel's Math Kernel Library ({\\tt MKL}) or AMD's Core Math Library ({\\tt\nACML}), our optimized version is found to be significantly faster. Detailed\nperformance comparisons are made for quantum chemical matrices with differently\nstructured sub-blocks. Finally, we discuss the potential of improved hardware\nprefetch to yield 2--3x speedups.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 05:33:01 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2012 22:42:22 GMT"}, {"version": "v3", "created": "Tue, 20 Mar 2012 21:49:56 GMT"}, {"version": "v4", "created": "Fri, 31 Aug 2012 22:30:22 GMT"}, {"version": "v5", "created": "Tue, 4 Sep 2012 18:10:32 GMT"}], "update_date": "2012-09-05", "authors_parsed": [["Bock", "Nicolas", ""], ["Challacombe", "Matt", ""]]}, {"id": "1203.1754", "submitter": "Marek Cygan", "authors": "Marek Cygan and Marcin Pilipczuk and Micha{\\l} Pilipczuk", "title": "Known algorithms for EDGE CLIQUE COVER are probably optimal", "comments": "To appear in SODA 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the EDGE CLIQUE COVER (ECC) problem, given a graph G and an integer k, we\nask whether the edges of G can be covered with k complete subgraphs of G or,\nequivalently, whether G admits an intersection model on k-element universe.\nGramm et al. [JEA 2008] have shown a set of simple rules that reduce the number\nof vertices of G to 2^k, and no algorithm is known with significantly better\nrunning time bound than a brute-force search on this reduced instance. In this\npaper we show that the approach of Gramm et al. is essentially optimal: we\npresent a polynomial time algorithm that reduces an arbitrary 3-CNF-SAT formula\nwith n variables and m clauses to an equivalent ECC instance (G,k) with k =\nO(log n) and |V(G)| = O(n + m). Consequently, there is no 2^{2^{o(k)}}poly(n)\ntime algorithm for the ECC problem, unless the Exponential Time Hypothesis\nfails. To the best of our knowledge, these are the first results for a natural,\nfixed-parameter tractable problem, and proving that a doubly-exponential\ndependency on the parameter is essentially necessary.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 11:19:09 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2012 08:51:46 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Cygan", "Marek", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1203.1830", "submitter": "Soubhik Chakraborty", "authors": "Niraj Kumar Singh, Mita Pal and Soubhik Chakraborty", "title": "Partition Sort Revisited: Reconfirming the Robustness in Average Case\n  and much more!", "comments": "8 pages", "journal-ref": "International Journal of Computer Science, Engineering and\n  Applications,Vol.2, No.1, Feb 2012, p 23-30", "doi": "10.5121/ijcsea.2012.2103", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our previous work there was some indication that Partition Sort could be\nhaving a more robust average case O(nlogn) complexity than the popular Quick\nSort. In our first study in this paper, we reconfirm this through computer\nexperiments for inputs from Cauchy distribution for which expectation\ntheoretically does not exist. Additionally, the algorithm is found to be\nsensitive to parameters of the input probability distribution demanding further\ninvestigation on parameterized complexity. The results on this algorithm for\nBinomial inputs in our second study are very encouraging in that direction.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 15:42:04 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2012 05:56:50 GMT"}], "update_date": "2012-03-28", "authors_parsed": [["Singh", "Niraj Kumar", ""], ["Pal", "Mita", ""], ["Chakraborty", "Soubhik", ""]]}, {"id": "1203.1940", "submitter": "Danupon Nanongkai", "authors": "Parinya Chalermsook, Shiva Kintali, Richard Lipton and Danupon\n  Nanongkai", "title": "Graph Pricing Problem on Bounded Treewidth, Bounded Genus and k-partite\n  graphs", "comments": "Preprint of the paper to appear in Chicago Journal of Theoretical\n  Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following problem. A seller has infinite copies of $n$ products\nrepresented by nodes in a graph. There are $m$ consumers, each has a budget and\nwants to buy two products. Consumers are represented by weighted edges. Given\nthe prices of products, each consumer will buy both products she wants, at the\ngiven price, if she can afford to. Our objective is to help the seller price\nthe products to maximize her profit.\n  This problem is called {\\em graph vertex pricing} ({\\sf GVP}) problem and has\nresisted several recent attempts despite its current simple solution. This\nmotivates the study of this problem on special classes of graphs. In this\npaper, we study this problem on a large class of graphs such as graphs with\nbounded treewidth, bounded genus and $k$-partite graphs.\n  We show that there exists an {\\sf FPTAS} for {\\sf GVP} on graphs with bounded\ntreewidth. This result is also extended to an {\\sf FPTAS} for the more general\n{\\em single-minded pricing} problem. On bounded genus graphs we present a {\\sf\nPTAS} and show that {\\sf GVP} is {\\sf NP}-hard even on planar graphs.\n  We study the Sherali-Adams hierarchy applied to a natural Integer Program\nformulation that $(1+\\epsilon)$-approximates the optimal solution of {\\sf GVP}.\nSherali-Adams hierarchy has gained much interest recently as a possible\napproach to develop new approximation algorithms. We show that, when the input\ngraph has bounded treewidth or bounded genus, applying a constant number of\nrounds of Sherali-Adams hierarchy makes the integrality gap of this natural\n{\\sf LP} arbitrarily small, thus giving a $(1+\\epsilon)$-approximate solution\nto the original {\\sf GVP} instance.\n  On $k$-partite graphs, we present a constant-factor approximation algorithm.\nWe further improve the approximation factors for paths, cycles and graphs with\ndegree at most three.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 21:42:51 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 07:17:50 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Chalermsook", "Parinya", ""], ["Kintali", "Shiva", ""], ["Lipton", "Richard", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1203.1952", "submitter": "Hung Ngo", "authors": "Hung Q. Ngo and Ely Porat and Christopher R\\'e and Atri Rudra", "title": "Worst-case Optimal Join Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient join processing is one of the most fundamental and well-studied\ntasks in database research. In this work, we examine algorithms for natural\njoin queries over many relations and describe a novel algorithm to process\nthese queries optimally in terms of worst-case data complexity. Our result\nbuilds on recent work by Atserias, Grohe, and Marx, who gave bounds on the size\nof a full conjunctive query in terms of the sizes of the individual relations\nin the body of the query. These bounds, however, are not constructive: they\nrely on Shearer's entropy inequality which is information-theoretic. Thus, the\nprevious results leave open the question of whether there exist algorithms\nwhose running time achieve these optimal bounds. An answer to this question may\nbe interesting to database practice, as it is known that any algorithm based on\nthe traditional select-project-join style plans typically employed in an RDBMS\nare asymptotically slower than the optimal for some queries. We construct an\nalgorithm whose running time is worst-case optimal for all natural join\nqueries. Our result may be of independent interest, as our algorithm also\nyields a constructive proof of the general fractional cover bound by Atserias,\nGrohe, and Marx without using Shearer's inequality. This bound implies two\nfamous inequalities in geometry: the Loomis-Whitney inequality and the\nBollob\\'as-Thomason inequality. Hence, our results algorithmically prove these\ninequalities as well. Finally, we discuss how our algorithm can be used to\ncompute a relaxed notion of joins.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 22:35:29 GMT"}], "update_date": "2012-03-12", "authors_parsed": [["Ngo", "Hung Q.", ""], ["Porat", "Ely", ""], ["R\u00e9", "Christopher", ""], ["Rudra", "Atri", ""]]}, {"id": "1203.2295", "submitter": "Eric Chi", "authors": "Eric C. Chi and Kenneth Lange", "title": "Techniques for Solving Sudoku Puzzles", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving Sudoku puzzles is one of the most popular pastimes in the world.\nPuzzles range in difficulty from easy to very challenging; the hardest puzzles\ntend to have the most empty cells. The current paper explains and compares\nthree algorithms for solving Sudoku puzzles. Backtracking, simulated annealing,\nand alternating projections are generic methods for attacking combinatorial\noptimization problems. Our results favor backtracking. It infallibly solves a\nSudoku puzzle or deduces that a unique solution does not exist. However,\nbacktracking does not scale well in high-dimensional combinatorial\noptimization. Hence, it is useful to expose students in the mathematical\nsciences to the other two solution techniques in a concrete setting. Simulated\nannealing shares a common structure with MCMC (Markov chain Monte Carlo) and\nenjoys wide applicability. The method of alternating projections solves the\nfeasibility problem in convex programming. Converting a discrete optimization\nproblem into a continuous optimization problem opens up the possibility of\nhandling combinatorial problems of much higher dimensionality.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2012 00:12:05 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2012 16:48:45 GMT"}, {"version": "v3", "created": "Thu, 16 May 2013 17:55:03 GMT"}], "update_date": "2013-05-17", "authors_parsed": [["Chi", "Eric C.", ""], ["Lange", "Kenneth", ""]]}, {"id": "1203.2414", "submitter": "Einollah Pira", "authors": "Einollah Pira", "title": "An Optimal Algorithm for Conflict-Free Coloring for Tree of Rings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  An optimal algorithm is presented about Conflict-Free Coloring for connected\nsubgraphs of tree of rings. Suppose the number of the rings in the tree is |T|\nand the maximum length of rings is |R|. A presented algorithm in [1] for a Tree\nof rings used O(log|T|.log|R|) colors but this algorithm uses O(log|T|+log|R|)\ncolors. The coloring earned by this algorithm has the unique-min property, that\nis, the unique color is also minimum.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 07:20:56 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Pira", "Einollah", ""]]}, {"id": "1203.2437", "submitter": "Henning Ulfarsson", "authors": "Anders Claesson and Henning \\'Ulfarsson", "title": "Sorting and preimages of pattern classes", "comments": "13 pages, 5 figures, to appear at FPSAC 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an algorithm to determine when a sorting operation, such as\nstack-sort or bubble-sort, outputs a given pattern. The algorithm provides a\nnew proof of the description of West-2-stack-sortable permutations, that is\npermutations that are completely sorted when passed twice through a stack, in\nterms of patterns. We also solve the long-standing problem of describing\nWest-3-stack-sortable permutations. This requires a new type of generalized\npermutation pattern we call a decorated pattern.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 09:43:31 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Claesson", "Anders", ""], ["\u00dalfarsson", "Henning", ""]]}, {"id": "1203.2505", "submitter": "Debajit Sensarma", "authors": "Debajit Sensarma, Subhashis Banerjee, Krishnendu Basuli, Saptarshi\n  Naskar, Samar Sen Sarma", "title": "On an optimization technique using Binary Decision Diagram", "comments": "10 pages,5 figures,Sensarma D., Banerjee S., Basuli K., Naskar S., &\n  Sarma, S. S \"Minimizing Boolean Sum of Products Functions Using Binary\n  Decision Diagram\", Advances in Computer Science and Information Technology:\n  Computer Science and Information Technology: Second International Conference.\n  Proceedings, Vol. 86, Part III, pp 36-48, CCSIT 2012", "journal-ref": "IJCSEA, Volume 2, Number 1, pg. 73-86, February 2012", "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-level logic minimization is a central problem in logic synthesis, and has\napplications in reliability analysis and automated reasoning. This paper\nrepresents a method of minimizing Boolean sum of products function with binary\ndecision diagram and with disjoint sum of product minimization. Due to the\nsymbolic representation of cubes for large problem instances, the method is\norders of magnitude faster than previous enumerative techniques. But the\nquality of the approach largely depends on the variable ordering of the\nunderlying BDD. The application of Binary Decision Diagrams (BDDs) as an\nefficient approach for the minimization of Disjoint Sums-of-Products (DSOPs).\nDSOPs are a starting point for several applications. The use of BDDs has the\nadvantage of an implicit representation of terms. Due to this scheme the\nalgorithm is faster than techniques working on explicit representations and the\napplication to large circuits that could not be handled so far becomes\npossible. Theoretical studies on the influence of the BDDs to the search space\nare carried out. In experiments the proposed technique is compared to others.\nThe results with respect to the size of the resulting DSOP are as good or\nbetter as those of the other techniques.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 14:26:23 GMT"}], "update_date": "2012-03-29", "authors_parsed": [["Sensarma", "Debajit", ""], ["Banerjee", "Subhashis", ""], ["Basuli", "Krishnendu", ""], ["Naskar", "Saptarshi", ""], ["Sarma", "Samar Sen", ""]]}, {"id": "1203.2538", "submitter": "Kitty Meeks", "authors": "Kitty Meeks and Alexander Scott", "title": "Spanning trees and the complexity of flood-filling games", "comments": "Final typos corrected", "journal-ref": null, "doi": "10.1007/s00224-013-9482-z", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider problems related to the combinatorial game (Free-)Flood-It, in\nwhich players aim to make a coloured graph monochromatic with the minimum\npossible number of flooding operations. We show that the minimum number of\nmoves required to flood any given graph G is equal to the minimum, taken over\nall spanning trees T of G, of the number of moves required to flood T. This\nresult is then applied to give two polynomial-time algorithms for flood-filling\nproblems. Firstly, we can compute in polynomial time the minimum number of\nmoves required to flood a graph with only a polynomial number of connected\nsubgraphs. Secondly, given any coloured connected graph and a subset of the\nvertices of bounded size, the number of moves required to connect this subset\ncan be computed in polynomial time.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 16:29:42 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2012 14:11:56 GMT"}, {"version": "v3", "created": "Wed, 29 May 2013 13:43:14 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Meeks", "Kitty", ""], ["Scott", "Alexander", ""]]}, {"id": "1203.2543", "submitter": "H\\'elio Mac\\^edo Filho", "authors": "H\\'elio B. Mac\\^edo Filho, Simone Dantas, Raphael C. S. Machado, and\n  Celina M. H. de Figueiredo", "title": "Biclique-colouring verification complexity and biclique-colouring power\n  graphs", "comments": "21 pages, 19 distinct figures. An extended abstract published in:\n  Proceedings of Cologne Twente Workshop (CTW) 2012, pp. 134--138", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Biclique-colouring is a colouring of the vertices of a graph in such a way\nthat no maximal complete bipartite subgraph with at least one edge is\nmonochromatic. We show that it is coNP-complete to check whether a given\nfunction that associates a colour to each vertex is a biclique-colouring, a\nresult that justifies the search for structured classes where the\nbiclique-colouring problem could be efficiently solved. We consider\nbiclique-colouring restricted to powers of paths and powers of cycles. We\ndetermine the biclique-chromatic number of powers of paths and powers of\ncycles. The biclique-chromatic number of a power of a path P_{n}^{k} is max(2k\n+ 2 - n, 2) if n >= k + 1 and exactly n otherwise. The biclique-chromatic\nnumber of a power of a cycle C_n^k is at most 3 if n >= 2k + 2 and exactly n\notherwise; we additionally determine the powers of cycles that are\n2-biclique-colourable. All proofs are algorithmic and provide polynomial-time\nbiclique-colouring algorithms for graphs in the investigated classes.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 16:45:21 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2013 16:28:52 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Filho", "H\u00e9lio B. Mac\u00eado", ""], ["Dantas", "Simone", ""], ["Machado", "Raphael C. S.", ""], ["de Figueiredo", "Celina M. H.", ""]]}, {"id": "1203.2672", "submitter": "Dan Olteanu", "authors": "Nurzhan Bakibayev, Dan Olteanu, and Jakub Z\\'avodn\\'y", "title": "FDB: A Query Engine for Factorised Relational Databases", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorised databases are relational databases that use compact factorised\nrepresentations at the physical layer to reduce data redundancy and boost query\nperformance. This paper introduces FDB, an in-memory query engine for\nselect-project-join queries on factorised databases. Key components of FDB are\nnovel algorithms for query optimisation and evaluation that exploit the\nsuccinctness brought by data factorisation. Experiments show that for data sets\nwith many-to-many relationships FDB can outperform relational engines by orders\nof magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 23:19:09 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Bakibayev", "Nurzhan", ""], ["Olteanu", "Dan", ""], ["Z\u00e1vodn\u00fd", "Jakub", ""]]}, {"id": "1203.2801", "submitter": "Eun Jung Kim", "authors": "Eun Jung Kim and Daniel Goncalves", "title": "On Exact Algorithms for Permutation CSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Permutation Constraint Satisfaction Problem (Permutation CSP) we are\ngiven a set of variables $V$ and a set of constraints C, in which constraints\nare tuples of elements of V. The goal is to find a total ordering of the\nvariables, $\\pi\\ : V \\rightarrow [1,...,|V|]$, which satisfies as many\nconstraints as possible. A constraint $(v_1,v_2,...,v_k)$ is satisfied by an\nordering $\\pi$ when $\\pi(v_1)<\\pi(v_2)<...<\\pi(v_k)$. An instance has arity $k$\nif all the constraints involve at most $k$ elements.\n  This problem expresses a variety of permutation problems including {\\sc\nFeedback Arc Set} and {\\sc Betweenness} problems. A naive algorithm, listing\nall the $n!$ permutations, requires $2^{O(n\\log{n})}$ time. Interestingly, {\\sc\nPermutation CSP} for arity 2 or 3 can be solved by Held-Karp type algorithms in\ntime $O^*(2^n)$, but no algorithm is known for arity at least 4 with running\ntime significantly better than $2^{O(n\\log{n})}$. In this paper we resolve the\ngap by showing that {\\sc Arity 4 Permutation CSP} cannot be solved in time\n$2^{o(n\\log{n})}$ unless ETH fails.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2012 13:38:45 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Kim", "Eun Jung", ""], ["Goncalves", "Daniel", ""]]}, {"id": "1203.2822", "submitter": "Marek Szyku{\\l}a", "authors": "Andrzej Kisielewicz, Jakub Kowalski, Marek Szyku{\\l}a", "title": "A Fast Algorithm Finding the Shortest Reset Words", "comments": "COCOON 2013. The final publication is available at\n  http://link.springer.com/chapter/10.1007%2F978-3-642-38768-5_18", "journal-ref": "In Computing and Combinatorics, volume 7936 of LNCS, pages\n  182-196, 2013", "doi": "10.1007/978-3-642-38768-5_18", "report-no": null, "categories": "cs.FL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new fast algorithm finding minimal reset words for\nfinite synchronizing automata. The problem is know to be computationally hard,\nand our algorithm is exponential. Yet, it is faster than the algorithms used so\nfar and it works well in practice. The main idea is to use a bidirectional BFS\nand radix (Patricia) tries to store and compare resulted subsets. We give both\ntheoretical and practical arguments showing that the branching factor is\nreduced efficiently. As a practical test we perform an experimental study of\nthe length of the shortest reset word for random automata with $n$ states and 2\ninput letters. We follow Skvorsov and Tipikin, who have performed such a study\nusing a SAT solver and considering automata up to $n=100$ states. With our\nalgorithm we are able to consider much larger sample of automata with up to\n$n=300$ states. In particular, we obtain a new more precise estimation of the\nexpected length of the shortest reset word $\\approx 2.5\\sqrt{n-5}$.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2012 14:29:24 GMT"}, {"version": "v2", "created": "Fri, 12 Dec 2014 18:43:33 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Kisielewicz", "Andrzej", ""], ["Kowalski", "Jakub", ""], ["Szyku\u0142a", "Marek", ""]]}, {"id": "1203.2886", "submitter": "Medha Atre", "authors": "Medha Atre, Vineet Chaoji, Mohammed J. Zaki", "title": "BitPath -- Label Order Constrained Reachability Queries over Large\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": "RPI-CS 12-02", "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus on the following constrained reachability problem over\nedge-labeled graphs like RDF -- \"given source node x, destination node y, and a\nsequence of edge labels (a, b, c, d), is there a path between the two nodes\nsuch that the edge labels on the path satisfy a regular expression\n\"*a.*b.*c.*d.*\". A \"*\" before \"a\" allows any other edge label to appear on the\npath before edge \"a\". \"a.*\" forces at least one edge with label \"a\". \".*\" after\n\"a\" allows zero or more edge labels after \"a\" and before \"b\". Our query\nprocessing algorithm uses simple divide-and-conquer and greedy pruning\nprocedures to limit the search space. However, our graph indexing technique --\nbased on \"compressed bit-vectors\" -- allows indexing large graphs which\notherwise would have been infeasible. We have evaluated our approach on graphs\nwith more than 22 million edges and 6 million nodes -- much larger compared to\nthe datasets used in the contemporary work on path queries.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2012 18:11:55 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Atre", "Medha", ""], ["Chaoji", "Vineet", ""], ["Zaki", "Mohammed J.", ""]]}, {"id": "1203.3284", "submitter": "Yoshio Okamoto", "authors": "Masashi Kiyomi, Yoshio Okamoto, Toshiki Saitoh", "title": "Efficient Enumeration of the Directed Binary Perfect Phylogenies from\n  Incomplete Data", "comments": "Extended abstract to appear in 11th International Symposium on\n  Experimental Algorithms (SEA 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a character-based phylogeny reconstruction problem when an\nincomplete set of data is given. More specifically, we consider the situation\nunder the directed perfect phylogeny assumption with binary characters in which\nfor some species the states of some characters are missing. Our main object is\nto give an efficient algorithm to enumerate (or list) all perfect phylogenies\nthat can be obtained when the missing entries are completed. While a simple\nbranch-and-bound algorithm (B&B) shows a theoretically good performance, we\npropose another approach based on a zero-suppressed binary decision diagram\n(ZDD). Experimental results on randomly generated data exhibit that the ZDD\napproach outperforms B&B. We also prove that counting the number of\nphylogenetic trees consistent with a given data is #P-complete, thus providing\nan evidence that an efficient random sampling seems hard.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 07:50:06 GMT"}], "update_date": "2012-03-16", "authors_parsed": [["Kiyomi", "Masashi", ""], ["Okamoto", "Yoshio", ""], ["Saitoh", "Toshiki", ""]]}, {"id": "1203.3415", "submitter": "Luis Meira", "authors": "Luis A. A. Meira, Vinicius R. M\\'aximo, \\'Alvaro L. Fazenda, Arlindo\n  F. da Concei\\c{c}\\~ao", "title": "acc-Motif Detection Tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network motif algorithms have been a topic of research mainly after the\n2002-seminal paper from Milo \\emph{et al}, that provided motifs as a way to\nuncover the basic building blocks of most networks. In Bioinformatics, motifs\nhave been mainly applied in the field of gene regulation networks. This paper\nproposes new algorithms to exactly count isomorphic pattern motifs of sizes 3,\n4 and 5 in directed graphs. Let $G(V,E)$ be a directed graph with $m=|E|$. We\ndescribe an $O({m\\sqrt{m}})$ time complexity algorithm to count isomorphic\npatterns of size 3. In order to count isomorphic patterns of size 4, we propose\nan $O(m^2)$ algorithm. To count patterns with 5 vertices, the algorithm is\n$O(m^2n)$. The new algorithms were implemented and compared with FANMOD and\nKavosh motif detection tools. The experiments show that our algorithms are\nexpressively faster than FANMOD and Kavosh's. We also let our motif-detecting\ntool available in the Internet.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 16:54:42 GMT"}, {"version": "v2", "created": "Wed, 2 May 2012 23:42:04 GMT"}, {"version": "v3", "created": "Mon, 30 Jul 2012 19:09:09 GMT"}, {"version": "v4", "created": "Thu, 21 Mar 2013 22:25:35 GMT"}, {"version": "v5", "created": "Fri, 19 Apr 2013 03:03:37 GMT"}], "update_date": "2013-04-22", "authors_parsed": [["Meira", "Luis A. A.", ""], ["M\u00e1ximo", "Vinicius R.", ""], ["Fazenda", "\u00c1lvaro L.", ""], ["da Concei\u00e7\u00e3o", "Arlindo F.", ""]]}, {"id": "1203.3487", "submitter": "Kalev Kask", "authors": "Kalev Kask, Rina Dechter, Andrew E. Gelfand", "title": "BEEM : Bucket Elimination with External Memory", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-268-276", "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major limitation of exact inference algorithms for probabilistic graphical\nmodels is their extensive memory usage, which often puts real-world problems\nout of their reach. In this paper we show how we can extend inference\nalgorithms, particularly Bucket Elimination, a special case of cluster (join)\ntree decomposition, to utilize disk memory. We provide the underlying ideas and\nshow promising empirical results of exactly solving large problems not solvable\nbefore.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Kask", "Kalev", ""], ["Dechter", "Rina", ""], ["Gelfand", "Andrew E.", ""]]}, {"id": "1203.3501", "submitter": "Sebastian Ordyniak", "authors": "Sebastian Ordyniak, Stefan Szeider", "title": "Algorithms and Complexity Results for Exact Bayesian Structure Learning", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-401-408", "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian structure learning is the NP-hard problem of discovering a Bayesian\nnetwork that optimally represents a given set of training data. In this paper\nwe study the computational worst-case complexity of exact Bayesian structure\nlearning under graph theoretic restrictions on the super-structure. The\nsuper-structure (a concept introduced by Perrier, Imoto, and Miyano, JMLR 2008)\nis an undirected graph that contains as subgraphs the skeletons of solution\nnetworks. Our results apply to several variants of score-based Bayesian\nstructure learning where the score of a network decomposes into local scores of\nits nodes. Results: We show that exact Bayesian structure learning can be\ncarried out in non-uniform polynomial time if the super-structure has bounded\ntreewidth and in linear time if in addition the super-structure has bounded\nmaximum degree. We complement this with a number of hardness results. We show\nthat both restrictions (treewidth and degree) are essential and cannot be\ndropped without loosing uniform polynomial time tractability (subject to a\ncomplexity-theoretic assumption). Furthermore, we show that the restrictions\nremain essential if we do not search for a globally optimal network but we aim\nto improve a given network by means of at most k arc additions, arc deletions,\nor arc reversals (k-neighborhood local search).\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Ordyniak", "Sebastian", ""], ["Szeider", "Stefan", ""]]}, {"id": "1203.3502", "submitter": "Thorsten J. Ottosen", "authors": "Thorsten J. Ottosen, Finn Verner Jensen", "title": "The Cost of Troubleshooting Cost Clusters with Inside Information", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-409-416", "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision theoretical troubleshooting is about minimizing the expected cost of\nsolving a certain problem like repairing a complicated man-made device. In this\npaper we consider situations where you have to take apart some of the device to\nget access to certain clusters and actions. Specifically, we investigate\ntroubleshooting with independent actions in a tree of clusters where actions\ninside a cluster cannot be performed before the cluster is opened. The problem\nis non-trivial because there is a cost associated with opening and closing a\ncluster. Troubleshooting with independent actions and no clusters can be solved\nin O(n lg n) time (n being the number of actions) by the well-known \"P-over-C\"\nalgorithm due to Kadane and Simon, but an efficient and optimal algorithm for a\ntree cluster model has not yet been found. In this paper we describe a\n\"bottom-up P-over-C\" O(n lg n) time algorithm and show that it is optimal when\nthe clusters do not need to be closed to test whether the actions solved the\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Ottosen", "Thorsten J.", ""], ["Jensen", "Finn Verner", ""]]}, {"id": "1203.3578", "submitter": "Takuro Fukunaga", "authors": "Takuro Fukunaga, Zeev Nutov and R. Ravi", "title": "Iterative rounding approximation algorithms for degree-bounded\n  node-connectivity network design", "comments": "A preliminary version of this paper appeared in proceedings of the\n  53rd Annual IEEE Symposium on Foundations of Computer Science (FOCS 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding a minimum edge cost subgraph of a graph\nsatisfying both given node-connectivity requirements and degree upper bounds on\nnodes. We present an iterative rounding algorithm of the biset LP relaxation\nfor this problem. For directed graphs and $k$-out-connectivity requirements\nfrom a root, our algorithm computes a solution that is a 2-approximation on the\ncost, and the degree of each node $v$ in the solution is at most $2b(v) + O(k)$\nwhere $b(v)$ is the degree upper bound on $v$. For undirected graphs and\nelement-connectivity requirements with maximum connectivity requirement $k$,\nour algorithm computes a solution that is a $4$-approximation on the cost, and\nthe degree of each node $v$ in the solution is at most $4b(v)+O(k)$. These\nratios improve the previous $O(\\log k)$-approximation on the cost and $O(2^k\nb(v))$ approximation on the degrees. Our algorithms can be used to improve\napproximation ratios for other node-connectivity problems such as undirected\n$k$-out-connectivity, directed and undirected $k$-connectivity, and undirected\nrooted $k$-connectivity and subset $k$-connectivity.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 21:35:37 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2012 19:14:25 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2013 02:22:28 GMT"}, {"version": "v4", "created": "Mon, 10 Aug 2015 01:44:59 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Fukunaga", "Takuro", ""], ["Nutov", "Zeev", ""], ["Ravi", "R.", ""]]}, {"id": "1203.3593", "submitter": "Erik Vee", "authors": "Peiji Chen, Wenjing Ma, Srinath Mandalapu, Chandrashekhar Nagarajan,\n  Jayavel Shanmugasundaram, Sergei Vassilvitskii, Erik Vee, Manfai Yu, Jason\n  Zien", "title": "Ad Serving Using a Compact Allocation Plan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large fraction of online display advertising is sold via guaranteed\ncontracts: a publisher guarantees to the advertiser a certain number of user\nvisits satisfying the targeting predicates of the contract. The publisher is\nthen tasked with solving the ad serving problem - given a user visit, which of\nthe thousands of matching contracts should be displayed, so that by the\nexpiration time every contract has obtained the requisite number of user\nvisits. The challenges of the problem come from (1) the sheer size of the\nproblem being solved, with tens of thousands of contracts and billions of user\nvisits, (2) the unpredictability of user behavior, since these contracts are\nsold months ahead of time, when only a forecast of user visits is available and\n(3) the minute amount of resources available online, as an ad server must\nrespond with a matching contract in a fraction of a second.\n  We present a solution to the guaranteed delivery ad serving problem using\n{\\em compact allocation plans}. These plans, computed offline, can be\nefficiently queried by the ad server during an ad call; they are small, using\nonly O(1) space for contract; and are stateless, allowing for distributed\nserving without any central coordination. We evaluate this approach on a real\nset of user visits and guaranteed contracts and show that the compact\nallocation plans are an effective way of solving the guaranteed delivery ad\nserving problem.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2012 00:31:18 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Chen", "Peiji", ""], ["Ma", "Wenjing", ""], ["Mandalapu", "Srinath", ""], ["Nagarajan", "Chandrashekhar", ""], ["Shanmugasundaram", "Jayavel", ""], ["Vassilvitskii", "Sergei", ""], ["Vee", "Erik", ""], ["Yu", "Manfai", ""], ["Zien", "Jason", ""]]}, {"id": "1203.3602", "submitter": "Erik Demaine", "authors": "Erik D. Demaine, Martin L. Demaine, Yair N. Minsky, Joseph S. B.\n  Mitchell, Ronald L. Rivest, Mihai Patrascu", "title": "Picture-Hanging Puzzles", "comments": "18 pages, 8 figures, 11 puzzles. Journal version of FUN 2012 paper", "journal-ref": "Theory of Computing Systems, 54(4):531-550, May 2014", "doi": "10.1007/s00224-013-9501-0", "report-no": null, "categories": "cs.DS math.GN math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to hang a picture by wrapping rope around n nails, making a\npolynomial number of twists, such that the picture falls whenever any k out of\nthe n nails get removed, and the picture remains hanging when fewer than k\nnails get removed. This construction makes for some fun mathematical magic\nperformances. More generally, we characterize the possible Boolean functions\ncharacterizing when the picture falls in terms of which nails get removed as\nall monotone Boolean functions. This construction requires an exponential\nnumber of twists in the worst case, but exponential complexity is almost always\nnecessary for general functions.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2012 02:05:19 GMT"}, {"version": "v2", "created": "Sat, 26 Apr 2014 16:15:59 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Demaine", "Erik D.", ""], ["Demaine", "Martin L.", ""], ["Minsky", "Yair N.", ""], ["Mitchell", "Joseph S. B.", ""], ["Rivest", "Ronald L.", ""], ["Patrascu", "Mihai", ""]]}, {"id": "1203.3619", "submitter": "Erik Vee", "authors": "Vijay Bharadwaj, Peiji Chen, Wenjing Ma, Chandrashekhar Nagarajan,\n  John Tomlin, Sergei Vassilvitskii, Erik Vee, Jian Yang", "title": "SHALE: An Efficient Algorithm for Allocation of Guaranteed Display\n  Advertising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of optimizing allocation in guaranteed display\nadvertising, we develop an efficient, lightweight method of generating a\ncompact {\\em allocation plan} that can be used to guide ad server decisions.\nThe plan itself uses just O(1) state per guaranteed contract, is robust to\nnoise, and allows us to serve (provably) nearly optimally. The optimization\nmethod we develop is scalable, with a small in-memory footprint, and working in\nlinear time per iteration. It is also \"stop-anytime\", meaning that\ntime-critical applications can stop early and still get a good serving\nsolution. Thus, it is particularly useful for optimizing the large problems\narising in the context of display advertising. We demonstrate the effectiveness\nof our algorithm using actual Yahoo! data.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2012 06:02:29 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Bharadwaj", "Vijay", ""], ["Chen", "Peiji", ""], ["Ma", "Wenjing", ""], ["Nagarajan", "Chandrashekhar", ""], ["Tomlin", "John", ""], ["Vassilvitskii", "Sergei", ""], ["Vee", "Erik", ""], ["Yang", "Jian", ""]]}, {"id": "1203.3636", "submitter": "Matthias M\\\"uller-Hannemann", "authors": "Annabell Berger and Matthias M\\\"uller-Hannemann", "title": "How to Attack the NP-complete Dag Realization Problem in Practice", "comments": "20 pages, 11 figures, extended abstract to appear in Proceedings of\n  SEA 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following fundamental realization problem of directed acyclic\ngraphs (dags). Given a sequence S:=(a_1,b_1),...,(a_n, b_n) with a_i, b_i in\nZ_0^+, does there exist a dag (no parallel arcs allowed) with labeled vertex\nset V:= {v_1,...,v_n} such that for all v_i in V indegree and outdegree of v_i\nmatch exactly the given numbers a_i and b_i, respectively? Recently this\ndecision problem has been shown to be NP-complete by Nichterlein (2011).\nHowever, we can show that several important classes of sequences are\nefficiently solvable.\n  In previous work (Berger and Mueller-Hannemann, FCT2011), we have proved that\nyes-instances always have a special kind of topological order which allows us\nto reduce the number of possible topological orderings in most cases\ndrastically. This leads to an exact exponential-time algorithm which\nsignificantly improves upon a straightforward approach. Moreover, a combination\nof this exponential-time algorithm with a special strategy gives a linear-time\nalgorithm. Interestingly, in systematic experiments we observed that we could\nsolve a huge majority of all instances by the linear-time heuristic. This\nmotivates us to develop characteristics like dag density and \"distance to\nprovably easy sequences\" which can give us an indicator how easy or difficult a\ngiven sequence can be realized.\n  Furthermore, we propose a randomized algorithm which exploits our structural\ninsight on topological sortings and uses a number of reduction rules. We\nobserve that it clearly outperforms all other variants and behaves surprisingly\nwell for almost all instances. Another striking observation is that our simple\nlinear-time algorithm solves a set of real-world instances from different\ndomains, namely ordered binary decision diagrams (OBDDs), train and flight\nschedules, as well as instances derived from food-web networks without any\nexception.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2012 08:42:58 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Berger", "Annabell", ""], ["M\u00fcller-Hannemann", "Matthias", ""]]}, {"id": "1203.3727", "submitter": "Anthony Perez", "authors": "Anthony Perez", "title": "Linear vertex-kernels for several dense ranking r-CSPs", "comments": "Several flaws (Lemma 2.17) appeared in the previous version; this\n  version corrects them", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Ranking r-Constraint Satisfaction Problem (ranking r-CSP) consists of a\nground set of vertices V, an arity r >= 2, a parameter k and a constraint\nsystem c, where c is a function which maps rankings of r-sized subsets of V to\n{0,1}. The objective is to decide if there exists a ranking of the vertices\nsatisfying all but at most k constraints. Famous ranking r-CSP include the\nFeedback Arc Set in Tournaments and Betweenness in Tournaments problems. We\nconsider these problems from the kernelization viewpoint. We prove that\nso-called l_r-simply characterized ranking r-CSPs admit linear vertex-kernels\nwhenever they admit constant-factor approximation algorithms. This implies that\nr-Betweenness in Tournaments and r-Transitive Feedback Arc Set In Tournaments,\ntwo natural generalizations of the previously mentioned problems, admit linear\nvertex-kernels. Moreover, we introduce another generalization of Feedback Arc\nSet in Tournaments, which does not fit the aforementioned framework. We obtain\na 5-approximation and a linear vertex-kernel for this problem.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2012 15:01:40 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2012 20:36:19 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2012 12:22:06 GMT"}], "update_date": "2012-10-26", "authors_parsed": [["Perez", "Anthony", ""]]}, {"id": "1203.3883", "submitter": "Igor Sergeev", "authors": "Igor S. Sergeev", "title": "A note on the fast power series' exponential", "comments": "7 pages, in English; 7 pages, in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that the exponential of a complex power series up to order n can\nbe implemented via (23/12+o(1))M(n) binary arithmetic operations over complex\nfield, where M(n) stands for the (smoothed) complexity of multiplication of\npolynomials of degree <n in FFT-model. Yet, it is shown how to raise a power\nseries to a constant power with the complexity (27/8+o(1))M(n).\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2012 18:20:32 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Sergeev", "Igor S.", ""]]}, {"id": "1203.4041", "submitter": "Christophe Weibel", "authors": "Amit Chakrabarti, Lisa Fleischer, Christophe Weibel", "title": "When the Cut Condition is Enough: A Complete Characterization for\n  Multiflow Problems in Series-Parallel Networks", "comments": "An extended abstract of this paper will be published at the 44th\n  Symposium on Theory of Computing (STOC 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be a supply graph and $H=(V,F)$ a demand graph defined on the\nsame set of vertices. An assignment of capacities to the edges of $G$ and\ndemands to the edges of $H$ is said to satisfy the \\emph{cut condition} if for\nany cut in the graph, the total demand crossing the cut is no more than the\ntotal capacity crossing it. The pair $(G,H)$ is called \\emph{cut-sufficient} if\nfor any assignment of capacities and demands that satisfy the cut condition,\nthere is a multiflow routing the demands defined on $H$ within the network with\ncapacities defined on $G$. We prove a previous conjecture, which states that\nwhen the supply graph $G$ is series-parallel, the pair $(G,H)$ is\ncut-sufficient if and only if $(G,H)$ does not contain an \\emph{odd spindle} as\na minor; that is, if it is impossible to contract edges of $G$ and delete edges\nof $G$ and $H$ so that $G$ becomes the complete bipartite graph $K_{2,p}$, with\n$p\\geq 3$ odd, and $H$ is composed of a cycle connecting the $p$ vertices of\ndegree 2, and an edge connecting the two vertices of degree $p$. We further\nprove that if the instance is \\emph{Eulerian} --- that is, the demands and\ncapacities are integers and the total of demands and capacities incident to\neach vertex is even --- then the multiflow problem has an integral solution. We\nprovide a polynomial-time algorithm to find an integral solution in this case.\nIn order to prove these results, we formulate properties of tight cuts (cuts\nfor which the cut condition inequality is tight) in cut-sufficient pairs. We\nbelieve these properties might be useful in extending our results to planar\ngraphs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2012 06:25:33 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Chakrabarti", "Amit", ""], ["Fleischer", "Lisa", ""], ["Weibel", "Christophe", ""]]}, {"id": "1203.4117", "submitter": "Michael Rink", "authors": "Martin Dietzfelbinger and Hendrik Peilke and Michael Rink", "title": "A More Reliable Greedy Heuristic for Maximum Matchings in Sparse Random\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new greedy algorithm for the maximum cardinality matching\nproblem. We give experimental evidence that this algorithm is likely to find a\nmaximum matching in random graphs with constant expected degree c>0,\nindependent of the value of c. This is contrary to the behavior of commonly\nused greedy matching heuristics which are known to have some range of c where\nthey probably fail to compute a maximum matching.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2012 14:27:00 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Dietzfelbinger", "Martin", ""], ["Peilke", "Hendrik", ""], ["Rink", "Michael", ""]]}, {"id": "1203.4587", "submitter": "Cagdas Bilen", "authors": "Cagdas Bilen, Yao Wang and Ivan Selesnick", "title": "High Speed Compressed Sensing Reconstruction in Dynamic Parallel MRI\n  Using Augmented Lagrangian and Parallel Processing", "comments": "Submitted to IEEE JETCAS, Special Issue on Circuits, Systems and\n  Algorithms for Compressed Sensing", "journal-ref": null, "doi": "10.1109/JETCAS.2012.2217032", "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) is one of the fields that the compressed\nsensing theory is well utilized to reduce the scan time significantly leading\nto faster imaging or higher resolution images. It has been shown that a small\nfraction of the overall measurements are sufficient to reconstruct images with\nthe combination of compressed sensing and parallel imaging. Various\nreconstruction algorithms has been proposed for compressed sensing, among which\nAugmented Lagrangian based methods have been shown to often perform better than\nothers for many different applications. In this paper, we propose new Augmented\nLagrangian based solutions to the compressed sensing reconstruction problem\nwith analysis and synthesis prior formulations. We also propose a computational\nmethod which makes use of properties of the sampling pattern to significantly\nimprove the speed of the reconstruction for the proposed algorithms in\nCartesian sampled MRI. The proposed algorithms are shown to outperform earlier\nmethods especially for the case of dynamic MRI for which the transfer function\ntends to be a very large matrix and significantly ill conditioned. It is also\ndemonstrated that the proposed algorithm can be accelerated much further than\nother methods in case of a parallel implementation with graphics processing\nunits (GPUs).\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 21:08:56 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Bilen", "Cagdas", ""], ["Wang", "Yao", ""], ["Selesnick", "Ivan", ""]]}, {"id": "1203.4619", "submitter": "Debmalya Panigrahi", "authors": "Yossi Azar and Debmalya Panigrahi", "title": "Online Load Balancing on Unrelated Machines with Startup Costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in energy-efficient scheduling in data centers,\nKhuller, Li, and Saha introduced the {\\em machine activation} problem as a\ngeneralization of the classical optimization problems of set cover and load\nbalancing on unrelated machines. In this problem, a set of $n$ jobs have to be\ndistributed among a set of $m$ (unrelated) machines, given the processing time\nof each job on each machine, where each machine has a startup cost. The goal is\nto produce a schedule of minimum total startup cost subject to a constraint\n$\\bf L$ on its makespan. While Khuller {\\em et al} considered the offline\nversion of this problem, a typical scenario in scheduling is one where jobs\narrive online and have to be assigned to a machine immediately on arrival. We\ngive an $(O(\\log (mn)\\log m), O(\\log m))$-competitive randomized online\nalgorithm for this problem, i.e. the schedule produced by our algorithm has a\nmakespan of $O({\\bf L} \\log m)$ with high probability, and a total expected\nstartup cost of $O(\\log (mn)\\log m)$ times that of an optimal offline schedule\nwith makespan $\\bf L$. The competitive ratios of our algorithm are (almost)\noptimal.\n  Our algorithms use the online primal dual framework introduced by Alon {\\em\net al} for the online set cover problem, and subsequently developed further by\nBuchbinder, Naor, and co-authors. To the best of our knowledge, all previous\napplications of this framework have been to linear programs (LPs) with either\npacking or covering constraints. One novelty of our application is that we use\nthis framework for a mixed LP that has both covering and packing constraints.\nWe hope that the algorithmic techniques developed in this paper to\nsimultaneously handle packing and covering constraints will be useful for\nsolving other online optimization problems as well.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 23:06:10 GMT"}], "update_date": "2012-03-22", "authors_parsed": [["Azar", "Yossi", ""], ["Panigrahi", "Debmalya", ""]]}, {"id": "1203.4627", "submitter": "Vasilis Gkatzelis", "authors": "Richard Cole, Vasilis Gkatzelis, Gagan Goel", "title": "Truthfulness, Proportional Fairness, and Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does one allocate a collection of resources to a set of strategic agents\nin a fair and efficient manner without using money? For in many scenarios it is\nnot feasible to use money to compensate agents for otherwise unsatisfactory\noutcomes. This paper studies this question, looking at both fairness and\nefficiency measures.\n  We employ the proportionally fair solution, which is a well-known fairness\nconcept for money-free settings. But although finding a proportionally fair\nsolution is computationally tractable, it cannot be implemented in a truthful\nfashion. Consequently, we seek approximate solutions. We give several truthful\nmechanisms which achieve proportional fairness in an approximate sense. We use\na strong notion of approximation, requiring the mechanism to give each agent a\ngood approximation of its proportionally fair utility. In particular, one of\nour mechanisms provides a better and better approximation factor as the minimum\ndemand for every good increases. A motivating example is provided by the\nmassive privatization auction in the Czech republic in the early 90s.\n  With regard to efficiency, prior work has shown a lower bound of 0.5 on the\napproximation factor of any swap-dictatorial mechanism approximating a social\nwelfare measure even for the two agents and multiple goods case. We surpass\nthis lower bound by designing a non-swap-dictatorial mechanism for this case.\nInterestingly, the new mechanism builds on the notion of proportional fairness.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 23:55:49 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2012 22:16:17 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Cole", "Richard", ""], ["Gkatzelis", "Vasilis", ""], ["Goel", "Gagan", ""]]}, {"id": "1203.4822", "submitter": "Francisco Soulignac", "authors": "Andrew R. Curtis, Min Chih Lin, Ross M. McConnell, Yahav Nussbaum,\n  Francisco J. Soulignac, Jeremy P. Spinrad, Jayme L. Szwarcfiter", "title": "Isomorphism of graph classes related to the circular-ones property", "comments": "25 pages, 9 figures", "journal-ref": "Discrete Math. Theor. Comput. Sci. 15 (2013), 157--182", "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a linear-time algorithm that checks for isomorphism between two 0-1\nmatrices that obey the circular-ones property. This algorithm leads to\nlinear-time isomorphism algorithms for related graph classes, including Helly\ncircular-arc graphs, \\Gamma-circular-arc graphs, proper circular-arc graphs and\nconvex-round graphs.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2012 20:03:53 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Curtis", "Andrew R.", ""], ["Lin", "Min Chih", ""], ["McConnell", "Ross M.", ""], ["Nussbaum", "Yahav", ""], ["Soulignac", "Francisco J.", ""], ["Spinrad", "Jeremy P.", ""], ["Szwarcfiter", "Jayme L.", ""]]}, {"id": "1203.4836", "submitter": "Anatolijs Gorbunovs", "authors": "Anatolijs Gorbunovs", "title": "On a New Method of Storing a Variable Size Array", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new data structure, log_vector, with the following\nproperties: constant time random access to individual elements; constant time\nelement addition to the end; constant time element removal from the end;\nconstant time empty data structure creation; amortized constant space per\nindividual elements; constant additional space used.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2012 21:15:44 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Gorbunovs", "Anatolijs", ""]]}, {"id": "1203.4900", "submitter": "Michael Kapralov", "authors": "Ashish Goel and Michael Kapralov and Ian Post", "title": "Single pass sparsification in the streaming model with edge deletions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we give a construction of cut sparsifiers of Benczur and Karger\nin the {\\em dynamic} streaming setting in a single pass over the data stream.\nPrevious constructions either required multiple passes or were unable to handle\nedge deletions. We use $\\tilde{O}(1/\\e^2)$ time for each stream update and\n$\\tilde{O}(n/\\e^2)$ time to construct a sparsifier. Our $\\e$-sparsifiers have\n$O(n\\log^3 n/\\e^2)$ edges. The main tools behind our result are an application\nof sketching techniques of Ahn et al.[SODA'12] to estimate edge connectivity\ntogether with a novel application of sampling with limited independence and\nsparse recovery to produce the edges of the sparsifier.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 07:45:13 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Goel", "Ashish", ""], ["Kapralov", "Michael", ""], ["Post", "Ian", ""]]}, {"id": "1203.4903", "submitter": "Edith Cohen", "authors": "Edith Cohen", "title": "Distance Queries from Sampled Data: Accurate and Efficient", "comments": "13 pages; This is a full version of a KDD 2014 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance queries are a basic tool in data analysis. They are used for\ndetection and localization of change for the purpose of anomaly detection,\nmonitoring, or planning. Distance queries are particularly useful when data\nsets such as measurements, snapshots of a system, content, traffic matrices,\nand activity logs are collected repeatedly.\n  Random sampling, which can be efficiently performed over streamed or\ndistributed data, is an important tool for scalable data analysis. The sample\nconstitutes an extremely flexible summary, which naturally supports domain\nqueries and scalable estimation of statistics, which can be specified after the\nsample is generated. The effectiveness of a sample as a summary, however,\nhinges on the estimators we have.\n  We derive novel estimators for estimating $L_p$ distance from sampled data.\nOur estimators apply with the most common weighted sampling schemes: Poisson\nProbability Proportional to Size (PPS) and its fixed sample size variants. They\nalso apply when the samples of different data sets are independent or\ncoordinated. Our estimators are admissible (Pareto optimal in terms of\nvariance) and have compelling properties.\n  We study the performance of our Manhattan and Euclidean distance ($p=1,2$)\nestimators on diverse datasets, demonstrating scalability and accuracy even\nwhen a small fraction of the data is sampled. Our work, for the first time,\nfacilitates effective distance estimation over sampled data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 08:06:09 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2013 20:10:58 GMT"}, {"version": "v3", "created": "Sun, 8 Jun 2014 13:06:42 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Cohen", "Edith", ""]]}, {"id": "1203.4920", "submitter": "Livio Colussi", "authors": "Livio Colussi", "title": "Work function algorithm can forget history without losing\n  competitiveness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Work Function Algorithm is the most effective deterministic on-line\nalgorithm for the k-server problem. Koutsoupias and Papadimitriou proved WFA is\n(2k-1) competitive. However the best known implementation of WFA requires time\nO(i^2) to process request r_i and this makes WFA impractical for long sequences\nof requests. The O(i^2) time is spent to compute the work function on the whole\nhistory of past requests. In order to make constant the time to process a\nrequest, Rudec and Menger proposed to restrict the history to a moving window\nof fixed size. However WFA restricted to a moving window loses its\ncompetitiveness. Here we give a condition that allows WFA to forget the whole\nprevious history and restart from scratch without losing competitiveness.\nMoreover for most of the metric spaces of practical interest (finite or bounded\nspaces) there is a constant bound on the length of the history before the\ncondition is verified and this makes O(1) the time to process each request.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 08:56:12 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Colussi", "Livio", ""]]}, {"id": "1203.5235", "submitter": "Bang Ye Wu", "authors": "Bang Ye Wu, Jun-Lin Guo, Yue-Li Wang", "title": "A linear time algorithm for the next-to-shortest path problem on\n  undirected graphs with nonnegative edge lengths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For two vertices $s$ and $t$ in a graph $G=(V,E)$, the next-to-shortest path\nis an $st$-path which length is minimum amongst all $st$-paths strictly longer\nthan the shortest path length. In this paper we show that, when the graph is\nundirected and all edge lengths are nonnegative, the problem can be solved in\nlinear time if the distances from $s$ and $t$ to all other vertices are given.\nThis result generalizes the previous work (DOI 10.1007/s00453-011-9601-7) to\nallowing zero-length edges.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2012 13:33:47 GMT"}], "update_date": "2012-03-26", "authors_parsed": [["Wu", "Bang Ye", ""], ["Guo", "Jun-Lin", ""], ["Wang", "Yue-Li", ""]]}, {"id": "1203.5387", "submitter": "Vibhor  Rastogi", "authors": "Vibhor Rastogi, Ashwin Machanavajjhala, Laukik Chitnis, Anish Das\n  Sarma", "title": "Finding Connected Components on Map-reduce in Logarithmic Rounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large graph G = (V,E) with millions of nodes and edges, how do we\ncompute its connected components efficiently? Recent work addresses this\nproblem in map-reduce, where a fundamental trade-off exists between the number\nof map-reduce rounds and the communication of each round. Denoting d the\ndiameter of the graph, and n the number of nodes in the largest component, all\nprior map-reduce techniques either require d rounds, or require about n|V| +\n|E| communication per round. We propose two randomized map-reduce algorithms --\n(i) Hash-Greater-To-Min, which provably requires at most 3log(n) rounds with\nhigh probability, and at most 2(|V| + |E|) communication per round, and (ii)\nHash-to-Min, which has a worse theoretical complexity, but in practice\ncompletes in at most 2log(d) rounds and 3(|V| + |E|) communication per rounds.\n  Our techniques for connected components can be applied to clustering as well.\nWe propose a novel algorithm for agglomerative single linkage clustering in\nmap-reduce. This is the first algorithm that can provably compute a clustering\nin at most O(log(n)) rounds, where n is the size of the largest cluster. We\nshow the effectiveness of all our algorithms through detailed experiments on\nlarge synthetic as well as real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 05:16:27 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2012 01:50:51 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Rastogi", "Vibhor", ""], ["Machanavajjhala", "Ashwin", ""], ["Chitnis", "Laukik", ""], ["Sarma", "Anish Das", ""]]}, {"id": "1203.5453", "submitter": "Aleksandar Nikolov", "authors": "S. Muthukrishnan, Aleksandar Nikolov", "title": "Optimal Private Halfspace Counting via Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A range counting problem is specified by a set $P$ of size $|P| = n$ of\npoints in $\\mathbb{R}^d$, an integer weight $x_p$ associated to each point $p\n\\in P$, and a range space ${\\cal R} \\subseteq 2^{P}$. Given a query range $R\n\\in {\\cal R}$, the target output is $R(\\vec{x}) = \\sum_{p \\in R}{x_p}$. Range\ncounting for different range spaces is a central problem in Computational\nGeometry.\n  We study $(\\epsilon, \\delta)$-differentially private algorithms for range\ncounting. Our main results are for the range space given by hyperplanes, that\nis, the halfspace counting problem. We present an $(\\epsilon,\n\\delta)$-differentially private algorithm for halfspace counting in $d$\ndimensions which achieves $O(n^{1-1/d})$ average squared error. This contrasts\nwith the $\\Omega(n)$ lower bound established by the classical result of Dinur\nand Nissim [PODS 2003] for arbitrary subset counting queries. We also show a\nmatching lower bound on average squared error for any $(\\epsilon,\n\\delta)$-differentially private algorithm for halfspace counting. Both bounds\nare obtained using discrepancy theory. For the lower bound, we use a modified\ndiscrepancy measure and bound approximation of $(\\epsilon,\n\\delta)$-differentially private algorithms for range counting queries in terms\nof this discrepancy. We also relate the modified discrepancy measure to\nclassical combinatorial discrepancy, which allows us to exploit known\ndiscrepancy lower bounds. This approach also yields a lower bound of\n$\\Omega((\\log n)^{d-1})$ for $(\\epsilon, \\delta)$-differentially private\northogonal range counting in $d$ dimensions, the first known superconstant\nlower bound for this problem. For the upper bound, we use an approach inspired\nby partial coloring methods for proving discrepancy upper bounds, and obtain\n$(\\epsilon, \\delta)$-differentially private algorithms for range counting with\npolynomially bounded shatter function range spaces.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 22:25:12 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Muthukrishnan", "S.", ""], ["Nikolov", "Aleksandar", ""]]}, {"id": "1203.5464", "submitter": "Ton Kloks", "authors": "Ton Kloks", "title": "A note on triangle partitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Koivisto studied the partitioning of sets of bounded cardinality. We improve\nhis time analysis somewhat, for the special case of triangle partitions, and\nobtain a slight improvement.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2012 02:16:00 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Kloks", "Ton", ""]]}, {"id": "1203.5675", "submitter": "Amitabha Roy", "authors": "Amitabha Roy", "title": "Memory Hierarchy Sensitive Graph Layout", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining large graphs for information is becoming an increasingly important\nworkload due to the plethora of graph structured data becoming available. An\naspect of graph algorithms that has hitherto not received much interest is the\neffect of memory hierarchy on accesses. A typical system today has multiple\nlevels in the memory hierarchy with differing units of locality; ranging across\ncache lines, TLB entries and DRAM pages. We postulate that it is possible to\nallocate graph structured data in main memory in a way as to improve the\nspatial locality of the data. Previous approaches to improving cache locality\nhave focused only on a single unit of locality, either the cache line or\nvirtual memory page. On the other hand cache oblivious algorithms can optimise\nlayout for all levels of the memory hierarchy but unfortunately need to be\nspecially designed for individual data structures. In this paper we explore\nhierarchical blocking as a technique for closing this gap. We require as input\na specification of the units of locality in the memory hierarchy and lay out\nthe input graph accordingly by copying its nodes using a hierarchy of breadth\nfirst searches. We start with a basic algorithm that is limited to trees and\nthen extend it to arbitrary graphs. Our most efficient version requires only a\nconstant amount of additional space. We have implemented versions of the\nalgorithm in various environments: for C programs interfaced with macros, as an\nextension to the Boost object oriented graph library and finally as a\nmodification to the traversal phase of the semispace garbage collector in the\nJikes Java virtual machine. Our results show significant improvements in the\naccess time to graphs of various structure.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2012 14:12:48 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Roy", "Amitabha", ""]]}, {"id": "1203.5737", "submitter": "Tom\\'a\\v{s} Oberhuber", "authors": "Martin Heller and Tom\\'a\\v{s} Oberhuber", "title": "Adaptive Row-grouped CSR Format for Storing of Sparse Matrices on GPU", "comments": "9 pages, 5 figures, 1 code listing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new adaptive format for storing sparse matrices on GPU. We compare\nit with several other formats including CUSPARSE which is today probably the\nbest choice for processing of sparse matrices on GPU in CUDA. Contrary to\nCUSPARSE which works with common CSR format, our new format requires\nconversion. However, multiplication of sparse-matrix and vector is\nsignificantly faster for many atrices. We demonstrate it on set of 1 600\nmatrices and we show for what types of matrices our format is profitable.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2012 17:29:58 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Heller", "Martin", ""], ["Oberhuber", "Tom\u00e1\u0161", ""]]}, {"id": "1203.5747", "submitter": "Raghu Meka", "authors": "Shachar Lovett, Raghu Meka", "title": "Constructive Discrepancy Minimization by Walking on The Edges", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing the discrepancy of a set system is a fundamental problem in\ncombinatorics. One of the cornerstones in this area is the celebrated six\nstandard deviations result of Spencer (AMS 1985): In any system of n sets in a\nuniverse of size n, there always exists a coloring which achieves discrepancy\n6\\sqrt{n}. The original proof of Spencer was existential in nature, and did not\ngive an efficient algorithm to find such a coloring. Recently, a breakthrough\nwork of Bansal (FOCS 2010) gave an efficient algorithm which finds such a\ncoloring. His algorithm was based on an SDP relaxation of the discrepancy\nproblem and a clever rounding procedure. In this work we give a new randomized\nalgorithm to find a coloring as in Spencer's result based on a restricted\nrandom walk we call \"Edge-Walk\". Our algorithm and its analysis use only basic\nlinear algebra and is \"truly\" constructive in that it does not appeal to the\nexistential arguments, giving a new proof of Spencer's theorem and the partial\ncoloring lemma.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2012 17:56:28 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2012 21:34:26 GMT"}], "update_date": "2012-10-15", "authors_parsed": [["Lovett", "Shachar", ""], ["Meka", "Raghu", ""]]}, {"id": "1203.6274", "submitter": "Zeev Nutov", "authors": "Zeev Nutov", "title": "Small $\\ell$-edge-covers in $k$-connected graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be a $k$-edge-connected graph with edge costs $\\{c(e):e \\in\nE\\}$ and let $1 \\leq \\ell \\leq k-1$. We show by a simple and short proof, that\n$G$ contains an $\\ell$-edge cover $I$ such that: $c(I) \\leq \\frac{\\ell}{k}c(E)$\nif $G$ is bipartite, or if $\\ell |V|$ is even, or if $|E| \\geq \\frac{k|V|}{2}\n+\\frac{k}{2\\ell}$; otherwise, $c(I) \\leq (\\frac{\\ell}{k}+\\frac{1}{k|V|})c(E)$.\nThe particular case $\\ell=k-1$ and unit costs already includes a result of\nCheriyan and Thurimella, that $G$ contains a $(k-1)$-edge-cover of size\n$|E|-\\lfloor |V|/2 \\rfloor$. Using our result, we slightly improve the\napproximation ratios for the {\\sf $k$-Connected Subgraph} problem (the\nnode-connectivity version) with uniform and $\\beta$-metric costs. We then\nconsider the dual problem of finding a spanning subgraph of maximum\nconnectivity $k^*$ with a prescribed number of edges. We give an algorithm that\ncomputes a $(k^*-1)$-connected subgraph, which is tight, since the problem is\nNP-hard.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 14:09:49 GMT"}], "update_date": "2012-03-29", "authors_parsed": [["Nutov", "Zeev", ""]]}, {"id": "1203.6397", "submitter": "Yuli Ye", "authors": "Allan Borodin, Aadhar Jain, Hyun Chul Lee and Yuli Ye", "title": "Max-Sum Diversification, Monotone Submodular Functions and Dynamic\n  Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Result diversification is an important aspect in web-based search, document\nsummarization, facility location, portfolio management and other applications.\nGiven a set of ranked results for a set of objects (e.g. web documents,\nfacilities, etc.) with a distance between any pair, the goal is to select a\nsubset $S$ satisfying the following three criteria: (a) the subset $S$\nsatisfies some constraint (e.g. bounded cardinality); (b) the subset contains\nresults of high \"quality\"; and (c) the subset contains results that are\n\"diverse\" relative to the distance measure. The goal of result diversification\nis to produce a diversified subset while maintaining high quality as much as\npossible. We study a broad class of problems where the distances are a metric,\nwhere the constraint is given by independence in a matroid, where quality is\ndetermined by a monotone submodular function, and diversity is defined as the\nsum of distances between objects in $S$. Our problem is a generalization of the\n{\\em max sum diversification} problem studied in \\cite{GoSh09} which in turn is\na generaliztion of the {\\em max sum $p$-dispersion problem} studied extensively\nin location theory. It is NP-hard even with the triangle inequality. We propose\ntwo simple and natural algorithms: a greedy algorithm for a cardinality\nconstraint and a local search algorithm for an arbitary matroid constraint. We\nprove that both algorithms achieve constant approximation ratios.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 23:56:46 GMT"}, {"version": "v2", "created": "Wed, 20 Aug 2014 04:24:58 GMT"}, {"version": "v3", "created": "Fri, 25 Nov 2016 03:48:21 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Borodin", "Allan", ""], ["Jain", "Aadhar", ""], ["Lee", "Hyun Chul", ""], ["Ye", "Yuli", ""]]}, {"id": "1203.6481", "submitter": "Krzysztof Fleszar", "authors": "Aparna Das, Krzysztof Fleszar, Stephen Kobourov, Joachim Spoerhase,\n  Sankar Veeramoni, Alexander Wolff", "title": "Polylogarithmic Approximation for Generalized Minimum Manhattan Networks", "comments": "14 pages, 5 figures; added appendix and figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of $n$ terminals, which are points in $d$-dimensional Euclidean\nspace, the minimum Manhattan network problem (MMN) asks for a minimum-length\nrectilinear network that connects each pair of terminals by a Manhattan path,\nthat is, a path consisting of axis-parallel segments whose total length equals\nthe pair's Manhattan distance. Even for $d=2$, the problem is NP-hard, but\nconstant-factor approximations are known. For $d \\ge 3$, the problem is\nAPX-hard; it is known to admit, for any $\\eps > 0$, an\n$O(n^\\eps)$-approximation.\n  In the generalized minimum Manhattan network problem (GMMN), we are given a\nset $R$ of $n$ terminal pairs, and the goal is to find a minimum-length\nrectilinear network such that each pair in $R$ is connected by a Manhattan\npath. GMMN is a generalization of both MMN and the well-known rectilinear\nSteiner arborescence problem (RSA). So far, only special cases of GMMN have\nbeen considered.\n  We present an $O(\\log^{d+1} n)$-approximation algorithm for GMMN (and, hence,\nMMN) in $d \\ge 2$ dimensions and an $O(\\log n)$-approximation algorithm for 2D.\nWe show that an existing $O(\\log n)$-approximation algorithm for RSA in 2D\ngeneralizes easily to $d>2$ dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2012 10:31:43 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2012 19:43:16 GMT"}], "update_date": "2012-04-24", "authors_parsed": [["Das", "Aparna", ""], ["Fleszar", "Krzysztof", ""], ["Kobourov", "Stephen", ""], ["Spoerhase", "Joachim", ""], ["Veeramoni", "Sankar", ""], ["Wolff", "Alexander", ""]]}, {"id": "1203.6559", "submitter": "Michiel de Bondt", "authors": "Michiel de Bondt", "title": "Solving Mahjong Solitaire boards with peeking", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first prove that solving Mahjong Solitaire boards with peeking is\nNP-complete, even if one only allows isolated stacks of the forms /aab/ and\n/abb/. We subsequently show that layouts of isolated stacks of heights one and\ntwo can always be solved with peeking, and that doing so is in P, as well as\nfinding an optimal algorithm for such layouts without peeking.\n  Next, we describe a practical algorithm for solving Mahjong Solitaire boards\nwith peeking, which is simple and fast. The algorithm uses an effective pruning\ncriterion and a heuristic to find and prioritize critical groups. The ideas of\nthe algorithm can also be applied to solving Shisen-Sho with peeking.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2012 15:34:39 GMT"}], "update_date": "2012-04-04", "authors_parsed": [["de Bondt", "Michiel", ""]]}, {"id": "1203.6695", "submitter": "Umang Bhaskar", "authors": "Umang Bhaskar and Lisa Fleischer", "title": "Online Mixed Packing and Covering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many problems, the inputs arrive over time, and must be dealt with\nirrevocably when they arrive. Such problems are online problems. A common\nmethod of solving online problems is to first solve the corresponding linear\nprogram, and then round the fractional solution online to obtain an integral\nsolution.\n  We give algorithms for solving linear programs with mixed packing and\ncovering constraints online. We first consider mixed packing and covering\nlinear programs, where packing constraints are given offline and covering\nconstraints are received online. The objective is to minimize the maximum\nmultiplicative factor by which any packing constraint is violated, while\nsatisfying the covering constraints. No prior sublinear competitive algorithms\nare known for this problem. We give the first such --- a\npolylogarithmic-competitive algorithm for solving mixed packing and covering\nlinear programs online. We also show a nearly tight lower bound.\n  Our techniques for the upper bound use an exponential penalty function in\nconjunction with multiplicative updates. While exponential penalty functions\nare used previously to solve linear programs offline approximately, offline\nalgorithms know the constraints beforehand and can optimize greedily. In\ncontrast, when constraints arrive online, updates need to be more complex.\n  We apply our techniques to solve two online fixed-charge problems with\ncongestion. These problems are motivated by applications in machine scheduling\nand facility location. The linear program for these problems is more\ncomplicated than mixed packing and covering, and presents unique challenges. We\nshow that our techniques combined with a randomized rounding procedure give\npolylogarithmic-competitive integral solutions. These problems generalize\nonline set-cover, for which there is a polylogarithmic lower bound. Hence, our\nresults are close to tight.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2012 01:37:11 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2012 21:29:28 GMT"}], "update_date": "2012-04-04", "authors_parsed": [["Bhaskar", "Umang", ""], ["Fleischer", "Lisa", ""]]}, {"id": "1203.6705", "submitter": "Ho Yee Cheung", "authors": "Ho Yee Cheung, Tsz Chiu Kwok, Lap Chi Lau", "title": "Fast Matrix Rank Algorithms and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing the rank of an m x n matrix A over a\nfield. We present a randomized algorithm to find a set of r = rank(A) linearly\nindependent columns in \\~O(|A| + r^\\omega) field operations, where |A| denotes\nthe number of nonzero entries in A and \\omega < 2.38 is the matrix\nmultiplication exponent. Previously the best known algorithm to find a set of r\nlinearly independent columns is by Gaussian elimination, with running time\nO(mnr^{\\omega-2}). Our algorithm is faster when r < max(m,n), for instance when\nthe matrix is rectangular. We also consider the problem of computing the rank\nof a matrix dynamically, supporting the operations of rank one updates and\nadditions and deletions of rows and columns. We present an algorithm that\nupdates the rank in \\~O(mn) field operations. We show that these algorithms can\nbe used to obtain faster algorithms for various problems in numerical linear\nalgebra, combinatorial optimization and dynamic data structure.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2012 03:15:57 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2012 01:33:55 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Cheung", "Ho Yee", ""], ["Kwok", "Tsz Chiu", ""], ["Lau", "Lap Chi", ""]]}, {"id": "1203.6786", "submitter": "Donald Sheehy", "authors": "Donald R. Sheehy", "title": "Linear-Size Approximations to the Vietoris-Rips Filtration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Vietoris-Rips filtration is a versatile tool in topological data\nanalysis. It is a sequence of simplicial complexes built on a metric space to\nadd topological structure to an otherwise disconnected set of points. It is\nwidely used because it encodes useful information about the topology of the\nunderlying metric space. This information is often extracted from its so-called\npersistence diagram. Unfortunately, this filtration is often too large to\nconstruct in full. We show how to construct an O(n)-size filtered simplicial\ncomplex on an $n$-point metric space such that its persistence diagram is a\ngood approximation to that of the Vietoris-Rips filtration. This new filtration\ncan be constructed in $O(n\\log n)$ time. The constant factors in both the size\nand the running time depend only on the doubling dimension of the metric space\nand the desired tightness of the approximation. For the first time, this makes\nit computationally tractable to approximate the persistence diagram of the\nVietoris-Rips filtration across all scales for large data sets.\n  We describe two different sparse filtrations. The first is a zigzag\nfiltration that removes points as the scale increases. The second is a\n(non-zigzag) filtration that yields the same persistence diagram. Both methods\nare based on a hierarchical net-tree and yield the same guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2012 12:22:46 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2013 13:36:57 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Sheehy", "Donald R.", ""]]}]