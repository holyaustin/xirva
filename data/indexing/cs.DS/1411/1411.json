[{"id": "1411.0048", "submitter": "Luis Meira", "authors": "Luis A. A. Meira and Rog\\'erio H. B. de Lima", "title": "Fusion Tree Sorting", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sorting problem is one of the most relevant problems in computer science.\nWithin the scope of modern computer science it has been studied for more than\n70 years. In spite of these facts, new sorting algorithms have been developed\nin recent years. Among several types of sorting algorithms, some are quicker;\nothers are more economic in relation to space, whereas others insert a few\nrestrictions in relation to data input. This paper is aimed at explaining the\nfusion tree data structure, which is responsible for the first sorting\nalgorithm with complexity time smaller than nlgn. The nlgn time complexity has\nled to some confusion and generated the wrong belief in part of the community\nof being the minimum possible for this type of problem.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 00:34:58 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Meira", "Luis A. A.", ""], ["de Lima", "Rog\u00e9rio H. B.", ""]]}, {"id": "1411.0062", "submitter": "Qilong Feng", "authors": "Feng Shi, Jianer Chen, Qilong Feng, Xiaojun Ding, Jianxin Wang", "title": "Algorithms for Maximum Agreement Forest of Multiple General Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Maximum Agreement Forest (Maf) problem is a well-studied problem in\nevolutionary biology, which asks for a largest common subforest of a given\ncollection of phylogenetic trees with identical leaf label-set. However, the\nprevious work about the Maf problem are mainly on two binary phylogenetic trees\nor two general (i.e., binary and non-binary) phylogenetic trees. In this paper,\nwe study the more general version of the problem: the Maf problem on multiple\ngeneral phylogenetic trees. We present a parameterized algorithm of running\ntime $O(3^k n^2m)$ and a 3-approximation algorithm for the Maf problem on\nmultiple rooted general phylogenetic trees, and a parameterized algorithm of\nrunning time $O(4^k n^2m)$ and a 4-approximation algorithm for the Maf problem\non multiple unrooted general phylogenetic trees. We also implement the\nparameterized algorithm and approximation algorithm for the Maf problem on\nmultiple rooted general phylogenetic trees, and test them on simulated data and\nbiological data.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 03:45:00 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Shi", "Feng", ""], ["Chen", "Jianer", ""], ["Feng", "Qilong", ""], ["Ding", "Xiaojun", ""], ["Wang", "Jianxin", ""]]}, {"id": "1411.0095", "submitter": "Deeparnab Chakrabarty", "authors": "Deeparnab Chakrabarty and Prateek Jain and Pravesh Kothari", "title": "Provable Submodular Minimization using Wolfe's Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to several applications in large scale learning and vision problems,\nfast submodular function minimization (SFM) has become a critical problem.\nTheoretically, unconstrained SFM can be performed in polynomial time [IFF 2001,\nIO 2009]. However, these algorithms are typically not practical. In 1976, Wolfe\nproposed an algorithm to find the minimum Euclidean norm point in a polytope,\nand in 1980, Fujishige showed how Wolfe's algorithm can be used for SFM. For\ngeneral submodular functions, this Fujishige-Wolfe minimum norm algorithm seems\nto have the best empirical performance.\n  Despite its good practical performance, very little is known about Wolfe's\nminimum norm algorithm theoretically. To our knowledge, the only result is an\nexponential time analysis due to Wolfe himself. In this paper we give a maiden\nconvergence analysis of Wolfe's algorithm. We prove that in $t$ iterations,\nWolfe's algorithm returns an $O(1/t)$-approximate solution to the min-norm\npoint on {\\em any} polytope. We also prove a robust version of Fujishige's\ntheorem which shows that an $O(1/n^2)$-approximate solution to the min-norm\npoint on the base polytope implies {\\em exact} submodular minimization. As a\ncorollary, we get the first pseudo-polynomial time guarantee for the\nFujishige-Wolfe minimum norm algorithm for unconstrained submodular function\nminimization.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 10:09:49 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Jain", "Prateek", ""], ["Kothari", "Pravesh", ""]]}, {"id": "1411.0149", "submitter": "Aleksandrs Slivkins", "authors": "Ittai Abraham, Omar Alonso, Vasilis Kandylas, Rajesh Patel, Steven\n  Shelford, Aleksandrs Slivkins", "title": "How Many Workers to Ask? Adaptive Exploration for Collecting High\n  Quality Labels", "comments": "SIGIR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing has been part of the IR toolbox as a cheap and fast mechanism\nto obtain labels for system development and evaluation. Successful deployment\nof crowdsourcing at scale involves adjusting many variables, a very important\none being the number of workers needed per human intelligence task (HIT). We\nconsider the crowdsourcing task of learning the answer to simple\nmultiple-choice HITs, which are representative of many relevance experiments.\nIn order to provide statistically significant results, one often needs to ask\nmultiple workers to answer the same HIT. A stopping rule is an algorithm that,\ngiven a HIT, decides for any given set of worker answers if the system should\nstop and output an answer or iterate and ask one more worker. Knowing the\nhistoric performance of a worker in the form of a quality score can be\nbeneficial in such a scenario. In this paper we investigate how to devise\nbetter stopping rules given such quality scores. We also suggest adaptive\nexploration as a promising approach for scalable and automatic creation of\nground truth. We conduct a data analysis on an industrial crowdsourcing\nplatform, and use the observations from this analysis to design new stopping\nrules that use the workers' quality scores in a non-trivial manner. We then\nperform a simulation based on a real-world workload, showing that our algorithm\nperforms better than the more naive approaches.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 18:28:49 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 01:52:19 GMT"}, {"version": "v3", "created": "Thu, 19 May 2016 19:11:46 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Abraham", "Ittai", ""], ["Alonso", "Omar", ""], ["Kandylas", "Vasilis", ""], ["Patel", "Rajesh", ""], ["Shelford", "Steven", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "1411.0168", "submitter": "Jerry Li", "authors": "Rati Gelashvili, Mohsen Ghaffari, Jerry Li, Nir Shavit", "title": "On the Importance of Registers for Computability", "comments": "12 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All consensus hierarchies in the literature assume that we have, in addition\nto copies of a given object, an unbounded number of registers. But why do we\nreally need these registers?\n  This paper considers what would happen if one attempts to solve consensus\nusing various objects but without any registers. We show that under a\nreasonable assumption, objects like queues and stacks cannot emulate the\nmissing registers. We also show that, perhaps surprisingly, initialization,\nshown to have no computational consequences when registers are readily\navailable, is crucial in determining the synchronization power of objects when\nno registers are allowed. Finally, we show that without registers, the number\nof available objects affects the level of consensus that can be solved.\n  Our work thus raises the question of whether consensus hierarchies which\nassume an unbounded number of registers truly capture synchronization power,\nand begins a line of research aimed at better understanding the interaction\nbetween read-write memory and the powerful synchronization operations available\non modern architectures.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 21:01:07 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Gelashvili", "Rati", ""], ["Ghaffari", "Mohsen", ""], ["Li", "Jerry", ""], ["Shavit", "Nir", ""]]}, {"id": "1411.0169", "submitter": "Ilias Diakonikolas", "authors": "Siu-On Chan, Ilias Diakonikolas, Rocco A. Servedio, Xiaorui Sun", "title": "Near-Optimal Density Estimation in Near-Linear Time Using Variable-Width\n  Histograms", "comments": "conference version appears in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $p$ be an unknown and arbitrary probability distribution over $[0,1)$. We\nconsider the problem of {\\em density estimation}, in which a learning algorithm\nis given i.i.d. draws from $p$ and must (with high probability) output a\nhypothesis distribution that is close to $p$. The main contribution of this\npaper is a highly efficient density estimation algorithm for learning using a\nvariable-width histogram, i.e., a hypothesis distribution with a piecewise\nconstant probability density function.\n  In more detail, for any $k$ and $\\epsilon$, we give an algorithm that makes\n$\\tilde{O}(k/\\epsilon^2)$ draws from $p$, runs in $\\tilde{O}(k/\\epsilon^2)$\ntime, and outputs a hypothesis distribution $h$ that is piecewise constant with\n$O(k \\log^2(1/\\epsilon))$ pieces. With high probability the hypothesis $h$\nsatisfies $d_{\\mathrm{TV}}(p,h) \\leq C \\cdot \\mathrm{opt}_k(p) + \\epsilon$,\nwhere $d_{\\mathrm{TV}}$ denotes the total variation distance (statistical\ndistance), $C$ is a universal constant, and $\\mathrm{opt}_k(p)$ is the smallest\ntotal variation distance between $p$ and any $k$-piecewise constant\ndistribution. The sample size and running time of our algorithm are optimal up\nto logarithmic factors. The \"approximation factor\" $C$ in our result is\ninherent in the problem, as we prove that no algorithm with sample size bounded\nin terms of $k$ and $\\epsilon$ can achieve $C<2$ regardless of what kind of\nhypothesis distribution it uses.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 21:03:59 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Chan", "Siu-On", ""], ["Diakonikolas", "Ilias", ""], ["Servedio", "Rocco A.", ""], ["Sun", "Xiaorui", ""]]}, {"id": "1411.0194", "submitter": "Jian Li", "authors": "Lingxiao Huang, Jian Li, Jeff M. Phillips, Haitao Wang", "title": "$\\epsilon$-Kernel Coresets for Stochastic Points", "comments": "50 pages, 4 figures. Cleaned several places. Improved the\n  construction of quant-kernel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the dramatic growth in the number of application domains that generate\nprobabilistic, noisy and uncertain data, there has been an increasing interest\nin designing algorithms for geometric or combinatorial optimization problems\nover such data. In this paper, we initiate the study of constructing\n$\\epsilon$-kernel coresets for uncertain points. We consider uncertainty in the\nexistential model where each point's location is fixed but only occurs with a\ncertain probability, and the locational model where each point has a\nprobability distribution describing its location. An $\\epsilon$-kernel coreset\napproximates the width of a point set in any direction. We consider\napproximating the expected width (an \\expkernel), as well as the probability\ndistribution on the width (an \\probkernel) for any direction. We show that\nthere exists a set of $O(1/\\epsilon^{(d-1)/2})$ deterministic points which\napproximate the expected width under the existential and locational models, and\nwe provide efficient algorithms for constructing such coresets. We show,\nhowever, it is not always possible to find a subset of the original uncertain\npoints which provides such an approximation. However, if the existential\nprobability of each point is lower bounded by a constant, an exp-kernel\\ (or an\nfpow-kernel) is still possible. We also construct an quant-kernel coreset in\nlinear time. Finally, combining with known techniques, we show a few\napplications to approximating the extent of uncertain functions, maintaining\nextent measures for stochastic moving points and some shape fitting problems\nunder uncertainty.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 02:29:54 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 22:31:58 GMT"}, {"version": "v3", "created": "Mon, 23 May 2016 15:11:29 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Huang", "Lingxiao", ""], ["Li", "Jian", ""], ["Phillips", "Jeff M.", ""], ["Wang", "Haitao", ""]]}, {"id": "1411.0257", "submitter": "Dieter Pfoser", "authors": "Alexandros Efentakis, Dieter Pfoser, Yannis Vassiliou", "title": "SALT. A unified framework for all shortest-path query variants on road\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although recent scientific output focuses on multiple shortest-path problem\ndefinitions for road networks, none of the existing solutions does efficiently\nanswer all different types of SP queries. This work proposes SALT, a novel\nframework that not only efficiently answers SP related queries but also\nk-nearest neighbor queries not handled by previous approaches. Our solution\noffers all the benefits needed for practical use-cases, including excellent\nquery performance and very short preprocessing times, thus making it also a\nviable option for dynamic road networks, i.e., edge weights changing frequently\ndue to traffic updates. The proposed SALT framework is a deployable software\nsolution capturing a range of network-related query problems under one\n\"algorithmic hood\".\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 14:16:44 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Efentakis", "Alexandros", ""], ["Pfoser", "Dieter", ""], ["Vassiliou", "Yannis", ""]]}, {"id": "1411.0279", "submitter": "Pascal Van Hentenryck", "authors": "Andres Abeliuk, Gerardo Berbeglia, Manuel Cebrian, and Pascal Van\n  Hentenryck", "title": "Optimizing Expected Utility in a Multinomial Logit Model with Position\n  Bias and Social Influence", "comments": null, "journal-ref": "4OR, 14 (1): 57-75 (2016)", "doi": "10.1007/s10288-015-0302-y", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in retail, online advertising, and cultural\nmarkets, this paper studies how to find the optimal assortment and positioning\nof products subject to a capacity constraint. We prove that the optimal\nassortment and positioning can be found in polynomial time for a multinomial\nlogit model capturing utilities, position bias, and social influence. Moreover,\nin a dynamic market, we show that the policy that applies the optimal\nassortment and positioning and leverages social influence outperforms in\nexpectation any policy not using social influence.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 17:05:52 GMT"}, {"version": "v2", "created": "Thu, 6 Nov 2014 03:30:30 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Abeliuk", "Andres", ""], ["Berbeglia", "Gerardo", ""], ["Cebrian", "Manuel", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "1411.0547", "submitter": "Gregory Puleo", "authors": "Gregory J. Puleo, Olgica Milenkovic", "title": "Correlation Clustering with Constrained Cluster Sizes and Extended\n  Weights Bounds", "comments": "17 pages, simplified the last section and fixed some other minor\n  errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of correlation clustering on graphs with constraints\non both the cluster sizes and the positive and negative weights of edges. Our\ncontributions are twofold: First, we introduce the problem of correlation\nclustering with bounded cluster sizes. Second, we extend the regime of weight\nvalues for which the clustering may be performed with constant approximation\nguarantees in polynomial time and apply the results to the bounded cluster size\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 16:17:31 GMT"}, {"version": "v2", "created": "Mon, 2 Feb 2015 15:26:44 GMT"}, {"version": "v3", "created": "Fri, 22 May 2015 16:41:01 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Puleo", "Gregory J.", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1411.0628", "submitter": "Valerii Sopin", "authors": "Valerii Sopin", "title": "PH = PSPACE", "comments": "The author greatly appreciate suggestions and help of Professor Lew\n  Gordeew", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that PSPACE is equal to 4th level in the polynomial\nhierarchy. We also deduce a lot of important consequences.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 19:41:08 GMT"}, {"version": "v10", "created": "Mon, 19 Sep 2016 19:40:19 GMT"}, {"version": "v11", "created": "Thu, 29 Apr 2021 17:42:35 GMT"}, {"version": "v12", "created": "Mon, 3 May 2021 07:16:41 GMT"}, {"version": "v13", "created": "Sat, 15 May 2021 06:36:59 GMT"}, {"version": "v14", "created": "Tue, 27 Jul 2021 19:02:02 GMT"}, {"version": "v2", "created": "Tue, 4 Nov 2014 15:29:22 GMT"}, {"version": "v3", "created": "Wed, 5 Nov 2014 01:20:47 GMT"}, {"version": "v4", "created": "Tue, 18 Nov 2014 22:45:44 GMT"}, {"version": "v5", "created": "Wed, 21 Jan 2015 08:11:18 GMT"}, {"version": "v6", "created": "Tue, 3 Feb 2015 02:14:00 GMT"}, {"version": "v7", "created": "Wed, 4 Feb 2015 23:09:45 GMT"}, {"version": "v8", "created": "Sun, 8 Feb 2015 01:49:50 GMT"}, {"version": "v9", "created": "Fri, 8 May 2015 15:58:54 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Sopin", "Valerii", ""]]}, {"id": "1411.0644", "submitter": "Kasper Green Larsen", "authors": "Allan Gr{\\o}nlund, Kasper Green Larsen", "title": "Towards Tight Lower Bounds for Range Reporting on the RAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the orthogonal range reporting problem, we are to preprocess a set of $n$\npoints with integer coordinates on a $U \\times U$ grid. The goal is to support\nreporting all $k$ points inside an axis-aligned query rectangle. This is one of\nthe most fundamental data structure problems in databases and computational\ngeometry. Despite the importance of the problem its complexity remains\nunresolved in the word-RAM. On the upper bound side, three best tradeoffs\nexists: (1.) Query time $O(\\lg \\lg n + k)$ with $O(nlg^{\\varepsilon}n)$ words\nof space for any constant $\\varepsilon>0$. (2.) Query time $O((1 + k) \\lg \\lg\nn)$ with $O(n \\lg \\lg n)$ words of space. (3.) Query time\n$O((1+k)\\lg^{\\varepsilon} n)$ with optimal $O(n)$ words of space. However, the\nonly known query time lower bound is $\\Omega(\\log \\log n +k)$, even for linear\nspace data structures.\n  All three current best upper bound tradeoffs are derived by reducing range\nreporting to a ball-inheritance problem. Ball-inheritance is a problem that\nessentially encapsulates all previous attempts at solving range reporting in\nthe word-RAM. In this paper we make progress towards closing the gap between\nthe upper and lower bounds for range reporting by proving cell probe lower\nbounds for ball-inheritance. Our lower bounds are tight for a large range of\nparameters, excluding any further progress for range reporting using the\nball-inheritance reduction.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 20:10:57 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Gr\u00f8nlund", "Allan", ""], ["Larsen", "Kasper Green", ""]]}, {"id": "1411.0763", "submitter": "Xu Yang Dr.", "authors": "Xu Yang, Hong Qiao, and Zhi-Yong Liu", "title": "A Weighted Common Subgraph Matching Algorithm", "comments": "6 pages, 5 figures, the second round revision in IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a weighted common subgraph (WCS) matching algorithm to find the\nmost similar subgraphs in two labeled weighted graphs. WCS matching, as a\nnatural generalization of the equal-sized graph matching or subgraph matching,\nfinds wide applications in many computer vision and machine learning tasks. In\nthis paper, the WCS matching is first formulated as a combinatorial\noptimization problem over the set of partial permutation matrices. Then it is\napproximately solved by a recently proposed combinatorial optimization\nframework - Graduated NonConvexity and Concavity Procedure (GNCCP).\nExperimental comparisons on both synthetic graphs and real world images\nvalidate its robustness against noise level, problem size, outlier number, and\nedge density.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 02:39:46 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Yang", "Xu", ""], ["Qiao", "Hong", ""], ["Liu", "Zhi-Yong", ""]]}, {"id": "1411.0871", "submitter": "Paul Wollan", "authors": "D\\'aniel Marx and Paul Wollan", "title": "An exact characterization of tractable demand patterns for maximum\n  disjoint path problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following general disjoint paths problem: given a supply graph\n$G$, a set $T\\subseteq V(G)$ of terminals, a demand graph $H$ on the vertices\n$T$, and an integer $k$, the task is to find a set of $k$ pairwise\nvertex-disjoint valid paths, where we say that a path of the supply graph $G$\nis valid if its endpoints are in $T$ and adjacent in the demand graph $H$. For\na class $\\mathcal{H}$ of graphs, we denote by $\\mathcal{H}$-Maximum Disjoint\nPaths the restriction of this problem when the demand graph $H$ is assumed to\nbe a member of $\\mathcal{H}$. We study the fixed-parameter tractability of this\nfamily of problems, parameterized by $k$. Our main result is a complete\ncharacterization of the fixed-parameter tractable cases of\n$\\mathcal{H}$-Maximum Disjoint Paths for every hereditary class $\\mathcal{H}$\nof graphs: it turns out that complexity depends on the existence of large\ninduced matchings and large induced skew bicliques in the demand graph $H$ (a\nskew biclique is a bipartite graph on vertices $a_1$, $\\dots$, $a_n$, $b_1$,\n$\\dots$, $b_n$ with $a_i$ and $b_j$ being adjacent if and only if $i\\le j$).\nSpecifically, we prove the following classification for every hereditary class\n$\\mathcal{H}$.\n  1. If $\\mathcal{H}$ does not contain every matching and does not contain\nevery skew biclique, then $\\mathcal{H}$-Maximum Disjoint Paths is FPT.\n  2. If $\\mathcal{H}$ does not contain every matching, but contains every skew\nbiclique, then $\\mathcal{H}$-Maximum Disjoint Paths is W[1]-hard, admits an FPT\napproximation, and the valid paths satisfy an analog of the Erd\\H{o}s-P\\'osa\nproperty.\n  3. If $\\mathcal{H}$ contains every matching, then $\\mathcal{H}$-Maximum\nDisjoint Paths is W[1]-hard and the valid paths do not satisfy the analog of\nthe Erd\\H{o}s-P\\'osa property.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 12:07:03 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Marx", "D\u00e1niel", ""], ["Wollan", "Paul", ""]]}, {"id": "1411.0921", "submitter": "Roland Glantz", "authors": "Roland Glantz, Henning Meyerhenke, Alexander Noe", "title": "Algorithms for Mapping Parallel Processes onto Grid and Torus\n  Architectures", "comments": "Accepted at PDP-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static mapping is the assignment of parallel processes to the processing\nelements (PEs) of a parallel system, where the assignment does not change\nduring the application's lifetime. In our scenario we model an application's\ncomputations and their dependencies by an application graph. This graph is\nfirst partitioned into (nearly) equally sized blocks. These blocks need to\ncommunicate at block boundaries. To assign the processes to PEs, our goal is to\ncompute a communication-efficient bijective mapping between the blocks and the\nPEs.\n  This approach of partitioning followed by bijective mapping has many degrees\nof freedom. Thus, users and developers of parallel applications need to know\nmore about which choices work for which application graphs and which parallel\narchitectures. To this end, we not only develop new mapping algorithms (derived\nfrom known greedy methods). We also perform extensive experiments involving\ndifferent classes of application graphs (meshes and complex networks),\narchitectures of parallel computers (grids and tori), as well as different\npartitioners and mapping algorithms. Surprisingly, the quality of the\npartitions, unless very poor, has little influence on the quality of the\nmapping.\n  More importantly, one of our new mapping algorithms always yields the best\nresults in terms of the quality measure maximum congestion when the application\ngraphs are complex networks. In case of meshes as application graphs, this\nmapping algorithm always leads in terms of maximum congestion AND maximum\ndilation, another common quality measure.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 14:31:19 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 12:18:04 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2015 20:11:49 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Glantz", "Roland", ""], ["Meyerhenke", "Henning", ""], ["Noe", "Alexander", ""]]}, {"id": "1411.0960", "submitter": "Sebastian Berndt", "authors": "Sebastian Berndt and Klaus Jansen and Kim-Manuel Klein", "title": "Fully Dynamic Bin Packing Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fully dynamic bin packing problem, where items arrive and\ndepart in an online fashion and repacking of previously packed items is\nallowed. The goal is, of course, to minimize both the number of bins used as\nwell as the amount of repacking. A recently introduced way of measuring the\nrepacking costs at each timestep is the migration factor, defined as the total\nsize of repacked items divided by the size of an arriving or departing item.\nConcerning the trade-off between number of bins and migration factor, if we\nwish to achieve an asymptotic competitive ration of $1 + \\epsilon$ for the\nnumber of bins, a relatively simple argument proves a lower bound of\n$\\Omega(\\frac{1}{\\epsilon})$ for the migration factor. We establish a nearly\nmatching upper bound of $O(\\frac{1}{\\epsilon}^4 \\log \\frac{1}{\\epsilon})$ using\na new dynamic rounding technique and new ideas to handle small items in a\ndynamic setting such that no amortization is needed. The running time of our\nalgorithm is polynomial in the number of items $n$ and in $\\frac{1}{\\epsilon}$.\nThe previous best trade-off was for an asymptotic competitive ratio of\n$\\frac{5}{4}$ for the bins (rather than $1+\\epsilon$) and needed an amortized\nnumber of $O(\\log n)$ repackings (while in our scheme the number of repackings\nis independent of $n$ and non-amortized).\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 16:47:56 GMT"}, {"version": "v2", "created": "Wed, 14 Jan 2015 12:22:14 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Berndt", "Sebastian", ""], ["Jansen", "Klaus", ""], ["Klein", "Kim-Manuel", ""]]}, {"id": "1411.0973", "submitter": "Andrew Winslow", "authors": "Benjamin Hescott, Caleb Malchik, Andrew Winslow", "title": "More Tight Bounds for Active Self-Assembly Using an Insertion Primitive", "comments": "A subset of the results appear in arXiv:1401.0359 and the proceedings\n  of ESA 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.ET cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove several limits on the behavior of a model of self-assembling\nparticles introduced by Dabby and Chen (SODA 2013), called insertion systems,\nwhere monomers insert themselves into the middle of a growing linear polymer.\nFirst, we prove that the expressive power of these systems is equal to\ncontext-free grammars, answering a question posed by Dabby and Chen.\n  Second, we give tight bounds on the maximum length and minimum expected time\nof constructed polymers in systems of three increasingly restricted classes. We\nprove that systems of $k$ monomer types can deterministically construct\npolymers of length $n = 2^{\\Theta(k^{3/2})}$ in $O(\\log^{5/3}(n))$ expected\ntime. We also prove that if non-deterministic construction of a finite number\nof polymers is permitted, then the expected construction time can be reduced to\n$O(\\log^{3/2}(n))$ at the trade-off of decreasing the length to\n$2^{\\Theta(k)}$. If the system is allowed to construct an infinite number of\npolymers, then constructing polymers of unbounded length in $O(\\log{n})$\nexpected time is possible. We follow these positive results with a set of lower\nbounds proving that these are the best possible polymer lengths and expected\nconstruction times.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 17:14:44 GMT"}, {"version": "v2", "created": "Thu, 13 Nov 2014 14:47:06 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Hescott", "Benjamin", ""], ["Malchik", "Caleb", ""], ["Winslow", "Andrew", ""]]}, {"id": "1411.0998", "submitter": "Justin Hsu", "authors": "Justin Hsu, Zhiyi Huang, Aaron Roth, Zhiwei Steven Wu", "title": "Jointly Private Convex Programming", "comments": null, "journal-ref": null, "doi": "10.1137/1.9781611974331.ch43", "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an extremely general method for approximately\nsolving a large family of convex programs where the solution can be divided\nbetween different agents, subject to joint differential privacy. This class\nincludes multi-commodity flow problems, general allocation problems, and\nmulti-dimensional knapsack problems, among other examples. The accuracy of our\nalgorithm depends on the \\emph{number} of constraints that bind between\nindividuals, but crucially, is \\emph{nearly independent} of the number of\nprimal variables and hence the number of agents who make up the problem. As the\nnumber of agents in a problem grows, the error we introduce often becomes\nnegligible.\n  We also consider the setting where agents are strategic and have preferences\nover their part of the solution. For any convex program in this class that\nmaximizes \\emph{social welfare}, there is a generic reduction that makes the\ncorresponding optimization \\emph{approximately dominant strategy truthful} by\ncharging agents prices for resources as a function of the approximately optimal\ndual variables, which are themselves computed under differential privacy. Our\nresults substantially expand the class of problems that are known to be\nsolvable under both privacy and incentive constraints.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 18:46:50 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 18:53:06 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Hsu", "Justin", ""], ["Huang", "Zhiyi", ""], ["Roth", "Aaron", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1411.1087", "submitter": "Praneeth Netrapalli", "authors": "Prateek Jain and Praneeth Netrapalli", "title": "Fast Exact Matrix Completion with Finite Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion is the problem of recovering a low rank matrix by observing\na small fraction of its entries. A series of recent works [KOM12,JNS13,HW14]\nhave proposed fast non-convex optimization based iterative algorithms to solve\nthis problem. However, the sample complexity in all these results is\nsub-optimal in its dependence on the rank, condition number and the desired\naccuracy.\n  In this paper, we present a fast iterative algorithm that solves the matrix\ncompletion problem by observing $O(nr^5 \\log^3 n)$ entries, which is\nindependent of the condition number and the desired accuracy. The run time of\nour algorithm is $O(nr^7\\log^3 n\\log 1/\\epsilon)$ which is near linear in the\ndimension of the matrix. To the best of our knowledge, this is the first near\nlinear time algorithm for exact matrix completion with finite sample complexity\n(i.e. independent of $\\epsilon$).\n  Our algorithm is based on a well known projected gradient descent method,\nwhere the projection is onto the (non-convex) set of low rank matrices. There\nare two key ideas in our result: 1) our argument is based on a $\\ell_{\\infty}$\nnorm potential function (as opposed to the spectral norm) and provides a novel\nway to obtain perturbation bounds for it. 2) we prove and use a natural\nextension of the Davis-Kahan theorem to obtain perturbation bounds on the best\nlow rank approximation of matrices with good eigen-gap. Both of these ideas may\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 21:16:23 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Jain", "Prateek", ""], ["Netrapalli", "Praneeth", ""]]}, {"id": "1411.1124", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Lorenzo Orecchia", "title": "Nearly Linear-Time Packing and Covering LP Solvers", "comments": "journal version (to appear in Mathematical Programming)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Packing and covering linear programs (PC-LPs) form an important class of\nlinear programs (LPs) across computer science, operations research, and\noptimization. In 1993, Luby and Nisan constructed an iterative algorithm for\napproximately solving PC-LPs in nearly linear time, where the time complexity\nscales nearly linearly in $N$, the number of nonzero entries of the matrix, and\npolynomially in $\\varepsilon$, the (multiplicative) approximation error.\nUnfortunately, all existing nearly linear-time algorithms for solving PC-LPs\nrequire time at least proportional to $\\varepsilon^{-2}$.\n  In this paper, we break this longstanding barrier by designing a packing\nsolver that runs in time $\\tilde{O}(N \\varepsilon^{-1})$ and covering LP solver\nthat runs in time $\\tilde{O}(N \\varepsilon^{-1.5})$. Our packing solver can be\nextended to run in time $\\tilde{O}(N \\varepsilon^{-1})$ for a class of\nwell-behaved covering programs. In a follow-up work, Wang et al. showed that\nall covering LPs can be converted into well-behaved ones by a reduction that\nblows up the problem size only logarithmically.\n  At high level, these two algorithms can be described as linear couplings of\nseveral first-order descent steps. This is an application of our linear\ncoupling technique to problems that are not amenable to blackbox applications\nknown iterative algorithms in convex optimization.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 01:09:04 GMT"}, {"version": "v2", "created": "Sun, 7 Aug 2016 20:08:57 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 23:29:06 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1411.1209", "submitter": "Peter Sanders", "authors": "Hamza Rihani, Peter Sanders, Roman Dementiev", "title": "MultiQueues: Simpler, Faster, and Better Relaxed Concurrent Priority\n  Queues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Priority queues with parallel access are an attractive data structure for\napplications like prioritized online scheduling, discrete event simulation, or\nbranch-and-bound. However, a classical priority queue constitutes a severe\nbottleneck in this context, leading to very small throughput. Hence, there has\nbeen significant interest in concurrent priority queues with a somewhat relaxed\nsemantics where deleted elements only need to be close to the minimum. In this\npaper we present a very simple approach based on multiple sequential priority\nqueues. It turns out to outperform previous more complicated data structures\nwhile at the same time improving the quality of the returned elements.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 10:08:56 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Rihani", "Hamza", ""], ["Sanders", "Peter", ""], ["Dementiev", "Roman", ""]]}, {"id": "1411.1220", "submitter": "Peter Sanders", "authors": "Jonathan Dimond and Peter Sanders", "title": "Faster Exact Search using Document Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how full-text search based on inverted indices can be accelerated by\nclustering the documents without losing results (SeCluD -- SEarch with\nCLUstered Documents). We develop a fast multilevel clustering algorithm that\nexplicitly uses query cost for conjunctive queries as an objective function.\nDepending on the inputs we get up to four times faster than non-clustered\nsearch. The resulting clusters are also useful for data compression and for\ndistributing the work over many machines.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 10:42:29 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Dimond", "Jonathan", ""], ["Sanders", "Peter", ""]]}, {"id": "1411.1263", "submitter": "Tigran Tonoyan", "authors": "Magnus M. Halldorsson and Tigran Tonoyan", "title": "How Well Can Graphs Represent Wireless Interference?", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Efficient use of a wireless network requires that transmissions be grouped\ninto feasible sets, where feasibility means that each transmission can be\nsuccessfully decoded in spite of the interference caused by simultaneous\ntransmissions. Feasibility is most closely modeled by a\nsignal-to-interference-plus-noise (SINR) formula, which unfortunately is\nconceptually complicated, being an asymmetric, cumulative, many-to-one\nrelationship. We re-examine how well graphs can capture wireless receptions as\nencoded in SINR relationships, placing them in a framework in order to\nunderstand the limits of such modelling. We seek for each wireless instance a\npair of graphs that provide upper and lower bounds on the feasibility relation,\nwhile aiming to minimize the gap between the two graphs. The cost of a graph\nformulation is the worst gap over all instances, and the price of (graph)\nabstraction is the smallest cost of a graph formulation. We propose a family of\nconflict graphs that is parameterized by a non-decreasing sub-linear function,\nand show that with a judicious choice of functions, the graphs can capture\nfeasibility with a cost of $O(\\log^* \\Delta)$, where $\\Delta$ is the ratio\nbetween the longest and the shortest link length. This holds on the plane and\nmore generally in doubling metrics. We use this to give greatly improved\n$O(\\log^* \\Delta)$-approximation for fundamental link scheduling problems with\narbitrary power control. We explore the limits of graph representations and\nfind that our upper bound is tight: the price of graph abstraction is\n$\\Omega(\\log^* \\Delta)$. We also give strong impossibility results for general\nmetrics, and for approximations in terms of the number of links.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 13:09:07 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Halldorsson", "Magnus M.", ""], ["Tonoyan", "Tigran", ""]]}, {"id": "1411.1279", "submitter": "Seyoung Yun", "authors": "Se-Young Yun and Marc Lelarge and Alexandre Proutiere", "title": "Streaming, Memory Limited Algorithms for Community Detection", "comments": "NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider sparse networks consisting of a finite number of\nnon-overlapping communities, i.e. disjoint clusters, so that there is higher\ndensity within clusters than across clusters. Both the intra- and inter-cluster\nedge densities vanish when the size of the graph grows large, making the\ncluster reconstruction problem nosier and hence difficult to solve. We are\ninterested in scenarios where the network size is very large, so that the\nadjacency matrix of the graph is hard to manipulate and store. The data stream\nmodel in which columns of the adjacency matrix are revealed sequentially\nconstitutes a natural framework in this setting. For this model, we develop two\nnovel clustering algorithms that extract the clusters asymptotically\naccurately. The first algorithm is {\\it offline}, as it needs to store and keep\nthe assignments of nodes to clusters, and requires a memory that scales\nlinearly with the network size. The second algorithm is {\\it online}, as it may\nclassify a node when the corresponding column is revealed and then discard this\ninformation. This algorithm requires a memory growing sub-linearly with the\nnetwork size. To construct these efficient streaming memory-limited clustering\nalgorithms, we first address the problem of clustering with partial\ninformation, where only a small proportion of the columns of the adjacency\nmatrix is observed and develop, for this setting, a new spectral algorithm\nwhich is of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 21:42:49 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Yun", "Se-Young", ""], ["Lelarge", "Marc", ""], ["Proutiere", "Alexandre", ""]]}, {"id": "1411.1319", "submitter": "Avery Miller", "authors": "Avery Miller, Andrzej Pelc", "title": "Election vs. Selection: Two Ways of Finding the Largest Node in a Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the node with the largest label in a network, modeled as an\nundirected connected graph, is one of the fundamental problems in distributed\ncomputing. This is the way in which $\\textit{leader election}$ is usually\nsolved. We consider two distinct tasks in which the largest-labeled node is\nfound deterministically. In $\\textit{selection}$, this node must output 1 and\nall other nodes must output 0. In $\\textit{election}$, the other nodes must\nadditionally learn the largest label. Our aim is to compare the difficulty of\nthese two tasks executed under stringent running time constraints. The measure\nof difficulty is the amount of information that nodes of the network must\ninitially possess in order to solve the given task in an imposed amount of\ntime. Following the standard framework of $\\textit{algorithms with advice}$,\nthis information (a single binary string) is provided to all nodes at the start\nby an oracle knowing the entire graph. The length of this string is called the\n$\\textit{size of advice}$. Consider the class of $n$-node graphs with any\ndiameter $diam \\leq D$. If time is larger than $diam$, then both tasks can be\nsolved without advice. For the task of $\\textit{election}$, we show that if\ntime is smaller than $diam$, then the optimal size of advice is $\\Theta(\\log\nn)$, and if time is exactly $diam$, then the optimal size of advice is\n$\\Theta(\\log D)$. For the task of $\\textit{selection}$, the situation changes\ndramatically, even within the class of rings. Indeed, for the class of rings,\nwe show that, if time is $O(diam^{\\epsilon})$, for any $\\epsilon <1$, then the\noptimal size of advice is $\\Theta(\\log D)$, and, if time is $\\Theta(diam)$ (and\nat most $diam$) then this optimal size is $\\Theta(\\log \\log D)$.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 16:48:34 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Miller", "Avery", ""], ["Pelc", "Andrzej", ""]]}, {"id": "1411.1519", "submitter": "Yannik Stein", "authors": "Wolfgang Mulzer, Huy L. Nguyen, Paul Seiferth, Yannik Stein", "title": "Approximate k-flat Nearest Neighbor Search", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $k$ be a nonnegative integer. In the approximate $k$-flat nearest\nneighbor ($k$-ANN) problem, we are given a set $P \\subset \\mathbb{R}^d$ of $n$\npoints in $d$-dimensional space and a fixed approximation factor $c > 1$. Our\ngoal is to preprocess $P$ so that we can efficiently answer approximate\n$k$-flat nearest neighbor queries: given a $k$-flat $F$, find a point in $P$\nwhose distance to $F$ is within a factor $c$ of the distance between $F$ and\nthe closest point in $P$. The case $k = 0$ corresponds to the well-studied\napproximate nearest neighbor problem, for which a plethora of results are\nknown, both in low and high dimensions. The case $k = 1$ is called approximate\nline nearest neighbor. In this case, we are aware of only one provably\nefficient data structure, due to Andoni, Indyk, Krauthgamer, and Nguyen. For $k\n\\geq 2$, we know of no previous results.\n  We present the first efficient data structure that can handle approximate\nnearest neighbor queries for arbitrary $k$. We use a data structure for\n$0$-ANN-queries as a black box, and the performance depends on the parameters\nof the $0$-ANN solution: suppose we have an $0$-ANN structure with query time\n$O(n^{\\rho})$ and space requirement $O(n^{1+\\sigma})$, for $\\rho, \\sigma > 0$.\nThen we can answer $k$-ANN queries in time $O(n^{k/(k + 1 - \\rho) + t})$ and\nspace $O(n^{1+\\sigma k/(k + 1 - \\rho)} + n\\log^{O(1/t)} n)$. Here, $t > 0$ is\nan arbitrary constant and the $O$-notation hides exponential factors in $k$,\n$1/t$, and $c$ and polynomials in $d$. Our new data structures also give an\nimprovement in the space requirement over the previous result for $1$-ANN: we\ncan achieve near-linear space and sublinear query time, a further step towards\npractical applications where space constitutes the bottleneck.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 08:13:15 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Mulzer", "Wolfgang", ""], ["Nguyen", "Huy L.", ""], ["Seiferth", "Paul", ""], ["Stein", "Yannik", ""]]}, {"id": "1411.1546", "submitter": "Michael Mahoney", "authors": "Aaron B. Adcock and Blair D. Sullivan and Michael W. Mahoney", "title": "Tree decompositions and social graphs", "comments": "v2 has 44 pages, 21 figures, 7 tables, 107 references. To appear in\n  Internet Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has established that large informatics graphs such as social and\ninformation networks have non-trivial tree-like structure when viewed at\nmoderate size scales. Here, we present results from the first detailed\nempirical evaluation of the use of tree decomposition (TD) heuristics for\nstructure identification and extraction in social graphs. Although TDs have\nhistorically been used in structural graph theory and scientific computing, we\nshow that---even with existing TD heuristics developed for those very different\nareas---TD methods can identify interesting structure in a wide range of\nrealistic informatics graphs. Our main contributions are the following: we show\nthat TD methods can identify structures that correlate strongly with the\ncore-periphery structure of realistic networks, even when using simple greedy\nheuristics; we show that the peripheral bags of these TDs correlate well with\nlow-conductance communities (when they exist) found using local spectral\ncomputations; and we show that several types of large-scale \"ground-truth\"\ncommunities, defined by demographic metadata on the nodes of the network, are\nwell-localized in the large-scale and/or peripheral structures of the TDs. Our\nother main contributions are the following: we provide detailed empirical\nresults for TD heuristics on toy and synthetic networks to establish a baseline\nto understand better the behavior of the heuristics on more complex real-world\nnetworks; and we prove a theorem providing formal justification for the\nintuition that the only two impediments to low-distortion hyperbolic embedding\nare high tree-width and long geodesic cycles. Our results suggest future\ndirections for improved TD heuristics that are more appropriate for realistic\nsocial graphs.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 09:53:15 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 19:58:18 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Adcock", "Aaron B.", ""], ["Sullivan", "Blair D.", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1411.1646", "submitter": "Frank-Michael Schleif", "authors": "Andrej Gisbrecht, Frank-Michael Schleif", "title": "Metric and non-metric proximity transformations at linear costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain specific (dis-)similarity or proximity measures used e.g. in alignment\nalgorithms of sequence data, are popular to analyze complex data objects and to\ncover domain specific data properties. Without an underlying vector space these\ndata are given as pairwise (dis-)similarities only. The few available methods\nfor such data focus widely on similarities and do not scale to large data sets.\nKernel methods are very effective for metric similarity matrices, also at large\nscale, but costly transformations are necessary starting with non-metric (dis-)\nsimilarities. We propose an integrative combination of Nystroem approximation,\npotential double centering and eigenvalue correction to obtain valid kernel\nmatrices at linear costs in the number of samples. By the proposed approach\neffective kernel approaches, become accessible. Experiments with several larger\n(dis-)similarity data sets show that the proposed method achieves much better\nruntime performance than the standard strategy while keeping competitive model\naccuracy. The main contribution is an efficient and accurate technique, to\nconvert (potentially non-metric) large scale dissimilarity matrices into\napproximated positive semi-definite kernel matrices at linear costs.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 16:03:26 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Gisbrecht", "Andrej", ""], ["Schleif", "Frank-Michael", ""]]}, {"id": "1411.1879", "submitter": "Carsten Grimm", "authors": "Prosenjit Bose, Jean-Lou De Carufel, Carsten Grimm, Anil Maheshwari,\n  Michiel Smid", "title": "Optimal Data Structures for Farthest-Point Queries in Cactus Networks", "comments": null, "journal-ref": null, "doi": "10.7155/jgaa.00345", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the continuum of points on the edges of a network, i.e., a\nconnected, undirected graph with positive edge weights. We measure the distance\nbetween these points in terms of the weighted shortest path distance, called\nthe network distance. Within this metric space, we study farthest points and\nfarthest distances. We introduce optimal data structures supporting queries for\nthe farthest distance and the farthest points on trees, cycles, uni-cyclic\nnetworks, and cactus networks.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 10:32:27 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Bose", "Prosenjit", ""], ["De Carufel", "Jean-Lou", ""], ["Grimm", "Carsten", ""], ["Maheshwari", "Anil", ""], ["Smid", "Michiel", ""]]}, {"id": "1411.1919", "submitter": "Seth Pettie", "authors": "Ran Duan, Seth Pettie, Hsin-Hao Su", "title": "Scaling Algorithms for Weighted Matching in General Graphs", "comments": "Extended abstract published in SODA'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new scaling algorithm for maximum (or minimum) weight perfect\nmatching on general, edge weighted graphs. Our algorithm runs in\n$O(m\\sqrt{n}\\log(nN))$ time, $O(m\\sqrt{n})$ per scale, which matches the\nrunning time of the best cardinality matching algorithms on sparse graphs. Here\n$m,n,$ and $N$ bound the number of edges, vertices, and magnitude of any edge\nweight. Our result improves on a 25-year old algorithm of Gabow and Tarjan,\nwhich runs in $O(m\\sqrt{n\\log n\\alpha(m,n)} \\log(nN))$ time.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 14:11:06 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2015 07:26:47 GMT"}, {"version": "v3", "created": "Tue, 5 Apr 2016 20:48:13 GMT"}, {"version": "v4", "created": "Sat, 23 Jul 2016 00:31:20 GMT"}, {"version": "v5", "created": "Tue, 1 Nov 2016 02:00:09 GMT"}, {"version": "v6", "created": "Mon, 9 Oct 2017 02:27:28 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Duan", "Ran", ""], ["Pettie", "Seth", ""], ["Su", "Hsin-Hao", ""]]}, {"id": "1411.1977", "submitter": "Pascal Schweitzer", "authors": "Pascal Schweitzer", "title": "Towards an Isomorphism Dichotomy for Hereditary Graph Classes", "comments": "37 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we resolve the complexity of the isomorphism problem on all but\nfinitely many of the graph classes characterized by two forbidden induced\nsubgraphs. To this end we develop new techniques applicable for the structural\nand algorithmic analysis of graphs. First, we develop a methodology to show\nisomorphism completeness of the isomorphism problem on graph classes by\nproviding a general framework unifying various reduction techniques. Second, we\ngeneralize the concept of the modular decomposition to colored graphs, allowing\nfor non-standard decompositions. We show that, given a suitable decomposition\nfunctor, the graph isomorphism problem reduces to checking isomorphism of\ncolored prime graphs. Third, we extend the techniques of bounded color valence\nand hypergraph isomorphism on hypergraphs of bounded color size as follows. We\nsay a colored graph has generalized color valence at most k if, after removing\nall vertices in color classes of size at most k, for each color class C every\nvertex has at most k neighbors in C or at most k non-neighbors in C. We show\nthat isomorphism of graphs of bounded generalized color valence can be solved\nin polynomial time.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 16:58:25 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Schweitzer", "Pascal", ""]]}, {"id": "1411.2021", "submitter": "He Sun", "authors": "Richard Peng and He Sun and Luca Zanetti", "title": "Partitioning Well-Clustered Graphs: Spectral Clustering Works!", "comments": "A preliminary version of this paper appeared in COLT'15; the full\n  version is to appear in SIAM Journal on Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study variants of the widely used spectral clustering that\npartitions a graph into k clusters by (1) embedding the vertices of a graph\ninto a low-dimensional space using the bottom eigenvectors of the Laplacian\nmatrix, and (2) grouping the embedded points into k clusters via k-means\nalgorithms. We show that, for a wide class of graphs, spectral clustering gives\na good approximation of the optimal clustering. While this approach was\nproposed in the early 1990s and has comprehensive applications, prior to our\nwork similar results were known only for graphs generated from stochastic\nmodels.\n  We also give a nearly-linear time algorithm for partitioning well-clustered\ngraphs based on computing a matrix exponential and approximate nearest neighbor\ndata structures.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 20:23:50 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 17:08:00 GMT"}, {"version": "v3", "created": "Tue, 31 Jan 2017 15:07:33 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Peng", "Richard", ""], ["Sun", "He", ""], ["Zanetti", "Luca", ""]]}, {"id": "1411.2022", "submitter": "Dmitry Kosolobov", "authors": "Dmitry Kosolobov", "title": "Online Square Detection", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The online square detection problem is to detect the first occurrence of a\nsquare in a string whose characters are provided as input one at a time. Recall\nthat a square is a string that is a concatenation of two identical strings. In\nthis paper we present an algorithm solving this problem in $O(n\\log\\sigma)$\ntime and linear space on ordered alphabet, where $\\sigma$ is the number of\ndifferent letters in the input string. Our solution is relatively simple and\ndoes not require much memory unlike the previously known online algorithm with\nthe same working time. Also we present an algorithm working in $O(n\\log n)$\ntime and linear space on unordered alphabet, though this solution does not\noutperform the previously known result with the same time bound.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 20:28:37 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Kosolobov", "Dmitry", ""]]}, {"id": "1411.2059", "submitter": "Sebastian Wild", "authors": "Conrado Mart\\'inez, Markus E. Nebel, Sebastian Wild", "title": "Analysis of Branch Misses in Quicksort", "comments": "to be presented at ANALCO 2015", "journal-ref": null, "doi": "10.1137/1.9781611973761.11", "report-no": null, "categories": "cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of algorithms mostly relies on counting classic elementary\noperations like additions, multiplications, comparisons, swaps etc. This\napproach is often sufficient to quantify an algorithm's efficiency. In some\ncases, however, features of modern processor architectures like pipelined\nexecution and memory hierarchies have significant impact on running time and\nneed to be taken into account to get a reliable picture. One such example is\nQuicksort: It has been demonstrated experimentally that under certain\nconditions on the hardware the classically optimal balanced choice of the pivot\nas median of a sample gets harmful. The reason lies in mispredicted branches\nwhose rollback costs become dominating.\n  In this paper, we give the first precise analytical investigation of the\ninfluence of pipelining and the resulting branch mispredictions on the\nefficiency of (classic) Quicksort and Yaroslavskiy's dual-pivot Quicksort as\nimplemented in Oracle's Java 7 library. For the latter it is still not fully\nunderstood why experiments prove it 10% faster than a highly engineered\nimplementation of a classic single-pivot version. For different branch\nprediction strategies, we give precise asymptotics for the expected number of\nbranch misses caused by the aforementioned Quicksort variants when their pivots\nare chosen from a sample of the input. We conclude that the difference in\nbranch misses is too small to explain the superiority of the dual-pivot\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 23:20:37 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Mart\u00ednez", "Conrado", ""], ["Nebel", "Markus E.", ""], ["Wild", "Sebastian", ""]]}, {"id": "1411.2250", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ognjen Arandjelovic, Duc-Son Pham, Svetha Venkatesh", "title": "Two maximum entropy based algorithms for running quantile estimation in\n  non-stationary data streams", "comments": "IEEE Transactions on Circuits and Systems for Video Technology, 2014.\n  arXiv admin note: text overlap with arXiv:1409.7289", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to estimate a particular quantile of a distribution is an important\nproblem which frequently arises in many computer vision and signal processing\napplications. For example, our work was motivated by the requirements of many\nsemi-automatic surveillance analytics systems which detect abnormalities in\nclose-circuit television (CCTV) footage using statistical models of low-level\nmotion features. In this paper we specifically address the problem of\nestimating the running quantile of a data stream when the memory for storing\nobservations is limited. We make several major contributions: (i) we highlight\nthe limitations of approaches previously described in the literature which make\nthem unsuitable for non-stationary streams, (ii) we describe a novel principle\nfor the utilization of the available storage space, (iii) we introduce two\nnovel algorithms which exploit the proposed principle in different ways, and\n(iv) we present a comprehensive evaluation and analysis of the proposed\nalgorithms and the existing methods in the literature on both synthetic data\nsets and three large real-world streams acquired in the course of operation of\nan existing commercial surveillance system. Our findings convincingly\ndemonstrate that both of the proposed methods are highly successful and vastly\noutperform the existing alternatives. We show that the better of the two\nalgorithms (data-aligned histogram) exhibits far superior performance in\ncomparison with the previously described methods, achieving more than 10 times\nlower estimate errors on real-world data, even when its available working\nmemory is an order of magnitude smaller.\n", "versions": [{"version": "v1", "created": "Sun, 9 Nov 2014 16:05:45 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Arandjelovic", "Ognjen", ""], ["Pham", "Duc-Son", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1411.2262", "submitter": "Khaled Elbassioni", "authors": "Khaled Elbassioni", "title": "A Polynomial Delay Algorithm for Generating Connected Induced Subgraphs\n  of a Given Cardinality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a polynomial delay algorithm, that for any graph $G$ and positive\ninteger $k$, enumerates all connected induced subgraphs of $G$ of order $k$.\nOur algorithm enumerates each subgraph in at most\n$O((k\\min\\{(n-k),k\\Delta\\})^2(\\Delta+\\log k))$ and uses linear space $O(n+m)$,\nwhere $n$ and $m$ are respectively the number of vertices and edges of $G$ and\n$\\Delta$ is the maximum degree.\n", "versions": [{"version": "v1", "created": "Sun, 9 Nov 2014 18:20:52 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2015 07:31:43 GMT"}, {"version": "v3", "created": "Mon, 22 Aug 2016 14:43:48 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Elbassioni", "Khaled", ""]]}, {"id": "1411.2311", "submitter": "Jos\\'e A. Soto", "authors": "Jos\\'e A. Soto, Claudio Telha", "title": "Independent sets and hitting sets of bicolored rectangular families", "comments": "36 pages, A preliminary version of this work appeared in IPCO 2011\n  under the name \"Jump Number of Two-Directional Orthogonal Ray Graphs\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bicolored rectangular family BRF is a collection of all axis-parallel\nrectangles contained in a given region Z of the plane formed by selecting a\nbottom-left corner from a set A and an upper-right corner from a set B. We\nprove that the maximum independent set and the minimum hitting set of a BRF\nhave the same cardinality and devise polynomial time algorithms to compute\nboth. As a direct consequence, we obtain the first polynomial time algorithm to\ncompute minimum biclique covers, maximum cross-free matchings and jump numbers\nin a class of bipartite graphs that significantly extends convex bipartite\ngraphs and interval bigraphs. We also establish several connections between our\nwork and other seemingly unrelated problems. Furthermore, when the bicolored\nrectangular family is weighted, we show that the problem of finding the maximum\nweight of an independent set is NP-hard, and provide efficient algorithms to\nsolve it on certain subclasses.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 02:09:58 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Soto", "Jos\u00e9 A.", ""], ["Telha", "Claudio", ""]]}, {"id": "1411.2344", "submitter": "Arnab Bhattacharyya", "authors": "Arnab Bhattacharyya and Vineet Nair", "title": "An explicit sparse recovery scheme in the L1-norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the approximate sparse recovery problem: given Ax, where A is a\nknown m-by-n dimensional matrix and x is an unknown (approximately) sparse\nn-dimensional vector, recover an approximation to x. The goal is to design the\nmatrix A such that m is small and recovery is efficient. Moreover, it is often\ndesirable for A to have other nice properties, such as explicitness, sparsity,\nand discreteness.\n  In this work, we show that we can use spectral expander graphs to explicitly\ndesign binary matrices A for which the column sparsity is optimal and for which\nthere is an efficient recovery algorithm (l1-minimization). In order to recover\nx that is close to {\\delta}n-sparse (where {\\delta} is a constant), we design\nan explicit binary matrix A that has m = O(sqrt{{\\delta}} log(1/{\\delta}) * n)\nrows and has O(log(1/{\\delta})) ones in each column. Previous such\nconstructions were based on unbalanced bipartite graphs with high vertex\nexpansion, for which we currently do not have explicit constructions. In\nparticular, ours is the first explicit non-trivial construction of a\nmeasurement matrix A such that Ax can be computed in O(n log(1/{\\delta})) time.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 07:59:37 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Nair", "Vineet", ""]]}, {"id": "1411.2404", "submitter": "Jelani Nelson", "authors": "Kasper Green Larsen, Jelani Nelson", "title": "The Johnson-Lindenstrauss lemma is optimal for linear dimensionality\n  reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CG cs.DS math.FA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any $n>1$ and $0<\\varepsilon<1/2$, we show the existence of an\n$n^{O(1)}$-point subset $X$ of $\\mathbb{R}^n$ such that any linear map from\n$(X,\\ell_2)$ to $\\ell_2^m$ with distortion at most $1+\\varepsilon$ must have $m\n= \\Omega(\\min\\{n, \\varepsilon^{-2}\\log n\\})$. Our lower bound matches the upper\nbounds provided by the identity matrix and the Johnson-Lindenstrauss lemma,\nimproving the previous lower bound of Alon by a $\\log(1/\\varepsilon)$ factor.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 12:53:41 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Larsen", "Kasper Green", ""], ["Nelson", "Jelani", ""]]}, {"id": "1411.2577", "submitter": "Ilya Razenshteyn", "authors": "Alexandr Andoni, Robert Krauthgamer, Ilya Razenshteyn", "title": "Sketching and Embedding are Equivalent for Norms", "comments": "33 pages, an extended abstract appeared in the proceedings of the\n  47th ACM Symposium on Theory of Computing (STOC 2015); changes in v2: added\n  quantitative bounds for the main results, preliminaries section with\n  necessary definitions and facts has been added; v3: several clarifications,\n  including a section on the basics of communication complexity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An outstanding open question posed by Guha and Indyk in 2006 asks to\ncharacterize metric spaces in which distances can be estimated using efficient\nsketches. Specifically, we say that a sketching algorithm is efficient if it\nachieves constant approximation using constant sketch size. A well-known result\nof Indyk (J. ACM, 2006) implies that a metric that admits a constant-distortion\nembedding into $\\ell_p$ for $p\\in(0,2]$ also admits an efficient sketching\nscheme. But is the converse true, i.e., is embedding into $\\ell_p$ the only way\nto achieve efficient sketching?\n  We address these questions for the important special case of normed spaces,\nby providing an almost complete characterization of sketching in terms of\nembeddings. In particular, we prove that a finite-dimensional normed space\nallows efficient sketches if and only if it embeds (linearly) into\n$\\ell_{1-\\varepsilon}$ with constant distortion. We further prove that for\nnorms that are closed under sum-product, efficient sketching is equivalent to\nembedding into $\\ell_1$ with constant distortion. Examples of such norms\ninclude the Earth Mover's Distance (specifically its norm variant, called\nKantorovich-Rubinstein norm), and the trace norm (a.k.a. Schatten $1$-norm or\nthe nuclear norm). Using known non-embeddability theorems for these norms by\nNaor and Schechtman (SICOMP, 2007) and by Pisier (Compositio. Math., 1978), we\nthen conclude that these spaces do not admit efficient sketches either, making\nprogress towards answering another open question posed by Indyk in 2006.\n  Finally, we observe that resolving whether \"sketching is equivalent to\nembedding into $\\ell_1$ for general norms\" (i.e., without the above\nrestriction) is equivalent to resolving a well-known open problem in Functional\nAnalysis posed by Kwapien in 1969.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 20:42:51 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2015 13:02:08 GMT"}, {"version": "v3", "created": "Wed, 15 Feb 2017 15:03:49 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Andoni", "Alexandr", ""], ["Krauthgamer", "Robert", ""], ["Razenshteyn", "Ilya", ""]]}, {"id": "1411.2647", "submitter": "Christina Lee Yu", "authors": "Asuman Ozdaglar, Devavrat Shah, and Christina Lee Yu", "title": "Asynchronous Approximation of a Single Component of the Solution to a\n  Linear System", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIT LIDS Report 3172", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a distributed asynchronous algorithm for approximating a single\ncomponent of the solution to a system of linear equations $Ax = b$, where $A$\nis a positive definite real matrix, and $b \\in \\mathbb{R}^n$. This is\nequivalent to solving for $x_i$ in $x = Gx + z$ for some $G$ and $z$ such that\nthe spectral radius of $G$ is less than 1. Our algorithm relies on the Neumann\nseries characterization of the component $x_i$, and is based on residual\nupdates. We analyze our algorithm within the context of a cloud computation\nmodel, in which the computation is split into small update tasks performed by\nsmall processors with shared access to a distributed file system. We prove a\nrobust asymptotic convergence result when the spectral radius $\\rho(|G|) < 1$,\nregardless of the precise order and frequency in which the update tasks are\nperformed. We provide convergence rate bounds which depend on the order of\nupdate tasks performed, analyzing both deterministic update rules via counting\nweighted random walks, as well as probabilistic update rules via concentration\nbounds. The probabilistic analysis requires analyzing the product of random\nmatrices which are drawn from distributions that are time and path dependent.\nWe specifically consider the setting where $n$ is large, yet $G$ is sparse,\ne.g., each row has at most $d$ nonzero entries. This is motivated by\napplications in which $G$ is derived from the edge structure of an underlying\ngraph. Our results prove that if the local neighborhood of the graph does not\ngrow too quickly as a function of $n$, our algorithm can provide significant\nreduction in computation cost as opposed to any algorithm which computes the\nglobal solution vector $x$. Our algorithm obtains an $\\epsilon \\|x\\|_2$\nadditive approximation for $x_i$ in constant time with respect to the size of\nthe matrix when the maximum row sparsity $d = O(1)$ and $1/(1-\\|G\\|_2) = O(1)$.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 22:15:17 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 13:07:38 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2015 20:08:03 GMT"}, {"version": "v4", "created": "Mon, 21 Jan 2019 23:03:26 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Ozdaglar", "Asuman", ""], ["Shah", "Devavrat", ""], ["Yu", "Christina Lee", ""]]}, {"id": "1411.2664", "submitter": "Vitaly Feldman", "authors": "Cynthia Dwork and Vitaly Feldman and Moritz Hardt and Toniann Pitassi\n  and Omer Reingold and Aaron Roth", "title": "Preserving Statistical Validity in Adaptive Data Analysis", "comments": "Updated related work with recent developments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A great deal of effort has been devoted to reducing the risk of spurious\nscientific discoveries, from the use of sophisticated validation techniques, to\ndeep statistical methods for controlling the false discovery rate in multiple\nhypothesis testing. However, there is a fundamental disconnect between the\ntheoretical results and the practice of data analysis: the theory of\nstatistical inference assumes a fixed collection of hypotheses to be tested, or\nlearning algorithms to be applied, selected non-adaptively before the data are\ngathered, whereas in practice data is shared and reused with hypotheses and new\nanalyses being generated on the basis of data exploration and the outcomes of\nprevious analyses.\n  In this work we initiate a principled study of how to guarantee the validity\nof statistical inference in adaptive data analysis. As an instance of this\nproblem, we propose and investigate the question of estimating the expectations\nof $m$ adaptively chosen functions on an unknown distribution given $n$ random\nsamples.\n  We show that, surprisingly, there is a way to estimate an exponential in $n$\nnumber of expectations accurately even if the functions are chosen adaptively.\nThis gives an exponential improvement over standard empirical estimators that\nare limited to a linear number of estimates. Our result follows from a general\ntechnique that counter-intuitively involves actively perturbing and\ncoordinating the estimates, using techniques developed for privacy\npreservation. We give additional applications of this technique to our\nquestion.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 23:44:49 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 20:57:38 GMT"}, {"version": "v3", "created": "Wed, 2 Mar 2016 07:04:07 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Dwork", "Cynthia", ""], ["Feldman", "Vitaly", ""], ["Hardt", "Moritz", ""], ["Pitassi", "Toniann", ""], ["Reingold", "Omer", ""], ["Roth", "Aaron", ""]]}, {"id": "1411.2680", "submitter": "Takuya Akiba", "authors": "Takuya Akiba, Yoichi Iwata", "title": "Branch-and-Reduce Exponential/FPT Algorithms in Practice: A Case Study\n  of Vertex Cover", "comments": "To appear in ALENEX 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the gap between theory and practice for exact branching\nalgorithms. In theory, branch-and-reduce algorithms currently have the best\ntime complexity for numerous important problems. On the other hand, in\npractice, state-of-the-art methods are based on different approaches, and the\nempirical efficiency of such theoretical algorithms have seldom been\ninvestigated probably because they are seemingly inefficient because of the\nplethora of complex reduction rules. In this paper, we design a\nbranch-and-reduce algorithm for the vertex cover problem using the techniques\ndeveloped for theoretical algorithms and compare its practical performance with\nother state-of-the-art empirical methods. The results indicate that\nbranch-and-reduce algorithms are actually quite practical and competitive with\nother state-of-the-art approaches for several kinds of instances, thus showing\nthe practical impact of theoretical research on branching algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 01:58:46 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Akiba", "Takuya", ""], ["Iwata", "Yoichi", ""]]}, {"id": "1411.2718", "submitter": "Alex Bowe", "authors": "Christina Boucher, Alex Bowe, Travis Gagie, Simon J. Puglisi, Kunihiko\n  Sadakane", "title": "Variable-Order de Bruijn Graphs", "comments": "Conference submission, 10 pages, +minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The de Bruijn graph $G_K$ of a set of strings $S$ is a key data structure in\ngenome assembly that represents overlaps between all the $K$-length substrings\nof $S$. Construction and navigation of the graph is a space and time bottleneck\nin practice and the main hurdle for assembling large, eukaryote genomes. This\nproblem is compounded by the fact that state-of-the-art assemblers do not build\nthe de Bruijn graph for a single order (value of $K$) but for multiple values\nof $K$. More precisely, they build $d$ de Bruijn graphs, each with a specific\norder, i.e., $G_{K_1}, G_{K_2}, ..., G_{K_d}$. Although, this paradigm\nincreases the quality of the assembly produced, it increases the memory by a\nfactor of $d$ in most cases. In this paper, we show how to augment a succinct\nde Bruijn graph representation by Bowe et al. (Proc. WABI, 2012) to support new\noperations that let us change order on the fly, effectively representing all de\nBruijn graphs of order up to some maximum $K$ in a single data structure. Our\nexperiments show our variable-order de Bruijn graph only modestly increases\nspace usage, construction time, and navigation time compared to a single order\ngraph.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 07:52:23 GMT"}, {"version": "v2", "created": "Mon, 17 Nov 2014 12:36:08 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Boucher", "Christina", ""], ["Bowe", "Alex", ""], ["Gagie", "Travis", ""], ["Puglisi", "Simon J.", ""], ["Sadakane", "Kunihiko", ""]]}, {"id": "1411.2785", "submitter": "Travis Gagie", "authors": "Guillermo de Bernardo, Travis Gagie, Susana Ladra, Gonzalo Navarro and\n  Diego Seco", "title": "Faster Compressed Quadtrees", "comments": "Journal version of DCC '15 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world point sets tend to be clustered, so using a machine word for each\npoint is wasteful. In this paper we first show how a compact representation of\nquadtrees using $O (1)$ bits per node can break this bound on clustered point\nsets, while offering efficient range searches. We then describe a new compact\nquadtree representation based on heavy path decompositions, which supports\nqueries faster than previous compact structures. We present experimental\nevidence showing that our structure is competitive in practice.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 12:53:54 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 05:02:19 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["de Bernardo", "Guillermo", ""], ["Gagie", "Travis", ""], ["Ladra", "Susana", ""], ["Navarro", "Gonzalo", ""], ["Seco", "Diego", ""]]}, {"id": "1411.2873", "submitter": "Glencora Borradaile", "authors": "Glencora Borradaile, W. Sean Kennedy, Gordon Wilfong, Lisa Zhang", "title": "Improving Robustness of Next-Hop Routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A weakness of next-hop routing is that following a link or router failure\nthere may be no routes between some source-destination pairs, or packets may\nget stuck in a routing loop as the protocol operates to establish new routes.\nIn this article, we address these weaknesses by describing mechanisms to choose\nalternate next hops.\n  Our first contribution is to model the scenario as the following {\\sc tree\naugmentation} problem. Consider a mixed graph where some edges are directed and\nsome undirected. The directed edges form a spanning tree pointing towards the\ncommon destination node. Each directed edge represents the unique next hop in\nthe routing protocol. Our goal is to direct the undirected edges so that the\nresulting graph remains acyclic and the number of nodes with outdegree two or\nmore is maximized. These nodes represent those with alternative next hops in\ntheir routing paths.\n  We show that {\\sc tree augmentation} is NP-hard in general and present a\nsimple $\\frac{1}{2}$-approximation algorithm. We also study 3 special cases. We\ngive exact polynomial-time algorithms for when the input spanning tree consists\nof exactly 2 directed paths or when the input graph has bounded treewidth. For\nplanar graphs, we present a polynomial-time approximation scheme when the input\ntree is a breadth-first search tree. To the best of our knowledge, {\\sc tree\naugmentation} has not been previously studied.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 16:21:10 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Borradaile", "Glencora", ""], ["Kennedy", "W. Sean", ""], ["Wilfong", "Gordon", ""], ["Zhang", "Lisa", ""]]}, {"id": "1411.3164", "submitter": "Anthony Widjaja Lin", "authors": "Anthony Widjaja Lin, Sanming Zhou", "title": "A linear time algorithm for the orbit problem over cyclic groups", "comments": "Accepted in Acta Informatica in Nov 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The orbit problem is at the heart of symmetry reduction methods for model\nchecking concurrent systems. It asks whether two given configurations in a\nconcurrent system (represented as finite strings over some finite alphabet) are\nin the same orbit with respect to a given finite permutation group (represented\nby their generators) acting on this set of configurations by permuting indices.\nIt is known that the problem is in general as hard as the graph isomorphism\nproblem, whose precise complexity (whether it is solvable in polynomial-time)\nis a long-standing open problem. In this paper, we consider the restriction of\nthe orbit problem when the permutation group is cyclic (i.e. generated by a\nsingle permutation), an important restriction of the problem. It is known that\nthis subproblem is solvable in polynomial-time. Our main result is a\nlinear-time algorithm for this subproblem.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 13:09:13 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2015 03:45:11 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Lin", "Anthony Widjaja", ""], ["Zhou", "Sanming", ""]]}, {"id": "1411.3212", "submitter": "Francesco Lettich", "authors": "Francesco Lettich, Salvatore Orlando, Claudio Silvestri and Christian\n  S. Jensen", "title": "Manycore processing of repeated range queries over massive moving\n  objects observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to timely process significant amounts of continuously updated\nspatial data is mandatory for an increasing number of applications. Parallelism\nenables such applications to face this data-intensive challenge and allows the\ndevised systems to feature low latency and high scalability. In this paper we\nfocus on a specific data-intensive problem, concerning the repeated processing\nof huge amounts of range queries over massive sets of moving objects, where the\nspatial extents of queries and objects are continuously modified over time. To\ntackle this problem and significantly accelerate query processing we devise a\nhybrid CPU/GPU pipeline that compresses data output and save query processing\nwork. The devised system relies on an ad-hoc spatial index leading to a problem\ndecomposition that results in a set of independent data-parallel tasks. The\nindex is based on a point-region quadtree space decomposition and allows to\ntackle effectively a broad range of spatial object distributions, even those\nvery skewed. Also, to deal with the architectural peculiarities and limitations\nof the GPUs, we adopt non-trivial GPU data structures that avoid the need of\nlocked memory accesses and favour coalesced memory accesses, thus enhancing the\noverall memory throughput. To the best of our knowledge this is the first work\nthat exploits GPUs to efficiently solve repeated range queries over massive\nsets of continuously moving objects, characterized by highly skewed spatial\ndistributions. In comparison with state-of-the-art CPU-based implementations,\nour method highlights significant speedups in the order of 14x-20x, depending\non the datasets, even when considering very cheap GPUs.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 15:46:39 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Lettich", "Francesco", ""], ["Orlando", "Salvatore", ""], ["Silvestri", "Claudio", ""], ["Jensen", "Christian S.", ""]]}, {"id": "1411.3530", "submitter": "Shiping Liu", "authors": "Fatihcan M. Atay and Shiping Liu", "title": "Cheeger constants, structural balance, and spectral clustering analysis\n  for signed graphs", "comments": "We add more details for the proof of Lemma 6.4, Theorem 6.2, Theorem\n  6.3. We also explain more details about the control of those various absolute\n  constant appearing in our estimates", "journal-ref": "Discrete Mathematics 343:111616 (2020)", "doi": "10.1016/j.disc.2019.111616", "report-no": null, "categories": "math.CO cs.DS math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a family of multi-way Cheeger-type constants $\\{h_k^{\\sigma},\nk=1,2,\\ldots, n\\}$ on a signed graph $\\Gamma=(G,\\sigma)$ such that\n$h_k^{\\sigma}=0$ if and only if $\\Gamma$ has $k$ balanced connected components.\nThese constants are switching invariant and bring together in a unified\nviewpoint a number of important graph-theoretical concepts, including the\nclassical Cheeger constant, those measures of bipartiteness introduced by\nDesai-Rao, Trevisan, Bauer-Jost, respectively, on unsigned graphs,, and the\nfrustration index (originally called the line index of balance by Harary) on\nsigned graphs. We further unify the (higher-order or improved) Cheeger and dual\nCheeger inequalities for unsigned graphs as well as the underlying algorithmic\nproof techniques by establishing their corresponding versions on signed graphs.\nIn particular, we develop a spectral clustering method for finding $k$\nalmost-balanced subgraphs, each defining a sparse cut. The proper metric for\nsuch a clustering is the metric on a real projective space. We also prove\nestimates of the extremal eigenvalues of signed Laplace matrix in terms of\nnumber of signed triangles ($3$-cycles).\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 13:04:12 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 12:56:19 GMT"}, {"version": "v3", "created": "Fri, 8 Dec 2017 08:10:56 GMT"}, {"version": "v4", "created": "Sun, 10 Mar 2019 21:30:46 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Atay", "Fatihcan M.", ""], ["Liu", "Shiping", ""]]}, {"id": "1411.3787", "submitter": "Ping Li", "authors": "Anshumali Shrivastava, Ping Li", "title": "Asymmetric Minwise Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minwise hashing (Minhash) is a widely popular indexing scheme in practice.\nMinhash is designed for estimating set resemblance and is known to be\nsuboptimal in many applications where the desired measure is set overlap (i.e.,\ninner product between binary vectors) or set containment. Minhash has inherent\nbias towards smaller sets, which adversely affects its performance in\napplications where such a penalization is not desirable. In this paper, we\npropose asymmetric minwise hashing (MH-ALSH), to provide a solution to this\nproblem. The new scheme utilizes asymmetric transformations to cancel the bias\nof traditional minhash towards smaller sets, making the final \"collision\nprobability\" monotonic in the inner product. Our theoretical comparisons show\nthat for the task of retrieving with binary inner products asymmetric minhash\nis provably better than traditional minhash and other recently proposed hashing\nalgorithms for general inner products. Thus, we obtain an algorithmic\nimprovement over existing approaches in the literature. Experimental\nevaluations on four publicly available high-dimensional datasets validate our\nclaims and the proposed scheme outperforms, often significantly, other hashing\nalgorithms on the task of near neighbor retrieval with set containment. Our\nproposal is simple and easy to implement in practice.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 04:18:33 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1411.3887", "submitter": "Nathaniel Kell", "authors": "Sungjin Im, Nathaniel Kell, Janardhan Kulkarni, Debmalya Panigrahi", "title": "Tight Bounds for Online Vector Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data centers face a key challenge of effectively serving user requests\nthat arrive online. Such requests are inherently multi-dimensional and\ncharacterized by demand vectors over multiple resources such as processor\ncycles, storage space, and network bandwidth. Typically, different resources\nrequire different objectives to be optimized, and $L_r$ norms of loads are\namong the most popular objectives considered. To address these problems, we\nconsider the online vector scheduling problem in this paper. Introduced by\nChekuri and Khanna (SIAM J of Comp. 2006), vector scheduling is a\ngeneralization of classical load balancing, where every job has a vector load\ninstead of a scalar load. In this paper, we resolve the online complexity of\nthe vector scheduling problem and its important generalizations. Our main\nresults are:\n  -For identical machines, we show that the optimal competitive ratio is\n$\\Theta(\\log d / \\log \\log d)$ by giving an online lower bound and an algorithm\nwith an asymptotically matching competitive ratio. The lower bound is\ntechnically challenging, and is obtained via an online lower bound for the\nminimum mono-chromatic clique problem using a novel online coloring game and\nrandomized coding scheme.\n  -For unrelated machines, we show that the optimal competitive ratio is\n$\\Theta(\\log m + \\log d)$ by giving an online lower bound that matches a\npreviously known upper bound. Unlike identical machines, however, extending\nthese results, particularly the upper bound, to general $L_r$ norms requires\nnew ideas. In particular, we use a carefully constructed potential function\nthat balances the individual $L_r$ objectives with the overall (convexified)\nmin-max objective to guide the online algorithm and track the changes in\npotential to bound the competitive ratio.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 12:51:42 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 16:40:15 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Im", "Sungjin", ""], ["Kell", "Nathaniel", ""], ["Kulkarni", "Janardhan", ""], ["Panigrahi", "Debmalya", ""]]}, {"id": "1411.4073", "submitter": "Vijaya Ramachandran", "authors": "Meghana Nasre and Matteo Pontecorvi and Vijaya Ramachandran", "title": "Decremental All-Pairs ALL Shortest Paths and Betweenness Centrality", "comments": "An extended abstract of this paper will appear in Proc. ISAAC 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the all pairs all shortest paths (APASP) problem, which maintains\nthe shortest path dag rooted at every vertex in a directed graph G=(V,E) with\npositive edge weights. For this problem we present a decremental algorithm\n(that supports the deletion of a vertex, or weight increases on edges incident\nto a vertex). Our algorithm runs in amortized O(\\vstar^2 \\cdot \\log n) time per\nupdate, where n=|V|, and \\vstar bounds the number of edges that lie on shortest\npaths through any given vertex. Our APASP algorithm can be used for the\ndecremental computation of betweenness centrality (BC), a graph parameter that\nis widely used in the analysis of large complex networks. No nontrivial\ndecremental algorithm for either problem was known prior to our work. Our\nmethod is a generalization of the decremental algorithm of Demetrescu and\nItaliano [DI04] for unique shortest paths, and for graphs with \\vstar =O(n), we\nmatch the bound in [DI04]. Thus for graphs with a constant number of shortest\npaths between any pair of vertices, our algorithm maintains APASP and BC scores\nin amortized time O(n^2 \\log n) under decremental updates, regardless of the\nnumber of edges in the graph.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 22:11:39 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Nasre", "Meghana", ""], ["Pontecorvi", "Matteo", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1411.4105", "submitter": "Shuo Han", "authors": "Shuo Han, Ufuk Topcu, George J. Pappas", "title": "Differentially Private Distributed Constrained Optimization", "comments": "Submitted to the IEEE Transactions on Automatic Control", "journal-ref": null, "doi": "10.1109/TAC.2016.2541298", "report-no": null, "categories": "math.OC cs.DS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many resource allocation problems can be formulated as an optimization\nproblem whose constraints contain sensitive information about participating\nusers. This paper concerns solving this kind of optimization problem in a\ndistributed manner while protecting the privacy of user information. Without\nprivacy considerations, existing distributed algorithms normally consist in a\ncentral entity computing and broadcasting certain public coordination signals\nto participating users. However, the coordination signals often depend on user\ninformation, so that an adversary who has access to the coordination signals\ncan potentially decode information on individual users and put user privacy at\nrisk. We present a distributed optimization algorithm that preserves\ndifferential privacy, which is a strong notion that guarantees user privacy\nregardless of any auxiliary information an adversary may have. The algorithm\nachieves privacy by perturbing the public signals with additive noise, whose\nmagnitude is determined by the sensitivity of the projection operation onto\nuser-specified constraints. By viewing the differentially private algorithm as\nan implementation of stochastic gradient descent, we are able to derive a bound\nfor the suboptimality of the algorithm. We illustrate the implementation of our\nalgorithm via a case study of electric vehicle charging. Specifically, we\nderive the sensitivity and present numerical simulations for the algorithm.\nThrough numerical simulations, we are able to investigate various aspects of\nthe algorithm when being used in practice, including the choice of step size,\nnumber of iterations, and the trade-off between privacy level and\nsuboptimality.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 02:33:50 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Han", "Shuo", ""], ["Topcu", "Ufuk", ""], ["Pappas", "George J.", ""]]}, {"id": "1411.4184", "submitter": "Marcin Pilipczuk", "authors": "Marek Cygan and D\\'aniel Marx and Marcin Pilipczuk and Micha{\\l}\n  Pilipczuk", "title": "Hitting forbidden subgraphs in graphs of bounded treewidth", "comments": "A full version of a paper presented at MFCS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of a generic hitting problem H-Subgraph Hitting,\nwhere given a fixed pattern graph $H$ and an input graph $G$, the task is to\nfind a set $X \\subseteq V(G)$ of minimum size that hits all subgraphs of $G$\nisomorphic to $H$. In the colorful variant of the problem, each vertex of $G$\nis precolored with some color from $V(H)$ and we require to hit only\n$H$-subgraphs with matching colors. Standard techniques shows that for every\nfixed $H$, the problem is fixed-parameter tractable parameterized by the\ntreewidth of $G$; however, it is not clear how exactly the running time should\ndepend on treewidth. For the colorful variant, we demonstrate matching upper\nand lower bounds showing that the dependence of the running time on treewidth\nof $G$ is tightly governed by $\\mu(H)$, the maximum size of a minimal vertex\nseparator in $H$. That is, we show for every fixed $H$ that, on a graph of\ntreewidth $t$, the colorful problem can be solved in time\n$2^{\\mathcal{O}(t^{\\mu(H)})}\\cdot|V(G)|$, but cannot be solved in time\n$2^{o(t^{\\mu(H)})}\\cdot |V(G)|^{O(1)}$, assuming the Exponential Time\nHypothesis (ETH). Furthermore, we give some preliminary results showing that,\nin the absence of colors, the parameterized complexity landscape of H-Subgraph\nHitting is much richer.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 20:47:20 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Cygan", "Marek", ""], ["Marx", "D\u00e1niel", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1411.4274", "submitter": "Christoph D\\\"urr", "authors": "Marek Chrobak, Christoph Durr, Aleksander Fabijan, Bengt Nilsson", "title": "Online Clique Clustering", "comments": "This paper combines and extends the results from 2 conference\n  publications at CIAC 2013 and CIAC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clique clustering is the problem of partitioning the vertices of a graph into\ndisjoint clusters, where each cluster forms a clique in the graph, while\noptimizing some objective function. In online clustering, the input graph is\ngiven one vertex at a time, and any vertices that have previously been\nclustered together are not allowed to be separated. The goal is to maintain a\nclustering with an objective value close to the optimal solution. For the\nvariant where we want to maximize the number of edges in the clusters, we\npropose an online strategy based on the doubling technique. It has an\nasymptotic competitive ratio at most 15.646 and an absolute competitive ratio\nat most 22.641. We also show that no deterministic strategy can have an\nasymptotic competitive ratio better than 6. For the variant where we want to\nminimize the number of edges between clusters, we show that the deterministic\ncompetitive ratio of the problem is $n-\\omega(1)$, where n is the number of\nvertices in the graph.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 16:17:49 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 20:33:11 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2016 13:30:08 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Chrobak", "Marek", ""], ["Durr", "Christoph", ""], ["Fabijan", "Aleksander", ""], ["Nilsson", "Bengt", ""]]}, {"id": "1411.4357", "submitter": "David Woodruff", "authors": "David P. Woodruff", "title": "Sketching as a Tool for Numerical Linear Algebra", "comments": "fixed minor errors/typos in section 4.3, e.g., Fact 6 and its\n  propagation, clarified when Lemma 4.2 can be applied, typos in section 4.2.3\n  (G should be applied on the left), other typos throughout", "journal-ref": "Foundations and Trends in Theoretical Computer Science, Vol 10,\n  Issue 1--2, 2014, pp 1--157", "doi": "10.1561/0400000060", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey highlights the recent advances in algorithms for numerical linear\nalgebra that have come from the technique of linear sketching, whereby given a\nmatrix, one first compresses it to a much smaller matrix by multiplying it by a\n(usually) random matrix with certain properties. Much of the expensive\ncomputation can then be performed on the smaller matrix, thereby accelerating\nthe solution for the original problem. In this survey we consider least squares\nas well as robust regression problems, low rank approximation, and graph\nsparsification. We also discuss a number of variants of these problems.\nFinally, we discuss the limitations of sketching methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 04:11:34 GMT"}, {"version": "v2", "created": "Mon, 22 Dec 2014 20:57:29 GMT"}, {"version": "v3", "created": "Tue, 10 Feb 2015 11:18:32 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Woodruff", "David P.", ""]]}, {"id": "1411.4384", "submitter": "Anthony Kim", "authors": "Zhiyi Huang and Anthony Kim", "title": "Welfare Maximization with Production Costs: A Primal Dual Approach", "comments": "To appear in the ACM-SIAM Symposium on Discrete Algorithms (SODA),\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online combinatorial auctions with production costs proposed by Blum\net al. using the online primal dual framework. In this model, buyers arrive\nonline, and the seller can produce multiple copies of each item subject to a\nnon-decreasing marginal cost per copy. The goal is to allocate items to\nmaximize social welfare less total production cost. For arbitrary (strictly\nconvex and differentiable) production cost functions, we characterize the\noptimal competitive ratio achievable by online mechanisms/algorithms. We show\nthat online posted pricing mechanisms, which are incentive compatible, can\nachieve competitive ratios arbitrarily close to the optimal, and construct\nlower bound instances on which no online algorithms, not necessarily incentive\ncompatible, can do better. Our positive results improve or match the results in\nseveral previous work, e.g., Bartal et al., Blum et al., and Buchbinder and\nGonen. Our lower bounds apply to randomized algorithms and resolve an open\nproblem by Buchbinder and Gonen.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 07:34:02 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Huang", "Zhiyi", ""], ["Kim", "Anthony", ""]]}, {"id": "1411.4476", "submitter": "Ashkan Norouzi-Fard", "authors": "Hyung-Chan An, Ashkan Norouzi-Fard, Ola Svensson", "title": "Dynamic Facility Location via Exponential Clocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\emph{dynamic facility location problem} is a generalization of the\nclassic facility location problem proposed by Eisenstat, Mathieu, and Schabanel\nto model the dynamics of evolving social/infrastructure networks. The\ngeneralization lies in that the distance metric between clients and facilities\nchanges over time. This leads to a trade-off between optimizing the classic\nobjective function and the \"stability\" of the solution: there is a switching\ncost charged every time a client changes the facility to which it is connected.\nWhile the standard linear program (LP) relaxation for the classic problem\nnaturally extends to this problem, traditional LP-rounding techniques do not,\nas they are often sensitive to small changes in the metric resulting in\nfrequent switches.\n  We present a new LP-rounding algorithm for facility location problems, which\nyields the first constant approximation algorithm for the dynamic facility\nlocation problem. Our algorithm installs competing exponential clocks on the\nclients and facilities, and connect every client by the path that repeatedly\nfollows the smallest clock in the neighborhood. The use of exponential clocks\ngives rise to several properties that distinguish our approach from previous\nLP-roundings for facility location problems. In particular, we use \\emph{no\nclustering} and we allow clients to connect through paths of \\emph{arbitrary\nlengths}. In fact, the clustering-free nature of our algorithm is crucial for\napplying our LP-rounding approach to the dynamic problem.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 13:46:13 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["An", "Hyung-Chan", ""], ["Norouzi-Fard", "Ashkan", ""], ["Svensson", "Ola", ""]]}, {"id": "1411.4498", "submitter": "Bogdan Chlebus", "authors": "Bogdan S. Chlebus and Gianluca De Marco and Dariusz R. Kowalski", "title": "Scalable Wake-up of Multi-Channel Single-Hop Radio Networks", "comments": null, "journal-ref": "Theoretical Computer Science, Volume 615, 15 February 2016, Pages\n  23--44", "doi": "10.1016/j.tcs.2015.11.046", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider single-hop radio networks with multiple channels as a model of\nwireless networks. There are $n$ stations connected to $b$ radio channels that\ndo not provide collision detection. A station uses all the channels\nconcurrently and independently. Some $k$ stations may become active\nspontaneously at arbitrary times. The goal is to wake up the network, which\noccurs when all the stations hear a successful transmission on some channel.\nDuration of a waking-up execution is measured starting from the first\nspontaneous activation. We present a deterministic algorithm for the general\nproblem that wakes up the network in $O(k\\log^{1/b} k\\log n)$ time, where $k$\nis unknown. We give a deterministic scalable algorithm for the special case\nwhen $b>d \\log \\log n$, for some constant $d>1$, which wakes up the network in\n$O(\\frac{k}{b}\\log n\\log(b\\log n))$ time, with $k$ unknown. This algorithm\nmisses time optimality by at most a factor of $O(\\log n(\\log b +\\log\\log n))$,\nbecause any deterministic algorithm requires $\\Omega(\\frac{k}{b}\\log\n\\frac{n}{k})$ time. We give a randomized algorithm that wakes up the network\nwithin $O(k^{1/b}\\ln \\frac{1}{\\epsilon})$ rounds with a probability that is at\nleast $1-\\epsilon$, for any $0<\\epsilon<1$, where $k$ is known. We also\nconsider a model of jamming, in which each channel in any round may be jammed\nto prevent a successful transmission, which happens with some known parameter\nprobability $p$, independently across all channels and rounds. For this model,\nwe give two deterministic algorithms for unknown~$k$: one wakes up the network\nin time $O(\\log^{-1}(\\frac{1}{p})\\, k\\log n\\log^{1/b} k)$, and the other in\ntime $O(\\log^{-1}(\\frac{1}{p}) \\, \\frac{k}{b} \\log n\\log(b\\log n))$ but\nassuming the inequality $b>\\log(128b\\log n)$, both with a probability that is\nat least $1-1/\\mbox{poly}(n)$.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 14:46:50 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 19:45:06 GMT"}, {"version": "v3", "created": "Wed, 26 Dec 2018 19:31:57 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Chlebus", "Bogdan S.", ""], ["De Marco", "Gianluca", ""], ["Kowalski", "Dariusz R.", ""]]}, {"id": "1411.4573", "submitter": "Chaitanya Swamy", "authors": "Ian Post and Chaitanya Swamy", "title": "Linear-Programming based Approximation Algorithms for Multi-Vehicle\n  Minimum Latency Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider various {\\em multi-vehicle versions of the minimum latency\nproblem}. There is a fleet of $k$ vehicles located at one or more depot nodes,\nand we seek a collection of routes for these vehicles that visit all nodes so\nas to minimize the total latency incurred, which is the sum of the client\nwaiting times. We obtain an $8.497$-approximation for the version where\nvehicles may be located at multiple depots and a $7.183$-approximation for the\nversion where all vehicles are located at the same depot, both of which are the\nfirst improvements on this problem in a decade. Perhaps more significantly, our\nalgorithms exploit various LP-relaxations for minimum-latency problems. We show\nhow to effectively leverage two classes of LPs---{\\em configuration LPs} and\n{\\em bidirected LP-relaxations}---that are often believed to be quite powerful\nbut have only sporadically been effectively leveraged for network-design and\nvehicle-routing problems. This gives the first concrete evidence of the\neffectiveness of LP-relaxations for this class of problems. The\n$8.497$-approximation the multiple-depot version is obtained by rounding a\nnear-optimal solution to an underlying configuration LP for the problem. The\n$7.183$-approximation can be obtained both via rounding a bidirected LP for the\nsingle-depot problem or via more combinatorial means. The latter approach uses\na bidirected LP to obtain the following key result that is of independent\ninterest: for any $k$, we can efficiently compute a rooted tree that is at\nleast as good, with respect to the prize-collecting objective (i.e., edge cost\n+ number of uncovered nodes) as the best collection of $k$ rooted paths. Our\nalgorithms are versatile and extend easily to handle various extensions\ninvolving: (i) weighted sum of latencies, (ii) constraints specifying which\ndepots may serve which nodes, (iii) node service times.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 17:56:06 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Post", "Ian", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "1411.4575", "submitter": "Marcin Pilipczuk", "authors": "P{\\aa}l Gr{\\o}n{\\aa}s Drange, Markus S. Dregi, Fedor V. Fomin, Stephan\n  Kreutzer, Daniel Lokshtanov, Marcin Pilipczuk, Micha{\\l} Pilipczuk, Felix\n  Reidl, Saket Saurabh, Fernando S\\'anchez Villaamil, Sebastian Siebertz,\n  Somnath Sikdar", "title": "Kernelization and Sparseness: the case of Dominating Set", "comments": "v2: new author, added results for r-Dominating Sets in bounded\n  expansion graphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that for every positive integer $r$ and for every graph class\n$\\mathcal G$ of bounded expansion, the $r$-Dominating Set problem admits a\nlinear kernel on graphs from $\\mathcal G$. Moreover, when $\\mathcal G$ is only\nassumed to be nowhere dense, then we give an almost linear kernel on $\\mathcal\nG$ for the classic Dominating Set problem, i.e., for the case $r=1$. These\nresults generalize a line of previous research on finding linear kernels for\nDominating Set and $r$-Dominating Set. However, the approach taken in this\nwork, which is based on the theory of sparse graphs, is radically different and\nconceptually much simpler than the previous approaches.\n  We complement our findings by showing that for the closely related Connected\nDominating Set problem, the existence of such kernelization algorithms is\nunlikely, even though the problem is known to admit a linear kernel on\n$H$-topological-minor-free graphs. Also, we prove that for any somewhere dense\nclass $\\mathcal G$, there is some $r$ for which $r$-Dominating Set is\nW[$2$]-hard on $\\mathcal G$. Thus, our results fall short of proving a sharp\ndichotomy for the parameterized complexity of $r$-Dominating Set on\nsubgraph-monotone graph classes: we conjecture that the border of tractability\nlies exactly between nowhere dense and somewhere dense graph classes.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 17:58:59 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 21:33:30 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Drange", "P\u00e5l Gr\u00f8n\u00e5s", ""], ["Dregi", "Markus S.", ""], ["Fomin", "Fedor V.", ""], ["Kreutzer", "Stephan", ""], ["Lokshtanov", "Daniel", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""], ["Reidl", "Felix", ""], ["Saurabh", "Saket", ""], ["Villaamil", "Fernando S\u00e1nchez", ""], ["Siebertz", "Sebastian", ""], ["Sikdar", "Somnath", ""]]}, {"id": "1411.4613", "submitter": "Shayan Oveis Gharan", "authors": "Nima Anari and Shayan Oveis Gharan", "title": "Effective-Resistance-Reducing Flows, Spectrally Thin Trees, and\n  Asymmetric TSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the integrality gap of the natural LP relaxation of the\nAsymmetric Traveling Salesman Problem is $\\text{polyloglog}(n)$. In other\nwords, there is a polynomial time algorithm that approximates the value of the\noptimum tour within a factor of $\\text{polyloglog}(n)$, where\n$\\text{polyloglog}(n)$ is a bounded degree polynomial of $\\log\\log(n)$. We\nprove this by showing that any $k$-edge-connected unweighted graph has a\n$\\text{polyloglog}(n)/k$-thin spanning tree.\n  Our main new ingredient is a procedure, albeit an exponentially sized convex\nprogram, that \"transforms\" graphs that do not admit any spectrally thin trees\ninto those that provably have spectrally thin trees. More precisely, given a\n$k$-edge-connected graph $G=(V,E)$ where $k\\geq 7\\log(n)$, we show that there\nis a matrix $D$ that \"preserves\" the structure of all cuts of $G$ such that for\na set $F\\subseteq E$ that induces an $\\Omega(k)$-edge-connected graph, the\neffective resistance of every edge in $F$ w.r.t. $D$ is at most\n$\\text{polylog}(k)/k$. Then, we use a recent extension of the seminal work of\nMarcus, Spielman, and Srivastava [MSS13] by the authors [AO14] to prove the\nexistence of a $\\text{polylog}(k)/k$-spectrally thin tree with respect to $D$.\nSuch a tree is $\\text{polylog}(k)/k$-combinatorially thin with respect to $G$\nas $D$ preserves the structure of cuts of $G$.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 20:03:26 GMT"}, {"version": "v2", "created": "Thu, 11 Dec 2014 04:57:55 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2015 04:02:35 GMT"}, {"version": "v4", "created": "Tue, 1 Sep 2015 22:36:02 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Anari", "Nima", ""], ["Gharan", "Shayan Oveis", ""]]}, {"id": "1411.4619", "submitter": "Alexandros A. Voudouris", "authors": "Ioannis Caragiannis, George A. Krimpas, Alexandros A. Voudouris", "title": "Aggregating partial rankings with applications to peer grading in\n  massive online open courses", "comments": "16 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the potential of using ordinal peer grading for the evaluation\nof students in massive online open courses (MOOCs). According to such grading\nschemes, each student receives a few assignments (by other students) which she\nhas to rank. Then, a global ranking (possibly translated into numerical scores)\nis produced by combining the individual ones. This is a novel application area\nfor social choice concepts and methods where the important problem to be solved\nis as follows: how should the assignments be distributed so that the collected\nindividual rankings can be easily merged into a global one that is as close as\npossible to the ranking that represents the relative performance of the\nstudents in the assignment? Our main theoretical result suggests that using\nvery simple ways to distribute the assignments so that each student has to rank\nonly $k$ of them, a Borda-like aggregation method can recover a $1-O(1/k)$\nfraction of the true ranking when each student correctly ranks the assignments\nshe receives. Experimental results strengthen our analysis further and also\ndemonstrate that the same method is extremely robust even when students have\nimperfect capabilities as graders. We believe that our results provide strong\nevidence that ordinal peer grading can be a highly effective and scalable\nsolution for evaluation in MOOCs.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 20:16:38 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Caragiannis", "Ioannis", ""], ["Krimpas", "George A.", ""], ["Voudouris", "Alexandros A.", ""]]}, {"id": "1411.4692", "submitter": "Ishay Haviv", "authors": "Ishay Haviv, Ning Xie", "title": "Sunflowers and Testing Triangle-Freeness of Functions", "comments": "21 pages, ITCS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A function $f: \\mathbb{F}_2^n \\rightarrow \\{0,1\\}$ is triangle-free if there\nare no $x_1,x_2,x_3 \\in \\mathbb{F}_2^n$ satisfying $x_1+x_2+x_3=0$ and\n$f(x_1)=f(x_2)=f(x_3)=1$. In testing triangle-freeness, the goal is to\ndistinguish with high probability triangle-free functions from those that are\n$\\varepsilon$-far from being triangle-free. It was shown by Green that the\nquery complexity of the canonical tester for the problem is upper bounded by a\nfunction that depends only on $\\varepsilon$ (GAFA, 2005), however the best\nknown upper bound is a tower type function of $1/\\varepsilon$. The best known\nlower bound on the query complexity of the canonical tester is\n$1/\\varepsilon^{13.239}$ (Fu and Kleinberg, RANDOM, 2014).\n  In this work we introduce a new approach to proving lower bounds on the query\ncomplexity of triangle-freeness. We relate the problem to combinatorial\nquestions on collections of vectors in $\\mathbb{Z}_D^n$ and to sunflower\nconjectures studied by Alon, Shpilka, and Umans (Comput. Complex., 2013). The\nrelations yield that a refutation of the Weak Sunflower Conjecture over\n$\\mathbb{Z}_4$ implies a super-polynomial lower bound on the query complexity\nof the canonical tester for triangle-freeness. Our results are extended to\ntesting $k$-cycle-freeness of functions with domain $\\mathbb{F}_p^n$ for every\n$k \\geq 3$ and a prime $p$. In addition, we generalize the lower bound of Fu\nand Kleinberg to $k$-cycle-freeness for $k \\geq 4$ by generalizing the\nconstruction of uniquely solvable puzzles due to Coppersmith and Winograd (J.\nSymbolic Comput., 1990).\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 23:04:38 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Haviv", "Ishay", ""], ["Xie", "Ning", ""]]}, {"id": "1411.4942", "submitter": "Ali Pinar", "authors": "Madhav Jha, C. Seshadhri, Ali Pinar", "title": "Path Sampling: A Fast and Provable Method for Estimating 4-Vertex\n  Subgraph Counts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting the frequency of small subgraphs is a fundamental technique in\nnetwork analysis across various domains, most notably in bioinformatics and\nsocial networks. The special case of triangle counting has received much\nattention. Getting results for 4-vertex patterns is highly challenging, and\nthere are few practical results known that can scale to massive sizes. Indeed,\neven a highly tuned enumeration code takes more than a day on a graph with\nmillions of edges. Most previous work that runs for truly massive graphs employ\nclusters and massive parallelization.\n  We provide a sampling algorithm that provably and accurately approximates the\nfrequencies of all 4-vertex pattern subgraphs. Our algorithm is based on a\nnovel technique of 3-path sampling and a special pruning scheme to decrease the\nvariance in estimates. We provide theoretical proofs for the accuracy of our\nalgorithm, and give formal bounds for the error and confidence of our\nestimates. We perform a detailed empirical study and show that our algorithm\nprovides estimates within 1% relative error for all subpatterns (over a large\nclass of test graphs), while being orders of magnitude faster than enumeration\nand other sampling based algorithms. Our algorithm takes less than a minute (on\na single commodity machine) to process an Orkut social network with 300 million\nedges.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 18:04:50 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Jha", "Madhav", ""], ["Seshadhri", "C.", ""], ["Pinar", "Ali", ""]]}, {"id": "1411.4982", "submitter": "Mikkel Thorup", "authors": "Mikkel Thorup", "title": "Sample(x)=(a*x<=t) is a distinguisher with probability 1/8", "comments": "To appear in different format in the proceedings of FOCS'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A random sampling function Sample:U->{0,1} for a key universe U is a\ndistinguisher with probability p if for any given assignment of values v(x) to\nthe keys x in U, including at least one non-zero v(x)!=0, the sampled sum sum{\nv(x) | x in U and Sample(x) } is non-zero with probability at least p. Here the\nkey values may come from any commutative monoid (addition is commutative and\nassociative and zero is neutral). Such distinguishers were introduced by\nVazirani [PhD thesis 1986], and Naor and Naor used them for their small bias\nprobability spaces [STOC'90]. Constant probability distinguishers are used for\ntesting in contexts where the key values are not computed directly, yet where\nthe sum is easily computed. A simple example is when we get a stream of key\nvalue pairs (x_1,v_1),(x_2,v_2),...,(x_n,v_n) where the same key may appear\nmany times. The accumulated value of key x is v(x)=sum{v_i | x_i=x}. For space\nreasons, we may not be able to maintain v(x) for every key x, but the sampled\nsum is easily maintained as the single value sum{v_i | Sample(x_i)}. Here we\nshow that when dealing with w-bit integers, if a is a uniform odd w-bit integer\nand t is a uniform w-bit integer, then Sample(x)=[ax mod 2^w <= t] is a\ndistinguisher with probability 1/8. Working with standard units, that is, w=8,\n16, 32, 64, we exploit that w-bit multiplication works modulo 2^w, discarding\noverflow automatically, and then the sampling decision is implemented by the\nC-code a*x<=t. Previous such samplers were much less computer friendly, e.g.,\nthe distinguisher of Naor and Naor [STOC'90] was more complicated and involved\na 7-independent hash function.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 19:46:46 GMT"}, {"version": "v2", "created": "Thu, 11 Dec 2014 21:21:34 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2015 14:06:00 GMT"}, {"version": "v4", "created": "Sun, 2 Aug 2015 02:02:36 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Thorup", "Mikkel", ""]]}, {"id": "1411.5050", "submitter": "Trung Thanh Nguyen", "authors": "Khaled Elbassioni and Trung Thanh Nguyen", "title": "Approximation Schemes for Binary Quadratic Programming Problems with Low\n  cp-Rank Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary quadratic programming problems have attracted much attention in the\nlast few decades due to their potential applications. This type of problems are\nNP-hard in general, and still considered a challenge in the design of efficient\napproximation algorithms for their solutions. The purpose of this paper is to\ninvestigate the approximability for a class of such problems where the\nconstraint matrices are {\\it completely positive} and have low {\\it cp-rank}.\nIn the first part of the paper, we show that a completely positive rational\nfactorization of such matrices can be computed in polynomial time, within any\ndesired accuracy. We next consider binary quadratic programming problems of the\nfollowing form: Given matrices $Q_1,...,Q_n\\in\\mathbb{R}_+^{n\\times n}$, and a\nsystem of $m$ constrains $x^TQ_ix\\le C_i^2$ ($x^TQ_ix\\ge C_i^2$), $i=1,...,m$,\nwe seek to find a vector $x^*\\in \\{0,1\\}^n$ that maximizes (minimizes) a given\nfunction $f$. This class of problems generalizes many fundamental problems in\ndiscrete optimization such as packing and covering integer programs/knapsack\nproblems, quadratic knapsack problems, submodular maximization, etc. We\nconsider the case when $m$ and the cp-ranks of the matrices $Q_i$ are bounded\nby a constant.\n  Our approximation results for the maximization problem are as follows. For\nthe case when the objective function is nonnegative submodular, we give an\n$(1/4-\\epsilon)$-approximation algorithm, for any $\\epsilon>0$; when the\nfunction $f$ is linear, we present a PTAS. We next extend our PTAS result to a\nwider class of non-linear objective functions including quadratic functions,\nmultiplicative functions, and sum-of-ratio functions. The minimization problem\nseems to be much harder due to the fact that the relaxation is {\\it not}\nconvex. For this case, we give a QPTAS for $m=1$.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 21:53:38 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Elbassioni", "Khaled", ""], ["Nguyen", "Trung Thanh", ""]]}, {"id": "1411.5123", "submitter": "Mikkel Thorup", "authors": "Ken-ichi Kawarabayashi and Mikkel Thorup", "title": "Deterministic Edge Connectivity in Near-Linear Time", "comments": "This is the full journal version. Has been accepted for J.ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic near-linear time algorithm that computes the\nedge-connectivity and finds a minimum cut for a simple undirected unweighted\ngraph G with n vertices and m edges. This is the first o(mn) time deterministic\nalgorithm for the problem. In near-linear time we can also construct the\nclassic cactus representation of all minimum cuts.\n  The previous fastest deterministic algorithm by Gabow from STOC'91 took\n~O(m+k^2 n), where k is the edge connectivity, but k could be Omega(n).\n  At STOC'96 Karger presented a randomized near linear time Monte Carlo\nalgorithm for the minimum cut problem. As he points out, there is no better way\nof certifying the minimality of the returned cut than to use Gabow's slower\ndeterministic algorithm and compare sizes.\n  Our main technical contribution is a near-linear time algorithm that contract\nvertex sets of a simple input graph G with minimum degree d, producing a\nmultigraph with ~O(m/d) edges which preserves all minimum cuts of G with at\nleast 2 vertices on each side.\n  In our deterministic near-linear time algorithm, we will decompose the\nproblem via low-conductance cuts found using PageRank a la Brin and Page\n(1998), as analyzed by Andersson, Chung, and Lang at FOCS'06. Normally such\nalgorithms for low-conductance cuts are randomized Monte Carlo algorithms,\nbecause they rely on guessing a good start vertex. However, in our case, we\nhave so much structure that no guessing is needed.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 06:34:41 GMT"}, {"version": "v2", "created": "Thu, 27 Nov 2014 11:20:45 GMT"}, {"version": "v3", "created": "Thu, 4 Dec 2014 12:37:11 GMT"}, {"version": "v4", "created": "Wed, 2 Dec 2015 21:33:12 GMT"}, {"version": "v5", "created": "Sat, 27 Oct 2018 11:08:50 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Kawarabayashi", "Ken-ichi", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1411.5127", "submitter": "Oscar Stiffelman", "authors": "Oscar Stiffelman", "title": "PivotCompress: Compression by Sorting", "comments": "preprint, compression by sorting, quicksort as universal code; this\n  version describes the permutation vector and its inverse", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorted data is usually easier to compress than unsorted permutations of the\nsame data. This motivates a simple compression scheme: specify the sorted\npermutation of the data along with a representation of the sorted data\ncompressed recursively. The sorted permutation can be specified by recording\nthe decisions made by quicksort. If the size of the data is known, then the\nquicksort decisions describe the data at a rate that is nearly as efficient as\nthe minimal prefix-free code for the distribution, which is bounded by the\nentropy of the distribution. This is possible even though the distribution is\nunknown ahead of time. Used in this way, quicksort acts as a universal code in\nthat it is asymptotically optimal for any stationary source. The Shannon\nentropy is a lower bound when describing stochastic, independent symbols.\nHowever, it is possible to encode non-uniform, finite strings below the entropy\nof the sample distribution by also encoding symbol counts because the values in\nthe sequence are no longer independent once the counts are known. The key\ninsight is that sparse quicksort comparison vectors can also be compressed to\nachieve an even lower rate when data is highly non-uniform while incurring only\na modest penalty when data is random.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 06:48:20 GMT"}, {"version": "v2", "created": "Fri, 21 Nov 2014 01:37:51 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Stiffelman", "Oscar", ""]]}, {"id": "1411.5383", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Rati Gelashvili, Silvio Micali, Nir Shavit", "title": "Johnson-Lindenstrauss Compression with Neuroscience-Based Constraints", "comments": "A shorter version of this paper has appeared in the Proceedings of\n  the National Academy of Sciences", "journal-ref": null, "doi": "10.1073/pnas.1419100111", "report-no": null, "categories": "q-bio.NC cs.DS math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Johnson-Lindenstrauss (JL) matrices implemented by sparse random synaptic\nconnections are thought to be a prime candidate for how convergent pathways in\nthe brain compress information. However, to date, there is no complete\nmathematical support for such implementations given the constraints of real\nneural tissue. The fact that neurons are either excitatory or inhibitory\nimplies that every so implementable JL matrix must be sign-consistent (i.e.,\nall entries in a single column must be either all non-negative or all\nnon-positive), and the fact that any given neuron connects to a relatively\nsmall subset of other neurons implies that the JL matrix had better be sparse.\n  We construct sparse JL matrices that are sign-consistent, and prove that our\nconstruction is essentially optimal. Our work answers a mathematical question\nthat was triggered by earlier work and is necessary to justify the existence of\nJL compression in the brain, and emphasizes that inhibition is crucial if\nneurons are to perform efficient, correlation-preserving compression.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 21:12:12 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Gelashvili", "Rati", ""], ["Micali", "Silvio", ""], ["Shavit", "Nir", ""]]}, {"id": "1411.5414", "submitter": "Francois Le Gall", "authors": "Andris Ambainis, Yuval Filmus, Fran\\c{c}ois Le Gall", "title": "Fast Matrix Multiplication: Limitations of the Laser Method", "comments": "38 pages + cover page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until a few years ago, the fastest known matrix multiplication algorithm, due\nto Coppersmith and Winograd (1990), ran in time $O(n^{2.3755})$. Recently, a\nsurge of activity by Stothers, Vassilevska-Williams, and Le Gall has led to an\nimproved algorithm running in time $O(n^{2.3729})$. These algorithms are\nobtained by analyzing higher and higher tensor powers of a certain identity of\nCoppersmith and Winograd. We show that this exact approach cannot result in an\nalgorithm with running time $O(n^{2.3725})$, and identify a wide class of\nvariants of this approach which cannot result in an algorithm with running time\n$O(n^{2.3078})$; in particular, this approach cannot prove the conjecture that\nfor every $\\epsilon > 0$, two $n\\times n$ matrices can be multiplied in time\n$O(n^{2+\\epsilon})$.\n  We describe a new framework extending the original laser method, which is the\nmethod underlying the previously mentioned algorithms. Our framework\naccommodates the algorithms by Coppersmith and Winograd, Stothers,\nVassilevska-Williams and Le Gall. We obtain our main result by analyzing this\nframework. The framework is also the first to explain why taking tensor powers\nof the Coppersmith-Winograd identity results in faster algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 01:07:08 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Ambainis", "Andris", ""], ["Filmus", "Yuval", ""], ["Gall", "Fran\u00e7ois Le", ""]]}, {"id": "1411.5630", "submitter": "Shi Li", "authors": "Shi Li", "title": "Approximating capacitated $k$-median with $(1+\\epsilon)k$ open\n  facilities", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the capacitated $k$-median (\\CKM) problem, we are given a set $F$ of\nfacilities, each facility $i \\in F$ with a capacity $u_i$, a set $C$ of\nclients, a metric $d$ over $F \\cup C$ and an integer $k$. The goal is to open\n$k$ facilities in $F$ and connect the clients $C$ to the open facilities such\nthat each facility $i$ is connected by at most $u_i$ clients, so as to minimize\nthe total connection cost.\n  In this paper, we give the first constant approximation for \\CKM, that only\nviolates the cardinality constraint by a factor of $1+\\epsilon$. This\ngeneralizes the result of [Li15], which only works for the uniform capacitated\ncase. Moreover, the approximation ratio we obtain is\n$O\\big(\\frac{1}{\\epsilon^2}\\log\\frac1\\epsilon\\big)$, which is an exponential\nimprovement over the ratio of $\\exp(O(1/\\epsilon^2))$ in [Li15]. The natural LP\nrelaxation for the problem, which almost all previous algorithms for \\CKM are\nbased on, has unbounded integrality gap even if $(2-\\epsilon)k$ facilities can\nbe opened. We introduce a novel configuration LP for the problem, that\novercomes this integrality gap.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 18:17:32 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2015 20:34:16 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Li", "Shi", ""]]}, {"id": "1411.5668", "submitter": "Matthew Hirn", "authors": "Ariel Herbert-Voss, Matthew J. Hirn, and Frederick McCollum", "title": "Computing minimal interpolants in $C^{1,1}(\\mathbb{R}^d)$", "comments": "41 pages, 6 figures. Replaces arXiv:1307.3292. v2: Minor edits,\n  formatting changed. v3: Revised version, which includes numerous updates,\n  corrections and edits for clarification. v4: Minor edits. Software available\n  at: https://github.com/matthew-hirn/C-1-1-Interpolation", "journal-ref": "Revista Matem\\'atica Iberoamericana, volume 33, issue 1, pages\n  29-66, 2017", "doi": "10.4171/rmi/927", "report-no": null, "categories": "math.NA cs.DS cs.NA math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following interpolation problem. Suppose one is given a\nfinite set $E \\subset \\mathbb{R}^d$, a function $f: E \\rightarrow \\mathbb{R}$,\nand possibly the gradients of $f$ at the points of $E$. We want to interpolate\nthe given information with a function $F \\in C^{1,1}(\\mathbb{R}^d)$ with the\nminimum possible value of $\\mathrm{Lip} (\\nabla F)$. We present practical,\nefficient algorithms for constructing an $F$ such that $\\mathrm{Lip} (\\nabla\nF)$ is minimal, or for less computational effort, within a small dimensionless\nconstant of being minimal.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 20:41:10 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 19:24:42 GMT"}, {"version": "v3", "created": "Fri, 17 Jun 2016 14:11:02 GMT"}, {"version": "v4", "created": "Wed, 26 Oct 2016 20:57:24 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Herbert-Voss", "Ariel", ""], ["Hirn", "Matthew J.", ""], ["McCollum", "Frederick", ""]]}, {"id": "1411.5739", "submitter": "Rahul Vaze", "authors": "Ashwin Pananjady, Vivek Kumar Bagaria, Rahul Vaze", "title": "The Online Disjoint Set Cover Problem and its Applications", "comments": "To appear in IEEE INFOCOM 2015", "journal-ref": null, "doi": "10.1109/INFOCOM.2015.7218497", "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a universe $U$ of $n$ elements and a collection of subsets\n$\\mathcal{S}$ of $U$, the maximum disjoint set cover problem (DSCP) is to\npartition $\\mathcal{S}$ into as many set covers as possible, where a set cover\nis defined as a collection of subsets whose union is $U$. We consider the\nonline DSCP, in which the subsets arrive one by one (possibly in an order\nchosen by an adversary), and must be irrevocably assigned to some partition on\narrival with the objective of minimizing the competitive ratio. The competitive\nratio of an online DSCP algorithm $A$ is defined as the maximum ratio of the\nnumber of disjoint set covers obtained by the optimal offline algorithm to the\nnumber of disjoint set covers obtained by $A$ across all inputs. We propose an\nonline algorithm for solving the DSCP with competitive ratio $\\ln n$. We then\nshow a lower bound of $\\Omega(\\sqrt{\\ln n})$ on the competitive ratio for any\nonline DSCP algorithm. The online disjoint set cover problem has wide ranging\napplications in practice, including the online crowd-sourcing problem, the\nonline coverage lifetime maximization problem in wireless sensor networks, and\nin online resource allocation problems.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 01:52:19 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Pananjady", "Ashwin", ""], ["Bagaria", "Vivek Kumar", ""], ["Vaze", "Rahul", ""]]}, {"id": "1411.5768", "submitter": "Sergey Polyakovskiy", "authors": "Sergey Polyakovskiy and Frank Neumann", "title": "Packing While Traveling: Mixed Integer Programming for a Class of\n  Nonlinear Knapsack Problems", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-18008-3_23", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Packing and vehicle routing problems play an important role in the area of\nsupply chain management. In this paper, we introduce a non-linear knapsack\nproblem that occurs when packing items along a fixed route and taking into\naccount travel time. We investigate constrained and unconstrained versions of\nthe problem and show that both are NP-hard. In order to solve the problems, we\nprovide a pre-processing scheme as well as exact and approximate mixed integer\nprogramming (MIP) solutions. Our experimental results show the effectiveness of\nthe MIP solutions and in particular point out that the approximate MIP approach\noften leads to near optimal results within far less computation time than the\nexact approach.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 04:53:50 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 10:22:53 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Polyakovskiy", "Sergey", ""], ["Neumann", "Frank", ""]]}, {"id": "1411.5849", "submitter": "Sigve Hortemo S{\\ae}ther", "authors": "Sigve Hortemo S{\\ae}ther", "title": "Solving Hamiltonian Cycle by an EPT Algorithm for a Non-sparse Parameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many hard graph problems, such as Hamiltonian Cycle, become FPT when\nparameterized by treewidth, a parameter that is bounded only on sparse graphs.\nWhen parameterized by the more general parameter clique-width, Hamiltonian\nCycle becomes W[1]-hard, as shown by Fomin et al. [5]. S{\\ae}ther and Telle\naddress this problem in their paper [13] by introducing a new parameter,\nsplit-matching-width, which lies between treewidth and clique-width in terms of\ngenerality. They show that even though graphs of restricted\nsplit-matching-width might be dense, solving problems such as Hamiltonian Cycle\ncan be done in FPT time.\n  Recently, it was shown that Hamiltonian Cycle parameterized by treewidth is\nin EPT [1, 6], meaning it can be solved in $n^{O(1)} 2^{O(k)}$-time. In this\npaper, using tools from [6], we show that also parameterized by\nsplit-matching-width Hamiltonian Cycle is EPT. To the best of our knowledge,\nthis is the first EPT algorithm for any \"globally constrained\" graph problem\nparameterized by a non-trivial and non-sparse structural parameter. To\naccomplish this, we also give an algorithm constructing a branch decomposition\napproximating the minimum split-matching-width to within a constant factor.\nCombined, these results show that the algorithms in [13] for Edge Dominating\nSet, Chromatic Number and Max Cut all can be improved. We also show that for\nHamiltonian Cycle and Max Cut the resulting algorithms are asymptotically\noptimal under the Exponential Time Hypothesis.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 12:50:23 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["S\u00e6ther", "Sigve Hortemo", ""]]}, {"id": "1411.5867", "submitter": "Eva Rotenberg", "authors": "Jacob Holm, Eva Rotenberg, Mikkel Thorup", "title": "Planar Reachability in Linear Space and Constant Time", "comments": "20 pages, 5 figures, submitted to FoCS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to represent a planar digraph in linear space so that distance\nqueries can be answered in constant time. The data structure can be constructed\nin linear time. This representation of reachability is thus optimal in both\ntime and space, and has optimal construction time. The previous best solution\nused $O(n\\log n)$ space for constant query time [Thorup FOCS'01].\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 13:28:14 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2015 14:08:58 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Holm", "Jacob", ""], ["Rotenberg", "Eva", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1411.6371", "submitter": "Yushi Uno", "authors": "Erik D. Demaine, David Eppstein, Adam Hesterberg, Hiro Ito, Anna\n  Lubiw, Ryuhei Uehara, and Yushi Uno", "title": "Folding a Paper Strip to Minimize Thickness", "comments": "9 pages, 7 figures", "journal-ref": "Journal of Discrete Algorithms 36: 18-26, 2016", "doi": "10.1016/j.jda.2015.09.003", "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study how to fold a specified origami crease pattern in\norder to minimize the impact of paper thickness. Specifically, origami designs\nare often expressed by a mountain-valley pattern (plane graph of creases with\nrelative fold orientations), but in general this specification is consistent\nwith exponentially many possible folded states. We analyze the complexity of\nfinding the best consistent folded state according to two metrics: minimizing\nthe total number of layers in the folded state (so that a \"flat folding\" is\nindeed close to flat), and minimizing the total amount of paper required to\nexecute the folding (where \"thicker\" creases consume more paper). We prove both\nproblems strongly NP-complete even for 1D folding. On the other hand, we prove\nthe first problem fixed-parameter tractable in 1D with respect to the number of\nlayers.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 07:45:36 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Demaine", "Erik D.", ""], ["Eppstein", "David", ""], ["Hesterberg", "Adam", ""], ["Ito", "Hiro", ""], ["Lubiw", "Anna", ""], ["Uehara", "Ryuhei", ""], ["Uno", "Yushi", ""]]}, {"id": "1411.6408", "submitter": "Michael Codish", "authors": "Michael Codish and Lu\\'is Cruz-Filipe and Peter Schneider-Kamp", "title": "Sorting Networks: the End Game", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-15579-1_52", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies properties of the back end of a sorting network and\nillustrates the utility of these in the search for networks of optimal size or\ndepth. All previous works focus on properties of the front end of networks and\non how to apply these to break symmetries in the search. The new properties\nhelp shed understanding on how sorting networks sort and speed-up solvers for\nboth optimal size and depth by an order of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 11:00:09 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Codish", "Michael", ""], ["Cruz-Filipe", "Lu\u00eds", ""], ["Schneider-Kamp", "Peter", ""]]}, {"id": "1411.6538", "submitter": "Akshay Gupte", "authors": "Nathan Adelgren, Pietro Belotti, Akshay Gupte", "title": "Efficient storage of Pareto points in biobjective mixed integer\n  programming", "comments": null, "journal-ref": "INFORMS Journal on Computing, 30 (2), pp. 324--338, 2018", "doi": "10.1287/ijoc.2017.0783", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biobjective mixed integer linear programs (BOMILPs), two linear objectives\nare minimized over a polyhedron while restricting some of the variables to be\ninteger. Since many of the techniques for finding or approximating the Pareto\nset of a BOMILP use and update a subset of nondominated solutions, it is highly\ndesirable to efficiently store this subset. We present a new data structure, a\nvariant of a binary tree that takes as input points and line segments in $\\R^2$\nand stores the nondominated subset of this input. When used within an exact\nsolution procedure, such as branch-and-bound (BB), at termination this\nstructure contains the set of Pareto optimal solutions.\n  We compare the efficiency of our structure in storing solutions to that of a\ndynamic list which updates via pairwise comparison. Then we use our data\nstructure in two biobjective BB techniques available in the literature and\nsolve three classes of instances of BOMILP, one of which is generated by us.\nThe first experiment shows that our data structure handles up to $10^7$ points\nor segments much more efficiently than a dynamic list. The second experiment\nshows that our data structure handles points and segments much more efficiently\nthan a list when used in a BB.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 17:28:46 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 20:35:34 GMT"}, {"version": "v3", "created": "Tue, 2 May 2017 23:57:51 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Adelgren", "Nathan", ""], ["Belotti", "Pietro", ""], ["Gupte", "Akshay", ""]]}, {"id": "1411.6581", "submitter": "Patrick Nicholson", "authors": "Pawel Gawrychowski and Patrick K. Nicholson", "title": "Optimal Encodings for Range Top-k, Selection, and Min-Max", "comments": "24 pages: a short version of this paper will be presented at ICALP\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider encoding problems for range queries on arrays. In these problems\nthe goal is to store a structure capable of recovering the answer to all\nqueries that occupies the information theoretic minimum space possible, to\nwithin lower order terms. As input, we are given an array $A[1..n]$, and a\nfixed parameter $k \\in [1,n]$. A range top-$k$ query on an arbitrary range\n$[i,j] \\subseteq [1,n]$ asks us to return the ordered set of indices $\\{\\ell_1,\n..., \\ell_{k}\\}$ such that $A[\\ell_m]$ is the $m$-th largest element in\n$A[i..j]$, for $1 \\le m \\le k$. A range selection query for an arbitrary range\n$[i,j] \\subseteq [1,n]$ and query parameter $k' \\in [1,k]$ asks us to return\nthe index of the $k'$-th largest element in $A[i..j]$. We completely resolve\nthe space complexity of both of these heavily studied problems---to within\nlower order terms---for all $k = o(n)$. Previously, the constant factor in the\nspace complexity was known only for $k=1$. We also resolve the space complexity\nof another problem, that we call range min-max, in which the goal is to return\nthe indices of both the minimum and maximum elements in a range.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 19:27:44 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 11:49:54 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Gawrychowski", "Pawel", ""], ["Nicholson", "Patrick K.", ""]]}, {"id": "1411.6667", "submitter": "Carol Wang", "authors": "Venkatesan Guruswami and Carol Wang", "title": "Deletion codes in the high-noise and high-rate regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The noise model of deletions poses significant challenges in coding theory,\nwith basic questions like the capacity of the binary deletion channel still\nbeing open. In this paper, we study the harder model of worst-case deletions,\nwith a focus on constructing efficiently decodable codes for the two extreme\nregimes of high-noise and high-rate. Specifically, we construct polynomial-time\ndecodable codes with the following trade-offs (for any eps > 0):\n  (1) Codes that can correct a fraction 1-eps of deletions with rate poly(eps)\nover an alphabet of size poly(1/eps);\n  (2) Binary codes of rate 1-O~(sqrt(eps)) that can correct a fraction eps of\ndeletions; and\n  (3) Binary codes that can be list decoded from a fraction (1/2-eps) of\ndeletions with rate poly(eps)\n  Our work is the first to achieve the qualitative goals of correcting a\ndeletion fraction approaching 1 over bounded alphabets, and correcting a\nconstant fraction of bit deletions with rate aproaching 1. The above results\nbring our understanding of deletion code constructions in these regimes to a\nsimilar level as worst-case errors.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 21:54:47 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Wang", "Carol", ""]]}, {"id": "1411.6673", "submitter": "Kashyap Dixit", "authors": "Kashyap Dixit and Martin F\\\"urer", "title": "Counting cliques and clique covers in random graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of counting the number of {\\em isomorphic} copies of a\ngiven {\\em template} graph, say $H$, in the input {\\em base} graph, say $G$. In\ngeneral, it is believed that polynomial time algorithms that solve this problem\nexactly are unlikely to exist. So, a lot of work has gone into designing\nefficient {\\em approximation schemes}, especially, when $H$ is a perfect\nmatching.\n  In this work, we present efficient approximation schemes to count\n$k$-Cliques, $k$-Independent sets and $k$-Clique covers in random graphs. We\npresent {\\em fully polynomial time randomized approximation schemes} (fpras) to\ncount $k$-Cliques and $k$-Independent sets in a random graph on $n$ vertices\nwhen $k$ is at most $(1+o(1))\\log n$, and $k$-Clique covers when $k$ is a\nconstant. [Grimmett and McDiarmid, 1975] present a simple greedy algorithm that\n{\\em detects} a clique (independent set) of size $(1+o(1))\\log_2 n$ in $G\\in\n\\mathcal{G}(n,\\frac{1}{2})$ with high probability. No algorithm is known to\ndetect a clique or an independent set of larger size with non-vanishing\nprobability. Furthermore, [Coja-Oghlan and Efthymiou, 2011] present some\nevidence that one cannot hope to easily improve a similar, almost 40 years old\nbound for sparse random graphs. Therefore, our results are unlikely to be\neasily improved.\n  We use a novel approach to obtain a recurrence corresponding to the variance\nof each estimator. Then we upper bound the variance using the corresponding\nrecurrence. This leads us to obtain a polynomial upper bound on the critical\nratio. As an aside, we also obtain an alternate derivation of the closed form\nexpression for the $k$-th moment of a binomial random variable using our\ntechniques. The previous derivation [Knoblauch (2008)] was based on the moment\ngenerating function of a binomial random variable.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 22:33:00 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2015 19:46:37 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Dixit", "Kashyap", ""], ["F\u00fcrer", "Martin", ""]]}, {"id": "1411.6756", "submitter": "Ariel Gabizon", "authors": "Ariel Gabizon, Daniel Lokshtanov, Michal Pilipczuk", "title": "Fast Algorithms for Parameterized Problems with Relaxed Disjointness\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In parameterized complexity, it is a natural idea to consider different\ngeneralizations of classic problems. Usually, such generalization are obtained\nby introducing a \"relaxation\" variable, where the original problem corresponds\nto setting this variable to a constant value. For instance, the problem of\npacking sets of size at most $p$ into a given universe generalizes the Maximum\nMatching problem, which is recovered by taking $p=2$. Most often, the\ncomplexity of the problem increases with the relaxation variable, but very\nrecently Abasi et al. have given a surprising example of a problem ---\n$r$-Simple $k$-Path --- that can be solved by a randomized algorithm with\nrunning time $O^*(2^{O(k \\frac{\\log r}{r})})$. That is, the complexity of the\nproblem decreases with $r$. In this paper we pursue further the direction\nsketched by Abasi et al. Our main contribution is a derandomization tool that\nprovides a deterministic counterpart of the main technical result of Abasi et\nal.: the $O^*(2^{O(k \\frac{\\log r}{r})})$ algorithm for $(r,k)$-Monomial\nDetection, which is the problem of finding a monomial of total degree $k$ and\nindividual degrees at most $r$ in a polynomial given as an arithmetic circuit.\nOur technique works for a large class of circuits, and in particular it can be\nused to derandomize the result of Abasi et al. for $r$-Simple $k$-Path. On our\nway to this result we introduce the notion of representative sets for\nmultisets, which may be of independent interest. Finally, we give two more\nexamples of problems that were already studied in the literature, where the\nsame relaxation phenomenon happens. The first one is a natural relaxation of\nthe Set Packing problem, where we allow the packed sets to overlap at each\nelement at most $r$ times. The second one is Degree Bounded Spanning Tree,\nwhere we seek for a spanning tree of the graph with a small maximum degree.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 08:03:19 GMT"}, {"version": "v2", "created": "Wed, 26 Nov 2014 16:29:26 GMT"}, {"version": "v3", "created": "Thu, 27 Nov 2014 10:06:43 GMT"}, {"version": "v4", "created": "Thu, 23 Apr 2015 18:05:56 GMT"}, {"version": "v5", "created": "Fri, 24 Apr 2015 08:40:05 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Gabizon", "Ariel", ""], ["Lokshtanov", "Daniel", ""], ["Pilipczuk", "Michal", ""]]}, {"id": "1411.6804", "submitter": "Leo van Iersel", "authors": "Katharina Huber, Leo van Iersel, Vincent Moulton, Celine Scornavacca\n  and Taoyang Wu", "title": "Reconstructing phylogenetic level-1 networks from nondense binet and\n  trinet sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binets and trinets are phylogenetic networks with two and three leaves,\nrespectively. Here we consider the problem of deciding if there exists a binary\nlevel-1 phylogenetic network displaying a given set $\\mathcal{T}$ of binary\nbinets or trinets over a set $X$ of taxa, and constructing such a network\nwhenever it exists. We show that this is NP-hard for trinets but\npolynomial-time solvable for binets. Moreover, we show that the problem is\nstill polynomial-time solvable for inputs consisting of binets and trinets as\nlong as the cycles in the trinets have size three. Finally, we present an\n$O(3^{|X|} poly(|X|))$ time algorithm for general sets of binets and trinets.\nThe latter two algorithms generalise to instances containing level-1 networks\nwith arbitrarily many leaves, and thus provide some of the first supernetwork\nalgorithms for computing networks from a set of rooted phylogenetic networks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 10:41:32 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Huber", "Katharina", ""], ["van Iersel", "Leo", ""], ["Moulton", "Vincent", ""], ["Scornavacca", "Celine", ""], ["Wu", "Taoyang", ""]]}, {"id": "1411.6818", "submitter": "Agnes Cseh", "authors": "\\'Agnes Cseh, Brian C. Dean", "title": "Improved Algorithmic Results for Unsplittable Stable Allocation Problems", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stable allocation problem is a many-to-many generalization of the\nwell-known stable marriage problem, where we seek a bipartite assignment\nbetween, say, jobs (of varying sizes) and machines (of varying capacities) that\nis \"stable\" based on a set of underlying preference lists submitted by the jobs\nand machines. We study a natural \"unsplittable\" variant of this problem, where\neach assigned job must be fully assigned to a single machine. Such unsplittable\nbipartite assignment problems generally tend to be NP-hard, including\npreviously-proposed variants of the unsplittable stable allocation problem. Our\nmain result is to show that under an alternative model of stability, the\nunsplittable stable allocation problem becomes solvable in polynomial time;\nalthough this model is less likely to admit feasible solutions than the model\nproposed iby McDermid and Manlove, we show that in the event there is no\nfeasible solution, our approach computes a solution of minimal total congestion\n(overfilling of all machines collectively beyond their capacities). We also\ndescribe a technique for rounding the solution of a stable allocation problem\nto produce \"relaxed\" unsplit solutions that are only mildly infeasible, where\neach machine is overcongested by at most a single job.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 11:17:51 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Cseh", "\u00c1gnes", ""], ["Dean", "Brian C.", ""]]}, {"id": "1411.6852", "submitter": "Gustavo Sacomoto", "authors": "Romeo Rizzi, Gustavo Sacomoto, Marie-France Sagot", "title": "Efficiently listing bounded length st-paths", "comments": "12 pages, accepted to IWOCA 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of listing the $K$ shortest simple (loopless) $st$-paths in a\ngraph has been studied since the early 1960s. For a non-negatively weighted\ngraph with $n$ vertices and $m$ edges, the most efficient solution is an\n$O(K(mn + n^2 \\log n))$ algorithm for directed graphs by Yen and Lawler\n[Management Science, 1971 and 1972], and an $O(K(m+n \\log n))$ algorithm for\nthe undirected version by Katoh et al. [Networks, 1982], both using $O(Kn + m)$\nspace. In this work, we consider a different parameterization for this problem:\ninstead of bounding the number of $st$-paths output, we bound their length. For\nthe bounded length parameterization, we propose new non-trivial algorithms\nmatching the time complexity of the classic algorithms but using only $O(m+n)$\nspace. Moreover, we provide a unified framework such that the solutions to both\nparameterizations -- the classic $K$-shortest and the new length-bounded paths\n-- can be seen as two different traversals of a same tree, a Dijkstra-like and\na DFS-like traversal, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 13:15:36 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Rizzi", "Romeo", ""], ["Sacomoto", "Gustavo", ""], ["Sagot", "Marie-France", ""]]}, {"id": "1411.6853", "submitter": "Akitoshi Kawamura", "authors": "Akitoshi Kawamura and Makoto Soejima", "title": "Simple strategies versus optimal schedules in multi-agent patrolling", "comments": "20 pages, 3 figures", "journal-ref": "Theoretical Computer Science 839 (2020) 195-206", "doi": "10.1016/j.tcs.2020.07.037", "report-no": null, "categories": "cs.DS cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that a set of mobile agents, each with a predefined maximum speed,\nwant to patrol a fence together so as to minimize the longest time interval\nduring which a point on the fence is left unvisited. In 2011, Czyzowicz,\nG\\k{a}sieniec, Kosowski and Kranakis studied this problem for the settings\nwhere the fence is an interval (a line segment) and a circle, and conjectured\nthat the following simple strategies are always optimal: for Interval\nPatrolling, the simple strategy partitions the fence into subintervals, one for\neach agent, and lets each agent move back and forth in the assigned subinterval\nwith its maximum speed; for Circle Patrolling, the simple strategy is to choose\na number r, place the r fastest agents equidistantly around the circle, and\nmove them at the speed of the rth agent. Surprisingly, these conjectures were\nthen proved false: schedules were found (for some settings of maximum speeds)\nthat slightly outperform the simple strategies. In this paper, we are\ninterested in the ratio between the performances of optimal schedules and\nsimple strategies. For the two problems, we construct schedules that are 4/3\ntimes (for Interval Patrolling) and 21/20 times (for Circle Patrolling) as\ngood, respectively, as the simple strategies. We also propose a new variant, in\nwhich we want to patrol a single point under the constraint that each agent can\nonly visit the point some predefined time after its previous visit. We obtain\nsome similar ratio bounds and NP-hardness results related to this problem.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 13:17:18 GMT"}, {"version": "v2", "created": "Wed, 26 Nov 2014 05:38:50 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 03:00:00 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Kawamura", "Akitoshi", ""], ["Soejima", "Makoto", ""]]}, {"id": "1411.6915", "submitter": "Jazm\\'in Romero", "authors": "Henning Fernau, Alejandro L\\'opez-Ortiz, Jazm\\'in Romero", "title": "Kernelization Algorithms for Packing Problems Allowing Overlaps\n  (Extended Version)", "comments": "25 pages, 24 references, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of discovering overlapping communities in networks\nwhich we model as generalizations of Graph Packing problems with overlap.\n  We seek a collection $\\mathcal{S}' \\subseteq \\mathcal{S}$ consisting of at\nleast $k$ sets subject to certain disjointness restrictions. In the $r$-Set\nPacking with $t$-Membership, each element of $\\mathcal{U}$ belongs to at most\n$t$ sets of $\\mathcal{S'}$ while in $t$-Overlap each pair of sets in\n$\\mathcal{S'}$ overlaps in at most $t$ elements. Each set of $\\mathcal{S}$ has\nat most $r$ elements.\n  Similarly, both of our graph packing problems seek a collection $\\mathcal{K}$\nof at least $k$ subgraphs in a graph $G$ each isomorphic to a graph $H \\in\n\\mathcal{H}$. In $\\mathcal{H}$-Packing with $t$-Membership, each vertex of $G$\nbelongs to at most $t$ subgraphs of $\\mathcal{K}$ while in $t$-Overlap each\npair of subgraphs in $\\mathcal{K}$ overlaps in at most $t$ vertices. Each\nmember of $\\mathcal{H}$ has at most $r$ vertices and $m$ edges.\n  We show NP-Completeness results for all of our packing problems and we give a\ndichotomy result for the $\\mathcal{H}$-Packing with $t$-Membership problem\nanalogous to the Kirkpatrick and Hell \\cite{Kirk78}. We reduce the $r$-Set\nPacking with $t$-Membership to a problem kernel with $O((r+1)^r k^{r})$\nelements while we achieve a kernel with $O(r^r k^{r-t-1})$ elements for the\n$r$-Set Packing with $t$-Overlap. In addition, we reduce the\n$\\mathcal{H}$-Packing with $t$-Membership and its edge version to problem\nkernels with $O((r+1)^r k^{r})$ and $O((m+1)^{m} k^{{m}})$ vertices,\nrespectively. On the other hand, we achieve kernels with $O(r^r k^{r-t-1})$ and\n$O(m^{m} k^{m-t-1})$ vertices for the $\\mathcal{H}$-Packing with $t$-Overlap\nand its edge version, respectively. In all cases, $k$ is the input parameter\nwhile $t$, $r$, and $m$ are constants.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 16:47:42 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 19:22:41 GMT"}, {"version": "v3", "created": "Mon, 26 Jan 2015 19:02:01 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Fernau", "Henning", ""], ["L\u00f3pez-Ortiz", "Alejandro", ""], ["Romero", "Jazm\u00edn", ""]]}, {"id": "1411.7055", "submitter": "Amri Nayyeri", "authors": "Glencora Borradaile, David Eppstein, Amir Nayyeri, Christian\n  Wulff-Nilsen", "title": "All-Pairs Minimum Cuts in Near-Linear Time for Surface-Embedded Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an undirected $n$-vertex graph $G$ with non-negative edge-weights, we\nconsider the following type of query: given two vertices $s$ and $t$ in $G$,\nwhat is the weight of a minimum $st$-cut in $G$? We solve this problem in\npreprocessing time $O(n\\log^3 n)$ for graphs of bounded genus, giving the first\nsub-quadratic time algorithm for this class of graphs. Our result also improves\nby a logarithmic factor a previous algorithm by Borradaile, Sankowski and\nWulff-Nilsen (FOCS 2010) that applied only to planar graphs. Our algorithm\nconstructs a Gomory-Hu tree for the given graph, providing a data structure\nwith space $O(n)$ that can answer minimum-cut queries in constant time. The\ndependence on the genus of the input graph in our preprocessing time is\n$2^{O(g^2)}$.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 22:23:14 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 22:22:07 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Borradaile", "Glencora", ""], ["Eppstein", "David", ""], ["Nayyeri", "Amir", ""], ["Wulff-Nilsen", "Christian", ""]]}, {"id": "1411.7101", "submitter": "Nitish Umang", "authors": "Nitish Umang, Alan L. Erera, Michel Bierlaire", "title": "The robust single machine scheduling problem with uncertain release and\n  processing times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the single machine scheduling problem with uncertain\nrelease times and processing times of jobs. We adopt a robust scheduling\napproach, in which the measure of robustness to be minimized for a given\nsequence of jobs is the worst-case objective function value from the set of all\npossible realizations of release and processing times. The objective function\nvalue is the total flow time of all jobs. We discuss some important properties\nof robust schedules for zero and non-zero release times, and illustrate the\nadded complexity in robust scheduling given non-zero release times. We propose\nheuristics based on variable neighborhood search and iterated local search to\nsolve the problem and generate robust schedules. The algorithms are tested and\ntheir solution performance is compared with optimal solutions or lower bounds\nthrough numerical experiments based on synthetic data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 03:56:59 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Umang", "Nitish", ""], ["Erera", "Alan L.", ""], ["Bierlaire", "Michel", ""]]}, {"id": "1411.7191", "submitter": "S{\\o}ren Dahlgaard", "authors": "S{\\o}ren Dahlgaard, Mathias B{\\ae}k Tejs Knudsen, Eva Rotenberg,\n  Mikkel Thorup", "title": "Hashing for statistics over k-partitions", "comments": "Appear at FOCS'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze a hash function for $k$-partitioning a set into\nbins, obtaining strong concentration bounds for standard algorithms combining\nstatistics from each bin.\n  This generic method was originally introduced by Flajolet and\nMartin~[FOCS'83] in order to save a factor $\\Omega(k)$ of time per element over\n$k$ independent samples when estimating the number of distinct elements in a\ndata stream. It was also used in the widely used HyperLogLog algorithm of\nFlajolet et al.~[AOFA'97] and in large-scale machine learning by Li et\nal.~[NIPS'12] for minwise estimation of set similarity.\n  The main issue of $k$-partition, is that the contents of different bins may\nbe highly correlated when using popular hash functions. This means that methods\nof analyzing the marginal distribution for a single bin do not apply. Here we\nshow that a tabulation based hash function, mixed tabulation, does yield strong\nconcentration bounds on the most popular applications of $k$-partitioning\nsimilar to those we would get using a truly random hash function. The analysis\nis very involved and implies several new results of independent interest for\nboth simple and double tabulation, e.g. a simple and efficient construction for\ninvertible bloom filters and uniform hashing on a given set.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 11:36:15 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2015 14:27:46 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2016 16:06:53 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Dahlgaard", "S\u00f8ren", ""], ["Knudsen", "Mathias B\u00e6k Tejs", ""], ["Rotenberg", "Eva", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1411.7228", "submitter": "Takanori Maehara", "authors": "Takanori Maehara, Mitsuru Kusumoto, Ken-ichi Kawarabayashi", "title": "Efficient SimRank Computation via Linearization", "comments": "46 pages, journal version of our papers appeared in SIGMOD14 and\n  ICDE15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SimRank, proposed by Jeh and Widom, provides a good similarity measure that\nhas been successfully used in numerous applications. While there are many\nalgorithms proposed for computing SimRank, their computational costs are very\nhigh. In this paper, we propose a new computational technique, \"SimRank\nlinearization,\" for computing SimRank, which converts the SimRank problem to a\nlinear equation problem. By using this technique, we can solve many SimRank\nproblems, such as single-pair compuation, single-source computation, all-pairs\ncomputation, top k searching, and similarity join problems, efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 13:57:03 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Maehara", "Takanori", ""], ["Kusumoto", "Mitsuru", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "1411.7277", "submitter": "Martin Derka", "authors": "Therese Biedl, Martin Derka", "title": "$1$-String $B_2$-VPG Representation of Planar Graphs", "comments": "arXiv admin note: text overlap with arXiv:1409.5816", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove that every planar graph has a 1-string $B_2$-VPG\nrepresentation---a string representation using paths in a rectangular grid that\ncontain at most two bends. Furthermore, two paths representing vertices $u,v$\nintersect precisely once whenever there is an edge between $u$ and $v$.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 15:59:43 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 14:25:28 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Biedl", "Therese", ""], ["Derka", "Martin", ""]]}, {"id": "1411.7315", "submitter": "Barna Saha", "authors": "Barna Saha", "title": "Language Edit Distance & Maximum Likelihood Parsing of Stochastic\n  Grammars: Faster Algorithms & Connection to Fundamental Graph Problems", "comments": "36 pages: This is an updated version of the previous submission\n  \"Faster Language Edit Distance, Connection to All-pairs Shortest Paths and\n  Related Problems\". Introduction is rewritten, an error in a previous lower\n  bound proof corrected, and the Sidon sequence construction is elaborated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a context free language $L(G)$ over alphabet $\\Sigma$ and a string $s\n\\in \\Sigma^*$, the language edit distance (Lan-ED) problem seeks the minimum\nnumber of edits (insertions, deletions and substitutions) required to convert\n$s$ into a valid member of $L(G)$. The well-known dynamic programming algorithm\nsolves this problem in $O(n^3)$ time (ignoring grammar size) where $n$ is the\nstring length [Aho, Peterson 1972, Myers 1985]. Despite its vast number of\napplications, there is no algorithm known till date that computes or\napproximates Lan-ED in true sub-cubic time.\n  In this paper we give the first such algorithm that computes Lan-ED almost\noptimally. For any arbitrary $\\epsilon > 0$, our algorithm runs in\n$\\tilde{O}(\\frac{n^{\\omega}}{poly(\\epsilon)})$ time and returns an estimate\nwithin a multiplicative approximation factor of $(1+\\epsilon)$, where $\\omega$\nis the exponent of ordinary matrix multiplication of $n$ dimensional square\nmatrices. It also computes the edit script. Further, for all substrings of $s$,\nwe can estimate their Lan-ED within $(1\\pm \\epsilon)$ factor in\n$\\tilde{O}(\\frac{n^{\\omega}}{poly(\\epsilon)})$ time with high probability. We\nalso design the very first sub-cubic ($\\tilde{O}(n^\\omega)$) algorithm to\nhandle arbitrary stochastic context free grammar (SCFG) parsing. SCFGs lie at\nthe foundation of statistical natural language processing, they generalize\nhidden Markov models, and have found widespread applications.\n  To complement our upper bound result, we show that exact computation of SCFG\nparsing, or Lan-ED with insertion as only edit operation in true sub-cubic time\nwill imply a truly sub-cubic algorithm for all-pairs shortest paths, and hence\nto a large range of problems in graphs and matrices. Known lower bound results\non parsing implies no improvement over our time bound of $O(n^\\omega)$ is\npossible for any nontrivial multiplicative approximation.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 17:57:04 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 04:42:28 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2015 08:41:23 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Saha", "Barna", ""]]}, {"id": "1411.7338", "submitter": "Katherine St. John", "authors": "Daniel Irving Bernstein, Lam Si Tung Ho, Colby Long, Mike Steel,\n  Katherine St. John, Seth Sullivant", "title": "Bounds on the Expected Size of the Maximum Agreement Subtree", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove polynomial upper and lower bounds on the expected size of the\nmaximum agreement subtree of two random binary phylogenetic trees under both\nthe uniform distribution and Yule-Harding distribution. This positively answers\na question posed in earlier work. Determining tight upper and lower bounds\nremains an open problem.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 19:20:08 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2015 11:44:57 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Bernstein", "Daniel Irving", ""], ["Ho", "Lam Si Tung", ""], ["Long", "Colby", ""], ["Steel", "Mike", ""], ["John", "Katherine St.", ""], ["Sullivant", "Seth", ""]]}, {"id": "1411.7346", "submitter": "Gautam Kamath", "authors": "Jayadev Acharya, Cl\\'ement L. Canonne, Gautam Kamath", "title": "A Chasm Between Identity and Equivalence Testing with Conditional\n  Queries", "comments": "39 pages. To appear in Theory of Computing. Preliminary version\n  appeared in RANDOM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent model for property testing of probability distributions (Chakraborty\net al., ITCS 2013, Canonne et al., SICOMP 2015) enables tremendous savings in\nthe sample complexity of testing algorithms, by allowing them to condition the\nsampling on subsets of the domain. In particular, Canonne, Ron, and Servedio\n(SICOMP 2015) showed that, in this setting, testing identity of an unknown\ndistribution $D$ (whether $D=D^\\ast$ for an explicitly known $D^\\ast$) can be\ndone with a constant number of queries, independent of the support size $n$ --\nin contrast to the required $\\Omega(\\sqrt{n})$ in the standard sampling model.\nIt was unclear whether the same stark contrast exists for the case of testing\nequivalence, where both distributions are unknown. While Canonne et al.\nestablished a $\\mathrm{poly}(\\log n)$-query upper bound for equivalence\ntesting, very recently brought down to $\\tilde O(\\log\\log n)$ by Falahatgar et\nal. (COLT 2015), whether a dependence on the domain size $n$ is necessary was\nstill open, and explicitly posed by Fischer at the Bertinoro Workshop on\nSublinear Algorithms (2014). We show that any testing algorithm for equivalence\nmust make $\\Omega(\\sqrt{\\log\\log n})$ queries in the conditional sampling\nmodel. This demonstrates a gap between identity and equivalence testing, absent\nin the standard sampling model (where both problems have sampling complexity\n$n^{\\Theta(1)}$).\n  We also obtain results on the query complexity of uniformity testing and\nsupport-size estimation with conditional samples. We answer a question of\nChakraborty et al. (ITCS 2013) showing that non-adaptive uniformity testing\nindeed requires $\\Omega(\\log n)$ queries in the conditional model. For the\nrelated problem of support-size estimation, we provide both adaptive and\nnon-adaptive algorithms, with query complexities $\\mathrm{poly}(\\log\\log n)$\nand $\\mathrm{poly}(\\log n)$, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 19:44:12 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2015 23:37:46 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 01:30:55 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Kamath", "Gautam", ""]]}, {"id": "1411.7359", "submitter": "Alejandro Salinger", "authors": "Arash Farzan, Alejandro L\\'opez-Ortiz, Patrick K. Nicholson, Alejandro\n  Salinger", "title": "Algorithms in the Ultra-Wide Word Model", "comments": "28 pages, 5 figures; minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effective use of parallel computing resources to speed up algorithms in\ncurrent multi-core parallel architectures remains a difficult challenge, with\nease of programming playing a key role in the eventual success of various\nparallel architectures. In this paper we consider an alternative view of\nparallelism in the form of an ultra-wide word processor. We introduce the\nUltra-Wide Word architecture and model, an extension of the word-RAM model that\nallows for constant time operations on thousands of bits in parallel. Word\nparallelism as exploited by the word-RAM model does not suffer from the more\ndifficult aspects of parallel programming, namely synchronization and\nconcurrency. For the standard word-RAM algorithms, the speedups obtained are\nmoderate, as they are limited by the word size. We argue that a large class of\nword-RAM algorithms can be implemented in the Ultra-Wide Word model, obtaining\nspeedups comparable to multi-threaded computations while keeping the simplicity\nof programming of the sequential RAM model. We show that this is the case by\ndescribing implementations of Ultra-Wide Word algorithms for dynamic\nprogramming and string searching. In addition, we show that the Ultra-Wide Word\nmodel can be used to implement a nonstandard memory architecture, which enables\nthe sidestepping of lower bounds of important data structure problems such as\npriority queues and dynamic prefix sums. While similar ideas about operating on\nlarge words have been mentioned before in the context of multimedia processors\n[Thorup 2003], it is only recently that an architecture like the one we propose\nhas become feasible and that details can be worked out.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 20:25:27 GMT"}, {"version": "v2", "created": "Sun, 7 Dec 2014 17:36:42 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Farzan", "Arash", ""], ["L\u00f3pez-Ortiz", "Alejandro", ""], ["Nicholson", "Patrick K.", ""], ["Salinger", "Alejandro", ""]]}, {"id": "1411.7460", "submitter": "Bharath Pattabiraman", "authors": "Bharath Pattabiraman, Md. Mostofa Ali Patwary, Assefaw H. Gebremedhin,\n  Wei-keng Liao, and Alok Choudhary", "title": "Fast Algorithms for the Maximum Clique Problem on Massive Graphs with\n  Applications to Overlapping Community Detection", "comments": "28 pages, 7 figures, 10 tables, 2 algorithms. arXiv admin note:\n  substantial text overlap with arXiv:1209.5818", "journal-ref": "Internet Mathematics 2014, Special Issue (WAW'13)", "doi": "10.1080/15427951.2014.986778", "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum clique problem is a well known NP-Hard problem with applications\nin data mining, network analysis, information retrieval and many other areas\nrelated to the World Wide Web. There exist several algorithms for the problem\nwith acceptable runtimes for certain classes of graphs, but many of them are\ninfeasible for massive graphs. We present a new exact algorithm that employs\nnovel pruning techniques and is able to find maximum cliques in very large,\nsparse graphs quickly. Extensive experiments on different kinds of synthetic\nand real-world graphs show that our new algorithm can be orders of magnitude\nfaster than existing algorithms. We also present a heuristic that runs orders\nof magnitude faster than the exact algorithm while providing optimal or\nnear-optimal solutions. We illustrate a simple application of the algorithms in\ndeveloping methods for detection of overlapping communities in networks.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 03:40:06 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Pattabiraman", "Bharath", ""], ["Patwary", "Md. Mostofa Ali", ""], ["Gebremedhin", "Assefaw H.", ""], ["Liao", "Wei-keng", ""], ["Choudhary", "Alok", ""]]}, {"id": "1411.7472", "submitter": "Yichong Xu", "authors": "Pingzhong Tang, Yifeng Teng, Zihe Wang, Shenke Xiao, Yichong Xu", "title": "Computational issues in time-inconsistent planning", "comments": "20 pages, 7 figures, submitted to EC'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-inconsistency refers to a paradox in decision making where agents\nexhibit inconsistent behaviors over time. Examples are procrastination where\nagents tends to costly postpone easy tasks, and abandonments where agents start\na plan and quit in the middle. These behaviors are undesirable in the sense\nthat agents make clearly suboptimal decisions over optimal ones. To capture\nsuch behaviors and more importantly, to quantify inefficiency caused by such\nbehaviors, [Kleinberg & Oren 2014] propose a graph model which is essentially\nsame as the standard planning model except for the cost structure. Using this\nmodel, they initiate the study of several interesting problems: 1) cost ratio:\nthe worst ratio between the actual cost of the agent and the optimal cost, over\nall graph instances; 2) motivating subgraph: how to motivate the agent to reach\nthe goal by deleting nodes and edges; 3) Intermediate rewards: how to motivate\nagents to reach the goal by placing intermediate rewards. Kleinberg and Oren\ngive partial answers to these questions, but the main problems are still open.\nIn fact, they raise these problems explicitly as open problems in their paper.\nIn this paper, we give answers to all three open problems in [Kleinberg & Oren\n2014]. First, we show a tight upper bound of cost ratio for graphs without\nAkerlof's structure, thus confirm the conjecture by Kleinberg and Oren that\nAkerlof's structure is indeed the worst case for cost ratio. Second, we prove\nthat finding a motivating subgraph is NP-hard, showing that it is generally\ninefficient to motivate agents by deleting nodes and edges in the graph. Last\nbut not least, we show that computing a strategy to place minimum amount of\ntotal reward is also NP-hard. Therefore, it is computational inefficient to\nmotivate agents by placing intermediate rewards. The techniques we use to prove\nthese results are nontrivial and of independent interests.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 05:15:08 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 07:41:27 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2015 14:37:34 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Tang", "Pingzhong", ""], ["Teng", "Yifeng", ""], ["Wang", "Zihe", ""], ["Xiao", "Shenke", ""], ["Xu", "Yichong", ""]]}, {"id": "1411.7614", "submitter": "Rohit Gurjar", "authors": "Rahul Arora and Ashu Gupta and Rohit Gurjar and Raghunath Tewari", "title": "Derandomizing Isolation Lemma for $K_{3,3}$-free and $K_5$-free\n  Bipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The perfect matching problem has a randomized NC algorithm, using the\ncelebrated Isolation Lemma of Mulmuley, Vazirani and Vazirani. The Isolation\nLemma states that giving a random weight assignment to the edges of a graph,\nensures that it has a unique minimum weight perfect matching, with a good\nprobability. We derandomize this lemma for $K_{3,3}$-free and $K_5$-free\nbipartite graphs, i.e. we give a deterministic log-space construction of such a\nweight assignment for these graphs. Such a construction was known previously\nfor planar bipartite graphs. Our result implies that the perfect matching\nproblem for $K_{3,3}$-free and $K_5$-free bipartite graphs is in SPL.\n  It also gives an alternate proof for an already known result -- reachability\nfor $K_{3,3}$-free and $K_5$-free graphs is in UL.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 14:51:00 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Arora", "Rahul", ""], ["Gupta", "Ashu", ""], ["Gurjar", "Rohit", ""], ["Tewari", "Raghunath", ""]]}, {"id": "1411.7631", "submitter": "Richard Peng", "authors": "Richard Peng", "title": "Approximate Undirected Maximum Flows in O(m polylog(n)) Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first O(m polylog(n)) time algorithms for approximating maximum\nflows in undirected graphs and constructing polylog(n) -quality\ncut-approximating hierarchical tree decompositions. Our algorithm invokes\nexisting algorithms for these two problems recursively while gradually\nincorporating size reductions. These size reductions are in turn obtained via\nultra-sparsifiers, which are key tools in solvers for symmetric diagonally\ndominant (SDD) linear systems.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 15:48:55 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 02:30:37 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Peng", "Richard", ""]]}, {"id": "1411.7838", "submitter": "Nimrod Talmon", "authors": "Laurent Bulteau, Stefan Fafianie, Vincent Froese, Rolf Niedermeier,\n  Nimrod Talmon", "title": "The Complexity of Finding Effectors", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NP-hard EFFECTORS problem on directed graphs is motivated by applications\nin network mining, particularly concerning the analysis of probabilistic\ninformation-propagation processes in social networks. In the corresponding\nmodel the arcs carry probabilities and there is a probabilistic diffusion\nprocess activating nodes by neighboring activated nodes with probabilities as\nspecified by the arcs. The point is to explain a given network activation state\nas well as possible by using a minimum number of \"effector nodes\"; these are\nselected before the activation process starts.\n  We correct, complement, and extend previous work from the data mining\ncommunity by a more thorough computational complexity analysis of EFFECTORS,\nidentifying both tractable and intractable cases. To this end, we also exploit\na parameterization measuring the \"degree of randomness\" (the number of \"really\"\nprobabilistic arcs) which might prove useful for analyzing other probabilistic\nnetwork diffusion problems as well.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 12:29:36 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 12:14:49 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Bulteau", "Laurent", ""], ["Fafianie", "Stefan", ""], ["Froese", "Vincent", ""], ["Niedermeier", "Rolf", ""], ["Talmon", "Nimrod", ""]]}]