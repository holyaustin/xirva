[{"id": "0910.0013", "submitter": "Radu Grigore", "authors": "Mikolas Janota, Joao Marques-Silva, Radu Grigore", "title": "Algorithms for finding dispensable variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note reviews briefly three algorithms for finding the set of\ndispensable variables of a boolean formula. The presentation is light on proofs\nand heavy on intuitions.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2009 20:10:03 GMT"}], "update_date": "2009-10-02", "authors_parsed": [["Janota", "Mikolas", ""], ["Marques-Silva", "Joao", ""], ["Grigore", "Radu", ""]]}, {"id": "0910.0110", "submitter": "Patrick Briest", "authors": "Patrick Briest and Sanjeev Khanna", "title": "Improved Hardness of Approximation for Stackelberg Shortest-Path Pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Stackelberg shortest-path pricing problem, which is defined\nas follows. Given a graph G with fixed-cost and pricable edges and two distinct\nvertices s and t, we may assign prices to the pricable edges. Based on the\npredefined fixed costs and our prices, a customer purchases a cheapest s-t-path\nin G and we receive payment equal to the sum of prices of pricable edges\nbelonging to the path. Our goal is to find prices maximizing the payment\nreceived from the customer. While Stackelberg shortest-path pricing was known\nto be APX-hard before, we provide the first explicit approximation threshold\nand prove hardness of approximation within 2-o(1).\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2009 09:15:06 GMT"}], "update_date": "2009-10-02", "authors_parsed": [["Briest", "Patrick", ""], ["Khanna", "Sanjeev", ""]]}, {"id": "0910.0112", "submitter": "Rasmus Pagh", "authors": "Andrea Campagna and Rasmus Pagh", "title": "Finding Associations and Computing Similarity via Biased Pair Sampling", "comments": "This is an extended version of a paper that appeared at the IEEE\n  International Conference on Data Mining, 2009. The conference version is (c)\n  2009 IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This version is ***superseded*** by a full version that can be found at\nhttp://www.itu.dk/people/pagh/papers/mining-jour.pdf, which contains stronger\ntheoretical results and fixes a mistake in the reporting of experiments.\n  Abstract: Sampling-based methods have previously been proposed for the\nproblem of finding interesting associations in data, even for low-support\nitems. While these methods do not guarantee precise results, they can be vastly\nmore efficient than approaches that rely on exact counting. However, for many\nsimilarity measures no such methods have been known. In this paper we show how\na wide variety of measures can be supported by a simple biased sampling method.\nThe method also extends to find high-confidence association rules. We\ndemonstrate theoretically that our method is superior to exact methods when the\nthreshold for \"interesting similarity/confidence\" is above the average pairwise\nsimilarity/confidence, and the average support is not too low. Our method is\nparticularly good when transactions contain many items. We confirm in\nexperiments on standard association mining benchmarks that this gives a\nsignificant speedup on real data sets (sometimes much larger than the\ntheoretical guarantees). Reductions in computation time of over an order of\nmagnitude, and significant savings in space, are observed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2009 09:02:54 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2010 09:32:14 GMT"}], "update_date": "2010-02-17", "authors_parsed": [["Campagna", "Andrea", ""], ["Pagh", "Rasmus", ""]]}, {"id": "0910.0366", "submitter": "Daniel Cederman", "authors": "Daniel Cederman and Philippas Tsigas", "title": "Supporting Lock-Free Composition of Concurrent Data Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": "2009-10", "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lock-free data objects offer several advantages over their blocking\ncounterparts, such as being immune to deadlocks and convoying and, more\nimportantly, being highly concurrent. But they share a common disadvantage in\nthat the operations they provide are difficult to compose into larger atomic\noperations while still guaranteeing lock-freedom. We present a lock-free\nmethodology for composing highly concurrent linearizable objects together by\nunifying their linearization points. This makes it possible to relatively\neasily introduce atomic lock-free move operations to a wide range of concurrent\nobjects. Experimental evaluation has shown that the operations originally\nsupported by the data objects keep their performance behavior under our\nmethodology.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2009 10:08:12 GMT"}], "update_date": "2009-10-05", "authors_parsed": [["Cederman", "Daniel", ""], ["Tsigas", "Philippas", ""]]}, {"id": "0910.0443", "submitter": "Danupon Nanongkai", "authors": "Parinya Chalermsook, Bundit Laekhanukit, Danupon Nanongkai", "title": "Stackelberg Pricing is Hard to Approximate within $2-\\epsilon$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stackelberg Pricing Games is a two-level combinatorial pricing problem\nstudied in the Economics, Operation Research, and Computer Science communities.\nIn this paper, we consider the decade-old shortest path version of this problem\nwhich is the first and most studied problem in this family.\n  The game is played on a graph (representing a network) consisting of {\\em\nfixed cost} edges and {\\em pricable} or {\\em variable cost} edges. The fixed\ncost edges already have some fixed price (representing the competitor's\nprices). Our task is to choose prices for the variable cost edges. After that,\na client will buy the cheapest path from a node $s$ to a node $t$, using any\ncombination of fixed cost and variable cost edges. The goal is to maximize the\nrevenue on variable cost edges.\n  In this paper, we show that the problem is hard to approximate within\n$2-\\epsilon$, improving the previous \\APX-hardness result by Joret [to appear\nin {\\em Networks}]. Our technique combines the existing ideas with a new\ninsight into the price structure and its relation to the hardness of the\ninstances.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2009 19:54:54 GMT"}], "update_date": "2009-10-05", "authors_parsed": [["Chalermsook", "Parinya", ""], ["Laekhanukit", "Bundit", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "0910.0460", "submitter": "Andreas Bj\\\"orklund", "authors": "Andreas Bj\\\"orklund", "title": "Exact Covers via Determinants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a k-uniform hypergraph on n vertices, partitioned in k equal parts such\nthat every hyperedge includes one vertex from each part, the k-dimensional\nmatching problem asks whether there is a disjoint collection of the hyperedges\nwhich covers all vertices. We show it can be solved by a randomized polynomial\nspace algorithm in time O*(2^(n(k-2)/k)). The O*() notation hides factors\npolynomial in n and k.\n  When we drop the partition constraint and permit arbitrary hyperedges of\ncardinality k, we obtain the exact cover by k-sets problem. We show it can be\nsolved by a randomized polynomial space algorithm in time O*(c_k^n), where\nc_3=1.496, c_4=1.642, c_5=1.721, and provide a general bound for larger k.\n  Both results substantially improve on the previous best algorithms for these\nproblems, especially for small k, and follow from the new observation that\nLovasz' perfect matching detection via determinants (1979) admits an embedding\nin the recently proposed inclusion-exclusion counting scheme for set covers,\ndespite its inability to count the perfect matchings.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2009 19:36:39 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2010 16:53:00 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2010 11:00:50 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""]]}, {"id": "0910.0504", "submitter": "Jos\\'e A. Soto", "authors": "Jos\\'e Soto", "title": "Improved Analysis of a Max Cut Algorithm Based on Spectral Partitioning", "comments": "9 pages, 2 figures. Final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trevisan [SICOMP 2012] presented an algorithm for Max-Cut based on spectral\npartitioning techniques. This is the first algorithm for Max-Cut with an\napproximation guarantee strictly larger than 1/2 that is not based on\nsemidefinite programming. Trevisan showed that its approximation ratio is of at\nleast 0.531. In this paper we improve this bound up to 0.614247. We also define\nand extend this result for the more general Maximum Colored Cut problem.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2009 19:58:59 GMT"}, {"version": "v2", "created": "Tue, 2 Dec 2014 14:20:14 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Soto", "Jos\u00e9", ""]]}, {"id": "0910.0553", "submitter": "Michel Goemans", "authors": "Michel X. Goemans", "title": "Combining Approximation Algorithms for the Prize-Collecting TSP", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a 1.91457-approximation algorithm for the prize-collecting\ntravelling salesman problem. This is obtained by combining a randomized variant\nof a rounding algorithm of Bienstock et al. and a primal-dual algorithm of\nGoemans and Williamson.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2009 15:48:44 GMT"}], "update_date": "2009-10-06", "authors_parsed": [["Goemans", "Michel X.", ""]]}, {"id": "0910.0582", "submitter": "Michael Lampis", "authors": "Michael Lampis", "title": "Algorithmic Meta-Theorems for Graphs of Bounded Vertex Cover", "comments": "In this version, the algorithmic results have been extended to a new\n  graph width, called neighborhood diversity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Possibly the most famous algorithmic meta-theorem is Courcelle's theorem,\nwhich states that all MSO-expressible graph properties are decidable in linear\ntime for graphs of bounded treewidth. Unfortunately, the running time's\ndependence on the MSO formula describing the problem is in general a tower of\nexponentials of unbounded height, and there exist lower bounds proving that\nthis cannot be improved even if we restrict ourselves to deciding FO logic on\ntrees.\n  In this paper we attempt to circumvent these lower bounds by focusing on a\nsubclass of bounded treewidth graphs, the graphs of bounded vertex cover. By\nusing a technique different from the standard decomposition and dynamic\nprogramming technique of treewidth we prove that in this case the running time\nimplied by Courcelle's theorem can be improved dramatically, from\nnon-elementary to doubly and singly exponential for MSO and FO logic\nrespectively. Our technique relies on a new graph width measure we introduce,\nfor which we show some additional results that may indicate that it is of\nindependent interest. We also prove lower bound results which show that our\nupper bounds cannot be improved significantly, under widely believed complexity\nassumptions. Our work answers an open problem posed by Michael Fellows.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2009 02:58:25 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2009 21:22:08 GMT"}], "update_date": "2009-11-05", "authors_parsed": [["Lampis", "Michael", ""]]}, {"id": "0910.0767", "submitter": "Serena Bradde", "authors": "M. Bailly-Bechet, S. Bradde, A. Braunstein, A. Flaxman, L. Foini, R.\n  Zecchina", "title": "Clustering with shallow trees", "comments": "11 pages, 7 figures", "journal-ref": "J. Stat. Mech. (2009) P12010", "doi": "10.1088/1742-5468/2009/12/P12010", "report-no": null, "categories": "cond-mat.dis-nn cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for hierarchical clustering based on the optimisation\nof a cost function over trees of limited depth, and we derive a\nmessage--passing method that allows to solve it efficiently. The method and\nalgorithm can be interpreted as a natural interpolation between two well-known\napproaches, namely single linkage and the recently presented Affinity\nPropagation. We analyze with this general scheme three biological/medical\nstructured datasets (human population based on genetic information, proteins\nbased on sequences and verbal autopsies) and show that the interpolation\ntechnique provides new insight.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2009 14:13:25 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2009 15:44:29 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Bailly-Bechet", "M.", ""], ["Bradde", "S.", ""], ["Braunstein", "A.", ""], ["Flaxman", "A.", ""], ["Foini", "L.", ""], ["Zecchina", "R.", ""]]}, {"id": "0910.0777", "submitter": "Brent Heeringa", "authors": "Glencora Borradaile, Brent Heeringa, Gordon Wilfong", "title": "The Knapsack Problem with Neighbour Constraints", "comments": "Full version of IWOCA 2011 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a constrained version of the knapsack problem in which dependencies\nbetween items are given by the adjacencies of a graph. In the 1-neighbour\nknapsack problem, an item can be selected only if at least one of its\nneighbours is also selected. In the all-neighbours knapsack problem, an item\ncan be selected only if all its neighbours are also selected. We give\napproximation algorithms and hardness results when the nodes have both uniform\nand arbitrary weight and profit functions, and when the dependency graph is\ndirected and undirected.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2009 14:34:09 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2010 16:59:28 GMT"}, {"version": "v3", "created": "Mon, 11 Jul 2011 16:25:45 GMT"}, {"version": "v4", "created": "Tue, 27 Sep 2011 01:03:05 GMT"}], "update_date": "2011-09-28", "authors_parsed": [["Borradaile", "Glencora", ""], ["Heeringa", "Brent", ""], ["Wilfong", "Gordon", ""]]}, {"id": "0910.0832", "submitter": "Maria Potop-Butucaru", "authors": "Anissa Lamani (LIP6), Maria Potop-Butucaru (LIP6, INRIA Rocquencourt),\n  S\\'ebastien Tixeuil (LIP6)", "title": "Optimal deterministic ring exploration with oblivious asynchronous\n  robots", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-13284-1_15", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of exploring an anonymous unoriented ring of size $n$\nby $k$ identical, oblivious, asynchronous mobile robots, that are unable to\ncommunicate, yet have the ability to sense their environment and take decisions\nbased on their local view. Previous works in this weak scenario prove that $k$\nmust not divide $n$ for a deterministic solution to exist. Also, it is known\nthat the minimum number of robots (either deterministic or probabilistic) to\nexplore a ring of size $n$ is 4. An upper bound of 17 robots holds in the\ndeterministic case while 4 probabilistic robots are sufficient. In this paper,\nwe close the complexity gap in the deterministic setting, by proving that no\ndeterministic exploration is feasible with less than five robots whenever the\nsize of the ring is even, and that five robots are sufficient for any $n$ that\nis coprime with five. Our protocol completes exploration in O(n) robot moves,\nwhich is also optimal.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2009 19:08:49 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Lamani", "Anissa", "", "LIP6"], ["Potop-Butucaru", "Maria", "", "LIP6, INRIA Rocquencourt"], ["Tixeuil", "S\u00e9bastien", "", "LIP6"]]}, {"id": "0910.1059", "submitter": "Nicolas Catusse", "authors": "Nicolas Catusse, Victor Chepoi, Yann Vax\\`es", "title": "Embedding into the rectilinear plane in optimal O*(n^2)", "comments": "12 pages, 13 figures", "journal-ref": "Theoretical Computer Science Volume 412, Issue 22 (2011), Pages\n  2425-2433", "doi": "10.1016/j.tcs.2011.01.038", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an optimal O*(n^2) time algorithm for deciding if a metric space\n(X,d) on n points can be isometrically embedded into the plane endowed with the\nl_1-metric. It improves the O*(n^2 log^2 n) time algorithm of J. Edmonds\n(2008). Together with some ingredients introduced by J. Edmonds, our algorithm\nuses the concept of tight span and the injectivity of the l_1-plane. A\ndifferent O*(n^2) time algorithm was recently proposed by D. Eppstein (2009).\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2009 17:25:59 GMT"}], "update_date": "2011-07-08", "authors_parsed": [["Catusse", "Nicolas", ""], ["Chepoi", "Victor", ""], ["Vax\u00e8s", "Yann", ""]]}, {"id": "0910.1191", "submitter": "Elchanan Mossel", "authors": "Mark Braverman and Elchanan Mossel", "title": "Sorting from Noisy Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies problems of inferring order given noisy information. In\nthese problems there is an unknown order (permutation) $\\pi$ on $n$ elements\ndenoted by $1,...,n$. We assume that information is generated in a way\ncorrelated with $\\pi$. The goal is to find a maximum likelihood $\\pi^*$ given\nthe information observed. We will consider two different types of observations:\nnoisy comparisons and noisy orders. The data in Noisy orders are permutations\ngiven from an exponential distribution correlated with \\pi (this is also called\nthe Mallow's model). The data in Noisy Comparisons is a signal given for each\npair of elements which is correlated with their true ordering.\n  In this paper we present polynomial time algorithms for solving both problems\nwith high probability. As part of our proof we show that for both models the\nmaximum likelihood solution $\\pi^{\\ast}$ is close to the original permutation\n$\\pi$.\n  Our results are of interest in applications to ranking, such as ranking in\nsports, or ranking of search items based on comparisons by experts.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2009 09:14:54 GMT"}], "update_date": "2009-10-08", "authors_parsed": [["Braverman", "Mark", ""], ["Mossel", "Elchanan", ""]]}, {"id": "0910.1387", "submitter": "Tobias Jacobs", "authors": "Tobias Jacobs", "title": "Simpler Proofs by Symbolic Perturbation", "comments": "This work introduces a fairly novel model for combinatorial problems\n  and algorithms. The author believes that the result is successfully\n  applicable to the analyes of a very broad class of algorithms. He would be\n  happy to receive feedback from the readers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In analyses of algorithms, a substantial amount of effort has often to be\nspent on the discussion of special cases. For example, when the analysis\nconsiders the cases X<Y and X>Y separately, one might have to be especially\ncareful about what happens when X=Y. On the other hand, experience tells us\nthat when a yet unregarded special case of this kind is discovered, one nearly\nalways finds a way to handle it. This is typically done by modifying the\nanalysis and/or the algorithm very slightly.\n  In this article we substantiate this observation theoretically. We\nconcentrate on deterministic algorithms for weighted combinatorial optimization\nproblems. A problem instance of this kind is defined by its structure and a\nvector of weights. The concept of a null case is introduced as set of problem\ninstances whose weight vectors constitute a nowhere open set (or null set) in\nthe space of all possible weight configurations. An algorithm is called robust\nif any null case can be disregarded in the analysis of both its solution\nquality and resource requirements.\n  We show that achieving robustness is only a matter of breaking ties the right\nway. More specifically, we show that the concept of symbolic perturbation known\nfrom the area of geometric algorithms guarantees that no surprises will happen\nin null cases. We argue that for a huge class of combinatorial optimization\nalgorithms it is easy to verify that they implicitly use symbolic perturbation\nfor breaking ties and thus can be analyzed under the assumption that some\narbitrary null case never occurs. Finally, we prove that there exists a\nsymbolic perturbation tie breaking policy for any algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2009 02:23:15 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2009 09:00:16 GMT"}], "update_date": "2009-11-04", "authors_parsed": [["Jacobs", "Tobias", ""]]}, {"id": "0910.1392", "submitter": "Tsung-Hsi Tsai", "authors": "Wei-Mei Chen, Hsien-Kuei Hwang, Tsung-Hsi Tsai", "title": "Simple, efficient maxima-finding algorithms for multidimensional samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New algorithms are devised for finding the maxima of multidimensional point\nsamples, one of the very first problems studied in computational geometry. The\nalgorithms are very simple and easily coded and modified for practical needs.\nThe expected complexity of some measures related to the performance of the\nalgorithms is analyzed. We also compare the efficiency of the algorithms with a\nfew major ones used in practice, and apply our algorithms to find the maximal\nlayers and the longest common subsequences of multiple sequences.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2009 02:53:58 GMT"}], "update_date": "2009-10-09", "authors_parsed": [["Chen", "Wei-Mei", ""], ["Hwang", "Hsien-Kuei", ""], ["Tsai", "Tsung-Hsi", ""]]}, {"id": "0910.1403", "submitter": "Ping Li", "authors": "Ping Li", "title": "On the Sample Complexity of Compressed Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed Counting (CC), based on maximally skewed stable random\nprojections, was recently proposed for estimating the p-th frequency moments of\ndata streams. The case p->1 is extremely useful for estimating Shannon entropy\nof data streams. In this study, we provide a very simple algorithm based on the\nsample minimum estimator and prove a much improved sample complexity bound,\ncompared to prior results.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2009 04:40:46 GMT"}], "update_date": "2009-10-09", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "0910.1409", "submitter": "James Lee", "authors": "James R. Lee, Anastasios Sidiropoulos", "title": "Pathwidth, trees, and random embeddings", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that, for every $k=1,2,...,$ every shortest-path metric on a graph\nof pathwidth $k$ embeds into a distribution over random trees with distortion\nat most $c$ for some $c=c(k)$. A well-known conjecture of Gupta, Newman,\nRabinovich, and Sinclair states that for every minor-closed family of graphs\n$F$, there is a constant $c(F)$ such that the multi-commodity max-flow/min-cut\ngap for every flow instance on a graph from $F$ is at most $c(F)$. The\npreceding embedding theorem is used to prove this conjecture whenever the\nfamily $F$ does not contain all trees.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2009 05:06:48 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2011 23:37:27 GMT"}, {"version": "v3", "created": "Sat, 6 Oct 2012 01:08:02 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Lee", "James R.", ""], ["Sidiropoulos", "Anastasios", ""]]}, {"id": "0910.1475", "submitter": "Rdv Ijcsis", "authors": "Annapurna P Patil, Narmada Sambaturu, Krittaya Chunhaviriyakul", "title": "Convergence Time Evaluation of Algorithms in MANETs", "comments": "6 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 5, No. 1, pp. 144-149, September 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the advent of wireless communication, the need for mobile ad hoc\nnetworks has been growing exponentially. This has opened up a Pandoras Box of\nalgorithms for dealing with mobile ad hoc networks, or MANETs, as they are\ngenerally referred to. Most attempts made at evaluating these algorithms so far\nhave focused on parameters such as throughput, packet delivery ratio, overhead\netc. An analysis of the convergence times of these algorithms is still an open\nissue. The work carried out fills this gap by evaluating the algorithms on the\nbasis of convergence time. Algorithms for MANETs can be classified into three\ncategories: reactive, proactive, and hybrid protocols. In this project, we\ncompare the convergence times of representative algorithms in each category,\nnamely Ad hoc On Demand Distance Vector (AODV) reactive, Destination Sequence\nDistance Vector protocol (DSDV) proactive, and Temporally Ordered Routing\nAlgorithm (TORA) hybrid. The algorithm performances are compared by simulating\nthem in ns2. Tcl is used to conduct the simulations, while perl is used to\nextract data from the simulation output and calculate convergence time. The\ndesign of the experiments carried on is documented using Unified modeling\nLanguage. Also, a user interface is created using perl, which enables the user\nto either run a desired simulation and measure convergence time, or measure the\nconvergence time of a simulation that has been run earlier.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2009 11:33:37 GMT"}], "update_date": "2009-10-09", "authors_parsed": [["Patil", "Annapurna P", ""], ["Sambaturu", "Narmada", ""], ["Chunhaviriyakul", "Krittaya", ""]]}, {"id": "0910.1495", "submitter": "Ping Li", "authors": "Ping Li", "title": "Estimating Entropy of Data Streams Using Compressed Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shannon entropy is a widely used summary statistic, for example, network\ntraffic measurement, anomaly detection, neural computations, spike trains, etc.\nThis study focuses on estimating Shannon entropy of data streams. It is known\nthat Shannon entropy can be approximated by Reenyi entropy or Tsallis entropy,\nwhich are both functions of the p-th frequency moments and approach Shannon\nentropy as p->1.\n  Compressed Counting (CC) is a new method for approximating the p-th frequency\nmoments of data streams. Our contributions include:\n  1) We prove that Renyi entropy is (much) better than Tsallis entropy for\napproximating Shannon entropy.\n  2) We propose the optimal quantile estimator for CC, which considerably\nimproves the previous estimators.\n  3) Our experiments demonstrate that CC is indeed highly effective\napproximating the moments and entropies. We also demonstrate the crucial\nimportance of utilizing the variance-bias trade-off.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2009 12:54:36 GMT"}], "update_date": "2009-10-09", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "0910.1643", "submitter": "Iris Reinbacher", "authors": "Hee-Kap Ahn, Sang Won Bae, Erik D. Demaine, Martin L. Demaine,\n  Sang-Sub Kim, Matias Korman, Iris Reinbacher, Wanbin Son", "title": "Covering Points by Disjoint Boxes with Outliers", "comments": "updated version: - changed problem from 'cover exactly n-k points' to\n  'cover at least n-k points' to avoid having non-feasible solutions. Results\n  are unchanged. - added Proof to Lemma 11, clarified some sections - corrected\n  typos and small errors - updated affiliations of two authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a set of n points in the plane, we consider the axis--aligned (p,k)-Box\nCovering problem: Find p axis-aligned, pairwise-disjoint boxes that together\ncontain n-k points. In this paper, we consider the boxes to be either squares\nor rectangles, and we want to minimize the area of the largest box. For general\np we show that the problem is NP-hard for both squares and rectangles. For a\nsmall, fixed number p, we give algorithms that find the solution in the\nfollowing running times:\n  For squares we have O(n+k log k) time for p=1, and O(n log n+k^p log^p k time\nfor p = 2,3. For rectangles we get O(n + k^3) for p = 1 and O(n log n+k^{2+p}\nlog^{p-1} k) time for p = 2,3.\n  In all cases, our algorithms use O(n) space.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2009 02:31:23 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2010 03:38:43 GMT"}], "update_date": "2010-07-28", "authors_parsed": [["Ahn", "Hee-Kap", ""], ["Bae", "Sang Won", ""], ["Demaine", "Erik D.", ""], ["Demaine", "Martin L.", ""], ["Kim", "Sang-Sub", ""], ["Korman", "Matias", ""], ["Reinbacher", "Iris", ""], ["Son", "Wanbin", ""]]}, {"id": "0910.1926", "submitter": "David Harvey", "authors": "David Harvey", "title": "Faster algorithms for the square root and reciprocal of power series", "comments": "6 pages, 1 figure, requires algorithm2e package", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new algorithms for the computation of square roots and reciprocals of\npower series in C[[x]]. If M(n) denotes the cost of multiplying polynomials of\ndegree n, the square root to order n costs (1.333... + o(1)) M(n) and the\nreciprocal costs (1.444... + o(1)) M(n). These improve on the previous best\nresults, respectively (1.8333... + o(1)) M(n) and (1.5 + o(1)) M(n).\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2009 15:41:04 GMT"}], "update_date": "2009-10-13", "authors_parsed": [["Harvey", "David", ""]]}, {"id": "0910.1969", "submitter": "Ajinkya Kale", "authors": "Ajinkya Kale, Shaunak Vaidya, Ashish Joglekar", "title": "A Generalized Recursive Algorithm for Binary Multiplication based on\n  Vedic Mathematics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalized algorithm for multiplication is proposed through recursive\napplication of the Nikhilam Sutra from Vedic Mathematics, operating in radix -\n2 number system environment suitable for digital platforms. Statistical\nanalysis has been carried out based on the number of recursions profile as a\nfunction of the smaller multiplicand. The proposed algorithm is efficient for\nsmaller multiplicands as well, unlike most of the asymptotically fast\nalgorithms. Further, a basic block schematic of Hardware Implementation of our\nalgorithm is suggested to exploit parallelism and speed up the implementation\nof the algorithm in a multiprocessor environment.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2009 04:57:22 GMT"}], "update_date": "2009-10-13", "authors_parsed": [["Kale", "Ajinkya", ""], ["Vaidya", "Shaunak", ""], ["Joglekar", "Ashish", ""]]}, {"id": "0910.2004", "submitter": "Christian Schulz", "authors": "Manuel Holtgrewe, Peter Sanders, Christian Schulz", "title": "Engineering a Scalable High Quality Graph Partitioner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an approach to parallel graph partitioning that scales to\nhundreds of processors and produces a high solution quality. For example, for\nmany instances from Walshaw's benchmark collection we improve the best known\npartitioning. We use the well known framework of multi-level graph\npartitioning. All components are implemented by scalable parallel algorithms.\nQuality improvements compared to previous systems are due to better\nprioritization of edges to be contracted, better approximation algorithms for\nidentifying matchings, better local search heuristics, and perhaps most\nnotably, a parallelization of the FM local search algorithm that works more\nlocally than previous approaches.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2009 14:36:07 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2010 14:35:26 GMT"}], "update_date": "2010-04-08", "authors_parsed": [["Holtgrewe", "Manuel", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "0910.2024", "submitter": "Assaf Naor", "authors": "Jeff Cheeger, Bruce Kleiner, Assaf Naor", "title": "A $(\\log n)^{\\Omega(1)}$ integrality gap for the Sparsest Cut SDP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Goemans-Linial semidefinite relaxation of the Sparsest Cut\nproblem with general demands has integrality gap $(\\log n)^{\\Omega(1)}$. This\nis achieved by exhibiting $n$-point metric spaces of negative type whose $L_1$\ndistortion is $(\\log n)^{\\Omega(1)}$. Our result is based on quantitative\nbounds on the rate of degeneration of Lipschitz maps from the Heisenberg group\nto $L_1$ when restricted to cosets of the center.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2009 17:39:05 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2009 20:09:45 GMT"}], "update_date": "2009-11-18", "authors_parsed": [["Cheeger", "Jeff", ""], ["Kleiner", "Bruce", ""], ["Naor", "Assaf", ""]]}, {"id": "0910.2026", "submitter": "Assaf Naor", "authors": "Jeff Cheeger, Bruce Kleiner, Assaf Naor", "title": "Compression bounds for Lipschitz maps from the Heisenberg group to $L_1$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.DS math.DG math.FA math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a quantitative bi-Lipschitz nonembedding theorem for the Heisenberg\ngroup with its Carnot-Carath\\'eodory metric and apply it to give a lower bound\non the integrality gap of the Goemans-Linial semidefinite relaxation of the\nSparsest Cut problem.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2009 17:49:44 GMT"}], "update_date": "2009-10-13", "authors_parsed": [["Cheeger", "Jeff", ""], ["Kleiner", "Bruce", ""], ["Naor", "Assaf", ""]]}, {"id": "0910.2028", "submitter": "Rdv Ijcsis", "authors": "Shahram Jamali, Morteza Analoui", "title": "Congestion Control in the Internet by Employing a Ratio dependent Plant\n  Herbivore Carnivorous Model", "comments": "7 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 5, No. 1, pp. 175-181, September 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for Internet based services has exploded over the last decade.\nMany organizations use the Internet and particularly the World Wide Web as\ntheir primary medium for communication and business. This phenomenal growth has\ndramatically increased the performance requirements for the Internet. To have a\nhigh performance Internet, a good congestion control system is essential for\nit. The current work proposes that the congestion control in the Internet can\nbe inspired from the population control tactics of the nature. Toward this\nidea, each flow (W) in the network is viewed as a species whose population size\nis congestion window size of the flow. By this assumption, congestion control\nproblem is redefined as population control of flow species. This paper defines\na three trophic food chain analogy in congestion control area, and gives a\nratio dependent model to control population size of W species within this plant\nherbivore carnivorous food chain. Simulation results show that this model\nachieves fair bandwidth allocation, high utilization and small queue size. It\ndoes not maintain any per flow state in routers and have few computational\nloads per packet, which makes it scalable.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2009 18:49:00 GMT"}], "update_date": "2009-10-13", "authors_parsed": [["Jamali", "Shahram", ""], ["Analoui", "Morteza", ""]]}, {"id": "0910.2317", "submitter": "Andreas Spillner", "authors": "A. Dress, K. T. Huber, J. Koolen, V. Moulton, A. Spillner", "title": "An algorithm for computing cutpoints in finite metric spaces", "comments": "17 pages, 1 eps-figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of the tight span, a cell complex that can be associated to every\nmetric $D$, offers a unifying view on existing approaches for analyzing\ndistance data, in particular for decomposing a metric $D$ into a sum of simpler\nmetrics as well as for representing it by certain specific edge-weighted\ngraphs, often referred to as realizations of $D$. Many of these approaches\ninvolve the explicit or implicit computation of the so-called cutpoints of (the\ntight span of) $D$, such as the algorithm for computing the \"building blocks\"\nof optimal realizations of $D$ recently presented by A. Hertz and S. Varone.\nThe main result of this paper is an algorithm for computing the set of these\ncutpoints for a metric $D$ on a finite set with $n$ elements in $O(n^3)$ time.\nAs a direct consequence, this improves the run time of the aforementioned\n$O(n^6)$-algorithm by Hertz and Varone by ``three orders of magnitude''.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2009 07:09:03 GMT"}], "update_date": "2009-10-14", "authors_parsed": [["Dress", "A.", ""], ["Huber", "K. T.", ""], ["Koolen", "J.", ""], ["Moulton", "V.", ""], ["Spillner", "A.", ""]]}, {"id": "0910.2370", "submitter": "Srikanth Srinivasan", "authors": "V. Arvind, Srikanth Srinivasan", "title": "On the hardness of the noncommutative determinant", "comments": "11 pages, v2: 18 pages, some typos removed, new section added on\n  Clifford algebras, and some reorganization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the computational complexity of computing the\nnoncommutative determinant. We first consider the arithmetic circuit complexity\nof computing the noncommutative determinant polynomial. Then, more generally,\nwe also examine the complexity of computing the determinant (as a function)\nover noncommutative domains. Our hardness results are summarized below:\n  1. We show that if the noncommutative determinant polynomial has small\nnoncommutative arithmetic circuits then so does the noncommutative permanent.\nConsequently, the commutative permanent polynomial has small commutative\narithmetic circuits. 2. For any field F we show that computing the n X n\npermanent over F is polynomial-time reducible to computing the 2n X 2n\n(noncommutative) determinant whose entries are O(n^2) X O(n^2) matrices over\nthe field F. 3. We also derive as a consequence that computing the n X n\npermanent over nonnegative rationals is polynomial-time reducible to computing\nthe noncommutative determinant over Clifford algebras of n^{O(1)} dimension.\n  Our techniques are elementary and use primarily the notion of the Hadamard\nProduct of noncommutative polynomials.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2009 11:58:22 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2009 09:25:42 GMT"}], "update_date": "2009-10-26", "authors_parsed": [["Arvind", "V.", ""], ["Srinivasan", "Srikanth", ""]]}, {"id": "0910.2582", "submitter": "Johannes Singler", "authors": "Mirko Rahn, Peter Sanders, Johannes Singler", "title": "Scalable Distributed-Memory External Sorting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We engineer algorithms for sorting huge data sets on massively parallel\nmachines. The algorithms are based on the multiway merging paradigm. We first\noutline an algorithm whose I/O requirement is close to a lower bound. Thus, in\ncontrast to naive implementations of multiway merging and all other approaches\nknown to us, the algorithm works with just two passes over the data even for\nthe largest conceivable inputs. A second algorithm reduces communication\noverhead and uses more conventional specifications of the result at the cost of\nslightly increased I/O requirements. An implementation wins the well known\nsorting benchmark in several categories and by a large margin over its\ncompetitors.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2009 12:18:28 GMT"}], "update_date": "2009-10-15", "authors_parsed": [["Rahn", "Mirko", ""], ["Sanders", "Peter", ""], ["Singler", "Johannes", ""]]}, {"id": "0910.2973", "submitter": "Mohab Safey El Din", "authors": "Mohab Safey El Din (LIP6, INRIA Rocquencourt), Lihong Zhi (KLMM)", "title": "Computing rational points in convex semi-algebraic sets and SOS\n  decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let ${\\cal P}=\\{h_1, ..., h_s\\}\\subset \\Z[Y_1, ..., Y_k]$, $D\\geq \\deg(h_i)$\nfor $1\\leq i \\leq s$, $\\sigma$ bounding the bit length of the coefficients of\nthe $h_i$'s, and $\\Phi$ be a quantifier-free ${\\cal P}$-formula defining a\nconvex semi-algebraic set. We design an algorithm returning a rational point in\n${\\cal S}$ if and only if ${\\cal S}\\cap \\Q\\neq\\emptyset$. It requires\n$\\sigma^{\\bigO(1)}D^{\\bigO(k^3)}$ bit operations. If a rational point is\noutputted its coordinates have bit length dominated by $\\sigma D^{\\bigO(k^3)}$.\nUsing this result, we obtain a procedure deciding if a polynomial $f\\in \\Z[X_1,\n>..., X_n]$ is a sum of squares of polynomials in $\\Q[X_1, ..., X_n]$. Denote\nby $d$ the degree of $f$, $\\tau$ the maximum bit length of the coefficients in\n$f$, $D={{n+d}\\choose{n}}$ and $k\\leq D(D+1)-{{n+2d}\\choose{n}}$. This\nprocedure requires $\\tau^{\\bigO(1)}D^{\\bigO(k^3)}$ bit operations and the\ncoefficients of the outputted polynomials have bit length dominated by $\\tau\nD^{\\bigO(k^3)}$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2009 19:18:05 GMT"}], "update_date": "2009-10-16", "authors_parsed": [["Din", "Mohab Safey El", "", "LIP6, INRIA Rocquencourt"], ["Zhi", "Lihong", "", "KLMM"]]}, {"id": "0910.3123", "submitter": "Johannes Fischer", "authors": "Johannes Fischer", "title": "Wee LCP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that longest common prefix (LCP) information can be stored in much\nless space than previously known. More precisely, we show that in the presence\nof the text and the suffix array, o(n) additional bits are sufficient to answer\nLCP-queries asymptotically in the same time that is needed to retrieve an entry\nfrom the suffix array. This yields the smallest compressed suffix tree with\nsub-logarithmic navigation time.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2009 13:50:12 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2010 09:19:42 GMT"}], "update_date": "2010-02-19", "authors_parsed": [["Fischer", "Johannes", ""]]}, {"id": "0910.3148", "submitter": "Riccardo Dondi", "authors": "Stefano Beretta, Paola Bonizzoni, Gianluca Della Vedova, Riccardo\n  Dondi and Yuri Pirola", "title": "Parameterized Complexity of the k-anonymity Problem", "comments": "22 pages, 2 figures", "journal-ref": "J. of Combinatorial Optimization 26.1 (2013) 19-43", "doi": "10.1007/s10878-011-9428-9", "report-no": null, "categories": "cs.DS cs.DB cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of publishing personal data without giving up privacy is becoming\nincreasingly important. An interesting formalization that has been recently\nproposed is the $k$-anonymity. This approach requires that the rows of a table\nare partitioned in clusters of size at least $k$ and that all the rows in a\ncluster become the same tuple, after the suppression of some entries. The\nnatural optimization problem, where the goal is to minimize the number of\nsuppressed entries, is known to be APX-hard even when the records values are\nover a binary alphabet and $k=3$, and when the records have length at most 8\nand $k=4$ . In this paper we study how the complexity of the problem is\ninfluenced by different parameters. In this paper we follow this direction of\nresearch, first showing that the problem is W[1]-hard when parameterized by the\nsize of the solution (and the value $k$). Then we exhibit a fixed parameter\nalgorithm, when the problem is parameterized by the size of the alphabet and\nthe number of columns. Finally, we investigate the computational (and\napproximation) complexity of the $k$-anonymity problem, when restricting the\ninstance to records having length bounded by 3 and $k=3$. We show that such a\nrestriction is APX-hard.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2009 16:01:33 GMT"}, {"version": "v2", "created": "Mon, 17 May 2010 08:06:06 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Beretta", "Stefano", ""], ["Bonizzoni", "Paola", ""], ["Della Vedova", "Gianluca", ""], ["Dondi", "Riccardo", ""], ["Pirola", "Yuri", ""]]}, {"id": "0910.3243", "submitter": "Krzysztof Onak", "authors": "Krzysztof Onak", "title": "Testing Distribution Identity Efficiently", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of testing distribution identity. Given a sequence of\nindependent samples from an unknown distribution on a domain of size n, the\ngoal is to check if the unknown distribution approximately equals a known\ndistribution on the same domain. While Batu, Fortnow, Fischer, Kumar,\nRubinfeld, and White (FOCS 2001) proved that the sample complexity of the\nproblem is O~(sqrt(n) * poly(1/epsilon)), the running time of their tester is\nmuch higher: O(n) + O~(sqrt(n) * poly(1/epsilon)). We modify their tester to\nachieve a running time of O~(sqrt(n) * poly(1/epsilon)).\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2009 22:26:02 GMT"}], "update_date": "2009-10-20", "authors_parsed": [["Onak", "Krzysztof", ""]]}, {"id": "0910.3292", "submitter": "Masud Hasan", "authors": "Jesun Sahariar Firoz, Masud Hasan, Ashik Zinnat Khan, and M. Sohel\n  Rahman", "title": "The 1.375 Approximation Algorithm for Sorting by Transpositions Can Run\n  in $O(n\\log n)$ Time", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting a Permutation by Transpositions (SPbT) is an important problem in\nBioinformtics. In this paper, we improve the running time of the best known\napproximation algorithm for SPbT. We use the permutation tree data structure of\nFeng and Zhu and improve the running time of the 1.375 Approximation Algorithm\nfor SPbT of Elias and Hartman to $O(n\\log n)$. The previous running time of EH\nalgorithm was $O(n^2)$.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2009 10:16:48 GMT"}], "update_date": "2009-10-20", "authors_parsed": [["Firoz", "Jesun Sahariar", ""], ["Hasan", "Masud", ""], ["Khan", "Ashik Zinnat", ""], ["Rahman", "M. Sohel", ""]]}, {"id": "0910.3301", "submitter": "Julian McAuley", "authors": "Julian J. McAuley, Tiberio S. Caetano", "title": "Faster Algorithms for Max-Product Message-Passing", "comments": "34 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum A Posteriori inference in graphical models is often solved via\nmessage-passing algorithms, such as the junction-tree algorithm, or loopy\nbelief-propagation. The exact solution to this problem is well known to be\nexponential in the size of the model's maximal cliques after it is\ntriangulated, while approximate inference is typically exponential in the size\nof the model's factors. In this paper, we take advantage of the fact that many\nmodels have maximal cliques that are larger than their constituent factors, and\nalso of the fact that many factors consist entirely of latent variables (i.e.,\nthey do not depend on an observation). This is a common case in a wide variety\nof applications, including grids, trees, and ring-structured models. In such\ncases, we are able to decrease the exponent of complexity for message-passing\nby 0.5 for both exact and approximate inference.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2009 13:42:35 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2009 04:02:16 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2009 03:41:24 GMT"}, {"version": "v4", "created": "Thu, 8 Apr 2010 05:24:55 GMT"}], "update_date": "2010-04-09", "authors_parsed": [["McAuley", "Julian J.", ""], ["Caetano", "Tiberio S.", ""]]}, {"id": "0910.3349", "submitter": "Ping Li", "authors": "Ping Li, Arnd Christian Konig", "title": "b-Bit Minwise Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes the theoretical framework of b-bit minwise hashing.\nThe original minwise hashing method has become a standard technique for\nestimating set similarity (e.g., resemblance) with applications in information\nretrieval, data management, social networks and computational advertising.\n  By only storing the lowest $b$ bits of each (minwise) hashed value (e.g., b=1\nor 2), one can gain substantial advantages in terms of computational efficiency\nand storage space. We prove the basic theoretical results and provide an\nunbiased estimator of the resemblance for any b. We demonstrate that, even in\nthe least favorable scenario, using b=1 may reduce the storage space at least\nby a factor of 21.3 (or 10.7) compared to using b=64 (or b=32), if one is\ninterested in resemblance > 0.5.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2009 03:39:56 GMT"}], "update_date": "2009-10-20", "authors_parsed": [["Li", "Ping", ""], ["Konig", "Arnd Christian", ""]]}, {"id": "0910.3503", "submitter": "Benjamin Burton", "authors": "Benjamin A. Burton", "title": "Searching a bitstream in linear time for the longest substring of any\n  given density", "comments": "22 pages, 19 figures; v2: minor edits and enhancements", "journal-ref": "Algorithmica 61 (2011), no. 3, 555-579", "doi": "10.1007/s00453-010-9424-y", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an arbitrary bitstream, we consider the problem of finding the longest\nsubstring whose ratio of ones to zeroes equals a given value. The central\nresult of this paper is an algorithm that solves this problem in linear time.\nThe method involves (i) reformulating the problem as a constrained walk through\na sparse matrix, and then (ii) developing a data structure for this sparse\nmatrix that allows us to perform each step of the walk in amortised constant\ntime. We also give a linear time algorithm to find the longest substring whose\nratio of ones to zeroes is bounded below by a given value. Both problems have\npractical relevance to cryptography and bioinformatics.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2009 10:06:34 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2010 20:44:36 GMT"}], "update_date": "2011-08-22", "authors_parsed": [["Burton", "Benjamin A.", ""]]}, {"id": "0910.4664", "submitter": "Adam Yedidia", "authors": "Adam B. Yedidia", "title": "Counting Independent Sets and Kernels of Regular Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chandrasekaran, Chertkov, Gamarnik, Shah, and Shin recently proved that the\naverage number of independent sets of random regular graphs of size n and\ndegree 3 approaches w^n for large n, where w is approximately 1.54563,\nconsistent with the Bethe approximation. They also made the surprising\nconjecture that the fluctuations of the logarithm of the number of independent\nsets were only O(1) as n grew large, which would mean that the Bethe\napproximation is amazingly accurate for all 3-regular graphs. Here, I provide\nnumerical evidence supporting this conjecture obtained from exact counts of\nindependent sets using binary decision diagrams. I also provide numerical\nevidence that supports the novel conjectures that the number of kernels of\n3-regular graphs of size n is given by y^n, where y is approximately 1.299, and\nthat the fluctuations in the logarithm of the number of kernels is also only\nO(1).\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2009 17:33:53 GMT"}], "update_date": "2009-10-27", "authors_parsed": [["Yedidia", "Adam B.", ""]]}, {"id": "0910.5147", "submitter": "Nikolaos Fountoulakis", "authors": "Nikolaos Fountoulakis and Konstantinos Panagiotou", "title": "Sharp Load Thresholds for Cuckoo Hashing", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paradigm of many choices has influenced significantly the design of\nefficient data structures and, most notably, hash tables. Cuckoo hashing is a\ntechnique that extends this concept. There,we are given a table with $n$\nlocations, and we assume that each location can hold one item. Each item to be\ninserted chooses randomly k>1 locations and has to be placed in any one of\nthem. How much load can cuckoo hashing handle before collisions prevent the\nsuccessful assignment of the available items to the chosen locations? Practical\nevaluations of this method have shown that one can allocate a number of\nelements that is a large proportion of the size of the table, being very close\nto 1 even for small values of k such as 4 or 5.\n  In this paper we show that there is a critical value for this proportion:\nwith high probability, when the amount of available items is below this value,\nthen these can be allocated successfully, but when it exceeds this value, the\nallocation becomes impossible. We give explicitly for each k>1 this critical\nvalue. This answers an open question posed by Mitzenmacher (ESA '09) and\nunderpins theoretically the experimental results. Our proofs are based on the\ntranslation of the question into a hypergraph setting, and the study of the\nrelated typical properties of random k-uniform hypergraphs.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2009 15:37:28 GMT"}], "update_date": "2009-10-28", "authors_parsed": [["Fountoulakis", "Nikolaos", ""], ["Panagiotou", "Konstantinos", ""]]}, {"id": "0910.5380", "submitter": "Rajesh Chitnis", "authors": "L. Sunil Chandran, Rajesh Chitnis, Ramanjit Kumar", "title": "On the SIG dimension of trees under $L_{\\infty}$ metric", "comments": "24 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the $SIG$ dimension of trees under $L_{\\infty}$ metric and answer an\nopen problem posed by Michael and Quint (Discrete Applied Mathematics: 127,\npages 447-460, 2003). Let $T$ be a tree with atleast two vertices. For each\n$v\\in V(T)$, let leaf-degree$(v)$ denote the number of neighbours of $v$ that\nare leaves. We define the maximum leaf-degree as $\\alpha(T) = \\max_{x \\in\nV(T)}$ leaf-degree$(x)$. Let $S = \\{v\\in V(T) |$ leaf-degree$(v) = \\alpha\\}$.\nIf $|S| = 1$, we define $\\beta(T) = \\alpha(T) - 1$. Otherwise define $\\beta(T)\n= \\alpha(T)$. We show that for a tree $T$, $SIG_\\infty(T) = \\lceil \\log_2(\\beta\n+ 2)\\rceil$ where $\\beta = \\beta (T)$, provided $\\beta$ is not of the form $2^k\n- 1$, for some positive integer $k \\geq 1$. If $\\beta = 2^k - 1$, then\n$SIG_\\infty (T) \\in \\{k, k+1\\}$. We show that both values are possible.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2009 13:55:42 GMT"}, {"version": "v2", "created": "Sun, 9 Oct 2011 16:51:33 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Chandran", "L. Sunil", ""], ["Chitnis", "Rajesh", ""], ["Kumar", "Ramanjit", ""]]}, {"id": "0910.5535", "submitter": "P\\'all Melsted", "authors": "Alan Frieze, P\\'all Melsted", "title": "Maximum Matchings in Random Bipartite Graphs and the Space Utilization\n  of Cuckoo Hashtables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the the following question in Random Graphs. We are given two\ndisjoint sets $L,R$ with $|L|=n=\\alpha m$ and $|R|=m$. We construct a random\ngraph $G$ by allowing each $x\\in L$ to choose $d$ random neighbours in $R$. The\nquestion discussed is as to the size $\\mu(G)$ of the largest matching in $G$.\nWhen considered in the context of Cuckoo Hashing, one key question is as to\nwhen is $\\mu(G)=n$ whp? We answer this question exactly when $d$ is at least\nfour. We also establish a precise threshold for when Phase 1 of the Karp-Sipser\nGreedy matching algorithm suffices to compute a maximum matching whp.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2009 02:29:42 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2009 16:16:33 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2009 19:49:44 GMT"}], "update_date": "2009-11-17", "authors_parsed": [["Frieze", "Alan", ""], ["Melsted", "P\u00e1ll", ""]]}, {"id": "0910.5599", "submitter": "Dror Rawitz", "authors": "Boaz Patt-Shamir, Dror Rawitz", "title": "Vector Bin Packing with Multiple-Choice", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-13731-0_24", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variant of bin packing called multiple-choice vector bin\npacking. In this problem we are given a set of items, where each item can be\nselected in one of several $D$-dimensional incarnations. We are also given $T$\nbin types, each with its own cost and $D$-dimensional size. Our goal is to pack\nthe items in a set of bins of minimum overall cost. The problem is motivated by\nscheduling in networks with guaranteed quality of service (QoS), but due to its\ngeneral formulation it has many other applications as well. We present an\napproximation algorithm that is guaranteed to produce a solution whose cost is\nabout $\\ln D$ times the optimum. For the running time to be polynomial we\nrequire $D=O(1)$ and $T=O(\\log n)$. This extends previous results for vector\nbin packing, in which each item has a single incarnation and there is only one\nbin type. To obtain our result we also present a PTAS for the multiple-choice\nversion of multidimensional knapsack, where we are given only one bin and the\ngoal is to pack a maximum weight set of (incarnations of) items in that bin.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2009 10:00:37 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Patt-Shamir", "Boaz", ""], ["Rawitz", "Dror", ""]]}, {"id": "0910.5744", "submitter": "Olivier Spanjaard", "authors": "Lucie Galand and Olivier Spanjaard", "title": "Exact algorithms for OWA-optimization in multiobjective spanning tree\n  problems", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the multiobjective version of the optimal spanning tree\nproblem. More precisely, we are interested in determining the optimal spanning\ntree according to an Ordered Weighted Average (OWA) of its objective values. We\nfirst show that the problem is weakly NP-hard. In the case where the weights of\nthe OWA are strictly decreasing, we then propose a mixed integer programming\nformulation, and provide dedicated optimality conditions yielding an important\nreduction of the size of the program. Next, we present two bounds that can be\nused to prune subspaces of solutions either in a shaving phase or in a branch\nand bound procedure. The validity of these bounds does not depend on specific\nproperties of the weights (apart from non-negativity). All these exact\nresolution algorithms are compared on the basis of numerical experiments,\naccording to their respective validity scopes.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2009 23:17:24 GMT"}], "update_date": "2009-11-02", "authors_parsed": [["Galand", "Lucie", ""], ["Spanjaard", "Olivier", ""]]}, {"id": "0910.5765", "submitter": "Frank Vallentin", "authors": "Jop Briet, Fernando Mario de Oliveira Filho, Frank Vallentin", "title": "The positive semidefinite Grothendieck problem with rank constraint", "comments": "(v3) to appear in Proceedings of the 37th International Colloquium on\n  Automata, Languages and Programming, 12 pages", "journal-ref": "ICALP, Part I, LNCS 6198, 2010, pages 31-42", "doi": "10.1007/978-3-642-14165-2_4", "report-no": null, "categories": "math.OC cs.DS math.CO math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a positive integer n and a positive semidefinite matrix A = (A_{ij}) of\nsize m x m, the positive semidefinite Grothendieck problem with\nrank-n-constraint (SDP_n) is\n  maximize \\sum_{i=1}^m \\sum_{j=1}^m A_{ij} x_i \\cdot x_j, where x_1, ..., x_m\n\\in S^{n-1}.\n  In this paper we design a polynomial time approximation algorithm for SDP_n\nachieving an approximation ratio of\n  \\gamma(n) = \\frac{2}{n}(\\frac{\\Gamma((n+1)/2)}{\\Gamma(n/2)})^2 = 1 -\n\\Theta(1/n).\n  We show that under the assumption of the unique games conjecture the achieved\napproximation ratio is optimal: There is no polynomial time algorithm which\napproximates SDP_n with a ratio greater than \\gamma(n). We improve the\napproximation ratio of the best known polynomial time algorithm for SDP_1 from\n2/\\pi to 2/(\\pi\\gamma(m)) = 2/\\pi + \\Theta(1/m), and we show a tighter\napproximation ratio for SDP_n when A is the Laplacian matrix of a graph with\nnonnegative edge weights.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2009 04:49:10 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2009 02:09:03 GMT"}, {"version": "v3", "created": "Mon, 3 May 2010 11:58:34 GMT"}], "update_date": "2010-09-17", "authors_parsed": [["Briet", "Jop", ""], ["Filho", "Fernando Mario de Oliveira", ""], ["Vallentin", "Frank", ""]]}, {"id": "0910.5816", "submitter": "Giuseppe Notarstefano", "authors": "Giuseppe Notarstefano and Francesco Bullo", "title": "Distributed Abstract Optimization via Constraints Consensus: Theory and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed abstract programs are a novel class of distributed optimization\nproblems where (i) the number of variables is much smaller than the number of\nconstraints and (ii) each constraint is associated to a network node. Abstract\noptimization programs are a generalization of linear programs that captures\nnumerous geometric optimization problems. We propose novel constraints\nconsensus algorithms for distributed abstract programs: as each node\niteratively identifies locally active constraints and exchanges them with its\nneighbors, the network computes the active constraints determining the global\noptimum. The proposed algorithms are appropriate for networks with weak\ntime-dependent connectivity requirements and tight memory constraints. We show\nhow suitable target localization and formation control problems can be tackled\nvia constraints consensus.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2009 09:06:19 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2009 13:39:28 GMT"}], "update_date": "2009-11-02", "authors_parsed": [["Notarstefano", "Giuseppe", ""], ["Bullo", "Francesco", ""]]}]