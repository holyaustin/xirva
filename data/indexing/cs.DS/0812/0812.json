[{"id": "0812.0146", "submitter": "Vladimir Pestov", "authors": "Vladimir Pestov", "title": "Lower Bounds on Performance of Metric Tree Indexing Schemes for Exact\n  Similarity Search in High Dimensions", "comments": "21 pages, revised submission to Algorithmica, an improved and\n  extended journal version of the conference paper arXiv:0812.0146v3 [cs.DS],\n  with lower bounds strengthened, and the proof of the main Theorem 4\n  simplified", "journal-ref": "Algorithmica 66 (2013), 310-328", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within a mathematically rigorous model, we analyse the curse of\ndimensionality for deterministic exact similarity search in the context of\npopular indexing schemes: metric trees. The datasets $X$ are sampled randomly\nfrom a domain $\\Omega$, equipped with a distance, $\\rho$, and an underlying\nprobability distribution, $\\mu$. While performing an asymptotic analysis, we\nsend the intrinsic dimension $d$ of $\\Omega$ to infinity, and assume that the\nsize of a dataset, $n$, grows superpolynomially yet subexponentially in $d$.\nExact similarity search refers to finding the nearest neighbour in the dataset\n$X$ to a query point $\\omega\\in\\Omega$, where the query points are subject to\nthe same probability distribution $\\mu$ as datapoints. Let $\\mathscr F$ denote\na class of all 1-Lipschitz functions on $\\Omega$ that can be used as decision\nfunctions in constructing a hierarchical metric tree indexing scheme. Suppose\nthe VC dimension of the class of all sets $\\{\\omega\\colon f(\\omega)\\geq a\\}$,\n$a\\in\\R$ is $o(n^{1/4}/\\log^2n)$. (In view of a 1995 result of Goldberg and\nJerrum, even a stronger complexity assumption $d^{O(1)}$ is reasonable.) We\ndeduce the $\\Omega(n^{1/4})$ lower bound on the expected average case\nperformance of hierarchical metric-tree based indexing schemes for exact\nsimilarity search in $(\\Omega,X)$. In paricular, this bound is superpolynomial\nin $d$.\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2008 15:17:22 GMT"}, {"version": "v2", "created": "Fri, 20 Aug 2010 03:42:50 GMT"}, {"version": "v3", "created": "Tue, 10 May 2011 16:17:39 GMT"}, {"version": "v4", "created": "Fri, 24 Feb 2012 18:38:50 GMT"}], "update_date": "2013-03-27", "authors_parsed": [["Pestov", "Vladimir", ""]]}, {"id": "0812.0209", "submitter": "Qin Zhang", "authors": "Ke Yi, Qin Zhang", "title": "Optimal Tracking of Distributed Heavy Hitters and Quantiles", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the the problem of tracking heavy hitters and quantiles in the\ndistributed streaming model. The heavy hitters and quantiles are two important\nstatistics for characterizing a data distribution. Let $A$ be a multiset of\nelements, drawn from the universe $U=\\{1,...,u\\}$. For a given $0 \\le \\phi \\le\n1$, the $\\phi$-heavy hitters are those elements of $A$ whose frequency in $A$\nis at least $\\phi |A|$; the $\\phi$-quantile of $A$ is an element $x$ of $U$\nsuch that at most $\\phi|A|$ elements of $A$ are smaller than $A$ and at most\n$(1-\\phi)|A|$ elements of $A$ are greater than $x$. Suppose the elements of $A$\nare received at $k$ remote {\\em sites} over time, and each of the sites has a\ntwo-way communication channel to a designated {\\em coordinator}, whose goal is\nto track the set of $\\phi$-heavy hitters and the $\\phi$-quantile of $A$\napproximately at all times with minimum communication. We give tracking\nalgorithms with worst-case communication cost $O(k/\\eps \\cdot \\log n)$ for both\nproblems, where $n$ is the total number of items in $A$, and $\\eps$ is the\napproximation error. This substantially improves upon the previous known\nalgorithms. We also give matching lower bounds on the communication costs for\nboth problems, showing that our algorithms are optimal. We also consider a more\ngeneral version of the problem where we simultaneously track the\n$\\phi$-quantiles for all $0 \\le \\phi \\le 1$.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2008 03:51:12 GMT"}], "update_date": "2008-12-02", "authors_parsed": [["Yi", "Ke", ""], ["Zhang", "Qin", ""]]}, {"id": "0812.0320", "submitter": "Gwena\\\"el Joret", "authors": "Gwena\\\"el Joret", "title": "Stackelberg Network Pricing is Hard to Approximate", "comments": null, "journal-ref": "Networks, vol. 57, no. 2, pp. 117--120, 2011", "doi": "10.1002/net.20391", "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Stackelberg Network Pricing problem, one has to assign tariffs to a\ncertain subset of the arcs of a given transportation network. The aim is to\nmaximize the amount paid by the user of the network, knowing that the user will\ntake a shortest st-path once the tariffs are fixed. Roch, Savard, and Marcotte\n(Networks, Vol. 46(1), 57-67, 2005) proved that this problem is NP-hard, and\ngave an O(log m)-approximation algorithm, where m denote the number of arcs to\nbe priced. In this note, we show that the problem is also APX-hard.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2008 16:15:58 GMT"}], "update_date": "2011-03-07", "authors_parsed": [["Joret", "Gwena\u00ebl", ""]]}, {"id": "0812.0382", "submitter": "Andrea Vattani", "authors": "Andrea Vattani", "title": "k-means requires exponentially many iterations even in the plane", "comments": "Submitted to SoCG 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-means algorithm is a well-known method for partitioning n points that\nlie in the d-dimensional space into k clusters. Its main features are\nsimplicity and speed in practice. Theoretically, however, the best known upper\nbound on its running time (i.e. O(n^{kd})) can be exponential in the number of\npoints. Recently, Arthur and Vassilvitskii [3] showed a super-polynomial\nworst-case analysis, improving the best known lower bound from \\Omega(n) to\n2^{\\Omega(\\sqrt{n})} with a construction in d=\\Omega(\\sqrt{n}) dimensions. In\n[3] they also conjectured the existence of superpolynomial lower bounds for any\nd >= 2.\n  Our contribution is twofold: we prove this conjecture and we improve the\nlower bound, by presenting a simple construction in the plane that leads to the\nexponential lower bound 2^{\\Omega(n)}.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2008 22:55:39 GMT"}], "update_date": "2008-12-03", "authors_parsed": [["Vattani", "Andrea", ""]]}, {"id": "0812.0387", "submitter": "Kevin Buchin", "authors": "Kevin Buchin", "title": "Delaunay Triangulations in Linear Time? (Part I)", "comments": "8 pages, no figures; added footnote about newer algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new and simple randomized algorithm for constructing the\nDelaunay triangulation using nearest neighbor graphs for point location. Under\nsuitable assumptions, it runs in linear expected time for points in the plane\nwith polynomially bounded spread, i.e., if the ratio between the largest and\nsmallest pointwise distance is polynomially bounded. This also holds for point\nsets with bounded spread in higher dimensions as long as the expected\ncomplexity of the Delaunay triangulation of a sample of the points is linear in\nthe sample size.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2008 23:09:13 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2009 08:58:17 GMT"}, {"version": "v3", "created": "Sun, 13 Dec 2009 21:54:48 GMT"}], "update_date": "2009-12-13", "authors_parsed": [["Buchin", "Kevin", ""]]}, {"id": "0812.0389", "submitter": "Stefanie Jegelka", "authors": "Stefanie Jegelka, Suvrit Sra, Arindam Banerjee", "title": "Approximation Algorithms for Bregman Co-clustering and Tensor Clustering", "comments": "18 pages; improved metric case", "journal-ref": "short version in ALT 2009", "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years powerful generalizations to the Euclidean k-means\nproblem have been made, such as Bregman clustering [7], co-clustering (i.e.,\nsimultaneous clustering of rows and columns of an input matrix) [9,18], and\ntensor clustering [8,34]. Like k-means, these more general problems also suffer\nfrom the NP-hardness of the associated optimization. Researchers have developed\napproximation algorithms of varying degrees of sophistication for k-means,\nk-medians, and more recently also for Bregman clustering [2]. However, there\nseem to be no approximation algorithms for Bregman co- and tensor clustering.\nIn this paper we derive the first (to our knowledge) guaranteed methods for\nthese increasingly important clustering settings. Going beyond Bregman\ndivergences, we also prove an approximation factor for tensor clustering with\narbitrary separable metrics. Through extensive experiments we evaluate the\ncharacteristics of our method, and show that it also has practical impact.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2008 23:17:35 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2009 12:40:18 GMT"}, {"version": "v3", "created": "Fri, 15 May 2009 22:23:02 GMT"}, {"version": "v4", "created": "Mon, 9 Nov 2009 15:50:32 GMT"}], "update_date": "2009-11-09", "authors_parsed": [["Jegelka", "Stefanie", ""], ["Sra", "Suvrit", ""], ["Banerjee", "Arindam", ""]]}, {"id": "0812.0598", "submitter": "Laura Poplawski", "authors": "Laura J. Poplawski, Rajmohan Rajaraman, Ravi Sundaram and Shang-Hua\n  Teng", "title": "Preference Games and Personalized Equilibria, with Applications to\n  Fractional BGP", "comments": "25 pages, 3 figures, v2: minor editorial changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of computing equilibria in two classes of network\ngames based on flows - fractional BGP (Border Gateway Protocol) games and\nfractional BBC (Bounded Budget Connection) games. BGP is the glue that holds\nthe Internet together and hence its stability, i.e. the equilibria of\nfractional BGP games (Haxell, Wilfong), is a matter of practical importance.\nBBC games (Laoutaris et al) follow in the tradition of the large body of work\non network formation games and capture a variety of applications ranging from\nsocial networks and overlay networks to peer-to-peer networks.\n  The central result of this paper is that there are no fully polynomial-time\napproximation schemes (unless PPAD is in FP) for computing equilibria in both\nfractional BGP games and fractional BBC games. We obtain this result by proving\nthe hardness for a new and surprisingly simple game, the fractional preference\ngame, which is reducible to both fractional BGP and BBC games.\n  We define a new flow-based notion of equilibrium for matrix games --\npersonalized equilibria -- generalizing both fractional BBC and fractional BGP\ngames. We prove not just the existence, but the existence of rational\npersonalized equilibria for all matrix games, which implies the existence of\nrational equilibria for fractional BGP and BBC games. In particular, this\nprovides an alternative proof and strengthening of the main result in [Haxell,\nWilfong]. For k-player matrix games, where k = 2, we provide a combinatorial\ncharacterization leading to a polynomial-time algorithm for computing all\npersonalized equilibria. For k >= 5, we prove that personalized equilibria are\nPPAD-hard to approximate in fully polynomial time. We believe that the concept\nof personalized equilibria has potential for real-world significance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2008 21:12:03 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2008 16:35:26 GMT"}], "update_date": "2008-12-05", "authors_parsed": [["Poplawski", "Laura J.", ""], ["Rajaraman", "Rajmohan", ""], ["Sundaram", "Ravi", ""], ["Teng", "Shang-Hua", ""]]}, {"id": "0812.0893", "submitter": "Darren Strash", "authors": "David Eppstein, Michael T. Goodrich and Darren Strash", "title": "Linear-Time Algorithms for Geometric Graphs with Sublinearly Many Edge\n  Crossings", "comments": "Expanded version of a paper appearing at the 20th ACM-SIAM Symposium\n  on Discrete Algorithms (SODA09)", "journal-ref": "SIAM J. Computing 39(8): 3814-3829, 2010", "doi": "10.1137/090759112", "report-no": null, "categories": "cs.CG cs.DM cs.DS cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide linear-time algorithms for geometric graphs with sublinearly many\ncrossings. That is, we provide algorithms running in O(n) time on connected\ngeometric graphs having n vertices and k crossings, where k is smaller than n\nby an iterated logarithmic factor. Specific problems we study include Voronoi\ndiagrams and single-source shortest paths. Our algorithms all run in linear\ntime in the standard comparison-based computational model; hence, we make no\nassumptions about the distribution or bit complexities of edge weights, nor do\nwe utilize unusual bit-level operations on memory words. Instead, our\nalgorithms are based on a planarization method that \"zeroes in\" on edge\ncrossings, together with methods for extending planar separator decompositions\nto geometric graphs with sublinearly many crossings. Incidentally, our\nplanarization algorithm also solves an open computational geometry problem of\nChazelle for triangulating a self-intersecting polygonal chain having n\nsegments and k crossings in linear time, for the case when k is sublinear in n\nby an iterated logarithmic factor.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2008 10:29:00 GMT"}, {"version": "v2", "created": "Thu, 14 May 2009 02:07:34 GMT"}], "update_date": "2010-12-16", "authors_parsed": [["Eppstein", "David", ""], ["Goodrich", "Michael T.", ""], ["Strash", "Darren", ""]]}, {"id": "0812.1012", "submitter": "Kamesh Munagala", "authors": "Sudipto Guha and Kamesh Munagala", "title": "Adaptive Uncertainty Resolution in Bayesian Combinatorial Optimization\n  Problems", "comments": "Journal version of the paper \"Model-driven Optimization using\n  Adaptive Probes\" that appeared in the ACM-SIAM Symposium on Discrete\n  Algorithms (SODA), 2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several applications such as databases, planning, and sensor networks,\nparameters such as selectivity, load, or sensed values are known only with some\nassociated uncertainty. The performance of such a system (as captured by some\nobjective function over the parameters) is significantly improved if some of\nthese parameters can be probed or observed. In a resource constrained\nsituation, deciding which parameters to observe in order to optimize system\nperformance itself becomes an interesting and important optimization problem.\nThis general problem is the focus of this paper.\n  One of the most important considerations in this framework is whether\nadaptivity is required for the observations. Adaptive observations introduce\nblocking or sequential operations in the system whereas non-adaptive\nobservations can be performed in parallel. One of the important questions in\nthis regard is to characterize the benefit of adaptivity for probes and\nobservation.\n  We present general techniques for designing constant factor approximations to\nthe optimal observation schemes for several widely used scheduling and metric\nobjective functions. We show a unifying technique that relates this\noptimization problem to the outlier version of the corresponding deterministic\noptimization. By making this connection, our technique shows constant factor\nupper bounds for the benefit of adaptivity of the observation schemes. We show\nthat while probing yields significant improvement in the objective function,\nbeing adaptive about the probing is not beneficial beyond constant factors.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2008 19:48:16 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2009 14:17:22 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2010 15:08:30 GMT"}], "update_date": "2010-01-28", "authors_parsed": [["Guha", "Sudipto", ""], ["Munagala", "Kamesh", ""]]}, {"id": "0812.1123", "submitter": "Jinshan Zhang", "authors": "Jinshan Zhang", "title": "Improved Approximation for the Number of Hamiltonian Cycles in Dense\n  Digraphs", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an improved algorithm for counting the number of Hamiltonian\ncycles in a directed graph. The basic idea of the method is sequential\nacceptance/rejection, which is successfully used in approximating the number of\nperfect matchings in dense bipartite graphs. As a consequence, a new bound on\nthe number of Hamiltonian cycles in a directed graph is proved, by using the\nratio of the number of 1-factors. Based on this bound, we prove that our\nalgorithm runs in expected time of $O(n^{8.5})$ for dense problems. This\nimproves the Markov chain method, the most powerful existing method, a factor\nof at least $n^{4.5}(\\log n)^{4}$ in running time. This class of dense problems\nis shown to be nontrivial in counting, in the sense that it is $#$P-Complete.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2008 12:28:57 GMT"}, {"version": "v2", "created": "Sun, 7 Dec 2008 17:15:05 GMT"}, {"version": "v3", "created": "Mon, 12 Jan 2009 13:05:39 GMT"}, {"version": "v4", "created": "Sat, 21 Nov 2009 08:13:36 GMT"}], "update_date": "2009-11-23", "authors_parsed": [["Zhang", "Jinshan", ""]]}, {"id": "0812.1126", "submitter": "Dimitris Kalles", "authors": "Dimitris Kalles, Alexis Kaporis", "title": "Emerge-Sort: Converging to Ordered Sequences by Simple Local Operators", "comments": "Contains 16 pages, 17 figures, 1 table. Text updated as of March 10,\n  2009. Submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine sorting on the assumption that we do not know in\nadvance which way to sort a sequence of numbers and we set at work simple local\ncomparison and swap operators whose repeating application ends up in sorted\nsequences. These are the basic elements of Emerge-Sort, our approach to\nself-organizing sorting, which we then validate experimentally across a range\nof samples. Observing an O(n2) run-time behaviour, we note that the n/logn\ndelay coefficient that differentiates Emerge-Sort from the classical comparison\nbased algorithms is an instantiation of the price of anarchy we pay for not\nimposing a sorting order and for letting that order emerge through the local\ninteractions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2008 12:57:00 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2009 22:42:43 GMT"}], "update_date": "2009-03-11", "authors_parsed": [["Kalles", "Dimitris", ""], ["Kaporis", "Alexis", ""]]}, {"id": "0812.1321", "submitter": "Aleksandrs Slivkins", "authors": "Matthew Andrews and Aleksandrs Slivkins", "title": "Oscillations with TCP-like Flow Control in Networks of Queues", "comments": "Preliminary version has appeared in IEEE INFOCOM 2006. The current\n  version is dated November 2005, with a minor revision in December 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a set of flows passing through a set of servers. The injection\nrate into each flow is governed by a flow control that increases the injection\nrate when all the servers on the flow's path are empty and decreases the\ninjection rate when some server is congested. We show that if each server's\ncongestion is governed by the arriving traffic at the server then the system\ncan *oscillate*. This is in contrast to previous work on flow control where\ncongestion was modeled as a function of the flow injection rates and the system\nwas shown to converge to a steady state that maximizes an overall network\nutility.\n", "versions": [{"version": "v1", "created": "Sat, 6 Dec 2008 23:57:44 GMT"}], "update_date": "2008-12-09", "authors_parsed": [["Andrews", "Matthew", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "0812.1385", "submitter": "Javaid Aslam", "authors": "Javaid Aslam", "title": "An Extension of the Permutation Group Enumeration Technique (Collapse of\n  the Polynomial Hierarchy: $\\mathbf{NP = P}$)", "comments": "Revisions: Some re-organization-- created a new Section 5 and minor\n  revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distinguishing result of this paper is a $\\mathbf{P}$-time enumerable\npartition of all the potential perfect matchings in a bipartite graph. This\npartition is a set of equivalence classes induced by the missing edges in the\npotential perfect matchings.\n  We capture the behavior of these missing edges in a polynomially bounded\nrepresentation of the exponentially many perfect matchings by a graph theoretic\nstructure, called MinSet Sequence, where MinSet is a P-time enumerable\nstructure derived from a graph theoretic counterpart of a generating set of the\nsymmetric group. This leads to a polynomially bounded generating set of all the\nclasses, enabling the enumeration of perfect matchings in polynomial time. The\nsequential time complexity of this $\\mathbf{\\#P}$-complete problem is shown to\nbe $O(n^{45}\\log n)$.\n  And thus we prove a result even more surprising than $\\mathbf{NP = P}$, that\nis, $\\mathbf{\\#P}=\\mathbf{FP}$, where $\\mathbf{FP}$ is the class of functions,\n$f: \\{0, 1\\}^* \\rightarrow \\mathbb{N} $, computable in polynomial time on a\ndeterministic model of computation.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2008 19:47:28 GMT"}, {"version": "v10", "created": "Mon, 30 Mar 2009 19:41:25 GMT"}, {"version": "v11", "created": "Tue, 7 Apr 2009 18:35:11 GMT"}, {"version": "v12", "created": "Mon, 19 Jan 2015 20:21:59 GMT"}, {"version": "v13", "created": "Thu, 22 Jan 2015 20:45:26 GMT"}, {"version": "v14", "created": "Thu, 5 Feb 2015 20:56:47 GMT"}, {"version": "v15", "created": "Sun, 22 Feb 2015 20:36:42 GMT"}, {"version": "v16", "created": "Wed, 15 Jul 2015 18:44:43 GMT"}, {"version": "v17", "created": "Thu, 30 Jul 2015 19:48:56 GMT"}, {"version": "v18", "created": "Thu, 8 Oct 2015 19:04:26 GMT"}, {"version": "v19", "created": "Mon, 12 Oct 2015 19:57:44 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2008 19:30:19 GMT"}, {"version": "v20", "created": "Thu, 15 Oct 2015 19:48:04 GMT"}, {"version": "v21", "created": "Sun, 18 Oct 2015 19:20:04 GMT"}, {"version": "v22", "created": "Sat, 2 Jan 2016 01:31:54 GMT"}, {"version": "v23", "created": "Thu, 3 Mar 2016 20:53:32 GMT"}, {"version": "v24", "created": "Sat, 26 Aug 2017 06:08:03 GMT"}, {"version": "v25", "created": "Sun, 17 Sep 2017 22:52:14 GMT"}, {"version": "v26", "created": "Mon, 30 Oct 2017 08:01:46 GMT"}, {"version": "v3", "created": "Thu, 25 Dec 2008 20:43:33 GMT"}, {"version": "v4", "created": "Mon, 12 Jan 2009 17:03:53 GMT"}, {"version": "v5", "created": "Tue, 20 Jan 2009 21:05:32 GMT"}, {"version": "v6", "created": "Mon, 26 Jan 2009 20:56:54 GMT"}, {"version": "v7", "created": "Wed, 28 Jan 2009 20:50:44 GMT"}, {"version": "v8", "created": "Fri, 6 Feb 2009 20:43:25 GMT"}, {"version": "v9", "created": "Mon, 9 Mar 2009 18:58:19 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Aslam", "Javaid", ""]]}, {"id": "0812.1587", "submitter": "Radu Mihaescu", "authors": "Radu Mihaescu, Cameron Hill, Satish Rao", "title": "Fast phylogeny reconstruction through learning of ancestral sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given natural limitations on the length DNA sequences, designing phylogenetic\nreconstruction methods which are reliable under limited information is a\ncrucial endeavor. There have been two approaches to this problem:\nreconstructing partial but reliable information about the tree (\\cite{Mo07,\nDMR08,DHJ06,GMS08}), and reaching \"deeper\" in the tree through reconstruction\nof ancestral sequences. In the latter category, \\cite{DMR06} settled an\nimportant conjecture of M.Steel, showing that, under the CFN model of\nevolution, all trees on $n$ leaves with edge lengths bounded by the Ising model\nphase transition can be recovered with high probability from genomes of length\n$O(\\log n)$ with a polynomial time algorithm. Their methods had a running time\nof $O(n^{10})$.\n  Here we enhance our methods from \\cite{DHJ06} with the learning of ancestral\nsequences and provide an algorithm for reconstructing a sub-forest of the tree\nwhich is reliable given available data, without requiring a-priori known bounds\non the edge lengths of the tree. Our methods are based on an intuitive minimum\nspanning tree approach and run in $O(n^3)$ time. For the case of full\nreconstruction of trees with edges under the phase transition, we maintain the\nsame sequence length requirements as \\cite{DMR06}, despite the considerably\nfaster running time.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2008 22:51:02 GMT"}], "update_date": "2008-12-10", "authors_parsed": [["Mihaescu", "Radu", ""], ["Hill", "Cameron", ""], ["Rao", "Satish", ""]]}, {"id": "0812.1595", "submitter": "Aparna Das", "authors": "Aparna Das, Claire Mathieu", "title": "A quasi-polynomial time approximation scheme for Euclidean capacitated\n  vehicle routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the capacitated vehicle routing problem, introduced by Dantzig and Ramser\nin 1959, we are given the locations of n customers and a depot, along with a\nvehicle of capacity k, and wish to find a minimum length collection of tours,\neach starting from the depot and visiting at most k customers, whose union\ncovers all the customers. We give a quasi-polynomial time approximation scheme\nfor the setting where the customers and the depot are on the plane, and\ndistances are given by the Euclidean metric.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2008 23:58:17 GMT"}], "update_date": "2008-12-10", "authors_parsed": [["Das", "Aparna", ""], ["Mathieu", "Claire", ""]]}, {"id": "0812.1628", "submitter": "Masoud Farivar", "authors": "Masoud Farivar, Behzad Mehrdad, Farid Ashtiani", "title": "Two Dimensional Connectivity for Vehicular Ad-Hoc Networks", "comments": "9 Pages, 10 figures,Submitted to INFOCOM 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on two-dimensional connectivity in sparse vehicular\nad hoc networks (VANETs). In this respect, we find thresholds for the arrival\nrates of vehicles at entrances of a block of streets such that the connectivity\nis guaranteed for any desired probability. To this end, we exploit a mobility\nmodel recently proposed for sparse VANETs, based on BCMP open queuing networks\nand solve the related traffic equations to find the traffic characteristics of\neach street and use the results to compute the exact probability of\nconnectivity along these streets. Then, we use the results from percolation\ntheory and the proposed fast algorithms for evaluation of bond percolation\nproblem in a random graph corresponding to the block of the streets. We then\nfind sufficiently accurate two dimensional connectivity-related parameters,\nsuch as the average number of intersections connected to each other and the\nsize of the largest set of inter-connected intersections. We have also proposed\nlower bounds for the case of heterogeneous network with two transmission\nranges. In the last part of the paper, we apply our method to several numerical\nexamples and confirm our results by simulations.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2008 07:16:10 GMT"}], "update_date": "2008-12-10", "authors_parsed": [["Farivar", "Masoud", ""], ["Mehrdad", "Behzad", ""], ["Ashtiani", "Farid", ""]]}, {"id": "0812.1915", "submitter": "Marcel Marquardt", "authors": "Wouter Gelade, Marcel Marquardt, Thomas Schwentick", "title": "Dynamic Complexity of Formal Languages", "comments": "Contains the material presenten at STACS 2009, extendes with proofs\n  and examples which were omitted due lack of space", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper investigates the power of the dynamic complexity classes DynFO,\nDynQF and DynPROP over string languages. The latter two classes contain\nproblems that can be maintained using quantifier-free first-order updates, with\nand without auxiliary functions, respectively. It is shown that the languages\nmaintainable in DynPROP exactly are the regular languages, even when allowing\narbitrary precomputation. This enables lower bounds for DynPROP and separates\nDynPROP from DynQF and DynFO. Further, it is shown that any context-free\nlanguage can be maintained in DynFO and a number of specific context-free\nlanguages, for example all Dyck-languages, are maintainable in DynQF.\nFurthermore, the dynamic complexity of regular tree languages is investigated\nand some results concerning arbitrary structures are obtained: there exist\nfirst-order definable properties which are not maintainable in DynPROP. On the\nother hand any existential first-order property can be maintained in DynQF when\nallowing precomputation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2008 14:13:57 GMT"}], "update_date": "2008-12-11", "authors_parsed": [["Gelade", "Wouter", ""], ["Marquardt", "Marcel", ""], ["Schwentick", "Thomas", ""]]}, {"id": "0812.1951", "submitter": "Jerome Leroux", "authors": "Alain Finkel (LSV), J\\'er\\^ome Leroux (LaBRI)", "title": "The convex hull of a regular set of integer vectors is polyhedral and\n  effectively computable", "comments": null, "journal-ref": "Information Processing Letters 96, 1 (2005) 30 - 35", "doi": "10.1016/j.ipl.2005.04.004", "report-no": null, "categories": "cs.CG cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Number Decision Diagrams (NDD) provide a natural finite symbolic\nrepresentation for regular set of integer vectors encoded as strings of digit\nvectors (least or most significant digit first). The convex hull of the set of\nvectors represented by a NDD is proved to be an effectively computable convex\npolyhedron.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2008 16:26:36 GMT"}], "update_date": "2008-12-13", "authors_parsed": [["Finkel", "Alain", "", "LSV"], ["Leroux", "J\u00e9r\u00f4me", "", "LaBRI"]]}, {"id": "0812.2011", "submitter": "Jerome Leroux", "authors": "J\\'er\\^ome Leroux (LaBRI), Gregoire Sutre (LaBRI)", "title": "Accelerated Data-Flow Analysis", "comments": null, "journal-ref": "Static Analysis, Kongens Lyngby : Danemark (2007)", "doi": "10.1007/978-3-540-74061-2_12", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acceleration in symbolic verification consists in computing the exact effect\nof some control-flow loops in order to speed up the iterative fix-point\ncomputation of reachable states. Even if no termination guarantee is provided\nin theory, successful results were obtained in practice by different tools\nimplementing this framework. In this paper, the acceleration framework is\nextended to data-flow analysis. Compared to a classical\nwidening/narrowing-based abstract interpretation, the loss of precision is\ncontrolled here by the choice of the abstract domain and does not depend on the\nway the abstract value is computed. Our approach is geared towards precision,\nbut we don't loose efficiency on the way. Indeed, we provide a cubic-time\nacceleration-based algorithm for solving interval constraints with full\nmultiplication.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2008 20:08:08 GMT"}], "update_date": "2008-12-11", "authors_parsed": [["Leroux", "J\u00e9r\u00f4me", "", "LaBRI"], ["Sutre", "Gregoire", "", "LaBRI"]]}, {"id": "0812.2014", "submitter": "Jerome Leroux", "authors": "J\\'er\\^ome Leroux (LaBRI)", "title": "Convex Hull of Arithmetic Automata", "comments": null, "journal-ref": "Static Analysis, Valencia : Espagne (2008)", "doi": "10.1007/978-3-540-69166-2_4", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arithmetic automata recognize infinite words of digits denoting\ndecompositions of real and integer vectors. These automata are known expressive\nand efficient enough to represent the whole set of solutions of complex linear\nconstraints combining both integral and real variables. In this paper, the\nclosed convex hull of arithmetic automata is proved rational polyhedral.\nMoreover an algorithm computing the linear constraints defining these convex\nset is provided. Such an algorithm is useful for effectively extracting\ngeometrical properties of the whole set of solutions of complex constraints\nsymbolically represented by arithmetic automata.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2008 20:33:27 GMT"}], "update_date": "2008-12-11", "authors_parsed": [["Leroux", "J\u00e9r\u00f4me", "", "LaBRI"]]}, {"id": "0812.2115", "submitter": "Gabrio Curzio Caimi", "authors": "Gabrio Caimi, Holger Flier, Martin Fuchsberger, Marc Nunkesser", "title": "Performance of a greedy algorithm for edge covering by cliques in\n  interval graphs", "comments": "8 pages, 3 pictures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a greedy algorithm to detect conflict cliques in interval\ngraphs and circular-arc graphs is analyzed. In a graph, a stable set requires\nthat at most one vertex is chosen for each edge. It is equivalent to requiring\nthat at most one vertex for each maximal clique is chosen. We show that this\nalgorithm finds all maximal cliques for interval graphs, i.e. it can compute\nthe convex hull of the stable set polytope. In case of circular-arc graphs, the\nalgorithm is not able to detect all maximal cliques, yet remaining correct.\nThis problem occurs in the context of railway scheduling. A train requests the\nallocation of a railway infrastructure resource for a specific time interval.\nAs one is looking for conflict-free train schedules, the used resource\nallocation intervals in a schedule must not overlap. The conflict-free choices\nof used intervals for each resource correspond to stable sets in the interval\ngraph associated to the allocation time intervals.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2008 15:35:45 GMT"}], "update_date": "2008-12-12", "authors_parsed": [["Caimi", "Gabrio", ""], ["Flier", "Holger", ""], ["Fuchsberger", "Martin", ""], ["Nunkesser", "Marc", ""]]}, {"id": "0812.2137", "submitter": "Marek Karpinski", "authors": "Piotr Berman, Marek Karpinski, Alex Zelikovsky", "title": "A Factor 3/2 Approximation for Generalized Steiner Tree Problem with\n  Distances One and Two", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a 3/2 approximation algorithm for the Generalized Steiner Tree\nproblem (GST) in metrics with distances 1 and 2. This is the first polynomial\ntime approximation algorithm for a wide class of non-geometric metric GST\ninstances with approximation factor below 2.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2008 12:50:54 GMT"}], "update_date": "2008-12-12", "authors_parsed": [["Berman", "Piotr", ""], ["Karpinski", "Marek", ""], ["Zelikovsky", "Alex", ""]]}, {"id": "0812.2291", "submitter": "Aleksandrs Slivkins", "authors": "Moshe Babaioff, Yogeshwer Sharma, Aleksandrs Slivkins", "title": "Characterizing Truthful Multi-Armed Bandit Mechanisms", "comments": "This is the full version of a conference paper published in ACM EC\n  2009. This revision is re-focused to emphasize the results that do not rely\n  on the \"IIA assumption\" (see the paper for the definition)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multi-round auction setting motivated by pay-per-click auctions\nfor Internet advertising. In each round the auctioneer selects an advertiser\nand shows her ad, which is then either clicked or not. An advertiser derives\nvalue from clicks; the value of a click is her private information. Initially,\nneither the auctioneer nor the advertisers have any information about the\nlikelihood of clicks on the advertisements. The auctioneer's goal is to design\na (dominant strategies) truthful mechanism that (approximately) maximizes the\nsocial welfare.\n  If the advertisers bid their true private values, our problem is equivalent\nto the \"multi-armed bandit problem\", and thus can be viewed as a strategic\nversion of the latter. In particular, for both problems the quality of an\nalgorithm can be characterized by \"regret\", the difference in social welfare\nbetween the algorithm and the benchmark which always selects the same \"best\"\nadvertisement. We investigate how the design of multi-armed bandit algorithms\nis affected by the restriction that the resulting mechanism must be truthful.\nWe find that truthful mechanisms have certain strong structural properties --\nessentially, they must separate exploration from exploitation -- and they incur\nmuch higher regret than the optimal multi-armed bandit algorithms. Moreover, we\nprovide a truthful mechanism which (essentially) matches our lower bound on\nregret.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2008 04:13:01 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2009 01:56:08 GMT"}, {"version": "v3", "created": "Fri, 20 Feb 2009 18:10:47 GMT"}, {"version": "v4", "created": "Tue, 23 Jun 2009 02:21:56 GMT"}, {"version": "v5", "created": "Fri, 18 Sep 2009 00:17:44 GMT"}, {"version": "v6", "created": "Tue, 15 May 2012 22:57:53 GMT"}, {"version": "v7", "created": "Mon, 3 Jun 2013 21:03:36 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Babaioff", "Moshe", ""], ["Sharma", "Yogeshwer", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "0812.2298", "submitter": "Francois Le Gall", "authors": "Francois Le Gall", "title": "Efficient Isomorphism Testing for a Class of Group Extensions", "comments": "17 pages, accepted to the STACS 2009 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.GR quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The group isomorphism problem asks whether two given groups are isomorphic or\nnot. Whereas the case where both groups are abelian is well understood and can\nbe solved efficiently, very little is known about the complexity of isomorphism\ntesting for nonabelian groups. In this paper we study this problem for a class\nof groups corresponding to one of the simplest ways of constructing nonabelian\ngroups from abelian groups: the groups that are extensions of an abelian group\nA by a cyclic group of order m. We present an efficient algorithm solving the\ngroup isomorphism problem for all the groups of this class such that the order\nof A is coprime with m. More precisely, our algorithm runs in time almost\nlinear in the orders of the input groups and works in the general setting where\nthe groups are given as black-boxes.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2008 09:39:02 GMT"}], "update_date": "2010-01-07", "authors_parsed": [["Gall", "Francois Le", ""]]}, {"id": "0812.2599", "submitter": "Sewoong Oh", "authors": "Raghunandan H. Keshavan, Andrea Montanari, Sewoong Oh", "title": "Learning Low Rank Matrices from O(n) Entries", "comments": "8 pages, 11 figures, Forty-sixth Allerton Conference on\n  Communication, Control and Computing, invited paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How many random entries of an n by m, rank r matrix are necessary to\nreconstruct the matrix within an accuracy d? We address this question in the\ncase of a random matrix with bounded rank, whereby the observed entries are\nchosen uniformly at random. We prove that, for any d>0, C(r,d)n observations\nare sufficient. Finally we discuss the question of reconstructing the matrix\nefficiently, and demonstrate through extensive simulations that this task can\nbe accomplished in nPoly(log n) operations, for small rank.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2008 18:30:44 GMT"}], "update_date": "2008-12-16", "authors_parsed": [["Keshavan", "Raghunandan H.", ""], ["Montanari", "Andrea", ""], ["Oh", "Sewoong", ""]]}, {"id": "0812.2636", "submitter": "Tobias Friedrich", "authors": "Karl Bringmann, Tobias Friedrich", "title": "Approximating the least hypervolume contributor: NP-hard in general, but\n  fast in practice", "comments": "22 pages, to appear in Theoretical Computer Science", "journal-ref": null, "doi": "10.1016/j.tcs.2010.09.026", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hypervolume indicator is an increasingly popular set measure to compare\nthe quality of two Pareto sets. The basic ingredient of most hypervolume\nindicator based optimization algorithms is the calculation of the hypervolume\ncontribution of single solutions regarding a Pareto set. We show that exact\ncalculation of the hypervolume contribution is #P-hard while its approximation\nis NP-hard. The same holds for the calculation of the minimal contribution. We\nalso prove that it is NP-hard to decide whether a solution has the least\nhypervolume contribution. Even deciding whether the contribution of a solution\nis at most $(1+\\eps)$ times the minimal contribution is NP-hard. This implies\nthat it is neither possible to efficiently find the least contributing solution\n(unless $P = NP$) nor to approximate it (unless $NP = BPP$).\n  Nevertheless, in the second part of the paper we present a fast approximation\nalgorithm for this problem. We prove that for arbitrarily given $\\eps,\\delta>0$\nit calculates a solution with contribution at most $(1+\\eps)$ times the minimal\ncontribution with probability at least $(1-\\delta)$. Though it cannot run in\npolynomial time for all instances, it performs extremely fast on various\nbenchmark datasets. The algorithm solves very large problem instances which are\nintractable for exact algorithms (e.g., 10000 solutions in 100 dimensions)\nwithin a few seconds.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2008 13:57:10 GMT"}, {"version": "v2", "created": "Fri, 24 Sep 2010 20:43:10 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Bringmann", "Karl", ""], ["Friedrich", "Tobias", ""]]}, {"id": "0812.2775", "submitter": "Johannes Fischer", "authors": "Johannes Fischer", "title": "Optimal Succinctness for Range Minimum Queries", "comments": "12 pages; to appear in Proc. LATIN'10", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a static array A of n ordered objects, a range minimum query asks for the\nposition of the minimum between two specified array indices. We show how to\npreprocess A into a scheme of size 2n+o(n) bits that allows to answer range\nminimum queries on A in constant time. This space is asymptotically optimal in\nthe important setting where access to A is not permitted after the\npreprocessing step. Our scheme can be computed in linear time, using only n +\no(n) additional bits at construction time. In interesting by-product is that we\nalso improve on LCA-computation in BPS- or DFUDS-encoded trees.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2008 12:03:31 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2009 07:35:41 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2009 09:22:49 GMT"}], "update_date": "2009-12-02", "authors_parsed": [["Fischer", "Johannes", ""]]}, {"id": "0812.2851", "submitter": "Amr Elmasry", "authors": "Amr Elmasry", "title": "The Violation Heap: A Relaxed Fibonacci-Like Heap", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a priority queue that achieves the same amortized bounds as Fibonacci\nheaps. Namely, find-min requires O(1) worst-case time, insert, meld and\ndecrease-key require O(1) amortized time, and delete-min requires $O(\\log n)$\namortized time. Our structure is simple and promises an efficient practical\nbehavior when compared to other known Fibonacci-like heaps. The main idea\nbehind our construction is to propagate rank updates instead of performing\ncascaded cuts following a decrease-key operation, allowing for a relaxed\nstructure.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2008 16:16:58 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2010 12:24:07 GMT"}], "update_date": "2010-02-11", "authors_parsed": [["Elmasry", "Amr", ""]]}, {"id": "0812.2868", "submitter": "Travis Gagie", "authors": "Pawel Gawrychowski and Travis Gagie", "title": "Minimax Trees in Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A minimax tree is similar to a Huffman tree except that, instead of\nminimizing the weighted average of the leaves' depths, it minimizes the maximum\nof any leaf's weight plus its depth. Golumbic (1976) introduced minimax trees\nand gave a Huffman-like, $\\Oh{n \\log n}$-time algorithm for building them.\nDrmota and Szpankowski (2002) gave another $\\Oh{n \\log n}$-time algorithm,\nwhich checks the Kraft Inequality in each step of a binary search. In this\npaper we show how Drmota and Szpankowski's algorithm can be made to run in\nlinear time on a word RAM with (\\Omega (\\log n))-bit words. We also discuss how\nour solution applies to problems in data compression, group testing and circuit\ndesign.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2008 17:15:51 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2009 13:45:39 GMT"}], "update_date": "2009-01-28", "authors_parsed": [["Gawrychowski", "Pawel", ""], ["Gagie", "Travis", ""]]}, {"id": "0812.3137", "submitter": "Olga Holtz", "authors": "Olga Holtz", "title": "Compressive sensing: a paradigm shift in signal processing", "comments": "A short survey of compressive sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.HO cs.DS cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey a new paradigm in signal processing known as \"compressive sensing\".\nContrary to old practices of data acquisition and reconstruction based on the\nShannon-Nyquist sampling principle, the new theory shows that it is possible to\nreconstruct images or signals of scientific interest accurately and even\nexactly from a number of samples which is far smaller than the desired\nresolution of the image/signal, e.g., the number of pixels in the image. This\nnew technique draws from results in several fields of mathematics, including\nalgebra, optimization, probability theory, and harmonic analysis. We will\ndiscuss some of the key mathematical ideas behind compressive sensing, as well\nas its implications to other fields: numerical analysis, information theory,\ntheoretical computer science, and engineering.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2008 19:53:30 GMT"}], "update_date": "2009-03-13", "authors_parsed": [["Holtz", "Olga", ""]]}, {"id": "0812.3702", "submitter": "Michael Mahoney", "authors": "Michael W. Mahoney, Lek-Heng Lim, and Gunnar E. Carlsson", "title": "Algorithmic and Statistical Challenges in Modern Large-Scale Data\n  Analysis are the Focus of MMDS 2008", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2008 Workshop on Algorithms for Modern Massive Data Sets (MMDS 2008),\nsponsored by the NSF, DARPA, LinkedIn, and Yahoo!, was held at Stanford\nUniversity, June 25--28. The goals of MMDS 2008 were (1) to explore novel\ntechniques for modeling and analyzing massive, high-dimensional, and\nnonlinearly-structured scientific and internet data sets; and (2) to bring\ntogether computer scientists, statisticians, mathematicians, and data analysis\npractitioners to promote cross-fertilization of ideas.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2008 03:53:03 GMT"}], "update_date": "2008-12-22", "authors_parsed": [["Mahoney", "Michael W.", ""], ["Lim", "Lek-Heng", ""], ["Carlsson", "Gunnar E.", ""]]}, {"id": "0812.3933", "submitter": "Masud Hasan", "authors": "Masud Hasan, Atif Rahman, M. Sohel Rahman, Mahfuza Sharmin, and\n  Rukhsana Yeasmin", "title": "Pancake Flipping with Two Spatulas", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study several variations of the \\emph{pancake flipping\nproblem}, which is also well known as the problem of \\emph{sorting by prefix\nreversals}. We consider the variations in the sorting process by adding with\nprefix reversals other similar operations such as prefix transpositions and\nprefix transreversals. These type of sorting problems have applications in\ninterconnection networks and computational biology. We first study the problem\nof sorting unsigned permutations by prefix reversals and prefix transpositions\nand present a 3-approximation algorithm for this problem. Then we give a\n2-approximation algorithm for sorting by prefix reversals and prefix\ntransreversals. We also provide a 3-approximation algorithm for sorting by\nprefix reversals and prefix transpositions where the operations are always\napplied at the unsorted suffix of the permutation. We further analyze the\nproblem in more practical way and show quantitatively how approximation ratios\nof our algorithms improve with the increase of number of prefix reversals\napplied by optimal algorithms. Finally, we present experimental results to\nsupport our analysis.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2008 03:10:11 GMT"}, {"version": "v2", "created": "Mon, 4 May 2009 12:06:02 GMT"}], "update_date": "2009-05-04", "authors_parsed": [["Hasan", "Masud", ""], ["Rahman", "Atif", ""], ["Rahman", "M. Sohel", ""], ["Sharmin", "Mahfuza", ""], ["Yeasmin", "Rukhsana", ""]]}, {"id": "0812.3946", "submitter": "Stephane Vialette", "authors": "Guillaume Blin (IGM), Sylvie Hamel (DIRO), St\\'ephane Vialette (IGM)", "title": "Comparing RNA structures using a full set of biologically relevant edit\n  operations is intractable", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arc-annotated sequences are useful for representing structural information of\nRNAs and have been extensively used for comparing RNA structures in both terms\nof sequence and structural similarities. Among the many paradigms referring to\narc-annotated sequences and RNA structures comparison (see\n\\cite{IGMA_BliDenDul08} for more details), the most important one is the\ngeneral edit distance. The problem of computing an edit distance between two\nnon-crossing arc-annotated sequences was introduced in \\cite{Evans99}. The\nintroduced model uses edit operations that involve either single letters or\npairs of letters (never considered separately) and is solvable in\npolynomial-time \\cite{ZhangShasha:1989}. To account for other possible RNA\nstructural evolutionary events, new edit operations, allowing to consider\neither silmutaneously or separately letters of a pair were introduced in\n\\cite{jiangli}; unfortunately at the cost of computational tractability. It has\nbeen proved that comparing two RNA secondary structures using a full set of\nbiologically relevant edit operations is {\\sf\\bf NP}-complete. Nevertheless, in\n\\cite{DBLP:conf/spire/GuignonCH05}, the authors have used a strong\ncombinatorial restriction in order to compare two RNA stem-loops with a full\nset of biologically relevant edit operations; which have allowed them to design\na polynomial-time and space algorithm for comparing general secondary RNA\nstructures. In this paper we will prove theoretically that comparing two RNA\nstructures using a full set of biologically relevant edit operations cannot be\ndone without strong combinatorial restrictions.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2008 08:04:25 GMT"}], "update_date": "2008-12-23", "authors_parsed": [["Blin", "Guillaume", "", "IGM"], ["Hamel", "Sylvie", "", "DIRO"], ["Vialette", "St\u00e9phane", "", "IGM"]]}, {"id": "0812.4073", "submitter": "Andreas Noack", "authors": "Andreas Noack, Randolf Rotta", "title": "Multi-level algorithms for modularity clustering", "comments": "12 pages, 10 figures, see\n  http://www.informatik.tu-cottbus.de/~rrotta/ for downloading the graph\n  clustering software", "journal-ref": "Proceedings of the 8th International Symposium on Experimental\n  Algorithms (SEA 2009). Lecture Notes in Computer Science 5526, Springer\n  (2009) 257-268", "doi": null, "report-no": null, "categories": "cs.DS cond-mat.stat-mech cs.DM physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modularity is one of the most widely used quality measures for graph\nclusterings. Maximizing modularity is NP-hard, and the runtime of exact\nalgorithms is prohibitive for large graphs. A simple and effective class of\nheuristics coarsens the graph by iteratively merging clusters (starting from\nsingletons), and optionally refines the resulting clustering by iteratively\nmoving individual vertices between clusters. Several heuristics of this type\nhave been proposed in the literature, but little is known about their relative\nperformance.\n  This paper experimentally compares existing and new coarsening- and\nrefinement-based heuristics with respect to their effectiveness (achieved\nmodularity) and efficiency (runtime). Concerning coarsening, it turns out that\nthe most widely used criterion for merging clusters (modularity increase) is\noutperformed by other simple criteria, and that a recent algorithm by Schuetz\nand Caflisch is no improvement over simple greedy coarsening for these\ncriteria. Concerning refinement, a new multi-level algorithm is shown to\nproduce significantly better clusterings than conventional single-level\nalgorithms. A comparison with published benchmark results and algorithm\nimplementations shows that combinations of coarsening and multi-level\nrefinement are competitive with the best algorithms in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2008 15:32:10 GMT"}, {"version": "v2", "created": "Mon, 29 Dec 2008 21:56:37 GMT"}], "update_date": "2009-09-22", "authors_parsed": [["Noack", "Andreas", ""], ["Rotta", "Randolf", ""]]}, {"id": "0812.4293", "submitter": "Michael Mahoney", "authors": "Christos Boutsidis, Michael W. Mahoney, and Petros Drineas", "title": "An Improved Approximation Algorithm for the Column Subset Selection\n  Problem", "comments": "17 pages; corrected a bug in the spectral norm bound of the previous\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of selecting the best subset of exactly $k$ columns\nfrom an $m \\times n$ matrix $A$. We present and analyze a novel two-stage\nalgorithm that runs in $O(\\min\\{mn^2,m^2n\\})$ time and returns as output an $m\n\\times k$ matrix $C$ consisting of exactly $k$ columns of $A$. In the first\n(randomized) stage, the algorithm randomly selects $\\Theta(k \\log k)$ columns\naccording to a judiciously-chosen probability distribution that depends on\ninformation in the top-$k$ right singular subspace of $A$. In the second\n(deterministic) stage, the algorithm applies a deterministic column-selection\nprocedure to select and return exactly $k$ columns from the set of columns\nselected in the first stage. Let $C$ be the $m \\times k$ matrix containing\nthose $k$ columns, let $P_C$ denote the projection matrix onto the span of\nthose columns, and let $A_k$ denote the best rank-$k$ approximation to the\nmatrix $A$. Then, we prove that, with probability at least 0.8, $$ \\FNorm{A -\nP_CA} \\leq \\Theta(k \\log^{1/2} k) \\FNorm{A-A_k}. $$ This Frobenius norm bound\nis only a factor of $\\sqrt{k \\log k}$ worse than the best previously existing\nexistential result and is roughly $O(\\sqrt{k!})$ better than the best previous\nalgorithmic result for the Frobenius norm version of this Column Subset\nSelection Problem (CSSP). We also prove that, with probability at least 0.8, $$\n\\TNorm{A - P_CA} \\leq \\Theta(k \\log^{1/2} k)\\TNorm{A-A_k} +\n\\Theta(k^{3/4}\\log^{1/4}k)\\FNorm{A-A_k}. $$ This spectral norm bound is not\ndirectly comparable to the best previously existing bounds for the spectral\nnorm version of this CSSP. Our bound depends on $\\FNorm{A-A_k}$, whereas\nprevious results depend on $\\sqrt{n-k}\\TNorm{A-A_k}$; if these two quantities\nare comparable, then our bound is asymptotically worse by a $(k \\log k)^{1/4}$\nfactor.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2008 21:16:55 GMT"}, {"version": "v2", "created": "Wed, 12 May 2010 02:44:25 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Boutsidis", "Christos", ""], ["Mahoney", "Michael W.", ""], ["Drineas", "Petros", ""]]}, {"id": "0812.4442", "submitter": "Sanjeev Khanna", "authors": "Julia Chuzhoy and Sanjeev Khanna", "title": "An $O(k^{3} log n)$-Approximation Algorithm for Vertex-Connectivity\n  Survivable Network Design", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Survivable Network Design problem (SNDP), we are given an undirected\ngraph $G(V,E)$ with costs on edges, along with a connectivity requirement\n$r(u,v)$ for each pair $u,v$ of vertices. The goal is to find a minimum-cost\nsubset $E^*$ of edges, that satisfies the given set of pairwise connectivity\nrequirements. In the edge-connectivity version we need to ensure that there are\n$r(u,v)$ edge-disjoint paths for every pair $u, v$ of vertices, while in the\nvertex-connectivity version the paths are required to be vertex-disjoint. The\nedge-connectivity version of SNDP is known to have a 2-approximation. However,\nno non-trivial approximation algorithm has been known so far for the vertex\nversion of SNDP, except for special cases of the problem. We present an\nextremely simple algorithm to achieve an $O(k^3 \\log n)$-approximation for this\nproblem, where $k$ denotes the maximum connectivity requirement, and $n$\ndenotes the number of vertices. We also give a simple proof of the recently\ndiscovered $O(k^2 \\log n)$-approximation result for the single-source version\nof vertex-connectivity SNDP. We note that in both cases, our analysis in fact\nyields slightly better guarantees in that the $\\log n$ term in the\napproximation guarantee can be replaced with a $\\log \\tau$ term where $\\tau$\ndenotes the number of distinct vertices that participate in one or more pairs\nwith a positive connectivity requirement.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2008 19:04:25 GMT"}], "update_date": "2008-12-24", "authors_parsed": [["Chuzhoy", "Julia", ""], ["Khanna", "Sanjeev", ""]]}, {"id": "0812.4547", "submitter": "Christos Boutsidis", "authors": "Christos Boutsidis, Petros Drineas", "title": "Random Projections for the Nonnegative Least-Squares Problem", "comments": "to appear in Linear Algebra and its Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained least-squares regression problems, such as the Nonnegative Least\nSquares (NNLS) problem, where the variables are restricted to take only\nnonnegative values, often arise in applications. Motivated by the recent\ndevelopment of the fast Johnson-Lindestrauss transform, we present a fast\nrandom projection type approximation algorithm for the NNLS problem. Our\nalgorithm employs a randomized Hadamard transform to construct a much smaller\nNNLS problem and solves this smaller problem using a standard NNLS solver. We\nprove that our approach finds a nonnegative solution vector that, with high\nprobability, is close to the optimum nonnegative solution in a relative error\napproximation sense. We experimentally evaluate our approach on a large\ncollection of term-document data and verify that it does offer considerable\nspeedups without a significant loss in accuracy. Our analysis is based on a\nnovel random projection type result that might be of independent interest. In\nparticular, given a tall and thin matrix $\\Phi \\in \\mathbb{R}^{n \\times d}$ ($n\n\\gg d$) and a vector $y \\in \\mathbb{R}^d$, we prove that the Euclidean length\nof $\\Phi y$ can be estimated very accurately by the Euclidean length of\n$\\tilde{\\Phi}y$, where $\\tilde{\\Phi}$ consists of a small subset of\n(appropriately rescaled) rows of $\\Phi$.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2008 16:43:22 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2009 20:17:36 GMT"}], "update_date": "2009-03-13", "authors_parsed": [["Boutsidis", "Christos", ""], ["Drineas", "Petros", ""]]}, {"id": "0812.4893", "submitter": "Jukka Suomela", "authors": "Patrik Flor\\'een, Petteri Kaski, Valentin Polishchuk, Jukka Suomela", "title": "Almost stable matchings in constant time", "comments": "20 pages", "journal-ref": "Algorithmica 58 (2010) 102-118", "doi": "10.1007/s00453-009-9353-9", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the ratio of matched individuals to blocking pairs grows\nlinearly with the number of propose--accept rounds executed by the\nGale--Shapley algorithm for the stable marriage problem. Consequently, the\nparticipants can arrive at an almost stable matching even without full\ninformation about the problem instance; for each participant, knowing only its\nlocal neighbourhood is enough. In distributed-systems parlance, this means that\nif each person has only a constant number of acceptable partners, an almost\nstable matching emerges after a constant number of synchronous communication\nrounds. This holds even if ties are present in the preference lists.\n  We apply our results to give a distributed $(2+\\epsilon)$-approximation\nalgorithm for maximum-weight matching in bicoloured graphs and a centralised\nrandomised constant-time approximation scheme for estimating the size of a\nstable matching.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2008 11:04:46 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Flor\u00e9en", "Patrik", ""], ["Kaski", "Petteri", ""], ["Polishchuk", "Valentin", ""], ["Suomela", "Jukka", ""]]}, {"id": "0812.4905", "submitter": "Jure Leskovec", "authors": "Jure Leskovec, Deepayan Chakrabarti, Jon Kleinberg, Christos Faloutsos\n  and Zoubin Ghahramani", "title": "Kronecker Graphs: An Approach to Modeling Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we model networks with a mathematically tractable model that allows\nfor rigorous analysis of network properties? Networks exhibit a long list of\nsurprising properties: heavy tails for the degree distribution; small\ndiameters; and densification and shrinking diameters over time. Most present\nnetwork models either fail to match several of the above properties, are\ncomplicated to analyze mathematically, or both. In this paper we propose a\ngenerative model for networks that is both mathematically tractable and can\ngenerate networks that have the above mentioned properties. Our main idea is to\nuse the Kronecker product to generate graphs that we refer to as \"Kronecker\ngraphs\".\n  First, we prove that Kronecker graphs naturally obey common network\nproperties. We also provide empirical evidence showing that Kronecker graphs\ncan effectively model the structure of real networks.\n  We then present KronFit, a fast and scalable algorithm for fitting the\nKronecker graph generation model to large real networks. A naive approach to\nfitting would take super- exponential time. In contrast, KronFit takes linear\ntime, by exploiting the structure of Kronecker matrix multiplication and by\nusing statistical simulation techniques.\n  Experiments on large real and synthetic networks show that KronFit finds\naccurate parameters that indeed very well mimic the properties of target\nnetworks. Once fitted, the model parameters can be used to gain insights about\nthe network structure, and the resulting synthetic graphs can be used for null-\nmodels, anonymization, extrapolations, and graph summarization.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2008 13:22:23 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2009 21:52:11 GMT"}], "update_date": "2009-08-22", "authors_parsed": [["Leskovec", "Jure", ""], ["Chakrabarti", "Deepayan", ""], ["Kleinberg", "Jon", ""], ["Faloutsos", "Christos", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "0812.4919", "submitter": "Ildik\\'o Schlotter", "authors": "D\\'aniel Marx, Ildik\\'o Schlotter", "title": "Obtaining a Planar Graph by Vertex Deletion", "comments": "16 pages, 4 figures. A preliminary version of this paper appeared in\n  the proceedings of WG 2007 (33rd International Workshop on Graph-Theoretic\n  Concepts in Computer Science). The paper has been submitted to Algorithmica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the k-Apex problem the task is to find at most k vertices whose deletion\nmakes the given graph planar. The graphs for which there exists a solution form\na minor closed class of graphs, hence by the deep results of Robertson and\nSeymour, there is an O(n^3) time algorithm for every fixed value of k. However,\nthe proof is extremely complicated and the constants hidden by the big-O\nnotation are huge. Here we give a much simpler algorithm for this problem with\nquadratic running time, by iteratively reducing the input graph and then\napplying techniques for graphs of bounded treewidth.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2008 14:57:14 GMT"}], "update_date": "2008-12-31", "authors_parsed": [["Marx", "D\u00e1niel", ""], ["Schlotter", "Ildik\u00f3", ""]]}, {"id": "0812.5101", "submitter": "Katarzyna Paluch", "authors": "Katarzyna Paluch, Marcin Mucha, Aleksander Madry", "title": "A 7/9 - Approximation Algorithm for the Maximum Traveling Salesman\n  Problem", "comments": "6 figures", "journal-ref": null, "doi": "10.1007/978-3-642-03685-9_23", "report-no": null, "categories": "cs.GT cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a 7/9 - Approximation Algorithm for the Maximum Traveling Salesman\nProblem.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2008 19:11:48 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Paluch", "Katarzyna", ""], ["Mucha", "Marcin", ""], ["Madry", "Aleksander", ""]]}]