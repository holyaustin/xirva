[{"id": "1905.00044", "submitter": "Chaitanya Swamy", "authors": "Deeparnab Chakrabarty and Chaitanya Swamy", "title": "Simpler and Better Algorithms for Minimum-Norm Load Balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Chakrabarty and Swamy (STOC 2019) introduced the {\\em minimum-norm\nload-balancing} problem on unrelated machines, wherein we are given a set $J$\nof jobs that need to be scheduled on a set of $m$ unrelated machines, and a\nmonotone, symmetric norm; We seek an assignment $\\sg:J\\mapsto[m]$ that\nminimizes the norm of the resulting load vector $\\lvec_\\sg\\in\\R_+^m$, where\n$\\lvec_\\sg(i)$ is the load on machine $i$ under the assignment $\\sg$. Besides\ncapturing all $\\ell_p$ norms, symmetric norms also capture other norms of\ninterest including top-$\\ell$ norms, and ordered norms. Chakrabarty and Swamy\n(STOC 2019) give a $(38+\\ve)$-approximation algorithm for this problem via a\ngeneral framework they develop for minimum-norm optimization that proceeds by\nfirst carefully reducing this problem (in a series of steps) to a problem\ncalled \\minmax ordered load balancing, and then devising a so-called\ndeterministic oblivious LP-rounding algorithm for ordered load balancing.\n  We give a direct, and simple $4$-approximation algorithm for the minimum-norm\nload balancing based on rounding a (near-optimal) solution to a novel\nconvex-programming relaxation for the problem. Whereas the natural convex\nprogram encoding minimum-norm load balancing problem has a large non-constant\nintegrality gap, we show that this issue can be remedied by including a key\nconstraint that bounds the \"norm of the job-cost vector.\" Our techniques also\nyield a (essentially) $4$-approximation for: (a) {\\em multi-norm load\nbalancing}, wherein we are given multiple monotone symmetric norms, and we seek\nan assignment respecting a given budget for each norm; (b) the best {\\em\nsimultaneous approximation factor} achievable for all symmetric norms for a\ngiven instance.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 18:03:51 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "1905.00066", "submitter": "Joan Boyar", "authors": "Joan Boyar, Lene M. Favrholdt, Shahin Kamali, and Kim S. Larsen", "title": "Online Bin Covering with Advice", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bin covering problem asks for covering a maximum number of bins with an\nonline sequence of $n$ items of different sizes in the range $(0,1]$; a bin is\nsaid to be covered if it receives items of total size at least 1. We study this\nproblem in the advice setting and provide tight bounds for the size of advice\nrequired to achieve optimal solutions. Moreover, we show that any algorithm\nwith advice of size $o(\\log \\log n)$ has a competitive ratio of at most 0.5. In\nother words, advice of size $o(\\log \\log n)$ is useless for improving the\ncompetitive ratio of 0.5, attainable by an online algorithm without advice.\nThis result highlights a difference between the bin covering and the bin\npacking problems in the advice model: for the bin packing problem, there are\nseveral algorithms with advice of constant size that outperform online\nalgorithms without advice. Furthermore, we show that advice of size $O(\\log\n\\log n)$ is sufficient to achieve a competitive ratio that is arbitrarily close\nto $0.53\\bar{3}$ and hence strictly better than the best ratio $0.5$ attainable\nby purely online algorithms. The technicalities involved in introducing and\nanalyzing this algorithm are quite different from the existing results for the\nbin packing problem and confirm the different nature of these two problems.\nFinally, we show that a linear number of bits of advice is necessary to achieve\nany competitive ratio better than 15/16 for the online bin covering problem.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 19:15:16 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 06:56:31 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Boyar", "Joan", ""], ["Favrholdt", "Lene M.", ""], ["Kamali", "Shahin", ""], ["Larsen", "Kim S.", ""]]}, {"id": "1905.00073", "submitter": "Ayoub Otmani", "authors": "Magali Bardet and Ayoub Otmani and Mohamed Saeed-Taha", "title": "Permutation Code Equivalence is not Harder than Graph Isomorphism when\n  Hulls are Trivial", "comments": "Accepted to 2019 IEEE International Symposium on Information Theory", "journal-ref": null, "doi": "10.1109/ISIT.2019.8849855", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with the problem of deciding if two finite-dimensional linear\nsubspaces over an arbitrary field are identical up to a permutation of the\ncoordinates. This problem is referred to as the permutation code equivalence.\nWe show that given access to a subroutine that decides if two weighted\nundirected graphs are isomorphic, one may deterministically decide the\npermutation code equivalence provided that the underlying vector spaces\nintersect trivially with their orthogonal complement with respect to an\narbitrary inner product. Such a class of vector spaces is usually called linear\ncodes with trivial hulls. The reduction is efficient because it essentially\nboils down to computing the inverse of a square matrix of order the length of\nthe involved codes. Experimental results obtained with randomly drawn binary\ncodes having trivial hulls show that permutation code equivalence can be\ndecided in a few minutes for lengths up to 50,000.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 19:34:05 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Bardet", "Magali", ""], ["Otmani", "Ayoub", ""], ["Saeed-Taha", "Mohamed", ""]]}, {"id": "1905.00079", "submitter": "Jianlin Shi", "authors": "Jianlin Shi, John F. Hurdle", "title": "FastContext: an efficient and scalable implementation of the ConText\n  algorithm", "comments": null, "journal-ref": "Journal of Biomedical Informatics, August 6, 2018", "doi": "10.1016/j.jbi.2018.08.002", "report-no": null, "categories": "cs.CL cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Objective: To develop and evaluate FastContext, an efficient, scalable\nimplementation of the ConText algorithm suitable for very large-scale clinical\nnatural language processing. Background: The ConText algorithm performs with\nstate-of-art accuracy in detecting the experiencer, negation status, and\ntemporality of concept mentions in clinical narratives. However, the speed\nlimitation of its current implementations hinders its use in big data\nprocessing. Methods: We developed FastContext through hashing the ConText's\nrules, then compared its speed and accuracy with JavaConText and\nGeneralConText, two widely used Java implementations. Results: FastContext ran\ntwo orders of magnitude faster and was less decelerated by rule increase than\nthe other two implementations used in this study for comparison. Additionally,\nFastContext consistently gained accuracy improvement as the rules increased\n(the desired outcome of adding new rules), while the other two implementations\ndid not. Conclusions: FastContext is an efficient, scalable implementation of\nthe popular ConText algorithm, suitable for natural language applications on\nvery large clinical corpora.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 19:57:47 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Shi", "Jianlin", ""], ["Hurdle", "John F.", ""]]}, {"id": "1905.00118", "submitter": "Yukun Yao", "authors": "Yukun Yao", "title": "Using Non-Linear Difference Equations to Study Quicksort Algorithms", "comments": "29 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using non-linear difference equations, combined with symbolic computations,\nwe make a detailed study of the running times of numerous variants of the\ncelebrated Quicksort algorithms, where we consider the variants of single-pivot\nand multi-pivot Quicksort algorithms as discrete probability problems. With\nnon-linear difference equations, recurrence relations and experimental\nmathematics techniques, explicit expressions for expectations, variances and\neven higher moments of their numbers of comparisons and swaps can be obtained.\nFor some variants, Monte Carlo experiments are performed, the numerical results\nare demonstrated and the scaled limiting distribution is also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 22:04:28 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 21:04:59 GMT"}, {"version": "v3", "created": "Wed, 26 Feb 2020 02:21:40 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Yao", "Yukun", ""]]}, {"id": "1905.00148", "submitter": "Yang Jiao", "authors": "Yang Jiao and R. Ravi", "title": "Inventory Routing Problem with Facility Location", "comments": "20 pages, preprint, full version of paper to appear in WADS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study problems that integrate depot location decisions along with the\ninventory routing problem of serving clients from these depots over time\nbalancing the costs of routing vehicles from the depots with the holding costs\nof demand delivered before they are due. Since the inventory routing problem is\nalready complex, we study the version that assumes that the daily vehicle\nroutes are direct connections from the depot thus forming stars as solutions,\nand call this problem the Star Inventory Routing Problem with Facility Location\n(SIRPFL). As a stepping stone to solving SIRPFL, we first study the Inventory\nAccess Problem (IAP), which is the single depot, single client special case of\nIRP. The Uncapacitated IAP is known to have a polynomial time dynamic program.\nWe provide an NP-hardness reduction for Capacitated IAP where each demand\ncannot be split among different trips. We give a $3$-approximation for the case\nwhen demands can be split and a $6$-approximation for the unsplittable case.\nFor Uncapacitated SIRPFL, we provide a $12$-approximation by rounding an LP\nrelaxation. Combining the ideas from Capacitated IAP and Uncapacitated SIRPFL,\nwe obtain a $24$-approximation for Capacitated Splittable SIRPFL and a\n$48$-approximation for the most general version, the Capacitated Unsplittable\nSIRPFL.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 01:04:02 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Jiao", "Yang", ""], ["Ravi", "R.", ""]]}, {"id": "1905.00163", "submitter": "Dominik K\\\"oppl", "authors": "Dominik K\\\"oppl", "title": "Separate Chaining Meets Compact Hashing", "comments": "Preprint of the local proceedings of the 173th SIGAL, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While separate chaining is a common strategy for resolving collisions in a\nhash table taught in most textbooks, compact hashing is a less common technique\nfor saving space when hashing integers whose domain is relatively small with\nrespect to the problem size. It is widely believed that hash tables waste a\nconsiderable amount of memory, as they either leave allocated space untouched\n(open addressing) or store additional pointers (separate chaining). For the\nformer, Cleary introduced the compact hashing technique that stores only a part\nof a key to save space. However, as can be seen by the line of research\nfocusing on compact hash tables with open addressing, there is additional\ninformation, called displacement, required for restoring a key. There are\nseveral representations of this displacement information with different space\nand time trade-offs. In this article, we introduce a separate chaining hash\ntable that applies the compact hashing technique without the need for the\ndisplacement information. Practical evaluations reveal that insertions in this\nhash table are faster or use less space than all previously known compact hash\ntables on modern computer architectures when storing sufficiently large\nsatellite data.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 02:22:14 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["K\u00f6ppl", "Dominik", ""]]}, {"id": "1905.00305", "submitter": "Astrid Pieterse", "authors": "Hans L. Bodlaender, Sudeshna Kolay, and Astrid Pieterse", "title": "Parameterized Complexity of Conflict-free Graph Coloring", "comments": "Accepted to WADS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph G, a q-open neighborhood conflict-free coloring or\nq-ONCF-coloring is a vertex coloring $c:V(G) \\rightarrow \\{1,2,\\ldots,q\\}$ such\nthat for each vertex $v \\in V(G)$ there is a vertex in $N(v)$ that is uniquely\ncolored from the rest of the vertices in $N(v)$. When we replace $N(v)$ by the\nclosed neighborhood $N[v]$, then we call such a coloring a q-closed\nneighborhood conflict-free coloring or simply q-CNCF-coloring. In this paper,\nwe study the NP-hard decision questions of whether for a constant q an input\ngraph has a q-ONCF-coloring or a q-CNCF-coloring. We will study these two\nproblems in the parameterized setting.\n  First of all, we study running time bounds on FPT-algorithms for these\nproblems, when parameterized by treewidth. We improve the existing upper\nbounds, and also provide lower bounds on the running time under ETH and SETH.\n  Secondly, we study the kernelization complexity of both problems, using\nvertex cover as the parameter. We show that both $(q \\geq 2)$-ONCF-coloring and\n$(q \\geq 3)$-CNCF-coloring cannot have polynomial kernels when parameterized by\nthe size of a vertex cover unless $NP \\in coNP/poly$. However, we obtain a\npolynomial kernel for 2-CNCF-coloring parameterized by vertex cover.\n  We conclude with some combinatorial results. Denote $\\chi_{ON}(G)$ and\n$\\chi_{CN}(G)$ to be the minimum number of colors required to ONCF-color and\nCNCF-color G, respectively. Upper bounds on $\\chi_{CN}(G)$ with respect to\nstructural parameters like minimum vertex cover size, minimum feedback vertex\nset size and treewidth are known. To the best of our knowledge only an upper\nbound on $\\chi_{ON}(G)$ with respect to minimum vertex cover size was known. We\nprovide tight bounds for $\\chi_{ON}(G)$ with respect to minimum vertex cover\nsize. Also, we provide the first upper bounds on $\\chi_{ON}(G)$ with respect to\nminimum feedback vertex set size and treewidth.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 13:23:30 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Bodlaender", "Hans L.", ""], ["Kolay", "Sudeshna", ""], ["Pieterse", "Astrid", ""]]}, {"id": "1905.00340", "submitter": "Yota Otachi", "authors": "R\\'emy Belmonte, Tesshu Hanaka, Michael Lampis, Hirotaka Ono, Yota\n  Otachi", "title": "Independent Set Reconfiguration Parameterized by Modular-Width", "comments": "14 pages, 1 figure, WG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent Set Reconfiguration is one of the most well-studied problems in\nthe setting of combinatorial reconfiguration. It is known that the problem is\nPSPACE-complete even for graphs of bounded bandwidth. This fact rules out the\ntractability of parameterizations by most well-studied structural parameters as\nmost of them generalize bandwidth. In this paper, we study the parameterization\nby modular-width, which is not comparable with bandwidth. We show that the\nproblem parameterized by modular-width is fixed-parameter tractable under all\npreviously studied rules TAR, TJ, and TS. The result under TAR resolves an open\nproblem posed by Bonsma [WG 2014, JGT 2016].\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 14:58:11 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Belmonte", "R\u00e9my", ""], ["Hanaka", "Tesshu", ""], ["Lampis", "Michael", ""], ["Ono", "Hirotaka", ""], ["Otachi", "Yota", ""]]}, {"id": "1905.00369", "submitter": "Anders Aamand", "authors": "Anders Aamand, Jakob B. T. Knudsen, Mathias B. T. Knudsen, Peter M. R.\n  Rasmussen, Mikkel Thorup", "title": "Fast hashing with Strong Concentration Bounds", "comments": "54 pages, 3 figures. An extended abstract appeared at the 52nd Annual\n  ACM Symposium on Theory of Computing (STOC20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work on tabulation hashing by Patrascu and Thorup from STOC'11 on\nsimple tabulation and from SODA'13 on twisted tabulation offered Chernoff-style\nconcentration bounds on hash based sums, e.g., the number of balls/keys hashing\nto a given bin, but under some quite severe restrictions on the expected values\nof these sums. The basic idea in tabulation hashing is to view a key as\nconsisting of $c=O(1)$ characters, e.g., a 64-bit key as $c=8$ characters of\n8-bits. The character domain $\\Sigma$ should be small enough that character\ntables of size $|\\Sigma|$ fit in fast cache. The schemes then use $O(1)$ tables\nof this size, so the space of tabulation hashing is $O(|\\Sigma|)$. However, the\nconcentration bounds by Patrascu and Thorup only apply if the expected sums are\n$\\ll |\\Sigma|$.\n  To see the problem, consider the very simple case where we use tabulation\nhashing to throw $n$ balls into $m$ bins and want to analyse the number of\nballs in a given bin. With their concentration bounds, we are fine if $n=m$,\nfor then the expected value is $1$. However, if $m=2$, as when tossing $n$\nunbiased coins, the expected value $n/2$ is $\\gg |\\Sigma|$ for large data sets,\ne.g., data sets that do not fit in fast cache.\n  To handle expectations that go beyond the limits of our small space, we need\na much more advanced analysis of simple tabulation, plus a new tabulation\ntechnique that we call \\emph{tabulation-permutation} hashing which is at most\ntwice as slow as simple tabulation. No other hashing scheme of comparable speed\noffers similar Chernoff-style concentration bounds.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 16:33:11 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 10:39:19 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 11:46:12 GMT"}, {"version": "v4", "created": "Fri, 17 Apr 2020 11:58:32 GMT"}, {"version": "v5", "created": "Mon, 10 Aug 2020 10:13:51 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Aamand", "Anders", ""], ["Knudsen", "Jakob B. T.", ""], ["Knudsen", "Mathias B. T.", ""], ["Rasmussen", "Peter M. R.", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1905.00518", "submitter": "David Eppstein", "authors": "Erik D. Demaine, David Eppstein, Adam Hesterberg, Kshitij Jain, Anna\n  Lubiw, Ryuhei Uehara, Yushi Uno", "title": "Reconfiguring Undirected Paths", "comments": "24 pages, 15 figures. Extended version of a paper to appear in the\n  Proceedings of the 16th Algorithms and Data Structures Symposium (WADS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider problems in which a simple path of fixed length, in an undirected\ngraph, is to be shifted from a start position to a goal position by moves that\nadd an edge to either end of the path and remove an edge from the other end. We\nshow that this problem may be solved in linear time in trees, and is\nfixed-parameter tractable when parameterized either by the cyclomatic number of\nthe input graph or by the length of the path. However, it is PSPACE-complete\nfor paths of unbounded length in graphs of bounded bandwidth.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 22:13:40 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Demaine", "Erik D.", ""], ["Eppstein", "David", ""], ["Hesterberg", "Adam", ""], ["Jain", "Kshitij", ""], ["Lubiw", "Anna", ""], ["Uehara", "Ryuhei", ""], ["Uno", "Yushi", ""]]}, {"id": "1905.00566", "submitter": "Amit Chakrabarti", "authors": "Suman K. Bera, Amit Chakrabarti, Prantar Ghosh", "title": "Graph Coloring via Degeneracy in Streaming and Other Space-Conscious\n  Models", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We study the problem of coloring a given graph using a small number of colors\nin several well-established models of computation for big data. These include\nthe data streaming model, the general graph query model, the massively parallel\ncomputation (MPC) model, and the CONGESTED-CLIQUE and the LOCAL models of\ndistributed computation. On the one hand, we give algorithms with sublinear\ncomplexity, for the appropriate notion of complexity in each of these models.\nOur algorithms color a graph $G$ using about $\\kappa(G)$ colors, where\n$\\kappa(G)$ is the degeneracy of $G$: this parameter is closely related to the\narboricity $\\alpha(G)$. As a function of $\\kappa(G)$ alone, our results are\nclose to best possible, since the optimal number of colors is $\\kappa(G)+1$.\n  On the other hand, we establish certain lower bounds indicating that\nsublinear algorithms probably cannot go much further. In particular, we prove\nthat any randomized coloring algorithm that uses $\\kappa(G)+1$ many colors,\nwould require $\\Omega(n^2)$ storage in the one pass streaming model, and\n$\\Omega(n^2)$ many queries in the general graph query model, where $n$ is the\nnumber of vertices in the graph. These lower bounds hold even when the value of\n$\\kappa(G)$ is known in advance; at the same time, our upper bounds do not\nrequire $\\kappa(G)$ to be given in advance.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 03:55:51 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Bera", "Suman K.", ""], ["Chakrabarti", "Amit", ""], ["Ghosh", "Prantar", ""]]}, {"id": "1905.00580", "submitter": "William Moses Jr.", "authors": "Yuval Emek, Shay Kutten, Ron Lavi, and William K. Moses Jr", "title": "Deterministic Leader Election in Programmable Matter", "comments": "33 pages, 16 figures, to appear in the proceedings of ICALP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Addressing a fundamental problem in programmable matter, we present the first\ndeterministic algorithm to elect a unique leader in a system of connected\namoebots assuming only that amoebots are initially contracted. Previous\nalgorithms either used randomization, made various assumptions (shapes with no\nholes, or known shared chirality), or elected several co-leaders in some cases.\n  Some of the building blocks we introduce in constructing the algorithm are of\ninterest by themselves, especially the procedure we present for reaching common\nchirality among the amoebots. Given the leader election and the chirality\nagreement building block, it is known that various tasks in programmable matter\ncan be performed or improved.\n  The main idea of the new algorithm is the usage of the ability of the\namoebots to move, which previous leader election algorithms have not used.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 05:56:08 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Emek", "Yuval", ""], ["Kutten", "Shay", ""], ["Lavi", "Ron", ""], ["Moses", "William K.", "Jr"]]}, {"id": "1905.00612", "submitter": "Sandor P. Fekete", "authors": "S\\'andor P. Fekete and Sven von H\\\"oveling and Christian Scheffer", "title": "Online Circle Packing", "comments": "13 pages, 11 figures, to appear in WADS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online problem of packing circles into a square container. A\nsequence of circles has to be packed one at a time, without knowledge of the\nfollowing incoming circles and without moving previously packed circles. We\npresent an algorithm that packs any online sequence of circles with a combined\narea not larger than 0.350389 0.350389 of the square's area, improving the\nprevious best value of {\\pi}/10 \\approx 0.31416; even in an offline setting,\nthere is an upper bound of {\\pi}/(3 + 2 \\sqrt{2}) \\approx 0.5390. If only\ncircles with radii of at least 0.026622 are considered, our algorithm achieves\nthe higher value 0.375898. As a byproduct, we give an online algorithm for\npacking circles into a 1\\times b rectangle with b \\geq 1. This algorithm is\nworst case-optimal for b \\geq 2.36.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 08:20:39 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Fekete", "S\u00e1ndor P.", ""], ["von H\u00f6veling", "Sven", ""], ["Scheffer", "Christian", ""]]}, {"id": "1905.00640", "submitter": "Siddharth Barman", "authors": "Siddharth Barman, Omar Fawzi, Suprovat Ghoshal, and Emirhan\n  G\\\"urp{\\i}nar", "title": "Tight Approximation Bounds for Maximum Multi-Coverage", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classic maximum coverage problem, we are given subsets $T_1, \\dots,\nT_m$ of a universe $[n]$ along with an integer $k$ and the objective is to find\na subset $S \\subseteq [m]$ of size $k$ that maximizes $C(S) := |\\cup_{i \\in S}\nT_i|$. It is well-known that the greedy algorithm for this problem achieves an\napproximation ratio of $(1-e^{-1})$ and there is a matching inapproximability\nresult. We note that in the maximum coverage problem if an element $e \\in [n]$\nis covered by several sets, it is still counted only once. By contrast, if we\nchange the problem and count each element $e$ as many times as it is covered,\nthen we obtain a linear objective function, $C^{(\\infty)}(S) = \\sum_{i \\in S}\n|T_i|$, which can be easily maximized under a cardinality constraint.\n  We study the maximum $\\ell$-multi-coverage problem which naturally\ninterpolates between these two extremes. In this problem, an element can be\ncounted up to $\\ell$ times but no more; hence, we consider maximizing the\nfunction $C^{(\\ell)}(S) = \\sum_{e \\in [n]} \\min\\{\\ell, |\\{i \\in S : e \\in\nT_i\\}| \\}$, subject to the constraint $|S| \\leq k$. Note that the case of $\\ell\n= 1$ corresponds to the standard maximum coverage setting and $\\ell = \\infty$\ngives us a linear objective.\n  We develop an efficient approximation algorithm that achieves an\napproximation ratio of $1 - \\frac{\\ell^{\\ell}e^{-\\ell}}{\\ell!}$ for the\n$\\ell$-multi-coverage problem. In particular, when $\\ell = 2$, this factor is\n$1-2e^{-2} \\approx 0.73$ and as $\\ell$ grows the approximation ratio behaves as\n$1 - \\frac{1}{\\sqrt{2\\pi \\ell}}$. We also prove that this approximation ratio\nis tight, i.e., establish a matching hardness-of-approximation result, under\nthe Unique Games Conjecture.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 09:44:28 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Barman", "Siddharth", ""], ["Fawzi", "Omar", ""], ["Ghoshal", "Suprovat", ""], ["G\u00fcrp\u0131nar", "Emirhan", ""]]}, {"id": "1905.00656", "submitter": "Vincent Cohen-Addad", "authors": "Vincent Cohen-Addad and Marcin Pilipczuk and Micha{\\l} Pilipczuk", "title": "Efficient approximation schemes for uniform-cost clustering problems in\n  planar graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the $k$-Median problem on planar graphs: given an edge-weighted\nplanar graph $G$, a set of clients $C \\subseteq V(G)$, a set of facilities $F\n\\subseteq V(G)$, and an integer parameter $k$, the task is to find a set of at\nmost $k$ facilities whose opening minimizes the total connection cost of\nclients, where each client contributes to the cost with the distance to the\nclosest open facility. We give two new approximation schemes for this problem:\n-- FPT Approximation Scheme: for any $\\epsilon>0$, in time\n$2^{O(k\\epsilon^{-3}\\log (k\\epsilon^{-1}))}\\cdot n^{O(1)}$ we can compute a\nsolution that\n  (1) has connection cost at most $(1+\\epsilon)$ times the optimum, with high\nprobability. -- Efficient Bicriteria Approximation Scheme: for any\n$\\epsilon>0$, in time $2^{O(\\epsilon^{-5}\\log (\\epsilon^{-1}))}\\cdot n^{O(1)}$\nwe can compute a set of at most $(1+\\epsilon)k$ facilities\n  (2) whose opening yields connection cost at most $(1+\\epsilon)$ times the\noptimum connection cost for opening at most $k$ facilities, with high\nprobability.\n  As a direct corollary of the second result we obtain an EPTAS for the Uniform\nFacility Location on planar graphs, with same running time.\n  Our main technical tool is a new construction of a \"coreset for facilities\"\nfor $k$-Median in planar graphs: we show that in polynomial time one can\ncompute a subset of facilities $F_0\\subseteq F$ of size $k\\cdot (\\log\nn/\\epsilon)^{O(\\epsilon^{-3})}$ with a guarantee that there is a\n$(1+\\epsilon)$-approximate solution contained in $F_0$.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 10:32:06 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1905.00731", "submitter": "Adam Elmachtoub", "authors": "Omar Besbes, Adam N. Elmachtoub, Yunjie Sun", "title": "Static Pricing: Universal Guarantees for Reusable Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a fundamental pricing model in which a fixed number of units of a\nreusable resource are used to serve customers. Customers arrive to the system\naccording to a stochastic process and upon arrival decide whether or not to\npurchase the service, depending on their willingness-to-pay and the current\nprice. The service time during which the resource is used by the customer is\nstochastic and the firm may incur a service cost. This model represents various\nmarkets for reusable resources such as cloud computing, shared vehicles,\nrotable parts, and hotel rooms. In the present paper, we analyze this pricing\nproblem when the firm attempts to maximize a weighted combination of three\ncentral metrics: profit, market share, and service level. Under Poisson\narrivals, exponential service times, and standard assumptions on the\nwillingness-to-pay distribution, we establish a series of results that\ncharacterize the performance of static pricing in such environments.\n  In particular, while an optimal policy is fully dynamic in such a context, we\nprove that a static pricing policy simultaneously guarantees 78.9% of the\nprofit, market share, and service level from the optimal policy. Notably, this\nresult holds for any service rate and number of units the firm operates. Our\nproof technique relies on a judicious construction of a static price that is\nderived directly from the optimal dynamic pricing policy. In the special case\nwhere there are two units and the induced demand is linear, we also prove that\nthe static policy guarantees 95.5% of the profit from the optimal policy. Our\nnumerical findings on a large testbed of instances suggest that the latter\nresult is quite indicative of the profit obtained by the static pricing policy\nacross all parameters.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 13:38:54 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 22:17:24 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 03:10:33 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Besbes", "Omar", ""], ["Elmachtoub", "Adam N.", ""], ["Sun", "Yunjie", ""]]}, {"id": "1905.00767", "submitter": "Xue Zhu", "authors": "Zhiyi Huang, Xue Zhu", "title": "Scalable and Jointly Differentially Private Packing", "comments": "22 pages, 46th International Colloquium on Automata, Languages and\n  Programming(ICALP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an $(\\epsilon, \\delta)$-jointly differentially private algorithm\nfor packing problems. Our algorithm not only achieves the optimal trade-off\nbetween the privacy parameter $\\epsilon$ and the minimum supply requirement (up\nto logarithmic factors), but is also scalable in the sense that the running\ntime is linear in the number of agents $n$. Previous algorithms either run in\ncubic time in $n$, or require a minimum supply per resource that is $\\sqrt{n}$\ntimes larger than the best possible.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 14:29:39 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Huang", "Zhiyi", ""], ["Zhu", "Xue", ""]]}, {"id": "1905.00812", "submitter": "Xue Zhu", "authors": "Zhiyi Huang, Xue Zhu", "title": "Near Optimal Jointly Private Packing Algorithms via Dual Multiplicative\n  Weight Update", "comments": "18 pages, ACM-SIAM Symposium on Discrete Algorithms (SODA 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an improved $(\\epsilon, \\delta)$-jointly differentially private\nalgorithm for packing problems. Our algorithm gives a feasible output that is\napproximately optimal up to an $\\alpha n$ additive factor as long as the supply\nof each resource is at least $\\tilde{O}(\\sqrt{m} / \\alpha \\epsilon)$, where $m$\nis the number of resources. This improves the previous result by Hsu et\nal.~(SODA '16), which requires the total supply to be at least $\\tilde{O}(m^2 /\n\\alpha \\epsilon)$, and only guarantees approximate feasibility in terms of\ntotal violation. Further, we complement our algorithm with an almost matching\nhardness result, showing that $\\Omega(\\sqrt{m \\ln(1/\\delta)} / \\alpha\n\\epsilon)$ supply is necessary for any $(\\epsilon, \\delta)$-jointly\ndifferentially private algorithm to compute an approximately optimal packing\nsolution. Finally, we introduce an alternative approach that runs in linear\ntime, is exactly truthful, can be implemented online, and can be\n$\\epsilon$-jointly differentially private, but requires a larger supply of each\nresource.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 15:39:34 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Huang", "Zhiyi", ""], ["Zhu", "Xue", ""]]}, {"id": "1905.00848", "submitter": "Georgios Amanatidis", "authors": "Georgios Amanatidis, Pieter Kleer, Guido Sch\\\"afer", "title": "Budget-Feasible Mechanism Design for Non-Monotone Submodular Objectives:\n  Offline and Online", "comments": "Accepted to EC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework of budget-feasible mechanism design studies procurement\nauctions where the auctioneer (buyer) aims to maximize his valuation function\nsubject to a hard budget constraint. We study the problem of designing truthful\nmechanisms that have good approximation guarantees and never pay the\nparticipating agents (sellers) more than the budget. We focus on the case of\ngeneral (non-monotone) submodular valuation functions and derive the first\ntruthful, budget-feasible and $O(1)$-approximate mechanisms that run in\npolynomial time in the value query model, for both offline and online auctions.\nPrior to our work, the only $O(1)$-approximation mechanism known for\nnon-monotone submodular objectives required an exponential number of value\nqueries.\n  At the heart of our approach lies a novel greedy algorithm for non-monotone\nsubmodular maximization under a knapsack constraint. Our algorithm builds two\ncandidate solutions simultaneously (to achieve a good approximation), yet\nensures that agents cannot jump from one solution to the other (to implicitly\nenforce truthfulness). Ours is the first mechanism for the problem\nwhere---crucially---the agents are not ordered with respect to their marginal\nvalue per cost. This allows us to appropriately adapt these ideas to the online\nsetting as well.\n  To further illustrate the applicability of our approach, we also consider the\ncase where additional feasibility constraints are present. We obtain\n$O(p)$-approximation mechanisms for both monotone and non-monotone submodular\nobjectives, when the feasible solutions are independent sets of a $p$-system.\nWith the exception of additive valuation functions, no mechanisms were known\nfor this setting prior to our work. Finally, we provide lower bounds suggesting\nthat, when one cares about non-trivial approximation guarantees in polynomial\ntime, our results are asymptotically best possible.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 16:51:24 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 17:17:56 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Amanatidis", "Georgios", ""], ["Kleer", "Pieter", ""], ["Sch\u00e4fer", "Guido", ""]]}, {"id": "1905.00850", "submitter": "Peilin Zhong", "authors": "Alexandr Andoni, Clifford Stein and Peilin Zhong", "title": "Log Diameter Rounds Algorithms for $2$-Vertex and $2$-Edge Connectivity", "comments": "ICALP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern parallel systems, such as MapReduce, Hadoop and Spark, can be\nmodeled well by the MPC model. The MPC model captures well coarse-grained\ncomputation on large data --- data is distributed to processors, each of which\nhas a sublinear (in the input data) amount of memory and we alternate between\nrounds of computation and rounds of communication, where each machine can\ncommunicate an amount of data as large as the size of its memory. This model is\nstronger than the classical PRAM model, and it is an intriguing question to\ndesign algorithms whose running time is smaller than in the PRAM model.\n  In this paper, we study two fundamental problems, $2$-edge connectivity and\n$2$-vertex connectivity (biconnectivity). PRAM algorithms which run in $O(\\log\nn)$ time have been known for many years. We give algorithms using roughly log\ndiameter rounds in the MPC model. Our main results are, for an $n$-vertex,\n$m$-edge graph of diameter $D$ and bi-diameter $D'$, 1) a $O(\\log\nD\\log\\log_{m/n} n)$ parallel time $2$-edge connectivity algorithm, 2) a $O(\\log\nD\\log^2\\log_{m/n}n+\\log D'\\log\\log_{m/n}n)$ parallel time biconnectivity\nalgorithm, where the bi-diameter $D'$ is the largest cycle length over all the\nvertex pairs in the same biconnected component. Our results are fully scalable,\nmeaning that the memory per processor can be $O(n^{\\delta})$ for arbitrary\nconstant $\\delta>0$, and the total memory used is linear in the problem size.\nOur $2$-edge connectivity algorithm achieves the same parallel time as the\nconnectivity algorithm of Andoni et al. (FOCS 2018). We also show an\n$\\Omega(\\log D')$ conditional lower bound for the biconnectivity problem.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 16:53:55 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Andoni", "Alexandr", ""], ["Stein", "Clifford", ""], ["Zhong", "Peilin", ""]]}, {"id": "1905.00948", "submitter": "Ehsan Kazemi", "authors": "Ehsan Kazemi and Marko Mitrovic and Morteza Zadimoghaddam and Silvio\n  Lattanzi and Amin Karbasi", "title": "Submodular Streaming in All its Glory: Tight Approximation, Minimum\n  Memory and Low Adaptive Complexity", "comments": "Proceedings of the 36th International Conference on Machine Learning,\n  Long Beach, California, PMLR 97, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming algorithms are generally judged by the quality of their solution,\nmemory footprint, and computational complexity. In this paper, we study the\nproblem of maximizing a monotone submodular function in the streaming setting\nwith a cardinality constraint $k$. We first propose Sieve-Streaming++, which\nrequires just one pass over the data, keeps only $O(k)$ elements and achieves\nthe tight $(1/2)$-approximation guarantee. The best previously known streaming\nalgorithms either achieve a suboptimal $(1/4)$-approximation with $\\Theta(k)$\nmemory or the optimal $(1/2)$-approximation with $O(k\\log k)$ memory. Next, we\nshow that by buffering a small fraction of the stream and applying a careful\nfiltering procedure, one can heavily reduce the number of adaptive\ncomputational rounds, thus substantially lowering the computational complexity\nof Sieve-Streaming++. We then generalize our results to the more challenging\nmulti-source streaming setting. We show how one can achieve the tight\n$(1/2)$-approximation guarantee with $O(k)$ shared memory while minimizing not\nonly the required rounds of computations but also the total number of\ncommunicated bits. Finally, we demonstrate the efficiency of our algorithms on\nreal-world data summarization tasks for multi-source streams of tweets and of\nYouTube videos.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 19:58:57 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 16:49:53 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Kazemi", "Ehsan", ""], ["Mitrovic", "Marko", ""], ["Zadimoghaddam", "Morteza", ""], ["Lattanzi", "Silvio", ""], ["Karbasi", "Amin", ""]]}, {"id": "1905.00973", "submitter": "Thibaut Vidal", "authors": "Leandro C. Coelho, Gilbert Laporte, Arinei Lindbeck, Thibaut Vidal", "title": "Benchmark Instances and Branch-and-Cut Algorithm for the Hashiwokakero\n  Puzzle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashiwokakero, or simply Hashi, is a Japanese single-player puzzle played on\na rectangular grid with no standard size. Some cells of the grid contain a\ncircle, called island, with a number inside it ranging from one to eight. The\nremaining positions of the grid are empty. The player must connect all of the\nislands by drawing a series of horizontal or vertical bridges between them,\nrespecting a series of rules: the number of bridges incident to an island\nequals the number indicated in the circle, at most two bridges are incident to\nany side of an island, bridges cannot cross each other or pass through islands,\nand each island must eventually be reachable from any other island. In this\npaper, we present some complexity results and relationships between Hashi and\nwell-known graph theory problems. We give a formulation of the problem by means\nof an integer linear mathematical programming model, and apply a branch-and-cut\nalgorithm to solve the model in which connectivity constraints are dynamically\ngenerated. We also develop a puzzle generator. Our experiments on 1440 Hashi\npuzzles show that the algorithm can consistently solve hard puzzles with up to\n400 islands.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 21:38:54 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Coelho", "Leandro C.", ""], ["Laporte", "Gilbert", ""], ["Lindbeck", "Arinei", ""], ["Vidal", "Thibaut", ""]]}, {"id": "1905.01134", "submitter": "Max Bannach", "authors": "Max Bannach, Sebastian Berndt", "title": "Positive-Instance Driven Dynamic Programming for Graph Searching", "comments": "WADS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on the similarity of a graph to being a tree - called the treewidth\nof the graph - has seen an enormous rise within the last decade, but a\npractically fast algorithm for this task has been discovered only recently by\nTamaki (ESA 2017). It is based on dynamic programming and makes use of the fact\nthat the number of positive subinstances is typically substantially smaller\nthan the number of all subinstances. Algorithms producing only such\nsubinstances are called positive-instance driven (PID). We give an alternative\nand intuitive view on this algorithm from the perspective of the corresponding\nconfiguration graphs in certain two-player games. This allows us to develop\nPID-algorithms for a wide range of important graph parameters such as\ntreewidth, pathwidth, and treedepth. We analyse the worst case behaviour of the\napproach on some well-known graph classes and perform an experimental\nevaluation on real world and random graphs.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 12:03:32 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Bannach", "Max", ""], ["Berndt", "Sebastian", ""]]}, {"id": "1905.01216", "submitter": "Kathrin Hanauer", "authors": "Kathrin Hanauer, Monika Henzinger, and Christian Schulz", "title": "Fully Dynamic Single-Source Reachability in Practice: An Experimental\n  Study", "comments": null, "journal-ref": "Proceedings of the Symposium on Algorithm Engineering and\n  Experiments, ALENEX 2020, Salt Lake City, UT, USA, January 5-6, 2020, pages\n  106-119, 2020", "doi": "10.1137/1.9781611976007.9", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a directed graph and a source vertex, the fully dynamic single-source\nreachability problem is to maintain the set of vertices that are reachable from\nthe given vertex, subject to edge deletions and insertions. It is one of the\nmost fundamental problems on graphs and appears directly or indirectly in many\nand varied applications. While there has been theoretical work on this problem,\nshowing both linear conditional lower bounds for the fully dynamic problem and\ninsertions-only and deletions-only upper bounds beating these conditional lower\nbounds, there has been no experimental study that compares the performance of\nfully dynamic reachability algorithms in practice. Previous experimental\nstudies in this area concentrated only on the more general all-pairs\nreachability or transitive closure problem and did not use real-world dynamic\ngraphs.\n  In this paper, we bridge this gap by empirically studying an extensive set of\nalgorithms for the single-source reachability problem in the fully dynamic\nsetting. In particular, we design several fully dynamic variants of well-known\napproaches to obtain and maintain reachability information with respect to a\ndistinguished source. Moreover, we extend the existing insertions-only or\ndeletions-only upper bounds into fully dynamic algorithms. Even though the\nworst-case time per operation of all the fully dynamic algorithms we evaluate\nis at least linear in the number of edges in the graph (as is to be expected\ngiven the conditional lower bounds) we show in our extensive experimental\nevaluation that their performance differs greatly, both on generated as well as\non real-world instances.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 15:12:00 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 10:14:44 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2020 15:45:19 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Hanauer", "Kathrin", ""], ["Henzinger", "Monika", ""], ["Schulz", "Christian", ""]]}, {"id": "1905.01254", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Rapha\\\"el Clifford, Pawe{\\l} Gawrychowski, Tomasz Kociumaka, Daniel P.\n  Martin and Przemys{\\l}aw Uzna\\'nski", "title": "RLE edit distance in near optimal time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the edit distance between two run-length encoded strings of\ncompressed lengths $m$ and $n$ respectively, can be computed in\n$\\mathcal{O}(mn\\log(mn))$ time. This improves the previous record by a factor\nof $\\mathcal{O}(n/\\log(mn))$. The running time of our algorithm is within\nsubpolynomial factors of being optimal, subject to the standard SETH-hardness\nassumption. This effectively closes a line of algorithmic research first\nstarted in 1993.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 16:19:31 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Clifford", "Rapha\u00ebl", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Kociumaka", "Tomasz", ""], ["Martin", "Daniel P.", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1905.01282", "submitter": "Frederic Koehler", "authors": "Jonathan Kelner, Frederic Koehler, Raghu Meka, Ankur Moitra", "title": "Learning Some Popular Gaussian Graphical Models without Condition Number\n  Bounds", "comments": "V2: Updated version with some new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Graphical Models (GGMs) have wide-ranging applications in machine\nlearning and the natural and social sciences. In most of the settings in which\nthey are applied, the number of observed samples is much smaller than the\ndimension and they are assumed to be sparse. While there are a variety of\nalgorithms (e.g. Graphical Lasso, CLIME) that provably recover the graph\nstructure with a logarithmic number of samples, they assume various conditions\nthat require the precision matrix to be in some sense well-conditioned.\n  Here we give the first polynomial-time algorithms for learning attractive\nGGMs and walk-summable GGMs with a logarithmic number of samples without any\nsuch assumptions. In particular, our algorithms can tolerate strong\ndependencies among the variables. Our result for structure recovery in\nwalk-summable GGMs is derived from a more general result for efficient sparse\nlinear regression in walk-summable models without any norm dependencies. We\ncomplement our results with experiments showing that many existing algorithms\nfail even in some simple settings where there are long dependency chains,\nwhereas ours do not.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 17:26:18 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 03:11:45 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 00:41:15 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Kelner", "Jonathan", ""], ["Koehler", "Frederic", ""], ["Meka", "Raghu", ""], ["Moitra", "Ankur", ""]]}, {"id": "1905.01325", "submitter": "Amr Elmasry", "authors": "Amr Elmasry", "title": "Breaking the Bellman-Ford Shortest-Path Bound", "comments": "There are mistakes in the proofs. The running time bounds may not be\n  correct", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we give a single-source shortest-path algorithm that breaks,\nafter over 60 years, the $O(n \\cdot m)$ time bound for the Bellman-Ford\nalgorithm, where $n$ is the number of vertices and $m$ is the number of arcs of\nthe graph. Our algorithm converts the input graph to a graph with nonnegative\nweights by performing at most $\\min(\\sqrt{n},\\sqrt{m/\\log n})$ calls to\nDijkstra's algorithm, such that the shortest-path tree is the same for the new\ngraph as that for the original. When Dijkstra's algorithm is implemented using\nFibonacci heaps, the running time of our algorithm is therefore $O(\\sqrt{n}\n\\cdot m + n \\cdot \\sqrt{m \\log n})$. We also give a second implementation that\nperforms few calls to Dijkstra's algorithm if the graph contains few negative\narcs on the shortest-path tree.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 18:04:34 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 22:46:14 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Elmasry", "Amr", ""]]}, {"id": "1905.01373", "submitter": "Kobbi Nissim", "authors": "Amos Beimel, Kobbi Nissim, Mohammad Zaheri", "title": "Exploring Differential Obliviousness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper Chan et al. [SODA '19] proposed a relaxation of the notion\nof (full) memory obliviousness, which was introduced by Goldreich and Ostrovsky\n[J. ACM '96] and extensively researched by cryptographers. The new notion,\ndifferential obliviousness, requires that any two neighboring inputs exhibit\nsimilar memory access patterns, where the similarity requirement is that of\ndifferential privacy. Chan et al. demonstrated that differential obliviousness\nallows achieving improved efficiency for several algorithmic tasks, including\nsorting, merging of sorted lists, and range query data structures.\n  In this work, we continue the exploration and mapping of differential\nobliviousness, focusing on algorithms that do not necessarily examine all their\ninput. This choice is motivated by the fact that the existence of logarithmic\noverhead ORAM protocols implies that differential obliviousness can yield at\nmost a logarithmic improvement in efficiency for computations that need to\nexamine all their input. In particular, we explore property testing, where we\nshow that differential obliviousness yields an almost linear improvement in\noverhead in the dense graph model, and at most quadratic improvement in the\nbounded degree model. We also explore tasks where a non-oblivious algorithm\nwould need to explore different portions of the input, where the latter would\ndepend on the input itself, and where we show that such a behavior can be\nmaintained under differential obliviousness, but not under full obliviousness.\nOur examples suggest that there would be benefits in further exploring which\nclass of computational tasks are amenable to differential obliviousness.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 22:33:16 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 21:49:48 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Beimel", "Amos", ""], ["Nissim", "Kobbi", ""], ["Zaheri", "Mohammad", ""]]}, {"id": "1905.01428", "submitter": "Robert Kleinberg", "authors": "Hedyeh Beyhaghi and Robert Kleinberg", "title": "Pandora's Problem with Nonobligatory Inspection", "comments": "This is the full version of a paper accepted to the 2019 ACM\n  Conference on Economics and Computation (EC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Martin Weitzman's \"Pandora's problem\" furnishes the mathematical basis for\noptimal search theory in economics. Nearly 40 years later, Laura Doval\nintroduced a version of the problem in which the searcher is not obligated to\npay the cost of inspecting an alternative's value before selecting it. Unlike\nthe original Pandora's problem, the version with nonobligatory inspection\ncannot be solved optimally by any simple ranking-based policy, and it is\nunknown whether there exists any polynomial-time algorithm to compute the\noptimal policy. This motivates the study of approximately optimal policies that\nare simple and computationally efficient. In this work we provide the first\nnon-trivial approximation guarantees for this problem. We introduce a family of\n\"committing policies\" such that it is computationally easy to find and\nimplement the optimal committing policy. We prove that the optimal committing\npolicy is guaranteed to approximate the fully optimal policy within a\n$1-\\frac1e = 0.63\\ldots$ factor, and for the special case of two boxes we\nimprove this factor to 4/5 and show that this approximation is tight for the\nclass of committing policies.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 04:27:51 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Beyhaghi", "Hedyeh", ""], ["Kleinberg", "Robert", ""]]}, {"id": "1905.01468", "submitter": "Steven Kelk", "authors": "Steven Kelk and Simone Linz", "title": "New reduction rules for the tree bisection and reconnection distance", "comments": "Accepted for journal publication. This version contains extra\n  figures. Keywords: fixed-parameter tractability, tree bisection and\n  reconnection, generator, kernelization, agreement forest, phylogenetic\n  network, phylogenetic tree, hybridization number", "journal-ref": "Annals of Combinatorics, 24:475-502, 2020", "doi": "10.1007/s00026-020-00502-7", "report-no": null, "categories": "cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it was shown that, if the subtree and chain reduction rules have\nbeen applied exhaustively to two unrooted phylogenetic trees, the reduced trees\nwill have at most 15k-9 taxa where k is the TBR (Tree Bisection and\nReconnection) distance between the two trees, and that this bound is tight.\nHere we propose five new reduction rules and show that these further reduce the\nbound to 11k-9. The new rules combine the ``unrooted generator'' approach\nintroduced in [Kelk and Linz 2018] with a careful analysis of agreement forests\nto identify (i) situations when chains of length 3 can be further shortened\nwithout reducing the TBR distance, and (ii) situations when small subtrees can\nbe identified whose deletion is guaranteed to reduce the TBR distance by 1. To\nthe best of our knowledge these are the first reduction rules that strictly\nenhance the reductive power of the subtree and chain reduction rules.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 10:00:09 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 06:51:19 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Kelk", "Steven", ""], ["Linz", "Simone", ""]]}, {"id": "1905.01495", "submitter": "Nikhil Bansal", "authors": "Nikhil Bansal, Ola Svensson, Luca Trevisan", "title": "New Notions and Constructions of Sparsification for Graphs and\n  Hypergraphs", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sparsifier of a graph $G$ (Bencz\\'ur and Karger; Spielman and Teng) is a\nsparse weighted subgraph $\\tilde G$ that approximately retains the cut\nstructure of $G$. For general graphs, non-trivial sparsification is possible\nonly by using weighted graphs in which different edges have different weights.\nEven for graphs that admit unweighted sparsifiers, there are no known\npolynomial time algorithms that find such unweighted sparsifiers.\n  We study a weaker notion of sparsification suggested by Oveis Gharan, in\nwhich the number of edges in each cut $(S,\\bar S)$ is not approximated within a\nmultiplicative factor $(1+\\epsilon)$, but is, instead, approximated up to an\nadditive term bounded by $\\epsilon$ times $d\\cdot |S| + \\text{vol}(S)$, where\n$d$ is the average degree, and $\\text{vol}(S)$ is the sum of the degrees of the\nvertices in $S$. We provide a probabilistic polynomial time construction of\nsuch sparsifiers for every graph, and our sparsifiers have a near-optimal\nnumber of edges $O(\\epsilon^{-2} n {\\rm polylog}(1/\\epsilon))$. We also provide\na deterministic polynomial time construction that constructs sparsifiers with a\nweaker property having the optimal number of edges $O(\\epsilon^{-2} n)$. Our\nconstructions also satisfy a spectral version of the ``additive\nsparsification'' property.\n  Our construction of ``additive sparsifiers'' with $O_\\epsilon (n)$ edges also\nworks for hypergraphs, and provides the first non-trivial notion of\nsparsification for hypergraphs achievable with $O(n)$ hyperedges when\n$\\epsilon$ and the rank $r$ of the hyperedges are constant. Finally, we provide\na new construction of spectral hypergraph sparsifiers, according to the\nstandard definition, with ${\\rm poly}(\\epsilon^{-1},r)\\cdot n\\log n$\nhyperedges, improving over the previous spectral construction (Soma and\nYoshida) that used $\\tilde O(n^3)$ hyperedges even for constant $r$ and\n$\\epsilon$.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 13:58:37 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Bansal", "Nikhil", ""], ["Svensson", "Ola", ""], ["Trevisan", "Luca", ""]]}, {"id": "1905.01498", "submitter": "Rui Portocarrero Sarmento MSc", "authors": "Rui Portocarrero Sarmento, Lu\\'is Lemos, M\\'ario Cordeiro, Giulio\n  Rossetti, Douglas Cardoso", "title": "DynComm R Package -- Dynamic Community Detection for Evolving Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the analysis of dynamics in networks represents a great deal in the\nSocial Network Analysis research area. To support students, teachers,\ndevelopers, and researchers in this work we introduce a novel R package, namely\nDynComm. It is designed to be a multi-language package, that can be used for\ncommunity detection and analysis on dynamic networks. The package introduces\ninterfaces to facilitate further developments and the addition of new and\nfuture developed algorithms to deal with community detection in evolving\nnetworks. This new package has the goal of abstracting the programmatic\ninterface of the algorithms, whether they are written in R or other languages,\nand expose them as functions in R.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 14:14:33 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Sarmento", "Rui Portocarrero", ""], ["Lemos", "Lu\u00eds", ""], ["Cordeiro", "M\u00e1rio", ""], ["Rossetti", "Giulio", ""], ["Cardoso", "Douglas", ""]]}, {"id": "1905.01644", "submitter": "Hendrik Fichtenberger", "authors": "Artur Czumaj, Hendrik Fichtenberger, Pan Peng, Christian Sohler", "title": "Testable Properties in General Graphs and Random Order Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework closely linking the areas of property testing\nand data streaming algorithms in the setting of general graphs. It has been\nrecently shown (Monemizadeh et al. 2017) that for bounded-degree graphs, any\nconstant-query tester can be emulated in the random order streaming model by a\nstreaming algorithm that uses only space required to store a constant number of\nwords. However, in a more natural setting of general graphs, with no\nrestriction on the maximum degree, no such results were known because of our\nlack of understanding of constant-query testers in general graphs and lack of\ntechniques to appropriately emulate in the streaming setting off-line\nalgorithms allowing many high-degree vertices.\n  In this work we advance our understanding on both of these challenges. First,\nwe provide canonical testers for all constant-query testers for general graphs,\nboth, for one-sided and two-sided errors. Such canonizations were only known\nbefore (in the adjacency matrix model) for dense graphs (Goldreich and Trevisan\n2003) and (in the adjacency list model) for bounded degree (di-)graphs\n(Goldreich and Ron 2011, Czumaj et al. 2016). Using the concept of canonical\ntesters, we then prove that every property of general graphs that is\nconstant-query testable with one-sided error can also be tested in\nconstant-space with one-sided error in the random order streaming model.\n  Our results imply, among others, that properties like $(s,t)$\ndisconnectivity, $k$-path-freeness, etc. are constant-space testable in random\norder streams.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 10:06:53 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Czumaj", "Artur", ""], ["Fichtenberger", "Hendrik", ""], ["Peng", "Pan", ""], ["Sohler", "Christian", ""]]}, {"id": "1905.01745", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi and Nisheeth K. Vishnoi", "title": "Faster polytope rounding, sampling, and volume computation via a\n  sublinear \"Ball Walk\"", "comments": "Accepted to IEEE Symposium on Foundations of Computer Science (FOCS)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of \"isotropically rounding\" a polytope\n$K\\subset\\mathbb{R}^n$, that is, computing a linear transformation which makes\nthe uniform distribution on the polytope have roughly identity covariance\nmatrix. We assume $K$ is defined by $m$ linear inequalities, with guarantee\nthat $rB\\subset K\\subset RB$, where $B$ is the unit ball. We introduce a new\nvariant of the ball walk Markov chain and show that, roughly, the expected\nnumber of arithmetic operations per-step of this Markov chain is $O(m)$ that is\nsublinear in the input size $mn$--the per-step time of all prior Markov chains.\nSubsequently, we give a rounding algorithm that succeeds with probability\n$1-\\varepsilon$ in\n$\\tilde{O}(mn^{4.5}\\mbox{polylog}(\\frac{1}{\\varepsilon},\\frac{R}{r}))$\narithmetic operations. This gives a factor of $\\sqrt{n}$ improvement on the\nprevious bound of\n$\\tilde{O}(mn^5\\mbox{polylog}(\\frac{1}{\\varepsilon},\\frac{R}{r}))$ for\nrounding, which uses the hit-and-run algorithm. Since the rounding\npreprocessing step is in many cases the bottleneck in improving sampling or\nvolume computation, our results imply these tasks can also be achieved in\nroughly\n$\\tilde{O}(mn^{4.5}\\mbox{polylog}(\\frac{1}{\\varepsilon},\\frac{R}{r})+mn^4\\delta^{-2})$\noperations for computing the volume of $K$ up to a factor $1+\\delta$ and\n$\\tilde{O}(mn^{4.5}\\mbox{polylog}(\\frac{1}{\\varepsilon},\\frac{R}{r})))$ for\nuniformly sampling on $K$ with TV error $\\varepsilon$. This improves on the\nprevious bounds of\n$\\tilde{O}(mn^5\\mbox{polylog}(\\frac{1}{\\varepsilon},\\frac{R}{r})+mn^4\\delta^{-2})$\nfor volume computation when roughly $m\\geq n^{2.5}$, and\n$\\tilde{O}(mn^5\\mbox{polylog}(\\frac{1}{\\varepsilon},\\frac{R}{r}))$ for sampling\nwhen roughly $m\\geq n^{1.5}$. We achieve this improvement by a novel method of\ncomputing polytope membership, where one avoids checking inequalities estimated\nto have a very low probability of being violated.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 20:52:25 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 02:09:14 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Mangoubi", "Oren", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1905.01748", "submitter": "Saeed Seddighin", "authors": "MohammadTaghi Hajiaghayi, Silvio Lattanzi, Saeed Seddighin, Cliff\n  Stein", "title": "MapReduce Meets Fine-Grained Complexity: MapReduce Algorithms for APSP,\n  Matrix Multiplication, 3-SUM, and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed processing frameworks, such as MapReduce, Hadoop, and Spark are\npopular systems for processing large amounts of data. The design of efficient\nalgorithms in these frameworks is a challenging problem, as the systems both\nrequire parallelism---since datasets are so large that multiple machines are\nnecessary---and limit the degree of parallelism---since the number of machines\ngrows sublinearly in the size of the data. Although MapReduce is over a dozen\nyears old~\\cite{dean2008mapreduce}, many fundamental problems, such as Matrix\nMultiplication, 3-SUM, and All Pairs Shortest Paths,\n  lack efficient MapReduce algorithms. We study these problems in the MapReduce\nsetting. Our main contribution is to exhibit smooth trade-offs between the\nmemory available on each machine, and the total number of machines necessary\nfor each problem. Overall, we take the memory available to each machine as a\nparameter, and aim to minimize the number of rounds and number of machines.\n  In this paper, we build on the well-known MapReduce theoretical framework\ninitiated by Karloff, Suri, and Vassilvitskii ~\\cite{karloff2010model} and give\nalgorithms for many of these problems. The key to efficient algorithms in this\nsetting lies in defining a sublinear number of large (polynomially sized)\nsubproblems, that can then be solved in parallel. We give strategies for\nMapReduce-friendly partitioning, that result in new algorithms for all of the\nabove problems. Specifically, we give constant round algorithms for the\nOrthogonal Vector (OV) and 3-SUM problems, and $O(\\log n)$-round algorithms for\nMatrix Multiplication, All Pairs Shortest Paths (APSP), and Fast Fourier\nTransform (FFT), among others. In all of these we exhibit trade-offs between\nthe number of machines and memory per machine.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 21:14:38 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Hajiaghayi", "MohammadTaghi", ""], ["Lattanzi", "Silvio", ""], ["Seddighin", "Saeed", ""], ["Stein", "Cliff", ""]]}, {"id": "1905.01822", "submitter": "Pradeesha Ashok", "authors": "Akanksha Agrawal, Pradeesha Ashok, Meghana M Reddy, Saket Saurabh,\n  Dolly Yadav", "title": "FPT Algorithms for Conflict-free Coloring of Graphs and Chromatic\n  Terrain Guarding", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present fixed parameter tractable algorithms for the conflict-free\ncoloring problem on graphs. Given a graph $G=(V,E)$, \\emph{conflict-free\ncoloring} of $G$ refers to coloring a subset of $V$ such that for every vertex\n$v$, there is a color that is assigned to exactly one vertex in the closed\nneighborhood of $v$. The \\emph{k-Conflict-free Coloring} problem is to decide\nwhether $G$ can be conflict-free colored using at most $k$ colors. This problem\nis NP-hard even for $k=1$ and therefore under standard complexity theoretic\nassumptions, FPT algorithms do not exist when parameterised by the solution\nsize. We consider the \\emph{k-Conflict-free Coloring} problem parameterised by\nthe treewidth of the graph and show that this problem is fixed parameter\ntractable. We also initiate the study of \\emph{Strong Conflict-free Coloring}\nof graphs. Given a graph $G=(V,E)$, \\emph{strong conflict-free coloring} of $G$\nrefers to coloring a subset of $V$ such that every vertex $v$ has at least one\ncolored vertex in its closed neighborhood and moreover all the colored vertices\nin $v$'s neighborhood have distinct colors. We show that this problem is in FPT\nwhen parameterised by both the treewidth and the solution size. We further\napply these algorithms to get efficient algorithms for a geometric problem\nnamely the Terrain Guarding problem, when parameterised by a structural\nparameter.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 04:29:28 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Agrawal", "Akanksha", ""], ["Ashok", "Pradeesha", ""], ["Reddy", "Meghana M", ""], ["Saurabh", "Saket", ""], ["Yadav", "Dolly", ""]]}, {"id": "1905.02133", "submitter": "Sahil Singla", "authors": "Naveen Garg, Anupam Gupta, Amit Kumar, and Sahil Singla", "title": "Non-clairvoyant Precedence Constrained Scheduling", "comments": "Preliminary version in ICALP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online problem of scheduling jobs on identical machines,\nwhere jobs have precedence constraints. We are interested in the demanding\nsetting where the jobs sizes are not known up-front, but are revealed only upon\ncompletion (the non-clairvoyant setting). Such precedence-constrained\nscheduling problems routinely arise in map-reduce and large-scale optimization.\nIn this paper, we make progress on this problem. For the objective of total\nweighted completion time, we give a constant-competitive algorithm. And for\ntotal weighted flow-time, we give an $O(1/\\epsilon^2)$-competitive algorithm\nunder $(1+\\epsilon)$-speed augmentation and a natural ``no-surprises''\nassumption on release dates of jobs (which we show is necessary in this\ncontext).\n  Our algorithm proceeds by assigning {\\em virtual rates} to all the waiting\njobs, including the ones which are dependent on other uncompleted jobs, and\nthen use these virtual rates to decide on the actual rates of minimal jobs\n(i.e., jobs which do not have dependencies and hence are eligible to run).\nInterestingly, the virtual rates are obtained by allocating time in a fair\nmanner, using a Eisenberg-Gale-type convex program (which we can also solve\noptimally using a primal-dual scheme). The optimality condition of this convex\nprogram allows us to show dual-fitting proofs more easily, without having to\nguess and hand-craft the duals. We feel that this idea of using fair virtual\nrates should have broader applicability in scheduling problems.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 16:39:08 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Garg", "Naveen", ""], ["Gupta", "Anupam", ""], ["Kumar", "Amit", ""], ["Singla", "Sahil", ""]]}, {"id": "1905.02149", "submitter": "Sebastian Wild", "authors": "David Durfee, Yu Gao, Anup B. Rao, Sebastian Wild", "title": "Efficient Second-Order Shape-Constrained Function Fitting", "comments": "accepted for WADS 2019; (v2 fixes various typos)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm to compute a one-dimensional shape-constrained function\nthat best fits given data in weighted-$L_{\\infty}$ norm. We give a single\nalgorithm that works for a variety of commonly studied shape constraints\nincluding monotonicity, Lipschitz-continuity and convexity, and more generally,\nany shape constraint expressible by bounds on first- and/or second-order\ndifferences. Our algorithm computes an approximation with additive error\n$\\varepsilon$ in $O\\left(n \\log \\frac{U}{\\varepsilon} \\right)$ time, where $U$\ncaptures the range of input values. We also give a simple greedy algorithm that\nruns in $O(n)$ time for the special case of unweighted $L_{\\infty}$ convex\nregression. These are the first (near-)linear-time algorithms for\nsecond-order-constrained function fitting. To achieve these results, we use a\nnovel geometric interpretation of the underlying dynamic programming problem.\nWe further show that a generalization of the corresponding problems to directed\nacyclic graphs (DAGs) is as difficult as linear programming.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 17:08:52 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 20:17:51 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Durfee", "David", ""], ["Gao", "Yu", ""], ["Rao", "Anup B.", ""], ["Wild", "Sebastian", ""]]}, {"id": "1905.02298", "submitter": "Pawe{\\l} Gawrychowski", "authors": "Giulia Bernardini, Pawe{\\l} Gawrychowski, Nadia Pisanti, Solon P.\n  Pissis and Giovanna Rosone", "title": "Elastic-Degenerate String Matching via Fast Matrix Multiplication", "comments": "Extended version of paper in ICALP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An elastic-degenerate (ED) string is a sequence of $n$ sets of strings of\ntotal length $N$, which was recently proposed to model a set of similar\nsequences. The ED string matching (EDSM) problem is to find all occurrences of\na pattern of length $m$ in an ED text. An $O(nm^{1.5}\\sqrt{\\log m}+N)$-time\nalgorithm for EDSM is known [Aoyama et al., CPM 2018]. The standard assumption\nin the prior work on this question is that $N$ is substantially larger than\nboth $n$ and $m$, and thus we would like to have a linear dependency on the\nformer. Under this assumption, the natural open problem is whether we can\ndecrease the 1.5 exponent in the time complexity, similarly as in the related\n(but, to the best of our knowledge, not equivalent) word break problem [Backurs\nand Indyk, FOCS 2016].\n  Our starting point is a conditional lower bound for EDSM. We use the popular\ncombinatorial Boolean Matrix Multiplication (BMM) conjecture stating that there\nis no truly subcubic combinatorial algorithm for BMM [Abboud and Williams, FOCS\n2014]. By designing an appropriate reduction we show that a combinatorial\nalgorithm solving the EDSM problem in $O(nm^{1.5-e}+N)$ time, for any $e>0$,\nrefutes this conjecture. Our reduction should be understood as an indication\nthat decreasing the exponent requires fast matrix multiplication.\n  String periodicity and fast Fourier transform are two standard tools in\nstring algorithms. Our main technical contribution is that we successfully\ncombine these tools with fast matrix multiplication to design a\nnon-combinatorial $\\tilde{O}(nm^{\\omega-1}+N)$-time algorithm for EDSM, where\n$\\omega$ denotes the matrix multiplication exponent. To the best of our\nknowledge, we are the first to combine these tools. In particular, using the\nfact that $\\omega<2.373$ [Le Gall, ISSAC 2014; Williams, STOC 2012], we obtain\nan $O(nm^{1.373}+N)$-time algorithm for EDSM.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 00:26:49 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 16:08:03 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 15:57:03 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Bernardini", "Giulia", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Pisanti", "Nadia", ""], ["Pissis", "Solon P.", ""], ["Rosone", "Giovanna", ""]]}, {"id": "1905.02303", "submitter": "Alexander Feldman", "authors": "Alexander Feldman and Johan de Kleer and Ion Matei", "title": "Design Space Exploration as Quantified Satisfaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.ET cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present novel algorithms for design and design space exploration. The\ndesigns discovered by these algorithms are compositions of function types\nspecified in component libraries. Our algorithms reduce the design problem to\nquantified satisfiability and use advanced solvers to find solutions that\nrepresent useful systems.\n  The algorithms we present in this paper are sound and complete and are\nguaranteed to discover correct designs of optimal size, if they exist. We apply\nour method to the design of Boolean systems and discover new and more optimal\nclassical digital and quantum circuits for common arithmetic functions such as\naddition and multiplication.\n  The performance of our algorithms is evaluated through extensive\nexperimentation. We created a benchmark consisting of specifications of\nscalable synthetic digital circuits and real-world mirochips. We have generated\nmultiple circuits functionally equivalent to the ones in the benchmark. The\nquantified satisfiability method shows more than four orders of magnitude\nspeed-up, compared to a generate and test method that enumerates all\nnon-isomorphic circuit topologies.\n  Our approach generalizes circuit optimization. It uses arbitrary component\nlibraries and has applications to areas such as digital circuit design,\ndiagnostics, abductive reasoning, test vector generation, and combinatorial\noptimization.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 00:39:16 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 02:01:25 GMT"}, {"version": "v3", "created": "Sun, 31 Jan 2021 22:55:06 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Feldman", "Alexander", ""], ["de Kleer", "Johan", ""], ["Matei", "Ion", ""]]}, {"id": "1905.02313", "submitter": "Zongchen Chen", "authors": "Zongchen Chen, Santosh S. Vempala", "title": "Optimal Convergence Rate of Hamiltonian Monte Carlo for Strongly\n  Logconcave Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Hamiltonian Monte Carlo (HMC) for sampling from a strongly\nlogconcave density proportional to $e^{-f}$ where $f:\\mathbb{R}^d \\to\n\\mathbb{R}$ is $\\mu$-strongly convex and $L$-smooth (the condition number is\n$\\kappa = L/\\mu$). We show that the relaxation time (inverse of the spectral\ngap) of ideal HMC is $O(\\kappa)$, improving on the previous best bound of\n$O(\\kappa^{1.5})$; we complement this with an example where the relaxation time\nis $\\Omega(\\kappa)$. When implemented using a nearly optimal ODE solver, HMC\nreturns an $\\varepsilon$-approximate point in $2$-Wasserstein distance using\n$\\widetilde{O}((\\kappa d)^{0.5} \\varepsilon^{-1})$ gradient evaluations per\nstep and $\\widetilde{O}((\\kappa d)^{1.5}\\varepsilon^{-1})$ total time.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 01:20:59 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Chen", "Zongchen", ""], ["Vempala", "Santosh S.", ""]]}, {"id": "1905.02316", "submitter": "Loic Landrieu", "authors": "Hugo Raguet and Loic Landrieu", "title": "Parallel Cut Pursuit For Minimization of the Graph Total Variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallel version of the cut-pursuit algorithm for minimizing\nfunctionals involving the graph total variation. We show that the decomposition\nof the iterate into constant connected components, which is at the center of\nthis method, allows for the seamless parallelization of the otherwise costly\ngraph-cut based refinement stage. We demonstrate experimentally the efficiency\nof our method in a wide variety of settings, from simple denoising on huge\ngraphs to more complex inverse problems with nondifferentiable penalties. We\nargue that our approach combines the efficiency of graph-cuts based optimizers\nwith the versatility and ease of parallelization of traditional proximal\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 08:17:08 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Raguet", "Hugo", ""], ["Landrieu", "Loic", ""]]}, {"id": "1905.02322", "submitter": "Yakov Nekrich", "authors": "Timothy M. Chan, Yakov Nekrich, Michiel Smid", "title": "Orthogonal Range Reporting and Rectangle Stabbing for Fat Rectangles", "comments": "extended version of a WADS'19 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study two geometric data structure problems in the special\ncase when input objects or queries are fat rectangles. We show that in this\ncase a significant improvement compared to the general case can be achieved.\n  We describe data structures that answer two- and three-dimensional orthogonal\nrange reporting queries in the case when the query range is a \\emph{fat}\nrectangle. Our two-dimensional data structure uses $O(n)$ words and supports\nqueries in $O(\\log\\log U +k)$ time, where $n$ is the number of points in the\ndata structure, $U$ is the size of the universe and $k$ is the number of points\nin the query range. Our three-dimensional data structure needs\n$O(n\\log^{\\varepsilon}U)$ words of space and answers queries in $O(\\log \\log U\n+ k)$ time. We also consider the rectangle stabbing problem on a set of\nthree-dimensional fat rectangles. Our data structure uses $O(n)$ space and\nanswers stabbing queries in $O(\\log U\\log\\log U +k)$ time.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 02:08:05 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Chan", "Timothy M.", ""], ["Nekrich", "Yakov", ""], ["Smid", "Michiel", ""]]}, {"id": "1905.02354", "submitter": "Zhewei Wei", "authors": "Zhewei Wei, Xiaodong He, Xiaokui Xiao, Sibo Wang, Yu Liu, Xiaoyong Du,\n  Ji-Rong Wen", "title": "PRSim: Sublinear Time SimRank Computation on Large Power-Law Graphs", "comments": "ACM SIGMOD 2019", "journal-ref": null, "doi": "10.1145/3299869.3319873", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  {\\it SimRank} is a classic measure of the similarities of nodes in a graph.\nGiven a node $u$ in graph $G =(V, E)$, a {\\em single-source SimRank query}\nreturns the SimRank similarities $s(u, v)$ between node $u$ and each node $v\n\\in V$. This type of queries has numerous applications in web search and social\nnetworks analysis, such as link prediction, web mining, and spam detection.\nExisting methods for single-source SimRank queries, however, incur query cost\nat least linear to the number of nodes $n$, which renders them inapplicable for\nreal-time and interactive analysis.\n  { This paper proposes \\prsim, an algorithm that exploits the structure of\ngraphs to efficiently answer single-source SimRank queries. \\prsim uses an\nindex of size $O(m)$, where $m$ is the number of edges in the graph, and\nguarantees a query time that depends on the {\\em reverse PageRank} distribution\nof the input graph. In particular, we prove that \\prsim runs in sub-linear time\nif the degree distribution of the input graph follows the power-law\ndistribution, a property possessed by many real-world graphs. Based on the\ntheoretical analysis, we show that the empirical query time of all existing\nSimRank algorithms also depends on the reverse PageRank distribution of the\ngraph.} Finally, we present the first experimental study that evaluates the\nabsolute errors of various SimRank algorithms on large graphs, and we show that\n\\prsim outperforms the state of the art in terms of query time, accuracy, index\nsize, and scalability.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 05:00:00 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Wei", "Zhewei", ""], ["He", "Xiaodong", ""], ["Xiao", "Xiaokui", ""], ["Wang", "Sibo", ""], ["Liu", "Yu", ""], ["Du", "Xiaoyong", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "1905.02358", "submitter": "John Michael Goddard Kallaugher", "authors": "John Kallaugher, Eric Price", "title": "Separations and Equivalences between Turnstile Streaming and Linear\n  Sketching", "comments": "Updated to remove incorrect reference to [Nelson, Yu '19]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A longstanding observation, which was partially proven in\n\\cite{LNW14,AHLW16}, is that any turnstile streaming algorithm can be\nimplemented as a linear sketch (the reverse is trivially true). We study the\nrelationship between turnstile streaming and linear sketching algorithms in\nmore detail, giving both new separations and new equivalences between the two\nmodels.\n  It was shown in \\cite{LNW14} that, if a turnstile algorithm works for\narbitrarily long streams with arbitrarily large coordinates at intermediate\nstages of the stream, then the turnstile algorithm is equivalent to a linear\nsketch. We show separations of the opposite form: if either the stream length\nor the maximum value of the stream are substantially restricted, there exist\nproblems where linear sketching is exponentially harder than turnstile\nstreaming.\n  A further limitation of the \\cite{LNW14} equivalence is that the turnstile\nsketching algorithm is neither explicit nor uniform, but requires an\nexponentially long advice string. We show how to remove this limitation for\ndeterministic streaming algorithms: we give an explicit small-space algorithm\nthat takes the streaming algorithm and computes an equivalent module.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 05:13:30 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 20:30:54 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 00:36:27 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Kallaugher", "John", ""], ["Price", "Eric", ""]]}, {"id": "1905.02367", "submitter": "Samson Zhou", "authors": "Dmitrii Avdiukhin, Slobodan Mitrovi\\'c, Grigory Yaroslavtsev, Samson\n  Zhou", "title": "Adversarially Robust Submodular Maximization under Knapsack Constraints", "comments": "To appear in KDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first adversarially robust algorithm for monotone submodular\nmaximization under single and multiple knapsack constraints with scalable\nimplementations in distributed and streaming settings. For a single knapsack\nconstraint, our algorithm outputs a robust summary of almost optimal (up to\npolylogarithmic factors) size, from which a constant-factor approximation to\nthe optimal solution can be constructed. For multiple knapsack constraints, our\napproximation is within a constant-factor of the best known non-robust\nsolution.\n  We evaluate the performance of our algorithms by comparison to natural\nrobustifications of existing non-robust algorithms under two objectives: 1)\ndominating set for large social network graphs from Facebook and Twitter\ncollected by the Stanford Network Analysis Project (SNAP), 2) movie\nrecommendations on a dataset from MovieLens. Experimental results show that our\nalgorithms give the best objective for a majority of the inputs and show strong\nperformance even compared to offline algorithms that are given the set of\nremovals in advance.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 06:16:53 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Avdiukhin", "Dmitrii", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Yaroslavtsev", "Grigory", ""], ["Zhou", "Samson", ""]]}, {"id": "1905.02383", "submitter": "Jinshuo Dong", "authors": "Jinshuo Dong, Aaron Roth, Weijie J. Su", "title": "Gaussian Differential Privacy", "comments": "v2 revises introduction, adds discussion and fixes some\n  inconsistencies. v3 fixes typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy has seen remarkable success as a rigorous and practical\nformalization of data privacy in the past decade. This privacy definition and\nits divergence based relaxations, however, have several acknowledged\nweaknesses, either in handling composition of private algorithms or in\nanalyzing important primitives like privacy amplification by subsampling.\nInspired by the hypothesis testing formulation of privacy, this paper proposes\na new relaxation, which we term `$f$-differential privacy' ($f$-DP). This\nnotion of privacy has a number of appealing properties and, in particular,\navoids difficulties associated with divergence based relaxations. First, $f$-DP\npreserves the hypothesis testing interpretation. In addition, $f$-DP allows for\nlossless reasoning about composition in an algebraic fashion. Moreover, we\nprovide a powerful technique to import existing results proven for original DP\nto $f$-DP and, as an application, obtain a simple subsampling theorem for\n$f$-DP.\n  In addition to the above findings, we introduce a canonical single-parameter\nfamily of privacy notions within the $f$-DP class that is referred to as\n`Gaussian differential privacy' (GDP), defined based on testing two shifted\nGaussians. GDP is focal among the $f$-DP class because of a central limit\ntheorem we prove. More precisely, the privacy guarantees of \\emph{any}\nhypothesis testing based definition of privacy (including original DP)\nconverges to GDP in the limit under composition. The CLT also yields a\ncomputationally inexpensive tool for analyzing the exact composition of private\nalgorithms.\n  Taken together, this collection of attractive properties render $f$-DP a\nmathematically coherent, analytically tractable, and versatile framework for\nprivate data analysis. Finally, we demonstrate the use of the tools we develop\nby giving an improved privacy analysis of noisy stochastic gradient descent.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 06:57:19 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 17:48:32 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 23:51:04 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Dong", "Jinshuo", ""], ["Roth", "Aaron", ""], ["Su", "Weijie J.", ""]]}, {"id": "1905.02424", "submitter": "Jakub Pawlewicz", "authors": "Marcin Mucha, Jesper Nederlof, Jakub Pawlewicz, Karol W\\k{e}grzycki", "title": "Equal-Subset-Sum Faster Than the Meet-in-the-Middle", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Equal-Subset-Sum problem, we are given a set $S$ of $n$ integers and\nthe problem is to decide if there exist two disjoint nonempty subsets $A,B\n\\subseteq S$, whose elements sum up to the same value. The problem is\nNP-complete. The state-of-the-art algorithm runs in $O^{*}(3^{n/2}) \\le\nO^{*}(1.7321^n)$ time and is based on the meet-in-the-middle technique. In this\npaper, we improve upon this algorithm and give $O^{*}(1.7088^n)$ worst case\nMonte Carlo algorithm. This answers the open problem from Woeginger's\ninspirational survey.\n  Additionally, we analyse the polynomial space algorithm for Equal-Subset-Sum.\nA naive polynomial space algorithm for Equal-Subset-Sum runs in $O^{*}(3^n)$\ntime. With read-only access to the exponentially many random bits, we show a\nrandomized algorithm running in $O^{*}(2.6817^n)$ time and polynomial space.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 09:14:59 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 12:43:44 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Mucha", "Marcin", ""], ["Nederlof", "Jesper", ""], ["Pawlewicz", "Jakub", ""], ["W\u0119grzycki", "Karol", ""]]}, {"id": "1905.02469", "submitter": "Adam Kasperski", "authors": "Marc Goerigk, Adam Kasperski, Pawel Zielinski", "title": "Robust two-stage combinatorial optimization problems under convex\n  uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a class of robust two-stage combinatorial optimization problems\nis discussed. It is assumed that the uncertain second stage costs are specified\nin the form of a convex uncertainty set, in particular polyhedral or\nellipsoidal ones. It is shown that the robust two-stage versions of basic\nnetwork and selection problems are NP-hard, even in a very restrictive cases.\nSome exact and approximation algorithms for the general problem are\nconstructed. Polynomial and approximation algorithms for the robust two-stage\nversions of basic problems, such as the selection and shortest path problems,\nare also provided.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 11:02:37 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Goerigk", "Marc", ""], ["Kasperski", "Adam", ""], ["Zielinski", "Pawel", ""]]}, {"id": "1905.02472", "submitter": "Ingo Van Duijn", "authors": "Chen Avin, Ingo van Duijn, Stefan Schmid", "title": "Self-Adjusting Linear Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging networked systems become increasingly flexible and reconfigurable.\nThis introduces an opportunity to adjust networked systems in a demand-aware\nmanner, leveraging spatial and temporal locality in the workload for online\noptimizations. However, it also introduces a trade-off: while more frequent\nadjustments can improve performance, they also entail higher reconfiguration\ncosts.\n  This paper initiates the formal study of linear networks which self-adjust to\nthe demand in an online manner, striking a balance between the benefits and\ncosts of reconfigurations. We show that the underlying algorithmic problem can\nbe seen as a distributed generalization of the classic dynamic list update\nproblem known from self-adjusting datastructures: in a network, requests can\noccur between node pairs. This distributed version turns out to be\nsignificantly harder than the classical problem in generalizes. Our main\nresults are a $\\Omega(\\log{n})$ lower bound on the competitive ratio, and a\n(distributed) online algorithm that is $O(\\log{n})$-competitive if the\ncommunication requests are issued according to a linear order.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 11:09:10 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Avin", "Chen", ""], ["van Duijn", "Ingo", ""], ["Schmid", "Stefan", ""]]}, {"id": "1905.02589", "submitter": "Alexandre P Francisco", "authors": "Diogo Costa and Lu\\'is M. S. Russo and Rui Henriques and Hideo Bannai\n  and Alexandre P. Francisco", "title": "Order-Preserving Pattern Matching Indeterminate Strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an indeterminate string pattern $p$ and an indeterminate string text\n$t$, the problem of order-preserving pattern matching with character\nuncertainties ($\\mu$OPPM) is to find all substrings of $t$ that satisfy one of\nthe possible orderings defined by $p$. When the text and pattern are\ndeterminate strings, we are in the presence of the well-studied exact\norder-preserving pattern matching (OPPM) problem with diverse applications on\ntime series analysis. Despite its relevance, the exact OPPM problem suffers\nfrom two major drawbacks: 1) the inability to deal with indetermination in the\ntext, thus preventing the analysis of noisy time series; and 2) the inability\nto deal with indetermination in the pattern, thus imposing the strict\nsatisfaction of the orders among all pattern positions. This paper provides the\nfirst polynomial algorithm to answer the $\\mu$OPPM problem when indetermination\nis observed on the pattern or text. Given two strings with length $m$ and\n$O(r)$ uncertain characters per string position, we show that the $\\mu$OPPM\nproblem can be solved in $O(mr\\lg r)$ time when one string is indeterminate and\n$r\\in\\mathbb{N}^+$. Mappings into satisfiability problems are provided when\nindetermination is observed on both the pattern and the text, and results\nconcerning the general problem complexity are presented as well, with $\\mu$OPPM\nproblem proved to be NP-hard in general.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 14:01:52 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Costa", "Diogo", ""], ["Russo", "Lu\u00eds M. S.", ""], ["Henriques", "Rui", ""], ["Bannai", "Hideo", ""], ["Francisco", "Alexandre P.", ""]]}, {"id": "1905.02592", "submitter": "Arnold Filtser", "authors": "Michael Elkin, Arnold Filtser, Ofer Neiman", "title": "Distributed Construction of Light Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $t$-{\\em spanner} $H$ of a weighted graph $G=(V,E,w)$ is a subgraph that\napproximates all pairwise distances up to a factor of $t$. The {\\em lightness}\nof $H$ is defined as the ratio between the weight of $H$ to that of the minimum\nspanning tree. An $(\\alpha,\\beta)$-{\\em Shallow Light Tree} (SLT) is a tree of\nlightness $\\beta$, that approximates all distances from a designated root\nvertex up to a factor of $\\alpha$. A long line of works resulted in efficient\nalgorithms that produce (nearly) optimal light spanners and SLTs.\n  Some of the most notable algorithmic applications of light spanners and SLTs\nare in distributed settings. Surprisingly, so far there are no known efficient\ndistributed algorithms for constructing these objects in general graphs. In\nthis paper we devise efficient distributed algorithms in the CONGEST model for\nconstructing light spanners and SLTs, with near optimal parameters.\nSpecifically, for any $k\\ge 1$ and $0<\\epsilon<1$, we show a\n$(2k-1)\\cdot(1+\\epsilon)$-spanner with lightness $O(k\\cdot n^{1/k})$ can be\nbuilt in $\\tilde{O}\\left(n^{\\frac12+\\frac{1}{4k+2}}+D\\right)$ rounds (where\n$n=|V|$ and $D$ is the hop-diameter of $G$). In addition, for any $\\alpha>1$ we\nprovide an $(\\alpha,1+\\frac{O(1)}{\\alpha-1})$-SLT in $(\\sqrt{n}+D)\\cdot\nn^{o(1)}$ rounds. The running time of our algorithms cannot be substantially\nimproved.\n  We also consider spanners for the family of doubling graphs, and devise a\n$(\\sqrt{n}+D)\\cdot n^{o(1)}$ rounds algorithm in the CONGEST model that\ncomputes a $(1+\\epsilon)$-spanner with lightness $(\\log n)/\\epsilon^{O(1)}$. As\na stepping stone, which is interesting in its own right, we first develop a\ndistributed algorithm for constructing nets (for arbitrary weighted graphs),\ngeneralizing previous algorithms that worked only for unweighted graphs.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 14:02:14 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Elkin", "Michael", ""], ["Filtser", "Arnold", ""], ["Neiman", "Ofer", ""]]}, {"id": "1905.02620", "submitter": "Grigorios Koumoutsos", "authors": "John Iacono, Ben Karsin and Grigorios Koumoutsos", "title": "External Memory Planar Point Location with Fast Updates", "comments": "Appeared in ISAAC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study dynamic planar point location in the External Memory Model or Disk\nAccess Model (DAM). Previous work in this model achieves polylog query and\npolylog amortized update time. We present a data structure with $O( \\log_B^2\nN)$ query time and $O(\\frac{1}{ B^{1-\\epsilon}} \\log_B N)$ amortized update\ntime, where $N$ is the number of segments, $B$ the block size and $\\epsilon$ is\na small positive constant, under the assumption that all faces have constant\nsize. This is a $B^{1-\\epsilon}$ factor faster for updates than the fastest\nprevious structure, and brings the cost of insertion and deletion down to\nsubconstant amortized time for reasonable choices of $N$ and $B$. Our structure\nsolves the problem of vertical ray-shooting queries among a dynamic set of\ninterior-disjoint line segments; this is well-known to solve dynamic planar\npoint location for a connected subdivision of the plane with faces of constant\nsize.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 14:50:10 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 21:09:49 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Iacono", "John", ""], ["Karsin", "Ben", ""], ["Koumoutsos", "Grigorios", ""]]}, {"id": "1905.02709", "submitter": "Manish Raghavan", "authors": "Manish Raghavan, Manish Purohit, Sreenivas Gollupadi", "title": "Hiring Under Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the hiring under uncertainty problem to model the\nquestions faced by hiring committees in large enterprises and universities\nalike. Given a set of $n$ eligible candidates, the decision maker needs to\nchoose the sequence of candidates to make offers so as to hire the $k$ best\ncandidates. However, candidates may choose to reject an offer (for instance,\ndue to a competing offer) and the decision maker has a time limit by which all\npositions must be filled. Given an estimate of the probabilities of acceptance\nfor each candidate, the hiring under uncertainty problem is to design a\nstrategy of making offers so that the total expected value of all candidates\nhired by the time limit is maximized. We provide a 2-approximation algorithm\nfor the setting where offers must be made in sequence, an 8-approximation when\noffers may be made in parallel, and a 10-approximation for the more general\nstochastic knapsack setting with finite probes.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 17:49:06 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Raghavan", "Manish", ""], ["Purohit", "Manish", ""], ["Gollupadi", "Sreenivas", ""]]}, {"id": "1905.02752", "submitter": "Vincent Cicirello", "authors": "Vincent A. Cicirello", "title": "Kendall Tau Sequence Distance: Extending Kendall Tau from Ranks to\n  Sequences", "comments": "Corrections of a few minor typos", "journal-ref": "EAI Endorsed Transactions on Industrial Networks and Intelligent\n  Systems, 7(23), e1, April 2020", "doi": "10.4108/eai.13-7-2018.163925", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An edit distance is a measure of the minimum cost sequence of edit operations\nto transform one structure into another. Edit distance is most commonly\nencountered within the context of strings, where Wagner and Fischer's string\nedit distance is perhaps the most well-known. However, edit distance is not\nlimited to strings. For example, there are several edit distance measures for\npermutations, including Wagner and Fischer's string edit distance since a\npermutation is a special case of a string. However, another edit distance for\npermutations is Kendall tau distance, which is the number of pairwise element\ninversions. On permutations, Kendall tau distance is equivalent to an edit\ndistance with adjacent swap as the edit operation. A permutation is often used\nto represent a total ranking over a set of elements. There exist multiple\nextensions of Kendall tau distance from total rankings (permutations) to\npartial rankings (i.e., where multiple elements may have the same rank), but\nnone of these are suitable for computing distance between sequences. We set out\nto explore extending Kendall tau distance in a different direction, namely from\nthe special case of permutations to the more general case of strings or\nsequences of elements from some finite alphabet. We name our distance metric\nKendall tau sequence distance, and define it as the minimum number of adjacent\nswaps necessary to transform one sequence into the other. We provide two $O(n\n\\lg n)$ algorithms for computing it, and experimentally compare their relative\nperformance. We also provide reference implementations of both algorithms in an\nopen source Java library.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 18:15:27 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 20:58:35 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 18:09:13 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Cicirello", "Vincent A.", ""]]}, {"id": "1905.02800", "submitter": "Sina Yazdanbod", "authors": "Roy Schwartz, Mohit Singh and Sina Yazdanbod", "title": "Online and Offline Greedy Algorithms for Routing with Switching Costs", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the use of high speed circuit switches in large scale data\ncenters, we consider the problem of circuit switch scheduling. In this problem\nwe are given demands between pairs of servers and the goal is to schedule at\nevery time step a matching between the servers while maximizing the total\nsatisfied demand over time. The crux of this scheduling problem is that once\none shifts from one matching to a different one a fixed delay $\\delta$ is\nincurred during which no data can be transmitted.\n  For the offline version of the problem we present a\n$(1-\\frac{1}{e}-\\epsilon)$ approximation ratio (for any constant $\\epsilon\n>0$). Since the natural linear programming relaxation for the problem has an\nunbounded integrality gap, we adopt a hybrid approach that combines the\ncombinatorial greedy with randomized rounding of a different suitable linear\nprogram. For the online version of the problem we present a (bi-criteria) $\n((e-1)/(2e-1)-\\epsilon)$-competitive ratio (for any constant $\\epsilon >0$ )\nthat exceeds time by an additive factor of $O(\\frac{\\delta}{\\epsilon})$. We\nnote that no uni-criteria online algorithm is possible. Surprisingly, we obtain\nthe result by reducing the online version to the offline one.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 20:40:29 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Schwartz", "Roy", ""], ["Singh", "Mohit", ""], ["Yazdanbod", "Sina", ""]]}, {"id": "1905.02805", "submitter": "David Wajc", "authors": "Bernhard Haeupler, David Wajc and Goran Zuzic", "title": "Network Coding Gaps for Completion Times of Multiple Unicasts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study network coding gaps for the problem of makespan minimization of\nmultiple unicasts. In this problem distinct packets at different nodes in a\nnetwork need to be delivered to a destination specific to each packet, as fast\nas possible. The network coding gap specifies how much coding packets together\nin a network can help compared to the more natural approach of routing.\n  While makespan minimization using routing has been intensely studied for the\nmultiple unicasts problem, no bounds on network coding gaps for this problem\nare known. We develop new techniques which allow us to upper bound the network\ncoding gap for the makespan of $k$ unicasts, proving this gap is at most\npolylogarithmic in $k$. Complementing this result, we show there exist\ninstances of $k$ unicasts for which this coding gap is polylogarithmic in $k$.\nOur results also hold for average completion time, and more generally any\n$\\ell_p$ norm of completion times.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 20:58:29 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 20:41:10 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 00:49:00 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Wajc", "David", ""], ["Zuzic", "Goran", ""]]}, {"id": "1905.03037", "submitter": "Sanaz Bahargam", "authors": "Sanaz Bahargam and Theodoros Lappas and Evimaria Terzi", "title": "The Guided Team-Partitioning Problem: Definition, Complexity, and\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long line of literature has focused on the problem of selecting a team of\nindividuals from a large pool of candidates, such that certain constraints are\nrespected, and a given objective function is maximized. Even though extant\nresearch has successfully considered diverse families of objective functions\nand constraints, one of the most common limitations is the focus on the\nsingle-team paradigm. Despite its well-documented applications in multiple\ndomains, this paradigm is not appropriate when the team-builder needs to\npartition the entire population into multiple teams. Team-partitioning tasks\nare very common in an educational setting, in which the teacher has to\npartition the students in her class into teams for collaborative projects. The\ntask also emerges in the context of organizations, when managers need to\npartition the workforce into teams with specific properties to tackle relevant\nprojects. In this work, we extend the team formation literature by introducing\nthe Guided Team-Partitioning (GTP) problem, which asks for the partitioning of\na population into teams such that the centroid of each team is as close as\npossible to a given target vector. As we describe in detail in our work, this\nformulation allows the team-builder to control the composition of the produced\nteams and has natural applications in practical settings. Algorithms for the\nGTP need to simultaneously consider the composition of multiple non-overlapping\nteams that compete for the same population of candidates. This makes the\nproblem considerably more challenging than formulations that focus on the\noptimization of a single team. In fact, we prove that GTP is NP-hard to solve\nand even to approximate. The complexity of the problem motivates us to consider\nefficient algorithmic heuristics, which we evaluate via experiments on both\nreal and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 05:12:32 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Bahargam", "Sanaz", ""], ["Lappas", "Theodoros", ""], ["Terzi", "Evimaria", ""]]}, {"id": "1905.03070", "submitter": "Oded Goldreich", "authors": "Oded Goldreich", "title": "Testing Bipartitness in an Augmented VDF Bounded-Degree Graph Model", "comments": "Added a comment about a forthcoming paper focus on the complexity of\n  approximating the effective support size of distributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent work (ECCC, TR18-171, 2018), we introduced models of testing\ngraph properties in which, in addition to answers to the usual graph-queries,\nthe tester obtains {\\em random vertices drawn according to an arbitrary\ndistribution $D$}. Such a tester is required to distinguish between graphs that\nhave the property and graphs that are far from having the property, {\\em where\nthe distance between graphs is defined based on the unknown vertex distribution\n$D$}. These (\"vertex-distribution free\" (VDF)) models generalize the standard\nmodels in which $D$ is postulated to be uniform on the vertex-set, and they\nwere studies both in the dense graph model and in the bounded-degree graph\nmodel.\n  The focus of the aforementioned work was on testers, called {\\sf strong},\nwhose query complexity depends only on the proximity parameter $\\epsilon$.\nUnfortunately, in the standard bounded-degree graph model, some natural\nproperties such as Bipartiteness do not have strong testers, and others (like\ncycle-freeness) do not have strong testers of one-sided error (whereas\none-sided error was shown inherent to the VDF model). Hence, it was suggested\nto study general (i.e., non-strong) testers of \"sub-linear\" complexity.\n  In this work, we pursue the foregoing suggestion, but do so in a model that\naugments the model presented in the aforementioned work. Specifically, we\nprovide the tester with an evaluation oracle to the unknown distribution $D$,\nin addition to samples of $D$ and oracle access to the tested graph. Our main\nresults are testers for Bipartitness and cycle-freeness, in this augmented\nmodel, having complexity that is almost-linear in the square root of the\n\"effective support size\" of $D$.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 13:48:13 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 11:57:02 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Goldreich", "Oded", ""]]}, {"id": "1905.03134", "submitter": "Ignasi Sau", "authors": "Guilherme C. M. Gomes, Ignasi Sau", "title": "Finding cuts of bounded degree: complexity, FPT and exact algorithms,\n  and kernelization", "comments": "24 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A matching cut is a partition of the vertex set of a graph into two sets $A$\nand $B$ such that each vertex has at most one neighbor in the other side of the\ncut. The MATCHING CUT problem asks whether a graph has a matching cut, and has\nbeen intensively studied in the literature. Motivated by a question posed by\nKomusiewicz et al. [IPEC 2018], we introduce a natural generalization of this\nproblem, which we call $d$-CUT: for a positive integer $d$, a $d$-cut is a\nbipartition of the vertex set of a graph into two sets $A$ and $B$ such that\neach vertex has at most $d$ neighbors across the cut. We generalize (and in\nsome cases, improve) a number of results for the MATCHING CUT problem. Namely,\nwe begin with an NP-hardness reduction for $d$-CUT on $(2d+2)$-regular graphs\nand a polynomial algorithm for graphs of maximum degree at most $d+2$. The\ndegree bound in the hardness result is unlikely to be improved, as it would\ndisprove a long-standing conjecture in the context of internal partitions. We\nthen give FPT algorithms for several parameters: the maximum number of edges\ncrossing the cut, treewidth, distance to cluster, and distance to co-cluster.\nIn particular, the treewidth algorithm improves upon the running time of the\nbest known algorithm for MATCHING CUT. Our main technical contribution,\nbuilding on the techniques of Komusiewicz et al. [IPEC 2018], is a polynomial\nkernel for $d$-CUT for every positive integer $d$, parameterized by the\ndistance to a cluster graph. We also rule out the existence of polynomial\nkernels when parameterizing simultaneously by the number of edges crossing the\ncut, the treewidth, and the maximum degree. Finally, we provide an exact\nexponential algorithm slightly faster than the naive brute force approach\nrunning in time $O^*(2^n)$.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 15:07:28 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Gomes", "Guilherme C. M.", ""], ["Sau", "Ignasi", ""]]}, {"id": "1905.03194", "submitter": "Philipp Fischbeck", "authors": "Katrin Casel, Philipp Fischbeck, Tobias Friedrich, Andreas G\\\"obel,\n  J.A. Gregor Lagodzinski", "title": "Zeros and approximations of Holant polynomials on the complex plane", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present fully polynomial approximation schemes for a broad class of Holant\nproblems with complex edge weights, which we call Holant polynomials. We\ntransform these problems into partition functions of abstract combinatorial\nstructures known as polymers in statistical physics. Our method involves\nestablishing zero-free regions for the partition functions of polymer models\nand using the most significant terms of the cluster expansion to approximate\nthem.\n  Results of our technique include new approximation and sampling algorithms\nfor a diverse class of Holant polynomials in the low-temperature regime and\napproximation algorithms for general Holant problems with small signature\nweights. Additionally, we give randomised approximation and sampling algorithms\nwith faster running times for more restrictive classes. Finally, we improve the\nknown zero-free regions for a perfect matching polynomial.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 16:25:00 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 12:45:50 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 13:22:46 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Casel", "Katrin", ""], ["Fischbeck", "Philipp", ""], ["Friedrich", "Tobias", ""], ["G\u00f6bel", "Andreas", ""], ["Lagodzinski", "J. A. Gregor", ""]]}, {"id": "1905.03204", "submitter": "Delia Fano Yela", "authors": "Delia Fano Yela, Florian Thalmann, Vincenzo Nicosia, Dan Stowell, and\n  Mark Sandler", "title": "Efficient On-line Computation of Visibility Graphs", "comments": "code https://github.com/delialia/bst", "journal-ref": "Phys. Rev. Research 2, 023069 (2020)", "doi": "10.1103/PhysRevResearch.2.023069", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A visibility algorithm maps time series into complex networks following a\nsimple criterion. The resulting visibility graph has recently proven to be a\npowerful tool for time series analysis. However its straightforward computation\nis time-consuming and rigid, motivating the development of more efficient\nalgorithms. Here we present a highly efficient method to compute visibility\ngraphs with the further benefit of flexibility: on-line computation. We propose\nan encoder/decoder approach, with an on-line adjustable binary search tree\ncodec for time series as well as its corresponding decoder for visibility\ngraphs. The empirical evidence suggests the proposed method for computation of\nvisibility graphs offers an on-line computation solution at no additional\ncomputation time cost. The source code is available online.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 16:44:09 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Yela", "Delia Fano", ""], ["Thalmann", "Florian", ""], ["Nicosia", "Vincenzo", ""], ["Stowell", "Dan", ""], ["Sandler", "Mark", ""]]}, {"id": "1905.03379", "submitter": "Diptapriyo Majumdar", "authors": "Diptapriyo Majumdar, M. S. Ramanujan, Saket Saurabh", "title": "On the Approximate Compressibility of Connected Vertex Cover", "comments": "1 figure; Revisions from the previous version incorporated based on\n  the comments from some anonymous reviewers", "journal-ref": "Algorithmica, 2020", "doi": "10.1007/s00453-020-00708-4", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Connected Vertex Cover problem, where the goal is to compute a minimum\nset of vertices in a given graph which forms a vertex cover and induces a\nconnected subgraph, is a fundamental combinatorial problem and has received\nextensive attention in various subdomains of algorithmics. In the area of\nkernelization, it is known that this problem is unlikely to have efficient\npreprocessing algorithms, also known as polynomial kernelizations. However, it\nhas been shown in a recent work of Lokshtanov et al. [STOC 2017] that if one\nconsidered an appropriate notion of approximate kernelization, then this\nproblem parameterized by the solution size does admit an approximate polynomial\nkernelization. In fact, Lokhtanov et al. were able to obtain a polynomial size\napproximate kernelization scheme (PSAKS) for Connected Vertex Cover\nparameterized by the solution size. A PSAKS is essentially a preprocessing\nalgorithm whose error can be made arbitrarily close to 0. In this paper we\nrevisit this problem, and consider parameters that are strictly smaller than\nthe size of the solution and obtain the first polynomial size approximate\nkernelization schemes for the Connected Vertex Cover problem when parameterized\nby the deletion distance of the input graph to the class of cographs, the class\nof bounded treewidth graphs, and the class of all chordal graphs.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 22:19:20 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 16:03:35 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 21:02:53 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Majumdar", "Diptapriyo", ""], ["Ramanujan", "M. S.", ""], ["Saurabh", "Saket", ""]]}, {"id": "1905.03424", "submitter": "Abhinav Nellore", "authors": "Abhinav Nellore, Austin Nguyen, and Reid F. Thompson", "title": "An invertible transform for efficient string matching in labeled\n  digraphs", "comments": "13 pages, 3 figures; v7 is the content of the camera-ready copy for\n  CPM 2021 incorporating reviewer feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G = (V, E)$ be a digraph where each vertex is unlabeled, each edge is\nlabeled by a character in some alphabet $\\Omega$, and any two edges with both\nthe same head and the same tail have different labels. The powerset\nconstruction gives a transform of $G$ into a weakly connected digraph $G' =\n(V', E')$ that enables solving the decision problem of whether there is a walk\nin $G$ matching an arbitrarily long query string $q$ in time linear in $|q|$\nand independent of $|E|$ and $|V|$. We show $G$ is uniquely determined by $G'$\nwhen for every $v_\\ell \\in V$, there is some distinct string $s_\\ell$ on\n$\\Omega$ such that $v_\\ell$ is the origin of a closed walk in $G$ matching\n$s_\\ell$, and no other walk in $G$ matches $s_\\ell$ unless it starts and ends\nat $v_\\ell$. We then exploit this invertibility condition to strategically\nalter any $G$ so its transform $G'$ enables retrieval of all $t$ terminal\nvertices of walks in the unaltered $G$ matching $q$ in $O(|q| + t \\log |V|)$\ntime. We conclude by proposing two defining properties of a class of transforms\nthat includes the Burrows-Wheeler transform and the transform presented here.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 03:08:32 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 18:44:19 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 15:22:49 GMT"}, {"version": "v4", "created": "Tue, 26 Jan 2021 18:41:46 GMT"}, {"version": "v5", "created": "Mon, 1 Feb 2021 16:59:07 GMT"}, {"version": "v6", "created": "Fri, 5 Feb 2021 08:27:02 GMT"}, {"version": "v7", "created": "Mon, 12 Apr 2021 08:43:38 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Nellore", "Abhinav", ""], ["Nguyen", "Austin", ""], ["Thompson", "Reid F.", ""]]}, {"id": "1905.03427", "submitter": "Luiz F. O. Moura Santos", "authors": "Luiz F. O. Moura Santos, Hugo T. Y. Yoshizaki, Claudio B. Cunha", "title": "Variable Neighborhood Search for the Bin Packing Problem with Compatible\n  Categories", "comments": "2018 SCALE Latin American Conference, Boston, USA, April 15th-16th,\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bin Packing with Conflicts (BPC) are problems in which items with\ncompatibility constraints must be packed in the least number of bins, not\nexceeding the capacity of the bins and ensuring that non-conflicting items are\npacked in each bin. In this work, we introduce the Bin Packing Problem with\nCompatible Categories (BPCC), a variant of the BPC in which items belong to\nconflicting or compatible categories, in opposition to the item-by-item\nincompatibility found in previous literature. It is a common problem in the\ncontext of last mile distribution to nanostores located in densely populated\nareas. To efficiently solve real-life sized instances of the problem, we\npropose a Variable Neighborhood Search (VNS) metaheuristic algorithm.\nComputational experiments suggest that the algorithm yields good solutions in\nvery short times while compared to linear integer programming running on a\nhigh-performance computing environment.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 03:14:15 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Santos", "Luiz F. O. Moura", ""], ["Yoshizaki", "Hugo T. Y.", ""], ["Cunha", "Claudio B.", ""]]}, {"id": "1905.03446", "submitter": "Andrii Arman", "authors": "Andrii Arman, Pu Gao and Nicholas Wormald", "title": "Fast uniform generation of random graphs with given degree sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide an algorithm that generates a graph with given\ndegree sequence uniformly at random. Provided that $\\Delta^4=O(m)$, where\n$\\Delta$ is the maximal degree and $m$ is the number of edges,the algorithm\nruns in expected time $O(m)$. Our algorithm significantly improves the\npreviously most efficient uniform sampler, which runs in expected time\n$O(m^2\\Delta^2)$ for the same family of degree sequences. Our method uses a\nnovel ingredient which progressively relaxes restrictions on an object being\ngenerated uniformly at random, and we use this to give fast algorithms for\nuniform sampling of graphs with other degree sequences as well. Using the same\nmethod, we also obtain algorithms with expected run time which is (i) linear\nfor power-law degree sequences in cases where the previous best was\n$O(n^{4.081})$, and (ii) $O(nd+d^4)$ for $d$-regular graphs when $d=o(\\sqrt\nn)$, where the previous best was $O(nd^3)$.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 05:22:40 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 00:06:10 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 08:21:58 GMT"}, {"version": "v4", "created": "Thu, 21 Jan 2021 19:53:46 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Arman", "Andrii", ""], ["Gao", "Pu", ""], ["Wormald", "Nicholas", ""]]}, {"id": "1905.03525", "submitter": "Peter Sanders", "authors": "Lorenz H\\\"ubschle-Schneider, Peter Sanders", "title": "Linear Work Generation of R-MAT Graphs", "comments": null, "journal-ref": "Net Sci 8 (2020) 543-550", "doi": "10.1017/nws.2020.21", "report-no": null, "categories": "cs.DS cs.DC cs.DM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  R-MAT is a simple, widely used recursive model for generating `complex\nnetwork' graphs with a power law degree distribution and community structure.\nWe make R-MAT even more useful by reducing the required work per edge from\nlogarithmic to constant. The algorithm works in an embarrassingly parallel way.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 10:46:34 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["H\u00fcbschle-Schneider", "Lorenz", ""], ["Sanders", "Peter", ""]]}, {"id": "1905.03631", "submitter": "Astrid Pieterse", "authors": "Eva-Maria C. Hols, Stefan Kratsch, and Astrid Pieterse", "title": "Elimination Distances, Blocking Sets, and Kernels for Vertex Cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Vertex Cover problem plays an essential role in the study of polynomial\nkernelization in parameterized complexity, i.e., the study of provable and\nefficient preprocessing for NP-hard problems. Motivated by the great variety of\npositive and negative results for kernelization for Vertex Cover subject to\ndifferent parameters and graph classes, we seek to unify and generalize them\nusing so-called blocking sets, which have played implicit and explicit roles in\nmany results.\n  We show that in the most-studied setting, parameterized by the size of a\ndeletion set to a specified graph class $\\mathcal{C}$, bounded minimal blocking\nset size is necessary but not sufficient to get a polynomial kernelization.\nUnder mild technical assumptions, bounded minimal blocking set size is showed\nto allow an essentially tight efficient reduction in the number of connected\ncomponents.\n  We then determine the exact maximum size of minimal blocking sets for graphs\nof bounded elimination distance to any hereditary class $\\mathcal{C}$,\nincluding the case of graphs of bounded treedepth. We get similar but not tight\nbounds for certain non-hereditary classes $\\mathcal{C}$, including the class\n$\\mathcal{C}_{LP}$ of graphs where integral and fractional vertex cover size\ncoincide. These bounds allow us to derive polynomial kernels for Vertex Cover\nparameterized by the size of a deletion set to graphs of bounded elimination\ndistance to, e.g., forest, bipartite, or $\\mathcal{C}_{LP}$ graphs.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 13:47:17 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Hols", "Eva-Maria C.", ""], ["Kratsch", "Stefan", ""], ["Pieterse", "Astrid", ""]]}, {"id": "1905.03643", "submitter": "Lars Jaffke", "authors": "Hans L. Bodlaender, Lars Jaffke, Jan Arne Telle", "title": "Typical Sequences Revisited --- Computing Width Parameters of Graphs", "comments": "30 pages, 10 figures; accepted at STACS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we give a structural lemma on merges of typical sequences, a\nnotion that was introduced in 1991 [Lagergren and Arnborg, Bodlaender and\nKloks, both ICALP 1991] to obtain constructive linear time parameterized\nalgorithms for treewidth and pathwidth. The lemma addresses a runtime\nbottleneck in those algorithms but so far it does not lead to asymptotically\nfaster algorithms. However, we apply the lemma to show that the cutwidth and\nthe modified cutwidth of series parallel digraphs can be computed in\n$\\mathcal{O}(n^2)$ time.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 14:06:26 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 13:45:26 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2020 14:58:54 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Bodlaender", "Hans L.", ""], ["Jaffke", "Lars", ""], ["Telle", "Jan Arne", ""]]}, {"id": "1905.03645", "submitter": "Tomas Balyo", "authors": "Kai Fieger, Tomas Balyo, Christian Schulz, Dominik Schreiber", "title": "Finding Optimal Longest Paths by Dynamic Programming in Parallel", "comments": "arXiv admin note: substantial text overlap with arXiv:1702.04170", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an exact algorithm for solving the longest simple path problem\nbetween two given vertices in undirected weighted graphs. By using graph\npartitioning and dynamic programming, we obtain an algorithm that is\nsignificantly faster than other state-of-the-art methods. This enables us to\nsolve instances that were previously unsolved and solve hard instances\nsignificantly faster. Lastly, we present a scalable parallelization which\nyields the first efficient parallel algorithm for the problem.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 15:33:07 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Fieger", "Kai", ""], ["Balyo", "Tomas", ""], ["Schulz", "Christian", ""], ["Schreiber", "Dominik", ""]]}, {"id": "1905.03674", "submitter": "Brandon Fain", "authors": "Xingyu Chen, Brandon Fain, Liang Lyu, Kamesh Munagala", "title": "Proportionally Fair Clustering", "comments": "To appear in ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the fair machine learning literature by considering the problem of\nproportional centroid clustering in a metric context. For clustering $n$ points\nwith $k$ centers, we define fairness as proportionality to mean that any $n/k$\npoints are entitled to form their own cluster if there is another center that\nis closer in distance for all $n/k$ points. We seek clustering solutions to\nwhich there are no such justified complaints from any subsets of agents,\nwithout assuming any a priori notion of protected subsets. We present and\nanalyze algorithms to efficiently compute, optimize, and audit proportional\nsolutions. We conclude with an empirical examination of the tradeoff between\nproportional solutions and the $k$-means objective.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 14:58:43 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 18:19:34 GMT"}, {"version": "v3", "created": "Sun, 11 Oct 2020 19:51:12 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chen", "Xingyu", ""], ["Fain", "Brandon", ""], ["Lyu", "Liang", ""], ["Munagala", "Kamesh", ""]]}, {"id": "1905.03718", "submitter": "Yanhao Wang", "authors": "Yanhao Wang, Yuchen Li, Kian-Lee Tan", "title": "Coresets for Minimum Enclosing Balls over Sliding Windows", "comments": "28 pages, 10 figures, to appear in The 25th ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining (KDD '19)", "journal-ref": null, "doi": "10.1145/3292500.3330826", "report-no": null, "categories": "cs.DS cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  \\emph{Coresets} are important tools to generate concise summaries of massive\ndatasets for approximate analysis. A coreset is a small subset of points\nextracted from the original point set such that certain geometric properties\nare preserved with provable guarantees. This paper investigates the problem of\nmaintaining a coreset to preserve the minimum enclosing ball (MEB) for a\nsliding window of points that are continuously updated in a data stream.\nAlthough the problem has been extensively studied in batch and append-only\nstreaming settings, no efficient sliding-window solution is available yet. In\nthis work, we first introduce an algorithm, called AOMEB, to build a coreset\nfor MEB in an append-only stream. AOMEB improves the practical performance of\nthe state-of-the-art algorithm while having the same approximation ratio.\nFurthermore, using AOMEB as a building block, we propose two novel algorithms,\nnamely SWMEB and SWMEB+, to maintain coresets for MEB over the sliding window\nwith constant approximation ratios. The proposed algorithms also support\ncoresets for MEB in a reproducing kernel Hilbert space (RKHS). Finally,\nextensive experiments on real-world and synthetic datasets demonstrate that\nSWMEB and SWMEB+ achieve speedups of up to four orders of magnitude over the\nstate-of-the-art batch algorithm while providing coresets for MEB with rather\nsmall errors compared to the optimal ones.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 15:55:03 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 05:11:48 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Wang", "Yanhao", ""], ["Li", "Yuchen", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "1905.03838", "submitter": "Pavel Vesel\\'y", "authors": "Graham Cormode and Pavel Vesel\\'y", "title": "Tight Lower Bound for Comparison-Based Quantile Summaries", "comments": "20 pages, 2 figures, major revison of the construction (Sec. 3) and\n  some other parts of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantiles, such as the median or percentiles, provide concise and useful\ninformation about the distribution of a collection of items, drawn from a\ntotally ordered universe. We study data structures, called quantile summaries,\nwhich keep track of all quantiles, up to an error of at most $\\varepsilon$.\nThat is, an $\\varepsilon$-approximate quantile summary first processes a stream\nof items and then, given any quantile query $0\\le \\phi\\le 1$, returns an item\nfrom the stream, which is a $\\phi'$-quantile for some $\\phi' = \\phi \\pm\n\\varepsilon$. We focus on comparison-based quantile summaries that can only\ncompare two items and are otherwise completely oblivious of the universe.\n  The best such deterministic quantile summary to date, due to Greenwald and\nKhanna (SIGMOD '01), stores at most $O(\\frac{1}{\\varepsilon}\\cdot \\log\n\\varepsilon N)$ items, where $N$ is the number of items in the stream. We prove\nthat this space bound is optimal by showing a matching lower bound. Our result\nthus rules out the possibility of constructing a deterministic comparison-based\nquantile summary in space $f(\\varepsilon)\\cdot o(\\log N)$, for any function $f$\nthat does not depend on $N$. As a corollary, we improve the lower bound for\nbiased quantiles, which provide a stronger, relative-error guarantee of $(1\\pm\n\\varepsilon)\\cdot \\phi$, and for other related computational tasks.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 20:08:04 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 16:57:01 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Cormode", "Graham", ""], ["Vesel\u00fd", "Pavel", ""]]}, {"id": "1905.04106", "submitter": "Arnaud Casteigts", "authors": "Arnaud Casteigts, Swan Dubois, Franck Petit, John M. Robson", "title": "Robustness: a New Form of Heredity Motivated by Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a special case of hereditary property in graphs, referred to\nas {\\em robustness}. A property (or structure) is called robust in a graph $G$\nif it is inherited by all the connected spanning subgraphs of $G$. We motivate\nthis definition using two different settings of dynamic networks. The first\ncorresponds to networks of low dynamicity, where some links may be permanently\nremoved so long as the network remains connected. The second corresponds to\nhighly-dynamic networks, where communication links appear and disappear\narbitrarily often, subject only to the requirement that the entities are\ntemporally connected in a recurrent fashion ({\\it i.e.} they can always reach\neach other through temporal paths). Each context induces a different\ninterpretation of the notion of robustness.\n  We start by motivating the definition and discussing the two interpretations,\nafter what we consider the notion independently from its interpretation, taking\nas our focus the robustness of {\\em maximal independent sets} (MIS). A graph\nmay or may not admit a robust MIS. We characterize the set of graphs \\forallMIS\nin which {\\em all} MISs are robust. Then, we turn our attention to the graphs\nthat {\\em admit} a robust MIS (\\existsMIS). This class has a more complex\nstructure; we give a partial characterization in terms of elementary graph\nproperties, then a complete characterization by means of a (polynomial time)\ndecision algorithm that accepts if and only if a robust MIS exists. This\nalgorithm can be adapted to construct such a solution if one exists.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 12:18:48 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Casteigts", "Arnaud", ""], ["Dubois", "Swan", ""], ["Petit", "Franck", ""], ["Robson", "John M.", ""]]}, {"id": "1905.04124", "submitter": "Kirill Simonov", "authors": "Fedor V. Fomin, Petr A. Golovach, Fahad Panolan, Kirill Simonov", "title": "Refined Complexity of PCA with Outliers", "comments": "To be presented at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is one of the most fundamental procedures\nin exploratory data analysis and is the basic step in applications ranging from\nquantitative finance and bioinformatics to image analysis and neuroscience.\nHowever, it is well-documented that the applicability of PCA in many real\nscenarios could be constrained by an \"immune deficiency\" to outliers such as\ncorrupted observations. We consider the following algorithmic question about\nthe PCA with outliers. For a set of $n$ points in $\\mathbb{R}^{d}$, how to\nlearn a subset of points, say 1% of the total number of points, such that the\nremaining part of the points is best fit into some unknown $r$-dimensional\nsubspace? We provide a rigorous algorithmic analysis of the problem. We show\nthat the problem is solvable in time $n^{O(d^2)}$. In particular, for constant\ndimension the problem is solvable in polynomial time. We complement the\nalgorithmic result by the lower bound, showing that unless Exponential Time\nHypothesis fails, in time $f(d)n^{o(d)}$, for any function $f$ of $d$, it is\nimpossible not only to solve the problem exactly but even to approximate it\nwithin a constant factor.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 12:40:31 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Panolan", "Fahad", ""], ["Simonov", "Kirill", ""]]}, {"id": "1905.04162", "submitter": "Alexandre Teiller", "authors": "Evripidis Bampis, Bruno Escoffier, Kevin Schewior, Alexandre Teiller", "title": "Online Multistage Subset Maximization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous combinatorial optimization problems (knapsack, maximum-weight\nmatching, etc.) can be expressed as \\emph{subset maximization problems}: One is\ngiven a ground set $N=\\{1,\\dots,n\\}$, a collection $\\mathcal{F}\\subseteq 2^N$\nof subsets thereof such that $\\emptyset\\in\\mathcal{F}$, and an objective\n(profit) function $p:\\mathcal{F}\\rightarrow\\mathbb{R}_+$. The task is to choose\na set $S\\in\\mathcal{F}$ that maximizes $p(S)$. We consider the\n\\emph{multistage} version (Eisenstat et al., Gupta et al., both ICALP 2014) of\nsuch problems: The profit function $p_t$ (and possibly the set of feasible\nsolutions $\\mathcal{F}_t$) may change over time. Since in many applications\nchanging the solution is costly, the task becomes to find a sequence of\nsolutions that optimizes the trade-off between good per-time solutions and\nstable solutions taking into account an additional similarity bonus. As\nsimilarity measure for two consecutive solutions, we consider either the size\nof the intersection of the two solutions or the difference of $n$ and the\nHamming distance between the two characteristic vectors. We study multistage\nsubset maximization problems in the \\emph{online} setting, that is, $p_t$\n(along with possibly $\\mathcal{F}_t$) only arrive one by one and, upon such an\narrival, the online algorithm has to output the corresponding solution without\nknowledge of the future. We develop general techniques for online multistage\nsubset maximization and thereby characterize those models (given by the type of\ndata evolution and the type of similarity measure) that admit a\nconstant-competitive online algorithm. When no constant competitive ratio is\npossible, we employ lookahead to circumvent this issue. When a constant\ncompetitive ratio is possible, we provide almost matching lower and upper\nbounds on the best achievable one.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 13:28:23 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Bampis", "Evripidis", ""], ["Escoffier", "Bruno", ""], ["Schewior", "Kevin", ""], ["Teiller", "Alexandre", ""]]}, {"id": "1905.04165", "submitter": "Roland Wittler", "authors": "Roland Wittler", "title": "Alignment- and reference-free phylogenomics with colored de-Bruijn\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new whole-genome based approach to infer large-scale phylogenies\nthat is alignment- and reference-free. In contrast to other methods, it does\nnot rely on pairwise comparisons to determine distances to infer edges in a\ntree. Instead, a colored de-Bruijn graph is constructed, and information on\ncommon subsequences is extracted to infer phylogenetic splits. Application to\ndifferent datasets confirms robustness of the approach. A comparison to other\nstate-of-the-art whole-genome based methods indicates comparable or higher\naccuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 13:33:06 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 08:00:21 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Wittler", "Roland", ""]]}, {"id": "1905.04209", "submitter": "Martin Cooper", "authors": "Martin C. Cooper, Achref El Mouelhi and Cyril Terrioux", "title": "Variable elimination in binary CSPs", "comments": "38 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate rules which allow variable elimination in binary CSP\n(constraint satisfaction problem) instances while conserving satisfiability. We\nstudy variable-elimination rules based on the language of forbidden patterns\nenriched with counting and quantification over variables and values. We propose\nnew rules and compare them, both theoretically and experimentally. We give\noptimised algorithms to apply these rules and show that each define a novel\ntractable class. Using our variable-elimination rules in preprocessing allowed\nus to solve more benchmark problems than without.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 15:12:42 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Cooper", "Martin C.", ""], ["Mouelhi", "Achref El", ""], ["Terrioux", "Cyril", ""]]}, {"id": "1905.04219", "submitter": "Jiehua Chen", "authors": "Matthias Bentert and Jiehua Chen and Vincent Froese and Gerhard J.\n  Woeginger", "title": "Good Things Come to Those Who Swap Objects on Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a simple exchange market, introduced by Gourv\\'{e}s, Lesca and\nWilczynski (IJCAI-17), where every agent initially holds a single object. The\nagents have preferences over the objects, and two agents may swap their objects\nif they both prefer the object of the other agent. The agents live in an\nunderlying social network that governs the structure of the swaps: Two agents\ncan only swap their objects if they are adjacent. We investigate the REACHABLE\nOBJECT problem, which asks whether a given starting situation can ever lead, by\nmeans of a sequence of swaps, to a situation where a given agent obtains a\ngiven object. Our results answer several central open questions on the\ncomplexity of REACHABLE OBJECT. First, the problem is polynomial-time solvable\nif the social network is a path. Second, the problem is NP-hard on cliques and\ngeneralized caterpillars. Finally, we establish a three-versus-four dichotomy\nresult for preference lists of bounded length: The problem is easy if all\npreference lists have length at most three, and the problem becomes NP-hard\neven if all agents have preference lists of length at most four.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 15:30:36 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Bentert", "Matthias", ""], ["Chen", "Jiehua", ""], ["Froese", "Vincent", ""], ["Woeginger", "Gerhard J.", ""]]}, {"id": "1905.04447", "submitter": "Zhao Song", "authors": "Yin Tat Lee, Zhao Song, Qiuyi Zhang", "title": "Solving Empirical Risk Minimization in the Current Matrix Multiplication\n  Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Many convex problems in machine learning and computer science share the same\nform: \\begin{align*} \\min_{x} \\sum_{i} f_i( A_i x + b_i), \\end{align*} where\n$f_i$ are convex functions on $\\mathbb{R}^{n_i}$ with constant $n_i$, $A_i \\in\n\\mathbb{R}^{n_i \\times d}$, $b_i \\in \\mathbb{R}^{n_i}$ and $\\sum_i n_i = n$.\nThis problem generalizes linear programming and includes many problems in\nempirical risk minimization. In this paper, we give an algorithm that runs in\ntime \\begin{align*} O^* ( ( n^{\\omega} + n^{2.5 - \\alpha/2} + n^{2+ 1/6} ) \\log\n(n / \\delta) ) \\end{align*} where $\\omega$ is the exponent of matrix\nmultiplication, $\\alpha$ is the dual exponent of matrix multiplication, and\n$\\delta$ is the relative accuracy. Note that the runtime has only a log\ndependence on the condition numbers or other data dependent parameters and\nthese are captured in $\\delta$. For the current bound $\\omega \\sim 2.38$\n[Vassilevska Williams'12, Le Gall'14] and $\\alpha \\sim 0.31$ [Le Gall,\nUrrutia'18], our runtime $O^* ( n^{\\omega} \\log (n / \\delta))$ matches the\ncurrent best for solving a dense least squares regression problem, a special\ncase of the problem we consider. Very recently, [Alman'18] proved that all the\ncurrent known techniques can not give a better $\\omega$ below $2.168$ which is\nlarger than our $2+1/6$. Our result generalizes the very recent result of\nsolving linear programs in the current matrix multiplication time [Cohen, Lee,\nSong'19] to a more broad class of problems. Our algorithm proposes two concepts\nwhich are different from [Cohen, Lee, Song'19] :\n  $\\bullet$ We give a robust deterministic central path method, whereas the\nprevious one is a stochastic central path which updates weights by a random\nsparse vector.\n  $\\bullet$ We propose an efficient data-structure to maintain the central path\nof interior point methods even when the weights update vector is dense.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 04:42:16 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Lee", "Yin Tat", ""], ["Song", "Zhao", ""], ["Zhang", "Qiuyi", ""]]}, {"id": "1905.04469", "submitter": "Yong Tan", "authors": "Yong Tan", "title": "A New Shortest Path Algorithm Generalized on Dynamic Graph for\n  Commercial Intelligent Navigation for Transportation Management", "comments": "14 pages; 9488 words; 4 tables; 3 pictures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic graph research is an essential subject in Computer Science. The\nshortest path problem is still the central in this field; moreover there is a\nvariety of applications in practical projects.\n  In this paper, we select the transportation activity as a paradigm to study.\nOf course there exists the most clear and appealing practical application over\nthe intelligent navigation, also which is an important part in intelligent\ntransportation system (ITS). Totally speaking, this activity is fairly simple\nand well-behavior under our consideration, for which, there is a lot of\nsophisticated theories and techniques to motivate our researches although this\nproblem relates to interdisciplinary; so that we can be dedicated to computing\naspect. Below, a host of practical problems will by the way be discussed in\ntheory and empiricism including correctness, sampling, accuracy, compatibility,\nquick query and the application into other research; therein some have been\nscarcely argued intentionally or not in literatures before. Through these\ncontents, we are able to have a smart system built which easy to incorporate\nother modules to realize a sophisticated industrial system.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 07:28:32 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Tan", "Yong", ""]]}, {"id": "1905.04564", "submitter": "Amar Saini", "authors": "Amar Saini", "title": "PrivateJobMatch: A Privacy-Oriented Deferred Multi-Match Recommender\n  System for Stable Employment", "comments": "45 pages, 28 figures, RecSys 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordination failure reduces match quality among employers and candidates in\nthe job market, resulting in a large number of unfilled positions and/or\nunstable, short-term employment. Centralized job search engines provide a\nplatform that connects directly employers with job-seekers. However, they\nrequire users to disclose a significant amount of personal data, i.e., build a\nuser profile, in order to provide meaningful recommendations. In this paper, we\npresent PrivateJobMatch -- a privacy-oriented deferred multi-match recommender\nsystem -- which generates stable pairings while requiring users to provide only\na partial ranking of their preferences. PrivateJobMatch explores a series of\nadaptations of the game-theoretic Gale-Shapley deferred-acceptance algorithm\nwhich combine the flexibility of decentralized markets with the intelligence of\ncentralized matching. We identify the shortcomings of the original algorithm\nwhen applied to a job market and propose novel solutions that rely on machine\nlearning techniques. Experimental results on real and synthetic data confirm\nthe benefits of the proposed algorithms across several quality measures. Over\nthe past year, we have implemented a PrivateJobMatch prototype and deployed it\nin an active job market economy. Using the gathered real-user preference data,\nwe find that the match-recommendations are superior to a typical decentralized\njob market---while requiring only a partial ranking of the user preferences.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 17:58:40 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Saini", "Amar", ""]]}, {"id": "1905.04612", "submitter": "Bruce MacLennan", "authors": "Chengrui Li and Bruce J. MacLennan", "title": "Continuous-Time Systems for Solving 0-1 Integer Linear Programming\n  Feasibility Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 0-1 integer linear programming feasibility problem is an important\nNP-complete problem. This paper proposes a continuous-time dynamical system for\nsolving that problem without getting trapped in non-solution local minima.\nFirst, the problem is transformed to an easier form in linear time. Then, we\npropose an \"impulse algorithm\" to escape from local traps and show its\nperformance is better than randomization for escaping traps. Second, we present\nthe time-to-solution distribution of the impulse algorithm and compare it with\nexhaustive search to see its advantages. Third, we show that the fractional\nsize of the basin of attraction of the global minimum is significantly larger\nthan $2^{-N}$, the corresponding discrete probability for exhaustive search.\nFinally, we conduct a case study to show that the location of the basin is\nindependent of different dimensions. These findings reveal a better way to\nsolve the 0-1 integer linear programming feasibility problem continuously and\nshow that its cost could be less than discrete methods in average cases.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 00:01:07 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Li", "Chengrui", ""], ["MacLennan", "Bruce J.", ""]]}, {"id": "1905.04660", "submitter": "Prasad Raghavendra", "authors": "Prasad Raghavendra and Morris Yau", "title": "List Decodable Learning via Sum of Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the list-decodable learning setup, an overwhelming majority (say a\n$1-\\beta$-fraction) of the input data consists of outliers and the goal of an\nalgorithm is to output a small list $\\mathcal{L}$ of hypotheses such that one\nof them agrees with inliers. We develop a framework for list-decodable learning\nvia the Sum-of-Squares SDP hierarchy and demonstrate it on two basic\nstatistical estimation problems\n  {\\it Linear regression:} Suppose we are given labelled examples\n$\\{(X_i,y_i)\\}_{i \\in [N]}$ containing a subset $S$ of $\\beta N$ {\\it inliers}\n$\\{X_i \\}_{i \\in S}$ that are drawn i.i.d. from standard Gaussian distribution\n$N(0,I)$ in $\\mathbb{R}^d$, where the corresponding labels $y_i$ are\nwell-approximated by a linear function $\\ell$. We devise an algorithm that\noutputs a list $\\mathcal{L}$ of linear functions such that there exists some\n$\\hat{\\ell} \\in \\mathcal{L}$ that is close to $\\ell$.\n  This yields the first algorithm for linear regression in a list-decodable\nsetting. Our results hold for any distribution of examples whose concentration\nand anticoncentration can be certified by Sum-of-Squares proofs.\n  {\\it Mean Estimation:}\n  Given data points $\\{X_i\\}_{i \\in [N]}$ containing a subset $S$ of $\\beta N$\n{\\it inliers} $\\{X_i \\}_{i \\in S}$ that are drawn i.i.d. from a Gaussian\ndistribution $N(\\mu,I)$ in $\\mathbb{R}^d$, we devise an algorithm that\ngenerates a list $\\mathcal{L}$ of means such that there exists $\\hat{\\mu} \\in\n\\mathcal{L}$ close to $\\mu$.\n  The recovery guarantees of the algorithm are analogous to the existing\nalgorithms for the problem by Diakonikolas \\etal and Kothari \\etal.\n  In an independent and concurrent work, Karmalkar \\etal \\cite{KlivansKS19}\nalso obtain an algorithm for list-decodable linear regression using the\nSum-of-Squares SDP hierarchy.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 07:27:53 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Raghavendra", "Prasad", ""], ["Yau", "Morris", ""]]}, {"id": "1905.04695", "submitter": "Juho Lauri", "authors": "Juho Lauri, Christodoulos Mitillos", "title": "Complexity of fall coloring for restricted graph classes", "comments": "To appear at the 30th International Workshop on Combinatorial\n  Algorithms (IWOCA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We strengthen a result by Laskar and Lyle (Discrete Appl. Math. (2009),\n330-338) by proving that it is NP-complete to decide whether a bipartite planar\ngraph can be partitioned into three independent dominating sets. In contrast,\nwe show that this is always possible for every maximal outerplanar graph with\nat least three vertices. Moreover, we extend their previous result by proving\nthat deciding whether a bipartite graph can be partitioned into $k$ independent\ndominating sets is NP-complete for every $k \\geq 3$. We also strengthen a\nresult by Henning et al. (Discrete Math. (2009), 6451-6458) by showing that it\nis NP-complete to determine if a graph has two disjoint independent dominating\nsets, even when the problem is restricted to triangle-free planar graphs.\nFinally, for every $k \\geq 3$, we show that there is some constant $t$\ndepending only on $k$ such that deciding whether a $k$-regular graph can be\npartitioned into $t$ independent dominating sets is NP-complete. We conclude by\nderiving moderately exponential-time algorithms for the problem.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 11:20:24 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Lauri", "Juho", ""], ["Mitillos", "Christodoulos", ""]]}, {"id": "1905.04770", "submitter": "Will Ma", "authors": "Will Ma, David Simchi-Levi", "title": "Algorithms for Online Matching, Assortment, and Pricing with Tight\n  Weight-dependent Competitive Ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the dynamic assortment offerings and item pricings occurring in\ne-commerce, we study a general problem of allocating finite inventories to\nheterogeneous customers arriving sequentially. We analyze this problem under\nthe framework of competitive analysis, where the sequence of customers is\nunknown and does not necessarily follow any pattern. Previous work in this\narea, studying online matching, advertising, and assortment problems, has\nfocused on the case where each item can only be sold at a single price,\nresulting in algorithms which achieve the best-possible competitive ratio of\n1-1/e.\n  In this paper, we extend all of these results to allow for items having\nmultiple feasible prices. Our algorithms achieve the best-possible\nweight-dependent competitive ratios, which depend on the sets of feasible\nprices given in advance. Our algorithms are also simple and intuitive; they are\nbased on constructing a class of universal ``value functions'' which integrate\nthe selection of items and prices offered.\n  Finally, we test our algorithms on the publicly-available hotel data set of\nBodea et al. (2009), where there are multiple items (hotel rooms) each with\nmultiple prices (fares at which the room could be sold). We find that applying\nour algorithms, as a ``hybrid'' with algorithms which attempt to forecast and\nlearn the future transactions, results in the best performance.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 18:59:53 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ma", "Will", ""], ["Simchi-Levi", "David", ""]]}, {"id": "1905.04853", "submitter": "Kazuya Haraguchi", "authors": "Kazuya Haraguchi, Kotaro Torii, Motomu Endo", "title": "Maximum Weighted Matching with Few Edge Crossings for 2-Layered\n  Bipartite Graph", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let c denote a non-negative constant. Suppose that we are given an\nedge-weighted bipartite graph G=(V,E) with its 2-layered drawing and a family X\nof intersecting edge pairs. We consider the problem of finding a maximum\nweighted matching M* such that each edge in M* intersects with at most c other\nedges in M*, and that all edge crossings in M* are contained in X. In the\npresent paper, we propose polynomial-time algorithms for the cases of c=1 and\n2. The time complexities of the algorithms are O((k+m)log n+n) for c=1 and\nO(k^3+k^2n+m(m+log n)) for c=2, respectively, where n=|V|, m=|E| and k=|X|.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 04:11:38 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 05:04:55 GMT"}, {"version": "v3", "created": "Sun, 15 Sep 2019 14:14:16 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Haraguchi", "Kazuya", ""], ["Torii", "Kotaro", ""], ["Endo", "Motomu", ""]]}, {"id": "1905.04897", "submitter": "Pavel Vesel\\'y", "authors": "Graham Cormode and Pavel Vesel\\'y", "title": "Streaming Algorithms for Bin Packing and Vector Scheduling", "comments": "19 pages, 1 figure, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems involving the efficient arrangement of simple objects, as captured\nby bin packing and makespan scheduling, are fundamental tasks in combinatorial\noptimization. These are well understood in the traditional online and offline\ncases, but have been less well-studied when the volume of the input is truly\nmassive, and cannot even be read into memory. This is captured by the streaming\nmodel of computation, where the aim is to approximate the cost of the solution\nin one pass over the data, using small space. As a result, streaming algorithms\nproduce concise input summaries that approximately preserve the optimum value.\n  We design the first efficient streaming algorithms for these fundamental\nproblems in combinatorial optimization. For Bin Packing, we provide a streaming\nasymptotic $1+\\varepsilon$-approximation with\n$\\widetilde{O}\\left(\\frac{1}{\\varepsilon}\\right)$ memory, where $\\widetilde{O}$\nhides logarithmic factors. Moreover, such a space bound is essentially optimal.\nOur algorithm implies a streaming $d+\\varepsilon$-approximation for Vector Bin\nPacking in $d$ dimensions, running in space\n$\\widetilde{O}\\left(\\frac{d}{\\varepsilon}\\right)$. For the related Vector\nScheduling problem, we show how to construct an input summary in space\n$\\widetilde{O}(d^2\\cdot m / \\varepsilon^2)$ that preserves the optimum value up\nto a factor of $2 - \\frac{1}{m} +\\varepsilon$, where $m$ is the number of\nidentical machines.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 07:45:47 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Cormode", "Graham", ""], ["Vesel\u00fd", "Pavel", ""]]}, {"id": "1905.04941", "submitter": "Kaito Fujii", "authors": "Kaito Fujii", "title": "An improved algorithm for the submodular secretary problem with a\n  cardinality constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the submodular secretary problem with a cardinality constraint. In\nthis problem, $n$ candidates for secretaries appear sequentially in random\norder. At the arrival of each candidate, a decision maker must irrevocably\ndecide whether to hire him. The decision maker aims to hire at most $k$\ncandidates that maximize a non-negative submodular set function. We propose an\n$(\\mathrm{e} - 1)^2 / (\\mathrm{e}^2 (1 + \\mathrm{e}))$-competitive algorithm\nfor this problem, which improves the best one known so far.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 10:08:07 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Fujii", "Kaito", ""]]}, {"id": "1905.04989", "submitter": "Iqra Altaf Gillani", "authors": "Iqra Altaf Gillani and Amitabha Bagchi", "title": "A Distributed Laplacian Solver and its Applications to Electrical Flow\n  and Random Spanning Tree Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use queueing networks to present a new approach to solving Laplacian\nsystems. This marks a significant departure from the existing techniques,\nmostly based on graph-theoretic constructions and sampling. Our distributed\nsolver works for a large and important class of Laplacian systems that we call\n\"one-sink\" Laplacian systems. Specifically, our solver can produce solutions\nfor systems of the form $Lx = b$ where exactly one of the coordinates of $b$ is\nnegative. Our solver is a distributed algorithm that takes\n$\\widetilde{O}(t_{hit} d_{\\max})$ time (where $\\widetilde{O}$ hides\n$\\text{poly}\\log n$ factors) to produce an approximate solution where $t_{hit}$\nis the worst-case hitting time of the random walk on the graph, which is\n$\\Theta(n)$ for a large set of important graphs, and $d_{\\max}$ is the\ngeneralized maximum degree of the graph. The class of one-sink Laplacians\nincludes the important voltage computation problem and allows us to compute the\neffective resistance between nodes in a distributed setting. As a result, our\nLaplacian solver can be used to adapt the approach by Kelner and M\\k{a}dry\n(2009) to give the first distributed algorithm to compute approximate random\nspanning trees efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 12:06:13 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 03:30:39 GMT"}, {"version": "v3", "created": "Sun, 13 Oct 2019 16:55:56 GMT"}, {"version": "v4", "created": "Thu, 10 Sep 2020 11:28:08 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Gillani", "Iqra Altaf", ""], ["Bagchi", "Amitabha", ""]]}, {"id": "1905.05067", "submitter": "Jan Van Den Brand", "authors": "Jan van den Brand, Danupon Nanongkai, Thatchaphol Saranurak", "title": "Dynamic Matrix Inverse: Improved Algorithms and Matching Conditional\n  Lower Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic matrix inverse problem is to maintain the inverse of a matrix\nundergoing element and column updates. It is the main subroutine behind the\nbest algorithms for many dynamic problems whose complexity is not yet\nwell-understood, such as maintaining the largest eigenvalue, rank and\ndeterminant of a matrix and maintaining reachability, distances, maximum\nmatching size, and $k$-paths/cycles in a graph. Understanding the complexity of\ndynamic matrix inverse is a key to understand these problems.\n  In this paper, we present (i) improved algorithms for dynamic matrix inverse\nand their extensions to some incremental/look-ahead variants, and (ii) variants\nof the Online Matrix-Vector conjecture [Henzinger et al. STOC'15] that, if\ntrue, imply that these algorithms are tight. Our algorithms automatically lead\nto faster dynamic algorithms for the aforementioned problems, some of which are\nalso tight under our conjectures, e.g. reachability and maximum matching size\n(closing the gaps for these two problems was in fact asked by Abboud and V.\nWilliams [FOCS'14]). Prior best bounds for most of these problems date back to\nmore than a decade ago [Sankowski FOCS'04, COCOON'05, SODA'07; Kavitha\nFSTTCS'08; Mucha and Sankowski Algorithmica'10; Bosek et al. FOCS'14].\n  Our improvements stem mostly from the ability to use fast matrix\nmultiplication ``one more time'', to maintain a certain transformation matrix\nwhich could be maintained only combinatorially previously (i.e. without fast\nmatrix multiplication). Oddly, unlike other dynamic problems where this\napproach, once successful, could be repeated several times (``bootstrapping''),\nour conjectures imply that this is not the case for dynamic matrix inverse and\nsome related problems. However, when a small additional ``look-ahead''\ninformation is provided we can perform such repetition to drive the bounds down\nfurther.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 14:47:53 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Brand", "Jan van den", ""], ["Nanongkai", "Danupon", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "1905.05254", "submitter": "Wei Quan Lim", "authors": "Wei Quan Lim", "title": "Optimal Multithreaded Batch-Parallel 2-3 Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a batch-parallel 2-3 tree T in an asynchronous dynamic\nmultithreading model that supports searches, insertions and deletions in sorted\nbatches and has essentially optimal parallelism, even under the restrictive\nQRMW (queued read-modify-write) memory contention model where concurrent\naccesses to the same memory location are queued and serviced one by one.\n  Specifically, if T has n items, then performing an item-sorted batch (given\nas a leaf-based balanced binary tree) of b operations on T takes O( b *\nlog(n/b+1) + b ) work and O( log b + log n ) span (in the worst case as b,n ->\ninf). This is information-theoretically work-optimal for b <= n, and also\nspan-optimal for pointer-based structures. Moreover, it is easy to support\noptimal intersection, union and difference of instances of T with sizes m <= n,\nnamely within O( m * log(n/m+1) ) work and O( log m + log n ) span.\nFurthermore, T supports other batch operations that make it a very useful\nbuilding block for parallel data structures.\n  To the author's knowledge, T is the first parallel sorted-set data structure\nthat can be used in an asynchronous multi-processor machine under a memory\nmodel with queued contention and yet have asymptotically optimal work and span.\nIn fact, T is designed to have bounded contention and satisfy the claimed work\nand span bounds regardless of the execution schedule.\n  Since all data structures and algorithms in this paper fit into the dynamic\nmultithreading paradigm, all their performance bounds are directly composable\nwith those of other data structures and algorithms in the same model. Finally,\nthe pipelining techniques in this paper are also likely to be very useful in\nasynchronous parallelization of other recursive data structures.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 19:23:36 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 08:27:48 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Lim", "Wei Quan", ""]]}, {"id": "1905.05291", "submitter": "Alok Chauhan", "authors": "Alok Chauhan, Madhusudan Verma", "title": "5/4 approximation for Symmetric TSP", "comments": "8 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Travelling Salesman Problem (TSP) is one of the unsolved problems in computer\nscience. TSP is NP Hard. Till now the best approximation ratio found for\nsymmetric TSP is three by two by Christofides Algorithm more than forty years\nago. There are different approaches to solve this problem. These range from\nmethods based on neural networks, genetic algorithm, swarm optimization, ant\ncolony optimization etc. The bound is further reduced from three by two but for\ngraphic TSP. A factor of thirteen by nine was found for Graphic TSP. A newly\nproposed heuristic called 2 RNN is considered here. It seems from experimental\nresults that five by four is the approximation ratio. Upper bound analysis for\napproximation ratio is done for this heuristic and it confirms experimental\nbound of five by four.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 07:21:54 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 08:51:57 GMT"}, {"version": "v3", "created": "Sat, 24 Apr 2021 19:47:17 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Chauhan", "Alok", ""], ["Verma", "Madhusudan", ""]]}, {"id": "1905.05304", "submitter": "Philipp Zschoche", "authors": "George B. Mertzios, Hendrik Molter, Rolf Niedermeier, Viktor Zamaraev,\n  and Philipp Zschoche", "title": "Computing Maximum Matchings in Temporal Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal graphs are graphs whose topology is subject to discrete changes over\ntime. Given a static underlying graph $G$, a temporal graph is represented by\nassigning a set of integer time-labels to every edge $e$ of $G$, indicating the\ndiscrete time steps at which $e$ is active. We introduce and study the\ncomplexity of a natural temporal extension of the classical graph problem\nMaximum Matching, taking into account the dynamic nature of temporal graphs. In\nour problem, Maximum Temporal Matching, we are looking for the largest possible\nnumber of time-labeled edges (simply time-edges) $(e,t)$ such that no vertex is\nmatched more than once within any time window of $\\Delta$ consecutive time\nslots, where $\\Delta \\in \\mathbb{N}$ is given. The requirement that a vertex\ncannot be matched twice in any $\\Delta$-window models some necessary \"recovery\"\nperiod that needs to pass for an entity (vertex) after being paired up for some\nactivity with another entity. We prove strong computational hardness results\nfor Maximum Temporal Matching, even for elementary cases. To cope with this\ncomputational hardness, we mainly focus on fixed-parameter algorithms with\nrespect to natural parameters, as well as on polynomial-time approximation\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 22:19:27 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 18:21:54 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 17:05:47 GMT"}, {"version": "v4", "created": "Thu, 9 Jul 2020 15:16:20 GMT"}, {"version": "v5", "created": "Fri, 10 Jul 2020 17:31:39 GMT"}, {"version": "v6", "created": "Tue, 29 Sep 2020 12:50:37 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Mertzios", "George B.", ""], ["Molter", "Hendrik", ""], ["Niedermeier", "Rolf", ""], ["Zamaraev", "Viktor", ""], ["Zschoche", "Philipp", ""]]}, {"id": "1905.05329", "submitter": "Sorrachai Yingchareonthawornchai", "authors": "Danupon Nanongkai, Thatchaphol Saranurak, Sorrachai\n  Yingchareonthawornchai", "title": "Computing and Testing Small Vertex Connectivity in Near-Linear Time and\n  Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new, simple, algorithm for the local vertex connectivity problem\n(LocalVC) introduced by Nanongkai~et~al. [STOC'19]. Roughly, given an\nundirected unweighted graph $G$, a seed vertex $x$, a target volume $\\nu$, and\na target separator size $k$, the goal of LocalVC is to remove $k$ vertices\n`near' $x$ (in terms of $\\nu$) to disconnect the graph in `local time', which\ndepends only on parameters $\\nu$ and $k$. In this paper, we present a simple\nrandomized algorithm with running time $O(\\nu k^2)$ and correctness probability\n$2/3$.\n  Plugging our new localVC algorithm in the generic framework of\nNanongkai~et~al. immediately lead to a randomized $\\tilde O(m+nk^3)$-time\nalgorithm for the classic $k$-vertex connectivity problem on undirected graphs.\n($\\tilde O(T)$ hides $\\text{polylog}(T)$.) This is the first near-linear time\nalgorithm for any $4\\leq k \\leq \\text{polylog} n$. Previous fastest algorithm\nfor small $k$ takes $\\tilde O(m+n^{4/3}k^{7/3})$ time [Nanongkai~et~al.,\nSTOC'19].\n  This work is inspired by the algorithm of Chechik~et~al. [SODA'17] for\ncomputing the maximal $k$-edge connected subgraphs. Forster and Yang [arXiv'19]\nhas independently developed local algorithms similar to ours, and showed that\nthey lead to an $\\tilde O(k^3/\\epsilon)$ bound for testing $k$-edge and -vertex\nconnectivity, resolving two long-standing open problems in property testing\nsince the work of Goldreich and Ron [STOC'97] and Orenstein and Ron [Theor.\nComput. Sci.'11]. Inspired by this, we use local approximation algorithms to\nobtain bounds that are near-linear in $k$, namely $\\tilde O(k/\\epsilon)$ and\n$\\tilde O(k/\\epsilon^2)$ for the bounded and unbounded degree cases,\nrespectively. For testing $k$-edge connectivity for simple graphs, the bound\ncan be improved to $\\tilde O(\\min(k/\\epsilon, 1/\\epsilon^2))$.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 00:46:31 GMT"}, {"version": "v2", "created": "Sat, 6 Jul 2019 18:09:18 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Nanongkai", "Danupon", ""], ["Saranurak", "Thatchaphol", ""], ["Yingchareonthawornchai", "Sorrachai", ""]]}, {"id": "1905.05376", "submitter": "Ruosong Wang", "authors": "Kenneth L. Clarkson, Ruosong Wang, David P. Woodruff", "title": "Dimensionality Reduction for Tukey Regression", "comments": "To appear in ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first dimensionality reduction methods for the overconstrained\nTukey regression problem. The Tukey loss function $\\|y\\|_M = \\sum_i M(y_i)$ has\n$M(y_i) \\approx |y_i|^p$ for residual errors $y_i$ smaller than a prescribed\nthreshold $\\tau$, but $M(y_i)$ becomes constant for errors $|y_i| > \\tau$. Our\nresults depend on a new structural result, proven constructively, showing that\nfor any $d$-dimensional subspace $L \\subset \\mathbb{R}^n$, there is a fixed\nbounded-size subset of coordinates containing, for every $y \\in L$, all the\nlarge coordinates, with respect to the Tukey loss function, of $y$. Our methods\nreduce a given Tukey regression problem to a smaller weighted version, whose\nsolution is a provably good approximate solution to the original problem. Our\nreductions are fast, simple and easy to implement, and we give empirical\nresults demonstrating their practicality, using existing heuristic solvers for\nthe small versions. We also give exponential-time algorithms giving provably\ngood solutions, and hardness results suggesting that a significant speedup in\nthe worst case is unlikely.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 03:24:40 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Clarkson", "Kenneth L.", ""], ["Wang", "Ruosong", ""], ["Woodruff", "David P.", ""]]}, {"id": "1905.05643", "submitter": "Cameron Musco", "authors": "Yonina C. Eldar, Jerry Li, Cameron Musco, Christopher Musco", "title": "Sample Efficient Toeplitz Covariance Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the sample complexity of estimating the covariance matrix $T$ of a\ndistribution $\\mathcal{D}$ over $d$-dimensional vectors, under the assumption\nthat $T$ is Toeplitz. This assumption arises in many signal processing\nproblems, where the covariance between any two measurements only depends on the\ntime or distance between those measurements. We are interested in estimation\nstrategies that may choose to view only a subset of entries in each vector\nsample $x \\sim \\mathcal{D}$, which often equates to reducing hardware and\ncommunication requirements in applications ranging from wireless signal\nprocessing to advanced imaging. Our goal is to minimize both 1) the number of\nvector samples drawn from $\\mathcal{D}$ and 2) the number of entries accessed\nin each sample.\n  We provide some of the first non-asymptotic bounds on these sample complexity\nmeasures that exploit $T$'s Toeplitz structure, and by doing so, significantly\nimprove on results for generic covariance matrices. Our bounds follow from a\nnovel analysis of classical and widely used estimation algorithms (along with\nsome new variants), including methods based on selecting entries from each\nvector sample according to a so-called sparse ruler. In many cases, we pair our\nupper bounds with matching or nearly matching lower bounds.\n  In addition to results that hold for any Toeplitz $T$, we further study the\nimportant setting when $T$ is close to low-rank, which is often the case in\npractice. We show that methods based on sparse rulers perform even better in\nthis setting, with sample complexity scaling sublinearly in $d$. Motivated by\nthis finding, we develop a new covariance estimation strategy that further\nimproves on all existing methods in the low-rank case: when $T$ is rank-$k$ or\nnearly rank-$k$, it achieves sample complexity depending polynomially on $k$\nand only logarithmically on $d$.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 14:34:58 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 20:37:26 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 15:11:34 GMT"}, {"version": "v4", "created": "Thu, 6 Jun 2019 12:43:44 GMT"}, {"version": "v5", "created": "Wed, 30 Oct 2019 05:47:07 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Eldar", "Yonina C.", ""], ["Li", "Jerry", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""]]}, {"id": "1905.05655", "submitter": "Christoph D\\\"urr", "authors": "Spyros Angelopoulos, Christoph D\\\"urr, Shendan Jin, Shahin Kamali and\n  Marc Renault", "title": "Online Computation with Untrusted Advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advice model of online computation captures a setting in which the\nalgorithm is given some partial information concerning the request sequence.\nThis paradigm allows to establish tradeoffs between the amount of this\nadditional information and the performance of the online algorithm. However, if\nthe advice is corrupt or, worse, if it comes from a malicious source, the\nalgorithm may perform poorly. In this work, we study online computation in a\nsetting in which the advice is provided by an untrusted source. Our objective\nis to quantify the impact of untrusted advice so as to design and analyze\nonline algorithms that are robust and perform well even when the advice is\ngenerated in a malicious, adversarial manner. To this end, we focus on\nwell-studied online problems such as ski rental, online bidding, bin packing,\nand list update. For ski-rental and online bidding, we show how to obtain\nalgorithms that are Pareto-optimal with respect to the competitive ratios\nachieved; this improves upon the framework of Purohit et al. [NeurIPS 2018] in\nwhich Pareto-optimality is not necessarily guaranteed. For bin packing and list\nupdate, we give online algorithms with worst-case tradeoffs in their\ncompetitiveness, depending on whether the advice is trusted or not; this is\nmotivated by work of Lykouris and Vassilvitskii [ICML 2018] on the paging\nproblem, but in which the competitiveness depends on the reliability of the\nadvice. Furthermore, we demonstrate how to prove lower bounds, within this\nmodel, on the tradeoff between the number of advice bits and the\ncompetitiveness of any online algorithm. Last, we study the effect of\nrandomization: here we show that for ski-rental there is a randomized algorithm\nthat Pareto-dominates any deterministic algorithm with advice of any size. We\nalso show that a single random bit is not always inferior to a single advice\nbit, as it happens in the standard model.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 15:00:41 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 15:35:50 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Angelopoulos", "Spyros", ""], ["D\u00fcrr", "Christoph", ""], ["Jin", "Shendan", ""], ["Kamali", "Shahin", ""], ["Renault", "Marc", ""]]}, {"id": "1905.05679", "submitter": "Pravesh K Kothari", "authors": "Sushrut Karmalkar, Adam R. Klivans, Pravesh K. Kothari", "title": "List-Decodable Linear Regression", "comments": "28 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first polynomial-time algorithm for robust regression in the\nlist-decodable setting where an adversary can corrupt a greater than $1/2$\nfraction of examples.\n  For any $\\alpha < 1$, our algorithm takes as input a sample $\\{(x_i,y_i)\\}_{i\n\\leq n}$ of $n$ linear equations where $\\alpha n$ of the equations satisfy $y_i\n= \\langle x_i,\\ell^*\\rangle +\\zeta$ for some small noise $\\zeta$ and\n$(1-\\alpha)n$ of the equations are {\\em arbitrarily} chosen. It outputs a list\n$L$ of size $O(1/\\alpha)$ - a fixed constant - that contains an $\\ell$ that is\nclose to $\\ell^*$.\n  Our algorithm succeeds whenever the inliers are chosen from a\n\\emph{certifiably} anti-concentrated distribution $D$. In particular, this\ngives a $(d/\\alpha)^{O(1/\\alpha^8)}$ time algorithm to find a $O(1/\\alpha)$\nsize list when the inlier distribution is standard Gaussian. For discrete\nproduct distributions that are anti-concentrated only in \\emph{regular}\ndirections, we give an algorithm that achieves similar guarantee under the\npromise that $\\ell^*$ has all coordinates of the same magnitude. To complement\nour result, we prove that the anti-concentration assumption on the inliers is\ninformation-theoretically necessary.\n  Our algorithm is based on a new framework for list-decodable learning that\nstrengthens the `identifiability to algorithms' paradigm based on the\nsum-of-squares method.\n  In an independent and concurrent work, Raghavendra and Yau also used the\nSum-of-Squares method to give a similar result for list-decodable regression.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 15:43:33 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 22:41:10 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 04:45:23 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Karmalkar", "Sushrut", ""], ["Klivans", "Adam R.", ""], ["Kothari", "Pravesh K.", ""]]}, {"id": "1905.06084", "submitter": "Yuchen Mao", "authors": "Siu-Wing Cheng and Yuchen Mao", "title": "Restricted Max-Min Allocation: Approximation and Integrality Gap", "comments": "An extended abstract of this full version is to appear in ICALP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asadpour, Feige, and Saberi proved that the integrality gap of the\nconfiguration LP for the restricted max-min allocation problem is at most $4$.\nHowever, their proof does not give a polynomial-time approximation algorithm. A\nlot of efforts have been devoted to designing an efficient algorithm whose\napproximation ratio can match this upper bound for the integrality gap. In\nICALP 2018, we present a $(6 + \\delta)$-approximation algorithm where $\\delta$\ncan be any positive constant, and there is still a gap of roughly $2$. In this\npaper, we narrow the gap significantly by proposing a\n$(4+\\delta)$-approximation algorithm where $\\delta$ can be any positive\nconstant. The approximation ratio is with respect to the optimal value of the\nconfiguration LP, and the running time is $\\mathit{poly}(m,n)\\cdot\nn^{\\mathit{poly}(\\frac{1}{\\delta})}$ where $n$ is the number of players and $m$\nis the number of resources. We also improve the upper bound for the integrality\ngap of the configuration LP to $3 + \\frac{21}{26} \\approx 3.808$.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 10:55:41 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Cheng", "Siu-Wing", ""], ["Mao", "Yuchen", ""]]}, {"id": "1905.06394", "submitter": "Taisuke Yasuda", "authors": "Manuel Fernandez, David P. Woodruff, Taisuke Yasuda", "title": "Tight Kernel Query Complexity of Kernel Ridge Regression and Kernel\n  $k$-means Clustering", "comments": "27 pages, to appear in ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present tight lower bounds on the number of kernel evaluations required to\napproximately solve kernel ridge regression (KRR) and kernel $k$-means\nclustering (KKMC) on $n$ input points. For KRR, our bound for relative error\napproximation to the minimizer of the objective function is\n$\\Omega(nd_{\\mathrm{eff}}^\\lambda/\\varepsilon)$ where\n$d_{\\mathrm{eff}}^\\lambda$ is the effective statistical dimension, which is\ntight up to a $\\log(d_{\\mathrm{eff}}^\\lambda/\\varepsilon)$ factor. For KKMC,\nour bound for finding a $k$-clustering achieving a relative error approximation\nof the objective function is $\\Omega(nk/\\varepsilon)$, which is tight up to a\n$\\log(k/\\varepsilon)$ factor. Our KRR result resolves a variant of an open\nquestion of El Alaoui and Mahoney, asking whether the effective statistical\ndimension is a lower bound on the sampling complexity or not. Furthermore, for\nthe important practical case when the input is a mixture of Gaussians, we\nprovide a KKMC algorithm which bypasses the above lower bound.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 19:04:16 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Fernandez", "Manuel", ""], ["Woodruff", "David P.", ""], ["Yasuda", "Taisuke", ""]]}, {"id": "1905.06626", "submitter": "Frances Cooper", "authors": "Frances Cooper and David Manlove", "title": "Two-sided profile-based optimality in the stable marriage problem", "comments": "40 pages including appendix, 16 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of finding \"fair\" stable matchings in the Stable\nMarriage problem with Incomplete lists (SMI). In particular, we seek stable\nmatchings that are optimal with respect to profile, which is a vector that\nindicates the number of agents who have their first-, second-, third-choice\npartner, etc. In a rank maximal stable matching, the maximum number of agents\nhave their first-choice partner, and subject to this, the maximum number of\nagents have their second-choice partner, etc., whilst in a generous stable\nmatching $M$, the minimum number of agents have their $d$th-choice partner, and\nsubject to this, the minimum number of agents have their $(d-1)$th-choice\npartner, etc., where $d$ is the maximum rank of an agent's partner in $M$.\nIrving et al. [18] presented an $O(nm^2\\log n)$ algorithm for finding a\nrank-maximal stable matching, which can be adapted easily to the generous\nstable matching case, where $n$ is the number of men / women and $m$ is the\nnumber of acceptable man-woman pairs. An $O(n^{0.5}m^{1.5})$ algorithm for the\nrank-maximal stable matching problem was later given by Feder [7]. However\nthese approaches involve the use of weights that are in general exponential in\n$n$. In this paper we present an $O(nm^2\\log n)$ algorithm for finding a\nrank-maximal stable matching using a vector-based approach that involves\nweights that are polynomially-bounded in $n$. We conduct an empirical\nevaluation, and show how this approach has a far reduced memory requirement (an\nestimated $100$-fold improvement for instances with $100, 000$ men or women)\nwhen compared to Irving et al.'s algorithm above. Additionally, we show how to\nadapt our algorithm for the generous case. Finally, we examine the complexity\nof the problem of finding profile-based optimal stable matchings in the Stable\nRoommates problem (SR).\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 09:53:23 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 15:49:09 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2020 07:03:23 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Cooper", "Frances", ""], ["Manlove", "David", ""]]}, {"id": "1905.06706", "submitter": "Christopher Weyand", "authors": "Thomas Bl\\\"asius, Tobias Friedrich, Maximilian Katzmann, Ulrich Meyer,\n  Manuel Penschuck and Christopher Weyand", "title": "Efficiently Generating Geometric Inhomogeneous and Hyperbolic Random\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperbolic random graphs (HRG) and geometric inhomogeneous random graphs\n(GIRG) are two similar generative network models that were designed to resemble\ncomplex real world networks. In particular, they have a power-law degree\ndistribution with controllable exponent $\\beta$, and high clustering that can\nbe controlled via the temperature $T$.\n  We present the first implementation of an efficient GIRG generator running in\nexpected linear time. Besides varying temperatures, it also supports underlying\ngeometries of higher dimensions. It is capable of generating graphs with ten\nmillion edges in under a second on commodity hardware. The algorithm can be\nadapted to HRGs. Our resulting implementation is the fastest sequential HRG\ngenerator, despite the fact that we support non-zero temperatures. Though\nnon-zero temperatures are crucial for many applications, most existing\ngenerators are restricted to $T = 0$. Our generators support parallelization,\nalthough this is not the focus of this paper. We note that our generators draw\nfrom the correct probability distribution, i.e., they involve no approximation.\n  Besides the generators themselves, we also provide an efficient algorithm to\ndetermine the non-trivial dependency between the average degree of the\nresulting graph and the input parameters of the GIRG model. This makes it\npossible to specify the expected average degree as input.\n  Moreover, we investigate the differences between HRGs and GIRGs, shedding new\nlight on the nature of the relation between the two models. Although HRGs\nrepresent, in a certain sense, a special case of the GIRG model, we find that a\nstraight-forward inclusion does not hold in practice. However, the difference\nis negligible for most use cases.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 12:57:54 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 07:59:10 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Friedrich", "Tobias", ""], ["Katzmann", "Maximilian", ""], ["Meyer", "Ulrich", ""], ["Penschuck", "Manuel", ""], ["Weyand", "Christopher", ""]]}, {"id": "1905.06783", "submitter": "Konstantinos Georgiou", "authors": "Jurek Czyzowicz, Konstantinos Georgiou, Ryan Killick, Evangelos\n  Kranakis, Danny Krizanc, Manuel Lafond, Lata Narayanan, Jaroslav Opatrny,\n  Sunil Shende", "title": "Time-Energy Tradeoffs for Evacuation by Two Robots in the Wireless Model", "comments": "This is the full version of the paper with the same title which will\n  appear in the proceedings of the 26th International Colloquium on Structural\n  Information and Communication Complexity (SIROCCO'19) L'Aquila, Italy during\n  July 1-4, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two robots stand at the origin of the infinite line and are tasked with\nsearching collaboratively for an exit at an unknown location on the line. They\ncan travel at maximum speed $b$ and can change speed or direction at any time.\nThe two robots can communicate with each other at any distance and at any time.\nThe task is completed when the last robot arrives at the exit and evacuates. We\nstudy time-energy tradeoffs for the above evacuation problem. The evacuation\ntime is the time it takes the last robot to reach the exit. The energy it takes\nfor a robot to travel a distance $x$ at speed $s$ is measured as $xs^2$. The\ntotal and makespan evacuation energies are respectively the sum and maximum of\nthe energy consumption of the two robots while executing the evacuation\nalgorithm.\n  Assuming that the maximum speed is $b$, and the evacuation time is at most\n$cd$, where $d$ is the distance of the exit from the origin, we study the\nproblem of minimizing the total energy consumption of the robots. We prove that\nthe problem is solvable only for $bc \\geq 3$. For the case $bc=3$, we give an\noptimal algorithm, and give upper bounds on the energy for the case $bc>3$.\n  We also consider the problem of minimizing the evacuation time when the\navailable energy is bounded by $\\Delta$. Surprisingly, when $\\Delta$ is a\nconstant, independent of the distance $d$ of the exit from the origin, we prove\nthat evacuation is possible in time $O(d^{3/2}\\log d)$, and this is optimal up\nto a logarithmic factor. When $\\Delta$ is linear in $d$, we give upper bounds\non the evacuation time.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 14:33:26 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Czyzowicz", "Jurek", ""], ["Georgiou", "Konstantinos", ""], ["Killick", "Ryan", ""], ["Kranakis", "Evangelos", ""], ["Krizanc", "Danny", ""], ["Lafond", "Manuel", ""], ["Narayanan", "Lata", ""], ["Opatrny", "Jaroslav", ""], ["Shende", "Sunil", ""]]}, {"id": "1905.06894", "submitter": "Ahad N. Zehmakan", "authors": "Ahad N. Zehmakan and Serge Galam", "title": "Fake news and rumors: a trigger for proliferation or fading away", "comments": null, "journal-ref": null, "doi": "10.1063/5.0006984", "report-no": null, "categories": "cs.DM cs.DS cs.SI math.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamics of fake news and rumor spreading is investigated using a model\nwith three kinds of agents who are respectively the Seeds, the Agnostics and\nthe Others. While Seeds are the ones who start spreading the rumor being\nadamantly convinced of its truth, Agnostics reject any kind of rumor and do not\nbelieve in conspiracy theories. In between, the Others constitute the main part\nof the community. While Seeds are always Believers and Agnostics are always\nIndifferents, Others can switch between being Believer and Indifferent\ndepending on who they are discussing with. The underlying driving dynamics is\nimplemented via local updates of randomly formed groups of agents. In each\ngroup, an Other turns into a Believer as soon as $m$ or more Believers are\npresent in the group. However, since some Believers may lose interest in the\nrumor as time passes by, we add a flipping fixed rate $0<d<1$ from Believers\ninto Indifferents. Rigorous analysis of the associated dynamics reveals that\nswitching from $m=1$ to $m\\ge2$ triggers a drastic qualitative change in the\nspreading process. When $m=1$ even a small group of Believers may manage to\nconvince a large part of the community very quickly. In contrast, for $m\\ge 2$,\neven a substantial fraction of Believers does not prevent the rumor dying out\nafter a few update rounds. Our results provide an explanation on why a given\nrumor spreads within a social group and not in another, and also why some\nrumors will not spread in neither groups.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 15:49:21 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Zehmakan", "Ahad N.", ""], ["Galam", "Serge", ""]]}, {"id": "1905.06895", "submitter": "Ahad N. Zehmakan", "authors": "Ahad N. Zehmakan, Jerri Nummenpalo, Alexander Pilz, and Daniel\n  Wolleb-Graf", "title": "Switches in Eulerian graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the graph transformation problem of turning a simple graph into\nan Eulerian one by a minimum number of single edge switches is NP-hard.\nFurther, we show that any simple Eulerian graph can be transformed into any\nother such graph by a sequence of 2-switches (i.e., exchange of two edge\npairs), such that every intermediate graph is also Eulerian. However, finding\nthe shortest such sequence also turns out to be an NP-hard problem.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 11:35:28 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Zehmakan", "Ahad N.", ""], ["Nummenpalo", "Jerri", ""], ["Pilz", "Alexander", ""], ["Wolleb-Graf", "Daniel", ""]]}, {"id": "1905.06896", "submitter": "Ahad N. Zehmakan", "authors": "Ahad N. Zehmakan", "title": "Target Set in Threshold Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a graph $G$ and an initial coloring, where each node is blue or red.\nIn each round, all nodes simultaneously update their color based on a\npredefined rule. In a threshold model, a node becomes blue if a certain number\nor fraction of its neighbors are blue and red otherwise. What is the minimum\nnumber of nodes which must be blue initially so that the whole graph becomes\nblue eventually? We study this question for graphs which have expansion\nproperties, parameterized by spectral gap, in particular the\nErd\\H{o}s-R\\'{e}nyi random graph and random regular graphs.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 09:40:58 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Zehmakan", "Ahad N.", ""]]}, {"id": "1905.06917", "submitter": "Marco Fiorucci", "authors": "Marco Fiorucci, Francesco Pelosin, Marcello Pelillo", "title": "Separating Structure from Noise in Large Graphs Using the Regularity\n  Lemma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we separate structural information from noise in large graphs? To\naddress this fundamental question, we propose a graph summarization approach\nbased on Szemer\\'edi's Regularity Lemma, a well-known result in graph theory,\nwhich roughly states that every graph can be approximated by the union of a\nsmall number of random-like bipartite graphs called `regular pairs'. Hence, the\nRegularity Lemma provides us with a principled way to describe the essential\nstructure of large graphs using a small amount of data. Our paper has several\ncontributions: (i) We present our summarization algorithm which is able to\nreveal the main structural patterns in large graphs. (ii) We discuss how to use\nour summarization framework to efficiently retrieve from a database the top-k\ngraphs that are most similar to a query graph. (iii) Finally, we evaluate the\nnoise robustness of our approach in terms of the reconstruction error and the\nusefulness of the summaries in addressing the graph search task.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 17:29:54 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 08:18:08 GMT"}, {"version": "v3", "created": "Tue, 21 May 2019 17:42:02 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Fiorucci", "Marco", ""], ["Pelosin", "Francesco", ""], ["Pelillo", "Marcello", ""]]}, {"id": "1905.07145", "submitter": "Jianing Zhai", "authors": "Jianing Zhai and Sid Chi-Kin Chau and Minghua Chen", "title": "Stay or Switch: Competitive Online Algorithms for Energy Plan Selection\n  in Energy Markets with Retail Choice", "comments": "e-Energy 2019 technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy markets with retail choice enable customers to switch energy plans\namong competitive retail suppliers. Despite the promising benefits of more\naffordable prices and better savings to customers, there appears subsided\nparticipation in energy retail markets from residential customers. One major\nreason is the complex online decision-making process for selecting the best\nenergy plan from a multitude of options that hinders average consumers. In this\npaper, we shed light on the online energy plan selection problem by providing\neffective competitive online algorithms. We first formulate the online energy\nplan selection problem as a metrical task system problem with temporally\ndependent switching costs. For the case of constant cancellation fee, we\npresent a 3-competitive deterministic online algorithm and a 2-competitive\nrandomized online algorithm for solving the energy plan selection problem. We\nshow that the two competitive ratios are the best possible among deterministic\nand randomized online algorithms, respectively. We further extend our online\nalgorithms to the case where the cancellation fee is linearly proportional to\nthe residual contract duration. Through empirical evaluations using real-world\nhousehold and energy plan data, we show that our deterministic online algorithm\ncan produce on average 14.6% cost saving, as compared to 16.2% by the offline\noptimal algorithm, while our randomized online algorithm can further improve\ncost saving by up to 0.5%.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 07:36:27 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 07:57:01 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Zhai", "Jianing", ""], ["Chau", "Sid Chi-Kin", ""], ["Chen", "Minghua", ""]]}, {"id": "1905.07224", "submitter": "Rayan Chikhi", "authors": "Ma\\\"el Kerbiriou and Rayan Chikhi", "title": "Parallel decompression of gzip-compressed files and random access to DNA\n  sequences", "comments": "HiCOMB'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decompressing a file made by the gzip program at an arbitrary location is in\nprinciple impossible, due to the nature of the DEFLATE compression algorithm.\nConsequently, no existing program can take advantage of parallelism to rapidly\ndecompress large gzip-compressed files. This is an unsatisfactory bottleneck,\nespecially for the analysis of large sequencing data experiments. Here we\npropose a parallel algorithm and an implementation, pugz, that performs fast\nand exact decompression of any text file. We show that pugz is an order of\nmagnitude faster than gunzip, and 5x faster than a highly-optimized sequential\nimplementation (libdeflate). We also study the related problem of random access\nto compressed data. We give simple models and experimental results that shed\nlight on the structure of gzip-compressed files containing DNA sequences.\nPreliminary results show that random access to sequences within a\ngzip-compressed FASTQ file is almost always feasible at low compression levels,\nyet is approximate at higher compression levels.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 12:24:17 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Kerbiriou", "Ma\u00ebl", ""], ["Chikhi", "Rayan", ""]]}, {"id": "1905.07245", "submitter": "Zhewei Wei", "authors": "Yuan Yin and Zhewei Wei", "title": "Scalable Graph Embeddings via Sparse Transpose Proximities", "comments": "ACM SIGKDD2019", "journal-ref": null, "doi": "10.1145/3292500.333086", "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embedding learns low-dimensional representations for nodes in a graph\nand effectively preserves the graph structure. Recently, a significant amount\nof progress has been made toward this emerging research area. However, there\nare several fundamental problems that remain open. First, existing methods fail\nto preserve the out-degree distributions on directed graphs. Second, many\nexisting methods employ random walk based proximities and thus suffer from\nconflicting optimization goals on undirected graphs. Finally, existing\nfactorization methods are unable to achieve scalability and non-linearity\nsimultaneously.\n  This paper presents an in-depth study on graph embedding techniques on both\ndirected and undirected graphs. We analyze the fundamental reasons that lead to\nthe distortion of out-degree distributions and to the conflicting optimization\ngoals. We propose {\\em transpose proximity}, a unified approach that solves\nboth problems. Based on the concept of transpose proximity, we design \\strap, a\nfactorization based graph embedding algorithm that achieves scalability and\nnon-linearity simultaneously. \\strap makes use of the {\\em backward push}\nalgorithm to efficiently compute the sparse {\\em Personalized PageRank (PPR)}\nas its transpose proximities. By imposing the sparsity constraint, we are able\nto apply non-linear operations to the proximity matrix and perform efficient\nmatrix factorization to derive the embedding vectors. Finally, we present an\nextensive experimental study that evaluates the effectiveness of various graph\nembedding algorithms, and we show that \\strap outperforms the state-of-the-art\nmethods in terms of effectiveness and scalability.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 05:20:09 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Yin", "Yuan", ""], ["Wei", "Zhewei", ""]]}, {"id": "1905.07439", "submitter": "Osman Asif Malik", "authors": "Osman Asif Malik, Stephen Becker", "title": "Randomization of Approximate Bilinear Computation for Matrix\n  Multiplication", "comments": "29 pages, 26 figures. International Journal of Computer Mathematics:\n  Computer Systems Theory, 2020", "journal-ref": null, "doi": "10.1080/23799927.2020.1861104", "report-no": null, "categories": "cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for randomizing formulas for bilinear computation of\nmatrix products. We consider the implications of such randomization when there\nare two sources of error: One due to the formula itself only being\napproximately correct, and one due to using floating point arithmetic. Our\ntheoretical results and numerical experiments indicate that our method can\nimprove performance when each of these error sources are present individually,\nas well as when they are present at the same time.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 18:48:01 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 21:31:51 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Malik", "Osman Asif", ""], ["Becker", "Stephen", ""]]}, {"id": "1905.07448", "submitter": "Ahmed Shokry", "authors": "Ahmed Shokry", "title": "Shortest Path Algorithms between Theory and Practice", "comments": "Master thesis, Alexandria University 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utilizing graph algorithms is a common activity in computer science.\nAlgorithms that perform computations on large graphs are not always efficient.\nThis work investigates the Single-Source Shortest Path (SSSP) problem, which is\nconsidered to be one of the most important and most studied graph problems.\nThis thesis contains a review of the SSSP problem in both theory and practice.\nIn addition, it discusses a new single-source shortest-path algorithm that\nachieves the same $O(n \\cdot m)$ time bound as the traditional\nBellman-Ford-Moore algorithm but outperforms it and other state-of-the-art\nalgorithms in practice.\n  The work is comprised of three parts. The first discusses some basic\nshortest-path and negative-cycle-detection algorithms in literature from the\ntheoretical and practical point of view. The second contains a discussion of a\nnew algorithm for the single-source shortest-path problem that outperforms most\nstate-of-the-art algorithms for several well-known families of graphs. The main\nidea behind the proposed algorithm is to select the fewest most-effective\nvertices to scan. We also propose a discussion of correctness, termination, and\nthe proof of the worst-case time bound of the proposed algorithm. This section\nalso suggests two different implementations for the proposed algorithm, the\nfirst runs faster while the second performs a fewer number of operations.\nFinally, an extensive computational study of the different shortest paths\nalgorithms is conducted. The results are proposed using a new evaluation metric\nfor shortest-path algorithms. A discussion of outcomes, strengths, and\nweaknesses of the various shortest path algorithms are also included in this\nwork.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 19:21:09 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Shokry", "Ahmed", ""]]}, {"id": "1905.07455", "submitter": "Satish Ramakrishna", "authors": "Satish Ramakrishna, Kamesh Aiyer", "title": "Speeding up the Karatsuba algorithm", "comments": "6 pages", "journal-ref": "J Multidis Res Rev Vol: 2, Issue: 1 (01-03) 2020", "doi": null, "report-no": null, "categories": "cs.DS math.NT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an $\\sim {\\cal O}(n)$ pre-compute technique to speed up\nthe Karatsuba algorithm for multiplying two numbers.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 12:39:10 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 00:07:07 GMT"}, {"version": "v3", "created": "Wed, 23 Oct 2019 14:56:50 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Ramakrishna", "Satish", ""], ["Aiyer", "Kamesh", ""]]}, {"id": "1905.07468", "submitter": "Yasamin Nazari", "authors": "Michael Dinitz, Yasamin Nazari, Zeyu Zhang", "title": "Lasserre Integrality Gaps for Graph Spanners and Related Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant recent progress on algorithms for approximating\ngraph spanners, i.e., algorithms which approximate the best spanner for a given\ninput graph. Essentially all of these algorithms use the same basic LP\nrelaxation, so a variety of papers have studied the limitations of this\napproach and proved integrality gaps for this LP in a variety of settings. We\nextend these results by showing that even the strongest lift-and-project\nmethods cannot help significantly, by proving polynomial integrality gaps even\nfor $n^{\\Omega(\\epsilon)}$ levels of the Lasserre hierarchy, for both the\ndirected and undirected spanner problems. We also extend these integrality gaps\nto related problems, notably Directed Steiner Network and Shallow-Light Steiner\nNetwork.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 20:44:10 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Dinitz", "Michael", ""], ["Nazari", "Yasamin", ""], ["Zhang", "Zeyu", ""]]}, {"id": "1905.07483", "submitter": "Sarel Cohen", "authors": "Noga Alon, Shiri Chechik and Sarel Cohen", "title": "Deterministic Combinatorial Replacement Paths and Distance Sensitivity\n  Oracles", "comments": "To appear in ICALP '19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we derandomize two central results in graph algorithms,\nreplacement paths and distance sensitivity oracles (DSOs) matching in both\ncases the running time of the randomized algorithms.\n  For the replacement paths problem, let G = (V,E) be a directed unweighted\ngraph with n vertices and m edges and let P be a shortest path from s to t in\nG. The {\\sl replacement paths} problem is to find for every edge e \\in P the\nshortest path from s to t avoiding e. Roditty and Zwick [ICALP 2005] obtained a\nrandomized algorithm with running time of ~O(m \\sqrt{n}). Here we provide the\nfirst deterministic algorithm for this problem, with the same ~O(m \\sqrt{n})\ntime.\n  For the problem of distance sensitivity oracles, let G = (V,E) be a directed\ngraph with real-edge weights. An f-Sensitivity Distance Oracle (f-DSO) gets as\ninput the graph G=(V,E) and a parameter f, preprocesses it into a\ndata-structure, such that given a query (s,t,F) with s,t \\in V and F \\subseteq\nE \\cup V, |F| \\le f being a set of at most f edges or vertices (failures), the\nquery algorithm efficiently computes the distance from s to t in the graph G\n\\setminus F ({\\sl i.e.}, the distance from s to t in the graph G after removing\nfrom it the failing edges and vertices F). For weighted graphs with real edge\nweights, Weimann and Yuster [FOCS 2010] presented a combinatorial randomized\nf-DSO with ~O(mn^{4-\\alpha}) preprocessing time and subquadratic\n~O(n^{2-2(1-\\alpha)/f}) query time for every value of 0 < \\alpha < 1. We\nderandomize this result and present a combinatorial deterministic f-DSO with\nthe same asymptotic preprocessing and query time.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 21:16:39 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Alon", "Noga", ""], ["Chechik", "Shiri", ""], ["Cohen", "Sarel", ""]]}, {"id": "1905.07533", "submitter": "Soheil Behnezhad", "authors": "Soheil Behnezhad and Laxman Dhulipala and Hossein Esfandiari and Jakub\n  {\\L}\\k{a}cki and Warren Schudy and Vahab Mirrokni", "title": "Massively Parallel Computation via Remote Memory Access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Adaptive Massively Parallel Computation (AMPC) model, which\nis an extension of the Massively Parallel Computation (MPC) model. At a high\nlevel, the AMPC model strengthens the MPC model by storing all messages sent\nwithin a round in a distributed data store. In the following round, all\nmachines are provided with random read access to the data store, subject to the\nsame constraints on the total amount of communication as in the MPC model. Our\nmodel is inspired by the previous empirical studies of distributed graph\nalgorithms using MapReduce and a distributed hash table service.\n  This extension allows us to give new graph algorithms with much lower round\ncomplexities compared to the best known solutions in the MPC model. In\nparticular, in the AMPC model we show how to solve maximal independent set in\n$O(1)$ rounds and connectivity/minimum spanning tree in $O(\\log\\log_{m/n} n)$\nrounds both using $O(n^\\delta)$ space per machine for constant $\\delta < 1$. In\nthe same memory regime for MPC, the best known algorithms for these problems\nrequire polylog $n$ rounds. Our results imply that the 2-Cycle conjecture,\nwhich is widely believed to hold in the MPC model, does not hold in the AMPC\nmodel.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 04:06:43 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Behnezhad", "Soheil", ""], ["Dhulipala", "Laxman", ""], ["Esfandiari", "Hossein", ""], ["\u0141\u0105cki", "Jakub", ""], ["Schudy", "Warren", ""], ["Mirrokni", "Vahab", ""]]}, {"id": "1905.07559", "submitter": "Ora Nova  Fandina", "authors": "Yair Bartal, Nova Fandina, Ofer Neiman", "title": "Covering Metric Spaces by Few Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A {\\em tree cover} of a metric space $(X,d)$ is a collection of trees, so\nthat every pair $x,y\\in X$ has a low distortion path in one of the trees. If it\nhas the stronger property that every point $x\\in X$ has a single tree with low\ndistortion paths to all other points, we call this a {\\em Ramsey} tree cover.\nTree covers and Ramsey tree covers have been studied by\n\\cite{BLMN03,GKR04,CGMZ05,GHR06,MN07}, and have found several important\nalgorithmic applications, e.g. routing and distance oracles. The union of trees\nin a tree cover also serves as a special type of spanner, that can be\ndecomposed into a few trees with low distortion paths contained in a single\ntree; Such spanners for Euclidean pointsets were presented by \\cite{ADMSS95}.\n  In this paper we devise efficient algorithms to construct tree covers and\nRamsey tree covers for general, planar and doubling metrics. We pay particular\nattention to the desirable case of distortion close to 1, and study what can be\nachieved when the number of trees is small. In particular, our work shows a\nlarge separation between what can be achieved by tree covers vs. Ramsey tree\ncovers.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 09:08:17 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Bartal", "Yair", ""], ["Fandina", "Nova", ""], ["Neiman", "Ofer", ""]]}, {"id": "1905.07821", "submitter": "Miroslav Rada", "authors": "Michal \\v{C}ern\\'y, Miroslav Rada, Ond\\v{r}ej Sokol", "title": "The algorithm by Ferson et al. is surprisingly fast: An NP-hard\n  optimization problem solvable in almost linear time with high probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We start with the algorithm of Ferson et al. (\\emph{Reliable computing} {\\bf\n11}(3), p.~207--233, 2005), designed for solving a certain NP-hard problem\nmotivated by robust statistics.\n  First, we propose an efficient implementation of the algorithm and improve\nits complexity bound to $O(n \\log n+n\\cdot 2^\\omega)$, where $\\omega$ is the\nclique number in a certain intersection graph. Then we treat input data as\nrandom variables (as it is usual in statistics) and introduce a natural\nprobabilistic data generating model. On average, we get $2^\\omega =\nO(n^{1/\\log\\log n})$ and $\\omega = O(\\log n / \\log\\log n)$. This results in\naverage computing time $O(n^{1+\\epsilon})$ for $\\epsilon > 0$ arbitrarily\nsmall, which may be considered as ``surprisingly good'' average time complexity\nfor solving an NP-hard problem. Moreover, we prove the following tail bound on\nthe distribution of computation time: ``hard'' instances, forcing the algorithm\nto compute in time $2^{\\Omega(n)}$, occur rarely, with probability tending to\nzero faster than exponentially with $n \\rightarrow \\infty$.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 22:35:00 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 11:01:19 GMT"}, {"version": "v3", "created": "Sun, 29 Dec 2019 20:45:07 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["\u010cern\u00fd", "Michal", ""], ["Rada", "Miroslav", ""], ["Sokol", "Ond\u0159ej", ""]]}, {"id": "1905.07986", "submitter": "Sebastian Berndt", "authors": "Sebastian Berndt and Valentin Dreismann and Kilian Grage and Klaus\n  Jansen and Ingmar Knof", "title": "Robust Online Algorithms for Dynamic Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online algorithms that allow a small amount of migration or recourse have\nbeen intensively studied in the last years. They are essential in the design of\ncompetitive algorithms for dynamic problems, where objects can also depart from\nthe instance. In this work, we give a general framework to obtain so called\nrobust online algorithms for these dynamic problems: these online algorithm\nachieve an asymptotic competitive ratio of $\\gamma+\\epsilon$ with migration\n$O(1/\\epsilon)$, where $\\gamma$ is the best known offline asymptotic\napproximation ratio. In order to use our framework, one only needs to construct\na suitable online algorithm for the static online case, where items never\ndepart. To show the usefulness of our approach, we improve upon the best known\nrobust algorithms for the dynamic versions of generalizations of Strip Packing\nand Bin Packing, including the first robust algorithm for general\n$d$-dimensional Bin Packing and Vector Packing.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 10:50:13 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Berndt", "Sebastian", ""], ["Dreismann", "Valentin", ""], ["Grage", "Kilian", ""], ["Jansen", "Klaus", ""], ["Knof", "Ingmar", ""]]}, {"id": "1905.08033", "submitter": "Roman Galay", "authors": "Roman Galay, Daniil Kalistratov", "title": "A new algebraic approach to the graph isomorphism and clique problems", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As it follows from G\\\"odel's incompleteness theorems, any consistent formal\nsystem of axioms and rules of inference should imply a true unprovable\nstatement. Actually, this fundamental principle can be efficiently applicable\nin computational mathematics and complexity theory concerning the computational\ncomplexity of problems from the class NP, particularly and especially the\nNP-complete ones. While there is a wide set of algorithms for these problems\nthat we call heuristic, the correctness or/and complexity of each concrete\nalgorithm (or the probability of its correct and polynomial-time work) on a\nclass of instances is often too difficult to determine, although we may also\nassume the existence of a variety of algorithms for NP-complete problems that\nare both correct and polynomial-time on all the instances from a given class\n(where the given problem remains NP-complete), but whose correctness or/and\npolynomial-time complexity on the class is impossible to prove as an example\nfor G\\\"odel's theorems. However, supposedly such algorithms should possess a\ncertain complicatedness of processing the input data and treat it in a certain\nalgebraically \"entangled\" manner. The same algorithmic analysis in fact\nconcerns all the other significant problems and subclasses of NP, such as the\ngraph isomorphism problem and its associated complexity class GI.\n  The following short article offers a couple of algebraically entangled\npolynomial-time algorithms for the graph isomorphism and clique problems whose\ncorrectness is yet to be determined either empirically or through attempting to\nfind proofs. The authors are grateful to Prof. Anuj Dawar (University of\nCambridge) for kindly endorsing the present article for publishing in arXiv.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 10:58:14 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 11:29:38 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Galay", "Roman", ""], ["Kalistratov", "Daniil", ""]]}, {"id": "1905.08127", "submitter": "Mahdi Boroujeni", "authors": "Mahdi Boroujeni, Sina Dehghani, Soheil Ehsani, MohammadTaghi\n  HajiAghayi, Saeed Seddighin", "title": "Subcubic Equivalences Between Graph Centrality Measures and\n  Complementary Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite persistent efforts, there is no known technique for obtaining\nunconditional super-linear lower bounds for the computational complexity of the\nproblems in P. Vassilevska Williams and Williams introduce a fruitful approach\nto advance a better understanding of the computational complexity of the\nproblems in P. In particular, they consider All Pairs Shortest Paths (APSP) and\nother fundamental problems such as checking whether a matrix defines a metric,\nverifying the correctness of a matrix product, and detecting a negative\ntriangle in a graph. Abboud, Grandoni, and Vassilevska Williams study\nwell-known graph centrality problems such as Radius, Median, etc., and make a\nconnection between their computational complexity to that of two fundamental\nproblems, namely APSP and Diameter. They show any algorithm with subcubic\nrunning time for these centrality problems, implies a subcubic algorithm for\neither APSP or Diameter. In this paper, we define vertex versions for these\ncentrality problems and based on that we introduce new complementary problems.\nThe main open problem of Abboud et al. is whether or not APSP and Diameter are\nequivalent under subcubic reduction. One of the results of this paper is APSP\nand CoDiameter, which is the complementary version of Diameter, are equivalent.\nMoreover, for some of the problems in this set, we show that they are\nequivalent to their complementary versions. Considering the slight difference\nbetween a problem and its complementary version, these equivalences give us the\nimpression that every problem has such a property, and thus APSP and Diameter\nare equivalent. This paper is a step forward in showing a subcubic equivalence\nbetween APSP and Diameter, and we hope that the approach introduced in our\npaper can be helpful to make this breakthrough happen.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 14:08:12 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Boroujeni", "Mahdi", ""], ["Dehghani", "Sina", ""], ["Ehsani", "Soheil", ""], ["HajiAghayi", "MohammadTaghi", ""], ["Seddighin", "Saeed", ""]]}, {"id": "1905.08302", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya and Cl\\'ement L. Canonne and Himanshu Tyagi", "title": "Inference under Information Constraints II: Communication Constraints\n  and Shared Randomness", "comments": "To appear in IEEE Transactions on Information Theory. An abridged\n  version of this work appeared in the 2019 International Conference on Machine\n  Learning (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A central server needs to perform statistical inference based on samples that\nare distributed over multiple users who can each send a message of limited\nlength to the center. We study problems of distribution learning and identity\ntesting in this distributed inference setting and examine the role of shared\nrandomness as a resource. We propose a general-purpose simulate-and-infer\nstrategy that uses only private-coin communication protocols and is\nsample-optimal for distribution learning. This general strategy turns out to be\nsample-optimal even for distribution testing among private-coin protocols.\nInterestingly, we propose a public-coin protocol that outperforms\nsimulate-and-infer for distribution testing and is, in fact, sample-optimal.\nUnderlying our public-coin protocol is a random hash that when applied to the\nsamples minimally contracts the chi-squared distance of their distribution to\nthe uniform distribution.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 19:18:47 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 04:56:00 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "1905.08320", "submitter": "Tianhao Wang", "authors": "Tianhao Wang, Milan Lopuha\\\"a-Zwakenberg, Zitao Li, Boris Skoric,\n  Ninghui Li", "title": "Locally Differentially Private Frequency Estimation with Consistency", "comments": "NDSS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local Differential Privacy (LDP) protects user privacy from the data\ncollector. LDP protocols have been increasingly deployed in the industry. A\nbasic building block is frequency oracle (FO) protocols, which estimate\nfrequencies of values. While several FO protocols have been proposed, the\ndesign goal does not lead to optimal results for answering many queries. In\nthis paper, we show that adding post-processing steps to FO protocols by\nexploiting the knowledge that all individual frequencies should be non-negative\nand they sum up to one can lead to significantly better accuracy for a wide\nrange of tasks, including frequencies of individual values, frequencies of the\nmost frequent values, and frequencies of subsets of values. We consider 10\ndifferent methods that exploit this knowledge differently. We establish\ntheoretical relationships between some of them and conducted extensive\nexperimental evaluations to understand which methods should be used for\ndifferent query tasks.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 19:49:52 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 21:56:31 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Wang", "Tianhao", ""], ["Lopuha\u00e4-Zwakenberg", "Milan", ""], ["Li", "Zitao", ""], ["Skoric", "Boris", ""], ["Li", "Ninghui", ""]]}, {"id": "1905.08448", "submitter": "Kirankumar Shiragur", "authors": "Moses Charikar, Kirankumar Shiragur, Aaron Sidford", "title": "Efficient Profile Maximum Likelihood for Universal Symmetric Property\n  Estimation", "comments": "68 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating symmetric properties of a distribution, e.g. support size,\ncoverage, entropy, distance to uniformity, are among the most fundamental\nproblems in algorithmic statistics. While each of these properties have been\nstudied extensively and separate optimal estimators are known for each, in\nstriking recent work, Acharya et al. 2016 showed that there is a single\nestimator that is competitive for all symmetric properties. This work proved\nthat computing the distribution that approximately maximizes \\emph{profile\nlikelihood (PML)}, i.e. the probability of observed frequency of frequencies,\nand returning the value of the property on this distribution is sample\ncompetitive with respect to a broad class of estimators of symmetric\nproperties. Further, they showed that even computing an approximation of the\nPML suffices to achieve such a universal plug-in estimator. Unfortunately,\nprior to this work there was no known polynomial time algorithm to compute an\napproximate PML and it was open to obtain a polynomial time universal plug-in\nestimator through the use of approximate PML. In this paper we provide a\nalgorithm (in number of samples) that, given $n$ samples from a distribution,\ncomputes an approximate PML distribution up to a multiplicative error of\n$\\exp(n^{2/3} \\mathrm{poly} \\log(n))$ in time nearly linear in $n$.\nGeneralizing work of Acharya et al. 2016 on the utility of approximate PML we\nshow that our algorithm provides a nearly linear time universal plug-in\nestimator for all symmetric functions up to accuracy $\\epsilon =\n\\Omega(n^{-0.166})$. Further, we show how to extend our work to provide\nefficient polynomial-time algorithms for computing a $d$-dimensional\ngeneralization of PML (for constant $d$) that allows for universal plug-in\nestimation of symmetric relationships between distributions.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 05:39:05 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Charikar", "Moses", ""], ["Shiragur", "Kirankumar", ""], ["Sidford", "Aaron", ""]]}, {"id": "1905.08563", "submitter": "Laurent Feuilloley", "authors": "L\\'elia Blin, Laurent Feuilloley, Gabriel Le Bouder", "title": "Memory lower bounds for deterministic self-stabilization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of self-stabilization, a \\emph{silent} algorithm guarantees\nthat the register of every node does not change once the algorithm has\nstabilized. At the end of the 90's, Dolev et al. [Acta Inf. '99] showed that,\nfor finding the centers of a graph, for electing a leader, or for constructing\na spanning tree, every silent algorithm must use a memory of $\\Omega(\\log n)$\nbits per register in $n$-node networks. Similarly, Korman et al. [Dist. Comp.\n'07] proved, using the notion of proof-labeling-scheme, that, for constructing\na minimum-weight spanning trees (MST), every silent algorithm must use a memory\nof $\\Omega(\\log^2n)$ bits per register. It follows that requiring the algorithm\nto be silent has a cost in terms of memory space, while, in the context of\nself-stabilization, where every node constantly checks the states of its\nneighbors, the silence property can be of limited practical interest. In fact,\nit is known that relaxing this requirement results in algorithms with smaller\nspace-complexity.\n  In this paper, we are aiming at measuring how much gain in terms of memory\ncan be expected by using arbitrary self-stabilizing algorithms, not necessarily\nsilent. To our knowledge, the only known lower bound on the memory requirement\nfor general algorithms, also established at the end of the 90's, is due to\nBeauquier et al.~[PODC '99] who proved that registers of constant size are not\nsufficient for leader election algorithms. We improve this result by\nestablishing a tight lower bound of $\\Theta(\\log \\Delta+\\log \\log n)$ bits per\nregister for self-stabilizing algorithms solving $(\\Delta+1)$-coloring or\nconstructing a spanning tree in networks of maximum degree~$\\Delta$. The lower\nbound $\\Omega(\\log \\log n)$ bits per register also holds for leader election.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 11:44:49 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 17:22:17 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Blin", "L\u00e9lia", ""], ["Feuilloley", "Laurent", ""], ["Bouder", "Gabriel Le", ""]]}, {"id": "1905.08565", "submitter": "Laurent Feuilloley", "authors": "L\\'elia Blin, Swan Dubois, Laurent Feuilloley", "title": "Silent MST approximation for tiny memory", "comments": "To appear at SSS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that approximation can help reduce the space used for\nself-stabilization. In the classic \\emph{state model}, where the nodes of a\nnetwork communicate by reading the states of their neighbors, an important\nmeasure of efficiency is the space: the number of bits used at each node to\nencode the state. In this model, a classic requirement is that the algorithm\nhas to be \\emph{silent}, that is, after stabilization the states should not\nchange anymore. We design a silent self-stabilizing algorithm for the problem\nof minimum spanning tree, that has a trade-off between the quality of the\nsolution and the space needed to compute it.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 11:45:13 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 17:14:29 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2020 21:07:11 GMT"}, {"version": "v4", "created": "Wed, 21 Oct 2020 13:01:54 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Blin", "L\u00e9lia", ""], ["Dubois", "Swan", ""], ["Feuilloley", "Laurent", ""]]}, {"id": "1905.08592", "submitter": "Marin Bougeret", "authors": "Marin Bougeret, Klaus Jansen, Michael Poss, Lars Rohwedder", "title": "Approximation results for makespan minimization with budgeted\n  uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study approximation algorithms for the problem of minimizing the makespan\non a set of machines with uncertainty on the processing times of jobs. In the\nmodel we consider, which goes back to~\\cite{BertsimasS03}, once the schedule is\ndefined an adversary can pick a scenario where deviation is added to some of\nthe jobs' processing times. Given only the maximal cardinality of these jobs,\nand the magnitude of potential deviation for each job, the goal is to optimize\nthe worst-case scenario. We consider both the cases of identical and unrelated\nmachines. Our main result is an EPTAS for the case of identical machines. We\nalso provide a $3$-approximation algorithm and an inapproximability ratio of\n$2-\\epsilon$ for the case of unrelated machines\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 12:58:12 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Bougeret", "Marin", ""], ["Jansen", "Klaus", ""], ["Poss", "Michael", ""], ["Rohwedder", "Lars", ""]]}, {"id": "1905.08621", "submitter": "David K\\\"ubel", "authors": "Herman Haverkort, David K\\\"ubel, and Elmar Langetepe", "title": "Shortest-Path-Preserving Rounding", "comments": "20 pages, 5 figures, pre-print of an article presented at IWOCA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various applications of graphs, in particular applications related to finding\nshortest paths, naturally get inputs with real weights on the edges. However,\nfor algorithmic or visualization reasons, inputs with integer weights would\noften be preferable or even required. This raises the following question: given\nan undirected graph with non-negative real weights on the edges and an error\nthreshold $\\varepsilon$, how efficiently can we decide whether we can round all\nweights such that shortest paths are maintained, and the change of weight of\neach shortest path is less than $\\varepsilon$? So far, only for path-shaped\ngraphs a polynomial-time algorithm was known. In this paper we prove, by\nreduction from 3-SAT, that, in general, the problem is NP-hard. However, if the\ngraph is a tree with $n$ vertices, the problem can be solved in $O(n^2)$ time.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 13:30:37 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Haverkort", "Herman", ""], ["K\u00fcbel", "David", ""], ["Langetepe", "Elmar", ""]]}, {"id": "1905.08649", "submitter": "Valentin Bakoev", "authors": "Valentin Bakoev", "title": "Fast Computing the Algebraic Degree of Boolean Functions", "comments": "The paper will be published in CAI 2019 LNCS 11545 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we consider an approach for fast computing the algebraic degree of\nBoolean functions. It combines fast computing the ANF (known as ANF transform)\nand thereafter the algebraic degree by using the weight-lexicographic order\n(WLO) of the vectors of the $n$-dimensional Boolean cube. Byte-wise and bitwise\nversions of a search based on the WLO and their implementations are discussed.\nThey are compared with the usual exhaustive search applied in computing the\nalgebraic degree. For Boolean functions of $n$ variables, the bitwise\nimplementation of the search by WLO has total time complexity $O(n.2^n)$. When\nsuch a function is given by its truth table vector and its algebraic degree is\ncomputed by the bitwise versions of the algorithms discussed, the total time\ncomplexity is $\\Theta((9n-2).2^{n-7})=\\Theta(n.2^n)$. All algorithms discussed\nhave time complexities of the same type, but with big differences in the\nconstants hidden in the $\\Theta$-notation. The experimental results after\nnumerous tests confirm the theoretical results - the running times of the\nbitwise implementation are dozens of times better than the running times of the\nbyte-wise algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 14:02:54 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Bakoev", "Valentin", ""]]}, {"id": "1905.08658", "submitter": "Simon Bruggmann", "authors": "Simon Bruggmann and Rico Zenklusen", "title": "An Optimal Monotone Contention Resolution Scheme for Bipartite Matchings\n  via a Polyhedral Viewpoint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relaxation and rounding approaches became a standard and extremely versatile\ntool for constrained submodular function maximization. One of the most common\nrounding techniques in this context are contention resolution schemes. Such\nschemes round a fractional point by first rounding each coordinate\nindependently, and then dropping some elements to reach a feasible set. Also\nthe second step, where elements are dropped, is typically randomized. This\nleads to an additional source of randomization within the procedure, which can\ncomplicate the analysis. We suggest a different, polyhedral viewpoint to design\ncontention resolution schemes, which avoids to deal explicitly with the\nrandomization in the second step. This is achieved by focusing on the marginals\nof a dropping procedure. Apart from avoiding one source of randomization, our\nviewpoint allows for employing polyhedral techniques. Both can significantly\nsimplify the construction and analysis of contention resolution schemes. We\nshow how, through our framework, one can obtain an optimal monotone contention\nresolution scheme for bipartite matchings. So far, only very few results are\nknown about optimality of monotone contention resolution schemes. Our\ncontention resolution scheme for the bipartite case also improves the lower\nbound on the correlation gap for bipartite matchings. Furthermore, we derive a\nmonotone contention resolution scheme for matchings that significantly improves\nover the previously best one. At the same time, our scheme implies that the\ncurrently best lower bound on the correlation gap for matchings is not tight.\nOur results lead to improved approximation factors for various constrained\nsubmodular function maximization problems over a combination of matching\nconstraints with further constraints.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 14:08:21 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Bruggmann", "Simon", ""], ["Zenklusen", "Rico", ""]]}, {"id": "1905.08841", "submitter": "Yang P. Liu", "authors": "Arun Jambulapati, Yang P. Liu, Aaron Sidford", "title": "Parallel Reachability in Almost Linear Work and Square Root Depth", "comments": "38 pages. v2 fixes a small typo in Section 4 found by Aaron\n  Bernstein. v3 fixes some overflow issues. v4 fixes the proof of Lemma 5.1. We\n  thank Aaron Bernstein for pointing this out", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a parallel algorithm that given any $n$-node\n$m$-edge directed graph and source vertex $s$ computes all vertices reachable\nfrom $s$ with $\\tilde{O}(m)$ work and $n^{1/2 + o(1)}$ depth with high\nprobability in $n$ . This algorithm also computes a set of $\\tilde{O}(n)$ edges\nwhich when added to the graph preserves reachability and ensures that the\ndiameter of the resulting graph is at most $n^{1/2 + o(1)}$. Our result\nimproves upon the previous best known almost linear work reachability algorithm\ndue to Fineman which had depth $\\tilde{O}(n^{2/3})$.\n  Further, we show how to leverage this algorithm to achieve improved\ndistributed algorithms for single source reachability in the CONGEST model. In\nparticular, we provide a distributed algorithm that given a $n$-node digraph of\nundirected hop-diameter $D$ solves the single source reachability problem with\n$\\tilde{O}(n^{1/2} + n^{1/3 + o(1)} D^{2/3})$ rounds of the communication in\nthe CONGEST model with high probability in $n$. Our algorithm is nearly optimal\nwhenever $D = O(n^{1/4 - \\epsilon})$ for any constant $\\epsilon > 0$ and is the\nfirst nearly optimal algorithm for general graphs whose diameter is\n$\\Omega(n^\\delta)$ for any constant $\\delta$.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 19:25:30 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 07:37:50 GMT"}, {"version": "v3", "created": "Mon, 30 Sep 2019 00:28:49 GMT"}, {"version": "v4", "created": "Fri, 6 Dec 2019 02:38:37 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Jambulapati", "Arun", ""], ["Liu", "Yang P.", ""], ["Sidford", "Aaron", ""]]}, {"id": "1905.08856", "submitter": "Jaroslav Opatrny", "authors": "Anne-Laure Ehresmann, Manuel Lafond, Lata Narayanan, Jaroslav Opatrny", "title": "Distributed Pattern Formation in a Ring", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.SI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Motivated by concerns about diversity in social networks, we consider the\nfollowing pattern formation problems in rings. Assume $n$ mobile agents are\nlocated at the nodes of an $n$-node ring network. Each agent is assigned a\ncolour from the set $\\{c_1, c_2, \\ldots, c_q \\}$. The ring is divided into $k$\ncontiguous {\\em blocks} or neighbourhoods of length $p$. The agents are\nrequired to rearrange themselves in a distributed manner to satisfy given\ndiversity requirements: in each block $j$ and for each colour $c_i$, there must\nbe exactly $n_i(j) >0$ agents of colour $c_i$ in block $j$. Agents are assumed\nto be able to see agents in adjacent blocks, and move to any position in\nadjacent blocks in one time step. When the number of colours $q=2$, we give an\nalgorithm that terminates in time $N_1/n^*_1 + k + 4$ where $N_1$ is the total\nnumber of agents of colour $c_1$ and $n^*_1$ is the minimum number of agents of\ncolour $c_1$ required in any block. When the diversity requirements are the\nsame in every block, our algorithm requires $3k+4$ steps, and is asymptotically\noptimal. Our algorithm generalizes for an arbitrary number of colours, and\nterminates in $O(nk)$ steps. We also show how to extend it to achieve arbitrary\nspecific final patterns, provided there is at least one agent of every colour\nin every pattern.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 20:21:31 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Ehresmann", "Anne-Laure", ""], ["Lafond", "Manuel", ""], ["Narayanan", "Lata", ""], ["Opatrny", "Jaroslav", ""]]}, {"id": "1905.08898", "submitter": "Umar Farooq Minhas", "authors": "Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan\n  Li, Hantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\n  David Lomet, Tim Kraska", "title": "ALEX: An Updatable Adaptive Learned Index", "comments": null, "journal-ref": null, "doi": "10.1145/3318464.3389711", "report-no": "MSR-TR-2020-12", "categories": "cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on \"learned indexes\" has changed the way we look at the\ndecades-old field of DBMS indexing. The key idea is that indexes can be thought\nof as \"models\" that predict the position of a key in a dataset. Indexes can,\nthus, be learned. The original work by Kraska et al. shows that a learned index\nbeats a B+Tree by a factor of up to three in search time and by an order of\nmagnitude in memory footprint. However, it is limited to static, read-only\nworkloads.\n  In this paper, we present a new learned index called ALEX which addresses\npractical issues that arise when implementing learned indexes for workloads\nthat contain a mix of point lookups, short range queries, inserts, updates, and\ndeletes. ALEX effectively combines the core insights from learned indexes with\nproven storage and indexing techniques to achieve high performance and low\nmemory footprint. On read-only workloads, ALEX beats the learned index from\nKraska et al. by up to 2.2X on performance with up to 15X smaller index size.\nAcross the spectrum of read-write workloads, ALEX beats B+Trees by up to 4.1X\nwhile never performing worse, with up to 2000X smaller index size. We believe\nALEX presents a key step towards making learned indexes practical for a broader\nclass of database workloads with dynamic updates.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 23:22:01 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 03:12:24 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Ding", "Jialin", ""], ["Minhas", "Umar Farooq", ""], ["Yu", "Jia", ""], ["Wang", "Chi", ""], ["Do", "Jaeyoung", ""], ["Li", "Yinan", ""], ["Zhang", "Hantian", ""], ["Chandramouli", "Badrish", ""], ["Gehrke", "Johannes", ""], ["Kossmann", "Donald", ""], ["Lomet", "David", ""], ["Kraska", "Tim", ""]]}, {"id": "1905.08974", "submitter": "Sung Gwan Park", "authors": "Sung Gwan Park, Amihood Amir, Gad M. Landau and Kunsoo Park", "title": "Cartesian Tree Matching and Indexing", "comments": "14 pages, 3 figures, Submitted to CPM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new metric of match, called Cartesian tree matching, which\nmeans that two strings match if they have the same Cartesian trees. Based on\nCartesian tree matching, we define single pattern matching for a text of length\nn and a pattern of length m, and multiple pattern matching for a text of length\nn and k patterns of total length m. We present an O(n+m) time algorithm for\nsingle pattern matching, and an O((n+m) log k) deterministic time or O(n+m)\nrandomized time algorithm for multiple pattern matching. We also define an\nindex data structure called Cartesian suffix tree, and present an O(n)\nrandomized time algorithm to build the Cartesian suffix tree. Our efficient\nalgorithms for Cartesian tree matching use a representation of the Cartesian\ntree, called the parent-distance representation.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 06:15:31 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Park", "Sung Gwan", ""], ["Amir", "Amihood", ""], ["Landau", "Gad M.", ""], ["Park", "Kunsoo", ""]]}, {"id": "1905.08977", "submitter": "Yiyan Qi", "authors": "Pinghui Wang, Yiyan Qi, Yuanming Zhang, Qiaozhu Zhai, Chenxu Wang,\n  John C.S. Lui, Xiaohong Guan", "title": "A Memory-Efficient Sketch Method for Estimating High Similarities in\n  Streaming Sets", "comments": null, "journal-ref": null, "doi": "10.1145/3292500.3330825", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating set similarity and detecting highly similar sets are fundamental\nproblems in areas such as databases, machine learning, and information\nretrieval. MinHash is a well-known technique for approximating Jaccard\nsimilarity of sets and has been successfully used for many applications such as\nsimilarity search and large scale learning. Its two compressed versions, b-bit\nMinHash and Odd Sketch, can significantly reduce the memory usage of the\noriginal MinHash method, especially for estimating high similarities (i.e.,\nsimilarities around 1). Although MinHash can be applied to static sets as well\nas streaming sets, of which elements are given in a streaming fashion and\ncardinality is unknown or even infinite, unfortunately, b-bit MinHash and Odd\nSketch fail to deal with streaming data. To solve this problem, we design a\nmemory efficient sketch method, MaxLogHash, to accurately estimate Jaccard\nsimilarities in streaming sets. Compared to MinHash, our method uses smaller\nsized registers (each register consists of less than 7 bits) to build a compact\nsketch for each set. We also provide a simple yet accurate estimator for\ninferring Jaccard similarity from MaxLogHash sketches. In addition, we derive\nformulas for bounding the estimation error and determine the smallest necessary\nmemory usage (i.e., the number of registers used for a MaxLogHash sketch) for\nthe desired accuracy. We conduct experiments on a variety of datasets, and\nexperimental results show that our method MaxLogHash is about 5 times more\nmemory efficient than MinHash with the same accuracy and computational cost for\nestimating high similarities.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 06:39:48 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Wang", "Pinghui", ""], ["Qi", "Yiyan", ""], ["Zhang", "Yuanming", ""], ["Zhai", "Qiaozhu", ""], ["Wang", "Chenxu", ""], ["Lui", "John C. S.", ""], ["Guan", "Xiaohong", ""]]}, {"id": "1905.09014", "submitter": "Orna Agmon Ben-Yehuda", "authors": "Liran Funaro, Orna Agmon Ben-Yehuda, Assaf Schuster", "title": "Efficient Multi-Resource, Multi-Unit VCG Auction", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimization problem of a multi-resource, multi-unit VCG\nauction that produces an optimal, i.e., non-approximated, social welfare. We\npresent an algorithm that solves this optimization problem with\npseudo-polynomial complexity and demonstrate its efficiency via our\nimplementation. Our implementation is efficient enough to be deployed in real\nsystems to allocate computing resources in fine time-granularity. Our algorithm\nhas a pseudo-near-linear time complexity on average (over all possible\nrealistic inputs) with respect to the number of clients and the number of\npossible unit allocations. In the worst case, it is quadratic with respect to\nthe number of possible allocations. Our experiments validate our analysis and\nshow near-linear complexity. This is in contrast to the unbounded,\nnonpolynomial complexity of known solutions, which do not scale well for a\nlarge number of agents.\n  For a single resource and concave valuations, our algorithm reproduces the\nresults of a well-known algorithm. It does so, however, without subjecting the\nvaluations to any restrictions and supports a multiple resource auction, which\nimproves the social welfare over a combination of single-resource auctions by a\nfactor of 2.5-50. This makes our algorithm applicable to real clients in a real\nsystem.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 08:27:39 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Funaro", "Liran", ""], ["Ben-Yehuda", "Orna Agmon", ""], ["Schuster", "Assaf", ""]]}, {"id": "1905.09016", "submitter": "Shreyas Pai", "authors": "Shreyas Pai, Sriram V. Pemmaraju", "title": "Connectivity Lower Bounds in Broadcast Congested Clique", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove three new lower bounds for graph connectivity in the $1$-bit\nbroadcast congested clique model, BCC$(1)$. First, in the KT-$0$ version of\nBCC$(1)$, in which nodes are aware of neighbors only through port numbers, we\nshow an $\\Omega(\\log n)$ round lower bound for CONNECTIVITY even for\nconstant-error randomized Monte Carlo algorithms. The deterministic version of\nthis result can be obtained via the well-known \"edge-crossing\" argument, but,\nthe randomized version of this result requires establishing new combinatorial\nresults regarding the indistinguishability graph induced by inputs. In our\nsecond result, we show that the $\\Omega(\\log n)$ lower bound result extends to\nthe KT-$1$ version of the BCC$(1)$ model, in which nodes are aware of IDs of\nall neighbors, though our proof works only for deterministic algorithms. Since\nnodes know IDs of their neighbors in the KT-$1$ model, it is no longer possible\nto play \"edge-crossing\" tricks; instead we present a reduction from the 2-party\ncommunication complexity problem PARTITION in which Alice and Bob are give two\nset partitions on $[n]$ and are required to determine if the join of these two\nset partitions equals the trivial one-part set partition. While our KT-$1$\nCONNECTIVITY lower bound holds only for deterministic algorithms, in our third\nresult we extend this $\\Omega(\\log n)$ KT-1 lower bound to constant-error Monte\nCarlo algorithms for the closely related CONNECTED COMPONENTS problem. We use\ninformation-theoretic techniques to obtain this result. All our results hold\nfor the seemingly easy special case of CONNECTIVITY in which an algorithm has\nto distinguish an instance with one cycle from an instance with multiple\ncycles. Our results showcase three rather different lower bound techniques and\nlay the groundwork for further improvements in lower bounds for CONNECTIVITY in\nthe BCC$(1)$ model.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 08:35:09 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Pai", "Shreyas", ""], ["Pemmaraju", "Sriram V.", ""]]}, {"id": "1905.09083", "submitter": "Ismail Berrada", "authors": "Rachid Oucheikh, Ismail Berrada, Outman El Hichami", "title": "A Hypergraph Based Approach for the 4-Constraint Satisfaction Problem\n  Tractability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Constraint Satisfaction Problem (CSP) is a framework for modeling and solving\na variety of real-world problems. Once the problem is expressed as a finite set\nof constraints, the goal is to find the variables' values satisfying them. Even\nthough the problem is in general NP-complete, there are some approximation and\npractical techniques to tackle its intractability. One of the most widely used\ntechniques is the Constraint Propagation. It consists in explicitly excluding\nvalues or combination of values for some variables whenever they make a given\nsubset of constraints unsatisfied. In this paper, we deal with a CSP subclass\nwhich we call 4-CSP and whose constraint network infers relations of the form:\n$\\{ x \\sim \\alpha, x-y \\sim \\beta , (x-y) - (z-t) \\sim \\lambda \\}$, where $x,\ny, z$ and $t$ are real variables, $\\alpha , \\beta$ and $ \\lambda $ are real\nconstants and $ \\sim \\in \\{\\leq , \\geq \\} $. The paper provides the first\ngraph-based proofs of the 4-CSP tractability and elaborates algorithms for\n4-CSP resolution based on the positive linear dependence theory, the hypergraph\nclosure and the constraint propagation technique. Time and space complexities\nof the resolution algorithms are proved to be polynomial.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 11:43:49 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Oucheikh", "Rachid", ""], ["Berrada", "Ismail", ""], ["Hichami", "Outman El", ""]]}, {"id": "1905.09175", "submitter": "Nikos Parotsidis", "authors": "Giuseppe F. Italiano, Silvio Lattanzi, Vahab S. Mirrokni and Nikos\n  Parotsidis", "title": "Dynamic Algorithms for the Massively Parallel Computation Model", "comments": "Accepted to the 31st ACM Symposium on Parallelism in Algorithms and\n  Architectures (SPAA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Massive Parallel Computing (MPC) model gained popularity during the last\ndecade and it is now seen as the standard model for processing large scale\ndata. One significant shortcoming of the model is that it assumes to work on\nstatic datasets while, in practice, real-world datasets evolve continuously. To\novercome this issue, in this paper we initiate the study of dynamic algorithms\nin the MPC model.\n  We first discuss the main requirements for a dynamic parallel model and we\nshow how to adapt the classic MPC model to capture them. Then we analyze the\nconnection between classic dynamic algorithms and dynamic algorithms in the MPC\nmodel. Finally, we provide new efficient dynamic MPC algorithms for a variety\nof fundamental graph problems, including connectivity, minimum spanning tree\nand matching.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 14:44:37 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Italiano", "Giuseppe F.", ""], ["Lattanzi", "Silvio", ""], ["Mirrokni", "Vahab S.", ""], ["Parotsidis", "Nikos", ""]]}, {"id": "1905.09356", "submitter": "Biyi Fang", "authors": "Biyi Fang and Diego Klabjan", "title": "Convergence Analyses of Online ADAM Algorithm in Convex Setting and\n  Two-Layer ReLU Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, online learning is an appealing learning paradigm, which is of\ngreat interest in practice due to the recent emergence of large scale\napplications such as online advertising placement and online web ranking.\nStandard online learning assumes a finite number of samples while in practice\ndata is streamed infinitely. In such a setting gradient descent with a\ndiminishing learning rate does not work. We first introduce regret with rolling\nwindow, a new performance metric for online streaming learning, which measures\nthe performance of an algorithm on every fixed number of contiguous samples. At\nthe same time, we propose a family of algorithms based on gradient descent with\na constant or adaptive learning rate and provide very technical analyses\nestablishing regret bound properties of the algorithms. We cover the convex\nsetting showing the regret of the order of the square root of the size of the\nwindow in the constant and dynamic learning rate scenarios. Our proof is\napplicable also to the standard online setting where we provide the first\nanalysis of the same regret order (the previous proofs have flaws). We also\nstudy a two layer neural network setting with ReLU activation. In this case we\nestablish that if initial weights are close to a stationary point, the same\nsquare root regret bound is attainable. We conduct computational experiments\ndemonstrating a superior performance of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 20:37:48 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 00:57:49 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Fang", "Biyi", ""], ["Klabjan", "Diego", ""]]}, {"id": "1905.09505", "submitter": "Yixin Cao", "authors": "Yixin Cao and Guozhen Rong and Jianxin Wang and Zhifeng Wang", "title": "Graph Searches and Their End Vertices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph search, the process of visiting vertices in a graph in a specific\norder, has demonstrated magical powers in many important algorithms. But a\nsystematic study was only initiated by Corneil et al.~a decade ago, and only by\nthen we started to realize how little we understand it. Even the apparently\nna\\\"{i}ve question \"which vertex can be the last visited by a graph search\nalgorithm,\" known as the end vertex problem, turns out to be quite elusive. We\ngive a full picture of all maximum cardinality searches on chordal graphs,\nwhich implies a polynomial-time algorithm for the end vertex problem of maximum\ncardinality search. It is complemented by a proof of NP-completeness of the\nsame problem on weakly chordal graphs.\n  We also show linear-time algorithms for deciding end vertices of\nbreadth-first searches on interval graphs, and end vertices of lexicographic\ndepth-first searches on chordal graphs. Finally, we present $2^n\\cdot\nn^{O(1)}$-time algorithms for deciding the end vertices of breadth-first\nsearches, depth-first searches, maximum cardinality searches, and maximum\nneighborhood searches on general graphs.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 07:18:21 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Cao", "Yixin", ""], ["Rong", "Guozhen", ""], ["Wang", "Jianxin", ""], ["Wang", "Zhifeng", ""]]}, {"id": "1905.09595", "submitter": "Thang Nguyen Kim", "authors": "Christoph D\\\"urr, Nguyen Kim Thang, Abhinav Srivastav, L\\'eo Tible", "title": "Non-monotone DR-submodular Maximization: Approximation and Regret\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diminishing-returns (DR) submodular optimization is an important field with\nmany real-world applications in machine learning, economics and communication\nsystems. It captures a subclass of non-convex optimization that provides both\npractical and theoretical guarantees. In this paper, we study the fundamental\nproblem of maximizing non-monotone DR-submodular functions over down-closed and\ngeneral convex sets in both offline and online settings. First, we show that\nfor offline maximizing non-monotone DR-submodular functions over a general\nconvex set, the Frank-Wolfe algorithm achieves an approximation guarantee which\ndepends on the convex set. Next, we show that the Stochastic Gradient Ascent\nalgorithm achieves a 1/4-approximation ratio with the regret of $O(1/\\sqrt{T})$\nfor the problem of maximizing non-monotone DR-submodular functions over\ndown-closed convex sets. These are the first approximation guarantees in the\ncorresponding settings. Finally we benchmark these algorithms on problems\narising in machine learning domain with the real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 11:38:52 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["D\u00fcrr", "Christoph", ""], ["Thang", "Nguyen Kim", ""], ["Srivastav", "Abhinav", ""], ["Tible", "L\u00e9o", ""]]}, {"id": "1905.09624", "submitter": "Timo Bingmann", "authors": "Timo Bingmann, Phelim Bradley, Florian Gauger, and Zamin Iqbal", "title": "COBS: a Compact Bit-Sliced Signature Index", "comments": "To appear in 26th International Symposium on String Processing and\n  Information Retrieval (SPIRE'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present COBS, a COmpact Bit-sliced Signature index, which is a cross-over\nbetween an inverted index and Bloom filters. Our target application is to index\n$k$-mers of DNA samples or $q$-grams from text documents and process\napproximate pattern matching queries on the corpus with a user-chosen coverage\nthreshold. Query results may contain a number of false positives which\ndecreases exponentially with the query length. We compare COBS to seven other\nindex software packages on 100000 microbial DNA samples. COBS' compact but\nsimple data structure outperforms the other indexes in construction time and\nquery performance with Mantis by Pandey et al. in second place. However, unlike\nMantis and other previous work, COBS does not need the complete index in RAM\nand is thus designed to scale to larger document sets.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 12:49:58 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 13:00:54 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Bingmann", "Timo", ""], ["Bradley", "Phelim", ""], ["Gauger", "Florian", ""], ["Iqbal", "Zamin", ""]]}, {"id": "1905.09656", "submitter": "Armin Wei{\\ss}", "authors": "Florian Stober, Armin Wei{\\ss}", "title": "On the Average Case of MergeInsertion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MergeInsertion, also known as the Ford-Johnson algorithm, is a sorting\nalgorithm which, up to today, for many input sizes achieves the best known\nupper bound on the number of comparisons. Indeed, it gets extremely close to\nthe information-theoretic lower bound. While the worst-case behavior is well\nunderstood, only little is known about the average case.\n  This work takes a closer look at the average case behavior. In particular, we\nestablish an upper bound of $n \\log n - 1.4005n + o(n)$ comparisons. We also\ngive an exact description of the probability distribution of the length of the\nchain a given element is inserted into and use it to approximate the average\nnumber of comparisons numerically. Moreover, we compute the exact average\nnumber of comparisons for $n$ up to 148.\n  Furthermore, we experimentally explore the impact of different decision trees\nfor binary insertion. To conclude, we conduct experiments showing that a\nslightly different insertion order leads to a better average case and we\ncompare the algorithm to the recent combination with (1,2)-Insertionsort by\nIwama and Teruyama.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 13:50:56 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Stober", "Florian", ""], ["Wei\u00df", "Armin", ""]]}, {"id": "1905.09719", "submitter": "Shaojie Tang", "authors": "Shaojie Tang", "title": "Price of Dependence: Stochastic Submodular Maximization with Dependent\n  Items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the stochastic submodular maximization problem with\ndependent items subject to packing constraints such as matroid and knapsack\nconstraints. The input of our problem is a finite set of items, and each item\nis in a particular state from a set of possible states. After picking an item,\nwe are able to observe its state. We assume a monotone and submodular utility\nfunction over items and states, and our objective is to select a group of items\nadaptively so as to maximize the expected utility. Previous studies on\nstochastic submodular maximization often assume that items' states are\nindependent, however, this assumption may not hold in general. This motivates\nus to study the stochastic submodular maximization problem with dependent\nitems. We first introduce the concept of \\emph{degree of independence} to\ncapture the degree to which one item's state is dependent on others'. Then we\npropose a non-adaptive policy that approximates the optimal adaptive policy\nwithin a factor of\n$\\alpha(1-e^{-\\frac{\\kappa}{2}+\\frac{\\kappa}{18m^2}}-\\frac{\\kappa+2}{3m\\kappa})$\nwhere the value of $\\alpha$ is depending on the type of constraints, e.g.,\n$\\alpha=1$ for matroid constraint, $\\kappa$ is the degree of independence,\ne.g., $\\kappa=1$ for independent items, and $m$ is the number of items. We also\nanalyze the adaptivity gap, i.e., the ratio of the values of best adaptive\npolicy and best non-adaptive policy, of our problem with prefix-closed\nconstraints.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 15:24:58 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 03:50:15 GMT"}, {"version": "v3", "created": "Thu, 11 Jul 2019 17:18:41 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Tang", "Shaojie", ""]]}, {"id": "1905.09750", "submitter": "Asaf Levin", "authors": "Asaf Levin", "title": "Approximation schemes for the generalized extensible bin packing problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new generalization of the extensible bin packing with unequal\nbin sizes problem. In our generalization the cost of exceeding the bin size\ndepends on the index of the bin and not only on the amount in which the size of\nthe bin is exceeded. This generalization does not satisfy the assumptions on\nthe cost function that were used to present the existing polynomial time\napproximation scheme (PTAS) for the extensible bin packing with unequal bin\nsizes problem. In this work, we show the existence of an efficient PTAS (EPTAS)\nfor this new generalization and thus in particular we improve the earlier PTAS\nfor the extensible bin packing with unequal bin sizes problem into an EPTAS.\nOur new scheme is based on using the shifting technique followed by a solution\nof polynomial number of $n$-fold programming instances. In addition, we present\nan asymptotic fully polynomial time approximation scheme (AFPTAS) for the\nrelated bin packing type variant of the problem.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 16:11:45 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Levin", "Asaf", ""]]}, {"id": "1905.09761", "submitter": "Md Faisal Mahbub Chowdhury", "authors": "Md Faisal Mahbub Chowdhury, Robert Farrell", "title": "An Efficient Approach for Super and Nested Term Indexing and Retrieval", "comments": "6 pages including references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a new approach, called Terminological Bucket Indexing\n(TBI), for efficient indexing and retrieval of both nested and super terms\nusing a single method. We propose a hybrid data structure for facilitating\nfaster indexing building. An evaluation of our approach with respect to widely\nused existing approaches on several publicly available dataset is provided.\nCompared to Trie based approaches, TBI provides comparable performance on\nnested term retrieval and far superior performance on super term retrieval.\nCompared to traditional hash table, TBI needs 80\\% less time for indexing.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 16:33:30 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Chowdhury", "Md Faisal Mahbub", ""], ["Farrell", "Robert", ""]]}, {"id": "1905.09898", "submitter": "Thodoris Lykouris", "authors": "Thodoris Lykouris, Eva Tardos, Drishti Wali", "title": "Feedback graph regret bounds for Thompson Sampling and UCB", "comments": "Appeared in ALT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stochastic multi-armed bandit problem with the graph-based\nfeedback structure introduced by Mannor and Shamir. We analyze the performance\nof the two most prominent stochastic bandit algorithms, Thompson Sampling and\nUpper Confidence Bound (UCB), in the graph-based feedback setting. We show that\nthese algorithms achieve regret guarantees that combine the graph structure and\nthe gaps between the means of the arm distributions. Surprisingly this holds\ndespite the fact that these algorithms do not explicitly use the graph\nstructure to select arms; they observe the additional feedback but do not\nexplore based on it. Towards this result we introduce a \"layering technique\"\nhighlighting the commonalities in the two algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 20:05:06 GMT"}, {"version": "v2", "created": "Sat, 28 Sep 2019 23:02:41 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 05:02:59 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Lykouris", "Thodoris", ""], ["Tardos", "Eva", ""], ["Wali", "Drishti", ""]]}, {"id": "1905.09952", "submitter": "Wenshuo Guo", "authors": "Wenshuo Guo, Nhat Ho, Michael I. Jordan", "title": "Fast Algorithms for Computational Optimal Transport and Wasserstein\n  Barycenter", "comments": "18 pages, 35 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide theoretical complexity analysis for new algorithms to compute the\noptimal transport (OT) distance between two discrete probability distributions,\nand demonstrate their favorable practical performance over state-of-art\nprimal-dual algorithms and their capability in solving other problems in\nlarge-scale, such as the Wasserstein barycenter problem for multiple\nprobability distributions. First, we introduce the \\emph{accelerated\nprimal-dual randomized coordinate descent} (APDRCD) algorithm for computing the\nOT distance. We provide its complexity upper bound\n$\\bigOtil(\\frac{n^{5/2}}{\\varepsilon})$ where $n$ stands for the number of\natoms of these probability measures and $\\varepsilon > 0$ is the desired\naccuracy. This complexity bound matches the best known complexities of\nprimal-dual algorithms for the OT problems, including the adaptive primal-dual\naccelerated gradient descent (APDAGD) and the adaptive primal-dual accelerated\nmirror descent (APDAMD) algorithms. Then, we demonstrate the better performance\nof the APDRCD algorithm over the APDAGD and APDAMD algorithms through extensive\nexperimental studies, and further improve its practical performance by\nproposing a greedy version of it, which we refer to as \\emph{accelerated\nprimal-dual greedy coordinate descent} (APDGCD). Finally, we generalize the\nAPDRCD and APDGCD algorithms to distributed algorithms for computing the\nWasserstein barycenter for multiple probability distributions.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 22:13:27 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 01:57:16 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 10:50:02 GMT"}, {"version": "v4", "created": "Mon, 15 Jun 2020 06:00:24 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Guo", "Wenshuo", ""], ["Ho", "Nhat", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1905.09992", "submitter": "Frederic Koehler", "authors": "Frederic Koehler", "title": "Fast Convergence of Belief Propagation to Global Optima: Beyond\n  Correlation Decay", "comments": "24 pages; comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief propagation is a fundamental message-passing algorithm for\nprobabilistic reasoning and inference in graphical models. While it is known to\nbe exact on trees, in most applications belief propagation is run on graphs\nwith cycles. Understanding the behavior of \"loopy\" belief propagation has been\na major challenge for researchers in machine learning, and positive convergence\nresults for BP are known under strong assumptions which imply the underlying\ngraphical model exhibits decay of correlations. We show that under a natural\ninitialization, BP converges quickly to the global optimum of the Bethe free\nenergy for Ising models on arbitrary graphs, as long as the Ising model is\n\\emph{ferromagnetic} (i.e. neighbors prefer to be aligned). This holds even\nthough such models can exhibit long range correlations and may have multiple\nsuboptimal BP fixed points. We also show an analogous result for iterating the\n(naive) mean-field equations; perhaps surprisingly, both results are\ndimension-free in the sense that a constant number of iterations already\nprovides a good estimate to the Bethe/mean-field free energy.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 01:42:31 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Koehler", "Frederic", ""]]}, {"id": "1905.10143", "submitter": "Hu Ding", "authors": "Hu Ding, Jiawei Huang and Haikuo Yu", "title": "The Effectiveness of Uniform Sampling for Center-Based Clustering with\n  Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering has many important applications in computer science, but\nreal-world datasets often contain outliers. Moreover, the presence of outliers\ncan make the clustering problems to be much more challenging. To reduce the\ncomplexities, various sampling methods have been proposed in past years.\nNamely, we take a small sample (uniformly or non-uniformly) from input and run\nan existing approximation algorithm on the sample. Comparing with existing\nnon-uniform sampling methods, the uniform sampling approach has several\nsignificant benefits. For example, it only needs to read the data in one-pass\nand is very easy to implement in practice. Thus, the effectiveness of uniform\nsampling for clustering with outliers is a natural and fundamental problem\ndeserving to study in both theory and practice. In this paper, we propose a new\nand unified framework for analyzing the effectiveness of uniform sampling for\nthree representative center-based clustering with outliers problems,\n$k$-center/median/means clustering with outliers. We introduce a \"significance\"\ncriterion and prove that the performance of uniform sampling depends on the\nsignificance degree of the given instance. In particular, we show that the\nsample size can be independent of the ratio $n/z$ and the dimensionality. More\nimportantly, to the best of our knowledge, our method is the first uniform\nsampling approach that allows to discard exactly $z$ outliers for these three\ncenter-based clustering with outliers problems. The results proposed in this\npaper also can be viewed as an extension of the previous sub-linear time\nalgorithms for the ordinary clustering problems (without outliers). The\nexperiments suggest that the uniform sampling method can achieve comparable\nclustering results with other existing methods, but greatly reduce the running\ntimes.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 10:51:46 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 14:43:12 GMT"}, {"version": "v3", "created": "Fri, 1 May 2020 01:49:08 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Ding", "Hu", ""], ["Huang", "Jiawei", ""], ["Yu", "Haikuo", ""]]}, {"id": "1905.10284", "submitter": "Michal Dory", "authors": "Nir Bachrach, Keren Censor-Hillel, Michal Dory, Yuval Efron, Dean\n  Leitersdorf, Ami Paz", "title": "Hardness of Distributed Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies lower bounds for fundamental optimization problems in the\nCONGEST model. We show that solving problems exactly in this model can be a\nhard task, by providing $\\tilde{\\Omega}(n^2)$ lower bounds for cornerstone\nproblems, such as minimum dominating set (MDS), Hamiltonian path, Steiner tree\nand max-cut. These are almost tight, since all of these problems can be solved\noptimally in $O(n^2)$ rounds. Moreover, we show that even in bounded-degree\ngraphs and even in simple graphs with maximum degree 5 and logarithmic\ndiameter, it holds that various tasks, such as finding a maximum independent\nset (MaxIS) or a minimum vertex cover, are still difficult, requiring a\nnear-tight number of $\\tilde{\\Omega}(n)$ rounds.\n  Furthermore, we show that in some cases even approximations are difficult, by\nproviding an $\\tilde{\\Omega}(n^2)$ lower bound for a\n$(7/8+\\epsilon)$-approximation for MaxIS, and a nearly-linear lower bound for\nan $O(\\log{n})$-approximation for the $k$-MDS problem for any constant $k \\geq\n2$, as well as for several variants of the Steiner tree problem.\n  Our lower bounds are based on a rich variety of constructions that leverage\nnovel observations, and reductions among problems that are specialized for the\nCONGEST model. However, for several additional approximation problems, as well\nas for exact computation of some central problems in $P$, such as maximum\nmatching and max flow, we show that such constructions cannot be designed, by\nwhich we exemplify some limitations of this framework.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 15:24:23 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Bachrach", "Nir", ""], ["Censor-Hillel", "Keren", ""], ["Dory", "Michal", ""], ["Efron", "Yuval", ""], ["Leitersdorf", "Dean", ""], ["Paz", "Ami", ""]]}, {"id": "1905.10337", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li", "title": "What Can ResNet Learn Efficiently, Going Beyond Kernels?", "comments": "V2 slightly improves lower bound, V3 strengthens experiments and adds\n  citation to \"backward feature correction\" which is an even stronger form of\n  hierarchical learning [2]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can neural networks such as ResNet efficiently learn CIFAR-10 with test\naccuracy more than 96%, while other methods, especially kernel methods, fall\nrelatively behind? Can we more provide theoretical justifications for this gap?\n  Recently, there is an influential line of work relating neural networks to\nkernels in the over-parameterized regime, proving they can learn certain\nconcept class that is also learnable by kernels with similar test error. Yet,\ncan neural networks provably learn some concept class BETTER than kernels?\n  We answer this positively in the distribution-free setting. We prove neural\nnetworks can efficiently learn a notable class of functions, including those\ndefined by three-layer residual networks with smooth activations, without any\ndistributional assumption. At the same time, we prove there are simple\nfunctions in this class such that with the same number of training examples,\nthe test error obtained by neural networks can be MUCH SMALLER than ANY kernel\nmethod, including neural tangent kernels (NTK).\n  The main intuition is that multi-layer neural networks can implicitly perform\nhierarchical learning using different layers, which reduces the sample\ncomplexity comparing to \"one-shot\" learning algorithms such as kernel methods.\nIn a follow-up work [2], this theory of hierarchical learning is further\nstrengthened to incorporate the \"backward feature correction\" process when\ntraining deep networks.\n  In the end, we also prove a computation complexity advantage of ResNet with\nrespect to other learning methods including linear regression over arbitrary\nfeature mappings.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 17:02:51 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 07:25:18 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2020 17:25:52 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1905.10360", "submitter": "Roy Frostig", "authors": "Vitaly Feldman, Roy Frostig, Moritz Hardt", "title": "The advantages of multiple classes for reducing overfitting from test\n  set reuse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Excessive reuse of holdout data can lead to overfitting. However, there is\nlittle concrete evidence of significant overfitting due to holdout reuse in\npopular multiclass benchmarks today. Known results show that, in the\nworst-case, revealing the accuracy of $k$ adaptively chosen classifiers on a\ndata set of size $n$ allows to create a classifier with bias of\n$\\Theta(\\sqrt{k/n})$ for any binary prediction problem. We show a new upper\nbound of $\\tilde O(\\max\\{\\sqrt{k\\log(n)/(mn)},k/n\\})$ on the worst-case bias\nthat any attack can achieve in a prediction problem with $m$ classes. Moreover,\nwe present an efficient attack that achieve a bias of $\\Omega(\\sqrt{k/(m^2\nn)})$ and improves on previous work for the binary setting ($m=2$). We also\npresent an inefficient attack that achieves a bias of $\\tilde\\Omega(k/n)$.\nComplementing our theoretical work, we give new practical attacks to\nstress-test multiclass benchmarks by aiming to create as large a bias as\npossible with a given number of queries. Our experiments show that the\nadditional uncertainty of prediction with a large number of classes indeed\nmitigates the effect of our best attacks.\n  Our work extends developments in understanding overfitting due to adaptive\ndata analysis to multiclass prediction problems. It also bears out the\nsurprising fact that multiclass prediction problems are significantly more\nrobust to overfitting when reusing a test (or holdout) dataset. This offers an\nexplanation as to why popular multiclass prediction benchmarks, such as\nImageNet, may enjoy a longer lifespan than what intuition from literature on\nbinary classification suggests.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 17:59:32 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Feldman", "Vitaly", ""], ["Frostig", "Roy", ""], ["Hardt", "Moritz", ""]]}, {"id": "1905.10415", "submitter": "Juan Miguel Arrazola", "authors": "Juan Miguel Arrazola, Alain Delgado, Bhaskar Roy Bardhan, Seth Lloyd", "title": "Quantum-inspired algorithms in practice", "comments": "A popular summary can be found at\n  https://medium.com/xanaduai/everything-you-always-wanted-to-know-about-quantum-inspired-algorithms-38ee1a0e30ef\n  . Source code is available at\n  https://github.com/XanaduAI/quantum-inspired-algorithms", "journal-ref": "Quantum 4, 307 (2020)", "doi": "10.22331/q-2020-08-13-307", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the practical performance of quantum-inspired algorithms for\nrecommendation systems and linear systems of equations. These algorithms were\nshown to have an exponential asymptotic speedup compared to previously known\nclassical methods for problems involving low-rank matrices, but with complexity\nbounds that exhibit a hefty polynomial overhead compared to quantum algorithms.\nThis raised the question of whether these methods were actually useful in\npractice. We conduct a theoretical analysis aimed at identifying their\ncomputational bottlenecks, then implement and benchmark the algorithms on a\nvariety of problems, including applications to portfolio optimization and movie\nrecommendations. On the one hand, our analysis reveals that the performance of\nthese algorithms is better than the theoretical complexity bounds would\nsuggest. On the other hand, their performance as seen in our implementation\ndegrades noticeably as the rank and condition number of the input matrix are\nincreased. Overall, our results indicate that quantum-inspired algorithms can\nperform well in practice provided that stringent conditions are met: low rank,\nlow condition number, and very large dimension of the input matrix. By\ncontrast, practical datasets are often sparse and high-rank, precisely the type\nthat can be handled by quantum algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 19:17:30 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 21:03:50 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 21:55:53 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Arrazola", "Juan Miguel", ""], ["Delgado", "Alain", ""], ["Bardhan", "Bhaskar Roy", ""], ["Lloyd", "Seth", ""]]}, {"id": "1905.10477", "submitter": "Jonathan Ullman", "authors": "Adam Sealfon and Jonathan Ullman", "title": "Efficiently Estimating Erdos-Renyi Graphs with Node Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple, computationally efficient, and node-differentially-private\nalgorithm for estimating the parameter of an Erdos-Renyi graph---that is,\nestimating p in a G(n,p)---with near-optimal accuracy. Our algorithm nearly\nmatches the information-theoretically optimal exponential-time algorithm for\nthe same problem due to Borgs et al. (FOCS 2018). More generally, we give an\noptimal, computationally efficient, private algorithm for estimating the\nedge-density of any graph whose degree distribution is concentrated on a small\ninterval.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 23:16:58 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Sealfon", "Adam", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1905.10510", "submitter": "Chang Xiao", "authors": "Chang Xiao, Peilin Zhong and Changxi Zheng", "title": "Enhancing Adversarial Defense by k-Winners-Take-All", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple change to existing neural network structures for better\ndefending against gradient-based adversarial attacks. Instead of using popular\nactivation functions (such as ReLU), we advocate the use of k-Winners-Take-All\n(k-WTA) activation, a C0 discontinuous function that purposely invalidates the\nneural network model's gradient at densely distributed input data points. The\nproposed k-WTA activation can be readily used in nearly all existing networks\nand training methods with no significant overhead. Our proposal is\ntheoretically rationalized. We analyze why the discontinuities in k-WTA\nnetworks can largely prevent gradient-based search of adversarial examples and\nwhy they at the same time remain innocuous to the network training. This\nunderstanding is also empirically backed. We test k-WTA activation on various\nnetwork structures optimized by a training method, be it adversarial training\nor not. In all cases, the robustness of k-WTA networks outperforms that of\ntraditional networks under white-box attacks.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 03:36:40 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 20:14:15 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 00:27:18 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Xiao", "Chang", ""], ["Zhong", "Peilin", ""], ["Zheng", "Changxi", ""]]}, {"id": "1905.10592", "submitter": "S\\\"oren Schmitt", "authors": "Yann Disser and S\\\"oren Schmitt", "title": "Evacuating Two Robots from a Disk: A Second Cut", "comments": "19 pages, 5 figures. This is the full version of the paper with the\n  same title accepted in the 26th International Colloquium on Structural\n  Information and Communication Complexity (SIROCCO'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an improved algorithm for the problem of evacuating two robots\nfrom the unit disk via an unknown exit on the boundary. Robots start at the\ncenter of the disk, move at unit speed, and can only communicate locally. Our\nalgorithm improves previous results by Brandt et al. [CIAC'17] by introducing a\nsecond detour through the interior of the disk. This allows for an improved\nevacuation time of $5.6234$. The best known lower bound of $5.255$ was shown by\nCzyzowicz et al. [CIAC'15].\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 13:01:58 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Disser", "Yann", ""], ["Schmitt", "S\u00f6ren", ""]]}, {"id": "1905.10670", "submitter": "Yota Otachi", "authors": "Hans L. Bodlaender, Tesshu Hanaka, Yasuaki Kobayashi, Yusuke\n  Kobayashi, Yoshio Okamoto, Yota Otachi, Tom C. van der Zanden", "title": "Subgraph Isomorphism on Graph Classes that Exclude a Substructure", "comments": "15 pages, 5 figures. CIAC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Subgraph Isomorphism on graph classes defined by a fixed forbidden\ngraph. Although there are several ways for forbidding a graph, we observe that\nit is reasonable to focus on the minor relation since other well-known\nrelations lead to either trivial or equivalent problems. When the forbidden\nminor is connected, we present a near dichotomy of the complexity of Subgraph\nIsomorphism with respect to the forbidden minor, where the only unsettled case\nis $P_{5}$, the path of five vertices. We then also consider the general case\nof possibly disconnected forbidden minors. We show fixed-parameter tractable\ncases and randomized XP-time solvable cases parameterized by the size of the\nforbidden minor $H$. We also show that by slightly generalizing the tractable\ncases, the problem becomes NP-complete. All unsettle cases are equivalent to\n$P_{5}$ or the disjoint union of two $P_{5}$'s. As a byproduct, we show that\nSubgraph Isomorphism is fixed-parameter tractable parameterized by vertex\nintegrity. Using similar techniques, we also observe that Subgraph Isomorphism\nis fixed-parameter tractable parameterized by neighborhood diversity.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 20:29:32 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Bodlaender", "Hans L.", ""], ["Hanaka", "Tesshu", ""], ["Kobayashi", "Yasuaki", ""], ["Kobayashi", "Yusuke", ""], ["Okamoto", "Yoshio", ""], ["Otachi", "Yota", ""], ["van der Zanden", "Tom C.", ""]]}, {"id": "1905.10775", "submitter": "Yannic Maus", "authors": "Janosch Deurer, Fabian Kuhn, Yannic Maus", "title": "Deterministic Distributed Dominating Set Approximation in the CONGEST\n  Model", "comments": "added better reasoning in the proof of Lemma 3.12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop deterministic approximation algorithms for the minimum dominating\nset problem in the CONGEST model with an almost optimal approximation\nguarantee. For $\\epsilon>1/{\\text{{poly}}}\\log \\Delta$ we obtain two algorithms\nwith approximation factor $(1+\\epsilon)(1+\\ln (\\Delta+1))$ and with runtimes\n$2^{O(\\sqrt{\\log n \\log\\log n})}$ and $O(\\Delta\\cdot\\text{poly}\\log \\Delta\n+\\text{poly}\\log \\Delta \\log^{*} n)$, respectively. Further we show how\ndominating set approximations can be deterministically transformed into a\nconnected dominating set in the \\CONGEST model while only increasing the\napproximation guarantee by a constant factor. This results in a deterministic\n$O(\\log \\Delta)$-approximation algorithm for the minimum connected dominating\nset with time complexity\n  $2^{O(\\sqrt{\\log n \\log\\log n})}$.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 10:02:35 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 08:58:36 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Deurer", "Janosch", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""]]}, {"id": "1905.10809", "submitter": "Tung-Wei Kuo", "authors": "Tung-Wei Kuo", "title": "Minimum Age TDMA Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a transmission scheduling problem in which multiple systems\nreceive update information through a shared Time Division Multiple Access\n(TDMA) channel. To provide timely delivery of update information, the problem\nasks for a schedule that minimizes the overall age of information. We call this\nproblem the Min-Age problem. This problem is first studied by He \\textit{et\nal.} [IEEE Trans. Inform. Theory, 2018], who identified several special cases\nwhere the problem can be solved optimally in polynomial time. Our contribution\nis threefold. First, we introduce a new job scheduling problem called the\nMin-WCS problem, and we prove that, for any constant $r \\geq 1$, every\n$r$-approximation algorithm for the Min-WCS problem can be transformed into an\n$r$-approximation algorithm for the Min-Age problem. Second, we give a\nrandomized 2.733-approximation algorithm and a dynamic-programming-based exact\nalgorithm for the Min-WCS problem. Finally, we prove that the Min-Age problem\nis NP-hard.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 14:52:44 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 07:06:14 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Kuo", "Tung-Wei", ""]]}, {"id": "1905.10825", "submitter": "Yunzong Xu", "authors": "David Simchi-Levi, Yunzong Xu", "title": "Phase Transitions in Bandits with Switching Constraints", "comments": "An enhanced version. Many new results are obtained. The presentation\n  is improved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical stochastic multi-armed bandit problem with a\nconstraint that limits the total cost incurred by switching between actions to\nbe no larger than a given switching budget. For this problem, we prove matching\nupper and lower bounds on the optimal (i.e., minimax) regret, and provide\nefficient rate-optimal algorithms. Surprisingly, the optimal regret of this\nproblem exhibits a non-conventional growth rate in terms of the time horizon\nand the number of arms. Consequently, we discover surprising \"phase\ntransitions\" regarding how the optimal regret rate changes with respect to the\nswitching budget: when the number of arms is fixed, there are equal-length\nphases, where the optimal regret rate remains (almost) the same within each\nphase and exhibits abrupt changes between phases; when the number of arms grows\nwith the time horizon, such abrupt changes become subtler and may disappear,\nbut a generalized notion of phase transitions involving certain new\nmeasurements still exists. The results enable us to fully characterize the\ntrade-off between the regret rate and the incurred switching cost in the\nstochastic multi-armed bandit problem, contributing new insights to this\nfundamental problem. Under the general switching cost structure, the results\nreveal interesting connections between bandit problems and graph traversal\nproblems, such as the shortest Hamiltonian path problem.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 16:07:12 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 03:13:54 GMT"}, {"version": "v3", "created": "Sat, 6 Jul 2019 16:06:47 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 22:20:40 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Simchi-Levi", "David", ""], ["Xu", "Yunzong", ""]]}, {"id": "1905.10833", "submitter": "Michal Dory", "authors": "Michal Dory, Mohsen Ghaffari", "title": "Improved Distributed Approximations for Minimum-Weight\n  Two-Edge-Connected Spanning Subgraph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum-weight $2$-edge-connected spanning subgraph (2-ECSS) problem is a\nnatural generalization of the well-studied minimum-weight spanning tree (MST)\nproblem, and it has received considerable attention in the area of network\ndesign. The latter problem asks for a minimum-weight subgraph with an edge\nconnectivity of $1$ between each pair of vertices while the former strengthens\nthis edge-connectivity requirement to $2$. Despite this resemblance, the 2-ECSS\nproblem is considerably more complex than MST. While MST admits a linear-time\ncentralized exact algorithm, 2-ECSS is NP-hard and the best known centralized\napproximation algorithm for it (that runs in polynomial time) gives a\n$2$-approximation.\n  In this paper, we give a deterministic distributed algorithm with round\ncomplexity of $\\widetilde{O}(D+\\sqrt{n})$ that computes a\n$(5+\\epsilon)$-approximation of 2-ECSS, for any constant $\\epsilon>0$. Up to\nlogarithmic factors, this complexity matches the\n$\\widetilde{\\Omega}(D+\\sqrt{n})$ lower bound that can be derived from Das Sarma\net al. [STOC'11], as shown by Censor-Hillel and Dory [OPODIS'17]. Our result is\nthe first distributed constant approximation for 2-ECSS in the nearly optimal\ntime and it improves on a recent randomized algorithm of Dory [PODC'18], which\nachieved an $O(\\log n)$-approximation in $\\widetilde{O}(D+\\sqrt{n})$ rounds.\n  We also present an alternative algorithm for $O(\\log n)$-approximation, whose\nround complexity is linear in the low-congestion shortcut parameter of the\nnetwork, following a framework introduced by Ghaffari and Haeupler [SODA'16].\nThis algorithm has round complexity $\\widetilde{O}(D+\\sqrt{n})$ in worst-case\nnetworks but it provably runs much faster in many well-behaved graph families\nof interest. For instance, it runs in $\\widetilde{O}(D)$ time in planar\nnetworks and those with bounded genus, bounded path-width or bounded\ntree-width.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 16:37:22 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 11:49:00 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Dory", "Michal", ""], ["Ghaffari", "Mohsen", ""]]}, {"id": "1905.10867", "submitter": "Igor Razgon", "authors": "Andrea Cali and Igor Razgon", "title": "Regular resolution for CNF of bounded incidence treewidth with few long\n  clauses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that Regular Resolution is FPT for two restricted families of\nCNFs of bounded incidence treewidth. The first includes CNFs having at most $p$\nclauses whose removal results in a CNF of primal treewidth at most $k$. The\nparameters we use in this case are $p$ and $k$. The second class includes CNFs\nof bounded one-sided (incidence) treewdth, a new parameter generalizing both\nprimal treewidth and incidence pathwidth. The parameter we use in this case is\nthe one-sided treewidth.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 20:02:21 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Cali", "Andrea", ""], ["Razgon", "Igor", ""]]}, {"id": "1905.10902", "submitter": "Darren Strash", "authors": "Damir Ferizovic and Demian Hespe and Sebastian Lamm and Matthias Mnich\n  and Christian Schulz and Darren Strash", "title": "Engineering Kernelization for Maximum Cut", "comments": "16 pages, 4 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernelization is a general theoretical framework for preprocessing instances\nof NP-hard problems into (generally smaller) instances with bounded size, via\nthe repeated application of data reduction rules. For the fundamental Max Cut\nproblem, kernelization algorithms are theoretically highly efficient for\nvarious parameterizations. However, the efficacy of these reduction rules in\npractice---to aid solving highly challenging benchmark instances to\noptimality---remains entirely unexplored.\n  We engineer a new suite of efficient data reduction rules that subsume most\nof the previously published rules, and demonstrate their significant impact on\nbenchmark data sets, including synthetic instances, and data sets from the VLSI\nand image segmentation application domains. Our experiments reveal that current\nstate-of-the-art solvers can be sped up by up to multiple orders of magnitude\nwhen combined with our data reduction rules. On social and biological networks\nin particular, kernelization enables us to solve four instances that were\npreviously unsolved in a ten-hour time limit with state-of-the-art solvers;\nthree of these instances are now solved in less than two seconds.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 23:12:33 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Ferizovic", "Damir", ""], ["Hespe", "Demian", ""], ["Lamm", "Sebastian", ""], ["Mnich", "Matthias", ""], ["Schulz", "Christian", ""], ["Strash", "Darren", ""]]}, {"id": "1905.11275", "submitter": "Hiroaki Shiokawa", "authors": "Hiroaki Shiokawa, Toshiyuki Amagasa, and Hiroyuki Kitagawa", "title": "Scaling Fine-grained Modularity Clustering for Massive Graphs", "comments": "Accepted by IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modularity clustering is an essential tool to understand complicated graphs.\nHowever, existing methods are not applicable to massive graphs due to two\nserious weaknesses. (1) It is difficult to fully reproduce ground-truth\nclusters due to the resolution limit problem. (2) They are computationally\nexpensive because all nodes and edges must be computed iteratively. This paper\nproposes gScarf, which outputs fine-grained clusters within a short running\ntime. To overcome the aforementioned weaknesses, gScarf dynamically prunes\nunnecessary nodes and edges, ensuring that it captures fine-grained clusters.\nExperiments show that gScarf outperforms existing methods in terms of running\ntime while finding clusters with high accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 14:51:56 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Shiokawa", "Hiroaki", ""], ["Amagasa", "Toshiyuki", ""], ["Kitagawa", "Hiroyuki", ""]]}, {"id": "1905.11287", "submitter": "Ingo Scholtes", "authors": "Luka V. Petrovic and Ingo Scholtes", "title": "Counting Causal Paths in Big Times Series Data on Networks", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph or network representations are an important foundation for data mining\nand machine learning tasks in relational data. Many tools of network analysis,\nlike centrality measures, information ranking, or cluster detection rest on the\nassumption that links capture direct influence, and that paths represent\npossible indirect influence. This assumption is invalidated in time-stamped\nnetwork data capturing, e.g., dynamic social networks, biological sequences or\nfinancial transactions. In such data, for two time-stamped links (A,B) and\n(B,C) the chronological ordering and timing determines whether a causal path\nfrom node A via B to C exists. A number of works has shown that for that reason\nnetwork analysis cannot be directly applied to time-stamped network data.\nExisting methods to address this issue require statistics on causal paths,\nwhich is computationally challenging for big data sets.\n  Addressing this problem, we develop an efficient algorithm to count causal\npaths in time-stamped network data. Applying it to empirical data, we show that\nour method is more efficient than a baseline method implemented in an\nOpenSource data analytics package. Our method works efficiently for different\nvalues of the maximum time difference between consecutive links of a causal\npath and supports streaming scenarios. With it, we are closing a gap that\nhinders an efficient analysis of big time series data on complex networks.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 15:17:46 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Petrovic", "Luka V.", ""], ["Scholtes", "Ingo", ""]]}, {"id": "1905.11350", "submitter": "Igor Shinkar", "authors": "Lucas Boczkowski, Igor Shinkar", "title": "On Mappings on the Hypercube with Small Average Stretch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $A \\subseteq \\{0,1\\}^n$ be a set of size $2^{n-1}$, and let $\\phi \\colon\n\\{0,1\\}^{n-1} \\to A$ be a bijection. We define the average stretch of $\\phi$ as\n${\\sf avgStretch}(\\phi) = {\\mathbb E}[{\\sf dist}(\\phi(x),\\phi(x'))]$, where the\nexpectation is taken over uniformly random $x,x' \\in \\{0,1\\}^{n-1}$ that differ\nin exactly one coordinate. In this paper we continue the line of research\nstudying mappings on the discrete hypercube with small average stretch. We\nprove the following results.\n  (1) For any set $A \\subseteq \\{0,1\\}^n$ of density $1/2$ there exists a\nbijection $\\phi_A \\colon \\{0,1\\}^{n-1} \\to A$ such that ${\\sf\navgstretch}(\\phi_A) = O(\\sqrt{n})$.\n  (2) For $n = 3^k$ let $A_{{\\sf rec\\text{-}maj}} = \\{x \\in \\{0,1\\}^n : {\\sf\nrec\\text{-}maj}(x) = 1\\}$, where ${\\sf rec\\text{-}maj} : \\{0,1\\}^n \\to \\{0,1\\}$\nis the function recursive majority of 3's. There exists a bijection $\\phi_{{\\sf\nrec\\text{-}maj}} \\colon \\{0,1\\}^{n-1} \\to A_{\\sf rec\\text{-}maj}$ such that\n${\\sf avgstretch}(\\phi_{\\sf rec\\text{-}maj}) = O(1)$.\n  (3) Let $A_{\\sf tribes} = \\{x \\in \\{0,1\\}^n : {\\sf tribes}(x) = 1\\}$. There\nexists a bijection $\\phi_{{\\sf tribes}} \\colon \\{0,1\\}^{n-1} \\to A_{\\sf\ntribes}$ such that ${\\sf avgstretch}(\\phi_{{\\sf tribes}}) = O(\\log(n))$.\n  These results answer the questions raised by Benjamini et al.\\ (FOCS 2014).\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 17:29:26 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Boczkowski", "Lucas", ""], ["Shinkar", "Igor", ""]]}, {"id": "1905.11512", "submitter": "Sanjeev Khanna", "authors": "Julia Chuzhoy and Sanjeev Khanna", "title": "A New Algorithm for Decremental Single-Source Shortest Paths with\n  Applications to Vertex-Capacitated Flow and Cut Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the vertex-decremental Single-Source Shortest Paths (SSSP) problem:\ngiven an undirected graph $G=(V,E)$ with lengths $\\ell(e)\\geq 1$ on its edges\nand a source vertex $s$, we need to support (approximate) shortest-path queries\nin $G$, as $G$ undergoes vertex deletions. In a shortest-path query, given a\nvertex $v$, we need to return a path connecting $s$ to $v$, whose length is at\nmost $(1+\\epsilon)$ times the length of the shortest such path, where\n$\\epsilon$ is a given accuracy parameter. The problem has many applications,\nfor example to flow and cut problems in vertex-capacitated graphs.\n  Our main result is a randomized algorithm for vertex-decremental SSSP with\ntotal expected update time $O(n^{2+o(1)}\\log L)$, that responds to each\nshortest-path query in $O(n\\log L)$ time in expectation, returning a\n$(1+\\epsilon)$-approximate shortest path. The algorithm works against an\nadaptive adversary. The main technical ingredient of our algorithm is an\n$\\tilde O(|E(G)|+ n^{1+o(1)})$-time algorithm to compute a \\emph{core\ndecomposition} of a given dense graph $G$, which allows us to compute short\npaths between pairs of query vertices in $G$ efficiently. We believe that this\ncore decomposition algorithm may be of independent interest. We use our result\nfor vertex-decremental SSSP to obtain $(1+\\epsilon)$-approximation algorithms\nfor maximum $s$-$t$ flow and minimum $s$-$t$ cut in vertex-capacitated graphs,\nin expected time $n^{2+o(1)}$, and an $O(\\log^4n)$-approximation algorithm for\nthe vertex version of the sparsest cut problem with expected running time\n$n^{2+o(1)}$. These results improve upon the previous best known results for\nthese problems in the regime where $m= \\omega(n^{1.5 + o(1)})$.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 21:23:18 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Chuzhoy", "Julia", ""], ["Khanna", "Sanjeev", ""]]}, {"id": "1905.11566", "submitter": "Qiong Wu", "authors": "Qiong Wu, Felix Ming Fai Wong, Zhenming Liu, Yanhua Li, Varun Kanade", "title": "Adaptive Reduced Rank Regression", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the low rank regression problem $\\my = M\\mx + \\epsilon$, where $\\mx$\nand $\\my$ are $d_1$ and $d_2$ dimensional vectors respectively. We consider the\nextreme high-dimensional setting where the number of observations $n$ is less\nthan $d_1 + d_2$. Existing algorithms are designed for settings where $n$ is\ntypically as large as $\\Rank(M)(d_1+d_2)$. This work provides an efficient\nalgorithm which only involves two SVD, and establishes statistical guarantees\non its performance. The algorithm decouples the problem by first estimating the\nprecision matrix of the features, and then solving the matrix denoising\nproblem. To complement the upper bound, we introduce new techniques for\nestablishing lower bounds on the performance of any algorithm for this problem.\nOur preliminary experiments confirm that our algorithm often out-performs\nexisting baselines, and is always at least competitive.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 01:47:31 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 20:51:44 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 21:30:29 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wu", "Qiong", ""], ["Wong", "Felix Ming Fai", ""], ["Liu", "Zhenming", ""], ["Li", "Yanhua", ""], ["Kanade", "Varun", ""]]}, {"id": "1905.11573", "submitter": "Philipp Bamberger", "authors": "Philipp Bamberger, Mohsen Ghaffari, Fabian Kuhn, Yannic Maus, Jara\n  Uitto", "title": "On the Complexity of Distributed Splitting Problems", "comments": null, "journal-ref": null, "doi": "10.1145/3293611.3331630", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental open problems in the area of distributed graph\nalgorithms is the question of whether randomization is needed for efficient\nsymmetry breaking. While there are fast, $\\text{poly}\\log n$-time randomized\ndistributed algorithms for all of the classic symmetry breaking problems, for\nmany of them, the best deterministic algorithms are almost exponentially\nslower. The following basic local splitting problem, which is known as the\n\\emph{weak splitting} problem takes a central role in this context: Each node\nof a graph $G=(V,E)$ has to be colored red or blue such that each node of\nsufficiently large degree has at least one node of each color among its\nneighbors. Ghaffari, Kuhn, and Maus [STOC '17] showed that this seemingly\nsimple problem is complete w.r.t. the above fundamental open question in the\nfollowing sense: If there is an efficient $\\text{poly}\\log n$-time determinstic\ndistributed algorithm for weak splitting, then there is such an algorithm for\nall locally checkable graph problems for which an efficient randomized\nalgorithm exists. In this paper, we investigate the distributed complexity of\nweak splitting and some closely related problems. E.g., we obtain efficient\nalgorithms for special cases of weak splitting, where the graph is nearly\nregular. In particular, we show that if $\\delta$ and $\\Delta$ are the minimum\nand maximum degrees of $G$ and if $\\delta=\\Omega(\\log n)$, weak splitting can\nbe solved deterministically in time\n$O\\big(\\frac{\\Delta}{\\delta}\\cdot\\text{poly}(\\log n)\\big)$. Further, if $\\delta\n= \\Omega(\\log\\log n)$ and $\\Delta\\leq 2^{\\varepsilon\\delta}$, there is a\nrandomized algorithm with time complexity\n$O\\big(\\frac{\\Delta}{\\delta}\\cdot\\text{poly}(\\log\\log n)\\big)$.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 02:18:53 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Bamberger", "Philipp", ""], ["Ghaffari", "Mohsen", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""], ["Uitto", "Jara", ""]]}, {"id": "1905.11580", "submitter": "Xin Yang", "authors": "Michael B. Cohen, Ben Cousins, Yin Tat Lee, Xin Yang", "title": "A near-optimal algorithm for approximating the John Ellipsoid", "comments": "COLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a simple and efficient algorithm for approximating the John\nEllipsoid of a symmetric polytope. Our algorithm is near optimal in the sense\nthat our time complexity matches the current best verification algorithm. We\nalso provide the MATLAB code for further research.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 02:56:40 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 11:21:03 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Cohen", "Michael B.", ""], ["Cousins", "Ben", ""], ["Lee", "Yin Tat", ""], ["Yang", "Xin", ""]]}, {"id": "1905.11612", "submitter": "Arnab Bhattacharyya", "authors": "Arnab Bhattacharyya, Philips George John, Suprovat Ghoshal, Raghu Meka", "title": "Average Bias and Polynomial Sources", "comments": "We found out one of the main results has a much easier and direct\n  proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify a new notion of pseudorandomness for randomness sources, which we\ncall the average bias. Given a distribution $Z$ over $\\{0,1\\}^n$, its average\nbias is: $b_{\\text{av}}(Z) =2^{-n} \\sum_{c \\in \\{0,1\\}^n} |\\mathbb{E}_{z \\sim\nZ}(-1)^{\\langle c, z\\rangle}|$. A source with average bias at most $2^{-k}$ has\nmin-entropy at least $k$, and so low average bias is a stronger condition than\nhigh min-entropy. We observe that the inner product function is an extractor\nfor any source with average bias less than $2^{-n/2}$.\n  The notion of average bias especially makes sense for polynomial sources,\ni.e., distributions sampled by low-degree $n$-variate polynomials over\n$\\mathbb{F}_2$. For the well-studied case of affine sources, it is easy to see\nthat min-entropy $k$ is exactly equivalent to average bias of $2^{-k}$. We show\nthat for quadratic sources, min-entropy $k$ implies that the average bias is at\nmost $2^{-\\Omega(\\sqrt{k})}$. We use this relation to design dispersers for\nseparable quadratic sources with a min-entropy guarantee.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 05:07:40 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 07:30:10 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["John", "Philips George", ""], ["Ghoshal", "Suprovat", ""], ["Meka", "Raghu", ""]]}, {"id": "1905.11743", "submitter": "Thomas Espitau", "authors": "Thomas Espitau and Antoine Joux", "title": "Certified lattice reduction", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA math.NT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quadratic form reduction and lattice reduction are fundamental tools in\ncomputational number theory and in computer science, especially in\ncryptography. The celebrated Lenstra-Lenstra-Lov\\'asz reduction algorithm\n(so-called LLL) has been improved in many ways through the past decades and\nremains one of the central methods used for reducing integral lattice basis. In\nparticular, its floating-point variants-where the rational arithmetic required\nby Gram-Schmidt orthogonalization is replaced by floating-point arithmetic-are\nnow the fastest known. However, the systematic study of the reduction theory of\nreal quadratic forms or, more generally, of real lattices is not widely\nrepresented in the literature. When the problem arises, the lattice is usually\nreplaced by an integral approximation of (a multiple of) the original lattice,\nwhich is then reduced. While practically useful and proven in some special\ncases, this method doesn't offer any guarantee of success in general. In this\nwork, we present an adaptive-precision version of a generalized LLL algorithm\nthat covers this case in all generality. In particular, we replace\nfloating-point arithmetic by Interval Arithmetic to certify the behavior of the\nalgorithm. We conclude by giving a typical application of the result in\nalgebraic number theory for the reduction of ideal lattices in number fields.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 11:16:08 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Espitau", "Thomas", ""], ["Joux", "Antoine", ""]]}, {"id": "1905.11822", "submitter": "Mauricio Toro", "authors": "Vincent Arcila, Isabel Piedrahita, Mauricio Toro", "title": "Robotic bees: Algorithms for collision detection and prevention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the following paper we will discuss data structures suited for distance\nthreshold queries keeping in mind real life application such as collision\ndetection on robotic bees. We will focus on spatial hashes designed to store 3D\npoints and capable of fastly determining which of them surpass a specific\nthreshold from any other. In this paper we will discuss related literature,\nexplain in depth the data structure chosen with its design criteria, operations\nand speed and memory efficiency analysis.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 02:43:41 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 22:21:25 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Arcila", "Vincent", ""], ["Piedrahita", "Isabel", ""], ["Toro", "Mauricio", ""]]}, {"id": "1905.11830", "submitter": "Nathaniel Lahn", "authors": "Nathaniel Lahn, Deepika Mulchandani, Sharath Raghvendra", "title": "A Graph Theoretic Additive Approximation of Optimal Transport", "comments": "Final version to appear in NeurIPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportation cost is an attractive similarity measure between probability\ndistributions due to its many useful theoretical properties. However, solving\noptimal transport exactly can be prohibitively expensive. Therefore, there has\nbeen significant effort towards the design of scalable approximation\nalgorithms. Previous combinatorial results [Sharathkumar, Agarwal STOC '12,\nAgarwal, Sharathkumar STOC '14] have focused primarily on the design of\nnear-linear time multiplicative approximation algorithms. There has also been\nan effort to design approximate solutions with additive errors [Cuturi NIPS\n'13, Altschuler \\etal\\ NIPS '17, Dvurechensky \\etal\\, ICML '18, Quanrud, SOSA\n'19] within a time bound that is linear in the size of the cost matrix and\npolynomial in $C/\\delta$; here $C$ is the largest value in the cost matrix and\n$\\delta$ is the additive error. We present an adaptation of the classical graph\nalgorithm of Gabow and Tarjan and provide a novel analysis of this algorithm\nthat bounds its execution time by $O(\\frac{n^2 C}{\\delta}+\n\\frac{nC^2}{\\delta^2})$. Our algorithm is extremely simple and executes, for an\narbitrarily small constant $\\varepsilon$, only $\\lfloor\n\\frac{2C}{(1-\\varepsilon)\\delta}\\rfloor + 1$ iterations, where each iteration\nconsists only of a Dijkstra-type search followed by a depth-first search. We\nalso provide empirical results that suggest our algorithm is competitive with\nrespect to a sequential implementation of the Sinkhorn algorithm in execution\ntime. Moreover, our algorithm quickly computes a solution for very small values\nof $\\delta$ whereas Sinkhorn algorithm slows down due to numerical instability.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 14:08:27 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 16:04:16 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 18:06:04 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Lahn", "Nathaniel", ""], ["Mulchandani", "Deepika", ""], ["Raghvendra", "Sharath", ""]]}, {"id": "1905.11838", "submitter": "Palash Dey", "authors": "Palash Dey, Neeldhara Misra, Swaprava Nath, and Garima Shakya", "title": "A Parameterized Perspective on Protecting Elections", "comments": "To appear in IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.CY cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the parameterized complexity of the optimal defense and optimal\nattack problems in voting. In both the problems, the input is a set of voter\ngroups (every voter group is a set of votes) and two integers $k_a$ and $k_d$\ncorresponding to respectively the number of voter groups the attacker can\nattack and the number of voter groups the defender can defend. A voter group\ngets removed from the election if it is attacked but not defended. In the\noptimal defense problem, we want to know if it is possible for the defender to\ncommit to a strategy of defending at most $k_d$ voter groups such that, no\nmatter which $k_a$ voter groups the attacker attacks, the outcome of the\nelection does not change. In the optimal attack problem, we want to know if it\nis possible for the attacker to commit to a strategy of attacking $k_a$ voter\ngroups such that, no matter which $k_d$ voter groups the defender defends, the\noutcome of the election is always different from the original (without any\nattack) one.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 14:20:33 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Dey", "Palash", ""], ["Misra", "Neeldhara", ""], ["Nath", "Swaprava", ""], ["Shakya", "Garima", ""]]}, {"id": "1905.11877", "submitter": "Charles Argue", "authors": "C.J. Argue, Anupam Gupta, Guru Guruganesh, Ziye Tang", "title": "Chasing Convex Bodies with Linear Competitive Ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of chasing convex bodies online: given a sequence of\nconvex bodies $K_t\\subseteq \\mathbb{R}^d$ the algorithm must respond with\npoints $x_t\\in K_t$ in an online fashion (i.e., $x_t$ is chosen before\n$K_{t+1}$ is revealed). The objective is to minimize the sum of distances\nbetween successive points in this sequence. Bubeck et al. (STOC 2019) gave a\n$2^{O(d)}$-competitive algorithm for this problem. We give an algorithm that is\n$O(\\min(d, \\sqrt{d \\log T}))$-competitive for any sequence of length $T$.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 15:20:24 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 21:54:12 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Argue", "C. J.", ""], ["Gupta", "Anupam", ""], ["Guruganesh", "Guru", ""], ["Tang", "Ziye", ""]]}, {"id": "1905.11888", "submitter": "Jayadev Acharya", "authors": "Jayadev Acharya, Ziteng Sun", "title": "Communication Complexity in Locally Private Distribution Estimation and\n  Heavy Hitters", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of distribution estimation and heavy hitter\n(frequency) estimation under privacy and communication constraints. While these\nconstraints have been studied separately, optimal schemes for one are\nsub-optimal for the other. We propose a sample-optimal $\\varepsilon$-locally\ndifferentially private (LDP) scheme for distribution estimation, where each\nuser communicates only one bit, and requires no public randomness. We show that\nHadamard Response, a recently proposed scheme for $\\varepsilon$-LDP\ndistribution estimation is also utility-optimal for heavy hitter estimation.\nFinally, we show that unlike distribution estimation, without public randomness\nwhere only one bit suffices, any heavy hitter estimation algorithm that\ncommunicates $o(\\min \\{\\log n, \\log k\\})$ bits from each user cannot be\noptimal.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 15:33:10 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Acharya", "Jayadev", ""], ["Sun", "Ziteng", ""]]}, {"id": "1905.11924", "submitter": "Ari Kobren", "authors": "Ari Kobren, Barna Saha, Andrew McCallum", "title": "Paper Matching with Local Fairness Constraints", "comments": "Appears at KDD 2019 Research Track, 20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically matching reviewers to papers is a crucial step of the peer\nreview process for venues receiving thousands of submissions. Unfortunately,\ncommon paper matching algorithms often construct matchings suffering from two\ncritical problems: (1) the group of reviewers assigned to a paper do not\ncollectively possess sufficient expertise, and (2) reviewer workloads are\nhighly skewed. In this paper, we propose a novel local fairness formulation of\npaper matching that directly addresses both of these issues. Since optimizing\nour formulation is not always tractable, we introduce two new algorithms,\nFairIR and FairFlow, for computing fair matchings that approximately optimize\nthe new formulation. FairIR solves a relaxation of the local fairness\nformulation and then employs a rounding technique to construct a valid matching\nthat provably maximizes the objective and only compromises on fairness with\nrespect to reviewer loads and papers by a small constant. In contrast, FairFlow\nis not provably guaranteed to produce fair matchings, however it can be 2x as\nefficient as FairIR and an order of magnitude faster than matching algorithms\nthat directly optimize for fairness. Empirically, we demonstrate that both\nFairIR and FairFlow improve fairness over standard matching algorithms on real\nconference data. Moreover, in comparison to state-of-the-art matching\nalgorithms that optimize for fairness only, FairIR achieves higher objective\nscores, FairFlow achieves competitive fairness, and both are capable of more\nevenly allocating reviewers.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 16:36:51 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Kobren", "Ari", ""], ["Saha", "Barna", ""], ["McCallum", "Andrew", ""]]}, {"id": "1905.11947", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne, Gautam Kamath, Audra McMillan, Jonathan Ullman,\n  Lydia Zakynthinou", "title": "Private Identity Testing for High-Dimensional Distributions", "comments": "Improved the bounds and the writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present novel differentially private identity\n(goodness-of-fit) testers for natural and widely studied classes of\nmultivariate product distributions: Gaussians in $\\mathbb{R}^d$ with known\ncovariance and product distributions over $\\{\\pm 1\\}^{d}$. Our testers have\nimproved sample complexity compared to those derived from previous techniques,\nand are the first testers whose sample complexity matches the order-optimal\nminimax sample complexity of $O(d^{1/2}/\\alpha^2)$ in many parameter regimes.\nWe construct two types of testers, exhibiting tradeoffs between sample\ncomplexity and computational complexity. Finally, we provide a two-way\nreduction between testing a subclass of multivariate product distributions and\ntesting univariate distributions, and thereby obtain upper and lower bounds for\ntesting this subclass of product distributions.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 17:07:38 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 20:33:34 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["Kamath", "Gautam", ""], ["McMillan", "Audra", ""], ["Ullman", "Jonathan", ""], ["Zakynthinou", "Lydia", ""]]}, {"id": "1905.11968", "submitter": "Mark Sellke", "authors": "Mark Sellke", "title": "Chasing Convex Bodies Optimally", "comments": "v2 adds section 6 unifying our proof with another concurrent proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the chasing convex bodies problem, an online player receives a request\nsequence of $N$ convex sets $K_1,\\dots, K_N$ contained in a normed space\n$\\mathbb R^d$. The player starts at $x_0\\in \\mathbb R^d$, and after observing\neach $K_n$ picks a new point $x_n\\in K_n$. At each step the player pays a\nmovement cost of $||x_n-x_{n-1}||$. The player aims to maintain a constant\ncompetitive ratio against the minimum cost possible in hindsight, i.e. knowing\nall requests in advance. The existence of a finite competitive ratio for convex\nbody chasing was first conjectured in 1991 by Friedman and Linial. This\nconjecture was recently resolved with an exponential $2^{O(d)}$ upper bound on\nthe competitive ratio.\n  In this paper, we drastically improve the exponential upper bound. We give an\nalgorithm achieving competitive ratio $d$ for arbitrary normed spaces, which is\nexactly tight for $\\ell^{\\infty}$. In Euclidean space, our algorithm achieves\nnearly optimal competitive ratio $O(\\sqrt{d\\log N})$, compared to a lower bound\nof $\\sqrt{d}$. Our approach extends another recent work which chases nested\nconvex bodies using the classical Steiner point of a convex body. We define the\nfunctional Steiner point of a convex function and apply it to the work function\nto obtain our algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 17:44:53 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 18:16:37 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Sellke", "Mark", ""]]}, {"id": "1905.12143", "submitter": "Naama Ben-David", "authors": "Marcos K. Aguilera, Naama Ben-David, Rachid Guerraoui, Virendra\n  Marathe, Igor Zablotchi", "title": "The Impact of RDMA on Agreement", "comments": "Full version of PODC'19 paper, strengthened broadcast algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote Direct Memory Access (RDMA) is becoming widely available in data\ncenters. This technology allows a process to directly read and write the memory\nof a remote host, with a mechanism to control access permissions. In this\npaper, we study the fundamental power of these capabilities. We consider the\nwell-known problem of achieving consensus despite failures, and find that RDMA\ncan improve the inherent trade-off in distributed computing between failure\nresilience and performance. Specifically, we show that RDMA allows algorithms\nthat simultaneously achieve high resilience and high performance, while\ntraditional algorithms had to choose one or another. With Byzantine failures,\nwe give an algorithm that only requires $n \\geq 2f_P + 1$ processes (where\n$f_P$ is the maximum number of faulty processes) and decides in two (network)\ndelays in common executions. With crash failures, we give an algorithm that\nonly requires $n \\geq f_P + 1$ processes and also decides in two delays. Both\nalgorithms tolerate a minority of memory failures inherent to RDMA, and they\nprovide safety in asynchronous systems and liveness with standard additional\nassumptions.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 00:26:25 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 18:50:01 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Aguilera", "Marcos K.", ""], ["Ben-David", "Naama", ""], ["Guerraoui", "Rachid", ""], ["Marathe", "Virendra", ""], ["Zablotchi", "Igor", ""]]}, {"id": "1905.12145", "submitter": "Marwa El Halabi", "authors": "Marwa El Halabi and Stefanie Jegelka", "title": "Optimal approximation for unconstrained non-submodular minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular function minimization is a well studied problem; existing\nalgorithms solve it exactly or up to arbitrary accuracy. However, in many\napplications, the objective function is not exactly submodular. No theoretical\nguarantees exist in this case. While submodular minimization algorithms rely on\nintricate connections between submodularity and convexity, we show that these\nrelations can be extended sufficiently to obtain approximation guarantees for\nnon-submodular minimization. In particular, we prove how a projected\nsubgradient method can perform well even for certain non-submodular functions.\n  This includes important examples, such as objectives for structured sparse\nlearning and variance reduction in Bayesian optimization. We also extend this\nresult to noisy function evaluations. Our algorithm works in the value oracle\nmodel. We prove that in this model, the approximation result we obtain is the\nbest possible with a subexponential number of queries.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 00:33:42 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 19:15:43 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 14:51:41 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Halabi", "Marwa El", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1905.12233", "submitter": "Yoichi Iwata", "authors": "Yoichi Iwata, Yusuke Kobayashi", "title": "Improved Analysis of Highest-Degree Branching for Feedback Vertex Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent empirical evaluations of exact algorithms for Feedback Vertex Set have\ndemonstrated the efficiency of a highest-degree branching algorithm with a\ndegree-based pruning heuristic. In this paper, we prove that this empirically\nfast algorithm runs in $O(3.460^k n)$ time, where $k$ is the solution size.\nThis improves the previous best $O(3.619^k n)$-time deterministic algorithm\nobtained by Kociumaka and Pilipczuk.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 06:10:26 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Iwata", "Yoichi", ""], ["Kobayashi", "Yusuke", ""]]}, {"id": "1905.12379", "submitter": "Philip Lazos", "authors": "Dimitris Fotakis, Loukas Kavouras, Panagiotis Kostopanagiotis, Philip\n  Lazos, Stratis Skoulakis, Nikolas Zarifis", "title": "Reallocating Multiple Facilities on the Line", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the multistage $K$-facility reallocation problem on the real line,\nwhere we maintain $K$ facility locations over $T$ stages, based on the\nstage-dependent locations of $n$ agents. Each agent is connected to the nearest\nfacility at each stage, and the facilities may move from one stage to another,\nto accommodate different agent locations. The objective is to minimize the\nconnection cost of the agents plus the total moving cost of the facilities,\nover all stages. $K$-facility reallocation was introduced by de Keijzer and\nWojtczak, where they mostly focused on the special case of a single facility.\nUsing an LP-based approach, we present a polynomial time algorithm that\ncomputes the optimal solution for any number of facilities. We also consider\nonline $K$-facility reallocation, where the algorithm becomes aware of agent\nlocations in a stage-by-stage fashion. By exploiting an interesting connection\nto the classical $K$-server problem, we present a constant-competitive\nalgorithm for $K = 2$ facilities.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 12:43:00 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Fotakis", "Dimitris", ""], ["Kavouras", "Loukas", ""], ["Kostopanagiotis", "Panagiotis", ""], ["Lazos", "Philip", ""], ["Skoulakis", "Stratis", ""], ["Zarifis", "Nikolas", ""]]}, {"id": "1905.12412", "submitter": "Zhize Li", "authors": "Guanghui Lan, Zhize Li, Yi Zhou", "title": "A unified variance-reduced accelerated gradient method for convex\n  optimization", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel randomized incremental gradient algorithm, namely,\nVAriance-Reduced Accelerated Gradient (Varag), for finite-sum optimization.\nEquipped with a unified step-size policy that adjusts itself to the value of\nthe condition number, Varag exhibits the unified optimal rates of convergence\nfor solving smooth convex finite-sum problems directly regardless of their\nstrong convexity. Moreover, Varag is the first accelerated randomized\nincremental gradient method that benefits from the strong convexity of the\ndata-fidelity term to achieve the optimal linear convergence. It also\nestablishes an optimal linear rate of convergence for solving a wide class of\nproblems only satisfying a certain error bound condition rather than strong\nconvexity. Varag can also be extended to solve stochastic finite-sum problems.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 13:20:27 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 17:39:13 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 22:11:01 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Lan", "Guanghui", ""], ["Li", "Zhize", ""], ["Zhou", "Yi", ""]]}, {"id": "1905.12442", "submitter": "Yariv Aizenbud", "authors": "Yariv Aizenbud and Boris Landa and Yoel Shkolnisky", "title": "Rank-one Multi-Reference Factor Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there is a growing need for processing methods aimed at\nextracting useful information from large datasets. In many cases the challenge\nis to discover a low-dimensional structure in the data, often concealed by the\nexistence of nuisance parameters and noise. Motivated by such challenges, we\nconsider the problem of estimating a signal from its scaled, cyclically-shifted\nand noisy observations. We focus on the particularly challenging regime of low\nsignal-to-noise ratio (SNR), where different observations cannot be\nshift-aligned. We show that an accurate estimation of the signal from its noisy\nobservations is possible, and derive a procedure which is proved to\nconsistently estimate the signal. The asymptotic sample complexity (the number\nof observations required to recover the signal) of the procedure is\n$1/\\operatorname{SNR}^4$. Additionally, we propose a procedure which is\nexperimentally shown to improve the sample complexity by a factor equal to the\nsignal's length. Finally, we present numerical experiments which demonstrate\nthe performance of our algorithms, and corroborate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 13:34:29 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 07:45:55 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Aizenbud", "Yariv", ""], ["Landa", "Boris", ""], ["Shkolnisky", "Yoel", ""]]}, {"id": "1905.12461", "submitter": "Yu Nakahata", "authors": "Yu Nakahata", "title": "On the Clique-Width of Unigraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clique-width is a well-studied graph parameter. For graphs of bounded\nclique-width, many problems that are NP-hard in general can be polynomial-time\nsolvable. The fact motivates many studies to investigate whether the\nclique-width of graphs in a certain class is bounded or not. We focus on\nunigraphs, that is, graphs uniquely determined by their degree sequences up to\nisomorphism. We show that every unigraph has clique-width at most 5. It follows\nthat many problems that are NP-hard in general are polynomial-time solvable for\nunigraphs.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 13:58:35 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Nakahata", "Yu", ""]]}, {"id": "1905.12477", "submitter": "Philipp Fischbeck", "authors": "Thomas Bl\\\"asius, Philipp Fischbeck, Tobias Friedrich, and Martin\n  Schirneck", "title": "Understanding the Effectiveness of Data Reduction in Public\n  Transportation Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a public transportation network of stations and connections, we want to\nfind a minimum subset of stations such that each connection runs through a\nselected station. Although this problem is NP-hard in general, real-world\ninstances are regularly solved almost completely by a set of simple reduction\nrules. To explain this behavior, we view transportation networks as hitting set\ninstances and identify two characteristic properties, locality and\nheterogeneity. We then devise a randomized model to generate hitting set\ninstances with adjustable properties. While the heterogeneity does influence\nthe effectiveness of the reduction rules, the generated instances show that\nlocality is the significant factor. Beyond that, we prove that the\neffectiveness of the reduction rules is independent of the underlying graph\nstructure. Finally, we show that high locality is also prevalent in instances\nfrom other domains, facilitating a fast computation of minimum hitting sets.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 14:12:42 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Bl\u00e4sius", "Thomas", ""], ["Fischbeck", "Philipp", ""], ["Friedrich", "Tobias", ""], ["Schirneck", "Martin", ""]]}, {"id": "1905.12730", "submitter": "Rina Panigrahy", "authors": "Badih Ghazi, Rina Panigrahy, Joshua R. Wang", "title": "Recursive Sketches for Modular Deep Learning", "comments": "Published in ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a mechanism to compute a sketch (succinct summary) of how a\ncomplex modular deep network processes its inputs. The sketch summarizes\nessential information about the inputs and outputs of the network and can be\nused to quickly identify key components and summary statistics of the inputs.\nFurthermore, the sketch is recursive and can be unrolled to identify\nsub-components of these components and so forth, capturing a potentially\ncomplicated DAG structure. These sketches erase gracefully; even if we erase a\nfraction of the sketch at random, the remainder still retains the `high-weight'\ninformation present in the original sketch. The sketches can also be organized\nin a repository to implicitly form a `knowledge graph'; it is possible to\nquickly retrieve sketches in the repository that are related to a sketch of\ninterest; arranged in this fashion, the sketches can also be used to learn\nemerging concepts by looking for new clusters in sketch space. Finally, in the\nscenario where we want to learn a ground truth deep network, we show that\naugmenting input/output pairs with these sketches can theoretically make it\neasier to do so.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 21:10:58 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 22:36:23 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Ghazi", "Badih", ""], ["Panigrahy", "Rina", ""], ["Wang", "Joshua R.", ""]]}, {"id": "1905.12753", "submitter": "Alessandro Epasto", "authors": "Sara Ahmadian, Alessandro Epasto, Ravi Kumar, Mohammad Mahdian", "title": "Clustering without Over-Representation", "comments": "10 pages, 6 figures, in KDD 2019", "journal-ref": "in Proceedings of The 25th ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining, KDD 2019", "doi": "10.1145/3292500.3330987", "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider clustering problems in which each point is endowed\nwith a color. The goal is to cluster the points to minimize the classical\nclustering cost but with the additional constraint that no color is\nover-represented in any cluster. This problem is motivated by practical\nclustering settings, e.g., in clustering news articles where the color of an\narticle is its source, it is preferable that no single news source dominates\nany cluster.\n  For the most general version of this problem, we obtain an algorithm that has\nprovable guarantees of performance; our algorithm is based on finding a\nfractional solution using a linear program and rounding the solution\nsubsequently. For the special case of the problem where no color has an\nabsolute majority in any cluster, we obtain a simpler combinatorial algorithm\nalso with provable guarantees. Experiments on real-world data shows that our\nalgorithms are effective in finding good clustering without\nover-representation.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 22:21:47 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Ahmadian", "Sara", ""], ["Epasto", "Alessandro", ""], ["Kumar", "Ravi", ""], ["Mahdian", "Mohammad", ""]]}, {"id": "1905.12778", "submitter": "Rajan Udwani", "authors": "Vineet Goyal and Rajan Udwani", "title": "Online Matching with Stochastic Rewards: Optimal Competitive Ratio via\n  Path Based Formulation", "comments": "Preliminary version in EC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of online matching with stochastic rewards is a generalization of\nthe online bipartite matching problem where each edge has a probability of\nsuccess. When a match is made it succeeds with the probability of the\ncorresponding edge. Introducing this model, Mehta and Panigrahi (FOCS 2012)\nfocused on the special case of identical edge probabilities. Comparing against\na deterministic offline LP, they showed that the Ranking algorithm of Karp et\nal. (STOC 1990) is 0.534 competitive and proposed a new online algorithm with\nan improved guarantee of $0.567$ for vanishingly small probabilities. For the\ncase of vanishingly small but heterogeneous probabilities Mehta et al. (SODA\n2015), gave a 0.534 competitive algorithm against the same LP benchmark. For\nthe more general vertex-weighted version of the problem, to the best of our\nknowledge, no results being $1/2$ were previously known even for identical\nprobabilities.\n  We focus on the vertex-weighted version and give two improvements. First, we\nshow that a natural generalization of the Perturbed-Greedy algorithm of\nAggarwal et al. (SODA 2011), is $(1-1/e)$ competitive when probabilities\ndecompose as a product of two factors, one corresponding to each vertex of the\nedge. This is the best achievable guarantee as it includes the case of\nidentical probabilities and in particular, the classical online bipartite\nmatching problem. Second, we give a deterministic $0.596$ competitive algorithm\nfor the previously well studied case of fully heterogeneous but vanishingly\nsmall edge probabilities. A key contribution of our approach is the use of\nnovel path-based analysis. This allows us to compare against the natural\nbenchmarks of adaptive offline algorithms that know the sequence of arrivals\nand the edge probabilities in advance, but not the outcomes of potential\nmatches.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 23:24:26 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 00:27:30 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 16:18:52 GMT"}, {"version": "v4", "created": "Thu, 13 Feb 2020 15:49:37 GMT"}, {"version": "v5", "created": "Mon, 24 Aug 2020 16:09:04 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Goyal", "Vineet", ""], ["Udwani", "Rajan", ""]]}, {"id": "1905.12854", "submitter": "Takuya Mieno", "authors": "Takuya Mieno, Dominik K\\\"oppl, Yuto Nakashima, Shunsuke Inenaga, Hideo\n  Bannai, Masayuki Takeda", "title": "Space-Efficient Algorithms for Computing Minimal/Shortest Unique\n  Substrings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a string $T$ of length $n$, a substring $u = T[i..j]$ of $T$ is called\na shortest unique substring (SUS) for an interval $[s,t]$ if (a) $u$ occurs\nexactly once in $T$, (b) $u$ contains the interval $[s,t]$ (i.e. $i \\leq s \\leq\nt \\leq j$), and (c) every substring $v$ of $T$ with $|v| < |u|$ containing\n$[s,t]$ occurs at least twice in $T$. Given a query interval $[s, t] \\subset\n[1, n]$, the interval SUS problem is to output all the SUSs for the interval\n$[s,t]$. In this article, we propose a $4n + o(n)$ bits data structure\nanswering an interval SUS query in output-sensitive $O(\\mathit{occ})$ time,\nwhere $\\mathit{occ}$ is the number of returned SUSs. Additionally, we focus on\nthe point SUS problem, which is the interval SUS problem for $s = t$. Here, we\npropose a $\\lceil (\\log_2{3} + 1)n \\rceil + o(n)$ bits data structure answering\na point SUS query in the same output-sensitive time. We also propose\nspace-efficient algorithms for computing the minimal unique substrings of $T$.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 05:07:09 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 05:29:07 GMT"}, {"version": "v3", "created": "Sun, 15 Sep 2019 05:28:41 GMT"}, {"version": "v4", "created": "Mon, 14 Sep 2020 04:49:46 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Mieno", "Takuya", ""], ["K\u00f6ppl", "Dominik", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1905.12913", "submitter": "Xuecheng Liu", "authors": "Xuecheng Liu, Luoyi Fu, Bo Jiang, Xiaojun Lin, Xinbing Wang", "title": "Information Source Detection with Limited Time Knowledge", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of utilizing network topology and partial\ntimestamps to detect the information source in a network. The problem incurs\nprohibitive cost under canonical maximum likelihood estimation (MLE) of the\nsource due to the exponential number of possible infection paths. Our main idea\nof source detection, however, is to approximate the MLE by an alternative\ninfection path based estimator, the essence of which is to identify the most\nlikely infection path that is consistent with observed timestamps. The source\nnode associated with that infection path is viewed as the estimated source\n$\\hat{v}$. We first study the case of tree topology, where by transforming the\ninfection path based estimator into a linear integer programming, we find a\nreduced search region that remarkably improves the time efficiency. Within this\nreduced search region, the estimator $\\hat{v}$ is provably always on a path\nwhich we term as \\emph{candidate path}. This notion enables us to analyze the\ndistribution of $d(v^{\\ast},\\hat{v})$, the error distance between $\\hat{v}$ and\nthe true source $v^{\\ast}$, on arbitrary tree, which allows us to obtain for\nthe first time, in the literature provable performance guarantee of the\nestimator under limited timestamps. Specifically, on the infinite $g$-regular\ntree with uniform sampled timestamps, we get a refined performance guarantee in\nthe sense of a constant bounded $d(v^{\\ast},\\hat{v})$. By virtue of time\nlabeled BFS tree, the estimator still performs fairly well when extended to\nmore general graphs. Experiments on both synthetic and real datasets further\ndemonstrate the superior performance of our proposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 08:39:22 GMT"}], "update_date": "2019-06-02", "authors_parsed": [["Liu", "Xuecheng", ""], ["Fu", "Luoyi", ""], ["Jiang", "Bo", ""], ["Lin", "Xiaojun", ""], ["Wang", "Xinbing", ""]]}, {"id": "1905.12987", "submitter": "Felipe A. Louza", "authors": "Felipe A. Louza, Sabrina Mantaci, Giovanni Manzini, Marinella\n  Sciortino, Guilherme P. Telles", "title": "Inducing the Lyndon Array", "comments": "Accepted to SPIRE'19", "journal-ref": null, "doi": "10.1007/978-3-030-32686-9_10", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a variant of the induced suffix sorting algorithm by\nNong (TOIS, 2013) that computes simultaneously the Lyndon array and the suffix\narray of a text in $O(n)$ time using $\\sigma + O(1)$ words of working space,\nwhere $n$ is the length of the text and $\\sigma$ is the alphabet size. Our\nresult improves the previous best space requirement for linear time computation\nof the Lyndon array. In fact, all the known linear algorithms for Lyndon array\ncomputation use suffix sorting as a preprocessing step and use $O(n)$ words of\nworking space in addition to the Lyndon array and suffix array. Experimental\nresults with real and synthetic datasets show that our algorithm is not only\nspace-efficient but also fast in practice.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 12:01:00 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 19:34:44 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Louza", "Felipe A.", ""], ["Mantaci", "Sabrina", ""], ["Manzini", "Giovanni", ""], ["Sciortino", "Marinella", ""], ["Telles", "Guilherme P.", ""]]}, {"id": "1905.13011", "submitter": "Pratyush Mahapatra", "authors": "Pratyush Mahapatra, Mark D. Hill, Michael M. Swift", "title": "Don't Persist All : Efficient Persistent Data Structures", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data structures used in software development have inbuilt redundancy to\nimprove software reliability and to speed up performance. Examples include a\nDoubly Linked List which allows a faster deletion due to the presence of the\nprevious pointer. With the introduction of Persistent Memory, storing the\nredundant data fields into persistent memory adds a significant write overhead,\nand reduces performance. In this work, we focus on three data structures -\nDoubly Linked List, B+Tree and Hashmap, and showcase alternate partly\npersistent implementations where we only store a limited set of data fields to\npersistent memory. After a crash/restart, we use the persistent data fields to\nrecreate the data structures along with the redundant data fields. We compare\nour implementation with the base implementation and show that we achieve\nspeedups around 5-20% for some data structures, and up to 165% for a\nflush-dominated data structure.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 17:56:55 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Mahapatra", "Pratyush", ""], ["Hill", "Mark D.", ""], ["Swift", "Michael M.", ""]]}, {"id": "1905.13064", "submitter": "Lum Ramabaja", "authors": "Lum Ramabaja", "title": "The Bloom Clock", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bloom clock is a space-efficient, probabilistic data structure designed\nto determine the partial order of events in highly distributed systems. The\nbloom clock, like the vector clock, can autonomously detect causality\nviolations by comparing its logical timestamps. Unlike the vector clock, the\nspace complexity of the bloom clock does not depend on the number of nodes in a\nsystem. Instead it depends on a set of chosen parameters that determine its\nconfidence interval, i.e. false positive rate. To reduce the space complexity\nfrom which the vector clock suffers, the bloom clock uses a 'moving window' in\nwhich the partial order of events can be inferred with high confidence. If two\nclocks are not comparable, the bloom clock can always deduce it, i.e. false\nnegatives are not possible. If two clocks are comparable, the bloom clock can\ncalculate the confidence of that statement, i.e. it can compute the false\npositive rate between comparable pairs of clocks. By choosing an acceptable\nthreshold for the false positive rate, the bloom clock can properly compare the\norder of its timestamps, with that of other nodes in a highly accurate and\nspace efficient way.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 14:12:03 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 15:24:04 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2019 20:28:45 GMT"}, {"version": "v4", "created": "Sun, 9 Jun 2019 19:51:19 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Ramabaja", "Lum", ""]]}, {"id": "1905.13095", "submitter": "Leila Taghavi", "authors": "Salman Beigi, Leila Taghavi", "title": "Quantum Speedup Based on Classical Decision Trees", "comments": "32 pages, 2 figures. Matches published version", "journal-ref": "Quantum 4, 241 (2020)", "doi": "10.22331/q-2020-03-02-241", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lin and Lin have recently shown how starting with a classical query algorithm\n(decision tree) for a function, we may find upper bounds on its quantum query\ncomplexity. More precisely, they have shown that given a decision tree for a\nfunction $f:\\{0,1\\}^n\\to[m]$ whose input can be accessed via queries to its\nbits, and a guessing algorithm that predicts answers to the queries, there is a\nquantum query algorithm for $f$ which makes at most $O(\\sqrt{GT})$ quantum\nqueries where $T$ is the depth of the decision tree and $G$ is the maximum\nnumber of mistakes of the guessing algorithm. In this paper we give a simple\nproof of and generalize this result for functions $f:[\\ell]^n \\to [m]$ with\nnon-binary input as well as output alphabets. Our main tool for this\ngeneralization is non-binary span program which has recently been developed for\nnon-binary functions, and the dual adversary bound. As applications of our main\nresult we present several quantum query upper bounds, some of which are new. In\nparticular, we show that topological sorting of vertices of a directed graph\n$\\mathcal{G}$ can be done with $O(n^{3/2})$ quantum queries in the adjacency\nmatrix model. Also, we show that the quantum query complexity of the maximum\nbipartite matching is upper bounded by $O(n^{3/4}\\sqrt m + n)$ in the adjacency\nlist model.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 15:16:40 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 17:37:25 GMT"}, {"version": "v3", "created": "Fri, 28 Feb 2020 16:32:57 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Beigi", "Salman", ""], ["Taghavi", "Leila", ""]]}, {"id": "1905.13229", "submitter": "Gautam Kamath", "authors": "Mark Bun, Gautam Kamath, Thomas Steinke, Zhiwei Steven Wu", "title": "Private Hypothesis Selection", "comments": "Appeared in NeurIPS 2019. Final version to appear in IEEE\n  Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a differentially private algorithm for hypothesis selection. Given\nsamples from an unknown probability distribution $P$ and a set of $m$\nprobability distributions $\\mathcal{H}$, the goal is to output, in a\n$\\varepsilon$-differentially private manner, a distribution from $\\mathcal{H}$\nwhose total variation distance to $P$ is comparable to that of the best such\ndistribution (which we denote by $\\alpha$). The sample complexity of our basic\nalgorithm is $O\\left(\\frac{\\log m}{\\alpha^2} + \\frac{\\log m}{\\alpha\n\\varepsilon}\\right)$, representing a minimal cost for privacy when compared to\nthe non-private algorithm. We also can handle infinite hypothesis classes\n$\\mathcal{H}$ by relaxing to $(\\varepsilon,\\delta)$-differential privacy.\n  We apply our hypothesis selection algorithm to give learning algorithms for a\nnumber of natural distribution classes, including Gaussians, product\ndistributions, sums of independent random variables, piecewise polynomials, and\nmixture classes. Our hypothesis selection procedure allows us to generically\nconvert a cover for a class to a learning algorithm, complementing known\nlearning lower bounds which are in terms of the size of the packing number of\nthe class. As the covering and packing numbers are often closely related, for\nconstant $\\alpha$, our algorithms achieve the optimal sample complexity for\nmany classes of interest. Finally, we describe an application to private\ndistribution-free PAC learning.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 18:00:00 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 23:12:32 GMT"}, {"version": "v3", "created": "Sat, 25 Jan 2020 23:50:16 GMT"}, {"version": "v4", "created": "Thu, 31 Dec 2020 05:22:24 GMT"}, {"version": "v5", "created": "Mon, 4 Jan 2021 18:30:15 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Bun", "Mark", ""], ["Kamath", "Gautam", ""], ["Steinke", "Thomas", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1905.13272", "submitter": "Alina Ene", "authors": "Alina Ene, Huy L. Nguyen", "title": "Parallel Algorithm for Non-Monotone DR-Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we give a new parallel algorithm for the problem of maximizing\na non-monotone diminishing returns submodular function subject to a cardinality\nconstraint. For any desired accuracy $\\epsilon$, our algorithm achieves a $1/e\n- \\epsilon$ approximation using $O(\\log{n} \\log(1/\\epsilon) / \\epsilon^3)$\nparallel rounds of function evaluations. The approximation guarantee nearly\nmatches the best approximation guarantee known for the problem in the\nsequential setting and the number of parallel rounds is nearly-optimal for any\nconstant $\\epsilon$. Previous algorithms achieve worse approximation guarantees\nusing $\\Omega(\\log^2{n})$ parallel rounds. Our experimental evaluation suggests\nthat our algorithm obtains solutions whose objective value nearly matches the\nvalue obtained by the state of the art sequential algorithms, and it\noutperforms previous parallel algorithms in number of parallel rounds,\niterations, and solution quality.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 19:35:18 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "1905.13283", "submitter": "Dylan Foster", "authors": "Dylan J. Foster and Andrej Risteski", "title": "Sum-of-squares meets square loss: Fast rates for agnostic tensor\n  completion", "comments": "To appear at COLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study tensor completion in the agnostic setting. In the classical tensor\ncompletion problem, we receive $n$ entries of an unknown rank-$r$ tensor and\nwish to exactly complete the remaining entries. In agnostic tensor completion,\nwe make no assumption on the rank of the unknown tensor, but attempt to predict\nunknown entries as well as the best rank-$r$ tensor.\n  For agnostic learning of third-order tensors with the square loss, we give\nthe first polynomial time algorithm that obtains a \"fast\" (i.e., $O(1/n)$-type)\nrate improving over the rate obtained by reduction to matrix completion. Our\nprediction error rate to compete with the best $d\\times{}d\\times{}d$ tensor of\nrank-$r$ is $\\tilde{O}(r^{2}d^{3/2}/n)$. We also obtain an exact oracle\ninequality that trades off estimation and approximation error.\n  Our algorithm is based on the degree-six sum-of-squares relaxation of the\ntensor nuclear norm. The key feature of our analysis is to show that a certain\ncharacterization for the subgradient of the tensor nuclear norm can be encoded\nin the sum-of-squares proof system. This unlocks the standard toolbox for\nlocalization of empirical processes under the square loss, and allows us to\nestablish restricted eigenvalue-type guarantees for various tensor regression\nmodels, with tensor completion as a special case. The new analysis of the\nrelaxation complements Barak and Moitra (2016), who gave slow rates for\nagnostic tensor completion, and Potechin and Steurer (2017), who gave exact\nrecovery guarantees for the noiseless setting. Our techniques are\nuser-friendly, and we anticipate that they will find use elsewhere.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 20:05:13 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Foster", "Dylan J.", ""], ["Risteski", "Andrej", ""]]}, {"id": "1905.13371", "submitter": "Yusuke Kobayashi", "authors": "Satoru Iwata and Yusuke Kobayashi", "title": "A Weighted Linear Matroid Parity Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The matroid parity (or matroid matching) problem, introduced as a common\ngeneralization of matching and matroid intersection problems, is so general\nthat it requires an exponential number of oracle calls. Nevertheless, Lov\\'asz\n(1980) showed that this problem admits a min-max formula and a polynomial\nalgorithm for linearly represented matroids. Since then efficient algorithms\nhave been developed for the linear matroid parity problem. In this paper, we\npresent a combinatorial, deterministic, polynomial-time algorithm for the\nweighted linear matroid parity problem. The algorithm builds on a polynomial\nmatrix formulation using Pfaffian and adopts a primal-dual approach based on\nthe augmenting path algorithm of Gabow and Stallmann (1986) for the unweighted\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 01:24:56 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Iwata", "Satoru", ""], ["Kobayashi", "Yusuke", ""]]}, {"id": "1905.13415", "submitter": "Elias Stehle", "authors": "Elias Stehle and Hans-Arno Jacobsen", "title": "ParPaRaw: Massively Parallel Parsing of Delimiter-Separated Raw Data", "comments": null, "journal-ref": "PVLDB, 13(5): 616-628, 2020", "doi": "10.14778/3377369.3377372", "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsing is essential for a wide range of use cases, such as stream\nprocessing, bulk loading, and in-situ querying of raw data. Yet, the\ncompute-intense step often constitutes a major bottleneck in the data ingestion\npipeline, since parsing of inputs that require more involved parsing rules is\nchallenging to parallelise. This work proposes a massively parallel algorithm\nfor parsing delimiter-separated data formats on GPUs. Other than the\nstate-of-the-art, the proposed approach does not require an initial sequential\npass over the input to determine a thread's parsing context. That is, how a\nthread, beginning somewhere in the middle of the input, should interpret a\ncertain symbol (e.g., whether to interpret a comma as a delimiter or as part of\na larger string enclosed in double-quotes). Instead of tailoring the approach\nto a single format, we are able to perform a massively parallel FSM simulation,\nwhich is more flexible and powerful, supporting more expressive parsing rules\nwith general applicability. Achieving a parsing rate of as much as 14.2 GB/s,\nour experimental evaluation on a GPU with 3584 cores shows that the presented\napproach is able to scale to thousands of cores and beyond. With an end-to-end\nstreaming approach, we are able to exploit the full-duplex capabilities of the\nPCIe bus and hide latency from data transfers. Considering the end-to-end\nperformance, the algorithm parses 4.8 GB in as little as 0.44 seconds,\nincluding data transfers.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 05:04:39 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 04:52:33 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Stehle", "Elias", ""], ["Jacobsen", "Hans-Arno", ""]]}, {"id": "1905.13492", "submitter": "Conor McMeel", "authors": "Conor McMeel, Panos Parpas", "title": "Majorisation-minimisation algorithms for minimising the difference\n  between lattice submodular functions", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimising functions represented as a difference\nof lattice submodular functions. We propose analogues to the SupSub, SubSup and\nModMod routines for lattice submodular functions. We show that our\nmajorisation-minimisation algorithms produce iterates that monotonically\ndecrease, and that we converge to a local minimum. We also extend additive\nhardness results, and show that a broad range of functions can be expressed as\nthe difference of submodular functions.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 10:11:29 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["McMeel", "Conor", ""], ["Parpas", "Panos", ""]]}, {"id": "1905.13651", "submitter": "Adriano Fazzone", "authors": "Aris Anagnostopoulos, Luca Becchetti, Adriano Fazzone, Cristina\n  Menghini, Chris Schwiegelshohn", "title": "Principal Fairness: Removing Bias via Projections", "comments": "Partially supported by the ERC Advanced Grant 788893 AMDROMA\n  \"Algorithmic and Mechanism Design Research in Online Markets\" and MIUR PRIN\n  project ALGADIMAR \"Algorithms, Games, and Digital Markets\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing hidden bias in the data and ensuring fairness in algorithmic data\nanalysis has recently received significant attention. We complement several\nrecent papers in this line of research by introducing a general method to\nreduce bias in the data through random projections in a \"fair\" subspace.\n  We apply this method to densest subgraph problem. For densest subgraph, our\napproach based on fair projections allows to recover both theoretically and\nempirically an almost optimal, fair, dense subgraph hidden in the input data.\nWe also show that, under the small set expansion hypothesis, approximating this\nproblem beyond a factor of 2 is NP-hard and we show a polynomial time algorithm\nwith a matching approximation bound.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 14:52:56 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2020 01:01:05 GMT"}, {"version": "v3", "created": "Sat, 6 Mar 2021 00:02:06 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Anagnostopoulos", "Aris", ""], ["Becchetti", "Luca", ""], ["Fazzone", "Adriano", ""], ["Menghini", "Cristina", ""], ["Schwiegelshohn", "Chris", ""]]}]